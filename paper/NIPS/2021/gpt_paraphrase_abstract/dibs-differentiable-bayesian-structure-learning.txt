We propose a new method called DiBS for Bayesian structure learning, which allows us to infer the structure of a Bayesian network while considering the uncertainty involved. DiBS operates in a continuous latent probabilistic graph representation, making it different from existing methods. It can handle complex Bayesian network models with nonlinear dependencies encoded by neural networks. Using DiBS, we also develop a variational inference method for approximating distributions over structural models. Our method outperforms other approaches in joint posterior inference, as demonstrated through evaluations on both simulated and real-world data.