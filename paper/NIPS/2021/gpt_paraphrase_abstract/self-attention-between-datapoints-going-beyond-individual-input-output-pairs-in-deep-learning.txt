We challenge the assumption in supervised deep learning that a model's prediction depends solely on its parameters and the features of one input. Instead, we propose a deep learning architecture that takes the entire dataset as input, utilizing self-attention to explicitly consider the relationships between data points. This approach combines non-parametric models with parametric attention mechanisms, allowing the model to learn from the data how to utilize other data points for prediction. Our empirical results demonstrate the effectiveness of our models in solving complex reasoning tasks and cross-datapoint lookup, surpassing traditional deep learning models. We also provide insights into how the model utilizes interactions between points in both tabular data and CIFAR-10 datasets.