Self-training is a successful method for semi-supervised learning, where the learner generates "pseudo-supervision" for unlabeled instances based on its current hypothesis. Pseudo-labeling, combined with consistency regularization, has shown promising results in computer vision and other domains. However, some argue that probability distributions used as pseudo-labels still assume too much knowledge. In our approach, we allow the learner to label instances using credal sets, which are sets of candidate probability distributions. This increases expressiveness and allows the learner to represent uncertainty and lack of knowledge more accurately. To learn from weakly labeled data, we utilize methods from superset learning. Through extensive empirical evaluation, we compare our approach to state-of-the-art self-supervision techniques and demonstrate competitive to superior performance, especially in scenarios with low-label data and high uncertainty.