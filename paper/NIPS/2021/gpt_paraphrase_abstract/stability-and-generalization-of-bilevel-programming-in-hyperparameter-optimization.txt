The bilevel programming framework, commonly used in hyperparameter optimization, has proven to be effective in practice. However, previous research has primarily focused on its optimization properties and has neglected the analysis of generalization. This paper aims to address this gap by introducing an expectation bound based on uniform stability with respect to the validation set. Our findings shed light on certain perplexing phenomena observed in bilevel programming, such as overfitting to the validation set. Additionally, we provide an expectation bound for the classical cross-validation algorithm, suggesting that gradient-based algorithms may outperform cross-validation under certain conditions. Furthermore, we demonstrate that incorporating regularization terms in both the outer and inner levels can mitigate the overfitting issue in gradient-based algorithms. Our theoretical results are supported by experiments on feature learning and data reweighting for noisy labels.