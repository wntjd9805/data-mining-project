We introduce a new compression framework for deep neural networks that automatically determines the best compression ratio for each layer while achieving the desired overall compression. Our approach involves dividing convolutional or fully-connected layers into multiple groups and decomposing each group using low-rank decomposition. The core of our algorithm is based on deriving layer-wise error bounds from the Eckart-Young-Mirsky theorem. We utilize these bounds to formulate the compression problem as an optimization problem, aiming to minimize the maximum compression error across layers. Our efficient algorithm demonstrates superior performance compared to existing low-rank compression methods on various networks and datasets. These findings pave the way for future research on the trade-offs between performance and size in modern neural networks.