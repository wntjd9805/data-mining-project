The objective of multi-task learning is to improve learning efficiency by sharing model structures for various tasks. However, the standard approach of minimizing the average loss across all tasks often leads to worse performance for each individual task. This is due to conflicting gradients, where different task objectives are not well aligned, resulting in detrimental effects on specific tasks. Previous methods have proposed heuristics to address this issue, but they lack convergence guarantee and may converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss while leveraging the worst local improvement of individual tasks to guide the algorithm's trajectory. CAGrad automatically balances the objectives and provably converges to a minimum over the average loss. It includes regular gradient descent and multiple gradient descent algorithm as special cases. Through experiments on challenging multi-task learning tasks, CAGrad outperforms prior methods in terms of performance. The code for CAGrad is available at https://github.com/Cranial-XIX/CAGrad.