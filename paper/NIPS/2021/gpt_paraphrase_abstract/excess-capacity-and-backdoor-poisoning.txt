A backdoor data poisoning attack involves an attacker injecting mislabeled training examples into a dataset. These examples have watermarks that do not affect the model's performance on regular data, but cause it to make errors on the watermarked examples. To better understand these attacks, we present a theoretical framework and analyze the statistical and computational aspects. We introduce the concept of memorization capacity to measure a learning problem's vulnerability to backdoor attacks. Our findings demonstrate that some learning problems are not susceptible to successful backdoor attacks. From a computational standpoint, we show that adversarial training can detect backdoors under certain assumptions. We also establish a connection between backdoor filtering, robust generalization, and the ability to identify watermarked examples in the training set. This implies that algorithms capable of detecting watermarked examples are necessary and sufficient for achieving good generalization and robustness against backdoors.