Graph neural networks (GNNs) are effective architectures for structured data. However, existing methods struggle to capture long-range dependencies. Increasing the depth or width of GNNs is not enough to expand the receptive fields, as larger GNNs face optimization issues like vanishing gradients and oversmoothing of representations. Meanwhile, pooling-based approaches are not as universally applicable as in computer vision. To address this, we propose using a Transformer-based self-attention mechanism to learn long-range pairwise relationships and obtain a global graph embedding. Inspired by recent findings in computer vision, where position-invariant attention is successful in learning long-range relationships, our method, called GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This straightforward architecture achieves state-of-the-art results on various graph classification tasks, surpassing methods that explicitly encode graph structure. Our findings suggest that purely learning-based approaches without graph structure can effectively learn high-level, long-range relationships on graphs. The code for GraphTrans is available at https://github.com/ucbrise/graphtrans.