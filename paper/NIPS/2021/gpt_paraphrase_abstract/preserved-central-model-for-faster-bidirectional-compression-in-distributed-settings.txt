We propose a new algorithm, MCM, to address communication limitations in distributed learning with a central server. Our algorithm performs bidirectional compression and achieves the same convergence rate as algorithms using only uplink compression. The key improvement is that MCM only impacts local models with downlink compression, preserving the global model. This differs from previous works where gradients were computed on perturbed models, making convergence proofs more challenging. To ensure control over perturbation, MCM combines model compression with a memory mechanism. This analysis introduces possibilities such as incorporating worker-dependent randomized models and partial participation.