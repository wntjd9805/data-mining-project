We examine the concept of fair classification in the presence of a highly knowledgeable adversary. This adversary has the ability to select a fraction of training samples and manipulate their protected attributes. This research is motivated by situations where protected attributes can be incorrect due to intentional misreporting, malicious individuals, or errors in data imputation. Previous approaches that rely on stochastic or independence assumptions may not provide reliable guarantees in this adversarial scenario. Our main contribution is the development of an optimization framework that allows for the learning of fair classifiers in the presence of such an adversary. This framework provides provable guarantees on both accuracy and fairness. It is capable of handling multiple and non-binary protected attributes, as well as perturbations beyond the protected attributes. We demonstrate the near-tightness of our framework's guarantees for various hypothesis classes, showing that no algorithm can significantly outperform it in terms of accuracy. Additionally, any algorithm that achieves better fairness must sacrifice accuracy. Through empirical evaluation, we assess the classifiers generated by our framework using real-world and synthetic datasets, considering a range of adversaries.