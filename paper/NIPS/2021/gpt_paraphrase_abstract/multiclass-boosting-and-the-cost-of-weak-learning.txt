Boosting is an algorithmic method that combines weak and somewhat inaccurate hypotheses to form a strong and accurate one. This study focuses on multiclass boosting, where there are numerous classes or categories. The approach considers weak hypotheses belonging to an "easy-to-learn" base class and uses an agnostic PAC learner as the weak learner, which is different from other complex losses used in previous studies. The goal is to learn a combination of weak hypotheses by repeatedly calling the weak learner. The analysis examines the resources required for boosting, particularly how they are influenced by the number of classes. It is found that the boosting algorithm itself only needs O(log k) samples, while the weak learner requires a number of samples that is at least polynomial in k, significantly more than the booster. Additionally, the weak learner's accuracy parameter must be smaller than an inverse polynomial in k when dealing with a large number of classes. A trade-off is also established between the number of calls to the weak learner and the resources required, meaning that fewer calls demand more from each call.