We present a novel algorithm for assessing the vulnerability of k-nearest neighbor classification models to adversarial examples. While previous research has mainly focused on neural networks, we demonstrate that other practical models also suffer from this issue. Our approach differs from previous methods as we propose a geometric search that expands outward from a given input point. The search radius grows to higher-order Voronoi cells until a cell is found that classifies differently from the input point, thus identifying a minimum-norm adversarial example. To handle large k values, we introduce approximation steps that discover perturbations with smaller norms compared to existing techniques across various datasets. Additionally, we analyze the dataset's structural properties where our algorithm outperforms other approaches.