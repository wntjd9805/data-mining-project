Developing safe reinforcement learning (RL) policies is a growing area of research. RL agents learn by trial and error to optimize rewards, but often fail to meet safety requirements. This study proposes a method to refine a trained RL policy based on counterexamples to improve safety. The method consists of two parts: discovering failure trajectories using Bayesian Optimization and modifying the policy's failure points using gradient-based updates. The approach has been tested on various RL environments, showing that targeted changes can make the policy adhere to safety specifications.