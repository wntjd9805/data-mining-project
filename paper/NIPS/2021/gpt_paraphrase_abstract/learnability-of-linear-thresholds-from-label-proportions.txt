We examine the problem of effectively learning linear threshold functions (LTFs) within the learning from label proportions (LLP) framework. LLP involves learning from a set of bags containing collections of feature vectors, where only the proportion of labels is known for each bag. We propose an algorithm that efficiently produces an LTF satisfying a significant fraction of the bags when given a collection of bags, each with a maximum size of two, whose label proportions are consistent with an unknown LTF. If the bags are non-monochromatic (i.e., bags with differently labeled feature vectors), the algorithm satisfies an even higher fraction of them. For the specific case of OR functions over d-dimensional boolean vectors, we present an algorithm that achieves an additional Ω(1/d) in accuracy for both cases. We also prove that it is computationally challenging to improve upon these algorithmic bounds, even when learning monotone ORs using LTFs. Specifically, given a collection of non-monochromatic bags that are all satisfied by some monotone OR, it is NP-hard to compute any function of a constant number of LTFs that satisfies a fraction greater than (1/2 + ε) of the bags for any constant ε > 0. This bound is optimal for the non-monochromatic bags case. These results highlight a contrast with the traditional supervised learning setup, where LTFs can be efficiently learned with arbitrary accuracy using linear programming. In the LLP setting, however, these techniques fail. We demonstrate that LLP learning of LTFs, even for monotone ORs, becomes significantly more complex as soon as bags of size two are allowed. Our work provides the first evidence of inapproximability for LLP learning LTFs and showcases a substantial complexity difference between LLP and traditional supervised learning.