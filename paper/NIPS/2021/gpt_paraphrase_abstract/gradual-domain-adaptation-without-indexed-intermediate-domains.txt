Unsupervised domain adaptation is less effective when there is a large difference between the source and target domains. Gradual domain adaptation (GDA) addresses this issue by utilizing additional unlabeled data that gradually shifts from the source to the target domain. By adapting the model sequentially across intermediate domains, GDA significantly improves adaptation performance. However, in practical scenarios, the unlabeled data may not be separated into intermediate domains, limiting the applicability of GDA. This paper explores how to discover the sequence of intermediate domains when it is not known beforehand. The proposed approach, called Intermediate DOmain Labeler (IDOL), introduces a coarse-to-fine framework. It begins with a coarse domain discovery step using progressive domain discriminator training, followed by a fine indexing step using a novel cycle-consistency loss. This loss ensures that the next intermediate domain retains sufficient discriminative knowledge from the current intermediate domain. The resulting domain sequence can then be used in a GDA algorithm. Experiments on benchmark datasets demonstrate that IDOL achieves comparable or even better adaptation performance compared to pre-defined domain sequences, enhancing the applicability and robustness of GDA. The source code is available at https://github.com/hongyouc/IDOL.