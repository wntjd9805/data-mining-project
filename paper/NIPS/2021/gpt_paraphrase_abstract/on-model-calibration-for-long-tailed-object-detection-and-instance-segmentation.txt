The bias towards detecting frequent objects in object detection and instance segmentation models is a common problem. Existing methods typically address this bias during training by re-sampling or re-weighting. However, this paper proposes a different approach â€“ post-processing calibration of confidence scores. The authors introduce a method called NORCAL, which re-weighs the predicted scores of each class based on its training sample size. They demonstrate that handling the background class separately and normalizing the scores over classes for each proposal are crucial for achieving better performance. The NORCAL method significantly improves the performance of baseline models on both rare and common classes in the LVIS dataset. The authors also provide extensive analysis and ablation studies to explain the choices and mechanisms of their approach. The code for NORCAL is publicly available at https://github.com/tydpan/NorCal.