We examine the training of sparse layers in large Transformer models by using hashing to assign different parameters to different inputs. Specifically, we modify the feedforward layer to assign unique weights based on the current token in the sequence. Our findings demonstrate that this method outperforms or competes with other approaches like SwitchTransformers and BASE Layers, without the need for routing parameters, load balancing loss, or complex assignment algorithms. We analyze different hashing techniques, hash sizes, and input features, and find that balanced and random hashes focusing on local features yield the best results compared to learning clusters or utilizing longer-range context. Our approach proves effective in language modeling, dialogue tasks, and downstream fine-tuning tasks.