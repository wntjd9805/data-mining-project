The current methods for learning human preferences assume that the human's environment is fully observable, but in reality, people often make decisions under uncertainty. To address this, we investigate inverse decision theory (IDT), a framework where a human's preferences are conveyed through a loss function that reflects the tradeoff between different types of mistakes made in non-sequential binary decisions. We conduct a statistical analysis of IDT, identifying the conditions necessary to determine these preferences and quantifying the sample complexity required to learn the tradeoff accurately. Surprisingly, we find that preferences are easier to identify in more uncertain decision problems. Additionally, we can relax the assumption that the human is an optimal decision maker while still identifying their exact preferences, and we provide sample complexities in this suboptimal case. Our analysis challenges the notion that partial observability makes preference learning more difficult and paves the way for improving methods for uncertain and suboptimal humans.