Recent interest in transformers has shown their potential as "universal" models for computer vision tasks. While previous studies have focused on discriminative models, we explore the use of transformers in more challenging vision tasks, specifically generative adversarial networks (GANs). Our goal is to create a GAN that relies solely on transformer-based architectures, without the use of convolutions. Our architecture, called TransGAN, includes a memory-friendly transformer-based generator and a multi-scale discriminator. We also introduce a new module called grid self-attention to address memory limitations and enable high-resolution generation. We have developed a unique training recipe that includes techniques to improve the stability of TransGAN, such as data augmentation, modified normalization, and relative position encoding. Our best architecture achieves competitive performance compared to state-of-the-art GANs using convolutional backbones. TransGAN achieves high scores on various datasets, including STL-10, CIFAR-10, and CelebA 128x128. It also performs well on higher-resolution tasks, such as CelebA-HQ and LSUN-Church. We have visualized the training dynamics of transformer-based generation models to understand their behavior compared to convolutional models. The code for TransGAN is available at the provided GitHub link.