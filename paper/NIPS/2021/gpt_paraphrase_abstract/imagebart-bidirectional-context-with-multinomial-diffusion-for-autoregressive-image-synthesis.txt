Autoregressive models have shown potential for image representation and synthesis. However, these models have limitations as they only consider previously synthesized image patches, ignoring large parts of the scene and global contextual information. To address this, we propose a hierarchical approach that combines autoregressive formulation with a multinomial diffusion process. This process progressively incorporates context from previous stages in a coarse-to-fine manner. Our experiments demonstrate improved image modification capabilities and high-fidelity image generation compared to autoregressive models. Additionally, our approach allows for local image editing using user-provided masks, enabling free-form image inpainting and text-guided image modification without requiring mask-specific training.