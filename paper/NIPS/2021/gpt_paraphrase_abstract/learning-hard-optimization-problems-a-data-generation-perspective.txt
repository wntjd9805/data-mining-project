Optimization problems are widespread in various sectors of the economy and often involve finding approximate solutions for large-scale instances. Machine learning frameworks offer a potential solution by learning to approximate these hard optimization problems, especially when multiple related instances need to be solved repeatedly. However, challenges arise when the outputs are approximations, the optimization problem has symmetric solutions, or the solver uses randomness. These factors can lead to significant differences in solutions for closely related instances, making the learning task more difficult. This paper highlights this challenge, discusses the relationship between variations in training data and model approximation, and proposes a method to generate solutions that are better suited for supervised learning. The method's effectiveness is demonstrated through testing on complex non-linear nonconvex and discrete combinatorial problems.