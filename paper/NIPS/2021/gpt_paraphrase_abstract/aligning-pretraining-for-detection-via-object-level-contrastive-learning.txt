Image-level contrastive representation learning has been highly effective for transfer learning, but it lacks specificity for specific downstream tasks. To address this, we propose a design principle that promotes alignment between the self-supervised pretext task and the downstream task. In this study, we apply this principle to object detection and introduce object-level representations using selective search bounding boxes, incorporate dedicated modules from the detection pipeline into the pretraining network architecture, and equip the pretraining with object detection properties. Our method, called Selective Object Contrastive learning (SoCo), achieves state-of-the-art transfer performance on COCO detection using a Mask R-CNN framework. The code is available at https://github.com/hologerry/SoCo.