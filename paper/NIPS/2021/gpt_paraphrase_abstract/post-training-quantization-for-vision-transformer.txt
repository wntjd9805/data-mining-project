In this study, the authors propose a post-training quantization algorithm to reduce the memory storage and computational costs of vision transformers. Vision transformers have shown impressive performance in computer vision applications, but their complex architectures make it challenging to deploy them on mobile devices. The quantization task involves finding optimal low-bit quantization intervals for weights and inputs. To maintain the functionality of the attention mechanism, the authors introduce a ranking loss to preserve the relative order of self-attention results after quantization. They also analyze the relationship between quantization loss and feature diversity and explore a mixed-precision quantization scheme using the nuclear norm of each attention map and output feature. The proposed method is evaluated on benchmark models and datasets, outperforming existing post-training quantization algorithms. For example, an 81.29% top-1 accuracy is achieved using the DeiT-B model on the ImageNet dataset with approximately 8-bit quantization. The code for the proposed method is available at https://gitee.com/mindspore/models/tree/master/research/cv/VT-PTQ.