The detection of out-of-distribution (OOD) data has become crucial for ensuring the safe use of neural networks. However, a major challenge is that neural network models often make confident predictions on OOD data, contradicting the principle that they should only be confident about in-distribution samples. To address this issue, we propose a technique called ReAct, which effectively reduces model overconfidence on OOD data. Our approach is based on analyzing the internal activations of neural networks, which exhibit distinct patterns for OOD distributions. ReAct can be applied to different network architectures and OOD detection scores with good generalization. Through empirical evaluation on various benchmark datasets, we show that ReAct achieves competitive OOD detection performance and provide theoretical explanations for its effectiveness. On the ImageNet benchmark, ReAct reduces the false positive rate (FPR95) by 25.05% compared to the previous best method.