Deep Metric Learning (DML) seeks to discover representations that can be effectively applied to unfamiliar test distributions. However, current evaluation methods only assess performance on a single fixed data split, where training and testing classes are randomly assigned. To provide a more realistic assessment, it is crucial to consider a wide range of distribution shifts with varying degrees of complexity. This study addresses this issue by systematically constructing increasingly challenging train-test splits and introducing the ooDML benchmark. The ooDML benchmark is designed to evaluate the generalization capabilities of DML models under more diverse and difficult distribution shifts. Using this benchmark, the authors conduct a comprehensive analysis of state-of-the-art DML methods. The results indicate that while generalization tends to decline as the difficulty of the distribution shift increases, certain methods exhibit better performance retention. Additionally, the authors propose the use of few-shot DML as an efficient approach to consistently enhance generalization when faced with unknown test shifts presented in ooDML1.