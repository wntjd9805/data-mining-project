Recent research has shown that private training data can be exposed through the sharing of gradients in distributed machine learning systems like federated learning (FL). Increasing the batch size as a defense mechanism against data leakage is commonly seen as effective. However, in this study, we challenge this defense strategy and introduce a new data leakage attack called catastrophic data leakage in vertical federated learning (CAFE). We provide theoretical justification for CAFE and demonstrate its effectiveness in recovering batch data from shared aggregated gradients through extensive experiments in vertical FL settings. Additionally, we propose a practical countermeasure to mitigate the CAFE attack. Our findings highlight the high risk of private data being leaked from training gradients in standard FL, particularly in vertical scenarios. This analysis reveals unprecedented and practical data leakage vulnerabilities in these learning settings. The code for our work can be found at https://github.com/DeRafael/CAFE.