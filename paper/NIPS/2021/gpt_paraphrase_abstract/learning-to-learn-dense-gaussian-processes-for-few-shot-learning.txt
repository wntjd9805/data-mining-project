Gaussian processes with deep neural networks have proven to be effective for few-shot learning by combining the strengths of deep learning and kernels while also capturing uncertainty. However, the challenge remains to utilize the shared knowledge from related tasks. This paper introduces a method for few-shot learning that involves learning Gaussian processes with dense inducing variables through meta-learning. Unlike sparse Gaussian processes, dense inducing variables are defined to be larger in size than the support set of each task, allowing them to gather prior knowledge from previous tasks. These dense inducing variables establish a shared Gaussian process prior over prediction functions for all tasks, which are learned using a variational inference framework and provide a strong inductive bias for learning new tasks. To achieve task-specific prediction functions, the inducing variables are adapted to each task using efficient gradient descent. Extensive experiments on common benchmark datasets demonstrate that our dense Gaussian processes outperform vanilla Gaussian processes and achieve comparable or even superior performance compared to state-of-the-art methods.