Policy optimization is a commonly used technique in reinforcement learning. However, its reliance on local search often requires additional assumptions on the underlying Markov Decision Processes (MDPs) to guarantee global optimality. In this study, we propose a general solution to address this limitation by introducing dilated bonuses to the policy update, enabling global exploration without the need for such assumptions. We demonstrate the effectiveness and versatility of this approach by applying it to various episodic MDP scenarios involving adversarial losses and bandit feedback, achieving improved and generalized outcomes compared to existing methods. Specifically, in the tabular case, our approach achieves O(T) regret, surpassing the O(T^(2/3)) regret bound achieved by previous research. In scenarios with an infinite number of states, assuming the state-action values are linear in low-dimensional features, we achieve O(T^(2/3)) regret with the assistance of a simulator, matching the results of previous work while eliminating the requirement for an exploratory policy. Notably, this is the first algorithm to achieve sublinear regret for linear function approximation in the presence of adversarial losses, bandit feedback, and without exploratory assumptions. Finally, we discuss potential avenues for further improving regret or eliminating the need for a simulator by utilizing dilated bonuses in conjunction with an exploratory policy.