Value function factorization (VFF) is a common method used in cooperative multi-agent reinforcement learning to learn local value functions based on global rewards. However, the current state-of-the-art VFF approach is only effective for a small number of agents in most scenarios. We believe that this limitation is due to the flat factorization scheme, where the VFF operator becomes a performance bottleneck as the number of agents increases. To address this issue, we propose a new approach called VFF with variable agent sub-teams (VAST). VAST allows for the approximation of a factorization for sub-teams, which can be defined in any way and can change over time to adapt to different situations. The values for each sub-team are then linearly decomposed for all members of the sub-team. This allows VAST to learn using a more focused and concise input representation of the original VFF operator. We evaluate VAST in three different multi-agent domains and demonstrate that it outperforms the current state-of-the-art VFF method when the number of agents is sufficiently large.