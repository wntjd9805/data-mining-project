Current coding education often requires students to create programs with user interaction and complex dynamic systems, like mouse-based games. However, there is a lack of contemporary methods for providing feedback on these interactive programs. Traditional unit tests are ineffective for grading such programs. In this study, we propose classifying Markov Decision Processes (MDPs) as a means of providing feedback to interactive programs. Each student's program represents an MDP, and we determine if the dynamics and reward model of the MDP should be categorized as correct or broken. By designing a cooperative objective between an agent and an autoregressive model, we can use the agent to generate different trajectories from the input MDP, which allows a classifier to determine membership. This method, called Play toGrade, offers an automatic feedback system for interactive code assignments. We provide a dataset of 711,274 student submissions with bug labels to support further research.