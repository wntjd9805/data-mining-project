Recent studies have demonstrated that having an excess number of parameters reduces variance for min-norm interpolators and max-margin classifiers. As a result, it has been suggested that ridge regularization becomes less beneficial in high-dimensional settings. However, we present evidence contradicting this notion by showing that even without noise, ridge regularization can greatly enhance generalization by preventing interpolation. Our findings are supported by theoretical proof for both linear regression and classification, marking the first theoretical result on robust overfitting.