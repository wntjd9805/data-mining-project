Imbalanced datasets are common in machine learning, leading to concerns about generalization and fairness. Deep neural networks can achieve perfect accuracy and fairness during training but perform poorly during testing. To address these challenges, we propose AutoBalance, a framework that automatically designs a training loss function to optimize both accuracy and fairness. It uses a bi-level optimization approach, with one level training the model weights and another level tuning the loss function based on desired objectives. Our approach allows for personalized treatment of different classes or groups using a parametric cross-entropy loss and individualized data augmentation. We evaluate the performance of AutoBalance in imbalanced and group-sensitive classification scenarios and show its superiority over existing approaches through extensive empirical evaluations. We also provide theoretical insights on loss function design and the benefits of train-validation split. The code for AutoBalance is available as open-source.