This study aims to combine the ideas of topographic organization and equivariance in neural networks. To achieve this, the researchers propose a new method called the Topographic VAE, which efficiently trains deep generative models with topographically organized latent variables. The results show that this model effectively organizes its activations based on important characteristics like digit class, width, and style in the MNIST dataset. Additionally, by incorporating temporal coherence, the model encourages predefined transformations in the latent space for observed input sequences, leading to a basic form of unsupervised learned equivariance. The model successfully learns sets of approximately equivariant features, known as "capsules," directly from sequences and achieves higher likelihood on transformed test sequences. The researchers quantitatively verify equivariance by measuring the approximate commutativity of the inference network and the sequence transformations. The model also demonstrates approximate equivariance to complex transformations, surpassing the capabilities of existing group equivariant neural networks.