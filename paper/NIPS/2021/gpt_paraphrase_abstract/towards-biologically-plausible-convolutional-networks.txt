Convolutional networks are widely used in deep learning for image processing due to their ability to reduce parameters, training time, and improve accuracy. However, as a model of the brain, they have a fundamental issue with weight sharing, which real neurons cannot do. Although real neurons can be locally connected like convolutional networks, they cannot be truly convolutional. Non-convolutional networks with local connections perform significantly worse than convolutional networks, posing a challenge for studies that use convolutional networks to explain visual system activity. To address this, we explore alternatives to weight sharing that aim to make each neuron within a group respond similarly to identical inputs. The most natural approach involves showing the network multiple translated versions of the same image, similar to saccades in animal vision. However, this method requires many translations and does not eliminate the performance gap. Instead, we propose adding lateral connectivity to a locally connected network and enabling learning through Hebbian plasticity. This approach requires occasional "weight sharing" phases resembling sleep. By implementing this method, locally connected networks achieve nearly convolutional performance on ImageNet and better fit the ventral stream data. Consequently, this supports convolutional networks as a model of the visual stream.