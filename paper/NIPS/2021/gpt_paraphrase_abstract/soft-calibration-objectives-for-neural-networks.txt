Deep neural networks often produce predictions that are either too confident or not confident enough, leading to suboptimal decision-making. To address this issue, various methods have been developed to improve the calibration of predictive uncertainty in these networks. In this study, we propose the use of differentiable losses to enhance calibration, based on a continuous version of the binning operation used in popular calibration-error estimators. When these soft calibration losses are incorporated into the training process, they achieve state-of-the-art calibration performance across multiple datasets with minimal impact on accuracy (less than 1% decrease). For example, on the CIFAR-100 dataset, we observe an 82% reduction in calibration error with only a 0.7% decrease in accuracy compared to the cross-entropy baseline. When applied post-training, our soft-binning-based calibration error objective outperforms temperature scaling, a commonly used recalibration method. Overall, our experiments demonstrate that using calibration-sensitive procedures leads to more accurate uncertainty estimates under dataset shift compared to the standard practice of using cross-entropy loss and post-hoc recalibration methods.