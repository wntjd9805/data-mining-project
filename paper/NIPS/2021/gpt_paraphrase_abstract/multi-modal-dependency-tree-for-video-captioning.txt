This paper explores the use of a graph structured model for generating captions for visual content. The existing methods typically use sequence models to predict words in a linear order. However, this paper proposes a novel approach that models the hierarchical structure of sentences using a multi-modal dependency tree. By representing the syntactic structure and semantic relationship in the sentence through the tree topology, the generated captions become more fluent and relevant. The method encodes both visual and textual representation features into each node of the tree to leverage information from both modalities. Unlike existing dependency parsing methods that generate uni-modal dependency trees for language understanding, this method constructs multi-modal dependency trees for language generation in videos. Additionally, a tree-structured reinforcement learning algorithm is proposed to effectively optimize the captioning model. A novel reward is designed to evaluate the semantic consistency between the generated sub-trees and the ground-truth tree. The proposed method is validated through extensive experiments on various video captioning datasets, demonstrating its effectiveness.