We introduce a new method for real-time estimation and learning in state-space models (SSMs), which are widely used for analyzing sequential data. Similar to traditional batch variational techniques, we utilize stochastic gradients to optimize a lower bound on the log evidence for both model parameters and a variational approximation of the posterior distribution of the states. However, our approach distinguishes itself by enabling online operation, meaning past observations do not need to be revisited once incorporated, and the computational cost per time step remains constant despite the increasing dimensionality of the joint posterior distribution. To achieve this, we employ backward decompositions of the joint posterior distribution and its variational approximation, along with Bellman-type recursions for the evidence lower bound and its gradients. We provide evidence of the effectiveness of our method through various examples, including high-dimensional SSMs and sequential Variational Auto-Encoders.