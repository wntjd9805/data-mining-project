This abstract discusses the use of policy gradient (PG) methods in reinforcement learning (RL) and the recent trend of enhancing existing PG methods by reducing variance. However, current variance-reduced PG methods rely on an assumption that cannot be checked. To address this issue, the paper proposes a gradient truncation mechanism. Additionally, a Truncated Stochastic Incremental Variance-Reduced Policy Gradient (TSIVR-PG) method is introduced, which aims to maximize both cumulative rewards and a general utility function over a policy's long-term visiting distribution. The study demonstrates that TSIVR-PG has a sample complexity of ˜O((cid:15)−3) to find an (cid:15)-stationary policy. Furthermore, by assuming policy overparameterization and exploiting the hidden convexity of the problem, it is proven that TSIVR-PG converges to a global (cid:15)-optimal policy with ˜O((cid:15)−2) samples.