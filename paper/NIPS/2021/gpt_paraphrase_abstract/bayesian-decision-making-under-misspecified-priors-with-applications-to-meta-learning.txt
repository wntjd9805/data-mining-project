Thompson sampling and other Bayesian sequential decision-making algorithms are widely used to address the explore/exploit trade-offs in bandits. The choice of prior in these algorithms allows for the incorporation of domain knowledge, but it can also result in poor performance if incorrectly specified. This paper shows that the performance of Thompson sampling degrades gracefully with misspecification. The expected reward obtained by Thompson sampling with a misspecified prior is proven to differ by at most ˜O(H 2ϵ) from Thompson sampling with a well-specified prior, where ϵ is the total-variation distance between priors and H is the learning horizon. This bound is not dependent on the specific form of the prior. Additionally, for priors with bounded support, the bound is independent of the size or structure of the action space. The study also establishes generic PAC guarantees for algorithms in the Bayesian meta-learning setting, and provides corollaries for various families of priors. The results apply to a broader range of Bayesian decision-making algorithms, including the knowledge gradient algorithm, and extend to Bayesian POMDPs, encompassing contextual bandits as a special case. Numerical simulations demonstrate the impact of prior misspecification and the use of one-step look-ahead on the convergence of meta-learning in bandit problems with structured and correlated priors.