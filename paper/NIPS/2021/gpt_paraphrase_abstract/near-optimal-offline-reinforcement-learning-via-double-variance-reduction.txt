We address the problem of offline reinforcement learning (RL), which involves optimizing policies using only historical data. The theoretical understanding of offline RL, including its optimal sample complexity, is still largely unknown, even in basic scenarios like tabular Markov Decision Processes (MDPs). In this paper, we propose a new algorithm called Off-Policy Double Variance Reduction (OPDVR) for offline RL. Our main result demonstrates that OPDVR can identify an approximately optimal policy with a sample complexity of O(H^2/dm^2), where H is the horizon length and dm is the minimal marginal state-action distribution induced by the behavior policy. This improves upon the previous best upper bound by a factor of H. We also establish a lower bound of â„¦(H^2/dm^2) using information theory, which certifies that OPDVR is nearly optimal. Additionally, we show that OPDVR achieves rate-optimal sample complexity in alternative settings, such as finite-horizon MDPs with non-stationary transitions and infinite-horizon MDPs with discounted rewards.