Current artistic style transfer methods using deep neural networks have made significant progress but still have issues with disharmonious colors and repetitive patterns. To address this, we propose an internal-external style transfer approach with two contrastive losses. We use the internal statistics of a single style image to determine colors and texture patterns, while also leveraging external information from a large-scale style dataset to learn human-aware style information. This improves the color distributions and texture patterns, making the stylized image more reasonable and harmonious. Additionally, we highlight that existing methods overlook stylization-to-stylization relations. To tackle this, we introduce contrastive losses that bring together multiple stylization embeddings when they share the same content or style, but push them apart otherwise. Extensive experiments demonstrate that our method not only produces visually pleasing artistic images but also enhances the stability and consistency of rendered video clips.