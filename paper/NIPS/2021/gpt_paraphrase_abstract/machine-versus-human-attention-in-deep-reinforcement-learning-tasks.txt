Deep reinforcement learning (RL) algorithms are effective in solving visuo-motor decision tasks. However, their interpretability is limited due to their representation as end-to-end deep neural networks. This study aims to understand the inner workings of these trained models by comparing the pixels attended to by RL agents and humans during task execution. Two previously unexplored questions are addressed: 1) How similar are the visual representations learned by RL agents and humans when performing the same task? and 2) How do these similarities and differences in representations explain RL agents' performance? The saliency maps of RL agents are compared to visual attention models of human experts in learning Atari games. Additionally, the impact of hyperparameters on the learned representations and saliency maps is analyzed. These findings can potentially contribute to the development of new algorithms that bridge the performance gap between RL agents and human experts.