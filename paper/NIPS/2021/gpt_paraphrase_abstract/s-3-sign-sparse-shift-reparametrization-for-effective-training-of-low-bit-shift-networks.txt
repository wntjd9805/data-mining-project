Shift neural networks are a computational technique that reduces complexity by replacing expensive multiplication operations and quantizing continuous weights into low-bit discrete values. However, existing shift networks suffer from issues related to weight initialization, leading to degraded performance due to problems such as vanishing gradient and weight sign freezing. To solve these problems, we propose a new technique called S3 re-parameterization for training low-bit shift networks. This method decomposes a discrete parameter in a sign-sparse-shift 3-fold manner, allowing for efficient learning of a low-bit network with weight dynamics similar to full-precision networks and insensitivity to weight initialization. Our proposed training method pushes the boundaries of shift neural networks and demonstrates that 3-bit shift networks can achieve competitive top-1 accuracy on ImageNet compared to their full-precision counterparts.