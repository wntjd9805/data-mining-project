Existing models for point cloud detection often struggle in domain adaptation scenarios due to variations in physical environments or LiDAR sensor configurations. This presents a challenge in learning transferable features between a labeled source domain and a new target domain without access to target labels. To address this, we introduce the 3D Contrastive Co-training (3D-CoCo) framework with two key contributions. First, we observe that bird-eye-view (BEV) features are more transferable than low-level geometry features, leading to the proposal of a co-training architecture with separate 3D encoders and a BEV transformation module for domain-invariant feature learning. Second, we extend the concept of contrastive instance alignment to point cloud detection, which has been hindered by the mismatch between the distribution of BEV features induced by pseudo-labels and the true distribution. 3D-CoCo mitigates this mismatch through carefully designed transformed point clouds, considering specific geometry priors. We create new benchmarks for domain adaptation using three large-scale 3D datasets and demonstrate that 3D-CoCo significantly reduces the domain gap and outperforms state-of-the-art methods by a substantial margin.