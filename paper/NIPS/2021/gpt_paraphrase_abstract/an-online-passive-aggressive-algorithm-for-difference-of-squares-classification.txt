We examine a quadratic classification model that is based on previous research on factorization machines, polynomial networks, and capsule-based architectures for visual object recognition. The model is defined by a pair of affine transformations and makes classifications by comparing the magnitudes of the vectors produced by these transformations. Additionally, the model is over-parameterized, meaning that different pairs of affine transformations can produce classifiers with the same decision boundary and confidence scores. We demonstrate that these pairs of transformations arise from discrete and continuous symmetries in the model's parameter space, specifically rotation and Lorentz transformation symmetry groups. We utilize these symmetries to develop procedures for aligning and averaging the model. Furthermore, we exploit the structure of the model's decision boundary to derive simple updates for online learning based on margins. We employ a passive-aggressive learning strategy, where we calculate the minimum parameter change needed to confidently predict the correct label for each example. These updates are obtained by solving a nonconvex but tractable quadratically constrained quadratic program (QCQP) using efficient elementary methods. We highlight the conceptual and practical contributions of this approach. Conceptually, it expands the application of passive-aggressive learning to a broader range of nonlinear classification models. Practically, we demonstrate that these models perform well on large-scale online learning problems.