Machine-learning systems like self-driving cars and virtual assistants consist of numerous models that perform tasks such as image recognition, speech transcription, natural language analysis, preference inference, and option ranking. These models are typically developed and trained independently, raising a concern about whether improving one model could negatively impact the overall system. We confirm that such improvements can indeed lead to the deterioration of downstream models, even after retraining them. This counterproductive outcome arises due to the entanglement between the models within the system. Through error decomposition, we analyze the types of errors that contribute to self-defeating improvements. Our experiments on a realistic stereo-based detection system for cars and pedestrians demonstrate the emergence of self-defeating improvements.