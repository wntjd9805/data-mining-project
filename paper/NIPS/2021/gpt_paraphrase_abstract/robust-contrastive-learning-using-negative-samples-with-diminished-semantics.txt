Unsupervised learning has made significant progress due to the development of more effective contrastive learning methods. However, CNNs tend to rely on low-level features that are not considered semantically meaningful by humans. This reliance may lead to a lack of robustness against image perturbations or domain shift. This study demonstrates that by generating carefully designed negative samples, contrastive learning can produce more robust representations that are less dependent on these non-semantic features. The approach involves using positive pairs that preserve semantic information but alter superficial features during training. Similarly, negative samples are generated in a reversed manner, preserving only the non-semantic features. Two methods, texture-based and patch-based augmentations, are proposed to generate these negative samples. The results show that these samples lead to improved generalization, especially in out-of-domain scenarios. Additionally, the study analyzes the method and the generated texture-based samples, highlighting the importance of texture features in classifying specific ImageNet classes, particularly finer classes. The analysis also reveals that model bias favors texture and shape features differently in different test settings. The code, trained models, and ImageNet-Texture dataset associated with this study can be accessed at https://github.com/SongweiGe/Contrastive-Learning-with-Non-Semantic-Negatives.