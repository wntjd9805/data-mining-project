We investigate the problem of predicting the 3D pose of objects using limited labeled examples and a set of unlabeled data. Our key contribution is a learning framework called neural view synthesis and matching, which effectively transfers the 3D pose annotation from labeled to unlabeled images, even in the presence of unseen 3D views and various nuisance variations such as object shape, texture, illumination, and scene context. Our approach represents objects as 3D cuboid meshes consisting of feature vectors at each mesh vertex. The model is initialized using labeled images and then generates synthesized feature representations of unseen 3D views. These synthesized views are matched with the feature representations of unlabeled images, generating pseudo-labels for the 3D pose. The pseudo-labeled data is then used to train the feature extractor, enhancing the invariance of the vertex features across different 3D views of the object. Our model is trained using an EM-type method that alternates between increasing the 3D pose invariance of the feature extractor and annotating unlabeled data through neural view synthesis and matching. We validate our semi-supervised learning framework on the PASCAL3D+ and KITTI datasets for 3D pose estimation. Our approach significantly outperforms all baseline methods, particularly in a challenging few-shot scenario with only 7 annotated images. Notably, our model exhibits exceptional robustness in out-of-distribution scenarios involving partial occlusion. The code for our approach is publicly available at https://github.com/Angtian/NeuralVS.