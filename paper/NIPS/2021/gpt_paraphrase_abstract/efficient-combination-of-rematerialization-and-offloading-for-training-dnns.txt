Rematerialization and offloading are two commonly used strategies in deep neural network training to save memory. Rematerialization exchanges memory for computation time, while offloading trades memory for data movements. Since these resources are independent, combining both strategies simultaneously can further save memory. We accurately model the costs and constraints of popular deep learning frameworks like PyTorch or TensorFlow, propose optimal algorithms to find valid memory-constrained operations sequences, and evaluate the performance of these algorithms on realistic networks and computation platforms. Our experiments demonstrate that offloading can reduce rematerialization overhead by one third, and when used together, they can decrease the memory used for activations by a factor of 4 to 6 with less than 20% overhead.