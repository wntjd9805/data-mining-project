Transformer architectures have become popular in computer vision due to their self-attention mechanism and convolution-free design. However, the tokenization process used in these architectures introduces jagged artifacts into attention maps, causing aliasing issues. Aliasing occurs when discrete patterns produce indistinguishable distortions in high frequency or continuous information. This problem has also been observed in modern convolution networks. In this study, we address the problem of aliasing in vision transformers and propose a plug-and-play Aliasing-Reduction Module (ARM) to mitigate this issue. We evaluate the effectiveness and generalization of our method across multiple tasks and different vision transformer families. Our lightweight design consistently outperforms several well-known structures, while also improving data efficiency and robustness of vision transformers.