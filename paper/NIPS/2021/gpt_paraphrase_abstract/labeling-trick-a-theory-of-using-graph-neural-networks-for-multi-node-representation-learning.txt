This paper presents a theory on using graph neural networks (GNNs) for learning representations of multiple nodes in a graph. While GNNs are typically used for single-node representation learning, previous approaches have attempted to aggregate individual node representations into a joint representation for multiple nodes. However, this approach fails to capture the interdependence between nodes in the set, resulting in ineffective representations. The authors observe that successful methods for multi-node representation learning, such as SEAL, Distance Encoding, and ID-GNN, all employ node labeling. These methods label nodes based on their relationships with the target node set before applying a GNN, and then aggregate the labeled node representations. The authors unify these node labeling techniques as a "labeling trick" and demonstrate that a sufficiently expressive GNN can learn the most expressive node set representations, effectively solving joint learning tasks over node sets. Experimental results on link prediction support the theory, explaining the superior performance of previous node-labeling-based methods and establishing a theoretical foundation for using GNNs in multi-node representation learning.