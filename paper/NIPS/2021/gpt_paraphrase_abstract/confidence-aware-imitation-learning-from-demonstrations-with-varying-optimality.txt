Current imitation learning approaches assume that the demonstrations come from experts who are optimal. However, relaxing this assumption allows for a wider range of data to be used. Standard imitation learning can result in suboptimal policies when learning from demonstrations with varying optimality. Previous methods have used confidence scores or rankings to capture useful information from these demonstrations, but these methods have limitations such as manually annotated confidence scores or high average optimality of the demonstrations. In this paper, we propose a general framework called Confidence-Aware Imitation Learning (CAIL) that learns both the confidence score and a high-performing policy from demonstrations with varying optimality. CAIL achieves this by learning from confidence-reweighted demonstrations and using an outer loss to track model performance and learn the confidence. We provide theoretical guarantees on the convergence of CAIL and evaluate its performance in simulated and real robot experiments. Our results demonstrate that CAIL significantly outperforms other imitation learning methods when learning from demonstrations with varying optimality. Additionally, we show that CAIL can still learn a successful policy even without access to any optimal demonstrations, surpassing previous work.