Extreme multi-label text classification (XMC) involves finding relevant labels from a large collection for a given text. XMC is used in various applications like recommendation systems and document tagging. Transformer-based XMC methods, such as X-Transformer and LightXML, have shown significant improvements. However, fine-tuning these models on a large label space takes a long time. This paper introduces XR-Transformer, a recursive approach that accelerates the fine-tuning process by recursively fine-tuning transformer models on multi-resolution objectives related to the original XMC objective. Empirical results demonstrate that XR-Transformer has significantly faster training time compared to other transformer-based XMC models, while also achieving better results. On the Amazon-3M dataset with 3 million labels, XR-Transformer is 20 times faster than X-Transformer and improves Precision@1 from 51% to 54%. The code for XR-Transformer is publicly available at https://github.com/amzn/pecos.