To establish the credibility of models in high stakes situations, black box explanations are commonly used. However, previous research has shown that these explanations generated by advanced techniques are inconsistent, unstable, and lack insight into their accuracy and reliability. Furthermore, these methods are computationally inefficient and require extensive tuning of hyper-parameters. This paper addresses these challenges by introducing a new Bayesian framework that generates local explanations and incorporates uncertainty. By applying this framework to LIME and KernelSHAP, we are able to obtain Bayesian versions that provide credible intervals for feature importances, capturing the associated uncertainty. These explanations not only allow us to make concrete assessments of their quality (e.g., there is a 95% chance that the feature importance falls within a given range), but also exhibit high consistency and stability. Through a comprehensive theoretical analysis, we exploit this uncertainty to determine the optimal number and method of perturbations for faster convergence. This work is the first attempt to simultaneously address multiple critical issues with popular explanation methods, resulting in consistent, stable, and reliable explanations with computational efficiency. Experimental evaluations using real-world datasets and user studies confirm the effectiveness of the proposed framework.