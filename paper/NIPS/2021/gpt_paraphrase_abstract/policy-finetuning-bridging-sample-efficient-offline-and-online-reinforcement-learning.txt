This paper introduces the concept of policy finetuning in reinforcement learning (RL) and aims to bridge the gap between online RL and offline RL. The study focuses on episodic Markov Decision Processes (MDPs) with S states, A actions, and horizon length H. The paper first presents an offline reduction algorithm that uses a reference policy µ to find an ε near-optimal policy within (cid:101)O(H 3SC (cid:63)/ε2) episodes, where C (cid:63) is the single-policy concentrability coefficient between µ and π(cid:63). This algorithm matches the sample complexity lower bound in this setting and resolves an open question in offline RL. The paper then establishes a sample complexity lower bound of Ω(H 3S min{C (cid:63), A}/ε2) for any policy finetuning algorithm, including those that adaptively explore the environment. This implies that the optimal policy finetuning algorithm is either offline reduction or a purely online RL algorithm that does not use µ. Finally, a new hybrid offline/online algorithm is proposed for policy finetuning that achieves better sample complexity than both vanilla offline reduction and purely online RL algorithms, under the condition that µ partially satisfies concentrability up to a certain time step. The results provide a quantitative understanding of the benefits of a good reference policy and contribute to bridging offline and online RL.