Formal specifications for neural networks often involve dealing with stochasticity and uncertainty. However, progress in verifying probabilistic specifications has been limited. To address this, we introduce a general formulation for probabilistic specifications that encompasses both probabilistic networks and uncertain inputs. We propose a technique that utilizes functional multipliers, replacing traditional Lagrangian multipliers, to verify these specifications. By choosing optimal functional multipliers, we achieve exact verification and develop practical verification algorithms. We validate our algorithms by applying them to Bayesian Neural Networks and MC Dropout Networks, improving the certified accuracy for tasks such as adversarial robustness and out-of-distribution detection. Our approach significantly enhances the reliability of neural networks, surpassing prior work in terms of certified performance.