We examine the AMSGrad algorithm, which is used for solving a constrained stochastic optimization problem with a weakly convex objective. Our analysis shows that this algorithm converges at a rate of O(tâˆ’1/2) for the squared norm of the gradient of the Moreau envelope, which is the standard measure of stationarity for this type of problem. This convergence rate is consistent with what is known for adaptive algorithms in the case of unconstrained smooth nonconvex stochastic optimization. Our analysis assumes a mini-batch size of 1, constant first and second order moment parameters, and potentially unbounded optimization domains. Additionally, we demonstrate the practical applications and potential extensions of our findings to specific problems and algorithms.