Current approaches to safe reinforcement learning (RL) require mathematical constraints, which limits their adoption. In this study, we propose a method for learning to interpret natural language constraints for safe RL. We introduce a new benchmark called HAZARD-WORLD, where agents must optimize rewards while adhering to constraints specified in free-form text. Our model consists of a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and a policy network that uses these representations to minimize constraint violations during training. Our method outperforms existing approaches, achieving higher rewards and fewer constraint violations across different domains in HAZARDWORLD. However, HAZARDWORLD still presents challenges for efficient learning, highlighting the need for future research.