Deep reinforcement learning (RL) agents can effectively adapt to new scenarios by being trained on a diverse range of environment and task configurations. Unsupervised Environment Design (UED) is a self-supervised RL approach that adjusts the parameters of an underspecified environment during training, resulting in the creation of various training environments. This study presents Prioritized Level Replay (PLR), a successful but unsupported method that selectively samples randomly-generated training levels, as a form of UED. It argues that PLR can also generate novel and complex levels for effective training by curating completely random levels. This insight introduces a new class of UED methods called Dual Curriculum Design (DCD), which includes both PLR and a popular UED algorithm called PAIRED. DCD inherits similar theoretical guarantees and allows for the development of novel theory for PLR, including a robustness guarantee at Nash equilibria. Moreover, the theory suggests a counterintuitive improvement to PLR: by preventing the agent from updating its policy on uncurated levels (reducing the amount of training data), convergence to Nash equilibria can be enhanced. Experimental results validate this new method, called PLR⊥, which achieves better results on out-of-distribution, zero-shot transfer tasks. Additionally, PLR⊥ improves the performance of PAIRED, incorporating its theoretical framework.