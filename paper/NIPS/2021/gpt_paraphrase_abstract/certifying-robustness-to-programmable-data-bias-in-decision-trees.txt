We aim to ensure that machine learning models are not biased by certifying their robustness against potential biases in datasets. This is a challenging task as it requires learning models for numerous datasets with consistent predictions. We specifically focus on decision-tree learning for its interpretability. Our approach allows for programmatically specifying bias models across various dimensions and targeting biases towards specific groups. To certify robustness, we utilize a unique symbolic technique to evaluate decision-tree learners on a large number of datasets, ensuring consistent predictions for a specific test point. We evaluate our approach on commonly used datasets in fairness literature and showcase its effectiveness on a range of bias models.