Current methods for unsupervised domain adaptation typically involve reducing the statistical distance between the source and target samples in the latent space. However, these methods often suffer from issues such as sampling variability, class imbalance, and data privacy concerns. In this study, we present a memory and computation-efficient probabilistic framework that overcomes these problems by extracting class prototypes and aligning the target features with them. Our approach is applicable to various scenarios, including single-source, multi-source, class-imbalance, and source-private domain adaptation. With no need for additional model parameters and only a moderate increase in computation compared to the source model alone, our method achieves competitive performance compared to state-of-the-art methods.