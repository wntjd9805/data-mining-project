Sampling from an unnormalized probability distribution is a key issue in machine learning, with various applications such as Bayesian modeling, latent factor inference, and energy-based model training. Despite decades of research, MCMC methods are still the default approach for sampling, despite their slow convergence. Auxiliary neural models can accelerate MCMC, but the training overhead can be problematic. In this study, we present a novel solution to this problem by introducing a new Hamiltonian dynamics with non-Newtonian momentum. Unlike traditional MCMC methods like Hamiltonian Monte Carlo, our approach does not require a stochastic step. Instead, the proposed deterministic dynamics in an extended state space directly sample the target distribution, defined by an energy function, assuming ergodicity. Alternatively, the dynamics can be seen as a normalizing flow that samples a specified energy model without the need for training. The proposed Energy Sampling Hamiltonian (ESH) dynamics have a simple form that can be solved using existing ODE solvers, but we develop a specialized solver that performs significantly better. ESH dynamics exhibit faster convergence compared to MCMC methods, enabling faster and more stable training of neural network energy models.