A new algorithm called Single-timescale Double-momentum Stochastic Approximation (SUSTAIN) is proposed for addressing stochastic unconstrained bilevel optimization problems. This algorithm focuses on problems where the lower level subproblem is strongly-convex and the upper level objective function is smooth. Unlike previous approaches that utilize two-timescale or double loop techniques, SUSTAIN introduces a stochastic momentum-assisted gradient estimator for both the upper and lower level updates. This enables control over the error in the stochastic gradient updates caused by inaccurate solutions to the subproblems. In the case where the upper objective function is smooth but possibly non-convex, SUSTAIN only requires a certain number of samples to find an ✏-stationary solution. An ✏-stationary solution is defined as a point where the squared norm of the gradient of the outer function is less than or equal to ✏. The total number of stochastic gradient samples needed for the upper and lower level objective functions matches the best-known complexity for single-level stochastic gradient algorithms. Additionally, the algorithm is analyzed for the scenario where the upper level objective function is strongly-convex. The proposed SUSTAIN algorithm provides an efficient solution for stochastic unconstrained bilevel optimization problems.