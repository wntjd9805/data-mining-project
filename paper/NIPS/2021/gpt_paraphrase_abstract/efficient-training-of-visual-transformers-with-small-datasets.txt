Visual Transformers (VTs) are a new type of architecture that offers an alternative to Convolutional networks (CNNs). Unlike CNNs, VTs can capture global relationships between elements in an image and potentially have a greater capacity for representation. However, the absence of the typical convolutional inductive bias in VTs means that they require more data to train effectively compared to CNNs. This is because certain local properties of the visual domain that are embedded in the CNN design need to be learned from samples in VTs. In this study, we empirically examine different VTs to compare their robustness when trained on small datasets. We find that although VTs trained on ImageNet perform similarly in terms of accuracy, their performance on smaller datasets can vary significantly. To address this issue, we propose an additional self-supervised task that extracts extra information from images with minimal computational overhead. This task encourages VTs to learn spatial relations within an image and enhances their robustness when training data is limited. Our approach can be easily integrated into existing VTs without requiring specific architectural modifications. Through extensive evaluations on various VTs and datasets, we demonstrate that our method substantially improves the final accuracy of VTs. The code for our approach can be found at the following GitHub repository: https://github.com/yhlleo/VTs-Drloc.