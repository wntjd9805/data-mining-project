This paper explores the training and generalization of Recurrent Neural Networks (RNNs) with random initialization. It improves upon previous works by addressing two key issues. Firstly, it demonstrates that RNNs can learn functions without requiring normalized conditions on the input sequence. By analyzing the neural tangent kernel matrix, the paper proves a generalization error bound and shows that certain concept classes can be learned with a scaling number of iterations and samples relative to the input length. Secondly, the paper introduces a novel result for learning N-variable functions of an input sequence in the form f (βT [Xl1, ..., XlN ]). This extends beyond the "additive" concept class and shows that when either N or the difference between the maximum and minimum l values is small, f (βT [Xl1, ..., XlN ]) can be learned with a scaling number of iterations and samples in relation to the input length.