The visual world consists of objects and their relationships, which can be described as a structured set. While there has been progress in designing deep neural networks to combine objects, less attention has been given to combining the relationships between objects. This is challenging because object placement is independent, while their relationships are dependent and intertwined. Existing methods address this by using holistic encoders like text or graphs. However, we propose representing each relationship as an unnormalized density, allowing us to combine relationships separately. This factorized approach enables our model to generate and modify scenes with multiple sets of relationships more accurately. Additionally, decomposition enhances our model's understanding of the underlying relational scene structure. For more information, visit our project page at https://composevisualrelations.github.io/.