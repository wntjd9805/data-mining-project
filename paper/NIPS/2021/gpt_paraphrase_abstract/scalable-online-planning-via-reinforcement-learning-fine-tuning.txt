Recent advancements in artificial intelligence (AI) have relied heavily on lookahead search, a crucial component in games like chess, go, and poker. However, the current search methods employed in these games, as well as many other domains, are tabular and do not scale efficiently with the size of the search space. This issue becomes even more pronounced when confronted with stochasticity and partial observability. In this study, we propose a novel approach that replaces tabular search with online model-based fine-tuning of a policy neural network using reinforcement learning. Our experiments demonstrate that this approach surpasses state-of-the-art search algorithms in benchmark settings. Specifically, we apply our search algorithm to self-play Hanabi, achieving a new state-of-the-art result. Furthermore, we showcase the versatility of our algorithm by outperforming tabular search in the Atari game Ms. Pacman.