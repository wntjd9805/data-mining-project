This paper presents a novel approach to reduce the model size and training costs of vision transformers (ViTs) while maintaining accuracy. Instead of training full ViTs, the authors dynamically extract and train sparse subnetworks within a fixed parameter budget. This approach optimizes model parameters and connectivity throughout training, resulting in a final sparse network. The authors also explore structured sparsity by guiding the prune-and-grow process of self-attention heads. Additionally, they introduce a learnable token selector to leverage data and architecture sparsity for further efficiency gains. Experimental results on ImageNet demonstrate the effectiveness of their approach, achieving significant computational cost reduction without compromising generalization. Surprisingly, sparse training can even improve ViT accuracy in some cases. For instance, their sparsified DeiT-Small model achieves a 0.28% increase in top-1 accuracy while saving 49.32% FLOPs and 4.40% running time. The code is available at https://github.com/VITA-Group/SViTE.