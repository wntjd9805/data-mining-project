In recent years, there has been significant interest in enhancing the robustness of neural networks in machine learning. However, most existing training algorithms aimed at improving robustness to adversarial and common corruptions come with a substantial computational overhead. These algorithms often require up to ten times the number of forward and backward passes to achieve convergence. To address this inefficiency, we propose a technique called BulletTrain, which involves mining boundary examples to significantly reduce the computational cost of robust training. Our key insight is that only a small fraction of examples are truly beneficial for enhancing robustness. Therefore, BulletTrain dynamically predicts these crucial examples and optimizes robust training algorithms to prioritize them. We have applied our technique to various robust training algorithms and observed a 2.2× speed-up for TRADES and MART on CIFAR-10, as well as a 1.7× speed-up for AugMix on CIFAR-10-C and CIFAR-100-C, all without compromising clean and robust accuracy.