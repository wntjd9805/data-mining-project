This paper explores the challenge of specifying a reward in reinforcement learning (RL) applications. In this RL setting, the agent can only obtain information about the reward by querying an expert. The expert can evaluate individual states or provide preferences over trajectories. The goal is to learn a model of the reward that allows RL algorithms to achieve high expected returns with minimal expert queries. To address this, the paper presents Information Directed Reward Learning (IDRL), which uses a Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between potentially optimal policies. Unlike previous active reward learning methods, IDRL can accommodate different types of queries and achieves similar or better performance with fewer queries. The focus is shifted from reducing the reward approximation error to improving the policy induced by the reward model. The findings are supported by extensive evaluations in various environments with different query types.