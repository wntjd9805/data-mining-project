We present a framework that views Reinforcement Learning (RL) as a sequence modeling problem. This allows us to utilize the simplicity and scalability of the Transformer architecture, along with advancements in language modeling like GPT-x and BERT. Our proposed architecture, called Decision Transformer, treats RL as conditional sequence modeling and generates optimal actions using a causally masked Transformer. By conditioning an autoregressive model on the desired reward, past states, and actions, Decision Transformer can generate future actions that achieve the desired reward. Despite its simplicity, Decision Transformer achieves performance that matches or surpasses state-of-the-art model-free offline RL baselines on tasks like Atari, OpenAI Gym, and Key-to-Door. The Decision Transformer architecture involves feeding states, actions, and returns into modality-specific linear embeddings, adding positional episodic timestep encoding, and passing tokens through a GPT architecture with causal self-attention mask. Our code is available at the provided link. We also provide an illustrative example of using Decision Transformer to find the shortest path in a fixed graph by formulating it as a reinforcement learning problem. The training dataset consists of random walk trajectories and their per-node returns-to-go. By conditioning on a starting state and generating the largest possible return at each node, Decision Transformer can sequence optimal paths.