In this paper, we introduce convex polytope trees (CPT) as an extension of decision trees. Unlike traditional decision trees that use a single hyperplane to split the data, CPTs utilize a logical disjunction of probabilistic linear decision-makers, represented by convex polytopes in the feature space, to define their splitting function. By allowing for a more flexible decision boundary, CPTs can achieve high accuracy with fewer nodes. To determine the size of the decision-making community at each node, we employ a nonparametric Bayesian prior, which encourages simpler decision boundaries by reducing the number of facets in the polytope. We propose a greedy method for constructing CPTs efficiently and scalable training algorithms when the tree structure is known. Through empirical tests on various classification and regression tasks, we demonstrate the effectiveness of CPTs compared to existing state-of-the-art decision trees.