We demonstrate that diffusion models can generate higher quality images compared to the current leading generative models. In the case of unconditional image synthesis, we enhance the architecture through a series of experiments, resulting in improved sample quality. Additionally, for conditional image synthesis, we enhance sample quality further by incorporating classifier guidance. This method efficiently balances diversity and fidelity by utilizing gradients from a classifier. Our results on ImageNet 128x128 yield an FID of 2.97, while on ImageNet 256x512 it is 4.59. Remarkably, even with only 25 forward passes per sample, our approach matches the performance of BigGAN-deep while maintaining better distribution coverage. Moreover, we find that combining classifier guidance with upsampling diffusion models leads to even better results, achieving an FID of 3.94 on ImageNet 512x256 and 7.72 on ImageNet 512x512.