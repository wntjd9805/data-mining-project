Transductive inference is commonly used in few-shot learning to improve performance compared to inductive methods. However, current few-shot benchmarks use perfectly class-balanced tasks during inference, which is unrealistic as it assumes known and fixed label probabilities. In realistic scenarios, unlabeled query sets have arbitrary and unknown label marginals. We introduce the concept of arbitrary class distributions in few-shot tasks, modeling class probabilities as Dirichlet-distributed random variables. This allows for principled and realistic sampling. We experimentally evaluate state-of-the-art transductive methods on three popular datasets and observe unexpected drops in performance compared to inductive methods. To address this, we propose a generalized mutual-information loss based on α-divergences, which effectively handles variations in class distributions. Our transductive α-divergence optimization outperforms current methods across different datasets, models, and few-shot settings. The code for our approach is publicly available at https://github.com/oveilleux/Realistic_Transductive_Few_Shot.