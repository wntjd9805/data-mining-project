The success of convolutional neural networks (CNNs) lies in their ability to process data locally and in a translationally-invariant manner. Determining which aspect is more important remains a challenge. This study examines the problem using a teacher-student framework for kernel regression, utilizing convolutional kernels inspired by simple CNN architectures. Using heuristic methods, it is found that in the absence of ridge, locality plays a key role in determining the learning curve exponent β, while translational invariance does not. Specifically, when the teacher's filter size is smaller than the student's, β depends only on the student's filter size and not on the input dimension. Empirical evidence supports these findings. Additionally, it is proven that employing a ridge that decreases with the training set size leads to similar learning curve exponents as those obtained in the absence of ridge, under a natural universality assumption.