The problem of recovering support of a sparse vector from simple measurements has been extensively studied in various frameworks such as compressed sensing, 1-bit compressed sensing, and single index models. This abstract introduces generalizations of this problem, namely mixtures of linear regressions and mixtures of linear classifiers, where the objective is to recover the supports of multiple sparse vectors using a small number of linear or 1-bit measurements. The main challenge in these problems is that the measurements from different vectors are randomly mixed. Recent attention has also been given to these problems. In mixtures of linear classifiers, an observation refers to the side of a queried hyperplane where a random unknown vector lies, while in mixtures of linear regressions, we observe the projection of a random unknown vector on the queried hyperplane. The first step in recovering the unknown vectors from the mixture is to identify the support of each individual component vector. This work focuses on studying the sufficient number of measurements needed to recover the supports of all the component vectors in both of these models. The provided algorithms can achieve this with high probability using a polynomial number of measurements in relation to the sparsity of the vectors, logarithm of the dimensionality, and quasi-polynomial factor.