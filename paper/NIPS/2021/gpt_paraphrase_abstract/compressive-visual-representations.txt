Learning visual representations that generalize effectively in the absence of human supervision is a crucial challenge for Machine Learning's application to various tasks. Recent advancements have been made in two self-supervised methods, namely contrastive learning (exemplified by SimCLR) and latent bootstrapping (exemplified by BYOL). In this study, we propose that incorporating explicit information compression into these algorithms can lead to improved and more resilient representations. To validate this hypothesis, we develop SimCLR and BYOL formulations compatible with the Conditional Entropy Bottleneck (CEB) objective, enabling us to measure and control the compression level in the learned representation and observe its impact on downstream tasks. Additionally, we investigate the relationship between Lipschitz continuity and compression, revealing a practical lower bound on the Lipschitz constant of the learned encoders. This connection between Lipschitz continuity and robustness provides a new explanation for the enhanced robustness of compressed models. Our experiments confirm that adding compression to SimCLR and BYOL significantly enhances linear evaluation accuracies and model robustness across diverse domain shifts. Notably, the compressed version of BYOL achieves a 76.0% Top-1 linear evaluation accuracy on ImageNet using ResNet-50 and 78.8% using ResNet-50 2x.1.