In computer vision, it is important to build models that are robust to various image corruptions. New data augmentations have been proposed to improve performance on a benchmark of such corruptions called ImageNet-C. However, there is still a lack of understanding on the relationship between data augmentations and test-time corruptions. To address this, we create a feature space for image transforms and use a measure called Minimal Sample Distance to show a strong correlation between similarity and performance. We find that recent data augmentations do not perform well when test-time corruptions are perceptually dissimilar from ImageNet-C in this feature space. Our findings suggest that training on perceptually similar augmentations can improve test error and that data augmentations may not generalize well beyond the benchmark. We provide code for further exploration.