Continual learning (CL) is a challenging task where a sequence of tasks must be learned without access to independent and identically distributed (i.i.d.) observations. Bayesian learning seems applicable in this context as recursive and one-off Bayesian updates yield the same outcome. However, approximate inference is often necessary for most models, leading to suboptimal solutions across tasks with recursive updating. To address this, we propose an alternative Bayesian approach that continually infers task-conditioned parameter distributions from data. We implement this approach using probabilistic task-conditioned hypernetworks, which we call posterior meta-replay. Our experiments on standard benchmarks demonstrate that our probabilistic hypernetworks effectively compress sequences of posterior parameter distributions with minimal forgetting. We achieve significant performance improvements compared to existing Bayesian CL methods, with task inference being the main limitation. This limitation is independent of the sequential setting, suggesting new avenues for progress in CL.