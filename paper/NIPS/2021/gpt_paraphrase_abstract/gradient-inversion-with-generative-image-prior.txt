Federated Learning (FL) is a decentralized learning framework that aims to protect the privacy of local data by keeping it on clients' devices. Instead of accessing the raw data, the server trains models by analyzing the gradients of the local data. However, this approach can be vulnerable to attacks where an adversary can exploit these gradients to uncover sensitive client information. In this study, we highlight that gradients alone are often insufficient to reconstruct user data without prior knowledge. We demonstrate that by utilizing a generative model that has been pretrained on the data distribution, privacy can be easily breached. Moreover, we explore the possibility of learning this prior knowledge from a sequence of gradients observed during FL training. Our experimental results show that it is indeed possible to learn the prior in the form of a generative model through iterative interactions in FL. These findings emphasize the need for additional privacy mechanisms to prevent leakage of sensitive information in FL.