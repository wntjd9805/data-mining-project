We propose a method called PALMS (Process for Adapting Language Models to Society) with Values-Targeted Datasets to address the issue of harmful and biased outputs from language models. This iterative process involves crafting and fine-tuning the model on a dataset that reflects a predetermined set of target values. We evaluate the effectiveness of PALMS using three metrics: human evaluations of output adherence to target values, toxicity scoring, and analysis of common words associated with social categories. We continuously add training examples based on evaluation feedback to improve the model's behavior. PALMS outperforms baseline and control models across various GPT-3 language model sizes without compromising its capability. Larger models show even better results. Our findings demonstrate that it is possible to significantly adjust language model behavior using a small, carefully curated dataset.