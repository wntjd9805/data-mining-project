Probabilistic modeling of biological sequences has numerous applications in biology and biomedicine, especially with advancements in sequencing technology. However, current methods lack the ability to accurately predict at the nucleotide level for whole genomes. This article introduces a new generative sequence model called the Bayesian embedded autoregressive (BEAR) model. The BEAR model combines a parametric autoregressive model with a nonparametric Bayesian Markov model to create a flexible and scalable framework. The model is applied to various statistical problems such as density estimation, parameter estimation, goodness-of-fit tests, and two-sample tests. The authors provide rigorous asymptotic consistency results and demonstrate the scalability of the BEAR model on datasets with billions of nucleotides. Experimental results on genomic, transcriptomic, and metagenomic sequence data show that the BEAR model outperforms parametric autoregressive models in predictive performance. Overall, the BEAR model offers a promising approach for building and evaluating generative models at the whole genome scale.