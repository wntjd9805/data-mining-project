Reducing reliance on unreliable correlations in the training data is a growing concern in deep learning. Previous methods have incorporated priors on the feature attribution of deep neural networks (DNNs) during training to mitigate this issue. However, these methods often required a trade-off between high-quality attributions and computational efficiency. This resulted in either long training times or ineffective attribution priors. In this study, we address this trade-off by introducing a class of DNNs called nonnegatively homogeneous DNNs or -DNNs. These -DNNs allow for efficient and axiomatically sound feature attribution, which can be computed with just one forward/backward pass. We prove the efficiency of -DNNs and demonstrate that they can be easily constructed from a range of regular DNNs by removing the bias term from each layer. Through various experiments, we show that -DNNs outperform state-of-the-art attribution methods on regular DNNs when trained with attribution priors.