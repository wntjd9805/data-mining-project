The use of Reinforcement Learning (RL) agents to solve complex tasks based on visual observations is becoming more common. However, these agents struggle to generalize their learned skills to new environments. Data augmentation is a promising technique for improving generalization in RL, but it often comes with drawbacks such as decreased sample efficiency and potential divergence. This study aims to address the instability issues caused by data augmentation in common off-policy RL algorithms. The researchers identify two problems related to high-variance Q-targets and propose a simple yet effective method to stabilize these algorithms when using augmentation. The proposed technique is tested extensively using ConvNets and Vision Transformers (ViT) on benchmark tasks from DeepMind Control Suite and robotic manipulation tasks. The results demonstrate significant improvements in stability and sample efficiency for ConvNets under augmentation, with competitive generalization performance compared to state-of-the-art image-based RL methods in environments with unseen visuals. The study also shows that the proposed method can be scaled to RL with ViT-based architectures and emphasizes the importance of data augmentation in this context.