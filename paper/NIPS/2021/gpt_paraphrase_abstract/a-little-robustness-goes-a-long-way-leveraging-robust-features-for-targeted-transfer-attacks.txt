Adversarial examples in neural network image classifiers are known to be transferable, meaning that examples designed to deceive one classifier often deceive others with different architectures. However, targeted adversarial examples, which are crafted to be classified as a specific target class, are less transferable between architectures. Previous research has focused on improving the optimization process to create transferable targeted attacks. In this study, we investigate the role of the source classifier and demonstrate that training the source classifier to be slightly robust to small adversarial examples significantly improves the transferability of targeted attacks, even across different architectures like convolutional neural networks and transformers. These findings shed light on the nature of adversarial examples and the mechanisms behind robust classifiers.