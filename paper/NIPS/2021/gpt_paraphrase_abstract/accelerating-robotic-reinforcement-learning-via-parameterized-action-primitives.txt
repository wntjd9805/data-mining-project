The training of reinforcement learning (RL) agents to solve robotics tasks is still difficult due to challenges in exploration. While most research focuses on improving RL methods, we propose improving the interface between the RL algorithm and the robot. We manually specify a library of robot action primitives (RAPS), which are parameterized with arguments learned by an RL policy. These primitives are expressive, easy to implement, promote efficient exploration, and can be transferred across different robots, tasks, and environments. Through empirical studies on challenging tasks in three domains with image input and sparse terminal rewards, we find that our approach significantly improves learning efficiency and task performance compared to prior methods. Our method outperforms techniques that learn skills from expert data. More information, including code and videos, can be found at https://mihdalal.github.io/raps/.