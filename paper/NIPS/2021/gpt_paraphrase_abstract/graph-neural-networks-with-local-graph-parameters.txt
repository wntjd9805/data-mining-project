Recent proposals have sought to enhance the effectiveness of Graph Neural Networks (GNNs) by enabling the propagation of features between groups of vertices. These "higher-order" GNNs are known to be limited by the k-dimensional Weisfeiler-Leman (WL) test, but their memory requirements of O(nk) restrict their practicality. Other approaches incorporate local higher-order graph structural information into GNNs from the beginning, which allows them to maintain the desirable O(n) memory requirement of GNNs, albeit with a one-time, potentially non-linear, preprocessing step. To study these approaches, we propose local graph parameter enabled GNNs as a framework. We accurately characterize their distinguishing power using a variant of the WL test and identify the graph structural properties they can consider. Local graph parameters can be easily incorporated into any GNN architecture and are computationally inexpensive. In terms of expressive power, our proposal lies between GNNs and their higher-order counterparts. Additionally, we suggest various techniques for selecting appropriate local graph parameters. Our findings establish connections between GNNs and important concepts in finite model theory and finite variable logics. Experimental evaluation demonstrates that adding local graph parameters often improves the performance of different GNNs, datasets, and graph learning tasks.