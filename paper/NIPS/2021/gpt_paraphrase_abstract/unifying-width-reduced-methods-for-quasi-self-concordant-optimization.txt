We propose various algorithms for solving convex optimization problems with constraints, such as softmax, `p regression, and logistic regression. Our approach is centered around the concept of width reduction, which has been highly effective in improving the iteration complexity of problems like maximum flow and `p regression. However, a limitation of existing methods is that they require problem-specific potentials and tailored analyses. Our main contribution is introducing a new approach that achieves improved rates for a broader class of problems, including quasi-self-concordant losses like logistic regression. To accomplish this, we develop a unified width reduction method that can handle these losses using a more general set of potentials. Moreover, we achieve these improved rates in the constrained setting without the need for explicit acceleration schemes, complementing recent work based on a ball-oracle approach.