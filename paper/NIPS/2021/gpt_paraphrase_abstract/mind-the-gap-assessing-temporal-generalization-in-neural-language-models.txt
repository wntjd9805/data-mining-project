Language is constantly changing and evolving, but current language models are trained and evaluated on static data. We show that Transformer-XL models perform poorly when predicting future utterances beyond their training period, and this performance worsens over time. Increasing model size doesn't solve this issue, but models that continually update their knowledge can mitigate the problem. With the availability of larger datasets and the need for up-to-date information in language-model-based applications, we propose a shift towards adaptive language models that can keep up with our dynamic world. We provide dynamic language modelling benchmarks to support the evaluation of models that consider temporal dynamics.