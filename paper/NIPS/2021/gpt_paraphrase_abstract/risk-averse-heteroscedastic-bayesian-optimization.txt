Black-box optimization tasks in high-stakes scenarios often require risk-averse decision-making. However, the traditional Bayesian optimization (BO) approach only optimizes the expected value. In this study, we extend BO by considering both the mean and input-dependent variance of the objective, which are assumed to be unknown initially. We introduce a new algorithm called risk-averse heteroscedastic Bayesian optimization (RAHBO) that aims to find a solution with high return and low noise variance while learning the noise distribution dynamically. To achieve this, we model both the expectation and variance as unknown functions using the reproducing kernel Hilbert space (RKHS) approach and propose a novel acquisition function that incorporates risk considerations. We provide regret bounds for our approach and present a robust rule for reporting the final decision point in situations where only one solution needs to be identified. We showcase the effectiveness of RAHBO through experiments on synthetic benchmark functions and hyperparameter tuning tasks.