Document intelligence has become increasingly important in automating information extraction from documents for various business applications. Recent advancements in self-supervised learning methods have shown promise in reducing the need for manual annotation by training models on large-scale unlabeled document datasets. However, existing document pretraining methods still heavily focus on language. To address this limitation, we propose UDoc, a unified pretraining framework for document understanding. UDoc extends the Transformer model to incorporate multimodal embeddings as input, combining words and visual features from semantic regions in document images. Notably, UDoc leverages three self-supervised losses to learn a generic representation that models sentences, captures similarities, and aligns different modalities. Through extensive empirical analysis, we demonstrate that UDoc's pretraining procedure leads to improved joint representations and enhanced performance in downstream tasks.