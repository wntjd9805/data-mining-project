The mean field theory of multilayer neural networks focuses on an infinite-width scaling, where the learning dynamics closely follow the mean field limit. However, little research has been done on the random fluctuations around this limit, especially in the case of multilayer networks. This is due to the challenge of capturing the stochastic dependency across both time and depth. In this study, we explore these fluctuations in multilayer networks using the neuronal embedding framework. We derive a system of dynamical equations, called the second-order mean field limit, to capture the distribution of these fluctuations. Through this framework, we uncover the complex interactions, stochasticity with cross-layer dependency, and nonlinear time evolution present in these fluctuations. We prove a limit theorem that quantitatively relates this limit to the fluctuations observed in large-width networks. We apply this result to demonstrate a stability property of gradient descent mean field training, where the network progressively biases towards a solution with minimal fluctuation in the learned output function. This phenomenon extends previous findings in shallow networks with a squared loss to multilayer networks with a more general loss function that is not necessarily convex.