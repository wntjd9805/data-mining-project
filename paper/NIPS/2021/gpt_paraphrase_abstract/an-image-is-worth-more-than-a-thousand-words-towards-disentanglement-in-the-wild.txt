Unsupervised disentanglement is considered impossible without biases on models and data. Instead, recent methods use limited supervision to identify and separate factors of variation. However, enumerating all factors for a real-world image distribution is impractical. Therefore, we propose a method that disentangles partially labeled factors while also separating unmentioned residual factors. Our success on synthetic benchmarks allows us to utilize image descriptors to annotate a subset of attributes in real image domains with minimal effort. We employ a language-image embedding model (CLIP) to annotate attributes in a zero-shot manner, achieving state-of-the-art disentangled image manipulation results.