We propose a Bayesian optimization method for networks of functions that takes into account the intermediate output within the network. This approach improves query efficiency compared to standard methods by leveraging information that is typically ignored. We achieve this by modeling the network nodes using Gaussian processes and selecting evaluation points based on the expected improvement computed with respect to the posterior on the objective. Although the posterior is non-Gaussian, we can efficiently maximize our acquisition function using sample average approximation. We prove the asymptotic consistency of our method, meaning it finds a globally optimal solution as the number of evaluations increases, even if the domain is not densely evaluated. Furthermore, we demonstrate that our approach significantly outperforms standard Bayesian optimization methods in various synthetic and real-world problems.