Training data and test data in machine learning often have different distributions, which can pose challenges. This is caused by various factors such as natural shifts, blind spots, or adversarial examples, which are referred to as out-of-distribution (OOD) test examples. We propose a model that allows for abstaining from making predictions at a fixed cost. Our transductive abstention algorithm takes labeled training examples and unlabeled test examples as input and provides predictions with optimal loss guarantees. The loss bounds are similar to standard generalization bounds when test examples are independent and identically distributed (i.i.d.) from the training distribution. However, we introduce an additional term that represents the cost of abstaining multiplied by the statistical distance between the train and test distribution, or the fraction of adversarial examples. For linear regression, we present a polynomial-time algorithm based on Celis-Dennis-Tapia optimization algorithms. For binary classification, we demonstrate an efficient implementation using a proper agnostic learner, specifically an Empirical Risk Minimizer, for the relevant class. Our work builds upon a recent abstention algorithm by Goldwasser, Kalais, and Montasser for transductive binary classification.