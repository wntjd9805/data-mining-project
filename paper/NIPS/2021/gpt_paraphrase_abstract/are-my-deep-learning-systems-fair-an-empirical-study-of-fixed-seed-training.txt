Deep learning systems have become increasingly popular in important tasks like credit evaluation and crime prediction. However, these systems require fairness to ensure they do not violate compliance laws and have negative social consequences. Recent research has shown that implementing deep learning software can introduce variance, resulting in different models even with identical training runs. This variance can lead to fairness violations in deep learning models and networks. This study is the first empirical investigation into the impact of software implementation on fairness and variance in deep learning systems. By examining 22 mitigation techniques and five baselines, we found that identical training runs with the same seeds can have up to 12.6% fairness variance. Furthermore, most debiasing algorithms negatively affect the model by reducing accuracy, increasing fairness variance, or increasing accuracy variance. Our survey of the literature revealed that only 34.4% of papers evaluating fairness in artificial intelligence conferences use multiple identical training runs, raising concerns about the validity of their results. We advocate for better evaluation and testing protocols to enhance fairness and fairness variance in deep learning systems, as well as improve the validity and reproducibility of deep learning research.