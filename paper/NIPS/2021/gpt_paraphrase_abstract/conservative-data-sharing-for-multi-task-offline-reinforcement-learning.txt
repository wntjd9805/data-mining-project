Offline reinforcement learning (RL) algorithms have achieved promising results in domains with abundant pre-collected data. However, existing methods focus on solving individual problems separately without considering how an offline RL agent can acquire multiple skills. We propose that offline RL can be effectively used in scenarios where large amounts of data from various scenarios can be pooled together to learn behaviors for multiple tasks collectively. However, sharing data across tasks in multi-task offline RL has been found to perform poorly in practice. Through empirical analysis, we have determined that data sharing can worsen the difference between the learned policy and the dataset, leading to policy divergence and subpar performance. To overcome this challenge, we have developed a technique called conservative data sharing (CDS) that routes data based on task-specific improvement. CDS can be combined with various single-task offline RL methods. In challenging multi-task locomotion, navigation, and vision-based robotic manipulation problems, CDS achieves comparable or superior performance compared to previous offline multi-task RL methods and data sharing approaches.