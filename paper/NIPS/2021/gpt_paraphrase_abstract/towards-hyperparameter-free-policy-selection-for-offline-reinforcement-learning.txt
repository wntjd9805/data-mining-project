Selecting between policies and value functions generated by different training algorithms in offline reinforcement learning is a crucial task for hyperparameter tuning. Current methods based on off-policy evaluation often require additional function approximation, leading to a dilemma. In this study, we propose hyperparameter-free algorithms for policy selection using BVFT, a recent theoretical advancement in value-function selection. We demonstrate the effectiveness of these algorithms in discrete-action benchmarks like Atari. To address performance degradation in continuous-action domains, we combine BVFT with off-policy evaluation to achieve the best results. Additionally, this combination yields a hyperparameter-tuning method for Q-function based off-policy evaluation with guaranteed theoretical outcomes.