Recent research suggests that robust training requires larger datasets than standard classification. This is evident in the significant difference in robust accuracy between models trained solely on the original dataset and those incorporating additional data from the "80 Million Tiny Images" dataset (80M-TI) on CIFAR-10 and CIFAR-100. In this study, we investigate how generative models trained exclusively on the original dataset can artificially expand the size of the training set and enhance robustness against perturbations bounded by the p-norm. We establish the conditions under which incorporating generated data can improve robustness and demonstrate that even the inclusion of non-realistic random data, generated through Gaussian sampling, can enhance robustness. Our evaluation on CIFAR-10, CIFAR-100, SVHN, and TINYIMAGENET against perturbations bounded by the ∞ and 2 norms reveals significant improvements in robust accuracy compared to previous state-of-the-art methods. For ∞ norm-bounded perturbations of size 8/255, our models achieve robust accuracies of 66.10% and 33.49% on CIFAR-10 and CIFAR-100, respectively, surpassing the state-of-the-art by +8.96% and +3.29%. Against 2 norm-bounded perturbations of size 128/255, our model achieves 78.31% robust accuracy on CIFAR-10, an improvement of +3.81%. These results outperform most prior works utilizing external data.