Recent advancements in learning neural radiance fields have allowed for realistic generation of novel views of a scene. However, these methods are limited to synthesizing images under the original fixed lighting conditions, making them inflexible for tasks such as relighting, scene editing, and scene composition. To address this limitation, recent approaches have proposed disentangling reflectance and illumination from the radiance field. However, these methods fail to consider participating media and only account for direct or limited indirect illumination, resulting in energy loss and neglecting high-order indirect illumination. In this study, we propose a novel method that learns neural representations for participating media by simulating global illumination. We estimate direct illumination using ray tracing and compute indirect illumination using spherical harmonics, avoiding the need for lengthy indirect bounces and eliminating energy loss. Our experiments demonstrate that our approach achieves superior visual quality and numerical performance compared to state-of-the-art methods, and it can also handle solid objects with opaque surfaces effectively.