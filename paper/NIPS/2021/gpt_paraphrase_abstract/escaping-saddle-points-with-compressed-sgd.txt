Stochastic gradient descent (SGD) is a widely used optimization method for distributed machine learning. However, in a distributed setting, communication becomes a bottleneck. To address this issue, gradient compression techniques have been developed. Previous research has shown that SGD combined with gradient compression can converge to an ε-ﬁrst-order stationary point. This paper extends these findings to convergence to an ε-second-order stationary point (ε-SOSP), which is a novel contribution. Furthermore, the study demonstrates that when the stochastic gradient is not Lipschitz, compressed SGD with the RAN-DOMK compressor can achieve convergence to an ε-SOSP in the same number of iterations as uncompressed SGD. Moreover, this compressed SGD method improves total communication by a factor of˜Θ( dε−3/4), where d represents the dimension of the optimization problem. The paper also presents additional results for cases involving arbitrary compressors and Lipschitz stochastic gradients.