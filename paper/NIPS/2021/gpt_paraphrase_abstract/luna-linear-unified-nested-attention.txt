The Transformer's attention mechanism has limited scalability due to its quadratic computational and memory complexities. To address this issue, we propose Luna, a linear unified nested attention mechanism. Luna approximates softmax attention using two nested linear attention functions, resulting in linear time and space complexity. Unlike traditional attention mechanisms, Luna incorporates an additional fixed-length sequence as input and output, enabling linear attention operations while storing contextual information effectively. We evaluate Luna on three sequence modeling benchmarks and achieve competitive or superior results compared to strong baseline methods, including full-rank attention and other efficient sparse and dense attention approaches. Our model's implementation can be found at https://github.com/XuezheMax/fairseq-apollo.