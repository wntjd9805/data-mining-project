Efficiently learning from limited data has been a major focus in model-based reinforcement learning, whether it is in the online case of interacting with the environment or the offline case of learning from a fixed dataset. However, no single algorithm has been able to achieve state-of-the-art results in both settings. This study introduces the Reanalyse algorithm, which utilizes model-based policy and value improvement operators to generate improved training targets based on existing data points. This approach enables efficient learning across a wide range of data budgets. Additionally, Reanalyse can also be applied to learn solely from demonstrations without any environment interactions, similar to offline reinforcement learning. By combining Reanalyse with the MuZero algorithm, a unified algorithm called MuZero Unplugged is introduced, which can handle any data budget, including offline RL. Notably, MuZero Unplugged achieves new state-of-the-art results in both the RL Unplugged offline RL benchmark and the online RL benchmark of Atari in the standard 200 million frame setting. Importantly, this algorithm does not require any special adjustments for off-policy or offline RL settings.