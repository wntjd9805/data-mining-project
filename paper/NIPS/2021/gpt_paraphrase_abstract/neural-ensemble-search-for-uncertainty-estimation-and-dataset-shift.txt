Ensembles of neural networks outperform individual networks in terms of accuracy, uncertainty calibration, and robustness to dataset shift. While deep ensembles, a leading uncertainty estimation method, only use different initializations of a fixed architecture, we propose two methods to automatically create ensembles with diverse architectures. These methods balance the strengths of individual architectures with the ensemble's diversity and leverage architectural variation for improved performance. Our experiments on various classification tasks and modern architecture search spaces demonstrate that these ensembles surpass deep ensembles in accuracy, uncertainty calibration, and dataset shift robustness. Further analysis confirms that the architectural variation leads to higher ensemble diversity, enabling the ensembles to outperform deep ensembles even with weaker base learners. To support reproducibility, our code is available at https://github.com/automl/nes.