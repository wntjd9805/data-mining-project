This study explores stochastic non-convex optimization problems where the objective is to find an approximate stationary point through an expectation over smooth loss functions. The commonly used variance reduction techniques are effective in achieving tight convergence rates that match the lower bounds. However, these techniques pose a challenge in hyperparameter tuning, specifically in maintaining anchor points and selecting appropriate "mega-batchsizes". A recent method called STORM eliminates the need for anchor points and large batchsizes by employing recursive momentum, but it relies on prior knowledge of smoothness and gradient norm bounds. To address these limitations, this study introduces STORM+, a parameter-free method that does not require large batchsizes and achieves the optimal O(1/T 1/3) rate for finding an approximate stationary point. STORM+ builds upon the STORM algorithm and incorporates a novel approach to dynamically adjust the learning rate and momentum parameters.