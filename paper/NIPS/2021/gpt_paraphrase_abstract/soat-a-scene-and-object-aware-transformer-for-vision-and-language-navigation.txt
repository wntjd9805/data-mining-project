This study introduces a transformer-based agent for vision-and-language navigation (VLN) that utilizes two visual encoders - a scene classification network and an object detector. These encoders generate features that correspond to scene descriptions and object references, respectively. By incorporating high-level contextual information from scene features, the model enhances object-level processing. Additionally, the agent leverages vision-and-language pretraining to significantly improve performance on the Room-to-Room (R2R) and Room-Across-Room (RxR) benchmarks, achieving a 1.8% absolute increase in SPL on R2R and a 3.7% absolute increase in SR on RxR. The analysis demonstrates even greater improvements for navigation instructions with six or more object references, indicating the agent's proficiency in utilizing object features and aligning them with instructions.