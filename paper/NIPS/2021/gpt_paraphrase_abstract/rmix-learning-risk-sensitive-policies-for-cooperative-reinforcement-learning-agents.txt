Existing methods in multi-agent reinforcement learning optimize individual Q values to guide agents' behaviors through centralized training with decentralized execution (CTDE). However, these methods fail to effectively train agents in complex environments due to the randomness of rewards and uncertainty in the environment. To address these issues, we propose a new method called RMIX. RMIX is a cooperative multi-agent reinforcement learning approach that utilizes the Conditional Value at Risk (CVaR) measure over the learned distributions of individual Q values. We first learn the return distributions of individuals and calculate CVaR for decentralized execution. To handle the stochastic outcomes during execution, we introduce a dynamic risk level predictor for risk level tuning. Lastly, we optimize the CVaR policies using CVaR values to estimate the target in TD error during centralized training, and the CVaR values are used as auxiliary local rewards for updating the local distribution via Quantile Regression loss. Our empirical results demonstrate that RMIX outperforms state-of-the-art methods in various multi-agent risk-sensitive navigation scenarios and challenging StarCraft II cooperative tasks. RMIX improves coordination among agents and enhances sample efficiency.