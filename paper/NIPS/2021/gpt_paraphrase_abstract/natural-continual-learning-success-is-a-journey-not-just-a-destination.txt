Biological organisms can learn and retain multiple tasks without experiencing a significant decline in performance, whereas artificial agents often suffer from "catastrophic forgetting" where their ability to perform previous tasks diminishes as they learn new ones. To address this issue, researchers have developed methods that encourage artificial agents to maintain proximity to previous task parameters. These methods include using specific parameter regularizers or guiding the optimization process by projecting gradients into subspaces that do not interfere with previous tasks. However, these techniques often yield subpar results, particularly in recurrent neural networks, which are important for studying neural dynamics in biological learning. To overcome these limitations, we propose Natural Continual Learning (NCL) â€“ a novel method that combines weight regularization and projected gradient descent. NCL utilizes Bayesian weight regularization to ensure good performance on all tasks and incorporates gradient projection with prior precision to prevent catastrophic forgetting during optimization. Our approach outperforms standard weight regularization techniques and projection-based methods in both feedforward and recurrent networks. Moreover, the trained networks exhibit task-specific dynamics that are preserved even as new tasks are learned, resembling findings in biological circuits.