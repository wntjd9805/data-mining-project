Adaptive masking of network weights during training is a successful method for sparsifying deep neural networks. By studying this masking process in the linear case, we discover a connection between adaptive methods and regularization through the "Î·-trick", which portrays both as reweighted optimizations carried out iteratively. We establish that any dropout strategy that adapts to the weights in a monotonic manner corresponds to a subquadratic regularization penalty, resulting in sparse solutions. We determine the effective penalties for various popular sparsification strategies, which closely resemble traditional penalties used in sparse optimization. Through an examination of variational dropout, we observe similar empirical behavior between adaptive dropout and classical methods in the context of deep network sparsification, thus confirming our theory.