We examine the statistical guarantees for Imitation Learning (IL) in episodic MDPs. Previous research has shown that in the worst case, a learner who can actively query the expert policy will experience suboptimality that grows quadratically with the horizon length. We study IL under the assumption of µ-recoverability, which states that the difference in Q-values under the expert policy across actions in a state does not deviate beyond a certain threshold. We demonstrate that a proposed reduction is statistically optimal, resulting in a suboptimality bound that is proportional to the product of the threshold, state space size, horizon length, and number of episodes. We also show that algorithms that do not interact with the MDP and instead use an offline dataset of expert trajectories will have suboptimality that grows quadratically with the product of the state space size and horizon length, even under the µ-recoverability assumption. This highlights a clear distinction between the active and no-interaction settings. We also investigate IL with linear function approximation and show that with high probability, the suboptimality of behavior cloning is proportional to the product of the feature dimension, horizon length, and number of rollouts. This can be improved if the expert has parameter-sharing across time steps. Furthermore, we examine the performance of the MIMIC-MD algorithm in the function approximation setting when the MDP transition structure is known. We introduce a new problem called conﬁdence set linear classiﬁcation, which can be used to construct sample-efficient IL algorithms.