The Transformer model has been successful in self-supervised pre-training for Natural Language Processing (NLP), but its potential in visual self-supervised learning has not been fully explored. Existing methods primarily focus on high-level features and global representations, which may not effectively transfer to dense prediction tasks that require attention to local features. To address this, we propose a new approach called Masked Self-supervised Transformer (MST) that captures both local context and global semantic information in images. Inspired by Masked Language Modeling (MLM) in NLP, we introduce a masked token strategy using multi-head self-attention maps to selectively mask tokens in local patches without compromising the structure needed for self-supervised learning. Additionally, we use a global image decoder to recover the masked tokens and preserve spatial information, making it suitable for downstream dense prediction tasks. Our experiments on various datasets demonstrate the effectiveness and versatility of MST. For example, with only 300-epoch pre-training, MST achieves a Top-1 accuracy of 76.9% on DeiT-S, outperforming supervised methods and comparable variants like DINO. In dense prediction tasks, MST achieves 42.7% mAP on MS COCO object detection and 74.04% mIoU on Cityscapes segmentation with just 100-epoch pre-training.