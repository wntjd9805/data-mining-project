To achieve efficiency in online episodic reinforcement learning, it is important to balance exploration and exploitation effectively. Progress has been made in characterizing the minimax-optimal regret in finite-horizon episodic Markov decision processes. However, existing solutions either have high memory requirements or are not optimal unless the sample size is very large.To address this issue, we have developed a novel model-free algorithm that achieves near-optimal regret with a significantly smaller sample size. Our algorithm has a space complexity of O(SAH) and requires a sample size on the order of SA poly(H) to achieve optimal performance. This represents a significant improvement over previous memory-efficient algorithms, reducing the initial burn-in cost by a factor of S5A3.Our algorithm utilizes a variance reduction strategy called reference-advantage decomposition and employs an early-settled reference update rule. It utilizes two Q-learning sequences with upper and lower confidence bounds. This design principle for early-settled variance reduction may be applicable to other reinforcement learning scenarios that involve complex exploration-exploitation trade-offs.