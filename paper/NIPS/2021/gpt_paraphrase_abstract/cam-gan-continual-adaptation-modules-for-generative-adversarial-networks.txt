We introduce a novel method for continual learning in generative adversarial networks (GANs) by utilizing parameter-efficient feature map transformations. Our approach involves learning a set of global parameters that remain fixed across tasks, while task-specific parameters serve as local adapters for each task, allowing for efficient acquisition of task-specific feature maps. Additionally, we suggest the use of element-wise addition of residual bias in the transformed feature space, which aids in stabilizing GAN training in such scenarios. Our method also takes advantage of task similarities based on the Fisher information matrix, leading to significant improvements in model performance by leveraging knowledge from previous tasks. Moreover, this similarity measure reduces parameter growth in continual adaptation and facilitates learning a compact model. In comparison to recent approaches for continually-learned GANs, our proposed method offers a memory-efficient solution for effective continual data generation. Extensive experiments on diverse and challenging datasets demonstrate that the feature-map-transformation approach surpasses state-of-the-art methods for continually-learned GANs while utilizing significantly fewer parameters. Furthermore, our method generates high-quality samples that can enhance the generative-replay-based continual learning for discriminative tasks.