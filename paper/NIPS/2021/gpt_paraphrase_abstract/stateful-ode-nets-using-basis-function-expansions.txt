The introduction of ordinary differential equation networks (ODE-Nets) has established a valuable connection between deep learning and dynamical systems. This study focuses on reevaluating the formulation of weights as continuous functions using linear combinations of basis functions, which enables the utilization of parameter transformations like function projections. This perspective allows the development of a new stateful ODE-Block that can handle stateful layers. The advantages of this innovative ODE-Block are two-fold: firstly, it facilitates the incorporation of meaningful continuous-in-depth batch normalization layers, leading to exceptional performance; secondly, it enables weight compression through a change of basis, without the need for retraining, while still maintaining high performance and reducing inference time and memory usage. The effectiveness of this approach is demonstrated through experiments involving image classification tasks with convolutional units and sentence-tagging tasks with transformer encoder units.