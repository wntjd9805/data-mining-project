We propose a method called conditional transport (CT) to measure the difference between two probability distributions, the source and target. CT utilizes the chain rule and Bayes' theorem to construct both a forward and backward component. The forward CT calculates the expected cost of moving a data point from the source to the target, considering their joint distribution defined by the product of the source probability density function (PDF) and a source-dependent conditional distribution derived from Bayes' theorem. The backward CT is defined by reversing the direction. To make CT cost computationally feasible, we approximate it by replacing the source and target PDFs with their discrete empirical distributions supported on mini-batches. This allows us to apply implicit distributions and optimize using stochastic gradient descent. When applied to training a generative model, CT effectively balances mode-covering and mode-seeking behaviors, preventing mode collapse. Experimental results on various benchmark datasets demonstrate that replacing the default statistical distance of a generative adversarial network with CT consistently improves performance. We provide PyTorch code for implementation.