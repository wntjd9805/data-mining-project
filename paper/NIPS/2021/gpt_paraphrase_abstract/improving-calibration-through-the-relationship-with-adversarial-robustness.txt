Neural networks are vulnerable to small changes in inputs that lead to incorrect predictions, making them lacking in adversarial robustness. This vulnerability also affects the trustworthiness of the model's predictions. In this study, we explore the relationship between adversarial robustness and calibration, discovering that inputs that are easily attacked tend to have poorly calibrated predictions. Building upon this insight, we propose a method called Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) that improves calibration by adjusting labels based on how easily an example can be attacked. Our approach considers the adversarial robustness of the data being used, resulting in improved calibration even when there are changes in the distribution of the data. Furthermore, AR-AdaLS can be applied to ensemble models to enhance calibration.