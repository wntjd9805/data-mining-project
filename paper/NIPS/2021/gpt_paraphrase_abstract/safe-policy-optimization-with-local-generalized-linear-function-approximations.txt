Safe exploration is crucial for implementing reinforcement learning (RL) in safety-critical systems. However, existing safe exploration techniques have limitations when applied to large-scale real problems. To address this issue, we propose a new algorithm called SPO-LF. This algorithm optimizes an agent's policy by learning the relationship between locally available sensor features and environmental reward/safety using generalized linear function approximations. We provide theoretical guarantees on the safety and optimality of SPO-LF. Our experimental results demonstrate that our algorithm is more efficient in terms of sample complexity and computational cost, and it is also more applicable to large-scale problems compared to previous safe RL methods with theoretical guarantees. Additionally, our algorithm is comparable in terms of sample efficiency and safety to existing advanced deep RL methods with safety constraints.