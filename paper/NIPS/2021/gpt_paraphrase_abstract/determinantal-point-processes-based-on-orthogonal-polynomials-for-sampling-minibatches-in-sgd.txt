Stochastic gradient descent (SGD) is a fundamental technique in machine learning. When dealing with a large dataset, SGD often relies on constructing an unbiased gradient estimator using a small subset called a minibatch. The default approach is to uniformly sample a subset, but there have been alternative methods explored to reduce variance. One such method is using determinantal point processes (DPPs) to draw minibatches, which favor diversity among selected items. However, the understanding of how and why DPPs are effective has been challenging. In this study, we propose a new approach called orthogonal polynomial-based determinantal point process paradigm for minibatch sampling in SGD. Our method takes advantage of the specific data distribution, making it more sensitive and powerful than existing data-agnostic methods. We provide a theoretical analysis of our approach, showing that specific DPPs and controlled approximations lead to gradient estimators with faster decaying variance compared to uniform sampling. This implies that DPP minibatches result in a smaller mean square approximation error compared to uniform minibatches when using a large enough batchsize and fixed gradient evaluation budget. Additionally, our estimators can be used with an algorithm that directly samples linear statistics of DPPs, reducing computational overhead. We support our claims with synthetic and real data experiments.