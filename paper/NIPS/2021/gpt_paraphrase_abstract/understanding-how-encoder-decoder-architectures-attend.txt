Encoder-decoder networks with attention have been effective in solving sequence-to-sequence tasks. Although attention helps align encoder and decoder states and visualize network behavior, the mechanisms behind generating attention matrices remain unclear. Additionally, how these mechanisms vary across different encoder and decoder architectures is not well understood. This study aims to explore how encoder-decoder networks tackle various sequence-to-sequence tasks. The researchers propose a method to decompose hidden states into temporal and input-driven components, shedding light on the formation of attention matrices. The findings reveal that networks prioritize either temporal or input-driven components based on task requirements, irrespective of the architecture used. These insights enhance our understanding of attention-based encoder-decoder networks.