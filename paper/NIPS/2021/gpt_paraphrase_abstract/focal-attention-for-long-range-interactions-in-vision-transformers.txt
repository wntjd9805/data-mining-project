Vision Transformer and its variants have shown promise in computer vision tasks. The key to their success is their ability to capture local and global visual dependencies through self-attention. However, this also poses challenges due to the computational overhead, especially for high-resolution tasks like object detection. Previous attempts to reduce the cost and improve performance have either used coarse-grained global attention or fine-grained local attention, but both approaches limit the modeling power of the original self-attention mechanism. In this study, we introduce focal attention, a new mechanism that combines fine-grained local and coarse-grained global interactions. With focal attention, we develop a new variant called Focal Transformers, which outperform existing Vision Transformers on image classification and object detection benchmarks. Our Focal Transformers achieve top performance on ImageNet classification and outperform the current state-of-the-art Swin Transformers in object detection across six different methods. The largest Focal Transformer achieves top performance in COCO and ADE20K datasets, setting new benchmarks in challenging computer vision tasks.