We propose a method for contrastive representation learning in semantic segmentation. By utilizing existing feature extractors, we can identify corresponding regions across images and use them as auxiliary labels. These labels help guide the selection of positive and negative samples at the pixel level, improving the effectiveness of contrastive learning. We demonstrate that auxiliary labels can be generated from different types of feature extractors, including unsupervised contrastive learning and small labeled data. We also introduce a new metric to evaluate the quality of auxiliary-labeling strategies and analyze various factors that impact the performance of contrastive learning. Our method performs well in both low-data and high-data scenarios on different datasets. Compared to standard ImageNet pre-training and other existing approaches, our auxiliary-labeling approach consistently enhances semantic segmentation accuracy.