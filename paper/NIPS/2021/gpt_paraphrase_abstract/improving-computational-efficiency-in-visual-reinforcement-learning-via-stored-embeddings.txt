Recent advancements in deep reinforcement learning (RL) have achieved impressive results in complex tasks using visual observations. However, these techniques require significant memory and computational resources. This paper introduces Stored Embeddings for Efficient Reinforcement Learning (SEER), a modification to existing off-policy RL methods that addresses these resource requirements. SEER reduces the computational overhead by freezing lower layers of convolutional neural network (CNN) encoders early in training. This is possible due to the early convergence of their parameters. Additionally, SEER reduces memory requirements by storing low-dimensional latent vectors instead of high-dimensional images for experience replay. This enables an adaptive increase in the replay buffer capacity, which is particularly useful in memory-constrained settings. Experimental results demonstrate that SEER maintains the performance of RL agents while significantly reducing computation and memory usage across various DeepMindControl environments and Atari games.