The lottery ticket hypothesis suggests that pruning the weights of randomly initialized neural networks is crucial for achieving good performance. Previous studies have shown that pruning alone can lead to impressive results, without the need for weight optimization. However, this pruning approach requires a larger number of parameters and memory space compared to weight optimization. To address this inefficiency, we propose a new framework called IteRand, which prunes randomly initialized neural networks by iteratively randomizing weight values. We provide a theoretical approximation theorem that demonstrates the effectiveness of randomizing operations in reducing the required number of parameters. Our empirical experiments on CIFAR-10 and ImageNet also confirm the efficiency of our approach. The code for our framework is available at https://github.com/dchiji-ntt/iterand.