This paper examines the process of estimating models in offline model-based reinforcement learning (MBRL) to facilitate policy improvement. The authors propose using a weight that considers the ratio of state-action distributions between offline data and real future data, which is challenging to estimate accurately in off-policy evaluation. As an alternative, they suggest using the ratio of state-action distributions between offline data and simulated future data, which can be estimated more easily using standard density ratio estimation techniques. The paper introduces a loss function and an optimization algorithm based on this artificial weight for offline MBRL. The authors argue that weighting with the artificial weight provides an upper bound of the policy evaluation error. Numerical experiments demonstrate the effectiveness of this approach.