Adversarial training is a defense mechanism against adversarial examples. Previous research has shown that wider neural networks perform better in adversarial training. However, it is unclear how network width affects model robustness. This study investigates the relationship between network width and model robustness. The findings reveal that model robustness is influenced by the tradeoff between natural accuracy and perturbation stability, which is regulated by a parameter called 位. Wider networks achieve higher natural accuracy but lower perturbation stability, resulting in potentially worse overall model robustness. The study also explores the connection between perturbation stability and the network's local Lipschitzness. Theoretical analysis using neural tangent kernels demonstrates that wider networks tend to have poorer perturbation stability. The analysis suggests that the common practice of fine-tuning 位 on small networks and then applying it to wider models can lead to decreased model robustness. To fully harness the robustness potential of wider models, it is necessary to appropriately increase 位. Finally, a new method called Width Adjusted Regularization (WAR) is proposed, which dynamically scales 位 for wider models and significantly reduces the tuning time.