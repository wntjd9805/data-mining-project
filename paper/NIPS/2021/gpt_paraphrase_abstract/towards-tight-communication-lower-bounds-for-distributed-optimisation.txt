In a distributed optimization setting, N machines with d-dimensional functions aim to collectively minimize the sum of their individual functions. This problem is commonly encountered in large-scale distributed function optimization, where various forms of gradient descent are typically applied. This study focuses on the communication complexity of this problem and presents the first unconditional bounds on the total number of bits that need to be exchanged between the N machines for a given error tolerance. The research shows that a total of ⌦(N d log d/N ") bits should be communicated among the machines to achieve an additive ✏-approximation to the minimum. The findings hold true for both deterministic and randomized algorithms and do not rely on any assumptions about the algorithm's structure. The lower bound is proven to be tight under certain parameter restrictions and is approximately matched for quadratic objectives by a new variant of quantized gradient descent, which is explained and analyzed. These results introduce communication complexity techniques to distributed optimization and hold potential for further applications.