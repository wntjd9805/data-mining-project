This paper proposes a method to speed up stochastic gradient descent (SGD) by parallelizing it across multiple workers. The traditional approach of averaging all stochastic gradients at every step requires excessive communication between workers and the server, limiting the benefits of parallelism. The Local SGD method suggests that machines should take many local steps before communicating with the server. Previous analysis showed that a minimum of Ω(T) communications were needed for T local gradient steps, but recent papers have improved this to Ω(N (poly(log T))) communications. This paper suggests a Local SGD scheme that communicates less frequently as the number of iterations increases. The analysis shows that this reduces the overall number of communications required while still achieving an error scaling of 1/(N T), independent of T. Empirical evidence supports this approach, and simulations show that excessive communication fails to achieve linear speed-up. Additionally, under certain assumptions, one-shot averaging with a single round of communication can also achieve optimal convergence rate asymptotically.