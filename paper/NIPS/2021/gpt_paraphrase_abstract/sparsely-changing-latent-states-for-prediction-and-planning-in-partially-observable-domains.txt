We propose a new recurrent neural network architecture called GateL0RD that is designed to maintain stable latent states in partially observable domains. This is achieved by incorporating a novel internal gating function and a penalty on the L0 norm of latent state changes. We demonstrate that GateL0RD performs as well as, or better than, existing RNNs in various prediction and control tasks. GateL0RD is able to encode the underlying factors of the environment, ignore irrelevant temporal dependencies, and improve sampling efficiency and overall performance in model-based planning and reinforcement learning. Additionally, the latent states generated by GateL0RD can be easily interpreted, which enhances the explainability of RNNs.