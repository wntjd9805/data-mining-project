Quantization is a technique used in deep neural networks (DNNs) to improve execution speed and hardware efficiency. Uniform post-training quantization (PTQ) methods are commonly used because they are efficient and do not require extensive hardware resources or a training set. However, reducing precision below 8 bits with PTQ is challenging as it leads to noticeable accuracy degradation due to increased quantization noise. This paper proposes a sparsity-aware quantization (SPARQ) method that leverages the unstructured and dynamic activation sparsity at different levels of representation. For example, instead of quantizing activation-by-activation to 4 bits, SPARQ focuses on pairs of 8-bit activations and checks if one of them is zero. If one is zero, the other can use its 4-bit budget. If both are non-zero, each is dynamically quantized to 4 bits. SPARQ achieves minimal accuracy degradation while being practical for hardware implementation. The code for SPARQ is available at https://github.com/gilshm/sparq.