This paper explores the concept of fairwashing, which refers to the manipulation of post-hoc explanations to make an unfair black-box model appear fair. The study investigates the trade-offs between fidelity (accuracy) and unfairness in fairwashing attacks. It reveals that fairwashed explanation models can extend beyond the group being explained, allowing them to rationalize subsequent unfair decisions. Additionally, fairwashing attacks can transfer across different black-box models, making their detection challenging. To quantify the risk of fairwashing, the paper proposes a method based on computing the range of unfairness in high-fidelity explainers.