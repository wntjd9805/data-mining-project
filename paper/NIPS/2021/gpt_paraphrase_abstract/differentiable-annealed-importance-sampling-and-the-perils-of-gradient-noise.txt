Annealed importance sampling (AIS) and similar methods are effective for estimating marginal likelihood, but they lack differentiability because of the use of non-differentiable Metropolis-Hastings correction steps. We propose a differentiable version called Differentiable AIS (DAIS) that eliminates the need for these correction steps and allows for mini-batch gradients. We provide a convergence analysis for Bayesian linear regression, showing that DAIS is consistent and has a sublinear convergence rate in the full-batch setting. However, in the stochastic variant of DAIS with mini-batch gradients, we find that it can perform poorly due to the conflicting goals of convergence to the posterior and elimination of stochastic error. This suggests that new ideas may be needed for annealing-based marginal likelihood estimation with stochastic gradients.