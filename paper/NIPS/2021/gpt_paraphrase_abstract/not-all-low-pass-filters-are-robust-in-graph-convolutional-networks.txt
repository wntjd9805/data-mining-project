Graph Convolutional Networks (GCNs) are a popular type of deep learning models used for learning representations of graph-structured data. However, these models are susceptible to adversarial attacks on the graph structure. This paper analyzes the vulnerability of GCNs to adversarial attacks and demonstrates that the low-frequency components of the symmetric normalized Laplacian, which is commonly used as the convolutional filter in GCNs, can be more robust against structural perturbations within a specific range of eigenvalues. This finding highlights that not all low-frequency components are equally resilient to adversarial attacks and provides insights into the relationship between the graph spectrum and the robustness of GCNs. Leveraging this theory, the authors propose GCN-LFR3, a general robust co-training framework for GCN-based models. GCN-LFR3 aims to transfer the robustness of low-frequency components using an auxiliary neural network, thereby enhancing the resistance of various GCN-based models against poisoning structural attacks. Extensive experiments conducted on five benchmark datasets and five GCN-based models validate the effectiveness of GCN-LFR in defending against adversarial attacks without compromising performance under normal conditions.