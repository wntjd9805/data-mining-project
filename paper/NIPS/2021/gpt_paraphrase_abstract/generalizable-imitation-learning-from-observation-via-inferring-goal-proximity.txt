We suggest a straightforward but efficient approach to imitation learning from observation to achieve a goal-directed task. By utilizing a learned goal proximity function as a task progress estimator, we can improve generalization to unseen states and goals. This goal proximity function is derived from both expert demonstrations and online agent experience, and serves as a dense reward for policy training. Our method demonstrates robust generalization capabilities in comparison to previous imitation learning methods, even when demonstrations only cover a subset of states.