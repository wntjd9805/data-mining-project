Sparse Mixture of Experts networks have shown great scalability in Natural Language Processing (NLP), but in Computer Vision, dense networks are typically more effective. In this study, we introduce a sparse version of the VisionTransformer called Vision MoE (V-MoE), which achieves comparable performance to dense networks while being more scalable. V-MoE performs image recognition tasks with the same accuracy as state-of-the-art networks but requires only half the computational resources during inference. We also propose an improved routing algorithm that allows for adaptive per-image computation, enabling V-MoE to balance performance and computational efficiency during testing. Additionally, we demonstrate the potential of V-MoE to scale vision models by training a 15 billion parameter model that achieves an impressive 90.35% accuracy on the ImageNet dataset.