The goal of online continual learning research is to enhance machine learning models by enabling them to learn continuously from sequentially encountered tasks. However, current approaches still struggle with catastrophic forgetting, which is a major challenge in continual learning. In this paper, we propose a novel method that addresses this issue through neuron calibration. We view the neurons in deep neural networks as calibrated units and develop a learning framework to train the calibrated model effectively. Our lightweight formulation for neuron calibration is applicable to various feed-forward neural network models. Experimental results on benchmark datasets demonstrate that neuron calibration significantly improves online continual learning performance, surpassing the state-of-the-art on all tested datasets.