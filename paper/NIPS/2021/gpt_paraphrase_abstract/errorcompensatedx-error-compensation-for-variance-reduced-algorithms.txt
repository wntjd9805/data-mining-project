Communication cost is a major limitation in scaling distributed learning. One method to reduce this cost is by compressing the gradient during communication. However, compressing the gradient directly slows down convergence and can lead to divergence with biased compression. Previous research attempted to address this issue by incorporating the compression error from the previous step in stochastic gradient descent. This approach was also extended to variance reduced algorithms by averaging historical gradients to reduce variance. However, our analysis reveals that the previous approach does not fully compensate for compression error. To overcome this, we propose ErrorCompensatedX, which incorporates the compression error from the previous two steps. We demonstrate that ErrorCompensatedX achieves the same convergence rate as uncompressed training. Additionally, we present a unified theoretical analysis framework for variance reduced algorithms, with or without error compensation.