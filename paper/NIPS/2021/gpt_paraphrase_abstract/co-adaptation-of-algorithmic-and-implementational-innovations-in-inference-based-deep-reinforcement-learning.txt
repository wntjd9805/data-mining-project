In recent times, numerous algorithms have been developed for reinforcement learning (RL) with function approximation. While these algorithms have distinct differences in their algorithms, they also have many implementation differences that are often overlooked. This mixing of algorithmic novelty and implementation craftsmanship makes it difficult to analyze the sources of performance improvements across these algorithms. In this study, we focus on three off-policy inference-based actor-critic algorithms (MPO, AWR, and SAC) and aim to separate their algorithmic innovations from their implementation decisions. We present unified derivations by considering a single control-as-inference objective, categorizing each algorithm as based on either Expectation-Maximization (EM) or direct Kullback-Leibler (KL) divergence minimization, and treating the remaining specifications as implementation details. Through extensive ablation studies, we found that there are significant drops in performance when there is a mismatch between implementation details and algorithmic choices. These findings reveal which implementation or code details are specifically adapted and evolved with algorithms, and which ones can be transferred across algorithms. For instance, we discovered that the tanh Gaussian policy and network sizes are highly adapted to specific algorithmic types, while layer normalization and ELU play a critical role in MPO's performance and also contribute to noticeable gains in SAC. We hope that our work will inspire future research to better understand the sources of performance improvements across multiple algorithms and encourage researchers to build upon each other's algorithmic and implementation innovations.