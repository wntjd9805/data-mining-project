Efficient exploration in deep reinforcement learning is a major challenge when rewards are sparse. While previous exploration methods have achieved strong results in difficult tasks, they often quickly focus on one novel area without adequately exploring others. This paper introduces a simple but effective criterion called NovelD, which weights every novel area equally, inspired by theoretical RL methods. The proposed algorithm outperforms state-of-the-art exploration methods in various challenging tasks, including procedurally-generated tasks in Mini-Grid and NetHack. In addition, NovelD surpasses RND in multiple Atari games. Through extensive analysis, NovelD is found to help the agent explore the environment more uniformly, with a focus on exploring beyond the boundary.