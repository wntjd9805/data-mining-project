Robotic agents often struggle to efficiently learn the sequence of object interactions required for complex physical tasks solely through their own experience. To address this, we propose a method for discovering activity-context priors from egocentric videos captured by human-worn cameras. These priors represent the compatible objects necessary for successful activities, such as a knife and cutting board for cutting a tomato. We incorporate these priors into a video-based reward function that encourages agents to bring compatible objects together before attempting interactions. By translating everyday human experience into embodied agent skills, our approach significantly accelerates agent learning. We validate our idea using unscripted kitchen activities captured in egocentric EPIC-Kitchens videos, benefiting virtual household robotic agents performing various complex tasks in AI2-iTHOR. More information can be found on our project page: http://vision.cs.utexas.edu/projects/ego-rewards/.