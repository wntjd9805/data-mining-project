Differentiable programming has expanded the possibilities of machine learning by using gradient-based optimization. However, current methods like autodiff have limitations as they require differentiability of the machine learning models. This paper aims to overcome this limitation by introducing a new approach that extends gradient-based optimization to functions that are well modeled by splines, which include a wide range of piecewise polynomial models. We derive the form of the weak Jacobian for these functions and demonstrate that it has a block-sparse structure that can be efficiently computed implicitly. By incorporating this redesigned Jacobian as a differentiable "layer" in predictive models, we achieve improved performance in various applications such as image segmentation, 3D point cloud reconstruction, and finite element analysis. Additionally, we have made the code available as open-source at https://github.com/idealab-isu/DSA.