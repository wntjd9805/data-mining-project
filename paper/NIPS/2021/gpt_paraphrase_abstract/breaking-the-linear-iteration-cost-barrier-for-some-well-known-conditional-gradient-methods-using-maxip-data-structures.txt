Conditional gradient methods (CGM) are commonly used in modern machine learning. The total running time of CGM is determined by the number of iterations and the cost of each iteration. Previous efforts have primarily focused on reducing the number of iterations to minimize the overall running time. In this study, however, we concentrate on enhancing the per iteration cost of CGM. The main bottleneck in CGM is the maximum inner product search (MaxIP), which necessitates a linear scan over the parameters. Although approximate MaxIP data structures have proven to be beneficial heuristics in practice, there is currently no theoretical understanding of how combining these data structures with CGM affects performance. In this research, we provide a formal framework that demonstrates the positive impact of combining locality sensitive hashing type approximate MaxIP data structures with CGM algorithms. Consequently, we present the first algorithm that achieves a sublinear cost per iteration in terms of the number of parameters for various fundamental optimization algorithms such as Frank-Wolfe, Herding algorithm, and policy gradient.