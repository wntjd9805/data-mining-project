The transformer model, which relies on self-attention, has become a prominent backbone in computer vision. However, it faces challenges due to its high computation and memory requirements. This paper introduces a framework called IA-RED2 that aims to reduce redundancy in the transformer model. By identifying and dropping uncorrelated input patches, IA-RED2 effectively decreases the computational cost. The framework is extended to a hierarchical structure, gradually removing uncorrelated tokens at different stages and significantly reducing computational resources. Experimental results demonstrate that IA-RED2 can speed up state-of-the-art models while sacrificing minimal accuracy. Additionally, unlike other acceleration methods, IA-RED2 is interpretable, providing visual evidence and bringing the vision transformer closer to a human-understandable architecture. The interpretability achieved by IA-RED2 surpasses the attention learned by the original visual transformer and other interpretation methods, as confirmed by qualitative and quantitative analysis. The project page for IA-RED2 is available at http://people.csail.mit.edu/bpan/ia-red/.