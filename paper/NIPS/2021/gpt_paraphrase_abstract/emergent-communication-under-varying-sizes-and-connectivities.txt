This study explores the possibility of artificial agents developing a shared language through deep neural networks. It examines the emergence of a shared language within group communication settings of varying sizes and connectivities. The findings indicate that as group size increases, agents begin to speak different languages, but their successful communication rate remains consistent. When communication is restricted to local connections, different dialects emerge. Additionally, the study provides optimization results for group communication graphs, demonstrating that restricting communication or penalizing distant agent pairs leads to improved communication success rates. The optimized graphs also exhibit the emergence of hub nodes and scale-free networks.