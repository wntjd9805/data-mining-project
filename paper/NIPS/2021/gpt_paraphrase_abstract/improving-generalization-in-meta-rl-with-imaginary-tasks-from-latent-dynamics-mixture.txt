Most meta-reinforcement learning (meta-RL) methods have limited ability to generalize to test tasks that are different from the ones used for training. To address this limitation, we propose Latent Dynamics Mixture (LDM). LDM trains a reinforcement learning agent using imaginary tasks created from mixtures of learned latent dynamics. By training on both original training tasks and mixture tasks, LDM enables the agent to prepare for unseen test tasks during training and prevents overfitting to the training tasks. In experiments on gridworld navigation and MuJoCo tasks, where the training and test task distributions are strictly separated, LDM outperforms standard meta-RL methods in terms of test returns.