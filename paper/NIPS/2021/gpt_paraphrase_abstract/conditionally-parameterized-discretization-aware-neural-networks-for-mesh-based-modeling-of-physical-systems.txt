Simulations of complex physical systems often involve discretizing partial differential equations (PDEs) on unstructured meshes. While neural networks have been used for surrogate and reduced order modeling of PDE solutions, they typically overlook interactions and hierarchical relationships between input features and treat them as concatenated mixtures. In this study, we propose a novel approach called conditional parameterization, where trainable functions of input parameters are used to generate the weights of a neural network. This approach allows us to incorporate critical information into the network architecture by considering choices of parameters such as physical quantities and mesh topology features. We apply this method to various scientific machine learning tasks, including discovering unmodeled physics, super-resolution of coarse fields, and simulating unsteady flows with chemical reactions. Our results demonstrate that the conditionally-parameterized networks outperform traditional networks in terms of performance. We introduce CP-GNet, an architecture that can be trained with minimal data snapshots, as the first deep learning model capable of independently predicting reacting flows on irregular meshes.