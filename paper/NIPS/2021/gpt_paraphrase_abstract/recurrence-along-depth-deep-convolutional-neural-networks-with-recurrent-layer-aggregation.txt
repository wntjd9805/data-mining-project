This paper presents a new approach called layer aggregation, which utilizes information from previous layers to enhance feature extraction in the current layer. While DenseNet is a well-known example of layer aggregation, its redundancy has been criticized in previous studies. To address this, we propose a lightweight module called recurrent layer aggregation (RLA) that leverages the sequential structure of layers in deep convolutional neural networks (CNNs). Our RLA module is compatible with popular CNN architectures like ResNets, Xception, and MobileNetV2. We conducted extensive experiments on image classification, object detection, and instance segmentation tasks, and our results demonstrate the effectiveness of RLA. We observed consistent improvements across CIFAR, ImageNet, and MS COCO datasets. Notably, our RLA-Nets achieved a remarkable 2-3% performance boost in object detection. These findings highlight the ability of our RLA module to enhance the learning of structural information in images by mainstream CNNs.