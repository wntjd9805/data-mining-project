An algorithm for optimizing smooth and convex or strongly convex objectives using minibatch stochastic gradient estimates is proposed and analyzed. The algorithm is optimal in terms of its dependence on both the minibatch size and minimum expected loss, which is an improvement over existing methods. The algorithm outperforms Lan's optimal method, which ignores the minimum expected loss, as well as Cotter et al.'s optimistic acceleration, which has suboptimal dependence on the minibatch size. Additionally, it surpasses Liu and Belkin's algorithm, which is limited to least squares problems and also has suboptimal dependence on the minibatch size. In the context of interpolation learning, the algorithm's superiority over Cotter et al. and Liu and Belkin results in a linear parallelization speedup instead of a square-root speedup.