We investigate the similarity between the representations learned by neural language models, translation models, and language tagging tasks. Using an encoder-decoder transfer learning method adapted from computer vision, we analyze 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. Our method reveals a low-dimensional structure that connects language models and translation models, smoothly transitioning between word embeddings, syntactic and semantic tasks, and future word embeddings. We refer to this structure as a language representation embedding, as it encodes the relationships necessary for processing language in various natural language processing (NLP) tasks. We discover that this representation embedding can accurately predict how well each individual feature space aligns with the human brain's response to natural language stimuli recorded using fMRI. Furthermore, we find that the primary dimension of this structure can be utilized to create a metric that highlights the brain's hierarchy in processing natural language. This suggests that the embedding captures a portion of the brain's natural language representation structure.