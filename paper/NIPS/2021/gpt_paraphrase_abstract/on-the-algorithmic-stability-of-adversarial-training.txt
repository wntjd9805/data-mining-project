This study focuses on the algorithmic stability of adversarial training algorithms, which are commonly used to protect deep learning models from adversarial attacks. While there is existing theoretical literature on the training loss of these algorithms, this paper examines their stability in order to establish an upper bound for generalization error. The research finds that the non-differentiability issue in adversarial training leads to lower algorithmic stability compared to natural counterparts. To address this issue, a noise injection method is proposed. By injecting noise, the training trajectory is more likely to avoid non-differentiability, thus improving the stability of adversarial training. The analysis also explores the relationship between algorithm stability and numerical approximation error of adversarial attacks.