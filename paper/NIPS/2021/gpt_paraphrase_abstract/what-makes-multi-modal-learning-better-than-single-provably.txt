Models that combine data from different modalities have been found to outperform models that only use one modality, as they are able to aggregate more information. Deep multi-modal learning, which combines deep learning with multiple modalities, has shown promising results in various applications. However, there is a lack of theoretical justifications in this field. This paper aims to answer the question of whether multi-modal learning can be proven to perform better than uni-modal learning. The paper focuses on a popular multi-modal fusion framework that encodes features from different modalities into a common latent space and maps them to the task space. The authors prove that learning with multiple modalities achieves a smaller population risk compared to using only a subset of modalities. This is because the former provides a more accurate estimate of the latent space representation. This is the first theoretical treatment to capture important qualitative phenomena observed in real multi-modal applications from a generalization perspective. Experimental results also support the theoretical findings, demonstrating that multi-modal learning has a formal guarantee of improved performance.