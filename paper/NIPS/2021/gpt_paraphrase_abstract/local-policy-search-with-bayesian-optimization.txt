Reinforcement learning (RL) aims to find the best strategy through interaction with an environment. However, this process often requires a large number of samples, which can be impractical. Instead of randomly selecting samples, we propose a method that combines the principles of RL and Bayesian optimization. Our algorithm uses a probabilistic model to determine where to query the environment for better gradient estimates. This approach improves sample efficiency and reduces variance compared to existing methods. We demonstrate the benefits of active sampling on both synthetic objectives and popular RL benchmarks.