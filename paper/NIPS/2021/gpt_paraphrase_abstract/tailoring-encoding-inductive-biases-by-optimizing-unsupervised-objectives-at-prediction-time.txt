Encoding biases into neural networks has greatly improved machine learning, from convolutional neural networks (CNNs) to attention mechanisms. Adding auxiliary losses to the main objective function is a common way of incorporating biases and improving representation learning. However, these auxiliary losses suffer from the same generalization gap as regular task losses since they are only minimized on training data. Additionally, by adding a term to the loss function, the model optimizes a different objective than the desired one. To address these issues, we propose a method called "tailoring" inspired by transductive learning. Before making predictions, we fine-tune our networks on unsupervised losses for each input, customizing the model to satisfy the inductive bias. We also introduce "meta-tailoring," a nested optimization similar to meta-learning, where models are trained to perform well on the task objective after adapting them with an unsupervised loss. Theoretical advantages of tailoring and meta-tailoring are discussed, and empirical demonstrations on various examples showcase their effectiveness.