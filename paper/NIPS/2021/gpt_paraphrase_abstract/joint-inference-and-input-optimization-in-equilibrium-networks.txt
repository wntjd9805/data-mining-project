Deep learning tasks often require optimizing inputs to a network in order to achieve a specific objective. This optimization process is typically expensive as it involves multiple passes through the network. However, recent research has introduced the deep equilibrium (DEQ) model, which computes network outputs by finding the fixed point of a single nonlinear layer. This paper explores the combination of DEQ models with optimization problems. By treating gradient-based optimization as a fixed point iteration, the overall speed of the process is significantly improved. This allows for the efficient training of DEQ models on tasks that traditionally require inner optimization loops. The authors demonstrate the effectiveness of this strategy on various tasks including training generative models, solving inverse problems, adversarial training, and gradient-based meta-learning.