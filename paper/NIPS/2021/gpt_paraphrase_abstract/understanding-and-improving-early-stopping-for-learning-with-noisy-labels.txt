The memorization effect of deep neural networks (DNNs) is important in label-noise learning methods. The early stopping trick is commonly used to exploit this effect, but current methods decide the stopping point for the entire DNN. However, we find that the latter layers of a DNN are more sensitive to label noise, while the former layers are more robust. This means that selecting a stopping point for the whole network may negatively affect the performance. To address this, we propose a method called progressive early stopping (PES) where the DNN is separated into parts and progressively trained. Instead of training the whole DNN at once, we initially train the former layers for a longer period and then progressively train the latter layers with a smaller number of epochs. This helps counteract the impact of noisy labels and leads to more promising and stable results compared to traditional early stopping. By combining PES with existing approaches, we achieve state-of-the-art performance on image classification benchmarks. The code for PES is publicly available at https://github.com/tmllab/PES.