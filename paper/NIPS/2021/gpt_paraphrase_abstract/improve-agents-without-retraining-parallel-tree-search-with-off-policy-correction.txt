Tree Search (TS) is vital in reinforcement learning but faces challenges of distribution shift and scalability. We discover that using TS and a pre-trained value function can lead to lower performance due to a distribution shift to areas with inaccurate value estimates. To address this, we introduce an off-policy correction term that penalizes under-sampled trajectories, eliminating the mismatch between the pre-trained value and TS policy. This correction significantly improves pre-trained Rainbow agents without additional training. We also tackle scalability by introducing Batch-BFS, a GPU breadth-first search that advances all nodes simultaneously, reducing runtime and enabling training at greater depths. Using TS, we train DQN agents from scratch and observe improvement in Atari games compared to both original DQN and Rainbow approaches.