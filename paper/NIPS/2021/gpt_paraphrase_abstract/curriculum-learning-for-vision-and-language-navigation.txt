We introduce a new training approach for Vision-and-Language Navigation (VLN) tasks, which involve an agent navigating an indoor environment based on human instructions. Existing methods overlook the varying difficulty levels of training samples, leading to potential performance degradation. To address this issue, we propose a curriculum-based training paradigm that balances human prior knowledge and agent learning progress. We redesign the Room-to-Room (R2R) dataset to accommodate curriculum training. Our approach, applicable to any model, significantly enhances the performance, generalizability, and training efficiency of current navigation agents without increasing model complexity.