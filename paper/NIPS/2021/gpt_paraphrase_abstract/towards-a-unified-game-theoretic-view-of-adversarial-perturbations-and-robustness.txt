This paper presents a comprehensive perspective on various adversarial attacks and defense strategies by examining the multi-order interactions between input variables of deep neural networks (DNNs). By analyzing these interactions, we reveal that adversarial attacks primarily exploit high-order interactions to deceive DNNs. Moreover, we discover that the resilience of adversarially trained DNNs is derived from category-specific low-order interactions. Our findings offer a potential approach to integrate adversarial perturbations and robustness, thereby providing a fundamental explanation for existing defense methods. Additionally, our findings correct previous misconceptions regarding the shape bias of adversarially learned features. The code related to our research can be accessed on https://github.com/Jie-Ren/A-Unified-Game-Theoretic-Interpretation-of-Adversarial-Robustness.