Large-scale matching platforms that connect two sides of a market face the challenge of finding market outcomes that align with user preferences while also learning these preferences from data. However, the uncertainty of preferences during the learning process makes it impossible to achieve the classical notion of stability. To address this issue, we propose a framework and algorithms that enable the learning of stable market outcomes under uncertainty. Specifically, we focus on matching with transferable utilities, where the platform matches agents and determines monetary transfers between them. We introduce an incentive-aware learning objective that measures the deviation of a market outcome from equilibrium. By analyzing the complexity of learning based on preference structure, we frame the learning process as a stochastic multi-armed bandit problem. We demonstrate that the principle of "optimism in the face of uncertainty," which underlies many bandit algorithms, can be applied to a primal-dual formulation of matching with transfers, leading to nearly optimal regret bounds. Our research provides initial insights into the emergence of stable matchings in large data-driven marketplaces.