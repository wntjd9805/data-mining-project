This paper examines offline Imitation Learning (IL), where an agent learns to imitate an expert without interacting with the environment online. Instead, the learner is given a static dataset of state-action-next state transitions from a potentially less skilled behavior policy. The authors propose a framework called Model-based IL from Offline data (MILO) that efficiently solves the offline IL problem both theoretically and practically. The authors show that even if the behavior policy is highly sub-optimal compared to the expert, MILO can effectively address the covariate shift issue in IL as long as the behavior policy data adequately covers the expert state-action traces. They also demonstrate that a practical implementation of MILO mitigates covariate shift in benchmark MuJoCo continuous control tasks. MILO successfully imitates the expert with very few expert state-action pairs, while traditional offline IL methods like behavior cloning fail. The source code for MILO is available at https://github.com/jdchang1/milo.