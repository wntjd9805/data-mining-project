We propose a method for maintaining privacy in a sequential analysis setting, where a dataset is used for adaptively-chosen analyses while ensuring that each participant's privacy loss remains within a predetermined budget. The current approach of bounding privacy loss for all individuals and possible data values in every analysis is overly cautious and not suitable for typical data points that incur minimal privacy loss. Our method introduces personalized privacy loss estimates for individuals in each analysis, allowing for tighter privacy loss accounting. We design a filter for RÃ©nyi differential privacy to implement this method, which ensures that the privacy parameter of a sequence of algorithms with adaptively-chosen privacy parameters stays within the budget. Our filter is simpler and more effective than an existing filter for differential privacy. We demonstrate the practicality and ease of implementing personalized accounting in the analysis of noisy gradient descent, and show that it improves the privacy-utility tradeoff.