We establish versions of the well-known bounded difference inequality, known as McDiarmid's inequality, for functions of independent random variables with sub-Gaussian and sub-exponential properties. These inequalities can be applied to vector-valued concentration and the method of Rademacher complexities, enabling the extension of uniform convergence outcomes for principal component analysis (PCA) and linear regression to scenarios involving unbounded input and output variables.