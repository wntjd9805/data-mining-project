The aim of this paper is to develop a method for creating neural signed distance fields (SDFs) that represent clothed humans using monocular depth observations. Current methods for generating realistic cloth deformations typically require watertight meshes or dense full-body scans as inputs, and they are computationally expensive due to the need for per-subject/cloth-type optimization. In contrast, our proposed approach uses meta-learning to quickly generate realistic clothed human avatars represented as controllable neural SDFs, using only monocular depth images as input. We achieve this by using a hypernetwork that predicts the parameters of the neural SDFs based on human poses. The hypernetwork is meta-learned to effectively incorporate priors of diverse body shapes and cloth types, making it faster to fine-tune compared to models trained from scratch. Our approach outperforms state-of-the-art methods that require complete meshes as inputs, running orders of magnitudes faster while only requiring depth frames as inputs. Additionally, we demonstrate the robustness of our meta-learned hypernetwork, as it can generate avatars with realistic dynamic cloth deformations using as few as 8 monocular depth frames.