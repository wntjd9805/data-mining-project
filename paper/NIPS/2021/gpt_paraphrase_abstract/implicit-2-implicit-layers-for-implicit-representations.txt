Recent research in deep learning has explored two different forms of "implicitness": implicit representations and implicit layers. Implicit representations involve using a low-dimensional neural network to directly model high-frequency data like images or 3D shapes, often using sinusoidal bases or nonlinearities. On the other hand, implicit layers involve computing the forward pass of a network through nonlinear dynamical systems like fixed-point or differential equation solutions, with the backward pass computed using the implicit function theorem.This study demonstrates that these seemingly unrelated concepts are actually well-suited for each other. By utilizing implicit layers to model implicit representations, the performance of the traditional explicit-layer-based approach can be significantly improved. Additionally, as implicit representation networks are typically trained in large-batch settings, the authors propose leveraging the property of implicit layers to amortize the cost of fixed-point forward/backward passes over training steps. This addresses one of the main challenges of implicit layers, which is the need for multiple iterations of black-box fixed-point solvers.The authors empirically evaluated their method on learning multiple implicit representations for images, audios, videos, and 3D models. The results showed that their approach, known as (Implicit)2, substantially improves upon existing models in terms of both training speed and memory efficiency.