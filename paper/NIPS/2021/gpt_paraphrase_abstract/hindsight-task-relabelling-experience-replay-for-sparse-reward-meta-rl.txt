Current meta-reinforcement learning (meta-RL) techniques struggle to learn in sparse reward environments. While they can adapt to new tasks with sparse rewards, they rely on hand-shaped reward functions or simple environments where random exploration is enough to encounter sparse rewards. In this study, we propose a method called hindsight relabeling for meta-RL, which allows learning entirely using sparse rewards by relabeling experience during meta-training. We demonstrate the effectiveness of our approach on challenging sparse reward goal-reaching environments that previously required dense rewards for training. Our method achieves comparable performance to training with a proxy dense reward function, using the true sparse reward function.