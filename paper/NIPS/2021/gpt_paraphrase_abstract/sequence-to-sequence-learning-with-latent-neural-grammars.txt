Sequence-to-sequence learning using neural networks has become the standard method for sequence prediction. However, these models often require large datasets and struggle with compositional generalization. This study proposes an alternative approach using hierarchical quasi-synchronous grammars, where each node in the target tree is transduced by a node in the source tree. Both the source and target trees are treated as latent and induced during training. A neural parameterization of the grammar is developed to allow parameter sharing without manual feature engineering. This latent neural grammar is applied to different domains, including compositional generalization testing, style transfer, and machine translation, and performs comparably to standard baselines.