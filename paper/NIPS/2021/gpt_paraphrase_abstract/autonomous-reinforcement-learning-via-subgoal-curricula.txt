Reinforcement learning (RL) holds promise for enabling agents to autonomously learn complex behaviors. However, current RL algorithms rely on starting each trial from a fixed initial state distribution, which requires significant human supervision and instrumentation of the environment. This undermines the goal of autonomous learning. To address this issue, we propose Value-accelerated Persistent Reinforcement Learning (VaPRL), which generates a curriculum of initial states. This allows the agent to build upon the success of easier tasks and efficiently learn harder tasks. Additionally, the agent learns to reach the proposed initial states, reducing the need for human intervention. Our experiments show that VaPRL requires significantly fewer interventions compared to episodic RL, while outperforming previous reset-free RL methods in terms of sample efficiency and asymptotic performance on various simulated robotics problems.