Bayesian model averaging is widely used for prediction, uncertainty evaluation, and model selection. One approach to efficiently capture information in the posterior distribution is to optimize a set of models simultaneously with interaction, similar to ensemble learning. Particle variational inference (PVI) is an approach that updates each model iteratively with a repulsion force to ensure diversity. However, the theoretical understanding of this repulsion and its connection to generalization ability is unclear. In this paper, we address this issue using PAC-Bayesian analysis. We introduce a new second-order Jensen inequality with a repulsion term based on the loss function, which provides a tighter bound than the standard Jensen inequality. We also derive a novel generalization error bound that can be reduced by enhancing model diversity. Finally, we propose a new PVI that directly optimizes the generalization error bound. Numerical experiments show that the performance of our proposed PVI method is comparable to existing methods.