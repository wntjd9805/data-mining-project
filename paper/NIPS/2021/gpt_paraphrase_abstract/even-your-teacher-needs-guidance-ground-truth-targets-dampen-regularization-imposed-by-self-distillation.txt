Knowledge distillation is a technique where a neural network is trained on the output of another network and the original targets to transfer knowledge between architectures. Self-distillation, a special case where the architectures are the same, has been found to improve generalization accuracy. This paper explores an iterative version of self-distillation in a kernel regression setting, incorporating both model outputs and ground-truth targets. The importance of using weighted ground-truth targets in self-distillation is theoretically analyzed. The focus is on fitting nonlinear functions to training data with a weighted mean square error objective function suitable for distillation. It is shown that any function obtained through self-distillation can be calculated directly from the initial fit, and infinite distillation steps yield the same optimization problem with increased regularization. A closed form solution for the optimal weighting parameter at each step is provided, along with an efficient method to estimate this parameter for deep learning, reducing computational requirements compared to a grid search.