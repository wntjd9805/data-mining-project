Sparse neural networks are an important tool for reducing computational resources and scaling up models. While previous work has focused on pruning techniques, little attention has been given to the impact of gradient-based training on model sparsity. This study introduces Powerpropagation, a weight-parameterization method that inherently leads to sparse models. By exploiting the behavior of gradient descent, Powerpropagation updates weights in a way that leaves low-magnitude parameters mostly unchanged. Models trained with this method perform similarly to traditional methods but have a higher density of zero parameters, allowing for safer pruning. Powerpropagation is easy to implement and can be combined with other techniques. The versatility of Powerpropagation is demonstrated in two different settings: resource-constrained training and overcoming catastrophic forgetting. In both cases, Powerpropagation significantly improves the effectiveness of existing methods.