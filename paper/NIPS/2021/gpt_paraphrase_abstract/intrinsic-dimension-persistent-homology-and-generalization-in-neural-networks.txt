Modern deep neural networks defy the traditional principles of statistical learning theory by generalizing well despite their large number of parameters. Recent findings suggest that the behavior of optimization algorithms during training can be fractal in nature, and the complexity of these fractals is linked to the networks' generalization error. However, accurately measuring this complexity, known as the intrinsic dimension, remains a challenging task. In this study, we approach this problem from the perspective of topological data analysis (TDA) and develop a reliable computational tool based on rigorous mathematical foundations. By establishing a connection between learning theory and TDA, we show that the generalization error can be bounded using a concept called the "persistent homology dimension" (PHD), without imposing additional assumptions on the training dynamics. Leveraging theoretical results and TDA tools, we present an efficient algorithm to estimate the PHD for modern deep neural networks and provide visualization tools to aid in understanding generalization in deep learning. Our experiments demonstrate that our approach can effectively compute the intrinsic dimension of a network in various scenarios, which serves as a predictor of its generalization error.