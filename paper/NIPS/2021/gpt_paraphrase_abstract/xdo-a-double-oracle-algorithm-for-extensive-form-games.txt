The Policy Space Response Oracles (PSRO) algorithm is a reinforcement learning method used in two-player zero-sum games. It has been proven to find approximate Nash equilibria in large games. However, PSRO can become exponentially slow as the number of information states increases. To address this issue, we propose the Extensive-Form Double Oracle (XDO) algorithm. XDO guarantees convergence to an approximate Nash equilibrium at a linear rate in the number of information states. Unlike PSRO, which mixes best responses at the root of the game, XDO mixes best responses at every information state. Additionally, we introduce Neural XDO (NXDO) where the best response is learned through deep reinforcement learning. Our experiments on Leduc poker show that XDO achieves an approximate Nash equilibrium in significantly fewer iterations than PSRO. Furthermore, our experiments on a modified Leduc poker game and Oshi-Zumo demonstrate that tabular XDO achieves lower exploitability than CFR with the same computational effort. We also find that NXDO outperforms PSRO and NFSP in a sequential multidimensional continuous-action game. NXDO is the first deep reinforcement learning method capable of finding an approximate Nash equilibrium in high-dimensional continuous-action sequential games. You can find the experiment code at https://github.com/indylab/nxdo.