The partially observable Markov decision process (POMDP) is a framework for modeling decision-making with uncertain states. Online planning is important for solving POMDPs. Instead of exact belief updating, methods for large-scale POMDPs use sampled states to approximate beliefs. Similarly, only a set of sampled observations are considered instead of all possible observations. Building on this idea, we propose an online planning algorithm called AdaOPS. It uses an adaptive particle filter technique to better approximate beliefs and balances estimation bias and variance by fusing similar observation branches. The algorithm is theoretically guaranteed to find an optimal policy with high probability given enough planning time. We evaluate AdaOPS on various POMDP domains and it outperforms the current state-of-the-art algorithms. The code is available at https://github.com/LAMDA-POMDP/AdaOPS.jl.