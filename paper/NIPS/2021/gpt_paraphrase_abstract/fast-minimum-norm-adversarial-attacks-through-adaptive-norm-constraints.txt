We propose a fast minimum-norm (FMN) attack that can evaluate the adversarial robustness of input samples by finding the minimum perturbation needed for misclassification. Unlike current gradient-based attacks, our FMN attack is robust to hyperparameter choices, does not require adversarial starting points, and converges quickly in a few lightweight steps. It works by iteratively finding the most confidently misclassified sample within a p-norm constraint while minimizing the distance to the decision boundary. Extensive experiments demonstrate that FMN outperforms existing attacks in terms of perturbation size, convergence speed, and computation time, while achieving comparable performance to state-of-the-art attacks. Our open-source code is available at: https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack.