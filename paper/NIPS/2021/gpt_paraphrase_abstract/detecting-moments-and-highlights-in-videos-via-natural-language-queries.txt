The detection of personalized moments and highlights from videos based on natural language user queries is an important but understudied area. A major challenge in this field is the lack of annotated data. To address this issue, we introduce the Query-based Video Highlights (QVHIGHLIGHTS) dataset, which consists of over 10,000 YouTube videos covering various topics. Each video in the dataset is annotated with a human-written NL query, relevant moments in the video related to the query, and saliency scores for the query-relevant clips. This comprehensive annotation allows us to develop and evaluate systems that can identify relevant moments and salient highlights for diverse user queries. We also present Moment-DETR, a transformer encoder-decoder model that serves as a strong baseline for this task. Moment-DETR treats moment retrieval as a set prediction problem, using video and query representations to predict moment coordinates and saliency scores. Despite not relying on human prior knowledge, our model performs competitively compared to well-engineered architectures. By employing weakly supervised pretraining with ASR captions, Moment-DETR outperforms previous methods. Additionally, we provide various ablations and visualizations of Moment-DETR. The data and code for our work are publicly available at https://github.com/jayleicn/moment_detr.