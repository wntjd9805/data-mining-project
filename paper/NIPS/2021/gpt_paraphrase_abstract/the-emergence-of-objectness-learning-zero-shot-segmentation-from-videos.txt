We aim to model segmentation and movement simultaneously in unlabeled videos. We believe that a video contains various perspectives of the same scene due to moving components, and by employing region segmentation and region flow, we can synthesize views and evaluate them within the data itself. Our model consists of two pathways: one for appearance and one for motion. The appearance pathway generates feature-based region segmentation for a single image, while the motion pathway produces motion features for a pair of images. These pathways are combined to create a joint region flow feature representation, enabling the prediction of segment flow that characterizes moving regions in the entire scene. Through training the model to minimize view synthesis errors based on segment flow, our approach automatically learns region segmentation and flow estimation without relying on low-level edges or optical flow. Our model exhibits remarkable objectness in the appearance pathway, outperforming previous methods in zero-shot object segmentation, moving object segmentation, and semantic image segmentation. It is the first end-to-end learned zero-shot object segmentation model from unlabeled videos, providing generic objectness for segmentation and tracking while surpassing image-based contrastive representation learning.