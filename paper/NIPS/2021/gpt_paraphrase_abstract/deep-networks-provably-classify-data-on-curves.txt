Data with low-dimensional nonlinear structure are common in engineering and scientific problems. We investigate a binary classification problem where a deep fully-connected neural network is employed to classify data taken from two distinct smooth curves on the unit sphere. There are minimal constraints on the configuration of the curves. We establish that when the network depth is significantly larger than certain geometric properties that determine the problem's difficulty, and the network width and sample size are polynomial in the depth, gradient descent with random initialization rapidly learns to accurately classify all points on the two curves with high probability. This is the first generalization guarantee for deep networks with nonlinear data that solely depends on intrinsic data properties. Our analysis relies on reducing the problem to dynamics in the neural tangent kernel (NTK) regime, where the network depth acts as a fitting resource for solving the classification task. By precisely controlling the decay properties of the NTK, we show that when the network is sufficiently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, ensuring convergence and generalization.