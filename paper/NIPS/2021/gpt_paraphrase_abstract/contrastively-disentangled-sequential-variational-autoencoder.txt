We introduce a new method called Con-trastively Disentangled Sequential Variational Autoencoder (C-DSVAE) for self-supervised disentangled representation learning in sequence modeling. This method aims to extract and separate the static and dynamic factors in the latent space, leading to improved interpretability and data generation capabilities, as well as enhanced efficiency for downstream tasks. Unlike previous sequential variational autoencoder approaches, we utilize a novel evidence lower bound that maximizes the mutual information between the input and the latent factors, while penalizing the mutual information between the static and dynamic factors. Our training process incorporates contrastive estimations of the mutual information terms, along with effective augmentation techniques, to introduce additional biases. Experimental results demonstrate that C-DSVAE outperforms existing methods, achieving superior performance across multiple metrics.