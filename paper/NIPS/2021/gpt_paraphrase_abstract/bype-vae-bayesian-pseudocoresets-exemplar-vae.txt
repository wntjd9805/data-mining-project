Advanced priors are crucial in deep generative models, and the Exemplar VAE has shown impressive results as a variant of VAE. However, this type of model typically requires a large amount of training data, resulting in computational complexity. To address this issue, we propose a new variant called ByPE-VAE, which uses a prior based on Bayesian pseudocoresets. This prior is conditioned on a small-scale pseudocoreset instead of the entire dataset, reducing computational cost and overfitting. We optimize the pseudocoreset using a stochastic algorithm during VAE training to minimize the Kullback-Leibler divergence. Experimental results demonstrate that ByPE-VAE outperforms state-of-the-art VAEs in density estimation, representation learning, and generative data augmentation. ByPE-VAE is also up to three times faster than Exemplar VAE on a basic architecture, while maintaining similar performance. The code is available at https://github.com/Aiqz/ByPE-VAE.