Rationales, which are snippets of text used to explain an inference, have become popular in interpretable natural language processing (NLP). These rationales typically involve a selector and a classifier that aim to maximize the mutual information between the selected text and the document label. However, methods based on mutual information often capture irrelevant patterns and result in nonsensical behaviors. This study explores whether counterfactual data augmentation (CDA), without human assistance, can enhance the performance of the selector by reducing the mutual information between spurious signals and the document label. The CDA approach generates counterfactuals using class-dependent generative models in an unsupervised manner. Through an information theoretic perspective, we determine the characteristics of the unaugmented dataset for which our CDA approach would be effective. We empirically evaluate the effectiveness of CDA by comparing it to various baselines, including an improved MMI-based rationale schema, on two multi-aspect datasets. Our results demonstrate that CDA produces rationales that better capture the intended signal of interest.