CODAC is an offline reinforcement learning (RL) algorithm designed for learning from observational data. The algorithm focuses on ensuring the learned policy is safe by quantifying the risk associated with different actions. In contrast to traditional RL algorithms, CODAC adopts a distributional approach by learning the distribution over returns rather than the expected return. This not only helps in quantifying risk but also improves planning and representation learning. CODAC achieves this by penalizing the predicted quantiles of the return for out-of-distribution actions. The algorithm has been proven to learn a conservative return distribution, particularly for finite Markov Decision Processes (MDPs), where it converges to a uniform lower bound on the quantiles of the return distribution. Experimental results demonstrate that CODAC successfully learns risk-averse policies using offline data collected from risk-neutral agents. Additionally, CODAC outperforms other state-of-the-art algorithms on the D4RL MuJoCo benchmark in terms of both expected and risk-sensitive performance. The code for implementing CODAC is available at https://github.com/JasonMa2016/CODAC.