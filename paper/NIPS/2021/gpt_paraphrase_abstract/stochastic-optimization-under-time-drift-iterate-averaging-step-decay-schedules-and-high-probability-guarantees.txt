This study addresses the issue of minimizing a convex function that changes over time due to unknown and potentially random dynamics. This problem is commonly found in the fields of machine learning and signal processing, referred to as concept drift and stochastic tracking. The researchers present new guarantees for the convergence of stochastic algorithms with iterate averaging, emphasizing bounds that hold both in expectation and with high probability. A significant finding is that the efficiency of the proximal stochastic gradient method in tracking depends logarithmically on the quality of the initialization, when combined with a step-decay schedule.