We present TransformerFusion, a transformer-based method for reconstructing 3D scenes. Using a monocular RGB video as input, we process the frames through a transformer network that combines the observations into a volumetric feature grid representing the scene. This grid is then decoded into an implicit 3D scene representation. Our approach leverages the transformer architecture to allow the network to focus on the most relevant frames for each location in the scene, guided solely by the scene reconstruction task. We fuse features in a coarse-to-fine manner, storing fine-level details only where necessary. This reduces memory usage and enables real-time fusion. To achieve a higher-resolution scene reconstruction, we decode the feature grid using a MLP-based surface occupancy prediction. Our approach outperforms existing methods for depth estimation, 3D reconstruction, and video sequence fusion using LSTM or GRU-based recurrent networks.