Unpredictable behavior of machine learning models on unseen data in the health domain raises concerns about their safety, as mistakes can have fatal consequences. This study explores the use of advanced out-of-distribution detectors to improve the reliability and trustworthiness of diagnostic predictions. Deep learning models related to various health conditions (such as skin cancer, lung sound, and Parkinson's disease) and using different types of input data (such as images, audio, and motion data) are selected. The study finds that these models make unreasonable predictions when tested on out-of-distribution datasets. However, using out-of-distribution detection methods based on Mahalanobis distance and Gram matrices, the models are able to accurately identify out-of-distribution data across different modalities. The out-of-distribution score is then translated into a confidence score that users can interpret, and a user study shows that participants only trust results with high confidence scores for making medical decisions, disregarding results with low scores. This work highlights the importance of considering dataset shift in high-stakes machine learning applications, particularly in medical diagnosis and healthcare, to ensure reliable and trustworthy predictions for users.