Traditional supervised learning methods, particularly deep learning models, are prone to being affected by out-of-distribution (OOD) examples. This is primarily due to the learned representation combining the semantic and variation factors, which are correlated with the specific domain, while only the semantic factor influences the output. To tackle this issue, we propose a novel approach called the Causal Semantic Generative model (CSG), which separates the two factors using causal reasoning. Additionally, we develop techniques for predicting OOD examples from a single training domain, a task that is both common and challenging. These methods are based on the principle of causal invariance, and we introduce a unique variational Bayes design to facilitate efficient learning and straightforward prediction. Theoretically, we demonstrate that, under specific conditions, CSG can accurately identify the semantic factor by fitting the training data. This semantic identification guarantees a limited OOD generalization error and successful adaptation. Empirical evaluation reveals that our proposed CSG model outperforms existing baselines in terms of OOD performance.