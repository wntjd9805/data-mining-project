Spectral normalization (SN) is a commonly used technique to enhance the stability and quality of Generative Adversarial Networks (GANs). However, the effectiveness of SN is not well understood. In this study, we demonstrate that SN addresses two critical issues in GAN training: exploding and vanishing gradients. We also establish a connection between SN and LeCun initialization, which explains why the popular implementation of SN requires no hyper-parameter tuning, unlike stricter versions with poor performance. Unlike LeCun initialization, which only controls gradient vanishing at the start of training, SN maintains this property throughout training. Based on this understanding, we propose a new technique called Bidirectional Scaled Spectral Normalization (BSSN), which incorporates insights from later improvements to LeCun initialization. Theoretical analysis shows that BSSN provides better gradient control than SN. Empirical results on various benchmark datasets demonstrate that BSSN outperforms SN in terms of sample quality and training stability.