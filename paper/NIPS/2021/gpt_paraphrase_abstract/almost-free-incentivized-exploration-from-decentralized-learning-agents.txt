In recent years, there has been increasing interest in incentivized exploration in multi-armed bandits (MAB), where a principal offers bonuses to agents for exploration. However, most existing studies focus on temporary myopic agents. This study breaks that barrier by examining incentivized exploration with multiple and long-term strategic agents, whose behaviors are more complex and commonly found in real-world applications. One important finding is that strategic agents' desire to learn actually benefits the principal's explorations by providing "free pulls" instead of harming them. Additionally, increasing the number of agents significantly reduces the burden of incentivizing for the principal. Surprisingly, the results show that when there are enough learning agents involved, the principal's exploration process can be nearly free. The main results of this study are based on three novel components: (1) a simple but effective strategy for providing incentives, (2) a carefully designed algorithm for identifying the best arm when rewards have unequal confidences, and (3) a lower bound analysis of UCB algorithms with high probability in finite time. Experimental results are provided to support the theoretical analysis.