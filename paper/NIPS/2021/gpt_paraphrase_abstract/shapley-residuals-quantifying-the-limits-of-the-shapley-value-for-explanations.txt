Popular techniques for determining the importance of features in nonlinear models often rely on approximations. These approximations involve defining a cooperative game that quantifies the value of different subsets of features in the model and then calculating the Shapley values of this game to distribute credit among the features. However, the specific scenarios in which the Shapley values fail to accurately approximate the true game have not been adequately described. This paper proposes a new approach that interprets Shapley values as the outcome of an orthogonal projection between vector spaces. By calculating the residual, which represents the kernel component of this projection, we can identify situations where the Shapley values are poor approximations. We present an algorithm to compute these residuals, classify different modeling settings based on their residual values, and demonstrate that the residuals provide additional insights about model predictions that cannot be captured by Shapley values alone. Thus, the Shapley residuals can serve as a useful indicator for practitioners, cautioning them against overly relying on Shapley-value-based explanations as a comprehensive understanding of the model.