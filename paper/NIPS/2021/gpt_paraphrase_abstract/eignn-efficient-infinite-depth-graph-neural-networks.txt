Graph neural networks (GNNs) are commonly used to model graph-structured data in various applications. However, current GNN models have limited ability to capture long-range dependencies in graphs due to their finite aggregation layers. To address this issue, we propose a GNN model called Efficient Infinite-Depth Graph Neural Networks (EIGNN) that can effectively capture very long-range dependencies. We derive a closed-form solution for EIGNN, making training a GNN model with infinite depth feasible. Additionally, we demonstrate that eigendecomposition can be used to achieve more efficient computation for training EIGNN. Through comprehensive experiments on both synthetic and real-world datasets, we show that EIGNN outperforms recent baselines in capturing long-range dependencies and consistently achieves state-of-the-art performance. Furthermore, our model proves to be more robust against noise and adversarial perturbations on node features.