To speed up large-scale training, parallelism is crucial. However, large batch training often sacrifices generalization performance. Previous methods have explored adaptive batching or hand-tuned static large batching, but they can only provide coarse-grained adaptation. In this paper, we propose a lightweight and automated adaptive batching methodology that allows for fine-grained batch size adaptation. Our method utilizes a efficient representation of critical gradient noise information. We have open-sourced our methodology as a plugin tool for mainstream machine learning frameworks. Extensive evaluations on popular benchmarks show that our methodology outperforms state-of-the-art approaches in both performance and batch size. Specifically, we achieve a new state-of-the-art batch size of 78k in BERT-Large pretraining with a SQuAD score of 90.69, surpassing the previous state-of-the-art with a batch size of 59k and a score of 90.58.