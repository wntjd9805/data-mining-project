Can the Transformer model perform 2D object and region recognition without explicit knowledge of the spatial structure? To investigate this, we introduce YOLOS, a set of object detection models based on the vanilla Vision Transformer. YOLOS requires minimal modifications, region priors, or task-specific biases. Surprisingly, YOLOS pretrained on mid-sized ImageNet-1k dataset achieves competitive performance on the challenging COCO object detection benchmark. For instance, the YOLOS-Base model, derived from the BERT-Base architecture, achieves a 42.0 box AP on COCO validation set. We also discuss the impact and limitations of current pre-training schemes and model scaling strategies for the Transformer in vision tasks. The code and pretrained models can be found at https://github.com/hustvl/YOLOS.