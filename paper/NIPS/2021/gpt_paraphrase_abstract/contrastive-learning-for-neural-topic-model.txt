Recent research has shown that adversarial topic models (ATM) can effectively capture semantic patterns in documents by distinguishing them from dissimilar samples. However, the current architecture of ATM has two limitations: it does not consider the similarity between similar documents that share the same distribution of important words, and it does not allow for the integration of external information such as document sentiments, which has been proven beneficial for training neural topic models. To overcome these limitations, we propose a new approach to reformulate the discriminative goal of the ATM as an optimization problem. We also introduce a novel sampling method that enables the integration of external variables. Our reformulation encourages the model to incorporate relationships among similar samples and enforces constraints on dissimilar ones. The sampling method, based on internal input and reconstructed output, helps the model identify salient words related to the main topic. Experimental results demonstrate that our framework outperforms other state-of-the-art neural topic models in terms of topic coherence across three benchmark datasets with varying domains, vocabulary sizes, and document lengths.