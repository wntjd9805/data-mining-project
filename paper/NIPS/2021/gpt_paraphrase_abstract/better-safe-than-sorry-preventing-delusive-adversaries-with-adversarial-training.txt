Delusive attacks aim to harm the accuracy of a learning model by making small changes to correctly labeled training examples. By defining this malicious attack as finding the worst-case training data within a specific Wasserstein ball, we demonstrate that minimizing adversarial risk on the perturbed data is equivalent to optimizing an upper bound of natural risk on the original data. This indicates that using adversarial training can be an effective defense against delusive attacks. Therefore, the decrease in test accuracy caused by delusive attacks can be largely recovered through adversarial training. We also discover that adversarial training can resist delusive perturbations by preventing the model from relying too heavily on non-robust features in a natural setting. Additionally, through experiments on popular benchmark datasets, we confirm that this defense is capable of withstanding various practical attacks. Both theoretical and empirical findings strongly support the use of adversarial training when dealing with delusive adversaries.