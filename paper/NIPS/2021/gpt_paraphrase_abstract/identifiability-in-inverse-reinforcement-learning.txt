Inverse reinforcement learning aims to reconstruct the reward function in a Markov decision problem based on agent actions. However, this problem has been identified as ill-posed and the reward function is not identifiable, even with perfect knowledge of optimal behavior. To address this issue, we propose a solution for problems with entropy regularization. In a given environment, we fully describe the reward functions that lead to a particular policy and demonstrate that by comparing actions taken under different discount factors or in sufficiently distinct environments, the hidden reward can be determined up to a constant value. Additionally, we present necessary and sufficient conditions for reconstructing time-homogeneous rewards on finite horizons, as well as action-independent rewards, building upon recent findings by Kim et al. [2021] and Fu et al. [2018].