Blau & Michaeli propose a mathematical concept of perceptual quality in lossy compression. They introduce the information rate-distortion-perception function, which extends the classical rate-distortion tradeoff. The idea of universal representations is explored, where the encoder remains fixed while the decoder varies to meet different distortion and perception constraints. The authors prove that the corresponding information-theoretic universal rate-distortion-perception function can be achieved approximately. For a Gaussian source, the entire distortion-perception tradeoff can be achieved asymptotically with a single encoder at the same rate. The achievable distortion-perception region for a fixed representation is characterized for arbitrary distributions, with approximate conditions identified. This motivates the exploration of practical constructions that are approximately universal across the rate-distortion-perception tradeoff, eliminating the need for a new encoder for each objective. Experimental results on MNIST and SVHN datasets suggest that machine learning models with a fixed encoder achieve operational tradeoffs with only a slight penalty compared to models with variable encoders.