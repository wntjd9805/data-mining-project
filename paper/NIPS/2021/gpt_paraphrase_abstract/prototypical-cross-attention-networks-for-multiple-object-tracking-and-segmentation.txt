We propose a new approach called Prototypical Cross-Attention Network (PCAN) for online multiple object tracking and segmentation. Unlike existing methods that primarily rely on temporal information for object association and single-frame predictions for segmentation, PCAN incorporates rich spatio-temporal information. It achieves this by distilling a space-time memory into prototypes and using cross-attention to retrieve valuable information from past frames. For object segmentation, PCAN employs a prototypical appearance module to learn foreground and background prototypes, which are then propagated over time. Our extensive experiments on the Youtube-VIS and BDD100K datasets demonstrate that PCAN outperforms current competition winners in video instance tracking and segmentation. It also proves effective for both one-stage and two-stage segmentation frameworks. Code and video resources can be found at http://vis.xyz/pub/pcan.