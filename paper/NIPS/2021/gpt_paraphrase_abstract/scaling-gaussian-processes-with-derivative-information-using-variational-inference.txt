Gaussian processes with derivative information are commonly used in scientific applications that involve Bayesian optimization and regression. However, incorporating derivative observations can be computationally expensive, especially for large problems. While previous research has addressed this issue for low-dimensional problems, the high-dimensional setting has not been explored. In this study, we propose a scalable method for Gaussian process regression with derivatives using variational inference. Similar to using inducing values to reduce the size of a training set, we introduce the concept of inducing directional derivatives to sparsify the partial derivative information. This allows us to construct a variational posterior that incorporates derivative information without depending on the dataset size or dimensionality. We demonstrate the scalability of our approach on various tasks, including a high-dimensional stellarator fusion regression task and training graph convolutional neural networks using Bayesian optimization on Pubmed. Surprisingly, our approach can even enhance regression performance when only label data is available.