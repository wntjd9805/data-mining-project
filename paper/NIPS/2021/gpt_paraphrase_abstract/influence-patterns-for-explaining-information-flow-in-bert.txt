Although the statement "attention is all you need" holds true, the reasons behind it remain unclear. Transformer models like BERT, which rely on attention mechanisms, outperform other models, but the specific flow of information from input to output is not well understood. To address this, we propose the concept of influence patterns, which represent sets of paths within a transformer model. These patterns help quantify and localize the information flow within the model. Our experiments reveal that a significant portion of information in BERT flows through skip connections rather than attention heads. We also observe that consistent patterns across different instances are indicative of BERT's performance. Moreover, we demonstrate that these patterns contribute significantly more to the model's performance compared to previous attention-based and layer-based methods.