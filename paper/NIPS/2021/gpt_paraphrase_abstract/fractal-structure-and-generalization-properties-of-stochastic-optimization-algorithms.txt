Understanding generalization in deep learning has been a challenge in statistical learning theory. Recent work has shown that the dataset and training algorithm play a role in determining generalization bounds. However, it is still unclear which properties of the data and algorithm affect generalization performance. In this study, we approach this problem from a dynamical systems theory perspective and model stochastic optimization algorithms as random iterated function systems (IFS). These IFSs have been well-studied in dynamical systems literature and can be shown to be ergodic with an invariant measure supported on fractal structures. Our main contribution is proving that the generalization error of a stochastic optimization algorithm can be bounded based on the complexity of the underlying fractal structure. Using results from dynamical systems theory, we link the generalization error to algorithm choice, hyperparameters, and problem geometry. We specialize our results to specific problems and algorithms and provide analytical estimates for our bound. For modern neural networks, we develop an efficient algorithm to compute the bound and validate our theory with experiments.