In practical scenarios of nonconvex matrix factorization, the rank of the true solution is often unknown, leading to an overspecified model rank. This results in slower convergence rates for local search algorithms. To address this issue, we propose a low-cost preconditioner for nonconvex matrix factorization, specifically the matrix sensing variant, which restores the convergence rate of gradient descent to a linear rate even in the overspecified case. This preconditioner also handles potential ill-conditioning in the ground truth. The singularity that occurs in classical gradient descent can be corrected by applying â„“2 regularization with a specific range of damping parameter values. An inexpensive estimation of the damping parameter can be obtained from the current iteration. The resulting algorithm, called preconditioned gradient descent (PrecGD), is robust to noise and converges linearly to an optimal error bound. Our experimental results demonstrate that PrecGD effectively restores linear convergence in other variants of nonconvex matrix factorization in the overspecified regime.