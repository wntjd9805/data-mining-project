Transfer learning aims to optimize the adaptation of pre-trained models to new settings with limited data for fine-tuning. Previous studies have shown that adversarial training on source data enhances the transferability of models to new domains, but the underlying reasons remain unclear. This paper presents a theoretical model that rigorously analyzes the impact of adversarial training on transfer learning. The findings demonstrate that adversarial training generates superior representations in the source data, resulting in more accurate predictions when fine-tuning on these representations for the target data. The paper also establishes that semi-supervised learning in the source data improves transfer learning by enhancing the representation, both theoretically and empirically. Furthermore, combining adversarial training with semi-supervised learning further enhances transferability, indicating that these two approaches benefit representations in complementary ways. Experimental validation is provided using popular datasets and deep learning architectures.