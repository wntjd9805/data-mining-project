Cooperative multi-agent reinforcement learning (MARL) has gained attention for its applications in science and engineering. However, a major challenge in MARL algorithms, such as the actor-critic framework, is the decentralized policy evaluation problem. This paper focuses on decentralized MARL policy evaluation with nonlinear function approximation, commonly found in deep MARL. The empirical decentralized MARL policy evaluation problem is reformulated as a decentralized nonconvex-strongly-concave minimax saddle point problem. A decentralized gradient-based descent ascent algorithm called GT-GDA is developed, which converges at a rate of O(1/T). To reduce sample complexity, two decentralized stochastic optimization algorithms, GT-SRVR and GT-SRVRI, are proposed, which improve GT-GDA with variance reduction techniques. All algorithms achieve an O(1/T) convergence rate to a stationary point of the reformulated minimax problem. Moreover, the convergence rates of GT-SRVR and GT-SRVRI imply O((cid:15)−2) communication complexity and O(mn(cid:15)−2) sample complexity, where m is the number of agents and n is the length of trajectories. This paper is the first to achieve O((cid:15)−2) in both sample and communication complexities in decentralized policy evaluation for cooperative MARL. Extensive experiments validate the theoretical results of the proposed decentralized policy evaluation algorithms.