This study addresses the challenge of automatically selecting an appropriate outlier detection algorithm and its hyperparameters for a new dataset. The authors propose METAOD, a data-driven approach based on meta-learning, to tackle this problem. Unlike model selection for classification and clustering, the task of unsupervised outlier model selection (UOMS) is difficult because there is no hold-out data with labels for model evaluation, and there is no universal objective function for model comparison. METAOD leverages the performance of various detection models on historical outlier detection benchmark datasets to automatically select an effective model for a new dataset without labels, evaluations, or comparisons. To capture task similarity, the authors introduce specialized meta-features that quantify the outlying characteristics of a dataset within their meta-learning framework. Extensive experiments demonstrate that selecting a model using METAOD outperforms not selecting a model at all, as well as other meta-learning techniques tailored for UOMS. Additionally, METAOD is highly efficient at test time, taking less than 1 second to select from a pool of over 300 models for a new task. The authors have made METAOD and their meta-learning database open-source to encourage practical use and further research on the UOMS problem.