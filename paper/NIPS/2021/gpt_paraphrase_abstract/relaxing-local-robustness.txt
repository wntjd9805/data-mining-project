CertiÔ¨Åable local robustness has gained attention for addressing security concerns in deep learning by preventing small-norm adversarial examples. However, in certain classification problems, strict separation between classes is unnecessary. In this study, we propose two relaxed safety properties for classifiers: (1) relaxed top-k robustness, analogous to top-k accuracy, and (2) affinity robustness, which specifies sets of labels that must be separated by a margin. We demonstrate how to efficiently certify models against these relaxed robustness properties and train them with minimal additional cost using gradient descent. Our experimental results show that these relaxed variants of robustness perform well in various classification problems, achieving higher certified accuracies and lower rejection rates compared to standard local robustness certification.