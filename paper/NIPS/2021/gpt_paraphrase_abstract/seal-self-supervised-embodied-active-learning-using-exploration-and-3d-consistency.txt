This paper investigates a method called Self-supervised Embodied Active Learning (SEAL) that leverages internet image data and models to enhance robot vision without the need for additional labels. SEAL utilizes perception models trained on internet images to learn an active exploration policy. By using 3D consistency, the observations gathered through this exploration policy are labeled and used to improve the perception model. The framework also incorporates 3D semantic maps to learn action and perception in a self-supervised manner. These semantic maps are used to compute intrinsic motivation rewards for training the exploration policy and labeling agent observations. The SEAL framework successfully closes the action-perception loop by improving object detection and instance segmentation performance, which in turn enhances Object Goal Navigation.