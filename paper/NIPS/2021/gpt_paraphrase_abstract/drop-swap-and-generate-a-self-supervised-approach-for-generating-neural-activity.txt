We present Swap-VAE, an innovative unsupervised method for learning meaningful representations of neural activity. By combining generative modeling with an alignment loss, our approach maximizes the similarity between transformed views of brain states. These transformed views are created by dropping out neurons and manipulating sample timing, resulting in representations that maintain temporal consistency and independence from specific neurons. Through evaluations on synthetic data and primate neural recordings, we demonstrate the ability to disentangle neural datasets along behavior-related latent dimensions.