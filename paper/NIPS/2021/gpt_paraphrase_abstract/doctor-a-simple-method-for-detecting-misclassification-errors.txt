Deep neural networks (DNNs) have proven to be highly effective in large-scale object recognition tasks and have gained widespread use in real-world applications, often implemented as "black boxes". To ensure their secure use, it is essential to determine which predictions of a DNN classifier can be trusted and which should be discarded. In this study, we present a method called DOCTOR that aims to identify the reliability of DNN predictions, allowing for acceptance or rejection accordingly. We investigate two scenarios: Totally Black Box (TBB), where only soft predictions are accessible, and Partially Black Box (PBB), where gradient-propagation for input pre-processing is permitted. Through empirical evaluation, we demonstrate that DOCTOR outperforms existing methods on various well-known image and sentiment analysis datasets. Notably, in the PBB scenario, we observe a reduction of up to 4% in the false rejection rate (FRR). DOCTOR can be applied to any pre-trained model, requires no prior information about the dataset, and is as simple as the existing methods in the literature.