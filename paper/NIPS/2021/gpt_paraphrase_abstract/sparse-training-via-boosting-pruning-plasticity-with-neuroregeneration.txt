Recent research on post-training pruning (LTH) and before-training pruning (SNIP) has generated significant interest. However, LTH is computationally expensive and SNIP often results in insufficient performance. In contrast, during-training pruning, which combines training and inference efficiency with comparable performance, has received less attention. To better understand during-training pruning, we investigate the impact of pruning throughout training on pruning plasticity, which refers to the ability of pruned networks to recover their original performance. Pruning plasticity helps explain various empirical observations on neural network pruning. We also discover that pruning plasticity can be enhanced by incorporating a brain-inspired mechanism called neuroregeneration, which involves regenerating the same number of connections as pruned. We propose a novel method called gradual pruning with zero-cost neuroregeneration (GraNet), which significantly advances the state of the art. Impressively, the sparse-to-sparse version of GraNet improves training performance on ImageNet using ResNet-50 without increasing the training time. The codes for GraNet are available at https://github.com/Shiweiliuiiiiiii/GraNet.