Vision-Language Pre-training (VLP) aims to learn multi-modal representations from image-text pairs for downstream vision-language tasks. Most VLP models use a CNN-Transformer architecture to embed images and align them with texts. However, CNNs have limitations in modeling long-range dependencies for visual relation learning. To address this, we propose a fully Transformer visual embedding for VLP. We introduce Inter-Modality Flow (IMF) to measure the interaction between vision and language, and Masked Feature Regression (MFR) to promote inter-modality learning. This is the first study to explore the benefits of Transformer for visual feature learning in VLP. Our method outperforms state-of-the-art VLP models and shows superiority on the IMF metric. We validate our approach on various vision-language tasks, including Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment, and Visual Reasoning.