Model quantization is a promising technique for compressing deep neural networks, particularly for use on lightweight devices. However, traditional model quantization methods require access to the original training data to maintain accuracy, which is often not feasible due to security and privacy concerns. One approach to overcome this limitation is to use synthetically generated samples, but this often relies on random noise input and fails to capture the distribution of the original data, particularly around decision boundaries. In response, we propose Qimera, a method that generates synthetic samples by superposing latent embeddings to capture boundary information. To improve the fidelity of the superposed embeddings, we introduce a disentanglement mapping layer and extract information from the full-precision model. Our experimental results demonstrate that Qimera achieves state-of-the-art performance in data-free quantization. The code for Qimera is available at https://github.com/iamkanghyunchoi/qimera.