Despite deep learning's success in various applications, it is widely recognized that it can fail catastrophically when presented with out-of-distribution data. To address this challenge, we focus on the domain generalization problem, where predictors are trained on related training domains and then evaluated on unseen test domains. We demonstrate that this problem can be viewed as an infinite-dimensional constrained statistical learning problem under a natural model of data generation and invariance condition. Our approach, called Model-Based Domain Generalization, is based on this equivalence. However, solving constrained optimization problems in deep learning is challenging, so we utilize nonconvex duality theory to develop unconstrained relaxations of the statistical problem with tight bounds on the duality gap. Motivated by this theory, we propose a novel domain generalization algorithm that guarantees convergence. In our experiments, we observe improvements of up to 30% compared to state-of-the-art domain generalization baselines on various benchmarks, including ColoredMNIST, Camelyon17-WILDS, FMoW-WILDS, and PACS. Our code is publicly available at the following link: https://github.com/arobey1/mbdg.