Shift invariance is a crucial aspect of convolutional neural networks (CNNs) that enhances their classification performance. However, our study reveals that being invariant to circular shifts can also make CNNs more susceptible to adversarial attacks. Initially, we examine the margin between classes when employing a shift-invariant linear classifier, and find that it solely depends on the DC component of the signals. Next, by leveraging findings related to infinitely wide networks, we demonstrate that fully connected and shift-invariant neural networks can generate linear decision boundaries in certain simple scenarios. Exploiting this observation, we establish that shift invariance in neural networks produces adversarial examples in the case of two classes, each comprising a single image with a black or white dot on a gray background. This finding holds significance beyond mere curiosity, as we empirically demonstrate that shift invariance diminishes adversarial robustness when applied to real datasets and realistic architectures. Lastly, we conduct preliminary experiments using synthetic data to investigate the underlying cause of this connection.