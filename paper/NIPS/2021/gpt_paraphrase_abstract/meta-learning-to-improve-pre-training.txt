Pre-training (PT) followed by fine-tuning (FT) has proven to be an effective approach for training neural networks and has shown significant performance improvements in various domains. However, the quality of the learned representations in PT can be influenced by design choices such as task and data reweighting strategies, augmentation policies, and noise models. Therefore, it is crucial to appropriately tune the hyperparameters associated with these strategies. Existing methods for hyperparameter tuning either struggle with high dimensions, are slow and memory-intensive, or cannot be directly applied to the two-stage PT and FT process. In this study, we propose a gradient-based algorithm for meta-learning PT hyperparameters that is efficient and scalable. We formalize the PT hyperparameter optimization problem and introduce a novel method for obtaining PT hyperparameter gradients using a combination of implicit differentiation and backpropagation through unrolled optimization. Our approach is demonstrated to improve predictive performance in two real-world domains. Firstly, we optimize high-dimensional task weighting hyperparameters in multitask pre-training on protein-protein interaction graphs, resulting in up to a 3.9% improvement in AUROC. Secondly, we optimize a data augmentation neural network for self-supervised PT with SimCLR on electrocardiography data, leading to up to a 1.9% improvement in AUROC.