Offline reinforcement learning (RL) algorithms aim to learn an optimal policy from a fixed dataset without actively collecting new data. There are two main methods used based on the composition of the offline dataset: imitation learning for expert datasets and vanilla offline RL for uniform coverage datasets. However, real-world datasets often deviate from these extremes, and the exact composition is usually unknown. To address this gap, we propose a new framework that smoothly combines imitation learning and vanilla offline RL. Our framework utilizes a weak version of the concentrability coefficient, which measures the deviation of the behavior policy from the expert policy. We explore whether an algorithm can be developed to achieve a minimax optimal rate that adapts to unknown data composition. To tackle this question, we introduce a lower confidence bound (LCB) algorithm that incorporates pessimism in the face of uncertainty in offline RL. We analyze the finite-sample properties of LCB and investigate information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our findings reveal surprising facts about optimality rates. Particularly, in contextual bandits and RL, LCB achieves a faster rate of 1/N for datasets close to expert-level compared to the usual rate of 1/N in offline RL, where N is the sample size of the batch dataset. In contextual bandits, we prove that LCB is adaptively optimal across the entire range of data compositions, smoothly transitioning from imitation learning to offline RL. Additionally, we demonstrate that LCB is nearly adaptively optimal in tabular MDPs.