Offline reinforcement learning (RL) tasks involve learning from a pre-collected dataset without further interactions with the environment. While RL-based methods have the potential to outperform behavioral policies, they are often impractical due to training instability and extrapolation errors that require careful hyperparameter tuning through online evaluation. On the other hand, offline imitation learning (IL) overcomes these issues by directly learning the policy without estimating the value function through bootstrapping. However, IL is limited in its ability to learn a strong policy and tends to learn a mediocre behavior from a dataset collected by a mixture of policies. In this study, we propose Curriculum Offline Imitation Learning (COIL) to address this limitation. COIL leverages behavior cloning to imitate neighboring policies with less data and uses an experience picking strategy to imitate from adaptive neighboring policies with higher returns. It also improves the current policy through curriculum stages. We compare COIL with both imitation-based and RL-based methods on continuous control benchmarks and find that COIL not only avoids learning a mediocre behavior on mixed datasets but also competes with state-of-the-art offline RL methods.