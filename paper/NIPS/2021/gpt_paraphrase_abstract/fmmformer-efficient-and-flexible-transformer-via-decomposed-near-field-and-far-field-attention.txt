We introduce FMMformers, a novel type of transformers inspired by the fast multipole method (FMM) used in particle simulation. FMMformers divide attention into near-field and far-field components, employing banded and low-rank matrices respectively. This approach enables FMMformers to compute the attention matrix with linear complexity in terms of computational time and memory usage, unlike the quadratic complexity of standard transformers. Through evaluations on the Long Range Arena and language modeling benchmarks, we demonstrate that FMMformers surpass standard transformers in terms of accuracy. Specifically, FMMformers achieve an average classification accuracy of 60.74% across the five Long Range Arena tasks, outperforming the average accuracy of 58.70% achieved by standard transformers.