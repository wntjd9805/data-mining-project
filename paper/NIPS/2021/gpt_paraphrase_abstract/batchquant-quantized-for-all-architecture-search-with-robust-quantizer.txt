The increasing use of deep learning models on edge devices requires fast adaptation to different scenarios with varying resource constraints. Model optimization strategies with adaptive configuration are popular for this purpose. However, existing methods face challenges such as instability during training and difficulty in navigating the search space. To address these issues, we propose BatchQuant, a robust quantizer formulation that enables stable training of a compact, single-shot, mixed-precision, weight-sharing supernet. Our approach, called Quantized-for-all (QFA), extends the one-shot weight-sharing NAS supernet to support subnets with arbitrary ultra-low bitwidth mixed-precision quantization policies without retraining. We demonstrate the effectiveness of our method on ImageNet, achieving state-of-the-art top-1 accuracy under a low complexity constraint. The code and models will be publicly available at the provided GitHub repository.