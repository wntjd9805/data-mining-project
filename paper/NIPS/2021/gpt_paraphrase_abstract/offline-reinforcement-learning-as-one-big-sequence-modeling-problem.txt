Reinforcement learning (RL) typically focuses on estimating stationary policies or single-step models by leveraging the Markov property. However, we can also view RL as a sequence modeling problem, aiming to generate a sequence of actions leading to high rewards. This perspective prompts us to consider whether high-capacity sequence prediction models used in other domains like natural language processing can effectively solve RL problems. In this study, we investigate the use of sequence modeling techniques in RL, employing a Transformer architecture to model trajectory distributions and repurposing beam search as a planning algorithm. By framing RL as a sequence modeling problem, we simplify the design choices and eliminate components commonly found in offline RL algorithms. We demonstrate the versatility of this approach in various tasks, including long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and offline RL. Furthermore, we combine this approach with existing model-free algorithms, resulting in a cutting-edge planner for sparse-reward, long-horizon tasks.