Deploying machine learning models in the real world can be difficult due to various challenges. Models trained on data from one location often fail when used with data from different locations. Agents trained in simulations struggle to adapt in real-world or new environments. Neural networks trained on a subset of the population may carry selection bias in their decision-making process. This study explores the problem of data shift from an information-theoretic perspective, identifying and describing different sources of error. It compares objectives from domain generalization and fair classification literature. The analysis and evaluation suggest that model selection should consider factors like observed data, correction methods, and the data-generating process.