Recent advancements in Reinforcement Learning (RL) have focused on problems with complex observation spaces. However, these studies assume that the optimal value function of the true MDP is realizable, which is often not the case in practical scenarios. This work addresses the more realistic scenario of agnostic RL with rich observation spaces and a fixed class of policies that may not include any near-optimal policy. An algorithm is proposed for this setting, which guarantees a bounded error based on the rank of the underlying MDP. The algorithm achieves a (H 4dK 3d log approximation, where H represents the length of the sample complexity bound of episodes, K is the number of actions, and "> 0 denotes the desired sub-optimality. Additionally, a nearly matching lower bound is provided for this agnostic setting, demonstrating that the exponential dependence on rank is inevitable without further assumptions.