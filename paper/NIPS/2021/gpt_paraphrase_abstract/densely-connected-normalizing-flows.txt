Normalizing flows are mappings that bijectively transform inputs into latent representations with a fully factorized distribution. They have advantages such as exact likelihood evaluation and efficient sampling. However, their capacity is often limited due to the bijectivity constraint. To address this issue, we introduce a method that incrementally adds noise to intermediate representations. We precondition the noise based on previous invertible units, which we call cross-unit coupling. Our invertible modules, similar to Glow, enhance the model's expressivity by combining a densely connected block with Nystr√∂m self-attention. We name our architecture DenseFlow, as both cross-unit and intra-module couplings rely on dense connectivity. Experimental results demonstrate significant improvements achieved by our proposed contributions, and our approach achieves state-of-the-art density estimation even with limited computational resources.