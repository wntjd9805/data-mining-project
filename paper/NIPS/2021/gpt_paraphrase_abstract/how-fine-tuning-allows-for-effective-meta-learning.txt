Representation learning has been a valuable tool for meta-learning, allowing for rapid acquisition of new tasks. Recent studies, such as MAML, learn task-specific representations by discovering an initial representation that requires minimal adjustment per task. We propose a theoretical framework to analyze MAML-like algorithms, assuming that all tasks require similar representations. Through gradient descent, we provide risk bounds on predictors obtained through fine-tuning, demonstrating that this method effectively utilizes the shared structure. We demonstrate these bounds in logistic regression and neural network scenarios. On the other hand, we identify scenarios where learning a single representation for all tasks fails. In the worst case, any algorithm using a "frozen representation" objective cannot outperform learning the target task without any additional information. This distinction highlights the advantages of fine-tuning-based objectives in few-shot learning.