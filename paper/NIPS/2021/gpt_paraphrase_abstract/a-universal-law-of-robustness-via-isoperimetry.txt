Traditionally, data interpolation requires a model with a number of parameters greater than the number of equations to be satisfied. However, in deep learning, models are often trained with significantly more parameters than required by classical theory. This phenomenon is perplexing. To explain it, we propose a theoretical understanding. Our research demonstrates that for a wide range of data distributions and model classes, overparametrization is necessary for smooth data interpolation. Specifically, we prove that achieving smooth interpolation necessitates d times more parameters than simple interpolation, where d represents the data dimension. This principle of robustness holds true for any smoothly parametrized function class with polynomial size weights, and any covariate distribution that adheres to isoperimetry. Previous work by Bubeck, Li, and Nagaraj conjectured this law for two-layer neural networks and Gaussian covariates. Additionally, our finding can be interpreted as an enhanced generalization bound for model classes comprising smooth functions.