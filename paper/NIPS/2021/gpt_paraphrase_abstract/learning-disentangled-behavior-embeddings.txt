This study explores the correlation between behavior and neural activity in neuroscience experiments. Traditionally, animal subjects are observed performing repeated motor tasks to gather data. However, recent advancements in computer vision and deep learning have opened up possibilities for automated analysis of behavior using large video datasets. In this research, the authors introduce a method called Disentangled Behavior Embedding (DBE) which learns robust behavioral representations from unlabeled, high-resolution videos of animals engaging in various behaviors across multiple sessions. They also propose an enhanced version called Variational Disentangled Behavior Embedding (VDBE) that combines DBE with a stochastic temporal model to generate interpretable behavioral videos and meaningful discrete behavior representations. The models achieve consistent behavior representations by separating dynamic behavioral factors (pose) from non-behavioral nuisance factors (context) using a deep autoencoder, while also leveraging the temporal structures of pose dynamics. Compared to other approaches, DBE and VDBE demonstrate superior performance in tasks such as fine-grained behavioral motif generation and behavior decoding.