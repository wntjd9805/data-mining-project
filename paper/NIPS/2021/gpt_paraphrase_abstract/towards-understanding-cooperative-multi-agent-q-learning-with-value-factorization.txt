Value factorization is a popular and promising approach for scaling up cooperative multi-agent reinforcement learning. However, its theoretical understanding is limited. In this paper, we propose a multi-agent fitted Q-iteration framework to analyze factorized multi-agent Q-learning. We explore linear value factorization and discover that this simple decomposition enables powerful counterfactual credit assignment, but it may not converge in certain scenarios. We further analyze that on-policy training or richer joint value function classes can enhance its local or global convergence properties, respectively. To validate our theoretical findings, we conduct empirical analysis on state-of-the-art deep multi-agent Q-learning algorithms using didactic examples and a wide range of StarCraft II unit micromanagement tasks.