Goal-conditioned hierarchical reinforcement learning (HRL) has shown promise in solving complex and long-horizon RL tasks. However, the large action space of the high-level policy in goal-conditioned HRL often leads to poor exploration and inefficient training. To address this issue, we propose a novel framework called HIerarchi-cal reinforcement learning Guided by Landmarks (HIGL) which trains a high-level policy with a reduced action space guided by landmarks, which are promising states for exploration. HIGL consists of two key components: (a) sampling informative landmarks based on state dispersion and novelty, and (b) encouraging the high-level policy to generate subgoals towards selected landmarks. The first landmark is selected as the shortest path in a graph of landmarks. Our experiments demonstrate that HIGL outperforms previous methods in a variety of control tasks due to efficient exploration guided by landmarks.