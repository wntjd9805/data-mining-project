Generalization is a major obstacle for implementing reinforcement learning (RL) systems in real-world scenarios. This paper highlights that the sequential nature of RL requires novel approaches to generalization, different from those commonly used in supervised learning. While supervised learning techniques can effectively generalize without explicitly considering uncertainty, this is not the case in RL. The authors reveal that generalizing to unseen test conditions from limited training conditions leads to implicit partial observability, which transforms fully-observed MDPs into POMDPs. Based on this insight, the authors reframe the generalization problem in RL as solving the induced partially observed Markov decision process, referred to as the epistemic POMDP. They demonstrate the shortcomings of algorithms that neglect this partial observability and propose a simple ensemble-based method for approximately solving the partially observed problem. Through empirical evaluation on the Procgen benchmark suite, they show that their algorithm derived from the epistemic POMDP significantly improves generalization compared to existing methods.