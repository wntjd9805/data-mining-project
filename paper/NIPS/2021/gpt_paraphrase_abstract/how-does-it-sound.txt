This study explores the challenge of capturing the rhythmic nature of free body movements in videos and proposes a new system called 'RhythmicNet' to generate soundtracks for these videos. RhythmicNet uses skeleton keypoints extracted from the human movements to create rhythmic sounds, following the process of music improvisation. It infers the music beat and style pattern from body keypoints to produce the rhythm and uses transformer-based and U-net models to generate drum hits, velocity, and offsets of the instruments. Additional instruments are added to the soundtrack based on the generated drum sounds. The effectiveness of RhythmicNet is evaluated on large-scale video datasets containing dance movements and various movements and actions from internet videos. The results demonstrate that the method can generate realistic music that aligns with different types of human movements.