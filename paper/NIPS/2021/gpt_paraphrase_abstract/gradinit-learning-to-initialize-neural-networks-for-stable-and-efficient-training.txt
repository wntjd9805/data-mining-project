Innovations in neural architectures have led to significant advancements in language modeling and computer vision. However, these new architectures often pose challenges in terms of choosing hyperparameters and ensuring stable training when network parameters are not properly initialized. Various initialization methods have been proposed for specific architectures, but they may not be applicable to new ones. This study introduces GradInit, an automated and architecture-agnostic approach for initializing neural networks. GradInit uses a simple heuristic to adjust the norm of each network layer, aiming to achieve the smallest possible loss value with a single step of SGD or Adam using prescribed hyperparameters. This adjustment involves introducing a scalar multiplier variable for each parameter block, which is optimized using a numerical scheme. GradInit enhances the convergence and test performance of convolutional architectures, with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, allowing it to be trained without learning rate warmup using either Adam or SGD with a wide range of learning rates and momentum coefficients. The code for GradInit can be accessed at https://github.com/zhuchen03/gradinit.