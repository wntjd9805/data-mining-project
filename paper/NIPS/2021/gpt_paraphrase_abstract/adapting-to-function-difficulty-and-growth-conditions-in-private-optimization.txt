We propose algorithms for private stochastic convex optimization that can adjust to the difficulty of the specific function being optimized. While previous studies provide worst-case bounds for any convex function, it is often the case that the function being considered belongs to a smaller class that has faster rates. Specifically, we demonstrate that for functions with κ-growth around the optimal point, our algorithms outperform the κ−1 rate. Importantly, our algorithms achieve these improved rates without prior knowledge of the growth constant κ. We build upon the inverse sensitivity mechanism and recent localization techniques in private optimization. We also provide lower bounds for these function classes that match our algorithms and show that our adaptive algorithm is minimax optimal for all κ ≥ 1 + c when c = Θ(1). Our algorithm achieves a privacy rate of ( d/nε)√κ, which is faster than the previous rate of ( d/nε).