We aim to develop an accurate positive-versus-negative classifier using only positive examples and unlabeled examples. This task involves two subtasks: Mixture Proportion Estimation (MPE) to determine the fraction of positive examples in the unlabeled data, and PU-learning to learn the desired classifier based on this estimate. However, existing methods for both subtasks struggle in high-dimensional settings. Recent heuristics lack theoretical coherence and heavily rely on hyperparameter tuning. In this study, we propose two straightforward techniques: BestBin Estimation (BBE) for MPE and Conditional Value Ignoring Risk (CVIR) as an objective for PU-learning. Empirically, both methods outperform previous approaches. We also provide formal guarantees for BBE when a model can successfully separate a small subset of positive examples. Our final algorithm, TEDn, alternates between these two procedures, significantly improving both the mixture proportion estimator and the classifier.