Recent advancements in self-supervised learning have greatly enhanced performance across various tasks. However, the focus of research in language model pre-training has primarily been on natural languages. It remains uncertain whether models like BERT and its variations are the most effective for pre-training when applied to other modalities, such as source code. This study introduces a novel pre-training objective called DOBF, which capitalizes on the structural characteristics of programming languages. DOBF pre-trains a model to reconstruct the original version of obfuscated source code. Our results demonstrate that models pre-trained with DOBF outperform existing methods in multiple downstream tasks. Specifically, we observe relative improvements of up to 12.2% in unsupervised code translation and 5.3% in natural language code search. Additionally, our pre-trained model exhibits the capability to deobfuscate fully obfuscated source files and suggest descriptive variable names.