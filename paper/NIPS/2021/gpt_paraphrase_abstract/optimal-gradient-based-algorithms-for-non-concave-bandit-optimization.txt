Bandit problems with linear or concave reward have been extensively studied, but there is limited research on bandits with non-concave reward. This study focuses on a wide range of bandit problems where the reward function is non-concave, including low-rank generalized linear bandit problems and two-layer neural network with polynomial activation bandit problems. For the low-rank generalized linear bandit problem, we introduce an optimal algorithm in the dimension, refuting previous conjectures. Our algorithms are based on a unified optimization approach that works well in various polynomial settings, achieving optimal rates. We also demonstrate the effectiveness of our algorithms in reinforcement learning with generative models, reducing sample complexity compared to previous methods. Additionally, we prove that standard optimistic algorithms like UCB are sub-optimal due to dimension factors. In the neural net setting with noiseless reward, we present a bandit algorithm with sample complexity equal to the intrinsic algebraic dimension. We also show that optimistic approaches have higher sample complexity, particularly polynomial in the extrinsic dimension, which can be exponentially worse in the polynomial degree.