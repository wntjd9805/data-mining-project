Approximate Bayesian inference has emerged as a reliable alternative to standard training for neural networks, offering strong performance on out-of-distribution data. However, Bayesian neural networks that employ high-fidelity approximate inference through full-batch Hamiltonian Monte Carlo suffer from poor generalization when faced with covariate shift, even underperforming classical estimation methods. This unexpected outcome can be attributed to the problematic nature of Bayesian model averaging under covariate shift, especially when input features exhibit linear dependencies that hinder posterior contraction. In contrast, various approximate inference techniques and classical maximum a-posteriori training remain unaffected by this issue. To address this challenge, we propose innovative priors that enhance the resilience of Bayesian neural networks to different forms of covariate shift.