We investigate the concept of private sampling from distributions. When given a dataset of n independent observations from an unknown distribution P, a sampling algorithm must generate a single observation from a distribution that is similar to P while also ensuring differential privacy. The main aim of sampling is to create small amounts of data that appear realistic. We establish precise lower and upper limits for the dataset size required to achieve this goal for three types of distributions: arbitrary distributions on {1, . . . , k}, arbitrary product distributions on {0, 1}d, and product distributions on {0, 1}d with a bias in each coordinate that is not close to 0 or 1. Our findings reveal that, in certain cases, private sampling requires significantly fewer observations compared to non-private learning about P; however, in other cases, private sampling is equally challenging as private learning. Interestingly, for certain distribution classes, the additional number of observations needed for private learning, as opposed to non-private learning, is entirely accounted for by the number of observations required for private sampling.