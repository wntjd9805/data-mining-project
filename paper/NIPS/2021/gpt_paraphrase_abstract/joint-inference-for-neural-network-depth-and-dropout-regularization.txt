Dropout regularization techniques are commonly used to prevent overfitting in neural networks. However, deep models still suffer from poor calibration, often exhibiting high confidence in incorrect predictions. To address this issue, we propose a unified Bayesian model selection approach that simultaneously infers the optimal network depth and applies dropout regularization. We introduce a beta process to infer the number of hidden layers, allowing for the possibility of an infinite number of layers. The activation probabilities of each layer are determined by binary vectors generated from a conjugate Bernoulli process. Experimental results across different domains demonstrate that our method outperforms state-of-the-art approaches in terms of both performance and well-calibrated uncertainty estimates. Additionally, our method enables neural networks to dynamically adjust their depths as new data becomes available, thereby mitigating catastrophic forgetting in continual learning scenarios.