In online reinforcement learning (RL), the exploration-exploitation trade-off is a challenge for the agent. Different objectives require different levels of exploration and exploitation. This paper proposes a decoupled approach to address this problem. It consists of an "objective-specific" algorithm that determines how many samples to collect at each state, as if it had a generative model of the environment, and an "objective-agnostic" sample collection strategy that generates the prescribed samples quickly. The paper presents an algorithm that can collect the desired samples in an unknown communicating MDP with a time complexity of O(BD + D^3/2S^2A), where B is the number of samples needed in each state-action pair, S is the number of states, A is the number of actions, and D is the diameter. The paper also demonstrates how this general-purpose exploration algorithm can be paired with "objective-specific" strategies to tackle various settings, such as model estimation, sparse reward discovery, and goal-free cost-free exploration in communicating MDPs, resulting in improved or novel sample complexity guarantees.