Machine learning models that are designed to be invariant under specific data transformations have demonstrated improved generalization in practice. However, the understanding of why this invariance leads to better generalization is currently limited. This study aims to investigate the generalization benefits of model invariance by introducing the concept of a sample cover induced by transformations. This refers to a representative subset of a dataset that can approximate the entire dataset through transformations. The paper provides refined generalization bounds for invariant models based on the sample cover for any data transformations. It also defines the "suitability" of a set of transformations based on the sample covering number, which represents the smallest size of its induced sample covers. The research reveals that generalization bounds can be further improved for transformations with a small sample covering number, indicating their suitability. The proposed sample covering number can be empirically evaluated, offering guidance for selecting transformations to enhance model invariance and improve generalization. Experimental evaluations on various datasets demonstrate that transformations with smaller sample covering numbers (e.g., the 3D-view transformation) result in a smaller gap between test and training error for invariant models, confirming the propositions made in this study.