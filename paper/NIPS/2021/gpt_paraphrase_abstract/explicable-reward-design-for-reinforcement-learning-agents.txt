We examine the creation of interpretable reward functions for a reinforcement learning agent that ensures an optimal policy aligns with a set of desired policies. Our aim is to make these reward functions both informative and easily understandable. The challenge lies in finding a balance between informative rewards, which typically require dense rewards for many learning tasks, and sparse rewards that are easier to interpret. To address this, we propose a new framework called EXPRD, which utilizes discrete optimization to design interpretable reward functions. EXPRD incorporates an informativeness criterion that assesses the optimality of target policies over different time horizons, considering actions taken from any starting state. We provide a mathematical analysis of EXPRD and demonstrate its relationship to existing reward design techniques like potential-based reward shaping. Experimental results on two navigation tasks validate the effectiveness of EXPRD in creating interpretable reward functions.