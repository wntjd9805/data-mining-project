We present a new approach to video instance segmentation using transformers. Current per-clip models have shown better performance compared to per-frame methods by utilizing information from multiple frames. However, these models require high computation and memory usage for frame-to-frame communication, which limits their practicality. In our work, we propose Inter-frame Communication Transformers (IFC) that significantly reduce the overhead for information-passing between frames by encoding the context within the input clip efficiently. We introduce concise memory tokens to convey information and summarize each frame scene. By exchanging information between these encoded memory tokens, we enhance and correlate the features of each frame with other frames. Our method achieves state-of-the-art performance on benchmark sets (AP 42.6 on YouTube-VIS 2019 val set using offline inference) while maintaining a fast runtime (89.4 FPS). It can also be used for near-online inference, processing videos in real-time with minimal delay. The code for our method is available at https://github.com/sukjunhwang/IFC.