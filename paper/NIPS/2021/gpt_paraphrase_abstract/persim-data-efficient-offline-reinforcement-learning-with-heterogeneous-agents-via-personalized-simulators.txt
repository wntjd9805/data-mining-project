We investigate offline reinforcement learning (RL) with heterogeneous agents in the face of limited data availability. Our study reveals that current offline and model-based RL methods perform poorly when only a single historical trajectory is observed for each agent under an unknown and potentially sub-optimal policy. Even well-known benchmark settings like "MountainCar" and "CartPole" show significant degradation in performance. To overcome this challenge, we propose PerSim, a model-based offline RL approach. PerSim first learns a personalized simulator for each agent by utilizing the historical trajectories of all agents. We posit that the transition dynamics across agents can be represented as a latent function of latent factors associated with agents, states, and actions. Theoretical analysis confirms that this function can be well approximated by a "low-rank" decomposition of separable agent, state, and action latent functions. Based on this representation, we design a regularized neural network architecture to effectively learn the transition dynamics per agent, even with limited offline data. We conduct extensive experiments on various benchmark environments and RL methods. The consistent improvement achieved by our approach, as measured by state dynamics prediction and eventual reward, demonstrates the effectiveness of our framework in leveraging limited historical data to simultaneously learn personalized policies for different agents.