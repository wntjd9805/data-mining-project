Federated learning is a distributed learning approach where multiple agents with access to local data collaborate to learn a global model. Recent research has focused on improving accuracy rates and ensuring social good properties, such as total error guarantees. One branch of this research has used game theory, treating federated learning as a hedonic game where error-minimizing players form coalitions. Previous work has shown the existence of stable coalition partitions but has not determined how far these solutions are from optimal. In this study, we introduce a notion of optimality based on average error rates among players and present an efficient algorithm to calculate an optimal arrangement. We then analyze the relationship between stability and optimality and find that in some cases, all stable arrangements are optimal. However, there are instances where stable arrangements have a higher cost than optimal. Finally, we provide a constant-factor bound on the performance gap between stability and optimality, demonstrating that the worst stable solution's total error is at most nine times higher than that of an optimal solution.