In recent years, personalized federated learning (pFL) has gained attention for its ability to handle statistical heterogeneity among clients. However, current pFL methods require all models to have the same structure and size, limiting their application in more diverse scenarios. To address this limitation, we propose a new training framework called KT-pFL that utilizes personalized models for different clients. KT-pFL transforms the aggregation procedure in pFL into a personalized group knowledge transfer training algorithm. This algorithm allows each client to maintain a personalized soft prediction at the server side, guiding the local training of others. KT-pFL achieves this by updating the personalized soft prediction of each client through a linear combination of all local soft predictions, using a knowledge coefficient matrix. This matrix adaptsively reinforces collaboration among clients with similar data distributions. To quantify each client's contribution to others' personalized training, the knowledge coefficient matrix is parameterized and trained simultaneously with the models. The matrix and model parameters are updated alternately in each round using gradient descent. Extensive experiments on various datasets (EMNIST, Fashion_MNIST, CIFAR-10) under different settings (heterogeneous models and data distributions) demonstrate that our framework is the first to achieve personalized model training through parameterized group knowledge transfer in federated learning. It also outperforms state-of-the-art algorithms, resulting in significant performance gains.