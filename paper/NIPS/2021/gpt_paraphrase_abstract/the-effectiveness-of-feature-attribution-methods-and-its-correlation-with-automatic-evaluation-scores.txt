The importance of explaining the decisions made by Artificial Intelligence (AI) models is growing in various real-world applications. Many research papers have proposed and discussed feature attribution methods, but most of them have only been evaluated using automated metrics. This study conducts the first user study to measure the effectiveness of attribution maps in assisting humans with ImageNet and StanfordDogs classification tasks. Surprisingly, feature attribution is not more effective than showing humans training-set examples. In fact, presenting attribution maps in fine-grained dog categorization tasks actually hinders human-AI team performance compared to AI alone. Additionally, automatic attribution-map evaluation measures poorly correlate with actual human-AI team performance. These findings emphasize the need for rigorous testing of methods in human-in-the-loop applications and a reconsideration of existing evaluation metrics.