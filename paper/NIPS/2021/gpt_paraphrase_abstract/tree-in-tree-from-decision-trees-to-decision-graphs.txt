This paper introduces the Tree in Tree decision graph (TnT), which extends the traditional decision tree to a more versatile and powerful directed acyclic graph. TnT constructs decision graphs by recursively growing decision trees within internal or leaf nodes, rather than using greedy training. TnT has a linear time complexity and can handle large datasets. Compared to decision trees, TnT achieves better classification performance with reduced model size, both on its own and as a base estimator in bagging/AdaBoost ensembles. Our proposed model is a novel, efficient, and accurate alternative to decision trees.