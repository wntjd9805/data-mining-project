This paper introduces a method called spatiotemporal joint filter decomposition, which separates the learning of spatial and temporal information in videos while maintaining their interdependency. The approach involves decomposing a 3D convolutional filter into separate spatial and temporal filter atoms. This results in three convolutional layers: a temporal atom layer, a spatial atom layer, and a joint coefficient layer. The decomposition allows for swapping spatial or temporal atoms with ones of different sizes while keeping the rest unchanged, enabling manipulation such as dilating temporal atoms to achieve tempo-invariance. The paper demonstrates how this decomposition facilitates the learning of 3D CNNs with full-size videos through two consecutive sub-stages: learning temporal atoms and joint coefficients with downsampled-spatial data in the temporal stage, and learning spatial atoms and joint coefficients with downsampled-temporal data in the spatial stage. Empirical results on multiple action recognition datasets show that this decoupled spatiotemporal learning approach significantly reduces model memory usage and enables deep 3D CNNs to model high-spatial long-temporal dependency with limited computational resources while achieving comparable performance.