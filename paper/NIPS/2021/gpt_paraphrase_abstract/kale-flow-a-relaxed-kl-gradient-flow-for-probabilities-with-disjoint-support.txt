We investigate the gradient flow of a relaxed approximation to the Kullback-Leibler (KL) divergence between a moving source and a fixed target distribution. This approximation, called KALE (KL Approximate Lower bound Estimator), solves a regularized version of the Fenchel dual problem that defines the KL over a restricted set of functions. By using a Reproducing Kernel Hilbert Space (RKHS) to define the function set, we demonstrate that the KALE smoothly transitions between the KL and the Maximum Mean Discrepancy (MMD). While the KALE, MMD, and other Integral Probability Metrics remain well-defined for mutually singular distributions, the KALE exhibits a higher sensitivity to discrepancy in the distribution's support compared to the MMD. This property makes the KALE gradient flow particularly suitable when the target distribution is supported on a low-dimensional manifold. Assuming sufficiently smooth trajectories, we prove the global convergence of the KALE flow. Additionally, we propose a particle implementation of the flow using initial samples from the source and target distribution to empirically validate the properties of the KALE.