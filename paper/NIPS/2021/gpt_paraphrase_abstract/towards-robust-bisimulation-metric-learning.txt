Deep reinforcement learning (DRL) requires learned representations to extract relevant information from complex observations. These representations must strike a balance between being robust to distractions and informative to the policy. Function approximation techniques are often used to learn stable and rich representations, enabling the practical application of the policy improvement theorem in high-dimensional continuous state-action spaces. Bisimulation metrics offer a solution to this representation learning problem by grouping functionally similar states together, promoting noise and distractor invariance. This study extends value function approximation bounds for on-policy bisimulation metrics to non-optimal policies and approximate environment dynamics. Theoretical findings reveal potential embedding pathologies that can arise in practical use, including issues stemming from underconstrained dynamics models and unstable dependence on the reward signal in environments with sparse rewards. To address these problems, the researchers propose two practical remedies: (i) a constraint on the representation space norm, and (ii) an extension of previous approaches involving intrinsic rewards and latent space regularization. The resulting method is shown to be more robust to sparse reward functions and capable of solving challenging continuous control tasks with distractions, which prior methods fail to achieve.