Recurrent Neural Networks (RNNs), particularly LSTM variants, are commonly used in deep learning for sequence-based learning tasks in text and speech. However, training these LSTM models is computationally intensive due to the repetitive nature of hidden state computation. While sparsity in Deep Neural Nets has been considered a way to reduce computation time, the use of non-ReLU activation in LSTM RNNs limits the potential for dynamic sparsity. In this study, we propose dropout-induced sparsity as a method to reduce computation in LSTM models. Dropout is a regularization mechanism that randomly drops neuron values during training iterations. We suggest structuring dropout patterns to drop the same set of neurons within a batch, resulting in sparsity at the column or row level of hidden states. This sparsity can be leveraged for computation reduction in SIMD hardware and systolic arrays. We analyze how dropout-induced sparsity propagates through network training stages and how it can be utilized at each stage. Our approach serves as a direct replacement for existing dropout-based settings and we demonstrate its effectiveness in three NLP tasks: language modeling, machine translation, and named entity recognition. Results show that our proposed approach reduces training time by 1.23× to 1.64× without sacrificing performance.