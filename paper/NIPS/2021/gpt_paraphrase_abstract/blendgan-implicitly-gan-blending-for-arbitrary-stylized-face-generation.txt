BlendGAN is a new approach to generating stylized faces using a flexible blending strategy and a generic artistic dataset. Unlike previous methods, BlendGAN can fit arbitrary styles in a single model without requiring many training images for each style. The approach involves training a self-supervised style encoder on a generic artistic dataset to extract style representations. A weighted blending module is then used to blend the face and style representations, allowing for control over the stylization effect. BlendGAN also introduces a new large-scale artistic face dataset called AAHQ. Experimental results show that BlendGAN outperforms existing methods in terms of visual quality and style diversity for both latent-guided and reference-guided stylized face synthesis. The project webpage is available at https://onion-liu.github.io/BlendGAN/.