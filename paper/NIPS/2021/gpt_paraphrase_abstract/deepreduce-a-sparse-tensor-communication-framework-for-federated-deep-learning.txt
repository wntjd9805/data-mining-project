Sparse tensors are commonly encountered in federated deep learning, either as a result of the gradients of deep neural networks or due to explicit sparsification procedures. However, existing communication methods do not adequately address the unique challenges of deep learning, resulting in unnecessary communication overhead. This study presents DeepReduce, a flexible framework specifically designed for the compressed communication of sparse tensors in federated deep learning. DeepReduce decomposes sparse tensors into two sets, values and indices, and allows for independent and combined compression of these sets. It offers support for various standard compressors like Deﬂate for values and Run-Length Encoding for indices. Additionally, the paper introduces two novel compression schemes, curve-ﬁtting based for values and bloom-ﬁlter based for indices, which achieve superior compression results. DeepReduce can be used alongside existing gradient sparsifiers without any user intervention, effectively reducing communication overhead. The framework is implemented on TensorFlow and PyTorch, and experimental results demonstrate that DeepReduce transmits 320% less data compared to existing sparsifiers, while maintaining accuracy. The code for DeepReduce is available at https://github.com/hangxu0304/DeepReduce.