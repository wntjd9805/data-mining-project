Predictor-based Neural Architecture Search (NAS) is a significant area of research aiming to reduce the time-consuming search process in traditional NAS methods. However, existing predictor-based methodologies face challenges in accurately representing the spatial topology information of graph structure data, leading to a decline in accuracy and generalization problems. Additionally, these methods do not utilize temporal information effectively during training. To address these issues, we propose a Transformer-based NAS performance predictor with a Laplacian matrix based positional encoding strategy. This approach improves the representation of topology information and achieves superior performance compared to previous state-of-the-art methods on NAS-Bench-101, NAS-Bench-201, and DARTS search space. Moreover, we introduce a self-evolution framework that leverages temporal information as guidance. By incorporating previous evaluation information as constraints in the current optimization iteration, our predictor's performance is further enhanced. Importantly, this framework is model-agnostic and can improve performance across various backbone structures for the prediction task. Notably, our proposed method helped us secure the 2nd position among all teams in the CVPR 2021 NAS Competition Track 2: Performance Prediction Track.