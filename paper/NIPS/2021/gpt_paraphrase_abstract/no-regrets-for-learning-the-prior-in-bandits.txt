We present AdaTS, a Thompson sampling algorithm that dynamically adjusts to bandit tasks it encounters. AdaTS adapts to an unknown task prior distribution by maintaining a parameter distribution. This uncertainty is taken into account when solving a bandit task. AdaTS is a fully-Bayesian algorithm that can be efficiently implemented in various bandit problems. We establish upper bounds on its Bayes regret, which measures the impact of not knowing the task prior, and demonstrate that it is minimal. Our experimental results support this theory, showing that AdaTS outperforms previous algorithms and performs well in challenging real-world scenarios.