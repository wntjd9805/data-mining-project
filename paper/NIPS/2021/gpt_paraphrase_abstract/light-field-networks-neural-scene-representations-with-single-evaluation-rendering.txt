Inferring 3D scene representations from 2D observations is a key challenge in computer graphics, computer vision, and artificial intelligence. The use of 3D-structured neural scene representations shows promise in understanding 3D scenes. This study introduces a new neural scene representation called LightField Networks (LFNs), which captures both the geometry and appearance of a 3D scene using a neural network to parameterize a 360-degree, four-dimensional light field. Unlike other methods, rendering a ray from an LFN only requires a single network evaluation, resulting in significant time and memory reductions and enabling real-time rendering. Additionally, by leveraging meta-learning, a prior over LFNs is learned, allowing for consistent light field reconstruction even from just a single image observation. The storage cost of a 360-degree light field using LFNs is much lower compared to traditional methods. The study also demonstrates the ability to extract sparse depth maps from LFNs using differentiability of neural implicit representations and a novel parameterization of light space.