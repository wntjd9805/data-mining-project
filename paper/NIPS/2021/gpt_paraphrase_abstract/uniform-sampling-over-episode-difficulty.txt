Episodic training is a crucial aspect of few-shot learning, which trains models on tasks with limited labeled data. Despite its success, the process of sampling episodes in episodic training has not received much attention. Therefore, we aim to determine the most effective approach for sampling episodes. In this study, we introduce a method to approximate the distributions of episode sampling based on their difficulty. We further conduct a comprehensive analysis and discover that uniformly sampling episodes according to their difficulty yields better results compared to other sampling techniques such as curriculum and easy-/hard-mining. Since our proposed sampling method is independent of the algorithm used, we can apply these findings to enhance the accuracy of few-shot learning across various episodic training algorithms. We validate the effectiveness of our method on popular few-shot learning datasets, algorithms, network architectures, and protocols.