Pretrained language models like BERT have significantly improved performance in various natural language processing tasks. However, fine-tuning these models requires a large number of training examples for each specific task. Many real-world NLP problems are "few shot" tasks, meaning they lack a sufficiently large training set. This study proposes a novel approach for few-shot text classification that utilizes a conditional neural process. The approach learns to transfer knowledge from other diverse tasks with rich annotation. Instead of representing tasks through input encoding, the proposed method represents tasks using gradient information from a base model. An adaptation network is then trained to modulate a text classifier based on the task representation. Experimental results demonstrate that this approach outperforms traditional fine-tuning, sequential transfer learning, and state-of-the-art meta learning approaches on a range of diverse few-shot tasks. Analysis and ablations were conducted to support the design choices made in this study.