Deep generative models have been incredibly successful, but there is a need for quantitative tools to evaluate their statistical performance. One evaluation framework that has gained attention is divergence frontiers, which can measure the trade-off between quality and diversity in these models. In this study, we establish non-asymptotic bounds on the sample complexity of divergence frontiers. Additionally, we introduce frontier integrals, which offer summary statistics of divergence frontiers. We demonstrate that smoothed estimators like Good-Turing or Krichevsky-TroÔ¨Åmov can address the missing mass problem and improve convergence rates. To showcase our findings, we present numerical examples from natural language processing and computer vision.