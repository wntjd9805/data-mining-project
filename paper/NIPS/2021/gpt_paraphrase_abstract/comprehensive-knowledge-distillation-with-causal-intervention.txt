This abstract discusses the concept of knowledge distillation (KD) in model compression. It highlights that existing distillation approaches focus on aligning sample representations between the teacher and student models, but overlook the transfer of class representations. It also points out that existing approaches force the student to imitate the teacher completely, disregarding the teacher's potential biases. To address these issues, the paper proposes a new method called comprehensive, interventional distillation (CID) that captures both sample and class representations from the teacher model while removing bias through causal intervention. Unlike existing literature, CID uses the softened logits of the teacher as context information rather than training targets, allowing for the removal of biased knowledge based on causal inference. The paper demonstrates that CID retains good representations while eliminating bias, resulting in better generalization ability and transferability across different datasets compared to existing state-of-the-art approaches. Extensive experiments on multiple benchmark datasets support these findings.