Multilingual models are efficient and effective in improving low-resource languages by leveraging crosslingual transfer. However, the optimization of multilingual models has not been well understood, particularly in situations where there is a data imbalance among languages. This imbalance creates tension between high resource and low resource languages, resulting in sub-optimal solutions for the low-resource languages. The common approach of upsampling low-resource data is not robust and can lead to underfitting of high-resource languages or overfitting of low-resource languages. To address this issue, we propose an optimization algorithm called Curvature Aware Task Scaling (CATS). CATS rescales gradients from different tasks to guide multilingual training towards neighborhoods with uniformly low loss for all languages. We conducted experiments on common benchmarks with varying degrees of data imbalance and found that CATS effectively improved multilingual optimization, leading to consistent gains in low-resource languages without negatively impacting high-resource languages. CATS is also robust to overparameterization and large batch size training, making it a promising method for training massive multilingual models that improve low-resource languages.