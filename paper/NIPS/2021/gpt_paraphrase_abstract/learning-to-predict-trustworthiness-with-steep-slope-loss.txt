The trustworthiness of predictions made by AI models is crucial for their safe and effective use. While previous methods have been reliable on small datasets, this study focuses on predicting trustworthiness on real-world large-scale datasets, which present greater challenges due to their high-dimensional features, diverse visual concepts, and large number of samples. The researchers found that existing trustworthiness predictors, trained with common loss functions, tend to view both correct and incorrect predictions as trustworthy. This is because correct predictions are more prevalent and it is difficult to differentiate between correct and incorrect predictions in complex real-world datasets. To address this, the researchers propose a new steep slope loss that separates features related to correct predictions from those related to incorrect predictions. They evaluate this loss using Vision Transformer and ResNet models as trustworthiness predictors and conduct comprehensive experiments on ImageNet. The results show that the proposed loss significantly improves the generalizability of trustworthiness predictors. The code and pre-trained predictors are available for reproducibility.