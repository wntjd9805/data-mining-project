We tackle the challenge of data efficiency in deep reinforcement learning by utilizing unlabeled data to pretrain an encoder, which is then fine-tuned using a small amount of task-specific data. Our approach incorporates latent dynamics modeling and unsupervised goal-conditioned RL to encourage the learning of representations that capture various aspects of the underlying Markov Decision Process (MDP). Even with only 100k steps of interaction on Atari games (equivalent to two hours of human experience), our method outperforms previous approaches combining offline representation pretraining with task-specific fine-tuning. Moreover, it compares favorably to other pretraining methods that require significantly more data. When combined with larger models and diverse, task-aligned observational data, our approach shows promise, nearing human-level performance and data efficiency on Atari in our best setting. The code associated with this work is available at https://github.com/mila-iqia/SGI.