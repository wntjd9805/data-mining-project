Recently, researchers have been using transformation-based self-supervised learning in generative adversarial networks (GANs) to address the issue of catastrophic forgetting in the discriminator. However, existing self-supervised GANs have separate self-supervised tasks that do not align with generative modeling because their self-supervised classifiers do not consider the generator distribution. To tackle this problem, we propose a new self-supervised GAN that combines the GAN task with the self-supervised task by augmenting the GAN labels through self-supervision of data transformation. In this approach, the original discriminator and self-supervised classifier are merged into a label-augmented discriminator that predicts the augmented labels to understand both the generator distribution and the data distribution under each transformation. This discrepancy between the two distributions is then used to optimize the generator. Theoretically, we demonstrate that the optimal generator can converge to replicate the real data distribution. Empirically, we show that our proposed method outperforms previous self-supervised and data augmentation GANs in both generative modeling and representation learning using benchmark datasets.