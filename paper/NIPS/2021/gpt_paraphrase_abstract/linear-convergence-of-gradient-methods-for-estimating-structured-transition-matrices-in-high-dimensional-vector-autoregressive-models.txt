This paper presents non-asymptotic optimization guarantees for gradient descent methods used to estimate structured transition matrices in high-dimensional vector autoregressive (VAR) models. The authors utilize projected gradient descent (PGD) for single-structured transition matrices and alternating projected gradient descent (AltPGD) for superposition-structured ones. Their analysis demonstrates that both gradient algorithms converge linearly to the statistical error, even in the absence of strong convexity of the objective function in high-dimensional settings. Additionally, their results align with the phase transition theory of the corresponding model with independent samples, making them sharp in terms of matching. This analysis is the first to provide non-asymptotic optimization guarantees for the linear rate of regularized estimation in high-dimensional VAR models. Numerical results are included to support the theoretical analysis.