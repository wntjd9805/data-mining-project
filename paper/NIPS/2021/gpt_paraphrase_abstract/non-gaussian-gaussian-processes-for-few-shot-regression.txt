Gaussian Processes (GPs) are commonly used in machine learning for modeling distributions over functions. They are particularly useful in few-shot learning applications due to their reliance on Normal distributions and ability to compute the posterior probability function. However, GPs assume high similarity between tasks, which is often not the case in real-world scenarios. To address this limitation, we introduce Non-Gaussian Gaussian Processes (NGGPs), which leverage the flexibility of Normalizing Flows to modulate the GP's posterior predictive distribution. Our method makes the GP posterior locally non-Gaussian, allowing for more complex distributions. We propose an invertible ODE-based mapping that operates on each component of the random variable vectors and shares parameters across all of them. Empirical testing on various few-shot learning regression datasets demonstrates the flexibility of NGGPs, as the mapping can incorporate context embedding information to model different noise levels for periodic functions. This allows our method to share the problem structure between tasks while adapting to dissimilarities. NGGPs outperform existing state-of-the-art approaches in diverse benchmarks and applications.