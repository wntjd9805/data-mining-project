Quantization is a widely used technique in neural networks to convert floating-point numbers into lower-precision representations, such as 8-bit integers. This reduces memory usage and computational costs, making it easier to deploy resource-intensive models. However, quantization can introduce behavioral differences between the original and quantized models, leading to misclassifications. We investigate whether these disparities can be exploited by adversaries to create specific behaviors during quantization. To explore this, we develop an adversarial quantization training framework and conduct three types of attacks: indiscriminate, targeted, and backdoor attacks. We find that a compromised model can successfully defeat various quantization methods, including robust techniques. Additionally, in a federated learning setting, a group of malicious participants can inject a quantization-activated backdoor. We discuss potential countermeasures and find that re-training consistently removes the attack artifacts. Our code is available at https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation.