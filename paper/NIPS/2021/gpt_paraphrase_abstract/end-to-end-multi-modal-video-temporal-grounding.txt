This study focuses on the problem of identifying the time interval of an event in a video based on a natural language description. Existing methods primarily use RGB images as visual features, but we propose a multi-modal framework that incorporates RGB images for appearance, optical flow for motion, and depth maps for image structure. By combining these modalities, we can extract complementary information from videos. We use optical flow to capture large motion and depth maps to understand the scene configuration when the action involves objects recognizable by their shapes. To effectively integrate these modalities and enable inter-modal learning, we employ a dynamic fusion scheme with transformers that model the interactions between modalities. Additionally, we apply intra-modal self-supervised learning to enhance feature representations across videos for each modality, facilitating multi-modal learning. Experimental results on the Charades-STA and ActivityNet Captions datasets demonstrate that our proposed method outperforms state-of-the-art approaches.