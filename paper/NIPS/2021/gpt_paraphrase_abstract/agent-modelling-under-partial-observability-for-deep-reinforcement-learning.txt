Modelling the behaviors of other agents is crucial for understanding their interactions and making effective decisions. Current agent modelling techniques assume knowledge of the local observations and actions of the agents being modeled. In order to overcome this assumption, we employ encoder-decoder architectures to extract representations from the controlled agent's local information. By using the observations and actions of the modelled agents during training, our models learn to extract representations solely based on the controlled agent's local observations. These representations enhance the controlled agent's decision policy, which is trained using deep reinforcement learning. As a result, the policy does not need access to other agents' information during execution. Through comprehensive evaluations and ablation studies in various multi-agent environments, including cooperative, competitive, and mixed scenarios, we demonstrate that our approach outperforms baseline methods that do not utilize the learned representations.