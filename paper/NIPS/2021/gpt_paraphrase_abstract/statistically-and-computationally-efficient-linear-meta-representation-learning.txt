Few-shot learning typically involves tasks with limited data, making it difficult to learn them individually. To overcome this data scarcity, meta-representation learning methods train across multiple related tasks to find a shared representation of the data that can accurately solve all tasks. The hypothesis is that new tasks can be quickly trained on this low-dimensional representation using only a few samples. However, the statistical and computational properties of this approach are not well understood. Existing theoretical studies provide suboptimal statistical error or require many samples for each task, which is impractical in few-shot learning. Additionally, these studies propose algorithms that differ from those used in practice or are computationally intractable. To better understand the success of popular meta-representation learning approaches, we examine the alternating gradient-descent minimization (AltMinGD) method, which underlies these approaches. In a simplified setting of shared linear representations, we demonstrate that AltMinGD achieves nearly-optimal estimation error, requiring only a polynomial logarithmic number of samples per task. This aligns with the observed effectiveness of this algorithm in practical few-shot learning scenarios.