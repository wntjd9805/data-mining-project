Offline reinforcement learning (RL) presents challenges in finding an optimal policy from a pre-existing static dataset due to function approximation errors caused by out-of-distribution (OOD) data points. Previous methods have used constraint or penalty terms to guide the policy to align with the dataset, but these methods often require accurate estimation of the behavior policy or sampling from OOD data points. Additionally, they do not fully utilize the generalization ability of deep neural networks and can lead to suboptimal solutions close to the given dataset. This study proposes an uncertainty-based offline RL approach that considers the confidence of Q-value predictions without the need for data distribution estimation or sampling. By applying clipped Q-learning, a technique commonly used in online RL, OOD data points with high prediction uncertainties can be effectively penalized. Surprisingly, increasing the number of Q-networks and utilizing clipped Q-learning can significantly outperform existing offline RL methods across various tasks. Building on this finding, the study introduces an ensemble-diversified actor-critic algorithm that reduces the number of required ensemble networks while achieving state-of-the-art performance on most of the D4RL benchmarks.