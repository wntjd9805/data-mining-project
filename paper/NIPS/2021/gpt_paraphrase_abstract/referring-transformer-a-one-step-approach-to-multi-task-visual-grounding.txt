To advance visual reasoning, researchers have extensively studied visual grounding, such as phrase localization and referring expression comprehension (REC) or segmentation (RES). However, previous approaches to REC or RES have limitations in performance or require complex task-specific architectures. In this paper, we propose a straightforward one-stage multi-task framework for visual grounding. We utilize a transformer architecture to fuse visual and linguistic information in an encoder. In the decoder, the model learns to generate context-based linguistic queries, which are used to directly regress bounding boxes and produce segmentation masks for referred regions. Our model outperforms state-of-the-art methods significantly in REC and RES tasks. Additionally, we demonstrate that simple pre-training on an external dataset further enhances performance. Extensive experiments and ablations confirm that our model benefits greatly from contextualized information and multi-task training.