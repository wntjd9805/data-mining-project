The dominance of pre-trained language models (PrLM) in natural language processing tasks is evident, especially when it comes to low-resource languages. However, the performance of multilingual PrLMs can still be improved. Existing PrLMs have shown that incorporating monolingual linguistic structure knowledge can enhance performance. Therefore, we propose a new multilingual PrLM that combines explicit universal dependency parsing and implicit language modeling. By using universal dependency parse as both a pre-training objective and a learned representation, our model offers unprecedented interpretability and convenience in downstream tasks. We have compared our model with popular multilingual PrLMs, multilingual-BERT and XLM-R, on cross-lingual natural language understanding benchmarks and linguistic structure parsing datasets. The results demonstrate the effectiveness and stronger cross-lingual modeling capabilities of our approach.