The invariance principle in causality is a key aspect of approaches like invariant risk minimization (IRM) that aim to address failures in generalization outside of the expected distribution. While these approaches show promise in theory, they struggle in common classification tasks where invariant features encompass all label information. This raises the question: are these failures due to the methods' inability to capture invariance, or is the invariance principle itself insufficient? To find answers, we examine the basic assumptions in linear regression tasks, where invariance-based approaches have proven to generalize OOD. Contrary to linear regression tasks, we find that linear classification tasks require stricter constraints on distribution shifts for successful OOD generalization. Additionally, even with appropriate restrictions in place, we find that the invariance principle alone is not enough. To address this, we introduce a form of the information bottleneck constraint alongside invariance, which helps overcome key failures when invariant features encompass all label information while preserving existing success when they do not. We propose an approach that incorporates both principles and demonstrate its effectiveness through various experiments.