Overestimation in Q-learning is a well-studied issue in single-agent reinforcement learning, but has been neglected in the multi-agent setting. This study shows that QMIX, a popular Q-learning algorithm for cooperative multi-agent reinforcement learning, suffers from significant overestimation in practice and existing solutions are ineffective. To address this, a new regularization-based update scheme is proposed to penalize large joint action-values that deviate from a baseline, resulting in improved learning stability. Additionally, a softmax operator is introduced to further reduce overestimation bias, which is efficiently approximated in the multi-agent context. The proposed approach, called Regularized Softmax (RES) Deep Multi-Agent Q-Learning, is applicable to any Q-learning based MARL algorithm. Experimental results demonstrate that RES successfully mitigates overestimation and significantly enhances performance, achieving state-of-the-art outcomes in various cooperative multi-agent tasks, including challenging StarCraft II micromanagement benchmarks.