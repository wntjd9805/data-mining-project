We introduce the Self-Instantiated Recurrent Unit (Self-IRU), which addresses the limitations of standard recurrent neural networks (RNNs) in dealing with diverse and expanding data types. Unlike traditional RNNs, the Self-IRU incorporates a bias towards dynamic soft recursion, allowing for flexible architectural adaptability. This is achieved through recursive self-instantiation, where the gating mechanisms of the Self-IRU are controlled by instances of itself. Additionally, the extent of recursion is controlled by gates with variable values between 0 and 1, enabling dynamic soft recursion depth at each time step. Our proposed approach is effective across various data modalities, as demonstrated by its superior performance on the logical inference dataset, even when compared to models with access to ground-truth syntactic information.