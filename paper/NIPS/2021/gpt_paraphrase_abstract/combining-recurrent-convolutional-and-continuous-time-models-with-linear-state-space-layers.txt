Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular deep learning models for time-series data. They each have their own strengths and weaknesses in terms of modeling power and computational efficiency. In this study, we propose a new sequence model called the Linear State-Space Layer (LSSL) that combines the advantages of these approaches while addressing their limitations. The LSSL model is inspired by control systems and maps a sequence u to an output y by simulating a linear continuous-time state-space representation. We demonstrate that LSSL models are closely related to RNNs, temporal convolutions, and NDEs, inheriting their strengths such as generalizing convolutions to continuous-time and providing time-scale adaptation. We also introduce a subset of structured matrices A that can be trained to endow LSSLs with long-range memory. Empirically, we show that stacking LSSL layers in a deep neural network achieves state-of-the-art results in various time series tasks, including sequential image classification, healthcare regression, and speech. In fact, LSSL outperforms prior approaches by 24 accuracy points in a challenging speech classification task with long sequences, even surpassing baselines that use hand-crafted features on significantly shorter sequences.