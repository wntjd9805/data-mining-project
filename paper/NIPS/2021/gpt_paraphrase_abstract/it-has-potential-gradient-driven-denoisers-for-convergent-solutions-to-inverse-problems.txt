In recent years, there has been growing interest in utilizing denoisers for solving general inverse problems. The regularization-by-denoising (RED) and plug-and-play priors (PnP) frameworks have emerged as leading approaches, incorporating likelihood functions and denoising algorithms to achieve state-of-the-art performance in imaging tasks. However, the convergence of these methods is still an active area of research. Previous studies have treated convolutional neural network (CNN) denoisers as approximations for maximum a posteriori (MAP) or minimum mean square error (MMSE) estimators to derive convergence results. However, since state-of-the-art denoisers do not exhibit symmetric Jacobians, they cannot be interpreted as either MAP or MMSE estimators. Additionally, enforcing the Lipschitz constant of CNN denoisers during training is impractical, making complete convergence guarantee impossible. In this work, we propose image denoisers that are derived as the gradients of smooth scalar-valued deep neural networks, acting as potentials. This ensures that the denoisers have symmetric Jacobians, allowing for MAP and MMSE estimators interpretation. Furthermore, we integrate these denoisers into RED and PnP schemes with a backtracking step size, eliminating the need for enforcing the Lipschitz constant. We develop a simple inversion method utilizing these denoisers and theoretically establish its convergence to stationary points of an underlying objective function. Numerical experiments demonstrate the effectiveness of our method in various imaging tasks, outperforming standard RED and PnP methods and providing additional provable stability.