Obtaining a Generalized self-concordance is a crucial property in the objective function of many important learning problems. We introduce a simple Frank-Wolfe variant that employs the open-loop step size strategy, resulting in a convergence rate of O(1/‚àöùë°) for this type of functions, considering the primal gap and Frank-Wolfe gap. This approach eliminates the need for second-order information or estimating local smoothness parameters as required in previous methods. Moreover, we demonstrate enhanced convergence rates for various common scenarios, such as uniformly convex or polyhedral feasible regions.