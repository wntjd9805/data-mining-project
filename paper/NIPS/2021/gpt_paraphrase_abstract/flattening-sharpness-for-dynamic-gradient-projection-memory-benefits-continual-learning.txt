Backpropagation networks are prone to catastrophic forgetting, where previously learned skills are forgotten when new ones are learned. Previous efforts to address this issue have focused on minimizing empirical risk through parameter regularization terms and episodic memory, without exploring the weight loss landscape. This paper investigates the relationship between the weight loss landscape and sensitivity-stability in continual learning and proposes a new method called Flattening Sharpness for Dynamic Gradient Projection Memory (FS-DGPM). FS-DGPM introduces a soft weight to represent the importance of each basis representing past tasks in GPM, allowing less important bases to be released dynamically to improve the sensitivity of new skill learning. Additionally, the paper introduces Flattening Sharpness (FS) to regulate the flatness of the weight loss landscape of all seen tasks, reducing the generalization gap. Empirical results demonstrate that FS-DGPM consistently outperforms baselines, effectively learning new skills while mitigating forgetting.