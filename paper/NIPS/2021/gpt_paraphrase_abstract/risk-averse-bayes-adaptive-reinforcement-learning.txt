This study focuses on risk-averse Bayes-adaptive reinforcement learning and aims to optimize the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs). The research shows that a policy optimizing CVaR in this context is cautious towards both the epistemic uncertainty arising from the prior distribution over MDPs and the aleatoric uncertainty caused by the inherent stochasticity of MDPs. To tackle this problem, the researchers reframe it as a two-player stochastic game and propose an approximate algorithm that utilizes Monte Carlo tree search and Bayesian optimization. Experimental results demonstrate that this approach surpasses baseline methods in addressing this problem.