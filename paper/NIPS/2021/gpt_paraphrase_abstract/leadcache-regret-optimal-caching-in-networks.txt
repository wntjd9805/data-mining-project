We examine an online prediction problem related to network caching, where multiple users are connected to caches through a bipartite network. Users can request files from a large catalog, and their requests can be fulfilled if the file is cached in any of the connected caches. Our goal is to predict, prefetch, and distribute files optimally across the caches to maximize cache hits. This problem is complex due to the non-convex and non-smooth nature of the objective function. To address this, we propose LeadCache, an efficient online caching policy based on the Follow-the-Perturbed-Leader approach. We prove that LeadCache is regret-optimal up to a factor of approximately O(n3/8), where n represents the number of users. We present two efficient implementations of LeadCache, using Pipage rounding and Madow's sampling, each making only one call to an LP-solver per iteration. Additionally, by assuming a Strong-Law-type condition, we demonstrate that the total number of file fetches under LeadCache is almost surely finite over an infinite horizon. Lastly, we establish an approximately tight regret lower bound using graph coloring results. Our findings indicate that the learning-based LeadCache policy outperforms existing caching policies both theoretically and empirically.