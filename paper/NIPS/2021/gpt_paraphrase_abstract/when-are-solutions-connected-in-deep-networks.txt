The issue of mode connectivity in deep neural network training has received significant attention in the research community. Two possible explanations have been proposed: the loss function having connected sublevel sets and the solutions found by stochastic gradient descent being dropout stable. However, these explanations are not always applicable in practice. This study improves upon these conditions by considering the quality of features at each intermediate layer and requiring less over-parameterization. The study shows that if the features at intermediate layers are linearly separable, then the connectivity can be achieved with fewer parameters. Experimental results confirm that this approach ensures connectivity even when the previous conditions are not met.