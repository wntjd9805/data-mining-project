This paper introduces a method for improving content-based image retrieval results through visual re-ranking. The authors propose a technique that utilizes contextual similarity among top-ranked images to enhance semantic relevance. The method involves representing each image in the top-K ranking list as an affinity feature vector, which is refined by aggregating contextual information using a transformer encoder. The refined affinity features are then used to recalculate similarity scores and re-rank the images. A new data augmentation scheme is also designed to enhance the robustness and performance of the re-ranking model. The proposed method can be applied to retrieval result lists obtained from various retrieval algorithms. The effectiveness and generality of the method are demonstrated through comprehensive experiments on four benchmark datasets.