The aim of this study is to explore methods for improving estimates of conditional average treatment effects by leveraging the structural similarities between potential outcomes (POs) under different treatments. Existing strategies for treatment effect estimation often encourage heterogeneity even when it may not exist and do not fully utilize shared structure. In this paper, we compare three end-to-end learning strategies - regularization, reparametrization, and a flexible multi-task architecture - that incorporate inductive bias to promote shared behavior across POs. We implement these strategies using neural networks and conduct a range of semi-synthetic experiments to assess their relative strengths. Our findings demonstrate that all three approaches yield significant improvements compared to various baselines, and we gain insight into their performance in different experimental settings.