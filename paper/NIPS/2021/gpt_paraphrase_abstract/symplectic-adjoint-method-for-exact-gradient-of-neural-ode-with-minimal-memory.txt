The neural ODE model has revolutionized the learning of continuous-time dynamical systems and probabilistic distributions with remarkable accuracy. By utilizing the same network repeatedly during numerical integration, this model has proven effective. However, the backpropagation algorithm used in this model consumes memory in proportion to the number of network uses and the network size, even when employing a checkpointing scheme. An alternative method, known as the adjoint method, only utilizes memory for a single network use but requires significant computational resources to mitigate numerical errors. To address these limitations, this study introduces the symplectic adjoint method, which is an adjoint method solved by a symplectic integrator. This method achieves an exact gradient with memory consumption proportional to the number of network uses plus the network size, while also demonstrating superior performance compared to naive backpropagation and checkpointing schemes. Moreover, the symplectic adjoint method is more robust against rounding errors and exhibits faster execution.