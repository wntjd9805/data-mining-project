The process of finding suitable Graph Neural Networks (GNNs) for different tasks requires a lot of time and effort. Neural Architecture Search (NAS) has recently been used to automate this process and discover effective GNN architectures that can perform as well as, or even better than, manually designed ones. However, existing approaches using NAS to search for GNN structures do not explain how NAS selects the desired architectures. In this paper, we address this question for the first time by conducting theoretical analysis and experiments. We find that gradient based NAS methods tend to choose architectures based on the relevance of different types of information to the target task. However, we also discover that these methods are affected by noise in the graph, leading to suboptimal architectures. To overcome this, we propose a new model called Graph differentiable Architecture Search model with Structure Optimization (GASSO). GASSO enables differentiable search of the architecture using gradient descent and incorporates graph structure learning to remove noise during the search process. Extensive experiments on real-world graph datasets demonstrate that our GASSO model achieves state-of-the-art performance compared to existing methods.