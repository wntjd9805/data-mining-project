The challenge of transferring a model from the lab to the real world lies in its ability to generalize beyond the training data. Previous efforts have focused on creating invariant features that perform well across different domains. However, it is difficult to find features that are perfectly transferable, and some algorithms perform better than others in this regard. This paper aims to understand and quantify the transferability of features in domain generalization. The authors define transferability and explain its relationship with discrepancy measures between domains. They provide a method to estimate transferability and derive a new upper bound for target error based on it. The authors evaluate the transferability of existing algorithms and find that many do not learn highly transferable features. As a result, they propose a new algorithm that improves transferability and test it on various benchmark datasets. The experimental results confirm the theoretical findings and show consistent improvement over state-of-the-art algorithms.