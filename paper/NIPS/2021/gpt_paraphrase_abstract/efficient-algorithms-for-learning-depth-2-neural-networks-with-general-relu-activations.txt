We present efficient algorithms for learning feedforward neural networks with ReLU activations. Our algorithms are able to handle networks with unknown depth and non-zero bias terms. We achieve this by decomposing higher order tensors and using the Hermite expansion of the network function. Additionally, we establish the identifiability of the network parameters with minimal assumptions.