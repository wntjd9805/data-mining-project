Advancements in neural network verification have called into question the idea of a convex barrier, which refers to a weakness in the convex relaxation of a neural network's output. A recent breakthrough has led to the development of a tight relaxation that can verify the robustness of a neural network to infinite input perturbations, along with efficient solvers for this relaxation. Inspired by this progress, we aim to create similar techniques for verifying robustness to input perturbations within the probability simplex. Surprisingly, we discover that not only can a tight relaxation be designed to overcome the convex barrier in this scenario, but the size of the relaxation remains linear relative to the number of neurons. This leads to simpler and more efficient algorithms. We demonstrate the scalability of our approach by applying it to CIFAR-10 and MNIST classification tasks, where our method improves the verified accuracy by up to 14.4% compared to the current state of the art. Additionally, we test the accuracy of our approach on the challenging task of verifying the robustness of a multi-modal classifier (text and image) to arbitrary changes in its textual input.