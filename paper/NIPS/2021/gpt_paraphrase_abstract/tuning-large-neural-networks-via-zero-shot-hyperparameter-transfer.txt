Hyperparameter tuning in deep learning can be a time-consuming and expensive process, particularly for neural networks with a large number of parameters. However, researchers have recently discovered that certain optimal hyperparameters remain stable even as the size of the model changes. This discovery has led to a new approach called µTransfer, where the target model is parametrized using the Maximal Update Parametrization (µP) and the hyperparameters are indirectly tuned on a smaller model. These tuned hyperparameters can then be transferred to the full-sized model without the need for direct tuning. The effectiveness of µTransfer has been verified on Transformer and ResNet models. For instance, by transferring hyperparameters from a 13M parameter model, better performance is achieved compared to published results of the much larger BERT-large model (350M parameters), with a tuning cost equivalent to pretraining BERT-large once. Similarly, by transferring from a 40M parameter model, better results are achieved compared to the 6.7B GPT-3 model, with a tuning cost only 7% of the total pretraining cost. A Pytorch implementation of this technique can be found on GitHub at github.com/microsoft/mup.