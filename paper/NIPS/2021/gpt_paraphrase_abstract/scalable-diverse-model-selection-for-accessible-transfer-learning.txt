Finding the best weights to fine-tune pretrained deep learning models can be challenging given the abundance of models available today. Existing methods for model selection and transferability estimation are either not scalable or do not perform well on a diverse range of off-the-shelf models. This paper introduces the concept of "Scalable Diverse Model Selection" and proposes benchmarks to evaluate this task. The authors find that current methods perform poorly and analyze the reasons behind this. They then propose simple techniques to enhance the performance and speed of these algorithms. The authors present PARC, an improved method that outperforms all others in diverse model selection. The benchmarks and method code have been released to encourage further research in model selection for accessible transfer learning.