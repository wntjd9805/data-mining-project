We explore teaching through demonstrations in sequential decision-making scenarios. Our focus is on developing a personalized curriculum that accelerates the learner's progress. We propose a unified curriculum strategy for two popular learner models: Maximum Causal Entropy Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy Behavioral Cloning (CrossEnt-BC). Our strategy ranks demonstrations based on difficulty scores, computed by comparing the teacher's optimal policy and the learner's current policy. Unlike existing approaches, our strategy does not require knowledge of the learner's internal dynamics and still guarantees similar convergence with mild technical conditions. We also adapt our curriculum strategy for scenarios without a teacher agent by using task-specific difficulty scores. Our experiments in synthetic car driving and navigation environments validate the effectiveness of our curriculum strategy.