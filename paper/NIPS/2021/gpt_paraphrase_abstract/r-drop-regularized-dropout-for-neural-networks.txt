In this paper, we propose a method called R-Drop to address the inconsistency issue caused by dropout in deep neural network training. Dropout, while effective, introduces randomness that leads to inconsistency between training and inference. R-Drop aims to regularize dropout by ensuring that the output distributions of different sub models generated by dropout are consistent. To achieve this, R-Drop minimizes the bidirectional KL-divergence between the output distributions of two sub models sampled by dropout for each training sample. Theoretical analysis confirms that R-Drop reduces the aforementioned inconsistency. We conducted experiments on 18 datasets across various deep learning tasks, such as neural machine translation, abstractive summarization, language understanding, language modeling, and image classification. The results demonstrate the universal effectiveness of R-Drop. Notably, when applied to fine-tuning large-scale pre-trained models, such as ViT, RoBERTa-large, and BART, R-Drop achieves state-of-the-art performances, surpassing even models trained with extra large-scale data and expert-designed advanced variants of Transformer models. The code for R-Drop is publicly available on GitHub.