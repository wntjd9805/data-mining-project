A new trend in neural network training is the exploration of sparsity for faster training on edge devices. This paper introduces a Memory-Economic Sparse Training (MEST) framework that enhances accuracy and execution speed on edge devices. The framework includes Elastic Mutation (EM) and Soft Memory Bound (S) techniques to improve accuracy at high sparsity ratios. The paper emphasizes the importance of sparsity schemes on training performance and proposes the use of data efficiency to further accelerate sparse training. Results show that the framework significantly improves accuracy on ImageNet compared to existing methods. The MEST framework consistently outperforms other state-of-the-art works in terms of accuracy, training speed, and memory footprint. The code for the framework is available at the provided GitHub link.