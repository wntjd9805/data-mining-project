Fine-tuning is a widely used technique in deep learning that achieves good results on new tasks with limited training data. However, its theoretical understanding is still lacking. In this study, we investigate the sample complexity of fine-tuning for regression with linear teachers in various architectures. We find that the success of fine-tuning depends on the similarity between the source and target tasks, but measuring this similarity is not straightforward. We demonstrate that generalization is related to a measure that considers the relationship between the source task, target task, and covariance structure of the target data. In linear regression, we show that substantial reduction in sample complexity is possible when this measure is low. For deep linear regression, we present a novel finding about the inductive bias of gradient-based training with pretrained weights. This finding reveals that the similarity measure in this case is also influenced by the network's depth. Additionally, we provide results on shallow ReLU models and analyze how the sample complexity depends on the source and target tasks in this scenario.