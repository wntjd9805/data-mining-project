Psychological research demonstrates that people's enjoyment of certain things diminishes over time due to satiation. However, existing algorithms for recommender systems do not take into account this phenomenon and assume that user preferences remain constant. In this study, we introduce a new approach called rebounding bandits, which models satiation dynamics as linear dynamical systems that do not change over time. The expected rewards for each option decrease with repeated exposure but bounce back to the initial level when the option is not chosen. Traditional bandit algorithms cannot handle rebounding bandits because they require planning and modeling of the satiation dynamics. We analyze the planning problem and find that a greedy policy is optimal when all options have the same deterministic dynamics. To deal with stochastic satiation dynamics with unknown parameters, we propose the Explore-Estimate-Plan algorithm, which systematically selects options, estimates the system dynamics, and plans accordingly.