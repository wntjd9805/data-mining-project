The problem of safety in reinforcement learning is addressed in this study. The issue is framed within an episodic framework of a constrained Markov decision process. Previous research has demonstrated that a reward regret of approximately O(K) can be achieved while allowing a constraint violation of approximately O(K) in K episodes. However, the question arises whether it is possible to further reduce the constraint violation. The authors propose that when a strictly safe policy is known, it is possible to confine the system to zero constraint violation with high probability while maintaining a reward regret of approximately O(K). This is achieved through an algorithm that utilizes the principle of optimistic pessimism to safely explore the environment despite uncertainty. In cases where no strictly safe policy is known, but it is known to exist, it is still possible to restrict the system to bounded constraint violation with high probability. This is achieved using a primal-dual algorithm with an optimistic primal estimate and a pessimistic dual update.