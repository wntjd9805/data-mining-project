Recent observations have indicated that neural networks have a lower sample complexity compared to kernel methods when the distribution is isotropic (with an identity covariance matrix). This sensitivity to the data distribution is not unique to neural networks, as quadratic classifiers (quadratic polynomials with a nuclear-norm constraint) exhibit the same phenomenon. We establish an upper bound on the Rademacher Complexity, which relies on two key factors: the intrinsic dimension (a measure of isotropy) and the largest eigenvalue of the second moment matrix. Our findings enhance the existing bound by improving the dependence on dimension and precisely quantifying the relationship between sample complexity and the level of isotropy in the distribution.