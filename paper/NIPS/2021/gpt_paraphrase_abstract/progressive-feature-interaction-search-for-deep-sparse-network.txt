Deep sparse networks (DSNs) have emerged as the leading models for predicting tasks involving high-sparsity features. However, these models suffer from low computation efficiency due to their large size and slow inference. This limits their practical application. To address this issue, we propose using neural architecture search to automatically find the most critical component of DSNs, the feature-interaction layer. We create a refined search space that encompasses the desired architectures using fewer parameters. Additionally, we develop a progressive search algorithm that efficiently explores this space and effectively captures the order-priority property in sparse prediction tasks. Our experiments on three benchmark datasets demonstrate promising results in terms of both accuracy and efficiency. Further studies confirm the feasibility of our search space and algorithm.