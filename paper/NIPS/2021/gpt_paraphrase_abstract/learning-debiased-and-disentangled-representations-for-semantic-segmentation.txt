Deep neural networks often learn biased models with tangled feature representations, resulting in subpar performances, especially for under-represented classes. This issue has mainly been addressed in classification tasks, with little research on more complex problems like semantic segmentation. To tackle this, we propose a model-agnostic and stochastic training approach for semantic segmentation. Our method enables the learning of unbiased and untangled representations by extracting class-specific information and suppressing information from randomly selected classes in the feature space. By reducing feature dependencies among classes, our approach allows the model to learn more unbiased and untangled representations. Our models achieve impressive results on multiple semantic segmentation benchmarks, particularly improving performance for under-represented classes.