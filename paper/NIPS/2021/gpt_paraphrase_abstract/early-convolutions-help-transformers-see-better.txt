The optimizability of Vision transformer (ViT) models is inferior compared to convolutional neural networks (CNNs). The issue is believed to be the patchify stem of ViT models, which uses a large-kernel and large-stride convolution, contrary to the typical design choices of convolutional layers. To test this hypothesis, the optimization behavior of ViT models with the original patchify stem and a simplified counterpart using stacked stride-two 3x3 convolutions was analyzed. The change in early visual processing significantly improved optimization stability and peak performance, with a 1-2% increase in top-1 accuracy on ImageNet-1k dataset. This improvement was observed across different model complexities and dataset scales. Based on these findings, it is recommended to use a standard, lightweight convolutional stem for ViT models in this scenario for a more robust architectural choice compared to the original design.