Federated Learning is a new approach to distributed machine learning that allows models to be trained without sharing data. However, this method faces challenges due to high communication latency. Existing approaches like FedAvg become less efficient as latency increases. To address this issue, we propose Delayed Gradient Averaging (DGA), which delays the averaging step and enables local computation in parallel with communication. We prove theoretically that DGA achieves a similar convergence rate as FedAvg and demonstrate empirically that our algorithm can handle high network latency without sacrificing accuracy. We conducted benchmark tests on various vision and language tasks, with both IID and non-IID partitions, and found that DGA can speed up training by a factor of 2.55. Additionally, we built a Raspberry Pi cluster and observed consistent speed improvements of up to 4.07 in real-world federated learning applications.