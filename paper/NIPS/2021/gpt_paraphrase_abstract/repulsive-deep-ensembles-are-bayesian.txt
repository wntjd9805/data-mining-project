Deep ensembles have become popular in the deep learning community due to their simplicity and efficiency. However, maintaining diversity among independently trained ensemble members is challenging. This can lead to issues such as saturation of ensemble performance and inaccurate uncertainty estimates for out-of-distribution data. We propose a solution by introducing a kernelized repulsive term in the update rule of deep ensembles. This modification not only ensures diversity among ensemble members but also enables proper Bayesian inference. We demonstrate the effectiveness of this approach by comparing it to standard ensembles and Bayesian baselines on various prediction tasks.