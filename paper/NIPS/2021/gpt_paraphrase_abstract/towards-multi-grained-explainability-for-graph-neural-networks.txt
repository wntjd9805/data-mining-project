When a graph neural network (GNN) makes a prediction, the question of explainability arises: "Which part of the input graph has the most influence on the model's decision?" Existing approaches to explainability focus on either local explanations, which explain each instance independently but fail to capture class-wise patterns, or global explanations, which identify globally important patterns but may not be relevant locally. This limitation hinders the flexibility and effectiveness of explainers. Our work addresses this issue by proposing a multi-grained explainability paradigm. We leverage the pre-training and fine-tuning concept to develop our explainer, which generates explanations at different levels of granularity. The pre-training phase highlights class-wise characteristics from a global perspective, while the fine-tuning phase adapts the explanations to the local context. Experimental results on synthetic and real-world datasets demonstrate the superiority of our explainer in terms of AUC for explaining graph classification compared to existing methods. The code and datasets used in our work are publicly available at https://github.com/Wuyxin/ReFine.