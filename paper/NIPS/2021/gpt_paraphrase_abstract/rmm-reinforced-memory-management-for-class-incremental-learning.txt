This study proposes a dynamic memory management strategy called reinforced memory management (RMM) for Class-Incremental Learning (CIL). CIL trains classifiers with a limited memory budget, discarding most of the data from previous phases to make space for new data. However, existing methods for memory allocation in CIL are often sub-optimal. RMM leverages reinforcement learning to optimize memory allocation for incremental phases and different object classes. To make RMM compatible with CIL, the policy function of RMM is trained on pseudo CIL tasks using data from the 0-th phase and then applied to target tasks. RMM involves two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2 allocates memory for each specific class. It is a general and optimizable method for memory management that can be applied to any replaying-based CIL method. The effectiveness of RMM is evaluated by integrating it into two top-performing baselines and conducting experiments on three benchmarks. The results demonstrate clear improvements in performance. The code for RMM is available at a specified URL.