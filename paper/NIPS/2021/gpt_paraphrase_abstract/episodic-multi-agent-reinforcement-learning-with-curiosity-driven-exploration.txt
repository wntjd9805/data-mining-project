Efficient exploration in complex coordination problems remains a challenge in deep cooperative multi-agent reinforcement learning (MARL). This paper presents a new approach called Episodic Multi-agent reinforcement learning with Curiosity-driven exploration (EMC). The authors leverage the insight from popular factorized MARL algorithms that the individual Q-values, which represent the utility functions for local execution, are embeddings of local action-observation histories. These Q-values can capture the interaction between agents through reward backpropagation during centralized training. To address exploration, the authors use prediction errors of individual Q-values as intrinsic rewards and utilize episodic memory to exploit informative experiences for policy training. By considering an agent's individual Q-value function dynamics, which captures state novelty and influence from other agents, the intrinsic reward induces coordinated exploration towards new or promising states. The method is validated through didactic examples and shows significant improvements over state-of-the-art MARL baselines in challenging tasks within the StarCraft II micromanagement benchmark.