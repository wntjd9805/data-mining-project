We investigate a model-based approach to reward-free reinforcement learning using linear function approximation for episodic Markov decision processes (MDPs). The learning process consists of two phases: exploration and planning. During exploration, the agent interacts with the environment and collects samples without receiving any reward. In the planning phase, the agent is provided with a specific reward function and utilizes the samples collected during exploration to learn an effective policy. We introduce a new algorithm called UCRL-RFE under the Linear Mixture MDP assumption, where the transition probability kernel of the MDP can be represented by a linear function over certain feature mappings defined on the state, action, and next state. We demonstrate that in order to obtain an ϵ-optimal policy for any reward function, UCRL-RFE only needs to sample a maximum of (cid:101)O(H 5d2ϵ−2) episodes during the exploration phase. Here, H represents the length of the episode and d represents the dimension of the feature mapping. We also propose a modified version of UCRL-RFE using a Bernstein-type bonus and prove that it only requires a maximum of (cid:101)O(H 4d(H + d)ϵ−2) samples to achieve an ϵ-optimal policy. Additionally, by constructing a special class of linear Mixture MDPs, we establish that any reward-free algorithm must sample at least (cid:101)Ω(H 2dϵ−2) episodes to obtain an ϵ-optimal policy. Our upper bound matches the lower bound in terms of the dependence on ϵ and d if H is greater than or equal to d.