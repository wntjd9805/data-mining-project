This abstract discusses the challenge of recognizing complex activities that involve multiple humans interacting with different objects. It emphasizes the need for a detailed understanding of actors' roles, objects' affordances, and their relationships. The abstract introduces Activity Parsing as the task of segmenting and classifying activities, sub-activities, and atomic actions in videos, while also understanding the actors, objects, and their relationships at an instance level. The traditional pair-wise relationships used in scene or action graphs are deemed inadequate for capturing the dynamics between multiple entities. To address this, the abstract introduces the Action Hypergraph, a new representation of spatial-temporal graphs that includes hyperedges with higher-order relationships. Additionally, the abstract introduces the MOMA benchmark and dataset dedicated to activity parsing. Finally, the HyperGraphActivity Parsing (HGAP) network is proposed as a method to parse videos, outperforming other baselines.