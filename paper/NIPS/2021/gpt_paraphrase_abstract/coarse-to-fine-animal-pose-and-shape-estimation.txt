Current methods for estimating animal pose and shape often use a parametric model called SMAL. This model is based on scans of toy animals, which limits its ability to represent real animals with diverse poses and shapes. As a result, the estimated meshes may not fit well with the 2D evidence available. To address this issue, we propose a two-stage approach for reconstructing 3D animal meshes from single images. In the first stage, we estimate the pose, shape, and translation parameters using the SMAL model. Then, in the refinement stage, we use a graph convolutional network (GCN) to predict vertex-level deformations based on the estimated meshes. This combination of parametric and non-parametric representations improves the accuracy of the reconstructed meshes. We design our mesh refinement GCN (MRGCN) with hierarchical feature representations to overcome the limitations of traditional GCNs. Additionally, we introduce a local feature extractor to capture detailed shape information for mesh refinement. We evaluate our approach on multiple datasets and achieve state-of-the-art results. Our code is available on the project website1.