The theory of reinforcement learning poses a fundamental question: Can sample-efficient reinforcement learning be achieved if the optimal Q-function is within the linear span of a given d-dimensional feature mapping? A recent study by Weisz et al. (2020) answers this question negatively by establishing an exponential lower bound on the required sample size, even when the agent has a generative model of the environment. It is possible to overcome this lower bound by assuming a constant gap between the optimal Q-value of the best action and the second-best action for all states. However, this assumption still leads to an exponential sample complexity lower bound. Surprisingly, this result highlights an exponential difference between online RL and the generative model setting, where sample-efficient RL is possible with a constant gap. Additionally, we present two positive findings: sample-efficient RL is achievable under a low-variance assumption or a novel hypercontractivity assumption.