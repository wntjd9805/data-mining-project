Current methods in reinforcement learning often rely on online trial-and-error, which can be time-consuming and costly. To address this, researchers have turned to learning from offline datasets. However, existing offline reinforcement learning approaches are limited by constraints imposed by the dataset, which can restrict the potential of the learned policies. In this paper, we propose a new approach called MAPLE (Offline Model-based Adaptable Policy Learning) that allows for decision-making in out-of-support regions. Instead of being confined to the dataset, our adaptable policy can adjust its behavior when deployed in these regions. We evaluate our method on MuJoCo controlling tasks with offline datasets and find that MAPLE outperforms state-of-the-art algorithms, showing robust decision-making capabilities in out-of-support regions.