Training complex and effective models often requires a large amount of training data, which can be costly and time-consuming. Batch active learning is a common approach to address this issue by issuing queries to a labeling oracle in batches. However, batch sampling can be less adaptive and may include redundant examples within a batch. This risk increases with the batch size. In this study, we propose an efficient active learning algorithm designed for large batch sizes. Our sampling method combines uncertainty and diversity, allowing it to scale to batch sizes several orders of magnitude larger than previous studies (100K-1M). It significantly improves model training efficiency compared to recent baselines. Additionally, we provide an initial theoretical analysis, demonstrating label complexity guarantees for a related sampling method that is approximately equivalent to ours in specific settings.