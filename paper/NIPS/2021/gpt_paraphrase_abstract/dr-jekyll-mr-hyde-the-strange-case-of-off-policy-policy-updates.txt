The policy gradient theorem suggests that policies should only be updated in states that are visited by the current policy. However, this approach leads to insufficient planning in off-policy states and can result in suboptimal policies. To address this issue, we extend the policy gradient theory to allow policy updates with respect to any state density. By doing so, we demonstrate convergence to optimality under certain conditions on the updates' state densities, effectively solving the planning problem. Additionally, we establish improved asymptotic convergence rates compared to previous policy gradient literature. To put our theory into practice, we introduce an agent called Dr Jekyll & Mr Hyde (J&H) with dual personalities: Dr Jekyll exploits while Mr Hyde explores. J&H's separate policies enable the recording of both on-policy and off-policy replay buffers, allowing for updates using a combination of on-policy and off-policy data. J&H serves as a guideline for actor-critic algorithms to meet the requirements identified in our analysis. We extensively test J&H on finite Markov Decision Processes (MDPs) and observe its superior ability to recover from suboptimal policies without sacrificing convergence speed. Furthermore, we implement a deep version of the algorithm and obtain promising results on a simple problem.