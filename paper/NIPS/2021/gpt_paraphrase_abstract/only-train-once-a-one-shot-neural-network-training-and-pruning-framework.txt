We propose a framework called Only-Train-Once (OTO) that addresses the limitations of existing pruning methods for deep neural networks (DNNs). OTO compresses DNNs into slimmer architectures while maintaining competitive performance and significantly reducing the number of floating point operations (FLOPs). OTO achieves this through two key components: (i) partitioning the DNN parameters into zero-invariant groups, allowing for pruning of these groups without affecting the output, and (ii) formulating a structured-sparsity optimization problem and introducing a novel optimization algorithm called Half-Space Stochastic Projected Gradient (HSPG) to promote zero groups. HSPG outperforms standard proximal methods on group sparsity exploration and maintains comparable convergence. We demonstrate the effectiveness of OTO by training and compressing full models simultaneously from scratch, without the need for fine-tuning, resulting in improved inference speed and reduced parameter count. Our approach achieves state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10, and Bert for SQuAD, as well as competitive results on ResNet50 for ImageNet. The source code for OTO is available at https://github.com/tianyic/only_train_once.