Detecting out-of-distribution (OOD) samples is crucial for developing machine learning models for critical safety systems. However, existing approaches for OOD detection rely on having access to OOD samples during training, which may not be feasible in real-life scenarios. In this study, we propose a predictive normalized maximum likelihood (pNML) learner that does not make any assumptions about the tested input. We derive an explicit expression for pNML and its generalization error, known as the regret, for a single layer neural network (NN). Our findings demonstrate that this learner performs well when the test vector is within the subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or when the test sample is far from the decision boundary. Additionally, we explain how to efficiently apply the derived pNML regret to any pretrained deep NN by utilizing the explicit pNML for the last layer followed by the softmax function. This application does not require extra tunable parameters or additional data. We extensively evaluate our approach on 74 OOD detection benchmarks using various models and datasets, including DenseNet-100, ResNet-34, and WideResNet-40 trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30. Our results demonstrate a significant improvement of up to 15.6% compared to recent leading methods.