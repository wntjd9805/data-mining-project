Accurate uncertainty quantification in deep neural network predictions is crucial for real-world decision-making. While reliable predictors should be accurate when confident and indicate high uncertainty when likely to be inaccurate, modern neural networks tend to be overconfident and poorly calibrated. Recent research has focused on model calibration through regularization techniques during training to address overconfidence. However, our study reveals that these regularized models, while better calibrated, are not easily calibratable with post-hoc methods like temperature scaling and histogram binning. We conducted empirical studies demonstrating that overconfidence does not necessarily harm final calibration performance when post-hoc calibration is allowed. Instead, confident outputs limit the potential for improvement in the post-hoc calibration phase. Our findings suggest a new approach to enhance DNN calibration by integrating main training and post-hoc calibration as a unified framework.