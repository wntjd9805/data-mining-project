Many real-world datasets have imbalanced class distributions, which poses a challenge for semi-supervised learning (SSL) algorithms. These algorithms rely on the prediction of unlabeled data, which can be biased towards the majority classes in class-imbalanced datasets. Traditional techniques for handling class imbalance in labeled data cannot be easily applied to SSL algorithms. To address this issue, we propose a scalable SSL algorithm that effectively utilizes unlabeled data while mitigating class imbalance. Our approach involves introducing an auxiliary balanced classifier (ABC) attached to the representation layer of an existing SSL algorithm. The ABC is trained with a class-balanced loss on a minibatch, using high-quality representations learned from all data points in the minibatch. We also incorporate consistency regularization, a recent SSL technique, to train the ABC to be balanced across classes by selecting unlabeled data with equal probability for each class. Through experiments on four benchmark datasets, our proposed algorithm achieves state-of-the-art performance in class-imbalanced SSL scenarios.