Despite the impressive accuracy achieved by Graph Neural Networks (GNNs), their trustworthiness remains unexplored. Previous studies have shown that many modern neural networks are overly confident in their predictions. However, we have discovered that GNNs are primarily under-confident. Thus, there is a high demand for confidence calibration in GNNs. In this study, we propose a novel GNN model, called CaGCN, which incorporates a topology-aware post-hoc calibration function. We first confirm that the confidence distribution in a graph exhibits a homophily property, which inspires us to design CaGCN to learn the calibration function. CaGCN transforms the logits of GNNs into calibrated confidence scores for each node, while preserving the order between classes and maintaining accuracy. Additionally, we integrate the calibration GNN into a self-training framework, demonstrating that using calibrated confidence leads to more reliable pseudo labels and improves performance. Extensive experiments validate the effectiveness of our proposed model in terms of both calibration and accuracy.