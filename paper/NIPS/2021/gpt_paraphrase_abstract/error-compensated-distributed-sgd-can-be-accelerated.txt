Gradient compression is a popular technique for reducing communication cost in distributed training of large-scale machine learning models. This study focuses on developing efficient distributed methods that can be used with any compressor satisfying a certain contraction property, including both unbiased and biased compressors. However, using gradient compression can introduce errors that hinder convergence or cause divergence. To address this issue, error compensation/error feedback techniques have been proposed. However, it is unclear whether error compensated gradient compression can be combined with acceleration, especially for biased compressors. This study demonstrates, for the first time, that error compensated gradient compression methods can indeed be accelerated. The authors propose and analyze the error compensated loopless Katyusha method, showcasing an accelerated linear convergence rate under standard assumptions. Numerical experiments confirm that the proposed method achieves convergence with significantly fewer communication rounds compared to previous error compensated algorithms.