Recently, various vision transformer architectures have been introduced for dense prediction tasks, highlighting the importance of spatial attention in their success. In this study, we revisit the design of spatial attention and demonstrate that a carefully designed yet simple mechanism can perform just as well as state-of-the-art approaches. Consequently, we propose two efficient and easily implementable vision transformer architectures called Twins-PCPVT and Twins-SVT. These architectures solely rely on optimized matrix multiplications, which are readily available in modern deep learning frameworks. Notably, our proposed architectures deliver outstanding performance across a wide range of visual tasks, including image-level classification, dense detection, and segmentation. The simplicity and impressive performance of our proposed architectures suggest that they can serve as strong foundations for various vision tasks. The code for our architectures can be found at: https://git.io/Twins.