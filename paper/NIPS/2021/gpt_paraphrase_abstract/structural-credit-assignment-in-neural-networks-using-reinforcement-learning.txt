The problem of assigning credit in neural networks has been a topic of interest for a long time, and various alternatives to backpropagation have been proposed to enable local training of nodes. One early approach was to treat each node as an agent and use a reinforcement learning method called REINFORCE to update the nodes individually using a global reward signal. In this study, we revisit this approach and explore the possibility of using other reinforcement learning techniques to enhance learning. We formulate the training of a neural network as a finite-horizon reinforcement learning problem and discuss how this allows us to incorporate ideas from reinforcement learning, such as off-policy learning. We demonstrate that the standard on-policy REINFORCE method, even with variance reduction techniques, learns suboptimal solutions. To address this, we introduce an off-policy approach that enables us to consider the optimal action for other agents and overcome stochasticity in their behavior. Additionally, we show that these networks of agents can exhibit greater robustness to correlated samples when learning online.