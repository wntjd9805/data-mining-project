We present MERLOT, a model that learns multimodal script knowledge by analyzing transcribed speech in millions of YouTube videos. This self-supervised model effectively matches images to corresponding words and understands global context over time. MERLOT demonstrates strong temporal commonsense representations and achieves top performance on various video QA datasets. It also extends its capabilities to static images, enabling reasoning about dynamic visual scenes. Compared to similar models using supervised data, MERLOT achieves 80.6% accuracy on Visual Commonsense Reasoning, outperforming them by over 3%. Ablation analyses highlight the importance of training on videos, increasing the scale and diversity of the video corpus, and utilizing diverse objectives for comprehensive multimodal reasoning. The figure illustrates the process of learning multimodal event representations over time, which can be applied to various tasks requiring commonsense or temporal visual reasoning.