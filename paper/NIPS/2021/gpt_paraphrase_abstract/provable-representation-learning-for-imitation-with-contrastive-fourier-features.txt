This study focuses on imitation learning and the use of offline experience datasets to improve the efficiency of downstream imitation learning. The challenge lies in the fact that the target policy may not exhibit low-dimensional behavior, which can lead to misrepresentation of states. To overcome this challenge, the study proposes a representation learning objective that provides an upper bound on the performance difference between the target policy and a low-dimensional policy. This objective can be implemented using contrastive learning with transition dynamics approximated by either an implicit energy-based model or an implicit linear model with random Fourier features. Experimental results on tabular environments and high-dimensional Atari games support the practical benefits of this approach.