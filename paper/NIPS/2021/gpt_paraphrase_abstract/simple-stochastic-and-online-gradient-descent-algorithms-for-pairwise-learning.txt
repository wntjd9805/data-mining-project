Pairwise learning involves learning tasks where the loss function is dependent on a pair of instances. This type of learning is commonly used in bipartite ranking and metric learning. One popular method for handling streaming data in pairwise learning is the online gradient descent (OGD) algorithm. However, this algorithm requires pairing the current instance with a large buffering set of previous instances, leading to scalability issues. In this paper, we propose new stochastic and online gradient descent methods for pairwise learning that address this scalability issue. Our approach only pairs the current instance with the previous one, reducing storage and computational complexity. We provide stability results, optimization, and generalization error bounds for both convex and nonconvex, as well as smooth and nonsmooth problems. We introduce techniques to separate the dependency of models and the previous instance in both optimization and generalization analysis. Our study answers an open question regarding the development of meaningful generalization bounds for OGD using a small fixed buffering set. We also extend our algorithms and stability analysis to develop differentially private SGD algorithms for pairwise learning, improving upon existing results.