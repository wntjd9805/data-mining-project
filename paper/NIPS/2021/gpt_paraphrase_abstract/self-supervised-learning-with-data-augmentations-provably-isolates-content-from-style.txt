Self-supervised representation learning has achieved significant success in various domains. Typically, data augmentation is employed through manual transformations to preserve the semantic information of the data. This study aims to comprehend the empirical effectiveness of this approach from a theoretical standpoint. The researchers propose a latent variable model that partitions the latent representation into a content component, which remains invariant under augmentation, and a style component, which can vary. Unlike previous work on disentanglement and independent component analysis, this study allows for both statistical and causal dependencies in the latent space. Sufficient conditions are established to identify the invariant content partition, up to an invertible mapping, in both generative and discriminative scenarios using pairs of observation views. Numerical simulations validate the theory, particularly when latent variables are dependent. Additionally, a new dataset called Causal3DIdent is introduced, comprising visually complex images with intricate causal relationships. This dataset is used to examine the impact of data augmentations commonly applied in practice.