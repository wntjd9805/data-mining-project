The Hessian of a neural network, which measures parameter interactions, plays a crucial role in deep learning. Previous research has mainly relied on empirical methods that overlook the network's structure. In contrast, our study employs theoretical tools to analyze the range of the Hessian map, revealing its rank deficiency and underlying structural causes. This allows us to derive precise formulas and upper bounds for the Hessian rank in deep linear networks, providing an elegant interpretation of rank deficiency. We also demonstrate that our bounds accurately estimate the numerical Hessian rank for a wider range of models. Additionally, we explore how model architecture, such as width, depth, and bias, affects rank deficiency. Overall, our research offers valuable insights into redundancy in overparameterized neural networks.