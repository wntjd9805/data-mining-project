Most existing methods for Neural Architecture Search (NAS) only focus on generating architectures and do not consider finding optimal parameters. Although some NAS methods use a supernet trained on a large dataset like ImageNet, they may not be optimal when the target tasks differ significantly from the dataset used to train the supernet. To overcome this limitation, we introduce a new problem called Neural Network Search (NNS), which aims to find the optimal pretrained network for a novel dataset and specific constraints (e.g., parameter count) from a model zoo. We propose a framework called Task-Adaptive Neural Network Search (TANS) to address this problem. TANS utilizes a model zoo consisting of networks pretrained on various datasets and employs an amortized meta-learning framework with contrastive loss. This framework learns a cross-modal latent space that maximizes the similarity between a dataset and a high-performing network trained on it, while minimizing the similarity between irrelevant dataset-network pairs. We evaluate our method on ten real-world datasets and compare it against existing NAS/AutoML baselines. The results demonstrate that our approach can quickly retrieve networks that outperform models obtained with baselines, requiring significantly fewer training steps to achieve the desired performance. This reduces the overall cost of finding a task-optimal network. Our code and model zoo are accessible at https://github.com/wyjeong/TANS.