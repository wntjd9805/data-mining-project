Despite recent advancements, current speech recognition systems still require labeled training data, which limits their use to a small number of languages. This paper introduces wav2vec-U, a method for training speech recognition models without any labeled data. The approach utilizes self-supervised speech representations to segment unlabeled audio and learns a mapping from these representations to phonemes through adversarial training. The success of this method relies on the use of appropriate representations. Compared to previous unsupervised methods, wav2vec-U significantly reduces the phone error rate on the TIMIT benchmark from 26.1 to 11.3. On the larger English Librispeech benchmark, wav2vec-U achieves a word error rate of 5.9 on test-other, which is comparable to systems trained on 960 hours of labeled data just two years ago. The method is also tested on nine other languages, including low-resource languages like Kyrgyz, Swahili, and Tatar. The code for wav2vec-U is available at https://github.com/pytorch/fairseq/tree/master/examples/wav2vec/unsupervised.