We suggest a novel approach to Imitation Learning (IL) by estimating the density of the expert's occupancy measure and then applying Maximum Occupancy Entropy Reinforcement Learning (RL) using the density as a reward. Our method maximizes a non-adversarial model-free RL objective that guarantees a lower bound on the reverse Kullback-Leibler divergence between the occupancy measures of the expert and imitator. We introduce a practical IL algorithm called Neural Density Imitation (NDI), which achieves superior demonstration efficiency on standard control tasks.