We introduce nested variational inference (NVI), a set of techniques that optimize proposals for nested importance samplers by minimizing forward or reverse KL divergence at each nesting level. NVI can be used with various commonly-used importance sampling strategies and allows for the learning of intermediate densities, which can act as heuristics to guide the sampler. In our experiments, we apply NVI to (a) generate samples from a multimodal distribution using a learned annealing path, (b) approximate the likelihood of future observations in a hidden Markov model, and (c) perform amortized inference in hierarchical deep generative models. We find that optimizing nested objectives improves sample quality in terms of log average weight and effective sample size.