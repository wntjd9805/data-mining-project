Knowledge graph embedding models aim to learn the representations of entities and relations in knowledge graphs in order to predict missing links between entities. The effectiveness of these models depends on their ability to model and infer various relation patterns such as symmetry, asymmetry, inversion, composition, and transitivity. While existing models can handle many of these patterns, transitivity, which is a common relation pattern, has not been fully supported. This paper presents a theoretical demonstration that transitive relations can be modeled using projections. The Rot-Pro model is then proposed, which combines projection and relational rotation to infer all the aforementioned relation patterns. Experimental results demonstrate that the Rot-Pro model effectively learns the transitivity pattern and achieves state-of-the-art performance in link prediction tasks involving datasets with transitive relations.