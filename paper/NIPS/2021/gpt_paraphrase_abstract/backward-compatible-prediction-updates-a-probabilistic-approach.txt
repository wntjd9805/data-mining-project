In this paper, we explore the challenges of integrating machine learning systems into real-world applications. While accuracy is important, there are other requirements to consider. With the availability of pre-trained and continually improving models, we face the problem of deciding which data points should be re-evaluated using new models, given limited resources. Additionally, we must determine whether to update predictions when the new model differs from the current one, taking into account the need for consistency in downstream applications. We propose a probabilistic approach to address these issues and formalize the Prediction Update Problem. Our method outperforms alternative strategies in experiments on standard classification benchmark datasets, particularly in terms of backward-compatible prediction updates.