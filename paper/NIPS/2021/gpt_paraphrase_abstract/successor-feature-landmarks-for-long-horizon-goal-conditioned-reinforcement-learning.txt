Goal-conditioned reinforcement learning (GCRL) is a challenging problem when dealing with long-horizon goals. Current approaches, which combine goal-conditioned policies with graph-based planning algorithms, struggle to handle large, high-dimensional state spaces and assume the availability of exploration mechanisms for efficient data collection. To address this, we propose Successor Feature Landmarks (SFL), a framework that explores large, high-dimensional environments to obtain a versatile policy for any goal. SFL utilizes successor features (SF) to capture transition dynamics, estimating state-novelty for exploration and creating a non-parametric landmark-based graph for high-level planning. Additionally, we use SF to directly compute a goal-conditioned policy for traversing between landmarks, executing plans to "frontier" landmarks at the edge of the explored state space. Experimental results on MiniGrid and ViZDoom demonstrate that SFL enables efficient exploration of large, high-dimensional state spaces and outperforms state-of-the-art baselines in long-horizon GCRL tasks.