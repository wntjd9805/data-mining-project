Different methods have been developed to estimate the generalization error of a supervised learning algorithm. However, these current estimates are often imprecise and lack guarantees. Consequently, they may not accurately represent the true generalization ability of the learning algorithm. Our primary contribution is a precise determination of the expected generalization error of the well-known Gibbs algorithm (also known as Gibbs posterior) by utilizing the symmetrized KL information between the input training samples and the output hypothesis. This finding can be used to improve existing estimates of expected generalization error and PAC-Bayesian bounds. Our approach is flexible, as it also evaluates the generalization error of the Gibbs algorithm with a data-dependent regularizer and in the asymptotic regime, where it converges to the empirical risk minimization algorithm. Importantly, our results emphasize the significance of symmetrized KL information in controlling the generalization error of the Gibbs algorithm.