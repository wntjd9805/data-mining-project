Model-agnostic meta-reinforcement learning faces challenges in estimating the Hessian matrix of value functions due to biased estimates resulting from repeatedly differentiating policy gradient estimates. This study presents a comprehensive framework for estimating higher-order derivatives of value functions using off-policy evaluation. The framework incorporates various previous approaches and clarifies the trade-off between bias and variance in Hessian estimates. Moreover, it introduces a new set of estimates that can be conveniently implemented using auto-differentiation libraries, ultimately improving performance. The code to replicate the findings is made publicly available.