Self-Supervised Learning (SSL) has proven to be effective in acquiring valuable and information-conserving representations. Despite the widespread use of Neural Networks (NNs), the nature of their weight space remains incompletely understood. Hence, we suggest employing SSL to acquire hyper-representations of NN weight populations. In order to achieve this, we introduce domain-specific data augmentations and an adjusted attention architecture. Our empirical assessment demonstrates that self-supervised representation learning in this field can successfully recover various NN model characteristics. Additionally, we demonstrate that the proposed learned representations surpass previous approaches in predicting hyper-parameters, test accuracy, generalization gap, and transferability to out-of-distribution scenarios. The code and datasets used in this study are publicly accessible.