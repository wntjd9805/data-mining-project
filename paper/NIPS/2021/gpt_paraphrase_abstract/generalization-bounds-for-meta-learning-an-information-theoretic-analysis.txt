We present a new analysis of meta-learning algorithms using information theory. Our analysis covers both traditional learning-to-learn approaches and modern model-agnostic meta-learning (MAML) algorithms. We also offer a data-dependent generalization bound for a stochastic version of MAML that is applicable to deep few-shot learning. Compared to previous bounds based on gradient norms, our bound is significantly tighter, as demonstrated through empirical validation on simulated data and a popular few-shot learning benchmark.