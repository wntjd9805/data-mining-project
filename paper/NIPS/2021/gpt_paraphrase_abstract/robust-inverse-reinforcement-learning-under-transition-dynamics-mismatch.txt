We investigate the problem of inverse reinforcement learning (IRL) when there is a discrepancy in transition dynamics between the expert and the learner. Our focus is on the Maximum Causal Entropy (MCE) IRL learner model, and we establish a precise upper limit on the degradation of the learner's performance based on the difference in transition dynamics. Drawing from the Robust RL literature, we propose a robust MCE IRL algorithm as a principled solution to address this mismatch. Through empirical experiments, we demonstrate that our algorithm maintains stable performance compared to the standard MCE IRL algorithm, even when transition dynamics mismatches occur in both finite and continuous MDP problems.