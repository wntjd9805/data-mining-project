Tensor network (TN) methods have been widely used in condensed matter physics and have recently gained attention in the machine learning community due to their ability to efficiently represent high-dimensional objects. These methods can be employed to learn linear models in exponentially large feature spaces. In this study, we establish both upper and lower bounds on the VC-dimension and pseudo-dimension of a broad range of TN models for classification, regression, and completion tasks. Our upper bounds are applicable to linear models parameterized by any TN structures, while our lower bounds are specific to common tensor decomposition models such as CP, Tensor Train, Tensor Ring, and Tucker. These results demonstrate the tightness of our general upper bound. Furthermore, we utilize these findings to derive a generalization bound that can be used in classification tasks involving low-rank matrices or linear classifiers based on commonly used tensor decomposition models. As a consequence of our research, we also provide a bound on the VC-dimension of the matrix product state classifier, addressing an open problem proposed by Cirac, Garre-Rubio, and Pérez-García.