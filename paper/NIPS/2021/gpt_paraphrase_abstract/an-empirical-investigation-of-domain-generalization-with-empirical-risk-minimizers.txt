This paper aims to further understand the phenomenon of deep neural networks trained using Empirical Risk Minimization (ERM) being able to generalize under distribution shift. The study investigates the extent to which the domain adaptation theory of Ben-David et al. explains the performance of ERMs. Surprisingly, the theory does not provide a complete explanation for the out-of-domain generalization observed in a large number of ERM models trained on three popular domain generalization datasets. As a result, the paper explores other potential measures that lack theoretical backing but could potentially explain generalization in this context. The investigation reveals that measures related to Fisher information, predictive entropy, and maximum mean discrepancy can serve as good predictors of the out-of-distribution generalization of ERM models. The authors hope that their work will encourage the community to develop a better understanding of when deep networks trained with ERM can generalize beyond their training distribution.