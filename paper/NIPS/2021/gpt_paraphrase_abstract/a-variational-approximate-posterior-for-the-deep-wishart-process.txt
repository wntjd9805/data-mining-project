Deep kernel processes have emerged as an alternative to neural networks, offering flexible learning of top-layer representations. In particular, the deep Wishart process (DWP) is intriguing because its prior can be equivalent to deep Gaussian process (DGP) priors for certain kernels. However, the lack of flexible distributions over positive semi-definite matrices has hindered inference in DWPs. This study introduces a novel approach by generalizing the Bartlett decomposition of the Wishart probability density to obtain flexible distributions. An approximate posterior for the DWP, incorporating layer dependency, is developed using this new distribution. Additionally, a doubly-stochastic inducing-point inference scheme for the DWP is proposed. Experimental results demonstrate that inference in the DWP can enhance performance compared to inference in a DGP with the equivalent prior.