We present a framework that enables the direct training of deep neural networks using popular non-decomposable performance measures such as AUC, multi-class AUC, and F-measure. Our optimization model involves solving Linear Programs (LP) during training, where the learned representations from upstream layers define the constraints or feasible set. Despite the large constraint matrix and modified constraints at each iteration, we propose a solution inspired by Mangasarian's ideas for 1-normSVMs, which uses a generalized Newton method to solve LPs efficiently on the GPU. This approach requires minimal unrolling and can handle large minibatch settings by utilizing sufficient dimension reduction. Our module is applicable without specific adjustments or relaxations, outperforming alternative methods that use surrogate lower bounds and specialized optimization schemes. We demonstrate the effectiveness of our approach on various use cases, highlighting its superior computational behavior and performance improvements on commonly used datasets.