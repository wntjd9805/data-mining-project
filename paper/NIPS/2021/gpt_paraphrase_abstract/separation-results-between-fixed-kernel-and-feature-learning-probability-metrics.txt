Previous studies have observed that discriminators that learn features perform better than discriminators with fixed-kernels in terms of the quality of generated samples. This research provides evidence of the differences between probability metrics used by fixed-kernel and feature-learning discriminators. The study focuses on function classes F2 and F1, which are used to analyze overparametrized neural networks with two layers. The researchers demonstrate that pairs of distributions over hyper-spheres cannot be distinguished by fixed-kernel integral probability metric (IPM) and Stein discrepancy (SD) in high dimensions, but can be distinguished by their feature learning counterparts. Additionally, the study explores the relationship between F1 and F2 IPMs and sliced Wasserstein distances to further investigate the separation. The findings suggest that fixed-kernel discriminators underperform compared to feature learning discriminators due to the weaker metrics they utilize.