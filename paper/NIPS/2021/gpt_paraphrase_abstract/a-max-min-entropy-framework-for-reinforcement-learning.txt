This paper introduces a new framework called max-min entropy for reinforcement learning (RL) that addresses a limitation of the soft actor-critic (SAC) algorithm in model-free sample-based learning. While maximum entropy RL focuses on reaching states with high entropy in the future, the max-min entropy framework aims to visit states with low entropy and increase the entropy of these states to enhance exploration. The paper presents an efficient algorithm for general Markov decision processes (MDPs) based on the proposed framework, which separates exploration and exploitation. Numerical results demonstrate that the proposed algorithm significantly outperforms current state-of-the-art RL algorithms.