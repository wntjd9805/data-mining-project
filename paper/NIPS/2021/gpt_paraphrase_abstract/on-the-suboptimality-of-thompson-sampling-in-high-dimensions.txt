This paper examines the use of Thompson Sampling (TS) for combinatorial semi-bandit problems. Surprisingly, it is found that TS is not optimal for this problem, as its regret increases exponentially with the ambient dimension and its minimax regret increases almost linearly. This holds true for various assumptions, including both linear and non-linear reward functions, Bernoulli distributed rewards, and uniform priors. Additionally, it is shown that adding forced exploration to TS does not solve the problem. Numerical results support these theoretical findings, demonstrating poor performance of TS in high dimensional scenarios.