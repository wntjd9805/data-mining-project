This study examines the limits of uniform convergence for offline policy evaluation (OPE) problems using model-based methods for episodic MDPs. The research establishes a unified framework for optimal learning in various offline tasks. The uniform OPE is a stronger measure compared to point-wise OPE and ensures offline learning when all policies are included in Π (the global class). The paper presents a lower bound of Ω(H 2S/dm(cid:15)2) for global uniform OPE and an upper bound of ˜O(H 2/dm(cid:15)2) for local uniform convergence, applicable to all near-empirically optimal policies for MDPs with stationary transition. The optimal rate of ˜O(H 2/dm(cid:15)2) is achieved through the introduction of a new analysis tool called singleton absorbing MDP, which works effectively with the model-based approach. The model-based framework is generalized to new settings, including offline task-agnostic and offline reward-free, with optimal complexities of ˜O(H 2 log(K)/dm(cid:15)2) (K is the number of tasks) and ˜O(H 2S/dm(cid:15)2) respectively. These findings offer a unified solution for simultaneous resolution of different offline RL problems.