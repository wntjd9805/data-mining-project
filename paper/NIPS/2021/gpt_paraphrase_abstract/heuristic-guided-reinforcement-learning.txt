We present a framework for speeding up reinforcement learning (RL) algorithms by incorporating domain knowledge or offline data. Traditional RL algorithms require extensive environment interactions or computation, which scales with the complexity of the decision-making task. With our framework, we demonstrate that using heuristic guidance in RL can lead to solving a shorter-horizon subproblem that guarantees solving the original task. Our framework acts as a horizon-based regularization technique, controlling bias and variance in RL within a limited interaction budget. Theoretical analysis highlights the attributes of a good heuristic and its impact on RL acceleration, including the concept of an improvable heuristic, which enables an RL agent to go beyond its existing knowledge. In practice, we apply our framework to enhance the performance of various state-of-the-art algorithms in tasks such as robotic control and procedural game generation. Our framework complements existing approaches of warm-starting RL with expert demonstrations or exploratory datasets, offering a systematic way to incorporate prior knowledge into RL.