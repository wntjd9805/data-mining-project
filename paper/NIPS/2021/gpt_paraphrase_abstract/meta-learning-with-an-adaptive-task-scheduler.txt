Meta-learning is a technique used to improve the learning of new tasks by transferring knowledge from previously learned tasks. However, existing meta-learning algorithms assume that all tasks are equally important and randomly sample tasks during the training process. This can lead to the inclusion of detrimental or imbalanced tasks, which can corrupt the meta-model or bias it towards majority tasks. To address this issue, this paper proposes an adaptive task scheduler (ATS) for meta-training.In ATS, a neural scheduler is introduced for the first time to determine which meta-training tasks should be sampled next. The scheduler predicts the probability of sampling each candidate task and is trained to optimize the generalization capacity of the meta-model for unseen tasks. Two factors related to the meta-model are used as inputs for the neural scheduler, representing the difficulty of a candidate task. Theoretical analysis shows that considering these factors improves the meta-training loss and optimization landscape.Under the scenario of meta-learning with noise and limited budgets, ATS significantly improves performance compared to state-of-the-art task schedulers. Specifically, ATS achieves up to a 13% improvement on the miniImageNet dataset and an 18% improvement on a real-world drug discovery benchmark.