We examine the convergence properties of random-search techniques for optimizing non-convex objective functions without using derivatives. We demonstrate that standard random-search methods, which do not utilize second-order information, converge to a second-order stationary point. However, these methods suffer from exponential complexity as the problem dimension increases. To address this issue, we introduce a new variant of random search that exploits negative curvature and relies solely on function evaluations. We prove that this approach converges to a second-order stationary point much more quickly than traditional methods, with a complexity that is linear in the problem dimension in terms of the number of function evaluations. Empirical testing of our algorithm aligns well with our theoretical findings.