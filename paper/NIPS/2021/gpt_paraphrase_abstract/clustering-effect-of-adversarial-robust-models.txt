Adversarial robustness has gained significant attention in the context of adversarial examples. Previous research has shown that robust models not only defend against various adversarial attacks but also enhance performance in downstream tasks. However, the underlying mechanism behind adversarial robustness remains unclear. This paper presents a new perspective on adversarial robustness by examining its relationship with linear components. The authors discover that robust models exhibit distinct hierarchical clustering effects on their linearized sub-networks when non-linear components are removed or replaced. Based on these findings, the authors propose a novel interpretation of adversarial robustness and apply it to tasks such as domain adaptation and robustness enhancement. Experimental results validate the effectiveness and superiority of their clustering strategy. The code for this research is available at https://github.com/bymavis/Adv_Weight_NeurIPS2021.