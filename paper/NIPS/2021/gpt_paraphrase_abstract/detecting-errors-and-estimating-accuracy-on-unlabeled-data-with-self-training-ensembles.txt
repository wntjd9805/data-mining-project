When deploying a deep learning model in real-world scenarios, it may encounter test data that differs from the training data and result in decreased performance. To ensure safe deployment, it is crucial to estimate the accuracy of the pre-trained model on the test data. However, obtaining labels for the test inputs is often impractical and expensive. This presents two challenging tasks: unsupervised accuracy estimation and error detection. In this paper, we propose a framework that addresses both tasks simultaneously. Our framework utilizes an ensemble of models to identify misclassified data points and improves the ensemble through self-training. Theoretical analysis demonstrates that our framework provides provable guarantees for accuracy estimation and error detection. We also present two implementations of the framework and achieve state-of-the-art results on 59 tasks, including significant improvements in accuracy estimation and error detection compared to existing methods. The work was conducted during an internship at Google, and the code is available at the provided GitHub link.