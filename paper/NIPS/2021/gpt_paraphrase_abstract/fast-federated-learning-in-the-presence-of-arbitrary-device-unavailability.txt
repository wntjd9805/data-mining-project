Federated Learning (FL) is a collaborative training method that allows multiple devices to train a shared model while protecting user privacy. However, FL faces challenges when devices drop out of the training process without control from the central server. This can greatly impact the convergence of popular FL algorithms like FedAvg. To address this challenge, we propose a new algorithm called Memory-augmented Impatient Federated Averaging (MIFA). MIFA effectively reduces latency caused by inactive devices and corrects gradient bias using stored updates from the devices. We prove that MIFA achieves optimal convergence rates for both strongly convex and non-convex smooth functions on non-i.i.d. data. Through a case study, we demonstrate the improvement of MIFA over baseline algorithms and validate our results using real-world dataset experiments.