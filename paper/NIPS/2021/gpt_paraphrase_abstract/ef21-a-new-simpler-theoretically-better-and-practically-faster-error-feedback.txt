Error feedback (EF) is a popular convergence stabilization mechanism used in distributed training of supervised machine learning models. It has been enhanced by the use of contractive communication compression mechanisms like Top-𝑘. Although EF has been widely used, its theoretical understanding has been limited until recently. Previous analyses either focused on the single node setting or made strong assumptions that may not hold in practice. To address these deficiencies, we propose a new EF mechanism called EF21, which outperforms EF in practice. Our analysis is based on standard assumptions and works in the distributed heterogeneous data setting, leading to better convergence rates. For smooth nonconvex problems, EF21 achieves a fast 𝒪(1/𝑇) convergence rate, surpassing the previous bound of 𝒪(1/𝑇 2/3) that relied on strong bounded gradients assumption. Additionally, for Polyak-Lojasiewicz functions, EF21 achieves a fast linear convergence rate without relying on unbiased compressors. Given the widespread use of EF, we believe that EF21 can significantly impact communication-efficient distributed learning.