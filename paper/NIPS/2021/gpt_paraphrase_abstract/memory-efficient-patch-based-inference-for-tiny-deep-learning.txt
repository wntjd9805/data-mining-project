Paraphrased abstract (1955 characters):Microcontroller units (MCUs) face challenges in implementing deep learning due to limited memory capacity. The main issue lies in the imbalanced distribution of memory usage in convolutional neural network (CNN) designs, where the initial blocks consume significantly more memory than the rest of the network. To address this problem, we propose a patch-by-patch inference scheduling technique that operates on a small spatial region of the feature map, thereby reducing peak memory usage. However, implementing this technique directly results in overlap of patches and additional computation overhead. To overcome this, we introduce receptive field redistribution, which shifts the receptive field and floating point operations (FLOPs) to the later stage, reducing computation overhead. Manual redistribution of the receptive field is challenging, so we automate the process using neural architecture search. This joint optimization of neural architecture and inference scheduling leads to the development of MCUNetV2. By employing patch-based inference, existing networks can reduce peak memory usage by 4-8 times. Co-designed with neural networks, MCUNetV2 achieves a record ImageNet accuracy of 71.8% on MCUs and attains over 90% accuracy on the visual wake words dataset with just 32kB SRAM. Furthermore, MCUNetV2 surpasses the state-of-the-art results by achieving 16.9% higher mean average precision (mAP) on Pascal VOC, enabling object detection on tiny devices. Our study effectively addresses the memory bottleneck in tinyML and opens doors for various vision applications beyond image classification.