We investigate acquisition functions for active learning (AL) in text classification. The Expected Loss Reduction (ELR) approach, enhanced with Mean Objective Cost of Uncertainty (MOCU), is commonly used to estimate the reduction in classification error. We modify the ELR framework to estimate the increase in proper scores such as log probability or negative mean square error, which we refer to as Bayesian Estimate of Mean Proper Scores (BEMPS2). We also demonstrate convergence results using techniques from MOCU. To facilitate experimentation with these new acquisition functions, we introduce a complementary batch AL algorithm that encourages diversity in the expected score changes for unlabeled data. To improve the performance of text classifiers, we combine ensembling and dynamic validation set construction on pretrained language models. Through extensive experiments, we evaluate the performance of different acquisition functions. The outcomes indicate that the use of BEMPS with mean square error and log probability produces robust acquisition functions that consistently outperform the other approaches tested.