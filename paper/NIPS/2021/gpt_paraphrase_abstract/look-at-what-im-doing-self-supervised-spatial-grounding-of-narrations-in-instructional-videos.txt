We propose a new task of locating interactions in videos based on narrations. Our approach involves learning to localize interactions through self-supervision on a large video dataset with transcribed narrations. To achieve this, we present a multilayer cross-modal attention network that optimizes a contrastive loss during training. We employ a divided strategy that computes attention within and between visual and natural language modalities, allowing effective training by contrasting the two modalities' representations. By self-training on the HowTo100M dataset and evaluating on the YouCook2 dataset, we demonstrate the superiority of our approach over alternative methods. Additionally, we use weak supervision on Flickr30K to ground phrases in images and show that stacking multiple attention layers, combined with a word-to-region loss, achieves state-of-the-art results.