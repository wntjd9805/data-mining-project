Bayesian treatment can address the issue of overconfidence in ReLU neural networks within the training data. However, when it comes to data far away from the training data, ReLU Bayesian neural networks can still underestimate uncertainty and become excessively confident. This occurs because the output variance of a Bayesian neural network is quadratic in the distance from the data region due to the finite number of features. On the other hand, Bayesian linear models with ReLU features, in the limit of infinite width, converge to a Gaussian process (GP) with a cubic growth in variance, preventing asymptotic overconfidence. Although this may seem primarily theoretical, we demonstrate that it can be practically beneficial for BNNs. By incorporating infinite ReLU features through the GP, we extend finite ReLU BNNs and create a model that is maximally uncertain in distant regions from the data while maintaining predictive power near the data. The resulting model approximates a full GP posterior, but it can be applied post-training to any pre-trained ReLU BNN at a low computational cost.