We present a new method for transferring knowledge from a teacher to a student using knowledge distillation. Unlike existing methods that focus on training student models given pretrained teachers, our approach focuses on learning teacher models that are more suitable for knowledge transfer. This means that when optimizing a teacher model, our algorithm also learns student branches to obtain student-friendly representations. Our method is primarily focused on training teacher models, and the subsequent knowledge distillation process is straightforward, allowing existing distillation methods to benefit from improved accuracy and convergence speed for different student models. Our algorithm achieves impressive accuracy in various knowledge distillation techniques, even when the teacher and student models have different architectures and no prior knowledge about student models is available during teacher network training.