We present Dynaboard, a framework that provides evaluation-as-a-service for hosting benchmarks and conducting comprehensive model comparisons in the field of natural language processing (NLP). Unlike conventional methods that rely on self-reported metrics or predictions on a single dataset, our platform directly evaluates NLP models. By submitting models for cloud-based evaluation, we overcome the challenges of reproducibility, accessibility, and backwards compatibility that often hinder benchmarking in NLP. This enables users to interact with uploaded models in real time, allowing them to assess the models' quality and collect additional metrics such as memory use, throughput, and robustness. These metrics, which are important to practitioners but traditionally absent from leaderboards, are aggregated into a novel utility-based measure called Dynascore. Users have the flexibility to customize the Dynascore according to their preferences, giving more or less weight to specific aspects of evaluation or datasets. As state-of-the-art NLP models push the boundaries of traditional benchmarks, Dynaboard provides a standardized solution for a more diverse and comprehensive evaluation of model quality.