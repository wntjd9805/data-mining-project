Training models with discrete latent variables is difficult due to the high variance of unbiased gradient estimators. Although low-variance reparameterization gradients of a continuous relaxation can be effective, this approach is not always feasible. Dong et al. (2020) and Yin et al. (2020) have introduced a reliable estimator that doesn't rely on continuous relaxations, but it is limited to binary random variables. In this study, we propose a new derivation of their estimator using importance sampling and statistical couplings, which we extend to the categorical setting. Inspired by the construction of a stick-breaking coupling, we introduce gradient estimators that reparameterize categorical variables as sequences of binary variables and employ Rao-Blackwellization. Through systematic experiments, we demonstrate that our proposed categorical gradient estimators achieve state-of-the-art performance. Previous estimators (Yin et al., 2019), even with additional Rao-Blackwellization, perform worse than a simpler REINFORCE with leave-one-out-baseline estimator (Kool et al., 2019).