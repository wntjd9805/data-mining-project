Distributionally robust optimization (DRO) is a commonly used method for developing models that can withstand changes in distribution. However, optimizing the objective function in DRO is more challenging than in standard optimization, and existing theoretical results often rely on strong assumptions about the loss function. This study aims to address this gap by examining DRO algorithms for smooth non-convex losses. By leveraging the specific structure of the DRO objective, the authors are able to provide non-asymptotic convergence guarantees, even when the objective function is non-convex, non-smooth, and has unbounded gradient noise. The authors demonstrate that a particular algorithm, the mini-batch normalized gradient descent with momentum, can find an approximately stationary point within a reasonable number of gradient computations. The study also explores the conditional value-at-risk (CVaR) setting and proposes a modified DRO objective that incorporates a smoothed version of CVaR. This modification allows for similar convergence guarantees. The theoretical findings are confirmed through various experiments, which demonstrate the proposed algorithm's consistent acceleration.