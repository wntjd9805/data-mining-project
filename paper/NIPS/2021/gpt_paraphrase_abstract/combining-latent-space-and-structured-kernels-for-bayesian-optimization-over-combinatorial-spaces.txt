This study focuses on optimizing combinatorial spaces, such as sequences, trees, and graphs, using costly black-box function evaluations. An efficient approach called Bayesian optimization (BO) selects inputs with high utility based on a surrogate model. To apply BO to combinatorial spaces, a recent method involves reducing it to BO in continuous spaces by learning a latent representation of structures using deep generative models (DGMs). However, the surrogate model in this method only utilizes information from the DGM, which may not accurately approximate the target black-box function. To address this limitation, this paper introduces a principled approach called LADDER. LADDER integrates structural information from decoded structures with the learned latent space representation by defining a novel structure-coupled kernel. Experimental results on real-world benchmarks demonstrate that LADDER significantly outperforms the BO over latent space method and performs comparably to state-of-the-art approaches.