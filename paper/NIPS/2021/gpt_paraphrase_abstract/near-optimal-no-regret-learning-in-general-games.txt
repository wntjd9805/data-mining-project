We demonstrate that Optimistic Hedge, a variant of multiplicative-weights-updates with recency bias, achieves a regret of poly(log T) in multi-player general-sum games. Specifically, when each player in the game utilizes Optimistic Hedge to update their strategies iteratively based on the play history, the total regret experienced by each player after T rounds is poly(log T). This bound is a significant improvement over the regret attained by standard no-regret learners (O(T 1/2)), no-regret learners with recency bias (O(T 1/4)), and OptimisticHedge in the case of two-player games (O(T 1/6)). Consequently, a corollary of our bound is that Optimistic Hedge converges to coarse correlated equilibrium in general games at a rate of approximately O(1/T).