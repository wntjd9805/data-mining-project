Data acquisition is a challenging task in machine learning applications, and it is expected that the performance of algorithms improves as more data points are added. However, it has been found that even standard algorithms that minimize empirical risk do not always exhibit this monotonic behavior. The phenomenon of non-monotonic risk and training instability, known as double descent, has been observed in popular deep learning approaches. These issues highlight the current lack of understanding in learning algorithms and generalization. Therefore, it is important to address this concern and provide a characterization of such behavior. In this study, we propose the first consistent and risk-monotonic algorithms for a general statistical learning scenario, addressing questions raised in previous research on avoiding non-monotonic risk curves. We also demonstrate that risk monotonicity does not necessarily result in worse excess risk rates. To achieve these results, we develop new empirical concentration inequalities that are applicable to non-independent and identically distributed processes such as Martingale Difference Sequences.