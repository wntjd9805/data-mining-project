Quality diversity (QD) is a field of research in stochastic optimization that focuses on generating a collection of solutions that not only maximize a given objective function but also exhibit diversity according to specific measure functions. However, existing QD algorithms treat these measure functions as "black boxes" and do not consider gradient information, even when the functions are differentiable. In this study, we introduce the differentiable quality diversity (DQD) problem, a subset of QD where both the objective and measure functions are first-order differentiable. We propose a new algorithm called MAP-Elites via a Gradient Arborescence (MEGA), which efficiently explores the joint range of the objective and measure functions by utilizing gradient information. Experimental results on two benchmark domains and in the latent space of a StyleGAN demonstrate that MEGA outperforms state-of-the-art QD algorithms. This highlights the potential of DQD for efficient quality diversity optimization in cases where gradient information is available. The source code for MEGA is publicly available at https://github.com/icaros-usc/dqd.