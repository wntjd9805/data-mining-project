We introduce Cross-lingual Open-Retrieval Answer Generation (CORA), a groundbreaking question answering (QA) model that can answer questions in multiple languages, even for languages without specific annotated data or knowledge sources. Our approach involves a novel dense passage retrieval algorithm, which is trained to retrieve relevant documents across languages for a given question. Combined with a multilingual autoregressive generation model, CORA can directly provide answers in the target language without relying on translation or in-language retrieval modules used in previous approaches. To address the lack of annotated data in low-resource languages, we propose an iterative training method that leverages annotated data from high-resource languages. Our experimental results demonstrate that CORA outperforms the previous state-of-the-art models on multilingual open QA benchmarks, including 26 languages, 9 of which were not seen during training. Furthermore, our analysis highlights the importance of cross-lingual retrieval and generation, particularly in low-resource settings. The code and trained model for CORA are publicly available at https://github.com/AkariAsai/CORA.