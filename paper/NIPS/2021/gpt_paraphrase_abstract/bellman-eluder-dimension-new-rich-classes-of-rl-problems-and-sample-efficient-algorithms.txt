This paper focuses on identifying the minimal structural assumptions that allow for efficient learning in Reinforcement Learning (RL). The authors introduce a new measure called Bellman Eluder (BE) dimension, which helps understand this question. They demonstrate that RL problems with low BE dimension are highly diverse and encompass many tractable RL problems, such as tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank problems, and low Eluder dimension problems. The paper also presents two algorithms, GOLF and OLIVE, designed to learn near-optimal policies for low BE dimension problems. The authors prove that both algorithms achieve this goal with a polynomial number of samples, regardless of the size of the state-action space. The regret and sample complexity results of these algorithms match or surpass the best existing results for various subclasses of low BE dimension problems.