We propose using neural architecture search to automate the process of designing more effective architectures for vision tasks. Our approach involves searching not only the architecture but also the search space, guided by the E-T Error computed using a weight-sharing supernet. We provide design guidelines and analysis of general vision transformers based on the search process, contributing to a better understanding of vision transformers. Our searched models, called S3, outperform recently proposed models like Swin, DeiT, and ViT on ImageNet. We also demonstrate the effectiveness of S3 on object detection, semantic segmentation, and visual question answering, showing its applicability to various vision and vision-language tasks. Code and models will be available for further use.