Adversarial examples have gained significant attention in research, as they are created by carefully manipulating data to deceive machine learning models. Recent studies have suggested that the presence of both robust and non-robust features is a key factor in the creation of adversarial examples. To explore this further, we propose a method that explicitly separates feature representations into robust and non-robust features using the Information Bottleneck technique. By introducing noise variation to each feature unit and analyzing the information flow, we can categorize feature units as either robust or non-robust based on the magnitude of noise variation. Our experiments demonstrate that these distilled features are closely related to adversarial predictions and contain semantic information that is perceptible to humans. Additionally, we present an attack mechanism that enhances the gradient of non-robust features, which directly impacts the model's prediction. We validate the effectiveness of this mechanism in compromising model robustness.