Graph Neural Networks (GNNs) have been successful in learning over graphs, but their local message-passing architecture limits their expressive power. To overcome this, we propose a new GNN architecture called the Neural Tree. Instead of performing message passing on the input graph, the Neural Tree operates on a tree-structured graph called the H-tree, which is constructed from the input graph. The H-tree organizes nodes hierarchically, with each node corresponding to a subgraph in the input graph and its parent representing a larger subgraph. We demonstrate that the Neural Tree can approximate any smooth probability distribution function over an undirected graph. The number of parameters required for an approximation is exponential in the treewidth of the input graph, but linear in its size. Additionally, we prove that the Neural Tree can approximate any continuous G-invariant/equivariant function through a nonlinear combination of probability distribution functions over G. We apply the Neural Tree to semi-supervised node classification in 3D scene graphs, where it outperforms traditional GNN architectures in terms of prediction accuracy. Furthermore, we demonstrate its suitability for citation networks with large treewidth using graph sub-sampling.