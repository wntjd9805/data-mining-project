End-to-end learnable models that combine discrete and continuous components have proven to be beneficial for supervised and reinforcement learning. These models are compositional, have better generalization capabilities, and are more interpretable. One popular approach involves integrating discrete probability distributions into neural networks using stochastic softmax tricks. However, previous research has primarily focused on computation graphs with a single discrete component on each execution path. In this study, we explore the behavior of more complex computation graphs with multiple sequential discrete components. We find that optimizing the parameters of these models is challenging due to small gradients and local minima. To overcome these challenges, we propose two strategies. First, we demonstrate that increasing the scale parameter of Gumbel noise perturbations during training improves the learning behavior. Second, we introduce dropout residual connections that are specifically designed for stochastic, discrete-continuous computation graphs. Through extensive experiments, we show that these strategies enable the training of complex discrete-continuous models that cannot be trained using standard stochastic softmax tricks. Furthermore, we demonstrate that these models outperform their continuous counterparts on various benchmark datasets in terms of generalization performance.