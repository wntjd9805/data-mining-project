This paper presents theoretical findings on estimation bounds and upper bounds for excess risk in support vector machines (SVM) with sparse multi-kernel representation. The study establishes convergence rates for multi-kernel SVM by analyzing a Lasso-type regularized learning scheme in composite multi-kernel spaces. The results demonstrate that the rates of convergence of classifiers depend on the complexity of the multi-kernels, sparsity, a Bernstein condition, and the sample size. These findings significantly improve upon previous results, even in additive or linear cases. Overall, the paper contributes unified theoretical results for multi-kernel SVMs and adds to the existing literature on high-dimensional nonparametric classification.