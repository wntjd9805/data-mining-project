Transformers, which have been highly successful in natural language processing, are now showing promise in computer vision. However, their self-attention operation has a quadratic complexity in time and memory, making them inefficient for long sequences and high-resolution images. To address this issue, we propose a transposed version of self-attention called cross-covariance attention (XCA), which operates across feature channels instead of tokens. XCA has linear complexity in the number of tokens, enabling efficient processing of high-resolution images. We introduce the cross-covariance image transformer (XCiT), which combines the accuracy of transformers with the scalability of convolutional architectures. We demonstrate the effectiveness and versatility of XCiT by achieving outstanding results on various vision benchmarks, including image classification, object detection, instance segmentation, and semantic segmentation.