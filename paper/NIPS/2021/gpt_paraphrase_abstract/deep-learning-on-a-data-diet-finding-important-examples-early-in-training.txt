The recent success of deep learning has been attributed to training overparametrized networks on large datasets. This raises the question of how much data is unnecessary, which examples are crucial for generalization, and how to identify them. This study reveals that simple scores, calculated by averaging over multiple weight initializations, can be used to identify important examples early in training in standard vision datasets. The proposed scores, Gradient Normed (GraNd) and Error L2-Norm (EL2N), are effective in pruning significant portions of training data without compromising test accuracy across various architectures and datasets. By using EL2N scores calculated a few epochs into training, it is possible to prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, these scores generalize across different architectures and hyper-parameter configurations. Unlike previous approaches that discard examples rarely forgotten during training, our scores utilize only local information at the beginning of training. Moreover, our scores can also detect noisy examples and provide insights into training dynamics by studying the impact of important examples on the loss surface and identifying stable subspaces in the model's data representation.