Multi-head attention is a powerful mechanism for sequence modeling as it allows each attention head to gather important information from different parts of an input sequence. However, when it comes to multilingual and multi-domain learning, where maximizing positive transfer and minimizing negative interference is crucial, non-selective attention sharing is not optimal. To address this, we propose attention sharing strategies that enable parameter sharing and specialization in multilingual and multi-domain sequence modeling. Our approach automatically learns shared and specialized attention heads for different languages and domains. Through evaluations in various tasks such as speech recognition and translation, our proposed strategies consistently improve the performance of sequence models using multi-head attention. In the case of speech-to-text translation, our approach achieves an average improvement of +2.0 BLEU across 13 language directions in a multilingual setting and +2.0 BLEU across 3 domains in a multi-domain setting.