This paper presents a new method for formulating mixed-integer models for ReLU neural networks. The method involves dividing node inputs into groups and creating convex hulls over these partitions using disjunctive programming. By adjusting the number of partitions, the method can balance between model size and tightness. The paper demonstrates that using fewer partitions can lead to smaller but still effective relaxations compared to existing formulations. The authors propose strategies for partitioning variables, supported by theoretical motivations and extensive computational experiments. The approach also complements existing algorithmic techniques such as optimization-based bound tightening.