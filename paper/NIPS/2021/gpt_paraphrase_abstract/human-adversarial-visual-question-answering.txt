Performance on the widely used VQA v2 dataset is improving but still falls short of human accuracy. To thoroughly test VQA models, we evaluate them against human-created adversarial examples. Participants interact with a top VQA model and attempt to find questions where the model's answers are incorrect. Our findings reveal that many state-of-the-art models struggle with these examples. We thoroughly analyze the collected adversarial examples and offer suggestions for future research. Our aim is for the Adversarial VQA benchmark to spur advancements in the field and push the boundaries of current capabilities.