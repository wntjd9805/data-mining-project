We examine the problem of estimating a discrete distribution in a d-dimensional space using limited communication. Previous research has focused on the overall error, but we analyze the local behavior of the estimation error and provide pointwise bounds based on the target distribution. We demonstrate that the error decreases with the square root of n for sufficiently large n, dependent on the half-norm of the target distribution rather than the dimensionality. We propose a two-round interactive estimation scheme that achieves this error rate uniformly across all distributions. This scheme can be extended to other loss functions. We establish a lower bound that matches the error of any interactive scheme, demonstrating that a certain amount of communication is necessary. Our upper and lower bounds indicate that log(k^(1/2)) bits of communication, where k is the Rényi entropy of order 2, are both sufficient and necessary for optimal performance. Thus, the Rényi entropy serves as the appropriate measure of the local communication complexity.