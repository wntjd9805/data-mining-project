We investigate the challenge of acquiring control policies for intricate tasks defined by logical specifications. Existing methods generate a reward function based on the given specification and employ a reinforcement learning algorithm to learn a policy that maximizes the expected reward. However, these methods struggle to handle complex tasks that involve high-level planning. To address this issue, we introduce a compositional learning approach called DIRL, which combines high-level planning and reinforcement learning. DIRL represents the specification as an abstract graph, where vertices and edges correspond to regions of the state space and simpler sub-tasks, respectively. By incorporating reinforcement learning, we train neural network policies for each edge (sub-task) using a Dijkstra-style planning algorithm to compute a high-level plan within the graph. We evaluate our approach on challenging control benchmarks with continuous state and action spaces and find that it outperforms state-of-the-art baselines.