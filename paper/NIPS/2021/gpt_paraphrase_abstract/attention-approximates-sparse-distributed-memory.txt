The effectiveness of Attention in deep learning is widely recognized, but its underlying mechanism is not well understood. In this study, we establish a connection between TransformerAttention and Kanerva's SparseDistributed Memory (SDM), a biologically realistic associative memory model. Our findings reveal that the conditions for this connection are met in pre-trained GPT2 Transformer models. This discovery has significant implications for understanding Attention, offering new insights from both computational and biological perspectives.