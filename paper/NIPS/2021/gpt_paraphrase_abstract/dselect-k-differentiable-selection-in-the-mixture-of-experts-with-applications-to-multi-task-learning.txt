The Mixture-of-Experts (MoE) architecture has shown promise in improving parameter sharing in multi-task learning (MTL) and scaling high-capacity neural networks. However, existing sparse gates used in MoE models, such as Top-k, are not smooth and can cause convergence and statistical performance issues during training. In this paper, we introduce DSelect-k, a continuously differentiable and sparse gate for MoE that overcomes these limitations. DSelect-k can be trained using first-order methods like stochastic gradient descent and allows explicit control over the number of experts selected. Our experiments on synthetic and real MTL datasets with up to 128 tasks demonstrate the effectiveness of DSelect-k, showing significant improvements in prediction and expert selection compared to popular MoE gates. Notably, in a large-scale recommender system, DSelect-k achieves a 22% improvement in predictive performance compared to Top-k. We provide an open-source implementation of DSelect-k.