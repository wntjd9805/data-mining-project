Recently, Natural Language Processing (NLP) has made significant progress using large pre-trained Transformer networks. However, these models have a large number of parameters, making it challenging to deploy them online due to latency constraints. To address this issue, hardware manufacturers have introduced dedicated hardware for NxM sparsity, which combines the flexibility of unstructured pruning with the efficiency of structured approaches. NxM sparsity allows for the selection of M parameters to retain from a group of N in the dense representation. However, standard sparse fine-tuning techniques often struggle to generalize well on downstream tasks with limited data resources. To overcome this problem, we propose a new learning framework called NxMTransformer, which induces semi-structured sparsity on pretrained language models for better performance in natural language understanding. We formulate NxM sparsity as a constrained optimization problem and use the Alternating Direction Method of Multipliers (ADMM) to optimize downstream tasks while considering hardware constraints. ADMM decomposes the sparsification problem into two sub-problems that can be solved sequentially, resulting in sparsified Transformer networks that achieve high accuracy and can be efficiently executed on new hardware. We apply our approach to various NLP tasks and achieve a 1.7-point higher accuracy in GLUE score compared to current best practices. We also analyze the impact of ADMM on fine-tuning accuracy and demonstrate how NxMTransformer further improves performance with knowledge distillation methods.