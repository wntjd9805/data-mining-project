Vision Transformers (ViT) have been successful in image recognition by dividing images into patches, each treated as a token. However, the number of tokens used affects prediction accuracy and computational cost. Typically, a fixed number of tokens like 16x16 or 14x14 is used to balance accuracy and speed. This paper argues that image characteristics should dictate the token number. Some "easy" images can be accurately predicted with fewer tokens (e.g., 4x4), while others require more fine-grained representation. To address this, a Dynamic Transformer is proposed to automatically determine the appropriate number of tokens for each image. Multiple Transformers with increasing token numbers are cascaded and activated adaptively during testing, stopping inference once a confident prediction is made. The Dynamic Transformer also incorporates efficient feature and relationship reuse mechanisms to reduce redundant computations. Experimental results on ImageNet, CIFAR-10, and CIFAR-100 demonstrate the superiority of the method in terms of computational efficiency and inference speed. Code and pre-trained models are available on GitHub.