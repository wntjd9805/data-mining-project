Temporal-difference learning with gradient correction (TDC) is a reinforcement learning algorithm used for policy evaluation. Originally designed for linear function approximation, it has since been expanded to include general smooth function approximation. While the asymptotic convergence of TDC in the on-policy setting with general smooth function approximation has been proven, the non-asymptotic convergence analysis remains unsolved. This is due to the complexities posed by the algorithm's non-linear and two-time-scale update structure, non-convex objective function, and the projection onto a time-varying tangent plane. In this study, we propose new techniques to tackle these challenges and provide a non-asymptotic error bound for the general off-policy setting using i.i.d. or Markovian samples. We demonstrate that the error bound converges as the number of samples (T) increases, with a factor of O(log T). Our approach can be applied to a wide range of value-based reinforcement learning algorithms that utilize general smooth function approximation, with a computational complexity of O(1/âˆšT).