Active inference is a theory that explains how the brain perceives and acts in the world by minimizing free energy. It suggests that the brain maintains an internal model of the world and strives to achieve preferred outcomes or goals. However, the applicability of active inference has been limited in complex environments. In this study, we propose a contrastive objective for active inference that reduces the computational burden in learning the agent's generative model and planning actions. Our method outperforms likelihood-based active inference in image-based tasks and is computationally more efficient and easier to train. We compare our approach to reinforcement learning agents that rely on human-designed reward functions and find that our method achieves similar performance. Additionally, we demonstrate that contrastive methods are more effective in environments with distractors and our method can generalize goals to variations in the background.