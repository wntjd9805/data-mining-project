Adversarial examples are often used to show the differences between computational models and biological sensory systems. Recent research suggests that incorporating biologically-inspired components into visual neural networks can enhance their ability to resist adversarial attacks. One particularly effective component is response stochasticity, similar to the variability seen in biological neurons. This study employs computational neuroscience techniques to investigate how adversarial perturbations affect the internal representations of standard, adversarially trained, and biologically-inspired stochastic networks. The analysis reveals distinct geometric features for each network type, indicating different mechanisms for achieving robust representations. The study extends these findings to the auditory domain, demonstrating that neural stochasticity also enhances the robustness of auditory models against adversarial perturbations. Geometric analysis shows that stochastic networks exhibit overlapping representations of clean and adversarially perturbed stimuli, suggesting a tradeoff between adversarial and clean performance mediated by competing geometric effects of stochasticity. These results provide insights into the strategies employed by adversarially trained and stochastic networks for robust perception and shed light on the potential benefits of stochasticity in both machine and biological computation.