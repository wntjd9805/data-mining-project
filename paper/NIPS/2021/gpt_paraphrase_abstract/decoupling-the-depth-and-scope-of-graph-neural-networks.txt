Current Graph Neural Networks (GNNs) face limitations in terms of scalability with regards to the size of the graph and model. When dealing with large graphs, increasing the model depth leads to exponential growth in the receptive field. However, there are two main challenges that arise when the model depth goes beyond a few layers: 1. reduced expressivity due to oversmoothing, and 2. increased computational cost due to neighborhood explosion. To address these challenges, we propose a design principle that separates the depth and scope of GNNs. This involves extracting a localized subgraph of limited size as the scope, and then applying a GNN of any depth to this subgraph. The extracted subgraph contains only critical neighbors, excluding irrelevant ones. The GNN then smooths the local neighborhood to create informative representations, rather than oversmoothing the entire global graph. Theoretical analysis shows that this decoupling approach improves the expressive power of GNNs in graph signal processing, function approximation, and topological learning. Empirical evaluation on seven graphs (up to 110M nodes) and six backbone GNN architectures demonstrates significant improvements in accuracy, while also achieving orders of magnitude reduction in computation and hardware cost.