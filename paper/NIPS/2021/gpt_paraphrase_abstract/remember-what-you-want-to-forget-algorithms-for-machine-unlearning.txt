We investigate the issue of removing data points from a learned model. Initially, the learner is given a dataset S that is randomly drawn from an unknown distribution and produces a model w that performs well on unseen samples from the same distribution. However, at some point in the future, the learner may be asked to remove a specific training data point z, which requires modifying the output model while maintaining the same level of accuracy. We conduct a comprehensive study on the generalization of machine unlearning, focusing on both computational and storage complexity. In the case of convex losses, we present an unlearning algorithm that can remove up to O(n/d1/4) samples, where d represents the problem dimension. In comparison, differentially private learning, which includes unlearning, only guarantees the deletion of O(n/d1/2) samples. This highlights a significant distinction between differential privacy and machine unlearning.