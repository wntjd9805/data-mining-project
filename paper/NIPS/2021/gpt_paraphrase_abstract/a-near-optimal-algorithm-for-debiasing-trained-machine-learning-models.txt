We introduce an efficient algorithm for reducing bias in trained models, such as deep neural networks (DNNs). Through our analysis, we demonstrate that our algorithm achieves near-optimal results by limiting its excess Bayes risk. We validate its effectiveness on widely-used benchmark datasets, comparing it to both traditional algorithms and state-of-the-art DNN architectures. Our algorithm surpasses previous post-processing methods and performs on par with in-processing. Moreover, it excels when applied to large-scale models, making it a practical choice for post-processing tasks.