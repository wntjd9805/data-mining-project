Modularity is a promising solution to the problem of continual learning, which involves modeling sequences of related tasks. By learning and combining modules to solve different tasks, we can address key challenges such as forgetting previous knowledge, transferring knowledge to new tasks, and managing model growth. Our approach, called local module composition (LMC), enables modular continual learning by providing each module with a local structural component that estimates its relevance to the input. We dynamically compose modules based on their local relevance scores, allowing us to be agnostic to task identities and adaptable to a wider range of continual learning scenarios compared to previous methods. LMC also tracks input statistics and adds new modules when outlier samples are detected. We conducted experiments that demonstrate the effectiveness of LMC on benchmark datasets, showing superior performance compared to existing methods without the need for task identities. Additionally, LMC's ability to locally learn structures enables it to interpolate to unseen tasks and compose modular networks trained on different task sequences without fine-tuning. However, LMC faces challenges when dealing with a large number of candidate modules, as local module selection becomes more difficult. Despite this limitation, LMC still performs well compared to an oracle-based baseline. The codebase for LMC is available for reference.