This abstract discusses the problem of limited generalization in deep reinforcement learning (RL) agents and proposes the use of data augmentation to address this issue. However, selecting the appropriate augmentation for each RL task typically requires expert knowledge. To overcome this, the paper introduces three methods for automatically finding effective augmentations for any RL task. These methods are combined with two new regularization terms for the policy and value function, ensuring the theoretical soundness of using data augmentation in actor-critic algorithms. The proposed approach achieves state-of-the-art performance on the Procgen benchmark and outperforms popular RL algorithms on DeepMind Control tasks with distractors. Furthermore, the agent learns policies and representations that are more robust to irrelevant environmental changes, such as the background. The code for this research is available at https://github.com/rraileanu/auto-drac.