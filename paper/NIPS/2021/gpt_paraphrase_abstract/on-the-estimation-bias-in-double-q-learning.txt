Double Q-learning, a method used to reduce overestimation bias in the Bellman operation, has been successful in improving learning performance and value prediction in deep Q-learning. However, previous research has shown that double Q-learning still suffers from underestimation bias, which can lead to multiple suboptimal fixed points under an approximate Bellman operator. To address this issue, we propose a simple yet effective solution that uses approximate dynamic programming to limit the target value. Through extensive evaluation in Atari benchmark tasks, we demonstrate that our approach significantly outperforms baseline algorithms.