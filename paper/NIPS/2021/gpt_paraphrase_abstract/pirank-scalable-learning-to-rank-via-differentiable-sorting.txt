Machine learning approaches for ranking face a challenge due to the disparity between the desired performance metrics and the surrogate loss functions used for optimization. This discrepancy arises because ranking metrics involve a non-differentiable sorting operation. Previous methods have suggested surrogates that are loosely related to ranking metrics but struggle to be effective in real-world applications. To address this, we propose PiRank, a novel class of differentiable surrogates for ranking. PiRank utilizes a continuous relaxation of the sorting operator, controlled by a temperature parameter, inspired by NeuralSort. In the limit of zero temperature, PiRank precisely recovers the desired metrics. Additionally, we introduce a divide-and-conquer extension that efficiently handles large list sizes. Theoretical analysis and practical experiments demonstrate the scalability and effectiveness of PiRank. Notably, we showcase the significance of larger list sizes during training and illustrate that PiRank outperforms comparable approaches on publicly available internet-scale learning-to-rank benchmarks.