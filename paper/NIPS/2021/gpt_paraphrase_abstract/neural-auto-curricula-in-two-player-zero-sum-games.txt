The abstract discusses the use of multi-agent reinforcement learning (MARL) algorithms in solving two-player zero-sum games. These algorithms typically create populations of agents and use manually developed game theoretical principles to determine the opponent mixture and find the best responses. However, this paper introduces a new framework called Neural Auto-Curricula (NAC) that automates the discovery of the learning update rule without human design. NAC utilizes meta-gradient descent to parameterize the opponent selection and best-response modules, and updates their parameters through interaction with the game engine. Surprisingly, the discovered MARL algorithms achieve competitive or better performance compared to state-of-the-art game solvers. NAC is also able to generalize from small games to large games, demonstrating superior performance compared to existing methods. This work opens up possibilities for discovering general MARL algorithms solely from data.