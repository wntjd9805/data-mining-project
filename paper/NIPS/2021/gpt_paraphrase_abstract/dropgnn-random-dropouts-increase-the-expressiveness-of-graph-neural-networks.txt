This study introduces Dropout Graph Neural Networks (DropGNNs) as a novel approach to address the limitations of traditional GNN frameworks. DropGNNs involve running multiple iterations of a GNN on the input graph, where a subset of nodes is randomly dropped in each iteration. The results from these iterations are then combined to obtain the final output. We demonstrate that DropGNNs are capable of distinguishing graph neighborhoods that cannot be separated by standard message passing GNNs. The paper establishes theoretical bounds on the number of iterations needed to ensure a reliable distribution of dropouts and presents various properties and limitations of DropGNNs. Our theoretical findings are experimentally validated, demonstrating the competitive performance of DropGNNs on established GNN benchmarks.