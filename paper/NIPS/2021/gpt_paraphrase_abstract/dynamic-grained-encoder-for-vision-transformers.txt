This paper presents a method for enhancing the efficiency of vision transformers, which are commonly used for language modeling. By introducing sparse queries, the method takes advantage of the spatial redundancy in natural images and reduces computational costs. The proposed Dynamic Grained Encoder assigns varying numbers of queries to different spatial regions, resulting in a detailed representation in important areas while maintaining high efficiency. The encoder is compatible with most vision transformer frameworks and achieves a significant reduction in computational complexity (40%-60%) without sacrificing performance in image classification. The effectiveness of the approach is further demonstrated through experiments in object detection and segmentation. The code for implementing this method is available at https://github.com/StevenGrove/vtpack.