Neural models struggle when faced with different distributions of train and test examples, unlike humans. Conventional sequence-to-sequence models fail to systematically generalize and interpret sentences representing new combinations of concepts seen during training. While traditional grammar formalisms excel in such situations, they are difficult to scale and maintain. Instead of relying on a grammar, we propose directly modeling segment-to-segment alignments as discrete structured latent variables within a neural seq2seq model. To efficiently explore alignments, we introduce a framework that prioritizes reordering before alignment, utilizing a neural reordering module that produces separable permutations. We present a dynamic programming algorithm that enables exact inference of separable permutations, allowing for end-to-end differentiable training. Our seq2seq model, trained in this manner, demonstrates improved systematic generalization compared to standard models in synthetic problems and NLP tasks such as semantic parsing and machine translation.