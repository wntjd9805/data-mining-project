A new approach called Multi-Dimensional Distributional DQN (MD3QN) is introduced to combine the benefits of distributional reinforcement learning (RL) and hybrid reward architectures (HRA). MD3QN extends distributional RL to model the joint return distribution from multiple reward sources, allowing it to capture both the randomness in returns for each reward source and the correlation between the randomness of different sources. The convergence of the joint distributional Bellman operator is proven, and an empirical algorithm is built using the Maximum Mean Discrepancy between the joint return distribution and its Bellman target. Experimental results demonstrate that MD3QN accurately models the joint return distribution in environments with correlated reward functions and outperforms previous RL methods in the control setting.