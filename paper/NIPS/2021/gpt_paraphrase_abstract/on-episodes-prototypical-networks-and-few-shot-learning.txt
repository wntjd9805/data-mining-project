Episodic learning, a widely used practice in few-shot learning, involves organizing training into episodes with small subsets for training and validation. This paper questions the necessity of episodic learning in nonparametric approaches like nearest neighbors within episodes. The study demonstrates that episodic learning imposes unnecessary constraints and leads to inefficient use of training batches. Through extensive experiments with Matching and Prototypical Networks, popular nonparametric methods, it is shown that their non-episodic counterparts are simpler, have fewer hyperparameters, and yield better performance in various few-shot classification datasets.