In statistical machine learning, non-convex learning problems are common and often have favorable statistical properties at critical points. While the convergence and estimation rates of algorithms for these problems are well-known, the uncertainty associated with the underlying training algorithm is not well-studied in the non-convex setting. To address this gap, this study establishes an asymptotic normality result for the widely used constant step size stochastic gradient descent (SGD) algorithm. By relating SGD to Markov Chains, it is shown that the average of SGD iterates converges to a normal distribution around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisfies a dissipativity property. The bias between this expected value and the critical points of the objective function is also characterized under different local regularity conditions. These results can be utilized to construct confidence intervals for non-convex problems trained using the SGD algorithm.