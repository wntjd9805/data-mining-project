Backdoor attacks pose a significant threat to deep neural networks (DNNs). Current defense methods can detect or erase backdoors, but it remains uncertain whether robust training methods can prevent the injection of backdoor triggers into the trained model in the first place. This paper introduces the concept of anti-backdoor learning, which aims to train clean models using backdoor-poisoned data. The learning process is framed as a dual-task of learning the clean and backdoor portions of the data. The weaknesses of backdoor attacks are identified as follows: 1) models learn backdoored data faster than clean data, with stronger attacks leading to faster convergence on backdoored data; 2) the backdoor task is tied to a specific class, known as the backdoor target class. Based on these weaknesses, a general learning scheme called Anti-Backdoor Learning (ABL) is proposed to prevent backdoor attacks during training. ABL employs a two-stage gradient ascent mechanism for standard training to isolate backdoor examples early on and break the correlation between backdoor examples and the target class later on. Extensive experiments on multiple benchmark datasets and 10 state-of-the-art attacks demonstrate that ABL-trained models on backdoor-poisoned data achieve the same performance as models trained on purely clean data. The code for ABL is available at https://github.com/bboylyg/ABL.