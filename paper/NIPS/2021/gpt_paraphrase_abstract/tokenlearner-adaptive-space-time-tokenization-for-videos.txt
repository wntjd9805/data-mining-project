This paper presents a new method for visual representation learning that utilizes adaptively learned tokens. This approach can be applied to both image and video understanding tasks. Instead of using pre-determined splitting strategies and processing numerous densely sampled patches for attention, our method learns to identify important tokens in visual data. This allows for the efficient identification of a few key visual tokens and enables the modeling of pairwise attention between these tokens over longer temporal periods in videos or the spatial content in image frames. Our experiments demonstrate excellent performance on various challenging benchmarks for video recognition tasks. Furthermore, our adaptive token approach achieves competitive results while significantly reducing computational costs. We achieve new state-of-the-art results on multiple video datasets, including Kinetics-400, Kinetics-600, Charades, and AViD. The code for our method is available at the provided GitHub link.