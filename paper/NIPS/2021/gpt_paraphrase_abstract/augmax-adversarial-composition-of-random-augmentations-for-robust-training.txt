Data augmentation is a simple yet effective method for enhancing the robustness of deep neural networks (DNNs). This is achieved through diversity and hardness, which are two complementary aspects of data augmentation. For instance, AugMix combines various augmentations to achieve broader coverage, while adversarial training generates challenging samples to identify weaknesses. To unify diversity and hardness, we propose AugMax, a data augmentation framework. AugMax randomly selects multiple augmentation operators and then learns an adversarial mixture of these operators. This leads to a significantly augmented input distribution, making model training more challenging. To address this, we introduce DuBIN, a disentangled normalization module that handles the feature heterogeneity resulting from AugMax. Experimental results demonstrate that AugMax-DuBIN significantly improves out-of-distribution robustness, outperforming previous approaches on various datasets. The code and pre-trained models can be found at https://github.com/VITA-Group/AugMax.