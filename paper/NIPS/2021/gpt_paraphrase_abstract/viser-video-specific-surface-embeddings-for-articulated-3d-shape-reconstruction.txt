ViSER is a novel approach for reconstructing articulated 3D shapes and dense 3D trajectories from monocular videos. Unlike previous methods that rely on multiple synchronized cameras, strong category-specific priors, or 2D keypoint supervision, ViSER only requires 2D object masks and two-frame optical flow as inputs. By estimating long-range correspondences in the video, ViSER matches 2D pixels to a deformable 3D mesh using video-specific surface embeddings. These embeddings capture the appearance features of each surface point and serve as a continuous set of keypoint descriptors, enabling dense long-range correspondences across pixels. The surface embeddings are implemented as coordinate-based MLPs, which are fine-tuned to each video using self-supervised losses. Experimental results demonstrate that ViSER outperforms previous methods on challenging videos of humans with loose clothing and unusual poses, as well as animal videos from DAVIS and YTVOS datasets.