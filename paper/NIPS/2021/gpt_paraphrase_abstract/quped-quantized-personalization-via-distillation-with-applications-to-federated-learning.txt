This study presents QuPeD, a quantized and personalized federated learning (FL) algorithm that addresses the challenges of data heterogeneity and diverse client resources. QuPeD enables collaborative training through knowledge distillation among clients with varying data and resources. It allows clients to learn compressed personalized models with different quantization parameters and dimensions/structures. The algorithm proposes a relaxed optimization problem for learning quantized models and optimizes quantization values. For compressed personalization, a framework is introduced with knowledge distillation loss for clients with different model requirements. An alternating proximal gradient update is developed to solve this problem, and its convergence properties are analyzed. Numerical validation shows that QuPeD outperforms other personalized FL methods in various heterogeneous settings compared to FedAvg and local training.