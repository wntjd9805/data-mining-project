We investigate the problem of learning mixtures of Gaussian distributions while adhering to the constraint of approximate differential privacy. Our research demonstrates that a sufficient number of samples for learning a mixture of k axis-aligned Gaussians in Rd, with a total variation distance α and satisfying (ε, δ)-differential privacy, is given by O(k2d log3/2(1/δ)/α2ε). This finding is significant as it is the first result for privately learning mixtures of unbounded axis-aligned (or unbounded univariate) Gaussians. Additionally, if the covariance matrices of the Gaussians are identity matrices, we prove that O(kd/α2 + kd log(1/δ)/αε) samples are necessary. Our approach involves the development of a novel technique for privately learning mixture distributions. We introduce the concept of list-decodable distributions, where a class of distributions F is considered list-decodable if an algorithm can output a list of distributions, one of which approximates a heavily corrupted sample from f ∈ F. By establishing that axis-aligned Gaussian distributions are privately list-decodable, we conclude that mixtures of such distributions can be privately learned.