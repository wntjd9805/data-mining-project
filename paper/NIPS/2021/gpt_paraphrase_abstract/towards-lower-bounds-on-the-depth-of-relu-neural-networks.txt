We aim to enhance our understanding of the types of functions that can be represented by a neural network with ReLU activations and a given architecture. By utilizing techniques from mixed-integer optimization, polyhedral theory, and tropical geometry, we offer a mathematical perspective that challenges the universal approximation theorems, which propose that a single hidden layer is sufficient for learning tasks. Specifically, we investigate whether the inclusion of additional layers (regardless of size) strictly expands the class of functions that can be accurately represented. This inquiry holds significance for algorithmic and statistical aspects as it sheds light on the range of functions that can be represented by neural hypothesis classes. Surprisingly, this question has not yet been explored in the existing neural network literature. Additionally, we present upper bounds on the network sizes necessary to represent functions within these neural hypothesis classes.