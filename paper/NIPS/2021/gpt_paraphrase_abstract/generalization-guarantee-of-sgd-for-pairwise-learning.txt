There is a growing interest in the study of pairwise learning, which encompasses important machine learning tasks such as metric learning, AUC maximization, and ranking. While stochastic gradient descent (SGD) is an efficient method, there is a lack of research on its generalization behavior for pairwise learning. This paper presents a systematic study on the generalization analysis of SGD for pairwise learning in order to understand the balance between generalization and optimization. A novel high-probability generalization bound is developed for uniformly-stable algorithms, which incorporates variance information for improved generalization. This leads to the establishment of the first nonsmooth learning algorithm that achieves nearly optimal high-probability and dimension-independent excess risk bounds with O(n) gradient computations. Both convex and nonconvex pairwise learning problems are considered. The stability analysis for convex problems demonstrates how interpolation can aid in generalization. Furthermore, a uniform convergence of gradients is established and applied to derive the first excess risk bounds on population gradients for nonconvex pairwise learning. Finally, the stability analysis is extended to pairwise learning with gradient-dominated problems.