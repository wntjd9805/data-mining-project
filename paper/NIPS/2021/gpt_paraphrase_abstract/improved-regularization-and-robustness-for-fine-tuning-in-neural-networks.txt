Fine-tuning is a commonly used algorithm for transfer learning, but it can lead to overfitting and memorization of training labels when the pre-trained model has a much larger capacity than the target dataset. This raises the question of how to regularize fine-tuning to ensure robustness to noise. To address this, we analyze the generalization properties of fine-tuning and propose a PAC-Bayes generalization bound that depends on the distance traveled in each layer during fine-tuning and the noise stability of the model. We introduce regularized self-labeling, which combines layer-wise regularization to limit the distance traveled in each layer and self label-correction and label-reweighting to correct mislabeled data points and give less weight to less confident data points. We experimentally validate our approach on various image and text datasets using multiple pre-trained model architectures. Our approach improves upon baseline methods for image classification tasks and a few-shot classification task, and performs significantly better in the presence of noisy labels.