We present a novel method for discretizing the mirror-Langevin diffusion and offer a concise proof of its convergence. Our approach leverages principles from convex optimization, such as relative convexity/smoothness and self-concordance, alongside a new finding in optimal transport that extends the displacement convexity of the entropy. Unlike previous studies, our method imposes weaker assumptions on the mirror map and the target distribution, and exhibits diminishing bias with smaller step sizes. Notably, when sampling from a log-concave distribution on a bounded set, our theoretical guarantees outperform existing results.