Current research on deep learning often uses metrics that condense all data-related information into a few numerical values. This study takes a different approach by focusing on individual examples. The researchers introduce a measure called the (effective) prediction depth, which quantifies the computational difficulty of making predictions for specific inputs. Through extensive analysis, they discover straightforward yet surprising connections between the prediction depth, the model's uncertainty, confidence, accuracy, and learning speed for each data point. The study also classifies challenging examples into three interpretable groups and demonstrates how these groups are processed differently within deep models. This understanding enables the researchers to enhance prediction accuracy. The insights gained from this investigation provide a coherent explanation for various phenomena previously reported in the literature, such as the generalization of early layers and the memorization of later layers, the faster convergence of early layers, and the tendency of networks to learn easy data and simple functions first.