Advancements in generative radiance fields have expanded the possibilities of 3D-aware image synthesis. To enhance the realism of 3D objects from different perspectives, these methods incorporate a multi-view constraint as regularization to learn valid 3D radiance fields from 2D images. However, the accuracy of capturing 3D shapes is often hindered by the ambiguity between shape and color, limiting their usefulness in downstream tasks. To address this issue, we propose a new shading-guided generative implicit model that significantly improves the representation of shapes. Our key insight is that an accurate 3D shape should also produce realistic renderings under various lighting conditions. We achieve this by explicitly modeling illumination and employing shading with different lighting setups. Gradients are obtained by passing the synthesized images through a discriminator. To reduce the computational burden caused by calculating surface normals, we develop an efficient volume rendering strategy through surface tracking, resulting in a 24% reduction in training time and a 48% reduction in inference time. Our experiments on multiple datasets demonstrate that our approach achieves photorealistic 3D-aware image synthesis while accurately capturing the underlying 3D shapes. We also showcase improved performance in 3D shape reconstruction compared to existing methods and demonstrate the applicability of our approach in image relighting. The code for our approach will be made available at https://github.com/XingangPan/ShadeGAN.