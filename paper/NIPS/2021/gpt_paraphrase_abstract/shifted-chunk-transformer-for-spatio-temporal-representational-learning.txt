Spatio-temporal representational learning is widely used in various fields, such as action recognition and video object segmentation. Previous approaches have mainly used ConvNets or sequential models like LSTM to learn intra-frame and inter-frame features. However, Transformer models, which have been successful in natural language processing and image classification, are not efficient for spatio-temporal learning as they require excessive memory and computation for fine-grained feature extraction. To address this issue and improve spatio-temporal learning, we propose a shifted chunk Transformer with pure self-attention blocks. This design allows the model to learn hierarchical spatio-temporal features from local patches to global video clips and effectively capture complex inter-frame variations. Additionally, we introduce a clip encoder based on Transformer to model long-term temporal dependencies. Through extensive experiments, we validate the effectiveness of each component and hyper-parameter in our shifted chunk Transformer, which outperforms previous state-of-the-art methods on datasets such as Kinetics-400, Kinetics-600, UCF101, and HMDB51.