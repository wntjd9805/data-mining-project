We examine the concept of online learning for optimal time allocation. In this scenario, a task is proposed to an agent in a sequential manner, following a Poisson process. The agent has the option to accept or reject each task. If accepted, the agent is occupied for the task's duration and receives a reward based on the duration. If rejected, the agent remains on hold until a new task is proposed. We analyze the agent's regret, first when knowing the reward function but not the task duration distribution, and then when knowing neither the reward function nor the task duration distribution. This situation is similar to contextual bandits, but differs in that the normalized reward linked to a context depends on the entire distribution of contexts.