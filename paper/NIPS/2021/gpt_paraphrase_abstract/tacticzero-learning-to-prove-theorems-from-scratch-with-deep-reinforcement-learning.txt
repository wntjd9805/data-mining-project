We present a new method for interactive theorem proving (ITP) that utilizes deep reinforcement learning. Our approach involves training a model to learn proof search strategies and predict tactics and arguments. By modeling the ITP process as a Markov decision process (MDP), each state represents a set of potential derivation paths. This allows us to incorporate a search mechanism that efficiently discards predicted dead-end derivations and starts anew from promising alternatives. The framework is implemented in the HOL4 theorem prover, and experimental results demonstrate that it outperforms existing automated theorem provers (such as hammers) when tested on unseen problems. Additionally, we conduct ablation studies to examine the significance of key components of the framework.