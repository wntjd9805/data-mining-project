In recent years, there has been significant interest in implicit representations of geometry and radiance, particularly through Neural Radiance Fields (NeRF). These models use a volumetric representation to capture scene structure, including translucent objects and atmospheric effects. However, since most real-world scenes consist of well-defined surfaces, we propose a surface-based alternative called Neural Reflectance Surfaces (NeRS). NeRS learns a neural shape representation of closed surfaces that are topologically equivalent to spheres, ensuring accurate reconstructions. Additionally, surface parameterizations enable NeRS to learn bidirectional surface reflectance functions (BRDFs), which separate view-dependent appearance into environmental illumination, diffuse color, and specular properties. To evaluate the effectiveness of NeRS, we create a dataset of multi-view images from online marketplaces. These "in-the-wild" images present challenges such as limited views and imprecise camera estimates. Our results show that surface-based neural reconstructions outperform volumetric neural rendering approaches when learning from such data. We envision NeRS as a starting point for the development of comprehensive libraries of real-world shape, materials, and illumination. More information, including code and video visualizations, can be found on the project page at jasonyzhang.com/ners. Our approach is demonstrated through 3D view synthesis on various objects, showcasing scalability and performance. This research paper will be presented at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).