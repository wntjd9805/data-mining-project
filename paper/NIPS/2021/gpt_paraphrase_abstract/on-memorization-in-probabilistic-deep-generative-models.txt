Recent advancements in deep generative models have yielded impressive results in various application domains. Due to concerns about potential memorization of input data by deep learning models, there has been a growing interest in comprehending the origins of memorization. This research expands on a recently introduced measure of memorization in supervised learning (Feldman, 2019) and applies it to the unsupervised density estimation problem, while also enhancing its computational efficiency. Additionally, we present a study illustrating how memorization can manifest in probabilistic deep generative models like variational autoencoders. This study highlights that the type of memorization these models are susceptible to fundamentally differs from issues like mode collapse and overfitting. Moreover, we demonstrate that the proposed memorization score captures a phenomenon not accounted for by commonly-used nearest neighbor tests. Finally, we discuss various strategies to mitigate memorization in practical scenarios. Overall, this work offers a framework for comprehending troublesome memorization in probabilistic generative models.