Understanding how deep neural networks generalize is a crucial task in deep learning. Despite progress, there is often a disparity between theoretical error bounds and empirical observations. This study introduces margin-based generalization bounds that utilize optimal transport costs between independent random subsets from the training distribution. These costs represent a generalized variance, capturing the structural properties of the learned feature space. The proposed bounds accurately estimate the generalization error on large datasets, considering training data and network parameters. Theoretical analysis confirms the importance of feature concentration and separation in generalization, aligning with empirical findings. The code can be found at https://github.com/chingyaoc/kV-Margin.