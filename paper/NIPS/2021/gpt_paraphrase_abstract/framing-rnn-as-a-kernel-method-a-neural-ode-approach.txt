We demonstrate that a recurrent neural network (RNN) can be seen as a linear function of the input sequence's signature, given certain conditions. This connection enables us to view the RNN as a kernel method within a reproducing kernel Hilbert space, leading to theoretical assurances regarding generalization and stability for various types of recurrent networks. We validate our findings using simulated datasets.