A new generative modeling technique called diffusion normalizing flow is introduced in this study. It is based on stochastic differential equations (SDEs) and consists of two neural SDEs. The forward SDE adds noise to the data gradually to transform it into Gaussian random noise, while the backward SDE removes the noise gradually to sample from the data distribution. By training the two neural SDEs together to minimize a common cost function, the backward SDE converges to a diffusion process that starts with a Gaussian distribution and ends with the desired data distribution. This method combines normalizing flow and diffusion probabilistic models and outperforms them in certain aspects. It is capable of learning distributions with sharp boundaries, unlike normalizing flow, and requires fewer discretization steps, resulting in better sampling efficiency compared to diffusion probabilistic models. The algorithm demonstrates competitive performance in both high-dimensional data density estimation and image generation tasks.