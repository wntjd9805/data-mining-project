We introduce a novel method to measure the behavioral distance in a Markov decision process (MDP) state space. This distance can effectively shape the learned representations of deep reinforcement learning agents. Existing approaches to state similarity are challenging to learn on a large scale due to computational constraints and the absence of sample-based algorithms. However, our newly proposed distance overcomes these challenges. Alongside the value function, learning this distance produces structured and informative representations, as supported by theoretical analysis and empirical evidence on the Arcade Learning Environment benchmark.