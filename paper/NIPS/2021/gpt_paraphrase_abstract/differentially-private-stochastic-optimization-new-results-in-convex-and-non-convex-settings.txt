We examine differentially private stochastic optimization in both convex and non-convex scenarios. In the case of convex problems, we focus on non-smooth generalized linear losses (GLLs). Our algorithm achieves optimal excess population risk in near-linear time for the ℓ2 setting, outperforming existing differentially private algorithms for general convex losses. Additionally, our algorithm for the ℓ1 setting achieves nearly-optimal excess population risk and surpasses the lower bound for general non-smooth convex losses. In the non-convex setting, we propose multiple new algorithms to approximate stationary points of the population risk. For the ℓ1-case with smooth losses and a polyhedral constraint, we introduce a dimension-independent rate in linear time. Similarly, for the constrained ℓ2-case with smooth losses, we develop a linear-time algorithm with a favorable rate. Finally, in the ℓ2-case, we present the first method for non-smooth weakly convex stochastic optimization with a rate that matches the best existing non-private algorithm when the dimension is proportional to the square root of the sample size. We also extend all our findings to the ℓp setting, where 1 < p ≤ 2, with only a polylogarithmic overhead in the rates.