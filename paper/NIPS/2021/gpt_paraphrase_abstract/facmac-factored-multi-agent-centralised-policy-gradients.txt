We present FACMAC, a novel approach for cooperative multi-agent reinforcement learning in both discrete and continuous action spaces. Similar to MADDPG, our method utilizes deep deterministic policy gradients for policy learning. However, FACMAC employs a centralised but factored critic, inspired by the QMIX algorithm, to combine per-agent utilities into a joint action-value function. Unlike QMIX, FACMAC allows for nonmonotonic factorisation, which enhances its representational capacity and enables solving tasks that are challenging for monolithic or monotonically factored critics. Furthermore, FACMAC employs a centralised policy gradient estimator that optimises over the entire joint action space, rather than separately optimising each agent's action space as in MADDPG. This facilitates more coordinated policy changes and fully leverages the benefits of a centralised critic. We evaluate FACMAC on various multi-agent environments, including a new multi-agent MuJoCo benchmark and a set of challenging StarCraft II micromanagement tasks. Empirical results demonstrate that FACMAC outperforms MADDPG and other baseline methods across all three domains.