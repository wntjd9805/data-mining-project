We examine the HSIC bottleneck as a method to improve the robustness of deep neural network classifiers against adversarial attacks. By adding regularization terms to each intermediate layer, we ensure that the latent representations maintain useful information for output prediction while eliminating redundant information. The HSIC bottleneck is proven to reduce the classiÔ¨Åer's sensitivity to adversarial examples, enhancing its robustness. Through experiments on various datasets and architectures, we demonstrate that incorporating an HSIC bottleneck regularizer achieves competitive accuracy and improves adversarial robustness, even without adversarial examples during training. Our code and adversarially robust models are publicly available.