We present a new method for accelerating the training of deep neural networks by utilizing sample-based approximation on tensor operations such as matrix multiplications and convolutions. We introduce and examine various sampling techniques, analyzing their theoretical properties and demonstrating that they offer the same convergence guarantees as traditional SGD training. We apply these approximate tensor operations to both single and multi-node training of MLP and CNN networks on popular datasets like MNIST, CIFAR-10, and ImageNet. Our experiments show that this approach can reduce computations and communication by up to 66% and achieve training times up to 1.37x faster, all while maintaining negligible or no impact on the final test accuracy.