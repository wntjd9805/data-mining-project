Deep neural networks (DNNs) have achieved impressive results on various tasks but lack intelligibility, making them unsuitable for high stakes decision-making areas like healthcare. To address this, we propose Neural Additive Models (NAMs) that combine the expressivity of DNNs with the interpretability of generalized additive models. NAMs learn a linear combination of neural networks, with each network focusing on a single input feature. These networks are trained together and can capture complex relationships between input and output. Our experiments demonstrate that NAMs outperform logistic regression and shallow decision trees in accuracy, while performing similarly to state-of-the-art generalized additive models. NAMs offer flexibility as they are based on neural nets instead of boosted trees. We showcase the versatility of NAMs through multitask learning on synthetic data and COMPAS recidivism data. Additionally, we show that the differentiability of NAMs enables them to train more complex interpretable models for COVID-19. The source code can be accessed at neural-additive-models.github.io.