Previous methods in offline reinforcement learning (RL) have typically used an iterative actor-critic approach with off-policy evaluation. However, this paper introduces a new approach that achieves surprisingly good results by simply performing one step of constrained/regularized policy improvement using an on-policy Q estimate of the behavior policy. This one-step algorithm outperforms iterative algorithms on a significant portion of the D4RL benchmark. Moreover, the one-step baseline is simpler and more robust to hyperparameters compared to previously proposed iterative algorithms. The subpar performance of iterative approaches is attributed to the high variance caused by off-policy evaluation and the repeated optimization of policies based on those estimates. Additionally, it is hypothesized that the strong performance of the one-step algorithm is attributed to the favorable structure in the environment and behavior policy.