Adapting large pretrained language models for downstream tasks by fine-tuning is the standard approach to achieve high performance in NLP benchmarks. However, fine-tuning all parameters of models with millions or billions of parameters is inefficient, unstable in low-resource scenarios, and requires storing separate copies of the model for each task. Recent research has developed more parameter-efficient fine-tuning methods, but these still require a relatively large number of parameters or perform worse than standard fine-tuning. In this study, we propose COMPACTER, a method that offers a better balance between task performance and trainable parameters compared to previous approaches. COMPACTER achieves this by incorporating ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers. Specifically, COMPACTER inserts task-specific weight matrices into a pretrained model's weights, which are efficiently computed as a sum of Kronecker products between shared "slow" weights and "fast" rank-one matrices defined per COMPACTER layer. By training only 0.047% of the pretrained model's parameters, COMPACTER performs similarly to standard fine-tuning on GLUE and outperforms it on SuperGLUE and low-resource settings. Our code is publicly available at https://github.com/rabeehk/compacter.