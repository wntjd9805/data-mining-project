The abstract discusses the treatment of the time dimension in video transformers, highlighting the need to model temporal correspondences in dynamic scenes. The authors propose a new block called trajectory attention that captures motion paths to improve learning. They also introduce a method to address the computational and memory limitations of large inputs. These ideas are applied to video action recognition, resulting in state-of-the-art performance on multiple datasets. The code and models for their proposed approach can be found at the provided GitHub link.