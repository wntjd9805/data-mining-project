Certifying the robustness of deep neural networks is important for safety-critical applications. Current training algorithms can certify robustness by computing a global bound on the network's Lipschitz constant. However, this bound is often too loose, resulting in over-regularization and decreased natural accuracy. To address this, we propose an efficient and trainable local Lipschitz upper bound that considers the interactions between activation functions and weight matrices. By eliminating rows and columns where the activation function is constant in the neighborhood of each data point, we achieve a provably tighter bound than the global Lipschitz constant. Our method can be easily integrated into existing certifiable training algorithms. Additionally, we suggest clipping activation functions with a learnable upper threshold and a sparsity loss to further improve the local Lipschitz bound. Experimental results on various datasets and network architectures demonstrate that our method consistently outperforms state-of-the-art approaches in terms of both clean and certified accuracy.