The problem of efficient exploration in multi-armed bandits is addressed in this paper. The authors introduce the metadata-based multi-task bandit problem, where an agent must solve multiple related bandit tasks and can use task-specific features (metadata) to share knowledge across tasks. They propose a framework using Bayesian hierarchical models to capture task relations and design a Thompson sampling algorithm to learn task relations, share information, and minimize cumulative regrets. The benefits of information sharing are demonstrated through analysis of Gaussian bandits, and extensive experiments support the proposed method.