A deep neural network (DNN) has achieved significant success in various machine learning tasks due to its high expressive power. However, it often exhibits bias towards undesirable features that are irrelevant to the target task and even imperceptible to humans, leading to poor generalization. To address this issue, researchers have proposed using out-of-distribution (OOD) examples that contain many undesirable features as a means to debias the predictions. Recent studies have shown that applying softmax-level calibration to OOD examples can effectively remove the impact of undesirable features in the last fully-connected layer of a classifier. However, this approach is limited to classification tasks and its effect on the DNN feature extractor is not well-explored. In this paper, we introduce TAUFE, a novel regularizer that deactivates undesirable features in the feature extraction layer using OOD examples, eliminating the need for task-specific softmax layers. We evaluate TAUFE on three tasks (classification, regression, and a combination of both) using CIFAR-10, CIFAR-100, ImageNet, CUB200, and CAR datasets. Our results demonstrate that TAUFE consistently outperforms state-of-the-art methods and baseline models without regularization, highlighting its task-agnostic nature.