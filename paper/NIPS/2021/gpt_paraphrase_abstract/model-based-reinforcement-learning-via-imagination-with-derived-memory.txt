Model-based reinforcement learning aims to enhance the efficiency of policy learning by modeling the environment's dynamics. The latent dynamics model has recently been developed to facilitate quick planning in a condensed space, summarizing an agent's high-dimensional experiences similar to human memory functions. Employing the latent model for policy learning through imagination exhibits promising potential for addressing complex tasks. However, relying solely on memories from actual experiences during the imagination process might restrict its benefits. Taking inspiration from neuroscientists' memory prosthesis concept, we introduce a novel framework for model-based reinforcement learning called Imagining with Derived Memory (IDM). IDM enables the agent to acquire policies from diverse and enriched imagination, incorporating a weighting mechanism based on prediction reliability. This approach enhances both sample efficiency and policy robustness. Experimental results on various visually demanding control tasks using the DMControl benchmark illustrate that IDM surpasses previous state-of-the-art techniques in terms of policy robustness while further improving the sample efficiency of the model-based approach.