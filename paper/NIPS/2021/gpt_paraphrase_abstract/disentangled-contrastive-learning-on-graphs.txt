Self-supervised learning for graph neural networks (GNNs) has gained significant attention due to their success in representing graph-structured data. However, real-world graphs are formed through complex interactions of latent factors. Existing self-supervised methods for GNNs overlook the entanglement of these factors, resulting in suboptimal representations that are hard to interpret and use in downstream tasks. Disentangling graph representations using self-supervised learning is a challenging problem that has been mostly ignored in the literature. In this study, we introduce the Disentangled Graph Contrastive Learning (DGCL) method, which learns disentangled graph-level representations. We identify the latent factors of the input graph and derive factorized representations, each capturing a specific aspect related to a latent factor. To ensure independence and expressiveness of these representations, we propose a novel factor-wise discrimination objective using contrastive learning. Extensive experiments on synthetic and real-world datasets demonstrate the superiority of our method compared to state-of-the-art baselines.