The poor predictive performance of large variational Bayesian Neural Networks (BNNs) and their sensitivity to prior weights have hindered recent interest in learning these networks. Current practice typically uses fixed or heuristically tuned prior parameters. This paper proposes a new approach that treats prior parameters in a distributional manner, resulting in tighter Evidence Lower Bounds (ELBOs) for variational inference (VI) in BNNs. Experimental results demonstrate that this new approach significantly improves the performance of Gaussian mean-field VI in BNNs across various datasets, even in deep models. Additionally, the tighter ELBOs can serve as effective optimization targets for learning the hyperparameters of hierarchical priors.