Policy gradient (PG) methods are widely used in reinforcement learning (RL), often with a baseline to reduce gradient estimate variance. However, in multi-agent RL (MARL), the effectiveness of multi-agent PG (MAPG) methods decreases as the variance rapidly increases with the number of agents. This paper provides a thorough analysis of MAPG methods, quantifying the impact of the number of agents and their explorations on variance. Based on this analysis, the optimal baseline (OB) is derived to achieve minimal variance. Existing MARL algorithms, such as vanilla MAPG and COMA, are found to have excess variance compared to the OB. Additionally, a surrogate version of OB is proposed for use with deep neural networks, seamlessly integrating with any existing PG methods in MARL. Through experiments on Multi-Agent MuJoCo and StarCraft challenges, the OB technique is shown to effectively stabilize training and significantly improve the performance of multi-agent PPO and COMA algorithms. The code for the OB technique is available on GitHub at https://github.com/morning9393/Optimal-Baseline-for-Multi-agent-Policy-Gradients.