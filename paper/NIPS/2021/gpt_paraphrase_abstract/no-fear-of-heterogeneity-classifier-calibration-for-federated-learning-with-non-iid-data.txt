Training classification models in real-world federated systems is challenging due to the presence of non-IID data. Existing approaches address this challenge by regularizing local optimization or improving model aggregation at the server. Some methods also use public datasets or synthesized samples to supplement training for under-represented classes or introduce personalization. However, these approaches lack a comprehensive understanding of how data heterogeneity affects different layers of a deep classification model. This paper fills this gap by conducting experimental analysis on the learned representations of different layers. Surprisingly, the classifier layer exhibits a greater bias compared to other layers, and the classification performance can be significantly enhanced by post-calibrating the classifier after federated training. Motivated by these findings, we propose a new and simple algorithm called Classifier Calibration with Virtual Representations (CCVR) which adjusts the classifier using virtual representations sampled from an approximated Gaussian mixture model. Experimental results demonstrate that CCVR achieves state-of-the-art performance on popular federated learning benchmarks such as CIFAR-10, CIFAR-100, and CINIC-10. We believe that our straightforward yet effective method can provide insights for future research on federated learning with non-IID data.