Today's reinforcement learning (RL) algorithms face challenges such as robustness, generalization, transfer, and computational efficiency, which are closely linked to compression. While prior research has highlighted the benefits of minimizing information in supervised learning, standard RL algorithms lack explicit compression mechanisms. However, RL is unique in that it allows agents to use past information to avoid future observations and optimize behavior for states requiring fewer bits. Leveraging these properties, we propose a method called RPC that combines ideas from information bottlenecks, model-based RL, and bits-back coding to develop a simple and theoretically justified algorithm for learning simple policies. Our approach simultaneously optimizes a latent-space model and policy to ensure self-consistency, where the policy avoids states where the model is inaccurate. Our method achieves significantly tighter compression compared to previous techniques, leading to up to 5Ã— higher rewards than a standard information bottleneck. Additionally, the policies learned by our method demonstrate robustness and generalization capabilities across new tasks.