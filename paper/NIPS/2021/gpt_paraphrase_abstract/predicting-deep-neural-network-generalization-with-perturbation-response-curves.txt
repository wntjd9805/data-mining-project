The field of Deep Learning has shown impressive human-like performance in prediction tasks. However, the recent Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020 competition highlights the need for more robust and efficient measures of network generalization. To address this, we propose a new framework for evaluating the generalization capabilities of trained networks. We introduce perturbation response (PR) curves that capture the accuracy change of a network when subjected to varying levels of training sample perturbation. From these curves, we derive novel statistics, namely the Gi-score and Pal-score, inspired by the Gini coefficient and Palma ratio, which accurately predict generalization gaps. By applying our framework to intra and inter-class sample mixup, we achieve better predictive scores than the current state-of-the-art measures in the PGDL competition. Additionally, our framework and statistics can assess the extent to which a trained network is invariant to specific parametric input transformations like rotation or translation. Therefore, these generalization gap prediction statistics also serve as a useful tool for selecting optimal network architectures and hyperparameters that are unaffected by certain perturbations.