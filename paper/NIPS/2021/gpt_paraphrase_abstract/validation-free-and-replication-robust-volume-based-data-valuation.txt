Data valuation is a complex issue in real-world scenarios like collaborative machine learning, federated learning, trusted data sharing, and data marketplaces. The value of data is often determined by the performance of a model trained on the data, which creates a strong connection between data valuation and validation. However, obtaining a validation set can be difficult, and reaching an agreement on its selection can be challenging for data providers. Another practical concern is data replication, where dishonest data providers may duplicate data points to exploit their value for greater rewards.We propose that the diversity of data points is an inherent characteristic of a dataset, separate from validation. To quantify diversity, we use the volume of the data matrix, specifically the determinant of its left Gram. This formalizes the relationship between data diversity and learning performance without the need for validation. Additionally, we introduce a robust volume measure that guarantees protection against replication, based on the notion that copying the same data points does not increase diversity.Through extensive experiments, we demonstrate the consistency and practical advantages of our approach compared to existing methods. Our method is adaptable to different neural networks, independent of the model or task.