The Transformer and its variations have proven to be effective at learning sequences in various domains. However, a major problem is the large number of parameters that need to be trained and the computational complexity of dot-product attention. This study addresses this issue by approximating the multi-head self-attention and point-wise feed-forward transformation components of the Transformer. By analyzing deep neural networks as solvers of ordinary differential equations, the researchers introduce TransEvolve, a temporal evolution scheme that reduces the need for costly dot-product attention in stacked layers. The researchers conducted extensive experiments and found that the level of approximation has different effects on performance depending on the task. While TransEvolve performs comparably to the original Transformer in encoder-decoder tasks, it consistently outperforms the Transformer and its variants in encoder-only tasks. The code for TransEvolve is available on GitHub.