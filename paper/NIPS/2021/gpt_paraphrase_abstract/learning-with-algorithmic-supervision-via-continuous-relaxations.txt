Recent attention has been given to incorporating algorithmic components into neural architectures, enabling the training of neural networks using alternative forms of supervision, such as ordering constraints or silhouettes, rather than relying solely on ground truth labels. While many existing approaches focus on continuously relaxing specific tasks and have shown promising results, their narrow focus limits their applicability to a limited range of applications. In this study, we expand upon these ideas by proposing a method that integrates algorithms into end-to-end trainable neural network architectures using a general approximation of discrete conditions. This involves relaxing the conditions within control structures, such as conditional statements, loops, and indexing, in order to make resulting algorithms differentiable. To obtain meaningful gradients, relevant variables are perturbed using logistic distributions, and the resulting expectation value is approximated. We evaluate this continuous relaxation model on four challenging tasks and demonstrate its ability to perform comparably to task-specific relaxations designed for each individual task.