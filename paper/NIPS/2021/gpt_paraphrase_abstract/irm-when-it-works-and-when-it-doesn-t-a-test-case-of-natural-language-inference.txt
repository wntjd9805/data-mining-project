Invariant Risk Minimization (IRM) is a framework used for generalizing out-of-distribution (o.o.d) data. Previous studies on IRM have mainly focused on theoretical aspects, simple models, and toy problems. This study aims to assess the effectiveness of IRM in mitigating bias, which is a specific case of o.o.d generalization, in more realistic settings and deep models. The researchers use natural language inference (NLI) as a test case and gradually progress from synthetic datasets and bias to fully realistic scenarios with natural datasets and bias. The findings reveal that when dealing with naturalistic settings, replacing bias with complex features proves challenging and only yields marginal improvements over empirical risk minimization. Additionally, the performance of IRM is found to be sensitive to random seeds and dependent on factors like dataset size, bias prevalence, and bias strength, thereby limiting its practical advantage. These results shed light on the difficulties of applying IRM to real-world scenarios and emphasize the need for a more realistic approach to o.o.d generalization.