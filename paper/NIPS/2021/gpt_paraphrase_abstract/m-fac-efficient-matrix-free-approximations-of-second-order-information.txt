Efficiently approximating local curvature information of the loss function is crucial for optimizing and compressing deep neural networks. However, most current methods for approximating second-order information are computationally expensive or require large storage, making them impractical. In this study, we explore matrix-free, linear-time approaches to estimate Inverse-Hessian Vector Products (IHVPs) when the Hessian can be approximated as a sum of rank-one matrices, similar to the empirical Fisher matrix approximation. We propose two new algorithms: the first focuses on network compression and can compute the IHVP for dimension d using O(dm^2) precomputation, O(dm) cost for computing the IHVP, and O(m) query cost for any single element of the inverse Hessian. The second algorithm is designed for optimization and computes the product between the inverse Hessian, estimated over a sliding window of optimization steps, and a given gradient direction, as needed for preconditioned SGD. This algorithm has a cost of O(dm + m^2) for computing the IHVP and O(dm + m^3) for adding or removing any gradient from the sliding window. Both algorithms achieve state-of-the-art results for network pruning and optimization with lower computational overhead compared to existing second-order methods. Implementations can be found at [9] and [17].