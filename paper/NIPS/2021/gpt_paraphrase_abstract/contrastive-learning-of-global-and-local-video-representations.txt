Contrastive learning has been successful in self-supervised tasks, but current approaches focus on optimizing representations for specific downstream scenarios. This limits their ability to generalize to tasks they were not designed for. In this study, we propose a method to learn video representations that can generalize to both tasks requiring global semantic information and tasks requiring local fine-grained spatio-temporal information. We accomplish this by optimizing two contrastive objectives that encourage the model to learn both global and local visual information from audio signals. Our experiments show that this approach improves the generalizability of the learned representations, outperforming approaches that only focus on one aspect. We validate our method on various tasks such as action/sound classification, lip reading, deepfake detection, and event/sound localization.