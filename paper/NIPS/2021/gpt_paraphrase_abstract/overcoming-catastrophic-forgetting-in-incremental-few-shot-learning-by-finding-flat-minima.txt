This paper investigates the problem of incremental few-shot learning, where a model needs to recognize new categories with limited examples. Existing methods face significant issues with catastrophic forgetting, a well-known problem in incremental learning exacerbated by data scarcity and imbalance in the few-shot scenario. Our analysis suggests that addressing catastrophic forgetting requires actions during the initial training of base classes rather than later few-shot learning sessions. To tackle this, we propose searching for flat local minima in the base training objective function and then fine-tuning the model parameters within this flat region for new tasks. This approach allows the model to efficiently learn new classes while preserving knowledge of old ones. Extensive experiments show that our method outperforms previous state-of-the-art approaches and is very close to the approximate upper bound. The source code can be found at https://github.com/moukamisama/F2M.