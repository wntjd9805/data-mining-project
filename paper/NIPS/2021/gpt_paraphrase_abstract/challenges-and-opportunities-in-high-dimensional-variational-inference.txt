Current methods for black-box variational inference (BBVI) lack guidance on important design choices, such as variational objective and approximating family. To address this, we propose a framework and experimental tools to understand the impact of these choices and suggest best practices for improving posterior approximation accuracy. By studying the pre-asymptotic tail behavior of density ratios and drawing insights from importance sampling literature, we differentiate the behavior of BBVI methods for low-dimensional versus moderate-to-high-dimensional posteriors. For the latter case, we find that mass-covering variational objectives are challenging to optimize and do not enhance accuracy. However, flexible variational families can improve accuracy and the effectiveness of importance sampling, albeit at the expense of additional optimization challenges. Consequently, for moderate-to-high-dimensional posteriors, we recommend using the exclusive KL divergence and enhancing the variational family or employing model parameter transformations to align the posterior and optimal variational approximation. In contrast, for low-dimensional settings, we observe that heavy-tailed variational families and mass-covering divergences are effective and increase the likelihood of improvement through importance sampling.