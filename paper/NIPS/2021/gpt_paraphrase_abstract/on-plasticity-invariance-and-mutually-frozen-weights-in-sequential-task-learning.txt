Plastic neural networks possess the capability to adjust to new tasks. However, in a continuous learning scenario, the parameters learned from previous tasks can hinder adaptability for future tasks. Specifically, our research demonstrates that when weight decay is employed, the weights in subsequent layers of a deep network may become "mutually frozen." This has a dual effect: it increases network updates' resistance to irrelevant factors, which can be helpful for future tasks, but it can also impede the learning of new tasks that require significantly different features. We find that the local input sensitivity of a deep model is correlated with its adaptability, resulting in a trade-off between adaptability and invariance when training a deep model multiple times. To address this issue, we propose a simple intervention that "resets" the mutually frozen connections, leading to improved transfer learning on various visual classification tasks. The effectiveness of this "resetting" intervention depends on the size of the target dataset and the disparity between the pre-training and target domains. Our approach achieves state-of-the-art results on certain datasets.