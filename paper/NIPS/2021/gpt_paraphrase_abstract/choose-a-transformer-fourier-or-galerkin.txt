This paper introduces the application of self-attention from the Transformer model to a data-driven operator learning problem related to partial differential equations. The goal is to explain and enhance the effectiveness of the attention mechanism. The study demonstrates that the softmax normalization in scaled dot-product attention is not necessary but sufficient. It shows that a linearized Transformer variant without softmax can achieve comparable approximation capacity to a Petrov-Galerkin projection layer-wise, regardless of sequence length. To allow scaling to propagate through attention layers, a new layer normalization scheme resembling the Petrov-Galerkin projection is proposed. This scheme helps the model achieve high accuracy in operator learning tasks with unnormalized data. The paper presents three operator learning experiments, including viscid Burgers' equation, interface Darcy flow, and inverse interface coefficient identification problem. The proposed attention-based operator learner, Galerkin Transformer, exhibits significant improvements in both training cost and evaluation accuracy compared to its softmax-normalized counterparts.