The rationality of the Vision Transformer is explained by drawing parallels with the practical Evolutionary Algorithm (EA). Both have similar mathematical representations. To enhance the transformer structure, we propose a more efficient EAT model that incorporates a dynamic local population similar to EA. We also introduce task-related heads to handle various tasks more flexibly. Additionally, we integrate a spatial-filling curve into the vision transformer to convert image data into a sequential format. This allows us to create a unified EAT framework that can address multi-modal tasks while separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classification task with fewer parameters and higher throughput compared to recent vision transformer works. Furthermore, we conduct multi-modal tasks such as Text-Based Image Retrieval, where our approach outperforms the baseline by improving the rank-1 by +3.7 points on the CSS dataset.