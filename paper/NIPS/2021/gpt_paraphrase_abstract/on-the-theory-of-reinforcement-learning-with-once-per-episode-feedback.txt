This research focuses on a theory of reinforcement learning (RL) where the learner only receives feedback once at the end of an episode. This differs from traditional RL practice where feedback is given at every time step. Despite being an extreme test case, this approach is more representative of real-world applications like self-driving cars and robotics, where evaluating the entire trajectory is easier than providing rewards at each step. The study investigates the possibility of learning in this challenging scenario by using an unknown parametric model to generate trajectory labels. The researchers develop an algorithm that achieves sublinear regret while being statistically and computationally efficient.