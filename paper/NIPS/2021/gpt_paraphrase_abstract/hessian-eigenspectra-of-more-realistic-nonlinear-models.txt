The Hessian matrix and its eigenspectrum are used in various ways for optimization problems, such as developing more efficient algorithms and analyzing models. However, when dealing with nonlinear and non-convex problems, simplifying assumptions are often made to analyze the Hessian eigenspectrum. This raises the question of how applicable these analyses are for realistic nonlinear models. In this study, we utilize random matrix theory to accurately characterize the Hessian eigenspectra for a wide range of nonlinear models, going beyond the limitations of previous assumptions. We demonstrate that the spectral behavior of the Hessian can vary depending on data properties, response model, and loss function. This includes bounded or unbounded support, single- or multi-bulk, and isolated eigenvalues on either side of the main eigenvalue bulk. By examining a simple yet complex model, our analysis sheds light on the theoretical origins of visually striking features observed in more realistic machine learning models.