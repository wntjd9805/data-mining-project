We investigate online convex optimization in the random order model, proposed by Garber et al. [8], where an adversary selects loss functions that are then presented to the online algorithm in a uniformly random order. We focus on the case where the cumulative loss function is (strongly) convex, but individual loss functions may be smooth yet non-convex. Our algorithms achieve optimal bounds and outperform the results of Garber et al. [8], eliminating dimension dependence and improving scaling with the strong convexity parameter. Our analysis uses new connections between algorithmic stability and generalization for sampling without-replacement, similar to those studied in the with-replacement i.i.d. setting, and a refined average stability analysis of stochastic gradient descent.