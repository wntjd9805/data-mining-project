Episodic control is a technique used in reinforcement learning to improve efficiency by recalling past experiences from a memory. In this study, we introduce a new model-based episodic memory system for trajectories that addresses the limitations of current episodic control methods. Our memory not only stores past experiences but also estimates the value of trajectories, helping the agent make better decisions. Using this memory, we develop a complementary learning model that combines model-based, episodic, and habitual learning into a single architecture. Through experiments, we demonstrate that our approach leads to significantly faster and more effective learning compared to other strong reinforcement learning agents in various environments, including those with stochastic and non-Markovian properties.