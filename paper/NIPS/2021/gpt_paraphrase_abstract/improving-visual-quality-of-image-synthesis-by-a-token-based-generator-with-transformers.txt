We present a novel approach to image synthesis by treating it as a visual token generation problem. Unlike existing methods that directly generate an entire image from a single input, our approach allows for flexible manipulation of different regions of the image, enabling content-aware and fine-grained style control. Our method takes a sequence of latent tokens as input to predict the visual tokens required for image synthesis. To accomplish this, we propose a token-based generator called TokenGAN. TokenGAN takes two types of visual tokens, constant content tokens and style tokens, and uses an attention mechanism with a Transformer to assign styles to content tokens, thereby controlling the image synthesis. Extensive experiments demonstrate that TokenGAN achieves state-of-the-art results on popular image synthesis benchmarks, including FFHQ and LSUN CHURCH, at various resolutions. Notably, our generator can synthesize high-fidelity images of size 1024 Ã— 1024 without using convolutions.