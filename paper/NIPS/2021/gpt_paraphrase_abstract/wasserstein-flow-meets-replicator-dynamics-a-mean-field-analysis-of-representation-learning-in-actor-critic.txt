Actor-critic (AC) algorithms, which utilize neural networks, have shown remarkable success in practice. However, existing theoretical support for AC algorithms is mostly limited to linear function approximations or linearized neural networks, which do not capture the essence of representation learning in neural AC. This study takes a mean-field perspective on the convergence and evolution of feature-based neural AC. It focuses on a version of AC where the actor and critic are represented by overparameterized two-layer neural networks and updated using two-timescale learning rates. The critic is updated using temporal-difference (TD) learning with a larger stepsize, while the actor is updated using proximal policy optimization (PPO) with a smaller stepsize. In the continuous-time and infinite-width limiting regime, with properly separated timescales, it is proven that neural AC achieves the globally optimal policy at a sublinear rate. Furthermore, it is also proven that the feature representation induced by the critic network can evolve within a neighborhood of the initial representation.