We examine the off-policy evaluation (OPE) problem in reinforcement learning using linear function approximation. This problem involves estimating the value function of a target policy based on data collected by a behavior policy. To enhance the efficiency of OPE, we propose incorporating variance information of the value function. Specifically, for time-inhomogeneous episodic linear Markov decision processes (MDPs), we introduce VA-OPE, an algorithm that employs the estimated variance of the value function to reweight the Bellman residual in Fitted Q-Iteration. Our algorithm achieves a more precise error bound compared to the current best-known result. Additionally, we provide a detailed analysis of the distribution shift between the behavior policy and the target policy. Extensive numerical experiments support our theoretical findings.