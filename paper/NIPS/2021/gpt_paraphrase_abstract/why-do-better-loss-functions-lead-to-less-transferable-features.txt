Many loss functions and regularizers have been proposed to improve test accuracy in image classification tasks. However, it is uncertain whether these loss functions also enhance the learning of representations for other tasks. This study examines how the choice of training objective affects the transferability of hidden representations in convolutional neural networks trained on ImageNet. The research demonstrates that several objectives significantly improve ImageNet accuracy compared to vanilla softmax cross-entropy. However, these improved fixed feature extractors do not transfer well to downstream tasks, and the choice of loss has minimal impact when networks are fully fine-tuned on new tasks. By using centered kernel alignment to measure the similarity between hidden representations, it is found that differences among loss functions only manifest in the last few layers of the network. Further exploration of representations in the penultimate layer reveals that different objectives and hyperparameter combinations result in varying levels of class separation. Representations with higher class separation achieve greater accuracy on the original task, but their features are less useful for downstream tasks. These findings suggest a trade-off between learning invariant features for the original task and features relevant for transfer tasks.