Decentralized machine learning involves workers computing model updates on their local data. This allows for distributed training on networks without all-to-all connectivity, which protects data privacy and reduces communication costs. However, a challenge in decentralized deep learning is handling differences in workers' local data distributions. To address this, we propose the RelaySum mechanism, which uses spanning trees to distribute information uniformly among workers with finite delays based on node distance. In comparison, the gossip averaging mechanism distributes data uniformly asymptotically but with the same communication volume per step as RelaySum. We demonstrate that RelaySGD, built on RelaySum, is not affected by data heterogeneity and can scale to many workers, enabling accurate decentralized deep learning on diverse data. Our code can be found at http://github.com/epfml/relaysgd.