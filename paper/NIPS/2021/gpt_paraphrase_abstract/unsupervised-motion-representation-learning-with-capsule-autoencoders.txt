We present the Motion Capsule Autoencoder (MCAE), which tackles the challenge of motion invariance in unsupervised learning of motion representations. MCAE utilizes a two-level hierarchy to model motion. At the lower level, the motion signal is divided into short, local, and semantic-agnostic snippets. At the higher level, these snippets are combined to create full-length semantic-aware segments. Motion is represented using a set of transformation invariant templates and the corresponding geometric transformations through a novel design of capsule autoencoders. This enables robust encoding of viewpoint changes. MCAE is tested on the Trajectory20 motion dataset and various real-world skeleton-based human action datasets. It outperforms baselines on Trajectory20 with fewer parameters and achieves state-of-the-art results on unsupervised skeleton-based action recognition.