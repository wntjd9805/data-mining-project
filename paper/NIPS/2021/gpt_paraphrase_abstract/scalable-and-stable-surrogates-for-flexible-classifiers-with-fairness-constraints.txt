We explore the scalability of fairness relaxations in deep neural networks for image and text classification. We propose a simple and reliable method for incorporating fairness constraints during training and demonstrate that certain existing fairness measures are ineffective for non-convex models. To address this, we introduce three new measures: adaptive data re-weighting, and two smooth upper-bounds that are more robust than previous approaches. Our measures achieve similar performance to the best existing methods on simple fairness benchmarks, while also achieving higher accuracy and stability for more complex tasks in computer vision and natural language processing.