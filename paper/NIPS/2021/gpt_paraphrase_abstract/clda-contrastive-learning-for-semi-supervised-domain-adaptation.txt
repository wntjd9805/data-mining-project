This study addresses the challenge of Semi-Supervised Domain Adaptation (SSDA), where existing Unsupervised Domain Adaptation (UDA) approaches do not perform well due to limited labeled samples from the target domain. To overcome this, the authors propose a novel framework called Contrastive Learning for semi-supervised Domain Adaptation (CLDA). CLDA aims to bridge the gap between labeled and unlabeled target distributions as well as the gap between the source and unlabeled target distributions in SSDA. The framework incorporates class-wise contrastive learning to reduce the inter-domain gap and instance-level contrastive alignment to minimize the intra-domain discrepancy. Experimental results on three well-known benchmark datasets (DomainNet, Office-Home, and Office31) demonstrate the superior performance of CLDA, achieving state-of-the-art results.