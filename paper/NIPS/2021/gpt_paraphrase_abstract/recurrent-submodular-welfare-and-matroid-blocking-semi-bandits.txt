Recent research has been focused on studying stochastic multi-armed bandits (MAB) with temporal correlations between the player's actions and the reward distributions of the arms. These correlations result in (sub-)optimal solutions that exhibit interesting dynamical patterns, posing new challenges for algorithms and learning. This study extends this research to a combinatorial semi-bandit setting and examines a variant of stochastic MAB where arms are subject to matroid constraints and become blocked for a fixed number of rounds after each play. The existing state-of-the-art solutions for blocking bandits and matroid bandits only guarantee a 1/2-approximation for general matroids. In this paper, a novel technique called correlated (interleaved) scheduling is developed, which enables the derivation of a polynomial-time (1 1/e)-approximation algorithm (asymptotically and in expectation) for any matroid. Along the way, a connection is discovered to a variant of Submodular Welfare Maximization, for which matching upper and lower approximability bounds are provided (asymptotically). In the case where the mean arm rewards are unknown, the technique separates the scheduling from the learning problem, allowing for control of the (1 1/e)-approximate regret of a UCB-based adaptation of the online algorithm.