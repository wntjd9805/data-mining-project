Implicit planning has become a popular method for combining learned models with reinforcement learning. This study focuses on implicit planners based on value iteration, a reliable algorithm for generating optimal policies in tabular environments. Previous approaches either assume the environment is in tabular form or infer local neighborhoods of states to run value iteration over, resulting in a bottleneck effect. This effect is caused by running the planning algorithm based on scalar predictions in every state, which can negatively impact data efficiency if the scalar predictions are inaccurate. To address these limitations, the proposed method, eXecuted Latent Value Iteration Networks (XLVINs), performs planning computations in a high-dimensional latent space, overcoming the algorithmic bottleneck. XLVINs maintain alignment with value iteration through neural graph-algorithmic reasoning and contrastive self-supervised learning. Experimental results demonstrate that XLVINs significantly improve data efficiency compared to value iteration-based implicit planners and other model-free baselines across various low-data settings. Additionally, empirical verification confirms the close alignment between XLVINs and value iteration.