This paper introduces a new model called Continuous Mean-Covariance Bandit (CMCB) to address the limitations of existing risk-aware multi-armed bandit models. These models focus on risk measures of individual options and cannot be directly applied to real-world decision-making problems with correlated options. The CMCB model considers option correlation and aims to achieve the best trade-off between reward and risk, measured with option covariance. The paper proposes algorithms with optimal regrets for three feedback settings: full-information, semi-bandit, and full-bandit feedback. Lower bounds are provided to validate the optimalities of the algorithms, and experimental results demonstrate their superiority. This is the first work to consider option correlation in risk-aware bandits and quantify the impact of arbitrary covariance structures on learning performance. The analytical techniques developed in this paper may have applications in other bandit analysis and be of independent interests.