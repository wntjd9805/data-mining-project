This paper presents new algorithms for causal bandit problems, where interventions on variables of a causal graph are used as actions. Previous research in this area has focused on practical applications, but has relied on the unrealistic assumption that the learner has full knowledge of the causal graph structure. Our algorithms, however, do not require prior knowledge of the causal graph. They are effective for various types of causal graphs, including causal trees and causal forests. Moreover, our algorithms provide significantly better regret guarantees compared to standard multi-armed bandit (MAB) algorithms, as long as certain mild conditions are met. We also demonstrate that these mild conditions are necessary, as without them, standard MAB algorithms cannot be improved upon.