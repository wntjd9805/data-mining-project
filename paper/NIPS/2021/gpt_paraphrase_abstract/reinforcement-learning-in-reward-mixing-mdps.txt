Learning a near optimal policy in a partially observable system is a difficult task in contemporary reinforcement learning. This study focuses on episodic reinforcement learning in a reward-mixing Markov decision process (MDP), where the agent is not aware of the chosen reward model for each episode. The problem is approached without making any assumptions on the dynamics, which is a departure from existing methods. The goal is to find a near optimal policy for two reward-mixing MDPs. The proposed algorithm can achieve this in polynomial time, requiring exploration of approximately O(poly(H, (cid:15)−1)·S2A2) episodes, where H represents the time-horizon and S and A represent the number of states and actions, respectively. This is the first efficient algorithm that does not rely on assumptions in partially observed environments with a smaller observation space compared to the latent state space.