Most existing methods for learning navigation policies rely on simulation environments, online policy interaction, and ground-truth maps for rewards. However, creating simulators is costly and transferring learned policies to real-world robotic platforms is challenging due to the sim-to-real domain gap. This paper questions the necessity of active interaction, ground-truth maps, and reinforcement learning (RL) for solving image-goal navigation tasks. Instead, the authors propose a self-supervised approach called No RL, No Simulator (NRNS) that learns to navigate solely from passive videos of roaming. NRNS is a simple and scalable method that surpasses RL-based approaches by a significant margin. The authors present NRNS as a robust baseline for future image-based navigation tasks involving RL or simulation.