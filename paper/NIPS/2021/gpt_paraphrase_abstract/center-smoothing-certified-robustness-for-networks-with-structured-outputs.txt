The focus of research on provable adversarial robustness has mainly been on classification tasks and models with one-dimensional real-valued outputs. However, we aim to expand the scope of certifiable robustness to problems with more diverse and structured outputs such as sets, images, and language. To achieve this, we consider the output space as a metric space using distance or similarity functions like intersection-over-union, perceptual similarity, or total variation distance. These models are commonly used in various machine learning applications including image segmentation, object detection, generative models, and image/audio-to-text systems.We introduce a technique called randomized smoothing, which allows us to produce certifiably robust models by ensuring that the change in output, as measured by the distance metric, remains small for any adversarial perturbation of the input within a given norm-bound. Our approach, known as center smoothing procedure, enables us to create robust models with different output spaces, ranging from sets to images. Importantly, our method generates meaningful certificates while maintaining the performance of the base model at a satisfactory level.