Normalization techniques have become essential in modern Convolutional Neural Networks (ConvNets). Recent studies have shown that promoting weight orthogonality can enhance the training of deep models and improve their robustness. However, existing methods for ConvNets often penalize or normalize weight matrices derived from concatenating or flattening convolutional kernels, which disregards the inherent structure of the kernels and can be computationally expensive. In contrast, our proposed "Convolutional Normalization" (ConvNorm) method fully utilizes the convolutional structure in the Fourier domain and can easily be incorporated into any ConvNet as a plug-and-play module. Inspired by pre-conditioning methods for convolutional sparse coding, our ConvNorm effectively promotes isometry within each layer's channels. Additionally, we demonstrate that ConvNorm reduces the spectral norm of weight matrices in each layer, improving the Lipschitzness of the network and facilitating easier training and enhanced robustness for deep ConvNets. We apply ConvNorm to classification tasks under noise corruptions and Generative Adversarial Networks (GANs), and observe improved robustness for common ConvNets like ResNet, as well as enhanced performance for GANs. Experimental results on CIFAR and ImageNet datasets validate our findings. Our ConvNorm implementation is publicly available at https://github.com/shengliu66/ConvNorm.