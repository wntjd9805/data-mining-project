Training Restricted Boltzmann Machines (RBMs) has long been challenging due to the difficulty of accurately calculating the log-likelihood gradient. Previous studies have proposed various training methods, but without considering the crucial factor of the mixing time, which is the number of iterations needed to sample new configurations from a model. In this study, we demonstrate that the mixing time significantly impacts the dynamics and stability of the trained model. RBMs operate in two distinct regimes, equilibrium and out-of-equilibrium, depending on the interplay between the model's mixing time and the number of steps used to approximate the gradient. Our empirical findings reveal that the mixing time increases with learning, often causing a transition from one regime to another when the number of steps becomes smaller than the mixing time. Notably, using popular approaches like k (persistent) contrastive divergence with a small k value results in extremely slow dynamics and strong out-of-equilibrium effects. Conversely, RBMs trained in equilibrium exhibit faster dynamics and smoothly converge to dataset-like configurations during sampling. We also discuss how to practically utilize both regimes depending on the desired task: (i) short k can generate convincing samples in a shorter learning time, and (ii) a large k (or increasingly large) is necessary to learn the correct equilibrium distribution of the RBM. The existence of these two operational regimes appears to be a general characteristic of energy-based models trained through likelihood maximization.