Implicit neural representations offer a promising method of representing general signals by using a continuous function parameterized as a neural network. This function maps the spatial coordinates of a signal to its pixel values, allowing for the conveyance of fine details in high-dimensional signals. Compared to conventional discrete representations, implicit neural representations offer several advantages. However, the current approach faces challenges when scaling up to a large number of signals or a dataset. Learning a separate neural representation for each signal requires significant memory and computational resources. To overcome this issue, we propose a solution that combines meta-learning and network compression under a sparsity constraint. This approach results in a well-initialized sparse parameterization that quickly adapts to represent a set of unseen signals during subsequent training. Our empirical results demonstrate that meta-learned sparse neural representations achieve smaller losses compared to dense meta-learned models with the same number of parameters when trained to fit each signal using the same number of optimization steps.