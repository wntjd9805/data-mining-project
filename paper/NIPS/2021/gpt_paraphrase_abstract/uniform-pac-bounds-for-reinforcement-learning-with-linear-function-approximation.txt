We introduce a new algorithm called FLUTE for reinforcement learning with linear function approximation. Existing algorithms for this problem have limitations in terms of convergence to the optimal policy. FLUTE addresses this issue by providing a uniform-PAC convergence guarantee, which is the strongest guarantee in the field. This guarantee implies both PAC and high probability regret bounds, making our algorithm superior to existing ones with linear function approximation. The key components of FLUTE are a unique minimax value function estimator and a multi-level partition scheme for selecting training samples from historical observations. These techniques are novel and have independent significance.