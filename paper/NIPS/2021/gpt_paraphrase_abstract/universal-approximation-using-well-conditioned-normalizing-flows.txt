This study focuses on normalizing flows, a type of generative model that is widely used in machine learning. In particular, the study examines affine-coupling models, which are a common type of normalizing flow that have a triangular structure in their latent-to-observable-variable transformation. This structure allows for efficient computation of the likelihood. However, despite their widespread usage, the representational power of affine couplings has been difficult to understand.Recently, three papers resolved the question of universal approximation for affine couplings, showing that they can approximate reasonably regular distributions well. However, these approximations require networks with nearly-singular Jacobians, which can be problematic for likelihood-based training. Therefore, the fundamental question remains: which distributions can be approximated using well-conditioned affine coupling flows?This paper presents a proof that any log-concave distribution can be approximated using well-conditioned affine-coupling flows. The proof uncovers and leverages connections between affine coupling architectures, underdamped Langevin dynamics, and HÃ©non maps. These connections provide theoretical evidence for the benefits of using Gaussian padding when training normalizing flows, which has been observed empirically but lacked theoretical grounding.In summary, this study demonstrates that well-conditioned affine-coupling flows can approximate any log-concave distribution. The findings also inform the practice of training affine couplings, highlighting the advantages of using Gaussian padding.