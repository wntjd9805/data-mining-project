The goal of audio-visual video parsing is to categorize a video into audio or visual events. However, annotating these events is time-consuming and hinders the learning process of a parsing model. To address this issue, we suggest using cross-video and cross-modality supervisory signals to aid weakly-supervised audio-visual video parsing. Our method utilizes both common and diverse event semantics across videos to identify audio or visual events. Additionally, we examine event co-occurrence across audio, visual, and audio-visual streams. By leveraging these cross-modality co-occurrences, we can localize segments of target events while excluding irrelevant ones. These discovered supervisory signals across different videos and modalities greatly enhance training using only video-level annotations. Quantitative and qualitative results demonstrate the superior performance of our proposed method compared to existing approaches in weakly-supervised audio-visual video parsing.