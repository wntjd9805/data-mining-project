Current Reinforcement Learning (RL) algorithms have limited practicality in real-world situations as they rely on numerous experiments to learn effective actions, which can be costly due to the need for exploration. To address this limitation, we propose a batch RL algorithm that learns effective policies using a fixed offline dataset instead of online interactions with the environment. However, the limited data in batch RL introduces uncertainty in value estimates for states/actions that were not adequately represented in the training data. This uncertainty becomes more pronounced when our candidate policies differ significantly from the one that generated the data. To mitigate this issue, we suggest two straightforward penalties: a policy-constraint to minimize divergence and a value-constraint to discourage overly optimistic estimations. We evaluate our approach on 32 continuous-action batch RL benchmarks and find that it outperforms state-of-the-art methods regardless of how the offline data was collected.