BinaryConnect (BC) has become the dominant method for neural network quantization, but there is still much to learn about its inner workings. In this study, we aim to bridge this knowledge gap by addressing four key aspects. Firstly, we demonstrate that existing quantization algorithms, including post-training quantization, share surprising similarities. Secondly, we advocate for proximal maps as an intuitive and analyzable family of quantizers. Thirdly, we refine the understanding that BC is a specific instance of dual averaging, which itself is a variant of the generalized conditional gradient algorithm. Lastly, building on these insights, we introduce ProxConnect (PC) as a generalization of BC and establish its convergence properties through the established connections. To validate our approach, we conduct experiments on CIFAR-10 and ImageNet datasets, and confirm that PC achieves competitive performance.