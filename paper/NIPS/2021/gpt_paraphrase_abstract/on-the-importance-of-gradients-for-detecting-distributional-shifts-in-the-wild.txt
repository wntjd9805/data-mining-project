Detecting out-of-distribution (OOD) data is crucial for the safe deployment of machine learning models in real-world scenarios. Existing methods for OOD detection focus mainly on the output or feature space, neglecting the valuable information provided by the gradient space. This paper introduces GradNorm, a straightforward and efficient approach that utilizes information extracted from the gradient space to detect OOD inputs. GradNorm calculates the vector norm of gradients obtained through backpropagation from the KL divergence between the softmax output and a uniform probability distribution. The main idea behind GradNorm is that the magnitude of gradients is higher for in-distribution (ID) data compared to OOD data, making it an informative feature for OOD detection. GradNorm outperforms previous methods, reducing the average FPR95 by up to 16.33%. The code and data for GradNorm are available at https://github.com/deeplearning-wisc/gradnorm_ood.