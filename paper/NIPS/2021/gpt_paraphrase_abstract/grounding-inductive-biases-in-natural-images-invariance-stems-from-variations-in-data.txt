To ensure machine learning models perform well on new and potentially different samples, it is important for them to have a predictable response to transformations that affect the input's variations. In this study, we examine the significance of different types of biases in achieving this predictability, including the choice of data, data augmentations, and model architectures. While previous research has focused on synthetic data, we aim to analyze the factors of variation in a real dataset, ImageNet, and investigate the invariance of standard residual networks and the newer vision transformer models to changes in these factors. We discover that standard augmentation primarily relies on a combination of translation and scale, with translation being the most impactful, despite the built-in translation invariance in convolutional architectures like residual networks. Surprisingly, both residual networks and vision transformers exhibit similar scale and translation invariance, despite their architectural differences. We also find that the training data itself is the primary source of invariance, and data augmentation only amplifies the learned invariances. Moreover, the invariances learned during training align with the factors of variation identified in ImageNet. Lastly, we observe that the main factors of variation in ImageNet are mainly related to appearance and are specific to each class.