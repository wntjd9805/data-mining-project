This paper focuses on video recognition using Transformers. While recent attempts have shown promising results in terms of accuracy, they often come with significant computational overhead due to the inclusion of temporal information. To address this issue, we propose a Video Transformer model that scales linearly with the number of frames in the video sequence, eliminating the overhead compared to an image-based Transformer model. Our model achieves this by making two approximations: (a) limiting time attention to a local temporal window and utilizing the Transformer's depth for full temporal coverage, and (b) employing efficient space-time mixing to attend to both spatial and temporal locations without additional cost. We also introduce two lightweight mechanisms for global temporal-only attention, which improve accuracy without adding computational burden. Our model demonstrates high recognition accuracy on popular video recognition datasets while maintaining superior efficiency compared to other Video Transformer models. The code for our method is available here.