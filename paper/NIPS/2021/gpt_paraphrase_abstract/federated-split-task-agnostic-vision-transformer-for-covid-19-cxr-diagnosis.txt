Federated learning is gaining popularity in healthcare as it allows training on decentralized data while preserving data privacy. However, exchanging weights between clients can consume a lot of network bandwidth. Split learning addresses this issue by dividing the neural network into client and server parts, but finding the optimal split without compromising network performance is challenging. To overcome this, we propose using the Vision Transformer, a deep learning architecture that is well-suited for split learning without sacrificing performance. Our framework achieves comparable results to data-centralized training, even with non-independent and identically distributed data from multiple sources. Additionally, our framework improves individual task performances, including COVID-19 diagnosis, without the need for sharing large weights. These findings demonstrate the effectiveness of the Transformer for collaborative learning in medical imaging and pave the way for real-world implementations.