Sparse data recovery is crucial in various applications of machine learning and signal processing. The LASSO estimator and Basis Pursuit approach are commonly used for such problems, but specialized algorithms are needed for solving high-dimensional non-smooth optimization in large instances. Iteratively Reweighted Least Squares (IRLS) is a popular algorithm known for its numerical performance. However, existing theory only guarantees convergence to the minimizer without providing a global convergence rate. This paper establishes that a modified version of IRLS achieves global linear convergence rate to a sparse solution, resulting in immediate linear error decrease from any initialization, given that measurements satisfy the null space property assumption. Numerical experiments validate our theory by demonstrating that the linear rate accurately captures the dimension dependence. Our theoretical findings are expected to offer new insights for various applications of the IRLS algorithm, including low-rank matrix recovery.