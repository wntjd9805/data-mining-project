Image-based virtual try-on is a promising application of human-centric image generation. Existing methods fit in-shop garments onto a target person, requiring paired training datasets that limit scalability. Some recent works attempt to transfer garments directly between people but lack supervised information, impacting performance. This study proposes a texture-preserving end-to-end network called PASTA-GAN for unsupervised virtual try-on. PASTA-GAN disentangles style and spatial information using a patch-routed disentanglement module and reconstructs normalized patches to the warped garment. It also introduces spatially-adaptive residual blocks to generate realistic garment details. Comparisons with paired and unpaired approaches show PASTA-GAN's superiority in generating high-quality try-on images for various garments, taking a step towards real-world scalability.