The partial ranking loss is a commonly used evaluation measure for multi-label classification. It is typically optimized using convex surrogates for efficiency. Previous theoretical analyses have focused on consistency, but there is a disconnect between theory and practice. Some inconsistent pairwise losses perform well, while consistent univariate losses do not consistently outperform in practice. This paper aims to bridge this gap by studying consistency and generalization error bounds of learning algorithms. The study identifies two factors that affect algorithm performance: instance-wise class imbalance and label size. In highly imbalanced cases, algorithms with consistent univariate losses have an error bound of O(c), while those with inconsistent pairwise losses depend on O(c^2). This explains the superior performance of pairwise methods in practice, where datasets are often imbalanced. The study also presents an inconsistent reweighted univariate loss-based algorithm, which achieves an error bound of O(c) while maintaining the computational efficiency of univariate losses. Experimental results validate the theoretical findings.