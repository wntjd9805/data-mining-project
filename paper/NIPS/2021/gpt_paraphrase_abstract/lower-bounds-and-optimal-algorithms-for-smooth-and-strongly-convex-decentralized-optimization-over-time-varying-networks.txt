We investigate the problem of minimizing the sum of smooth and strongly convex functions stored in a decentralized manner across a communication network with changing links. We address two key challenges: determining the minimum number of communication rounds and local computations needed to achieve an Ïµ-accurate solution, and developing optimal algorithms to meet these requirements. We propose two algorithms: one based on the ADOM algorithm with a multi-consensus subroutine, optimal when dual gradients are accessible; and a novel algorithm called ADOM+, optimal when primal gradients are accessible. We validate the efficiency of these algorithms through experimental comparisons with existing methods.