The understanding of learning in the brain is limited, and there is a lack of knowledge regarding learning rules that respect biological constraints while producing deep hierarchical representations. In this study, we propose a learning rule that combines principles from neuroscience and recent advancements in self-supervised deep learning. Instead of back-propagating error signals, our rule minimizes a simple layer-specific loss function and relies on local, Hebbian weight updates based on neuronal activity, predictive dendritic input, and widely broadcasted modulation factors. This learning rule is implemented in a biological context using saccades (rapid shifts in gaze direction) and applies contrastive predictive learning. Our findings demonstrate that networks trained with this self-supervised and local rule are capable of constructing deep hierarchical representations of images, speech, and video.