We propose a method called aligned structured sparsity learning (ASSL) to address the challenges of reducing network redundancy in lightweight image super-resolution (SR) networks. Unlike other model compression techniques, ASSL uses network pruning, which is a cost-effective approach. However, applying filter pruning to SR networks is difficult. To overcome this, ASSL introduces a weight normalization layer and applies L2 regularization to the scale parameters for sparsity. It also includes a sparsity structure alignment penalty term to align the pruned filter locations across different layers. We train an efficient image SR network called ASSLN using this strategy, which outperforms recent methods in terms of both quantitative and visual performance. ASSLN has a smaller model size and lower computation compared to state-of-the-art methods.