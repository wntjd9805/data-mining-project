Deep neural networks have been widely used to learn the latent structure of datasets in various modalities. However, existing models are specific to each modality and require customized architectures and objectives. In this study, we propose a modality-independent approach using neural fields to capture the underlying structure of images, shapes, audio, and cross-modal audiovisual domains. Our approach involves learning a low-dimensional, locally linear subspace that represents the data manifold. By enforcing coverage, local linearity, and local isometry, our model, GEM, can effectively capture the underlying structure across modalities. This allows us to interpolate between samples in a perceptually consistent manner and generate diverse completions of input images or hallucinations of audio or image signals. Furthermore, we demonstrate that by traversing the underlying manifold of GEM, we can generate new samples in our signal domains.