Reducing the processing time of large deep learning models is a fundamental challenge in many real-world applications. To address this, early exit methods have been developed, which involve attaching additional Internal Classifiers (ICs) to intermediate layers of a neural network. These ICs can quickly provide predictions for easy examples, reducing the overall inference time of the model. However, if an IC does not decide to return an answer early, its predictions are discarded and its computations are wasted. To address this issue, we propose a new approach called Zero Time Waste (ZTW). In ZTW, each IC reuses predictions from its predecessors by adding direct connections between ICs and combining previous outputs in an ensemble-like manner. We conducted extensive experiments on various datasets and architectures to evaluate the effectiveness of ZTW. Our results demonstrate that ZTW achieves a significantly better trade-off between accuracy and inference time compared to other recently proposed early exit methods.