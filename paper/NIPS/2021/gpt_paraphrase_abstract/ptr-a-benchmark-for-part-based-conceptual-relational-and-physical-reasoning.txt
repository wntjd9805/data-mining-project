The ability to break down visual scenes into individual objects and their parts is a crucial aspect of human visual perception. This process forms part-whole hierarchies, which contain a wealth of semantic concepts and relationships. Part-based reasoning is more challenging than object-centric reasoning due to the finer details, complex geometry relations, and physics involved. To address the need for part-based conceptual, relational, and physical reasoning, we have created a large-scale dataset called PTR. This dataset includes around 70k RGBD synthetic images with annotations for object and part-level information such as semantic instance segmentation, color attributes, spatial and geometric relationships, and physical properties like stability. It also includes 700k machine-generated questions that cover various types of reasoning. We evaluated several state-of-the-art visual reasoning models on this dataset and found that they still make surprising mistakes that humans can easily avoid. We believe this dataset will provide new opportunities for part-based reasoning. The PTR dataset and baseline models are publicly available for use.