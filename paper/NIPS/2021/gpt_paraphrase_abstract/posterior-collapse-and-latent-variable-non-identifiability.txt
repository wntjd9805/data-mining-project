Variational autoencoders (VAEs) are used to model complex data by positing lower-dimensional latent variables that are transformed through a flexible distribution. However, VAEs often suffer from a problem called posterior collapse, where the posterior of the latent variables becomes equal to its prior, rendering the VAE ineffective in generating meaningful representations. Previous approaches attributed posterior collapse to neural networks or optimization issues. In this paper, we propose that posterior collapse is a result of latent variable non-identifiability in the generative model. We prove that posterior collapse occurs only when the latent variables are non-identifiable. This implies that posterior collapse is not specific to flexible distributions or approximate inference but can also occur in classical probabilistic models with exact inference. To address this problem, we introduce a class of latent-identifiable VAEs that enforce identifiability without sacrificing flexibility. These models utilize bijective Brenier maps parameterized by convex neural networks, without the need for special variational inference objectives or optimization techniques. Our experiments on synthetic and real datasets demonstrate that latent-identifiable VAEs outperform existing methods in mitigating posterior collapse and generating meaningful representations of the data.