In machine learning, many problems involving probabilistic modeling use gradient-based optimization with expectations as the objective. However, optimizing these objectives can be difficult when the parameters being optimized determine the probability distribution under which the expectation is taken. The naive Monte Carlo procedure is not differentiable, but reparameterization gradients provide a way to make the optimization efficient by transforming the expectation into a differentiable form. However, this approach is typically limited to distributions with simple forms and tractable normalization constants. This paper introduces a method to differentiate samples from slice sampling, a Markov chain Monte Carlo algorithm, in order to compute slice sampling reparameterization gradients. Slice sampling only requires a density function that can be evaluated point-wise up to a normalization constant, making it applicable to various inference problems and unnormalized models. The approach is based on the observation that when the slice endpoints are known, the sampling path is a deterministic and differentiable function of the pseudo-random variables. The method is evaluated on synthetic examples and applied to different applications involving reparameterization of unnormalized probability distributions.