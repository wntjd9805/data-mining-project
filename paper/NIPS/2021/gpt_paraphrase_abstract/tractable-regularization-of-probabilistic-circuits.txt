Probabilistic Circuits (PCs) are a promising approach for probabilistic modeling, combining the benefits of probabilistic graphical models (PGMs) and neural networks (NNs). PCs are tractable models, enabling efficient and accurate computation of various probabilistic inference queries. However, PCs are susceptible to overfitting, which existing regularization techniques for PGMs and NNs fail to address effectively. To address this, we propose two novel regularization techniques for PCs: data softening and entropy regularization. Data softening adds uncertainty to datasets in a principled manner, implicitly regularizing PC parameters. Entropy regularization directly regularizes the exact entropy of the PC's encoded distribution. Both techniques consistently improve the generalization performance of PCs, and when combined with a simple PC structure, achieve state-of-the-art results on standard discrete density estimation benchmarks. The code and experiments are available at https://github.com/UCLA-StarAI/Tractable-PC-Regularization.