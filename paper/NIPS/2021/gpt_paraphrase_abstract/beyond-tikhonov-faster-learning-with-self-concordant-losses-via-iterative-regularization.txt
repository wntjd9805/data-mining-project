The application of spectral filtering is a valuable technique for understanding the statistical properties of kernel-based learning. Specifically, it allows for the derivation of various regularization methods that result in faster convergence rates of the excess risk compared to Tikhonov regularization. These improvements are made possible through the utilization of source and capacity conditions, which describe the complexity of the learning task. Previous research by Marteau-Ferey et al. [1] extended the theory of Tikhonov regularization to include generalized self concordant loss functions (GSC), such as the logistic loss. Building upon this work, our paper takes a further step and demonstrates that the iterated Tikhonov regularization approach, which is closely linked to the proximal point method in optimization, can achieve rapid and optimal rates for GSC. This approach overcomes the limitations of classical Tikhonov regularization.