The Configurable Markov Decision Process (CMDP) framework consists of a Reinforcement Learning agent and a configurator. The configurator can modify environmental parameters to enhance the agent's performance, assuming both entities have the same reward functions. However, what if the configurator does not share the agent's intentions? This study introduces the Non-Cooperative Configurable Markov Decision Process (NC-CMDP), which allows for different reward functions for the configurator and the agent. The focus is on an online learning problem, where the configurator must choose the best configuration from a finite set. Two learning algorithms are proposed to minimize the configurator's expected regret, depending on the agent's feedback. While a simple application of the UCB algorithm leads to regret that grows indefinitely, our approach ensures bounded regret. The algorithm's performance is empirically validated through simulations.