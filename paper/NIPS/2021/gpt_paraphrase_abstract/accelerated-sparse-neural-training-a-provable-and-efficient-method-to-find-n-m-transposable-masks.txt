Unstructured pruning reduces memory usage in deep neural networks (DNNs). Researchers have recently proposed structural pruning methods to reduce computation complexity. In this study, we introduce a new measure called mask-diversity, which is correlated with the expected accuracy of different types of structural pruning. We focus on N : M fine-grained block sparsity mask, which allows for acceleration in the inference phase but not in the training phase. To address this, we propose a novel transposable fine-grained sparsity mask that can be used for both forward and backward passes, enabling acceleration in both phases. We formulate the problem of finding the optimal transposable mask as a minimum-cost flow problem and introduce a fast linear-time approximation to speed up the computation. Our experiments show a 2x speed-up in matrix multiplications with no loss of accuracy in vision and language models. Furthermore, we propose a method to convert a pre-trained model with unstructured sparsity to an N : M fine-grained block sparsity model with minimal training. A reference implementation is available at https://github.com/papers-submission/structured_transposable_masks.