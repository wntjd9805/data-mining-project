This paper examines the training of implicit models with infinite layers. Previous research has used implicit differentiation to compute the exact gradient for backward propagation, but this can be computationally expensive. In this study, we propose a new gradient estimation method called the phantom gradient, which avoids the costly computation of the exact gradient while still providing an effective update direction for training implicit models. We analyze the conditions under which an ascent direction of the loss landscape can be found and present two specific implementations of the phantom gradient based on damped unrolling and Neumann series. Our experiments on large-scale tasks show that these lightweight phantom gradients significantly speed up the backward passes in training implicit models by approximately 1.7 times and even outperform approaches using the exact gradient on ImageNet.