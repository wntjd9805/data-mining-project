We investigate learning in the stochastic shortest path (SSP) scenario, where an agent aims to minimize the expected cost until reaching a goal state. Our new model-based algorithm, EB-SSP, manipulates empirical transitions and costs with an exploration bonus to create an optimistic SSP problem, guaranteeing convergence of the associated value iteration scheme. We prove that EB-SSP achieves the minimax regret rate of O(BSAK), where K is the number of episodes, S is the number of states, A is the number of actions, and B bounds the expected cumulative cost of the optimal policy from any state. Notably, EB-SSP is parameter-free and does not require prior knowledge of B or the expected time-to-goal (T) of the optimal policy from any state. We also demonstrate cases, such as positive costs or when an order-accurate estimate of T is available, where the regret only has a logarithmic dependence on T, providing a horizon-free regret bound beyond finite-horizon MDP settings.