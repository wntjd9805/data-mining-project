Bilevel optimization is widely used in machine learning for tasks like hyperparameter optimization and meta-learning. Momentum-based algorithms have recently been introduced to solve these problems more quickly. However, these algorithms do not have better computational complexity than the SGD-based algorithm. In this study, we propose two new algorithms for bilevel optimization. The first algorithm uses momentum-based recursive iterations, while the second algorithm reduces variance through recursive gradient estimations in nested loops. We prove that both algorithms achieve a complexity of O(n^-1.5), which is significantly better than existing algorithms. Our experiments support these findings and demonstrate the superior performance of our algorithms in hyperparameter applications.