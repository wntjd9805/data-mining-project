Contrastive learning methods have been successful in learning visual representations with limited labeled data. This suggests the possibility of incorporating more unlabeled images from external sources to further improve performance. However, using a larger amount of unlabeled data requires more computing resources and can lead to data imbalance and distraction issues. To address this, we propose a principled approach called Model-Aware K-center (MAK) for selecting unlabeled data. MAK follows three principles: prioritizing tail classes, rejecting outliers, and ensuring diversity. Experimental results show that MAK consistently improves representation quality and class balance. The code is available at: https://github.com/VITA-Group/MAK.