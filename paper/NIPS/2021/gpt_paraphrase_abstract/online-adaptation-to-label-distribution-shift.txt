This paper focuses on adapting machine learning models to label distribution shifts in the online setting, where the test-time label distribution is continuously changing. The model needs to adapt dynamically without access to the true label. Through a unique analysis, it is demonstrated that the lack of true label does not impede the estimation of expected test loss, allowing for the reduction of online label shift adaptation to conventional online learning. Based on this observation, adaptation algorithms inspired by classical online learning techniques like Follow The Leader (FTL) and Online Gradient Descent (OGD) are proposed, and their regret bounds are derived. The findings are empirically validated using simulated and real-world label distribution shifts, with OGD proving to be particularly effective and robust in various challenging label shift scenarios.