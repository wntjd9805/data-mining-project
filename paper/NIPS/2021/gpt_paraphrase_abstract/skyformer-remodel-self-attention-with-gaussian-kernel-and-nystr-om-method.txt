To address the high computational cost of training Transformers, this study explores the use of approximation techniques commonly employed in kernel machines. By replacing the softmax structure with a Gaussian kernel and adapting the Nystr√∂m method for non-positive semidefinite matrices, a new model called Skyformer is introduced. Theoretical analysis demonstrates that the proposed method has a small matrix approximation error in the spectral norm. Experimental results on the LongRange Arena benchmark indicate that Skyformer achieves comparable or superior performance to full self-attention while utilizing fewer computational resources.