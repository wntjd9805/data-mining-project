Gradient compression is commonly used to address communication issues in distributed training of deep neural networks (DNNs). Top-k sparsification, which involves selecting only a small percentage of the gradient for communication, can achieve similar model quality to uncompressed training with fewer iterations. However, we propose a new perspective that considers the overall training process rather than per-iteration optimization. We introduce the concept of total error, which encompasses compression errors across all iterations, and develop a communication complexity model that minimizes this error under a communication budget for the entire training. We find that the hard-threshold sparsifier, a variant of Top-k with a constant threshold, is the optimal choice for this model. We provide convergence analyses for the hard-threshold sparsifier with error-feedback and demonstrate through experiments on various DNNs that it is more communication-efficient than Top-k. Code is available at https://github.com/sands-lab/rethinking-sparsification.