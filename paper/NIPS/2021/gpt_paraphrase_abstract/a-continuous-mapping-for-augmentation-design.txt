Automated data augmentation (ADA) techniques have been effective in enhancing the performance of deep models. However, these techniques are often limited by the discretization of the search space. To overcome this limitation, we propose a novel approach that involves creating a continuous mapping from Rd to image transformations. By treating ADA as a continuous optimization problem and using Stochastic Gradient Langevin Dynamics for learning and sampling augmentations, we are able to explore an infinite number of possible augmentations. This approach differs from the traditional discretization-based view of ADA and allows us to leverage gradient-based algorithms for efficient optimization. Our experiments on various benchmarks demonstrate the improved efficiency of our method compared to previous techniques.