Pretrained language models have been successful in achieving top performance in various NLP tasks. However, analyzing these models is difficult due to the disparity between the pretraining and downstream tasks. To address this, we propose a framework that connects the two tasks using a latent variable generative model of text. In this framework, the downstream classifier must recover a function of the posterior distribution over the latent variables. We examine head tuning and prompt tuning in this context. Our analysis incorporates a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component to capture long-term dependencies in natural language. Our findings demonstrate that: 1) simple classification heads can solve the downstream task under certain conditions on the HMM, 2) prompt tuning achieves downstream guarantees with weaker conditions, and 3) the memory-augmented HMM offers stronger recovery guarantees compared to the vanilla HMM due to the easier retrieval of task-relevant information from the long-term memory. We support our theoretical conclusions with experiments conducted on synthetically generated data from HMMs.