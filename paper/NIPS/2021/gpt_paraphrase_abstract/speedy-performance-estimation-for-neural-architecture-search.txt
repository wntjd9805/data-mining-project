Efficiently evaluating the generalization performance of a proposed neural architecture is crucial for successful neural architecture search (NAS). However, traditional approaches have limitations such as high costs, poor correlation between early stopped validation accuracy and fully trained performance, and the need for large training sets. In this study, we propose a simple and efficient estimator for estimating the final test performance based on training speed. This estimator is motivated by the connection between generalization and training speed and is inspired by a Bayesian reformulation of a PAC-Bayes bound. Our model-free estimator is easy to implement, cost-effective, and does not require hyperparameter tuning or surrogate training. We demonstrate that our estimator consistently outperforms other alternatives in accurately ranking the true test performance. Additionally, we show that our estimator can be easily integrated into both query-based and one-shot NAS methods to improve the search speed or quality.