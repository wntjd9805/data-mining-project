In recent times, deep reinforcement learning (DRL) techniques have shown impressive results in various domains. However, the policies generated by these methods are not easily understandable by humans and struggle to adapt to new situations. To tackle these challenges, previous studies have explored the use of programmatic policies that are more interpretable and adaptable. However, these approaches either use limited policy representations or require strong supervision. In this study, we propose a framework that learns to synthesize a program, which provides a flexible and expressive solution to a task based solely on reward signals. To overcome the difficulty of learning to compose programs from scratch, we suggest first learning a program embedding space that continuously parameterizes different behaviors in an unsupervised manner. Then, we search within this embedding space to find a program that maximizes the return for a given task. Experimental results demonstrate that our framework not only successfully synthesizes task-solving programs but also outperforms DRL and program synthesis baselines. Furthermore, the policies generated are interpretable and capable of generalizing well. We also provide justification for the two-stage learning approach proposed and analyze various methods for learning the program embedding. More information can be found at https://clvrai.com/leaps.