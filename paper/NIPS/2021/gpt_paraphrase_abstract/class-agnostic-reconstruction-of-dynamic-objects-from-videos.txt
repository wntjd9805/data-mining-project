We present REDO, a framework that can reconstruct DynamicObjects from RGBD or calibrated videos, regardless of their class. Our approach tackles a more realistic and challenging problem setting compared to previous methods. We aim to reconstruct the complete shape of objects even when occlusion or camera settings prevent them from being fully visible. Additionally, we handle various object dynamics such as rigid motion, non-rigid motion, and articulation, and we can reconstruct different categories of objects using a unified framework.To address these challenges, we introduce two innovative modules. Firstly, we propose a canonical 4D implicit function that aligns pixels with aggregated temporal visual cues. This function helps to accurately represent object shape. Secondly, we develop a 4D transformation module that captures object dynamics, enabling temporal propagation and aggregation of information.We evaluate the effectiveness of REDO through extensive experiments on synthetic RGBD video datasets (SAIL-VOS 3D and DeformingThings4D++) as well as real-world video data (3DPW). Our results show that REDO outperforms state-of-the-art dynamic reconstruction methods by a significant margin. Ablation studies confirm the importance and effectiveness of each component we developed.Overall, our framework REDO provides a robust and versatile solution for reconstructing DynamicObjects from RGBD or calibrated videos, overcoming challenges related to occlusion, object dynamics, and object categories.