Previous research has demonstrated that recurrent neural networks (RNNs) can achieve Turing-completeness. However, these proofs rely on the assumption of unbounded precision in the neurons, which is impractical and biologically implausible. To address this limitation, we propose a memory module composed of neurons with fixed precision that can dynamically grow when additional memories are required and shrink when memories become irrelevant. We prove that a bounded-precision RNN with 54 neurons and growing memory modules can simulate a Universal Turing Machine. The time complexity of this simulation is linear in the simulated machine's time and independent of the memory size. This result can be applied to other stack-augmented RNNs as well. Furthermore, our analysis explores the Turing completeness of both unbounded-precision and bounded-precision RNNs, contributing to the theoretical foundations of RNNs.