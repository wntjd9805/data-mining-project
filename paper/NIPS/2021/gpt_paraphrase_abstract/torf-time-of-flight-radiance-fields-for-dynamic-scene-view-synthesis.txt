Neural networks have been successful in accurately reconstructing radiance fields for static 3D scenes. However, applying these networks to dynamic scenes captured with monocular video poses challenges due to the under-constrained nature of the problem. Previous methods have relied on data-driven priors to handle dynamic content reconstruction. In this study, we propose a new approach that replaces these priors with measurements from a time-of-flight (ToF) camera. We introduce a neural representation based on an image formation model specifically designed for continuous-wave ToF cameras. Unlike existing methods that work with processed depth maps, we utilize the raw ToF sensor measurements to improve reconstruction quality and overcome issues related to low reï¬‚ectance regions, multi-path interference, and limited depth range. Our approach demonstrates enhanced robustness in reconstructing dynamic scenes, even in the presence of calibration errors and large motions. We also discuss the advantages and limitations of integrating RGB+ToF sensors available on modern smartphones.