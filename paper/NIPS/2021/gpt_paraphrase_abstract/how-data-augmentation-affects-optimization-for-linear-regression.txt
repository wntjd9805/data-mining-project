Data augmentation is a widely used tool in machine learning optimization, but its impact on optimization and its interaction with hyperparameters like learning rate is not well understood. This study focuses on the effect of augmentation on optimization in the context of linear regression with mean squared error loss, a simple convex setting. By examining different schedules for learning rate and data augmentation, the study identifies combinations that lead to provable convergence in augmented gradient descent and describes the resulting minimum. The findings demonstrate that the relationship between learning rates and augmentations is intricate even in the convex setting, regardless of the specific augmentation scheme used. The authors interpret augmented stochastic gradient descent as a stochastic optimization method for a sequence of proxy losses that vary over time. This interpretation allows for a unified analysis of learning rate, batch size, and various types of augmentations, ranging from additive noise to random projections. The results also provide convergence rates and can be seen as Monro-Robbins type conditions for augmented stochastic gradient descent.