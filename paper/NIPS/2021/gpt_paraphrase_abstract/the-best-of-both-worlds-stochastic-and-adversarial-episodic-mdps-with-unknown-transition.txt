We address the problem of learning an episodic Markov Decision Process with T episodes, aiming for O(T) regret for adversarial losses and O(polylog(T)) regret for (almost) stochastic losses. Previous work has achieved this when the fixed transition is known, but the case of unknown transition remains unresolved. In this study, we solve this open problem by utilizing the Follow-the-Regularized-Leader (FTRL) framework and introducing new techniques. We propose a loss-shifting trick in the FTRL analysis that simplifies the approach and improves results for the known transition case. Furthermore, we extend this idea to the unknown transition case and develop a novel analysis that bounds the transition estimation error by a fraction of the regret itself in the stochastic setting, ensuring O(polylog(T)) regret.