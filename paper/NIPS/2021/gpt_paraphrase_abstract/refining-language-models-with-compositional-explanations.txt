Pre-trained language models have achieved success in text classification tasks but are susceptible to learning biases from biased datasets, making them vulnerable when applied to new domains. Previous research has addressed this issue by using post-hoc explanation algorithms to identify and eliminate spurious patterns learned by the models. However, these regularization techniques have limitations as they only adjust the importance scores of predefined features and cannot incorporate more complex human knowledge such as feature interaction and pattern generalization.   To overcome these limitations, this study proposes a method to refine a pre-trained language model for a specific domain by collecting human-provided explanations that address observed biases. These explanations are parsed into logic rules that can be applied to more training examples, allowing for generalization. Additionally, a regularization term is introduced to adjust both the importance and interaction of features, further improving the model's behavior. The effectiveness of this approach is demonstrated on two text classification tasks, showing improved performance in the target domain and enhanced model fairness after refinement.