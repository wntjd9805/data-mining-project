Training neural networks using batch normalization and weight decay has become a common practice. This study reveals an unexpected periodic behavior in the optimization dynamics when these techniques are combined. The training process experiences periodic destabilizations, but does not completely diverge and instead enters a new phase of training. Through empirical and theoretical analysis, we investigate the underlying mechanism of this periodic behavior and identify the conditions in which it occurs. Furthermore, we demonstrate that this behavior can be seen as a generalization of the equilibrium and instability perspectives on training with batch normalization and weight decay.