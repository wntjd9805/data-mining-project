Tuning hyperparameters in machine learning is a challenging task, especially in federated learning where models are trained on a distributed network of different devices. This is because the data needs to be kept on the devices and local training makes it difficult to efficiently train and evaluate configurations. In this study, we explore the problem of federated hyperparameter tuning, identifying the challenges and adapting standard approaches to create baselines for the federated setting. We also introduce a new method called FedEx, which is based on the weight-sharing technique used in neural architecture search. FedEx can accelerate federated hyperparameter tuning for popular optimization methods like FedAvg. Theoretically, we demonstrate that a variant of FedEx can accurately tune the learning rate for on-device learning in online convex optimization across devices. Empirically, we show that FedEx outperforms natural baselines for federated hyperparameter tuning on benchmark datasets like Shakespeare, FEMNIST, and CIFAR-10, achieving higher accuracy with the same training budget.