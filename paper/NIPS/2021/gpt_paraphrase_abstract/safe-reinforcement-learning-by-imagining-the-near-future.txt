Safe reinforcement learning is a promising approach for applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors can have negative consequences. This study focuses on situations where unsafe states can be avoided by planning ahead a short time into the future. By using a model-based agent with an accurate model, unsafe states can be avoided. The researchers propose a model-based algorithm that imposes severe penalties on unsafe trajectories and provide guarantees that the algorithm can prevent unsafe states under certain assumptions. Experimental results show that the algorithm can achieve competitive rewards while minimizing safety violations in various continuous control tasks.