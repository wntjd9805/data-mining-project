This study proposes utilizing out-of-distribution samples, which are unlabeled samples from classes outside the target, to enhance few-shot learning. The approach involves using readily available out-of-distribution samples (e.g., from base classes) to guide the classifier in avoiding irrelevant features. This is achieved by maximizing the distance between prototypes and out-of-distribution samples, while minimizing the distance to in-distribution samples (support and query data). The method is easy to implement, independent of feature extractors, lightweight without requiring pre-training, and applicable to both inductive and transductive settings. Extensive experiments on standard benchmarks consistently demonstrate improved performance of pretrained networks with different architectures. The code for this approach is accessible at https://github.com/VinAIResearch/poodle.