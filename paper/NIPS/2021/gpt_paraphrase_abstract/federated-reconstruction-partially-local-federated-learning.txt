Personalization techniques in federated learning aim to find a balance between the advantages of federated and local training, considering factors such as data availability, communication cost, and client heterogeneity. However, some methods that require clients to communicate all model parameters may raise concerns regarding privacy and communication limitations. Additionally, other approaches that rely on always-available or stateful clients are not practical for large-scale cross-device scenarios. To address these issues, we propose Federated Reconstruction, a model-agnostic framework for partially local federated learning that is suitable for both training and inference at scale. We establish the framework's foundation by connecting it to model-agnostic meta learning and demonstrate its superior performance compared to existing methods in collaborative filtering and next word prediction through empirical experiments. Furthermore, we provide an open-source library for evaluating different approaches within this framework. Additionally, we recount the successful implementation of this approach on a large scale for federated collaborative filtering in a mobile keyboard application.