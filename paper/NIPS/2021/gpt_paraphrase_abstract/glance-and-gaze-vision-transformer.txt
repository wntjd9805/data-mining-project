The emergence of vision Transformers has shown great performance in a more compact model size compared to traditional convolutional neural networks, due to the Transformers' ability to model long-range dependencies. However, the self-attention component of Transformers has a quadratic complexity, leading to increased computation and memory costs with longer input sequences. This poses challenges when applying Transformers to vision tasks that require dense predictions based on high-resolution feature maps. To address these issues, we propose the Glance-and-Gaze Transformer (GG-Transformer), inspired by human behavior when recognizing objects in natural scenes. GG-Transformer utilizes two parallel branches: the Glance branch performs self-attention on adaptively-dilated partitions of the input, achieving a linear complexity with a global receptive field, while the Gaze branch compensates for local image context using a simple depth-wise convolutional layer. Empirical results demonstrate that GG-Transformer consistently outperforms previous state-of-the-art Transformers on various vision tasks and benchmarks.