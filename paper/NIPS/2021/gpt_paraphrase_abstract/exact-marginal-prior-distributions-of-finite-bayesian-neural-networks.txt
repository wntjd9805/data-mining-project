Bayesian neural networks in the infinite-width limit have Gaussian priors over network weights and outputs. Recent studies suggest that finite Bayesian networks could outperform infinite ones, but their non-Gaussian function space priors have only been understood through perturbative approaches. In this study, we derive exact solutions for the function space priors of individual input examples in a class of finite fully-connected feedforward Bayesian neural networks. For deep linear networks, the prior can be expressed simply using the Meijer G-function. The prior of a finite ReLU network is a combination of the priors of linear networks with smaller widths, representing different numbers of active units in each layer. Our findings provide a unified understanding of previous descriptions of finite network priors in terms of their tail decay and behavior at large widths.