Traditional software programs allow for easy tracing of program logic, applying assertion statements to prevent errors, and combining programs. However, these functionalities are often sacrificed in deep learning programs. To address this, we propose the Decodable Neural Network (DecNN), which trains a generative model to ensure that neural network activations can be decoded back to inputs. This enables compositionality in neural networks, allowing for recursive composition of DecNN to create ensemble models with uncertainty. In our experiments, we demonstrate the usefulness of this uncertainty in tasks such as out-of-distribution detection, adversarial example detection, and calibration, while maintaining accuracy comparable to standard neural networks. We also explore combining DecNN with pretrained models, showing promising results in regularization and protection of features.