This study introduces an important finding that random GeneralValue Functions (GVFs), which are deep action-conditional predictions, can be effective auxiliary tasks for reinforcement learning (RL) problems. The researchers demonstrate that using random deep action-conditional predictions as auxiliary tasks produces state representations that achieve competitive control performance compared to hand-crafted tasks like value prediction, pixel control, and CURL in both Atari and DeepMind Lab tasks. Additionally, they conduct experiments where they prevent gradients from flowing from the RL part of the network to the state representation learning part, and surprisingly find that the auxiliary tasks alone are sufficient to learn state representations that outperform an end-to-end trained actor-critic baseline. The code for this study is available on GitHub at https://github.com/Hwhitetooth/random_gvfs.