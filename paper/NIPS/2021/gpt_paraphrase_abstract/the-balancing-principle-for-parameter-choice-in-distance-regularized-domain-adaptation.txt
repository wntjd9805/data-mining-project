We tackle the challenge of selecting a suitable regularization parameter for unsupervised domain adaptation, where there are no labels available in the target domain. Our approach is based on the observation that minimizing the source error, penalized by the distance between source and target feature representations, is similar to regularized ill-posed inverse problems. In such problems, the regularization parameters are chosen optimally by balancing approximation and sampling errors. We apply this principle to balance learning errors and domain distance, resulting in a justified rule for selecting the regularization parameter. Unlike previous methods, our approach can handle source and target distributions with disjoint supports. Experimental results on benchmark datasets confirm the effectiveness of our approach.