This study examines the concept of running multiple instances of multi-armed bandit (MAB) problems simultaneously, with a focus on online recommendation systems. These systems aim to efficiently learn users' preferences for different items by exploiting their similarity. The research considers the adversarial MAB setting, where an adversary selects users and losses during the learning process. The users are part of a social network, and the learner has prior knowledge of the strength of social links between users. If two users have a strong social link, they are likely to share the same action. The regret is measured based on a function mapping users to actions, and the smoothness of the function is captured by a dispersion measure called Ψ. The study presents two learning algorithms, GABA-I and GABA-II, which leverage the network structure to prioritize functions with low Ψ values. GABA-I has an expected regret bound of O((cid:112)ln(N K/Ψ)ΨKT) and a per-trial time complexity of O(K ln(N)), while GABA-II has a weaker regret of O((cid:112)ln(N/Ψ) ln(N K/Ψ)ΨKT) but a better per-trial time complexity of O(ln(K) ln(N)). The algorithms demonstrate improvements over running independent standard MABs for individual users.