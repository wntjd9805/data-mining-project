Recent research on Bayesian neural networks (BNNs) has emphasized the need to better comprehend the implications of using Gaussian priors in conjunction with the compositional structure of network architecture. Similar to the analysis done to improve initialization schemes for neural networks (such as He or Xavier initialization), we establish a precise description of the prior predictive distribution of finite-width ReLU networks with Gaussian weights. Previous studies have obtained theoretical results regarding their heavy-tailedness, but the complete characterization of the prior predictive distribution (including its density, cumulative distribution function, and moments) was previously unknown. Through our analysis utilizing the Meijer-G function, we are able to measure the impact of architectural choices, such as network width and depth, on the shape of the resulting prior predictive distribution. We also formally connect our findings to previous work done in the infinite width setting, demonstrating that the moments of the distribution converge to those of a normal log-normal mixture as the depth approaches infinity. Our results offer valuable guidance for prior design, enabling control over predictive variance through informed priors based on network depth and width.