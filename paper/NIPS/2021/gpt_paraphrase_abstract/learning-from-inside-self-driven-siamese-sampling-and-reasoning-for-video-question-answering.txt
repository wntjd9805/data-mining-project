Recent advancements in the field of video question answering (VideoQA) have achieved impressive results by fine-tuning each pair of video clips and text independently using a pretrained transformer-based model through supervised learning. However, it is important to recognize that multiple video clips should be interdependent in order to capture similar visual and semantic information within the same video. To address this, we propose a Siamese Sampling and Reasoning (SiaSamRea) approach. This approach involves a siamese sampling mechanism that generates sparse and similar clips, referred to as siamese clips, from the same video. Additionally, we introduce a novel reasoning strategy to integrate the interdependent knowledge between contextual clips into the network. This reasoning strategy comprises two modules: siamese knowledge generation to learn the inter-relationship among clips, and siamese knowledge reasoning to refine the soft label by propagating the weights of inter-relationship to all predicted candidates of the clips. Ultimately, our SiaSamRea method equips the current multimodal reasoning paradigm with the capability to learn from within, guided by soft labels. Through extensive experiments, we demonstrate that SiaSamRea achieves state-of-the-art performance on five VideoQA benchmarks, including significant gains on several metrics such as MSRVTT-QA, MSVD-QA, ActivityNet-QA, How2QA, and TGIF-QA (action).