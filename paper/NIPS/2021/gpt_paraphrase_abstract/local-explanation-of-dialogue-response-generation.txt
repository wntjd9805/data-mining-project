Compared to the interpretation of classification models, the explanation of sequence generation models, specifically in the context of dialogue response generation, has received little attention. This paper introduces a new method called local explanation of response generation (LERG) to gain insights into how a generation model reasons. LERG considers explanations as the interaction between segments in input and output sentences. It treats sequence prediction as uncertainty estimation of a human response and creates explanations by perturbing the input and measuring the change in certainty over the human response. LERG satisfies desired properties of explanation for text generation, such as unbiased approximation, consistency, and cause identification. Experimental results demonstrate that LERG consistently improves upon other commonly used methods for this task, as evaluated both automatically and by human evaluators, with performance gains ranging from 4.4% to 12.8%. Analysis shows that LERG can extract both explicit and implicit relationships between input and output segments.