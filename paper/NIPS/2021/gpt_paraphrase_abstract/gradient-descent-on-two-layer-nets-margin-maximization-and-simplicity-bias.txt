Efforts have been made to understand how gradient descent (GD) converges to low-loss solutions in overparametrized deep nets that generalize well. While previous analysis focused on the "lazy" or "NTK" regime, real-life neural networks are initialized randomly and trained with cross-entropy loss. Recent theoretical evidence suggests that GD may converge to the "max-margin" solution with zero loss, which is believed to generalize well. However, the global optimality of this margin is only proven for infinitely or exponentially wide neural nets. This paper establishes the global optimality of margin for two-layer Leaky ReLU nets trained with gradient flow on linearly separable and symmetric data, regardless of the width. The analysis also justifies empirical findings on the simplicity bias of GD towards linear or other "simple" solutions, especially early in training. However, the paper cautions that these results are fragile, as a simple data manipulation can cause gradient flow to converge to a suboptimal linear classifier.