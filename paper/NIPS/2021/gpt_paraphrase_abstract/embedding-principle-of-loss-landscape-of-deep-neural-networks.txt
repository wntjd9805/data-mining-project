The structure of the loss landscape in deep neural networks (DNNs) is crucial to understand. This study establishes an embedding principle, showing that the loss landscape of a DNN encompasses all critical points of narrower DNNs. A critical embedding is proposed, enabling any critical point of a narrower DNN to be embedded in a critical point/affine subspace of the target DNN with higher degeneracy, while preserving the DNN output function. This embedding structure holds for any training data, differentiable loss function, and differentiable activation function. Unlike other nonconvex problems, such as protein-folding, the general structure of DNNs exhibits significant differences. Empirical findings indicate that wide DNNs are often attracted to highly-degenerate critical points embedded from narrower DNNs. This embedding principle offers a fresh perspective on the optimization process and potential low-complexity regularization in wide DNN training. Ultimately, this study provides a foundation for exploring the loss landscape of DNNs and its implications, leading to a more comprehensive understanding in the near future.