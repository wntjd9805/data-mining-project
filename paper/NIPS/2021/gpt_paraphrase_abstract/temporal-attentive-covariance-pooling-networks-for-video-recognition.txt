This paper introduces a novel approach called Temporal-attentive Covariance Pooling (TCP) for generating powerful video representations in video recognition tasks. Existing methods, such as global average pooling (GAP), have limited ability to capture the complex dynamics of videos. While covariance pooling has been shown to have stronger representation ability in image recognition tasks, it cannot model the spatio-temporal structure of videos. TCP addresses this limitation by incorporating a temporal attention module to calibrate spatio-temporal features and generate attentive covariance representations. A temporal covariance pooling is then used to capture both intra-frame correlations and inter-frame cross-correlations of the calibrated features, enabling the capture of complex temporal dynamics. Additionally, a fast matrix power normalization technique is introduced to exploit the geometry of covariance representations. TCP can be integrated into any video architecture and is demonstrated to outperform existing methods on six benchmark datasets, with strong generalization ability. The source code for TCPNet is publicly available.