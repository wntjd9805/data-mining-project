Recent research has demonstrated that deep reinforcement learning agents can be easily manipulated by small adversarial changes to their inputs, which raises concerns about their deployment in real-world scenarios. To tackle this issue, we propose RADIAL-RL, a systematic framework designed to train reinforcement learning agents that are more robust against adversarial attacks within a certain lp-norm bound. Our framework is compatible with popular deep reinforcement learning algorithms, such as deep Q-learning, A3C, and PPO. We evaluate the performance of RADIAL-RL on three well-known deep RL benchmarks (Atari, MuJoCo, and ProcGen) and demonstrate its effectiveness in comparison to previous methods. Our RADIAL-RL agents consistently outperform existing approaches when subjected to attacks of varying strengths, while also being computationally efficient to train. Additionally, we introduce a novel evaluation method called Greedy Worst-Case Reward (GWC) to measure the attack-agnostic robustness of deep RL agents. GWC provides a reliable estimate of the reward achieved under the worst possible sequence of adversarial attacks and can be evaluated efficiently. The code for our experiments is publicly available at https://github.com/tuomaso/radial_rl_v2.