We introduce ACProp, an adaptive optimizer that combines centering of second momentum and asynchronous update. ACProp exhibits strong theoretical properties and empirical performance. Compared to synchronous optimizers like Adam, RMSProp, and AdaBelief, asynchronous optimizers like ACProp have a weaker convergence condition. Additionally, centering of second momentum further weakens the convergence condition within asynchronous optimizers. ACProp demonstrates a convergence rate of O(1/√T) for stochastic non-convex scenarios, matching the oracle rate and outperforming the rates of RMSProp and Adam (O(logT/√)). Extensive empirical studies validate ACProp's superiority over SGD, other adaptive optimizers in image classification with CNN, and well-tuned adaptive optimizers in training GAN models, reinforcement learning, and transformers. In summary, ACProp possesses favorable theoretical properties, including a weak convergence condition and optimal convergence rate, along with strong empirical performance like SGD's generalization and Adam's training stability. The implementation can be found at https://github.com/juntang-zhuang/ACProp-Optimizer.