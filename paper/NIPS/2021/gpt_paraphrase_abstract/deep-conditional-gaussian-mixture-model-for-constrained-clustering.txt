Constrained clustering, a popular topic in machine learning, has become increasingly important due to the availability of partially labeled data and the use of prior information. We propose a new framework for constrained clustering called DC-GMM, which combines deep generative models with stochastic gradient variational inference. By incorporating domain knowledge in the form of probabilistic relations, our model efficiently uncovers the underlying data distribution based on prior clustering preferences expressed as pairwise constraints. These constraints guide the clustering process to produce a desired partition of the data, indicating which samples should or should not belong to the same cluster. Through extensive experiments, we demonstrate that DC-GMM outperforms state-of-the-art deep constrained clustering methods in terms of clustering performance and robustness across various datasets. Additionally, we showcase the effectiveness of our approach in two challenging real-world applications.