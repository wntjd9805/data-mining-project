Simulation is commonly used in autonomous systems, specifically in robotic manipulation. Typically, a controller is trained in simulation and then transferred to the real system. In this study, we propose a new approach that allows learning from both simulation and interaction with the real environment simultaneously. To address the challenge of balancing the large number of samples from simulation and the limited, expensive samples from the real environment, we introduce an algorithm that maintains a replay buffer for each environment the agent interacts with. We analyze this multi-environment interaction theoretically and provide convergence properties through a novel replay buffer analysis. To validate our method, we conduct experiments in a sim-to-real environment.