Conditional Generative Adversarial Networks (cGANs) are generative models that can sample from class-conditional distributions. Many existing cGANs use different discriminator designs and training objectives. In earlier works, including a classifier during training was a popular design choice because it was believed that good classifiers could eliminate samples generated with incorrect classes. However, this approach often resulted in cGANs only generating samples that were easy to classify. Recently, some cGANs have achieved state-of-the-art performance without using classifiers. It is still unclear whether classifiers can be reintroduced to improve cGANs. In this study, we show that classifiers can indeed be used to enhance cGANs. We connect the goals of cGANs and classification by decomposing the joint probability distribution, creating a unified framework. This framework, combined with a classic energy model for parameterizing distributions, justifies the use of classifiers in cGANs in a principled way. It also explains popular cGAN variants like ACGAN, ProjGAN, and ContraGAN as special cases with different levels of approximations, offering new insights into understanding cGANs. Experimental results demonstrate that our framework outperforms state-of-the-art cGANs on multiple benchmark datasets, particularly on the challenging ImageNet. The code for our approach is available at https://github.com/sian-chen/PyTorch-ECGAN.