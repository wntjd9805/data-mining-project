The goal of few-shot segmentation is to train a model that can quickly adapt to new classes with only a few examples. Traditionally, models have used semantic-level prototypes to make predictions on query images based on support images. However, this approach does not utilize all the pixel-wise support information, which is crucial for segmentation. In this paper, we propose a new module called CyCTR that aggregates pixel-wise support features into query features using cross-attention. To address the issue of irrelevant support features, we introduce a cycle-consistent attention mechanism to filter out harmful features and encourage the query features to focus on the most informative pixels from the support images. Experimental results on various few-shot segmentation benchmarks show that our CyCTR module achieves significant improvements compared to previous state-of-the-art methods. For example, on Pascal-5i and COCO-20i datasets, we achieve 67.5% and 45.6% mIoU for 5-shot segmentation, surpassing the previous state-of-the-art method by 5.6% and 7.1% respectively.