Multi-task learning is a technique that allows one task to benefit from the information learned by other tasks. However, training all tasks together in one model often leads to degraded performance, and searching for the optimal task groupings can be expensive. Therefore, identifying which tasks should be trained together efficiently is a challenging problem. In this study, we propose a method to select task groupings in multi-task learning models. Our approach involves training all tasks together and measuring the impact of one task's gradient on another task's loss. On the Taskonomy computer vision dataset, our method reduces test loss by 10.0% compared to training all tasks together, while also being 11.6 times faster than a state-of-the-art task grouping method.