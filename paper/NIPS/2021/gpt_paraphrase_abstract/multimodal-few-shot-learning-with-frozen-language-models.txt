Auto-regressive language models have the ability to learn new language tasks with minimal examples. We propose a method to transfer this few-shot learning ability to a multimodal setting by training a vision encoder using aligned image and caption data. The vision encoder represents each image as a sequence of continuous embeddings, allowing a pre-trained language model to generate appropriate captions. This system serves as a multimodal few-shot learner, capable of learning new tasks when conditioned on examples in the form of interleaved image and text embeddings. We show that it can quickly learn new words and visual categories, perform visual question-answering with limited examples, and utilize external knowledge. We evaluate the model on various benchmarks, demonstrating its versatility and effectiveness.