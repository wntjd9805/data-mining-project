This abstract discusses the Diffusion-Decoding models with Contrastive representations (D2C) paradigm, which is used for training unconditional variational autoencoders (VAEs) in few-shot conditional image generation. D2C incorporates a learned diffusion-based prior for improved generation and utilizes contrastive self-supervised learning to enhance representation quality. D2C can adapt to new generation tasks with minimal labeled examples, achieving better performance than state-of-the-art VAEs and diffusion models. In terms of conditional image manipulation, D2C generates images much faster than StyleGAN2 and is preferred by a majority of human evaluators. The code for D2C is available at https://github.com/jiamings/d2c.