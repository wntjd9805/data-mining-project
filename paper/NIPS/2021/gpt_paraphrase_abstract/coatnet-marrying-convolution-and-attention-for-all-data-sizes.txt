Transformers have gained attention in computer vision, but they lag behind convolutional networks in terms of performance. This is because Transformers lack the right inductive bias for effective generalization. To address this, we introduce CoAtNets, a hybrid model that combines the strengths of Transformers and convolutional networks. CoAtNets leverage the unification of depthwise Convolution and self-Attention through simple relative attention, and vertically stack convolution and attention layers in a principled manner to improve generalization, capacity, and efficiency. Experimental results demonstrate that CoAtNets achieve state-of-the-art performance across different datasets and resource constraints. Without additional data, CoAtNet achieves 86.0% top-1 accuracy on ImageNet. When pre-trained with 13M images from ImageNet-21K, CoAtNet achieves 88.56% top-1 accuracy, matching the performance of ViT-huge pre-trained with 300M images from JFT-300M while using significantly less data (23x less). Notably, when scaled up with JFT-3B, CoAtNet achieves a new state-of-the-art result with 90.88% top-1 accuracy on ImageNet.