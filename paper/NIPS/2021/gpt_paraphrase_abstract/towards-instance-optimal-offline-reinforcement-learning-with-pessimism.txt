We examine the problem of offline reinforcement learning (RL), which involves learning a policy that maximizes rewards in an unknown Markov Decision Process (MDP) using data from a given policy µ. We focus on the sample complexity of offline RL for MDPs with finite horizons. Previous studies have looked at this problem under different assumptions about data coverage, but their learning guarantees are expressed using covering coefficients, which lack explicit characterization of system quantities. In this study, we analyze the Adaptive Pessimistic Value Iteration (APVI) algorithm and derive an upper bound on suboptimality that closely matches the expression (1). This expression captures the intrinsic bound of offline RL, which implies all existing optimal results, such as the minimax rate under a uniform data-coverage assumption, horizon-free setting, single policy concentrability, and tight problem-dependent results. We also establish a per-instance information-theoretical lower bound under the assumption that the marginal state-action probability is nonzero. This lower bound, based on local minimaxity, is a stronger criterion as it applies to individual instances separately. We further extend our results to the assumption-free regime, where no assumptions are made about µ, and obtain the assumption-free intrinsic bound. The intrinsic bound, with its generic form, can shed light on what makes a specific problem challenging and reveal the fundamental difficulties in offline RL.