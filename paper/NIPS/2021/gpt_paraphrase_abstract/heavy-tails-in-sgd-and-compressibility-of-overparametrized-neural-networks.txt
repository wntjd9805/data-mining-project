Neural network compression methods are gaining popularity due to their ability to significantly reduce storage and computation requirements for large networks. Simple pruning techniques have been found to be surprisingly effective, and theoretical studies suggest that compressible networks should have low generalization errors. However, the underlying causes that make networks amenable to such compression methods have not been fully understood. In this study, we focus on stochastic gradient descent (SGD) and establish a connection between compressibility and two properties of SGD: (i) as network size increases, the system converges to a mean-field limit where network weights behave independently, and (ii) for a large step-size/batch-size ratio, SGD iterates converge to a heavy-tailed stationary distribution. Assuming both phenomena occur simultaneously, we prove that networks are guaranteed to be "p-compressible," with compression errors becoming arbitrarily small as network size increases. We also provide generalization bounds consistent with the observation that more compressible networks have lower generalization errors. Our theoretical framework and numerical experiments demonstrate that large step-size/batch-size ratios introduce heavy-tails, which, combined with overparametrization, enable compressibility.