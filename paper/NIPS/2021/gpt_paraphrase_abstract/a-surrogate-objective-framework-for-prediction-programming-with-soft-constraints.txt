In the real world, it is common to use prediction and optimization together, where we predict problem parameters before solving the optimization problem. However, the criteria used to train prediction models often do not align with the goal of the optimization problem. To address this, decision-focused prediction approaches have been proposed, but they struggle to handle soft constraints with the max operator that are frequently found in real-world objectives. This paper introduces a new framework that can handle linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. The framework provides theoretical bounds on constraint multipliers and derives closed-form solutions and gradients for any variable in the problem based on predictive parameters. The method is evaluated in three applications with soft constraints, showing superior performance compared to traditional two-staged methods and other decision-focused approaches.