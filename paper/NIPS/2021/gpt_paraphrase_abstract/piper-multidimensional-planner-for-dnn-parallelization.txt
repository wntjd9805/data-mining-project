The increasing size of state-of-the-art DNN models has resulted in higher compute and memory requirements for model training. To address this, various execution schemes such as data parallelism, pipeline model parallelism, and tensor model parallelism have been developed. However, none of these approaches have effectively dealt with the challenge of optimally partitioning the DNN computation graph across multiple accelerators while incorporating all parallelism modes and optimizations. In this study, we propose Piper, an efficient optimization algorithm that solves this complex problem using a two-level dynamic programming approach. Our approach leverages tensor-parallelization techniques for individual layers, which reduces the search space and makes the overall problem more manageable. By focusing on layer-level tensor parallel configurations instead of the entire DNN operator graph, Piper provides a tractable solution.