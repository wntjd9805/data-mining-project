Over-parameterized neural networks have loss terrains with multiple global minima. However, stochastic gradient descent (SGD) can only converge to sufficiently flat minima. This paper investigates the impact of this mechanism on the trained model's function. The study extends the understanding of stability to non-differentiable minima prevalent in ReLU networks. Additionally, a single hidden layer univariate ReLU network is analyzed. The results show that SGD favors functions with bounded weighted L1 norms in their second derivative, irrespective of initialization. Furthermore, the function implemented by the network becomes smoother as the learning rate increases. The weight for the second derivative is higher in the center of the training distribution's support and decreases towards its boundaries. This suggests that trained models tend to be smoother at the center of the training distribution.