Mutual information (MI) maximization is a useful method for creating representations of data. In reinforcement learning (RL), these representations can enhance learning by eliminating irrelevant and redundant information while preserving the necessary information for control. Previous studies have focused on the challenges of estimating MI from high-dimensional observations, but less attention has been given to determining which MI objectives are adequate for RL from a theoretical standpoint. This paper establishes the sufficiency of a state representation for learning and representing the optimal policy and investigates several popular MI-based objectives in this regard. Surprisingly, it is discovered that two of these objectives can result in insufficient representations under mild and common assumptions about the Markov Decision Process (MDP) structure. The theoretical findings are verified through empirical experiments conducted on a simulated game environment with visual observations.