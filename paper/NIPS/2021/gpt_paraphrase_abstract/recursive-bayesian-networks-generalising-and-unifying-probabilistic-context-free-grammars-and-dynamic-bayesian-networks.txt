Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) are popular sequence models. PCFGs allow for nested hierarchical dependencies but require discrete latent variables, while DBNs allow for continuous latent variables but only support sequential dependencies. This paper introduces Recursive Bayesian Networks (RBNs), which combine the strengths of PCFGs and DBNs and can handle both discrete and continuous latent variables with a nested hierarchical dependency structure. The challenge lies in performing joint inference over the exponential number of possible structures and continuous variables. The paper proposes two solutions: generalizing inside and outside probabilities from PCFGs to the mixed discrete-continuous case, and deriving an analytic approximation for Gaussian RBNs. The capacity and applications of RBNs are demonstrated through synthetic data evaluation and hierarchical music analysis.