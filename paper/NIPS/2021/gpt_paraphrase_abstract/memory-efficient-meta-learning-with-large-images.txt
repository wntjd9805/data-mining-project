Meta learning approaches for few-shot classification are efficient at test time but require a lot of memory to train. This is because the entire support set of a task needs to be processed before optimization can occur. To overcome this limitation, we propose LITE, a memory-efficient training scheme that allows meta-training on large tasks with large images on a single GPU. We achieve this by decomposing the gradients for a task into a sum of gradients over the training images, allowing us to back-propagate only a random subset of these images. This subset is shown to be an unbiased approximation of the full gradient. With LITE, we achieve state-of-the-art accuracy on real-world benchmarks and outperform leading meta-learners on three out of four parts of the VTAB+MD benchmark. LITE also enables meta-learners to be competitive with transfer learning approaches while significantly reducing the computational cost at test time. This challenges the notion that transfer learning is the only solution for few-shot classification.