This paper introduces a new framework called Non-Euclidean Monotone Operator Network (NEMON) for designing well-posed and robust implicit neural networks. These networks are a type of deep learning model that solve fixed point equations to perform function evaluation. While implicit models have advantages such as improved accuracy and reduced memory consumption, they can suffer from ill-posedness and convergence instability. The NEMON framework addresses these issues by incorporating contraction theory for the non-Euclidean norm. It includes a condition for well-posedness based on one-sided Lipschitz constants, an average iteration method for computing fixed-points, and explicit estimates on input-output Lipschitz constants. The framework also includes a training problem that enforces the well-posedness condition and average iteration as constraints, and uses the input-output Lipschitz constant as a regularizer to achieve robust models. The well-posedness condition in the NEMON framework allows for a larger training search space compared to existing conditions, and the average iteration method leads to accelerated convergence. The framework is evaluated on image classification tasks using the MNIST and CIFAR-10 datasets, and the numerical results demonstrate improved accuracy and robustness of the implicit models with smaller input-output Lipschitz bounds. The code for the framework is available at a specified GitHub repository.