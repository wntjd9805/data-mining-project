Convolutional neural networks (CNNs) have been highly successful in Computer Vision, thanks to their built-in translation equivariance. However, group equivariant CNNs (G-CNNs), which offer more equivariance, face challenges related to spatial-agnosticity and computational cost. In this study, we present a comprehensive framework that encompasses G-CNNs and equivariant self-attention layers as special cases. Our framework explicitly decomposes the feature aggregation operation into a kernel generator and an encoder, separating spatial and geometric dimensions. Consequently, our filters are dynamic rather than spatial-agnostic. Through complexity analysis and experiments, we demonstrate that our Equivariant model (E4-Net) is parameter-efficient, computationally efficient, and data-efficient. Extensive experiments confirm that our model significantly outperforms previous approaches with a smaller model size. Notably, when trained on just 1/5 of the CIFAR10 dataset, our model achieves a 5%+ accuracy improvement over G-CNNs while using only 56% of the parameters and 68% of the FLOPs.