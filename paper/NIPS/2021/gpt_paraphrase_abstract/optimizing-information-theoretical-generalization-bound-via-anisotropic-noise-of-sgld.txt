We have recently discovered that the information-theoretical framework can provide useful generalization bounds for large models trained using Stochastic Gradient Langevin Dynamics (SGLD) with isotropic noise. In this study, we focus on optimizing these generalization bounds by manipulating the noise structure in SGLD. By considering a constraint to ensure low empirical risk, we demonstrate that the optimal noise covariance is equivalent to the square root of the expected gradient covariance when both the prior and the posterior are jointly optimized. This finding confirms that the optimal noise closely resembles the empirical gradient covariance. To support our analysis, we introduce a new information-theoretical bound and utilize matrix analysis to determine the form of the optimal noise covariance. Our experimental observations validate the presented constraint and results.