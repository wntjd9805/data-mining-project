In order to deploy neural architecture search effectively, it is important to consider the hardware constraints of the target device. Existing methods for hardware-aware neural architecture search require collecting a large number of samples from different devices, which is impractical and costly. To overcome this limitation, we propose a new approach called Hardware-adaptive Efficient Latency Predictor (HELP). HELP uses meta-learning to estimate the latency of a model's performance on an unseen device with only a few samples. We introduce hardware embeddings to represent devices as black-box functions that output latencies, and use these embeddings to train the device-specific latency predictor. Our experiments show that HELP achieves high latency estimation performance with as few as 10 measurement samples, outperforming other methods. Additionally, we demonstrate that incorporating HELP into end-to-end neural architecture search frameworks reduces the total time cost in latency-constrained scenarios. The code for HELP is available at https://github.com/HayeonLee/HELP.