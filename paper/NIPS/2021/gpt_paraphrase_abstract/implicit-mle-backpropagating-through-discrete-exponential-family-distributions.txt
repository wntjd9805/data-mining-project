We present Implicit Maximum Likelihood Estimation (I-MLE), a framework that addresses the challenges of combining discrete probability distributions, combinatorial optimization problems, and neural network components. I-MLE enables end-to-end learning of models by incorporating discrete exponential family distributions and differentiable neural components. Unlike other methods, I-MLE does not rely on smooth relaxations and only requires the ability to compute the most probable states. The framework encompasses various approaches, including perturbation-based implicit differentiation and differentiation through black-box combinatorial solvers. To approximate marginals, we introduce a new class of noise distributions called perturb-and-MAP. Additionally, we demonstrate that I-MLE simplifies to maximum likelihood estimation in certain learning settings involving combinatorial solvers. Experimental results on multiple datasets indicate that I-MLE is competitive with, and often superior to, existing approaches that rely on relaxation techniques specific to the problem.