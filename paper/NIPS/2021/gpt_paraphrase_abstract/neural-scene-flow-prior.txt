Before the rise of deep learning, perception algorithms relied on runtime optimization and strong regularization penalties. In computer vision, optical and scene flow algorithms exemplify this approach. However, supervised learning has diminished the need for explicit regularization by utilizing large labeled datasets to capture prior statistics. Nevertheless, these learning solutions are often domain-specific and struggle to generalize to different scenarios. This paper introduces a novel approach to the scene flow problem by incorporating a neural scene flow prior, which leverages the architecture of neural networks as an implicit regularizer. Unlike existing learning-based methods, our approach performs optimization at runtime and does not require offline datasets, making it suitable for deployment in new environments like autonomous driving. We demonstrate that a multilayer perceptron (MLP) architecture can serve as an effective scene flow prior, achieving competitive or superior results on scene flow benchmarks. Additionally, our neural prior enables the estimation of dense long-term correspondences across a sequence of point clouds, representing motion information through scene flow fields that propagate points through time via motion vectors. We showcase this capability by accumulating a sequence of lidar point clouds.