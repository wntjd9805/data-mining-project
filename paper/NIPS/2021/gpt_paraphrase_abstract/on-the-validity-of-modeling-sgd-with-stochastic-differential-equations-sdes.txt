Finite learning rate (LR) is widely acknowledged as crucial for achieving good generalization in real-life deep neural networks, in contrast to infinitesimal LR. Previous explanations have suggested approximating finite-LR stochastic gradient descent (SGD) with Itˆo Stochastic Differential Equations (SDEs), but the formal justification for this approximation has only been established for SGD with extremely small LR. However, conducting experimental verification of this approximation is computationally impractical. To shed light on this issue, this paper makes the following contributions: (a) The introduction of an efficient simulation algorithm called SVAG, which guarantees convergence to the commonly used Itˆo SDE approximation. (b) The proposal of a testable necessary condition for the SDE approximation and its well-known implication, the linear scaling rule, based on theoretical motivation. (c) The utilization of this simulation to conduct experiments demonstrating that the previously suggested SDE approximation can effectively capture the training and generalization properties of popular deep neural networks.