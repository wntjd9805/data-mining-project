Efficient and risk-aware reinforcement learning (RL) is crucial for complex decision-making. To improve sample efficiency, transfer learning can be utilized, while optimizing a utility function of the return can address risk-awareness. However, the problem of transferring skills in a risk-aware manner is not well understood. This study aims to address this issue by focusing on risk-aware policy transfer between tasks in a common domain with varying reward functions. Risk is measured based on the variance of reward streams. The approach proposed in this paper extends the concept of generalized policy improvement to maximize entropic utilities. This allows for policy improvement via dynamic programming across sets of policies and levels of risk-aversion. Additionally, the concept of successor features (SF), a value function representation that separates environment dynamics from rewards, is extended to capture the variance of returns. The resulting risk-aware successor features (RaSF) seamlessly integrate within the RL framework, inheriting the superior task generalization ability of SFs and incorporating risk-awareness into decision-making. Experimental results in a discrete navigation domain and control of a simulated robotic arm demonstrate that RaSFs outperform alternative methods, including SFs, when considering the risk associated with learned policies.