The scarcity of labeled data poses a major challenge for deep learning. Semi-supervised learning (SSL) offers a potential solution by utilizing unlabeled data through pseudo labels. However, SSL tends to perform poorly and inconsistently when the labeled data is extremely limited, likely due to the low quality of the pseudo labels. In this study, we introduce a novel SSL method called DP-SSL, which employs a data programming (DP) scheme to generate probabilistic labels for unlabeled data. Unlike existing DP methods that rely on human experts for initial labeling functions (LFs), we develop a multiple-choice learning (MCL) approach to automatically generate LFs from scratch in SSL fashion. Using the noisy labels produced by the LFs, we design a label model to reconcile conflicts and overlaps among the noisy labels, ultimately inferring probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks demonstrate that DP-SSL can provide reliable labels for unlabeled data and achieve superior classification performance on test sets compared to existing SSL methods, particularly when only a small number of labeled samples are available. Specifically, for CIFAR-10 with just 40 labeled samples, DP-SSL achieves an annotation accuracy of 93.82% on unlabeled data and a classification accuracy of 93.46% on test data, surpassing the state-of-the-art results.