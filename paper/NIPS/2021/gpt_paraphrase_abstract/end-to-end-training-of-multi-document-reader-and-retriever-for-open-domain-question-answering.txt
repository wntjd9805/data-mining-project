We introduce a differentiable training method for retrieval-based open-domain question answering systems that utilize information from multiple retrieved documents. We treat retrieval decisions as hidden variables over sets of relevant documents, and due to the computational complexity of marginalizing over these sets, we approximate the process using an expectation-maximization algorithm. We estimate the value of the latent variable iteratively and update the parameters of the retriever and reader accordingly. Our hypothesis is that this end-to-end training approach enables better flow of training signals to both the reader and the retriever compared to stage-wise training. This leads to a retriever that can select more relevant documents for a given question and a reader that is trained on more accurate documents to generate answers. Experimental results on three benchmark datasets demonstrate that our proposed method outperforms existing approaches of similar size by 2-3 absolute exact match points, establishing new state-of-the-art performance. Furthermore, our results indicate that it is possible to learn retrieval to enhance answer generation without explicit supervision of retrieval decisions. The output of the answer format is limited to the abstraction.