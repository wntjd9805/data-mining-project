Decision-makers seek reliable predictions when faced with uncertainty. Machine learning providers can instill trust by ensuring that their predictions are distribution calibrated. This means that the predicted class probabilities accurately reflect the actual distribution over classes. However, achieving distribution calibration for multi-class prediction problems is often impractical due to the exponential sample complexity associated with the number of classes.To address this issue, we propose a new concept called decision calibration. This notion requires that the predicted distribution and the true distribution are "indistinguishable" to a group of downstream decision-makers. Decision calibration is equivalent to distribution calibration when considering all possible decision-makers. However, when only decision-makers selecting from a limited number of actions are considered (e.g., polynomial in the number of classes), our main finding demonstrates that decision calibration becomes feasible.We present a recalibration algorithm that achieves decision calibration with a polynomial sample complexity, taking into account both the number of actions and the number of classes. Through empirical validation, we demonstrate the effectiveness of our recalibration algorithm. Compared to existing methods, decision calibration significantly enhances decision-making in skin lesion and ImageNet classification tasks using state-of-the-art neural network predictors.