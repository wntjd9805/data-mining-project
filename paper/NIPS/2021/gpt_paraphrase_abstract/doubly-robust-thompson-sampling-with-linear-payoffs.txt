The bandit problem poses a challenge as only the chosen arm provides a stochastic reward, leaving the rewards of other arms unknown. The complexity of regret analysis is compounded by the arm choice depending on past context and reward pairs. To address this, we introduce a new algorithm called Doubly Robust (DR) Thompson Sampling, which incorporates the doubly-robust estimator from missing data literature into Thompson Sampling with contexts (LinTS). Unlike previous methods that rely on missing data techniques, our algorithm allows for a novel additive regret decomposition that improves the regret bound to approximately O(φ−2T), where φ2 is the minimum eigenvalue of the covariance matrix of contexts. This is the first regret bound for LinTS using φ2 without considering the dimension of the context, d. By considering the relationship between φ2 and d, the regret bound of our algorithm is approximately O(dT) in d. The proposed method has the benefit of utilizing all context data, chosen or not chosen, thus avoiding the technical definition of unsaturated arms used in the theoretical analysis of LinTS. Empirical studies demonstrate the superiority of our algorithm over LinTS.