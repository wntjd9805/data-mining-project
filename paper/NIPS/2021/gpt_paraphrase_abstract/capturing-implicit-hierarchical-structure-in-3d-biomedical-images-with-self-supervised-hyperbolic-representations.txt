We examine how to learn representations for unsupervised segmentation of 3D biomedical images on a voxel grid. We find that models that capture hierarchical relationships between subvolumes perform better in this task. To achieve this, we use encoder-decoder architectures with a hyperbolic latent space to explicitly capture the hierarchical relationships present in the subvolumes. We propose a 3D hyperbolic variational autoencoder with a novel gyroplane convolutional layer to map the embedding space back to 3D images. To capture these relationships, we introduce a self-supervised loss that infers approximate hierarchies and encourages subvolumes that are implicitly related to be closer in the embedding space. We conduct experiments on both synthetic and biomedical data to validate our hypothesis.