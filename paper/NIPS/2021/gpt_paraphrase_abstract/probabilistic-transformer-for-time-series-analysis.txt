Generating accurate and diverse forecasts for multivariate time series data is challenging due to complex and non-deterministic dynamics over long periods. This study introduces deep probabilistic methods that combine state-space models (SSMs) with transformer architectures. Unlike previous SSMs, our approaches utilize attention mechanisms to capture non-Markovian dynamics in the latent space without relying on recurrent neural networks. Additionally, our models incorporate multiple layers of stochastic variables organized hierarchically to enhance expressiveness. Unlike transformer models, our models are probabilistic, non-autoregressive, and capable of generating diverse long-term forecasts while accounting for uncertainty. Extensive experiments demonstrate that our models consistently outperform competing methods on various tasks and datasets, including time series forecasting and human motion prediction.