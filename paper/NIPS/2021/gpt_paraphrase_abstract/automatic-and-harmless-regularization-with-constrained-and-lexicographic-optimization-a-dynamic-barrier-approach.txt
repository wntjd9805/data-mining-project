Many machine learning tasks involve balancing two loss functions, such as the main data-fitness loss and an auxiliary loss. The usual approach is to optimize a linear combination of these objectives, but this requires manual tuning and is not suitable for non-convex functions. This study proposes a more principled method called constrained optimization, with a focus on lexicographic optimization. This approach optimizes a secondary loss within the optimal set of the main loss. A dynamic barrier gradient descent algorithm is introduced to provide a unified solution for both constrained and lexicographic optimization. The convergence of this method is proven for general non-convex functions. Experimental results on real-world deep learning tasks demonstrate that lexicographic optimization allows for incorporating side loss functions without negatively impacting the main objective. Additionally, combining constrained and lexicographic optimization offers an automatic approach to profiling Pareto sets, particularly when linear combination methods fail in non-convex problems.