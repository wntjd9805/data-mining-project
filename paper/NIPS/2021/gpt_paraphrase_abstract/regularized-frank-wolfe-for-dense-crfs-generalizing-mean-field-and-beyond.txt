We present regularized Frank-Wolfe, a versatile and efficient algorithm for inference and learning of dense conditional random fields (CRFs). This algorithm optimizes a nonconvex continuous relaxation of the CRF inference problem using vanilla Frank-Wolfe with approximate updates, which minimize a regularized energy function. Our method is a generalization of existing algorithms like mean field or concave-convex procedure, providing a unified analysis and the ability to explore different variants for improved performance. Empirical results on standard semantic segmentation datasets demonstrate that our regularized Frank-Wolfe outperforms mean field inference, both as a standalone component and as an end-to-end trainable layer in a neural network. Additionally, we show that combining dense CRFs with our new algorithms leads to significant improvements over strong CNN baselines.