This study addresses the challenge of cross-view geo-localization, which involves determining the location of a street view image by comparing it with a database of aerial images. Existing methods mainly rely on CNN, but this study introduces a new approach called Layer-to-Layer Transformer (L2LTR). L2LTR utilizes self-attention in Transformer to model global dependencies and reduce visual ambiguities in cross-view geo-localization. The positional encoding of the Transformer is also used to help understand and correspond geometric configurations between ground and aerial images. Unlike other methods that rely on strong assumptions about geometry, L2LTR learns positional embeddings through training, making it more practical in real-world scenarios. Additionally, this paper proposes a self-cross attention mechanism to improve the quality of learned representations by considering correlations between layers. This mechanism enhances training stability, generalization ability, and prevents over-similar intermediate features. Extensive experiments show that L2LTR outperforms state-of-the-art methods in various cross-view geo-localization tasks. The code for L2LTR is available online.