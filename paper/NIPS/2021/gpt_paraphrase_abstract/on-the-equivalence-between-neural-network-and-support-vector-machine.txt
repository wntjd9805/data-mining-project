Recent research has shown that the behavior of a neural network trained by gradient descent can be described using the Neural Tangent Kernel (NTK). This equivalence has been established for ridge regression, but not for other kernel machines like support vector machines (SVMs). In this study, we aim to establish the equivalence between neural networks and SVMs, specifically for wide neural networks trained with soft margin loss and SVMs trained with NTK using subgradient descent. Our main theoretical contributions include establishing the equivalence between neural networks and a broad family of regularized kernel machines, which was not addressed in previous work, and showing that every finite-width neural network trained with these regularized loss functions can be approximated by a kernel machine. Additionally, we demonstrate the practical applications of our theory, including non-vacuous generalization bounds for neural networks, nontrivial robustness certification for wide neural networks, and intrinsically more robust wide neural networks compared to previous kernel regression methods.