Integrating physics models into machine learning models has great potential for creating robust and interpretable models that can extrapolate beyond the training data. This study focuses on incorporating incomplete physics models into deep generative models, specifically variational autoencoders (VAEs). The challenge is to effectively combine the physics-based latent variables with trainable components like neural networks. To address this, a regularized learning method is proposed to control the influence of the trainable components and preserve the semantics of the physics-based variables. The results demonstrate improved generative performance on both synthetic and real-world datasets, as well as the ability to consistently extrapolate beyond the training distribution. The study also highlights the interpretable control of the generative process.