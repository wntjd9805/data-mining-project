Efficient methods for optimizing the stochastic compositional optimization problem in machine learning are necessary. Current approaches only consider single machine scenarios, which is insufficient for distributed data. To address this, we propose decentralized stochastic compositional gradient descent methods for training large-scale problems. Our work is the first to enable decentralized training for this problem. We also provide convergence analysis, demonstrating that our methods achieve linear speedup with increasing devices. Finally, we apply our decentralized training methods to the model-agnostic meta-learning problem and observe superior performance through experimental results.