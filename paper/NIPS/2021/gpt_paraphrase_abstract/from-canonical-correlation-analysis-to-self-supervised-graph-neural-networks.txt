We present a novel approach for self-supervised representation learning on graph data. Our method generates two views of an input graph using data augmentation, but instead of focusing on instance-level discrimination like previous methods, we optimize a feature-level objective inspired by Canonical Correlation Analysis. Unlike other approaches, our method does not require parameterized mutual information estimators, additional projectors, asymmetric structures, or negative samples. Our objective aims to learn invariant representations by discarding augmentation-variant information and prevents degenerated solutions by decorrelating features. Theoretical analysis reveals that our objective is an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method achieves competitive performance on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG.