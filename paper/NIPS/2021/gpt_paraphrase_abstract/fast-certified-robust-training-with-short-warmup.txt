Recently, researchers have proposed bound propagation based certified robust training methods for training neural networks with guaranteed robustness. However, these state-of-the-art methods, such as interval bound propagation (IBP) and CROWN-IBP, require a long warmup schedule with hundreds or thousands of epochs to achieve the best performance, making them time-consuming. This paper addresses two key issues in existing methods: the problem of exploded bounds at initialization and the imbalance in ReLU activation states, which make certified training unstable and difficult. To overcome these issues and achieve faster certified training with shorter warmup schedules, the authors propose three improvements to IBP training. First, they introduce a new weight initialization method. Second, they suggest adding Batch Normalization (BN) to each layer to reduce the imbalance in ReLU activation states. Finally, they design regularization techniques to tighten certified bounds and balance ReLU activation states during warmup. By implementing these improvements, the authors achieve impressive results on CIFAR-10 and Tiny-ImageNet datasets using significantly shorter training schedules compared to previous state-of-the-art methods. The code for this research is available at a specified GitHub repository.