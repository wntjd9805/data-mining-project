Many real-world applications face a challenge when dealing with severe class imbalance, especially when it comes to accurately classifying and generalizing minority classes. This is particularly relevant in computer vision and natural language processing, where long-tail behavior is common. Current solutions rely on sampling or weighting adjustments to address the extreme imbalance, or on inductive bias to prioritize generalizable associations. In this study, we propose a new approach based on the principles of causality to enhance sample efficiency and model generalization. We introduce a meta-distributional scenario where the causal generating mechanism for label-conditional features remains invariant across different labels. This causal assumption allows for efficient knowledge transfer from dominant classes to their under-represented counterparts, even if their feature distributions appear different. To achieve this, we leverage a causal data augmentation procedure to increase the representation of minority classes. Our approach is independent of existing imbalanced data learning techniques and can be easily integrated. We validate our method on various synthetic and real-world tasks, comparing it to state-of-the-art solutions.