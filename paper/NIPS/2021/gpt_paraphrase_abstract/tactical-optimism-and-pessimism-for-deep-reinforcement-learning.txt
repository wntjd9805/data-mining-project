Deep off-policy actor-critic algorithms have become popular in reinforcement learning for continuous control. These algorithms use pessimistic value updates to address function approximation errors and improve performance. However, pessimism reduces exploration, contradicting the belief in the effectiveness of optimism in uncertain situations. This study explores the optimal level of optimism, finding that it varies across tasks and learning stages. Based on this finding, a new approach called Tactical Optimistic and Pessimistic (TOP) estimation is introduced. TOP alternates between optimistic and pessimistic value learning using a multi-arm bandit problem formulation. Experimental results demonstrate that TOP outperforms existing methods in challenging pixel-based environments. The simplicity of implementing these changes suggests they can be easily incorporated into various off-policy algorithms.