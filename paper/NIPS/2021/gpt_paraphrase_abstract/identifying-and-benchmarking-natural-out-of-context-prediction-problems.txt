Deep learning systems often struggle with predicting outcomes that are out of context (OOC), meaning they struggle to make accurate predictions on uncommon or unusual inputs or subsets of the training data. Recently, several benchmarks have been introduced to measure OOC performance. In this study, we propose a framework that unifies the existing literature on OOC performance measurement and showcases how additional information can be used to identify potential OOC examples within current datasets. We introduce NOOCH, a collection of naturally-occurring "challenge sets," and demonstrate how different interpretations of context can be utilized to investigate specific OOC failure modes. Through experiments, we analyze the tradeoffs between various learning approaches on these challenge sets and highlight how the design of OOC benchmarks can lead to differing conclusions.