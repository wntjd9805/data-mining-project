This study aims to provide a fair and thorough comparison between Transformers and Convolutional Neural Networks (CNNs) in terms of robustness. Previous claims about Transformers' superiority over CNNs were made under unfair experimental conditions, where the two models were compared at different scales and with different training frameworks. However, our research challenges this belief by showing that CNNs can achieve comparable robustness to Transformers when they adopt similar training methods. Additionally, we demonstrate that pre-training on large-scale datasets is not necessary for Transformers to outperform CNNs in generalization to out-of-distribution samples. Instead, the self-attention-like architectures of Transformers play a significant role in their stronger generalization abilities. We hope that this study will contribute to a better understanding and benchmarking of the robustness of Transformers and CNNs. The code and models used in this research are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs.