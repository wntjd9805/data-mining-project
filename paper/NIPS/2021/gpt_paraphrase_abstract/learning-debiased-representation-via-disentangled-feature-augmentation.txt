Image classification models often rely on peripheral attributes of data items that are strongly correlated with a target variable, resulting in biased decisions. These biased models struggle to generalize well when evaluated on unbiased datasets. Current debiasing methods identify and prioritize samples that do not exhibit this correlation without predefining the bias type. However, such bias-conflicting samples are scarce in biased datasets, limiting the effectiveness of these approaches. This study demonstrates the importance of training with diverse bias-conflicting samples from outside the training set for successful debiasing and generalization. Based on this finding, a novel feature-level data augmentation technique is proposed to synthesize diverse bias-conflicting samples. The method learns the disentangled representation of intrinsic attributes (that define a certain class) and bias attributes (that cause the bias) from a large number of bias-aligned samples. By swapping latent features, bias-conflicting samples containing diverse intrinsic attributes are synthesized. Incorporating these diversified bias-conflicting features during training improves classification accuracy and debiasing results compared to existing methods on both synthetic and real-world datasets.