This study focuses on addressing the challenges of large-scale fine-grained image retrieval by ranking images that depict the same sub-category labels highest based on their fine-grained details. The goal is to overcome the difficulties posed by small inter-class variations with large intra-class variations and the exponential growth of fine-grained data. To achieve this, the researchers propose an Attribute-Aware hashing Network (A2-NET) that generates attribute-aware hash codes. These hash codes not only make the retrieval process efficient but also establish explicit connections between the hash codes and visual attributes.   The A2-NET utilizes attention-based visual representations to develop an encoder-decoder network structure for a reconstruction task. This task helps unsupervisedly extract high-level attribute-specific vectors from appearance-specific visual representations without relying on attribute annotations. The A2-NET also incorporates a feature decorrelation constraint to enhance the representation abilities of these attribute vectors. Finally, the required hash codes are generated using the attribute vectors while preserving the original similarities.   Qualitative experiments conducted on five benchmark fine-grained datasets demonstrate the superiority of the proposed method over competing approaches. Additionally, quantitative results show that the obtained hash codes strongly correspond to important properties of fine-grained objects.