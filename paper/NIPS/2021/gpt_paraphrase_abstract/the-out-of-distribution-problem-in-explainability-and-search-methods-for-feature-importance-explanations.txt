Feature importance (FI) explanations are commonly used to understand the importance of input features in a model's decision-making process. However, there are several dimensions of FI explanations that have not been thoroughly explored. This paper addresses these gaps by proposing improvements to FI explanations.Firstly, the paper highlights the issue of removing features from inputs when creating or evaluating explanations. This is problematic because the resulting explanations are socially misaligned, as the counterfactual inputs are out-of-distribution (OOD) to the models. The influence of model prior and random weight initialization on explanations and explanation metrics is unintended. To address this problem, a simple alteration to the model training process is proposed, resulting in more socially aligned explanations and metrics.Secondly, the paper compares five different approaches for removing features from model inputs. It is found that some methods generate more OOD counterfactuals than others. Recommendations are provided for selecting a feature-replacement function based on these findings.Lastly, the paper introduces four search-based methods for identifying FI explanations and compares them to strong baselines such as LIME, Anchors, and IntegratedGradients. Through experiments conducted on six text classification datasets, a Parallel Local Search (PLS) method is found to consistently outperform random search. The improvements over the second best method are significant, with increases of 5.4 points for Sufficiency and 17 points for Comprehensiveness.In summary, this paper explores various dimensions of FI explanations and proposes improvements to address the issues related to removing features from inputs. It also compares different approaches for feature removal and introduces new search-based methods for identifying FI explanations. Experimental results demonstrate the effectiveness of these proposed methods.