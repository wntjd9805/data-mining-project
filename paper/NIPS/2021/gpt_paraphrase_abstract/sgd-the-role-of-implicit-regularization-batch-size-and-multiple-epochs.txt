Stochastic Gradient Descent (SGD) is commonly used for learning with large over-parameterized models. It is believed that the algorithm's success is due to its implicit regularization. In this study, we focus on Stochastic Convex Optimization (SCO) and examine the impact of implicit regularization, batch size, and multiple epochs on SGD. Our findings are threefold: 1. We demonstrate that Regularized Empirical Risk Minimization fails to learn in certain SCO problems, disproving implicit regularization as the sole explanation for SGD's success. 2. We establish a difference in sample complexity between SGD and Gradient Descent on empirical loss (GD), showing that GD can only achieve suboptimal learning rates in some SCO problems. 3. We introduce a multi-epoch variant of SGD and prove its superiority to single pass SGD in worst-case scenarios. Additionally, multiple passes over the dataset can significantly improve performance in certain SCO problems. We extend our results to the general learning setting, showing that SGD outperforms Regularized Empirical Risk Minimization for a specific problem and any data distribution. Finally, we discuss the implications of our findings for deep learning and highlight the distinction between SGD and Empirical Risk Minimization for two-layer diagonal neural networks.