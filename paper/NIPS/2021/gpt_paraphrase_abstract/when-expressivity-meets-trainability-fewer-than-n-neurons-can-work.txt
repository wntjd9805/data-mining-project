Modern neural networks are often wide, resulting in high memory and computation costs. Therefore, there is a growing interest in training narrower networks. However, training narrow neural networks remains challenging. This study aims to address two theoretical questions: Can narrow networks exhibit the same expressivity as wide networks? And if so, does the loss function have a favorable optimization landscape? The findings provide partially affirmative answers to both questions for 1-hidden-layer networks with fewer than n neurons (sample size) when the activation is smooth. Firstly, it is proven that if the network's width m is greater than or equal to 2n/d (where d is the input dimension), it possesses strong expressivity, meaning there is at least one global minimizer with zero training loss. Secondly, a favorable local region is identified, which lacks local minima or saddle points. However, it remains uncertain whether gradient descent can remain in this region. Thirdly, a constrained optimization formulation is considered, where the feasible region is the identified favorable local region, and it is proven that every KKT point is a nearly global minimizer. Although it is expected that projected gradient methods converge to KKT points under mild technical conditions, a rigorous convergence analysis is left for future work. Comprehensive numerical results demonstrate that projected gradient methods, when applied to this constrained formulation, significantly outperform SGD in training narrow neural networks.