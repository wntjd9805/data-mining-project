This paper examines α-divergence minimization methods for Variational Inference. It focuses on optimizing the mixture weights of a mixture model approximation of the posterior density using α-divergence minimization algorithms. The Power Descent algorithm, which converges to optimal mixture weights when α < 1, is thoroughly proven in this work. Additionally, the paper extends the Power Descent to α = 1, resulting in an Entropic Mirror Descent. The relationship between Power Descent and Entropic Mirror Descent is explored, and a first-order approximation called Rényi Descent is introduced. The convergence rate of Rényi Descent is proven to be O(1/N). Finally, a numerical comparison between unbiased Power Descent and biased Rényi Descent is conducted, and the potential advantages of each algorithm are discussed.