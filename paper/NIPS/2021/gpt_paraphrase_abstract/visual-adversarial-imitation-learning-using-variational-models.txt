Reward function specification is a challenging and time-consuming process in deep reinforcement learning. To overcome this, we propose using visual demonstrations as a more natural and efficient way to teach agents. In this approach, the agent is provided with a dataset of visual demonstrations and must learn to solve the task using these demonstrations and unsupervised environment interactions. However, this presents several challenges such as representation learning for visual observations, high sample complexity, and learning instability due to the lack of a fixed reward signal. To address these challenges, we introduce a variational model-based adversarial imitation learning (V-MAIL) algorithm. This model-based approach enhances representation learning, improves sample efficiency, and enhances the stability of training through on-policy learning. Through experiments on various vision-based tasks, we demonstrate that V-MAIL learns successful visuomotor policies efficiently, exhibits better stability compared to previous methods, and achieves higher asymptotic performance. Additionally, we show that V-MAIL can learn new tasks from visual demonstrations without requiring additional environment interactions. Further details and videos of our results can be found at https://sites.google.com/view/variational-mail.