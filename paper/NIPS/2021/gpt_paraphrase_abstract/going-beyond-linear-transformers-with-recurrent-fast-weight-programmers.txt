Linear Transformers, which utilize linearized attention, have proven to be practical and effective in implementing Fast WeightProgrammers (FWPs) based on outer products. However, the original FWP formulation is more versatile than that of linear Transformers, as it involves a slow neural network continually reprogramming the weights of a fast neural network with any desired architecture. Unlike existing linear Transformers, which consist of single-layer feedforward neural networks, we investigate new variations by introducing recurrence to both the slow and fast networks. We assess the performance of our novel recurrent FWPs (RFWPs) on various tasks including algorithmic tasks, language models, and Atari 2600 games. Our models exhibit characteristics of both Transformers and recurrent neural networks (RNNs). In the context of reinforcement learning, we observe significant improvements over LSTM in several Atari games. Our code is publicly available.