Anderson mixing has been commonly used in reinforcement learning (RL) algorithms to speed up convergence and improve sampling efficiency in deep RL. However, there has been no rigorous mathematical justification for the benefits of Anderson mixing in RL. This paper aims to provide a deeper understanding of a specific class of acceleration schemes based on Anderson mixing that enhance the convergence of deep RL algorithms. The main findings establish a connection between Anderson mixing and quasi-Newton methods, showing that Anderson mixing increases the convergence radius of policy iteration schemes by an additional contraction factor. The analysis primarily focuses on the fixed-point iteration nature of RL. Additionally, a stabilization strategy is proposed by introducing a stable regularization term in Anderson mixing and a differentiable, non-expansive MellowMax operator that enables faster convergence and more stable behavior. Extensive experiments confirm that the proposed method improves the convergence, stability, and performance of RL algorithms.