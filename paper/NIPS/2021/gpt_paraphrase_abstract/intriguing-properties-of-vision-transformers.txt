Vision transformers (ViT) have achieved remarkable results in various machine vision tasks. These models utilize multi-head self-attention mechanisms to attend to image patches and capture contextual information. Our study investigates how this flexibility in attending to image-wide context can enhance the model's ability to handle challenges in natural images, such as occlusions, domain shifts, perturbations, and adversarial attacks. We conduct extensive experiments involving three ViT families and compare their performance with a high-performing convolutional neural network (CNN). Our findings reveal several intriguing properties of ViTs: (a) Transformers demonstrate high robustness against severe occlusions, perturbations, and domain shifts, maintaining up to 60% top-1 accuracy on ImageNet even when 80% of the image content is randomly occluded.(b) The robustness of ViTs to occlusions does not stem from texture bias, but rather ViTs exhibit significantly less bias towards local textures compared to CNNs. When trained to encode shape-based features, ViTs exhibit shape recognition capabilities comparable to the human visual system, surpassing previous literature.(c) ViTs can accurately perform semantic segmentation without pixel-level supervision when used to encode shape representation.(d) Features extracted from a single ViT model can be combined to create a feature ensemble, resulting in high accuracy rates across various classification datasets in traditional and few-shot learning scenarios. The effectiveness of ViT features is attributed to the flexible and dynamic receptive fields enabled by self-attention mechanisms. The code for our study is available at https://git.io/Js15X.