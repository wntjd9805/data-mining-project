This study explores the use of Bayesian neural networks (BNNs) to model uncertainty in data and predictions. The problem addressed is verifying safety when using a BNN policy in infinite time horizon systems. Existing sampling-based approaches are not suitable for this setting, so the authors propose a method that involves training a separate deterministic neural network as a safety certificate for infinite time horizons. The certificate network ensures safety within a subset of the BNN weight posterior's support. The approach involves computing a safe weight set and modifying the BNN's weight posterior to reject samples outside this set. The authors also demonstrate how this method can be extended to a safe-exploration reinforcement learning setting to avoid unsafe trajectories during policy training. The approach is evaluated on various reinforcement learning benchmarks, including non-Lyapunovian safety specifications.