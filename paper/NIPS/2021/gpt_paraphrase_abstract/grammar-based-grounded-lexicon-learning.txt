We propose Grammar-Based Grounded Lexicon Learning (G2L2), a method for learning the meaning of language from paired images and texts. G2L2 uses a collection of lexicon entries that map each word to a syntactic type and a neuro-symbolic semantic program. For instance, the word "shiny" is associated with the syntactic type "adjective" and a semantic program that uses a neural network embedding to classify shiny objects. When given an input sentence, G2L2 retrieves the lexicon entries for each word and composes their meanings based on syntax to derive the overall meaning of the sentence as an executable neuro-symbolic program. This program can then be executed on grounded inputs. To handle the exponentially-growing compositional space, we introduce a joint parsing and expected execution algorithm that reduces training time by performing local marginalization over derivations. We evaluate G2L2 in the domains of visual reasoning and language-driven navigation and find that it can generalize from limited data to understand novel combinations of words.