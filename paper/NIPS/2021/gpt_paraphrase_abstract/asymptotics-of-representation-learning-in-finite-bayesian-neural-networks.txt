Recent research suggests that finite Bayesian neural networks may perform better than infinite networks because they can adapt their internal representations. However, our understanding of how the learned hidden layer representations of finite networks differ from fixed representations of infinite networks is still incomplete. While perturbative finite-width corrections to the network prior and posterior have been studied, the asymptotics of learned features have not been fully understood. In this study, we argue that the main finite-width corrections to the average feature kernels of any Bayesian network with linear readout and Gaussian likelihood have a mostly universal form. We demonstrate this using three manageable network architectures: deep linear fully-connected and convolutional networks, and networks with a single nonlinear hidden layer. Our findings provide insight into how task-specific learning signals influence the hidden layer representations of wide Bayesian neural networks.