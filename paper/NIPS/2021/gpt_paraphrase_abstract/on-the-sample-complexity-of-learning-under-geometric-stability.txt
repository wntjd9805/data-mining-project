Supervised learning problems often involve high-dimensional data like images, text, or graphs. To effectively use this data, it is beneficial to utilize certain geometric priors, such as invariance to translations, permutation subgroups, or stability to small deformations. This study focuses on the sample complexity of learning problems where the target function exhibits these invariance and stability properties. By examining spherical harmonic decompositions of such functions on the sphere, we establish non-parametric rates of convergence for kernel methods. We demonstrate that using an invariant kernel over the group, as opposed to a non-invariant kernel, can result in improved sample complexity by a factor equal to the size of the group. These improvements are significant for large sample sizes and depend on the spectral properties of the group in terms of asymptotic behavior. Additionally, we extend these benefits to cover geometric stability to small deformations, which are modeled as subsets of permutations rather than subgroups.