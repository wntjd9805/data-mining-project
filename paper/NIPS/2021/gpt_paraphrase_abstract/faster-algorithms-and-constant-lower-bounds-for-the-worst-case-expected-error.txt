Recent research by Chen, Valiant, and Valiant (NeurIPS 2020) introduced the concept of statistical estimation without assuming the distribution of data values, but with knowledge of data collection methods. The objective is to design estimators that minimize the expected error in the worst-case scenario. This expectation is based on a known, randomized data collection process from a specific population, where the data values for each element are assumed to be worst-case. Chen, Valiant, and Valiant demonstrated that when the data values are normalized, there exists a polynomial time algorithm that can compute an estimator for the mean with a worst-case expected error within a factor of π 2 of the optimal solution, within the class of semilinear estimators. However, their algorithm involves optimizing a complex concave objective function over a constrained set of positive semidefinite matrices, and thus lacks explicit runtime guarantees beyond being polynomial in input size. In this paper, we propose efficient algorithms for approximating the optimal semilinear estimator using online convex optimization. For normalized data values, our algorithm achieves a π 2 -approximation by iteratively solving a sequence of standard SDPs. In the case of 2-normalized data values, our algorithm iteratively computes the top eigenvector of a sequence of matrices without losing any multiplicative approximation factor. Additionally, we demonstrate through experiments in scenarios where sample membership is correlated with data values (e.g., "importance sampling" and "snowball sampling") that our 2-normalized algorithm provides a similar advantage to the original ∞-normalized algorithm of Chen, Valiant, and Valiant in terms of improved estimators, but with significantly lower computational complexity. We also present a combinatorial condition that, if satisfied by a data collection process, guarantees that any estimator for the mean (not necessarily semilinear) has constant worst-case expected error.