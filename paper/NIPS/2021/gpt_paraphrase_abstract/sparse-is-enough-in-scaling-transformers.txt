Sparse variants of the Transformer model can help address the issues of high training and decoding costs. We propose Scaling Transformers, a new family of Transformer models that utilize sparse layers to efficiently scale and improve decoding speed. Remarkably, these sparse layers achieve the same perplexity as the standard Transformer model with the same number of parameters. Additionally, we combine previous sparsity techniques with attention to enable fast inference on long sequences, even with limited memory. This approach yields competitive performance in long text summarization, comparable to state-of-the-art models.