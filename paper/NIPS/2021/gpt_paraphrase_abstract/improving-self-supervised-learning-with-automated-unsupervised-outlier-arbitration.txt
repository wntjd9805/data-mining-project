Our research uncovers a flaw in current self-supervised learning methods. While these methods typically assume perfect instance level invariance, we have identified the pitfalls associated with this assumption. Specifically, we have found that the existing augmentation process used to generate multiple positive views often introduces out-of-distribution (OOD) samples that hinder the learning of downstream tasks. Simply generating diverse positive augmentations does not always benefit these tasks. To address this inherent deficiency, we propose a lightweight latent variable model called UOTA, which focuses on the issue of view sampling in self-supervised learning. UOTA dynamically searches for the most important sampling region to create views, offering a robust approach to handling outliers. Our method can be applied to various mainstream self-supervised learning approaches, regardless of the type of loss used. Through empirical experiments, we demonstrate that UOTA outperforms state-of-the-art self-supervised paradigms, highlighting the presence of the OOD sample issue in existing methods. Furthermore, we theoretically prove that our proposal leads to reduced estimator variance and bias. The code for our method is available at: https://github.com/ssl-codelab/uota.