This study focuses on minimizing regret in reinforcement learning within latent Markov Decision Processes (LMDP). In LMDP, an MDP is chosen randomly from a set of M possible MDPs, but the agent is not informed about the chosen MDP. Our research demonstrates that approximating the optimal policy in general instances of LMDPs requires a minimum of â„¦((SA)M ) episodes. However, under certain assumptions, we establish that learning effective policies can be achieved in a polynomial number of episodes. We identify the concept of separation between the MDP system dynamics as a crucial factor in this process. By ensuring sufficient separation, we propose an efficient algorithm that guarantees sublinear regret when provided with a good initialization. Additionally, we show that if we have commonly used statistical sufficiency assumptions from the Predictive State Representation (PSR) literature and a reachability assumption, the need for initialization can be eliminated.