This paper explores the use of information parallelism in online decision making problems and aims to find an efficient way to balance the exploration-exploitation trade-off. The authors propose a batch Thompson Sampling framework for stochastic multi-arm bandit and linear contextual bandit problems. Their policy achieves the same regret bound as a fully sequential approach but requires only O(log T) batch queries over a time horizon T. To achieve this reduction, the batch policy dynamically determines the duration of each batch to balance exploration and exploitation. Experimental results show that dynamic batch allocation outperforms static batch allocations.