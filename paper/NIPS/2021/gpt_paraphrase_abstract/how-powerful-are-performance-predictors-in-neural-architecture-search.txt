In the field of neural architecture search (NAS), early methods required training numerous neural networks, resulting in high computational costs. To address this, many techniques have been proposed to predict the final performance of neural architectures. However, there is a lack of consensus on evaluation metrics and optimization for different constraints. This study presents the first comprehensive analysis of 31 performance predictors, including learning curve extrapolation, weight-sharing, supervised learning, and zero-cost proxies. Various correlation- and rank-based performance measures are tested, along with the ability of each technique to accelerate predictor-based NAS frameworks. The findings offer recommendations for the best predictors in different scenarios and demonstrate the potential of combining predictor families for improved predictive power, suggesting promising research directions. The code with a library of 31 performance predictors can be accessed at https://github.com/automl/naslib.