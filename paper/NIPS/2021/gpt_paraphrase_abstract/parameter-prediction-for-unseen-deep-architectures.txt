Deep learning has been successful in automating feature design in machine learning pipelines. However, the optimization algorithms for neural network parameters are often inefficient and manually designed. This study explores the use of deep learning to directly predict these parameters by leveraging past knowledge of training other networks. We introduce a dataset called DEEPNETS-1M, consisting of various computational graphs of neural architectures, and use it to investigate parameter prediction on CIFAR-10 and ImageNet. Utilizing graph neural networks, we propose a hypernetwork that can predict efficient parameters in a single forward pass, even on a CPU, within a fraction of a second. Surprisingly, our model achieves impressive performance on diverse and unseen networks, such as predicting all 24 million parameters of a ResNet-50 with 60% accuracy on CIFAR-10. On ImageNet, some of our networks approach a top-5 accuracy of 50%. This research could potentially lead to a new paradigm of training networks that is more computationally efficient. Additionally, our model learns a robust representation of neural architectures, enabling their analysis.