Variational multi-task learning (VMTL) is a technique used to improve individual tasks by exploring their relatedness, which is especially important when there is limited data available for each task. To address this challenge, we propose a general probabilistic inference framework called VMTL. In this framework, we treat multi-task learning as a variational Bayesian inference problem and use priors to explore task relatedness. We incorporate shared knowledge into each task by designing the prior as a learnable mixture of the variational posteriors of other related tasks, using the Gumbel-Softmax technique. Our VMTL approach differs from previous methods as it can exploit task relatedness for both representations and classifiers. By jointly inferring their posteriors, individual tasks can leverage the inductive biases provided by related tasks, leading to improved overall performance. Experimental results demonstrate the effectiveness of our VMTL approach in various challenging multi-task learning scenarios with limited training data for both classification and regression. Our method consistently outperforms previous methods, including strong Bayesian approaches, and achieves state-of-the-art performance on five benchmark datasets.