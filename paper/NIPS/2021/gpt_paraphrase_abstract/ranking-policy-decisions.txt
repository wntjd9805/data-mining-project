Reinforcement Learning (RL) policies are often overly complicated, making them difficult to analyze and understand. We propose a new method for ranking the importance of decisions made by a trained RL policy based on statistical fault localization. This ranking helps explain and interpret the policy by identifying the states in the environment where important decisions are made. We evaluate the quality of the ranking by creating simplified policies through pruning unimportant decisions and measuring the impact on performance. Our experiments on various benchmarks demonstrate that pruned policies can perform just as well as the original policies. In contrast, naive approaches to ranking policy decisions based on state visit frequency do not result in high-performing pruned policies.