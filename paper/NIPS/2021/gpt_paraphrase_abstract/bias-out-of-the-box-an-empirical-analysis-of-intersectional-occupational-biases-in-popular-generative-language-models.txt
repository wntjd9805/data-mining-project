In recent years, natural language models trained on large-scale data have significantly improved in their capabilities. Open source libraries like Hugging-Face have made these models easily accessible. While previous studies have identified biases in these models, this paper focuses on biases present in the most popular versions when used without customization for specific tasks. The study specifically examines GPT-2, a widely used text generation model on HuggingFace. By analyzing occupational associations in relation to protected categories such as gender, religion, sexuality, ethnicity, political affiliation, and name origin, the researchers collected and analyzed 396K sentence completions generated by GPT-2. The findings reveal that GPT-2 tends to produce less diverse and more stereotypical job predictions for women compared to men, particularly at the intersections of multiple protected categories. The study also highlights the importance of considering intersectional interactions in occupational associations, as demonstrated by fitting 262 logistic models. Additionally, GPT-2 generally reflects the gender and ethnicity distribution found in US Labor Bureau data for most occupations, and even adjusts towards gender parity in cases where its predictions deviate from real-world observations. This raises a normative question about whether language models should reflect existing inequalities or strive to correct them.