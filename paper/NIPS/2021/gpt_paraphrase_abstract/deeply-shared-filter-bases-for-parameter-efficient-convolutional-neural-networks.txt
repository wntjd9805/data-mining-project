This study introduces a novel approach to address the challenges of parameter sharing in convolutional neural networks (CNNs). While previous methods have proposed sharing parameters across identical convolution blocks to reduce the number of parameters, they often suffer from limited representational power and the vanishing/exploding gradients problem. The authors present a recursive convolution block design and training method that separates and learns a shareable part called a filter basis. By enforcing the orthogonality of the filter basis elements, the issue of vanishing/exploding gradients is effectively controlled. The authors empirically demonstrate that their proposed orthogonality regularization improves the flow of gradients during training. Experimental results on image classification and object detection tasks reveal that their approach outperforms previous parameter-sharing methods without compromising performance. The proposed recursive convolution block design and orthogonality regularization not only prevent performance degradation but also consistently enhance representation capability while recursively sharing a significant number of parameters.