Model-based reinforcement learning (RL) algorithms for offline RL have shown promise, but rely on uncertain quantification, which can be unreliable with complex models like deep neural networks. We propose a new algorithm called COMBO that addresses this limitation by training a value function using the offline dataset and data generated from model rollouts. We also regularize the value function on out-of-support state-action tuples from model rollouts, resulting in a conservative estimate without explicit uncertainty estimation. Theoretically, COMBO satisfies a policy improvement guarantee in the offline setting. In extensive experiments, COMBO outperforms prior offline RL methods on tasks that require generalization to new tasks and matches or surpasses performance on widely studied offline RL benchmarks.