Deep neural networks have achieved impressive results in various learning tasks, but they struggle when it comes to learning continuously without forgetting previous knowledge. In contrast, humans can build upon their prior knowledge to efficiently learn new concepts without losing what they already know. In this study, we use meta-learning techniques to enable the model to learn how to learn continuously. Taking inspiration from human concept learning, we develop a generative classifier that can effectively learn new concepts even with limited data, while also preventing forgetting. Through extensive experiments on standard benchmarks, we demonstrate the effectiveness of our approach, which allows the model to remember all previous concepts without adding significant computational or structural complexity. This suggests that generative models offer a promising solution to alleviate the problem of catastrophic forgetting, a major limitation of discriminative models. The code for our method is publicly available at https://github.com/aminbana/GeMCL.