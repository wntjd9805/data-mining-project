Reinforcement learning has made significant strides in various applications, but there is still a challenge in achieving sample efficiency. Many existing methods require millions or even billions of environment steps to train effectively. While recent advances have improved sample efficiency in image-based RL algorithms, consistently achieving human-level performance on the Atari game benchmark remains difficult. In this study, we propose a model-based visual RL algorithm called EfficientZero, which builds on MuZero. Our approach achieves impressive results, surpassing human performance levels on the Atari 100k benchmark with only two hours of real-time game experience. It also outperforms the state-of-the-art SAC in some tasks on the DMControl 100k benchmark. This is a significant achievement as EfficientZero achieves super-human performance with minimal data. Furthermore, our algorithm demonstrates comparable performance to DQN at 200 million frames while using 500 times less data. The low sample complexity and high performance of EfficientZero bring RL closer to real-world applications. We have implemented our algorithm in a user-friendly manner and made it publicly available to promote further research in MCTS-based RL algorithms. Figure 1 illustrates the superior performance of EfficientZero compared to previous state-of-the-art methods, highlighting its potential for real-world applications.