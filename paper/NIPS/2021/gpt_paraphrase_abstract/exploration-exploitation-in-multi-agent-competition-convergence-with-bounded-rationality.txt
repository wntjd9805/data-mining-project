Exploration and exploitation in competitive multi-agent learning remain poorly understood. To address this, we examine smoothQ-learning, a model that explicitly considers the balance between game rewards and exploration costs. Our study demonstrates that Q-learning always converges to the unique quantal-response equilibrium (QRE), the standard solution concept for games under bounded rationality. This holds true for weighted zero-sum polymatrix games with diverse learning agents employing positive exploration rates. In contrast to previous findings on convergence in weighted potential games, we establish that Q-learning achieves rapid convergence in competitive scenarios, regardless of the number of agents and without requiring parameter adjustment. Through experiments in network zero-sum games, our theoretical findings offer crucial guarantees for developing an algorithmic solution to the unresolved problem of equilibrium selection in competitive multi-agent environments.