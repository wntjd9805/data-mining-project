Previous studies have demonstrated the effectiveness of combining provably robust loss functions, such as mean absolute error (MAE), with standard categorical loss functions, such as cross entropy (CE), to enhance learnability. In this study, we propose the utilization of Jensen-Shannon divergence as a robust loss function, which exhibits an interesting interpolation between CE and MAE through a controllable mixing parameter. Additionally, we make a crucial finding that CE lacks consistency around noisy data points. Consequently, we employ a generalized version of Jensen-Shannon divergence for multiple distributions to promote consistency around data points. By employing this loss function, we achieve state-of-the-art results on both synthetic (CIFAR) and real-world (e.g., WebVision) data with varying noise rates.