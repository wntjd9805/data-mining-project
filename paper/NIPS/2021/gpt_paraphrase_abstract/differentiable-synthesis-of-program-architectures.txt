Differentiable programs have become popular due to their interpretability, compositionality, and efficient training capabilities. However, synthesizing these programs involves optimizing a large space of program architectures, which is a computationally expensive task. Existing approaches typically enumerate the discrete search space of program architectures, leading to inefficiency. In this study, we propose a new approach that encodes program architecture search as learning the probability distribution over all possible program derivations defined by a context-free grammar. This enables our search algorithm to efficiently prune unlikely program derivations and synthesize optimal program architectures. We develop a gradient-descent based method that conducts program architecture search in a continuous relaxation of the discrete grammar rule space. Our experimental results on four sequence classification tasks demonstrate that our program synthesizer outperforms state-of-the-art methods by discovering program architectures that yield differentiable programs with higher F1 scores, while also being more efficient.