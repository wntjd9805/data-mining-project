We establish precise limits for the estimation error in bidirectional GAN (BiGAN) models, using the Dudley distance to compare the latent joint distribution and the data joint distribution. Our findings provide the first theoretical assurance for the effectiveness of the bidirectional GAN learning approach. Notably, our results do not require the reference and data distributions to have the same dimensions or bounded support, which are commonly assumed in the convergence analysis of unidirectional GANs but may not hold in practice. Furthermore, our results can be applied to the Wasserstein bidirectional GAN if the target distribution has bounded support. To prove these results, we create neural network functions that map an empirical distribution to another empirical distribution in a possibly different-dimensional space. We also introduce a novel decomposition of the integral probability metric to analyze the error in bidirectional GANs. These fundamental theoretical findings have independent significance and can be utilized in other related learning problems.