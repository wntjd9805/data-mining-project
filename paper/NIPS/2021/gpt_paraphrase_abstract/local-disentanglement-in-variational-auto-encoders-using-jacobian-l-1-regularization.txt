Unsupervised representation learning faces challenges in model identification due to rotations of the latent space, despite recent advancements in this field. Variational Auto-Encoders (VAEs) and their extensions, like Î²-VAEs, have been proven to enhance the alignment of latent variables with PCA directions, thus improving model disentanglement under specific circumstances. Drawing inspiration from Independent Component Analysis (ICA) and sparse coding, we propose incorporating an L1 loss into the VAE's generative Jacobian during training to promote local alignment of latent variables with independent factors of variation in images containing multiple objects or parts. Through various datasets, we demonstrate the effectiveness of our approach, presenting qualitative and quantitative results using information theoretic and modularity measures that verify how the added L1 cost encourages alignment of the latent representation with individual factors of variation.