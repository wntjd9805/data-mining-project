This study examines the phenomenon observed in reinforcement learning (RL) for continuous control, where trained agents tend to favor actions at the boundaries of the action space. The authors establish a theoretical connection to optimal control's bang-bang behavior and conduct extensive empirical evaluations using various RL algorithms. They propose using a Bernoulli distribution instead of the normal Gaussian distribution to create a bang-bang controller that only considers extreme actions along each dimension. Surprisingly, this approach achieves state-of-the-art performance on several continuous control benchmarks. To reduce the impact of exploration on the analysis, additional imitation learning experiments are conducted. The authors demonstrate that their findings extend to real-world environments and suggest strategies to mitigate the emergence of bang-bang solutions. These findings highlight the challenges in benchmarking continuous control algorithms, particularly in relation to potential real-world applications.