Hierarchical reinforcement learning (HRL) has the potential to efficiently learn complex tasks with long time horizons. Previous approaches have shown that allowing a higher level to assign subgoals to a lower level can enable fast learning on difficult problems. However, these subgoal-based methods were designed for static environments and struggle to handle dynamic elements that are common in real-world scenarios. This paper introduces Hierarchical reinforcement learning with Timed Subgoals (HiTS), an HRL algorithm that enables the agent to adapt its timing to a dynamic environment. HiTS not only specifies what goal state should be reached, but also when it should be reached. By communicating with the lower level using timed subgoals, the higher level faces a more stable learning problem. Experimental results on standard benchmarks and challenging dynamic reinforcement learning environments demonstrate that HiTS achieves sample-efficient learning, outperforming existing state-of-the-art subgoal-based HRL methods.