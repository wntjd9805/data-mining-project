Knowledge distillation is a widely used method for training a smaller student network to replicate a larger teacher model, like an ensemble of networks. However, our research reveals that knowledge distillation does not always achieve its intended outcome. Despite the student's ability to perfectly imitate the teacher, there often remains a notable difference between their predictive distributions. We attribute this discrepancy to optimization challenges faced by the student during training. Additionally, we demonstrate that the choice of dataset used for distillation impacts how closely the student can replicate the teacher. Surprisingly, a closer match to the teacher does not always result in improved student generalization.