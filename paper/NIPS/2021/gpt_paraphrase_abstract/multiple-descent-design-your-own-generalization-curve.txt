This study examines the generalization loss in linear regression models with varying parameterization, including both under-parameterized and over-parameterized models. The research demonstrates that the generalization curve can exhibit multiple peaks, and the positions of these peaks can be explicitly manipulated. These findings indicate that the commonly observed U-shaped generalization curve and the recently discovered double descent curve are not inherent characteristics of the model family. Rather, their appearance is a result of the interplay between data properties and the inductive biases of learning algorithms.