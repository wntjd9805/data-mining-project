Mainstream methods for unsupervised domain adaptation (UDA) aim to learn representations that are invariant across different domains in order to address the problem of domain shift. However, these approaches face theoretical challenges due to the difficulty or impossibility of achieving perfect domain invariance. A recent trend in UDA is the use of self-training, which involves training with pseudo-labels assigned to unlabeled target data. However, this study shows that under distributional shift, the pseudo-labels may be unreliable and significantly different from the true labels. To address this issue, we propose a new self-training algorithm called Cycle Self-Training (CST). CST iteratively alternates between a forward step and a reverse step until convergence. In the forward step, CST generates pseudo-labels for the target data using a classifier trained on the source data. In the reverse step, CST trains a target classifier using the pseudo-labeled data and updates the shared representations to improve the performance on the source data. We introduce a confidence-friendly regularization technique called Tsallis entropy to enhance the quality of the pseudo-labels. Theoretical analysis shows that CST can recover the true labels in challenging scenarios where both invariant feature learning and traditional self-training fail. Experimental results demonstrate that CST outperforms existing methods on visual recognition and sentiment analysis benchmarks.