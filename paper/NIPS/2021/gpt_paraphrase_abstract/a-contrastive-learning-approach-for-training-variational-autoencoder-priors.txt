Variational autoencoders (VAEs) are commonly used generative models, but they often struggle to produce high-quality images. This is particularly evident when generating samples directly from the prior distribution. One reason for this poor performance is the "prior hole problem," where the prior distribution does not align well with the approximate posterior distribution. As a result, certain areas in the latent space have a high density under the prior but do not correspond to any meaningful encoded image, leading to corrupted image outputs. To address this issue, we propose an energy-based prior that combines a base prior distribution with a reweighting factor. This helps to bring the base prior closer to the aggregate posterior, improving the generative quality. We train the reweighting factor using noise contrastive estimation and extend this approach to hierarchical VAEs with multiple latent variable groups. Our experiments demonstrate that our noise contrastive priors significantly enhance the generative performance of state-of-the-art VAEs on various datasets. This method is straightforward and can be applied to different types of VAEs to enhance the expressiveness of their prior distribution.