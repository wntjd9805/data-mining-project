We introduce a new method for enhancing interpretability in deep learning. Our framework allows for the simultaneous learning of a predictive model and an interpretation model, providing both local and global interpretability. The interpretation model utilizes high-level attribute functions that are easily understandable by humans, without sacrificing accuracy. To achieve this, we employ a dedicated architecture and carefully chosen regularization penalties. Our goal is to create a compact set of attribute functions that take the outputs of selected hidden layers as inputs and feed into a linear classifier. We enforce simplicity in attribute activation using an entropy-based criterion while ensuring fidelity to the inputs and outputs of the predictive model. Additionally, we have developed a detailed pipeline for visualizing the learned features. Furthermore, our approach can also be applied to provide post-hoc interpretations for pre-trained neural networks. We have validated our method against various state-of-the-art techniques on multiple datasets, demonstrating its effectiveness in both types of tasks.