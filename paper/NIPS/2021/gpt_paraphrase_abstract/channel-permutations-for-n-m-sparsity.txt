We propose using channel permutations to maximize the accuracy of N:M sparse networks. N:M sparsity involves having N out of M consecutive elements as zero, and it has been shown to maintain accuracy for various models and tasks through a prune and fine-tune workflow. By permuting weight matrices along the channel dimension and adjusting the surrounding layers, we achieve accuracy recovery even in small, parameter-efficient networks without impacting inference run-time. We introduce a quality metric to simplify the evaluation of permutations and efficient methods to search for high-quality permutations, including two optimizations to avoid local minima. Additionally, we provide an ablation study to demonstrate the significance of each component in our search algorithm. Experimental results show a correlation between our quality metric and the final network accuracy, improved accuracy in sparse networks with minimal impact on training time, and the transformation of unstructured to structured sparse workloads. The code for generating a 2:4 sparse network using these techniques is available at https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity.