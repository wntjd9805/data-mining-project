We propose a novel approach called Gauge Equivariant Transformer (GET) to address the issue of introducing attention to manifolds. By employing multi-head self-attention, our model is able to incorporate both position-based and content-based information while being agnostic to the orientation of local coordinate systems. In intermediate layers, we use regular field of cyclic groups as feature fields and introduce a method to parallel transport feature vectors in these fields to enhance expressive ability. Additionally, we achieve rotation invariance by projecting the position vector of each point onto its local coordinate system. Our GET model, which is the first to introduce gauge equivariance to self-attention, can be efficiently implemented on triangle meshes. Extensive experiments demonstrate that GET achieves state-of-the-art performance on two common recognition tasks.