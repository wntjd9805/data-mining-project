We propose a novel method for computing eigenvalue and singular value decompositions in proximal splitting algorithms for convex optimization problems over matrices. These algorithms often suffer from the computational burden of computing full decompositions at each iteration. Our method, based on the Jacobi method, offers several advantages: (a) it can utilize an approximate decomposition as an initial point, obtained from the previous iterate, (b) it is parallelizable, making it suitable for hardware accelerators like GPUs commonly used in machine learning, and (c) it has a simple termination criterion, allowing us to balance accuracy and computation time. Through experiments, we demonstrate that our approach significantly reduces computation time compared to standard approaches, achieving 5 to 10x speed-ups on GPUs. The effectiveness of our method is supported by theoretical results that guarantee the quality of approximations obtained using approximate decompositions in proximal operators.