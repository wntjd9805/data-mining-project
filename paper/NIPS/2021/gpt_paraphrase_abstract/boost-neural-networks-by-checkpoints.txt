Ensembling multiple deep neural networks (DNNs) can enhance predictive performance, but the associated training cost makes it impractical and inefficient. Existing methods that save and ensemble DNN checkpoints have limitations in accuracy improvement and risk of divergence. To overcome these issues, we propose a novel method that uses a boosting scheme to accelerate model convergence and maximize checkpoint diversity. Theoretical proof shows convergence by reducing exponential loss. Empirical evaluation demonstrates that our ensemble outperforms single models and existing ensembles in accuracy and efficiency. With the same training budget, our method achieves 4.16% lower error on Cifar-100 and 6.96% on Tiny-ImageNet using ResNet-110 architecture. Additionally, our method's adaptive sample weights effectively address imbalanced class distribution, leading to up to 5.02% higher accuracy on imbalanced datasets compared to single EfÔ¨ÅcientNet-B0.