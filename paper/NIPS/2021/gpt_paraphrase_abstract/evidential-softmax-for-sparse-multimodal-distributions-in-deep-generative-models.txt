Generative models often require marginalization of high-dimensional output probability distributions. Sparse normalization functions can make this computation easier, but they often require alternative loss functions for training and may collapse the multimodality of distributions. This study introduces ev-softmax, a sparse normalization function that preserves multimodality. The properties and gradient of ev-softmax are derived, and a family of approximations with full support is introduced. These approximations can be trained with probabilistic loss functions like negative log-likelihood and Kullback-Leibler divergence. The method is evaluated on various generative models and shown to outperform existing dense and sparse normalization techniques in distributional accuracy. The study demonstrates that ev-softmax effectively reduces the dimensionality of probability distributions while maintaining multimodality.