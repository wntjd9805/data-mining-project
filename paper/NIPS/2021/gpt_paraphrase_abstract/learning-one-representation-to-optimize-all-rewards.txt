We present the forward-backward (FB) representation for reward-free Markov decision processes. This representation allows us to obtain optimal policies for any given reward after an unsupervised learning phase. During this phase, we utilize off-the-shelf deep learning methods and temporal difference learning to learn two representations. In the test phase, we estimate the reward representation either from reward observations or an explicit reward description. We can directly obtain the optimal policy for that reward using these representations without planning. We assume access to an exploration scheme or replay buffer during the first phase. The unsupervised loss ensures that the policies obtained are provably optimal for any reward function if the training is perfect. However, with imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions through a predictive occupancy map, eliminating the need to synthesize states as in model-based approaches. This approach is a step towards training controllable agents in black-box stochastic environments. It performs well compared to goal-oriented RL algorithms in various scenarios, including discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. Additionally, we demonstrate the agent's ability to adapt to new tasks beyond goal-oriented RL.