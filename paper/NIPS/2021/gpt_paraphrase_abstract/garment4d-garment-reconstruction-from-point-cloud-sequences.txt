Garment reconstruction for dressing 3D human bodies in different poses is crucial. Existing approaches using 2D images face difficulties with scale and pose uncertainties. To overcome these limitations, we propose Garment4D, a principled framework that utilizes 3D point cloud sequences of dressed humans. Garment4D consists of three steps: sequential garments registration, canonical garment estimation, and posed garment reconstruction. The main challenges involve learning effective 3D features for fine details and capturing the dynamics of garments on the human body, particularly loose garments like skirts. To address these challenges, we introduce a novel Proposal-Guided Hierarchical Feature Network and Iterative Graph Convolution Network, which combine high-level semantic features and low-level geometric features for accurate reconstruction. Additionally, we propose a Temporal Transformer to capture smooth garment motions. Unlike non-parametric methods, our approach produces interpretable garment meshes that are separate from the human body, making them suitable for downstream tasks. Through extensive experiments, we demonstrate high-quality reconstruction results both qualitatively and quantitatively. The codes for our method are available at https://github.com/hongfz16/Garment4D.