Modeling dynamical systems from real-world data samples is challenging because the distribution of data can change depending on the environment in which they are captured, and the dynamics of the system can vary across different environments. Traditional approaches either treat the data as independent and identically distributed (i.i.d.) and learn a single model for all situations, or they learn environment-specific models. However, both approaches have limitations. Treating the data as i.i.d. ignores the differences between environments and leads to biased solutions, while learning environment-specific models fails to capture potential commonalities and suffers from scarcity issues.To address these limitations, we propose a novel framework called LEADS. This framework leverages the commonalities and differences among known environments to improve model generalization. It achieves this by using a tailored training formulation that captures common dynamics within a shared model, while also considering environment-specific dynamics through additional terms. We provide theoretical grounding for our approach and demonstrate that it reduces the sample complexity compared to traditional alternatives.We validate our framework by applying it to linear dynamics as a simplified case, where theory and practice align. Additionally, we instantiate the framework for neural networks and evaluate it experimentally on various families of nonlinear dynamics. Our results show that this new approach can effectively utilize knowledge extracted from environment-dependent data and improve generalization for both known and novel environments.