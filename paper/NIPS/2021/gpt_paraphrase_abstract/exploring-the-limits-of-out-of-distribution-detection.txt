Large-scale pre-trained transformers have been shown to significantly enhance the performance of deep neural networks on near out-of-distribution (OOD) tasks across various data modalities. For example, in OOD detection between CIFAR-100 and CIFAR-10, the AUROC is improved from the current state-of-the-art of 85% to 96% using VisionTransformers pre-trained on ImageNet-21k. In genomics OOD detection, the AUROC is improved from 66% to 77% using transformers and unsupervised pre-training. By applying pre-trained transformers in the few-shot outlier exposure setting, where a few examples from outlier classes are available, the AUROC of OOD detection on CIFAR-100 vs CIFAR-10 can be further improved to 98.7% with just 1 image per OOD class, and 99.46% with 10 images per OOD class. Additionally, for multi-modal image-text pre-trained transformers like CLIP, utilizing only the names of outlier classes as information without accompanying images surpasses the previous state-of-the-art on standard vision OOD benchmark tasks.