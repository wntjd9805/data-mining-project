We examine efficient distributed algorithms for principal component analysis and leading eigenvector computation on randomly distributed data across computational nodes. We introduce a quantized version of Riemannian gradient descent and demonstrate its convergence with high probability, assuming necessary spherical-convexity properties. We provide estimates for the number of transmitted bits by the algorithm with various initialization schemes, and analyze the algorithm's dependency on the problem dimension.