The aim of visual question generation (VQG) is to generate human-like questions based on an image and other relevant information. However, current methods often struggle with mapping one image to multiple questions, resulting in uninformative and non-referential questions. Recent research has shown that by using both visual and answer hints, better quality questions can be generated. However, obtaining accurate visual hints is challenging, as existing methods rely on a rule-based similarity matching method which can be noisy. In this paper, we propose a new learning approach for VQG that utilizes double-hints. We treat the salient visual regions of interest as a constraint to improve question generation. By focusing on estimating the probability of being ground-truth questions based on predicted visual hints, we indirectly measure the quality of the hints. Experimental results on benchmark datasets demonstrate that our method outperforms existing approaches in terms of various metrics, including both automatic machine metrics and human evaluation. The answer format only provides the abstraction.