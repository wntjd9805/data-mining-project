Importance Sampling (IS) is commonly used in various off-policy estimation and learning algorithms. However, recent empirical and theoretical studies have demonstrated that vanilla IS produces inaccurate estimations when the behavioral and target policies are significantly different. This paper examines the theoretical properties of the IS estimator and introduces a novel anticoncentration bound that explains its unfavorable behavior. Additionally, a new class of IS transformations is proposed, based on the concept of power mean. To the best of our knowledge, this resulting estimator is the first to satisfy two important properties: (i) it exhibits a subgaussian concentration rate, and (ii) it maintains differentiability in the target distribution. Numerical simulations are conducted on synthetic examples and contextual bandits, comparing the performance of the proposed estimator with off-policy evaluation and learning baselines.