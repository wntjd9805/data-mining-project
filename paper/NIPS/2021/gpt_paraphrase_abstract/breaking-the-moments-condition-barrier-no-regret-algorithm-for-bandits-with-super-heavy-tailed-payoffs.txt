Efforts have been made to address heavy-tailed error in machine learning, but little is known about situations where the error moments cease to exist. This occurs when the random noise η satisfies the condition Pr[|η| > |y|] ≤ 1/|y|α for some α > 0. In this study, we propose a novel statistical estimator called the mean of medians, which estimates a random variable by calculating the empirical mean of a sequence of empirical medians. Additionally, we introduce a generic algorithmic framework for solving bandit learning problems, including multi-armed and linear bandit problems. By utilizing the mean of medians estimator as a black-box filter for reward signals, we can achieve similar regret bounds as if the reward was sub-Gaussian, even in the presence of super heavy-tailed noise. We demonstrate the near-optimality of the regret bound and validate our theoretical findings through empirical experiments.