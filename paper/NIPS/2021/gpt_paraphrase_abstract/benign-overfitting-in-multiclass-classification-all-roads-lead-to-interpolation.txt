The existing research on "benign overfitting" in overparameterized models has primarily focused on regression or binary classification settings. However, the most successful applications of modern machine learning have been observed in multiclass settings. To address this discrepancy, we investigate benign overfitting in multiclass linear classification. We examine three popular training algorithms: empirical risk minimization (ERM) with cross-entropy loss, ERM with least-squares loss, and the one-vs-all SVM classifier. Our main finding is that, under a simple sufficient condition, all three algorithms produce classifiers that interpolate the training data and achieve the same accuracy. This condition holds when the data is generated from Gaussian mixtures or a multinomial logistic model, given a high enough level of effective overparameterization. Additionally, we establish new error bounds for the accuracy of the min-norm interpolating (MNI) classifier, demonstrating that all three training algorithms exhibit benign overfitting with sufficient overparameterization. Our analysis reveals that SVM solutions can achieve good generalization beyond the scope of typical margin-based bounds.