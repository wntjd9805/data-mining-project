This abstract discusses the introduction of generalized nonlinear Hebbian learning rules to account for the nonlinearities observed in biological synaptic plasticity. These rules enable a neuron to learn tensor decompositions of higher-order input correlations. The location of the nonlinearities in the plasticity rule determines the specific input correlation being decomposed and the form of the decomposition. For biologically motivated parameters, the neuron learns eigenvectors of higher-order input correlation tensors. The abstract also mentions the proof that tensor eigenvectors are attractors and provides information about their basins of attraction. Additionally, it states that any learning rule with a finite Taylor expansion into the neural input and output has stable equilibria at generalized eigenvectors of higher-order input correlation tensors. Overall, the presence of nonlinearities in synaptic plasticity allows a neuron to encode higher-order input correlations in a straightforward manner.