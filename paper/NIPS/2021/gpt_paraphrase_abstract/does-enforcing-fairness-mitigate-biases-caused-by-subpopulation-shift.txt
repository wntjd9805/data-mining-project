This paper examines the impact of enforcing algorithmic fairness during training on the performance of ML models in the target domain. It addresses the issue of algorithmic bias caused by subpopulation shifts, where certain demographic groups that are underrepresented in the training data may experience worse performance. The study explores scenarios where enforcing fairness does not improve performance and may even harm it. Additionally, the paper establishes necessary and sufficient conditions under which enforcing algorithmic fairness leads to the Bayes model in the target domain. Practical implications of the theoretical findings are demonstrated through simulations and real data analysis.