Offline reinforcement learning (RL) faces challenges due to the distributional shift between the learning policy and the given dataset. Recent methods introduce conservatism bias to encourage learning in high-confidence areas. Model-free approaches use conservative regularizations or network structures, but their constrained policy search limits generalization. Model-based approaches learn forward dynamics models and generate imaginary trajectories, but suffer from overgeneralization. To address this, we propose Reverse Offline Model-based Imagination (ROMI), which learns a reverse dynamics model and a novel reverse policy. ROMI generates rollouts leading to target goal states within the dataset, enabling conservative generalization beyond the dataset. ROMI effectively combines with model-free algorithms, achieving state-of-the-art performance on offline RL benchmark tasks.