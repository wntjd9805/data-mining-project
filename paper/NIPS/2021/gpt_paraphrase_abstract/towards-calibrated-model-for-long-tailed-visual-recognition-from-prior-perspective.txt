Real-world data often suffers from a class-imbalance problem where most labels have limited instances. This imbalance can lead to biased models that struggle with generalization and lack calibration. To address this issue, we propose two new methods. First, we introduce Uniform Mixup (UniMix), a data augmentation technique that promotes mixing in long-tailed scenarios by focusing on the minority class. Second, we identify the Bayes Bias (Bayias), an inherent bias caused by prior inconsistency, and compensate for it by modifying the standard cross-entropy loss. We demonstrate that both methods ensure classification calibration through theoretical and empirical analysis. Our experiments on CIFAR-LT, ImageNet-LT, and iNaturalist 2018 show that our strategies improve model calibration and achieve state-of-the-art performance.