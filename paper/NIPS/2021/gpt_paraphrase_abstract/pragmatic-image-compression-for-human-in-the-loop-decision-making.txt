Traditional image compression algorithms aim to minimize the number of bits required to transmit an image while preserving its overall appearance. However, for user-related tasks such as selecting a product on a shopping website, the actual information needed is often much less. In order to achieve a lower bitrate, we propose training a compression model using human-in-the-loop learning, where the model is trained to produce compressed images that elicit the same user actions as the original images. To approximate the loss function for this model, we utilize a discriminator that distinguishes between user actions in response to compressed or original images. We evaluate our approach through experiments on tasks such as reading digits, verifying faces, browsing online shopping, and playing a car racing game. Our results demonstrate that our method effectively matches user actions with and without compression at lower bitrates compared to baseline methods. Furthermore, the compression model adapts to user behavior by preserving important features such as digit number, hats, eyeglasses, perceived price, and upcoming bends in the road, while randomizing irrelevant details.