Biased datasets can negatively impact the performance of machine learning models, as they tend to make predictions based on the biased features present in the data. This lack of generalization becomes apparent when the correlations between biases and targets change. To address this issue, we introduce the Bias-Contrastive (BiasCon) loss, which utilizes bias labels within the contrastive learning framework to effectively combat bias. Additionally, we propose Bias-Balanced (BiasBal) regression, which trains classification models to align with a balanced target-bias correlation in the data distribution. To handle datasets without bias labels, we present the Soft Bias-Contrastive (SoftCon) loss, which softens the pair assignment of the BiasCon loss based on the distance in the feature space of the bias-capturing model. Our experiments demonstrate that these methods greatly enhance previous debiasing techniques across a range of realistic datasets.