This paper introduces a novel algorithm, T3A, for domain generalization (DG) that aims to enhance the robustness of a model to unknown distribution shifts. Unlike existing methods that focus on the training phase, our approach focuses on the test phase by correcting predictions during test time. T3A adjusts a trained linear classifier (the last layer of deep neural networks) through the following steps: (1) calculating a pseudo-prototype representation for each class using online unlabeled data augmented by the base classifier trained in the source domains, and (2) classifying each sample based on its distance to the pseudo-prototypes. T3A modifies only the linear layer and does not require back-propagation, resulting in minimal computational cost during inference and avoiding potential failures caused by stochastic optimization. Despite its simplicity, T3A effectively leverages knowledge about the target domain using readily available test-time data, leading to improved performance. We conducted experiments on four domain generalization benchmarks (PACS, VLCS, OfficeHome, and TerraIncognita) using various backbone networks (ResNet18, ResNet50, Big Transfer, Vision Transformers, and MLP-Mixer). The results demonstrate that T3A consistently enhances performance on unseen domains across different backbone networks and outperforms existing domain generalization methods.