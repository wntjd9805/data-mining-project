The rise of deep reinforcement learning (DRL) techniques has created a need for understanding and interpreting DRL policies. Although recent research has developed explanation methods for understanding an agent's decision-making process, they fail to capture the significance of actions and states in determining a game's final outcome. In this study, we propose a new self-explainable model that combines a Gaussian process with a customized kernel function and an interpretable predictor. Additionally, we introduce a parameter learning procedure that improves learning efficiency through the use of inducing points and variational inference. Our proposed model allows us to predict an agent's final rewards based on its game episodes and extract the importance of each time step within these episodes as strategy-level explanations. By conducting experiments on Atari and MuJoCo games, we validate the accuracy of our method in providing explanations and demonstrate its usefulness in understanding agent behavior, identifying policy vulnerabilities, addressing policy errors, and defending against adversarial attacks.