Adversarial Transferability is a fascinating property where adversarial perturbations created for one model can also be effective against another model, even if they belong to different model families or have different training processes. To enhance the protection of ML systems against adversarial attacks, this paper aims to address the following questions: What are the sufficient conditions for adversarial transferability and how can we limit it? Is there a way to decrease the transferability to improve the robustness of an ensemble ML model? To answer these questions, the authors first conduct a theoretical analysis to establish the necessary conditions for adversarial transferability between models. They then propose a practical algorithm to reduce transferability within an ensemble model, thereby enhancing its robustness. The analysis reveals that promoting gradient orthogonality alone is insufficient to ensure low transferability; model smoothness also plays a crucial role. The authors provide lower and upper bounds for adversarial transferability under specific conditions.Motivated by their theoretical findings, the authors introduce a training strategy called Transferability Reduced Smooth (TRS) ensemble. This strategy enforces both gradient orthogonality and model smoothness among base models to train a robust ensemble with low transferability. Extensive experiments on TRS demonstrate its superiority over six state-of-the-art ensemble baselines when tested against eight whitebox attacks on various datasets.In summary, this work investigates adversarial transferability and its implications for ML systems. The authors analyze the conditions for transferability and propose an algorithm to reduce it in ensemble models. Their theoretical analysis highlights the importance of both gradient orthogonality and model smoothness. The proposed TRS ensemble training strategy outperforms existing baselines in terms of robustness against adversarial attacks.