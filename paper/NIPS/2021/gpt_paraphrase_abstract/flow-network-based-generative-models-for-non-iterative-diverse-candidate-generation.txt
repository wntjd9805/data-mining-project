This paper addresses the problem of learning a stochastic policy for generating objects, such as molecular graphs, based on a sequence of actions. The objective is to generate objects with a probability proportional to a given positive reward. While standard return maximization methods tend to converge to a single high-return solution, there are scenarios where a diverse set of high-return solutions is desired, such as in black-box function optimization with limited rounds and large batches of queries. This diversity is also important in tasks like designing new molecules. Instead of using expensive and locally explorative Markov Chain Monte Carlo (MCMC) methods, the paper proposes a generative policy called GFlowNet. GFlowNet amortizes the search cost during training and enables fast generation. It leverages insights from Temporal Difference learning and views the generative process as a flow network. This approach handles cases where different trajectories can lead to the same final state, such as sequentially adding atoms to generate molecular graphs. By casting the set of trajectories as a flow and converting flow consistency equations into a learning objective, GFlowNet can generate samples from the desired distribution. The paper provides proof that any global minimum of the proposed objectives yields a policy that samples from the desired distribution. The improved performance and diversity of GFlowNet are demonstrated in experiments on a domain with multiple modes in the reward function and a molecule synthesis task.