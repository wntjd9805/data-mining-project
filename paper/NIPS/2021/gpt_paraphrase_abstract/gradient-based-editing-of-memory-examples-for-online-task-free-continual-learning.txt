We investigate task-free continual learning (CL), where a model is trained to prevent catastrophic forgetting without explicit task boundaries or identities. Memory-based approaches, which store and replay a subset of training examples, have been used in task-free CL. However, the usefulness of stored examples may decrease over time as CL models are continuously updated. To address this, we propose Gradient based Memory EDiting (GMED), a framework that uses gradient updates to edit stored examples in continuous input space. This editing creates more challenging examples for replay, resulting in increased loss during future model updates and better performance in overcoming catastrophic forgetting. GMED can be easily combined with other memory-based CL algorithms to further enhance performance. Experimental results demonstrate the effectiveness of GMED, with our best method outperforming baselines and previous state-of-the-art on five out of six datasets.