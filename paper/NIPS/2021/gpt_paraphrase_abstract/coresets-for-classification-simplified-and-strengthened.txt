We present a method for training linear classifiers using relative error coresets, which can handle various loss functions such as logistic loss and hinge loss. Our approach achieves a relative error of (1 ± ε) with approximately O(d · µy(X)2/ε2) data points. This construction is based on subsampling data points using Lewis weights. It outperforms existing bounds and performs well in practice, surpassing uniform subsampling and other importance sampling methods. Our sampling distribution is label-independent, making it suitable for active learning. Additionally, it is agnostic to the specific loss function, enabling its use in multiple training scenarios.