This paper introduces the Long-Short Transformer (Transformer-LS), a more efficient self-attention mechanism for modeling long sequences in both language and vision tasks. The Transformer-LS uses a long-range attention with dynamic projection to capture distant correlations and a short-term attention for local correlations. A dual normalization strategy is proposed to account for the scale mismatch between the two attention mechanisms. The Transformer-LS outperforms state-of-the-art models in language and vision domains, achieving better results on benchmarks and tasks such as autoregressive language modeling and ImageNet classification. It achieves these results with fewer parameters and is faster, while also being able to handle longer sequences compared to the full-attention version. Additionally, the Transformer-LS obtains state-of-the-art results on ImageNet with more scalability for high-resolution images. The source code and models are available at the provided GitHub repository.