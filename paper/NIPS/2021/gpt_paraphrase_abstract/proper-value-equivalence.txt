Model-based reinforcement learning (RL) faces the challenge of determining which aspects of the environment should be modeled. The value-equivalence (VE) principle offers a solution: a model should capture the environment aspects relevant to value-based planning. VE categorizes models based on policies and functions, whereby a model is VE if its induced Bellman operators yield correct results for the policies and functions. As the number of policies and functions increases, the set of VE models decreases until only a perfect model remains. The key question is how to select the smallest sets of policies and functions sufficient for planning. This study extends VE to order-k counterparts, creating a family of VE classes that increase with k → ∞. In the limit, all functions become value functions, resulting in proper VE (PVE). PVE may contain multiple models even when all value functions are used, but all models are sufficient for planning, producing optimal policies. A loss function for learning PVE models is developed, and it is argued that algorithms like MuZero minimize an upper bound of this loss. By leveraging this connection, a modification to MuZero is proposed, leading to improved performance in practice.