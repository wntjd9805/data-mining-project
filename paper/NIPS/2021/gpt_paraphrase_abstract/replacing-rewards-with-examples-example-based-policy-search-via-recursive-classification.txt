Reinforcement learning (RL) algorithms typically rely on users manually specifying tasks by writing down a reward function. However, this process can be time-consuming and requires technical expertise. Is it possible to develop RL algorithms that allow users to specify tasks by simply providing examples of successful outcomes? This paper presents a control algorithm that maximizes the future probability of these successful outcome examples. Unlike previous approaches that involve a two-stage process of learning a reward function and then optimizing it with another RL algorithm, our method directly learns a value function from transitions and successful outcomes, eliminating the need for an intermediate reward function. As a result, our method requires fewer hyperparameters to adjust and lines of code to debug. We demonstrate that our method satisfies a new data-driven Bellman equation, where examples replace the traditional reward function term. Experimental results indicate that our approach outperforms previous methods that rely on explicit reward functions.