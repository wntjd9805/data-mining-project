Decentralized SGD is a training method for deep learning that is gaining popularity due to its faster communication per iteration compared to parallel SGD. However, the trade-off is that decentralized SGD requires more iterations for training due to inexact averaging. To address this issue, efficient decentralized SGD requires a strategic choice of communication topology, which is an area that has not been extensively explored.This research focuses on exponential graphs, where each node is connected to approximately O(log(n)) neighbors, with n being the total number of nodes. The study demonstrates that these exponential graphs allow for both fast communication and effective averaging. Additionally, the researchers discover that a sequence of log(n) one-peer exponential graphs, where each node communicates with only one neighbor per iteration, can achieve exact averaging. This property enables the one-peer exponential graph to perform as effectively as its static counterpart while communicating more efficiently.The exponential graphs are applied in decentralized (momentum) SGD, resulting in a state-of-the-art balance between per-iteration communication and iteration complexity compared to other commonly-used topologies. Experimental results across various tasks and models demonstrate that decentralized (momentum) SGD using exponential graphs offers both fast and high-quality training.The research code has been implemented through BlueFog and is accessible at https://github.com/Bluefog-Lib/NeurIPS2021-Exponential-Graph.