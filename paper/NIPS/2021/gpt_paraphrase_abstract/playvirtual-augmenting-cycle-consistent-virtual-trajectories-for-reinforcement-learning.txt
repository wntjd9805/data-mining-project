We propose PlayVirtual, a method that improves data efficiency in deep reinforcement learning (RL) by augmenting virtual trajectories. Limited experience in RL often leads to data inefficiency during training. Our approach predicts future states based on the current state and action, and then predicts previous states using a backward dynamics model, creating a trajectory cycle. By enforcing the cycle consistency constraint, we generate a large number of virtual state-action trajectories without the need for groundtruth state supervision. We validate our method on Atari and DeepMind Control Suite benchmarks, achieving state-of-the-art performance. Our code is available at https://github.com/microsoft/Playvirtual.