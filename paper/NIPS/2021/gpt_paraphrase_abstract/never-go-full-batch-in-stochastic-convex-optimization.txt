We investigate the performance of full-batch optimization algorithms in stochastic convex optimization. These algorithms, which include gradient descent, mirror descent, and their regularized and/or accelerated versions, solely rely on the exact gradient of the empirical risk rather than individual data point gradients. Our research reveals a novel finding: while stochastic gradient descent can achieve generalization and optimize the population risk within Œµ after ùëÇ (1/Œµ2) iterations, full-batch methods require at least ‚Ñ¶(1/Œµ4) iterations or encounter a sample complexity dependent on the dimensionality.