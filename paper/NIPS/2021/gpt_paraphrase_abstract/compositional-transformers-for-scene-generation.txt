We present the GANformer2 model, an object-oriented transformer designed for generative modeling. This model incorporates strong structural priors to accurately represent the compositional nature of visual scenes and generates images through a sequential process. The model consists of two stages: a quick planning phase where a high-level scene layout is created, followed by an attention-based execution phase where the layout is refined to produce a detailed image. Unlike traditional black-box GAN architectures, our model promotes efficiency, controllability, and interpretability through its transparent design. We evaluate GANformer2 on various datasets and demonstrate its superior performance in terms of visual quality, diversity, and consistency. Additionally, our experiments reveal the model's disentanglement and provide insights into its generative process, showcasing its ability to transform rough sketches into intricate and vibrant real-world scenes. For model implementation, please refer to https://github.com/dorarad/gansformer.