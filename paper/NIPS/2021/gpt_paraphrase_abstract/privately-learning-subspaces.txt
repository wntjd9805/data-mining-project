Private data analysis faces the expensive challenge of dimensionality. Nonetheless, the data typically exhibits a hidden low-dimensional pattern. An instance of this is when gradients in gradient descent optimization tend to lie in or near a low-dimensional space. By identifying this low-dimensional structure, it becomes possible to avoid the costs associated with the high ambient dimension in terms of privacy or accuracy. In this study, we propose differentially private techniques that utilize input data obtained from a low-dimensional linear subspace, potentially with slight errors, to produce the subspace itself or an approximation of it. These algorithms can be employed as a preliminary step for other procedures.