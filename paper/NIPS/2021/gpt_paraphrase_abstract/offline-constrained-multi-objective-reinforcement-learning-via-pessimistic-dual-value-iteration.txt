We aim to learn a policy in constrained multi-objective reinforcement learning (RL) that achieves optimal performance specified by a multi-objective preference function while adhering to a constraint. Our focus is on the offline setting, where the RL agent learns the optimal policy from a given dataset. This is common in real-world applications where interactions with the environment are costly and constraint violation is risky. To address this, we convert the original constrained problem into a primal-dual formulation and solve it using dual gradient ascent. Additionally, we propose combining this approach with pessimism to overcome uncertainty in offline data, resulting in our Pessimistic Dual Iteration (PEDI) method. We provide upper bounds on suboptimality and constraint violation for policies learned by PEDI based on any dataset, proving its sample efficiency. We also specialize PEDI for linear function approximation. To the best of our knowledge, we present the first provably efficient constrained multi-objective RL algorithm using offline data without assuming dataset coverage.