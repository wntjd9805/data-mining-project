Most computational models of neural networks focus on changes in synaptic connectivity but neglect the impact of intrinsic properties. However, altering intrinsic parameters can significantly affect neural computations. In this study, we incorporate intrinsic parameters into the learning process, along with synaptic weights, to achieve specific input-output functions. By using sparse feedback signals indicating target spike times and gradient-based updates, we demonstrate that intrinsic parameters can be learned. We employ a teacher-student paradigm where a leaky integrate-and-fire or resonate-and-fire neuron learns the parameters of a teacher neuron. Our approach allows for online learning of complex temporal functions without backpropagation through time, relying solely on event-based updates. This research represents a step towards biologically inspired online learning of neural computations using ungraded and unsigned sparse feedback signals.