This study focuses on multi-task reinforcement learning (RL) in tabular episodic Markov decision processes (MDPs). The objective is to enhance collective performance by allowing a group of players to share information while facing similar but not identical MDPs. We propose an algorithm based on model transfer and analyze its effectiveness. Our analysis includes gap-dependent and gap-independent upper and lower bounds that provide insights into the complexity of the problem.