The theoretical properties of kernel k-means clustering have been extensively studied, but the existing risk bounds are not consistent with the lower bound in terms of the number of clusters and training set size. This paper focuses on the statistical properties of kernel k-means and Nyström-based kernel k-means, and proposes improved risk bounds. Using a refined upper bound of Rademacher complexity, an optimal risk bound for empirical risk minimizer is derived and extended to general cases. The statistical effect of computational approximations of Nyström kernel k-means is analyzed, showing that it achieves the same accuracy as the original method with a reduced number of landmark points. The restriction on landmark points is further relaxed under a mild condition. The theoretical findings are validated through numerical experiments.