We examine the learning dynamics of gradient descent-ascent (GDA) in continuous action zero-sum games with timescale separation. The game involves a minimizing player facing a nonconvex optimization problem and a maximizing player optimizing a Polyak-Łojasiewicz (PŁ) or strongly-concave (SC) objective. Unlike previous studies that focused on stationarity, we evaluate convergence based on game-theoretic equilibria. Through our analysis, we demonstrate that the τ -GDA system's locally stable points correspond to strict local minmax equilibria in both game classes. By exploiting timescale separation, we construct a potential function that, along with the stability characterization and an asymptotic saddle avoidance result, guarantees global asymptotic almost-sure convergence for the discrete-time GDA update to a set of strict local minmax equilibria. Additionally, we provide convergence rates for the GDA dynamics with timescale separation to approximate stationary points.