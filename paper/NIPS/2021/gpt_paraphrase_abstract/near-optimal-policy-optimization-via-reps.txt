This paper addresses the lack of performance guarantees for relative entropy policy search (REPS) when using stochastic, gradient-based solvers. The authors provide guarantees and convergence rates for the sub-optimality of a policy learned using first-order optimization methods applied to the REPS objective. They first discuss the scenario with access to exact gradients and show how near-optimality of the objective leads to near-optimality of the policy. They then tackle the scenario with stochastic gradients and introduce a technique that utilizes generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy.