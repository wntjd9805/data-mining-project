This study explores an approach to enhance model-based reinforcement learning (RL) by promoting consistency between a learned model and value function. Unlike traditional planning methods, we propose multiple self-consistency updates that go beyond updating values to match the model. Through evaluations in both tabular and function approximation scenarios, we demonstrate that self-consistency improves both policy evaluation and control in RL.