In this study, the focus is on learning an imitation policy based on demonstrated behavior from multiple environments, with the goal of being able to deploy it in an unfamiliar environment. Learning individual policies directly from observable features in each setting can lead to misleading correlations and poor generalization. However, the expert's policy is often influenced by a shared underlying structure that remains consistent across environments. To address this, a new technique called Invariant Causal Imitation Learning (ICIL) is proposed. ICIL learns a feature representation that remains invariant across domains, and then learns an imitation policy based on this representation to match the expert's behavior. To account for differences in transition dynamics, ICIL learns a shared representation of causal features that is independent of the specific noise variables in each environment. Additionally, ICIL estimates the energy of the expert's observations and uses a regularization term to minimize the energy of the imitator policy's next state, ensuring that the learned policy matches the observation distribution of the expert's policy. Experimental comparisons against various benchmarks in control and healthcare tasks demonstrate the effectiveness of ICIL in learning imitation policies that can generalize to unseen environments.