AI-based game-playing agents have shown exceptional performance in sequential games like chess, Go, and poker. However, existing multi-agent learning techniques do not consider the behavior of human players, leading to a significant performance gap. This study addresses the issue by developing artificial agents that can effectively exploit unknown human opponents in online repeated games. The agent's strategy is constrained to ensure the human player's expected utility falls within specific bounds. This framework has practical applications in human engagement and education through serious games. The study formulates linear inequalities to encode the agent's strategy conditions in each iteration to adhere to the given utility bounds for the human player. An upper confidence bound algorithm is then employed, which guarantees the satisfaction of constraints with high probability at each iteration and exhibits sublinear regret. The algorithm's convergence is empirically evaluated using standard testbeds of sequential games.