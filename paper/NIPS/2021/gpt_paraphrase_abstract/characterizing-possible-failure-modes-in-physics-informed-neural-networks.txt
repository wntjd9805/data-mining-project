Recent advancements in scientific machine learning have led to the development of physics-informed neural network (PINN) models. These models incorporate domain knowledge in the form of soft constraints on a loss function and utilize existing machine learning techniques for training. However, we have discovered that while PINN methodologies perform well for simple problems, they struggle to capture complex physical phenomena. Our analysis focuses on learning differential equations with convection, reaction, and diffusion operators, which are of great interest in the field. We have found that the soft regularization used in PINNs, which involves PDE-based differential operators, can introduce various issues, such as worsening the problem's condition. Importantly, our research reveals that these failure modes are not caused by limitations in the neural network architecture, but rather due to the difficulty in optimizing the loss landscape of PINNs. To address these challenges, we propose two promising solutions. The first involves curriculum regularization, where the loss term starts with a simple PDE regularization and progressively increases in complexity during training. The second solution suggests treating the problem as a sequence-to-sequence learning task instead of predicting the entire space-time simultaneously. Extensive testing demonstrates that these approaches significantly reduce errors compared to regular PINN training, achieving improvements of up to 1-2 orders of magnitude.