Federated Learning (FL) is becoming increasingly popular in various machine learning tasks. However, the heterogeneity of clients poses challenges for fairness, training performance, and accuracy. While efforts have been made to address statistical data heterogeneity, the diversity in processing capabilities and network bandwidth, known as system heterogeneity, has been largely ignored. Existing solutions either ignore many devices or limit the model's capacity based on the least capable participants. In this study, we propose Ordered Dropout, a mechanism that allows the extraction of smaller submodels from deep neural networks without retraining. We also demonstrate that Ordered Dropout is equivalent to SVD for linear maps. We apply this technique, along with self-distillation, in a FL framework called FjORD, which adjusts the model width to match the client's capabilities. Extensive evaluations on CNNs and RNNs across different modalities show that FjORD consistently outperforms existing baselines while maintaining its nested structure.