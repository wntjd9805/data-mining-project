To address the need for interpretable ML models, various queries and scores have been proposed. These include "anchors", which are parts of an instance that justify its classification, and "feature-perturbation" scores like SHAP. In order to develop flexible and reliable interpretability methods, there is a need for declarative languages that can specify different explainability queries. We propose using FOIL, a logic, as the foundation for such a language. FOIL allows for expressing simple but important explainability queries and can potentially be expanded into more expressive interpretability languages. We analyze the computational complexity of FOIL queries on ML models like decision trees and decision diagrams, which are considered easily interpretable. However, due to the exponential number of possible inputs, achieving tractability in FOIL evaluation requires either restricting the model structure or the evaluated fragment of FOIL. Additionally, we provide a prototype implementation of FOIL within a high-level declarative language and demonstrate its practical utility through experiments.