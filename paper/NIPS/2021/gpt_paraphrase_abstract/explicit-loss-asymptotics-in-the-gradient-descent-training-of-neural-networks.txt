Current theoretical results on optimization trajectories of neural networks trained using gradient descent typically provide rigorous but potentially loose bounds on the loss values. In this study, we adopt a different approach and demonstrate that the learning path of a wide network in a lazy training regime can be described by an explicit asymptotic behavior at large training times. Specifically, the dominant term in the asymptotic expansion of the loss follows a power lawL(t) ∼ Ct−ξ, where the exponent ξ depends solely on the data dimension, the smoothness of the activation function, and the class of functions being approximated. Our findings are derived from spectral analysis of the integral operator that represents the linearized evolution of a large network trained on the expected loss. Importantly, our techniques are not dependent on a specific data distribution (e.g., Gaussian), thereby ensuring the universality of our results.