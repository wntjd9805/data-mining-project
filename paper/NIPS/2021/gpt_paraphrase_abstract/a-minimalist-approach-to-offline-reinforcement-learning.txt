Offline reinforcement learning (RL) involves learning from a fixed set of data. To address value estimation errors caused by out-of-distribution actions, most offline RL algorithms restrict or regularize the policy using the dataset's actions. However, modifying existing RL algorithms to work offline increases complexity. Offline RL algorithms introduce new hyperparameters and often rely on additional components like generative models, while adjusting the underlying RL algorithm. In this study, we aim to enable a deep RL algorithm to work offline with minimal changes. We achieve comparable performance to state-of-the-art offline RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. This approach yields a straightforward baseline algorithm that is easy to implement and tune, while significantly reducing overall runtime by eliminating the additional computational overhead of previous methods.