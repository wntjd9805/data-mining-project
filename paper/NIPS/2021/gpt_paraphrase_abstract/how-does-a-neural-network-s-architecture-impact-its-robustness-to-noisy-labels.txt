This study investigates the impact of a network's architecture on its ability to handle noisy labels in large real-world datasets. Previous research has not extensively explored this aspect. The authors propose a formal framework that connects a network's robustness to the alignment between its architecture and the target/noise functions. They measure a network's robustness by evaluating its predictive power through a linear model trained on learned representations using a small set of clean labels. The authors hypothesize that a network is more robust to noisy labels when its architecture aligns more closely with the target function rather than the noise. The hypothesis is supported by theoretical and empirical evidence across different neural network architectures and domains. Additionally, the study finds that a well-aligned network exhibits improved predictive power in representations compared to state-of-the-art noisy-label-training methods, and it can even outperform sophisticated methods that utilize clean labels.