Understanding and designing proteins requires modeling the impact of sequence variation on their function. To achieve this, unsupervised models can be trained using sequence data, as evolution embeds functional information in protein sequences. Previous approaches have involved fitting models to groups of related sequences, which is limited as it requires training a new model for each prediction task. However, we demonstrate that protein language models can accurately capture the functional effects of sequence variation using zero-shot inference alone, without the need for experimental data or additional training. These models perform at a state-of-the-art level.