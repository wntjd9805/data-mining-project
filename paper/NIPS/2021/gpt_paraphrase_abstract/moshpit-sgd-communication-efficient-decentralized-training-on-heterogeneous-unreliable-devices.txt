Distributed training of deep neural networks on large datasets can be accelerated using multiple compute nodes. This approach requires reliable high-speed networking, typically available in dedicated clusters. However, real-world applications, like federated learning and cloud-based distributed training, often operate on devices with unstable network bandwidth. These applications are therefore limited to using parameter servers or gossip-based averaging protocols. This study proposes Moshpit All-Reduce, an iterative averaging protocol that exponentially converges to the global average. The protocol allows for efficient distributed optimization with strong theoretical guarantees. Experimental results demonstrate a 1.3x speedup for ResNet-50 training on ImageNet compared to competitive gossip-based strategies and a 1.5x speedup when training ALBERT-large on preemptible compute nodes.