This paper investigates the generalization properties of Model-Agnostic Meta-Learning (MAML) algorithms in the context of supervised learning. The study focuses on training the MAML model on multiple tasks, each consisting of a set number of data points. The paper examines the generalization error of MAML from two perspectives. Firstly, when the test task is one of the training tasks, it is shown that the expected excess population loss is bounded by O(1/mn) for strongly convex objective functions. Secondly, the generalization error of the MAML algorithm for unseen tasks is analyzed, and it is found to depend on the total variation distance between the distributions of the new task and the tasks observed during training. The proof techniques utilized in the paper leverage the connections between algorithmic stability and generalization bounds of algorithms. A new definition of stability for meta-learning algorithms is proposed, which enables the consideration of both the number of tasks (m) and the number of samples per task (n) in determining the generalization error of MAML.