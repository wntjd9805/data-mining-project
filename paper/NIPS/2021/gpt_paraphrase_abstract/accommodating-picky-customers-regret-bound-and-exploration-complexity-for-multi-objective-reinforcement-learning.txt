This paper examines the concept of multi-objective reinforcement learning with preferences. The preferences are often adversarial, such as in situations where customers have specific demands. The problem is formalized as an episodic learning problem on a Markov decision process, where the transitions are unknown and the reward function is determined by the inner product of a preference vector and pre-defined multi-objective reward functions. Two settings are considered: online setting and preference-free exploration. In the online setting, the agent receives an adversarial preference every episode and proposes policies to interact with the environment. A model-based algorithm is introduced that achieves a nearly minimax optimal regret bound. The algorithm is proven to be efficient with a nearly optimal trajectory complexity. This addresses a previously open problem.