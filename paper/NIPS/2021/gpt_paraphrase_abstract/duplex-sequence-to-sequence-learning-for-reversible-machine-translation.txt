The paper introduces REDER, a parameter-efficient model for sequence-to-sequence learning. Existing approaches for utilizing supervision signals from both directions either require two separate models or a multitask-learned model with inferior performance. REDER allows simultaneous input and output of distinct languages at either end, enabling reversible machine translation by flipping the input and output ends. Experimental results show that REDER achieves the first successful implementation of reversible machine translation, outperforming its multitask-trained baselines by up to 1.3 BLEU.