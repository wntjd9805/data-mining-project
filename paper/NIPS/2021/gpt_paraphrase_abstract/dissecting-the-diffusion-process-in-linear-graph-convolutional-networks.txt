Graph Convolutional Networks (GCNs) have gained increasing attention in recent years. A typical GCN layer includes a linear feature propagation step and a nonlinear transformation step. However, recent studies have shown that a linear GCN can achieve similar performance to the original nonlinear GCN while being significantly more computationally efficient. This paper focuses on analyzing the feature propagation steps of linear GCNs in the context of continuous graph diffusion. It investigates why linear GCNs fail to benefit from additional propagation steps. Based on this analysis, we propose a new approach called Decoupled Graph Convolution (DGC), which separates the terminal time and feature propagation steps. This decoupling makes DGC more flexible and capable of leveraging a larger number of feature propagation steps. Experimental results demonstrate that our proposed DGC significantly improves linear GCNs and enables them to compete with many modern nonlinear GCN variants. The code for DGC is available at https://github.com/yifeiwang77/DGC.