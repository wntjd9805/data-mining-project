This abstract discusses the importance of feature selection in reducing data dimension and gaining interpretability in learning and inference processes. The paper introduces an unsupervised feature selection algorithm that achieves algorithmic stability, ensuring it is not sensitive to perturbations in input samples. The algorithm consists of a feature scorer and a feature selector, utilizing a neural network to globally score features and a sub-network to locally evaluate representation abilities. The paper also presents an analysis of algorithmic stability and demonstrates its performance guarantee through a generalization error bound. Experimental results on real-world datasets show that the proposed algorithm outperforms strong baseline methods in terms of generalization performance. The theoretical analysis and stability of the algorithm-selected features are empirically confirmed.