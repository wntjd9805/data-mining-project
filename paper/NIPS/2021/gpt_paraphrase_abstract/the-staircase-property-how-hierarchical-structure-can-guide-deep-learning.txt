This paper discusses a property of data distributions called the "staircase" property, which allows deep neural networks to learn hierarchically. The property states that higher-order Fourier coefficients can be reached from lower-order coefficients along increasing chains. The authors prove that functions with this property can be learned efficiently using layerwise stochastic coordinate descent on regular neural networks. Their analysis shows that the algorithm learns high-level features by combining lower-level features greedily along the depth of the network. Experimental results using ResNet architectures with stochastic gradient descent support the theoretical findings. These findings highlight the importance of the staircase property in understanding gradient-based learning on regular networks compared to general polynomial-size networks.