Goal-conditioned reinforcement learning (RL) often faces challenges such as sparse reward and inefficient exploration in long-horizon tasks. Planning can provide dense reward/guidance, but it requires a precise environment model and may lack accuracy. To address these issues, we propose a collaborative approach called "CO-PILOT" where an RL agent and a learnable path-planner train each other on a curriculum of tree-structured sub-tasks. In CO-PILOT, the planner decomposes the long-horizon task into a tree of sub-tasks in a top-down manner. The layers of this tree represent coarse-to-fine sub-task sequences that serve as plans to complete the original task. The planning policy is trained to minimize the RL agent's cost of completing each layer, gradually increasing the difficulty of the sub-tasks and forming an easy-to-hard curriculum for the planner. A bottom-up traversal of the tree is then performed to train the RL agent from easier sub-tasks with denser rewards to harder ones. The agent's cost on each sub-task is collected to train the planner in the next episode. This mutual training process is repeated for multiple episodes before switching to a new task, ensuring that both the RL agent and planner are fully optimized to support each other's training. We evaluated CO-PILOT against RL, planning, and their combination on navigation and continuous control tasks. CO-PILOT demonstrated significant improvements in success rate and sample efficiency. Our code is publicly available at https://github.com/Shuang-AO/CO-PILOT.