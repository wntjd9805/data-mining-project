The success of BatchNorm has led to the development of various normalization layers in deep learning. However, in order to accurately predict the success or failure of these layers, the properties of BatchNorm need to be generalized. This study aims to achieve this by extending the known properties of BatchNorm to other normalization layers in randomly initialized deep neural networks (DNNs). The findings indicate that activations-based normalization layers can prevent exponential growth of activations in ResNets, but parametric techniques require explicit remedies. GroupNorm ensures informative forward propagation but results in increasingly indistinguishable activations for different samples with larger group sizes, explaining slow convergence speed in models with LayerNorm. Additionally, small group sizes lead to large gradient norm in earlier layers, explaining training instability issues in Instance Normalization and highlighting a tradeoff between speed and stability in GroupNorm. Overall, this analysis provides a unified understanding of the mechanisms underlying the success of normalization methods in deep learning, offering a framework for exploring different DNN normalization layers.