Neural module networks (NMN) are widely used for solving tasks involving both visual and textual inputs, such as visual question answering (VQA) and visual referring expression recognition (REF). However, previous implementations of NMN have a limitation in capturing the relationship between visual and textual information, which hinders their ability to generalize. For example, they struggle to understand new concepts that are combinations of known concepts. To address this limitation, this study introduces a language-guided adaptive convolution layer (LG-Conv) into NMN. This layer incorporates a language-guided kernel that adapts the convolutional filter weights to the spatial context of the textual input. By allowing the neural module to co-attend to relevant objects in both visual and textual inputs, our model improves the effectiveness of NMN. Through extensive experiments on VQA and REF tasks, we demonstrate the success of our approach. Furthermore, we propose a new challenging test split called C3-Ref+ for evaluating NMN's ability to generalize to adversarial perturbations and unseen combinations of known concepts. The experiments on C3-Ref+ further validate the generalization capabilities of our approach.