Differential Privacy (DP) is a crucial technology for protecting privacy in machine learning systems. It helps measure and limit the risk associated with an individual's involvement in a computation. However, recent findings have shown that DP learning systems can lead to bias and unfairness for different groups of people. This study investigates the causes of these disparities in the context of differentially private empirical risk minimization, focusing on two commonly used DP learning methods: output perturbation and differentially private stochastic gradient descent. The research examines the data and model properties that contribute to the unequal impacts and proposes guidelines for mitigating these effects. The proposed approach is tested on various datasets and settings.