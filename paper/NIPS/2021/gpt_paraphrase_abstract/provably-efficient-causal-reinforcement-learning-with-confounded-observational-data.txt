This study explores the integration of offline collected datasets to improve the efficiency of deep reinforcement learning (DRL) in critical scenarios like autonomous driving and personalized medicine. The challenges faced include the dependence of the observational data on unobserved random variables and the need to quantify uncertainty in exploration with both observational and interventional data. To address these challenges, the authors propose the deconfounded optimistic value iteration (DOVI) algorithm, which efficiently incorporates confounded observational data. DOVI adjusts for confounding bias and constructs a bonus based on information gain from the offline setting. The study proves that DOVI outperforms the optimal regret achievable in the pure online setting when the confounded observational data provide informative adjustments.