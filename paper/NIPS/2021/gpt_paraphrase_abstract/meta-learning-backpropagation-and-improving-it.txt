Various meta learning concepts have been proposed for neural networks, such as reprogramming fast weights, Hebbian plasticity, learned learning rules, and meta recurrent networks. Our approach, Variable Shared Meta Learning (VSML), combines these concepts and demonstrates that simple weight-sharing and sparsity in a neural network can express powerful learning algorithms in a reusable manner. By replacing the weights of a neural network with small LSTMs, a basic implementation of VSML enables the backpropagation learning algorithm to be implemented solely through forward-mode execution. It can also meta learn new algorithms that differ from online backpropagation and can generalize to datasets beyond the meta training distribution without explicit gradient calculation. Introspection reveals that our meta learned algorithms learn through fast association, which is qualitatively different from gradient descent.