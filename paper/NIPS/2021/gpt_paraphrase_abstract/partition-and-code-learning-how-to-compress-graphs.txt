This study investigates the possibility of using machine learning to compress graph data. Traditional compression algorithms face challenges in compressing graphs due to the lack of ordering. Existing graph compression methods rely on domain-specific representations and cannot adapt to different graph distributions. To overcome these limitations, the researchers propose a lossless graph compression method that follows certain principles to approach the entropy storage lower bound. Instead of assuming a specific graph distribution, they develop a probabilistic model that can be learned from data and generalize to unseen instances. Their "Partition and Code" framework involves three steps: graph partitioning, mapping subgraphs to a small dictionary with a learned probability distribution, and encoding the representation into bits using an entropy encoder. All components of the framework can be trained using gradient descent. The study theoretically compares the compression quality of different graph encodings and proves that the proposed method achieves compression gains that grow linearly or quadratically with the number of vertices under mild conditions. Empirically, the approach shows significant improvements in compressing various real-world networks.