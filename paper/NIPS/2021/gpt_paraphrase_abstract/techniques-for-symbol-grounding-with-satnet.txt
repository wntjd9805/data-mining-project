Experts argue that the future of artificial intelligence (AI) relies on integrating symbolic logical reasoning with deep learning. SATNet, a differentiable MAXSAT solver, has made a breakthrough by successfully integrating with traditional neural networks to solve visual reasoning problems. However, SATNet faces a challenge known as the Symbol Grounding Problem, as it struggles to map visual inputs to symbolic variables without explicit supervision. To address this limitation, we propose a self-supervised pre-training pipeline that enables SATNet to overcome this challenge, expanding its ability to solve problems without intermediary labels. Our method allows SATNet to achieve full accuracy even in scenarios where label leakage is not possible. Additionally, we introduce a proofreading method that further enhances the performance of SATNet architectures, surpassing the current state-of-the-art on Visual Sudoku.