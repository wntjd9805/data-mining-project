Current advanced neural models for source code are typically assessed based on their ability to generate individual code expressions and lines, but struggle with longer tasks like generating entire method bodies. To overcome this limitation, we propose a neurosym-bolic approach that incorporates weak supervision from a static program analyzer. This method allows a deep generative model to symbolically compute long-distance semantic relationships in the generated code by utilizing a static-analysis tool. During training, the model learns to generate programs conditioned on these observed relationships. We apply this approach to the task of generating complete Java methods given the remaining class. Our experiments demonstrate that our method outperforms state-of-the-art transformers and a model explicitly designed to learn program semantics in terms of producing error-free programs that syntactically match the intended output.