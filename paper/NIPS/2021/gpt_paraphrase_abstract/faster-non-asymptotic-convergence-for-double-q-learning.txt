Double Q-learning has been successful in practice for addressing the overestimation issue in Q-learning. However, the theoretical understanding of double Q-learning is limited. A recent analysis used a polynomial learning rate, resulting in a slower convergence rate. This paper focuses on the more challenging case of a constant learning rate and introduces new analytical tools to significantly improve the convergence rate. The synchronous algorithm achieves an accurate global optimum with a time complexity of ˜Ω, while the asynchronous algorithm achieves a time complexity of ˜Ω. These results greatly enhance the convergence rate in terms of key parameters. Overall, this paper is a significant step towards fully understanding the fast convergence of double-Q learning.