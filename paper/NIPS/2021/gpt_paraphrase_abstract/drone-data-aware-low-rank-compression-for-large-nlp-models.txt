Large-scale NLP models like BERT are widely used but face challenges in terms of efficiency when deployed on mobile devices due to their large size. Most operations in BERT involve matrix multiplications, which are not easily approximated using traditional matrix decompositions. However, we have observed that the learned representations in each layer of BERT exist within a low-dimensional space. Based on this finding, we propose DRONE, a data-aware low-rank compression technique that provably optimally decomposes weight matrices. DRONE can be applied to both fully-connected and self-attention layers in BERT, and can also be used on distilled BERT models to further improve compression. Experimental results demonstrate that DRONE significantly improves model size and inference speed with minimal loss in accuracy. For instance, DRONE alone achieves a 1.92x speedup on the MRPC task with only a 1.5% loss in accuracy, and when combined with distillation, it achieves over a 12.3x speedup on various natural language inference tasks.