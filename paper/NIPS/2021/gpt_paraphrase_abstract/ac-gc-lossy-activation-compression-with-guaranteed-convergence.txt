Parallel hardware devices such as graphics processor units have limited capacity for high-bandwidth memory. This limitation has a negative impact on training deep neural networks (DNNs) as it either increases the time it takes to train or decreases the accuracy when reducing the model or batch size to fit within this limited capacity. Lossy compression is a potential solution to address memory constraints, but existing methods require a search for hyperparameters to find the right balance between convergence and compression, which negates the runtime benefits. In this study, we build upon recent developments in Stochastic Gradient Descent convergence to establish an upper limit on the expected increase in loss when training with compressed activation storage. We then quantify the compression error in terms of this limit, allowing the compression rate to adapt automatically to the training conditions. Our approach, called AC-GC, offers significant compression without a significant increase in error with just a single training run, given a predetermined acceptable increase in loss. When combined with error-bounded methods, AC-GC achieves a compression rate of 15.1× with an average accuracy change of only 0.1% on text and image datasets. AC-GC is applicable to any model composed of the analyzed layers and, by avoiding the need for compression rate search, reduces overall training time by 4.6× compared to SuccessiveHalving.