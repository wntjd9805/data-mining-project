Deep multi-agent reinforcement learning (MARL) has demonstrated its potential in solving complex cooperative tasks by allowing agents to share parameters. However, this parameter sharing can result in similar agent behavior and limit their ability to coordinate effectively. This paper aims to address this limitation by introducing diversity in both the optimization and representation of shared multi-agent reinforcement learning. To achieve this, an information-theoretical regularization technique is proposed to maximize the mutual information between agents' identities and their trajectories. This encourages extensive exploration and diverse individualized behaviors. Additionally, agent-specific modules are incorporated into the shared neural network architecture, with L1-norm regularization to promote learning sharing while maintaining necessary diversity. Empirical results indicate that our proposed method achieves state-of-the-art performance on challenging tasks such as Google Research Football and super hard StarCraftII micromanagement tasks.