Efficiently estimating the performance of neural architectures is a major challenge in neural architecture search (NAS). To reduce costs, one-shot estimators (OSEs) share parameters between architectures, while zero-shot estimators (ZSEs) require no training. However, the quality of these estimations has not been thoroughly studied. In this study, we assess OSEs and ZSEs on five NAS benchmarks and analyze their biases and variances. We identify unsatisfying OSE estimations and suggest ways to mitigate the correlation gap. Our findings provide suggestions for future application and development of efficient architecture performance estimators. The analysis framework proposed in this work can also be used to understand newly designed estimators. The code is available at https://github.com/walkerning/aw_nas [24].