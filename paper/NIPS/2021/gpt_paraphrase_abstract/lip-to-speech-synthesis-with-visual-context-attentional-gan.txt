This paper introduces a novel lip-to-speech generative adversarial network called VCA-GAN. The network is capable of simultaneously modeling both local and global lip movements during speech synthesis. The VCA-GAN synthesizes speech by mapping viseme-to-phoneme using local lip visual features, while incorporating global visual context to address mapping ambiguity caused by homophene. To achieve this, a visual context attention module is proposed to encode global representations from local visual features and provide the desired global visual context to the generator through audio-visual attention. Synchronization learning is also introduced to guide the generator in synthesizing speech that aligns with the input lip movements. Experimental results demonstrate that the proposed VCA-GAN outperforms existing methods and effectively synthesizes speech from multi-speaker inputs, which has been a challenging task in previous works.