Various machine learning tasks, including knowledge base completion, ontology alignment, and multi-label classification, can benefit from incorporating differentiable representations of graphs or taxonomies into the learning process. While Euclidean space can theoretically represent any graph, recent research suggests that alternative embeddings such as complex, hyperbolic, order, or box embeddings offer better geometric properties for modeling real-world graphs. However, empirical experiments show that these benefits are more prominent in lower dimensions and diminish in higher dimensions. This study introduces a novel variation of box embeddings that utilizes a learned smoothing parameter to enhance representational capacity in low dimensions, while also avoiding performance saturation observed in other geometric models in high dimensions. The research also provides theoretical evidence that box embeddings can represent any Directed Acyclic Graph (DAG). Rigorous empirical evaluations of vector, hyperbolic, and region-based geometric representations are conducted on various synthetic and real-world directed graphs. The analysis of these results reveals correlations between different families of graphs, graph characteristics, model size, and embedding geometries, offering valuable insights into the inductive biases of differentiable graph representations.