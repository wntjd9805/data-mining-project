Modern neural networks can effectively generalize within the training distribution when provided with large amounts of data. However, they struggle to generalize systematically to data from unseen but related distributions, which requires compositional reasoning and knowledge reuse. To address this limitation, we propose Neural Interpreters, an architecture that decomposes inference in a self-attention network into modules called functions. These functions are learned end-to-end, allowing inputs to pass through a sequence of functions. The Neural Interpreters architecture enables flexible composition of computation in terms of width and depth and can easily extend its capacity after training. We evaluate the versatility of Neural Interpreters in two different scenarios: image classification and visual abstract reasoning using Raven Progressive Matrices. In image classification, Neural Interpreters achieve comparable performance to the vision transformer with fewer parameters and demonstrate transferability to new tasks with efficient sampling. In visual abstract reasoning, Neural Interpreters exhibit competitive performance in terms of systematic generalization, comparable to the current state-of-the-art approaches.