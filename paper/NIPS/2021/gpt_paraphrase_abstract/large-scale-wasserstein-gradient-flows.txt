The use of Wasserstein gradient flows is a valuable tool for understanding and solving diffusion equations. In particular, Fokker-Planck equations, which model the diffusion of probability measures, can be seen as descending gradients of entropy functionals in Wasserstein space. This concept, introduced by Jordan, Kinderlehrer, and Otto, led to the development of the JKO scheme, which approximates these diffusion processes through an implicit discretization of the gradient flow in Wasserstein space. However, solving the optimization problem associated with each JKO step is computationally challenging. To address this issue, we propose a scalable method for approximating Wasserstein gradient flows, with a focus on machine learning applications. Our approach utilizes input-convex neural networks (ICNNs) to discretize the JKO steps, which can be optimized using stochastic gradient descent. Unlike previous methods, our approach does not require domain discretization or particle simulation. This allows us to sample from the measure at each time step of the diffusion and compute its probability density. We demonstrate the effectiveness of our algorithm by computing diffusions based on the Fokker-Planck equation and applying it to tasks such as unnormalized density sampling and nonlinear filtering.