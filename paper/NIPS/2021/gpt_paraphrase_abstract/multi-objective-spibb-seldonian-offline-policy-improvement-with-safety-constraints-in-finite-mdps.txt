We investigate the issue of Safe Policy Improvement (SPI) under constraints in the offline Reinforcement Learning (RL) setting. We examine a situation where: (i) we possess a dataset collected under a known baseline policy, (ii) multiple reward signals are obtained from the environment, resulting in multiple objectives to optimize. We propose an SPI formulation for this RL setting that considers the preferences of the algorithm's user in managing trade-offs for different reward signals, while ensuring that the new policy performs at least as well as the baseline policy for each individual objective. Our approach builds upon traditional SPI algorithms and introduces a novel method called Safe Policy Iteration with Baseline Bootstrapping (SPIBB, Laroche et al., 2019), which provides reliable guarantees on the agent's performance in the actual environment. We demonstrate the effectiveness of our method in both a synthetic grid-world safety task and a real-world critical care scenario involving the development of a policy for the administration of IV fluids and vasopressors to treat sepsis.