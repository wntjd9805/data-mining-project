This study focuses on the segmentation of hands and hand-held objects based on motion. The proposed system utilizes a single RGB image and hand location as input to accurately segment the hand and the object it holds. To enable learning, responsibility maps are generated, indicating the extent to which a hand's motion explains the motion of other pixels in a video. These responsibility maps are employed as pseudo-labels for training a weakly-supervised neural network. The network is trained using an attention-based similarity loss and contrastive loss. Compared to other methods, our system demonstrates superior performance on the 100DOH, EPIC-KITCHENS, and HO3D datasets.