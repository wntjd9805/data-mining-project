Graph contrastive learning involves manipulating node representations based on their similarity or dissimilarity. This technique can be combined with a low-dimensional embedding to preserve the inherent properties of a graph. In this study, we enhance Laplacian Eigenmaps with contrastive learning, resulting in a new method called COntrastive Laplacian EigenmapS (COLES). We demonstrate that the commonly used Jensen-Shannon divergence in contrastive graph embedding models is not effective when dealing with disjoint positive and negative distributions that may arise during sampling in the contrastive setting. However, COLES minimizes a surrogate of Wasserstein distance, which is known to handle disjoint distributions well. Additionally, we prove that the loss function of COLES belongs to the block-contrastive loss family, which has been shown to outperform pair-wise losses typically used in contrastive methods. Through experiments on popular benchmarks, we compare COLES to various baselines such as DeepWalk, GCN, Graph2Gauss, DGI, and GRACE, and demonstrate its superior accuracy and scalability.