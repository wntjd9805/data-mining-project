Recently, there has been considerable interest in developing classifiers that are resistant to adversarial examples. However, a major limitation of the current robust learning framework is that it assumes a fixed robustness radius for all inputs. This overlooks the fact that data can be highly diverse, suggesting that robustness regions should vary in size across different regions of the data. To address this drawback, we propose a new classifier called the neighborhood optimal classifier, which extends the Bayes optimal classifier beyond its support by using the label of the nearest in-support point. We argue that this classifier maximizes the size of its robustness regions while maintaining accuracy equal to the Bayes optimal. Additionally, we present conditions under which general non-parametric methods, represented as weight functions, converge to this limit. We demonstrate that both nearest neighbors and kernel classifiers satisfy these conditions under specific circumstances.