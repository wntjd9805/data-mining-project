This presentation introduces a novel method for generating new video clips from existing ones using self-supervised learning. The approach incorporates contextual information and ancillary data to improve spatial resolution and realism. The prediction model operates in both the latent space of an autoencoder and the image space, ensuring temporal continuity and enforcing spatio-temporal consistency through a learnable optical flow module. Adversarial training enhances the realism of the generated output. By inserting a quantizer, the model can handle multimodal ancillary data and account for the uncertain nature of the future by generating multiple predictions. Experimental results demonstrate the effectiveness of the proposed approach across various tasks and benchmarks.