Data deletion algorithms aim to remove the impact of deleted data points from trained models in a computationally efficient manner. However, previous research in the non-convex setting only provides valid guarantees for deletion sequences that are chosen independently of the published models. This paper presents a general approach that converts guarantees for adaptive deletion sequences to guarantees for non-adaptive deletion sequences. By incorporating differential privacy and its connection to max information, along with ideas from prior work, this approach enables flexible algorithms to handle various model classes and training methodologies. The proposed method ensures strong provable deletion guarantees for adaptive deletion sequences. The paper also demonstrates the failure of previous approaches against adaptive deletion sequences and presents a practical attack on the SISA algorithm by Bourtoule et al. [2021] using CIFAR-10, MNIST, and Fashion-MNIST datasets.