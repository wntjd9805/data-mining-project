Large-scale vision and language representation learning has shown promising advancements in various vision-language tasks. Many current approaches utilize a multimodal encoder based on transformers to jointly model visual tokens (region-based image features) and word tokens. However, aligning these unaligned visual and word tokens poses a challenge for the multimodal encoder to effectively learn image-text interactions. This paper introduces a contrastive loss called ALBEF (ALign the image and text representations BEfore Fusing) that addresses this issue by incorporating cross-modal attention, resulting in more grounded vision and language representation learning. Unlike other methods, our approach does not rely on bounding box annotations or high-resolution images. To enhance learning from noisy web data, we propose momentum distillation, a self-training technique that learns from pseudo-targets generated by a momentum model. Theoretical analysis of ALBEF from a mutual information maximization perspective demonstrates that different training tasks can be viewed as generating various views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple vision-language tasks. In image-text retrieval, ALBEF outperforms methods pretrained on significantly larger datasets. Additionally, in VQA and NLVR2, ALBEF achieves absolute improvements of 2.37% and 3.84% respectively compared to the current state-of-the-art, while maintaining faster inference speed. The code and models are available at https://github.com/salesforce/ALBEF.