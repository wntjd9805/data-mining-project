We propose an adaptive version of the Condat-V˜u algorithm to solve the problem of finding a saddle point for the convex-concave objective g∗(y), where f is a convex function with a Lipschitz gradient and g is convex and possibly non-smooth. Our method combines primal gradient steps and dual proximal steps, achieving stepsize adaptivity and incorporating the norm of recently computed gradients. We demonstrate the convergence of our method with a simple rule involving the ergodic convergence rate of f. Additionally, when f is locally strongly convex and A has full row rank, our method achieves linear convergence. Numerical experiments are conducted to illustrate the practical performance of the algorithm.