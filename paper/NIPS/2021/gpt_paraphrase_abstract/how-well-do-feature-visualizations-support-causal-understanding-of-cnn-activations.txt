Understanding why units in an artificial network respond to specific stimuli is crucial for explainable artificial intelligence. One common approach is to use activation maximization to visualize unit responses. These visualizations are believed to provide precise information about the features in an image that activate a unit, giving humans an advantage over using natural dataset samples. To test this hypothesis, we conducted experiments where humans had to determine which occlusion caused a larger change in a unit's activation. The results showed that the feature visualizations by Olah et al. did help humans on this task, but they did not offer significant advantages over other visualizations. In conclusion, our objective psychophysical task found no evidence that the widely-used feature visualization method provides humans with better causal understanding of unit activations than simpler alternative visualizations.