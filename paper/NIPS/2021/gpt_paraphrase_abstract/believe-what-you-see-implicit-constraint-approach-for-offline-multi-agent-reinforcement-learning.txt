Ofﬂine learning, which involves learning from datasets without interaction with environments, is a crucial step in applying Reinforcement Learning (RL) algorithms in real-world situations. However, ofﬂine multi-agent RL, which deals with multiple agents and larger state and action spaces, poses greater challenges and has received less attention compared to its single-agent counterpart. This paper highlights the ineffectiveness of current ofﬂine RL algorithms in multi-agent systems due to the accumulation of extrapolation errors. To address this, we propose a new ofﬂine RL algorithm called ImplicitConstraint Q-learning (ICQ), which mitigates extrapolation errors by relying solely on the state-action pairs provided in the dataset for value estimation. Additionally, we extend ICQ to multi-agent tasks by decomposing the joint-policy under the implicit constraint. Experimental results demonstrate successful control of the extrapolation error within a reasonable range, independent of the number of agents. Furthermore, ICQ achieves state-of-the-art performance in challenging multi-agent ofﬂine tasks, specifically in the case of StarCraft II. The code for ICQ is publicly available at https://github.com/YiqinYang/ICQ.