Deep reinforcement learning (DRL) frameworks have shown promise in solving difficult routing problems like the traveling salesman problem (TSP) without expert knowledge. However, DRL frameworks still struggle to match the performance of state-of-the-art heuristics. This paper introduces a new approach called learning collaborative policies (LCP) that uses two iterative DRL policies: the seeder and reviser. The seeder generates diverse candidate solutions by exploring the full range of possible actions. The seeder's policy is trained using entropy regularization to encourage the discovery of diverse solutions. The reviser modifies each candidate solution by dividing it into sub-tours and minimizing the distance traveled in each sub-tour. The reviser is trained to improve the quality of the candidate solution in a reduced solution space. Experimental results show that the proposed collaborative scheme outperforms single-policy DRL frameworks on various routing problems, including TSP, PCTSP, and CVRP.