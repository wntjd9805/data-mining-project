Most advanced Graph Neural Networks (GNNs) use graph convolution through message passing between neighboring nodes. To apply GNNs to large graphs, sampling techniques have been proposed to address the problem of excessive messages. However, these methods are not suitable for GNNs that require multi-hop or global context, and they do not improve model inference speed. In this study, we introduce VQ-GNN, a novel framework that utilizes Vector Quantization (VQ) to scale up convolution-based GNNs without sacrificing performance. Instead of sampling, our approach preserves all messages passed to a mini-batch of nodes by learning and updating a small set of quantized reference vectors of global node representations within each GNN layer. By combining quantized representations with a low-rank version of the graph convolution matrix, our framework overcomes the "neighbor explosion" issue. We demonstrate the effectiveness of our compact approach theoretically and experimentally. Additionally, we develop an approximated message passing algorithm and a back-propagation rule tailored for our framework. Experimental results on various GNN backbones show that our framework achieves scalability and competitive performance in large-graph node classification and link prediction tasks.