We propose a History Aware Multimodal Transformer (HAMT) for vision-and-language navigation (VLN) tasks. Unlike existing approaches that use recurrent states for memory, HAMT incorporates a long-horizon history into decision making. It efficiently encodes past panoramic observations using a hierarchical vision transformer, considering both spatial and temporal relations between images. HAMT combines text, history, and current observation to predict the next action. We train HAMT using proxy tasks and reinforcement learning, achieving state-of-the-art performance on various VLN tasks. HAMT is particularly effective for navigation tasks with longer trajectories.