Variational auto-encoders (VAEs) are effective for unsupervised learning by using variational inference (VI) to approximate posterior inference in latent-variable models. However, the shared encoder in VAEs maps a given observation and its semantics-preserving transformation to different latent representations, leading to inconsistent encodings that affect the quality of learned representations and generalization. This paper proposes a regularization method to enforce consistency in VAEs by minimizing the Kullback-Leibler (KL) divergence between the variational distributions conditioned on the observation and its random semantic-preserving transformation. The regularization improves representation quality and generalization across various VAE variants on benchmark datasets. When applied to the nouveau VAE (NVAE), the regularization achieves state-of-the-art performance on mnist, cifar-10, and celeba datasets. Additionally, the method is applied to 3D data and demonstrates superior representation learning performance compared to the triplet loss, a popular contrastive learning-based method.