The support vector machine (SVM) and minimum Euclidean norm least squares regression are distinct methods for fitting linear models. However, they have recently been connected in models for high-dimensional data through support vector proliferation, where each training example becomes a support vector. This paper investigates the generality of this phenomenon and makes the following contributions. Firstly, a lower bound on the dimension required for support vector proliferation is proven in independent feature models, matching previous upper bounds. A sharp phase transition is identified in Gaussian feature models, with the width of this transition bounded, and experimental evidence supports its universality. It is hypothesized that this phase transition occurs only in much higher-dimensional settings in the ℓ1 variant of the SVM. Finally, a new geometric characterization of the problem is presented, which may elucidate this phenomenon for the general ℓp case.