We propose FedLin, a new algorithmic framework for federated learning (FL) that addresses the challenges of objective heterogeneity, systems heterogeneity, and infrequent and imprecise communication. Existing FL algorithms suffer from a trade-off between speed and accuracy, either converging quickly to an incorrect point or converging to the global minimum at a slower rate. In contrast, FedLin guarantees linear convergence to the global minimum even with arbitrary heterogeneity. We establish upper and lower bounds on the convergence rate of FedLin that account for infrequent communication. We also demonstrate that FedLin maintains linear convergence rates with aggressive gradient sparsity, and quantify the impact of compression on convergence. Our work provides the first tight guarantees for linear convergence and comprehensively analyzes gradient sparsity in FL.