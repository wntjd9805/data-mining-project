We divide the assessment of meta-learning into two scenarios: in-distribution (ID) and out-of-distribution (OOD). The ID scenario involves sampling train and test tasks from the same underlying task distribution, while the OOD scenario does not. Although most meta-learning theory and some few-shot learning (FSL) applications focus on the ID scenario, we observe that existing few-shot classification benchmarks predominantly reflect the OOD evaluation. This discrepancy poses a problem as meta-learning methods that excel on OOD datasets may perform significantly worse in the ID scenario, as demonstrated by our experiments on various benchmarks. Moreover, in the OOD scenario, we identify concerns related to model selection and performance comparison between different methods in current FSL benchmarks. To address these concerns, we propose suggestions for constructing FSL benchmarks that enable both ID and more reliable OOD evaluation. Our objective is to raise awareness within the meta-learning community about the significance and distinction between ID and OOD evaluation, as well as the intricacies of OOD evaluation using existing benchmarks.