This study addresses the issue of safety violations during the training phase of reinforcement learning algorithms. The objective is to develop safe RL algorithms that do not violate safety measures during training, even when starting with a policy that has trivial rewards and no prior knowledge of the dynamics or additional offline data. The proposed algorithm, called CRABS, learns barrier certificates, dynamics models, and policies iteratively. The barrier certificates are learned through adversarial training and ensure the policy's safety based on calibrated learned dynamics. A regularization term is also introduced to encourage larger certified regions for better exploration. Empirical simulations demonstrate the difficulty of achieving zero safety violations in simple environments with low-dimensional state spaces, especially when high-reward policies need to approach the safety boundary. Existing methods typically require numerous violations to achieve satisfactory rewards, whereas the proposed algorithm achieves zero violations.