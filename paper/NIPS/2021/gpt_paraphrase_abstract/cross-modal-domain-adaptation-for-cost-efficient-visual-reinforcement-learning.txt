To bridge the gap between simulated and real-world images in visual-input sim-to-real scenarios, domain adaptation is a promising approach. Existing methods focus on adapting within the same modality, which requires high-quality image rendering and can be costly. This paper proposes a more cost-effective approach, where only low-dimensional states are simulated. However, previous methods for aligning representation spaces suffer from an ill-posed objective, particularly when crossing modalities. To address this, the proposed algorithm, CODAS, leverages the sequential structure of data sampling in RL tasks. Experimental results demonstrate that agents deployed with CODAS achieve similar performance to the source domain, while those deployed with previous methods for same-modal adaptation experience a larger performance gap.