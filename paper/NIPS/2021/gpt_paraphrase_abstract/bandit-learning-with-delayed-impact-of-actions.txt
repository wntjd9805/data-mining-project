This study addresses the issue of delayed impact of actions in a stochastic multi-armed bandit (MAB) problem. The authors highlight that actions taken in the past can have a significant influence on future rewards. They illustrate this phenomenon with the example of loan applications, where a history of rejections for a specific group can perpetuate a cycle of loan denials. The objective of this paper is to incorporate and understand this delayed and long-term impact within the MAB framework, considering the bias created by action history during learning. The authors propose an algorithm that minimizes regret and achieves a regret of ˜ (KT 2/3). They also establish a lower bound of ⌦(KT 2/3) to demonstrate the algorithm's performance. The findings of this research contribute to the bandit literature by providing techniques to handle actions with long-term consequences and have implications for designing fair algorithms.