The Variational Autoencoder (VAE) is a deep learning model used for unsupervised learning that encodes observations into a meaningful latent space. However, VAEs are prone to catastrophic forgetting when faced with sequential tasks and only have access to data from the current task. This study aims to address this issue by focusing on the choice of the prior distribution over the latent space, which is crucial for VAEs in non-sequential settings. The authors propose learning an approximation of the aggregated posterior as a prior for each task. This approximation is parameterized as an additive mixture of distributions generated by the encoder, evaluated at trainable pseudo-inputs. A greedy boosting-like approach with entropy regularization is employed to learn the components, encouraging diversity among them to memorize the current task with as few components as possible. An end-to-end approach for continual learning of VAEs is introduced based on this learnable prior. The proposed method is evaluated on various datasets and successfully prevents catastrophic forgetting in a fully automatic manner.