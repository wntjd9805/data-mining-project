Iteratively reweighted least square (IRLS) is a widely used method in machine learning for solving regression problems with sparsity constraints. However, current approaches often rely on specific pruning schemes. In this study, we propose a simple re-parametrization of IRLS combined with a bilevel resolution, which outperforms existing methods across various sparsity constraints, regularization strengths, and design matrices. Unlike IRLS, our method minimizes a smooth function, although it is non-convex. We demonstrate that there are no spurious minima and saddle points can be traversed, ensuring a descent direction. We suggest using a BFGS quasi-Newton solver, which makes our approach simple, robust, and efficient. We conduct a numerical benchmark comparing the convergence speed of our algorithm with state-of-the-art solvers for different types of problems, such as Lasso, group Lasso, trace norm, and linearly constrained problems. The results demonstrate the versatility of our approach, eliminating the need for different solvers depending on the specific machine learning problem being studied.