Self-supervised speech representation learning (speech SSL) has proven the value of large-scale training in developing robust representations for Automatic Speech Recognition (ASR) when paired data is limited. This study explores the potential of sparse subnetworks within pre-trained speech SSL models to further enhance low-resource ASR performance. However, conventional pruning methods like the Lottery Ticket Hypothesis (LTH) are computationally expensive and offer minimal performance improvement compared to the original dense network. To address this, we propose Prune-Adjust-Re-Prune (PARP), a method that identifies and fine-tunes subnetworks for significantly improved performance with just one downstream ASR finetuning run. Our surprising observation is that subnetworks pruned for pre-training tasks only require slight adjustments to achieve substantial performance boosts in downstream ASR tasks. Extensive experiments on low-resource ASR confirm the existence of sparse subnetworks in mono-lingual/multi-lingual pre-trained speech SSL models, and demonstrate the computational advantage and performance gains of PARP over baseline pruning methods. Notably, PARP achieves an absolute 10.9%/12.6% Word Error Rate (WER) reduction compared to the full model on the 10-minute Librispeech split without language model decoding. Additionally, we showcase the effectiveness of PARP in cross-lingual pruning without compromising phone recognition, discovering a multi-lingual subnetwork for 10 spoken languages in a single finetuning run, and its applicability to pre-trained BERT/XLNet for natural language tasks.