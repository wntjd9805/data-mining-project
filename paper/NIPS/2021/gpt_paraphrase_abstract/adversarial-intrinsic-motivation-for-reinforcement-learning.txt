This paper explores the effectiveness of utilizing the Wasserstein-1 distance as an objective for reinforcement learning tasks. Specifically, it focuses on goal-conditioned reinforcement learning, where the target distribution is unachievable and concentrated at the goal. The paper introduces a quasimetric specific to Markov Decision Processes (MDPs) and uses it to estimate the Wasserstein-1 distance. It demonstrates that the policy minimizing this distance achieves the goal in the fewest steps possible. The approach, called Adversarial Intrinsic Motivation (AIM), estimates the distance through its dual objective and uses it to calculate a supplemental reward function. Experimental results show that this reward function changes smoothly with MDP transitions, effectively guiding the agent's exploration towards finding the goal efficiently. The paper also combines AIM with Hindsight Experience Replay (HER) and shows that this combined algorithm significantly accelerates learning on various simulated robotics tasks compared to other exploration-focused or learning-accelerating reward functions.