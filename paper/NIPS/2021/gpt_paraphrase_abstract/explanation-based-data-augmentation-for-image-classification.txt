Current studies have provided explanations for the decisions made by deep neural networks in order to gain insights into their behavior. We have observed that these explanations can also be utilized to identify the concepts responsible for misclassifications. This enables us to comprehend the potential limitations of the dataset used for model training, specifically the regions that are not adequately represented in the dataset. In this research, we introduce a framework that employs concept-based explanations to automatically enhance the dataset by incorporating new images that cover these under-represented regions, thereby enhancing the model's performance. The framework can utilize explanations generated by both interpretable classifiers and post-hoc explanations from black-box classifiers. Experimental results demonstrate that this approach enhances classifier accuracy compared to current augmentation strategies.