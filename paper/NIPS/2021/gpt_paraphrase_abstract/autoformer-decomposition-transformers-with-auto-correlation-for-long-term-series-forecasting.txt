This study addresses the need for extending forecasting time in real applications like weather warnings and energy planning. Previous models based on Transformers have limitations in discovering reliable dependencies and suffer from information utilization bottlenecks. To overcome these issues, the authors propose a novel architecture called Auto-former, which incorporates an Auto-Correlation mechanism. This mechanism leverages series periodicity to improve efficiency and accuracy in discovering dependencies. The Autoformer outperforms self-attention in long-term forecasting, achieving a 38% relative improvement across various benchmarks in energy, traffic, economics, weather, and disease. The code for Autoformer is available at the provided repository link.