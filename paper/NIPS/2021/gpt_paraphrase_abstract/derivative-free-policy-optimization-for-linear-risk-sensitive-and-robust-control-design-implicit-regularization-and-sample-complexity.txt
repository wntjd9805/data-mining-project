This study explores the convergence theory of policy gradient (PG) methods for learning linear risk-sensitive and robust controllers in continuous control tasks. The researchers develop PG methods that can be implemented without derivatives by sampling system trajectories. They establish global convergence and sample complexity results for solving two fundamental settings in risk-sensitive and robust control. Additionally, the study provides the first sample complexity for the global convergence of PG methods in solving zero-sum linear-quadratic dynamic games, a nonconvex-nonconcave minimax optimization problem used in multi-agent reinforcement learning. The algorithms developed in this study preserve a certain level of robustness/risk-sensitivity of the controller during the learning phase, which is crucial for safety-critical control systems.