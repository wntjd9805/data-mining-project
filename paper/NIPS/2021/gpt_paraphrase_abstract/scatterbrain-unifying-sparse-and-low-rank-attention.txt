Efficient Transformers have made progress by utilizing the sparsity or low-rank properties of attention matrices to address computational and memory limitations in modeling long sequences. However, finding the right balance between model quality and efficiency remains a challenge when attempting to create a universal approximation for different tasks. To gain a better understanding of this trade-off, it has been observed that sparse and low-rank approximations perform well in different scenarios, as determined by the softmax temperature in attention. It has also been found that combining sparse and low-rank techniques can outperform either method alone. Drawing inspiration from the robust-PCA algorithm for sparse and low-rank decomposition, a new approach called Scatterbrain has been proposed. Scatterbrain combines sparse approximation (using locality sensitive hashing) and low-rank approximation (using kernel feature map) to achieve accurate and efficient approximation, with an unbiased estimation and low error. Empirical results demonstrate that Scatterbrain outperforms existing methods in tasks such as BigGAN image generation and pre-trained T2T-ViT, achieving significantly lower error rates. Additionally, Scatterbrain reduces attention memory by 98% without fine-tuning on a pre-trained T2T Vision transformer, with only a 1% decrease in accuracy. Furthermore, Scatterbrain improves perplexity and average accuracy by up to 4 and 5 points respectively, compared to sparse or low-rank efficient transformers, in end-to-end training for language modeling and long-range-arena tasks.