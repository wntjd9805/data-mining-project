The Lottery Ticket Hypothesis (LTH) focuses on finding sparse trainable subnetworks, or winning tickets, that can achieve similar or better performance than full models. The most effective method for identifying winning tickets is Iterative Magnitude-based Pruning (IMP), but it is computationally expensive and needs to be repeated for each different network. This raises the question of whether we can transfer a winning ticket from one network to another with a different architecture, without redoing the expensive pruning process. This question is not only relevant for efficient ticket finding, but also for understanding scalable sparse patterns in networks. We conducted experiments on CIFAR-10 and ImageNet, and proposed strategies to modify winning tickets from different networks of the same model family (e.g., ResNets). Based on our results, we introduce the Elastic Lottery Ticket Hypothesis (E-LTH), which suggests that by replicating, dropping, and re-ordering layers in a network, we can stretch or squeeze its winning ticket to create a subnetwork for another network from the same family, with similar performance to the original winning ticket found by IMP. We compared E-LTH with other pruning and training methods, and discussed its generalizability across different model families, layer types, and datasets. The code is available at https://github.com/VITA-Group/ElasticLTH.