We examine risk-sensitive reinforcement learning (RL) using the entropic risk measure. While previous research has provided regret guarantees for this problem, there is a significant gap between the upper and lower bounds. We identify the shortcomings in existing algorithms and their analysis that result in this gap. To address these issues, we explore a simple transformation of the risk-sensitive Bellman equations, known as the exponential Bellman equation. This equation inspires us to develop a new analysis of Bellman backup procedures in risk-sensitive RL algorithms and also drives the creation of a novel exploration mechanism. By combining these analytical and algorithmic innovations, we are able to achieve improved upper bounds on regret compared to existing methods.