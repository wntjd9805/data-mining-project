We introduce two novel algorithms, called FedDR and asyncFedDR, to address a key nonconvex composite optimization problem in federated learning. Our approaches combine a nonconvex Douglas-Rachford splitting method, randomized block-coordinate strategies, and asynchronous implementation. They can also accommodate convex regularizers. Unlike recent methods like FedSplit and FedPD, our algorithms update only a subset of users during each communication round, potentially in an asynchronous manner, enhancing their practicality. These algorithms are capable of handling statistical and system heterogeneity, the primary challenges in federated learning, and achieve the best-known communication complexity. In fact, our algorithms match the lower bound of communication complexity up to a constant factor, assuming standard conditions. Our numerical experiments demonstrate the superiority of our methods over existing algorithms on both synthetic and real datasets.