Policy networks play a crucial role in deep reinforcement learning algorithms for continuous control as they allow for the estimation and sampling of valuable actions. From a variational inference perspective, policy networks, when combined with entropy or KL regularization, serve as a type of amortized optimization. However, relying solely on direct amortized mappings can lead to suboptimal policy estimates and restricted distributions, thereby limiting performance and exploration. To address this issue, we propose the use of iterative amortized optimizers, which offer more flexibility. Our experiments show that this approach, known as iterative amortized policy optimization, outperforms direct amortization on various continuous control tasks. The accompanying code for our technique can be found at github.com/joelouismarino/variational_rl.