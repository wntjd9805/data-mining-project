The step-size used in stochastic gradient descent plays a crucial role in determining its convergence, particularly for non-convex problems like neural network training. Although step decay schedules (where the step-size is constant and then reduced) are commonly employed due to their favorable convergence and generalization properties, their theoretical characteristics are not yet fully understood. This study presents convergence results for step decay in the non-convex scenario, showing that the gradient norm diminishes at a rate of O(ln T /T). Additionally, the paper offers near-optimal convergence guarantees for a wide range of problems, including non-smooth, convex, and strongly convex ones. The effectiveness of the step decay step-size is demonstrated through successful application in various large-scale deep neural network training tasks.