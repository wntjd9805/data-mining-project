This study investigates the issue of positional information in attention-based Transformer neural networks. These networks are permutation-invariant without positional information. To address this, two popular methods of incorporating positional information are discussed: absolute and relative positional embeddings. However, absolute positional embeddings suffer from generalization problems with longer sequences, while relative positions are more complex and less efficient.   To overcome these limitations, the authors propose an augmentation-based approach called CAPE for absolute positional embeddings. CAPE combines the advantages of both absolute (simplicity and speed) and relative positional embeddings (better generalization). Experimental evaluations on various models in machine translation, image recognition, and speech recognition demonstrate that CAPE improves generalization performance and stability in training hyper-parameters.