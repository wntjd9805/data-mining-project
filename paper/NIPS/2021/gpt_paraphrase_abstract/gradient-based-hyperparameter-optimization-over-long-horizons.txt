Gradient-based hyperparameter optimization is popular in few-shot meta-learning but impractical for tasks with long horizons due to memory scaling and gradient degradation issues. To address this, we propose forward-mode differentiation with sharing (FDS), an efficient algorithm that tackles memory scaling issues using forward-mode differentiation and gradient degradation issues by sharing contiguous hyperparameters. We provide theoretical guarantees on noise reduction and demonstrate the efficiency of FDS by differentiating through 104 gradient steps of unrolled optimization. In experiments on CIFAR-10, FDS outperforms greedy gradient-based alternatives and achieves a 20x speedup compared to state-of-the-art black-box methods. Code is available at: https://github.com/polo5/FDS.