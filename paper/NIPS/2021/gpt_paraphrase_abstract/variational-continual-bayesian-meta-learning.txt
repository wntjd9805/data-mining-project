This paper proposes the Variational Continual Bayesian Meta-Learning (VC-BML) algorithm for handling a more complex online setting where tasks arrive sequentially and follow a non-stationary distribution. To adapt to diverse tasks and avoid negative knowledge transfer, VC-BML maintains a Dynamic Gaussian Mixture Model for meta-parameters. The number of component distributions is determined using a Chinese Restaurant Process, allowing for a larger parameter space. In order to infer the posteriors of model parameters and avoid forgetting knowledge, a more robust posterior approximation method called structured variational inference is developed. Experimental results on tasks from non-stationary distributions demonstrate that VC-BML outperforms other methods in transferring knowledge and mitigating catastrophic forgetting in an online setting.