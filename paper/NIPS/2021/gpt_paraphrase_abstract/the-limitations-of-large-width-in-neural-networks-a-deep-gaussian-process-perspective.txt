Recent research in deep learning has focused on the question of whether wider networks outperform narrower ones. However, answering this question has been challenging due to the potential masking effects of width on the representational power of conventional networks. In this paper, we analyze the impact of width on neural networks by utilizing Deep Gaussian Processes (Deep GP), a class of hierarchical models that encompass neural nets. By decoupling capacity and width through Deep GP, we aim to understand how width affects neural networks once they have enough capacity for a given modeling task. Our theoretical and empirical findings indicate that large width can be detrimental to hierarchical models. Surprisingly, we demonstrate that even nonparametric Deep GP converges to Gaussian processes, effectively becoming shallower without increasing representational power. The posterior, which consists of a mixture of data-adaptable basis functions, becomes less dependent on data with width. Our analysis reveals that depth and width have opposite effects: depth enhances a model's non-Gaussianity, while width increases its Gaussian nature. We identify a "sweet spot" for maximum test performance before the limiting Gaussian process behavior hinders adaptability, which occurs at width = 1 or width = 2 for nonparametric Deep GP. These results offer strong predictions about the same phenomenon in conventional neural networks trained with L2 regularization, suggesting that such networks may require 500-1000 hidden units for sufficient capacity depending on the dataset, with further width leading to performance degradation.