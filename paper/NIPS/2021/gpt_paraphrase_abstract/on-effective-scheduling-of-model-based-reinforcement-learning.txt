Model-based reinforcement learning has become popular for its efficient use of samples. However, determining the appropriate hyperparameter schedule for optimal performance remains uncertain. This paper focuses on the real data ratio for policy optimization in Dyna-style model-based algorithms. Through theoretical analysis, it is found that gradually increasing the real data ratio leads to improved performance. Based on this insight, a framework called AutoMBPO is introduced to automatically schedule the real data ratio and other hyperparameters in the MBPO algorithm. Experimental results on continuous control tasks demonstrate that the AutoMBPO-trained MBPO outperforms the original version, and the real data ratio schedule determined by AutoMBPO aligns with the theoretical analysis.