Despite recent successes in reinforcement learning (RL), many RL algorithms are still sensitive to hyperparameters. To address this issue, researchers have become interested in AutoRL, which aims to automate design decisions for more versatile algorithms. Population based approaches have shown promise as AutoRL algorithms by learning hyperparameter schedules on the fly. The PB2 algorithm, for example, achieves strong performance in RL tasks by formulating online hyperparameter optimization as a time-varying GP-bandit problem and providing theoretical guarantees. However, PB2 is limited to continuous hyperparameters, which restricts its practicality. In this paper, we propose a new efficient hierarchical approach for optimizing both continuous and categorical variables. We introduce a new time-varying bandit algorithm specifically designed for the population based training regime. We evaluate our approach on the challenging Procgen benchmark and demonstrate that explicitly modeling the dependence between data augmentation and other hyperparameters enhances generalization.