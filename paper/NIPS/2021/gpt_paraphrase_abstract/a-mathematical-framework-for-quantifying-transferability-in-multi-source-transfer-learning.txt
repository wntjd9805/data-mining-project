The current focus of transfer learning algorithms is on the similarities between the source and target tasks, without adequately considering the sample sizes of these tasks. This study presents a mathematical framework for quantifying transferability in multi-source transfer learning problems, considering both task similarities and sample complexity. The proposed framework involves linearly combining models learned from different tasks to learn the target task, using optimal combining coefficients to measure transferability. The study derives an analytical expression for this transferability measure, incorporating sample sizes, model complexity, and task similarities. The analyses are applied to practical learning tasks, establishing a quantifiable transferability measure using a parameterized model. An alternating iterative algorithm is developed to implement the theoretical results for training deep neural networks in multi-source transfer learning. Experimental results on image classification tasks demonstrate that the proposed approach outperforms existing transfer learning algorithms in multi-source and few-shot scenarios.