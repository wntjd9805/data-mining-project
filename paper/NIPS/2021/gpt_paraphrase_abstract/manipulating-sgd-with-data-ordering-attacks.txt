Machine learning is susceptible to various attacks, including poisoning the model or introducing backdoors by altering the data distribution. This paper introduces a new type of training-time attack that does not require modifying the dataset or model architecture, but simply changes the order in which data is fed to the model. The attacker can hinder the model's learning process or manipulate it to adopt desired behaviors. Even a single adversarially-ordered epoch can significantly slow down or reset the learning progress. These attacks exploit the stochastic nature of modern learning procedures and are not limited to specific models or datasets. Extensive evaluations on computer vision and natural language benchmarks demonstrate that these attacks can disrupt model training and potentially introduce backdoors.