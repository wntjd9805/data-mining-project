Test-time training (TTT) using self-supervised learning (SSL) is a new approach to address distributional shifts. However, its effectiveness is not well understood. This study explores the limitations of TTT and shows that it can actually worsen test-time performance in the presence of significant distribution shifts. To overcome this problem, the researchers propose a test-time feature alignment strategy that uses offline feature summarization and online moment matching to regularize adaptation without revisiting training data. They also enhance this strategy in the online setting by decoupling batch and queue, enabling robust moment estimates even with limited batch size. By analyzing the performance of TTT after adaptation, the researchers highlight its potential and justify the use of more informative self-supervision, specifically contrastive learning, for visual recognition tasks. Experimental results demonstrate that their modified version of TTT, called TTT++, outperforms existing methods by a significant margin on multiple benchmarks. This suggests that incorporating additional information, in addition to model parameters, can improve test-time adaptation. The code for this study is available at https://github.com/vita-epfl/ttt-plus-plus.