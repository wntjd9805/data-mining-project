We have developed a framework to understand the internal workings of continuous deep learning models by reducing their network architectures. Our experiments demonstrate that pruning enhances the performance of neural ordinary differential equations (ODEs) in generative modeling by preventing mode-collapse and smoothing the loss surface. Additionally, we find that pruning leads to more efficient neural ODE representations with significantly fewer parameters, up to 98% reduction, without sacrificing accuracy. We believe that our findings will encourage further investigation into the trade-offs between performance and model size in contemporary continuous-depth models.