Adversarial training is commonly used to create models that can withstand small adversarial changes. However, it is also important for machine learning practitioners to have models that can handle other types of natural changes, such as variations in style or lighting in input images. These changes in input distribution can be effectively represented as shifts in the mean and variance of deep image features. We propose a modification to adversarial training where instead of perturbing the pixels of an image, we directly perturb the statistics of its features. This approach allows us to create models that are robust to unseen distributional shifts. We visualize these perturbations and observe their relationship with distributional shifts. Our method, called Adversarial Batch Normalization (AdvBN), involves adding a single network layer that generates worst-case feature perturbations during training. By fine-tuning neural networks using these adversarial feature distributions, we achieve improved robustness to various unseen distributional shifts, including style variations and image corruptions. Additionally, we demonstrate that our adversarial feature perturbation can complement existing data augmentation methods in the image space, resulting in enhanced performance. The source code and pre-trained models can be accessed at https://github.com/azshue/AdvBN.