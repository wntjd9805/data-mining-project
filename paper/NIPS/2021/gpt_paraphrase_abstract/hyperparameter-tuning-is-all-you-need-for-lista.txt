The Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) is a method that trains an iterative algorithm as a neural network and has been successful in sparse recovery. This paper introduces the concept of adding momentum to intermediate variables in the LISTA network, which improves its convergence rate. The network with instance-optimal parameters shows superlinear convergence. Additionally, the paper presents new theoretical results that allow for the automatic and adaptive calculation of parameters in a LISTA network layer based on its previous layers. This adaptive-parameter procedure reduces the training of LISTA to tuning only three hyperparameters, setting a new record for simplicity. The result is a lightweight network called HyperLISTA. HyperLISTA performs almost as well as state-of-the-art LISTA models on known data distributions and outperforms them on unknown distributions with different sparsity levels and nonzero magnitudes. The code for HyperLISTA is available at https://github.com/VITA-Group/HyperLISTA.