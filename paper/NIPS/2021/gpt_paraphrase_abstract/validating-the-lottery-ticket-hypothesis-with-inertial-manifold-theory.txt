Traditional network pruning techniques often rely on manual heuristics to generate sparser networks, but it is challenging to ensure that these pruned networks perform as well as the original dense networks. Recent research has identified the Lottery Ticket Hypothesis (LTH), which suggests that randomly-initialized dense neural networks contain extremely sparse subnetworks that can achieve similar accuracy. However, empirical evidence alone is not enough, and multiple rounds of training and pruning are typically required to find these sparse subnetworks with low accuracy loss. This study uses dynamical systems theory and inertial manifold theory to theoretically validate the LTH. By formulating the neural network optimization problem as a gradient dynamical system and reducing it onto inertial manifolds, the researchers obtain a lower-dimensional system for pruned subnetworks. They demonstrate the existence and precondition of pruned subnetworks and propose pruning the original networks based on the gap in their spectrum, resulting in subnetworks with the smallest dimensions. This approach explores the potential for theoretically lossless and one-time pruning, distinguishing it from existing pruning techniques and LTH methods.