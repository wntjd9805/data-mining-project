Policy gradient methods often struggle when faced with high-dimensional action-spaces or multiple objectives. This is due to the quadratic scaling of variance on score-based gradient estimators. To address this issue, we propose a factor baseline that utilizes the independence structure encoded in a new action-target influence network. Our approach, called factored policy gradients (FPGs), offers a unified framework for analyzing leading algorithms, extends traditional policy gradients, and enables the incorporation of prior knowledge about a problem domain's generative processes. We analyze the proposed estimator, identifying the conditions under which variance is reduced. We also discuss the algorithmic aspects of FPGs, including optimal policy factorization based on minimum biclique coverings, and the impact of incorrectly specifying the network on the bias-variance trade-off. Furthermore, we showcase the performance benefits of our algorithm on large-scale bandit and traffic intersection problems, presenting a spatial approximation that contributes to the latter.