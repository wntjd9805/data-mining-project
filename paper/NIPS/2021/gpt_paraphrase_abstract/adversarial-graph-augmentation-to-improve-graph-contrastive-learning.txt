Self-supervised learning of graph neural networks (GNN) is crucial due to the limited availability of labeled data in real-world graph/network data. Graph contrastive learning (GCL) addresses this issue by training GNNs to enhance the correspondence between representations of the same graph in different augmented forms, eliminating the need for labels. However, traditional GCL methods often result in GNNs that capture redundant graph features, leading to subpar performance in downstream tasks. To address this, we propose adversarial-GCL (AD-GCL), a novel principle that prevents GNNs from capturing redundant information during training by optimizing adversarial graph augmentation strategies. We pair AD-GCL with theoretical explanations and present a practical implementation based on trainable edge-dropping graph augmentation. Through experiments on 18 benchmark datasets involving molecule property regression and classification, as well as social network classification, we demonstrate that AD-GCL outperforms state-of-the-art GCL methods, achieving performance gains of up to 14% in unsupervised, 6% in transfer, and 3% in semi-supervised learning settings.