This study explores the issue of computing the equilibrium of competitive games, which is commonly represented as a constrained saddle-point optimization problem with probability simplex constraints. Despite recent progress in understanding the convergence of extragradient methods in unconstrained settings, the theoretical basis for these methods in constrained settings, particularly when using multiplicative updates, is still lacking. This paper introduces efficient extragradient methods that are guaranteed to find the quantal response equilibrium (QRE) - solutions to zero-sum two-player matrix games with entropy regularization - at a linear rate. These algorithms can be implemented in a decentralized manner, with each player using its own payoff to execute symmetric and multiplicative updates iteratively without directly observing the opponent's actions. Moreover, by adjusting the level of entropy regularization, the proposed algorithms can identify an approximate Nash equilibrium of the unregularized matrix game at a sublinear rate without assuming the uniqueness of the Nash equilibrium. The methods also enable the development of efficient policy extragradient algorithms for solving entropy-regularized zero-sum Markov games at a linear rate. Notably, all convergence rates are almost independent of the size of the state and action spaces, emphasizing the positive impact of entropy regularization on accelerating convergence.