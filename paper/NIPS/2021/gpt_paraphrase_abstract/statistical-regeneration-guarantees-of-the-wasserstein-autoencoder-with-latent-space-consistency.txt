Variational Autoencoders (VAE) have revolutionized representation learning models and inspired the development of successors like Wasserstein Autoencoder (WAE) that offer enhanced generative capabilities comparable to Generative Adversarial Networks (GANs). Despite the resurgence of statistical analyses on GANs, similar examinations for Autoencoders have been lacking. This paper addresses this gap by investigating the statistical properties of WAE. Using the Vapnik-Chervonenkis theory, we provide statistical guarantees that WAE achieves the desired distribution in the latent space. Our main result ensures the regeneration of the input distribution by leveraging Optimal Transport of measures under the Wasserstein metric. This study sheds light on the range of distributions that WAE can reconstruct after compression in the form of a latent law.