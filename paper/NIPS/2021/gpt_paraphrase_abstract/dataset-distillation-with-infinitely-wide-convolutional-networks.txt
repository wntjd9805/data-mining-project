Machine learning algorithms are effective because they can extract useful features from large amounts of data. As the size of models and datasets increases, it becomes important to develop methods that can compress large datasets into smaller ones without losing performance. In this study, we propose a new distributed kernel-based meta-learning framework for dataset distillation. We apply this framework to infinitely wide convolutional neural networks and achieve state-of-the-art results for dataset distillation. For example, using only 10 datapoints (0.02% of the original dataset), we achieve over 65% test accuracy on the CIFAR-10 image classification task, surpassing the previous best accuracy of 40%. Our results also show improvements in other settings such as MNIST, Fashion-MNIST, CIFAR-100, and SVHN. Additionally, we conduct preliminary analyses on our distilled datasets to understand how they differ from naturally occurring data.