The field of few-shot natural language processing (NLP) research is active but lacks cohesive evaluation methods and careful experimental design. As a result, it is unclear which techniques perform best and how they compare to simple baselines. To address this, we propose the FLEX Principles, which establish requirements and best practices for rigorous and cost-effective evaluation in few-shot NLP. These principles include Sample Size Design, a new approach to benchmark design that balances statistical accuracy and evaluation costs. Following these principles, we introduce the FLEX benchmark, which consists of four few-shot transfer settings, zero-shot evaluation, and a public leaderboard covering various NLP tasks. Additionally, we present UniFew, a prompt-based model for few-shot learning that combines pretraining and finetuning prompt formats, simplifying the adaptation of downstream task formats to language model pretraining objectives. Despite its simplicity, UniFew achieves competitive results compared to popular meta-learning and prompt-based approaches.