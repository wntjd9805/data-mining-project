We present a reduction that demonstrates the difficulty of learning a single periodic neuron over isotropic Gaussian distributions in the presence of noise. This reduction shows that any algorithm capable of learning such functions under small noise in polynomial time implies a quantum algorithm for solving worst-case lattice problems in polynomial time. These lattice problems form the basis of lattice-based cryptography. Our core family of hard functions can be well approximated by one-layer neural networks and have the form of a univariate periodic function applied to an affine projection of the data. Previous works have shown the hardness of these functions against gradient-based and Statistical Query (SQ) algorithms. We extend this hardness result to all polynomial-time algorithms, including those beyond gradient-based and SQ algorithms, by introducing (polynomially) small noise to the labels. Additionally, we demonstrate that noise is necessary for the hardness result by designing a polynomial-time algorithm that can learn certain families of these functions under exponentially small adversarial noise. Our algorithm is not gradient-based or an SQ algorithm, but instead relies on the Lenstra-Lenstra-Lov√°sz (LLL) lattice basis reduction algorithm. Furthermore, in the absence of noise, this algorithm can be directly applied to solve CLWE detection and phase retrieval with an optimal sample complexity of d + 1 samples, improving upon previous work.