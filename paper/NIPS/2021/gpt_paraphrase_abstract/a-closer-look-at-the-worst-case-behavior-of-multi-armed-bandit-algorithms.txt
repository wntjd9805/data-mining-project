The abstract discusses the complexity of the classical multi-armed bandit (MAB) problem, focusing on the difference between mean rewards in the top two arms. The Upper Confidence Bound (UCB) policy is a simple algorithm that adapts to this difference and achieves optimal results for large gaps and near-optimal results for small gaps. The paper presents new findings on the arm-sampling behavior of UCB, revealing that the rates are deterministic regardless of problem complexity. This discovery allows for new asymptotics and a different proof for the minimax regret of UCB. Additionally, the paper provides a comprehensive characterization of the MAB problem under UCB in the conventional diffusion scaling. The comparison with Thompson Sampling highlights distinct behaviors, including an incomplete learning phenomenon in the latter.