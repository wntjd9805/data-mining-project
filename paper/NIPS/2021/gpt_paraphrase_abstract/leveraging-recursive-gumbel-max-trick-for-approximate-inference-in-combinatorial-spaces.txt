Structured latent variables are valuable in deep learning models as they allow for meaningful prior knowledge to be incorporated. However, learning with these variables is challenging due to their discrete nature. Currently, the standard approach involves defining a latent variable as a perturbed algorithm output and using a differentiable surrogate for training. Unfortunately, this surrogate introduces additional constraints and results in biased gradients. To address these limitations, we propose an extension of the Gumbel-Max trick to define distributions over structured domains. By leveraging score function estimators for optimization, we avoid the need for differentiable surrogates. We introduce a family of recursive algorithms with a common feature called stochastic invariant, which enables us to construct reliable gradient estimates and control variates without imposing further constraints on the model. In our experiments, we demonstrate the effectiveness of our approach by achieving competitive results compared to relaxation-based methods in various structured latent variable models.