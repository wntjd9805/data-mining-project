Deep learning models have achieved impressive results in various tasks, but they often learn spurious correlations that are not actually predictive. To address this issue, we propose a training framework based on causality to minimize the impact of observed confounders on these correlations. We analyze the underlying Structural Causal Model (SCM) and suggest using Maximum Likelihood Estimation (MLE) on the interventional distribution, which is typically hidden in observational data. To achieve this, we introduce two algorithms, Implicit CMLE and Explicit CMLE, along with two upper bounds for expected negative log-likelihood. We evaluate our approach on simulated data as well as real-world tasks like Natural Language Inference (NLI) and Image Captioning. The results demonstrate that CMLE methods perform better than regular MLE in terms of out-of-domain generalization and reducing spurious correlations, while maintaining comparable performance on regular evaluations.