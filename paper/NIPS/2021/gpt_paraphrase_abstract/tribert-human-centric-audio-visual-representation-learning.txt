Transformer models like BERT have been successful in language tasks and have sparked interest in using them for multi-modal feature learning. However, most multi-modal variants have focused on visual-linguistic data and have not explored audio-visual modalities or their application in tasks like sound source separation and localization. To address this gap, we propose TriBERT, a transformer-based architecture inspired by ViLBERT. TriBERT enables contextual feature learning across three modalities: vision, pose, and audio, using flexible co-attention. We incorporate pose keypoints, which have been shown to enhance performance in audio-visual scenarios involving human interaction. Our model includes a learned visual tokenization scheme based on spatial attention and leverages weak-supervision for cross-modal interactions between visual and pose modalities. Additionally, we introduce a sound-source separation loss across all three streams. We pre-train TriBERT on the MUSIC21 dataset and demonstrate improved performance in audio-visual sound source separation, as well as other audio-visual tasks, through fine-tuning. Our experiments show that TriBERT representations are not only specific to sound source separation but also improve performance in cross-modal audio-visual-pose retrieval by up to 66.7% in top-1 accuracy.