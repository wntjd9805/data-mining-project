The demand for explaining machine learning predictions is increasing in high-stakes fields as machine learning applications become more prevalent. However, recent studies have shown that interpretation methods can be unreliable and sensitive to disturbances or transformations of input data. To address this issue, we propose a self-interpretable model that learns robust interpretations through transformation equivariant regularization. This model can capture valid interpretations that are equivariant to geometric transformations. Additionally, our model maintains a high expressive capability comparable to state-of-the-art deep learning models in complex tasks while providing visualizable and faithful interpretations. Unlike existing self-interpretable models, our model prioritizes both interpretation quality and expressive power. We compare our model with other related methods and validate its interpretation quality and consistency.