This paper investigates the phenomenon of "benign overfitting" in modern machine learning systems, specifically focusing on the maximum margin classifier for linear classification problems. These systems, such as deep neural networks, are often highly over-parameterized to fit noisy training data exactly while achieving low test errors. The study examines data generated from sub-Gaussian mixtures and provides a precise risk bound for the maximum margin linear classifier in the over-parameterized setting. The results offer a better understanding of the conditions under which benign overfitting can occur in linear classification problems and also have implications for over-parameterized logistic regression.