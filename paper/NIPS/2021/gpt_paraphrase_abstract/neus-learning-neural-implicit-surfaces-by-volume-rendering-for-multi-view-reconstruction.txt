We introduce a new technique called NeuS for reconstructing objects and scenes from 2D images. Existing methods for neural surface reconstruction struggle with self-occlusion and thin structures, as they require foreground masks and can get stuck in local minima. On the other hand, recent methods for novel view synthesis produce robust neural scene representations using volume rendering. However, extracting high-quality surfaces from these representations is challenging due to the lack of surface constraints. In NeuS, we propose representing surfaces as zero-level sets of signed distance functions (SDFs) and develop a new volume rendering method to train a neural SDF representation. We address inherent geometric errors in conventional volume rendering by formulating a bias-free approximation, resulting in more accurate surface reconstruction without the need for mask supervision. Experimental results on the DTU and BlendedMVS datasets demonstrate that NeuS outperforms existing methods in reconstructing high-quality surfaces, particularly for complex structures and self-occlusion.