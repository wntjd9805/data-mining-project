Metric measure spaces, which combine metric spaces and probability distributions, are frequently used in machine learning. The Gromov-Wasserstein (GW) distance is commonly used to compare such spaces, but it is limited to spaces with probability distributions. To address this limitation, we propose two Unbalanced Gromov-Wasserstein formulations: a distance measure and a more manageable upper-bounding relaxation. These formulations enable the comparison of metric spaces with arbitrary positive measures up to isometries. The first formulation is a positive and definite divergence that relaxes the mass conservation constraint using a novel type of quadratically-homogeneous divergence. This divergence is compatible with the entropic regularization approach, commonly used to solve large-scale optimal transport problems. We demonstrate that the associated non-convex optimization problem can be efficiently solved using a parallelizable and GPU-friendly iterative scheme. The second formulation is a distance measure between metric measure spaces up to isometries, based on a conic lifting technique. We conduct numerical experiments on synthetic examples and domain adaptation data with a Positive-Unlabeled learning task to showcase the distinctive characteristics of the unbalanced divergence and its potential applications in machine learning.