Many image classifiers achieve high accuracy on popular benchmarks, but this can mask a problem known as overinterpretation. Overinterpretation occurs when a classifier provides a confident decision without relying on semantically meaningful features. In this study, we demonstrate that convolutional neural networks trained on CIFAR-10 and ImageNet suffer from overinterpretation. Even when 95% of input images are masked, these models still make confident predictions. We introduce a new method called Batched Gradient SIS to identify sufficient input subsets for complex datasets. We find that border pixels in ImageNet are sufficient for training and testing. Although overinterpretation raises concerns about model fragility in real-world scenarios, these patterns are valid statistical patterns of the benchmark and contribute to high test accuracy. Unlike adversarial examples, overinterpretation does not require any modifications to the image pixels. We also discover that ensembling and input dropout techniques can help mitigate overinterpretation.