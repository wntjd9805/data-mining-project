Sparse training is a method to speed up the training of deep neural networks and reduce memory usage. However, existing methods often fail to achieve this because they require dense computation during the backward propagation step. This paper introduces an efficient sparse training method that allows for completely sparse forward and backward passes. The training process is formulated as a continuous minimization problem with a global sparsity constraint. The optimization process is divided into two steps: weight update and structure parameter update. The weight update step utilizes the conventional chain rule to exploit the sparse structure. The structure parameter update step proposes a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, achieving completely sparse training. The paper proves that the variance of the gradient estimator is bounded. Extensive experiments on real-world datasets demonstrate that the proposed algorithm is significantly more effective in accelerating the training process, up to an order of magnitude faster than previous methods.