In the context of stochastic linear contextual bandit problems, there are various minimax approaches for exploration using reactive policies. However, implementing these algorithms can be challenging, particularly when the data is collected in a distributed manner or when a different policy needs to be implemented with human involvement. In such cases, it is advantageous to explore using a single non-reactive policy. By leveraging a batch of available contexts, we propose a stochastic policy that can collect a high-quality dataset, from which a near-optimal policy can be derived. Our approach is supported by theoretical analysis and numerical experiments on both synthetic and real-world datasets.