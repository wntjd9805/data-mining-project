Thompson Sampling is a highly effective method for contextual bandits and has been extended to posterior sampling for certain MDP settings. However, current posterior sampling methods for reinforcement learning have limitations such as being model-based or lacking worst-case theoretical guarantees beyond linear MDPs. This paper introduces a new model-free formulation of posterior sampling that can be applied to more general episodic reinforcement learning problems and provides theoretical guarantees. We utilize innovative proof techniques to demonstrate that, under appropriate conditions, our posterior sampling method exhibits worst-case regret that matches the performance of optimization-based methods, which are known to be the best. In the linear MDP setting, our algorithm's regret scales linearly with the dimension, whereas existing posterior sampling-based exploration algorithms have a quadratic dependence on the dimension.