Poisoning attacks pose a significant security risk to machine learning algorithms by manipulating the training set to degrade the performance of the output model. Some of these attacks require full knowledge of the training data, but it is possible to achieve similar attack results without this knowledge. This study focuses on the problem of feature selection with LASSO and demonstrates that adversaries with full information about the training data are more effective than those without it. The distinction between data-aware and data-oblivious attackers is crucial, as their attack and defense results differ fundamentally.