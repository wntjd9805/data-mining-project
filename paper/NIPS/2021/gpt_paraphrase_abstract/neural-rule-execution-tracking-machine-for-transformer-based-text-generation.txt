Sequence-to-Sequence (Seq2Seq) models, such as BART and T5, have shown impressive performance in generating natural language. However, their lack of transparency limits their usability in tasks that require specific rules or constraints. Previous approaches either design specific model structures or implement specialized algorithms to execute rules during text generation, making it difficult to support multiple rules at once. This paper introduces a new module called Neural Rule-Execution Tracking Machine (NRETM) that can be integrated into transformer-based generators. This module enables multiple rules to be simultaneously applied, resulting in improved generation performance in a unified and scalable manner. Extensive experiments on various benchmarks confirm the effectiveness of our proposed model in both controlled and general text generation tasks.