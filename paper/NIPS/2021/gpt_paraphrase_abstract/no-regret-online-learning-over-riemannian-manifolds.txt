We examine online optimization on Riemannian manifolds, where a learner aims to minimize a series of time-varying loss functions on these manifolds. Although Euclidean online convex optimization algorithms have been widely used in various fields, less attention has been given to their Riemannian counterparts. In this study, we investigate Riemannian online gradient descent (R-OGD) on Hadamard manifolds for both geodesically convex and strongly geodesically convex loss functions. We also analyze the Riemannian bandit algorithm (R-BAN) on Hadamard homogeneous manifolds for geodesically convex functions. We establish upper bounds on the regrets of the problem in terms of time horizon, manifold curvature, and manifold dimension. Additionally, we discover a universal lower bound for the achievable regret by constructing an online convex optimization problem on Hadamard manifolds. Our obtained regret bounds align with the corresponding results in Euclidean spaces. Finally, we conduct numerical experiments to validate our theoretical findings.