The optimization of a possibly non-convex and expensive objective function is examined in a sequential manner. This problem can be seen as a GaussianProcess (GP) bandit, with the objective function residing in a reproducing kernel Hilbert space (RKHS). Current analysis of learning algorithms reveals a significant disparity between the lower and upper bounds on the performance of simple regret. By considering the number of exploration trials (N) and the maximal information gain (γN), we prove a tighter bound of ˜O((cid:112)γN /N ) on the simple regret performance for a pure exploration algorithm. We demonstrate that this bound is order optimal, with logarithmic factors, in cases where a regret lower bound is known. Additionally, we establish novel and precise confidence intervals for GP models that are applicable to RKHS elements, which may have broader relevance.