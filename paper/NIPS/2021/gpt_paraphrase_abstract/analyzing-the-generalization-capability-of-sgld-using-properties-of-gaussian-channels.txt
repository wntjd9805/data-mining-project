This study focuses on the stochastic gradient Langevin dynamics (SGLD) algorithm and its impact on the generalization of machine learning models. The authors propose a new generalization bound by connecting SGLD with Gaussian channels from information and communication theory. This bound incorporates the variance of gradients to measure the sharpness of the loss landscape. Additionally, the authors investigate the generalization capability of differentially private SGD (DP-SGD) and prove that it can be improved through iteration. By including a time-decaying factor, the contributions of early iterations to the bound can decrease over time. The authors validate their findings through numerical experiments, demonstrating the predictive ability of the proposed generalization bound.