We propose a transfer learning framework using modular variational Gaussian processes (GP). Our approach involves using a dictionary of well-fitted GPs to create ensemble GP models without the need to revisit any data. Each model is characterized by its hyperparameters, pseudo-inputs, and corresponding posterior densities. This method eliminates the need for data centralization, reduces computational costs, and allows for the transfer of learned uncertainty metrics. By utilizing high-dimensional integral operators augmented with the Kullback-Leibler divergence, we introduce an efficient lower bound that applies to all sparse variational GPs, regardless of complexity or likelihood distribution. Our method can also be applied to multi-output GPs, enabling the learning of correlations between independent modules. Extensive results demonstrate the effectiveness of our framework in large-scale and multi-task experiments, compared to existing exact inference methods.