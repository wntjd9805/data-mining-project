Large-scale language model pretraining has advanced text generation, but controlling the attributes of the generated text remains a challenge. Fine-tuning pretrained models is commonly used but is computationally expensive and may lack appropriate data. To address this, we propose MUCOCO, a flexible and modular algorithm for controllable inference from pretrained models. We frame the decoding process as an optimization problem, allowing for easy integration of multiple attributes as differentiable constraints. By converting the discrete optimization to a continuous one, we utilize Lagrangian multipliers and gradient-descent techniques for text generation. Our approach is evaluated in controllable machine translation and style transfer, showing significant improvements over baselines.