Counterfactual explanations are becoming popular for addressing the negative effects of algorithmic decisions. However, there is a lack of understanding regarding their vulnerabilities. In this study, we present a framework that identifies the vulnerabilities of counterfactual explanations and demonstrates how they can be manipulated. We find that counterfactual explanations can vary significantly under slight changes, indicating a lack of robustness. Building on this, we propose a new objective for training fair models that provide lower-cost recourse for specific subgroups. Our experiments on loan and crime prediction datasets show that certain subgroups receive significantly lower-cost recourse. These findings highlight concerns about the reliability of current counterfactual explanation methods and call for further research on robust counterfactual explanations.