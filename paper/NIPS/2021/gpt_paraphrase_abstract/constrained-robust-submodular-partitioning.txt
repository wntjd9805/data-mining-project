The robust submodular partitioning problem involves allocating items into blocks in order to maximize the evaluation of the minimum block based on a submodular function. This promotes diversity within each block and has various applications in machine learning, such as data partitioning for consistent gradient computation in distributed training. We examine an extension of this problem with additional constraints on each block, such as cardinality, multiple matroids, or knapsack constraints. We propose two types of algorithms: Min-Block Greedy based algorithms with a ‚å¶(1/m) bound and Round-Robin Greedy based algorithms with a constant bound. We demonstrate that these algorithms provide good approximation guarantees under different constraints. Notably, combining the two algorithms achieves a strongly polynomial running time while preserving the approximation guarantee. Finally, we apply the algorithms to a real-world machine learning data partitioning problem and obtain positive outcomes.