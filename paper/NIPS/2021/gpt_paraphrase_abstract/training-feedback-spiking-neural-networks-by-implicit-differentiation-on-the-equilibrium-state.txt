Spiking neural networks (SNNs) are models inspired by the brain that can be implemented efficiently on neuromorphic hardware. However, training SNNs is challenging due to the discontinuity of the spiking neuron model. Existing methods try to imitate backpropagation and feedforward architectures used in artificial neural networks, but they suffer from approximation errors and limited information propagation. In this study, we propose a new training method for feedback SNNs that does not rely on exact reverse computation. We observe that the firing rates of SNNs with feedback connections reach an equilibrium state over time, following a fixed-point equation. By treating the forward computation of feedback SNNs as a black-box solver for this equation, we can compute parameter gradients using implicit differentiation without considering the exact forward procedure. This decouples the forward and backward procedures and avoids the problem of non-differentiable spiking functions. We also discuss the biological plausibility of implicit differentiation, which only requires computing another equilibrium. Extensive experiments on various datasets demonstrate the superior performance of our method for feedback models with fewer neurons and parameters in a small number of time steps. The code for our method is available at https://github.com/pkuxmq/IDE-FSNN.