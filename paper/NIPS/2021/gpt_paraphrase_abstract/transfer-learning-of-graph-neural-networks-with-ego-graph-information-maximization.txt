Graph neural networks (GNNs) have achieved impressive results in various applications, but training them for large-scale graphs can be expensive. Some recent studies have explored pre-training GNNs, but they lack theoretical insights into their framework design and clear requirements and guarantees for transferability. In this research, we propose a theoretically grounded and practical framework for transfer learning of GNNs. Firstly, we introduce a new perspective on essential graph information and emphasize its capture as the objective of transferable GNN training. This motivates the development of EGI (Ego-Graph Information maximization) to analytically achieve this objective. Secondly, we analyze the transferability of EGI when node features are structure-relevant, considering the differences between the local graph Laplacians of the source and target graphs. We conduct controlled synthetic experiments to validate our theoretical findings. Furthermore, comprehensive experiments on two real-world network datasets demonstrate consistent results in direct transfer scenarios, while experiments on large-scale knowledge graphs show promising results in the more practical setting of transfer learning with fine-tuning.