The aim of this study is to develop a policy gradient algorithm that is not sensitive to the time scale δ used in reinforcement learning. Previous methods have failed to perform well as δ approaches 0, leading to a divergence in the variance of the policy gradient estimator in stochastic environments. While action repetition methods have been used to address this issue, they are unable to react effectively to unexpected situations. To overcome these limitations, a new algorithm called Safe Action Repetition (SAR) is proposed. SAR can adaptively respond to changes in states during action repetition, making it robust to stochasticity. Empirical results demonstrate that SAR outperforms previous δ-invariant approaches on various environments. The code for SAR is available at https://vision.snu.ac.kr/projects/sar.