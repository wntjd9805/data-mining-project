Deterministic Fourier features are an improvement over random Fourier features for large-scale machine learning problems with kernel methods. However, their applicability is limited to low-dimensional scenarios due to the curse of dimensionality. In this study, we propose a solution to this problem by exploiting the tensor product structure of deterministic Fourier features. This allows us to represent model parameters as a low-rank tensor decomposition. We develop a block coordinate descent algorithm with linear complexity for a regularized squared loss function, which enables us to learn a parsimonious model using deterministic Fourier features. Our experiments show that our low-rank tensor approach achieves the same performance as the nonparametric model, consistently outperforming random Fourier features.