Exploration methods in reinforcement learning that rely on pseudo-count of transitions or curiosity of dynamics have shown promise in dealing with sparse rewards. However, these approaches are often sensitive to irrelevant environmental dynamics, such as white noise. To address this issue, we introduce the Dynamic Bottleneck (DB) model, which obtains a dynamics-relevant representation by leveraging the information-bottleneck principle. Building upon the DB model, we propose DB-bonus, which incentivizes the agent to explore state-action pairs that yield high information gain. We establish theoretical connections between DB-bonus, the upper confidence bound (UCB) for linear cases, and the visiting count for tabular cases. Our evaluation on Atari games with dynamics-irrelevant noises demonstrates that exploration using DB-bonus outperforms several state-of-the-art exploration methods in noisy environments.