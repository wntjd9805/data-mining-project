Missing not at random (MNAR) data, which occurs when the cause of missing values in real-world datasets is not fully observed, poses challenges for imputation methods. Many existing methods do not consider the missingness mechanism, leading to biased imputation values. While a few methods have addressed MNAR, they often lack guaranteed identifiability of their models. This means that model parameters cannot be uniquely determined, resulting in potentially biased imputation results. Modern deep generative models also overlook this issue. To address this gap, we analyze the identifiability of generative models under MNAR and propose a practical deep generative model that ensures identifiability under mild assumptions for various MNAR mechanisms. Our method demonstrates its superiority in tasks involving synthetic data and real-world scenarios with MNAR data.