We investigate offline reinforcement learning for episodic time-homogeneous Markov Decision Processes (MDP). We consider tabular MDPs with S states and A actions, as well as linear MDPs with anchor points and feature dimension d. By utilizing K episodes of collected data with a minimum visiting probability of (anchor) state-action pairs dm, we derive nearly horizon H-free sample complexity bounds for offline reinforcement learning when the total reward is bounded by 1.Specifically, for offline policy evaluation, we obtain an approximate error bound for the plug-in estimator that matches the lower bound up to logarithmic factors and does not have an additional dependency on polynomial terms in higher-order terms. The error bound is given by 1Kdm⇣q⌘ 1Kdm+ min(S,d)Kdm.For offline policy optimization, we derive an approximate sub-optimality gap for the empirical optimal policy that approaches the lower bound up to logarithmic factors and a high-order term. This improves upon the previous best-known result, which included additional polynomial factors in the main term.To the best of our knowledge, these are the first nearly horizon-free bounds for episodic time-homogeneous offline tabular MDPs and linear MDPs with anchor points. Our analysis relies on a simple yet effective recursive method to bound a "total variance" term in the offline scenarios, which may be of individual interest.