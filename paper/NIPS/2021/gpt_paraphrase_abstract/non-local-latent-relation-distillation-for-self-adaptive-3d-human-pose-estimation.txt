Current 3D human pose estimation methods rely on either strong or weak supervision, which can be inconvenient to obtain for new target environments. In this study, we address this issue by approaching 3D pose learning as a self-supervised adaptation problem. We propose a method that transfers knowledge from a labeled source domain to an unpaired target domain. Our approach involves inferring image-to-pose by using image-to-latent and latent-to-pose mappings obtained from a prior-enforcing generative adversarial auto-encoder. Additionally, we introduce relation distillation to align the unpaired cross-modal samples, such as target videos and 3D pose sequences. We propose a new set of non-local relations to capture long-range latent pose interactions, unlike traditional contrastive relations that only consider local couplings. We also provide a way to quantify non-localness and select the most effective relation set. Our method achieves state-of-the-art performance in 3D human pose estimation on standard benchmarks.