Current methods for reducing bias in deep neural network (DNN) models focus on debiasing the encoders, but this approach requires extensive annotations for sensitive attributes and may not eliminate all fairness-sensitive information. In order to overcome these limitations, we investigate whether we can mitigate discrimination in DNN models by solely debiasing the classification head, even when biased representations are inputted. To address this research question, we propose a new technique called Representation Neutralization for Fairness (RNF), which achieves fairness by debiasing only the task-specific classification head of DNN models. We achieve this by training the classification head using neutralized representations of samples with the same true label but different sensitive attributes. RNF discourages the classification head from capturing unwanted correlations between fairness-sensitive information in encoder representations and specific class labels. Additionally, in low-resource settings where sensitive attribute annotations are unavailable, we employ a bias-amplified model to generate proxy annotations. Experimental results on various benchmark datasets demonstrate that our RNF framework effectively reduces discrimination in DNN models with minimal impact on task-specific performance.