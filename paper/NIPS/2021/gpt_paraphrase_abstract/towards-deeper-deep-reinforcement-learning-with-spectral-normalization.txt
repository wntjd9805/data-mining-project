In computer vision and natural language processing, increasing the capacity of model architecture has consistently improved performance. However, in reinforcement learning (RL), state-of-the-art algorithms often use small MLPs and performance gains come from algorithmic advancements. It has been hypothesized that small datasets in RL require simple models to avoid overfitting, but this hypothesis remains untested. This study investigates the impact of replacing small MLPs in RL agents with larger modern networks that include skip connections and normalization, specifically focusing on actor-critic algorithms. The results show that simply adopting these larger architectures leads to instability and poor performance, which explains why simple models are popular in practice. However, the study demonstrates that dataset size is not the limiting factor, and instead attributes the instability to taking gradients through the critic. By applying spectral normalization (SN), the issue can be mitigated, enabling stable training with larger modern architectures. After implementing SN, larger models show significant performance improvements, suggesting that focusing on model architectures alongside algorithmic innovations can lead to substantial gains.