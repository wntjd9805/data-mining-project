The challenge of determining how closely machine-generated text resembles human language remains a significant issue in the field of open-ended text generation. This abstract introduces MAUVE, a comparison measure for evaluating the quality of machine-generated text. MAUVE utilizes divergence frontiers to directly compare the distribution of text generated by a machine to the distribution of human-written text. It is designed to be compatible with modern text generation models by calculating information divergences in a quantized embedding space. Extensive empirical testing on three open-ended generation tasks demonstrates that MAUVE effectively identifies characteristics of generated text, scales well with model size, and correlates with human judgments. Furthermore, MAUVE offers more flexibility compared to existing evaluation metrics based on distributional analysis.