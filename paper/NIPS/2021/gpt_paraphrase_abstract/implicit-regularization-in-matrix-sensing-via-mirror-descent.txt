We investigate the use of discrete-time mirror descent for matrix sensing problems without regularization. By analyzing the Bregman divergence in terms of potential-based techniques, we demonstrate that mirror descent, with different choices of mirror maps, converges to a matrix that minimizes a quantity explicitly related to the nuclear norm, Frobenius norm, and von Neumann entropy. This suggests that mirror descent, a first-order algorithm for minimizing unregularized empirical risk, can recover low-rank matrices under the same assumptions required for nuclear-norm minimization. Additionally, when the sensing matrices are symmetric and commute, we show that gradient descent with full-rank factorized parametrization is a first-order approximation to mirror descent. This result allows us to derive an explicit characterization of the implicit bias of gradient flow.