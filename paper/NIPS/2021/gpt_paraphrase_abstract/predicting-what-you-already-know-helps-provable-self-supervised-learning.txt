Self-supervised representation learning involves solving pretext tasks without labeled data to obtain useful semantic representations. These tasks are created using input features, such as predicting missing image patches or words in text. By predicting this known information, effective representations for downstream tasks can be learned. We propose a mechanism that exploits statistical connections between certain reconstruction-based pretext tasks to ensure the learning of good representations. We quantify how the approximate independence between task components allows for solving downstream tasks with just a linear layer on top of the learned representation. We prove that the linear layer produces small approximation errors even for complex ground truth functions, resulting in reduced labeled sample requirements.