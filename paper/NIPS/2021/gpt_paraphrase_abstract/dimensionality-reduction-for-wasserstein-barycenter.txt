The Wasserstein barycenter is a geometric concept used to determine centrality in probability distributions, with applications in machine learning. However, current algorithms for finding an approximate barycenter face a problem of exponential dependence on the dimension of the underlying space. To address this issue, we examine dimensionality reduction techniques for the Wasserstein barycenter problem. By restricting the barycenter to a support of size n, we demonstrate that randomized dimensionality reduction can map the problem to a space of dimension O(log n), independent of the original space dimension and the number of distributions. The reduced dimension solution maintains its cost with minimal error in the original space. We establish both upper and lower bounds on the size of the reduced dimension, proving the optimality of our methods. Additionally, we introduce a coreset construction that significantly reduces the number of input distributions for the Wasserstein barycenter problem. These coresets can be combined with random projections to further enhance computation time. Our experimental results confirm the effectiveness of dimensionality reduction in improving speed while preserving solution quality.