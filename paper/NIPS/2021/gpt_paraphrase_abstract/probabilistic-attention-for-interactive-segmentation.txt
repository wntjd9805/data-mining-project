The abstract introduces a probabilistic interpretation of attention in transformers and explains how it can be applied to online adaptation of model parameters. It suggests using Expectation Maximization algorithms to incorporate inference-time information provided by external agents and propagate it to other tokens effectively. The approach is demonstrated in an interactive semantic segmentation task, where annotators and models collaborate to enhance annotation efficiency. Experimental results on standard benchmarks show that adapting key parameters improves model performance in low feedback scenarios, while propagating values enhances responsiveness in high feedback scenarios. Additionally, a Py-Torch layer implementation of the proposed probabilistic attention model is provided.