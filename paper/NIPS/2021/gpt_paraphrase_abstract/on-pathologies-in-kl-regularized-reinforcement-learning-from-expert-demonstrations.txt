KL-regularized reinforcement learning has been effective in enhancing the efficiency of deep reinforcement learning algorithms for real-world tasks. However, we have discovered that when using expert demonstrations as behavioral reference policies, this approach can suffer from problematic training dynamics. These dynamics result in slow, unstable, and suboptimal online learning. Our empirical evidence confirms that this issue is prevalent across commonly chosen behavioral policy classes, negatively impacting sample efficiency and online policy performance. To address this problem, we propose the use of non-parametric behavioral reference policies, which successfully alleviate the pathology. This modified approach significantly surpasses state-of-the-art methods in various challenging tasks involving locomotion and dexterous hand manipulation.