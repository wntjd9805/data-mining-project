Large-scale deep models have been successful in recent years, but their computational complexity and storage requirements make it difficult to deploy them on resource-limited devices. Knowledge distillation, a model compression and acceleration method, improves the performance of small models by transferring knowledge from a teacher detector. However, existing distillation-based detection methods have limitations. They only imitate features near bounding boxes, ignoring beneficial features outside the boxes and imitating features mistakenly identified as background by the teacher detector. To overcome these limitations, we propose a novel Feature-Richness Score (FRS) method that selects important features to enhance detectability during distillation. Our method effectively retrieves important features outside the bounding boxes and eliminates detrimental features within them. Extensive experiments demonstrate the excellent performance of our methods on both anchor-based and anchor-free detectors. For instance, RetinaNet with ResNet-50 achieves a 39.7% mean average precision (mAP) on the COCO2017 dataset, surpassing the ResNet-101 based teacher detector's 38.9% by 0.8%. Our implementation can be found at https://github.com/duzhixing/FRS.