Methods for Visual Question Answering (VQA) often rely on dataset biases instead of reasoning, limiting their ability to generalize. Recent research has revealed that advanced VQA models exhibit improved reasoning capabilities when trained on perfect visual inputs. However, transferring this knowledge to practical models poses challenges as significant information loss occurs during the transfer. To address this issue, we propose a method that incorporates a regularization term in the loss function, supervising the sequence of reasoning operations necessary for answering questions. Through theoretical analysis using PAC-learning, we demonstrate that predicting reasoning operations can reduce sample complexity under reasonable assumptions. Additionally, we experimentally validate the effectiveness of our approach on the GQA dataset and highlight its complementarity to BERT-like self-supervised pre-training. Our method focuses on abstracting the answer format.