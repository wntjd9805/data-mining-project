This paper explores the application of value-based reinforcement learning algorithms in Newcomblike decision problems, which have been extensively studied in decision theory but not in reinforcement learning. The study aims to address fundamental theoretical questions about the behavior of these algorithms in such environments. The findings reveal that a value-based reinforcement learning agent cannot converge to a policy that is not ratiﬁable, meaning it must only choose actions that are optimal given that policy. This insight provides a valuable tool for understanding the limit behavior of agents, demonstrating that there are Newcomblike environments where a reinforcement learning agent cannot converge to any optimal policy. Furthermore, the research shows that a ratiﬁable policy always exists in this setting, but there are cases where a reinforcement learning agent typically cannot converge to it, resulting in failure to converge at all. Additionally, the study presents several results concerning the potential limit behaviors of agents in situations where they do not converge to any policy.