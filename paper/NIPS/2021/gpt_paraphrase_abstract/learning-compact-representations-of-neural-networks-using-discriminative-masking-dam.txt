A key objective in deep learning is to develop efficient representations of features in neural networks, which is important for both unsupervised learning and network pruning. However, current state-of-the-art pruning methods suffer from two main limitations: instability during training and the need for resource-intensive fine-tuning. These limitations stem from a lack of a systematic approach that simultaneously prunes and refines weights during training in a single stage, without requiring additional fine-tuning for optimal performance. To address this, we propose a new pruning method called DiscriminAtive Masking (DAM). DAM selectively refines certain neurons during training while gradually masking out others. Our DAM approach demonstrates excellent performance across various applications, including dimensionality reduction, recommendation systems, graph representation learning, and structured pruning for image classification. The learning objective of DAM is also theoretically linked to minimizing the L0 norm of the masking layer. Our code and datasets for DAM are available at https://github.com/jayroxis/dam-pytorch.