Recently, there has been an interest in using human explanation to enhance communication between humans and agents in interactive machine learning. While this has been explored in supervised learning tasks, it is unclear how to incorporate this kind of human knowledge into deep reinforcement learning. In this study, we investigate the use of human visual explanations in human-in-the-loop reinforcement learning (HIRL) specifically for the task of learning from feedback. We propose a method called EXPAND (EXPlanationAugmeNted feeDback) that encourages the model to focus on task-relevant features by augmenting the data in a context-aware manner that perturbs only irrelevant features in human salient information. We evaluate this approach on five tasks, including Pixel-Taxi and four Atari games, to assess its performance and sample efficiency. The results show that our method outperforms supervised learning-based methods that utilize human explanation and RL baselines that only utilize evaluative feedback.