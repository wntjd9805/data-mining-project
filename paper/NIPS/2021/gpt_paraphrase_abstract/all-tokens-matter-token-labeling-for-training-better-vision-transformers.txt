This paper introduces a new training objective called token labeling for training high-performance vision transformers (ViTs). Unlike the standard training objective of ViTs, which calculates the classification loss on a trainable class token, token labeling utilizes all image patch tokens to compute the training loss in a dense manner. It reformulates the image classification problem into multiple token-level recognition problems and assigns each patch token with an individual location-specific supervision generated by a machine annotator. Experimental results demonstrate that token labeling consistently improves the performance of various ViT models across a wide range. For example, a vision transformer with 26M learnable parameters achieves 84.4% Top-1 accuracy on ImageNet with token labeling. By slightly increasing the model size to 150M, the accuracy can be further improved to 86.4%, making it the smallest model among previous models (250M+) that reach 86%. The study also shows that token labeling enhances the generalization capability of pretrained models on downstream tasks like semantic segmentation. The code and model are publicly available at https://github.com/zihangJiang/TokenLabeling.