Current policy learning methods in reinforcement learning (RL) and behavioral cloning (BC) require high-quality supervision signals, such as well-designed rewards or expert demonstrations. However, obtaining these signals is often impractical or costly. In this study, we propose a unified framework that utilizes cheap weak supervisions to efficiently perform policy learning. We treat the weak supervision as imperfect information from a peer agent and evaluate the learning agent's policy based on correlated agreement with the peer agent's policy. This approach penalizes policy overfitting to weak supervision. Through theoretical guarantees and extensive evaluations on tasks with noisy rewards, weak demonstrations, and standard policy co-training, we demonstrate that our method significantly improves performance, particularly in complex or noisy learning environments.