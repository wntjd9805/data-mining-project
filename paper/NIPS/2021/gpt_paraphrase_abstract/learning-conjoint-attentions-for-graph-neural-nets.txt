This paper introduces Conjoint Attentions (CAs), a novel set of learning-to-attend techniques for graph neural networks (GNNs). CAs go beyond considering the node features propagated within the GNN and incorporate additional structural interventions, such as node cluster embedding and higher-order structural correlations. By incorporating these interventions, attention scores can be computed, allowing the propagation of significant node features within the GNN. Based on the Conjoint Attention strategies, Graph conjoint attention networks (CATs) are proposed to learn representations embedded with significant latent features identified by Conjoint Attentions. The discriminative capacity of CATs is theoretically validated. Extensive testing on benchmark datasets and comprehensive comparisons with state-of-the-art baselines demonstrate the effectiveness of the proposed Conjoint Attentions.