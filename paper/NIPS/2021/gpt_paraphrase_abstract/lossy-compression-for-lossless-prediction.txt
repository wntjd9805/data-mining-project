Data compressors aim to preserve perceptual fidelity rather than just the information required by algorithms. This paper focuses on determining the bit-rate needed to ensure optimal performance on predictive tasks that remain unchanged under a set of transformations. By leveraging this understanding, we propose unsupervised objectives for training neural compressors. Through these objectives, we successfully train a versatile image compressor that achieves significant rate reduction (over 1000 times on ImageNet) across eight datasets, while maintaining classification performance.