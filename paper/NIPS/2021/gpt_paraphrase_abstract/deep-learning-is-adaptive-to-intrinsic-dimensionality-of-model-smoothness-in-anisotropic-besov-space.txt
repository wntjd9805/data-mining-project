Deep learning has shown remarkable performance in various tasks, particularly with high-dimensional datasets like images. To comprehend this capability, we explore deep learning's ability to approximate and estimate anisotropic Besov spaces. These spaces possess direction-dependent smoothness and encompass various function classes that have been previously studied. Our findings reveal that the approximation and estimation errors of deep learning rely solely on the average smoothness parameters across all directions. Consequently, if the smoothness of the target function is highly anisotropic, the curse of dimensionality can be avoided. Our analysis differs from previous studies as it does not require a low-dimensional structure of the input data. We also examine the minimax optimality of deep learning and compare its performance with linear estimators, specifically the kernel method. Results indicate that deep learning exhibits better dependence on input dimensionality when the target function possesses anisotropic smoothness. Additionally, it achieves an adaptive rate for functions with spatially inhomogeneous smoothness.