The "cold posterior effect" (CPE) in Bayesian deep learning refers to the observation that the predictive performance of Bayesian neural networks can be significantly improved by artificially sharpening the Bayes posterior using a temperature parameter T < 1. Despite extensive research efforts, the CPE remains poorly understood. This study presents new evidence regarding existing explanations for the CPE, examining three hypotheses: 1. The dataset curation hypothesis, which suggests that the CPE is influenced by the curation of the dataset. The study demonstrates that the CPE does not occur in real curated datasets but can be generated in controlled experiments with varying curation strength. 2. The data augmentation hypothesis, which proposes that data augmentation is necessary for the CPE to occur. The study shows that while data augmentation can contribute to the CPE, it is not a necessary condition. 3. The bad prior hypothesis, which suggests that the CPE is linked to the prior used in the Bayesian model. The study conducts a simple experiment that highlights the strong connection between the CPE and the prior. The results indicate that the CPE can occur independently of synthetic curation, data augmentation, or bad priors. Therefore, it is unlikely that cold posteriors observed in real-world scenarios have a single simple cause, making it challenging to find a straightforward solution for cold posteriors.