This paper focuses on addressing the challenge of cross-modal alignment in Vision-and-Language Navigation (VLN) tasks. The goal of VLN is to navigate through 3D indoor environments based on given instructions. The accuracy of the predicted trajectory matching the instructions is crucial. The paper proposes a fine-grained approach to tackle the cross-modal alignment challenge. Firstly, a human-annotated fine-grained VLN dataset called Landmark-RxR is introduced to provide better supervision. Secondly, focal-oriented rewards with soft and hard forms are explored to enhance local cross-modal alignment. These rewards concentrate on critical points sampled from the Landmark-RxR dataset. Additionally, a re-initialization mechanism is proposed to evaluate the navigation process, making metrics insensitive to difficult points that may cause the agent to deviate from correct trajectories. Experimental results demonstrate the superior navigation performance of the proposed agent on Landmark-RxR, en-RxR, and R2R datasets. The dataset and code for this research are available at https://github.com/hekj/Landmark-RxR.