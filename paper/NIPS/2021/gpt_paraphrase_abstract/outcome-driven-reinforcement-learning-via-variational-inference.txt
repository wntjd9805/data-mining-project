Reinforcement learning algorithms are effective in acquiring optimal policies, but their practical application requires making several design decisions. These decisions include manually designing reward functions that not only define the task but also provide enough guidance to accomplish it. In this study, we propose a different approach to reinforcement learning, viewing it as inferring policies that achieve desired outcomes rather than maximizing rewards. To address this inference problem, we introduce a novel variational inference formulation that allows us to derive a well-shaped reward function directly from interactions with the environment. Additionally, we develop a new probabilistic Bellman backup operator based on this variational objective and use it to create an off-policy algorithm for goal-directed tasks. Our experimental results demonstrate that this method eliminates the need for manually crafting reward functions for a wide range of manipulation and locomotion tasks, leading to successful goal-directed behaviors.