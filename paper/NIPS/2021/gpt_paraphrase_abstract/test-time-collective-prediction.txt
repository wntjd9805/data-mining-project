In machine learning, there is a growing trend of multiple parties with their own data wanting to collaborate and make joint predictions for future test points. These parties aim to benefit from the collective expertise of all agents to improve individual predictions, but they may not be willing to share labeled data or model parameters. In this study, we introduce a decentralized mechanism for making collective predictions at test time, drawing inspiration from social science literature on human consensus-making. Our approach allows agents to exchange information through a query model and utilizes their pre-trained models without external validation, model retraining, or data pooling. Theoretical analysis reveals that our approach achieves optimal combination of independent, unbiased estimators by recovering inverse mean-squared-error weighting in the large-sample limit. Empirical results demonstrate that our scheme effectively combines models with varying quality across the input space, outperforming classical model averaging and even weighted averaging schemes with access to additional validation data. Additionally, we propose a decentralized Jackknife procedure to assess the sensitivity of collective predictions to individual agent opinions.