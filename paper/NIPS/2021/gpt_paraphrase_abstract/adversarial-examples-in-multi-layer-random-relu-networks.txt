Adversarial examples in ReLU networks with independent Gaussian parameters are investigated. The study focuses on networks with constant depth and a wide range of widths. It is found that even small perturbations in input vectors can lead to significant changes in outputs. This extends previous findings on networks with rapidly decreasing width and two-layer networks. The proof demonstrates that adversarial examples occur because the functions computed by these networks are locally similar to random linear functions. The presence of bottleneck layers is crucial, as the width of the network up to a certain point determines the scales and sensitivities of the mappings computed up to that point. While the main result applies to networks with constant depth, it is also shown that some constraint on depth is necessary, as deep networks have the potential to compute functions that are close to constant with constant probability.