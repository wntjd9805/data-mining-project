Recent advancements in computer vision have focused on the use of Transformers. However, most studies have only explored the application of Transformers in feature representation learning for tasks like image classification and dense predictions. The generalizability of Transformers in other areas remains unknown. This study aims to investigate the potential of using Transformers for image matching and metric learning using pairs of images. It is found that existing Transformer models like Vision Transformer (ViT) and vanilla Transformer with decoders are not suitable for image matching due to their lack of image-to-image attention. To address this issue, two naive solutions are proposed: query-gallery concatenation in ViT and query-gallery cross-attention in the vanilla Transformer. The latter shows some improvement but is still limited. This suggests that the attention mechanism in Transformers is primarily designed for global feature aggregation and not naturally suited for image matching. To overcome this limitation, a new simplified decoder is proposed, which eliminates the full attention implementation and only focuses on query-key similarity computation. Additionally, global max pooling and a multilayer perceptron (MLP) head are used to decode the matching result. This simplified decoder is not only computationally more efficient but also more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identification, with significant performance gains in Rank-1 and mAP metrics on multiple popular datasets. The code for TransMatcher is available at https://github.com/ShengcaiLiao/QAConv.