Unsupervised representation learning methods have been successful in vision tasks by learning invariant representations to data augmentations. However, this invariance can be detrimental to downstream tasks that rely on specific characteristics of the augmentations, such as location or color. This issue is not limited to unsupervised learning but also occurs in supervised learning, where the model predicts the same label for all augmented samples of an instance. To address this, we propose AugSelf, an auxiliary self-supervised loss that learns the difference in augmentation parameters between two randomly augmented samples. AugSelf aims to preserve augmentation-aware information in representations for improved transferability. It can be easily integrated into existing state-of-the-art representation learning methods with minimal additional training cost. Extensive experiments show that AugSelf consistently enhances the transferability of representations learned by supervised and unsupervised methods in various transfer learning scenarios. The code can be accessed at https://github.com/hankook/AugSelf.