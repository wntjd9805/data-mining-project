This paper tackles the challenge of selecting policies in domains where there is an abundance of logged data but a limited interaction budget. This problem is crucial for safely evaluating and deploying offline reinforcement learning policies in various fields such as industry, robotics, and recommendation systems. While several off-policy evaluation (OPE) techniques have been developed to assess policy value using logged data, there is still a significant gap between OPE evaluation and full online evaluation in real-world environments. However, practical constraints often prevent the collection of large amounts of online interactions. To address this issue, we propose an innovative approach called active offline policy selection. This sequential decision approach combines logged data with online interaction to identify the best policy. It utilizes OPE estimates to initialize the online evaluation process. Furthermore, to make efficient use of limited environment interactions, we employ a Bayesian optimization method with a kernel function that represents policy similarity to determine which policy to evaluate next. Through multiple benchmark tests with numerous candidate policies, we demonstrate that our approach outperforms state-of-the-art OPE estimates and pure online policy evaluation.