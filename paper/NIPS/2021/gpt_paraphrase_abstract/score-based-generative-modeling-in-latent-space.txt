We propose a new approach called Latent Score-based Generative Model (LSGM) that trains generative models in a latent space using the variational autoencoder framework. This allows us to create more expressive models, handle non-continuous data, and generate smoother models with fewer network evaluations and faster sampling. To make LSGM scalable and stable, we introduce a new score-matching objective, a parameterization of the score function, and techniques for reducing variance in the training objective. LSGM achieves state-of-the-art results on CIFAR-10 and CelebA-HQ-256 datasets, outperforming existing generative models in terms of sample quality and sampling time. It also achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our implementation is available at https://github.com/NVlabs/LSGM.