Deep networks have achieved impressive results in object detection. However, when these networks are subsequently trained on new classes without any examples from the original classes, their performance drops significantly. This is known as catastrophic forgetting. Existing incremental learning methods have been proposed to address this issue, but they require the base classes to be present in the training data of the new classes, which is not practical in real-world scenarios. In this study, we propose a more practical approach where there is no co-occurrence of the base and new classes in the object detection task. We suggest using unlabeled in-the-wild data to bridge this non co-occurrence. To achieve this, we introduce a blind sampling strategy based on the responses of the base-class model and pre-trained novel-class model to select a smaller relevant dataset from the large in-the-wild dataset for incremental learning. Additionally, we design a dual-teacher distillation framework to transfer knowledge from the base- and novel-class teacher models to the student model using the sampled in-the-wild data. Experimental results on the PASCAL VOC and MS COCO datasets demonstrate the superiority of our proposed method compared to other state-of-the-art class-incremental object detection methods when there is no co-occurrence between the base and novel classes during training.