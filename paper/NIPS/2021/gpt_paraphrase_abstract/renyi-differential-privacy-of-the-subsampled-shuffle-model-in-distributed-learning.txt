We investigate privacy in a distributed learning setting where clients collectively construct a learning model through interactions with a privacy-preserving server. We focus on the case where a small fraction of data samples are randomly selected in each round for the learning process, which enhances privacy. To further enhance local privacy guarantees, we adopt the shuffle privacy model, where each client randomizes its response using a local differentially private mechanism and the server only receives a random permutation (shuffle) of the clients' responses without knowing their association to each client. The main contribution of this paper is a trade-off between privacy and optimization performance for discrete randomization mechanisms in this sub-sampled shuffle privacy model. We introduce a new theoretical technique to analyze the Rényi Differential Privacy of the sub-sampled shuffle model, which allows us to achieve significant improvements in privacy guarantees compared to the state-of-the-art approximate Differential Privacy guarantee for sub-sampled shuffled models. Numerical experiments also demonstrate notable enhancements in privacy-learning performance using real datasets. However, there remains an open question regarding the gap between lower and upper privacy bounds in our Rényi Differential Privacy analysis.