The Transformer architecture is widely used in natural language processing and computer vision but has not performed well in graph-level prediction tasks compared to other graph neural network (GNN) models. This paper introduces Graphormer, a modified version of the Transformer architecture that achieves excellent results in graph representation learning, particularly in the OGB Large-Scale Challenge. The key insight of Graphormer is effectively encoding the structural information of a graph into the model. The paper proposes simple yet effective methods for structural encoding, which improve the modeling of graph-structured data. Additionally, the expressive power of Graphormer is mathematically characterized, and it is shown that popular GNN variants can be seen as special cases of Graphormer when using these structural encoding methods. The code and models of Graphormer will be publicly available at https://github.com/Microsoft/Graphormer.