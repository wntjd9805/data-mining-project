The robustness of machine learning models is crucial for security applications, as adversaries aim to evade neural network based detectors. Previous research has primarily focused on creating adversarial examples with small, uniform perturbations across features to maintain imperceptibility. However, this approach does not produce realistic adversarial examples in domains like malware, finance, and social networks where features have meaningful dependencies. Our proposed approach introduces non-uniform perturbations that accurately represent these feature dependencies during adversarial training. We utilize characteristics of the empirical data distribution, including feature correlations and importance, and evaluate our method using datasets for malware classification, credit risk prediction, and spam detection. The results demonstrate that our approach is more robust against real-world attacks. Additionally, we introduce robustness certification using non-uniform perturbation bounds and show that they achieve better certification outcomes.