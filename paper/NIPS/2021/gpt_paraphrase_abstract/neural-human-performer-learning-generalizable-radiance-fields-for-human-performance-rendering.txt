This paper presents a new method called Neural Human Performer, which aims to synthesize a free-viewpoint video of a human performance using sparse multi-view cameras. The method addresses the challenge of heavy occlusions and dynamic articulations of body parts in capturing human appearance. It introduces a temporal transformer to aggregate tracked visual features based on skeletal body motion over time, and a multi-view transformer to integrate observations from multiple views. Experimental results demonstrate that the proposed method outperforms existing methods on unseen identities and poses. The video results and code can be found at the provided URL.