Unsupervised domain adaptation is a method used to align a labeled source domain and an unlabeled target domain. However, accessing the source data raises concerns about data privacy, portability, and transmission efficiency. In this study, we focus on unsupervised model adaptation (UMA), which aims to adapt source-trained models to target distributions without accessing the source data. To achieve this, we propose a novel technique called historical contrastive learning (HCL). HCL addresses the UMA challenge by introducing two perspectives: historical contrastive instance discrimination (HCID) and historical contrastive category discrimination (HCCD). HCID learns from target samples by contrasting their embeddings generated by the currently adapted model and historical models. It encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. HCCD pseudo-labels target samples to learn category-discriminative target representations. It re-weights pseudo labels based on their prediction consistency across the current and historical models. Extensive experiments demonstrate that HCL outperforms state-of-the-art methods across various visual tasks and setups.