Previous successful AI algorithms in complex games have focused on games with a limited number of actions. However, Diplomacy, a game with an extremely large number of possible actions, has not been effectively addressed. To overcome this challenge, we propose an algorithm that combines action exploration and equilibrium approximation. The algorithm learns a policy proposal network while performing value iteration and uses a double oracle step to discover new actions to include in the policy proposals. Equilibrium search is employed to compute the target state value and policy during model training. We apply this algorithm to train an agent called DORA from scratch in a two-player variant of Diplomacy and demonstrate its superior performance. Moreover, we extend our approach to no-press Diplomacy and achieve the first instance of training an agent without any human data. Our findings reveal that the agent's strategy differs from those trained with human data, indicating the existence of multiple equilibria in Diplomacy. This suggests that relying solely on self play may not be enough to achieve superhuman performance in Diplomacy.