Stochastic model-based methods have gained attention for their robustness and efficiency. We propose two extensions to improve these methods for stochastic weakly convex optimization. Firstly, we suggest using a set of samples to approximate the model function in each iteration, achieving linear speedup even for non-smooth and non-convex problems. We develop a novel sensitivity analysis of the proximal mapping involved in each iteration. Secondly, we introduce a new stochastic extrapolated model-based method inspired by momentum stochastic gradient descent. This extends the Polyak momentum technique to a wider class of stochastic algorithms for weakly convex optimization. We establish convergence rates to a natural stationarity condition over a flexible range of extrapolation terms. We also apply these methods to stochastic convex optimization, providing a new complexity bound and promising linear speedup. Additionally, we present an accelerated model-based method based on Nesterov's momentum, achieving optimal complexity for reaching optimality.