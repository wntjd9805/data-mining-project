Experience replay is a technique used in reinforcement learning to store and reuse past samples. Prioritized sampling is a method that aims to make better use of these stored samples. Previous prioritization criteria, such as TD error, recentness, and corrective feedback, have been designed heuristically. This study proposes an optimal prioritization strategy for the Bellman update based on the objective of minimizing regret, which directly maximizes the policy's return. The theory suggests assigning higher weights during sampling to data with higher hindsight TD error, better on-policiness, and more accurate Q value. While previous criteria only partially consider this strategy, this work provides theoretical justifications for them and introduces two new methods to compute the prioritization weight: ReMERN and ReMERT. ReMERN utilizes an error network, while ReMERT leverages the temporal ordering of states. Both methods outperform previous prioritized sampling algorithms in challenging reinforcement learning benchmarks like MuJoCo, Atari, and Meta-World.