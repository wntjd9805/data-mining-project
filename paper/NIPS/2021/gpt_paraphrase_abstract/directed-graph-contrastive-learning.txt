Graph Contrastive Learning (GCL) is a technique used to learn generalizable representations by leveraging contrastive views. However, there are two main concerns with current GCL methods. Firstly, changing the graph structure through data augmentation to generate contrastive views can lead to a loss of important graph structural information, particularly in directed graphs. Secondly, existing GCL approaches often use predefined contrastive views with manually selected parameters, which limits the utilization of the informative potential of data augmentation and results in incomplete structure information for model learning.To address these issues, this paper introduces a new method called Laplacian perturbation for directed graph data augmentation. Theoretical analysis demonstrates that this method can provide contrastive information without altering the directed graph structure. Additionally, a directed graph contrastive learning framework is proposed, which dynamically learns from all possible contrastive views generated by Laplacian perturbation. The model is trained using multi-task curriculum learning, progressively learning from easier to more challenging contrastive views.Empirical results show that our model is able to retain more structural features of directed graphs compared to other GCL models, thanks to its ability to provide complete contrastive information. Experimental evaluations on various benchmarks demonstrate the superiority of our approach over state-of-the-art methods.