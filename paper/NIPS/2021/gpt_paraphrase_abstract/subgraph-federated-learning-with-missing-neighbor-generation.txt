Graphs are commonly used in data mining and machine learning because they provide a unique representation of real-world objects and their interactions. As graphs become larger, it is common for subgraphs to be collected and stored separately in local systems. This creates a need for subgraph federated learning, where each local system holds a biased subgraph and aims to collaboratively train a powerful graph mining model without directly sharing their graph data. This study introduces two techniques: FedSage, which integrates node features, link structures, and task labels on multiple local subgraphs, and FedSage+, which deals with missing links across local subgraphs. Empirical results on real-world graph datasets demonstrate the effectiveness and efficiency of these techniques, with theoretical implications for their generalization ability on global graphs.