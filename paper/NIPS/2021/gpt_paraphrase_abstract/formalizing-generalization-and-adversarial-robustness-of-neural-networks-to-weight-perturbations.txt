Understanding the impact of weight perturbation on neural networks' performance is crucial for various machine learning tasks. This paper presents the first comprehensive study on feed-forward neural networks, focusing on their robustness in pairwise class margin and generalization under weight perturbation. Additionally, a novel loss function is proposed to train neural networks that are both generalizable and robust against weight perturbations. Empirical experiments are conducted to validate the theoretical analysis, providing valuable insights into the generalization and robustness of neural networks against weight perturbations.