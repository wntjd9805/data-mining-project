Bayesian optimization (BO) is a powerful method for optimizing functions that are expensive to evaluate and have unknown properties. To balance the trade-off between cost and accuracy, many applications allow the function to be evaluated at different levels of fidelity. This paper introduces a new approach called Batch Multi-fidelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN) that aims to reduce optimization cost while maximizing the benefit-cost ratio. BMBO-DARN utilizes a set of Bayesian neural networks to create a fully auto-regressive model, capable of capturing complex relationships across all levels of fidelity, thereby improving surrogate learning and optimization performance. Additionally, a simple and efficient batch querying method is proposed to enhance the quality and diversity of queries. This method is based on the Max-value Entropy Search principle and penalizes highly correlated queries while encouraging diversity. The acquisition function is computed using posterior samples and moment matching, and an alternating optimization process is employed for each fidelity-input pair, ensuring continuous improvement. The effectiveness of BMBO-DARN is demonstrated through experiments on four real-world hyperparameter optimization applications.