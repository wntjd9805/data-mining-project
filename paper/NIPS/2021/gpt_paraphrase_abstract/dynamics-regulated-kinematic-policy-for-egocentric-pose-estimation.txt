We propose a novel approach for estimating the pose of objects in a 3D egocentric viewpoint. Our method combines kinematics modeling, dynamics modeling, and scene object information to achieve accurate results. Unlike previous approaches that separately utilize kinematics or dynamics, we integrate these two approaches through dynamics-regulated training. In each timestep, a kinematic model uses video evidence and simulation state to provide a target pose. Then, a prelearned dynamics model attempts to replicate the kinematic pose in a physics simulator. By comparing the poses instructed by the kinematic model and generated by the dynamics model, we can enhance the accuracy of the kinematic model. Additionally, incorporating the 6DoF pose of objects in the scene allows us to estimate realistic 3D human-object interactions using just a single wearable camera. We evaluate the effectiveness of our egocentric pose estimation method in both controlled laboratory settings and real-world scenarios.