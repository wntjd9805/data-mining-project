We investigate privacy concerns in the context of finite-horizon Markov Decision Processes (MDPs) for reinforcement learning algorithms. To protect sensitive user data from third parties, we propose obfuscating information on the user side using local differential privacy (LDP). We demonstrate that ensuring privacy through LDP has a multiplicative effect on regret minimization in MDPs, making the learning problem more complex. However, we present an optimistic algorithm that satisfies ε-LDP requirements and achieves √K/ε regret in any finite-horizon MDP after K episodes, matching the lower bound dependency on the number of episodes.