To address the increasing compute demands of modern deep learning applications, large corporations and institutions rely on expensive High-Performance Computing clusters. This creates a disparity, as smaller organizations cannot afford such resources, limiting their research opportunities. To bridge this gap, smaller groups can pool their computational resources and engage in collaborative experiments, known as grid- or volunteer computing. However, using this approach for machine learning is challenging due to latency, bandwidth issues, and unique volunteer computing challenges. In this study, we carefully analyze these constraints and propose a new algorithmic framework specifically designed for collaborative training. We validate our approach by successfully applying it to SwAV and ALBERT pretraining under realistic conditions, achieving performance comparable to traditional setups at a significantly lower cost. Additionally, we present a detailed report on the successful collaborative pretraining of a language model involving 40 participants.