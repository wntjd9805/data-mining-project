Vision transformers (ViTs) have achieved state-of-the-art performance in visual recognition tasks by tokenizing images into patches and applying self-attention. However, self-attention modules have a quadratic complexity in computation and memory usage. Previous attempts to approximate self-attention with linear complexity in Natural Language Processing have proven to be flawed or ineffective for visual recognition. This study identifies that the limitations of these approaches stem from the softmax self-attention operation used in conventional models. To address this, a novel method called softmax-free transformer (SOFT) is proposed. SOFT replaces the dot-product similarity with a Gaussian kernel function and avoids normalization, allowing for the approximation of a full self-attention matrix through low-rank matrix decomposition. The robustness of the approximation is ensured by calculating its Moore-Penrose inverse using a Newton-Raphson method. Experimental results on ImageNet demonstrate that SOFT significantly improves the computational efficiency of existing ViT variants. Moreover, the linear complexity of SOFT enables the use of longer token sequences, resulting in a better trade-off between accuracy and complexity.