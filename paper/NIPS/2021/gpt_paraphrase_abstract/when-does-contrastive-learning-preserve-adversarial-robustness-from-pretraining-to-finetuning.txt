Contrastive learning (CL) is a method that can learn feature representations and achieve high performance in downstream tasks by fine-tuning a linear classifier. However, it is unclear whether CL can maintain robustness to adversarial attacks in image classification. This is because there is a mismatch in learning tasks from pretraining to fine-tuning, leading to a challenge called "cross-task robustness transferability." To address this challenge, this paper proposes a new framework called ADVCL, which enhances robustness in CL by considering the design of contrastive views and augmenting CL with pseudo-supervision stimulus. ADVCL is shown to improve cross-task robustness transferability without sacrificing model accuracy and fine-tuning efficiency. Experimental results demonstrate that ADVCL outperforms existing self-supervised robust learning methods across multiple datasets and fine-tuning schemes. The code for ADVCL is available at https://github.com/LijieFan/AdvCL.