Different graph contrastive learning models have been proposed in recent years to enhance learning tasks on graph datasets. However, these models are typically customized and differ in view augmentations, architectures, and objectives. This raises the question of how to build a graph contrastive learning model from scratch for specific graph learning tasks and datasets. To address this gap, we propose a framework called InfoGCL that focuses on the transformation and transfer of graph information during the contrastive learning process. Our framework follows the Information Bottleneck principle to minimize the mutual information between contrastive parts while preserving task-relevant information at both the individual module and overall framework levels. We demonstrate that our framework unifies all recent graph contrastive learning methods and validate our theoretical analysis through empirical experiments on node and graph classification benchmark datasets. Our algorithm outperforms state-of-the-art methods, showcasing the effectiveness of our approach.