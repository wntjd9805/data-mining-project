This paper focuses on model-free robust reinforcement learning, which seeks to find a policy that optimizes the worst-case performance over a set of uncertain Markov Decision Processes (MDPs). The uncertainty set is centered around a misspecified MDP, and its specific form is unknown. To address this, the authors propose a sample-based approach to estimate the unknown uncertainty set. They then develop robust Q-learning and robust TDC algorithms for tabular and function approximation settings, respectively. These algorithms can be implemented online and incrementally. The authors prove that the robust Q-learning algorithm converges to the optimal robust Q function, while the robust TDC algorithm converges asymptotically to stationary points. Unlike previous work, the proposed algorithms do not require additional conditions on the discount factor for convergence. The authors also provide finite-time error bounds for both algorithms and demonstrate through numerical experiments that they are robust. Furthermore, they suggest that their approach can be extended to other algorithms such as TD, SARSA, and other GTD algorithms.