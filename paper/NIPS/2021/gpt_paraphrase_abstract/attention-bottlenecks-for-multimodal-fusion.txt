Traditional machine perception models are typically optimized for processing information from a single modality, such as vision or audio. As a result, the fusion of multimodal inputs is often done at a late stage, which is known as "late-fusion." In this study, we propose a new transformer-based architecture that incorporates "fusion bottlenecks" at multiple layers to facilitate modality fusion. Unlike traditional approaches, our model forces information between different modalities to pass through a small number of bottleneck layers, allowing the model to condense and share relevant information. This strategy improves fusion performance while also reducing computational cost. We conducted extensive experiments and achieved state-of-the-art results on various audio-visual classification benchmarks. The code and models used in this study will be made publicly available.