The predict-then-optimize framework aims to train a model that predicts optimization problem parameters based on environment features, in order to maximize decision quality. Recent research suggests that incorporating the optimization problem in the training process improves decision quality and generalization compared to using an intermediate loss function. This study focuses on applying this framework to sequential decision problems solved through reinforcement learning. The challenges faced include dealing with large state and action spaces and high-dimensional policy spaces. To address these challenges, unbiased derivative sampling is used to approximate and differentiate through optimality conditions, and a low-rank approximation is used for high-dimensional sample-based derivatives. Bellman-based and policy gradient-based decision-focused learning are implemented on three different problems, demonstrating improved generalization to unseen tasks.