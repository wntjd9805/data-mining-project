Federated learning (FL) is an efficient approach for training models using local data on edge devices. However, two major challenges in FL are slow devices (stragglers) and malicious attacks from adversaries. These issues are concerning in practical FL systems, but no existing schemes effectively address them together. To tackle both stragglers and adversaries, we propose Sageflow, a method that combines staleness-aware grouping, entropy-based filtering, and loss-weighted averaging. By grouping and weighting models based on their arrival delay, Sageflow can handle stragglers effectively. Additionally, the combination of entropy-based filtering and loss-weighted averaging provides robust defense against various adversary attacks. We establish a theoretical bound to gain insights into the convergence behavior of Sageflow. Extensive experiments demonstrate that Sageflow outperforms existing methods designed to handle stragglers and adversaries.