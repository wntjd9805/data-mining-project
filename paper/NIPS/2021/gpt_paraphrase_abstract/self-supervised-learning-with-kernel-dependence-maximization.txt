We propose a self-supervised learning method called SSL-HSIC that focuses on the statistical dependence between image representations. By maximizing the dependence between transformed image representations and the image identity, while minimizing the kernelized variance of those representations, SSL-HSIC offers a new perspective on InfoNCE, a lower bound on mutual information (MI) between different transformations. Unlike MI, which can lead to meaningless representations, the bound of SSL-HSIC approximates it more effectively. Our approach also sheds light on BYOL, a negative-free self-supervised learning method, as SSL-HSIC also learns local neighborhoods of samples. SSL-HSIC allows for direct optimization of statistical dependence with a time complexity linear to the batch size, without relying on restrictive data assumptions or indirect mutual information estimators. Regardless of whether a target network is used or not, SSL-HSIC achieves state-of-the-art performance on various tasks such as ImageNet evaluation, semi-supervised learning, and transfer learning for classification, semantic segmentation, depth estimation, and object recognition. The code is available at https://github.com/deepmind/ssl_hsic.