This paper introduces SBO-RNN, a family of recurrent neural networks (RNNs) formulated using stochastic bilevel optimization (SBO). By utilizing stochastic gradient descent (SGD), the SBO problem is transformed into an RNN, where feedforward and backpropagation handle the lower and upper-level optimization respectively. The paper proves that training SBO-RNN does not suffer from vanishing or exploding gradient under certain conditions. Empirical results on benchmark datasets demonstrate the superior performance of SBO-RNN, with fewer parameters, less training data, and faster convergence. The code is available at https://zhang-vislab.github.io.