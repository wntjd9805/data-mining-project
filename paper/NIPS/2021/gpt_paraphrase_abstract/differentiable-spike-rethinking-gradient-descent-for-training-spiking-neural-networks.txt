Spike neural networks (SNNs) mimic the spiking nature of brain neurons and are known for their energy efficiency. However, training high-performing SNNs from scratch is challenging due to the discrete spike nature, which hinders gradient calculation. To address this, the surrogate gradient (SG) approach has been proposed as a continuous relaxation. However, it is unclear how SG benefits SNN training. In this study, we analyze the gradient descent problem in SNN training and propose a new family of Differentiable Spike (Dspike) functions that can adaptively evolve during training to optimize gradient estimation. Experimental results demonstrate that training SNNs with Dspike consistently outperforms existing methods. For instance, on the CIFAR10-DVS classification task, a spiking ResNet-18 achieves a top-1 accuracy of 75.4% with 10 time steps.