In this paper, we propose a new approach to task-agnostic exploration. Instead of assuming isolated environments and no prior knowledge, we acknowledge that agents learn in multiple environments and have prior experiences. We suggest that exploration is a lifelong process and should be evaluated accordingly. Our new setup involves the agent first learning to explore across various environments without any specific goal. Then, the agent can effectively transfer this learned exploration policy to better explore new environments when solving tasks. We evaluate different baseline exploration strategies and present a simple yet effective method for learning task-agnostic exploration policies. Our approach considers two components of exploration: the agent-centric component encourages exploration of unseen parts of the environment based on the agent's beliefs, while the environment-centric component encourages exploration of inherently interesting objects. We demonstrate the effectiveness of our formulation and its consistency across multiple training-testing environment pairs. Additionally, we introduce benchmarks and metrics to evaluate task-agnostic exploration strategies. The source code for our work is available at https://github.com/sparisi/cbet/.