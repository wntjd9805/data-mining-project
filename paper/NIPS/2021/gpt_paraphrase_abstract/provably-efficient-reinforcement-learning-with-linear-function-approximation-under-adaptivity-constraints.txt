This study examines reinforcement learning (RL) with linear function approximation while considering two limited adaptivity models: batch learning and rare policy switch. The researchers propose two efficient online RL algorithms for episodic linear Markov decision processes, where the transition probability and reward function can be represented as a linear function of a known feature mapping.   For the batch learning model, the proposed LSVI-UCB-Batch algorithm achieves regret of O(d3H3T + dHT/B), where d is the dimension of the feature mapping, H is the episode length, T is the number of interactions, and B is the number of batches. The result suggests that using only pT/dH batches is sufficient to obtain O(d3H3T) regret.  For the rare policy switch model, the proposed LSVI-UCB-RareSwitch algorithm achieves regret of O(pd3H3T[1 + T/(dH)]dH/B), implying a regret of dH log T policy d3H3T. These algorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al., 2020) with less adaptivity.   Additionally, a lower bound is established for the batch learning model, indicating that the dependency on B in the regret bound is tight.