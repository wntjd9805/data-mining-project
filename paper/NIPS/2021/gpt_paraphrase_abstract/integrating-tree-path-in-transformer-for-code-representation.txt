This paper investigates the integration of two path encoding methods into the attention module of the Transformer model for learning distributed representation of source code. The encoding methods involve capturing pairwise paths between code tokens and paths from leaf nodes to the tree root for each token in the syntax tree. The study explores the interaction between these two types of paths within the Transformer framework. Through extensive experiments and ablation studies on code summarization in various languages, the authors introduce TPTrans, a novel representation model that outperforms strong baselines. The effectiveness of the proposed approaches is demonstrated, and the code for TPTrans is made available at https://github.com/AwdHanPeng/TPTrans.