Research on continual learning has resulted in various strategies for addressing the issue of catastrophic forgetting in feed-forward classification networks. Surprisingly, little attention has been given to continual learning of recurrent models in the context of image captioning. In this study, we systematically investigate the continual learning of LSTM-based models for image captioning. We propose an attention-based approach called Recurrent Attention to Transient Tasks (RATT), which explicitly accounts for the dynamic nature of vocabularies in continual image captioning tasks, where task vocabularies are not mutually exclusive. Additionally, we demonstrate how existing continual learning techniques based on weight regularization and knowledge distillation can be adapted for recurrent continual learning problems. To evaluate our approaches, we introduce two new continual learning benchmarks using the MS-COCO and Flickr30 datasets, specifically designed for incremental image captioning. Our results show that RATT successfully learns five captioning tasks sequentially without forgetting previously learned ones.