Metric learning is a technique used to improve distance-based methods like the nearest neighbor classifier. While efforts have been made to enhance its performance and analyze its generalization ability, the impact of algorithmic parameters on metric learning remains unclear. This paper addresses this question by proving the Probably Approximately Correct (PAC) learnability for metric learning algorithms with non-convex objective functions optimized using gradient descent (GD). The study shows that the generalization PAC bound is sufficient for agnostic PAC learnability and can be achieved by ensuring uniform convergence on a densely concentrated subset of the parameter space. Additionally, the research demonstrates that the generalizability of classifiers optimized using GD can be guaranteed if both the classifier and loss function are Lipschitz smooth, and can be further improved by reducing the number of iterations. To apply these theoretical findings, a novel metric learning method called SMILE is proposed, which satisfies the Lipschitz smoothness property and is learned using GD with an early stopping mechanism. This method aims to enhance discriminability and reduce computational costs for nearest neighbor classification.