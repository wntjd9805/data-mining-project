We investigate the problem of sequential decision-making, where each agent aims to maximize the expected total reward while satisfying a constraint on the expected total utility. To address this problem, we propose a new method called Natural Policy Gradient Primal-Dual (NPG-PD) for Constrained Markov Decision Processes (CMDPs). Our method updates the primal variable using natural policy gradient ascent and the dual variable using projected sub-gradient descent. Despite the nonconcave objective function and nonconvex constraint set, we prove that our method achieves global convergence with sublinear rates for both the optimality gap and the constraint violation. This convergence is independent of the size of the state-action space. We also establish sublinear convergence rates for the general smooth policy class, accounting for function approximation errors caused by restricted policy parametrization. Additionally, we demonstrate that two sample-based NPG-PD algorithms inherit these convergence properties and provide guarantees on their finite-sample complexity. Our work is the first to establish non-asymptotic convergence guarantees for policy-based primal-dual methods in solving infinite-horizon discounted CMDPs. We support our findings with computational results that highlight the advantages of our approach.