Active learning, a semi-supervised method, was introduced to determine if adaptive labeling can learn concepts with significantly fewer labeled samples. Previous studies have shown that active learning is not superior to supervised learning for certain concept classes like linear separators. However, by incorporating weak distributional assumptions and allowing comparison queries, active learning can achieve exponential reduction in sample size. Moreover, these findings also apply to the Reliable and Probably Useful (RPU) learning model, where the learner cannot make mistakes but can answer "I don't know". While previous results indicated that RPU learning has high sample complexity for label queries, we demonstrate that comparison queries make it at most logarithmically more expensive in both passive and active learning scenarios.