Learning disentangled representations is crucial for effectively understanding and modeling the underlying structure of environments. Physics has achieved significant success in describing the universe through symmetry-preserving transformations. Building upon this concept, we propose a framework based on group representation theory to learn representations of dynamic environments by focusing on the transformations that drive their evolution. In our experimental approach, we successfully learn the structure of explicitly symmetric environments without needing supervision, relying solely on observational data from sequential interactions. To ensure the interpretability of the learned representations, we introduce a disentanglement regularization that encourages intuitive separability. Our method demonstrates accurate long-term predictions, and we establish a correlation between the quality of predictions and the degree of disentanglement in the latent space.