A recent advancement in deep learning theory has introduced the concept of a neural tangent kernel (NTK), which characterizes the training of over-parameterized deep neural networks. However, it is acknowledged that this theory does not align perfectly with practical scenarios, as it requires network weights to remain close to their initial values throughout training and does not accommodate regularizers or gradient noises. This study presents a generalized analysis of the neural tangent kernel and demonstrates that noisy gradient descent with weight decay can still exhibit similar behavior to a kernel function. Consequently, the training loss converges linearly until a certain level of accuracy is reached. Furthermore, a new bound for generalization error is established for two-layer neural networks trained using noisy gradient descent with weight decay.