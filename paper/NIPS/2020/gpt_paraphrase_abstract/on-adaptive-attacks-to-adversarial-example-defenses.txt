Adaptive attacks have become the standard for evaluating defenses against adversarial examples. However, current evaluations are incomplete. We show that thirteen defenses from recent conferences can be bypassed despite being tested with adaptive attacks. Unlike previous evaluation papers, our focus is on the methodology and approach needed for adaptive attacks. While some attack strategies are generalizable, no single strategy works for all defenses. This highlights the need for careful tuning to each defense and emphasizes that adaptive attacks cannot be automated. Our analysis provides guidance for properly conducting adaptive attacks and encourages progress in developing more robust models.