Current deep learning systems lack the ability to continuously learn and adapt to new tasks while retaining previously acquired knowledge. This research introduces a new methodology called MERLIN (Meta-Consolidation for Continual Learning) that addresses this limitation. MERLIN assumes that the weights of a neural network for solving a specific task come from a meta-distribution, which is incrementally consolidated. The study focuses on the challenging online continual learning setting, where each data point is only seen once by the model. Experimental results using MNIST, CIFAR-10, CIFAR-100, and Mini-ImageNet datasets demonstrate consistent improvement over five baselines, including a recent state-of-the-art technique, confirming the potential of MERLIN.