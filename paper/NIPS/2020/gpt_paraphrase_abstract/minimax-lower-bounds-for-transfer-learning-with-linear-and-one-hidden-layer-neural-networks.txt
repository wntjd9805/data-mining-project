Transfer learning is a valuable technique for improving the performance of machine learning models when there is limited labeled training data available for a specific task. It involves using a model trained on a different but related task, which has abundant labeled data, as a starting point for training a model on the target task. Despite the success of transfer learning in practice, its benefits and limitations are not well understood. In this study, we develop a statistical minimax framework to better understand the fundamental limits of transfer learning in regression tasks using linear and one-hidden layer neural network models. We establish a lower-bound for the generalization error of any algorithm in the target task, based on the number of labeled data in both the source and target tasks, as well as the similarity between the tasks. Our lower-bound provides new insights into the advantages and limitations of transfer learning. We also support our theoretical findings with experimental evidence.