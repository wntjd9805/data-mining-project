Out-of-distribution (OOD) testing is becoming increasingly popular in evaluating the ability of a machine learning system to generalize beyond the biases of its training set. OOD benchmarks are created to present a different distribution of data and labels between training and testing phases. However, in the case of VQA-CP, the standard OOD benchmark for visual question answering, we have identified three concerning practices in its current usage. Firstly, most published methods rely on explicit knowledge of how the OOD splits are constructed, often involving "inverting" the label distribution. Secondly, the OOD test set is used for model selection. Lastly, the in-domain performance of a model is assessed after retraining it on in-domain splits that have a more balanced label distribution. These practices undermine the objective of evaluating generalization and call into question the value of methods specifically designed for this dataset. Surprisingly simple methods, such as generating random answers, outperform state-of-the-art approaches on certain question types. We propose short- and long-term solutions to avoid these issues and fully leverage the benefits of OOD evaluation. The answer format only provides the abstraction.