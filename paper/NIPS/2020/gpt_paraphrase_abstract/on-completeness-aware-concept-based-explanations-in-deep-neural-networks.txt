This paper explores the concept-based explainability of Deep Neural Networks (DNNs) by focusing on human explanations of high-level decisions. The authors introduce the concept of completeness, which measures the sufficiency of a set of concepts in explaining a model's prediction behavior. They propose a concept discovery method that aims to identify a complete set of concepts that are also interpretable, addressing the limitations of existing methods. To assign an importance score to each concept, they adapt game-theoretic notions and propose ConceptSHAP. The effectiveness of their method is validated through metrics and user studies on synthetic and real-world datasets, confirming its ability to find complete and interpretable concepts for decision explanations.