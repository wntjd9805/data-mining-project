We investigate the problem of estimating the mean of a distribution in high dimensions, considering scenarios where the samples are maliciously manipulated or the distribution has heavy tails. Recent advancements have introduced effective and nearly optimal procedures for each scenario individually. However, these algorithms are complex and not easily applicable to the other scenario, often requiring intricate analyses. Our paper introduces a meta-problem and an equivalence theorem that offer a unified perspective on robust and heavy-tailed mean estimation in high dimensions. We demonstrate that the meta-problem can be solved using either a modified version of the FILTER algorithm, a robust estimation method from recent literature, or the quantum entropy scoring scheme (QUE) proposed by Dong, Hopkins, and Li (NeurIPS '19). By utilizing our equivalence theorem, we derive simple and efficient algorithms for both robust and heavy-tailed settings. Notably, the QUE-based approach exhibits the same runtime as the fastest known algorithms in both scenarios. We analyze the FILTER algorithm using the regret bound of the multiplicative weights update method, enabling us to avoid the technical complexities of previous works and enhance the runtime analysis of a gradient-descent-based algorithm for robust mean estimation presented by Cheng, Diakonikolas, Ge, and Soltanolkotabi (ICML '20).