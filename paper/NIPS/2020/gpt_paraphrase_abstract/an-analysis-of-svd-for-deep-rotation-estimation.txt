SVD orthogonalization and similar techniques are commonly used for projecting matrices onto O(n) or SO(n) in computer vision applications. However, these methods are often overlooked in deep learning models, which prefer other representations like unit quaternions or Euler angles. Despite the lack of a universally effective representation for 3D rotations, we investigate the potential of SVD orthogonalization in neural networks. Through theoretical analysis and extensive quantitative evaluation, we demonstrate that replacing existing representations with SVD orthogonalization achieves state-of-the-art performance in various deep learning tasks, including supervised and unsupervised training.