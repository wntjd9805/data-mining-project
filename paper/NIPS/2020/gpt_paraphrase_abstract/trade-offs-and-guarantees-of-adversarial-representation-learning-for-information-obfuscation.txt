The use of crowdsourced data in machine learning services may contain sensitive information that users do not wish to disclose. Various methods have been proposed to reduce the risk of leaking this sensitive information while still maintaining task accuracy. However, there is limited understanding of the underlying theory behind these methods. To address this gap, we introduce a new theoretical framework for attribute obfuscation. Within this framework, we propose a minimax optimization approach to protect specific attributes and analyze its effectiveness against worst-case adversaries. It is important to note that there is often a trade-off between minimizing information leakage and maximizing task accuracy. To explore this trade-off, we establish an information-theoretic lower bound that precisely characterizes the balance between accuracy and information leakage. We validate our theoretical findings through experiments on two real-world datasets, which confirm the effectiveness of our approach and the existence of the trade-off. Our results demonstrate that adversarial learning offers the best compromise between attribute obfuscation and accuracy maximization.