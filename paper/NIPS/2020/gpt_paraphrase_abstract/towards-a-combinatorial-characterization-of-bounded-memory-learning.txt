The concept of combinatorial dimensions is crucial in the field of machine learning. Different dimensions, such as VC dimension, SQ dimension, and Littlestone dimension, have been used to characterize various types of learning. In this study, we focus on developing combinatorial dimensions that can describe learning with limited memory. Specifically, we propose a potential solution for strong learning when the data follows a known distribution, based on the SQ dimension of neighboring distributions. We provide both upper and lower bounds for our proposed solution, which coincide in certain parameter settings. This is the first characterization of strong learning with memory constraints in any parameter setting. Interestingly, in this particular parameter range, bounded memory learning is equivalent to SQ learning. We hypothesize that our characterization holds true in a broader range of parameter settings.