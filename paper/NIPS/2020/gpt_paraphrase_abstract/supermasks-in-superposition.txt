We introduce the SupSup model, which allows for the sequential learning of numerous tasks without experiencing catastrophic forgetting. Our method involves using a pre-initialized base network and finding a subnetwork (supermask) for each task that achieves good performance. The correct subnetwork can be retrieved with minimal memory usage if the task identity is known at test time. If not provided, the SupSup model can determine the task through gradient-based optimization, finding a combination of learned supermasks that minimizes the output entropy. We have observed that a single gradient step is often sufficient to identify the correct mask even among 2500 tasks. Additionally, we present two extensions of the SupSup model. Firstly, the model can be trained without any task identity information by allocating an additional supermask when uncertain about new data. Secondly, a constant-sized reservoir can store the entire set of supermasks by implicitly storing them as attractors in a fixed-sized Hopfield network.