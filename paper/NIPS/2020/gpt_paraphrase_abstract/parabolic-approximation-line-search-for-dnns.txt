A major problem in current deep learning optimization research is finding the optimal step sizes for each update step. The shape of the loss in the update step direction is closely related to the optimal step size, but this shape has not been thoroughly examined. This study demonstrates through empirical evidence that the batch loss over lines in the negative gradient direction is mostly locally convex and well-suited for one-dimensional parabolic approximations. By leveraging this parabolic property, a simple and robust line search approach is introduced, which adjusts the update steps based on the shape of the loss. This approach combines established methods such as parabolic approximation, line search, and conjugate gradient to achieve efficient performance. It outperforms other step size estimation methods and competes well with common optimization methods across a range of experiments without requiring manually designed step size schedules. This makes it particularly valuable for objectives where step size schedules are unknown or ineffective. The evaluation includes extensive hyperparameter grid searches on various datasets and architectures. Furthermore, a comprehensive investigation of exact line searches in the context of batch losses and exact losses is provided, including their relationship to the proposed line search approach.