Entropy regularization, smoothing of Q-values, and neural network function approximation are important elements of state-of-the-art reinforcement learning (RL) algorithms like Soft Actor-Critic. However, their impact on the convergence of RL algorithms is not fully understood. In this study, we analyze these techniques using the approximate dynamic programming framework. Our analysis reveals that value smoothing enhances algorithm stability but slows down convergence. Additionally, entropy regularization reduces overestimation errors but modifies the original problem. We also investigate the combination of these techniques in the Soft Actor-Critic algorithm.