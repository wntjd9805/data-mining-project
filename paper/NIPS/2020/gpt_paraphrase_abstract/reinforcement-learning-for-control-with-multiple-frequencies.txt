This paper addresses the challenge of solving real-world sequential decision problems that involve multiple action variables with different control frequencies. These problems can be formulated using the concept of multiple action persistences in factored-action MDPs (FA-MDPs), but finding efficient solutions is difficult because action-persistent policies constructed from stationary policies can be suboptimal. The authors propose a solution method called Action-Persistent Policy Iteration (AP-PI), which guarantees convergence to an optimal solution and only incurs a modest increase in time complexity compared to standard policy iteration for FA-MDPs. They also introduce Action-Persistent Actor-Critic (AP-AC), a scalable RL algorithm for high-dimensional control tasks. Experimental results demonstrate that AP-AC outperforms baselines on various control tasks, highlighting the effectiveness of the method in optimizing non-stationary policies for tasks with multiple control frequencies.