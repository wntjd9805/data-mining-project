This paper focuses on stochastic multi-armed bandits (MABs) with heavy-tailed rewards that have bounded p-th moments (1 < p ≤ 2) by a constant νp. The authors introduce a new robust estimator that does not require prior knowledge of νp, unlike existing estimators. They demonstrate that the proposed estimator has an exponentially decaying error probability. Utilizing this estimator, they propose a perturbation-based exploration strategy and develop a generalized regret analysis scheme that establishes upper and lower regret bounds by examining the relationship between regret and the cumulative density function of the perturbation. Through this analysis scheme, they derive gap-dependent and gap-independent regret bounds for various perturbations and identify optimal hyperparameters for each perturbation, achieving the minimax optimal regret bound in terms of total rounds. Simulation results indicate that the proposed estimator performs favorably compared to existing robust estimators for different p values, and the proposed perturbation strategy outperforms existing exploration methods in MAB problems.