Sample-based planning algorithms are widely used for generating intelligent behavior based on a model of the environment. The effectiveness of these algorithms depends on the quality of the candidate actions generated, especially in scenarios with continuous or large action spaces. Traditionally, candidate action generation involves exhaustive exploration of the action space or leveraging domain knowledge. More recently, learning a stochastic policy has been used to guide the search. This paper proposes a new approach that explicitly learns a candidate action generator by optimizing a novel objective called marginal utility. The marginal utility measures how much value an action adds compared to previously generated actions. The effectiveness of this approach is demonstrated in both a stochastic domain with continuous state and action spaces (curling) and a location game with a discrete but large action space. The results show that a generator trained with the marginal utility objective outperforms hand-coded schemes based on domain knowledge, trained stochastic policies, and other common objectives for generating actions in sample-based planners.