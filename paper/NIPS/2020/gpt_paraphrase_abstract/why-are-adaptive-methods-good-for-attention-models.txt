Stochastic gradient descent (SGD) is commonly used in deep learning, but adaptive methods like Clipped SGD/Adam have been found to perform better in important tasks like attention models. It is not well understood why SGD performs poorly compared to adaptive methods. This study provides empirical and theoretical evidence that a heavy-tailed distribution of noise in stochastic gradients is one reason for SGD's poor performance. The study also presents the first accurate upper and lower convergence bounds for adaptive gradient methods under heavy-tailed noise. Additionally, the study demonstrates the importance of gradient clipping in addressing heavy-tailed gradient noise. An adaptive coordinate-wise clipping algorithm (ACClip) is introduced as a practical approach for applying clipping, and its superior performance is showcased in BERT pretraining and finetuning tasks.