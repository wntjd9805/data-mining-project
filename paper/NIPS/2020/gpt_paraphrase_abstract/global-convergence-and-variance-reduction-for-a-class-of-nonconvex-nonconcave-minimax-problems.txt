Nonconvex minimax problems are common in machine learning applications like generative adversarial networks and adversarial learning. Gradient descent ascent (GDA) algorithms are commonly used to solve these problems, but they can potentially diverge even in convex-concave settings. This study demonstrates that the alternating gradient descent ascent (AGDA) algorithm converges globally at a linear rate and the stochastic AGDA achieves a sublinear rate for a subset of nonconvex-nonconcave objectives that satisfy a two-sided Polyak-≈Åojasiewicz inequality. Additionally, a variance reduced algorithm is developed that achieves a faster rate than AGDA for problems with finite-sum structure.