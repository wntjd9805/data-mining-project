Recent advancements in Generative Adversarial Networks (GANs) have made them popular for generating high-quality synthetic images. However, these models often produce unrealistic samples that are outside the range of real data. Some recently proposed techniques try to address this issue by either rejecting these unrealistic samples after generating them or by limiting the model's latent space. Although these methods are effective, they are inefficient as they waste training time and model capacity on samples that are ultimately discarded. In this study, we propose a new approach to enhance sample quality by selecting instances from the training dataset before model training. This refinement of the empirical data distribution redirects the model's capacity towards regions with high data density, resulting in improved sample fidelity, reduced model capacity requirements, and significantly shorter training time. The code for implementing this approach can be found at https://github.com/uoguelph-mlrg/instance_selection_for_gans.