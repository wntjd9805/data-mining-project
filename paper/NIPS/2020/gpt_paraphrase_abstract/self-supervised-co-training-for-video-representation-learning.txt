This paper aims to enhance self-supervised video representation learning using visual-only information. The authors contribute in three ways: (i) they explore the benefits of incorporating semantic-class positives into the training process, which improves performance; (ii) they propose a new co-training scheme that leverages the information from different views (RGB and optical flow) to enhance the popular infoNCE loss; (iii) they evaluate the learned representation on action recognition and video retrieval tasks, demonstrating competitive performance with other self-supervised approaches while requiring less training data.