The recent progress in machine learning has been accelerated by the ability to leverage large-scale hardware parallelism. Efforts have been made to develop efficient parallel versions of classic machine learning algorithms, but some algorithms are difficult to parallelize while maintaining convergence. This paper focuses on efficient parallel algorithms for inference on graphical models, specifically the belief propagation algorithm. We address the challenge of parallelizing this classic algorithm by using scalable relaxed schedulers. Our approach outperforms previous parallel implementations in terms of scalability and convergence time, as demonstrated in an empirical study on practical applications.