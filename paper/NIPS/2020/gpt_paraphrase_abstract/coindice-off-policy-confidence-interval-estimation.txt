We investigate the estimation of confidence intervals for the value of a target policy in reinforcement learning, even when only given a static dataset collected by unknown behavior policies. We propose a new algorithm called CoinDICE, which utilizes a function space embedding and generalized estimating equation constraints to solve an optimization problem. By applying the generalized empirical likelihood method to the Lagrangian, we can efficiently compute the confidence intervals. The validity of these intervals is proven theoretically in both asymptotic and finite-sample scenarios. Through empirical experiments on various benchmarks, we demonstrate that our method produces tighter and more accurate confidence interval estimates compared to existing approaches.