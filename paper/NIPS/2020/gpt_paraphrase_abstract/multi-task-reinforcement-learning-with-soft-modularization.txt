Multi-task learning in reinforcement learning is a challenging problem due to the difficulty in determining which parameters should be shared across tasks and how gradients from different tasks may interfere with each other. Instead of blindly sharing parameters, we propose a method that uses modularization to address this optimization issue. We introduce a routing network that determines different routing strategies for each task, allowing the base network to be reconfigured accordingly. Our task-specific policy combines all possible routes using soft modularization, making it suitable for sequential tasks. We conduct experiments on various robotics manipulation tasks and demonstrate that our method significantly improves sample efficiency and performance compared to strong baselines. More information and code can be found on our project page at https://rchalyang.github.io/SoftModule/.