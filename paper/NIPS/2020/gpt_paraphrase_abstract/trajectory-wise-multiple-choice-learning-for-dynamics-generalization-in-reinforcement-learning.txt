This study introduces a new model-based reinforcement learning algorithm called trajectory-wise multiple choice learning. The algorithm aims to overcome the challenge of learning a dynamics model that can adapt to changes in dynamics by utilizing a multi-headed model. Each head is specialized in certain environments with similar dynamics, achieved through clustering environments. Additionally, the algorithm incorporates context learning to encode dynamics-specific information from past experiences, enabling online adaptation to unseen environments. To maximize the effectiveness of the specialized prediction heads, an adaptive planning method is proposed, which selects the most accurate head based on recent experiences. The algorithm demonstrates superior zero-shot generalization performance in various control tasks compared to existing RL methods. The source code and videos can be accessed at https://sites.google.com/view/trajectory-mcl.