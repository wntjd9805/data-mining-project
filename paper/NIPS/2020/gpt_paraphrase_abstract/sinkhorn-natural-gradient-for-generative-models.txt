We present a new algorithm called Sinkhorn Natural Gradient (SiNG) for minimizing a functional over a parameterized family of probability measures. This problem is particularly relevant in the training of generative adversarial networks. SiNG is a steepest descent method that operates on the probability space using the Sinkhorn divergence. Unlike existing natural gradient methods, SiNG allows for accurate evaluation of the Sinkhorn information matrix (SIM) with a complexity that scales logarithmically relative to the desired accuracy. We also propose an empirical estimator for SIM when only Monte-Carlo integration is available. Through experiments, we compare SiNG with other SGD-type solvers on generative tasks and demonstrate its efficiency and effectiveness.