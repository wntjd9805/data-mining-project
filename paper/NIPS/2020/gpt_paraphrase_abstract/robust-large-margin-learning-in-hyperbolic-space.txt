There has been a recent increase in interest in representation learning in hyperbolic spaces due to their ability to represent hierarchical data with fewer dimensions than Euclidean spaces. However, the potential benefits of using hyperbolic spaces for machine learning tasks have not been extensively explored. This paper introduces the first theoretical guarantees for learning a classifier in hyperbolic space compared to Euclidean space. The paper presents a hyperbolic perceptron algorithm that converges to a separating hyperplane, and an efficient algorithm to learn a large-margin hyperplane using adversarial examples. Furthermore, it proves that for hierarchical data that fits well into hyperbolic space, the lower embedding dimension offers superior guarantees when learning the classifier directly in hyperbolic space.