We propose a sampling approach to effectively train robust Reinforcement Learning (RL) agents. By utilizing Stochastic Gradient Langevin Dynamics, we develop a scalable two-player RL algorithm that is a variation of the two-player policy gradient method. Our algorithm consistently outperforms existing methods in terms of generalization across various training and testing conditions in multiple MuJoCo environments. Furthermore, our experiments demonstrate that our sampling approach remains highly robust compared to standard RL algorithms, even when objective functions do not account for potential environmental shifts.