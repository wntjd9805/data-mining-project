Natural Gradient Descent (NGD) is a technique used to speed up the convergence of gradient descent in deep neural networks. However, its high computational cost makes approximations necessary in large-scale networks. While empirical studies have shown that some approximate NGD methods can converge quickly in practice, it is not clear why or under what conditions these approximations work well. In this study, we investigate the theoretical aspects of NGD with approximate Fisher information. We analyze the training dynamics of NGD in function space using the neural tangent kernel, focusing on deep neural networks in the infinite-width limit. We demonstrate that under specific conditions, NGD with approximate Fisher information achieves the same fast convergence to global minima as exact NGD. This holds true for various layer-wise approximations, such as block diagonal, block tri-diagonal, and K-FAC approximations. Additionally, we find that a unit-wise approximation can also achieve fast convergence under certain assumptions. Importantly, all of these different approximations result in an isotropic gradient in the function space, which plays a crucial role in achieving the same convergence properties in training. Overall, this study provides a novel and unified theoretical foundation for understanding NGD methods in deep learning.