We introduce a new technique called position-based scaled gradient (PSG) that adjusts the gradient based on the position of a weight vector, making it more suitable for compression. We demonstrate that incorporating PSG into standard gradient descent (GD), referred to as PSGD, is equivalent to operating in a warped weight space created by an invertible function. Additionally, we show through experiments that PSG acts as a regularizer for weight vectors, making it beneficial for compression techniques like quantization and pruning. PSG reduces the discrepancy between the weight distributions of a full-precision model and its compressed version, allowing for flexible deployment based on available resources. Our experimental results on CIFAR-10/100 and ImageNet datasets validate the effectiveness of PSG in both pruning and quantization, even for extremely low bit rates. The code for PSG is available on Github.