Neural networks are often seen as complex and difficult to understand due to their nonlinearity and nonconvexity. However, this study shows that in the early stages of learning, neural networks can be simplified to behave like linear models. This simplicity can also be observed in deeper networks and convolutional networks. The analysis relies on bounding the spectral norm of the difference between the Neural Tangent Kernel (NTK) and an affine transform of the data kernel. Unlike previous studies, this analysis does not require the network to have a large width and allows the network to deviate from the kernel regime during training.