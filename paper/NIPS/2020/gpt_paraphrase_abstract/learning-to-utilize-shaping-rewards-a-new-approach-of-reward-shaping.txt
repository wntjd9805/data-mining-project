Reward shaping is a useful technique in reinforcement learning that incorporates domain knowledge. However, existing methods often fail to fully utilize shaping rewards due to imperfect transformation of human knowledge into numeric values. In this paper, we propose a solution to adaptively use shaping rewards by formulating it as a bi-level optimization problem. We derive the gradient of expected true reward with respect to shaping weight function parameters and propose three learning algorithms based on different assumptions. Our experiments demonstrate that our algorithms effectively utilize beneficial shaping rewards while ignoring or transforming unbeneficial ones in various environments.