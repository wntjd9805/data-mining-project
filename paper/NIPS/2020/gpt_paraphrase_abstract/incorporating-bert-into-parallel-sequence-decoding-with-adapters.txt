Large pre-trained language models like BERT have been successful in natural language understanding tasks, but incorporating them into sequence-to-sequence models for text generation is challenging. This paper proposes a solution by using two different BERT models as the encoder and decoder, and fine-tuning them with lightweight adapter modules. These modules are inserted between BERT layers and tuned on task-specific data. This approach creates a flexible and efficient model that leverages information from both the source-side and target-side BERT models without the problem of catastrophic forgetting. Each component in the framework can be considered as a plug-in unit, making it flexible and suitable for different tasks. The framework uses a parallel sequence decoding algorithm called Mask-Predict, which takes into account the bidirectional and conditional independent nature of BERT. The proposed method outperforms autoregressive baselines in neural machine translation tasks while reducing inference latency by half. It achieves high BLEU scores on German-English and English-German/French translations, comparable to state-of-the-art baseline models.