This abstract discusses the concept of uniform stability in algorithmic stability, specifically in the context of stochastic gradient descent (SGD) on convex losses. Previous work has provided upper bounds on the uniform stability of SGD on smooth convex losses, leading to progress in understanding generalization properties and applications to differentially private convex optimization. However, this work is the first to address the uniform stability of SGD on nonsmooth convex losses. The authors provide sharp upper and lower bounds for several forms of SGD and full-batch gradient descent (GD) on arbitrary Lipschitz nonsmooth convex losses. The lower bounds demonstrate that (S)GD can be less stable in the nonsmooth case compared to the smooth case, while the upper bounds show that (S)GD is stable enough to derive new generalization error bounds. Notably, the authors obtain the first dimension-independent generalization bounds for multi-pass SGD in the nonsmooth case. Additionally, their bounds allow them to develop a new algorithm for differentially private nonsmooth stochastic convex optimization with optimal excess population risk, which is simpler and more efficient than previous algorithms for the nonsmooth case.