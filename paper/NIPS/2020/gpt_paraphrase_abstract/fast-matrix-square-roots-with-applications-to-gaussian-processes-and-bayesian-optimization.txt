Matrix square roots and their inverses are commonly used in machine learning, such as when sampling from high-dimensional Gaussians or "whitening" a vector against a covariance matrix. Current methods for computing these square roots typically require a lot of computation. However, we have developed a highly efficient algorithm that can compute these square roots and their derivatives using matrix-vector multiplication. Our method combines Krylov subspace methods with a rational approximation, and it can achieve high accuracy with fewer computations. Additionally, the backward pass requires minimal additional computation. We have tested our method on large matrices and found that it performs well with little approximation error. This increased scalability allows for more powerful models and higher accuracy in variational Gaussian processes, Bayesian optimization, and Gibbs sampling. We have successfully used our method for variational GP inference with up to 10,000 inducing points and Gibbs sampling on a 25,000-dimensional problem.