Normalizing flows (NF) are a useful tool for approximating posterior distributions, evaluating densities, and generating samples by applying a series of differentiable invertible transformations. However, the current trend in NF research focuses on creating deeper and more complex transformations to improve flexibility. In contrast, we propose Gradient Boosted Normalizing Flows (GBNF) as an alternative approach. GBNF builds a density model by gradually adding new NF components using gradient boosting. Each new component is trained to optimize a weighted likelihood objective, allowing it to capture the residuals left by previously trained components. This formulation results in a mixture model structure that becomes more flexible with the addition of more components. GBNFs offer a broader approach instead of solely relying on deeper transformations, enhancing existing NFs through additional training rather than increased complexity. Our experiments demonstrate the effectiveness of GBNFs for density estimation and generative modeling of images when combined with a variational autoencoder. GBNFs outperform non-boosted NFs and, in certain cases, achieve superior results with smaller and simpler flows.