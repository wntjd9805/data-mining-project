As language models become more advanced, the limitations of the data and metrics used for training and evaluation are becoming increasingly apparent. For instance, current summarization models are trained to predict human reference summaries and evaluated using the ROUGE metric. However, these metrics are not accurate indicators of the quality of the summaries. This study aims to improve summary quality by training a model to optimize for human preferences. A dataset of human comparisons between summaries is collected, and a model is trained to predict the preferred summary. This model is then used as a reward function to fine-tune a summarization policy using reinforcement learning. The method is applied to a dataset of Reddit posts and performs better than human reference summaries and larger models fine-tuned with supervised learning alone. The models also transfer well to news articles, producing high-quality summaries without any news-specific fine-tuning. Extensive analysis is conducted on the human feedback dataset and the fine-tuned models. The study establishes that the reward model generalizes to new datasets and that optimizing the reward model leads to better summaries compared to optimizing ROUGE according to humans. The findings of this research highlight the importance for machine learning researchers to consider how their training loss affects the desired behavior of the model.