Determining whether decision makers, whether they are brains or algorithms, employ the same strategies is a key challenge in cognitive science, behavioural neuroscience, machine learning, and artificial intelligence research. Simply measuring accuracy is insufficient because different strategies can yield similar accuracy. To address this, we propose a quantitative analysis called trial-by-trial error consistency, which assesses whether two decision making systems consistently make errors on the same inputs. This analysis can be applied to compare algorithms with algorithms, humans with humans, and algorithms with humans. When applying this analysis to visual object recognition, we find that Convolutional Neural Networks (CNNs) exhibit high consistency with each other, regardless of their architecture. However, the consistency between CNNs and human observers is only slightly above chance, suggesting that humans and CNNs likely employ distinct strategies. Additionally, a recurrent model called CORnet-S, considered the best model of the primate ventral visual stream, fails to capture important aspects of human behavioral data and behaves similarly to a standard purely feedforward model in our analysis. These findings indicate that human and machine vision strategies remain very different, but our error consistency analysis can be a valuable tool for quantifying future progress.