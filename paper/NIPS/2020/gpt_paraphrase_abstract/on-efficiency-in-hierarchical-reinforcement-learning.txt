Hierarchical Reinforcement Learning (HRL) approaches offer more efficient solutions to sequential decision-making problems, both statistically and computationally. Although empirical evidence supports this claim across various tasks, there is a lack of theoretical results quantifying the benefits of such methods. This paper focuses on the structure of a Markov decision process that enables efficient HRL methods. We formalize the idea that HRL can leverage repetitive "subMDPs" with similar reward and transition structures. By making reasonable assumptions, we demonstrate that a model-based Thompson sampling-style HRL algorithm exploiting this structure is statistically efficient, as evidenced by a finite-time regret bound. We also identify conditions under which planning with structure-induced options is nearly optimal and computationally efficient.