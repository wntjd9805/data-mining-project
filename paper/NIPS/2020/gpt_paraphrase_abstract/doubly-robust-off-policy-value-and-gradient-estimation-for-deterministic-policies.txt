Offline reinforcement learning is essential in situations where conducting experiments is limited, such as in the field of medicine. This approach involves using previously collected data from a fixed behavior policy to evaluate and learn new policies. Our research focuses on estimating the value and gradient of a deterministic policy when actions are continuous. Deterministic policies are particularly important because optimal policies are always deterministic, although they may have ties. However, traditional methods like standard importance sampling and doubly robust estimators fail in this scenario due to the absence of density ratios. To address this challenge, we propose several novel doubly robust estimators that utilize different kernelization techniques. We analyze the asymptotic mean-squared error of these estimators under mild rate conditions for nuisance estimators. Notably, we demonstrate how to achieve a rate that is independent of the length of the horizon.