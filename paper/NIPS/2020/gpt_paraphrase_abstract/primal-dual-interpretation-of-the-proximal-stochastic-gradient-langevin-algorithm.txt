This paper explores the task of sampling from a log concave probability distribution. The distribution's potential is assumed to be composed of a smooth convex term and a nonsmooth convex term that may take infinite values. The target distribution can be seen as a minimizer of the Kullback-Leibler divergence defined on the Wasserstein space, which is the space of probability measures. The paper establishes a strong duality result for this minimization problem in the first part. In the second part, the duality gap resulting from the first part is utilized to analyze the complexity of the Proximal Stochastic Gradient Langevin Algorithm (PSGLA), which is an extension of the Projected Langevin Algorithm. The approach considers PSGLA as a primal dual algorithm and covers scenarios where the target distribution is not fully supported. Specifically, it is demonstrated that if the potential is strongly convex, the complexity of PSGLA is O(1/ε^2) in terms of the 2-Wasserstein distance. In contrast, the complexity of the Projected Langevin Algorithm is O(1/ε^12) in terms of total variation when the potential is convex.