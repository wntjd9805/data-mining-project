We examine the low rank regression problem, where y = M x + (cid:15), with x and y being d1 and d2 dimensional vectors respectively. Our focus is on the extreme high-dimensional scenario where the number of observations n is smaller than d1 + d2. Current algorithms are designed for cases where n is typically as large as rank(M)(d1 + d2). In this study, we propose an efficient algorithm that only requires two singular value decompositions (SVD) and provides statistical guarantees on its performance. The algorithm tackles the problem by first estimating the precision matrix of the features and then solving the matrix denoising problem. To complement the upper bound, we introduce novel techniques for establishing lower bounds on the performance of any algorithm for this problem. Our initial experiments validate that our algorithm often outperforms existing baselines and is consistently competitive.