The traditional backpropagation algorithm, widely used in artificial neural network models, has features that are not biologically plausible for real neural circuits. An alternative method called target propagation aims to address this issue by using a top-down model of neural activity to convert output errors into layer-wise targets that are biologically plausible. These targets can then be used to update the weights of the network during training. However, so far, target propagation has been proposed heuristically without being proven to be equivalent to backpropagation. In this study, we establish an exact equivalence between backpropagation and a modified version of target propagation called GAIT-prop, where the target is a small perturbation of the forward pass. Specifically, backpropagation and GAIT-prop produce the same weight updates when the synaptic weight matrices are orthogonal. Through simple computer vision experiments, we demonstrate that backpropagation and GAIT-prop with a soft orthogonality-inducing regularizer exhibit nearly identical performance.