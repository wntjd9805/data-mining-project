The current machine learning method for training deep neural networks, backpropagation, is not biologically plausible due to several reasons. While backpropagation solves the weight transport problem, it requires neurons to know their outgoing weights and relies on precise labels for training. Alternatives like feedback alignment address the weight transport problem but not the other issues. As a result, fully biologically plausible learning rules have been difficult to achieve. In this study, we propose a new family of learning rules that overcomes these problems. Our approach is motivated by the information bottleneck principle and incorporates kernel methods. The rules have a 3-factor Hebbian structure, utilizing pre- and post-synaptic firing rates and an error signal. Unlike backpropagation, our rules do not require precise labels but instead rely on the similarity between desired outputs. Additionally, our rules incorporate divisive normalization, a characteristic of biological networks, to achieve good performance on challenging problems while remaining biologically plausible. Simulations demonstrate that our rules perform almost as well as backpropagation in image classification tasks.