Diverse image captioning models aim to learn mappings between different types of data, such as images and texts. Current methods rely on generative latent variable models but have limitations in capturing the true diversity of the underlying generative process. To overcome this, we propose a new approach called the context-object split, which leverages contextual descriptions in the dataset to model diversity in captions across images and texts. Our framework allows for diverse captioning and can also generate captions for images with novel objects and without paired captions in the training data. We evaluate our approach on standard and novel object datasets, demonstrating significant improvements in both accuracy and diversity.