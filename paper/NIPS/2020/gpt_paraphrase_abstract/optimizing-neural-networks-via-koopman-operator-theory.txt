The Koopman operator theory, which is used to understand the dynamics of nonlinear systems, has been found to have a close connection with neural network training. This study explores this connection and proposes using Koopman operator theory to optimize network weights and biases, potentially leading to faster training, particularly for deep networks where optimization is challenging. The researchers demonstrate that their approach can accurately predict the weights and biases of feedforward deep networks during a specific training period. They also find that their method is more than 10 times faster than traditional gradient descent methods such as Adam, Adadelta, and Adagrad. The paper concludes by highlighting future research directions and possible generalizations of their findings.