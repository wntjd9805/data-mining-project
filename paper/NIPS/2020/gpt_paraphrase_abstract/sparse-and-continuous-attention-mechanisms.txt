Exponential families are extensively employed in machine learning, encompassing various distributions in both continuous and discrete domains such as Gaussian, Dirichlet, Poisson, and categorical distributions through softmax transformation. These distributions have fixed support. However, recent research has focused on sparse alternatives to softmax, like sparsemax and α-entmax, for finite domains. Unlike softmax, these alternatives can assign zero probability to irrelevant categories due to their varying support. This study expands on this research in two ways. Firstly, it extends α-entmax to continuous domains, establishing a connection with Tsallis statistics and deformed exponential families. Secondly, it introduces continuous-domain attention mechanisms, enabling efficient gradient backpropagation algorithms for α ∈ {1, 2}. Experiments conducted on attention-based text classification, machine translation, and visual question answering demonstrate the application of continuous attention in 1D and 2D, showcasing its ability to focus on time intervals and compact regions.