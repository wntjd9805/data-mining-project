The challenge of maintaining a balanced dataset becomes greater as class sizes increase, especially when there are multiple instances of interest within a single unit. Existing methods for long-tailed classification in deep learning rely on heuristics without a solid theoretical foundation. In this paper, we introduce a causal inference framework that not only explains the reasoning behind previous methods but also proposes a new principled solution. Our theory demonstrates that the momentum in stochastic gradient descent (SGD) serves as a confounder in long-tailed classification. While it can have a detrimental causal effect on tail prediction, it also benefits representation learning and head prediction through induced mediation. Our framework effectively addresses this paradoxical effect by focusing on the direct causal effect of an input sample. We utilize causal intervention during training and counterfactual reasoning during inference to eliminate the negative impact while preserving the positive. This approach achieves state-of-the-art results on three long-tailed visual recognition benchmarks: Long-tailed CIFAR-10/-100, ImageNet-LT for image classification, and LVIS for instance segmentation.