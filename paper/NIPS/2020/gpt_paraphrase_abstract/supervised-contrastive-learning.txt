Contrastive learning has gained popularity in the field of self-supervised representation learning, especially for training deep image models without labels. Modern batch contrastive approaches have shown better performance compared to traditional contrastive losses like triplet loss. This study extends the batch contrastive approach to the fully-supervised setting, allowing the utilization of label information. By pulling together points belonging to the same class and pushing apart samples from different classes in the embedding space, we propose two versions of the supervised contrastive (SupCon) loss and identify the most effective formulation. Our experiments on ResNet-200 demonstrate a top-1 accuracy of 81.4% on the ImageNet dataset, surpassing the best reported result for this architecture by 0.8%. We also observe consistent outperformance over cross-entropy on other datasets and ResNet variants. Additionally, our loss function exhibits benefits in terms of robustness to natural corruptions and stability to hyperparameter settings. It is straightforward to implement, and we provide reference TensorFlow code at https://t.ly/supcon 1.