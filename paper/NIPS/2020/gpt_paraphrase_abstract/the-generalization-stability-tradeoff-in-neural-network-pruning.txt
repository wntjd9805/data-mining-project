Pruning neural network parameters is commonly used to compress models, but it is also employed to prevent overfitting. Interestingly, various pruning techniques have been found to improve test accuracy despite significant reductions in parameter counts. To understand this phenomenon, we examine the behavior of pruning during training and discover that the extent to which pruning improves generalization increases with its instability, measured as the drop in test accuracy immediately after pruning. We propose that this "generalization-stability tradeoff" exists across different pruning methods and suggest that pruning acts as a form of regularization similar to noise injection. Our findings indicate that less pruning stability leads to flatter models and that the benefits of pruning are not dependent on permanently removing parameters. These results shed light on the compatibility between pruning-based generalization improvements and the high generalization observed in networks with excessive parameters.