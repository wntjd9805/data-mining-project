Recent research suggests that the dopamine system uses distributional temporal difference backups to learn the entire distributions of long-term state values, rather than just their expected values. However, existing distributional codes rely on a complex imputation step that requires knowledge of the states of all units involved, making it challenging to implement in realistic neural circuits. In this study, we introduce the Laplace code, a local temporal difference code for distributional reinforcement learning that is both powerful in its representation and computationally simple. This code breaks down value distributions and prediction errors into three separate dimensions: reward magnitude, temporal discounting, and time horizon. It allows for local learning rules and recovers the temporal evolution of immediate reward distributions, enabling flexible computations that can adapt to changing horizons or discount factors.