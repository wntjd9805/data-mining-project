The Cooperative hierarchical Transformer (COOT) is a proposed method for handling video-text tasks that involve different levels of granularity. These tasks can include frames and words, clip and sentences, or videos and paragraphs, each with their own distinct meanings. COOT aims to model the interactions between these different levels of granularity and different modalities. The method consists of three main components: an attention-aware feature aggregation layer, which considers the local temporal context within a clip, a contextual transformer that learns the interactions between low-level and high-level semantics (e.g., clip-video, sentence-paragraph), and a cross-modal cycle-consistency loss that connects video and text. The method performs well on various benchmarks compared to existing approaches, all while utilizing a small number of parameters. The source code for COOT is available open-source at https://github.com/gingsi/coot-videotext.