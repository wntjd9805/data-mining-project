Graph neural networks (GNNs) have become widely used for machine learning tasks on graphs. Most GNNs are message passing neural networks (MPNNs), which update vertex representations iteratively and aggregate them to compute graph vector representations. However, MPNNs do not have hidden layers in the form of graphs as their update procedure is parameterized by fully-connected layers. To address this, we propose a more intuitive architecture called Random Walk Graph Neural Network (RWNN). RWNN consists of trainable "hidden graphs" in the first layer, which are compared to input graphs using a random walk kernel to generate graph representations. These representations are then passed to a fully-connected neural network for output. The random walk kernel used is differentiable, making the proposed model end-to-end trainable. We demonstrate the transparency of the model on synthetic datasets and evaluate its performance on graph classification datasets, showing competitive results.