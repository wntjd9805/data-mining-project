Imitation Learning (IL) algorithms are typically evaluated in the same environment as the demonstrations they were trained on. This limits their ability to generalize to different deployment settings. To address this, the MAGICAL benchmark suite is introduced, which allows for systematic evaluation of generalization by measuring the algorithm's robustness to various distribution shifts encountered in practice. The benchmark confirms that current IL algorithms tend to overfit to the demonstration context and highlights the limitations of standard methods for reducing overfitting. New approaches are needed to enable robust generalization of the demonstrator's intent. The MAGICAL suite's code and data are available at https://github.com/qxcv/magical/.