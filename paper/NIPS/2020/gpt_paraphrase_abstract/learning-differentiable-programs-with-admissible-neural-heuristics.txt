We investigate the problem of training differentiable functions represented as programs in a specific language. These programmatic models have advantages such as composability and interpretability, but learning them involves optimizing over a combinatorial space of program structures. To tackle this optimization problem, we propose a novel approach that frames it as a search in a weighted graph, where the paths represent top-down derivations of program syntax. Our key contribution is considering various neural network classes as continuous relaxations over the program space, enabling the completion of partial programs. This relaxation results in a differentiable program that can be trained end-to-end, and the training loss serves as an approximately admissible heuristic for guiding the combinatorial search. We apply our method on top of the Aâ‡¤ algorithm and an iteratively deepened branch-and-bound search, using them to learn programmatic classifiers in three sequence classification tasks. Our experimental results demonstrate that our algorithms outperform existing program learning methods. Additionally, the learned programmatic classifiers exhibit natural interpretations and achieve competitive accuracy.