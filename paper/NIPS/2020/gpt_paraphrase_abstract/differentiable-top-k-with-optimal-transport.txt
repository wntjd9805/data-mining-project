The top-k operation, which involves finding the k largest or smallest elements from a set of scores, is commonly used in information retrieval, machine learning, and data mining. However, implementing this operation algorithmically using methods like bubble algorithm prevents the model from being trained end-to-end using gradient descent algorithms. This is because these implementations involve swapping indices, whose gradient cannot be computed. Additionally, the mapping from input scores to the indicator vector of whether an element belongs to the top-k set is discontinuous. To solve this problem, we propose a smoothed approximation called the Scalable Optimal Transport-based Differentiable (SOFT) top-k operator. Our SOFT top-k operator approximates the output of the top-k operation by solving an Entropic Optimal Transport (EOT) problem. The gradient of the SOFT operator can be efficiently approximated based on the optimality conditions of the EOT problem. We apply this operator to the k-nearest neighbors and beam search algorithms and observe improved performance.