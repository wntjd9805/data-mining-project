Neural networks used in safety-critical applications like self-driving cars and health monitors require input attribution for analysis and network compression for efficient edge computing. However, these two techniques conflict with each other as compression distorts the attributions, posing risks for crucial applications. This is because conventional compression methods prioritize preserving network predictions over attribution quality. To address this problem, we propose a framework that maintains attributions while compressing the network. Our approach, the Weighted Collapsed Attribution Matching regularizer, matches the attribution maps of the compressed network to its pre-compression state. We demonstrate the effectiveness of our algorithm quantitatively and qualitatively using various compression methods.