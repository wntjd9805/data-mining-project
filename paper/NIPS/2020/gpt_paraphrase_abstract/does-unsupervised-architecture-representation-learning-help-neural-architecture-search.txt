Current methods for Neural Architecture Search (NAS) have limitations in terms of scalability and search bias. The learned architecture representations in NAS are not well understood. We have observed that when architecture representation learning and search are combined, the structural properties of neural architectures are not effectively preserved in the latent space, leading to less effective search performance. To address this, we have empirically found that pre-training architecture representations using only neural architectures (without accuracy labels) improves the efficiency of downstream architecture search. We explain this finding by showing how unsupervised architecture representation learning encourages the clustering of neural architectures with similar connections and operators. This clustering helps map neural architectures with similar performance to the same regions in the latent space, resulting in a smoother transition of architectures and benefiting diverse downstream search strategies.