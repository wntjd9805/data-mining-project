We present a tabular episodic reinforcement learning (RL) algorithm that incorporates constraints. Our approach offers a modular analysis with strong theoretical guarantees for two main scenarios. The first scenario involves a convex-concave setting, where the goal is to maximize a concave reward function while ensuring that the expected values of certain vector quantities, such as unsafe actions, lie within a convex set. The second scenario is the knapsack setting, where the objective is to maximize reward while keeping the total consumption of specified resources below specified levels throughout the learning process. Existing research in constrained RL has been limited to linear expectation constraints (a specific case of the convex-concave setting), feasibility questions, or single-episode settings. Our experiments demonstrate that our algorithm outperforms these approaches in constrained episodic benchmarks.