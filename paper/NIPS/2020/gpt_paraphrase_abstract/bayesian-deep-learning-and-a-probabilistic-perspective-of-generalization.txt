The main characteristic of a Bayesian approach is the use of marginalization instead of relying on a single set of weights. By applying Bayesian marginalization, the accuracy and calibration of modern deep neural networks can be significantly enhanced. These networks are often not fully specified by the available data and can have multiple potentially valid solutions. We demonstrate that deep ensembles offer an effective method for approximating Bayesian marginalization. Additionally, we propose a related technique that further improves the predictive distribution by marginalizing within basins of attraction, without incurring significant additional computational costs. We also explore the implications of a vague distribution over neural network weights, which leads to a prior over functions. This probabilistic perspective helps explain the generalization characteristics of such models. Moreover, we clarify certain phenomena previously considered mysterious and unique to neural network generalization, such as the ability to fit images with random labels, by showing that similar results can be achieved using Gaussian processes. Furthermore, we demonstrate that Bayesian model averaging mitigates the occurrence of double descent, resulting in consistent performance improvements as flexibility increases.