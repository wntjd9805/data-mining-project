Stein discrepancies (SDs) are useful for assessing convergence in approximate inference when exact integration and sampling are not feasible. However, computing SDs can be time-consuming if the Stein operator is expensive to evaluate. To address this issue, we propose stochastic Stein discrepancies (SSDs) that use subsampled approximations of the Stein operator. SSDs maintain the convergence control properties of SDs with high probability. Additionally, we prove the convergence of Stein variational gradient descent (SVGD) on unbounded domains, resolving a previously unanswered question. In our experiments, we demonstrate that SSDs provide comparable results to SDs with significantly fewer likelihood evaluations in tasks such as hyperparameter tuning, sampler selection, and stochastic SVGD.