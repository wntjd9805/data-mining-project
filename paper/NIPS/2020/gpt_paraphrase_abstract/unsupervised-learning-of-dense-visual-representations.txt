Contrastive self-supervised learning has shown promise for unsupervised visual representation learning. While these methods typically focus on global representations at the image level, many visual tasks require dense representations at the pixel level. This paper introduces View-Agnostic Dense Representation (VADeR) for unsupervised learning of dense representations. VADeR achieves this by enforcing consistency of local features across different viewing conditions through pixel-level contrastive learning. Matching features are brought close in an embedding space, while non-matching features are pushed apart. VADeR naturally supports dense prediction tasks and performs better than ImageNet supervised pretraining and strong unsupervised baselines in multiple such tasks.