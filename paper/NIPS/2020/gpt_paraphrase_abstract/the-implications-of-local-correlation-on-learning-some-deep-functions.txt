Learning deep neural networks is known to be computationally difficult, as even weakly learning these networks proves to be challenging. This means that finding a predictor that performs slightly better than a random guess cannot be efficiently achieved. However, it has been observed that on natural image distributions, small patches of the input image are correlated to the target label, making efficient weak learning trivial in such cases. While boosting results have shown that weak learning leads to strong learning in distribution-free settings, this is not necessarily true in distribution-specific settings. To address this, we introduce the concept of "local correlation", which involves the correlation between small image patches and intermediate layers of the target function with the target label. Through empirical analysis on CIFAR and ImageNet datasets, we confirm the presence of this property. The main technical contribution of this study is the proof that, for certain classes of deep functions, weak learning implies efficient strong learning under the assumption of "local correlation".