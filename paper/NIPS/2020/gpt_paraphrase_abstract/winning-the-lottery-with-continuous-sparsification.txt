Efficient and sparse deep neural network models are typically achieved through pruning, which involves training a dense network and then removing parameters based on a manual heuristic. The Lottery Ticket Hypothesis suggests that it is possible to find smaller sub-networks that can match the performance of the original dense network when trained from scratch. This study revisits pruning algorithms and identifies missing components in previous methods. The researchers propose a new method called Continuous Sparsification that searches for sparse networks using an innovative approximation of an intractable regularization. The method is compared to existing heuristic-based approaches for both pruning and finding sparse sub-networks that can be re-trained successfully. The empirical results demonstrate that Continuous Sparsification outperforms state-of-the-art methods across various models and datasets, including VGG trained on CIFAR-10 and ResNet-50 trained on ImageNet. In addition to setting a new standard for pruning, Continuous Sparsification enables fast parallel ticket search, which opens up new possibilities for applying the Lottery Ticket Hypothesis.