Score-based generative models can generate high-quality images similar to GANs without adversarial optimization. However, current training methods are limited to low-resolution images (usually below 32 × 32) and can be unstable in certain situations. We present a new theoretical analysis of learning and sampling from score-based models in high-dimensional spaces, addressing existing issues and proposing general solutions applicable to different datasets. To improve stability, we suggest maintaining an exponential moving average of model weights. With these enhancements, we can scale score-based generative models to various image datasets, with resolutions ranging from 64 × 64 to 256 × 256. Our models produce high-fidelity samples comparable to top-performing GANs on different image datasets such as CelebA, FFHQ, and various LSUN categories.