Long-term temporal credit assignment is a significant challenge in deep reinforcement learning. This refers to the agent's ability to attribute actions to consequences that may occur after a considerable time interval. Current policy-gradient and Q-learning algorithms heavily rely on dense environmental rewards for credit assignment, but struggle with tasks that involve delays between actions and corresponding rewards. Recent works have proposed algorithms to learn dense guidance rewards, which can be used as substitutes for sparse or delayed environmental rewards to make credit assignment easier. This paper follows a similar approach, using a surrogate RL objective that involves trajectory-space smoothing to develop a new algorithm for learning guidance rewards. The guidance rewards have a straightforward interpretation and can be obtained without training additional neural networks. We demonstrate the effectiveness of our approach by integrating the guidance rewards into popular algorithms (Q-learning, Actor-Critic, Distributional-RL) and present results in both single-agent and multi-agent tasks. Our findings highlight the benefits of our approach when dealing with sparse or delayed environmental rewards.