The consistency properties of surrogate risk minimization algorithms in multiclass classification are a fundamental question. These algorithms minimize a convex surrogate to the multiclass 0-1 loss. The framework of calibrated surrogates has been used to analyze the Bayes consistency of these algorithms, which refers to their convergence to a Bayes optimal classifier. However, it has been suggested that this framework may not be as useful when studying H-consistency. Previous work has shown that certain convex calibrated surrogates fail to recover the true model when minimizing them over linear scoring functions. This paper investigates this issue and finds that while some calibrated surrogates may not provide H-consistency when minimized over a na√Øvely chosen scoring function class, they can potentially provide H-consistency when minimized over a carefully chosen class of scoring functions. For the popular one-vs-all hinge and logistic surrogates, a form of scoring function class is derived that enables H-consistency. When the class of linear models is considered, this class of scoring functions consists of certain piecewise linear functions that have the same number of parameters as the linear case. Minimization over this class can be done using an adaptation of the min-pooling idea from neural network training. Experiments confirm that training the one-vs-all surrogates over this class of nonlinear scoring functions yields better linear multiclass classifiers than training them over standard linear scoring functions.