We propose a method called CaSPR that learns object-centric Canonical Spatiotemporal Point Cloud Representations for moving or evolving objects. Our aim is to enable the aggregation of information over time and the examination of object state at any spatiotemporal neighborhood in the past, regardless of whether it was observed or not. Unlike previous approaches, CaSPR learns representations that support continuity in space and time, can handle variable and irregularly sampled point clouds, and can generalize to unseen object instances. Our approach involves two steps. First, we encode time by mapping a sequence of input point clouds to a canonicalized object space in terms of space and time. We then utilize this canonicalization to learn a latent representation of the object's shape using neural ordinary differential equations and a generative model that captures the object's dynamic evolution using continuous normalizing flows. We demonstrate the effectiveness of our method in various applications, such as shape reconstruction, camera pose estimation, spatiotemporal sequence reconstruction, and correspondence estimation from irregularly or intermittently sampled observations.