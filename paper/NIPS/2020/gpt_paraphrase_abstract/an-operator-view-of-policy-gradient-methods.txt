Policy gradient methods can be seen as the application of two operators: a policy improvement operator that improves the policy, and a projection operator that finds the best approximation of the improved policy within the set of feasible policies. This perspective helps to understand existing policy gradient methods like REINFORCE and PPO. Additionally, by examining the roles of these operators, a new global lower bound for expected return is proposed. This new viewpoint also highlights the relationship between policy-based and value-based methods, showing how REINFORCE and the Bellman optimality operator are interconnected.