DL frameworks often use GPUs to improve the speed of DL inference and training. However, current frameworks have inefficiencies in scheduling GPU tasks, leading to large scheduling overhead and unnecessary serial execution. To address this problem, we introduce Nimble, a DL execution engine that minimizes scheduling overhead by using a technique called ahead-of-time (AoT) scheduling. Nimble schedules tasks before executing the GPU kernel, reducing runtime scheduling overhead. Additionally, Nimble parallelizes GPU task execution by utilizing multiple GPU streams on a single GPU. Evaluation results demonstrate that Nimble significantly improves inference and training speeds compared to PyTorch, TensorRT, and TVM. It also outperforms state-of-the-art inference methods.