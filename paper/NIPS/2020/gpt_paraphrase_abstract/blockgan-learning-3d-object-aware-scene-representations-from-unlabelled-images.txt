We introduce BlockGAN, a novel model for generating images that learns 3D scene representations directly from unlabelled 2D images. Current approaches either ignore the scene background or treat the entire scene as a single object. Other methods that consider scene compositionality only treat objects as image patches or 2D layers. In contrast, BlockGAN leverages the computer graphics pipeline to generate 3D features for both the background and foreground objects. These features are then combined to create 3D representations for the entire scene, which are rendered into realistic images. BlockGAN can reason about occlusion and object interactions, such as shadows and lighting, while maintaining image realism. Importantly, it allows for control over each object's 3D pose and identity. BlockGAN is trained end-to-end using unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Experimental results demonstrate that BlockGAN can learn disentangled representations for both foreground and background objects, as well as their properties such as pose and identity. Our code is publicly available at https://github.com/thunguyenphuoc/BlockGAN.