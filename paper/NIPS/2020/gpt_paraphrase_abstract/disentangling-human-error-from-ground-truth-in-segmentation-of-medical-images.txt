Supervised learning methods have become popular for segmentation tasks, but their performance relies on the quality of labels. This is especially problematic in medical imaging, where labels are costly and subject to inter-observer variability. Typically, different human experts provide their own biased estimates of true labels, limiting the accuracy of automatic segmentation algorithms. In this study, we propose a method that jointly learns the reliability of annotators and the true segmentation labels using two CNNs. We evaluate the algorithm on both toy and real medical imaging datasets, including multiple-sclerosis lesions, brain tumors, and lung abnormalities. Our method outperforms other approaches, especially when the number of annotations is small and disagreement is high. Additionally, our method effectively captures the spatial characteristics of annotators' mistakes. The code for our method is available at the provided GitHub link.