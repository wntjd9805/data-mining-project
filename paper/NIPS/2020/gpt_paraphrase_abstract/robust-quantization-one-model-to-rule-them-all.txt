Neural network quantization methods often involve training models to simulate the quantization process, which makes the model highly dependent on the specific bit-width and quantization method used. However, robust quantization offers an alternative approach that is more tolerant to different data types and quantization policies. This opens up new possibilities for applications where the quantization process can vary to adapt to different circumstances. To address this issue, we propose a method that provides intrinsic robustness to the model, allowing it to handle a wide range of quantization processes. Our method is based on theoretical arguments and enables us to use a single generic model that can operate at various bit-widths and quantization policies. We have tested the effectiveness of our method on different ImageNet models and have provided a reference implementation.