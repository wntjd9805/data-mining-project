This study investigates the extent to which deep models capture information at different scales in language, from subwords to documents, and explores methods to enhance their ability to capture structure across this hierarchy. The focus is on analyzing the behavior of individual neurons and their activations at different timescales. The researchers demonstrate that signal processing can be used to separate structure across scales, allowing for the extraction of scale-specific information in existing embeddings and training models to learn more about particular scales. By applying spectral filters to neuron activations, they generate filtered embeddings that perform well on tasks at specific scales, such as word-level part of speech tagging, utterance-level dialog speech acts classification, and document-level topic classification. Additionally, they introduce a prism layer that utilizes spectral filters to constrain different neurons in modeling structure at different scales. The proposed BERT + Prism model improves masked token prediction by incorporating long-range context and produces multiscale representations that outperform previous models on utterance- and document-level tasks. The methods presented in this study can be applied to domains beyond language, including images, audio, and video.