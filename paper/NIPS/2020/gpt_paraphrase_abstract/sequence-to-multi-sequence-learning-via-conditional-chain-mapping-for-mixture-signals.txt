This study focuses on the application of neural sequence-to-sequence models in solving one-to-many sequence transduction problems. These problems involve extracting multiple sequential sources from a mixed sequence. The researchers propose a conditional multi-sequence model that explicitly considers the relationship between multiple output sequences using the probabilistic chain rule. This extended model can infer output sequences by utilizing both the input and previously-estimated contextual output sequences. It also includes a simple and efficient stop criterion for determining the end of the transduction process, allowing for the inference of a variable number of output sequences. The researchers evaluate their methods using speech data, which often contains multiple sources. Experimental results on various tasks, such as speech separation and multi-speaker speech recognition, demonstrate that the proposed conditional multi-sequence models consistently outperform the conventional non-conditional models.