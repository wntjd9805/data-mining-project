The brain's ability to create an internal model of the world to guide actions based on ambiguous sensory information is a fundamental question in neuroscience. This can be seen as a reinforcement learning problem, where an agent must estimate relevant variables in the world, anticipate future states, and choose actions that optimize reward. Control theory can solve this problem by finding optimal actions based on system dynamics and objective function. However, animals often behave suboptimally, suggesting they have their own flawed internal model. We propose the concept of Inverse Rational Control (IRC) to identify the internal model that best explains an agent's actions. Our contribution extends past work on IRC by accommodating continuous nonlinear dynamics and continuous actions. We use deep reinforcement learning to build an optimal Bayesian agent that learns a policy over the entire model space. This allows us to compute a likelihood over models for experimentally observable action trajectories from a suboptimal agent. We then use gradient ascent to find the model parameters that maximize the likelihood. Our method successfully recovers the true model of rational agents and provides a foundation for understanding animal behavior and neural dynamics during complex tasks.