We investigate dropout in neural networks with ReLU activations. With a sufficiently large number of parameters and assuming that the kernel can distinguish the data distribution with a positive margin, we demonstrate that dropout training using logistic loss can achieve near-optimal results in test error within a limited number of iterations.