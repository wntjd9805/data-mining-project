We propose a method for lifelong/continual learning of convolutional neural networks (CNN) that avoids catastrophic forgetting when transitioning between tasks. We demonstrate that the activation maps generated by the CNN trained on the previous task can be adjusted using a small number of calibration parameters to become relevant for the new task. To achieve this, we calibrate the activation maps of each network layer using spatial and channel-wise calibration modules and only train these calibration parameters for each new task. Our calibration modules require less computation and parameters compared to approaches that expand the network dynamically. Our method overcomes catastrophic forgetting by storing task-specific calibration parameters, which contain all the task-specific knowledge and are exclusive to each task. Additionally, our approach does not rely on storing data samples from previous tasks, as done in replay-based methods. We conduct extensive experiments on various benchmark datasets and observe significant improvements over state-of-the-art methods. For example, we achieve a 29% absolute increase in accuracy on CIFAR-100 with 10 classes at a time. On large-scale datasets such as ImageNet-100 and MS-Celeb-10K, our approach achieves a 23.8% and 9.7% absolute increase in accuracy, respectively, while utilizing a minimal number of task-adaptive calibration parameters (0.51% and 0.35% of model parameters).