This study examines the expressive capabilities of neural networks and investigates whether there are functions that can only be approximated by deep networks, given a fixed size. Previous research has only explored networks of depths 2 and 3, leaving the question of higher depths unanswered. The focus of this paper is on feedforward ReLU networks, and it establishes fundamental limitations to proving results beyond depth 4. By reducing the problem to circuit complexity and open problems, the study demonstrates barriers to achieving such results. In addition, the paper addresses the question of whether polynomially-bounded functions require super-polynomial weights to be approximated by constant-depth neural networks. It provides a negative and constructive answer by showing that if a function can be approximated by a polynomially-sized, constant-depth k network with large weights, it can also be approximated by a polynomially-sized, depth 3k + 3 network with bounded weights.