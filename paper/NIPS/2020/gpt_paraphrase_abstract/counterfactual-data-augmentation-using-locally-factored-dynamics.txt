Dynamic processes in robotic control and reinforcement learning often involve interacting subprocesses. While these subprocesses are not independent, their interactions are usually sparse, allowing for decomposition into locally independent causal mechanisms. This local causal structure can enhance the efficiency of sequence prediction and off-policy reinforcement learning. To formalize this concept, we introduce local causal models (LCMs) derived from a global causal model by conditioning on a subset of the state space. In this study, we propose a method for inferring these structures using an object-oriented state representation. Additionally, we present a novel algorithm called Counterfactual Data Augmentation (CoDA), which utilizes local structures and experience replay to generate causally valid counterfactual experiences in the global model. Our results demonstrate that CoDA significantly enhances the performance of RL agents in locally factored tasks, including batch-constrained and goal-conditioned settings.