Training neural networks with binary activations and weights using gradient descent is challenging due to the piecewise constant response of the model. To address this, we introduce stochastic binary networks by adding noise to the activations. This results in a smoother function of parameters, making gradient estimation more feasible. Our method combines sampling and analytic approximation steps to estimate the gradient with reduced variance and a small bias. This tradeoff is practical compared to existing unbiased and biased estimators. Additionally, we demonstrate that one additional linearization step leads to a deep straight-through estimator, previously considered an ad-hoc heuristic. Experimental results show that both proposed methods improve gradient estimation accuracy, leading to more stable and better performing training for deep convolutional models.