We introduce new PAC-Bayes learning guarantees for randomized algorithms with sample-dependent priors. Our bounds are very general and do not assume anything about the priors. They are expressed in terms of covering numbers under the infinite-Rényi divergence and the (cid:96)1 distance. We demonstrate how these bounds can be used to derive learning bounds in cases where the sample-dependent priors follow an infinite-Rényi divergence or (cid:96)1-distance sensitivity condition. In addition, we offer a flexible framework for computing PAC-Bayes bounds, assuming certain stability conditions on the sample-dependent priors. This framework allows for more precise bounds when the priors satisfy an infinite-Rényi divergence sensitivity condition.