Deep learning algorithms are known for fitting training data well, including outliers and mislabeled data. The phenomenon of memorizing training data labels has attracted research interest, but no compelling explanation has been provided. Feldman's recent work proposes an explanation based on the combination of two insights: the long-tailed nature of natural image and data distributions and the necessity of memorization for optimal generalization error in such distributions. However, empirical evidence for this explanation is lacking. In this study, we conduct experiments to test the key ideas of this theory. Estimating the influence of each training example on accuracy at each test example and the memorization values of training examples directly is computationally expensive. However, we show that we can estimate closely related subsampled influence and memorization values more efficiently. Our experiments demonstrate the significant benefits of memorization for generalization on standard benchmarks and provide quantitative and visually compelling evidence for Feldman's theory.