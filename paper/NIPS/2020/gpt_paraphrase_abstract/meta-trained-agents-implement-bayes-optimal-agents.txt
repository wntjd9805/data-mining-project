Memory-based meta-learning is a powerful technique for creating agents that quickly adapt to any task within a target distribution. A previous theoretical study argued that this effectiveness is due to the meta-training protocol encouraging agents to behave optimally according to Bayes' rule. We conducted empirical investigations on various prediction and bandit tasks to verify this claim. Drawing inspiration from theoretical computer science, we found that meta-learned agents and Bayes-optimal agents not only exhibit similar behavior but also possess similar computational structures, suggesting that one can simulate the other to some extent. Additionally, we discovered that Bayes-optimal agents are fixed points in the meta-learning process. These findings imply that memory-based meta-learning has the potential to approximate Bayes-optimal agents, even in cases where tractable models are currently unavailable.