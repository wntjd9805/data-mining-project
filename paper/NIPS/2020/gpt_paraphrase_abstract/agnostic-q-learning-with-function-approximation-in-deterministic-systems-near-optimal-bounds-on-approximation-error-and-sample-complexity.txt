This study explores the issue of agnostic Q-learning with function approximation in deterministic systems, where the optimal Q-function can be approximated by a function in the class F with an approximation error δ ≥ 0. We introduce a new recursion-based algorithm and demonstrate that if δ = O (cid:0)ρ/ (cid:1), it is possible to find the optimal policy using O(dimE) trajectories. Here, ρ represents the difference between the optimal Q-value of the best actions and the second-best actions, while dimE refers to the Eluder dimension of F. Our findings have two important implications. Firstly, in combination with the lower bound established in [Du et al., 2020], our upper bound (cid:1) is both necessary and sufficient for algorithms with polynomial sample complexity. Secondly, in conjunction with the obvious lower bound in the tabular case, our upper bound suggests that the sample complexity (cid:101)Θ (dimE) is precise in the agnostic setting. Consequently, we contribute to addressing the open problem of agnostic Q-learning introduced in [Wen and Van Roy, 2013]. Additionally, we extend our algorithm to the stochastic reward setting and achieve similar outcomes.