Reward decomposition is a technique used in reinforcement learning to break down the overall reward into smaller sub-rewards. This approach has been shown to improve the efficiency of learning from samples. However, existing methods for discovering reward decomposition are mostly reliant on specific policies, which limits the diversity and independence of behavior between different sub-rewards. In this study, we propose a new set of principles for policy-independent reward decomposition. These principles focus on ensuring that different sub-rewards are represented by unique and compact state representations. By encouraging minimal relevant features for each sub-reward while maintaining their uniqueness, our approach promotes effective decomposition. We have developed a deep learning algorithm, called RD2, which learns reward decomposition and disentangled representation simultaneously based on these principles. To evaluate the effectiveness of RD2, we conducted experiments on a toy case with a known reward structure, as well as Atari environments where the reward structure is unknown to the agent. The results demonstrate the superiority of RD2 compared to existing reward decomposition methods.