Reinforcement learning (RL) algorithms can struggle to generalize because of the disparity between simulated and real-world environments. To address this, robust adversarial RL (RARL) has been used, which incorporates the gap by introducing an adversary during policy training. This study reevaluates the effectiveness of RARL in the linear quadratic (LQ) case, a fundamental robust control scenario. It is discovered that the commonly used RARL approach, which alternates agents' updates, can destabilize the system. As a solution, several other policy-based RARL algorithms are proposed and their convergence behaviors are analyzed empirically and theoretically. It is found that the conventional RARL framework can learn a destabilizing policy if the initial policy lacks robust stability against the adversary. However, with robustly stabilizing initializations, the proposed double-loop RARL algorithm converges to the global optimal cost while maintaining robust stability. The stability and convergence issues of other policy-based RARL algorithms are also examined, and methods for learning robustly stabilizing initializations are discussed. This research offers new insights into RARL from a robust control perspective, addressing stability concerns in the continuous control LQ setting. The findings contribute to a better theoretical understanding of policy-based RARL, as introduced by Pinto et al. in 2017.