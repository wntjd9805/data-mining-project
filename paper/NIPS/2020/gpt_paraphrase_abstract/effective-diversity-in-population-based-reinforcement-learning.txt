Exploration is a major challenge in reinforcement learning as agents can only learn from the data they gather in the environment. To address this, maintaining a population of agents is an appealing approach as it allows for data collection with a diverse range of behaviors. However, existing methods that aim to enhance behavioral diversity often rely on mean field updates based on pairwise distances, which can lead to cycling behaviors and redundancy. Moreover, explicitly prioritizing diversity can negatively affect the optimization of already successful behaviors. These trade-offs are typically handled using heuristics, and the methods often require handcrafted representations specific to the problem domain. This paper proposes a novel approach to simultaneously optimize all members of a population. Instead of pairwise distance, the volume of the entire population in a behavioral manifold is measured using task-agnostic behavioral embeddings. Additionally, the algorithm called Diversity via Determinants (DvD) dynamically adjusts the degree of diversity during training using online learning techniques. Two versions of DvD, evolutionary and gradient-based, are introduced and shown to effectively improve exploration without sacrificing performance when enhanced exploration is not necessary.