New 3D object detectors for self-driving cars typically learn from either a bird's eye view or a range view. However, each view has its own advantages and disadvantages. To address this, we present a new framework that combines the benefits of both views. We introduce a hybrid voxelization technique that allows for learning from both bird's eye view and range view feature maps. We found that simply adding detection on another view does not lead to good performance, so we propose using cross-view transformers to transform the feature maps into the other view and introduce cross-view consistency loss. Our experiments on the challenging NuScenes Dataset demonstrate the effectiveness of our method, achieving an mAP of 55.8%. This outperforms all published approaches by at least 3% in overall performance and up to 16.5% in safety-crucial categories like cyclist.