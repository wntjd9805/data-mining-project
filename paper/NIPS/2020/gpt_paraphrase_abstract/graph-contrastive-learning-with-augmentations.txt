Current graph neural networks (GNNs) face challenges in achieving generalizable, transferrable, and robust representation learning on graph-structured data. Unlike convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training methods for GNNs have not been extensively explored. This paper introduces a graph contrastive learning (GraphCL) framework for unsupervised representation learning on graph data. Four types of graph augmentations are designed to incorporate different priors. The impact of various combinations of these augmentations on multiple datasets is systematically studied in different settings, including semi-supervised, unsupervised, transfer learning, and adversarial attacks. The results demonstrate that the GraphCL framework can produce graph representations with similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods, even without extensive augmentation tuning or sophisticated GNN architectures. Preliminary experiments also reveal performance improvements with parameterized graph augmentation extents and patterns. The code for this framework is available at: https://github.com/Shen-Lab/GraphCL.