We investigate the use of reinforcement learning (RL) in a tabular Markov Decision Process (MDP) setting where the agent receives additional observations in the form of transition samples. These observations can come from auxiliary sensors or prior knowledge about the environment. We introduce a feedback graph that represents the relationship between state-action pairs and demonstrate that incorporating these additional observations can improve the efficiency of learning in model-based algorithms. We provide a regret bound that depends on the size of the maximum acyclic subgraph of the feedback graph, rather than the number of states and actions alone. We also discuss the challenges of leveraging a small dominating set of the feedback graph and propose a new algorithm that can learn a near-optimal policy faster using such a dominating set.