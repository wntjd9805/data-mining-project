We examine the effectiveness of post-hoc model explanations for diagnosing model errors, also known as model debugging. Many explanation methods have been proposed to explain model predictions, but their effectiveness remains uncertain. We categorize bugs into three types: data contamination bugs, model contamination bugs, and test-time contamination bugs. We evaluate several explanation methods to determine their ability to detect data contamination, diagnose mislabeled training examples, differentiate between re-initialized and trained models, and detect out-of-distribution inputs. Our findings show that the tested methods can identify spurious correlation artifacts but struggle to conclusively identify mislabeled training examples. Additionally, certain methods that modify the back-propagation algorithm are ineffective for diagnosing model contamination. We also conduct a human subject study and find that subjects rely primarily on model predictions rather than explanations to identify defective models. Overall, our results provide guidance for practitioners and researchers using explanations for model debugging.