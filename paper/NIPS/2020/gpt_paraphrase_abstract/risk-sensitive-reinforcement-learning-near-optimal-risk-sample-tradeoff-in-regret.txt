This study focuses on risk-sensitive reinforcement learning in episodic Markov decision processes with unknown transition kernels. The objective is to optimize total reward under the risk measure of exponential utility. Two efficient model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ), are proposed. These algorithms adapt to both risk-seeking and risk-averse modes of exploration by implementing risk-sensitive optimism in the face of uncertainty. It is proven that RSVI achieves an upper bound regret of √(O(|β|H^2)·H^4SAT), where β represents the risk parameter, S is the number of states, A is the number of actions, T is the total number of timesteps, and H is the episode length. On the other hand, a regret lower bound is established, demonstrating that the exponential dependence on |β| and H is inevitable for any algorithm with an √(O(T)) regret. This certifies the near-optimality of the proposed algorithms. The results highlight the tradeoff between risk sensitivity and sample efficiency in reinforcement learning, quantifying the exponential cost in |β| and H. This analysis is the first of its kind for risk-sensitive reinforcement learning with exponential utility.