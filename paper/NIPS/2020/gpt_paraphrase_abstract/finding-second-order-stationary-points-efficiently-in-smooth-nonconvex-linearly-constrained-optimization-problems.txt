This paper presents two efficient algorithms for approximating second-order stationary points (SOSPs) in problems with smooth non-convex objective functions and linear constraints. While finding SOSPs in this class of problems is computationally challenging, the authors demonstrate that generic instances can be solved efficiently. The algorithms, called SNAP and SNAP+, exploit a strict complementarity condition observed in all Karush-Kuhn-Tucker solutions of generic problem instances. SNAP is a second-order algorithm that uses either conventional gradient projection or negative curvature-based projection steps to find SOSPs. It requires a maximum of O(1/ϵ^2) iterations to compute an (ϵG, ϵH)-SOSP, where eO hides the iteration complexity for eigenvalue-decomposition. Building on SNAP, SNAP+ is a first-order algorithm that requires a maximum of O(1/ϵ^2.5) iterations to compute an (ϵ,ϵ)-SOSP. The computational complexities of both algorithms are polynomial in the number of constraints and problem dimension. This marks the first time that first-order algorithms with polynomial per-iteration complexity and global sublinear rate have been designed for finding SOSPs in non-convex problems with linear constraints.