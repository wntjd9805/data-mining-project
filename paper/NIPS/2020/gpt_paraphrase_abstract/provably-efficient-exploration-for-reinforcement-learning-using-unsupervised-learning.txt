This study explores the effectiveness of using unsupervised learning in reinforcement learning (RL) problems. The researchers focus on episodic Markov decision processes with rich observations generated from a small number of latent states. They propose a general algorithmic framework that combines an unsupervised learning algorithm with a no-regret tabularRL algorithm. The study proves that if the unsupervised learning algorithm has a polynomial sample complexity guarantee, a near-optimal policy can be found with a sample complexity polynomial in the number of latent states. This is significantly smaller than the number of observations. The researchers also provide empirical evidence by applying their framework to challenging exploration problems, demonstrating the practicality of their theory.