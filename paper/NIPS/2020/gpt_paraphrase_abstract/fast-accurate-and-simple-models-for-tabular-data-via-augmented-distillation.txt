Automated machine learning (AutoML) can generate complex model ensembles using various individual models such as trees, deep networks, and nearest neighbor estimators. However, these ensemble predictors are often large, slow, and difficult to interpret. To enhance the deployment of AutoML on tabular data, we introduce FAST-DAD. This approach distills complex ensemble predictors into individual models like boosted trees, random forests, and deep networks. A key component of FAST-DAD is a data augmentation technique based on Gibbs sampling from a self-attention pseudolikelihood estimator.   In comparison to standard training on the original data, FAST-DAD distillation yields significantly improved individual models across 30 datasets involving regression and binary/multiclass classification tasks. Our distilled individual models outperform ensemble predictors produced by AutoML tools like H2O/AutoSklearn in terms of both speed and accuracy, achieving over 10Ë† improvement.