Graph Neural Networks (GNNs) have made significant progress in semi-supervised node classification. However, existing models often rely on deterministic classification functions, which lack flexibility in dealing with imperfect observed data like scarce labeled nodes and noisy graph structures. To address this limitation, this paper introduces a new framework called Graph Stochastic Neural Networks (GSNN). GSNN aims to model the uncertainty of the classification function by learning a family of functions, i.e., a stochastic function. This is achieved by combining a learnable graph neural network with a high-dimensional latent variable to capture the distribution of the classification function. To handle missing labels and the latent variable, amortized variational inference is adopted to approximate the intractable joint posterior. By maximizing the lower-bound likelihood of observed node labels, the proposed models can be effectively trained end-to-end. Experimental results on three real-world datasets demonstrate that GSNN outperforms state-of-the-art baselines in various scenarios.