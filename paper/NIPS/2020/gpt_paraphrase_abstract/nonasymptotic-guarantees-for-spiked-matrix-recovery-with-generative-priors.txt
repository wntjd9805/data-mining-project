The reconstruction of a rank-one signal matrix from noisy data is often necessary in statistics and machine learning. To improve recovery performance, it is important to incorporate additional prior information on the rank-one component. One common prior is sparsity, which leads to the problem of sparse principal component analysis. However, there is evidence of a gap between computation and statistics in this problem.  This study focuses on an alternative prior, where the low-rank component is within the range of a trained generative network. The authors provide a non-asymptotic analysis with optimal sample complexity for rank-one matrix recovery under an expansive-Gaussian network prior. They establish that a favorable global optimization landscape exists for a nonlinear least squares objective, as long as the number of samples is proportional to the dimensionality of the input to the generative model. This finding suggests that generative priors do not suffer from a computational-to-statistical gap in the finite data, non-asymptotic regime. The analysis is demonstrated using both the Wishart and Wigner spiked matrix models.