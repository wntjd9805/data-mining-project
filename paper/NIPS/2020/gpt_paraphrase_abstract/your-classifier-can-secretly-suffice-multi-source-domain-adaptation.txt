The abstract discusses the concept of Multi-Source Domain Adaptation (MSDA), which involves transferring knowledge from multiple labeled source domains to an unlabeled target domain that has a different distribution. Current methods focus on reducing this distribution difference through auxiliary alignment objectives. However, this work proposes a new perspective on MSDA, where deep models are observed to align the domains implicitly when supervised by labels. The goal is to leverage this implicit alignment without additional training objectives for adaptation. This is achieved by using pseudo-labeled target samples and enforcing classifier agreement on the pseudo-labels, called Self-supervised Implicit Alignment (SImpAl). The effectiveness of SImpAl is demonstrated even in cases of category-shift among the source domains. Additionally, the paper suggests using classifier agreement as a cue for training convergence, resulting in a simple training algorithm. The proposed approach is thoroughly evaluated on five benchmarks, providing detailed insights into each component.