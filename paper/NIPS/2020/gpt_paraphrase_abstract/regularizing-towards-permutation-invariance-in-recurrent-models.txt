Temporal architectures like RNNs have been widely studied for machine learning problems where the output should not depend on the order of the input. Despite the inherent dependence of RNNs on order, we argue that they are highly relevant for such problems. We propose a regularization technique to make RNNs more permutation invariant, resulting in more compact models compared to non-recurrent architectures. Instead of restricting the learning problem to permutation invariant hypothesis classes, our approach enforces permutation invariance through regularization, resulting in semi permutation invariant models. We demonstrate that our method outperforms other permutation invariant approaches on both synthetic and real world datasets.