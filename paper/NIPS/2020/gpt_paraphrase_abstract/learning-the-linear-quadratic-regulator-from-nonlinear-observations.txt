We present a novel problem setting called RichLQR, which focuses on continuous control. In this setting, the environment is represented by a low-dimensional latent state with linear dynamics and quadratic costs, while the agent operates on high-dimensional, nonlinear observations such as camera images. To facilitate efficient learning, we assume the learner has access to a flexible class of decoder functions (e.g., neural networks) that can capture the mapping from observations to latent states. We propose a new algorithm called RichID, which learns a near-optimal policy for RichLQR with sample complexity that scales only with the dimension of the latent state space and the capacity of the decoder function class. RichID is efficient and uses a least-squares regression oracle to access the decoder class. Our results provide the first provable sample complexity guarantee for continuous control when the system model has an unknown nonlinearity.