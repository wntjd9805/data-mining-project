To prevent the catastrophic forgetting of past information in deep learning methods, weight regularisation has been commonly used. Although functional regularisation is believed to be more effective, it is computationally expensive and does not often yield better results. In this study, we propose a new functional-regularisation approach that overcomes this limitation. By employing a Gaussian Process formulation of deep networks, our method allows training in weight-space and identifies important past examples to prevent forgetting. This approach achieves state-of-the-art performance on standard benchmarks and presents a novel direction for lifelong learning by combining regularisation and memory-based methods.