Deep learning relies on effective regularizations to model various priors in data, such as robustness to adversarial perturbations and correlations between multiple modalities. However, most regularizers are defined in terms of hidden layer outputs, which are not optimization variables themselves. In contrast to common methods that indirectly optimize these outputs through model weights, we propose adding a new layer to the deep network called proximal mapping. This layer directly and explicitly generates well regularized hidden layer outputs. This technique is closely related to kernel warping and dropout and we have developed novel algorithms for robust temporal learning and multiview modeling. These algorithms outperform existing methods.