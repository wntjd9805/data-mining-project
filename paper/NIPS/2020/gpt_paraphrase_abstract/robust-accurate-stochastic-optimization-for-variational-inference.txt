The problem of fitting variational posterior approximations using stochastic optimization methods is examined. The quality of these approximations depends on how well the variational family matches the true posterior distribution, the choice of divergence, and the optimization of the variational objective. It is found that common stochastic optimization methods perform poorly in creating accurate variational approximations when the problem dimension is moderately large, even under the best-case scenario where the exact posterior belongs to the assumed variational family. Additionally, these methods are not reliable across different model types. To address these issues, a more robust and accurate stochastic optimization framework is proposed. This framework treats the underlying optimization algorithm as a Markov chain and includes a diagnostic for convergence and a novel stopping rule that can handle noisy evaluations of the objective function. Empirical results demonstrate the effectiveness of the proposed framework on various models, as it can automatically detect failures in stochastic optimization and inaccurate variational approximations.