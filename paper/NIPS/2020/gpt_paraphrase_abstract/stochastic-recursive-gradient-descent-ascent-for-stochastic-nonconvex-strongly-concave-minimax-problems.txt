We investigate nonconvex-concave minimax optimization problems in the form of minimizing f(x, y) while maximizing y∈Y, where f is strongly-concave in y but possibly nonconvex in x and Y is a convex and compact set. Our focus is on the stochastic setting, where we can only access an unbiased stochastic gradient estimate of f during each iteration. This formulation encompasses various machine learning applications, including robust optimization and adversary training. Our objective is to find an O(ε)-stationary point of the function Φ(·) = maxy∈Y f(·, y). The widely used stochastic gradient descent ascent algorithm, which necessitates O(κ3ε−4) stochastic gradient evaluations (with κ being the condition number), is commonly employed to solve this problem. In this study, we propose a novel approach called Stochastic Recursive gradiEnt Descent Ascent (SREDA) that employs variance reduction to estimate gradients more effectively. Our method achieves the most efficient known stochastic gradient complexity of O(κ3ε−3) and has an optimal dependency on ε for this problem.