Deep reinforcement learning is a powerful technique for learning effective policies in various tasks. However, it is known to be difficult to use due to instability and sensitivity to hyperparameters. The reasons behind these issues are not well understood. In this study, we examine how reinforcement learning methods based on bootstrapping-based Q-learning can be affected by a problematic interaction between function approximation and the data distribution used for training the Q-function. In standard supervised learning, online data collection provides corrective feedback, where new data corrects mistakes made by previous predictions. However, with Q-learning, this feedback may be absent, leading to instability, sub-optimal convergence, and poor results when learning from noisy, sparse, or delayed rewards. Based on these findings, we propose a new algorithm called DisCor that explicitly optimizes for data distributions capable of correcting accumulated errors in the value function. DisCor calculates a tractable approximation of the distribution that induces optimal corrective feedback by reweighting samples based on the estimated accuracy of their target values. By using this distribution during training, DisCor significantly improves performance in challenging reinforcement learning scenarios, including multi-task learning and learning from noisy reward signals.