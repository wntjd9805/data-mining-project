Effective planning is essential for an instruction-following agent. However, current methods struggle with connecting language instructions to the agent's knowledge of the environment and performing long-range planning. In this study, we propose the Evolving Graphical Planner (EGP), which uses raw sensory input to create a graphical representation and allows for flexible decision making. The EGP achieves superior performance in a challenging Vision-and-Language Navigation task compared to previous architectures, achieving a 53% success rate through pure imitation learning. This outperforms previous architectures by up to 5%.