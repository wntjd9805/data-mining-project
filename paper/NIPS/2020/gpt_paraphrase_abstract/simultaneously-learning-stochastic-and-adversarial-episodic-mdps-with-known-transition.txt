This study focuses on learning episodic Markov Decision Processes with known transition and bandit feedback. The researchers have developed a new algorithm that offers the best of both worlds. It achieves O(log T) regret when the losses are stochastic, and provides worst-case robustness with O(T) regret even when the losses are adversarial. Additionally, the algorithm performs well in an intermediate setting where the losses are corrupted by a total amount of C. The algorithm is based on the Follow-the-Regularized-Leader method and incorporates a hybrid regularizer inspired by recent works. The regularizer has a non-diagonal Hessian with a complex inverse. The analysis of this regularizer and the derivation of a self-bounding regret guarantee are the key technical contributions of this research.