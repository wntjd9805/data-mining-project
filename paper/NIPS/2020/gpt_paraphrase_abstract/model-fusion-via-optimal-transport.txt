The combination of different models is commonly used in machine learning. However, the traditional approach of forming an ensemble and averaging their predictions can be impractical due to resource constraints. In this study, we propose a layer-wise model fusion algorithm that uses optimal transport to align neurons across models before averaging their parameters. This method allows for successful knowledge transfer between neural networks trained on different types of data without requiring retraining. We demonstrate that our approach outperforms traditional averaging in both i.i.d. and non-i.i.d. settings for various types of networks on different datasets. Additionally, our method offers a systematic way to combine neural networks with different widths, and we explore its potential for model compression. The code for our algorithm is available at the provided link.