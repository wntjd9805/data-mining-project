We introduce a new type of explanation for Reinforcement Learning centered on the concept of intended outcome. These explanations detail the desired result an agent aims to accomplish through its actions. We establish that conventional post-hoc explanations are not feasible in traditional reinforcement learning through a straightforward demonstration. Instead, the necessary information for explanations must be gathered concurrently with agent training. We develop specific approaches that generate local explanations based on intention for various forms of Q-function approximation and verify the consistency between the explanations and the learned Q-values. Our method is showcased on multiple reinforcement learning problems, and we provide code1 to assist researchers in examining their RL environments and algorithms.