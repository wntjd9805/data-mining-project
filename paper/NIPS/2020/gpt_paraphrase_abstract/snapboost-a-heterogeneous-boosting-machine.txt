Modern gradient boosting frameworks like XGBoost and LightGBM use Newton descent in a functional space to find the closest base hypothesis from a given class. However, in this study, we introduce a Heterogeneous Newton Boosting Machine (HNBM) where the base hypothesis class can change during boosting iterations. Each boosting iteration selects a base hypothesis class from a fixed set of subclasses using a probability distribution. We establish a global linear convergence rate for the HNBM, which aligns with existing rates for Newton's method when the base hypothesis perfectly fits the Newton direction each iteration. We present SnapBoost as a specific implementation of HNBM, where each boosting iteration randomly chooses between a decision tree with variable depth or a linear regressor with random Fourier features. We detail the implementation of SnapBoost, with a focus on training complexity. Experimental results using OpenML and Kaggle datasets demonstrate that SnapBoost achieves superior generalization loss compared to other boosting frameworks, without significantly increasing tuning time.