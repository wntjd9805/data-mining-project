We present a novel regularization technique called Gradient Augmentation (GradAug) to address the problem of overfitting in deep neural networks. GradAug involves using randomly transformed training samples to regularize a set of sub-networks that are created by sampling the width of the original network during training. This introduces self-guided disturbances to the network's raw gradients, leading to improved generalization and the learning of more diverse representations. GradAug is easy to implement and can be applied to various network structures and applications. We demonstrate its effectiveness by achieving a new state-of-the-art accuracy of 78.79% on ImageNet classification using ResNet-50, which can be further boosted to 79.67% when combined with CutMix. GradAug also outperforms other state-of-the-art methods in COCO object detection and instance segmentation tasks. It is robust to image distortions and FGSM adversarial attacks, and performs well even in low data regimes. The code for implementing GradAug is available at https://github.com/taoyang1122/GradAug.