Autodiff systems like TensorFlow and PyTorch are commonly used for differentiating functions in machine learning algorithms. However, these systems are also applied to functions with non-differentiable parts, such as neural networks using ReLU. This raises the question of whether autodiff systems are formally correct when applied to non-differentiable functions. In this paper, we address this question and provide a positive answer. We identify flaws in informal arguments that claim non-differentiabilities in deep learning have no impact because they form a measure-zero set. We introduce a class of functions called PAP functions, which include most functions in deep learning, and propose a new type of derivatives called intensional derivatives. We prove that these intensional derivatives always exist and coincide with standard derivatives for almost all inputs. Additionally, we demonstrate that these intensional derivatives are what most autodiff systems compute or aim to compute. By doing so, we formally establish the correctness of autodiff systems applied to non-differentiable functions.