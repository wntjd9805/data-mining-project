Pre-trained language models like BERT and its variations have achieved impressive results in natural language understanding tasks. However, BERT's heavy reliance on global self-attention leads to high memory usage and computational expenses. We have observed that some attention heads in BERT only need to learn local dependencies, indicating redundancy in computations. To address this, we propose a new approach called span-based dynamic convolution to model local dependencies instead of self-attention heads. This mixed attention design, combining both convolution and self-attention heads, enhances both global and local context learning, resulting in our ConvBERT model. Experimental results demonstrate that ConvBERT surpasses BERT and its variants in different downstream tasks, while requiring lower training costs and fewer parameters. Notably, the ConvBERTBASE model achieves an 86.4 GLUE score, 0.7 higher than ELECTRABASE, with less than 1/4 of the training cost. The code and pre-trained models will be made available.