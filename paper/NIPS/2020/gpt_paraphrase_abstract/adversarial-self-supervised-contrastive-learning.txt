Current adversarial learning approaches typically use class labels to generate adversarial samples that cause incorrect predictions, which are then used to enhance the model's robustness through additional training. Although some recent methods propose semi-supervised adversarial learning techniques that utilize unlabeled data, they still rely on class labels. This paper questions the necessity of class labels in adversarially robust training of deep neural networks. The authors propose a novel adversarial attack for unlabeled data, which confuses the model by altering the instance-level identities of perturbed data samples. Additionally, they introduce a self-supervised contrastive learning framework to train a robust neural network without labeled data. This framework aims to maximize the similarity between a randomly augmented data sample and its instance-wise adversarial perturbation. The authors validate their method, called Robust Contrastive Learning (RoCL), on multiple benchmark datasets. RoCL achieves comparable robust accuracy to state-of-the-art supervised adversarial learning methods and demonstrates significantly improved robustness against black box and unseen types of attacks. Additionally, when jointly fine-tuned with supervised adversarial loss, RoCL achieves even higher robust accuracy compared to self-supervised learning alone. Notably, RoCL also shows impressive results in robust transfer learning.