MARGE is a pre-trained sequence-to-sequence model that learns to paraphrase text in multiple languages. It differs from other models by reconstructing target text using a set of related texts, rather than using masked language modeling. This approach allows MARGE to learn retrieval and reconstruction simultaneously from random initialization. The objective of MARGE captures various aspects of paraphrasing, translation, summarization, and information retrieval, resulting in impressive zero-shot performance on different tasks. For instance, without task-specific training, MARGE achieves high BLEU scores of up to 35.8 for document translation. Additionally, fine-tuning MARGE produces strong results across various discriminative and generative tasks in multiple languages, making it the most versatile pre-training method available.