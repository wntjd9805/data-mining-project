Minimum distance estimation (MDE) has recently gained attention as a method for generative modeling. It involves minimizing a statistical distance between the empirical data distribution and the model's distribution. However, the curse of dimensionality poses challenges for obtaining meaningful results. To address this, we propose the use of smooth 1-Wasserstein distance (SWD) as a framework for MDE. SWD has been shown to preserve the metric and topological structure of classic Wasserstein distances, with dimension-free empirical convergence rates. In this study, we thoroughly analyze the statistical properties of minimum smooth Wasserstein estimators (MSWEs). We establish the measurability and asymptotic consistency of these estimators, and characterize the limit distribution of the optimal model parameters. We also derive a generalization bound for generative modeling based on MSWE, which holds in any dimension. Our key contribution is a novel high-dimensional limit distribution result for empirical SWD. This result is significant as it contrasts with the known result for empirical 1-Wasserstein distance, which is limited to one-dimensional cases. Empirical results support the validity of our theory, highlighting the potential of SWD as a powerful tool for learning and inference in high-dimensional settings.