We present a novel online contextual bandit algorithm called Gated Linear Contextual Bandits (GLCB) that utilizes the Gated Linear Networks (GLNs) architecture. GLCB effectively estimates prediction uncertainty without any additional computational overhead by leveraging the data-dependent gating properties of GLN. We conduct empirical evaluations comparing GLCB with nine state-of-the-art algorithms that employ deep neural networks on a standard benchmark suite of contextual bandit problems. Despite being the only online method, GLCB consistently achieves the highest mean rank. Moreover, we provide a theoretical analysis of GLCB's convergence properties to further support our empirical findings.