Recent research has demonstrated that highly overparameterized neural networks can be seen as equivalent to kernel regressors utilizing Neural Tangent Kernels (NTKs). Empirical evidence supports the notion that these kernel methods yield similar performance to real neural networks. In this study, we establish a close relationship between NTKs for fully connected networks with ReLU activation and the standard Laplace kernel. Theoretical analysis reveals that both kernels possess the same eigenfunctions and exhibit polynomial decay in their eigenvalues when applied to normalized data on a hypersphere. This implies that their Reproducing Kernel Hilbert Spaces (RKHS) encompass the same sets of functions, thereby endowing them with comparable smoothness properties. While the two kernels diverge for data situated off the hypersphere, experimental results indicate that these disparities become insignificant with proper data normalization. Moreover, we conduct experiments on actual data to compare NTK with the Laplace kernel and a broader range of Î³-exponential kernels, and find that their performances are almost indistinguishable. These findings suggest that the well-established Laplace kernel, which possesses a straightforward closed form, can provide valuable insights into the behavior of neural networks.