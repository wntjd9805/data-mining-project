Reducing the variance of the gradient estimator is known to enhance the convergence rate of stochastic gradient-based optimization and sampling algorithms. One approach to achieve this is by designing importance sampling strategies. Recently, the problem of designing such schemes was formulated as an online learning problem with bandit feedback, resulting in algorithms with sub-linear static regret. Building upon this framework, we propose Avare, a simple and efficient algorithm for adaptive importance sampling in finite-sum optimization and sampling with decreasing step-sizes. Under standard technical conditions, we demonstrate that Avare achieves O(T 2/3) and O(T 5/6) dynamic regret for SGD and SGLD respectively, when implemented with O(1/t) step sizes. We accomplish this dynamic regret bound by leveraging our understanding of the algorithm's dynamics and combining ideas from online learning and variance-reduced stochastic optimization. Through empirical validation, we confirm the performance of our algorithm and identify scenarios where it leads to significant improvements.