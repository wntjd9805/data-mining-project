The current image-to-image translation methods have achieved impressive results, but they struggle when it comes to translating between classes that require significant shape changes. This is likely due to the high-resolution bottlenecks used in these methods. To address this issue, we propose a new deep hierarchical image-to-image translation method called DeepI2I. Our approach leverages both structural information from shallow layers and semantic information from deep layers to learn a more effective model. To overcome the challenge of training deep I2I models with limited datasets, we introduce a novel transfer learning method that utilizes pre-trained GANs. Specifically, we use the discriminator of a pre-trained GAN (such as BigGAN or StyleGAN) to initialize the encoder and discriminator of our model, and the pre-trained generator to initialize the generator. To resolve the alignment problem between the encoder and generator caused by knowledge transfer, we introduce an adaptor network. Our experiments on three datasets (Animal faces, Birds, and Foods) demonstrate that our method reduces mFID (a measure of image quality) by at least 35% compared to the state-of-the-art. We also show that transfer learning significantly enhances the performance of I2I systems, particularly for small datasets. Additionally, we are the first to perform I2I translations for domains with over 100 classes. Our code and models are publicly available at: https://github.com/yaxingwang/DeepI2I.