We propose a new variant of Stochastic Gradient Descent (SGD) called CSER, which addresses the scalability limitations caused by communication bottlenecks. CSER incorporates a technique called "error reset" that adapts compressors for SGD, resulting in bifurcated local models with periodic reset of residual errors. Additionally, CSER introduces partial synchronization for both gradients and models, taking advantage of their benefits. We prove the convergence of CSER for smooth non-convex problems. Empirical results demonstrate that when combined with aggressive compressors, CSER significantly accelerates distributed training by approximately 10× for CIFAR-100 and 4.5× for ImageNet.