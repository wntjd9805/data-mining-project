Ensemble techniques are commonly used to enhance the accuracy and stability of learning models, particularly when dealing with small datasets. While tree-based methods have received significant attention in this area, there has been relatively less focus on developing effective ensemble designs for neural networks. This paper presents a principled ensemble technique called the diversiÔ¨Åed ensemble layer, which combines multiple networks as individual modules. Theoretical analysis demonstrates that each individual model in this ensemble layer corresponds to optimized weights in different directions. Additionally, the proposed ensemble layer can be seamlessly integrated into popular neural architectures such as CNNs, RNNs, and GCNs. Extensive experiments conducted on tabular datasets, images, and texts validate the effectiveness of our approach. By adopting a weight sharing approach, our method significantly improves the accuracy and stability of original neural networks with minimal additional time and space requirements.