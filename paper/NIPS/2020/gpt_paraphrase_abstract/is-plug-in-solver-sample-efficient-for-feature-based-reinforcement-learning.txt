A model-based approach in reinforcement learning (RL) is believed to be crucial in reducing sample complexity. However, the understanding of the sample optimality of model-based RL, even in the linear case, is still lacking. This study focuses on the sample complexity of finding an optimal policy in a Markov decision process (MDP) using a linear additive feature representation and only having access to a generative model. The problem is solved using a plug-in solver approach, where an empirical model is constructed and planning is done within this empirical model using any plug-in solver. It is proven that under the anchor-state assumption (which implies non-negativity in the feature space), the minimax sample complexity of finding an optimal policy in a γ-discounted MDP is O(K/(1 − γ)3), where K is the dimensionality of the feature space and there is no dependence on the state or action space. Additionally, the results are extended to a relaxed setting where anchor-states may not exist, demonstrating that the plug-in approach can still be sample efficient and offers a flexible way to design model-based RL algorithms.