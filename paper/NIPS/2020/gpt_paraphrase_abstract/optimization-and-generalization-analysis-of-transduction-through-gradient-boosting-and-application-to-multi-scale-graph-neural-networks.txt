Graph neural networks (GNNs) face challenges in achieving depth due to over-smoothing. To address this issue, multi-scale GNNs have emerged as a potential solution. However, the empirical success of multi-scale GNNs lacks a thorough explanation from a learning theory perspective. This study aims to establish the optimization and generalization guarantees for transductive learning algorithms incorporating multi-scale GNNs. By leveraging boosting theory, we demonstrate the convergence of training error under certain weak learning conditions. Additionally, we combine this with bounds on transductive Rademacher complexity to establish a test error bound for a specific type of multi-scale GNNs that decreases with the number of node aggregations, given certain conditions. These findings provide theoretical insights into the effectiveness of the multi-scale structure in mitigating the over-smoothing problem. Furthermore, we apply boosting algorithms to train multi-scale GNNs for real-world node prediction tasks and confirm their performance comparability to existing GNNs, aligning with the theoretical observations. The source code is available at https://github.com/delta2323/GB-GNN.