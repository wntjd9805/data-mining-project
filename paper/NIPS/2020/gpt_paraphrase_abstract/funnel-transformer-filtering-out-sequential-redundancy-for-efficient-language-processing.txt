To make use of unlabeled data more efficiently and at a lower cost, there is a need for scalable architectures in language pretraining. In order to improve efficiency, we explore the redundancy of maintaining a full-length token-level representation, particularly for tasks that only require a single-vector representation of the sequence. Based on this idea, we propose Funnel-Transformer, which gradually compresses the hidden states sequence to reduce computation costs. Additionally, by reinvesting the saved computational operations in constructing a deeper or wider model, we enhance the model capacity. Funnel-Transformer is also capable of recovering a deep representation for each token through a decoder, enabling token-level predictions as needed for common pretraining objectives. In empirical evaluations, Funnel-Transformer outperforms the standard Transformer on various sequence-level prediction tasks such as text classification, language understanding, and reading comprehension, while utilizing comparable or fewer computational operations (FLOPs).