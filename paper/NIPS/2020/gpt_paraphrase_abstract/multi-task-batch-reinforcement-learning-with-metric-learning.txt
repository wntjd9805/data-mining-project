We address the Multi-task Batch Reinforcement Learning problem by training a multi-task policy using multiple datasets from different tasks. The goal is for the policy to perform well on unseen tasks from the same distribution without knowing their specific identities. In order to achieve this, the policy needs to infer the task identity based on the collected transitions, taking into account states, actions, and rewards. However, the divergence in state-action distributions across different datasets can lead to the task inference module disregarding rewards and incorrectly correlating state-action pairs with the task identity, resulting in poor performance during testing. To address this issue, we propose a new approach using the triplet loss to enhance task inference. By relabeling transitions from training tasks based on approximated reward functions, we can mine hard negative examples. When the trained policy is used as an initialization for further training on unseen tasks, it significantly improves convergence speed compared to randomly initialized policies (up to 80% improvement across 5 different Mujoco task distributions). Our method is called MBML (Multi-task Batch RL with Metric Learning).