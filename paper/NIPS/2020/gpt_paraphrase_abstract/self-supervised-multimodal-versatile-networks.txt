This work focuses on learning representations from videos using self-supervision and leveraging the visual, audio, and language modalities present in videos. The authors propose a multimodal versatile network that can handle multiple modalities and produce representations suitable for various downstream tasks. They explore methods to combine these modalities while preserving fine-grained representations of visual and audio information, and integrating text into a shared embedding. They also introduce a process called deï¬‚ation to apply the networks effortlessly to visual data in the form of videos or static images. The authors demonstrate the effectiveness of their approach by training these networks on large collections of unlabeled video data and achieving state-of-the-art performance on challenging benchmarks, including UCF101, HMDB51, Kinetics600, AudioSet, and ESC-50, surpassing previous self-supervised methods. The models developed in this work are publicly available.