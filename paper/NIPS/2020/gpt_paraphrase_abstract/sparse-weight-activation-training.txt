We present a new algorithm called Sparse Weight Activation Training (SWAT) for training convolutional neural networks (CNNs). SWAT is designed to improve computation and memory efficiency compared to traditional training methods. It achieves this by modifying the back-propagation process based on the observation that network convergence is generally unaffected by removing small weights and activations during both the forward and backward passes. We evaluate SWAT on popular CNN architectures using various datasets, including CIFAR-10, CIFAR-100, and ImageNet. In the case of ResNet-50 on ImageNet, SWAT reduces the total floating-point operations (FLOPs) during training by 80%, resulting in a 3.3Ã— speedup when run on a simulated sparse learning accelerator. This speedup comes with only a 1.63% reduction in validation accuracy. Additionally, SWAT significantly reduces the memory footprint during the backward pass, with reductions ranging from 23% to 50% for activations and 50% to 90% for weights. The code for SWAT is available at https://github.com/AamirRaihan/SWAT.