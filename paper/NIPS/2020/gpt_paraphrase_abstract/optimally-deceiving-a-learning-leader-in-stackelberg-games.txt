Recent findings in the ML community have demonstrated that learning algorithms utilized to determine the optimal strategy for the leader in a Stackelberg game can be manipulated by the follower. These algorithms rely on querying the follower's best responses or payoffs, which allows the follower to deceive the algorithm by providing false payoffs. The follower's main challenge is to identify the payoffs that would cause the learning algorithm to compute a commitment, enabling them to maximize their utility based on the true payoffs. Although this issue has been previously explored, existing literature has only examined simplified scenarios with finite payoff spaces, leaving the general problem unresolved. This study addresses this gap by demonstrating that the follower can efficiently compute nearly optimal payoffs in various learning interaction scenarios between the leader and the follower.