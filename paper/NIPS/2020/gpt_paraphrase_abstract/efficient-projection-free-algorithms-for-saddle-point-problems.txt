The Frank-Wolfe algorithm is a well-known approach for solving optimization problems with constraints. Recently, it has gained popularity in various machine learning applications due to its efficiency in iterations without the need for projections. This study focuses on developing projection-free algorithms that can handle convex-strongly-concave saddle point problems with complex constraints. By combining Conditional Gradient Sliding with Mirror-Prox, our method achieves satisfactory results with a minimal number of gradient evaluations and linear optimizations in the batch setting. Additionally, we extend our approach to the stochastic setting and propose the first stochastic projection-free algorithms for saddle point problems. Experimental results validate the effectiveness of our algorithms and support our theoretical guarantees.