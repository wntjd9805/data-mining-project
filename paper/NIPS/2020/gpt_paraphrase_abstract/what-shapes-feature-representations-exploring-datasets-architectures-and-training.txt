The focus of this study is to determine which features a model uses in naturalistic learning problems. The researchers aim to understand the basis of models' decisions and build adaptable representations that can be useful beyond the initial training task. Synthetic datasets were used to directly control the task-relevance of input features. The findings indicate that when two features redundantly predict labels, the model tends to represent one feature, based on what is most linearly decodable from the untrained model. During training, task-relevant features are enhanced while task-irrelevant features are partially suppressed. Interestingly, in some cases, an easier but weakly predictive feature can suppress a more strongly predictive but difficult one. Models trained on both easy and hard features tend to have representations similar to models that use only the easy feature. Easy features also lead to more consistent representations across model runs compared to hard features. Furthermore, the models' representations are more similar to an untrained model than to models trained on a different task. These findings demonstrate the complex processes that determine the features a model represents.