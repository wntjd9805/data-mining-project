Learning how to grasp objects using robots based on visual observations is a challenging yet promising task. Recent studies have demonstrated the potential of using large-scale synthetic datasets for this purpose. However, existing methods for the popular 6-DOF grasp setting of a parallel-jaw gripper rely on heuristic sampling and evaluation, which limits the efficiency and coverage of optimal grasps. In this work, we propose a novel approach called Grasp Proposal Network (GPNet) that predicts a diverse set of 6-DOF grasps for unseen objects from a single unknown camera view. GPNet utilizes a grasp proposal module that defines grasp centers at discrete 3D grid corners, allowing for precise or diverse grasp predictions. We evaluate GPNet using a synthetic dataset of 6-DOF object grasps and compare it to existing methods using rule-based criteria, simulation tests, and real tests. The results show the superiority of our approach, particularly in simulation where GPNet achieves better coverage. Our code and dataset are publicly available for further research.