Generating structured discrete data, such as source code or molecules, that satisfy specific constraints or exhibit desired properties remains challenging. Current methods rely on expensive heuristic search or reinforcement learning algorithms. This paper proposes a novel approach using conditional generative models to directly address this problem by modeling the distribution of discrete structures based on desired properties. However, maximum likelihood training often fails to produce samples that adequately respect the input properties. To overcome this, the paper introduces a new method that optimizes a reinforcement learning objective to maximize expected reward. By sampling from an approximation of normalized rewards, the approach avoids high-variance score-function estimators and enables simple Monte Carlo estimation of model gradients. The proposed methodology is tested on two tasks: generating molecules with user-defined properties and identifying short Python expressions that evaluate to a given target value. The results show improvements over maximum likelihood estimation and other baselines.