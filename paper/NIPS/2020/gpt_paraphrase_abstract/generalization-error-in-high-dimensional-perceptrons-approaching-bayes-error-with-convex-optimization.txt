We examine the generalization performance of standard classifiers in a high-dimensional setting using a synthetic dataset. The dataset's labels are generated by feeding random inputs into a one-layer neural network. In this regime, where the ratio of samples to dimensions is finite, we make three contributions. Firstly, we provide a formula for the generalization error achieved by regularized classifiers that minimize a convex loss. This formula was previously obtained using the replica method of statistical physics. Secondly, we find that while ridge regression performs poorly, logistic and hinge regression can closely approach the Bayes-optimal generalization error when the regularization strength is optimized. This holds true even as the ratio of samples to dimensions approaches infinity, which is not predicted by margin-based generalization error bounds. Lastly, we propose an optimal loss and regularizer that guarantee Bayes-optimal generalization error.