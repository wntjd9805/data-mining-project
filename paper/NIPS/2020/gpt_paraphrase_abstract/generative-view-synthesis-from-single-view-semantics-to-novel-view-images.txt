In applications like virtual reality, content creation can be time-consuming and tedious. However, recent advances in image synthesis have made this task easier by providing tools that can generate new views from just one input image or convert a semantic map into a realistic image. In this study, we propose a new method called Generative View Synthesis (GVS) that can generate multiple realistic views of a scene based on a single semantic map. We found that existing techniques, such as translating semantics to images and then synthesizing views, fail to capture the scene's structure. In contrast, our approach combines semantics-to-image translation with the estimation of the scene's 3D layout, resulting in novel views that maintain both geometric and semantic consistency. We achieve this by representing the 2D semantic map as a 3D layered representation in feature space, preserving the semantic labels of the scene's geometric structures. By projecting these layered features onto target views, we can generate high-quality novel-view images. We have validated our method against advanced baselines using three different datasets, demonstrating its superiority. Additionally, our approach allows for style manipulation and image editing operations, such as adding or removing objects, by manipulating style images and semantic maps. For more information, including code and additional results, please visit our project page at https://gvsnet.github.io.