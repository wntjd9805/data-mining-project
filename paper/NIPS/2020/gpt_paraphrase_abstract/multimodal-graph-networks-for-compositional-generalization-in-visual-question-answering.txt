Compositional generalization is a major challenge in connecting natural language with visual perception. Although deep learning models have been successful in tasks like visual question answering, recent studies have shown that they struggle to adapt to new inputs that combine elements from the training data. In this study, we propose using neural factor graphs to address this challenge by establishing stronger connections between concepts in different modalities, such as images and text. Graph representations are inherently compositional and allow us to capture entities, attributes, and relationships in a scalable way. Our model creates a multimodal graph, processes it using a graph neural network to generate a factor correspondence matrix, and then produces a symbolic program to predict answers to questions. In experiments, our model achieves nearly perfect scores on a caption truth prediction problem and outperforms previous approaches on the CLOSURE dataset, improving overall accuracy across seven compositional templates by 4.77%. The answer format focuses on providing only the abstraction.