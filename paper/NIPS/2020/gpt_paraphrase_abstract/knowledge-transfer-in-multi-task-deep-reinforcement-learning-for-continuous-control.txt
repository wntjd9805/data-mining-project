This paper introduces a framework called KTM-DRL that enables a single DRL agent to achieve expert-level performance in multiple continuous control tasks. The framework utilizes a knowledge transfer algorithm to quickly learn a control policy from task-specific teachers and then employs an online learning algorithm to further improve itself with new transition samples. The effectiveness of KTM-DRL is demonstrated through empirical experiments on two common benchmarks, showing its superiority over existing approaches by a significant margin.