Sparse deep learning aims to address the issue of large storage consumption by deep neural networks and to recover the sparse structure of target functions. While many empirical successes have been achieved, most current sparse deep learning algorithms lack theoretical support. Conversely, other theoretical frameworks proposed are computationally infeasible. This paper introduces a fully Bayesian treatment for training sparse deep neural networks using spike-and-slab priors. Additionally, a set of computationally efficient variational inferences is developed through the continuous relaxation of Bernoulli distribution. The consistency of the proposed variational Bayes method is justified by providing the variational posterior contraction rate. Importantly, empirical results demonstrate that this variational procedure offers uncertainty quantification through Bayesian predictive distribution and achieves consistent variable selection by training a sparse multi-layer neural network.