This study focuses on identifying redundant and important feature groups in learning problems. The concept of feature redundancy is formally defined using information theory, specifically representation and relevant redundancies. The researchers propose a formulation for discovering feature groups on a per-instance basis and provide a theoretical guideline for determining the appropriate number of groups. Mutual information is approximated using a variational lower bound, and Gumbel-Softmax is used to learn the feature group and selector indicators during optimization. Synthetic data experiments confirm the theoretical claims, while experiments on MNIST, Fashion MNIST, and gene expression datasets demonstrate the high classification accuracies achieved by the proposed method.