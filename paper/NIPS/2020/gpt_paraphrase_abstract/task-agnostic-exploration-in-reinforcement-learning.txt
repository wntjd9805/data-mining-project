Efficient exploration is a major challenge in reinforcement learning (RL). Most current algorithms assume the presence of a single reward function for exploration. However, in practical scenarios where an agent needs to learn multiple skills simultaneously or balance conflicting objectives, there is often no single underlying reward function. To address this, we propose the task-agnostic RL framework. In this framework, the agent first explores the environment without the guidance of a reward function to collect trajectories. It then aims to find near-optimal policies for N tasks using the collected trajectories and sampled rewards for each task. We introduce UCBZERO, an efficient task-agnostic RL algorithm that can find ε-optimal policies for N arbitrary tasks after a maximum of ˜O(log(N)H5SA/ε2) exploration episodes, where H represents the episode length, S is the state space size, and A is the action space size. We also prove an Ω(log(N)H2SA/ε2) lower bound, demonstrating the unavoidable logarithmic dependency on N. Additionally, we provide a sample complexity bound for UCBZERO in the reward-free setting, where the ground truth reward functions are known, regardless of the value of N.