Paraphrased abstract:  Finding approximate Nash equilibria in large zero-sum imperfect-information games is difficult. Policy Space ResponseOracles (PSRO) is a deep reinforcement learning algorithm based on game theory that converges to an approximate Nash equilibrium. However, PSRO is slow for large games as it requires training a reinforcement learning policy at each iteration. Existing approaches, DCH and Rectified PSRO, fail to converge even in small games. To address this, we propose Pipeline PSRO (P2SRO), the first scalable PSRO-based method for finding approximate Nash equilibria in large games. P2SRO parallelizes PSRO by maintaining a hierarchical pipeline of reinforcement learning workers. Each worker trains against policies generated by lower levels in the hierarchy. P2SRO converges faster with more parallel workers and achieves state-of-the-art performance on Barrage Stratego, a variant of Stratego. The open-source environment and experiment code for Barrage Stratego can be found at https://github.com/JBLanier/pipeline-psro.