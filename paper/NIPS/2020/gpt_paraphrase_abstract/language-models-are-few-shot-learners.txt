We show that increasing the size of language models significantly enhances their ability to perform tasks without specific training. In particular, we train GPT-3, an autoregressive language model with 175 billion parameters, which is 10 times larger than any previous non-sparse language model. We evaluate GPT-3's performance in few-shot scenarios, where it does not receive any gradient updates or fine-tuning, and tasks are specified solely through text interactions. GPT-3 achieves impressive results on various NLP datasets, such as translation, question-answering, and cloze tasks. However, we observe limitations in GPT-3's few-shot learning on certain datasets and encounter methodological challenges related to training on extensive web data.