This study focuses on the task of learning and representing dense correspondences in deformable object categories. Previous solutions for this problem, particularly for humans, have been ad-hoc and required significant manual work. However, in order to scale geometry understanding to all objects in nature, automated approaches are needed that can express correspondences between related but geometrically different objects. In this study, a new image-based representation of dense correspondences is proposed. The model predicts an embedding vector for each pixel in a 2D image, establishing correspondences between image pixels and 3D object geometry. The proposed approach performs as well as or better than state-of-the-art methods for dense pose estimation for humans, while also being simpler conceptually. A new dataset of dense correspondences for animal classes is collected, and it is shown that the framework naturally scales to these new deformable object categories.