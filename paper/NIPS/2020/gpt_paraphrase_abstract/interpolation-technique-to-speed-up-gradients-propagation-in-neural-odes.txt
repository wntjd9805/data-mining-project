We present a straightforward method using interpolation to effectively estimate gradients in neural ODE models. In order to train neural ODEs for classification, density estimation, and inference approximation tasks, we compare our method to the commonly known reverse dynamic method (adjoint method). We also provide a theoretical justification for our approach using logarithmic norm formalism. Our method has been confirmed and validated through extensive numerical experiments on various standard benchmarks, demonstrating faster model training compared to the reverse dynamic method.