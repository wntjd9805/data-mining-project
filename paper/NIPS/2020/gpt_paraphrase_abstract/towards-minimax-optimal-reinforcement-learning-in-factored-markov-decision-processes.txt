The focus of this study is minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs). FMDPs are a type of MDP where the transition components are conditionally independent. The study proposes two model-based algorithms for FMDPs, assuming the factorization is known. The first algorithm guarantees minimax optimal regret for a wide range of factored structures, while the second algorithm has better computational complexity but slightly worse regret. A novel aspect of these algorithms is the inclusion of a bonus term to guide exploration. Additionally, the study presents several structure-dependent lower bounds on regret for FMDPs, highlighting the challenges posed by the intricate structures.