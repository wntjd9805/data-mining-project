We present a novel approach to address the problem of limited visual relationship models caused by imbalanced predicate distribution in current human-annotated datasets. Our self-supervised method learns visual relationships without relying on ground-truth annotations. It achieves this through intra- and inter-modality encodings that model relationships within each modality separately and jointly, as well as relationship probing to discover graph structures within each modality. We utilize masked language modeling, contrastive learning, and dependency tree distances for self-supervision, resulting in enhanced object features and implicit visual relationships. Our method demonstrates improved visual relationship understanding in various vision-language tasks.