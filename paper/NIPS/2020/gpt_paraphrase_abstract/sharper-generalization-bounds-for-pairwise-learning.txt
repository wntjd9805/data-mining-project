Pairwise learning, such as ranking and metric learning, involves loss functions that depend on pairs of training examples. There has been a growing interest in understanding the practical behavior of pairwise learning through generalization analysis. However, the current stability analysis provides suboptimal generalization bounds with high probability. In this study, we present a more refined stability analysis that achieves generalization n-times faster than existing results, where n is the sample size. This implies that both regularized risk minimization and stochastic gradient descent have excess risk bounds of O(nâˆ’1/2) (with a logarithmic factor). Additionally, we introduce a new on-average stability measure that allows for optimistic bounds in a low noise setting. We apply our findings to ranking and metric learning, demonstrating the superiority of our generalization bounds compared to existing analysis.