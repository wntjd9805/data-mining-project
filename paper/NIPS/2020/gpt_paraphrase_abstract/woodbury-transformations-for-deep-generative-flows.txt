Normalizing flows are deep generative models that excel in efficient likelihood calculation and sampling. To achieve this advantage, these models must be constructed using functions that can be efficiently inverted, with the ability to compute the determinant of the function's Jacobian also being efficient. While various flow operations have been introduced by researchers, only a few of them enable rich interactions between variables without incurring significant computational costs. This paper introduces Woodbury transformations, which utilize the Woodbury matrix identity for efficient invertibility and Sylvester's determinant identity for efficient determinant calculation. Unlike other operations used in state-of-the-art normalizing flows, Woodbury transformations offer three key advantages: (1) the ability to handle high-dimensional interactions, (2) efficient sampling, and (3) efficient likelihood evaluation. Comparable operations like 1x1 convolutions, emerging convolutions, or periodic convolutions can only provide at most two of these advantages. Experimental results on multiple image datasets demonstrate that Woodbury transformations enable the learning of higher-likelihood models compared to other flow architectures, while still maintaining their efficiency benefits.