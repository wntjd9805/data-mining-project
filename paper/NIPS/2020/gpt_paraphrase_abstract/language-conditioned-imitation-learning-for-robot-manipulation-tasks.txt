Imitation learning in robotics often focuses on extracting policy parameters from execution traces, overlooking the need for a communication channel between human experts and robots. This channel is necessary for conveying critical aspects of the task, such as target object properties or intended motion shape. Taking inspiration from human teaching processes, we propose a method that incorporates unstructured natural language into imitation learning. During training, the expert can provide verbal descriptions alongside demonstrations to convey underlying intent. This approach enables the encoding of correlations between language, perception, and motion, resulting in language-conditioned visuomotor policies. These policies can then be conditioned at runtime on new human commands, allowing for finer control and reduced situational ambiguity. We conducted simulation experiments to showcase our approach's ability to learn language-conditioned manipulation policies for a seven-degree-of-freedom robot arm. We compared our results with various alternative methods.