Probabilistic neural networks often use independent weight priors, which do not account for correlations between weights and lack an efficient way to express properties in the function space. This paper proposes two novel approaches. Firstly, a hierarchical model for network weights based on unit priors is introduced, which can represent correlated weight structures effectively. Secondly, input-dependent versions of these weight priors are developed, allowing for regularization of the function space using kernels defined on contextual inputs. The models are shown to produce accurate uncertainty estimates for out-of-distribution data and demonstrate improved performance in both interpolation and extrapolation tasks through the use of kernels.