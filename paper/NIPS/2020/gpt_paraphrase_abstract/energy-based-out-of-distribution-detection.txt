Detecting whether inputs are out-of-distribution (OOD) is crucial for safely deploying machine learning models in real-world scenarios. Existing methods that rely on softmax confidence scores often suffer from overconfident posterior distributions for OOD data. In this study, we propose a unified framework for OOD detection using an energy score. Our results demonstrate that energy scores are more effective in distinguishing between in-distribution and out-of-distribution samples compared to traditional softmax scores. Energy scores are aligned with the probability density of the inputs and are less affected by overconfidence issues. This framework allows for the flexible use of energy scores as a scoring function for any pre-trained neural classifier and as a trainable cost function to explicitly shape the energy surface for OOD detection. Experimental results on a CIFAR-10 pre-trained WideResNet model show that using the energy score reduces the average false positive rate (at a true positive rate of 95%) by 18.03% compared to softmax confidence scores. Additionally, our method surpasses the state-of-the-art on common benchmarks when energy-based training is employed.