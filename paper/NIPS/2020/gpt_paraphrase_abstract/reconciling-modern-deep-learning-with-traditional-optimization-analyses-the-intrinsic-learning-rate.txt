Recent research (Li and Arora, 2020) indicates that the prevalent normalization techniques used in deep learning, such as Batch Normalization, deviate significantly from traditional optimization approaches. This paper aims to explore additional aspects in which normalized networks differ from traditional viewpoints and proposes a formal framework to study their mathematical properties. The framework involves modeling the training trajectory induced by stochastic gradient descent (SGD) using a stochastic differential equation (SDE) that incorporates gradient noise. The analysis of the SDE leads to the identification of a new parameter called "intrinsic learning rate," which is the product of the normal learning rate and weight decay factor. The SDE analysis also challenges the common belief that large learning rates are necessary for good generalization at the beginning of training. Moreover, through experiments and mathematical reasoning, it is suggested that the number of steps required to reach equilibrium in function space scales inversely with the intrinsic learning rate, contrary to the exponential time convergence bound implied by SDE analysis. This observation is termed the "Fast Equilibrium Conjecture" and is believed to be the key to understanding the effectiveness of Batch Normalization.