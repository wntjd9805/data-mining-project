Operator-valued kernels have been successful in supervised learning problems involving functional inputs and outputs. However, their effectiveness relies on the assumption that these kernels are positive definite, which can be limiting. This study explores operator-valued kernels that may not necessarily be positive definite. To address the issue of indefiniteness, the researchers utilize Reproducing Kernel Krein Spaces (RKKS) of function-valued functions. They present a representer theorem that establishes a suitable loss stabilization problem for supervised learning with function-valued inputs and outputs. The study also analyzes the generalization properties of this framework. To solve the loss stabilization problem, an iterative algorithm called Operator based Minimum Residual (OpMINRES) is proposed. The researchers conduct experiments using indefinite operator-valued kernels on both synthetic and real datasets, and demonstrate the effectiveness of their approach.