Feature attributions are widely used to explain the behavior of Deep Neural Networks (DNNs). However, recent studies have revealed that these attributions can be manipulated to provide different explanations for similar inputs. This lack of reliability poses a significant problem in high-stakes applications where misleading explanations could compromise safety and trust. By building on previous research that examines the geometric aspects of these attacks, we have identified certain conditions, specifically Lipschitz continuity on the gradients of models, that can ensure robust gradient-based attributions. Furthermore, we have observed that the smoothness of a model's decision surface is linked to the transferability of attacks across various attribution methods. To address these vulnerabilities, we propose two mitigation techniques. Firstly, we suggest a cost-effective regularization method that encourages the fulfillment of these conditions in DNNs. Additionally, we introduce a stochastic smoothing technique that does not require re-training. Through experiments on different image models, we consistently demonstrate that both of these mitigations enhance the robustness of attributions. Our findings confirm the significance of smooth geometry in these attacks on real and large-scale models.