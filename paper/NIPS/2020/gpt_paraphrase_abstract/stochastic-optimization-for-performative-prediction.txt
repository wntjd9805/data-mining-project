We investigate the field of stochastic optimization for performative prediction, where the choice of model influences the distribution of future data. Unlike traditional stochastic optimization, updating the model parameters and deploying the new model have different effects on the distribution. We establish convergence rates for both deploying models greedily after each update and taking multiple updates before deployment. Our results show that as performativity decreases, the convergence rates approach the optimal O(1/k) rate. Additionally, we find that the choice between greedy deployment and lazy deployment depends on the strength of performative effects, with each approach outperforming the other in different scenarios. We conduct experiments on synthetic data and a strategic classification simulator to explore this trade-off.