We introduce the SwappingAutoencoder, a deep generative model specifically designed for image manipulation rather than random sampling. The model encodes an image into two independent components and ensures that any swapped combination results in a realistic image. One component encodes co-occurrent patch statistics across different parts of the image, representing structure, while the other represents texture. Training the model with an encoder makes finding the latent codes for a new input image simple. This allows us to manipulate real input images in various ways, such as texture swapping, local and global editing, and latent code vector arithmetic. Our experiments on multiple datasets demonstrate that our model produces superior results and is significantly more efficient than recent generative models. See Figure 1 for an illustration of our Swapping Autoencoder and visit our project webpage for a demo video showcasing our editing method.