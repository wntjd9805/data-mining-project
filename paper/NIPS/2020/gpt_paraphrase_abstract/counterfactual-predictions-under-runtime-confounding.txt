Algorithms are commonly used to predict outcomes based on decisions or interventions. To learn these prediction models, it is important to measure all factors that influence both the decision and outcome. However, in some cases, certain factors cannot be used in the prediction model due to practical or ethical reasons. This scenario is referred to as "runtime confounding." In this study, we propose a doubly-robust procedure for learning counterfactual prediction models in such situations. Our analysis and experiments show that our method often outperforms other approaches. Additionally, we introduce a validation procedure to evaluate the performance of counterfactual prediction methods.