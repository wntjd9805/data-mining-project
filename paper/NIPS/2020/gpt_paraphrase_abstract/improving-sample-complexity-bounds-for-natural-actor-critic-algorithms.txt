The actor-critic (AC) algorithm is commonly used in reinforcement learning to determine the best policy. Previous research has established the convergence rate of AC and natural actor-critic (NAC) algorithms under independent and identically distributed (i.i.d.) sampling and single-sample update. This paper, however, examines the convergence rate and sample complexity of AC and NAC under Markovian sampling, with mini-batch data for each iteration, and with the actor utilizing a general policy class approximation. The findings show that the overall sample complexity of mini-batch AC improves upon the existing sample complexity of AC by O((cid:15)−1 log(1/(cid:15))), while the overall sample complexity of mini-batch NAC improves upon the existing sample complexity of NAC by O((cid:15)−2/ log(1/(cid:15))). Additionally, the sample complexity of AC and NAC surpasses that of policy gradient (PG) and natural policy gradient (NPG) by a factor of O((1 − γ)−3) and O((1 − γ)−4(cid:15)−2/ log(1/(cid:15))), respectively. This is the first theoretical study to demonstrate that AC and NAC outperform PG and NPG under infinite horizon scenarios, thanks to the incorporation of a critic.