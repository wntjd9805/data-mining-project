We propose a method called LICA that addresses the credit assignment problem in fully cooperative settings. Our approach relies on a centralized critic and decentralized agents. We believe that explicit credit assignment is not necessary as long as the policy gradients from the centralized critic provide enough information for agents to optimize their joint action value through optimal cooperation. Additionally, we enforce exploration throughout training by using adaptive entropy regularization, which rescales entropy gradients based on current policy stochasticity to maintain consistent levels of exploration. We achieve the integration of a latent state representation into policy gradients by formulating the centralized critic as a hyper-network. We evaluate our algorithm on various benchmarks, including multi-agent particle environments and StarCraft II micromanagement tasks, and demonstrate that LICA outperforms previous methods.