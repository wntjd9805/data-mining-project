Deep multimodal fusion using multiple data sources has proven to be more advantageous than unimodal fusion in various applications. However, current fusion methods have limitations in balancing inter-modal fusion and intra-modal processing, resulting in performance bottlenecks. To address this issue, this paper introduces Channel-Exchanging-Network (CEN), a parameter-free framework for multimodal fusion. CEN dynamically exchanges channels between sub-networks of different modalities based on the individual channel importance measured by the magnitude of the Batch-Normalization scaling factor during training. The validity of this channel exchanging process is ensured by sharing convolutional filters while maintaining separate Batch-Normalization layers across modalities. Additionally, this approach allows the multimodal architecture to be compact like a unimodal network. Extensive experiments on semantic segmentation and image translation tasks validate the effectiveness of CEN compared to current state-of-the-art methods. Ablation studies further confirm the advantages of each proposed component. The code for CEN is available at https://github.com/yikaiw/CEN.