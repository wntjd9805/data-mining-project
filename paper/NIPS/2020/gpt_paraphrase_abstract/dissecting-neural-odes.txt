Continuous deep learning architectures known as Neural Ordinary Differential Equations (Neural ODEs) have recently gained attention. These architectures offer a unique perspective by theoretically connecting deep learning and dynamical systems. However, understanding the inner workings of Neural ODEs remains a challenge, as they are often used as generic black-box modules. In this study, we aim to shed light on the underlying dynamics of Neural ODEs by delving into their continuous-depth formulation. We explore various design choices and their impact on the model's behavior, ultimately providing a clearer understanding of these architectures.