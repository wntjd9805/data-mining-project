In the past decade, Stochastic Gradient Descent (SGD) has had a significant impact on various aspects of our lives. It has become the preferred optimization tool in machine learning, particularly in the success of deep neural networks (DNNs). While SGD is known to converge to a local optimum, in certain cases, the specific local optimum found may be important, depending on the context. This is especially true in machine learning scenarios involving shape versus texture features, ensemble methods, and zero-shot coordination. In such cases, standard loss functions used with SGD may not yield the desired solutions as it tends to converge to easier solutions. In this study, we propose an alternative approach called Ridge Rider (RR). Instead of following the gradient, which represents a locally greedy direction, we follow the eigenvectors of the Hessian, referred to as "ridges". By iteratively navigating through and branching among these ridges, we effectively explore the loss surface to discover qualitatively different solutions. Theoretical and experimental evidence supports the potential of RR for addressing a variety of challenging problems.