Synchronization plays a crucial role in data-parallel distributed machine learning. Different synchronization systems and strategies have varying performance, and achieving optimal parallel training throughput requires adaptable synchronization strategies that suit model structures and cluster configurations. Current synchronization systems often focus on only one or a few aspects of synchronization, leaving the ML practitioners to decide the appropriate strategy, which may be challenging without the necessary expertise. This study introduces a model- and resource-dependent representation for synchronization, encompassing various synchronization aspects such as architecture, message partitioning, placement scheme, and communication topology. Using this representation, an end-to-end pipeline called AutoSync is developed to automatically optimize synchronization strategies based on model structures and resource specifications, simplifying data-parallel distributed ML. Through learning from a small dataset of 200 trial runs, AutoSync can discover synchronization strategies that are up to 1.6 times better than manually optimized ones. Transfer-learning mechanisms are also implemented to reduce the cost of auto-optimization, allowing simulators to transfer knowledge between similar model architectures, similar cluster configurations, or both. Additionally, a dataset containing approximately 10,000 strategy and run-time pairs for various models and cluster specifications is presented.