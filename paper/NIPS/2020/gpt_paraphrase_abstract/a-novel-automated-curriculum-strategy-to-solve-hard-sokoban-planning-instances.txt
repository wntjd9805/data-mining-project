Recent advancements in deep reinforcement learning (RL) have shown remarkable progress in tasks like Go, Chess, video games, and robot control. However, RL approaches face significant challenges in combinatorial domains such as AI planning. The main difficulty lies in the scarcity of positive reward signals as the length of the solution increases, making it challenging for RL to train effectively. To address this, a curriculum-driven learning approach has been proposed to solve a single difficult instance. In this study, we introduce an innovative automated curriculum approach that dynamically selects training instances of varying complexity using a difficulty quantum momentum strategy. We demonstrate how the smoothness of task hardness affects the learning outcomes. As the instance pool size increases, the "hardness gap" decreases, resulting in a smoother automated curriculum-based learning process. Our automated curriculum approach surpasses previous methods and is evaluated on the Sokoban planning problem, a traditional PSPACE-complete problem that poses a significant challenge. Our RL agent successfully solves difficult instances that were previously unsolvable by state-of-the-art Sokoban solvers. Furthermore, our approach can generate plans requiring hundreds of steps, while previous search methods would take years to solve such instances. Additionally, we show that combining our automated curriculum approach with a curiosity-driven search strategy and a graph neural net representation further enhances RL performance.