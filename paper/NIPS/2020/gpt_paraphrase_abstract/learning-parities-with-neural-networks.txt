In recent years, there has been a significant growth in research on the learnability of various models using common neural network algorithms. However, most of the results only demonstrate the learnability of models that can be learned using linear methods, with only a few exceptions. This is unsatisfactory because neural networks are much more successful than linear methods and linear models fail to capture the depth of deep networks. In this paper, we take a step towards addressing this issue by showing that under certain distributions, sparse parities can be learned efficiently using gradient descent on a depth-two network. In contrast, linear methods are not effective in learning these parities under the same distributions.