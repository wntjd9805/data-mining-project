Training Neural Ordinary Differential Equations (ODEs) can be computationally expensive due to the complexity of solving the ODE during the forward pass. To address this, recent studies have suggested regularizing the dynamics of the ODE. In this research, we introduce a new regularization technique: randomly sampling the end time of the ODE during training. This regularization method is easy to implement, has minimal additional computational cost, and proves to be effective across various tasks. Furthermore, it can be used in combination with other regularization methods for ODE dynamics. Through experiments on normalizing flows, time series models, and image recognition, we demonstrate that this regularization technique significantly reduces training time and can even enhance the performance of baseline models.