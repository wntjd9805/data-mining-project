Invertible neural networks that use coupling flows (CF-INNs) have various applications in machine learning, such as image synthesis and representation learning. However, these networks have limitations in terms of the functional forms they can represent, despite their desirable characteristics like analytic invertibility. This raises the question of whether CF-INNs can approximate any invertible function, making their reliability as a model class uncertain. We address this question by establishing a criterion: a CF-INN is considered universal if its layers can include affine coupling and invertible linear functions as special cases. As a result, we can resolve a previously unsolved problem regarding whether normalizing flow models based on affine coupling can serve as universal distributional approximators. In proving the universality, we also present a general theorem that demonstrates the equivalence of universality within certain diffeomorphism classes, providing valuable theoretical insight.