Reinforcement learning (RL) applied to neural combinatorial optimization (CO) can transform a deep neural network into a fast and powerful heuristic solver for NP-hard problems. This approach has practical applications as it enables the discovery of near-optimal solutions without the need for domain experts with extensive knowledge. We propose an end-to-end approach called Policy Optimization with Multiple Optima (POMO) for building such a solver. POMO can be applied to various CO problems and leverages the symmetries in the solution representation. It utilizes a modified REINFORCE algorithm to ensure diverse rollouts that cover all optimal solutions. POMO's low-variance baseline enables faster and more stable RL training, making it less vulnerable to local minima compared to previous methods. Additionally, we introduce an augmentation-based inference method that complements POMO effectively. Our experiments demonstrate POMO's effectiveness by solving three NP-hard problems: traveling salesman (TSP), capacitated vehicle routing (CVRP), and 0-1 knapsack (KP). In all three cases, our POMO-based solver outperforms recent learned heuristics, achieving an optimality gap of 0.14% for TSP100 while significantly reducing inference time.