Neural Architecture Search (NAS) has shown promise in finding better neural network designs. However, the most reliable approach, sample-based NAS, is computationally expensive. To address this, the one-shot approach has emerged as a popular technique for accelerating NAS using weight-sharing. However, the one-shot approach is less reliable than the sample-based approach due to weight-sharing of different networks. In this study, we propose BONAS, a sample-based NAS framework that uses weight-sharing to evaluate multiple related architectures simultaneously. We use a Graph Convolutional Network predictor as a surrogate model for Bayesian Optimization to select multiple candidate models in each iteration. By training multiple candidate models simultaneously through weight-sharing, our approach significantly accelerates the sample-based approach while maintaining its reliability. Extensive experiments are conducted to validate the effectiveness of our method compared to other algorithms.