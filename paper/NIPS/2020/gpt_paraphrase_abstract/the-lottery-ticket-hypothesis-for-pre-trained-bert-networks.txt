Enormous pre-trained models like BERT have become the standard for training in natural language processing (NLP) and other areas of deep learning. The lottery ticket hypothesis suggests that smaller subnetworks within these models can be trained separately and still achieve high accuracy and transferability to other tasks. This study aims to determine if such trainable subnetworks exist in pre-trained BERT models. The findings reveal matching subnetworks at varying levels of sparsity for different downstream tasks, even at the initialization stage. Subnetworks from the masked language modeling task transfer universally, while those from other tasks have limited transferability. These results highlight the ongoing relevance of the lottery ticket hypothesis in the context of large-scale pre-training. The codes for this study are available at https://github.com/VITA-Group/BERT-Tickets.