We investigate the convergence rates of nonparametric density estimation when the data includes outliers from an unknown distribution. We introduce a family of losses called Besov integral probability metrics (IPMs), which include commonly used metrics such as Lp, Wasserstein, Kolmogorov-Smirnov, and Cramer-von Mises. Assuming smoothness conditions on the population and outlier distributions, we demonstrate that a re-scaled thresholding wavelet estimator achieves the optimal convergence rates under various losses and also adapts to the contamination proportion. We extend the estimator to be data-dependent, allowing it to adapt to both the unknown contamination proportion and the smoothness of the true density. Additionally, we show that certain generative adversarial network (GAN) architectures are robustly minimax optimal, based on recent connections between density estimation under IPM losses and GANs.