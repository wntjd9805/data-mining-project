We address the issue of estimating the number of optimization steps required for a pre-trained deep network to reach a specific loss value. We exploit the similarity between the training dynamics of a deep network during fine-tuning and a linearized model, allowing us to approximate the training loss and accuracy using a low-dimensional Stochastic Differential Equation (SDE) in function space. This approximation enables us to predict the time required for Stochastic Gradient Descent (SGD) to fine-tune a model without actual training. Our experiments demonstrate that we can accurately predict the training time of a ResNet with a 20% error margin on various datasets and hyper-parameters, while reducing costs by 30 to 45 times compared to actual training. We also discuss methods to further decrease the computational and memory cost of our approach, including exploiting the spectral properties of the gradients' matrix to predict training time on large datasets using only a subset of samples.