We introduce PLLay, a new layer for deep learning models that utilizes persistence landscapes to leverage the underlying topological characteristics of input data. Our layer is differentiable and can be placed anywhere in the network, providing crucial topological information to improve learning. The structure of PLLay is learned during training, eliminating the need for input featurization or data preprocessing. We adapt the DTM function-based filtration and demonstrate the layer's robustness to noise and outliers through stability analysis. Classification experiments on multiple datasets validate the effectiveness of our approach.