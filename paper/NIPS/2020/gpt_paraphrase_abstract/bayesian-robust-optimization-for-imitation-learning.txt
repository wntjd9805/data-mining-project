One of the main challenges in imitation learning is determining what action an agent should take when it encounters states that were not part of the demonstrations. Inverse reinforcement learning (IRL) can help with this by learning a reward function that can be used to generalize to new states. However, there is still uncertainty about the true reward function and the optimal policy that corresponds to it. Existing safe imitation learning approaches based on IRL address this uncertainty by assuming an adversarial reward function and optimizing a policy using a maxmin framework. On the other hand, risk-neutral IRL approaches optimize a policy based on the mean or most likely reward function. However, completely ignoring risk can lead to aggressive and unsafe policies, while optimizing in an adversarial sense can result in overly cautious policies that perform poorly in practice. To find a balance between these extremes, we propose BayesianRobust Optimization for Imitation Learning (BROIL). BROIL combines Bayesian reward function inference with a user-defined risk tolerance to efficiently optimize a robust policy that considers both expected return and conditional value at risk. Our experiments demonstrate that BROIL offers a way to interpolate between return-maximizing and risk-minimizing behaviors and outperforms existing risk-sensitive and risk-neutral IRL algorithms.