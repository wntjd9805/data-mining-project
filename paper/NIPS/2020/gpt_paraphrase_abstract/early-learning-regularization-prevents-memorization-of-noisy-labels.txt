We present a new framework for utilizing deep learning in classification tasks with noisy annotations. Deep neural networks often initially fit clean labels before memorizing examples with false labels, a phenomenon known as early learning and memorization. We demonstrate that this phenomenon exists even in simple linear models and provide a theoretical explanation. Motivated by these findings, we propose a novel technique that takes advantage of the progress made during the early learning phase. Unlike existing approaches, our method uses regularization to capitalize on early learning rather than attempting to detect or correct false labels. We incorporate semi-supervised learning techniques to generate target probabilities based on the model outputs and design a regularization term that guides the model towards these targets, preventing memorization of false labels. Our framework demonstrates robustness to noisy annotations on various benchmark and real-world datasets, achieving comparable results to the state of the art.