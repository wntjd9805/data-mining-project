The need to control the behavior of learning solutions has become crucial as they are increasingly used in social, industrial, and medical fields. It is evident that learning without specific guidelines can result in biased, unsafe, and prejudiced outcomes. To address these issues, we propose a generalization theory of constrained learning based on the probably approximately correct (PAC) learning framework. We demonstrate that imposing constraints does not make learning more difficult, as any PAC learnable class can also be PAC constrained learnable by utilizing a constrained version of the empirical risk minimization (ERM) rule. However, for typical parametrized models, solving the constrained non-convex optimization problem involved in this learner can be challenging, making it difficult to obtain a feasible solution. To overcome this challenge, we prove that under mild conditions, the empirical dual problem of constrained learning can also serve as a PAC constrained learner, enabling us to develop a practical constrained learning algorithm that only requires solving unconstrained problems. We analyze the generalization properties of this solution and demonstrate how constrained learning can effectively address issues related to fair and robust classification.