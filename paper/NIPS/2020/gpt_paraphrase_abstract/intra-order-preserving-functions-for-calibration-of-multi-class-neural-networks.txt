Calibrating confidence scores for multi-class deep networks is crucial to prevent costly errors. Existing post-hoc calibration techniques use simple functions to transform network outputs, but they may not effectively handle the complexity of deep networks. In this study, we aim to learn general post-hoc calibration functions that preserve the top-k predictions of any deep network. We introduce a new neural network architecture that combines common components to represent a class of intra order-preserving functions. We also propose order-invariant and diagonal sub-families as regularization techniques for improved generalization with small training data. Our method surpasses state-of-the-art calibration methods in various evaluation metrics across different datasets and classifiers.