The support/query (S/Q) episodic training strategy is commonly used in modern meta-learning algorithms to improve their ability to generalize to test environments. This paper investigates the effects of this strategy on generalization through a theoretical analysis. The analysis shows that the S/Q episodic training strategy results in a counterintuitive generalization bound of O(1/n), where n is the number of tasks, regardless of the sample size within each task. This bound implies strong generalization guarantees for modern meta-learning algorithms in the few-shot learning regime, where the sample size is typically smaller than the number of tasks. To further explore the impact of training strategies on generalization, the paper proposes a leave-one-out (LOO) training strategy for meta-learning and compares it with S/Q training. Experimental results using popular meta-learning algorithms on standard few-shot regression and classification tasks support the theoretical analysis.