We present a new method for addressing temporal inconsistency in videos caused by applying image processing algorithms independently to each frame. Unlike previous methods that rely on optical flow, our approach trains a convolutional network on a pair of original and processed videos using the Deep Video Prior. We also propose an iteratively reweighted training strategy to tackle the multimodal inconsistency problem. Our method demonstrates superior performance compared to state-of-the-art techniques in blind video temporal consistency across 7 computer vision tasks. The source codes for our approach are publicly accessible at github.com/ChenyangLEI/deep-video-prior.