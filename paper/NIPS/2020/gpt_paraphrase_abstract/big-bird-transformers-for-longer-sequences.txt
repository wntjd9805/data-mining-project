Transformers, like BERT, have been highly successful in NLP, but they suffer from a quadratic dependency on sequence length due to their full attention mechanism. To address this, we introduce BIGBIRD, a sparse attention mechanism that reduces the quadratic dependency to linear. We demonstrate that BIGBIRD is capable of approximating sequence functions universally and is Turing complete, preserving the properties of the quadratic attention model. Our analysis highlights the advantages of having O(1) global tokens, which attend to the entire sequence as part of the sparse attention mechanism. With the ability to handle sequences up to 8x longer than previous models, BIGBIRD significantly enhances performance in NLP tasks like question answering and summarization. We also propose innovative applications of BIGBIRD to genomics data.