This paper introduces a novel approach to continual online model-based reinforcement learning, which can solve task-agnostic problems with unknown task boundaries without the need for pre-training. The approach utilizes a mixture of experts to handle nonstationarity and employs Gaussian Processes to effectively leverage collected data and model uncertainty. A transition prior is proposed to account for temporal dependencies in streaming data, and the mixture is updated online using sequential variational inference. The approach successfully handles task distribution shifts by generating new models for unseen dynamics and reusing old models for previously encountered dynamics. Experimental results demonstrate that the approach outperforms alternative methods in tasks with changing dynamics and decision making in driving scenarios. The code for the approach is available at: https://github.com/mxu34/mbrl-gpmm.