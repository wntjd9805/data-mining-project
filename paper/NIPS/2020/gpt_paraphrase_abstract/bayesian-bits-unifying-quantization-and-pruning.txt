We present Bayesian Bits, a practical approach for combining mixed precision quantization and pruning using gradient-based optimization. Bayesian Bits utilizes a unique method of decomposing the quantization process, where the bit width is sequentially doubled. At each new bit width, we quantize the residual error between the full precision value and the previously rounded value. We then decide whether to include this quantized residual error to achieve higher effective bit width and lower quantization noise. This decomposition always generates hardware-friendly configurations by starting with a power-of-two bit width. Additionally, our technique offers a unified perspective on pruning and quantization through an optional 0-bit choice. Bayesian Bits introduces learnable stochastic gates that collectively control the bit width of a tensor, allowing us to obtain low bit solutions by approximating inference over these gates. Experimental results on various benchmark datasets validate our method, demonstrating that pruned, mixed precision networks learned using Bayesian Bits achieve a better balance between accuracy and efficiency compared to their static bit width counterparts.