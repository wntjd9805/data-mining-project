This study introduces central limit theorems for cross-validation and consistent estimators of its asymptotic variance. These theorems are applicable under weak stability conditions on the learning algorithm. The main significance of these findings is the provision of practical and asymptotically accurate confidence intervals for k-fold test error. Additionally, they enable valid and powerful hypothesis tests to determine whether one learning algorithm has a lower k-fold test error compared to another. Notably, these results are the first of their kind for leave-one-out cross-validation, which is widely used. Real-data experiments involving various learning algorithms demonstrate that the resulting intervals and tests surpass the most popular alternative methods found in existing literature.