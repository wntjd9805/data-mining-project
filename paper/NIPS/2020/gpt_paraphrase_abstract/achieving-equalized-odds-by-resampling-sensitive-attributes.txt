We present a flexible framework that enables the learning of predictive models that approximate fairness according to the equalized odds concept. This is accomplished by introducing a general discrepancy measure that accurately quantifies violations of this criterion. We use this differentiable measure as a penalty to guide the model parameters towards achieving equalized odds. To evaluate the fitted models, we develop a formal hypothesis test to identify whether a prediction rule violates this property, which is the first test of its kind in the existing literature. Both the model fitting and hypothesis testing rely on a resampled version of the sensitive attribute that adheres to equalized odds by design. We demonstrate the effectiveness and validity of our framework in regression and multi-class classification problems, outperforming existing methods. Additionally, we propose techniques to incorporate equitable uncertainty quantification, ensuring unbiased results for each group studied.