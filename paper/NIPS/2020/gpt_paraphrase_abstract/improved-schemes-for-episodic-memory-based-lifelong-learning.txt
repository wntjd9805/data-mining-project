Deep neural networks excel at single tasks but tend to forget previously learned knowledge when trained on a sequence of tasks, known as catastrophic forgetting. Episodic memory-based approaches like GEM and A-GEM have shown promising results. This paper presents a unified view of episodic memory-based approaches from an optimization perspective and introduces two improved schemes called MEGA-I and MEGA-II. These schemes balance the importance of old and new tasks by integrating current and episodic memory gradients. GEM and A-GEM are shown to be degenerate cases of MEGA-I and MEGA-II, which always prioritize the current task. The proposed schemes address this issue with novel loss-balancing updating rules, significantly outperforming GEM and A-GEM. Extensive experiments on lifelong learning benchmarks demonstrate up to an 18% reduction in error. The implementation can be found at: https://github.com/yunhuiguo/MEGA.