We challenge the commonly held belief that the mean-field approximation for variational inference in Bayesian neural networks is highly restrictive. In the context of deep networks, we demonstrate that this assumption is incorrect. Through various proofs, we establish that deep mean-field variational weight posteriors can generate similar distributions in function-space as shallower networks with complex weight posteriors. We validate our theoretical findings through empirical analysis, including the examination of weight posteriors using Hamiltonian Monte Carlo in small models and a comparison between diagonal- and structured-covariance in large settings. Our results indicate that using mean-field variational inference in a deeper model is an attractive and justified alternative to implementing costly and cumbersome complex variational posteriors.