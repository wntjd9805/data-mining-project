Backpropagation (BP) is a successful algorithm for training artificial neural networks, but it has several differences from learning in the brain (BL). It is unclear if BP can be implemented exactly in BL, as BP requires non-local information for weight updates while BL only uses local information. BP also lacks autonomy, relying on external control of the neural network. Bridging these gaps between BP and BL is of interest in neuroscience and machine learning. Previous models have only approximated BP, without demonstrating equivalence. In this study, we propose a BL framework that achieves the same weight updates as BP while using local computations and local plasticity. We also present an alternative BL model that works autonomously. Our findings provide evidence for the question of whether the brain can perform BP.