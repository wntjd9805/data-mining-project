There has been a recent increase in interest regarding the implicit regularization properties of gradient-based optimization algorithms. This paper investigates the statistical guarantees of early-stopped unconstrained mirror descent algorithms when applied to the unregularized empirical risk with squared loss for linear models and kernel methods. By establishing a connection between offset Rademacher complexities and potential-based convergence analysis of mirror descent methods, we are able to provide excess risk guarantees for the path followed by the iterations of mirror descent. These guarantees depend on the choice of mirror map, initialization point, step-size, and number of iterations. We apply our findings to recover and improve upon recent results in the implicit regularization literature.