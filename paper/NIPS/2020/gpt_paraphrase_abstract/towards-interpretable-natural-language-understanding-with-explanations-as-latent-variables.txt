Recently, the generation of natural language explanations has shown promising results in providing interpretable explanations and enhancing prediction. However, existing approaches require a large set of human annotated explanations for training, which is time-consuming and expensive. This paper presents a general framework for interpretable natural language understanding that only requires a small set of human annotated explanations for training. The framework considers natural language explanations as latent variables that model the reasoning process of a neural model. A variational EM framework is developed for optimization, where an explanation generation module and an explanation-augmented prediction module are alternately optimized and mutually enhance each other. Additionally, an explanation-based self-training method is proposed for semi-supervised learning within this framework. This method iteratively improves predictions by assigning pseudo-labels to unlabeled data and generating new explanations. Experimental results on two natural language understanding tasks demonstrate that the framework is effective in supervised and semi-supervised settings, generating high-quality natural language explanations.