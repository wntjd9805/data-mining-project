Neural networks often become biased by relying too much on spurious correlations in the dataset. Previous approaches address this issue by explicitly labeling the correlated attributes or assuming a specific bias type. However, we propose a more cost-effective and versatile solution using generic human knowledge. Our observation is that neural networks rely on spurious correlations when they are easier to learn than the desired knowledge, especially during early training stages. To mitigate this bias, we introduce a failure-based debiasing scheme that involves training two neural networks simultaneously. The first network is intentionally biased by amplifying its prejudice, while the second network is debiased by focusing on samples that contradict the biased network's prejudice. Extensive experiments demonstrate the effectiveness of our method in reducing various types of biases in both synthetic and real-world datasets. Surprisingly, our framework sometimes even outperforms debiasing methods that require explicit supervision of the correlated attributes.