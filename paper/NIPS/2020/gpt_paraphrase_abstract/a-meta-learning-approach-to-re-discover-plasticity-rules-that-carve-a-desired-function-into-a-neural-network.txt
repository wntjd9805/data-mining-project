Current models of synaptic plasticity, although based on experimental data, often fail to produce neural dynamics that serve complex functions. This suggests that these models are not adequately constrained by existing data. In this study, we propose an alternative approach using meta-learning to discover plausible synaptic plasticity rules. Instead of relying on experimental data, our approach constrains the rules based on the functions they implement and the desired structure. We parameterize the rules using a Volterra expansion and employ supervised learning methods to optimize a problem-specific loss function. We validate our approach by rediscovering known plasticity rules and expanding the problem to network-level scenarios. Our algorithm successfully discovers rules that achieve desired functions, such as recovering principal components of input space in a two-layer firing-rate network. Additionally, we apply our approach to networks of integrate-and-fire neurons with plastic inhibitory afferents, successfully training rules that achieve target firing rates. This work demonstrates an automated and unbiased approach to uncover synaptic plasticity rules that adhere to biological constraints and can solve complex functions.