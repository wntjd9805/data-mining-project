With the increasing impact of deep learning models on human lives, addressing bias has become a significant concern. Existing debiasing algorithms can be categorized into pre-processing, in-processing, and post-processing methods. However, in computer vision and natural language applications, a common approach is to start with a generic model and then fine-tune it for specific use cases. Pre- and in-processing methods require retraining the entire model, while post-processing methods lack access to the model's weights. Consequently, debiasing algorithms tailored for this fine-tuning scenario have been overlooked. This study introduces a new paradigm called intra-processing, which bridges the gap between in-processing and post-processing methods. Intra-processing methods are specifically designed to address bias in large models trained on generic datasets and fine-tuned for specific tasks. The study repurposes existing in-processing methods for this purpose and proposes three baseline algorithms: random perturbation, layerwise optimization, and adversarial debiasing. The effectiveness of these methods is evaluated on three popular datasets from the AIF360 toolkit and the CelebA faces dataset.