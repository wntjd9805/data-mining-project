Learning from Observations (LfO) is a practical scenario in reinforcement learning that can benefit various applications by reusing incomplete resources. LfO is more challenging than conventional imitation learning (IL) due to the absence of expert action guidance. Both IL and LfO rely on distribution matching, but traditional approaches are costly and rely on on-policy transitions for policy learning. Some off-policy solutions have been proposed for sample efficiency, but they lack theoretical justifications or depend on expert actions. This study introduces a sample-efficient LfO approach that enables off-policy optimization in a principled manner. To speed up learning, the policy update is regulated using an inverse action model, which helps with distribution matching from a mode-covering perspective. Extensive experiments on locomotion tasks demonstrate that our approach achieves comparable sample efficiency and asymptotic performance to state-of-the-art methods.