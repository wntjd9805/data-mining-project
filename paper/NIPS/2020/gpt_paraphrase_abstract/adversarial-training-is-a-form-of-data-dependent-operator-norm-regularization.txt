We establish a theoretical connection between adversarial training and operator norm regularization in deep neural networks. Our proof shows that by using a specific method of adversarial training with certain constraints and loss functions, we can achieve the same results as data-dependent operator norm regularization. This discovery supports the argument that a network's vulnerability to adversarial examples is related to its spectral properties and suggests new methods for defending against such attacks. We provide empirical evidence using advanced network architectures to validate our theoretical findings.