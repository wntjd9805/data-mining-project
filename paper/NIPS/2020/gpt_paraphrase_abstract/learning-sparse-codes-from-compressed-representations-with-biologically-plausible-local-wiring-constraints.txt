Sparse coding is a valuable technique for learning independent features in neural coding models. Existing algorithms for learning these representations do not consider the information bottlenecks in fiber pathways connecting different areas of the brain. Recent studies have shown that dimensionality reduction using randomized linear operators can effectively enable learning of sparse representations. However, current methods for sparse coding in compressed spaces require a centralized compression process that is biologically unrealistic due to local wiring constraints in neural circuits. This paper proposes a theoretical neuroscience model that uses structured random matrices for communication between cortical areas, taking into account the observed local wiring constraints in neuroanatomy. The authors demonstrate through analysis and experiments that unsupervised learning of sparse representations can still be achieved in compressed spaces, even with significant local wiring constraints. The learned representations maintain their quality, exhibit similar performance in training and generalization, and align with measurements of macaque V1 receptive fields.