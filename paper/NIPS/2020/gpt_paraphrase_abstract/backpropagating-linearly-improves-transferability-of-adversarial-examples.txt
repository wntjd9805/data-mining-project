This paper investigates the vulnerability of deep neural networks (DNNs) to adversarial examples and focuses on the transferability of these examples. The transferability of adversarial examples is important as it forms the basis for black-box attacks on DNNs. The authors revisit a hypothesis proposed by Goodfellow et al. and reveal that the transferability can be improved by enhancing the linearity of DNNs. They introduce a method called linear backpropagation (LinBP) that performs backpropagation in a more linear manner using existing attacks that exploit gradients. LinBP calculates forward as usual but backpropagates loss as if some nonlinear activations were not encountered during the forward pass. Experimental results demonstrate that this simple yet effective method outperforms current state-of-the-art techniques in crafting transferable adversarial examples on CIFAR-10 and ImageNet datasets. This leads to more successful attacks on various DNNs. The code for LinBP is available at the provided GitHub link.