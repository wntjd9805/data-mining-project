This article demonstrates that deterministic Gradient Descent (GD), which does not utilize stochastic gradient approximation, can still exhibit stochastic behaviors. Specifically, it shows that when the objective function displays multiscale behaviors, in a large learning rate regime that only captures the macroscopic details and not the microscopic ones, the deterministic GD dynamics can become chaotic and converge not to a local minimizer but to a statistical distribution. Thus, even though no stochasticity is introduced, deterministic GD resembles stochastic GD. The article also establishes a sufficient condition for approximating this long-term statistical limit using a rescaled Gibbs distribution, which enables quantification of escapes from local minima. The findings are supported by both theoretical analysis and numerical simulations, with the theoretical aspect relying on a stochastic map that employs bounded noise instead of Gaussian noise.