We propose Individually Inferred Communication (I2C) as a model for agents to learn how to communicate with each other. This model utilizes causal inference and a neural network to determine who to communicate with based on an agent's observations. We quantify the necessity of communication using the joint action-value function in multi-agent reinforcement learning. Additionally, we regulate the agent's policy to better utilize communicated messages. Our empirical results demonstrate that I2C reduces communication overhead and enhances performance in various multi-agent cooperative scenarios compared to existing approaches.