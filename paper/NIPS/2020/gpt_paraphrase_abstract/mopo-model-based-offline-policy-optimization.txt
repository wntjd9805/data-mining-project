Offline reinforcement learning (RL) is the task of learning policies solely from a large batch of previously collected data. This approach has the advantage of not requiring costly or dangerous active exploration. However, it is a difficult problem due to the distributional shift between the training data and the states visited by the learned policy. While recent progress has been made, the most successful methods are model-free and limited to the data's support, making generalization to unseen states challenging. This paper explores the use of an existing model-based RL algorithm in the offline setting and proposes modifications to address the distributional shift issue. By penalizing rewards based on the uncertainty of the dynamics, the algorithm maximizes a lower bound of the policy's return under the true MDP. The trade-off between gain and risk is also characterized. The proposed algorithm, Model-based Offline Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free offline RL algorithms on existing benchmarks and challenging continuous control tasks that require generalization from different task data.