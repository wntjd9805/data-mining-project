We introduce a new model of supervised learning that takes into account privacy constraints. In this model, a dataset is collected from both healthy and unhealthy individuals, with the unhealthy individuals wanting strict privacy protection for their data. The population consists of a mixture of private (unhealthy) and public (healthy) sub-populations, which can be significantly different. Our goal is to develop a learning algorithm that ensures differential privacy specifically for the private examples. Previous studies in this area assumed a homogeneous population where private and public data came from the same distribution. We propose a solution that overcomes this assumption by focusing on learning linear classifiers in Rd. We demonstrate that when the privacy status is correlated with the target label, it is possible to learn linear classifiers in Rd with a sample complexity comparable to non-private learning. This is not feasible if all the data is considered private.