The problem of optimizing neural networks is approached as Bayesian filtering, with backpropagated gradients serving as observations. Previous studies on neural network optimization using natural gradient methods, which are closely related to Bayesian inference, were unable to replicate standard optimizers like Adam and RMSprop with a root-mean-square gradient normalizer. Instead, they obtained a mean-square normalizer. To recover the root-mean-square normalizer, it is necessary to consider the temporal dynamics of all other parameters during optimization. The resulting optimizer, named AdaBayes, dynamically transitions between SGD-like and Adam-like behaviors. It effectively recovers AdamW, a state-of-the-art variant of Adam with decoupled weight decay, and demonstrates competitive generalization performance compared to SGD.