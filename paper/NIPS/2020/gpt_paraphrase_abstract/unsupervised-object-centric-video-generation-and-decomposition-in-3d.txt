A new approach to generating videos is proposed, where videos are represented as a combination of moving objects in a 3D scene. Previous methods focused on modeling 2D sprites over a changing background, but did not consider the underlying 3D scene. This new model is trained on monocular videos without supervision and is capable of generating coherent 3D scenes with multiple moving objects. The model is evaluated on two datasets and outperforms state-of-the-art approaches in tasks such as depth-prediction, 3D object detection, 2D instance segmentation, and tracking.