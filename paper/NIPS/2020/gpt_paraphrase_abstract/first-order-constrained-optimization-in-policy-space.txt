Reinforcement learning involves an agent learning effective behaviors by interacting with the environment and receiving rewards. However, certain behaviors that should be avoided, like unsafe actions, are better captured through constraints. To address this, we present a new method called FOCOPS, which maximizes an agent's reward while ensuring it satisfies a set of cost constraints. FOCOPS uses data from the current policy to find the optimal update policy through a constrained optimization problem in the nonparameterized policy space. It then projects this update policy back into the parametric policy space. Our approach maintains an approximate upper bound on worst-case constraint violation during training and is simple to implement. Empirical evidence shows that our straightforward approach achieves better performance on constrained robotics locomotive tasks.