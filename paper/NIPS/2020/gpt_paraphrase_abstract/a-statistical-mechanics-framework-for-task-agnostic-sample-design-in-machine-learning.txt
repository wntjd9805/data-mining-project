This paper presents a statistical mechanics framework that examines the impact of sampling properties of training data on the generalization gap of machine learning algorithms. The study connects the generalization gap to the spatial properties of a sample design, specifically the pair correlation function (PCF). By expressing the generalization gap in terms of the power spectra of the sample design and the function to be learned, the authors demonstrate that space-filling sample designs, such as blue noise and Poisson disk sampling, which optimize spectral properties, outperform random designs in terms of the generalization gap. The analysis also provides insights into design principles for constructing task-agnostic sample designs that minimize the generalization gap. The findings are supported by regression experiments with neural networks on synthetic functions and a complex scientific simulator for inertial confinement fusion (ICF).