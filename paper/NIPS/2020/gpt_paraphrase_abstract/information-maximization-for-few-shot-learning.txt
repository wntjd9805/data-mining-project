We present Transductive Information Maximization (TIM) as a new approach for few-shot learning. Our method focuses on maximizing the mutual information between query features and their label predictions in a few-shot task. We also incorporate a supervision loss based on the support set. To optimize the mutual-information loss, we propose an alternating-direction solver, which improves the convergence of transductive inference compared to gradient-based optimization while maintaining similar accuracy. TIM is modular and can be used with any base-training feature extractor. Our comprehensive experiments demonstrate that TIM outperforms state-of-the-art methods significantly across various datasets and networks. It achieves these results by using a fixed feature extractor trained with simple cross-entropy on the base classes, without relying on complex meta-learning approaches. TIM consistently improves accuracy by 2% to 5% compared to the best performing method, not only on established few-shot benchmarks but also in more challenging scenarios with domain shifts and larger numbers of classes.