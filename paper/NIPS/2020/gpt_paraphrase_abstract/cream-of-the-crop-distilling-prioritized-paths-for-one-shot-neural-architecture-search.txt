One-shot weight sharing methods have gained attention in neural architecture search due to their efficiency and competitive performance. However, weight sharing between models leads to insufficient training of subnetworks in hypernetworks. To address this issue, we propose a simple yet effective architecture distillation method. Our approach allows subnetworks to collaboratively learn and teach each other, enhancing the convergence of individual models. We introduce the concept of prioritized paths, which are architecture candidates with superior performance during training. By distilling knowledge from these prioritized paths, we improve the training of subnetworks. The prioritized paths are dynamically selected based on their performance and complexity, resulting in the best possible paths. We select the most promising path from the prioritized paths as the final architecture, without relying on complex search methods like reinforcement learning or evolution algorithms. Experimental results on ImageNet demonstrate that our path distillation method improves convergence ratio and performance of the hypernetwork, as well as enhancing the training of subnetworks. The discovered architectures outperform recent MobileNetV3 and EfficientNet families under aligned settings. Furthermore, experiments on object detection and more challenging search spaces validate the generality and robustness of our proposed method. Code and models are available at https://github.com/microsoft/cream.git2.