We introduce Look-aheadMAML (La-MAML), an optimization-based meta-learning algorithm for online-continual learning. La-MAML addresses the challenge of training models with limited capacity to perform well on sequentially arriving tasks. Unlike current training procedures, La-MAML is fast, efficient, and less dependent on hyper-parameters. It incorporates per-parameter learning rate modulation, drawing from previous work on hypergradients and meta-descent. This approach provides a more flexible and effective method for mitigating catastrophic forgetting compared to prior-based methods. La-MAML outperforms replay-based, prior-based, and meta-learning approaches in real-world visual classification benchmarks for continual learning.