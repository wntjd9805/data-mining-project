Current approaches in training neural networks for lossy image compression do not effectively maintain both perceptual quality and abstract features at very low bitrates. Inspired by recent success in learning discrete representations using Vector Quantized Variational Autoencoders (VQ-VAEs), we propose the use of a hierarchy of VQ-VAEs to achieve high compression ratios. By incorporating stochastic quantization and hierarchical latent structure, we demonstrate that likelihood-based image compression can be improved. To accomplish this, we introduce a novel objective for training hierarchical VQ-VAEs. The resulting approach generates a sequence of latent variables that reconstruct images with high-perceptual quality while preserving semantically meaningful features. We evaluate our method on the CelebA and MNIST datasets using qualitative and quantitative measures.