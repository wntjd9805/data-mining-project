We examine ReLU networks with random weights, where the dimension decreases at each layer. We demonstrate that for the majority of these networks, most input examples x can be perturbed adversarially at a distance of O(sqrt(d)), where d represents the input dimension. Furthermore, we show that this perturbation can be discovered using gradient flow or gradient descent with small enough steps. This finding provides an explanation for the prevalence of adversarial examples and why they are commonly discovered through gradient descent.