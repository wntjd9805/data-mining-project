Implicit algorithms have proven to be successful in online learning, but their regret analyses only show slight improvements over Online Mirror Descent. This study aims to understand this behavior through a thorough regret analysis. A new static regret bound is derived, which depends on the temporal variability of the loss function sequence, a factor commonly encountered in dynamic competitors. The study demonstrates that if the temporal variability is constant and the learning rate is appropriately tuned, the regret can remain constant without the requirement of smooth losses. Additionally, an adaptive algorithm is introduced that achieves this regret bound without prior knowledge of the temporal variability, and a corresponding lower bound is proven. The theoretical findings are then validated using classification and regression datasets.