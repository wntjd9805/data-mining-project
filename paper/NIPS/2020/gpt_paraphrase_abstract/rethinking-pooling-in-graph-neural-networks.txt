Graph pooling is a crucial part of many graph neural network (GNN) architectures. Traditionally, graph pooling has been approached as a cluster assignment problem, similar to local patches in regular grids. However, the impact of this design choice on the effectiveness of GNNs has not been thoroughly evaluated. In this study, we examine different variations of GNNs that challenge the need for locality-preserving representations, using randomization or clustering on the complement graph. Surprisingly, our experiments show that these variations do not result in any performance decrease. To understand this, we analyze the relationship between convolutional layers and pooling layers. We discover that convolutions play a more significant role in the learned representations than commonly believed. Local pooling is not the sole factor responsible for the success of GNNs on widely-used benchmarks.