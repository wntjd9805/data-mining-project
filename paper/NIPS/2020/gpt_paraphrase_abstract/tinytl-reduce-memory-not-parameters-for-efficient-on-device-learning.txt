This study introduces Tiny-Transfer-Learning (TinyTL), a method for memory-efficient on-device learning. It addresses the challenge of adapting AI models to new data with limited memory on edge devices. Previous approaches focused on reducing trainable parameters, but the bottleneck is actually the activations. TinyTL addresses this by freezing weights and only learning bias modules, eliminating the need to store intermediate activations. To maintain adaptation capacity, a new memory-efficient bias module called the lite residual module is introduced, which refines the feature extractor by learning small residual feature maps with minimal memory overhead. Experimental results demonstrate that TinyTL significantly reduces memory usage (up to 6.5 times) with minimal loss in accuracy compared to fine-tuning the full network. Compared to fine-tuning the last layer, TinyTL achieves significant accuracy improvements (up to 34.1%) with minimal memory overhead. Additionally, when combined with feature extractor adaptation, TinyTL achieves 7.3-12.9 times memory savings without sacrificing accuracy compared to fine-tuning the full Inception-V3 model.