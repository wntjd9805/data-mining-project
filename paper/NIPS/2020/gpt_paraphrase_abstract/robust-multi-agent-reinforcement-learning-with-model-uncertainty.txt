This study focuses on the problem of multi-agent reinforcement learning (MARL) with model uncertainty, which is referred to as robust MARL. Many multi-agent applications involve agents with imperfect knowledge of the model, such as reward functions of other agents. However, prior work on MARL has not addressed this uncertainty. In contrast, we approach the problem as a robust Markov game, aiming to find policies that are robust to model uncertainty and prevent agents from deviating from equilibrium. We introduce the concept of robust Nash equilibrium and develop a Q-learning algorithm to find such equilibrium policies. To handle large state-action spaces, we derive policy gradients for robust MARL and develop an actor-critic algorithm with function approximation. Our experiments demonstrate that our algorithm outperforms baseline MARL methods in uncertain cooperative and competitive environments.