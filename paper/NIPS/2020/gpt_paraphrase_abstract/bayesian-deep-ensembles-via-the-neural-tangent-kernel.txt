We investigate the relationship between deep ensembles and Gaussian processes (GPs) using the Neural Tangent Kernel (NTK). The NTK helps us understand the training dynamics of wide neural networks (NNs). Previous research has shown that when NNs become GPs in the infinite width limit, a deep ensemble trained with squared error loss does not have a GP posterior interpretation. To address this, we propose a modification to deep ensemble training by adding a computationally-tractable, randomised, and untrainable function to each member of the ensemble. This modification allows for a posterior interpretation in the infinite width limit. When combined, our trained NNs provide an approximation of a posterior predictive distribution. We prove that our Bayesian deep ensembles make more conservative predictions compared to standard deep ensembles in the infinite width limit. Additionally, we demonstrate that our Bayesian deep ensembles accurately replicate the analytic posterior predictive when available using finite width NNs. Furthermore, our Bayesian deep ensembles outperform standard deep ensembles in various out-of-distribution scenarios for regression and classification tasks.