Solving optimization problems in real-life analytics applications is crucial for decision making. However, these problems often involve uncertain coefficients that depend on external factors. Machine learning models, particularly neural networks, are increasingly used to estimate these coefficients in a data-driven manner. As a result, there is growing interest in end-to-end predict-and-optimize approaches that consider the effectiveness of predicted values in solving optimization problems. For integer linear programming problems, a common approach is to add a quadratic penalty term to the continuous relaxation to address their non-differentiability. In this study, we explore the use of a more principled logarithmic barrier term, commonly employed in interior point solvers for linear programming. Instead of differentiating the KKT conditions, we focus on the homogeneous self-dual formulation of the LP and establish the relationship between the interior point step direction and the corresponding gradients required for learning. Our empirical experiments show that our approach performs as well as, if not better than, the state-of-the-art QPTL formulation by Wilder et al. and the SPO approach by Elmachtoub and Grigas.