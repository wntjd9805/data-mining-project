Stochastic Approximation (SA) is a widely used technique for solving fixed-point equations in the presence of noise. This study focuses on an SA method that involves a contraction mapping with respect to any norm and examines its finite-sample error bounds under different step sizes. The key idea is to create a smooth Lyapunov function using the generalized Moreau envelope and demonstrate that the SA iterates have a negative drift based on this function. The findings have practical implications in Reinforcement Learning (RL), particularly in establishing the first-known convergence rate of the V-trace algorithm for off-policy TD-learning. Notably, our approach ensures that the convergence bound is logarithmically dependent on the state-space size.