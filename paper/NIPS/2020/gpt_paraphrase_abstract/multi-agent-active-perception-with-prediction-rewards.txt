This study focuses on multi-agent active perception, which involves a group of agents working together to gather observations and calculate a joint estimate of a hidden variable. The task is decentralized, meaning the joint estimate can only be computed after the task is completed by combining the observations of all agents. The main objective is to maximize the accuracy of this joint estimate. The accuracy is measured using a centralized prediction reward, determined by a central decision-maker who evaluates the observations collected by all agents after the task.   In this paper, the authors propose a model that represents multi-agent active perception as a decentralized partially observable Markov decision process (Dec-POMDP) with a convex centralized prediction reward. By introducing individual prediction actions for each agent, the problem is transformed into a standard Dec-POMDP with a decentralized prediction reward. The authors prove that the loss caused by decentralization is limited, and they provide a condition under which this loss is zero.   These findings allow for the application of any Dec-POMDP solution algorithm to multi-agent active perception problems, enabling planning to reduce uncertainty without explicitly computing joint estimates. The authors demonstrate the practical value of their results by applying a standard Dec-POMDP algorithm to multi-agent active perception problems, which leads to improved scalability in the planning horizon.