We examine regret-minimizing strategies in the Bayesian multi-armed bandit problem with k arms and T time periods. We find that subsampling the time horizon is crucial for designing optimal policies. The standard UCB algorithm is suboptimal in the many-armed scenario, but a subsampled version of UCB (SS-UCB) is rate-optimal when applied only to a subset of arms. However, even SS-UCB performs poorly due to excessive exploration of suboptimal arms. Empirical experiments demonstrate that a simple greedy algorithm outperforms SS-UCB, indicating the benefits of free exploration in the many-armed regime. We establish a connection between this new form of free exploration and the distribution of a specific tail event for the prior distribution of arm rewards. We prove that the subsampled greedy algorithm is rate-optimal for Bernoulli bandits when k > T and achieves sublinear regret with more general distributions. These findings highlight the practical advantage of using a variant of the greedy algorithm in the many-armed regime.