EfficientNets is a framework for developing deep neural architectures that combines resolution, depth, and width to achieve high efficiency and performance. However, we have observed that for smaller networks, resolution and depth are more important than width. Therefore, the original method of compound scaling in EfficientNet is not suitable for downsizing neural architectures. In this paper, we propose a new approach called TinyNet, which uses a series of smaller models derived from EfficientNet-B0 with a constraint on computational costs. Our experimental results on the ImageNet benchmark show that TinyNet outperforms the smaller version of EfficientNets using the inverse scaling formula. For example, TinyNet-E achieves a 59.9% Top-1 accuracy with only 24M FLOPs, which is 1.9% higher than the previous best MobileNetV3 with similar computational cost. The code for TinyNet is available at the provided links.