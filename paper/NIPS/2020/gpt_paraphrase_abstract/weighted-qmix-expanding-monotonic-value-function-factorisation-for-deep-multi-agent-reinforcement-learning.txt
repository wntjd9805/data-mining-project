The Q-learning algorithm QMIX is widely used in cooperative multi-agent reinforcement learning (MARL) with a centralized training and decentralized execution approach. However, QMIX has a limitation in representing value functions where an agent's action preferences depend on other agents' actions. To analyze this limitation, we formalize the objective of QMIX and view it as an operator that computes Q-learning targets and then projects them into the space representable by QMIX. This projection aims to minimize the squared error across all joint actions, but it can fail to recover the optimal policy due to equal weighting on each joint action. To address this issue, we introduce a weighting scheme into the projection to give more importance to better joint actions. We propose two weighting schemes that recover the correct maximal action for any joint action Q-values, including Qâˆ—. Additionally, we present two scalable versions of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX, which show improved performance on predator-prey and multi-agent StarCraft benchmark tasks.