Batch normalization greatly increases the maximum trainable depth of residual networks, which has been crucial for the successful performance of deep residual networks on various benchmarks. The reason for this benefit is that, during initialization, batch normalization scales down the residual branch compared to the skip connection by a factor proportional to the square root of the network depth. As a result, at the beginning of training, the function computed by normalized residual blocks in deep networks closely resembles the identity function on average. This insight allows us to develop a straightforward initialization method that enables training of deep residual networks without the need for normalization. Additionally, our empirical study demonstrates that while batch normalized networks can be trained with higher learning rates, this advantage is only significant under specific computational conditions and offers minimal benefits when the batch size is small.