We examine a stochastic bandit problem with numerous arms belonging to a finite set of types, each with a distinct average reward. Additionally, there is a fixed distribution over types that determines the proportion of each type in the arm population. The decision maker lacks knowledge of arm types and the distribution, but knows the total number of types present. We propose an adaptive online learning algorithm that achieves an expected cumulative regret of O(log n) relative to the distribution, regardless of the number of plays. We also demonstrate that this level of regret is the best possible. Our algorithm's analysis relies on newly discovered concentration and convergence properties of optimism-based policies, such as UCB, in infinite-armed bandit problems with zero gap. These findings may have independent significance.