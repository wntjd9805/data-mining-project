This paper focuses on Monte-Carlo planning in continuous state-action spaces, which is a less understood problem with applications in control and robotics. The authors propose an algorithm called POLY-HOOT that combines Monte-Carlo Tree Search with a continuous armed bandit strategy called Hierarchical Optimistic Optimization (HOO). They enhance HOO by using a polynomial bonus term instead of a logarithmic one in the upper confidence bounds, motivated by its success in AlphaGo Zero and its role in achieving theoretical guarantees. The authors investigate the regret of the enhanced HOO algorithm in non-stationary bandit problems and establish non-asymptotic convergence guarantees for POLY-HOOT. Experimental results support their theoretical findings.