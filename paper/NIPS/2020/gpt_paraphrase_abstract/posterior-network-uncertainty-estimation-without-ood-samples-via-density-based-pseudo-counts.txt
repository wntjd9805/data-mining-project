Accurate estimation of aleatoric and epistemic uncertainty is crucial for creating safe and reliable systems. Traditional methods, like dropout and ensemble techniques, estimate uncertainty by sampling probability predictions from different submodels, which can be slow during inference. Recent research addresses this limitation by directly predicting parameters of prior distributions over probability predictions using a neural network. However, this approach requires arbitrary target parameters for in-distribution data and assumes unrealistic knowledge of out-of-distribution (OOD) data during training.   To overcome these challenges, we propose the Posterior Network (PostNet) in this study. PostNet utilizes Normalizing Flows to predict individual closed-form posterior distributions over predicted probabilities for any input sample. These posterior distributions accurately capture uncertainty for both in-distribution and OOD data, without the need for OOD data during training. Our experiments demonstrate that PostNet achieves state-of-the-art results in OOD detection and uncertainty calibration even when faced with dataset shifts.