This study examines neural network representations using a probabilistic approach. Bayesian nonparametrics is employed to create models of neural activations in CNNs and latent representations in VAEs. This enables the development of a manageable complexity measure for distributions of neural activations and exploration of the global structure of VAEs' latent spaces. The research investigates how memorization and two common regularization techniques, dropout and input augmentation, impact the complexity of CNN representations. The findings indicate that networks capable of leveraging data patterns learn significantly less complex representations compared to networks that rely on memorization. The study also reveals notable differences in the effects of input augmentation and dropout, with the latter being highly influenced by network width. Additionally, the research analyzes the latent representations learned by standard β-VAEs and MMD β-VAEs. The results demonstrate that the aggregated posterior in standard VAEs tends to collapse to the diagonal prior as regularization strength increases. In contrast, MMD-VAEs learn more complex posterior distributions even with strong regularization, although they do not exhibit independence of latent dimensions. Lastly, the probabilistic models developed in this study are utilized as an effective sampling strategy for latent codes, leading to improved sample quality in VAEs with rich posteriors.