As datasets for classification tasks become larger, it becomes increasingly difficult and expensive to annotate them with all the required labels. For example, annotating the complete OpenImage test set can cost $6.5M. Consequently, current large-scale benchmarks like OpenImages and LVIS only have less than 1% of the labels annotated across all the images. Standard classification models typically ignore these un-annotated labels, resulting in a loss of supervisory signal and reduced performance. However, in this paper, we propose a method that leverages the relationships between images and labels to extract more supervisory signal from the un-annotated labels. We evaluate the effectiveness of our approach on various multi-label computer vision benchmarks, including CIFAR100, MS-COCO panoptic segmentation, OpenImage, and LVIS datasets. Our method consistently outperforms baselines by 2-10% in terms of mean average precision (mAP) and mean F1 metrics across all the datasets.