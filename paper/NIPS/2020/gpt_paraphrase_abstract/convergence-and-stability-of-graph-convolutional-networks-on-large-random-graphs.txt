The behavior of Graph Convolutional Networks (GCNs) is examined on random graphs using random latent variables to represent nodes and a similarity kernel to determine edges. This approach overcomes the challenges of dealing with discrete concepts like isomorphisms on large graphs by focusing on geometric aspects. The convergence of GCNs to their continuous counterpart is explored as the number of nodes increases, with non-asymptotic results applicable to sparse graphs with a logarithmically growing average degree. Additionally, the stability of GCNs to small deformations of the random graph model is analyzed. Unlike previous studies in discrete settings, the continuous setup allows for more intuitive metrics to measure stability, which aids in understanding the effectiveness of convolutional representations on Euclidean domains.