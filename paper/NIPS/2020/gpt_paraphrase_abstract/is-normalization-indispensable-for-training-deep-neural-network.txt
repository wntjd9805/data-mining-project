Normalization operations are commonly used in training deep neural networks, as they can enhance both convergence and generalization in most tasks. Researchers have been actively exploring the effectiveness of normalization and developing new forms of it. This paper aims to investigate the necessity of normalization in training deep neural networks. By removing normalization layers from the networks, we examine the consequences and propose a method to train deep neural networks without normalization layers without compromising performance. Our approach achieves comparable or slightly improved performance in various tasks such as image classification, object detection, segmentation, video classification, and machine translation. This study contributes to a better understanding of the role of normalization layers and provides a viable alternative to them. The source code can be accessed at https://github.com/hukkai/rescaling.