We propose a method called firefly neural architecture descent, which allows for the progressive and dynamic growth of neural networks while optimizing both their parameters and architectures. Our approach uses a steepest descent strategy to iteratively find the best network within a functional neighborhood of the original network, which includes various candidate network structures. Through Taylor approximation and a greedy selection process, we can identify the optimal network structure within this neighborhood. The firefly descent method enables the flexible growth of networks in terms of both width and depth, and it can be employed to learn accurate and efficient neural architectures that prevent catastrophic forgetting in continual learning. In empirical evaluations, firefly descent demonstrates promising performance in both neural architecture search and continual learning. Notably, when applied to a challenging continual image classification task, it learns smaller networks with higher average accuracy compared to state-of-the-art methods.