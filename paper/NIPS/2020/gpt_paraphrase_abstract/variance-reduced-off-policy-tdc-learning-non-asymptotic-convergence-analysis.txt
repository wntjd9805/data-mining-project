Variance reduction techniques have been successfully applied to temporal-difference (TD) learning to improve the sample efficiency in policy evaluation. However, previous research focused on applying variance reduction to either the less popular one time-scale TD algorithm or the two time-scale GTD algorithm, both of which only work in the on-policy setting and with a limited number of independent and identically distributed (i.i.d.) samples. In this study, we introduce a variance reduction scheme for the two time-scale TDC algorithm in the off-policy setting and analyze its convergence rate over both i.i.d. and Markovian samples. Our algorithm achieves a lower sample complexity of O((cid:15)− 3 5 log (cid:15)−1) in the i.i.d. setting compared to the current state-of-the-art result of O((cid:15)−1 log (cid:15)−1). In the Markovian setting, our algorithm achieves a near-optimal sample complexity of O((cid:15)−1 log (cid:15)−1). Experimental results demonstrate that our proposed variance-reduced TDC algorithm outperforms both the conventional TDC and the variance-reduced TD algorithms in terms of asymptotic convergence error.