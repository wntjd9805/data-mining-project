The rapid advancement of Graph Neural Networks (GNNs) has resulted in the development of numerous new architectures and applications. However, current research primarily focuses on proposing and evaluating specific GNN designs, such as GCN, GIN, or GAT, rather than exploring the broader design possibilities of GNNs. Furthermore, GNN designs are often tailored to a single task, with limited understanding of how to quickly identify the best design for a new task or dataset. This study defines and systematically examines the architectural design space for GNNs, encompassing 315,000 different designs across 32 predictive tasks. The approach introduces three key innovations: (1) a comprehensive GNN design space, (2) a task space with a similarity metric to facilitate the identification and transfer of optimal architectures for new tasks/datasets, and (3) an efficient evaluation method to extract insights from a large number of model-task combinations. The findings include a set of guidelines for designing high-performing GNNs, the ability to transfer the best designs across different tasks using the GNN task space, and the achievement of state-of-the-art performance with models discovered through the design space. This work provides a systematic and scalable approach to move beyond individual GNN designs for specific tasks and explore the broader GNN design and task space. Additionally, the authors introduce GraphGym, a powerful platform for exploring various GNN designs and tasks, featuring modularized GNN implementation, standardized evaluation, and reproducible and scalable experiment management.