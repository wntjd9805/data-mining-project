We present GEORGE, a method to address the issue of hidden stratiﬁcation in real-world classiﬁcation tasks where subclasses are not labeled. Hidden stratiﬁcation refers to the variability in performance across subclasses when models are trained using only coarse-grained class labels. This phenomenon is particularly important in safety-critical applications like medicine.   To tackle this problem, we leverage the separability of unlabeled subclasses in the feature space of deep models. We utilize clustering techniques to estimate subclass labels for the training data. These approximate subclass labels are then incorporated as noisy supervision in a distributionally robust optimization objective.   We provide theoretical analysis on the worst-case generalization error of GEORGE across any subclass. Our empirical validation on both real-world and benchmark image classiﬁcation datasets demonstrates the effectiveness of our approach. We observe an improvement of up to 14 percentage points in worst-case subclass accuracy compared to standard training techniques, without the need for subclass information.