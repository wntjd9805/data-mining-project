Current 2D generative adversarial networks can synthesize high-resolution images but lack an understanding of the 3D world and image formation. This limits their ability to control camera viewpoint and object pose precisely. Recent methods have tried using voxel-based representations and differentiable rendering to address this issue, but they either produce low-resolution images or struggle to separate camera and scene properties. In this paper, we propose a generative model for radiance fields, which have shown success in synthesizing new views of a single scene. Unlike voxel-based representations, radiance fields are not constrained to a coarse discretization of 3D space and allow for separating camera and scene properties while handling reconstruction ambiguity. We introduce a multi-scale patch-based discriminator to train our model using unposed 2D images and demonstrate high-resolution image synthesis. Through systematic analysis on challenging datasets, our experiments show that radiance fields are a powerful representation for generative image synthesis, resulting in 3D consistent models with high fidelity.