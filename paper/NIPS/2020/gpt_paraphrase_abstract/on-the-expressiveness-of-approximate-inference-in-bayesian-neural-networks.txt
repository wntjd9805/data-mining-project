This study examines the effectiveness of common variational methods in approximating the Bayesian predictive distribution for Bayesian neural networks (BNNs). The researchers find that mean-field Gaussian and Monte Carlo dropout, two commonly used distributions in weight-space, have limitations in accurately representing uncertainty in certain cases. Exact inference, on the other hand, does not exhibit this limitation, indicating that it is caused by the approximation methods rather than the model itself. For deep networks, the researchers prove a universality result, showing that approximate posteriors in the aforementioned classes can provide flexible uncertainty estimates. However, they also find that similar limitations can persist when using variational inference in deeper networks. These findings emphasize the need for careful consideration of the implications of approximate inference methods in BNNs.