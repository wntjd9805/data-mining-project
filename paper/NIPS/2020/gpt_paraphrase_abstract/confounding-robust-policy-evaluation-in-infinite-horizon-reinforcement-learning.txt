In applications of batch reinforcement learning like education and healthcare, evaluating sequential decision policies from observational data is challenging due to unobserved variables that confound observed actions. This makes it impossible to accurately evaluate new policies. To address this issue, we propose a robust approach that estimates precise bounds on the value of a given policy in an infinite-horizon problem, using data from another policy with unobserved confounding. Our method takes into account stationary unobserved confounding and computes bounds by optimizing over a set of stationary state-occupancy ratios that align with a new partially identified estimating equation and a sensitivity model. We demonstrate that as more confounded data is collected, our approach converges to sharp bounds. Although checking set membership is a linear program, determining the support function involves a challenging nonconvex optimization problem. To overcome this, we develop approximations based on nonconvex projected gradient descent and demonstrate the effectiveness of these bounds through empirical evaluation.