In 1988, Eric B. Baum demonstrated that two-layer neural networks with a threshold activation function can perfectly memorize the binary labels of n points in Rd using only (n/d) neurons. However, we have found that with ReLU networks, four times as many neurons are needed to fit arbitrary real labels. Additionally, for approximate memorization with an error margin of ε, the neural tangent kernel can achieve this with only O(n d · log(1/ε)) neurons (assuming well-dispersed data). However, these approaches result in suboptimal weights for the neurons. In contrast, we propose a new training procedure for ReLU networks that utilizes complex recombination of the neurons, leading to approximate memorization with both O(n d · log(1/ε)) neurons and nearly-optimal weight sizes.