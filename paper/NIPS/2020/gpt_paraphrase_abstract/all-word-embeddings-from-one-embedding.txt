In neural network-based models for natural language processing (NLP), a significant portion of the parameters is typically devoted to word embeddings. These models often require a large embedding matrix, which can be costly in terms of memory and storage. To address this issue, this study proposes a method called ALONE (all word embeddings from one) that represents the embeddings for all words by transforming a shared embedding. The shared embedding is modified with a non-trainable, word-specific filter vector to construct the embedding for each word. This constructed embedding is then inputted into a feed-forward neural network to enhance its expressiveness. The proposed method reduces the total number of parameters and introduces a memory-efficient filter construction approach. Experimental results demonstrate that ALONE can be used as a sufficient word representation, as evidenced by its ability to reconstruct pre-trained word embeddings. Additionally, ALONE is applied to NLP tasks such as machine translation and summarization, where it is combined with the Transformer model. The results show that ALONE achieves comparable scores on translation and summarization tasks while using fewer parameters.