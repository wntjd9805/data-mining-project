Lossy gradient compression is a useful tool for improving communication efficiency in distributed machine learning training. However, decentralized training algorithms for compressed communication over arbitrary networks have been complex and required extra resources. We present a simple algorithm that directly compresses model differences between neighboring workers using low-rank linear compressors. Inspired by the PowerSGD algorithm, our method maximizes information transfer per bit through power iteration steps. We demonstrate that our algorithm requires no additional hyperparameters, converges faster than previous methods, and is independent of network and compression. In various deep learning benchmarks, our compressors perform similarly to state-of-the-art algorithms. The code for our algorithm is available at https://github.com/epfml/powergossip.