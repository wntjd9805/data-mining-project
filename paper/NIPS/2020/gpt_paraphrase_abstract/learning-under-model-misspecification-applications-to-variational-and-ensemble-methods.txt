Most machine learning models used for predictions do not perfectly represent reality, resulting in learning occurring under model misspecification. This study introduces a new analysis of the generalization performance of Bayesian model averaging when the model is misspecified and the data is independent and identically distributed. The analysis demonstrates that Bayesian model averaging provides suboptimal generalization performance in the presence of model misspecification. Consequently, the study presents strong theoretical evidence that Bayesian methods are not optimal for learning predictive models unless the model class is perfectly specified. By utilizing second-order PAC-Bayes bounds, the study develops a new family of Bayesian-like algorithms that can be implemented as variational and ensemble methods. These algorithms yield a new posterior distribution, distinct from the Bayesian posterior, which leads to a posterior predictive distribution with improved generalization performance. Experiments conducted with Bayesian neural networks support these findings.