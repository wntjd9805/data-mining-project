Score function-based natural language generation (NLG) methods like RE-INFORCE face challenges with low sample efficiency and training instability. This is because sampling in the discrete space is non-differentiable, requiring these methods to treat the discriminator as a black box and ignore gradient information. To address these issues and enhance sample efficiency, we propose TaylorGAN. TaylorGAN improves gradient estimation through off-policy update and first-order Taylor expansion. This allows us to train NLG models from scratch with smaller batch sizes, without the need for maximum likelihood pre-training. TaylorGAN surpasses existing GAN-based methods in terms of quality and diversity, as measured by multiple metrics.