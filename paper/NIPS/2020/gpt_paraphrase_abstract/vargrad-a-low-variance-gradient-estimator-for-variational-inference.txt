We examine an unbiased gradient estimator for variational inference called the evidence lower bound (ELBO). This estimator is based on the score function method with leave-one-out control variates. We introduce a new loss, known as the log-variance loss, which measures the variance of the log-ratio between the exact posterior and the variational approximation. We demonstrate that the gradient of the log-variance loss is equivalent to the gradient of the negative ELBO under certain conditions. We theoretically prove that this gradient estimator, referred to as VarGrad, has lower variance than the score function method in specific scenarios, and the leave-one-out control variate coefficients are near-optimal. Through empirical analysis on a discrete variational autoencoder (VAE), we show that VarGrad provides a favorable trade-off between variance and computation compared to other advanced estimators.