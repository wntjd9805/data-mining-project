Rectified linear unit (ReLU) activations in deep neural networks (DNNs) can be seen as gates that either allow or block the input signal based on its positivity or negativity. These gates change their on/off status across different input examples and network weights. Only a subset of gates are active for a specific input example, and the connected weights determine the output. This paper aims to analyze the role of these gates and active sub-networks in deep learning. The authors introduce a new concept called neural path feature (NPF) to represent the on/off state of the gates for a given input, and neural path value (NPV) to encode the weights of the DNN. They demonstrate that the network's output is the inner product of NPF and NPV. The main finding of the study is that the neural path kernel associated with NPF characterizes the information stored in the gates of a DNN. Experimental results on MNIST and CIFAR-10 datasets reveal that NPFs are learned during training and play a crucial role in generalization. Additionally, NPFs and NPVs can be learned separately in two networks and still achieve good generalization. The experiments indicate that most of the information learned by DNNs with ReLU activations is stored in the gates, highlighting the importance of investigating their role.