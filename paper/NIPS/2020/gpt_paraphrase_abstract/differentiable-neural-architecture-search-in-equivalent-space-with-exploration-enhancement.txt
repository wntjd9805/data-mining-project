Recent studies on One-Shot Neural Architecture Search (NAS) have primarily utilized a two-step optimization process to optimize the supernet weights and architecture parameters. This involves converting the discrete search space into a differentiable space. However, the inconsistency in the relaxation methods used raises concerns about whether the optimization in the continuous space accurately represents the optimization in the discrete space. In contrast, this paper introduces a variational graph autoencoder to transform the discrete architecture space into a continuous latent space, ensuring congruence between the two. To promote intelligent exploration during architecture search and prevent local optima, a probabilistic exploration enhancement method is proposed. Additionally, to address the issue of catastrophic forgetting in differentiable One-Shot NAS, which impairs the predictive ability of the supernet and hampers the efficiency of bilevel optimization, an architecture complementation method is suggested. The effectiveness of the proposed approach is analyzed, and a series of experiments is conducted to compare its performance against state-of-the-art One-Shot NAS methods.