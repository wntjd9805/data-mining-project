Cycle-Contrastive Learning (CCL) is a new self-supervised technique for video representation learning. Unlike existing methods, CCL considers the relationship between frames and videos, aiming to find correspondences across both domains. By utilizing an R3D architecture, CCL learns frame and video representations from a single network, using a shared non-linear transformation for embedding features. The effectiveness of CCL is demonstrated through improved performance in nearest neighbor retrieval and action recognition tasks on UCF101, HMDB51, and MMAct datasets. The learned video representation can be successfully transferred to downstream video understanding tasks.