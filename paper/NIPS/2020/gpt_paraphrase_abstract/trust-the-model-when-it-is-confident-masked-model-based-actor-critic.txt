Model-based Reinforcement Learning (RL) is believed to be more efficient than model-free RL, but in reality, this is not always the case due to significant model errors. In complex and noisy environments, model-based RL struggles to effectively utilize the model if it lacks knowledge of when to trust its predictions. This study highlights the importance of better model usage. The researchers demonstrate that restricting the use of model-generated data to instances where the model error is low can reduce the performance gap between model and real rollouts. This finding motivates the development of the Masked Model-based Actor-Critic (M2AC) algorithm, which optimizes policy by maximizing a model-based lower-bound of the true value function. M2AC incorporates a masking mechanism based on the model's uncertainty to determine whether or not to utilize its predictions. As a result, this new algorithm yields robust policy improvements. Experimental results on continuous control benchmarks show that M2AC performs exceptionally well in noisy environments, even when using long model rollouts, and significantly outperforms previous state-of-the-art methods.