We examine deep neural networks (DNNs) that are trained on natural images using random labels. While this approach is commonly used in literature to explore memorization, generalization, and other phenomena, there is limited understanding of what these DNNs actually learn in this scenario. In this paper, we demonstrate that a correlation between the principal components of network parameters and the data occurs when training with random labels, both in convolutional and fully connected networks. We investigate this alignment effect by studying pre-trained neural networks on randomly labeled image data, followed by fine-tuning on separate datasets with either random or real labels. Our findings reveal that this alignment leads to a positive transfer: DNNs pre-trained with random labels exhibit faster training on downstream tasks compared to training from scratch, even after accounting for basic factors like weight scaling. We also analyze the potential influence of competing effects, such as specialization in later layers, which may mask the positive transfer. Our analysis encompasses various network architectures, including VGG16 and ResNet18, applied to CIFAR10 and ImageNet datasets.