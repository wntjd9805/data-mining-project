Recent research has shown that incorporating adversarial training into self-supervised pre-training can enhance robustness. This study aims to enhance the robustness of self-supervised pre-training by learning representations that remain consistent under both data augmentations and adversarial perturbations. The approach utilizes a contrastive learning framework that maximizes feature consistency under differently augmented views. This is particularly beneficial for achieving adversarial robustness, as small input perturbations can cause significant changes in features or predicted labels. The study explores different approaches to formulate the contrastive task and demonstrates that contrastive pre-training, when combined with adversarial perturbations, can yield models that are both label-efficient and robust. The proposed Adversarial Contrastive Learning (ACL) consistently outperforms existing methods, surpassing the previous state-of-the-art unsupervised robust pre-training approach by 2.99% in robust accuracy and 2.14% in standard accuracy on the CIFAR-10 dataset. Furthermore, ACL pre-training proves to enhance semi-supervised adversarial training, even with limited labeled examples. The codes and pre-trained models for ACL are publicly available at https://github.com/VITA-Group/Adversarial-Contrastive-Learning.