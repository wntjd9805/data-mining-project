Visual attributes have long been recognized as crucial in zero-shot learning. To enhance the transfer of attribute-based knowledge from known to unknown classes, we propose a novel framework that integrates attribute localization into image representation. Our approach simultaneously learns discriminative global features and local features using only class-level attributes. A visual-semantic embedding layer is employed to learn global features, while a attribute prototype network is utilized to regress and decorrelate attributes from intermediate features for learning local features. Our locality augmented image representations surpass the state-of-the-art performance on three zero-shot learning benchmarks. Additionally, our model provides visual evidence of attributes in an image, confirming the improved attribute localization ability of our image representation.