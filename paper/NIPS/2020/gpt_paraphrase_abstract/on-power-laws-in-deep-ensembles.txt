Ensembles of deep neural networks have proven to be highly effective in uncertainty estimation and improving accuracy. This study focuses on a classification problem and examines the behavior of both non-calibrated and calibrated negative log-likelihood (CNLL) in a deep ensemble based on the ensemble size and member network size. The study identifies the conditions under which CNLL follows a power law with respect to either the ensemble size or member network size and analyzes the dynamics of the corresponding power law parameters. An important practical finding is that a single large network may underperform compared to an ensemble of several medium-sized networks with the same total number of parameters, referred to as a memory split. By leveraging the power law-like dependencies discovered, the study enables the prediction of (1) the potential benefits of ensembling networks with a given structure and (2) the optimal memory split based on a limited number of trained networks, given a memory budget.