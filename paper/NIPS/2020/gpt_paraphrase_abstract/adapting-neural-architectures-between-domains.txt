Neural architecture search (NAS) has shown impressive results in automatically designing high-performance neural networks. However, the architecture search is typically conducted on a smaller dataset to save time, which may not guarantee optimal performance on larger and more challenging datasets. This paper proposes a method called AdaptNAS, which aims to improve the generalization of neural architectures through domain adaptation. The authors analyze the relationship between the derived architecture, validation error, and data distribution distance on different domains. Based on these analyses, AdaptNAS is developed as a principled approach to adapt neural architectures between domains in NAS. Experimental evaluation demonstrates that AdaptNAS can successfully extend its architectural success from a small portion of ImageNet to the entire dataset, outperforming state-of-the-art comparison algorithms.