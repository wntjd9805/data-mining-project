The abstract discusses the effectiveness of Variational Bayes (VB) as a scalable alternative to Markov chain Monte Carlo for Bayesian inference. The study focuses on a mean-field spike and slab VB approximation of Bayesian model selection priors in sparse high-dimensional logistic regression. The researchers provide non-asymptotic theoretical guarantees for the VB posterior in terms of both estimation accuracy and prediction loss for sparse data. The results highlight the optimal convergence rates achieved by the VB algorithm, independent of the unknown truth, which has implications for selecting effective priors. Numerical analysis confirms the superior performance of the VB algorithm compared to other sparse VB approaches.