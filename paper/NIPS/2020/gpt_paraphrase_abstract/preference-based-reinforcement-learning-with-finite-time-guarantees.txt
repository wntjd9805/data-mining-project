Preference-based Reinforcement Learning (PbRL) uses preferences instead of numerical reward values in traditional reinforcement learning to better capture human opinion on the objective. This paper presents the first finite-time analysis for general PbRL problems. It is shown that if preferences over trajectories are deterministic, a unique optimal policy may not exist in PbRL. However, if preferences are stochastic and related to hidden reward values, algorithms for PbRL are proposed that can identify the best policy with high probability and accuracy up to Îµ. The method explores under-explored states in the state space and combines dueling bandits and policy search to solve PbRL. Real-world experiments demonstrate the effectiveness of the proposed method.