Shortcut connections in deep neural networks, such as ResNets, have enabled effective training of very deep networks. However, the additional computation costs associated with these shortcuts are often overlooked. For instance, during online inference, ResNet-50 shortcuts contribute to 40% of the memory usage on feature maps because the preceding layer features cannot be released until subsequent calculations are finished. This study introduces a novel joint-training framework that trains plain convolutional neural network (CNN) models by utilizing the gradients of the corresponding ResNet. During forward propagation, the early-stage feature maps of the plain CNN are passed through both its own later stages and the ResNet's later stages to calculate the loss. During backpropagation, gradients from this combination are used to update the plain CNN network, addressing the issue of gradient vanishing. Extensive experiments on several datasets demonstrate that our approach enables the plain CNN network without shortcuts to achieve the same accuracy as the ResNet baseline, with a speed-up of 1.4 times and a memory reduction of 1.25 times. Moreover, the feature transferability of our ImageNet pretrained plain-CNN network is confirmed through fine-tuning on other datasets. The results show that the performance of the plain-CNN is slightly better than that of the baseline ResNet-50. The code for this work is available at https://github.com/leoozy/JointRD_Neurips2020, and the MindSpore code can be found at https://www.mindspore.cn/resources/hub.