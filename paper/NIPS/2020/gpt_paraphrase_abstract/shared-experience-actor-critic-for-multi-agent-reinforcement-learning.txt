Exploring in multi-agent reinforcement learning poses a difficult challenge, particularly in environments with sparse rewards. To address this, we propose a method for efficient exploration by enabling agents to share their experiences. Our algorithm, called Shared Experience Actor-Critic (SEAC), incorporates experience sharing within an actor-critic framework by combining the gradients of different agents. Through evaluations in various sparse-reward multi-agent environments, we consistently observe that SEAC outperforms several baselines and state-of-the-art algorithms. It achieves this by learning in fewer steps and converging to higher returns. Notably, in more challenging environments, experience sharing proves crucial in enabling the learning of task solutions.