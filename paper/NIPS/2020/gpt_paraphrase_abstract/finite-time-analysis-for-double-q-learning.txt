While Q-learning is a successful algorithm in reinforcement learning, its implementation often suffers from overestimation of Q-function values due to random sampling. The double Q-learning algorithm, proposed by Hasselt (2010), addresses this issue by randomly switching the update between two Q-estimators. However, the theoretical understanding of double Q-learning is limited, with only asymptotic convergence established so far. This paper provides the first non-asymptotic analysis for double Q-learning, showing that both synchronous and asynchronous versions of the algorithm converge to a neighborhood of the global optimum. The analysis establishes finite-time bounds on the convergence rate, which depend on the decay parameter of the learning rate and the discount factor. The paper introduces novel techniques to derive these bounds, which is a new contribution to the literature of stochastic approximation.