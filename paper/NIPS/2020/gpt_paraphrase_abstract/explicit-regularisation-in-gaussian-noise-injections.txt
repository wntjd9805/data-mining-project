The regularizing effect of injecting Gaussian noise into neural networks is examined in this study. While previous research has focused on the impact of noise injections on data, little is known about their regularization effect on network activations. The study derives the explicit regularizer of Gaussian noise injections by considering the injected noise, and demonstrates that it penalizes functions with high-frequency components in the Fourier domain, especially in layers closer to the network's output. Through analytical and empirical analysis, it is shown that this regularization leads to well-calibrated classifiers with significant classification margins.