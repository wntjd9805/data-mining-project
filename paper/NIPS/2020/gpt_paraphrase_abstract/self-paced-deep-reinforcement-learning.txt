Curriculum reinforcement learning (CRL) is a technique that enhances an agent's learning speed and stability by exposing it to a customized sequence of tasks. However, the challenge lies in automatically generating a curriculum for a given reinforcement learning (RL) agent without manual intervention. This paper addresses this issue by considering curriculum generation as an inference problem. The proposed approach involves progressively learning distributions over tasks to approach the desired target task. This automatic curriculum generation method is controlled by the agent itself, supported by strong theoretical foundations, and easily integrated with deep RL algorithms. Experimental results demonstrate that the curricula generated using this algorithm significantly improve learning performance in various environments and outperform existing CRL algorithms.