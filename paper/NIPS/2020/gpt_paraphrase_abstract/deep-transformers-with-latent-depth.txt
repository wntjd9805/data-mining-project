The Transformer model has achieved impressive performance in various sequence modeling tasks. However, effectively utilizing the model capacity with large or variable depths remains a challenge. To address this, we propose a probabilistic framework that learns the posterior distributions of layer selection, allowing the model to automatically determine which layer(s) to use. Additionally, we introduce a novel approach for training a shared Transformer network for multilingual machine translation. This method employs different layer selection posteriors for each language pair, mitigating the vanishing gradient problem and enabling stable training of deep Transformers (e.g. 100 layers). Our experiments on WMT English-German machine translation and masked language modeling tasks demonstrate that our approach surpasses existing methods for training deeper Transformers. Moreover, our method proves effective in leveraging increased model capacity for multilingual machine translation, leading to universal improvements in both many-to-one and one-to-many translation scenarios with diverse language pairs.