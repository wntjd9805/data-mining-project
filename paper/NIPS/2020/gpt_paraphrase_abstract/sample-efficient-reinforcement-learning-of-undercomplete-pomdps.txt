Partial observability is a common issue in many reinforcement learning scenarios, requiring agents to remember past information, infer hidden states, and incorporate this knowledge into their exploration. This poses computational and statistical challenges for learning Partially Observable Markov Decision Processes (POMDPs) in general. However, this research demonstrates that these challenges do not prevent efficient reinforcement learning for certain subclasses of POMDPs. Specifically, a sample-efficient algorithm called OOM-UCB is introduced for episodic finite undercomplete POMDPs, where the number of observations exceeds the number of latent states and exploration is crucial for learning. OOM-UCB achieves optimal sample complexity for finding an Îµ-optimal policy, along with being polynomial in other relevant quantities. Additionally, a computationally and statistically efficient algorithm is provided for POMDPs with deterministic state transitions.