Meta-learning improves generalization of machine learning models when faced with new tasks by leveraging experiences from related prior tasks. To enhance generalization further, we propose a novel task representation called model-aware task embedding (MATE) that considers both the data distributions of different tasks and the complexity of these tasks through the models used. We incorporate task complexity by combining a unique variant of kernel mean embedding with an instance-adaptive attention mechanism inspired by an SVM-based feature selection algorithm. MATE can be easily integrated into existing meta learners as a plug-and-play module through conditioning layers in deep neural networks. Although MATE is applicable to various tasks involving the concept of task/environment, we demonstrate its effectiveness in few-shot learning by consistently improving a state-of-the-art model on two benchmarks. The source codes for this paper can be found at https://github.com/VITA-Group/MATE.