In various scenarios, contextual policies are utilized to tailor system parameters and actions according to the specific requirements of a given situation. However, in certain real-world situations like randomized controlled trials or A/B tests, it may not be feasible to measure the outcomes of policies on an individual context level, leading to only observing aggregate rewards across multiple contexts. This poses a challenge for policy optimization as it requires solving a complex optimization problem across the entire space of contextual policies, where existing optimization methods are inadequate. To address this, we propose effective models that leverage the structure of the search space to directly optimize contextual policies using Bayesian optimization based on aggregate rewards. Through simulation studies, we evaluate the performance and robustness of these models, revealing that our approach of inferring a low-dimensional context embedding yields the best results. Furthermore, we demonstrate the successful application of our contextual policy optimization in a real-world example involving video bitrate policies.