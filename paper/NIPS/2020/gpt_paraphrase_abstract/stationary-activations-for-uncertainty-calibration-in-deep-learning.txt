We present a novel set of nonlinear activation functions for neural networks that imitate the characteristics of the popular Matérn family of kernels in Gaussian process models. This collection covers a range of locally stationary models with varying levels of mean-square differentiability. By analyzing a network with an infinitely wide hidden layer, we establish a direct connection between these activation functions and the corresponding Gaussian process models. When the smoothness approaches infinity, the Matérn family yields the radial basis function (RBF) kernel, thus recovering RBF activations. Matérn activation functions exhibit similar desirable properties to their counterparts in Gaussian process models. We illustrate that the combination of local stationarity and limited mean-square differentiability leads to effective performance and uncertainty calibration in Bayesian deep learning tasks. Specifically, local stationarity enhances the calibration of out-of-distribution uncertainty. We validate these properties through experiments on classification and regression benchmarks, as well as a radar emitter classification task.