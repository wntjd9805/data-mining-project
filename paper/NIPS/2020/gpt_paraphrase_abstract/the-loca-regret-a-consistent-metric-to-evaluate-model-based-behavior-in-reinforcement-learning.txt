Deep model-based Reinforcement Learning (RL) has the potential to significantly enhance the efficiency of deep RL by reducing the number of samples required. Although deep model-based methods have recently shown promising results, the absence of a consistent metric for evaluation makes it challenging to compare different approaches. The existing metric, which measures sample efficiency in a single-task setting, fails to isolate the improvements resulting from model-based learning. This hinders accurate assessment of progress in model-based RL. To address this issue, we propose an experimental setup inspired by neuroscience research on identifying model-based behavior in humans and animals. Our metric, called Local Change Adaptation (LoCA) regret, evaluates how quickly an RL method adapts to a local change in the environment. Unlike traditional metrics, LoCA can detect model-based behavior even when the method employs a suboptimal representation. It also provides insights into the proximity of a method's behavior to optimal model-based behavior. We employ this setup to assess the model-based behavior of MuZero in a modified version of the Mountain Car task.