The presence of adversarial examples that can deceive trained neural network classifiers highlights the need for a deeper understanding of potential attacks in order to develop effective defenses against them. This includes considering attacks in the challenging non-interactive blackbox scenario, where adversarial attacks are generated without any access to the target model. Previous attacks in this setting have relied on empirical observations without solid guarantees. This study presents a theoretical foundation for creating transferable adversarial examples across entire hypothesis classes. A framework called Adversarial Example Games (AEG) is introduced, which models the generation of adversarial examples as a game between an attack generator and a classifier. AEG allows for the design of adversarial examples by training the generator and classifier in an adversarial manner using a given hypothesis class. It is proven that this game has an equilibrium, and the optimal generator can create adversarial examples that can attack any classifier in the corresponding hypothesis class. The effectiveness of AEG is demonstrated on the MNIST and CIFAR-10 datasets, surpassing previous state-of-the-art methods with notable improvements against both undefended and robust models.