This study addresses the challenge of machines localizing sounding objects in mixed sound scenes, similar to how humans can do so. The authors propose a two-stage learning framework for self-supervised class-aware sounding object localization. Firstly, they suggest learning robust object representations by aggregating sound localization results in single source scenes. Then, they generate class-aware object localization maps in cocktail-party scenarios by utilizing pre-learned object knowledge. Sounding objects are selected based on matching audio and visual object category distributions, using audiovisual consistency as a self-supervised signal. Experimental results, both in realistic and synthesized cocktail-party videos, demonstrate the model's effectiveness in filtering out silent objects and accurately locating sounding objects of different classes. The code for this study can be found at https://github.com/DTaoo/Discriminative-Sounding-Objects-Localization.