In various natural domains, a small modification to an entity can result in a significant change in its meaning. This can be observed in instances where a single word alteration can completely transform the interpretation of a sentence or when a single amino acid change can cause a viral protein to evade antiviral treatments or immune responses. While it is valuable to identify such mutations, quantifying the rules that govern semantic change can be challenging. In this study, we introduce a novel problem called constrained semantic change search (CSCS), which involves identifying mutations that have a substantial impact on semantics while adhering to complex constraints like English grammar or biological viability. To address this problem, we propose an unsupervised solution that utilizes language models to learn continuous latent representations. Our approach demonstrates promising results in CSCS by accurately predicting single-word mutations in news headlines, mapping a continuous semantic space of viral variation, and achieving unprecedented zero-shot prediction of escape mutations in key influenza and HIV proteins. These findings suggest a meaningful connection between modeling natural language and the evolution of pathogenic organisms.