Bayesian meta-learning is a technique that allows for quick and reliable adaptation to new tasks while considering uncertainty. It involves using empirical Bayes inference to create a hierarchical model. In this study, we expand on this approach by incorporating various existing methods and introducing our own variant using the gradient-EM algorithm. Our method improves computational efficiency by avoiding the need for back-propagation computation during the meta-update step, which can be resource-intensive for deep neural networks. Additionally, it enhances flexibility by separating the inner-update optimization procedure from the meta-update. Through experiments on sinusoidal regression, few-shot image classification, and policy-based reinforcement learning, we demonstrate that our method achieves higher accuracy with lower computation costs and exhibits greater robustness to uncertainty.