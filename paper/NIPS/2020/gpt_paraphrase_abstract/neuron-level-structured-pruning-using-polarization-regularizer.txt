Neuron-level structured pruning is an effective technique to reduce computation in neural networks while maintaining prediction accuracy. Previous methods achieve structured pruning by applying L1 regularization to scaling factors of neurons and pruning those with factors below a threshold. However, L1 regularization lacks discrimination between neurons as it pushes all scaling factors towards zero. To address this, we propose a new regularizer called polarization regularizer, which selectively suppresses unimportant neurons while preserving important ones. Theoretical analysis demonstrates that polarization regularizer pushes some scaling factors to zero and others to a positive value. Experimental results on CIFAR and ImageNet datasets validate that structured pruning with polarization regularizer outperforms L1 regularization, achieving state-of-the-art results.