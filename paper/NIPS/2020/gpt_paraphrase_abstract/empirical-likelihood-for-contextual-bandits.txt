We present a new method for estimating the value of a policy in the contextual bandit setting using off-policy data. Our approach utilizes empirical likelihood techniques to formulate the estimator and confidence interval as convex optimization problems. We also introduce an off-policy policy optimization algorithm that searches for policies with high reward lower bounds based on the lower bound of our confidence interval. Experimental results demonstrate the superiority of our estimator and confidence interval compared to previous methods in finite sample scenarios. Furthermore, our proposed policy optimization algorithm outperforms a strong baseline system for learning from off-policy data.