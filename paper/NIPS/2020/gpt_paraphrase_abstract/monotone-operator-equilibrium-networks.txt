Implicit-depth models like Deep Equilibrium Networks have shown superior performance to traditional deep networks while requiring less memory. However, these models face issues with unstable convergence and lack guarantees for finding a solution. On the other hand, Neural ODEs guarantee the existence of a unique solution but perform poorly compared to traditional networks. To address these challenges, we introduce a new implicit-depth model called Monotone Operator Equilibrium Network (monDEQ) based on the theory of monotone operators. We establish a connection between finding the equilibrium point of an implicit network and solving a form of monotone operator splitting problem, which allows for efficient solvers with stable convergence. We develop a parameterization that ensures all operators remain monotone, guaranteeing the existence of a unique equilibrium point. Furthermore, we demonstrate how to instantiate various versions of these models and implement iterative solvers for structured linear operators like multi-scale convolutions. Our models outperform Neural ODE-based models while maintaining computational efficiency. The code for our models is available at http://github.com/locuslab/monotone_op_net.