Planning algorithms for large-scale Markov decision processes (MDPs) need to be efficient regardless of the MDP's state count. We propose a planning approach that utilizes linear value function approximation with minimal requirements: a small approximation error for the optimal value function and a set of "core" states that encompass other states' features. This method does not assume representability of policies or value functions for non-optimal policies. By using a generative oracle (simulator) for the MDP, our algorithm generates near-optimal actions for any state. Its computation time scales polynomially with the number of features, core states, actions, and the effective horizon.