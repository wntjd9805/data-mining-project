We introduce the first agnostic online boosting algorithm, which enhances a weak learner with slightly better-than-trivial regret guarantees into a strong learner with sublinear regret. Our algorithm utilizes a simple reduction to online convex optimization, efficiently transforming any online convex optimizer into a boosting algorithm. This reduction applies to both the statistical and online realizable settings, thereby unifying the four cases of statistical/online and agnostic/realizable boosting.