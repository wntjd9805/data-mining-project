This paper examines the concept of over-parametrization in training neural networks. It explores the idea of using larger networks to avoid suboptimal solutions during optimization. The study focuses on tensor decomposition, specifically finding a rank m decomposition for an l-th order tensor in (Rd)⊗l of rank r. The authors demonstrate that in a lazy training regime, a minimum of m = Ω(dl−1) is required, while a variant of gradient descent can find an approximate tensor when m = O∗(r2.5l log d). These findings suggest that gradient descent can exploit low-rank structures in the data, surpassing the limitations of lazy training and maximizing the benefits of over-parametrization.