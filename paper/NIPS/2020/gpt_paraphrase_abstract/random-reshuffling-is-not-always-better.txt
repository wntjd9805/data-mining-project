The order in which training examples are presented can significantly impact the effectiveness of learning algorithms. Random reshuffling, which involves sampling training examples without replacement, is generally believed to lead to faster convergence. However, we provide a counterexample to the long-standing conjecture that random reshuffling improves performance in learning algorithms. Specifically, we demonstrate a learning task and algorithm in which random sampling with replacement outperforms random reshuffling.