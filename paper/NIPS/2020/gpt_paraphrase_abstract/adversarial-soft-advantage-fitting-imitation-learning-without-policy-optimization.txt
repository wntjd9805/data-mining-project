Adversarial Imitation Learning faces challenges in practice due to the instability of adversarial training and the inefficiency of reinforcement learning. To address this, we propose a new approach that eliminates the need for policy optimization steps. Our method involves a discriminator that is conditioned on both the previous generator's policy and a learnable policy. By optimizing this discriminator, we directly learn the optimal generator's policy, eliminating the need for an additional optimization loop. This significantly reduces the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase entirely. We demonstrate through various tasks that our simplified approach is competitive with existing Imitation Learning methods.