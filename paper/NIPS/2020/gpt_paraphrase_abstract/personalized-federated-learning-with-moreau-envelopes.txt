Federated learning (FL) is a decentralized and privacy-preserving method for machine learning. It involves clients collaborating with a server to learn a global model without sharing their data. However, FL faces a challenge of statistical diversity among clients, which limits the global model's performance on individual client tasks. To tackle this issue, we propose a personalized FL algorithm called pFedMe. This algorithm uses Moreau envelopes as clients' regularized loss functions to separate personalized model optimization from global model learning. The convergence rate of pFedMe is theoretically proven to be state-of-the-art, achieving quadratic speedup for strongly convex objectives and sublinear speedup of order 2/3 for smooth non-convex objectives. In experiments, pFedMe demonstrates superior empirical performance compared to vanilla FedAvg and Per-FedAvg, a meta-learning based personalized FL algorithm.