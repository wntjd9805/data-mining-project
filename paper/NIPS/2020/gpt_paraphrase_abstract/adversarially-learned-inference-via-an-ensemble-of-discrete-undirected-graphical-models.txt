We propose a novel framework called AGMs that addresses the limitation of re-training undirected graphical models (EGMs) for solving inference tasks that were not seen during training. AGMs produce an infinitely-large ensemble of graphical models, optimized within the GAN framework, which allows for generating data. Inference is then performed using a finite subset of these models. AGMs perform similarly to EGMs on inference tasks that EGMs were specifically optimized for, but they demonstrate significantly better generalization to unseen inference tasks. This generalization is superior to EGMs as well as other deep neural architectures like GibbsNet and VAEAC, which allow arbitrary conditioning. Additionally, AGMs enable fast data sampling, comparable to Gibbs sampling from EGMs.