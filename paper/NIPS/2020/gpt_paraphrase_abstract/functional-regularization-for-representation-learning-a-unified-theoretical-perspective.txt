Unsupervised and self-supervised learning methods are widely used for downstream prediction tasks, but their theoretical understanding is lacking. To bridge this gap, we propose a unified perspective that considers these approaches as imposing regularization on representations through a learnable function using unlabeled data. We introduce a discriminative theoretical framework to analyze the sample complexity of these methods, demonstrating that carefully chosen hypothesis classes and learnable regularization functions can reduce the need for labeled data. We provide two examples of functional regularization, involving auto-encoders and masked self-supervision, and apply our framework to quantify the decrease in sample complexity. Empirical results support our analysis.