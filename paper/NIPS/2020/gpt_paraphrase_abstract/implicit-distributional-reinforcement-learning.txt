We propose a method called implicit distributional actor-critic (IDAC) to enhance the efficiency of policy-gradient based reinforcement learning algorithms. IDAC consists of a distributional critic, composed of two deep generator networks (DGNs), and a semi-implicit actor (SIA), which utilizes a flexible policy distribution. We adopt a distributional approach to the discounted cumulative return and represent it with a state-action-dependent implicit distribution, approximated by the DGNs taking state-action pairs and random noises as input. The SIA provides a semi-implicit policy distribution by combining policy parameters with a reparameterizable distribution that lacks an analytic density function. This allows the policy's marginal distribution to be implicit, enabling the modeling of complex properties such as covariance structure and skewness while still being able to estimate its parameter and entropy. We incorporate these features into an off-policy algorithm framework for solving problems with continuous action space and evaluate IDAC against state-of-the-art algorithms on representative OpenAI Gym environments. Our results show that IDAC outperforms these baseline algorithms in most tasks. We provide Python code for implementation.