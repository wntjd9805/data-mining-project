We investigate the problem of reinforcement learning in finite-horizon episodic Markov Decision Processes (MDPs) with a limited number of states, actions, and episode length. We introduce a model-free algorithm called UCB-ADVANTAGE and demonstrate that it achieves a regret of approximately OpH 2SAT q, where T â‰¤ KH and K represents the number of episodes. Our regret bound surpasses the findings of Jin et al. (2018) and matches the most effective model-based algorithms and the lower bound based on information theory, with only logarithmic differences. Furthermore, we illustrate that UCB-ADVANTAGE minimizes local switching cost and is applicable to concurrent reinforcement learning, improving upon the recent outcomes of Bai et al. (2019).