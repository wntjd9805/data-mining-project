We propose a novel method for efficient exploration by using a low-dimensional representation of the environment. This representation is learned through a combination of model-based and model-free objectives. To measure novelty, we employ intrinsic rewards based on the distance between nearest neighbors in the low-dimensional space. By utilizing these rewards, we can explore challenging tasks with sparse rewards more effectively through planning routines in the representational space. Our approach incorporates information theoretic principles to shape the representations, allowing us to go beyond merely considering pixel similarity. Through experiments on maze tasks and a control problem, we demonstrate that our exploration method outperforms strong baselines in terms of sample efficiency.