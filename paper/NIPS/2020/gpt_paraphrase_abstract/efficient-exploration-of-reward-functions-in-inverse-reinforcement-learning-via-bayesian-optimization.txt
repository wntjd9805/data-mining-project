Inverse reinforcement learning (IRL) is a challenging problem in various domains, such as value alignment and robot learning from demonstration. Although there have been significant advancements in IRL algorithms, the problem itself remains ambiguous as multiple reward functions can explain the observed behavior, making it difficult to identify the true reward function without prior knowledge or additional information. In this study, we introduce a framework called Bayesian optimization-IRL (BO-IRL) that efficiently explores the reward function space to find multiple solutions consistent with expert demonstrations. BO-IRL achieves this by employing Bayesian Optimization and a novel kernel that maps policy invariant reward function parameters to a single point in a latent space, ensuring that nearby points in this space correspond to similar reward functions. This mapping enables the use of standard stationary kernels to capture correlations across the reward function space. Experimental results on both simulated and real-world environments demonstrate that BO-IRL is able to discover multiple reward functions while minimizing the need for expensive exact policy optimizations.