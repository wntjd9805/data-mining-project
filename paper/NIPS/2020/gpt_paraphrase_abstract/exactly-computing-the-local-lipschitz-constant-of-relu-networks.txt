The local Lipschitz constant of a neural network is a valuable measure used in assessing robustness, generalization, and fairness. We introduce new analytical findings that establish a connection between the local Lipschitz constant of nonsmooth vector-valued functions and a maximization involving the norm of the generalized Jacobian. We offer a sufficient condition guaranteeing that backpropagation consistently yields an element of the generalized Jacobian, allowing us to address a wide range of functions. Our study demonstrates the difficulty of approximating the Lipschitz constants of ReLU networks and proposes an algorithm to accurately compute these values. By employing this algorithm, we assess the accuracy of various Lipschitz estimators and examine the impact of regularized training on the Lipschitz constant.