Randomized smoothing is a technique that provides reliable protection for high-dimensional classification problems. It determines the certainty of a classifier's prediction by considering the probabilities of the top two most likely classes around a given input point. However, existing smoothing methods lack information about the confidence level of the underlying classifier, such as a deep neural network. To address this limitation, we propose a method to generate certified radii that measure the prediction confidence of the smoothed classifier. We evaluate two measures of confidence: the average prediction score of a class and the margin between the average prediction scores of different classes. By modifying the Neyman-Pearson lemma, a fundamental theorem in randomized smoothing, we develop a procedure to compute the certified radius while ensuring that the confidence level remains above a specified threshold. Our experiments on CIFAR-10 and ImageNet datasets demonstrate that incorporating information about the confidence scores significantly improves the certified radius compared to disregarding it. This highlights the importance of leveraging additional information about the base classifier at the input point to enhance the reliability of the smoothed classifier. The code for our experiments is available at https://github.com/aounon/cdf-smoothing.