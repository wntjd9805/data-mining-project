AdaBelief is a new optimization method for deep learning that aims to combine the benefits of adaptive methods and stochastic gradient descent (SGD). While adaptive methods like Adam converge faster, they often struggle with generalization compared to SGD. In contrast, AdaBelief seeks to achieve fast convergence, good generalization, and training stability simultaneously. It does this by adjusting the stepsize based on the "belief" in the current gradient direction. If the observed gradient deviates significantly from the predicted gradient, a small step is taken, indicating distrust. Conversely, if the observed gradient is close to the prediction, a large step is taken, indicating trust. Extensive experiments demonstrate that AdaBelief outperforms other methods in terms of fast convergence and high accuracy in image classification and language modeling tasks. In fact, on ImageNet, AdaBelief achieves accuracy comparable to SGD. Additionally, in GAN training on Cifar10, AdaBelief exhibits high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. The code for AdaBelief is available at https://github.com/juntang-zhuang/Adabelief-Optimizer.