This paper introduces ShiftAddNet, a new type of deep neural network that replaces multiplications with additions and logical bit-shifts. This hardware-inspired approach significantly reduces the resource costs associated with multiplication, making it suitable for deployment on resource-constrained edge devices. ShiftAddNet maintains the expressive capacity of standard deep neural networks while enabling finer control over the model's learning capacity. It also improves robustness to quantization and pruning. The authors conducted extensive experiments and ablation studies, supported by their FPGA-based implementation and energy measurements. ShiftAddNet achieves over 80% reduction in hardware-quantified energy cost compared to existing DNNs, without sacrificing accuracy. The code and pre-trained models are available at https://github.com/RICE-EIC/ShiftAddNet.