Recent advancements in self-supervised representation learning have surpassed supervised learning methods in tasks such as object detection and image classification. It is intriguing that these improvements come from training instance classification models, where each image and its augmented versions are treated as samples of a single class. In this study, we conducted quantitative experiments to unravel the reasons behind these performance gains. Our findings reveal that approaches like MOCO and PIRL learn representations that are robust to occlusion but lack viewpoint and category instance invariance, which are essential for object recognition. Furthermore, we demonstrate that these approaches benefit from using a clean object-centric training dataset like Imagenet. To enhance viewpoint invariance, we propose leveraging unstructured videos to learn representations. Our results demonstrate that these learned representations outperform MOCOv2, trained on the same data, in terms of encoded invariances and performance on downstream tasks such as image classification and semantic segmentation.