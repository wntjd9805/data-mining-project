Curriculum learning is a technique that improves model performance by presenting easy samples before difficult ones during training. Recent studies have shown that this can be done dynamically and in a self-supervised manner. However, existing approaches are limited to specific tasks and require extra data annotations, layers, parameters, and dedicated training procedures. In contrast, we propose a simple and generic method called SuperLoss. This method appends a novel loss function to any existing task loss, automatically downweighting the contribution of hard samples. This mimics the core principle of curriculum learning and prevents the memorization of noisy samples. Our experimental results across various tasks demonstrate consistent improvements, particularly in the presence of noise.