A common method for training encoders in lossy compression is to use additive uniform noise, which approximates test-time quantization. However, we propose using universal quantization to implement a uniform noise channel during test time, eliminating the discrepancy between training and testing while maintaining differentiability. Although implementing the uniform noise channel is a specific case of the broader challenge of communicating a sample, we prove that it is computationally difficult without assumptions about its distribution. Nonetheless, the uniform special case is both efficient and easy to implement, making it highly practical. Additionally, we demonstrate that quantization can be achieved as a limit of a soft quantizer applied to the uniform noise channel, connecting compression with and without quantization.