Current neural networks often struggle with noisy labels in real-world datasets, leading to overfitting. Although progress has been made, existing techniques lack theoretical guarantees for the performance of neural networks trained with noisy labels. Our proposed approach offers a novel solution with strong theoretical guarantees for robust training of deep networks with noisy labels. Our method involves selecting weighted subsets of clean data points, known as coresets, which result in a low-rank Jacobian matrix. We provide proof that applying gradient descent to these subsets avoids overfitting the noisy labels. Extensive experiments validate our theory, showing that deep networks trained on our subsets achieve significantly better performance compared to state-of-the-art approaches. For instance, on CIFAR-10 with 80% noisy labels, our method improves accuracy by 6%, and on mini Webvision1, it improves accuracy by 7%.