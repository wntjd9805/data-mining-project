We propose a method for estimating models that are described using conditional moment restrictions, specifically focusing on non-parametric instrumental variable regression. Our approach involves formulating the estimation problem as a zero-sum game between a modeler and an adversary, where the modeler optimizes over the hypothesis space of the target model and the adversary identifies violating moments over a test function space. We analyze the statistical estimation rate of our estimator for any hypothesis space, considering the mean squared error metric for ill-posed inverse problems. By regularizing the minimax criterion with a second moment penalty and using a rich test function space, we demonstrate that the estimation rate scales with the critical radius of the hypothesis and test function spaces, which typically provides accurate and fast rates. To support our findings, we introduce a novel localized Rademacher analysis of statistical learning problems defined by minimax objectives. Additionally, we apply our main results to various hypothesis spaces commonly used in practice, such as reproducing kernel Hilbert spaces, high dimensional sparse linear functions, shape-constrained spaces, ensemble estimators like random forests, and neural networks. For each application, we provide computationally efficient optimization methods for solving the corresponding minimax problem. In some cases, we show that our modified mean squared error rate, combined with conditions that restrict the ill-posedness of the inverse problem, leads to accurate mean squared error rates. Lastly, we present extensive experimental analysis of our proposed methods.