We propose a new algorithm to solve a specific type of minimax problem that arises in machine learning and robust optimization. This problem is challenging due to its nonsmooth, nonconvex, nonlinear, and non-separable nature. Our approach combines recent techniques such as smoothing and biased variance reduction. Our algorithm and its variations have several advantages over existing methods, including simplicity of implementation, minimal parameter tuning, and compatibility with different types of derivative estimators and step sizes. We demonstrate the effectiveness of our algorithms through numerical examples, including a challenging minimax model. Our algorithms achieve a convergence rate of O(T âˆ’2/3) and have the best known oracle complexity under standard assumptions, where T is the iteration counter.