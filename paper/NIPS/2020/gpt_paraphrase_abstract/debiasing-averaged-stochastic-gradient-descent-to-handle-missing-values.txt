We present an averaged stochastic gradient algorithm for linear models that can handle missing values in large-scale learning scenarios. This algorithm does not require any modeling of the data distribution and can accommodate varying proportions of missing values. In both streaming and finite-sample settings, we demonstrate that the algorithm achieves a convergence rate of O(1/n) at iteration n, which is equivalent to the convergence rate without missing values. We validate the effectiveness of the algorithm using synthetic data as well as real datasets, including medical register data.