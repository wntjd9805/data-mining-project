Autoregressive models utilize the chain rule to define a joint probability distribution by multiplying conditionals. However, the need for normalization of these conditionals limits the functional families that can be used. To enhance flexibility, we propose autoregressive conditional score models (AR-CSM), where the joint distribution is parameterized using univariate log-conditionals' derivatives (scores), which do not require normalization. To train the AR-CSM, we introduce a new divergence measure called Composite Score Matching (CSM) that efficiently computes and optimizes the divergence between data and model distributions without the need for costly sampling or adversarial training. Our method outperforms previous score matching algorithms in terms of scalability to high-dimensional data and stability of optimization. Through extensive experiments, we demonstrate its applicability to density estimation, image generation, image denoising, and training latent variable models with implicit encoders.