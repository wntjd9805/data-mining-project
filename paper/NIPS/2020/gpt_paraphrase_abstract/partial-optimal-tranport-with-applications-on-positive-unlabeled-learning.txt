This paper introduces new approaches to address the limitations of the classical optimal transport problem. While the classical problem requires equal masses between two probability distributions, this may not be suitable for certain applications like color or shape matching, where the distributions may have arbitrary masses or only a fraction of the total mass needs to be transported. The authors propose the partial Wasserstein and Gromov-Wasserstein problems as alternative formulations and present exact algorithms to solve them. The effectiveness of these new metrics is demonstrated in a positive-unlabeled (PU) learning application, which is the first known application of optimal transport in this context. The authors show that partial Wasserstein-based metrics are effective in typical PU learning scenarios, and partial Gromov-Wasserstein metrics are efficient when dealing with samples from different domains or with different features in the positive and unlabeled datasets.