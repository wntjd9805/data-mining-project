The abstract discusses the phenomenon of similar feature distributions in deep neural networks (DNNs) with different parameter initialization. The authors introduce a technique called neural network grafting, which maintains the similarity of feature distributions during the training process. They then explain this phenomenon by analyzing the loss landscape of an overparameterized convolutional neural network (CNN) in the continuous limit. Despite the non-convex nature of the overparameterized CNN, the authors demonstrate that it can be reformulated as a convex function when considering the feature distributions in the hidden layers. Reparameterizing the neural networks in terms of feature distributions simplifies the characterization of the landscape. The authors also argue that training based on network parameters results in a fixed trajectory in the feature distributions.