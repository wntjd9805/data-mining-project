Recent advancements in image generation have been made through the use of style-based image generators. These approaches aim to disentangle latent factors in different image scales and encode them as "style" to control image synthesis. However, current methods struggle to further disentangle fine-grained semantics conveyed through feature channels. In this study, we propose a new image synthesis approach called SariGAN, which learns the relative importance of feature channels based on their semantic meaning in Generative Adversarial Networks (GANs). Our model achieves this by fusing latent codes and feature channels in a channel-/group-wise manner, allowing for the disentanglement of latent factors according to the semantics of each channel. Additionally, we introduce adaptive group-wise normalization (AdaGN) to independently control the styles of different channel groups. For instance, we can manipulate the mouth of a human face while keeping other facial features unchanged. To optimize our model, we employ adversarial training, a channel grouping loss, and a mutual information loss. This not only enables high-fidelity image synthesis but also enhances interpretability. Extensive experiments demonstrate that our approach surpasses state-of-the-art style-based methods in both unconditional image generation and conditional image inpainting tasks.