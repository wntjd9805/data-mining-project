Sampling probability distributions specified up to a normalization constant is a significant issue in machine learning and statistical mechanics. While traditional stochastic sampling methods like Markov Chain Monte Carlo (MCMC) or Langevin Dynamics (LD) can be slow due to mixing times, there is a growing interest in using normalizing flows to learn the transformation from a simple prior distribution to the desired target distribution. In this study, we propose Stochastic Normalizing Flows (SNF), which is a combined approach that involves a sequence of deterministic invertible functions and stochastic sampling blocks to sample target densities. We demonstrate that incorporating stochasticity overcomes the limitations of normalizing flows caused by the invertibility constraint, while trainable transformations between sampling steps enhance the efficiency of pure MCMC/LD throughout the flow. Inspired by non-equilibrium statistical mechanics, we develop an efficient training procedure that optimizes both the sampler's and the flow's parameters end-to-end. This procedure allows us to compute exact importance weights without the need to marginalize the randomness of the stochastic blocks. We showcase the representational power, sampling efficiency, and asymptotic correctness of SNFs on various benchmarks, including their application in sampling molecular systems in equilibrium.