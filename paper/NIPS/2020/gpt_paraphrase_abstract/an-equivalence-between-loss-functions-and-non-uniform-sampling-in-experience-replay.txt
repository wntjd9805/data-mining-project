The technique of Prioritized Experience Replay (PER) in deep reinforcement learning involves agents learning from transitions sampled with varying probabilities based on their temporal-difference error. We discovered that any loss function evaluated using non-uniformly sampled data can be transformed into a uniformly sampled loss function with the same expected gradient. Surprisingly, in certain environments, we found that PER can be entirely replaced by this new loss function without affecting empirical performance. This finding suggests a new avenue for improving PER by correcting its uniformly sampled loss function equivalent. We validated the effectiveness of our proposed modifications to PER and the equivalent loss function in multiple MuJoCo and Atari environments.