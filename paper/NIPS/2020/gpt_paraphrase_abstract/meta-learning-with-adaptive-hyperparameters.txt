Few-shot learning aims to quickly learn and generalize from limited examples. Meta-learning algorithms, like model-agnostic meta-learning (MAML), learn to learn new tasks efficiently. However, recent studies have questioned the effectiveness of MAML's initialization when test tasks differ from training tasks. Instead of improving the initialization, we focus on enhancing the inner-loop optimization, or fast adaptation, within the MAML framework. We propose a new weight update rule that improves the fast adaptation process by introducing a small meta-network that generates adaptive per-step hyperparameters, such as learning rate and weight decay coefficients. Our experimental results demonstrate that Adaptive Learning of hyperparameters for Fast Adaptation (ALFA) is an equally important factor often overlooked in recent few-shot learning approaches. Surprisingly, ALFA combined with random initialization already outperforms MAML.