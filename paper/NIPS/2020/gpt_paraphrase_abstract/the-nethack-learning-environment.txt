The development of challenging environments in reinforcement learning (RL) algorithms is crucial for progress in the field. However, existing RL environments are either complex or fast to simulate, but rarely both. In this study, we introduce the NetHack Learning Environment (NLE), which is a scalable, procedurally generated, stochastic, rich, and challenging environment for RL research. NLE is based on the popular single-player terminal-based roguelike game, NetHack. We argue that NetHack offers sufficient complexity for long-term research on exploration, planning, skill acquisition, and language-conditioned RL, while reducing computational resources needed for gathering experience. We compare NLE to other alternatives and explain why it is an ideal platform for testing the robustness and systematic generalization of RL agents. We demonstrate the early success of using a distributed Deep RL baseline and Random Network Distillation exploration in the NLE, along with qualitative analysis of various trained agents. NLE is open source and accessible at https://github.com/facebookresearch/nle.