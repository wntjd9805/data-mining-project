Learning object-centric representations of multi-object scenes is a promising approach in machine intelligence, enabling high-level reasoning and control using visual sensory data. However, current methods for unsupervised object-centric scene representation lack the ability to combine information from multiple observations of a scene. These methods rely solely on a single 2D observation, leading to inaccuracies and spatial ambiguities. To overcome this limitation, we propose MulMON, a Multi-View and Multi-Object Network. MulMON learns accurate and object-centric representations of multi-object scenes by leveraging multiple views. It addresses the challenge of maintaining object correspondences across views by iteratively updating the latent object representations. To ensure a comprehensive understanding of the 3D scene, MulMON is trained to predict the appearance of the scene from novel viewpoints. Experimental results demonstrate that MulMON outperforms single-view methods by resolving spatial ambiguities, learning more accurate and disentangled object representations, and predicting object segmentations for novel viewpoints.