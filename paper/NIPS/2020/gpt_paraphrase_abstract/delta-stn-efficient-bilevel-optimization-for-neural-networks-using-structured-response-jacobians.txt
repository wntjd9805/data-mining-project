Hyperparameter optimization of neural networks can be effectively addressed as a bilevel optimization problem. In the field of neural network optimization, implicit differentiation and unrolling have been widely used, but recently hypernetworks like Self-Tuning Networks (STNs) have gained popularity due to their ability to optimize the inner objective in a more efficient manner. However, this paper identifies certain issues in the training process of STNs and proposes a new hypernetwork architecture called ∆-STN to overcome these problems. The main idea behind ∆-STN is to accurately approximate the Jacobian of the best-response instead of the entire best-response function. This is achieved by reparameterizing the hypernetwork and linearizing the network around the current parameters. Experimental results demonstrate that ∆-STN performs better than existing methods in terms of tuning regularization hyperparameters, including weight decay, dropout, and the number of cutout holes. It achieves higher accuracy, faster convergence, and improved stability.