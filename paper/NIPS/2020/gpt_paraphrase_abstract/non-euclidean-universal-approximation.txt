The impact of modifying a neural network's input and output layers on its approximation capabilities is not well understood. This study presents general conditions that describe the preservation of a neural network's ability to approximate any continuous functions uniformly on compacts when modifying its feature and readout maps. The findings show that modifying the final layer of a universal approximation architecture to produce binary values creates a new architecture capable of deterministically approximating any classifier. Additionally, the study extends the results to deep convolutional neural networks and deep feed-forward networks, as well as within the context of geometric deep learning. The results also allow for the extension of commonly used non-Euclidean regression models and hyperbolic feed-forward networks to universal deep neural networks. The study also demonstrates that randomizing all but the last two layers of a deep neural network produces a universal family of functions with high probability. Furthermore, conditions are provided for the connections and activation functions of a deep neural network's first and last layers to have widths equal to the input and output space's dimensions without negatively impacting the architecture's approximation capabilities.