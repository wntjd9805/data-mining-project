Batch reinforcement learning (RL) presents challenges in producing reliable new policies in large domains. The problem arises when the new decision policy explores states and actions not covered by the batch data, leading to overly optimistic estimates of future performance. Some recent approaches have attempted to address this issue but still suffer from optimistic outcomes. Theoretical work that guarantees policy performance relies on a concentrability assumption, which is not suitable when there is a large difference between the state-action distributions of the behavior policy and candidate policies. This is because the error bound in traditional analysis scales with this ratio. To overcome this limitation, we propose using pessimistic value estimates in Bellman optimality and evaluation back-up in low-data regions. This approach provides more adaptive and stronger guarantees even when the concentrability assumption does not hold. In certain scenarios, our method can find the best policy within the explored state-action space without requiring prior assumptions of concentrability. We demonstrate the necessity of our pessimistic update and compare our algorithm with other state-of-the-art batch RL methods using standard benchmarks.