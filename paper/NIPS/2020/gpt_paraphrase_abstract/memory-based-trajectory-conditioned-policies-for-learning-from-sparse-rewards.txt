Reinforcement learning becomes challenging when rewards are sparse, as agents rarely receive non-zero rewards. This makes gradient-based optimization of parameterized policies slow and incremental. Previous research has shown that using a memory buffer of successful trajectories can lead to more effective policies. However, existing methods may rely too heavily on past successful experiences, resulting in sub-optimal and short-sighted behaviors. In this study, we propose a different approach that focuses on learning a trajectory-conditioned policy and expanding diverse past trajectories from the memory buffer. Our method enables the agent to explore diverse regions in the state space and improve upon previous trajectories to reach new states. Through empirical evaluation, we demonstrate that our approach outperforms count-based exploration methods and self-imitation learning on complex tasks with local optima. Notably, we achieve state-of-the-art scores on challenging Atari games like Montezuma's Revenge and Pitfall, without expert demonstrations or arbitrary state resetting, within a frame limit of five billion.