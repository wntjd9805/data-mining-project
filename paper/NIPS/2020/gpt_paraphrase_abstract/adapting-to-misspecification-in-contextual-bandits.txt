Researchers in the field of contextual bandits are working on developing algorithms that are both computationally efficient and capable of flexible, general-purpose function approximation. While reward-based algorithms have shown strong performance, they often rely on well-specified models and can fail when these models are not accurate. The question is whether it is possible to design algorithms that are efficient and flexible, yet still perform well even when the model is not perfectly specified. This study introduces a new family of algorithms for ε-misspecified contextual bandits that can adapt to unknown model misspecification, regardless of whether the action settings are finite or infinite. By utilizing an online oracle for square loss regression, the algorithm achieves optimal regret and dependence on the misspecification level without any prior knowledge. Specifically, in the case of linear contextual bandits with infinite actions in d dimensions, this algorithm achieves the first optimal regret bound of ˜O(d dT) for unknown ε. These results are made possible by a new optimization-based perspective on the regression oracle reduction framework, which is expected to have broader applicability.