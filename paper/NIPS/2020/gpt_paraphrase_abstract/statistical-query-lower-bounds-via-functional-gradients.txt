We present the first lower bounds for statistically querying the agnostic learning of non-polynomial activation functions (e.g., ReLU, sigmoid, sign) with Gaussian marginals. Specifically, for the problem of agnostic learning a ReLU regression, we show that any statistical-query algorithm with a tolerance of nâˆ’(1/(cid:15))b (cid:15) queries, where n is the dimension and (cid:15) is the accuracy parameter, is ruled out. This is significant as it eliminates general statistical-query learning algorithms for real-valued learning problems, rather than just correlational ones. Our approach involves utilizing gradient boosting to enhance recent lower bounds on the statistical-query dimension of functions computed by two-layer neural networks, as demonstrated by Diakonikolas et al. (COLT 2020) and Goel et al. (ICML 2020). A key element of our method is the incorporation of a nonstandard convex functional during the boosting procedure. Additionally, this work establishes a optimal reduction between two commonly studied learning models: agnostic learning and probabilistic concepts.