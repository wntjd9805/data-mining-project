Current semi-supervised learning (SSL) algorithms treat all unlabeled examples equally by using a single weight to balance the loss. However, not all unlabeled data have the same importance. This paper explores the use of different weights for each unlabeled example. Instead of manually adjusting these weights, we propose an algorithm based on the influence function, which measures a model's reliance on a specific training example. To enhance efficiency, we introduce a fast and effective approximation of the influence function. Our technique surpasses state-of-the-art methods in semi-supervised image and language classification tasks.