We examine a scenario where an agent has a limited amount of resources to allocate to multiple actions. Each action can only be done once and yields a reward with an unknown average value. The agent's objective is to maximize their overall reward. Strategies can be employed when additional information about the actions is available, such as covariates. Focusing on a nonparametric situation where the average reward depends on a one-dimensional covariate, we propose an optimal strategy. Under reasonable assumptions about the reward function, we demonstrate that the optimal regret scales as O(T 1/3) with poly-logarithmic factors when the resource budget T is proportional to the number of actions N. As T decreases compared to N, a smooth transition occurs where the regret increases to a rate of O(T 1/2) seen in continuum-armed bandits.