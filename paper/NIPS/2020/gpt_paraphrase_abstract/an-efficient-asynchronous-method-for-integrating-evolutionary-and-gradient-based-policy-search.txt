Deep reinforcement learning (DRL) and evolution strategies (ES) have both demonstrated high performance in various tasks. However, they possess opposite characteristics, with DRL being sample efficient but unstable, and ES being stable but lacking sample efficiency. Previous attempts to combine these algorithms relied on synchronous updates, limiting the benefits of parallelism in ES. To address this challenge, we propose an Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) approach that maximizes parallel efficiency and integrates ES with policy gradient methods. Our framework merges ES and DRL asynchronously and introduces various asynchronous update methods to leverage the advantages of asynchronism, ES, and DRL, including exploration and time efficiency, stability, and sample efficiency. Through evaluation in continuous control benchmark work, our proposed framework and update methods exhibit superior performance and time efficiency compared to previous approaches.