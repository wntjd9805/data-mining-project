We examine a natural model of online preference aggregation, where sets of preferred items are presented sequentially along with a demand for a certain number of items from each set. The learner aims to maintain a ranking that ensures a high placement of the demanded items in each set, without any prior knowledge of the sets or demands. This problem is relevant in various scenarios such as ordering products or news items on web pages based on user behavior. The Generalized Min-Sum-Set-Cover (GMSSC) problem, which is known to be NP-hard, serves as a formal representation of this setting. However, the standard approach of using no-regret online learning algorithms in this context is computationally inefficient as they operate in the space of rankings. In this study, we propose a polynomial-time solution to achieve low regret for GMSSC. By utilizing dimensionality reduction from rankings to doubly stochastic matrices, we apply Online Gradient Descent. To efficiently compute subgradients, we solve the dual of a configuration LP. Finally, we use oblivious deterministic and randomized rounding schemes to convert the doubly stochastic matrices back to rankings with minimal loss in the GMSSC objective.