Graph Neural Networks (GNNs) are vulnerable to perturbations that target the graph structure, and traditional defenses like adversarial training are ineffective in improving their robustness. This study aims to address this issue by considering adversarially injected edges as additional samples in a node's neighborhood aggregation function, causing distorted aggregations to accumulate over the layers. Conventional GNN aggregation functions, such as sum or mean, can be easily distorted by outliers. To tackle this, we propose a robust aggregation function inspired by robust statistics. Our approach, called Soft Medoid, has a breakdown point of 0.5, meaning that the aggregation bias remains bounded as long as the fraction of adversarial edges is below 50%. Soft Medoid is a fully differentiable generalization of the Medoid, making it suitable for end-to-end deep learning. Incorporating Soft Medoid into a GNN significantly enhances its robustness against structural perturbations, achieving a 3-fold improvement on Cora ML (and 5.5-fold on Citeseer) and an 8-fold improvement for low-degree nodes.