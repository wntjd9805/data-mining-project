Previous research on continual learning has primarily addressed the issue of catastrophic forgetting, where tasks are assumed to be dissimilar and possess little shared knowledge. Some efforts have also been made to transfer previously acquired knowledge to new tasks when the tasks are similar and possess shared knowledge. However, a technique that can handle a sequence of mixed similar and dissimilar tasks, addressing forgetting while also facilitating knowledge transfer, has not been proposed to our knowledge. This study introduces a novel approach to learn both types of tasks within a single network. For dissimilar tasks, the algorithm focuses on mitigating forgetting, while for similar tasks, it selectively transfers knowledge from previous similar tasks to enhance learning. The algorithm also automatically identifies if a new task is similar to any previous tasks. Experimental evaluation using mixed task sequences demonstrates the efficacy of the proposed model.