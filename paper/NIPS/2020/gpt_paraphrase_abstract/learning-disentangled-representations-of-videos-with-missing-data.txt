We propose DIVE, a deep generative model that addresses the challenges of missing data in learning representations of video sequences. DIVE introduces a missingness latent variable and disentangles hidden video representations into static and dynamic factors for each object. It imputes missing data by predicting the trajectory of each object. Experimental results on a moving MNIST dataset show that DIVE outperforms existing baselines significantly. We also evaluate DIVE on a real-world pedestrian dataset, demonstrating its practical value. The code and data for DIVE are available at https://github.com/Rose-STL-Lab/DIVE.