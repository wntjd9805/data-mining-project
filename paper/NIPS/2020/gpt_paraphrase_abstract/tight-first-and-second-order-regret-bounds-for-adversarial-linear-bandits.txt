We present new algorithms for adversarial linear bandits that have regret bounds related to both first and second order. These bounds indicate that our algorithms perform well when there is an action that results in a small cumulative loss or when the loss has a small variance. Furthermore, our algorithms require weaker assumptions compared to existing algorithms, as they can handle both discrete and continuous action sets without prior knowledge about losses. Additionally, they are efficient if a linear optimization oracle for the action set is available. We achieve these results by combining optimistic online optimization, continuous multiplicative weight update methods, and a technique called distribution truncation. We also demonstrate that the regret bounds of our algorithms are nearly optimal, with only a small increase in complexity.