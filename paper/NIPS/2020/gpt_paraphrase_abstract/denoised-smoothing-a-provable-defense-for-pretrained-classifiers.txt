We introduce a method to effectively protect any pretrained image classifier against adversarial attacks. This technique enables providers and users of public vision APIs to transform non-robust classification services into provably robust ones. By combining a custom-trained denoiser with an off-the-shelf image classifier and utilizing randomized smoothing, we create a new classifier that is guaranteed to be robust against adversarial examples. Importantly, this method does not require modifying the pretrained classifier and can be applied in both white-box and black-box settings. We refer to this defense strategy as "denoised smoothing" and demonstrate its effectiveness through extensive experiments on ImageNet and CIFAR-10 datasets. Additionally, we successfully apply our approach to defend the image classification APIs of Azure, Google, AWS, and ClarifAI. The code for replicating all the experiments mentioned in this paper can be accessed at: https://github.com/microsoft/denoised-smoothing.