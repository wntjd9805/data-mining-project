Machine learning transparency requires interpretable explanations of how inputs are related to predictions. Feature attribution is a method to analyze the impact of features on predictions. Feature interactions refer to the contextual dependence between features that jointly affect predictions. Various methods exist to extract feature interactions in prediction models, but the methods that assign attributions to interactions are either uninterpretable, specific to certain models, or not based on fundamental principles. To address these issues and ensure scalability in real-world scenarios, we propose a framework called Archipelago for interaction attribution and detection. Our experiments using standard annotation labels demonstrate that our approach offers significantly more interpretable explanations compared to similar methods. This is crucial for understanding the impact of interactions on predictions. Additionally, we provide visualizations that provide new insights into deep neural networks.