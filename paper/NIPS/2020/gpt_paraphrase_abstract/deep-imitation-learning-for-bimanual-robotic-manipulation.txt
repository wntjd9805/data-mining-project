We introduce a novel deep imitation learning approach for robotic bimanual manipulation in a continuous state-action space. The main challenge lies in adapting manipulation skills to objects located in various positions. Our hypothesis is that incorporating relational information about the environment can greatly enhance generalization. To address this, we propose a three-step process: (i) breaking down the multi-modal dynamics into elemental movement primitives, (ii) using a recurrent graph neural network to parameterize each primitive and capture interactions, and (iii) combining a high-level planner for sequential composition of primitives with a low-level controller that merges primitive dynamics and inverse kinematics control. Our model is a deep, hierarchical, modular architecture. Compared to existing methods, our model demonstrates superior generalization capabilities and achieves higher success rates across multiple simulated bimanual robotic manipulation tasks. The code for simulation, data, and models can be accessed at: https://github.com/Rose-STL-Lab/HDR-IL.