This paper explores the concept of compositionality in biological and artificial neural networks. It discusses the challenges of learning compositional functions through gradient descent, such as vanishing and exploding gradients, which require careful learning rate tuning. The paper presents a proof that multiplicative weight updates can effectively address these challenges by satisfying a descent lemma specifically designed for compositional functions. Building on this lemma, the authors introduce Madam, a multiplicative variant of the Adam optimizer, which can train advanced neural network architectures without the need for learning rate tuning. The authors also demonstrate that Madam can be easily adapted to train compressed neural networks by utilizing a logarithmic number system to represent weights. Additionally, the paper highlights the connection between multiplicative weight updates and recent discoveries in biological synapses.