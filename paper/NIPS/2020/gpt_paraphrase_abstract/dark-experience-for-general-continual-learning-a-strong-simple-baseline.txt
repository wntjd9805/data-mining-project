Many current approaches to Continual Learning overlook the challenges of real-world scenarios where data streams cannot be structured as a sequence of tasks and offline training is not possible. This study focuses on General Continual Learning (GCL), which deals with scenarios where task boundaries are blurred and domain and class distributions shift gradually or suddenly. The authors propose a method called Dark Experience Replay, which combines rehearsal, knowledge distillation, and regularization. This method matches the network's past logits sampled during optimization, promoting consistency with previous knowledge. The authors conduct a comprehensive analysis on standard benchmarks and a new GCL evaluation setting (MNIST-360), demonstrating that this simple baseline outperforms established approaches and optimizes limited resources. They also explore the generalization capabilities of their objective, revealing the benefits of regularization beyond performance. The code for their method is available at the provided GitHub link.