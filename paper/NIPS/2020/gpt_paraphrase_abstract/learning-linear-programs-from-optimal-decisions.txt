We present a versatile approach using gradients to learn linear programs from optimal decisions. Typically, linear programs are manually specified based on prior knowledge of costs and constraints. However, in certain cases, it is necessary to learn these programs from observed optimal decisions. This presents a challenging problem known as bilevel learning, which has been previously explored in specific cases. In our study, we address this problem in a general setting by jointly learning all parameters and allowing flexible parameterizations of costs, constraints, and loss functions. We also tackle specific challenges related to learning linear programs, such as empty feasible regions and non-unique optimal decisions. Our experiments demonstrate the success of our method in learning synthetic linear programs and minimum-cost multi-commodity flow instances, where existing methods are not directly applicable. Additionally, we provide a fast batch-mode PyTorch implementation of the homogeneous interior point algorithm, which enables gradient-based learning using implicit differentiation or backpropagation.