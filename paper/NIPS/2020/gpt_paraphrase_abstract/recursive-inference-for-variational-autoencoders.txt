This study introduces a new algorithm for accurate amortized inference in Variational Autoencoders (VAEs). Traditional VAEs have relatively inaccurate posterior approximations due to their amortized inference networks. Semi-amortized approaches have been proposed to address this issue, but their iterative gradient update procedures are computationally demanding. To overcome these challenges, the authors propose a recursive mixture estimation algorithm for VAEs. This algorithm iteratively adds new components to the mixture in order to minimize the divergence between the variational and true posteriors. The authors use a functional gradient approach to determine the new mixture component, considering both improvement in data likelihood and increased representational diversity. Compared to boosted variational inference (BVI), which relies on non-amortized single optimization instance, the proposed method uses amortized inference. An advantage of this approach is that it only requires a single feed-forward pass through the mixture inference network during testing, making it significantly faster than semi-amortized approaches. Experimental results demonstrate that the proposed method achieves higher test data likelihood than existing methods on multiple benchmark datasets.