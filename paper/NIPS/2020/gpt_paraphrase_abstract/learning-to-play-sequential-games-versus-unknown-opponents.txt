We aim to develop strategies for a learner to effectively engage with an opponent in a repeated sequential game. Unlike previous methods that assume knowledge of the opponent's model, we focus on situations where the opponent's model is unknown. To tackle this, we utilize kernel-based regularity assumptions to understand and exploit the opponent's response structure. Our proposed algorithm combines concepts from bilevel optimization and online learning to strike a balance between exploration (learning about the opponent's model) and exploitation (selecting actions with high rewards for the learner) when facing an adversarial sequence of opponents. Our algorithm guarantees limited regret, which depends on the regularity of the opponent's response, and the regret scales sublinearly with the number of game rounds. We also apply our approach to repeated Stackelberg games and demonstrate its effectiveness through experiments in traffic routing and wildlife conservation tasks.