Recent research has shown that multilingual pretrained language models have the ability to align different languages. This study explores a new approach called CRISS, which improves cross-lingual alignment by training seq2seq models on sentence pairs mined from their own encoder outputs. By applying mining and training processes iteratively, CRISS enhances both cross-lingual alignment and translation ability. Using this method, the researchers achieved state-of-the-art unsupervised machine translation results in 9 language directions, with an average improvement of 2.4 BLEU. Additionally, CRISS showed a significant improvement of 21.5% in absolute accuracy on the Tatoeba sentence retrieval task in the XTREME benchmark across 16 languages. When compared to mBART, CRISS also achieved an average improvement of 1.8 BLEU in supervised machine translation downstream tasks. The code and pretrained models used in this study are publicly available.