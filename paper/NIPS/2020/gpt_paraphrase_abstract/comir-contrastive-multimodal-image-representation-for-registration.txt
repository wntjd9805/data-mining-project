We propose a method called contrastive coding, which uses CoMIRs (Contrastive Multimodal Image Representations) to learn shared, dense image representations. CoMIRs address the problem of registering multimodal images, where existing methods struggle due to insufficiently similar image structures. By reducing the multimodal registration problem to a monomodal one, CoMIRs allow the application of general intensity-based and feature-based registration algorithms. Our approach involves training a neural network per modality on aligned images using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods used for classification, our approach generates image-like representations that capture the shared information between modalities. We introduce a modification to InfoNCE that enforces rotational equivariance of the learned representations, which is crucial for the registration task. We evaluate the rotational equivariance and stability of the representations using a remote sensing dataset and assess the performance of the learned representations by registering a biomedical dataset of bright-field and second-harmonic generation microscopy images. Our proposed approach using CoMIRs outperforms GAN-based image-to-image translation and a state-of-the-art application-specific method. The code for our approach is available at the provided GitHub link. The research was presented at the 34th Conference on Neural Information Processing Systems (NeurIPS 2020) in Vancouver, Canada.