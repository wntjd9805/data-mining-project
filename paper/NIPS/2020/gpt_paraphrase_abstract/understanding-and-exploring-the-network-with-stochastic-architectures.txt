The use of stochastic architectures in training neural networks is a growing trend, allowing for different architectures to be used during inference. However, current research on this topic is closely tied to neural architecture search (NAS), which limits its applicability. In this study, we separate the training of a network with stochastic architectures (NSA) from NAS and conduct a comprehensive investigation on it as a standalone problem. We examine various aspects of NSA, including training stability, convergence, predictive behavior, and generalization to unseen architectures. We identify issues with NSA, such as training/test differences and function mode collapse, and propose solutions based on theory and empirical insights. These findings can also be used as guidelines for NAS. Building on this understanding, we apply the improved NSA to different scenarios, such as model ensemble, uncertainty estimation, and semi-supervised learning. The results demonstrate remarkable performance, with an error rate of 2.75% and an expected calibration error of 0.0032 on CIFAR-10. This validates the effectiveness of the model and opens up new possibilities for exploring the potential of networks with stochastic architectures, beyond NAS.