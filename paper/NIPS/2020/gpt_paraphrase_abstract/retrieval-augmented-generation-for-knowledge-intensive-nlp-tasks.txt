Large pre-trained language models have been proven to store factual knowledge in their parameters and achieve top-notch results when fine-tuned on downstream NLP tasks. However, their ability to access and manipulate knowledge precisely is still limited, causing them to underperform on knowledge-intensive tasks compared to task-specific architectures. Additionally, there is ongoing research on providing provenance for their decisions and updating their world knowledge. To address these challenges, we propose a general-purpose fine-tuning approach for retrieval-augmented generation (RAG) models that combine pre-trained parametric and non-parametric memory for language generation. In our RAG models, the parametric memory is a pre-trained seq2seq model, and the non-parametric memory is a dense vector index of Wikipedia, accessed using a pre-trained neural retriever. We compare two formulations of RAG models, one that conditions on the same retrieved passages throughout the generated sequence and another that can use different passages for each token. We fine-tune and evaluate our models on various knowledge-intensive NLP tasks and achieve state-of-the-art performance on three open domain question answering tasks, surpassing parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we observe that RAG models generate more specific, diverse, and factually accurate language compared to a state-of-the-art parametric-only seq2seq baseline.