Transformers, widely used in sequence modeling, face a limitation due to the quadratic complexity of computing their attention matrix for large sequences. To overcome this, we introduce clustered attention, which groups queries into clusters and computes attention only for the centroids. Additionally, we identify keys with the highest attention per query using the computed clusters and calculate exact key/query dot products. This approach reduces the complexity to linear with respect to the sequence length for a fixed number of clusters. Our evaluation on automatic speech recognition datasets reveals that our model consistently outperforms vanilla transformers within a given computational budget. Moreover, we demonstrate that our model can approximate complex attention distributions with minimal clusters, achieving comparable performance to a pretrained BERT model on GLUE and SQuAD benchmarks using only 25 clusters.