Boosting is a widely utilized machine learning technique for solving classification problems. It involves predicting the label of an example by using a group of weak classifiers. While boosting has been successful in many classification tasks with tabular data, it struggles with complex tasks involving low-level features like image classification. This is because boosting constructs an additive model of weak classifiers, each with limited predictive power. As a result, the resulting models are often insufficient to capture the intricate decision boundaries of real-world classification problems. In this study, we introduce a general framework for boosting that aims to enhance the performance of a weak learner and transform it into a strong learner. Unlike traditional boosting, our framework allows for more sophisticated methods of aggregating weak learners. Specifically, we focus on function composition as one form of aggregation. We demonstrate that several popular algorithms for learning deep neural networks can be derived from our framework by utilizing function compositions. Furthermore, we identify the limitations of these algorithms and propose new ones to address these issues. Through extensive empirical evaluation, we show that our learning algorithms outperform traditional additive boosting algorithms and existing greedy learning techniques for DNNs. A noteworthy aspect of our algorithms is that they offer strong theoretical guarantees.