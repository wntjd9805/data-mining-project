Many fairness criteria in machine learning aim to achieve equal outcomes for different protected groups, such as race or gender. However, practitioners often struggle with the challenge of unreliable or biased information about these groups. This study investigates the consequences of relying on inaccurate labels for protected groups and establishes an upper limit for fairness violations when the criteria are met based on these labels. To address this issue, two new robust optimization approaches are proposed, which not only consider the unreliable labels but also ensure fairness on the actual protected groups while minimizing training objectives. Theoretical guarantees are provided for one of these approaches, demonstrating its convergence to an optimal solution. Through two case studies, empirical evidence demonstrates that the robust approaches outperform the naive approach in achieving fairness for the true protected groups.