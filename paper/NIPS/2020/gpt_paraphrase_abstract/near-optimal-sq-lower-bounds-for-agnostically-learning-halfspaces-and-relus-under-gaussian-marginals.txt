We investigate the fundamental challenges of learning halfspaces and ReLUs with Gaussian marginals in an agnostic setting. In the first problem, we aim to generate a hypothesis that minimizes the 0-1 loss OPT + ε, where OPT represents the best-fitting halfspace, using labeled examples (x, y). The distribution of x follows a standard Gaussian, while the labels y can be arbitrary. Similarly, in the second problem, we aim to output a hypothesis that minimizes the square loss OPT + ε, where OPT represents the best-fitting ReLU. Again, the labeled examples (x, y) are obtained from an unknown distribution on Rd × R, with the marginal distribution on x being a standard Gaussian. We establish Statistical Query (SQ) lower bounds of dpoly(1/ε) for both problems. These lower bounds provide strong evidence that the current upper bounds for these tasks are essentially optimal.