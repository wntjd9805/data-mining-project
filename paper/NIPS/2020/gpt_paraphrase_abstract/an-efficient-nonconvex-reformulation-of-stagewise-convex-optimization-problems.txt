Convex optimization problems with a staged structure are common in various domains such as optimal control, deep neural network verification, and isotonic regression. Existing solvers for these problems may have scalability issues. To address this, we propose a nonconvex reformulation that takes advantage of the staged structure. Our reformulation involves simple bound constraints, allowing for solution using projected gradient methods and their accelerated variants. This method generates a sequence of primal and dual feasible solutions, making it easy to certify optimality. Theoretical analysis shows that our nonconvex formulation is mostly free of spurious local minima and achieves the same global optimum as the convex problem. We enhance the projected gradient descent algorithm to avoid spurious local minimizers, ensuring convergence to the global minimizer. In the context of neural network verification, our approach achieves small duality gaps within a few gradient steps. Consequently, it efficiently solves large-scale verification problems faster than both generic and specialized solvers.