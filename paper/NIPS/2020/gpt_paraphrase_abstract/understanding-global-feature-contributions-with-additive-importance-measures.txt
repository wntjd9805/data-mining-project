Understanding how complex machine learning models work has been a longstanding challenge, with recent research focusing on local interpretability. In order to evaluate the significance of individual input features on a global scale, we consider defining feature importance based on their predictive power. We introduce two concepts of predictive power (model-based and universal) and formalize this approach using a framework of additive importance measures, which encompasses various existing methods. Additionally, we propose SAGE, a model-agnostic technique that quantifies predictive power while taking into account feature interactions. Our experiments demonstrate that SAGE can be efficiently computed and provides more accurate importance values compared to other approaches.