A new approach called residual force control (RFC) is proposed to address the challenge of synthesizing realistic human behaviors using reinforcement learning. The main difficulty lies in the dynamics mismatch between the humanoid model and real humans. The RFC-based policy learns to apply external residual forces to the humanoid to compensate for this mismatch and better imitate reference motions. Experimental results show that the proposed approach outperforms existing methods in terms of convergence speed and the quality of learned motions. The approach is showcased through a physics-based virtual character performing agile ballet dance moves. Additionally, a dual-policy control framework is introduced, where a kinematic policy and an RFC-based policy work together to synthesize diverse long-term human motions without task guidance or user input. This is the first humanoid control method to successfully learn from a large-scale human motion dataset and generate a variety of long-term motions. The code and videos are available at the provided website.