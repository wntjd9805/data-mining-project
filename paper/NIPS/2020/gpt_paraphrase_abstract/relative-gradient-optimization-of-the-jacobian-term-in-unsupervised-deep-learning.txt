Learning expressive probabilistic models is a common challenge in machine learning. One popular method involves mapping observations to a representation space with a simple joint distribution, similar to nonlinear independent component analysis. Deep density models are often used for this task, but their maximum likelihood training requires estimating the log-determinant of the Jacobian, which is computationally expensive. This creates a trade-off between computation and expressive power.This study introduces a new approach to effectively train neural networks for this task. By utilizing relative gradients and exploiting the matrix structure of network parameters, updates can be computed efficiently even in high-dimensional spaces. Unlike naive approaches, the computational cost of training is quadratic in input size instead of cubic, enabling faster training with objective functions involving the log-determinant of the Jacobian. This approach does not impose constraints on the structure of the Jacobian, distinguishing it from autoregressive normalizing flows.