This paper explores new ways to improve contrastive learning using probabilistic modeling. We introduce a form of contrastive loss called Joint Contrastive Learning (JCL), which allows for simultaneous learning of multiple query-key pairs. This leads to stricter constraints when looking for invariant features. We provide an upper bound on this formulation, which enables analytical solutions in end-to-end training. While JCL is effective in computer vision applications, we also uncover the underlying mechanisms that govern its behavior. Our proposed formulation strongly emphasizes similarity within each class, making it advantageous for finding discriminative features among different instances. We evaluate these ideas on various benchmarks and observe significant improvements compared to existing algorithms. The code is publicly available at: https://github.com/caiqi/Joint-Contrastive-Learning.