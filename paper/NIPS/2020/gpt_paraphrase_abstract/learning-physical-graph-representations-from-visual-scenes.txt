Convolutional Neural Networks (CNNs) have been highly successful in learning representations for visual object categorization. However, they lack explicit encoding of objects, parts, and physical properties, limiting their performance on tasks that require structured understanding of visual scenes. To address this limitation, we propose the concept of "Physical Scene Graphs" (PSGs) which represent scenes as hierarchical graphs, where nodes correspond to object parts at different scales and edges denote physical connections between parts. Each node is associated with a vector of latent attributes representing object properties like shape and texture. We introduce PSGNet, a network architecture that learns to extract PSGs by reconstructing scenes through a PSG-structured bottleneck. PSGNet extends CNNs by incorporating recurrent feedback connections for combining low and high-level image information, graph pooling and vectorization operations to convert feature maps into object-centric graph structures, and perceptual grouping principles to facilitate the identification of meaningful scene elements. Experimental results demonstrate that PSGNet outperforms alternative self-supervised scene representation algorithms in scene segmentation tasks, particularly on complex real-world images, and generalizes well to unseen object types and scene arrangements. PSGNet also exhibits the ability to learn from physical motion, improving scene estimation even for static images. Ablation studies confirm the significance of each component of the PSGNet architecture, analyses verify that learned latent attributes capture intuitive scene properties, and the use of PSGs for compositional scene inference is illustrated.