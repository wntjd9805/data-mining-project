This abstract discusses reinforcement learning (RL) in episodic MDPs with adversarial full-information reward feedback and unknown fixed transition kernels. The authors propose two model-free policy optimization algorithms, POWER and POWER++, and establish guarantees for their dynamic regret. Dynamic regret, which considers the non-stationarity of environments, is a stronger notion than static regret. The proposed algorithms achieve dynamic regret that adapts to different levels of non-stationarity and matches the (near-)optimal static regret under slow-changing environments. The dynamic regret bound consists of two components: exploration, which addresses the uncertainty of transition kernels, and adaptation, which deals with non-stationary environments. POWER++ improves over POWER in the adaptation component by actively adjusting to non-stationarity through prediction. This work is the first to analyze dynamic regret in model-free RL algorithms in non-stationary environments.