BERT is a successful pre-training model that uses masked language modeling (MLM). However, it does not consider the dependency among predicted tokens. To address this issue, XLNet introduces permuted language modeling (PLM). However, XLNet suffers from a position discrepancy problem between pre-training and fine-tuning. To overcome these limitations, we propose MPNet, a novel pre-training method that combines the advantages of BERT and XLNet. MPNet uses permuted language modeling for leveraging token dependency and incorporates auxiliary position information to reduce the position discrepancy. We conducted experiments on various downstream tasks and found that MPNet significantly outperforms MLM and PLM. It also achieves better results compared to other state-of-the-art pre-trained models like BERT, XLNet, and RoBERTa. The code and pre-trained models of MPNet are available at the following link: https://github.com/microsoft/MPNet.