Learning in the brain involves changes in synaptic connections, which are triggered by error signals sent back from the output. Backpropagation, the gold standard for this process in artificial neural networks, is not biologically plausible. Many proposed biologically-plausible schemes rely on learning a separate set of feedback connections, making error assignment dependent on another learning problem. We present a method where feedforward network transformations can be inverted through dynamics, without the need for a separate learning problem. This dynamic inversion is achieved through the reuse of the forward transformation and interaction with fixed or random feedback. Accurate inversion is guaranteed through network dynamics. We apply this approach to generic feedforward networks and demonstrate good performance on various datasets. We also discuss the potential connection between dynamic inversion and second-order optimization. Our work offers an alternative perspective on credit assignment in the brain and highlights the importance of temporal dynamics and feedback control in learning.