Modifying the density of a generator in a Generative Adversarial Network (GAN) can improve its performance. By combining the log-density of the implicit generator with the logit score of the discriminator, an energy function is created that accurately represents the data density even when the generator is not perfect but the discriminator is optimal. This modified density can be used to generate samples by sampling in the latent space according to an energy-based model derived from the sum of the latent prior log-density and the discriminator output score. This approach, called Discriminator Driven Latent Sampling (DDLS), is more efficient than previous methods that operate in the high-dimensional pixel space. DDLS can be applied to enhance various types of pre-trained GANs. Experimental evaluation on both synthetic and real-world datasets demonstrates the effectiveness of DDLS. For example, applying DDLS to an off-the-shelf pre-trained SN-GAN results in a significant improvement in the InceptionScore from 8.22 to 9.09 on CIFAR-10, which is comparable to the performance of the class-conditional BigGAN model. This achievement sets a new state-of-the-art in unconditional image synthesis without the need for additional parameters or training.