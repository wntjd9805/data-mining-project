This study focuses on the challenge of balancing flexibility for learning and robustness for memory in biological systems and machine learning. The researchers propose a new learning rule for recurrent networks that minimizes interference between sequentially learned tasks. This rule preserves network dynamics within previously learned task subspaces while encouraging the exploration of orthogonal subspaces for new tasks. The approach successfully eliminates catastrophic interference and outperforms previous continual learning algorithms. The study also demonstrates that networks trained with this approach can reuse similar dynamical structures across similar tasks, enabling faster learning. Additionally, the researchers identify organizational differences that arise when training tasks sequentially compared to simultaneously.