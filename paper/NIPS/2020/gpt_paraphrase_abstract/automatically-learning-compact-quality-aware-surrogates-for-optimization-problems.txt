Optimization problems with unknown parameters often require a predictive model to estimate the values of these parameters. Recent research has shown that incorporating the optimization problem as a layer in the model training process leads to better decision quality. However, this approach is computationally expensive and can sometimes fail to improve solution quality due to non-smoothness issues. To overcome these challenges, we propose learning a low-dimensional surrogate model of the optimization problem by representing the feasible space using meta-variables. These meta-variables are linear combinations of the original variables. By training this surrogate model together with the predictive model, we achieve two benefits: i) significant reduction in training and inference time, and ii) improved performance by focusing on the more important variables and learning in a smoother space. We validate our approach on various tasks, including non-convex adversary modeling, submodular recommendation, and convex portfolio optimization.