This study examines the dynamics of wide shallow neural networks trained using gradient descent. The researchers focus on the asymptotic mean-field limit when the width of the network approaches infinity. They find that random initialization of the network parameters leads to deviations from the mean-field limit, but these deviations are influenced by correlations induced by gradient descent. The researchers derive a dynamical Central Limit Theorem (CLT) to show that the fluctuations around the mean limit remain bounded throughout training. This upper bound is determined by a Monte-Carlo resampling error, which depends on the 2-norm of the underlying measure and affects the generalization error. The study suggests using this 2-norm as a regularization term during training. Additionally, if the mean-field dynamics converge to a measure that interpolates the training data, the deviations eventually vanish in the CLT scaling. The researchers support their findings with numerical experiments.