This research focuses on studying bandit convex optimization methods that can adapt to the norm of the comparator. Previously, this topic has only been explored in the context of full-information scenarios. The objective is to develop convex bandit algorithms that have small regret bounds when the norm of the comparator is small. Initially, techniques from the full-information setting are employed to create comparator-adaptive algorithms for linear bandits. These ideas are then extended to convex bandits with Lipschitz or smooth loss functions by introducing a novel version of the standard single-point gradient estimator and using well-designed surrogate losses.