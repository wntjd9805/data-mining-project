Neural Ordinary Differential Equations (NODEs) are a novel type of models that use infinite-depth architectures to continuously transform data. They have proven to be effective in learning the dynamics of complex physical systems. However, most previous research has focused on first order ODEs, while many systems, especially in classical physics, are governed by second order laws. In this study, we introduce Second Order Neural ODEs (SONODEs) and extend the adjoint sensitivity method to handle them. We demonstrate that optimizing a first order coupled ODE is equivalent to optimizing a SONODE, but with improved computational efficiency. Additionally, we expand the theoretical understanding of Augmented NODEs (ANODEs) by showing that they can learn higher order dynamics using a minimal number of augmented dimensions, albeit sacrificing interpretability. This suggests that the advantages of ANODEs extend beyond the additional space provided by augmented dimensions. Finally, we compare SONODEs and ANODEs on both synthetic and real dynamical systems and find that SONODEs generally exhibit faster training and better performance, indicating their superior inductive biases.