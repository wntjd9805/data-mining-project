Recent research has made progress in black-box variational inference (VI), but the current understanding of automatic posterior inference is unclear. One advancement is the use of normalizing flows to create flexible posterior densities for deep latent variable models. Another approach involves integrating Monte-Carlo methods, which serve two purposes: improving variational objectives for optimization and expanding variational families through sampling. However, both flows and variational Monte-Carlo methods have not been extensively explored in black-box VI. Additionally, there is a lack of guidance in the literature regarding optimization considerations such as step-size scheme, parameter initialization, and choice of gradient estimators. This paper argues that black-box VI can be best addressed by carefully combining multiple algorithmic components. The authors evaluate optimization, flows, and Monte-Carlo methods on 30 models from the Stan model library and demonstrate that the combination of these components significantly improves "out of the box" variational inference.