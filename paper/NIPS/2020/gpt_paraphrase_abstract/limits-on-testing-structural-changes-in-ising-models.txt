We introduce new limits based on information theory for detecting sparse changes in Ising models. This problem is relevant in various applications where network changes can occur due to external stimuli. Our findings reveal that even in scenarios with local sparsity, the sample complexity for detecting sparse changes is no better than learning the entire model. This contradicts prior research that relied on sparse recovery methods, which suggested that the sample complexity only depends on the number of network changes. To gain insight into cases where change detection is easier than structured learning, we examine the testing of edge deletion in forest-structured graphs and high-temperature ferromagnets. Our analysis demonstrates that testing small changes is equally challenging, while testing large changes is significantly different from structure learning. Consequently, these results indicate that concepts like restricted strong convexity, typically employed for sparsity pattern recovery, may not be suitable for testing graphical models. Instead, algorithm development should focus on detecting substantial changes.