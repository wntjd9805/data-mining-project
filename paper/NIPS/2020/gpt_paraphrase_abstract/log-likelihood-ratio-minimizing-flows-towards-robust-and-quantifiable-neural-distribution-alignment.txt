Distribution alignment in deep learning has various applications such as domain adaptation and unsupervised image-to-image translation. Previous approaches to unsupervised distribution alignment either focus on minimizing non-parametric statistical distances or rely on adversarial alignment. However, the former fails to capture the complexities of real-world distributions, while the latter is challenging to train and lacks guaranteed convergence or validation procedures. To address these limitations, this paper proposes a novel distribution alignment method using a log-likelihood ratio statistic and normalizing flows. The combination of these techniques results in a deep neural likelihood-based minimization objective that achieves a lower bound upon convergence. Experimental results demonstrate that minimizing this objective preserves the local structure of input domains during domain alignment.