The hyperparameter optimization task can be solved by computing the gradient of model hyperparameters, also known as hypergradient. However, traditional gradient-based methods may not lead to optimal solutions due to the complex and non-convex nature of the hyperparameter space. To address this issue, we introduce a new algorithm called hyperparameter mutation (HPM). HPM incorporates a trade-off between global and local search by utilizing a population of student models that explore the hyperparameter space guided by hypergradient. Additionally, a teacher model is employed to mutate underperforming students by leveraging the top performers. The teacher model uses an attention mechanism to dynamically learn a mutation schedule for different hyperparameters. We provide empirical evidence using synthetic functions, demonstrating that HPM significantly outperforms hypergradient. Furthermore, experiments on benchmark datasets validate the effectiveness of HPM for training deep neural networks compared to several strong baselines.