Recognizing objects in various states is challenging due to their dramatic visual variations. However, contextual cues can provide strong indications of an object's appearance in a scene. Recognizing these cues not only improves object detection accuracy and understanding of object properties, but also helps infer human-object interactions. Manually labeling data for this task is impractical due to the extensive range of object appearances. Instead, we propose a framework called Contextualized OBject Embeddings (COBE) that learns from narrations of instructional videos. By training a visual detector to predict contextualized word embeddings of objects and their associated narrations, we create an object representation that relates concepts based on semantic language metrics. Our experiments demonstrate that the detector effectively predicts a wide range of contextual object information and performs well in few-shot and zero-shot learning scenarios.