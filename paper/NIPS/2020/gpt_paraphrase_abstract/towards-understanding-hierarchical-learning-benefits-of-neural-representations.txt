Deep neural networks have demonstrated efficient hierarchical learning by learning useful representations of data through layers. However, recent theories that relate these representations to "shallow learners" like kernels do not explain how they are utilized. This study shows that intermediate neural representations offer more flexibility and advantages over raw inputs. By treating a randomly initialized neural network as a representation function fed into another trainable network, we find that neural representations can achieve improved sample complexities compared to raw inputs. For learning a low-rank degree-p polynomial (where p is greater than 4), neural representation requires fewer samples than raw input. We contrast this result with a lower bound that shows neural representations do not improve on raw input in the infinite width limit when the trainable network is a neural tangent kernel. These findings provide insight into when neural representations are beneficial and offer a new perspective on the importance of depth in deep learning.