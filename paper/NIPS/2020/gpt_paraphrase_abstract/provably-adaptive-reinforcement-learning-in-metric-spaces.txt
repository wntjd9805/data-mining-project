We examine reinforcement learning in continuous state and action spaces with a metric. We conduct a detailed analysis of the Sinclair, Banerjee, and Yu algorithm (2019) and demonstrate that its regret is determined by the zooming dimension of the problem. This dimension, derived from bandit studies, measures the size of subsets containing near optimal actions and is consistently smaller than the covering dimension employed in earlier analyses. Consequently, our findings represent the first verifiable adaptive assurances for reinforcement learning in metric spaces.