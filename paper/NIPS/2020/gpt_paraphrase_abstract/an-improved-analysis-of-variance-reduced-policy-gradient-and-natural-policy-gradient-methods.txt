This paper focuses on enhancing the convergence of policy gradient (PG), natural PG (NPG) methods, and their variance-reduced variants, using general smooth policy parametrizations. The authors demonstrate that a leading variance-reduced PG method, previously only proven to converge to stationary points, can actually converge to the globally optimal value, accounting for policy parametrization errors. Furthermore, they establish that NPG has a lower sample complexity. The authors propose a novel approach called SRVR-NPG, which integrates variance-reduction into the NPG update. Their findings reveal that the convergence of (variance-reduced) PG and NPG methods can mutually enhance each other. The stationary convergence analysis of PG is applicable to NPG, and the global convergence analysis of NPG can establish the global convergence of (variance-reduced) PG methods. By combining these two lines of work, the authors achieve variance-reduction for NPG with global convergence and efficient finite-sample complexity.