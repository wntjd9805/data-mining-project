Randomized classifiers have shown promise in achieving certified robustness against adversarial attacks in deep learning. However, existing methods primarily use Gaussian smoothing noise and are limited to ℓ2 perturbations. We propose a general framework for adversarial certification that incorporates non-Gaussian noise and caters to various types of attacks. Our framework explores a trade-off between accuracy and robustness by designing smoothing distributions, leading to the development of new families of non-Gaussian smoothing distributions. These distributions are more efficient in handling different ℓp settings, such as ℓ1, ℓ2, and ℓ∞ attacks. Our proposed methods outperform previous works in terms of certification results, providing a fresh perspective on randomized smoothing certification.