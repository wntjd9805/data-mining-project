Adversarial learning has been successful in unsupervised domain adaptation by learning domain-invariant representations. However, it has limitations when label distributions differ between domains. This paper proposes a new assumption, generalized label shift (GLS), to address this issue. GLS states that, given the label, there exists an invariant representation of the input between domains. The paper provides theoretical guarantees on transfer performance under GLS and identifies necessary and sufficient conditions for GLS to hold. The authors propose a weight estimation method that can be easily applied to existing domain adaptation algorithms, with minimal computational overhead. They modify three algorithms and evaluate their performance on various tasks, showing significant improvements, especially for large label distribution mismatches. The code for their algorithms is publicly available.