Machine learning system components are not currently considered to be security vulnerabilities. Developers of machine learning systems have not yet implemented secure coding practices, such as avoiding reliance on confidential inputs. In this study, we examine the security of machine learning system code by investigating how nucleus sampling, a popular text generation technique, unintentionally exposes texts entered by users. Our main finding is that the sequence of nucleus sizes for many English word sequences can serve as a unique identifier. We then demonstrate how an attacker can deduce the typed text by analyzing these identifiers through a side channel, such as cache access times. We also explore the potential of this attack for de-anonymizing anonymous texts and discuss possible defense strategies.