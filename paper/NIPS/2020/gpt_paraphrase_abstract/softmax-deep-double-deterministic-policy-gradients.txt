The Deep Deterministic Policy Gradients (DDPG) algorithm, commonly used for continuous control in reinforcement learning, suffers from the problem of overestimation, which can harm its performance. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, although an improvement, still has a significant issue with underestimation bias. This paper proposes the use of the Boltzmann softmax operator to estimate the value function in continuous control. The theoretical analysis of the softmax operator in continuous action space reveals its ability to smooth the optimization landscape, offering new insights into its benefits. Additionally, two new algorithms, Softmax Deep Deterministic Policy Gradients (SD2) and Softmax Deep Double Deterministic Policy Gradients (SD3), are introduced. These algorithms, incorporating the softmax operator with single and double estimators, effectively address both the overestimation and underestimation bias. Extensive experiments on challenging continuous control tasks demonstrate that SD3 outperforms existing methods.