Current research on interpretable machine learning has mainly focused on two approaches: developing inherently interpretable models, which sacrifice accuracy for interpretability, or creating post-hoc explanation systems that may produce unpredictable explanations. In contrast, our proposed method, called EXPO, combines these approaches by incorporating regularizers into the model training process to enhance explanation quality. These regularizers are differentiable, applicable to any model, and do not require any specialized knowledge. Our experiments show that post-hoc explanations generated by EXPO-regularized models outperform other models in terms of fidelity and stability. Furthermore, a user study confirms that improved fidelity and stability result in significantly more useful explanations for real-world tasks.