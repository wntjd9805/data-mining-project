The Gumbel-Max trick is widely used in relaxed gradient estimators, known for their ease of implementation and low variance. However, there is a need to extend these estimators to larger combinatorial distributions. In this study, we propose stochastic softmax tricks within the perturbation model framework, which generalize the Gumbel-Softmax trick to combinatorial spaces. Our approach offers a unified perspective on existing relaxed estimators and introduces several new relaxations. We develop structured relaxations for subset selection, spanning trees, arborescences, and other scenarios. Comparing our method to less structured baselines, we find that stochastic softmax tricks enable better training of latent variable models and the discovery of more latent structure.