This paper introduces a novel method called clipped-SSTM for smooth convex stochastic optimization with heavy-tailed distributed noise in stochastic gradients. By utilizing a variant of accelerated Stochastic Gradient Descent (SGD) and clipping of stochastic gradients, we propose a new approach that closes the existing gap in the theory of stochastic optimization with heavy-tailed noise. Additionally, we extend our method to the strongly convex case and present complexity bounds that outperform current state-of-the-art results. Furthermore, we expand our proof technique to derive the first non-trivial high-probability complexity bounds for SGD with clipping, without assuming light-tails on the noise.