Federated Learning (FL) is susceptible to adversarial attacks, specifically backdoors inserted during training. Backdoors aim to undermine the model's performance on specific sub-tasks, like misclassifying green cars as frogs. While existing literature proposes FL backdoor attacks and defense methods, it remains uncertain whether FL systems can truly be made resistant to backdoors. This study challenges the notion of achieving robustness against backdoors in FL. The authors establish that being robust to backdoors implies being robust to adversarial examples, a significant challenge in itself. Additionally, detecting the presence of a backdoor in an FL model is unlikely using first-order oracles or polynomial time. The study introduces a new type of backdoor attack called "edge-case backdoors." These backdoors exploit the model's misclassification of seemingly easy inputs that are unlikely to be part of the training or test data, residing on the tail of the input distribution. The authors highlight the potential negative impact of edge-case backdoors on fairness and demonstrate their effectiveness across various machine learning tasks. Even state-of-the-art defense mechanisms can be bypassed with careful adversary tuning.