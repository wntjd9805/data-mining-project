There is a growing body of literature on generating local contrastive/counterfactual explanations for black-box models, such as neural networks. These methods provide explanations in the form of a contrast point that differs from the original input in only a few features and belongs to a different class. Other approaches aim to create globally interpretable models, like decision trees and rule lists, using either actual labels or predictions from black-box models. However, while these global models can be useful, they may not align with the local explanations provided by a specific black-box model. This study investigates whether it is possible to develop a transparent global model that is both accurate and consistent with the local explanations of the black-box model. We propose a novel method that utilizes a natural local consistency metric to assess the alignment between the local explanations and predictions of the black-box model and a proxy global transparent model. By creating custom boolean features from sparse local contrastive explanations and training a globally transparent model solely on these features, we demonstrate empirically that our approach achieves higher local consistency compared to other strategies, while still maintaining performance similar to models trained with access to the original data.