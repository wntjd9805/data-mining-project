The challenge of vision-and-language navigation (VLN) is training an agent that can navigate new environments, rather than memorizing specific routes and visual details from training. To address this, we propose a learning strategy that incorporates observations from both real and counterfactual environments. We develop an algorithm to generate counterfactual observations by combining existing environments. Additionally, we introduce a novel training objective that encourages the agent to maintain consistent actions between original and counterfactual environments, removing biased features. Our experiments demonstrate that this approach significantly improves generalization in Room-to-Room navigation and Embodied Question Answering benchmarks.