This paper presents MDP homomorphic networks for deep reinforcement learning, which are neural networks that maintain symmetry in the joint state-action space of an MDP. Unlike existing approaches, we leverage this structural knowledge to reduce the solution space by incorporating equivariance constraints into policy and value networks. Specifically, we focus on group-structured symmetries, such as invertible transformations. Moreover, we introduce a numerical method for constructing equivariant network layers, eliminating the need for manual constraint solving. We develop MDP homomorphic MLPs and CNNs that maintain equivariance under reflections or rotations. Our results demonstrate that these networks achieve faster convergence compared to unstructured baselines on CartPole, a grid world, and Pong.