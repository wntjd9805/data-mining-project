Neural backdoor attacks have emerged as a potential threat to deep learning systems. While these systems perform well on clean data, they behave abnormally when exposed to inputs with predefined triggers. Current backdoor techniques rely on easily detectable uniform trigger patterns, which can be mitigated by existing defense methods. To address this, we propose a new backdoor attack technique that incorporates varying triggers for each input. We achieve this by developing an input-aware trigger generator that utilizes diversity loss. Additionally, we introduce a cross-trigger test to ensure trigger nonreusability, rendering backdoor verification impossible. Our experiments demonstrate the effectiveness of our method across different attack scenarios and datasets. Furthermore, we show that our backdoor can successfully evade state-of-the-art defense methods. The stealthiness of our attack is confirmed through analysis with a renowned neural network inspector. Our code is publicly available.