This paper addresses the computation of the barycenter of probability distributions under the Sinkhorn divergence. This problem has been applied in various fields such as graphics, learning, and vision for effective knowledge aggregation. Instead of working directly with probability measures, we propose a new approach called SinkhornDescent (SD) that formulates the Sinkhorn barycenter problem as a functional optimization. We prove that SD converges to a stationary point at a sublinear rate and, under reasonable assumptions, it asymptotically finds a global minimizer of the Sinkhorn barycenter problem. Additionally, we demonstrate through mean-field analysis that SD preserves the weak convergence of empirical measures. Notably, SD has linear computational complexity in the dimension d, as demonstrated by solving a 100-dimensional Sinkhorn barycenter problem.