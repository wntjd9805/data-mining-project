The neural estimation of mutual information (MI) has been successful in modeling the expected dependency between high-dimensional random variables. However, MI cannot measure the point-wise dependency between different events. Instead, we focus on estimating point-wise dependency (PD), which quantifies the likelihood of two outcomes co-occurring. We show that PD can be obtained naturally when optimizing MI neural variational bounds. However, optimizing these bounds is challenging due to high variance. To overcome this, we propose two methods (Probabilistic Classifier and Density-Ratio Fitting) that do not require optimizing MI variational bounds. We demonstrate the effectiveness of our approaches in MI estimation, self-supervised representation learning, and cross-modal retrieval tasks.