Unsupervised image representations have made significant progress in narrowing the gap with supervised pretraining methods, particularly contrastive learning techniques. However, these methods often require computationally expensive pairwise feature comparisons. This paper introduces an online algorithm called SwAV that addresses this limitation by simultaneously clustering data and ensuring consistency between cluster assignments for different image augmentations. Instead of directly comparing features, SwAV employs a "swapped" prediction mechanism, predicting the code of one view from the representation of another. Unlike previous contrastive methods, SwAV is more memory-efficient as it does not rely on a large memory bank or a special momentum network. The paper also proposes a novel data augmentation strategy called multi-crop, which uses a combination of views with different resolutions to replace two full-resolution views without increasing memory or compute requirements. Experimental results demonstrate the effectiveness of SwAV, achieving 75.3% top-1 accuracy on ImageNet with ResNet-50 and outperforming supervised pretraining in various transfer tasks.