Model-based reinforcement learning algorithms with probabilistic dynamical models are known for their efficient learning capabilities due to their ability to distinguish between different types of uncertainty. However, most of these algorithms fail to consider this uncertainty when optimizing the policy, resulting in limited exploration. This paper proposes a practical solution called H-UCRL, an optimistic exploration algorithm that addresses this issue by incorporating epistemic uncertainty directly into the control process. By expanding the input space with hallucinated inputs, H-UCRL can be solved using standard greedy planners. The paper also provides a theoretical analysis of H-UCRL, presenting a regret bound for well-calibrated models, particularly Gaussian Process models. This analysis demonstrates the sublinear performance of H-UCRL. Additionally, the paper showcases how optimistic exploration can be easily integrated with state-of-the-art reinforcement learning algorithms and different probabilistic models. Experimental results indicate that optimistic exploration significantly enhances learning speed, particularly in scenarios with action penalties, which are typically challenging for existing model-based reinforcement learning algorithms.