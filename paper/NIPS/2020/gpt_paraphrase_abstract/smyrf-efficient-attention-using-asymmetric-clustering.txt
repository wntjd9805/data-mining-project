We propose a new clustering algorithm called SMYRF that approximates attention in a more efficient way. Our algorithm reduces the attention complexity from O(N^2) to O(N log N), where N represents the sequence length. By utilizing Locality Sensitive Hashing (LSH) in a unique manner, SMYRF introduces new Asymmetric transformations and an adaptive scheme that generates balanced clusters. The major advantage of SMYRF is that it can be easily integrated into existing models without the need for retraining. In contrast, previous fast attention methods impose restrictions and require starting the training process from scratch. We apply SMYRF to state-of-the-art models in Natural Language Processing and Computer Vision and observe significant improvements in memory usage and speed. Notably, SMYRF-BERT performs slightly better than BERT on the GLUE benchmark while utilizing 50% less memory. Additionally, we demonstrate that SMYRF can be used interchangeably with dense attention both before and after training. Lastly, we utilize SMYRF to train Generative Adversarial Networks (GANs) with attention in high resolutions. With just a single TPU, we successfully scale attention to 128x128=16k and 256x256=65k tokens on BigGAN for the CelebA-HQ dataset.