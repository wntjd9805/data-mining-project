High-dimensional black-box optimization is a difficult problem to solve, with applications in various domains. Traditional methods like Bayesian Optimization suffer from the curse of dimensionality, while greedy search may lead to suboptimal results. LaNAS has shown promising results in Neural Architecture Search by recursively dividing the search space based on function values. This paper introduces LA-MCTS, which extends LaNAS to other domains. LA-MCTS learns the partition of the search space using a few samples and their function values in an online manner. Unlike LaNAS, LA-MCTS uses a nonlinear decision boundary and learns a local model to select good candidates. If the nonlinear partition function and the local model fit well with the true black-box function, LA-MCTS can achieve good partitions and candidates with fewer samples. LA-MCTS serves as a meta-algorithm, utilizing existing black-box optimizers as its local models, and performs well in general black-box optimization and reinforcement learning benchmarks, especially for high-dimensional problems.