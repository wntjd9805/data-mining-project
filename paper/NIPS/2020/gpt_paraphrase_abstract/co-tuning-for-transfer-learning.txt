Transfer learning, the process of fine-tuning pre-trained deep neural networks (DNNs) to a new dataset, is commonly used in computer vision and NLP. Typically, practitioners discard task-specific layers and only fine-tune the bottom layers, resulting in a loss of task-specific parameters that make up a significant portion of the pre-trained models. To address this issue, we propose a two-step framework called Co-Tuning. First, we learn the relationship between the categories in the pre-trained model and the target dataset using calibrated predictions. Then, we collaboratively supervise the fine-tuning process using target labels (one-hot labels) and source labels (probabilistic labels) translated through the category relationship. Our framework demonstrates strong results in visual and NLP classification tasks, with a relative improvement of up to 20%. Unlike existing techniques that focus on regularization in data-limited scenarios, Co-Tuning works effectively with both medium-scale (100 samples per class) and large-scale (1000 samples per class) datasets. It relies on the assumption that the pre-trained dataset is sufficiently diverse, indicating its broad applicability.