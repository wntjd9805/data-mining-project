A common objective in neural data analysis is to compress large population recordings into interpretable, low-dimensional latent trajectories. This can be achieved using Gaussian process (GP)-based methods, which offer uncertainty quantification and principled model selection. However, standard GP priors do not differentiate between underlying dynamical processes and other types of temporal autocorrelation. In this study, we propose a new family of "dynamical" priors for trajectories, in the form of GP covariance functions that capture a characteristic shared by most dynamical systems: temporal non-reversibility. Non-reversibility is a universal feature of autonomous dynamical systems where state trajectories follow consistent flow fields, making it impossible for any observed trajectory to occur in reverse. Our novel multi-output GP kernels can be seamlessly used as substitutes for standard kernels in multivariate regression and latent variable models like Gaussian process factor analysis (GPFA). To demonstrate the effectiveness of our approach, we introduce GPFADS (Gaussian Process Factor Analysis with Dynamical Structure), which models neural population activity in single trials using low-dimensional, non-reversible latent processes. Unlike previously proposed non-reversible multi-output kernels, our approach allows for fast and memory-efficient learning and inference through a Kronecker factorization. We validate GPFADS using synthetic data and confirm its ability to accurately recover ground truth phase portraits. Furthermore, GPFADS offers a probabilistic extension of jPCA, a method originally designed for identifying latent rotational dynamics in neural data. When applied to monkey M1 neural recordings, GPFADS successfully uncovers latent trajectories characterized by strong dynamical structure in the form of rotations.