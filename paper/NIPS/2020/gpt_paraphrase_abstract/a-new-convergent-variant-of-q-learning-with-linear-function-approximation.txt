This study introduces a new variation of Q-learning with linear function approximation that guarantees convergence with probability 1. The algorithm operates on two time scales, with a faster scale incorporating an update similar to DQN but with attenuated bootstrapping using a Q-value estimate similar to DQN's target network. The slower time scale involves a modified target network update. The convergence of the algorithm is proven, an error bound is provided, and the results are compared to existing convergence results in reinforcement learning with function approximation. The effectiveness of the proposed method is demonstrated in domains where standard Q-learning fails to converge.