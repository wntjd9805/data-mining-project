Successful deep neural network architectures typically consist of structured elements such as convolutional neural networks and gated recurrent neural networks. Recently, graph neural networks (GNNs) have been effectively applied to graph-structured data like point cloud and molecular data. However, GNNs primarily consider pairwise dependencies due to operating on a graph structure. In this study, we introduce a generalized version of GNN called factor graph neural network (FGNN), which allows for incorporating dependencies among multiple variables. We demonstrate that FGNN can represent Max-Product belief propagation, an approximate inference method on probabilistic graphical models. This provides a theoretical understanding of the capabilities of both FGNN and related GNNs. Experimental results on synthetic and real datasets validate the potential effectiveness of the proposed FGNN architecture.