Most deep models update their weights using multiple gradient signals from multiple loss terms. However, this can hinder optimal training as the updates may pull the model in conflicting directions. To address this issue, we propose Gradient Sign Dropout (GradDrop), a probabilistic masking procedure that samples gradients at an activation layer based on their consistency. GradDrop is a simple deep layer that can be integrated into any deep net and works well with other gradient balancing approaches. Our experiments demonstrate that GradDrop outperforms existing multiloss methods in multitask and transfer learning scenarios. Additionally, we discuss how GradDrop sheds light on the connection between optimal multiloss training and gradient stochasticity.