The self-attention mechanism is commonly used in various tasks but suffers from a high computational cost when dealing with long inputs. To address this issue, we propose a method called Sparse Adaptive Connection (SAC). SAC treats the input sequence as a graph and performs attention operations between connected nodes. Unlike previous self-attention models with predefined structures, SAC learns to construct attention edges, improving task-specific performance. By selecting the most important nodes, SAC reduces the computational complexity regardless of the sequence length. We demonstrate that previous self-attention models are special cases of SAC. Through extensive experiments, including neural machine translation, language modeling, graph representation learning, and image classification, we show that SAC achieves competitive performance while significantly reducing memory usage.