This study focuses on the Stein Variational Gradient Descent (SVGD) algorithm, which is used to approximate a target probability distribution. The algorithm performs gradient descent on the KL divergence with respect to the target distribution by optimizing a set of particles. The gradient is smoothed using a kernel integral operator. This paper presents a new analysis of the SVGD algorithm, providing a descent lemma that proves the algorithm decreases the objective at each iteration. It also establishes rates of convergence for the averaged Stein Fisher divergence (KernelStein Discrepancy). Additionally, the paper demonstrates the convergence of the finite particle system used in the practical implementation of SVGD to its population version.