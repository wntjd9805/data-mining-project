Variational Bayesian Monte Carlo (VBMC) is a framework that uses Gaussian process surrogates to perform approximate Bayesian inference in models with non-cheap likelihoods. This study extends VBMC to deal with noisy log-likelihood evaluations, such as those from simulation-based models. New "global" acquisition functions, including expected information gain (EIG) and variational interquantile range (VIQR), are introduced to handle noise and can be efficiently evaluated within the VBMC framework. The performance of VBMC + VIQR is compared to other methods in a noisy-inference benchmark using real datasets from computational and cognitive neuroscience. Results show that VBMC + VIQR outperforms "local" acquisition functions and other surrogate-based inference methods while maintaining a small algorithmic cost. This benchmark confirms that VBMC is a versatile technique for efficient black-box Bayesian inference, even in the presence of noisy models.