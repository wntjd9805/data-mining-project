To make decisions based on a model fit with auto-encoding variational Bayes (AEVB), practitioners often use the variational distribution as a substitute for the posterior distribution. However, this approach leads to biased estimates of expected risk and ultimately poor decisions for two main reasons. Firstly, the AEVB model may not accurately represent the underlying data distribution. Secondly, the variational distribution may not match the posterior distribution under the fitted model. To address these issues, we explore fitting the variational distribution based on objective functions other than the evidence lower bound (ELBO), while still fitting the generative model using the ELBO. We examine how different objective functions for fitting the variational distribution impact the quality of downstream decisions using the probabilistic principal component analysis model as an example. We analyze the variation in importance sampling error and bias of model parameter estimates across various approximate posteriors used as proposal distributions.Based on our theoretical findings, we suggest using a posterior approximation that is distinct from the variational distribution for decision-making. Motivated by these results, we propose learning multiple approximate proposals for the best model and combining them using multiple importance sampling. We provide toy examples to illustrate our proposed approach, and present a comprehensive case study on single-cell RNA sequencing. In this challenging scenario of multiple hypothesis testing, our proposed approach outperforms the current state of the art.