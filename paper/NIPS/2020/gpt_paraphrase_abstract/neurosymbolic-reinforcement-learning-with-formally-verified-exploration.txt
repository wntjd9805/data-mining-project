We introduce REVEL, a partially neural reinforcement learning (RL) framework for ensuring safe exploration in continuous state and action spaces. The main challenge in safe deep RL is the computational infeasibility of repeatedly verifying neural networks during the learning process. To overcome this challenge, we utilize two policy classes: a general neurosymbolic class with approximate gradients, and a more limited symbolic policy class that allows for efficient verification. Our learning algorithm employs mirror descent over policies, where in each iteration, we safely elevate a symbolic policy to the neurosymbolic space, apply safe gradient updates to the resulting policy, and then project the updated policy back into the safe symbolic subset. Importantly, this approach does not require explicit verification of neural networks. Our experimental results demonstrate that REVEL achieves safe exploration in scenarios where Constrained Policy Optimization fails, and it also discovers policies that outperform those learned through previous verified exploration methods.