Humans can solve visual puzzles using logical reasoning with only a few examples, but deep reasoning models require extensive training to achieve similar performance. This study proposes using analogical reasoning to solve these few-shot visual reasoning problems. Analogical and non-analogical training pairs are constructed, with the former involving perturbing or shuffling the original problem. The structural relations between elements in each pair are extracted by maximizing similarities among analogical relations and minimizing similarities among non-analogical relations. This analogical contrastive learning method effectively learns relational representations for abstract reasoning tasks. The proposed method outperforms state-of-the-art techniques on the RAVEN dataset, particularly when training data is limited. Additionally, the analogical contrastive learning model is meta-learned on tasks with diverse attributes and successfully generalizes to unseen attribute visual reasoning problems.