Untrained deep neural networks (DNNs) differ from trained ones in terms of the location and values of their weights. The location of weights holds most of the training information. Based on this observation, we propose a method called lookahead permutation (LaPerm) to train DNNs by reconnecting the weights. We demonstrate the effectiveness of LaPerm in various scenarios, showing comparable or better performance than regular optimizers like Adam. LaPerm is able to produce accurate results even with random and sparse initial weights, and it can also achieve high accuracy with weight agnostic neural networks.