Negative sampling is commonly used in implicit collaborative filtering to obtain negative labels from large amounts of unlabeled data. However, recent approaches still face challenges in terms of efficiency and effectiveness due to complex structures and the risk of false negative instances. In this study, we propose a new understanding of negative instances, noting that only a few instances are crucial for model learning and false negatives tend to have consistent predictions. Based on these observations, we simplify the model by sampling from a designed memory that only stores a small number of important candidates. Additionally, we address the issue of false negatives by prioritizing high-variance samples stored in memory, resulting in efficient sampling of true negatives with high quality. Our experimental results on synthetic and real-world datasets demonstrate the robustness and superiority of our negative sampling method. The implementation of our approach can be found at https://github.com/dingjingtao/SRNS.