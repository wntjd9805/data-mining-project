Fine-tuning deep networks on a small dataset is crucial in the deep learning process. However, overfitting becomes a major concern when dealing with limited data. Current approaches address this issue by applying regularization techniques on parameters or features and reusing pre-trained parameters to transfer prior knowledge. In this study, we propose an alternative method by modifying the widely used Batch Normalization module. Our approach involves a two-branch design where one branch is normalized using mini-batch statistics and the other branch is normalized using moving statistics. By stochastically selecting between the two branches during training, we prevent excessive reliance on specific sample statistics, resulting in a strong regularization effect known as "architecture regularization." We call this approach stochastic normalization (StochNorm). The two-branch architecture also allows for the incorporation of pre-trained moving statistics in Batch Normalization layers during fine-tuning, enabling better utilization of prior knowledge from pre-trained networks. Extensive experiments demonstrate that StochNorm effectively prevents overfitting when fine-tuning with small datasets. Furthermore, StochNorm can be easily integrated into modern CNN backbones and can work in conjunction with other fine-tuning methods to achieve even stronger regularization.