Discovering successful collective behaviors in multi-agent reinforcement learning is difficult due to the exponential growth of the joint action space with the number of agents. While independent agent-wise exploration is appealing, it fails on tasks that require complex group strategies. To address this, we propose two policy regularization methods: TeamReg, which focuses on inter-agent action predictability, and CoachReg, which emphasizes synchronized behavior selection. We evaluate these approaches on challenging tasks with sparse rewards and varying levels of coordination, as well as on the Google Research Football environment. Our experiments demonstrate improved performance in cooperative multi-agent problems. Furthermore, we analyze the effects of our methods on the learned policies and find that they successfully enforce qualities indicative of coordinated behaviors.