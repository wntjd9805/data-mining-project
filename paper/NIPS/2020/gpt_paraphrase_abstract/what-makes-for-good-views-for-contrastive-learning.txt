Contrastive learning, which involves comparing different views of data, has recently shown impressive performance in self-supervised representation learning. However, the impact of different view choices has not been extensively studied. This paper aims to explore the significance of view selection and proposes reducing the mutual information (MI) between views while preserving task-related information. The hypothesis is validated through the development of unsupervised and semi-supervised frameworks that prioritize the reduction of MI in order to learn effective views. Additionally, the use of data augmentation is considered as a means to decrease MI, resulting in improved downstream classification accuracy. As a result, this approach achieves a new state-of-the-art accuracy for unsupervised pre-training on ImageNet classification, reaching a top-1 linear readout accuracy of 73% using a ResNet-50 model.