Reinforcement learning (RL) often requires large amounts of data due to the challenge of transferring knowledge between tasks. In traditional multi-task RL scenarios, data collected while attempting to solve one task is typically unhelpful for that specific task and is therefore wasted. However, we propose that this seemingly irrelevant data can actually be valuable for other tasks. To maximize the use of this data, we introduce Generalized Hindsight, an approximate inverse reinforcement learning technique that relabels behaviors with the appropriate tasks. Essentially, Generalized Hindsight takes a behavior generated for one task and assigns it to a different task that it is better suited for. This relabeled behavior is then utilized by an off-policy RL optimizer. Our experiments on various multi-task navigation and manipulation tasks demonstrate that Generalized Hindsight significantly improves the efficiency of sample reuse compared to traditional relabeling techniques.