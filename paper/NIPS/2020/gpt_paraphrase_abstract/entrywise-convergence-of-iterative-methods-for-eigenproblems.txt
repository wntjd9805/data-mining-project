Many problems in machine learning, statistics, and related fields require the computation of eigenvectors. When dealing with large-scale problems, iterative methods like subspace iteration or Krylov methods are commonly used. While there is existing analysis on the convergence of subspaces using the spectral norm, other measures of subspace distance are more suitable for modern applications. Recent theoretical work has focused on perturbations of subspaces using the (cid:96)2 norm, but has not considered the actual computation of eigenvectors. In this study, we investigate the convergence of subspace iteration when distances are measured using the (cid:96)2 norm and provide deterministic bounds. We also propose a practical stopping criterion and validate its effectiveness through numerical experiments. Our findings demonstrate that comparable performance in downstream tasks can be achieved with fewer iterations, leading to significant computational time savings.