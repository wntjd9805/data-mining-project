We examine whether Differentially Private SGD provides better privacy in practice compared to its current state-of-the-art analysis. To do this, we employ innovative data poisoning attacks that accurately represent real privacy threats. Although previous research (Ma et al., arXiv 2019) suggested the connection between differential privacy and data poisoning as a defense against the latter, our study focuses on using it as a means to evaluate the privacy of a particular mechanism, which is a novel approach. In general, our research takes a quantitative and empirical standpoint in assessing the level of privacy offered by specific implementations of differentially private algorithms. We believe that this approach has the potential to complement and influence analytical investigations on differential privacy.