We enhance the Approximate-Proximal Point (APROX) model-based methods for solving stochastic convex optimization problems by extending them to the minibatch setting. We introduce two minibatched algorithms and prove a non-asymptotic upper bound on their convergence rate, indicating a linear improvement with increasing minibatch size. Unlike standard stochastic gradient methods, our algorithms can achieve linear speedup in the minibatch setting even for non-smooth functions. These algorithms retain the favorable characteristics of the APROX family, such as robustness to initial step size selection. Furthermore, we demonstrate improved convergence rates for "interpolation" problems, offering a novel parallelization approach for alternating projections. Extensive empirical testing confirms the benefits of accurate modeling and minibatching, validating our theoretical findings.