This study explores the underlying theory behind the superior generalization performance of deep residual networks (ResNets) compared to deep feedforward networks (FFNets). The authors approach the problem from the perspective of a "neural tangent kernel" and make several key findings. Firstly, they demonstrate that when the width of the network approaches infinity, training deep ResNets can be seen as learning reproducing kernel functions with a specific kernel function. Secondly, they compare the kernels of deep ResNets and deep FFNets and observe that the class of functions induced by the kernel of FFNets becomes asymptotically unlearnable as the depth increases. In contrast, the class of functions induced by the kernel of ResNets does not exhibit this degeneracy. These findings provide a partial explanation for the advantages of deep ResNets in terms of generalization abilities. The authors support their claims with numerical results.