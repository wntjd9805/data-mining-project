Adversarial training is a popular technique used to enhance the robustness of neural networks against adversarial perturbations. However, the underlying reasons for its effectiveness under natural conditions have not been fully explained. While convergence theories for standard training have been developed for over-parametrized networks, it remains challenging to extend these theories to adversarial training due to the min-max objective. Previous attempts using online learning tools were limited by the exponential dependence on input dimension and the use of non-practical activation functions. Our research addresses these limitations by proving convergence to low robust training loss for polynomial width and running time, with the use of ReLU activation. A key aspect of our proof is demonstrating that ReLU networks can approximate the step function near initialization, which is of independent interest.