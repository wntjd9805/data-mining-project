Neural Architecture Search (NAS) aims to achieve top performance by discovering new architecture patterns without human intervention. However, relying too much on expert knowledge in designing the search space has led to improved performance without significant architectural advancements, hindering the discovery of truly innovative solutions. In this study, we address this issue by 1) treating NAS as the search for the optimal network generator and 2) proposing a novel hierarchical and graph-based search space that can represent a wide variety of network types with minimal continuous hyper-parameters. This reduces the complexity of the problem, allowing us to effectively use Bayesian Optimization for the search strategy. Additionally, we introduce a multi-objective learning approach to expand the range of valid architectures. We validate our approach on six benchmark datasets and demonstrate that our search space produces lightweight yet highly competitive models. Code for this work is available at https://github.com/rubinxin/vega_NAGO.