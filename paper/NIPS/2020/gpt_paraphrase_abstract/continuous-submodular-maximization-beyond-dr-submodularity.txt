This paper introduces novel continuous optimization algorithms that can achieve a constant factor approximation guarantee for the problem of maximizing monotone continuous submodular functions subject to a linear constraint. The first algorithm, called COORDINATE-ASCENT+, is a simple variant of vanilla coordinate ascent. It guarantees an approximation ratio of ( e−1 2e−1 − ε) while performing O(n/ε) iterations, with each iteration having a computational complexity of roughly O(n/ε + n log n). The second algorithm, COORDINATE-ASCENT++, achieves a tighter approximation ratio of (1 − 1/e − ε) with the same number of iterations as COORDINATE-ASCENT+, but at a higher computational complexity of roughly O(n3/ε2.5 + n3 log n/ε2) per iteration. However, the computation for each round of COORDINATE-ASCENT++ can be easily parallelized, resulting in a computational cost of O(n/ε + n log n) per machine.