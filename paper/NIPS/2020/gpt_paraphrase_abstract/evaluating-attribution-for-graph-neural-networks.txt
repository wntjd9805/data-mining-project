Machine learning models require interpretability for scientific understanding, AI safety, and debugging purposes. Attribution is an interpretability approach that identifies the influential input dimensions in a neural network's prediction. However, evaluating attribution methods for image and text models is mainly qualitative due to the need for expensive and unreliable human judgment to obtain ground truth attributions. Graph neural networks (GNNs), which make predictions on graphs of any size, have not received sufficient attention in attribution studies. This research aims to address this gap by adapting common attribution methods for GNNs and quantitatively evaluating them based on attribution accuracy, stability, faithfulness, and consistency. Synthetic graph problems with known ground-truth attributions are used to enable quantitative benchmarking. The study provides recommendations for suitable attribution methods for GNNs, along with the necessary data and code for the benchmarking suite. A rigorous and open-source benchmarking of attribution methods in graphs has the potential to facilitate the development of new methods and expand the application of attribution in real-world machine learning tasks.