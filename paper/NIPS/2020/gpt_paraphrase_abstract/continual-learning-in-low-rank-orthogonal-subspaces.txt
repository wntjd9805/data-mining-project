This study focuses on continual learning (CL), where a learner is presented with a series of tasks and aims to remember them all once the learning experience is complete. Previous approaches in CL have used techniques such as episodic memory, parameter regularization, or extensible network structures to reduce interference between tasks. However, these approaches ultimately learn different tasks in a shared vector space, which we believe leads to interference. To address this, we propose learning tasks in separate vector subspaces that are orthogonal to each other to minimize interference. Additionally, we introduce isometric mappings to ensure that the gradients from different tasks remain orthogonal. Through our experiments, we demonstrate strong results compared to experience-replay baselines, both with and without memory, on standard classification benchmarks in continual learning.