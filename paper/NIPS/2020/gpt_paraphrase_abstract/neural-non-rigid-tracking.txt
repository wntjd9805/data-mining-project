We present a new non-rigid tracker that can achieve state-of-the-art non-rigid reconstruction through learned robust optimization. Our approach uses a convolutional neural network to predict dense correspondences and their confidences between two input RGB-D frames of a non-rigidly moving object. These correspondences are then used as constraints in an as-rigid-as-possible optimization problem. By allowing gradient back-propagation through the non-linear least squares solver, we can learn correspondences and confidences in an end-to-end manner that is optimal for non-rigid tracking. This enables us to learn correspondence confidences through self-supervision, leading to a robust optimization where outliers and incorrect correspondences are down-weighted for effective tracking. Our algorithm outperforms existing methods in terms of reconstruction performance and is also 85 times faster in predicting correspondences compared to similar deep-learning based methods. The code for our approach is available at https://github.com/DeformableFriends/NeuralTracking.