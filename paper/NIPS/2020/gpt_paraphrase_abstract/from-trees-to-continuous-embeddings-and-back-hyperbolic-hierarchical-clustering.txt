Similarity-based Hierarchical Clustering (HC) is a traditional unsupervised machine learning algorithm that has historically been solved using heuristic algorithms like Average-Linkage. However, a recent study by Dasgupta proposed a new approach by treating HC as a discrete optimization problem and introducing a global cost function to measure the quality of a tree. In this research, we present the first continuous relaxation of Dasgupta's discrete optimization problem, with guaranteed quality results. Our method, called HYPHC, establishes a direct connection between discrete trees and continuous representations using hyperbolic embeddings of leaf nodes. By leveraging this connection, we are able to search the space of discrete binary trees using continuous optimization. We introduce a continuous version of the lowest common ancestor concept based on analogies between trees and hyperbolic space, which leads to a continuous relaxation of Dasgupta's objective. We demonstrate that by decoding the continuous relaxation, we can obtain a discrete tree that approximates Dasgupta's optimal tree with a (1 + ε)-factor approximation, where ε can be minimized to address optimization challenges. We evaluate HYPHC on various HC benchmarks and find that even approximate solutions obtained through gradient descent outperform agglomerative heuristics and other gradient-based algorithms in terms of clustering quality. Additionally, we showcase the flexibility of HYPHC by applying it to an end-to-end training scenario in a downstream classification task.