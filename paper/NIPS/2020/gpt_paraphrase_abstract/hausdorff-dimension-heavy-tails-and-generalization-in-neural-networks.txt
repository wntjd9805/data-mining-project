Despite the success of stochastic gradient descent (SGD) in various applications, understanding its generalization properties in non-convex deep learning problems remains a significant challenge. Recent studies have used stochastic differential equations (SDE) to model the trajectories of SGD with heavy-tailed gradient noise, revealing interesting characteristics. However, a comprehensive analysis of the generalization properties of such SDEs within a learning theoretical framework is still lacking. To address this gap, this study establishes generalization bounds for SGD by assuming that its trajectories can be well approximated by a Feller process. Feller processes encompass various recent SDE representations, including both Brownian and heavy-tailed cases. The study demonstrates that the generalization error can be controlled by the Hausdorff dimension of the trajectories, which is closely related to the tail behavior of the driving process. Consequently, processes with heavier tails are expected to exhibit better generalization, making the tail-index of the process a useful capacity metric. Experimental results on deep neural networks confirm that the proposed capacity metric accurately estimates the generalization error, and it is not necessarily dependent on the number of parameters like existing metrics in the literature.