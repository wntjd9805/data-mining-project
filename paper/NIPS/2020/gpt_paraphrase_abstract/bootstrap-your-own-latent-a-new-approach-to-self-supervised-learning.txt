We present BYOL, a novel self-supervised image representation learning method that utilizes two neural networks, namely online and target networks, to learn from each other. By training the online network to predict the target network representation of an augmented view of an image, and simultaneously updating the target network with a slow-moving average of the online network, BYOL achieves state-of-the-art results without using negative pairs. With a ResNet-50 architecture, BYOL achieves a top-1 classification accuracy of 74.3% on ImageNet, and 79.6% with a larger ResNet. We demonstrate that BYOL performs as well as or better than current state-of-the-art methods on transfer and semi-supervised benchmarks. Our implementation and pretrained models are available on GitHub.