A popular method for self-supervised representation learning involves contrasting similar and dissimilar pairs of samples. Typically, dissimilar samples are randomly selected, assuming they have different labels. However, we have discovered that performance improves when negative examples are chosen from samples with truly different labels, even without access to the true labels. Based on this finding, we introduce a debiased contrastive objective which addresses the issue of sampling same-label datapoints without knowing the true labels. Through empirical evaluation, our proposed objective consistently outperforms the current state-of-the-art in vision, language, and reinforcement learning benchmarks. Furthermore, we establish generalization bounds for the downstream classification task.