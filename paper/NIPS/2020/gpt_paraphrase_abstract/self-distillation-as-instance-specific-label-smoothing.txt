Recent research has shown that multi-generational self-distillation can improve generalization. However, the reasons behind this improvement are not well understood. In this study, we conducted experiments to demonstrate that the enhanced performance of multi-generational self-distillation is partly due to the increasing diversity in teacher predictions. Based on this finding, we propose a new interpretation of teacher-student training as amortized MAP estimation, where teacher predictions provide instance-specific regularization. Our framework establishes a theoretical connection between self-distillation and label smoothing, a commonly used technique for regularizing predictive uncertainty. Additionally, our results from experiments using various datasets and neural network architectures highlight the importance of predictive diversity in addition to predictive uncertainty. Finally, we introduce a novel instance-specific label smoothing technique that promotes predictive diversity without requiring a separately trained teacher model. Empirical evaluation of this method shows that it often outperforms traditional label smoothing techniques.