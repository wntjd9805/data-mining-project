Training neural network models with discrete latent variables can be computationally challenging due to the need for marginalization over large sets. To address this, current methods rely on sampling-based approximations, which introduce noise in gradient estimators. In this paper, we propose a new training strategy that replaces these approximations with an exact and efficient marginalization technique. We achieve this by parameterizing discrete distributions using differentiable sparse mappings, such as sparsemax. This significantly reduces the support of the distributions, enabling efficient marginalization. We demonstrate the effectiveness of our approach in three tasks involving latent variable modeling, including a deep generative model, a latent communication game, and a generative model with a bit-vector latent representation. Our method achieves good performance while maintaining the practicality of sampling-based approximations.