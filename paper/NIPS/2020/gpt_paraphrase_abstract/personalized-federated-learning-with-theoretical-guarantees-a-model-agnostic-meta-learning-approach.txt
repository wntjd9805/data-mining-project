Federated Learning aims to train models across multiple computing units without exchanging data samples. While this approach allows users to benefit from the computational power of others and obtain a richer model, it lacks personalization for individual users. This paper explores a personalized variant of federated learning that seeks to find an initial shared model that can be easily adapted by each user to their local dataset. By incorporating the Model-Agnostic Meta-Learning framework, this approach maintains the advantages of federated learning while delivering a more personalized model for each user. The study evaluates the performance of a personalized version of the Federated Averaging algorithm in terms of gradient norm for non-convex loss functions. Additionally, the impact of distribution distances, such as Total Variation and 1-Wasserstein metric, on performance is analyzed.