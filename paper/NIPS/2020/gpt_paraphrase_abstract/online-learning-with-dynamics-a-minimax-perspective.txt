This study focuses on online learning with dynamics, where a learner interacts with a stateful environment over multiple rounds. The learner selects a policy in each round, which incurs a cost based on the chosen policy and the current state. The dynamics of the state and the costs can vary over time in an adversarial manner. The goal is to minimize policy regret, and this study provides upper bounds on the minimax rate for the problem. The results establish sufficient conditions for online learnability and characterize the rates based on the expressiveness of the policy class and the deviation of the instantaneous loss from a counterfactual loss. Matching lower bounds are also provided to demonstrate the necessity of the complexity terms. The analysis of this study encompasses various well-studied problems, such as online learning with memory, control of linear quadratic regulators, Markov decision processes, and tracking adversarial targets. Additionally, the study offers tight regret bounds for new problems involving non-linear dynamics and non-convex losses, where such bounds were previously unknown.