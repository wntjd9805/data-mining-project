This paper explores communication efficient distributed algorithms for distribitionally robust federated learning using periodic averaging and adaptive sampling. Unlike standard empirical risk minimization, the optimization problem in this context has a minimax structure, making it challenging to update the global parameter controlling the mixture of local losses frequently. To address this issue, the authors propose a novel algorithm called Distributionally Robust Federated Averaging (DRFA) that approximates the accumulation of history gradients of the mixing parameter using a snapshotting scheme. The convergence rate of DRFA is analyzed in both convex-linear and nonconvex-linear settings. The authors also extend the proposed approach to objectives with regularization on the mixture parameter and introduce a proximal variant called DRFA-Prox with provable convergence rates. Additionally, the paper analyzes an alternative optimization method for regularized cases in strongly-convex-strongly-concave and non-convex (under PL condition)-strongly-concave settings. This paper is the first to address distributionally robust federated learning with reduced communication and to analyze the efficiency of local descent methods on distributed minimax problems. The theoretical results are supported by experimental evidence in federated learning settings.