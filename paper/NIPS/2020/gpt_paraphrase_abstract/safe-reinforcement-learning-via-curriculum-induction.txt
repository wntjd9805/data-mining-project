This paper proposes a new approach to safe reinforcement learning in safety-critical applications. Traditional methods rely on prior knowledge to avoid dangerous situations during exploration, but these methods are not always applicable in scenarios like autonomous driving. Instead, this study suggests a model inspired by human teaching, where an automatic instructor supervises the learning process and prevents the agent from violating constraints. The instructor does not need to know how to perform the task or understand the environment, but instead uses a library of reset controllers to intervene when the agent behaves dangerously. The choice of reset controller affects the speed of agent learning, and the instructor learns a policy for selecting the appropriate reset controllers based on the agent's progress. The proposed approach is evaluated in two challenging environments to optimize safe and efficient learning.