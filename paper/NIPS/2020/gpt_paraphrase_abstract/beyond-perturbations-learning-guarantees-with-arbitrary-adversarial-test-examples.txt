We present a new learning algorithm that can handle both labeled training examples from a specific distribution and unlabeled test examples that may be chosen by an adversary. Unlike previous approaches that assume test examples are slightly different from the distribution, our algorithm can abstain from making predictions on certain examples. This selective transductive learning algorithm provides guarantees for learning classes with a limited VC dimension, even when the train and test distributions are arbitrary. This is a significant improvement as no previous guarantees were known for simple classes like intervals on a line. Our algorithm is efficient and relies on an Empirical Risk Minimizer (ERM) for the given class. It also provides guarantees even when test examples are chosen by a white-box adversary and extends to generalization, agnostic, and unsupervised settings.