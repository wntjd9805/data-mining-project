Machine learning on small IoT devices using microcontroller units (MCU) is both attractive and challenging due to limited memory. In this study, we introduce MCUNet, a framework that combines an efficient neural architecture (TinyNAS) and a lightweight inference engine (TinyEngine) to enable ImageNet-scale inference on microcontrollers. TinyNAS utilizes a two-stage neural architecture search approach to optimize the search space and adapt to resource constraints. It can handle various constraints, such as device, latency, energy, and memory, while keeping search costs low. TinyEngine, co-designed with TinyNAS, is a memory-efficient inference library that reduces memory usage by 3.4× and accelerates inference by 1.7-3.3× compared to existing solutions. MCUNet achieves over 70% ImageNet top1 accuracy on a commercial microcontroller, using significantly less SRAM and Flash compared to other models. It also outperforms MobileNetV2 and ProxylessNAS-based solutions on visual and audio wake word tasks, running 2.4-3.4× faster with smaller peak SRAM. This study demonstrates the feasibility of always-on machine learning on IoT devices.