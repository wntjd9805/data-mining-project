Modern variational inference (VI) is a technique that allows for large-scale probabilistic inference in complex models by using stochastic gradients to avoid intractable expectations. VI involves finding an approximating distribution from a family of distributions that is closest to the exact posterior. Traditionally, VI algorithms minimize the "exclusive Kullback-Leibler (KL)" divergence, but recent research has also focused on the "inclusive KL" divergence, which has better statistical properties for certain inference problems. This paper introduces a new algorithm called Markovian score climbing (MSC) that reliably minimizes the inclusive KL using stochastic gradients with vanishing bias. MSC converges to a local optimum of the inclusive KL and avoids the systematic errors present in existing methods like ReweightedWake-Sleep and Neural Adaptive Sequential Monte Carlo. The effectiveness of MSC is demonstrated on a toy model as well as Bayesian probit regression for classification and a stochastic volatility model for financial data.