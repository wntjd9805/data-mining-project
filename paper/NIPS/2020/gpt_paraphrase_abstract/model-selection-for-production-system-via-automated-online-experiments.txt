Machine learning practitioners in the industry often struggle with selecting the most suitable model for deployment. Online controlled experiments like A/B tests provide the most reliable estimation of a system's effectiveness but are limited by budget constraints to compare only a few models. To address this challenge, we propose an automated online experimentation mechanism that efficiently performs model selection from a large pool of models using a small number of online experiments. By training a Bayesian surrogate model with historical logs, we obtain the probability distribution of the metric of interest, which captures model uncertainty. Our method identifies the best model by sequentially selecting and deploying a list of models from the candidate set, ensuring a balance between exploration and exploitation. Through simulations based on real data, we demonstrate the effectiveness of our approach on two different tasks.