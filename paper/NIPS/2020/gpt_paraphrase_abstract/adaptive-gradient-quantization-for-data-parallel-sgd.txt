Many variants of SGD that prioritize efficient communication use gradient quantization techniques. However, these techniques are often based on heuristics and remain fixed throughout the training process. To address this limitation, we conducted empirical observations and found that the gradient statistics of deep models change during training. Building upon this insight, we propose two adaptive quantization schemes, ALQ and AMQ. In these schemes, processors update their compression methods in parallel by efficiently calculating sufficient statistics from a parametric distribution. Our experiments demonstrate that these adaptive schemes lead to an improvement in validation accuracy of nearly 2% on CIFAR-10 and 1% on ImageNet, particularly in low-cost communication setups. Additionally, our adaptive methods exhibit greater robustness to hyperparameter selection.