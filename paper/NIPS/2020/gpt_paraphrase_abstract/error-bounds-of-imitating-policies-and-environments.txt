This study examines the value gap between expert policies and imitated policies generated by two imitation learning methods: behavioral cloning and generative adversarial imitation. The findings indicate that generative adversarial imitation reduces compounding errors compared to behavioral cloning, resulting in better sample complexity. Additionally, the study explores the use of imitation learning to learn environment models by considering the environment transition model as a dual agent. The results demonstrate that generative adversarial imitation is more effective than behavioral cloning in imitating environment models, suggesting a novel application of adversarial imitation for model-based reinforcement learning. These insights have the potential to inspire future advancements in imitation learning and model-based reinforcement learning.