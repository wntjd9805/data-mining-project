Neural networks tend to perform better as their size increases. Research on over-parameterized networks suggests that this improvement is due to a more favorable loss landscape. In this study, we explore over-parameterization through ensembling, where multiple independent models with the same architecture are trained together. We find that the optimization process becomes simpler when the number of models in the ensemble is large, resembling wide models but with better scalability. By utilizing theoretical findings on the Neural Tangent Kernel, we efficiently search for optimal architectures that either minimize capacity or maximize trainability within certain constraints. These ensembles can be implemented practically using group convolutions and block diagonal layers. Additionally, we demonstrate how our framework allows for the analytical derivation of optimal group convolution modules, eliminating the need for expensive grid searches or model training.