Neural networks have proven to be highly effective in classifying structured, high-dimensional datasets. However, there is still limited understanding of how these networks learn. This study focuses on the training dynamics of a simple neural network with a single hidden layer for classification tasks. By examining the mean-field limit, we find that this case can be simplified to a single-node learning problem with a time-dependent dataset determined by the average population of nodes. We specifically analyze the dynamics for linearly separable data and a linear hinge loss, providing explicit solutions in the infinite dataset limit. This enables us to explore various phenomena observed in modern networks, such as training slowdown, the transition between rich and lazy learning, and overfitting, in a straightforward manner. Additionally, we evaluate the applicability of mean-field theory by considering scenarios with a large but finite number of nodes and training samples.