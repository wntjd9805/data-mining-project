We introduce a new method for continual learning called AGS-CL, which uses two group sparsity-based penalties. Our approach applies these penalties selectively to each neural network node based on its importance, which is updated adaptively after learning each task. By using the proximal gradient descent method, we ensure exact sparsity and freezing of the model during the learning process, allowing the learner to control the model's capacity. Additionally, we re-initialize the weights of unimportant nodes after each task to facilitate efficient learning and prevent negative transfer. Our AGS-CL method requires significantly less memory space for storing regularization parameters compared to other techniques and achieves superior performance on various benchmarks for both supervised and reinforcement learning.