Actor-critic methods have been successful in reinforcement learning, with the actor improving the learning policy and the critic estimating the policy gradient. While the asymptotic convergence of actor-critic methods has been extensively studied, their non-asymptotic convergence and sample complexity remain largely unknown. This study presents a non-asymptotic analysis of two time-scale actor-critic methods in a non-i.i.d. setting. It is proven that the actor-critic method can find a first-order stationary point of the non-concave performance function, with a sample complexity of (cid:15)âˆ’2.5. This is the first work to provide finite-time analysis and sample complexity bounds for two time-scale actor-critic methods.