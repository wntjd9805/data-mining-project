We propose that an agent without a predefined model, but with representations that can predict future states beyond rewards, will be better at solving and adapting to new reinforcement learning problems. To test this, we use Deep InfoMax (DIM) to train the agent to predict the future by maximizing the mutual information between its internal representation of successive time steps. Our experiments in synthetic settings show that our approach successfully learns representations that can predict the future. Furthermore, we enhance the performance of C51, a strong reinforcement learning baseline, by incorporating our temporal DIM objective. This augmentation leads to improved performance on a continual learning task and the Procgen environment.