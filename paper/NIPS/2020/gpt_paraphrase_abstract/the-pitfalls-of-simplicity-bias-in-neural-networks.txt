The Simplicity Bias (SB) is a concept that explains why neural networks tend to generalize well during training. However, the exact definition of simplicity is unclear, and previous studies have not considered the non-robustness of neural networks. To address this, we introduce piecewise-linear and image-based datasets that incorporate a precise notion of simplicity and capture the non-robustness of neural networks trained on real data. Through theoretical analysis and experiments, we make several observations: SB can be extreme, causing neural networks to solely rely on simple features and disregard complex ones. This extreme SB can explain why distribution shifts and adversarial perturbations degrade model performance. Surprisingly, SB can also hinder generalization even when simple features are less predictive than complex ones. Traditional approaches like ensembles and adversarial training may not effectively mitigate SB and its drawbacks. Our proposed datasets and methods can serve as a testbed for evaluating new algorithms that aim to overcome the pitfalls of SB in neural network training.