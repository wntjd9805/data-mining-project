Recent research has seen a rise in batch Deep Reinforcement Learning (DRL), where the objective is to learn an effective policy from a given dataset without additional interactions with the environment. In this study, we introduce a new algorithm called Best-Action Imitation Learning (BAIL) that prioritizes simplicity and performance. BAIL learns a V function, utilizes this function to select high-performing actions, and then trains a policy network using imitation learning based on these actions. To assess the effectiveness of BAIL, we conduct a thorough experimental analysis on the MuJoCo benchmark, comparing its performance to four other batch Q-learning and imitation-learning approaches across various batch datasets. The results demonstrate that BAIL outperforms the other methods in terms of performance and computational efficiency.