Reward-free reinforcement learning (RL) is a framework suitable for batch RL and scenarios with multiple reward functions. During exploration, an agent collects samples without using a predefined reward function. After exploration, a reward function is provided, and the agent uses the collected samples to compute a nearly optimal policy. Jin et al. [2020] demonstrated that in the tabular setting, the agent only needs a polynomial number of samples based on the number of states, actions, and planning horizon for reward-free RL. However, in real-world applications, the number of states and actions can be large, necessitating function approximation for generalization. In this study, we present positive and negative outcomes for reward-free RL with linear function approximation. We propose an algorithm for reward-free RL in the linear Markov decision process setting, where both the transition and reward have linear representations. Our algorithm's sample complexity is polynomial in the feature dimension and planning horizon, independent of the number of states and actions. Additionally, we provide an exponential lower bound for reward-free RL when only the optimal Q-function has a linear representation. Our findings reveal interesting exponential differences in the sample complexity of reward-free RL.