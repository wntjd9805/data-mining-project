This paper introduces a new method for text representation in natural language that focuses on learning embeddings with desirable properties related to the distributional tails. Unlike existing approaches that use embeddings learned on large corpora, this method allows for the analysis of points far away from the distribution bulk by applying multivariate extreme value theory. By utilizing this method, a classifier dedicated to the tails of the embedding is obtained, which exhibits a scale invariance property. This property is then exploited in a novel text generation technique for dataset augmentation while preserving labels. Experimental results using both synthetic and real text data demonstrate the effectiveness of this framework, as it generates meaningful sentences with controllable attributes like positive or negative sentiments.