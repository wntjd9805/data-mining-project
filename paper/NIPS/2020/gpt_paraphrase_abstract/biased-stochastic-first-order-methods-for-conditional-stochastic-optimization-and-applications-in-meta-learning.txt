Conditional stochastic optimization is a challenging task due to its composition structure, making it difficult to construct unbiased gradient estimators. In this study, we introduce a biased stochastic gradient descent (BSGD) algorithm as an alternative approach and analyze the tradeoff between bias and variance under different structural assumptions. We determine the sample complexities of BSGD for strongly convex, convex, and weakly convex objectives under both smooth and non-smooth conditions. Our analysis reveals that the sample complexities of BSGD cannot be improved for general convex objectives and nonconvex objectives, except for smooth nonconvex objectives with Lipschitz continuous gradient estimator. To address this special case, we propose an accelerated algorithm called biased SpiderBoost (BSpiderBoost) that matches the lower bound complexity. Furthermore, we perform numerical experiments on invariant logistic regression and model-agnostic meta-learning to demonstrate the effectiveness of BSGD and BSpiderBoost.