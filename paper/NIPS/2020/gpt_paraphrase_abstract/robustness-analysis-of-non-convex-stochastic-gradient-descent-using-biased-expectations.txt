This study presents a fresh analysis of stochastic gradient descent (SGD) for smooth and non-convex optimization. The analysis focuses on the influence of the probability distribution of gradient noise on the convergence rate of the gradient norm. For sub-Gaussian and centered noise, the study proves that the number of iterations required to reach a desired precision for the squared gradient norm is O(ε−2 ln(1/δ)), where δ is a parameter. In the case of centered and integrable heavy-tailed noise, it is shown that although the expectation of the iterates may be infinite, the squared gradient norm still converges with probability 1 iterations, with a convergence rate of O(ε−pδ−q), where p and q are greater than 2. This finding indicates that SGD is resilient to gradient noise with unbounded variance, which is relevant for DeepLearning. Furthermore, the study suggests that selecting a step size proportional to T −1/b, where b is the tail-parameter of the noise and T is the number of iterations, yields the best convergence rates. Both results are derived from a unified analysis using the innovative concept of biased expectations, a straightforward and intuitive mathematical tool for deriving concentration inequalities. The study also introduces a new metric for quantifying the amount of noise added to the gradient and discusses its significance in various scenarios.