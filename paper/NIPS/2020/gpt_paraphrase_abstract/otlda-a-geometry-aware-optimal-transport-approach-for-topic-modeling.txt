We introduce a new approach to topic modeling that incorporates semantic regularities in language. While existing models like LDA neglect these regularities, recent attempts to bridge this gap have been limited to specific document lengths. Our framework utilizes optimal transport distance to integrate word semantics, taking advantage of efficient computation techniques. By leveraging the geometric structures of semantically related words, our approach produces more interpretable topics. Experimental results demonstrate that our framework outperforms other methods in terms of topic coherence and classification accuracy, across both long and short documents.