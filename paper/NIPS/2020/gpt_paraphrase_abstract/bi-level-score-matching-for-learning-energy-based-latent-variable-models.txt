Score matching (SM) is a useful technique for learning energy-based models (EBMs) without the need to calculate the partition function. However, it has limited applicability in learning energy-based latent variable models (EBLVMs), except for certain special cases. This study proposes a bi-level score matching (BiSM) approach to overcome this limitation and learn EBLVMs with general structures. BiSM reformulates SM as a bi-level optimization problem, with the higher level incorporating a variational posterior of the latent variables and optimizing a modified SM objective, while the lower level optimizes the variational posterior to fit the true posterior. To efficiently solve BiSM, a stochastic optimization algorithm with gradient unrolling is developed. The consistency of BiSM and the convergence of the stochastic algorithm are theoretically analyzed. Empirical results demonstrate the effectiveness of BiSM in Gaussian restricted Boltzmann machines and highly nonstructural EBLVMs parameterized by deep convolutional neural networks. BiSM performs comparably to widely used contrastive divergence and SM methods when applicable, and it can successfully learn complex EBLVMs with intractable posteriors for generating natural images.