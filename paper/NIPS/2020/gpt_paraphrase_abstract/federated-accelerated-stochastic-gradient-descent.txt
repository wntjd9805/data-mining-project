We present Federated Accelerated Stochastic Gradient Descent (FEDAC), a principled improvement to the distributed optimization method known as Federated Averaging (FEDAVG or Local SGD). FEDAC is the first provable acceleration technique for FEDAVG that enhances convergence speed and communication efficiency for different types of convex functions. In particular, when using M workers for strongly convex and smooth functions, FEDAC achieves a linear speedup in M with only approximately three rounds of synchronization, compared to the previous state-of-the-art FEDAVG analysis which required approximately M rounds. Additionally, we provide stronger guarantees for FEDAC when dealing with third-order smooth objectives. Our approach is based on a novel analysis technique involving potential-based perturbed iterate analysis, stability analysis of generalized accelerated SGD, and a strategic tradeoff between acceleration and stability.