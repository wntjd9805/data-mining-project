Despite the recent success of neural networks, there are challenges in understanding their complexity. Previous studies have focused on the network architecture while assuming that the weights within the network are arbitrary. However, it is more reasonable to assume that these weights possess some generic properties that enable efficient learning. This assumption is supported by the belief that real-world networks have weights that exhibit random-like properties with respect to natural distributions. In this study, we present negative results by showing that for depth-2 networks and various natural weight distributions such as the normal and uniform distribution, most networks are difficult to learn. This means that there is no efficient learning algorithm that can reliably succeed for most weights and input distributions. Consequently, there is no generic property that can be applied to random networks to facilitate efficient learning.