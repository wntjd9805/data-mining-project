We explore the threat of targeted adversarial attacks in deep neural network (DNN) image classifiers using a blackbox transfer-based approach. Instead of solely focusing on crossing decision boundaries at the output layer, our method perturbs representations across the feature hierarchy to resemble different classes. Our attack framework allows for multi-layer perturbations and achieves state-of-the-art targeted transfer performance between ImageNet DNNs. We also demonstrate the effectiveness of our feature space methods even when the source and target models are trained on different datasets and label spaces, achieving a significantly higher targeted success rate compared to other blackbox transfer methods. Additionally, we analyze the reasons behind the superior performance of our proposed methods and present an extension for cases with limited queries to the blackbox model.