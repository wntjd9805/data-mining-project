Online continual learning is a difficult task where a model must learn from a continuous flow of data without revisiting previously encountered instances. The problem of catastrophic forgetting is exacerbated in this scenario, as the model must address forgetting at both the task-level and the instance-level within the same task. To address this issue, we utilize the concept of "instance awareness" in neural networks. Each data instance is classified by a path in the network, determined by a controller that searches a meta-graph. To preserve knowledge from previous instances, we propose a method that restricts gradient updates for an instance if it is not similar to previous instances, preventing it from overriding past updates. Conversely, if an incoming instance is similar to previous instances, the method encourages fine-tuning of the path. The controller naturally selects paths based on instance similarity, and it is compact and updated online. Experimental results demonstrate that our proposed method outperforms existing approaches in online continual learning. Additionally, we evaluate our method in a realistic setting where task boundaries are unclear, and the results confirm its superiority on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.