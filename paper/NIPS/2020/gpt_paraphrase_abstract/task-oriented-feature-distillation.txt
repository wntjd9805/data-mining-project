Feature distillation is a widely used technique for improving accuracy in knowledge distillation. Existing methods typically use manually designed transformations to distill features in the teacher network. In this paper, we propose a new method called task-oriented feature distillation (TOFD) that utilizes convolutional layers trained with task loss to capture and distill task-specific information in the features to students. Additionally, we introduce an orthogonal loss to the feature resizing layer in TOFD, which enhances the performance of knowledge distillation. Experimental results demonstrate that TOFD significantly outperforms other distillation methods in both image classification and 3D classification tasks. The code for TOFD is available on Github3.