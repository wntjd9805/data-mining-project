Saliency methods are commonly used to highlight important input features in model predictions, but their application to time series data has been relatively unexplored. This study aims to compare the performance of different saliency-based interpretability methods on various neural architectures, including Recurrent Neural Networks, Temporal Convolutional Networks, and Transformers, using a new benchmark for synthetic time series data. Multiple metrics are proposed and reported to evaluate the effectiveness of these methods in detecting feature importance over time, considering precision and recall. The findings suggest that in general, both network architectures and saliency methods struggle to accurately identify feature importance over time in time series data. This can be attributed to the confusion between the time and feature domains. However, the study introduces a two-step temporal saliency rescaling (TSR) approach that significantly improves the quality of saliency maps. This approach first calculates the importance of each time step before evaluating the importance of each feature at that specific time step.