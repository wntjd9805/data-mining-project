Deep reinforcement learning (RL) agents often struggle to generalize to new testing environments when trained on a limited set of environments, leading to overfitting. Previous attempts to address this issue, such as data augmentation techniques like cutout and random convolution, have had limited success in increasing data diversity and improving generalization. In this study, we propose a new approach called mixreg, which trains agents on a mixture of observations from different training environments while imposing linearity constraints on observation and supervision interpolations. This effectively enhances data diversity and promotes the learning of smoother policies. We validate the effectiveness of mixreg by conducting extensive experiments on the large-scale Procgen benchmark. The results demonstrate that mixreg significantly outperforms established baselines in unseen testing environments. Furthermore, mixreg is a simple, effective, and versatile technique that can be applied to both policy-based and value-based RL algorithms. The code for mixreg is available at https://github.com/kaixin96/mixreg.