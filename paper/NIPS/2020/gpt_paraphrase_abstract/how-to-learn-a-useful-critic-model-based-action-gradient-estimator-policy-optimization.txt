Deterministic-policy actor-critic algorithms aim to enhance the actor's performance in continuous control tasks by utilizing the critic. This is achieved by ascending the action-value gradient, which is obtained by combining the actor's Jacobian matrix with the critic's gradient in relation to input actions. However, the critic is typically trained solely to predict expected returns, which is insufficient for policy optimization. In this study, we present a model-based actor-critic algorithm called MAGE, which is rooted in policy gradients theory and explicitly learns the action-value gradient. MAGE utilizes backpropagation through the learned dynamics to compute gradient targets in temporal difference learning, resulting in a critic specifically designed for policy improvement. Through experiments on MuJoCo continuous-control tasks, we demonstrate the efficacy of MAGE compared to state-of-the-art model-free and model-based baselines.