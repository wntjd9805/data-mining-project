This study presents a novel technique to identify and reduce the impact of ambiguous or mislabeled samples in training sets for neural networks. The proposed method utilizes the Area Under the Margin (AUM) statistic, which leverages differences in the training dynamics of accurate and mislabeled samples. By introducing an additional class containing purposely mislabeled threshold samples, the algorithm learns an upper bound of AUM that effectively isolates mislabeled data. The approach consistently outperforms previous methods on both synthetic and real-world datasets. In the WebVision50 classification task, our method removes 17% of training data, resulting in a significant 1.6% improvement in test error. Similarly, on CIFAR100, eliminating 13% of the data leads to a 1.2% decrease in error.