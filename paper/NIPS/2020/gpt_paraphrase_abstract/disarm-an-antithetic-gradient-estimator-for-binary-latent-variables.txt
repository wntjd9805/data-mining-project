Training models with discrete latent variables is difficult due to the challenge of accurately estimating gradients. Recent progress has been made by using continuous relaxations, but this approach is not always possible. An alternative method called Augment-REINFORCE-Merge (ARM) estimator uses continuous augmentation instead of relaxation. By applying antithetic sampling, a low-variance and unbiased estimator can be obtained for models with binary latent variables. However, the augmentation process increases variance. To address this, we propose a new estimator called DisARM which analytically integrates out the randomness introduced by augmentation, leading to substantial variance reduction. DisARM is simple to implement and has the same computational cost as ARM. We evaluate DisARM on generative modeling benchmarks and find that it consistently outperforms ARM and a strong independent sample baseline in terms of both variance and log-likelihood. Additionally, we propose a local version of DisARM for optimizing the multi-sample variational bound, which outperforms the current state-of-the-art method VIMCO.