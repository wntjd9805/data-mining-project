Offline reinforcement learning methods have the potential to bridge the gap between research and real-world applications. They allow for learning policies from offline datasets, addressing concerns related to online data collection such as cost, safety, and ethical issues. This paper introduces a benchmark called RL Unplugged, which evaluates and compares offline RL methods. RL Unplugged includes diverse datasets from various domains, including games and simulated motor control problems. These datasets cover different types of observability, action types, and dynamics. The paper proposes evaluation protocols for each domain and conducts a comprehensive analysis of supervised learning and offline RL methods using these protocols. The authors plan to release data and open-source algorithms for all tasks. The aim is to enhance experiment reproducibility, enable the study of challenging tasks with limited computational resources, and make RL research more systematic and accessible. RL Unplugged will continue to evolve with contributions from the research community and the authors. The project page can be found on GitHub.