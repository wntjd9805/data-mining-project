Classical learning theory suggests that machine learning models perform best at an intermediate level of complexity, with simpler models having high bias and more complex models having high variance. However, deep learning models defy this trade-off by achieving low bias and variance even in overparameterized scenarios. The challenge lies in understanding the multiple sources of randomness in deep learning algorithms. To address this, we propose an interpretable decomposition of variance into sampling, initialization, and label-related components. We analyze this decomposition for random feature kernel regression and discover interesting patterns. While bias decreases with network width, variance exhibits non-monotonic behavior and can diverge at the interpolation boundary, even without label noise. This divergence is due to the interaction between sampling and initialization, which can be eliminated through bagging or ensemble learning.