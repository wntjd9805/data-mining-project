Neural conversation models have shown promise in generating informative and engaging responses by incorporating external knowledge. However, obtaining knowledge-grounded dialogues for training these models is challenging and costly. To address this issue, we propose a zero-resource approach that does not require context-knowledge-response triples for training. Instead, we represent the knowledge connecting a context and a response as latent variables and develop a variational approach to estimate a generation model using independent dialogue and knowledge corpora. Our evaluation on three knowledge-grounded dialogue benchmarks demonstrates that our model achieves comparable performance to state-of-the-art methods that rely on knowledge-grounded dialogues for training. Furthermore, our model exhibits strong generalization capabilities across different topics and datasets.