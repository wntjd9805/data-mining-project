This study focuses on improving the efficiency of convolutional neural networks (CNNs) by utilizing the inherent redundancy in their structure. The researchers introduce the concept of Composite Kernel structures, which enable efficient convolution operations through scaled sum-pooling components. They propose Structured Convolutions as a special case of Composite Kernels, which allows for a lower complexity and fewer weights in the convolution operation. This decomposition can be applied to 2D and 3D kernels as well as fully-connected layers. The researchers also introduce a Structural Regularization loss that encourages neural network layers to leverage this desired structure, resulting in negligible performance loss after training. By applying their method to various CNN architectures, they demonstrate "structured" versions of ResNets that are up to 2 times smaller and a new Structured-MobileNetV2 that is more efficient while maintaining an accuracy loss of only 1% on ImageNet and CIFAR-10 datasets. They also show similar structured versions of EfficientNet on ImageNet and HRNet architecture for semantic segmentation on the Cityscapes dataset. The researchers found that their method outperforms existing tensor decomposition and channel pruning methods in terms of complexity reduction.