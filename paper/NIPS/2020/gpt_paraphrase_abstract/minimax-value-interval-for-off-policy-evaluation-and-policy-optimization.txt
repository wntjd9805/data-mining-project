We examine minimax techniques for off-policy evaluation (OPE) utilizing value functions and marginalized importance weights. Although these methods offer potential solutions for the high variance issue in conventional importance sampling, there are a few key challenges that need to be addressed. Firstly, they necessitate function approximation and tend to be biased. To ensure reliable OPE, can we quantify these biases? Secondly, they are divided into two categories: "weight-learning" and "value-learning." Is it possible to merge these approaches? In this study, we provide positive responses to both inquiries. By making slight modifications to existing methods, we combine them into a single value interval that exhibits a unique form of double robustness. When either the value function or the importance weight class is accurately specified, the interval remains valid, and its length measures the misrepresentation of the other class. Our interval also offers a unified perspective and new insights into recent techniques. Additionally, we explore the implications of our findings on exploration and exploitation in off-policy policy optimization with insufficient data coverage.