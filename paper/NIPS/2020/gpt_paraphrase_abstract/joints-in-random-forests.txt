Decision Trees (DTs) and Random Forests (RFs) are widely used in machine learning and data science. However, they lack effective methods to handle missing features or detect outliers. This paper presents a new approach that interprets DTs and RFs as generative models, connecting them to Probabilistic Circuits. This reinterpretation creates Generative Decision Trees (GeDTs) and Generative Forests (GeFs), which are hybrid generative-discriminative models. GeDTs and GeFs can handle missing features and maintain consistency under certain assumptions. Empirical results show that these models outperform common techniques like K-nearest neighbour imputation and can naturally detect outliers.