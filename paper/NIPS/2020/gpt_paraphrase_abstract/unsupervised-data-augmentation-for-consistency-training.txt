Semi-supervised learning has shown promise in improving deep learning models with limited labeled data. Recent approaches have utilized consistency training on a large amount of unlabeled data to enforce model predictions to be robust to input noise. This study introduces a new perspective on the importance of high-quality noise in unlabeled examples, specifically by using advanced data augmentation techniques such as RandAugment and back-translation. By replacing simple noise operations with these methods, significant improvements are achieved across multiple language and vision tasks within the consistency training framework. For instance, on the IMDb text classification dataset, our method achieves an error rate of 4.20 with only 20 labeled examples, outperforming the state-of-the-art model trained on 25,000 labeled examples. Similarly, on the CIFAR-10 benchmark, our method surpasses previous approaches with an error rate of 5.43 using only 250 examples. Additionally, our method complements transfer learning, such as fine-tuning from BERT, and demonstrates improvements in high-data scenarios like ImageNet, whether with only 10% labeled data or a full labeled set with 1.3M additional unlabeled examples.