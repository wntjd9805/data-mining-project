Sparse neural networks are becoming increasingly important as the field aims to improve existing models by scaling them up while reducing power consumption and computational footprint. However, current methods for inducing performant sparse models still require dense parameters or gradients during training, which can be prohibitive for large models. This study proposes Top-KAST, a method that maintains constant sparsity throughout training. The effectiveness of this approach is demonstrated by its comparable or better performance than previous works on the ImageNet benchmark, while maintaining sparsity. The approach is also applied to language modeling, where sparse versions of large architectures can be run with fewer resources. Additionally, the proposed approach is straightforward and easily implementable in existing machine learning frameworks. The hope is that this contribution will enable the broader community to explore the potential of massive models without incurring massive computational costs.