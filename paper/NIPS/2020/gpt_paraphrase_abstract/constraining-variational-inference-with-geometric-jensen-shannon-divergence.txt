This study focuses on the issue of controlling divergences in latent space regularization for variational autoencoders. The goal is to reconstruct examples while also ensuring generalizable latent representations. The researchers propose a regularization mechanism based on the skew-geometric Jensen-Shannon divergence. They introduce a variation in JSG, which allows for an intuitive interpolation between forward and reverse KL in terms of both distributions and divergences. The potential benefits of this approach for VAEs are illustrated through low-dimensional examples, and quantitative and qualitative results are presented. The experiments show that skewing the variant of JSG leads to improved reconstruction and generation compared to baseline VAEs. Importantly, this approach is unsupervised and requires only one easily interpretable hyperparameter in the latent space.