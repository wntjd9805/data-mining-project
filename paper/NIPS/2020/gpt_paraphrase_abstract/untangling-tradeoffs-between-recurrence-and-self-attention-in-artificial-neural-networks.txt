Attention and self-attention mechanisms play a crucial role in deep learning for sequential tasks. However, recent advancements have relied on heuristic approaches without a comprehensive understanding of how attention affects model optimization and computation. Furthermore, these approaches require significant memory and computational resources that do not scale well. In this study, we conduct a formal analysis to investigate the impact of self-attention on gradient propagation in recurrent networks. Our analysis demonstrates that self-attention addresses the issue of vanishing gradients that arise when capturing long-term dependencies, and we provide concrete bounds for gradient norms. Utilizing these findings, we propose a relevancy screening mechanism inspired by memory consolidation, which allows for the efficient use of sparse self-attention with recurrence, while guaranteeing the avoidance of vanishing gradients. Through numerical experiments, we illustrate the tradeoffs between performance and computational resources by effectively balancing attention and recurrence. Based on our results, we suggest a specific research direction to enhance the scalability of attentive networks.