This study compares two methods for learning to map an input I to a function hI : X → R. The first method is an embedding-based approach where I is encoded as a conditioning signal and the function takes the form hI (x) = q(x, e(I)). The second method involves hypernetworks, where the weights θI of the function hI (x) = g(x; θI) are determined by a hypernetwork f as θI = f(I). The paper focuses on the property of modularity, which refers to the ability to effectively learn a different function for each input instance. The authors adopt an expressivity perspective and extend previous work to provide a lower bound on the complexity of neural networks as function approximators. The results demonstrate that under certain conditions, when the functions e and f are allowed to be large, the complexity of the hypernetwork method (g) can be significantly smaller than the embedding-based method (q). This highlights the modularity advantage of hypernetworks. Additionally, the study shows that for a structured target function, the overall number of trainable parameters in a hypernetwork is much smaller than that of a standard neural network and an embedding method.