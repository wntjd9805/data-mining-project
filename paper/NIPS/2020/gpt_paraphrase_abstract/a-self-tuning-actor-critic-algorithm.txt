This paper addresses the issue of manually selecting hyperparameters for reinforcement learning algorithms. The authors propose a method called Self-Tuning Actor-Critic (STAC) that uses metagradients to automatically adapt hyperparameters. STAC is applied to optimize the hyperparameters of an actor-critic loss function, discover auxiliary tasks, and improve off-policy learning using a leaky V-trace operator. The results of ablative studies show that the performance of STAC improves as more hyperparameters are adapted. When tested on the Arcade Learning Environment and the DM Control suite, STAC significantly improves the scores compared to previous methods.