We examine how graph neural networks perform in the task of semi-supervised node classification when the network exhibits heterophily or low homophily, meaning that connected nodes have different class labels and dissimilar features. Many popular GNNs struggle to generalize in this scenario and are outperformed by models that do not consider the graph structure, such as multilayer perceptrons. To address this limitation, we identify several key design elements that enhance learning from the graph structure in the presence of heterophily. These include separating ego- and neighbor-embeddings, incorporating higher-order neighborhoods, and combining intermediate representations. We combine these designs into a graph neural network called H2GCN and empirically evaluate their effectiveness. Our experiments, conducted on synthetic and real networks with heterophily, demonstrate that the identified designs improve GNN accuracy by up to 40% and 27% compared to models without them. Moreover, these designs also yield competitive performance in networks with homophily.