Orthogonal Monte Carlo (OMC) is a sampling algorithm that enforces orthogonality on samples to reduce variance. It has been widely used in various challenging machine learning applications, such as scalable kernel methods, predictive recurrent neural networks, generative models, and reinforcement learning. However, there is limited theoretical understanding of OMC. In this paper, we provide new insights into the theoretical principles of OMC by applying the theory of negatively dependent random variables, which leads to several concentration results. As a result, we achieve the first uniform convergence results for OMCs, leading to stronger guarantees for kernel ridge regression. Additionally, we introduce a novel extension of the method called Near-Orthogonal Monte Carlo (NOMC), which leverages the theory of algebraic varieties over finite fields and particle algorithms. NOMC consistently outperforms OMC in applications such as kernel methods and approximating distances in probabilistic metric spaces.