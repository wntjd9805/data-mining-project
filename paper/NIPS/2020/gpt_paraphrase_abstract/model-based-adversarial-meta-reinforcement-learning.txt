This paper introduces Model-based Adversarial Meta-Reinforcement Learning (AdMRL), a method that addresses the problem of task distribution shift in meta-reinforcement learning (meta-RL). Existing meta-RL algorithms tend to perform poorly when the test task distribution differs from the training task distribution. AdMRL aims to minimize the worst-case sub-optimality gap across all tasks by using a model-based approach. The proposed approach involves learning the dynamics model on a fixed task and finding the adversarial task for the current model. A formula for the gradient of the suboptimality with respect to the task parameters is derived, and the gradient estimator is efficiently implemented using the conjugate gradient method and a novel use of the REINFORCE estimator. The effectiveness of AdMRL is demonstrated through evaluations on continuous control benchmarks, showing superior performance in worst-case scenarios, generalization to out-of-distribution tasks, and efficiency in training and test time samples compared to existing meta-RL algorithms.