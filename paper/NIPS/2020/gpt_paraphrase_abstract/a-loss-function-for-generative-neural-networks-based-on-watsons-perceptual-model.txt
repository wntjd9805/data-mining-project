In order to effectively train Variational Autoencoders (VAEs) to produce realistic images, a loss function that accurately represents human perception of image similarity is necessary. We propose a novel loss function based on Watson's perceptual model, which calculates a weighted distance in frequency space and takes into account luminance and contrast masking. We have expanded this model to handle color images, improved its ability to handle translations by utilizing the Fourier Transform, eliminated artifacts caused by image block splitting, and ensured its differentiability. Experimental results demonstrate that VAEs trained with this new loss function generate high-quality, realistic image samples. In comparison to using the Euclidean distance and the Structural Similarity Index, the images produced were less blurry. Additionally, when compared to deep neural network based losses, our approach requires fewer computational resources and produces images with fewer artifacts.