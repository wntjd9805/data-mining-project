The Lottery Ticket Hypothesis suggests that every large neural network contains a smaller subnetwork that can achieve similar performance when trained in isolation. A stronger version of this hypothesis has recently been proven, stating that every sufficiently overparameterized network contains a subnetwork that can achieve comparable accuracy to the trained large network even without training. However, this result relies on strong assumptions and guarantees a polynomial reduction in the size of the large network compared to the target function. In this study, we remove the previous work's most limiting assumptions and provide significantly tighter bounds. Our findings show that the overparameterized network only requires a logarithmic factor in the number of neurons per weight of the target subnetwork, except for depth.