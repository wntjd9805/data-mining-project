Contrastive learning is an essential part of self-supervised learning for computer vision, as it helps train effective visual representations. Recent studies have emphasized the importance of heavy data augmentation and large sets of negatives in this learning process. However, the significance of hard negatives in contrastive learning has been overlooked. To address this, we propose incorporating hard negatives in the form of feature-level mixing, which can be computed efficiently. Through extensive testing, we demonstrate that our approach enhances the quality of visual representations learned by a state-of-the-art self-supervised learning method.