The use of weakly supervised object detection (WSOD) has become a valuable method for training object detectors using only image-level category labels. However, WSOD detectors are prone to detecting bounding boxes on salient objects, clustered objects, and discriminative object parts when object-level labels are not available. Additionally, image-level category labels do not ensure consistent object detection across different image transformations. To address these challenges, we propose a new training approach called Comprehensive Attention Self-Distillation (CASD) for WSOD. CASD balances feature learning among all object instances by computing comprehensive attention from multiple transformations and feature layers of the same images. It also enforces consistent spatial supervision on objects through self-distillation on the WSOD networks, approximating comprehensive attention using multiple transformations and feature layers. CASD achieves state-of-the-art results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO.