Potential-based reward shaping is a technique used to design effective reward functions that enhance the learning process. However, finding suitable potential functions for complex environments is challenging, similar to learning a value function from scratch. To address this issue, we propose a novel framework that combines graph representation learning with probabilistic inference in reinforcement learning. Our approach utilizes Graph Convolutional Networks to facilitate message passing from rewarding states. These propagated messages can then serve as potential functions for reward shaping, thereby expediting the learning process. Through empirical analysis, we demonstrate that our approach significantly enhances performance in both small and high-dimensional control problems.