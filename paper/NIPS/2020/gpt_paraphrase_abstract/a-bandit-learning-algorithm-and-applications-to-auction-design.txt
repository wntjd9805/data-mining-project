We examine online bandit learning, where an algorithm must make decisions and observe rewards at each time step. The aim is to design efficient algorithms that achieve a total reward close to the best fixed decision in hindsight. This paper introduces the concept of (λ, µ)-concave functions and presents a bandit learning algorithm that guarantees performance based on the concavity parameters λ and µ. The algorithm utilizes the mirror descent algorithm, with update directions following the gradient of the multilinear extensions of the reward functions. The regret bound induced by the algorithm is nearly optimal at O(T). The algorithm is applied to auction design, specifically welfare maximization, revenue maximization, and no-envy learning in auctions. For welfare maximization, a version of fictitious play in smooth auctions guarantees a competitive regret bound based on the smooth parameters. For revenue maximization, the algorithm focuses on simultaneous second-price auctions with reserve prices in multi-parameter environments, achieving at least 1/2 times the total revenue of the best fixed reserve prices in hindsight. In no-envy learning, the algorithm addresses the bandit item selection problem with submodular player valuation, providing an efficient 1/2-approximation no-envy algorithm.