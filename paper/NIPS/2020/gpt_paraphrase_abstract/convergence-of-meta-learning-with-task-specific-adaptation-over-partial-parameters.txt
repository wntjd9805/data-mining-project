The model-agnostic meta-learning (MAML) algorithm is successful in meta-learning but is computationally expensive due to updating all model parameters in both the inner and outer loops. A more efficient algorithm called ANIL was proposed, which updates only a small subset of parameters in the inner loop and has lower computational cost. However, the convergence of ANIL has not been studied. This paper investigates the convergence rate and computational complexity of ANIL under two types of inner-loop loss geometries: strongly-convex and nonconvex. The results demonstrate that the inner-loop loss geometry significantly affects the convergence performance of ANIL. Additionally, the complexity analysis quantifies the improved efficiency of ANIL compared to MAML. Experimental results on few-shot meta-learning benchmarks support the theoretical findings.