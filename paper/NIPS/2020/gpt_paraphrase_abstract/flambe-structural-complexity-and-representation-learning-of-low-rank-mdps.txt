To address the issue of high dimensionality in reinforcement learning (RL), it is common to assume that values or policies depend on a low dimensional feature space. This study focuses on the problem of learning these features. By assuming that the underlying dynamics can be represented by a transition matrix of low rank, we demonstrate that the task of representation learning is connected to a specific nonlinear matrix decomposition problem. We establish a clear relationship between these low rank Markov decision processes (MDPs) and latent variable models, which extends previous formulations like block MDPs for representation learning in RL. We propose an algorithm called FLAMBE that combines exploration and representation learning to achieve efficient RL in low rank transition models. Our analysis eliminates the need for reachability assumptions present in prior results on the simpler block MDP model, which holds its own significance.