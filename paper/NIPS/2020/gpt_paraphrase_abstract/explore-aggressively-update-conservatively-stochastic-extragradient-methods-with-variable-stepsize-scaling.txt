Extragradient methods are commonly used in machine learning for solving large-scale saddle-point problems due to their stability and fast convergence. These methods involve an extrapolation step before updating, which helps overcome non-convergence issues seen in gradient descent/ascent schemes. However, we demonstrate in this study that using vanilla extragradient with stochastic gradients can hinder convergence, even in simple bilinear models. To address this problem, we propose a double stepsize extragradient algorithm where the exploration step progresses at a more aggressive pace compared to the update step. This modification enables the algorithm to converge even with stochastic gradients, and we establish precise convergence rates under an error bound condition.