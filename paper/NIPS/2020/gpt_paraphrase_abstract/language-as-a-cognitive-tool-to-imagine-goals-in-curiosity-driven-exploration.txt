Developmental machine learning focuses on how artificial agents can learn a wide range of skills similar to how children do. These agents must be able to create and prioritize goals, as well as learn how to achieve them. Previous approaches have used fixed or learned goal spaces, limiting the agents to only sample goals within known boundaries. However, we argue that the ability to imagine goals outside of these boundaries is crucial for creative discoveries and continuous learning. Children achieve this by using language to imagine new outcomes and treat them as goals during play. In this study, we introduce IMAGINE, a deep reinforcement learning architecture that models this imaginative ability. Similar to children, these agents benefit from the guidance of a social peer who provides language descriptions. To effectively leverage goal imagination, agents must be able to interpret these descriptions and apply them to their imagined out-of-distribution goals. Modularity is key to achieving this generalization, as it allows for a separation between the learned goal-achievement reward function and policy, using deep sets, gated attention, and object-centered representations. We introduce the Playground environment to evaluate how goal imagination improves generalization and exploration compared to agents without this capacity. Additionally, we analyze the properties of goal imagination that contribute to these results and study the effects of modularity and social interactions.