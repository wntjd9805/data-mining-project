This study explores the concept of weight correlation in deep neural networks and discusses its impact on the networks' ability to generalize. In fully-connected layers, weight correlation is defined as the average similarity between weight vectors of neurons, while in convolutional layers, it is defined as the similarity between filter matrices. The research demonstrates that weight correlation should be incorporated into the PAC Bayesian framework for network generalization, and the resulting generalization bound is directly influenced by weight correlation. A new complexity measure is proposed, which incorporates weight correlation into the PAC Bayes measure and effectively ranks the generalization errors of different networks. Furthermore, a novel regularizer is developed for training, and extensive experiments confirm that it significantly reduces generalization error.