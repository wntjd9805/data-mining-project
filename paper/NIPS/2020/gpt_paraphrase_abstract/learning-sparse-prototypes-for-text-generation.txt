Prototype-driven text generation involves using non-parametric models that select from a library of sentence prototypes and modify them to generate text. While effective, these methods are inefficient during testing because they require storing and indexing the entire training corpus. Additionally, current methods often rely on heuristics to determine which prototypes to reference during training. In this paper, we propose a new generative model that automatically learns a sparse prototype support set while still achieving strong language modeling performance. We accomplish this by applying a prior that encourages sparsity in the prototype selection distribution and using amortized variational inference to learn a prototype retrieval function. Our experiments demonstrate that our model performs better than previous prototype-driven language models, while also reducing memory usage by up to 1000 times and improving test time speed by up to 1000 times. Furthermore, we show that the learned prototypes can capture different levels of semantics and syntax depending on the sparsity of prototype selection, and that specific sentence attributes can be controlled by specifying the prototype for generation.