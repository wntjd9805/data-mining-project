Humans have the ability to learn rules and create compositional rule-based systems from limited examples, particularly in language-related tasks. However, current neural architectures struggle to generalize in a similar compositional manner, especially when faced with systematic variations during evaluation. This study introduces a neuro-symbolic model that learns entire rule systems from a small number of examples. Instead of directly predicting outputs from inputs, the model is trained to deduce the explicit rules governing a given set of examples. Techniques from neural program synthesis are utilized. The rule-synthesis approach outperforms neural meta-learning techniques in three domains: an artificial instruction-learning domain, the SCAN challenge datasets, and translating number words into integers across various human languages.