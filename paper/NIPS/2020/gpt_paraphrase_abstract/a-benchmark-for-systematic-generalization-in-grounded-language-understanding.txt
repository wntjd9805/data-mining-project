This paper introduces a new benchmark called gSCAN, which aims to evaluate the ability of neural networks to understand and interpret novel compositions in situated language understanding. Unlike previous benchmarks that focused on syntactic aspects, gSCAN grounds the language in the states of a grid world, allowing for evaluations of linguistically motivated rules. For instance, agents are tested on their understanding of how adjectives like 'small' are interpreted in relation to the current world state, or how adverbs like 'cautiously' combine with new verbs. The study tests a strong multi-modal baseline model and a state-of-the-art compositional method, and finds that both models fail when faced with generalization that requires systematic compositional rules.