Graph neural networks (GNNs) have gained popularity for their simplicity and effectiveness in various fields. However, training these networks requires a large amount of labeled data, which can be expensive to obtain in certain domains. This study focuses on active learning for GNNs, specifically how to efficiently label graph nodes to reduce annotation costs. The problem is approached as a sequential decision process on graphs, training a GNN-based policy network using reinforcement learning to learn the optimal query strategy. By training on multiple source graphs with full labels, a transferable active learning policy is learned, which can be applied to unlabeled target graphs. Experimental results on diverse datasets demonstrate the effectiveness of the learned policy in improving active learning performance, both within the same domain and across different domains.