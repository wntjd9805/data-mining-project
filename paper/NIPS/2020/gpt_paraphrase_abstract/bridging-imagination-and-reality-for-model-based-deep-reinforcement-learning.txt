Deep reinforcement learning has faced challenges in terms of sample efficiency. To tackle this issue, a model-based approach called model-based reinforcement learning has emerged. This method involves planning on hypothetical trajectories using a learned world model. However, there is a risk of overfitting the world model to training trajectories, leading to suboptimal local policies. To address this, we introduce a new model-based reinforcement learning algorithm called BrIdging Reality and Dream (BIRD). BIRD aims to maximize the mutual information between imaginary and real trajectories, enabling the policy improvements learned from hypothetical trajectories to be applied to real-world scenarios easily. Our experiments demonstrate that BIRD enhances the sample efficiency of model-based planning and achieves outstanding results on challenging visual control benchmarks.