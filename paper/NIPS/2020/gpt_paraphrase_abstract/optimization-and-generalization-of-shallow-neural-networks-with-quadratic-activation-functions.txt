We investigate the optimization dynamics and generalization properties of one-hidden layer neural networks using quadratic activation function in the over-parametrized regime. This regime refers to cases where the width of the layer is greater than the input dimension. We explore a scenario where a teacher network, with a hidden layer of smaller width than the student network, guides the learning process. We analyze how the landscape of empirical loss is influenced by the number of data samples and the width of the teacher network. Specifically, we determine the conditions under which the neural network can recover the teacher by examining the probability of having no spurious minima on the empirical loss. Additionally, we demonstrate that gradient descent dynamics on the empirical loss converges and results in small generalization error, enabling practical recovery. We also investigate the convergence rate of gradient descent as the number of samples increases. Numerical experiments support these findings.