This paper examines the statistical limits of imitation learning (IL) in episodic Markov Decision Processes (MDPs). The focus is on the scenario where the learner is provided with a dataset of expert trajectories and cannot interact with the MDP. The paper demonstrates that a policy that mimics the expert whenever possible is, on average, suboptimal compared to the value of the expert. This suboptimality is quantified as (cid:46) |S|H 2 log(N ), where S is the state space and H is the length of the episode. A lower bound of (cid:38) |S|H 2/N is established, which holds even if the expert is deterministic or if the learner can query the expert during interactions with the MDP. This is the first algorithm with suboptimality that does not depend on the number of actions. Additionally, the paper proposes a new algorithm based on minimum-distance functionals for the case where the transition model is given and the expert is deterministic. This algorithm has a suboptimality of (cid:46) |S|H 3/2/N and overcomes the O(H 2) error compounding barrier of IL.