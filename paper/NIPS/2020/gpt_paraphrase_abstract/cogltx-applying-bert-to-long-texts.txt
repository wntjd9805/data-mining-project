BERT has limitations when it comes to processing long texts due to the increasing memory and time required. Common solutions, like slicing the text or simplifying transformers, have their own drawbacks. However, humans also have a limited working memory capacity for processing information. Inspired by cognitive theory, the proposed CogLTX framework addresses this issue by identifying key sentences, concatenating them, and enabling multi-step reasoning. Instead of using relevance annotations, interventions are used to create supervision. CogLTX outperforms or achieves similar results compared to state-of-the-art models in various tasks, with memory overheads that are not dependent on the text length. The answer format remains unchanged.