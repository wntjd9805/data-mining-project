Over-parameterized models like DeepNets and ConvNets are widely used but pose challenges for Bayesian inference. Variational inference provides a scalable solution, but it struggles with over-regularization in these models. This paper introduces Walsh-Hadamard Variational Inference (WHVI), inspired by kernel methods and structured approximations of random matrix distributions. WHVI utilizes Walsh-Hadamard-based factorization strategies to reduce parameterization and accelerate computations, avoiding over-regularization problems. Extensive theoretical and empirical analyses demonstrate that WHVI achieves significant speedups and model reductions compared to other approximate inference techniques for over-parameterized models. This shows how advancements in kernel methods can be applied to improve approximate Bayesian inference for Deep Learning.