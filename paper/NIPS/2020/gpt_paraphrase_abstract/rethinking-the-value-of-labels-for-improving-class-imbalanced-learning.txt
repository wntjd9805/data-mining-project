In the context of imbalanced learning, the value of labels is a persistent dilemma. While supervised learning typically yields better results than unsupervised learning, imbalanced data introduces "label bias" in the classifier. This bias can significantly alter the decision boundary due to the majority classes. In this study, we examine these two aspects of labels and demonstrate that imbalanced learning can benefit from both semi-supervised and self-supervised approaches. We show that leveraging unlabeled data with the original labels can reduce label bias and improve the final classifier. However, we also argue that pre-training classifiers in a self-supervised manner consistently outperform their baselines. Through extensive experiments on large-scale imbalanced datasets, we validate our strategies and achieve superior performance compared to previous state-of-the-art methods. These findings emphasize the need to reconsider the utilization of imbalanced labels in real-world long-tailed tasks. The code for this study is available at https://github.com/YyzHarry/imbalanced-semi-self.