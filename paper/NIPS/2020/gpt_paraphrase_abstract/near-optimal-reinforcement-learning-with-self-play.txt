This study addresses the issue of developing optimal algorithms for reinforcement learning in two-player zero-sum games. The focus is on self-play algorithms, which learn the best strategy by playing against themselves without external supervision. Currently, the most effective algorithm for approximating a Nash equilibrium in a tabular episodic Markov game with S states, A max-player actions, and B min-player actions requires approximately O(S2AB) game-playing steps. In contrast, the best lower bound scales as Î©(S(A + B)), demonstrating a significant gap from the upper bound. This paper aims to bridge this gap by introducing an optimistic variant of the Nash Q-learning algorithm with a sample complexity of O(SAB) and a new Nash V-learning algorithm with a sample complexity of O(S(A + B)). The latter result matches the information-theoretic lower bound in all problem-dependent parameters, except for a polynomial factor related to the length of each episode. Furthermore, the study presents a computational hardness result for learning the best responses against a fixed opponent in Markov games, which differs from the objective of finding the Nash equilibrium.