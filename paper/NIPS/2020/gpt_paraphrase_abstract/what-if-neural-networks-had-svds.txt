Various Neural Networks use computationally expensive matrix operations, such as matrix inversion. However, some of these operations can be computed faster by utilizing the Singular Value Decomposition (SVD). Techniques from previous studies (references [10, 17]) have been developed to incorporate the SVD into Neural Networks without the need for explicit computation. While these techniques have the potential to accelerate matrix operations in theory, their practical implementation falls short in terms of speed. In this study, we propose an algorithm that overcomes this limitation and achieves significant acceleration for several matrix operations. Our algorithm enhances the level of parallelism in the matrix multiplication process, specifically in cases where the matrix H is orthogonal and can be represented as a product of Householder matrices.