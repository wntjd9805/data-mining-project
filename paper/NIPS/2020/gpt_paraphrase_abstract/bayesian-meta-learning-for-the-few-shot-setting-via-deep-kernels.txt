Recently, new machine learning techniques have emerged to address the challenging task of few-shot learning, where there is only a small labeled dataset available for a specific task. One common approach is meta-learning, which involves learning to learn on new problems based on previous experience. We propose a Bayesian approach for the inner loop of meta-learning using deep kernels, which allows us to learn a kernel that can be transferred to new tasks. We refer to this method as DeepKernel Transfer (DKT). DKT offers several advantages, including easy implementation as a single optimizer, uncertainty quantification, and no need for estimating task-specific parameters. Through empirical evaluation, we demonstrate that DKT outperforms other state-of-the-art algorithms in few-shot classification and is also superior in cross-domain adaptation and regression tasks. This suggests that a simpler Bayesian model can replace complex meta-learning routines without sacrificing accuracy.