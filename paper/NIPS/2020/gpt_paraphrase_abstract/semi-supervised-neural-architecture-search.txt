Neural architecture search (NAS) requires a good controller to generate better architectures or predict their accuracy. However, training the controller is expensive and requires abundant and high-quality architecture-accuracy pairs. In this paper, we propose SemiNAS, a semi-supervised NAS approach that utilizes unlabeled architectures to overcome this challenge. SemiNAS trains an initial accuracy predictor with a small set of architecture-accuracy pairs, then uses this predictor to predict the accuracy of a large number of architectures without evaluation. The generated data pairs are added to the original data to improve the predictor further. This trained predictor can be used in various NAS algorithms to predict the accuracy of candidate architectures. SemiNAS offers two advantages: it reduces computational cost while achieving comparable accuracy with gradient-based methods, and it achieves higher accuracy with the same computational cost. It outperforms baselines on different benchmarks and tasks, including achieving 97% intelligibility rate and 15% test error rate improvements over the baseline in low-resource and robustness settings, respectively.