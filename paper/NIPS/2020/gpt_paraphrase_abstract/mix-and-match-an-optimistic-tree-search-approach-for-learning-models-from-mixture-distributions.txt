We address the issue of covariate shift, where multiple training datasets and a small validation set are available for the same learning problem. The datasets may differ due to unobserved features, causing a distribution shift. Our goal is to find the optimal mixture distribution over the training datasets, considering only observed features, in order to achieve the best performance on the validation set. Our proposed algorithm, called Mix&Match, combines stochastic gradient descent (SGD) with optimistic tree search and model re-use. We introduce a novel high probability bound on the final SGD iterate, independent of a global gradient norm bound, and demonstrate the benefits of model re-use. We also provide regret guarantees for our algorithm in terms of recovering the optimal mixture within a given SGD evaluation budget. Finally, we validate our algorithm on two real-world datasets.