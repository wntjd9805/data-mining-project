Despite its widespread use in deep learning, the backpropagation algorithm has limitations. It hinders parallelization of the training process due to its sequential layer updates and is also being questioned for its biological plausibility. Various alternative techniques have been proposed, but none have been able to handle modern deep learning tasks and architectures due to the constraint of synaptic asymmetry. In this study, we challenge this viewpoint and investigate the suitability of Direct Feedback Alignment (DFA) for neural view synthesis, recommender systems, geometric learning, and natural language processing. Unlike previous research focused on computer vision tasks, our results demonstrate that DFA can effectively train a wide range of state-of-the-art deep learning architectures, achieving performance similar to fine-tuned backpropagation. In cases where a larger discrepancy between DFA and backpropagation is observed, such as with Transformers, we suggest reevaluating common practices for large and complex architectures. Contrary to common beliefs, our work highlights that challenging tasks can be tackled without relying on weight transport.