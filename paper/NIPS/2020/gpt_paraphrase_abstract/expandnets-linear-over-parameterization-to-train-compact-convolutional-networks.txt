We present a method for training a compact network by leveraging over-parameterization, which is known to enhance optimization and generalization of neural networks. Our approach involves expanding each linear layer of the compact network into multiple consecutive linear layers, without introducing nonlinearity. This expanded network, called ExpandNet, can be easily contracted back to the compact network during inference. We propose two convolutional expansion strategies and demonstrate their advantages in various tasks like image classification, object detection, and semantic segmentation. Our experimental results show that our approach outperforms training the compact network from scratch and using knowledge distillation from a teacher. Additionally, our linear over-parameterization technique reduces gradient confusion during training and improves network generalization.