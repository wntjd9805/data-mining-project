Offline reinforcement learning (RL), also known as batch RL, offers the possibility of optimizing policies using pre-recorded datasets without real-time interaction with the environment. It tackles challenges related to the cost of data collection and safety, which are especially relevant in real-world RL applications. However, most off-policy algorithms struggle to learn effectively from fixed datasets. In this study, we introduce a new offline RL algorithm that utilizes critic-regularized regression (CRR) to learn policies from data. Surprisingly, CRR demonstrates impressive performance and scalability in tasks with high-dimensional state and action spaces. It outperforms several state-of-the-art offline RL algorithms by a significant margin across various benchmark tasks.