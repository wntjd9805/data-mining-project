As neural network parameterized differential equations become more complex during training, the computational cost of solving them increases. To address this issue, we propose a solution that encourages the learned dynamics to be easier to solve. Specifically, we introduce a differentiable substitute for the time cost of standard numerical solvers, utilizing higher-order derivatives of solution trajectories. These derivatives can be efficiently computed using Taylor-mode automatic differentiation. By optimizing this additional objective, we can balance model performance with the time required to solve the learned dynamics. Through experiments in supervised classification, density estimation, and time-series modeling tasks, we demonstrate that our approach enables significantly faster training while maintaining comparable accuracy.