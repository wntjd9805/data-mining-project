AI systems often rely on a specified goal and an optimization algorithm to determine optimal behavior. This approach aims to benefit the user on whose behalf the AI agent acts. However, the objectives given to these agents often only partially align with the user's goals. In this study, we analyze a model of a user and an AI agent in a resource-constrained world where the state has multiple attributes corresponding to the user's utility. We assume that the agent's reward function only considers a subset of these attributes. Our contributions include proposing a new model for incomplete user-agent problems in AI, establishing necessary and sufficient conditions where optimizing for incomplete objectives leads to low overall utility, and demonstrating that modifying the setup to incorporate full state references or allowing the user to update objectives can improve utility. These findings suggest that designing reward functions should be an interactive and dynamic process, highlighting the importance of interactivity in certain scenarios.