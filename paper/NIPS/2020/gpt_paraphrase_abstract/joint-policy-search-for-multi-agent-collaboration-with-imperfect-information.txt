Learning effective joint policies for multi-agent collaboration in the presence of imperfect information is a challenging task. While coordinate-ascent approaches have been successful in optimizing policies for two-player zero-sum games, they often lead to suboptimal outcomes in multi-agent cooperative settings. This is due to the complex interactions between policies in imperfect information games. In this paper, we propose a novel approach called Joint Policy Search (JPS) that decomposes global changes in game values into localized policy changes at each information set. By introducing a new term called policy-change density, JPS iteratively improves joint policies without re-evaluating the entire game. We demonstrate the effectiveness of JPS on collaborative tabular games and show that it outperforms existing algorithms designed for collaborative policy learning. Additionally, we apply JPS to the game of Contract Bridge, a 4-player imperfect-information game, where it significantly improves collaboration between team players and outperforms a championship-winning software. Our results demonstrate the potential of JPS in solving real-world games with exponential states. The code for JPS is available at https://github.com/facebookresearch/jps.