Recent research has identified a "double descent" phenomenon in deep learning, where increasing the number of training examples causes neural networks to have higher generalization error when the number of parameters is of the same order as the training examples. This phenomenon was previously observed in simpler models like linear regression, where the error peaks when the number of examples is equal to the input dimension. These peaks are often confused in the literature because they occur at the interpolation threshold. However, this paper demonstrates that these two scenarios are fundamentally different. In noisy regression tasks, both peaks can coexist, with the size of the peaks determined by the nonlinearity of the activation function. The paper provides a theoretical explanation for this "triple descent" phenomenon using random feature models. The nonlinear peak at N = P is a true divergence caused by the sensitivity of the output function to noise and initialization. This peak can be suppressed by regularization. On the other hand, the linear peak at N = D is solely due to overfitting noise and is influenced by the nonlinearity, making it more prominent at high noise levels and less affected by explicit regularization. The paper compares analytical results from the random feature model with numerical experiments using deep neural networks.