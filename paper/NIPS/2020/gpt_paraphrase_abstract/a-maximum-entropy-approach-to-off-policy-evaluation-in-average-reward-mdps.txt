This study focuses on off-policy evaluation (OPE) using function approximation in infinite-horizon undiscounted Markov decision processes (MDPs). We present a finite-sample OPE error bound for a model-based approach in ergodic and linear MDPs, extending previous results beyond episodic and discounted cases. In a more general scenario, where feature dynamics are approximately linear and rewards are arbitrary, we propose a heuristic approach to estimate stationary distributions with function approximation. This involves finding the maximum-entropy distribution that matches feature expectations under empirical dynamics. Our approach parallels maximum-entropy methods in supervised learning and demonstrates its effectiveness in various environments.