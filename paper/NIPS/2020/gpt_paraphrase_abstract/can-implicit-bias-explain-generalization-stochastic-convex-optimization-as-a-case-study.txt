The concept of implicit bias, also known as implicit regularization, has been proposed to explain why modern overparameterized learning algorithms have strong generalization abilities. Implicit bias refers to the optimization algorithm's tendency to converge towards a particular structured solution that often generalizes well. Recent studies have explored implicit regularization and identified this phenomenon in various scenarios. In this research, we focus on Stochastic Gradient Descent (SGD) in Stochastic Convex Optimization, which is a simple yet non-trivial setup. Initially, we prove that there is no distribution-independent implicit regularizer governing the generalization ability of SGD. Furthermore, we present a learning problem that disproves a broad class of distribution-dependent implicit regularizers, including strongly convex and norm-based regularizations, from explaining generalization. Our findings highlight the challenges in fully explaining an algorithm's generalization performance solely based on its implicit regularization properties.