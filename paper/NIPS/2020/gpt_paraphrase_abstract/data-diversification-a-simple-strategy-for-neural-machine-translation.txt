We present a technique called Data Diversification, which is a straightforward yet powerful approach to enhance the performance of neural machine translation (NMT). This method involves diversifying the training data by incorporating predictions from multiple forward and backward models and merging them with the original dataset used to train the final NMT model. Unlike other methods, our approach does not require additional monolingual data or increase computational complexity and parameters through model ensembles. Our technique achieves impressive BLEU scores of 30.7 and 43.7 in the WMTâ€™14 English-German and English-French translation tasks, respectively. It also significantly improves translation performance in eight other tasks, including four IWSLT tasks (English-German and English-French) and four low-resource translation tasks (English-Nepali and English-Sinhala). Our experiments demonstrate that our approach outperforms knowledge distillation and dual learning techniques, exhibits a strong correlation with model ensembles, and strikes a balance between perplexity and BLEU score.