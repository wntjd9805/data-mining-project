Learning representations of sets of nodes in a graph is crucial for various applications such as discovering node roles, predicting links, and classifying molecules. Graph Neural Networks (GNNs) have been successful in learning graph representations. However, the expressive power of GNNs is limited by the 1-Weisfeiler-Lehman (WL) test, causing them to generate identical representations for different graph substructures. More powerful GNNs have been proposed recently by mimicking higher-order WL tests, but they are computationally inefficient and cannot utilize the sparsity of the underlying graph. To address these limitations, we introduce Distance Encoding (DE), a class of structure-related features. DE enables GNNs to represent any set of nodes and offers greater expressive power than the 1-WL test. DE captures the distance between the node set and each node in the graph using graph-distance measures like shortest path distance or generalized PageRank scores. We present two approaches for GNNs to utilize DEs: (1) as additional node features and (2) as controllers of message aggregation. Both approaches take advantage of the sparse structure of the graph, leading to computational efficiency and scalability. We demonstrate that DE can distinguish node sets in almost all regular graphs where traditional GNNs fail. We evaluate DE on three tasks across six real networks and show that our models outperform GNNs without DE by up to 15% in accuracy and AUROC. Additionally, our models outperform other state-of-the-art methods specifically designed for these tasks.