This study proposes a novel approach for generating 3D human body textures from a single image using a human parsing based texture transfer model. By utilizing the semantic parsing of the human body, both the shape and pose information are incorporated to reduce appearance variation and preserve the spatial distribution of semantic parts. To enhance texture prediction for invisible parts, the consistency across different views of the same subject is enforced by exchanging predicted textures during training. The optimization process involves perceptual loss and total variation regularization to maximize similarity between rendered and input images, without requiring additional 3D texture supervision. Experimental results on pedestrian images and fashion photos demonstrate that our method produces higher quality textures with detailed accuracy compared to other texture generation techniques. The code for our model is available at https://github.com/zhaofang0627/HPBTT.