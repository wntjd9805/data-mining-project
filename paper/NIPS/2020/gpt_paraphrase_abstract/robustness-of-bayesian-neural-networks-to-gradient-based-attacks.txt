The vulnerability of deep learning to adversarial attacks is a major obstacle in its use for safety-critical applications. Despite significant efforts, the problem remains unsolved. This study examines the geometric properties of adversarial attacks on Bayesian Neural Networks (BNNs) in the large-data, overparametrized limit. It demonstrates that vulnerability to gradient-based attacks occurs when the data distribution becomes degenerate, meaning the data lies on a lower-dimensional submanifold. Consequently, in the limit, BNN posteriors are robust against gradient-based adversarial attacks. Experimental results on the MNIST and Fashion MNIST datasets, using BNNs trained with Hamiltonian Monte Carlo and Variational Inference, support this argument by showing that BNNs can achieve high accuracy while remaining robust against gradient-based adversarial attacks.