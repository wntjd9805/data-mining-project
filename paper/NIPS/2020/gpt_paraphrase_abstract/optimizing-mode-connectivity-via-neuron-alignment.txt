The loss landscapes of deep neural networks are not well understood due to their complex nature. Local minima of these loss functions can be connected by a learned curve called mode connectivity. However, current curve finding algorithms do not consider the impact of symmetry in the loss surface caused by weight permutations. To address this, we propose a framework that explores the effect of symmetry on landscape connectivity by accounting for weight permutations. We introduce a heuristic called neuron alignment to approximate the optimal permutation by promoting similarity between intermediate activations along the curve. Theoretical analysis confirms the benefits of alignment on mode connectivity. Empirical results demonstrate that optimizing weight permutations is crucial for efficiently learning a low-loss curve between networks that generalizes well. Our alignment method also alleviates the robust loss barrier between adversarial robust models, leading to more robust and accurate models. The code for our method is available at https://github.com/IBM/NeuronAlignment.