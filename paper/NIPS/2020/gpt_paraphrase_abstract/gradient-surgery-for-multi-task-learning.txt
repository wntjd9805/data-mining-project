Deep learning and deep reinforcement learning have shown impressive results in various domains, but data efficiency remains a challenge. Multi-task learning has emerged as a promising approach to improve efficiency by sharing structure across tasks. However, optimizing multi-task learning is difficult compared to single-task learning, and the reasons for this are not fully understood. In this study, we identify three conditions that cause detrimental gradient interference in the multi-task optimization landscape and propose a simple solution to avoid this interference. Our approach, called gradient surgery, projects a task's gradient onto the normal plane of any other task with conflicting gradients. We demonstrate the effectiveness of this approach on challenging multi-task supervised and RL problems, resulting in significant improvements in efficiency and performance. Furthermore, this approach is model-agnostic and can be combined with existing multi-task architectures for enhanced performance.