We present the SE(3)-Transformer, a modified self-attention module designed for 3D point clouds and graphs. This variant ensures equivariance under continuous 3D roto-translations, which is crucial for stable and predictable performance when dealing with transformations in the input data. Equivariance also enhances weight-tying within the model. By leveraging self-attention, the SE(3)-Transformer can effectively handle large point clouds and graphs with varying numbers of points while maintaining SE(3)-equivariance for robustness. Our model is evaluated on a toy N-body particle simulation dataset, demonstrating its robustness against input rotations. It also performs competitively on real-world datasets, namely ScanObjectNN and QM9. In all scenarios, our model outperforms a strong attention baseline that lacks equivariance and an equivariant model without attention.