We propose that capturing the predictive information, which refers to the mutual information between the past and the future (I(Xpast; Xfuture)), can be beneficial in reinforcement learning (RL) as it is crucial for successful completion of various tasks. To test this hypothesis, we trained Soft Actor-Critic (SAC) agents using pixel inputs and an auxiliary task that learns a condensed representation of the predictive information in the RL environment dynamics. This was achieved through a contrastive version of the Conditional Entropy Bottleneck (CEB) objective. These agents, referred to as Predictive Information SAC (PI-SAC) agents, demonstrated significant improvement in sample efficiency compared to challenging baseline methods on continuous control tasks from the DM Control suite. We evaluated the performance of PI-SAC agents against uncompressed PI-SAC agents, other compressed and uncompressed agents, as well as SAC agents trained directly from pixels. The implementation details can be found on our GitHub repository.