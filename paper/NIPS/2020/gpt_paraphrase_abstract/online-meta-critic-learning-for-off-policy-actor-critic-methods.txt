Off-Policy Actor-Critic (OffP-AC) methods have achieved success in various continuous control tasks. Typically, the critic's action-value function is updated using temporal-difference, and this critic then guides the actor by providing a loss that encourages actions with higher expected return. In this study, we propose a flexible meta-critic framework that observes the learning process and meta-learns an additional loss for the actor, enhancing and expediting the actor-critic learning. Unlike existing meta-learning algorithms, our meta-critic is learned quickly online for a single task, rather than gradually over multiple tasks. Importantly, our meta-critic is specifically designed for off-policy learners, which are currently the most efficient in reinforcement learning. We demonstrate that combining online meta-critic learning with contemporary OffP-AC methods such as DDPG, TD3, and SAC yields benefits in various continuous control tasks.