This study examines the impact of L2 regularization in deep learning and uncovers the relationship between model performance, the L2 coefficient, learning rate, and training steps. These relationships are found to be consistent in overparameterized networks and can be used to predict the optimal regularization parameter for a specific model. Furthermore, based on these findings, a dynamic schedule for the regularization parameter is proposed, resulting in improved performance and faster training. The proposed approaches are tested in modern image classification scenarios. Finally, the study demonstrates that these empirical relationships can be theoretically understood in the context of infinitely wide networks by analyzing the gradient flow dynamics. The role of L2 regularization in this context is compared to that of linear models.