We introduce Reverse Reinforcement Learning (Reverse RL) as a method to capture retrospective knowledge. While General Value Functions (GVFs) have been effective in representing predictive knowledge, they are unable to address questions related to past events. To answer such questions, we propose Reverse GVFs, which utilize Reverse RL to understand the influence of past events on the present. Through empirical analysis, we demonstrate the effectiveness of Reverse GVFs in representation learning and anomaly detection. The output format focuses on providing an abstraction of the answer.