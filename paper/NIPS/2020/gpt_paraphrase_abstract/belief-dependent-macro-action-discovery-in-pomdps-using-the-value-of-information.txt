This study presents a novel approach to macro-action discovery in partially observable Markov decision processes (POMDPs) using value-of-information (VoI). POMDPs are commonly used for planning under uncertainty. Previous methods have incorporated high-level macro-actions in POMDP policies to simplify planning. However, these macro-actions are often heuristic and lack performance guarantees. In this work, we propose a method to extract belief-dependent, variable-length macro-actions directly from a low-level POMDP model. We create macro-actions by combining sequences of open-loop actions when the task-specific value of information (VoI) is low. Importantly, we offer performance guarantees for the resulting VoI macro-action policies in terms of bounded regret compared to the optimal policy. Through simulated tracking experiments, we demonstrate that our approach outperforms closed-loop and hand-coded macro-action baselines by selectively using VoI macro-actions to reduce planning complexity while maintaining near-optimal task performance.