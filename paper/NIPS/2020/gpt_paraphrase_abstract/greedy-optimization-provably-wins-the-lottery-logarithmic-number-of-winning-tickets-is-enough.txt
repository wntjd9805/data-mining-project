Despite the success of deep learning, recent research has shown that large deep neural networks can be significantly reduced in size without sacrificing accuracy. However, it is still unknown how much we can prune a neural network while maintaining a specified level of accuracy. This paper introduces a greedy optimization-based pruning method that offers a solution to this question. The proposed method guarantees that the difference between the pruned network and the original network decreases rapidly as the size of the pruned network decreases. This guarantee holds true under weak assumptions that are applicable to most practical scenarios. Empirically, our method outperforms previous approaches when pruning different network architectures such as ResNet, MobilenetV2/V3 on ImageNet.