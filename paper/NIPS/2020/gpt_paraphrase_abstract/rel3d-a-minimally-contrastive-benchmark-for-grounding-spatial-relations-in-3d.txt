This paper addresses the need for a large-scale, high-quality dataset with 3D ground truth information to improve the understanding of spatial relations in visual input. The authors introduce Rel3D, the first human-annotated dataset for grounding spatial relations in 3D. By utilizing this dataset, the effectiveness of 3D information in predicting spatial relations can be measured on a large-scale human data. Additionally, the authors propose a novel crowdsourcing method called minimally contrastive data collection to reduce dataset bias. This method involves creating pairs of 3D scenes that are nearly identical, except for the presence or absence of a spatial relation. The authors demonstrate that minimally contrastive examples can help identify issues with current relation detection models and improve training efficiency. The code and data for this dataset are available at https://github.com/princeton-vl/Rel3D.