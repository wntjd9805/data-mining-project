With the increasing use of predictive models in important decision-making processes, there is a growing interest in developing algorithms that can offer solutions for affected individuals. However, it is crucial to thoroughly analyze and interpret these predictive models and ensure that the solutions they provide are meaningful and unbiased before deploying them in the real world. In order to address this, we propose a new framework called Actionable Recourse Summaries (AReS) that can generate global counterfactual explanations. These explanations provide an understandable and accurate summary of solutions for the entire population. Our framework aims to optimize the correctness of these solutions and the interpretability of the explanations, while minimizing the overall costs associated with implementing the solutions for the entire population. Our approach allows us to learn a small number of concise rule sets that capture solutions for specific subpopulations within the data, guaranteeing the correctness of the solutions. Furthermore, we demonstrate that our framework encompasses several existing approaches for generating individual solutions. Through experiments with real-world datasets and user studies, we show that our framework can provide decision makers with a comprehensive overview of solutions for any black box model. This can help detect any undesirable biases or discrimination within the model.