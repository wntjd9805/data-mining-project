Deep classifiers have been successful in visual recognition tasks, but they struggle with real-world data that is long-tailed, resulting in a mismatch between training and testing distributions. This paper presents Balanced Softmax, an unbiased extension of the Softmax function commonly used in classification tasks, to address this issue. The authors derive a generalization bound for multiclass Softmax regression and demonstrate that their loss function minimizes this bound. They also introduce Balanced Meta-Softmax, which incorporates a Meta Sampler to estimate the optimal class sample rate and improve long-tailed learning. Experimental results show that Balanced Meta-Softmax outperforms existing solutions for long-tailed classification in visual recognition and instance segmentation tasks.