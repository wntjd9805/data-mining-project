Bayesian optimization (BO) is a widely used framework for optimizing black-box functions. In certain cases, the objective function can be evaluated at different levels of accuracy, allowing for a trade-off between cost and precision. To reduce optimization costs, several methods for multi-fidelity BO have been proposed. However, these methods often overlook or oversimplify the intricate correlations between different levels of accuracy, leading to inefficient estimation of the objective function. To address this issue, we introduce Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO), which effectively captures complex relationships between different levels of accuracy, thereby improving objective function estimation and optimization performance. Our approach utilizes sequential, fidelity-wise Gauss-Hermite quadrature and moment-matching techniques to compute a mutual information based acquisition function in a practical and highly efficient manner. We demonstrate the advantages of our method through experiments on synthetic benchmark datasets and real-world applications in engineering design.