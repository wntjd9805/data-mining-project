Deep neural networks typically use end-to-end backpropagation to learn weights, but this method is not biologically plausible and creates synchronization constraints in weight updates across layers. Recent advancements in unsupervised contrastive representation learning have raised the question of whether a learning algorithm can be made local, meaning that lower layer updates are not directly dependent on upper layer computations. However, the Greedy InfoMax approach, which learns each block separately with a local objective, has been found to consistently reduce readout accuracy in state-of-the-art unsupervised contrastive learning algorithms, possibly due to the greedy objective and gradient isolation. In this study, we discovered that by overlapping local blocks stacked on top of each other, we effectively increase the depth of the decoder and allow upper blocks to indirectly provide feedback to lower blocks. This simple design significantly narrows the performance gap between local learning and end-to-end contrastive learning algorithms for the first time. In addition to standard ImageNet experiments, we also demonstrate promising results on complex downstream tasks such as object detection and instance segmentation using readout features directly.