This study examines the eigenvalue distributions of the Conjugate Kernel (CK) and Neural Tangent Kernel (NTK) in multi-layer feedforward neural networks. The research focuses on an asymptotic scenario where network width increases linearly with sample size, random initialization of weights, and input samples that exhibit approximate pairwise orthogonality. The study reveals that, in this scenario, the eigenvalue distributions of the CK and NTK converge to deterministic limits. The limit for the CK involves iterating the Marcenko-Pastur map across hidden layers, while the limit for the NTK is equivalent to a linear combination of the CK matrices across layers, described by recursive fixed-point equations that extend the Marcenko-Pastur map. The study confirms these asymptotic predictions by analyzing spectra from synthetic and CIFAR-10 training data, and also conducts a small simulation to explore how these spectra evolve during training.