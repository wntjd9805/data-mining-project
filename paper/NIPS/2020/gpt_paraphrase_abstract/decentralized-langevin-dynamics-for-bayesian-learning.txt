We present a collaborative Bayesian learning algorithm based on decentralized Langevin dynamics in a non-convex setting. Our analysis demonstrates that the initial KL-divergence between the Markov Chain and the target posterior distribution decreases exponentially. Additionally, the contribution to the overall KL-divergence from additive noise decreases in polynomial time. We also find that the polynomial-term speeds up with the number of agents and provide sufficient conditions on the time-varying step-sizes for convergence to the desired distribution. We evaluate the algorithm on various machine learning tasks and observe that individual agents with local data perform comparably to the centralized setting, with a notable improvement in convergence rate.