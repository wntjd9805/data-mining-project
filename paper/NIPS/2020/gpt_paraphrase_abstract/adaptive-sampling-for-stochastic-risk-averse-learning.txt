A risk-averse approach is important in machine learning applications to perform well on difficult examples. This study introduces an adaptive sampling algorithm for training models that optimizes the Conditional Value-at-Risk (CVaR) of a loss distribution, which measures performance on challenging examples. The problem is formulated as a zero-sum game using a distributionally robust CVaR and solved efficiently through regret minimization. The approach utilizes structured Determinantal Point Processes (DPPs) for sampling, allowing scalability to large datasets. Empirical results show the effectiveness of the proposed method on both convex and non-convex learning tasks.