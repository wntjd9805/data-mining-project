Deep reinforcement learning is a collection of algorithms that use deep neural networks to represent internal concepts like value function or policy. Each algorithm optimizes its parameters based on an objective, such as Q-learning or policy gradient, which defines its meaning. In this study, we introduce a meta-gradient descent algorithm that discovers its own objective by interacting with the environment and using a deep neural network as a flexible parameterization. This allows the agent to improve its learning abilities over time. Additionally, since the objective is discovered online, it can adapt to changes. We demonstrate that this algorithm effectively deals with important issues in reinforcement learning, including bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm continuously adapts and eventually surpasses the median score of a strong actor-critic baseline, achieving higher efficiency in learning.