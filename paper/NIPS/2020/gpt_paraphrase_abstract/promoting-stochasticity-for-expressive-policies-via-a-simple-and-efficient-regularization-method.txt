Recent reinforcement learning (RL) methods often incorporate entropy regularization to enhance exploration and robustness by learning stochastic policies. However, integrating entropy regularization with expressive policies in continuous action spaces is challenging and typically necessitates complex inference procedures. To address this issue, we present a novel regularization technique that is compatible with a wide range of expressive policy architectures. A notable advantage of our method is that estimating the regularization terms is simple and efficient, even when the policy distributions are unknown. We demonstrate the effectiveness of our approach in promoting exploration in continuous action spaces. Additionally, we propose an off-policy actor-critic algorithm based on our regularization. Experimental results showcase that our algorithm surpasses existing regularized RL methods in continuous control tasks.