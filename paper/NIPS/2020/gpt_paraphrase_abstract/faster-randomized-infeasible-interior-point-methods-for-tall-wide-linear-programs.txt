Linear programming (LP) is widely used in various machine learning applications. InteriorPoint Methods (IPMs) are commonly used to solve LPs. However, the complexity of IPMs is mainly determined by the cost of solving a system of linear equations at each iteration. This paper focuses on infeasible IPMs for cases where the number of variables is much larger or smaller than the number of constraints. By utilizing Randomized Linear Algebra techniques, the paper proposes a preconditioning technique that, when combined with the Conjugate Gradient iterative solver, guarantees convergence to a feasible and approximately optimal solution without increasing iteration complexity. Empirical evaluations confirm the theoretical results on both real and synthetic data.