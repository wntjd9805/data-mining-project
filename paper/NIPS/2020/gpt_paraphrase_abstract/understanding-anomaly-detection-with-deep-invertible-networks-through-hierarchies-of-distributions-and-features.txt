Deep generative networks trained on natural image datasets often struggle with anomaly detection, assigning high likelihoods to images from datasets with different objects. This is due to a combination of model bias and domain prior, where convolutional networks learn similar low-level feature distributions regardless of the dataset they are trained on. These low-level features dominate the likelihood, making it challenging to detect high-level differences between inliers and outliers, such as object shapes. To address this, we propose two methods. Firstly, we use the log likelihood ratios of two models trained on different datasets to remove the negative impact of model bias and domain prior. Secondly, we show that low-level features are mainly captured at early scales in a multi-scale model, so using only the likelihood contribution of the final scale is effective for detecting high-level feature differences. This method is especially useful when a suitable general distribution is not available. Our methods achieve strong anomaly detection performance in the unsupervised setting and slightly underperform compared to state-of-the-art classifier-based methods in the supervised setting. The code for our methods can be found at the provided GitHub link.