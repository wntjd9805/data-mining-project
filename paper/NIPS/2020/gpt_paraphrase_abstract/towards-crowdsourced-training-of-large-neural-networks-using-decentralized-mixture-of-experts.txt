Recent advancements in deep learning have been achieved by training larger models on extensive datasets. However, this approach is often unaffordable due to the high costs involved. For example, the training of GPT-3 using a cluster costs over $250 million. Consequently, many researchers are unable to participate in the development of state-of-the-art models. To address this issue, we propose a concept called Learning@home, which involves crowd-sourcing the training of large neural networks using regular PCs volunteered by individuals. The combined computing power of thousands of desktops significantly surpasses that of a multimillion-dollar server pod. Nonetheless, conventional distributed training methods are inefficient in utilizing this vast power. Hence, we introduce a novel training paradigm that can effectively handle numerous participants with limited connectivity. We evaluate the performance, reliability, and architectural constraints of Learning@home, comparing it to existing distributed training techniques.