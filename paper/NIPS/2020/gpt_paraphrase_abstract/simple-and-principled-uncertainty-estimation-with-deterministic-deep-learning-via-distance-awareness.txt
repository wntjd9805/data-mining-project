Bayesian neural networks and deep ensembles are effective methods for estimating uncertainty in deep learning models. However, their practical use in real-time, large-scale applications is limited due to high memory and inference costs. This has led us to explore alternative approaches to uncertainty estimation that require only a single deep neural network (DNN). By formulating uncertainty quantification as a minimax learning problem, we have identified the importance of distance awareness in DNNs for achieving high-quality uncertainty estimation. To address this, we introduce the Spectral-normalized Neural Gaussian Process (SNGP), a straightforward technique that enhances the distance awareness of modern DNNs. SNGP achieves this by incorporating weight normalization during training and replacing the output layer with a Gaussian Process. Through experiments on various vision and language understanding tasks using popular architectures such as Wide-ResNet and BERT, we demonstrate that SNGP performs competitively with deep ensembles in terms of prediction accuracy, calibration, and out-of-domain detection. Furthermore, SNGP outperforms other single-model approaches.