Various formulations of multiclass support vector machines (SVM) have been developed. A recent study compared nine such formulations and recommended the variant proposed by Weston and Watkins (WW). However, it was noted that the WW-hinge loss used in this formulation is not calibrated with respect to the 0-1 loss. In this study, we propose a new discrete loss function called the ordered partition loss, and demonstrate that the WW-hinge loss is calibrated with respect to this loss function. We also argue that the ordered partition loss is the most suitable among discrete losses that satisfy this property. Furthermore, we use our theory to explain why the WW-SVM performs well in the presence of significant label noise, which is a challenging scenario for multiclass SVMs.