We present a method to explain neurons in deep representations by identifying compositional logical concepts that closely resemble their behavior. This approach allows for a more precise and expressive characterization of neuron behavior compared to previous methods that use atomic labels as explanations. We apply this method to investigate interpretability in vision and natural language processing models. Our findings reveal that in image classification, many neurons learn abstract but conceptually coherent visual concepts, while others detect unrelated features. In natural language inference, neurons learn shallow lexical heuristics influenced by dataset biases. Additionally, we explore the relationship between compositional explanations and model performance. Vision neurons that detect interpretable concepts are positively correlated with task performance, whereas NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Lastly, we demonstrate how compositional explanations enable end users to generate simple "copy-paste" adversarial examples that predictably alter model behavior.