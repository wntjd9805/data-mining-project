This abstract discusses the combination of deep reinforcement learning and search in both training and testing phases, which has been successful in single-agent settings and perfect-information games like AlphaZero. However, previous algorithms in this form cannot handle imperfect-information games. This paper introduces ReBeL, a general framework for self-play reinforcement learning and search that guarantees convergence to a Nash equilibrium in any two-player zero-sum game. In perfect-information games, ReBeL functions similarly to AlphaZero. Results from two different imperfect-information games demonstrate that ReBeL converges to an approximate Nash equilibrium. Additionally, ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker without requiring as much domain knowledge as previous poker AI systems.