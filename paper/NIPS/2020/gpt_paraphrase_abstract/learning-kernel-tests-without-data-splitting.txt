Modern large-scale kernel-based tests like maximum mean discrepancy (MMD) and kernelized Stein discrepancy (KSD) optimize kernel hyperparameters using data splitting to obtain powerful test statistics. However, data splitting reduces test power due to smaller sample size. To address this, we propose an approach inspired by selective inference that learns hyperparameters and tests on the full sample without data splitting. Our approach correctly calibrates the test in the presence of dependency and provides a closed-form test threshold. Empirical results show that our approach has higher test power compared to the data-splitting approach at the same significance level, regardless of the split proportion.