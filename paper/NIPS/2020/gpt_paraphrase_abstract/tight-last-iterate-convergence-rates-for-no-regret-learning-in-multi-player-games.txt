We examine the issue of achieving convergence rates for no-regret learning algorithms in multi-player games. Our research focuses on the optimistic gradient (OG) algorithm with a constant step-size, which is a no-regret algorithm. We demonstrate that this algorithm achieves a convergence rate of O(1/pT) in smooth monotone games, as measured by the gap function. This finding addresses a question raised by Mertikopoulos & Zhou (2018) regarding the applicability of extra-gradient approaches, like OG, in improving guarantees in multi-agent learning. Our proof relies on a novel technique involving the adaptive selection of a potential function at each iteration. Furthermore, we establish that the O(1/pT) rate is the best possible for all p-SCLI algorithms, including OG. In addition, our analysis provides a direct proof of a conjecture made by Arjevani et al. (2015), which is more straightforward than previous methods.