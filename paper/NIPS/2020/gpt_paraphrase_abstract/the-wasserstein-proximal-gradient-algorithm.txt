Wasserstein gradient flows are continuous time dynamics that minimize an objective function over probability measures. These dynamics have been used in machine learning to approximate probability distributions. However, the discrete-time behavior of these algorithms may differ from the continuous time dynamics, and little is known about the effectiveness of discretized gradient flows. In this study, we propose a ForwardBackward discretization scheme for minimizing a objective function that consists of smooth and nonsmooth geodesically convex terms. Using techniques from convex optimization and optimal transport, we analyze the convergence of the ForwardBackward scheme on the Wasserstein space. Our results demonstrate that the scheme has similar convergence guarantees to the proximal gradient algorithm in Euclidean spaces.