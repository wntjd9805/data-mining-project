Counterfactual learning is an important topic in recommendation systems due to the prevalence of missing-not-at-random (MNAR) data. Previous methods for addressing MNAR data, such as randomized controlled trials (RCTs), are costly and impractical. To overcome this, we propose an alternative approach called counterfactual variational information bottleneck (CVIB). By separating the task-aware mutual information term, we introduce a contrastive information loss and an output confidence penalty to achieve balanced learning between factual and counterfactual domains. Our empirical evaluation on real-world datasets demonstrates that CVIB significantly improves both shallow and deep models, providing insights into counterfactual learning in recommendation beyond the use of RCTs.