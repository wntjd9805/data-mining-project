Pre-training is widely used in computer vision, particularly in initializing object detection and segmentation models with supervised ImageNet pre-training. However, a study by He et al. challenges the effectiveness of ImageNet pre-training for COCO object detection. To explore alternative methods, we investigate the use of self-training, comparing it to ImageNet pre-training. Our research uncovers three key findings: 1) the value of pre-training diminishes with stronger data augmentation and more labeled data, 2) self-training consistently improves performance with stronger data augmentation, regardless of the amount of labeled data available, and 3) when pre-training is beneficial, self-training further enhances the results. For instance, in the case of COCO object detection, pre-training is helpful when only a fifth of the labeled data is used, but hurts accuracy when all labeled data is utilized. On the other hand, self-training shows positive improvements in performance across all dataset sizes, with gains ranging from +1.3 to +3.4AP. In essence, self-training proves effective in situations where pre-training fails (such as using ImageNet to aid COCO). In the case of the PASCAL segmentation dataset, which is smaller than COCO, pre-training does offer significant benefits, but self-training surpasses the performance of the pre-trained model. Specifically, we achieve 54.3AP for COCO object detection, a +1.5AP improvement over the strongest SpineNet model, and 90.5 mIOU for PASCAL segmentation, a +1.5% mIOU improvement over the previous state-of-the-art result obtained by DeepLabv3+.