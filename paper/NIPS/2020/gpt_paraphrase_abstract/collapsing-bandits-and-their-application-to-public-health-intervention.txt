We introduce a new type of restless multi-armed bandit (RMAB) called Collapsing Bandits, where each arm follows a Markovian process with a binary state. When an arm is played, its state is fully observed, eliminating uncertainty, but when an arm is passive, no observation is made, allowing uncertainty to evolve. The objective is to maximize the number of arms in the "good" state using a limited budget of actions per round. This setting is relevant to healthcare domains where health workers need to monitor patients and deliver interventions to maximize their overall health. Our contributions include deriving conditions for the indexability of Collapsing Bandits, leveraging threshold policies to compute the Whittle index efficiently, and evaluating our algorithm on various data distributions, including a real-world healthcare task. Our algorithm achieves a significant speedup compared to existing techniques while maintaining similar performance. The code is available at the provided GitHub repository.