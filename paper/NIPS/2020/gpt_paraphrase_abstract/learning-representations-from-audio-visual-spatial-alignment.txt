We present a new method for learning representations from audio-visual content. Previous approaches focused on video-level correspondences, predicting whether audio and video clips come from the same or different video instances. We also introduced audio-visual temporal synchronization to discriminate negative pairs from the same video instance but at different moments in time. However, these methods neglect spatial cues present in audio and visual signals. To address this, we trained a network to perform contrastive audio-visual spatial alignment using 360â—¦ video and spatial audio. We employed a transformer architecture to combine representations from multiple viewpoints, enabling spatial alignment. We demonstrated the effectiveness of our approach on various audio and visual tasks, including audio-visual correspondence, spatial alignment, action recognition, and video semantic segmentation. The dataset and code are available at https://github.com/pedro-morgado/AVSpatialAlignment.