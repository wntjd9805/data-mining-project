Scaling up the size of convolutional neural networks (CNNs) is known to improve model accuracy, but it poses challenges for training on resource-constrained edge devices. Federated learning (FL) is a desirable approach for privacy and confidentiality, but it can strain the compute capability of edge nodes. To address this, we propose a new training algorithm called FedGKT, which treats FL as a group knowledge transfer problem. FedGKT uses an alternating minimization approach to train small CNNs on edge nodes and periodically transfers their knowledge to a larger server-side CNN using knowledge distillation. This approach offers several benefits, including reduced demand for edge computation, lower communication bandwidth for large CNNs, and asynchronous training, while maintaining model accuracy comparable to FedAvg. We conducted experiments using different datasets and found that FedGKT achieves similar or slightly higher accuracy compared to FedAvg. Importantly, FedGKT significantly reduces the computational power and parameter requirements for edge training compared to FedAvg. Our source code is available at FedML (https://fedml.ai).