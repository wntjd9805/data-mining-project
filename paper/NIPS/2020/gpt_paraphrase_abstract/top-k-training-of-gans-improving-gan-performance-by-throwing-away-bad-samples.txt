We propose a simple modification to the Generative Adversarial Network (GAN) training algorithm that enhances results without increasing computational cost. This modification involves zeroing out the gradient contributions from the batch elements deemed as least realistic by the critic when updating the generator parameters. Our experiments on various GAN variants demonstrate that this "top-k update" approach is a broadly applicable improvement. To better understand this enhancement, we extensively analyze a simple mixture-of-Gaussians dataset and uncover several intriguing phenomena. For instance, we observe that updating gradients based on the worst-scoring batch elements can actually push samples further away from their nearest mode. Additionally, we apply our method to recent GAN variants and achieve a notable improvement in the state-of-the-art FID for conditional generation, reducing it from 9.21 to 8.57 on CIFAR-10.