Attention modules have been shown to improve the representation ability of neural networks by adjusting spatial or channel features. However, designing these attention operations requires a lot of computation and expertise. This paper introduces Auto Learning Attention (AutoLA), the first method for automatic attention design. The authors propose a new attention module called high order group attention (HOGA) which is represented as a directed acyclic graph (DAG). Each group in the DAG represents a node, and each edge represents a different attention operation. The authors use the differential AutoLA method to automatically search for a HOGA architecture, which takes only 1 GPU day using the ResNet-20 backbone on CIFAR10. The searched attention module can be used with various backbones and outperforms manually designed channel and spatial attentions for tasks such as image classification, object detection, and human keypoint detection. The code for AutoLA is available at https://github.com/btma48/AutoLA.