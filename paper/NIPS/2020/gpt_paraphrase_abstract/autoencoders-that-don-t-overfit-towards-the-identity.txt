Autoencoders are neural networks that aim to reproduce the output from the input. However, they often suffer from overfitting, where they learn to predict each feature in the output from itself in the input. This becomes problematic when using autoencoders for prediction tasks in the presence of noisy data. To prevent overfitting, denoising autoencoders are commonly used, where some features are dropped out and the remaining features are used to predict the dropped-out ones. In this study, we focus on linear autoencoders for their analytical solutions and demonstrate that denoising only partially prevents overfitting, depending on the L2-norm regularization penalty. We present a main theorem that shows the effectiveness of an emphasized denoising autoencoder in completely eliminating overfitting. Our derivations also reveal new insights, such as the closed-form solution of the full-rank model and a new orthogonality constraint in the low-rank model. Although this constraint differs from existing regularizers, our experiments show similar effects on learned embeddings. We validate our theoretical findings through experiments on three well-known datasets.