Despite claims that certain models are more interpretable than others, such as linear models being more interpretable than deep neural networks, there is currently no formal notion of interpretability to compare different classes of models. To address this, we investigate whether commonly believed interpretability claims correspond to computational complexity theory. Specifically, we examine local post-hoc explainability queries that aim to explain why individual inputs are classified a certain way by a model. We propose that a class of models, C1, is more interpretable than another class, C2, if answering post-hoc queries for models in C2 is computationally more complex than for models in C1. We demonstrate that this notion aligns with current beliefs on model interpretability, proving that both linear and tree-based models are more interpretable than neural networks under our definition and assuming standard complexity-theoretical assumptions like P â‰  NP. However, our complexity analysis does not definitively differentiate between linear and tree-based models, as results vary depending on the specific post-hoc explanations considered. Additionally, through a more detailed complexity analysis based on parameterized complexity, we provide theoretical evidence suggesting that shallow neural networks are more interpretable than deeper ones.