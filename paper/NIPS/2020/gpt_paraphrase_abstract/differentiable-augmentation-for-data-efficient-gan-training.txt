The performance of generative adversarial networks (GANs) declines when there is limited training data because the discriminator simply memorizes the training set. To address this issue, we propose a method called DifferentiableAugmentation (DiffAugment) that enhances the data efficiency of GANs by applying differentiable augmentations to both real and fake samples. Previous attempts to augment the training data by manipulating the distribution of real images have shown limited benefits. DiffAugment allows us to apply differentiable augmentation to the generated samples, leading to more stable training and better convergence. Our experiments demonstrate consistent improvements across various GAN architectures and loss functions, for both unconditional and class-conditional generation. With DiffAugment, we achieve state-of-the-art results on ImageNet 128x128, with a FID of 6.80 and an IS of 100.8. We also achieve significant reductions in FID with only 1,000 images on datasets like FFHQ and LSUN. Additionally, using only 20% of the training data, we can achieve top performance on CIFAR-10 and CIFAR-100. Furthermore, our method can generate high-fidelity images using only 100 images without pre-training, while performing on par with existing transfer learning algorithms. The code for our method is available at https://github.com/mit-han-lab/data-efficient-gans.