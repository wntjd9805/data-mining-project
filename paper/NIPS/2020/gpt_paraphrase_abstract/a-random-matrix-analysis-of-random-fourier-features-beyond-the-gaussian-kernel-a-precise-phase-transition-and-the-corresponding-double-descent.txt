This article investigates the behavior of random Fourier feature (RFF) regression in a realistic scenario where the number of data samples, their dimension, and the dimension of the feature space are all large and comparable. The study reveals that the random RFF Gram matrix does not converge to the well-known Gaussian kernel matrix, but still exhibits a manageable behavior. The analysis provides accurate estimates of training and test regression errors for large-scale datasets. Additionally, the research identifies two distinct learning phases and the transition between them, as well as deriving the double descent test error curve based on this phase transition behavior. These findings are not reliant on strong assumptions about the data distribution and align well with empirical results from real-world datasets.