This paper explores the performance dynamics of a deep neural network (DNN) on individual samples during its learning process. By developing a measure called dynamic instance hardness (DIH), which is based on the exponential moving average of a sample's instantaneous hardness over the training history, the authors are able to create an adaptive curriculum that facilitates faster and more accurate learning of DNN models. DIH is found to be a more stable measure compared to using only instantaneous hardness, as it takes into account the stochastic nature of training and the non-smoothness of DNNs. The DIH-guided curriculum learning (DIHCL) procedure is computationally inexpensive as it utilizes back-propagation as a byproduct and does not require additional inference. Experimental results on 11 datasets demonstrate that DIHCL outperforms random mini-batch SGD and recent curriculum learning methods in terms of efficiency and final performance. The DIHCL code is available at https://github.com/tianyizhou/DIHCL.