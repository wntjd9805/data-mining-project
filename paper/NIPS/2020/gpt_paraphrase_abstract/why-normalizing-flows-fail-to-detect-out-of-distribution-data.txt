Detecting out-of-distribution (OOD) data is essential for ensuring the reliability of machine learning systems. Normalizing flows, which are powerful deep generative models, often struggle to differentiate between in-distribution and out-of-distribution data. For example, a flow trained on clothing images may assign a higher likelihood to handwritten digits. In this study, we explore the reasons behind the poor performance of normalizing flows in OOD detection. We find that flows primarily learn local pixel correlations and generic image-to-latent-space transformations that are not specific to the target dataset. Specifically, we focus on flows based on coupling layers. By modifying the architecture of these coupling layers, we can bias the flow to better capture the semantic structure of the target data, thereby improving OOD detection. Our investigation uncovers that properties enabling flows to generate high-quality images can actually hinder their ability to detect OOD data.