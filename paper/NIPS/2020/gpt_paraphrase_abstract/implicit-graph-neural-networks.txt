Graph Neural Networks (GNNs) are popular deep learning models for learning meaningful representations from graph data. However, existing GNN methods struggle with capturing long-range dependencies in graphs. To address this issue, we propose Implicit Graph Neural Networks (IGNN2), a graph learning framework that solves a fixed-point equilibrium equation using implicitly defined state vectors for making predictions. We ensure the well-posedness of the framework by employing Perron-Frobenius theory and derive a tractable projected gradient descent method for training. Experimental results across various tasks demonstrate that IGNNs consistently capture long-range dependencies and outperform current state-of-the-art GNN models.