This paper investigates the capabilities of Microsoft Floating Point (MSFP), a novel type of datatypes designed for large-scale inferencing on custom hardware in cloud environments. By combining hardware design and algorithms, MSFP16 achieves a 3× cost reduction compared to Bﬂoat16, and MSFP12 offers a 4× cost reduction compared to INT8, while maintaining a similar or improved accuracy. MSFP has minimal impact on accuracy (<1%) and does not require any changes to the model structure. It seamlessly integrates with a well-established cloud production pipeline and supports various deep learning models, including CNNs, RNNs, and Transformers, without modification. The study evaluates the accuracy and implementation of MSFP and demonstrates its effectiveness in multiple production scenarios, including web search, question-answering, and image classification.