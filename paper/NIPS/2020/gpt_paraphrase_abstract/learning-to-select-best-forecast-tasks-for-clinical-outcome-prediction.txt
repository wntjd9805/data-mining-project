The technique of "pretraining" on auxiliary tasks and then "finetuning" on a target task has been successful in various domains. However, when there are numerous auxiliary tasks with complex relationships to the target task, manually selecting the most effective pretraining setups becomes inefficient and suboptimal. To overcome this challenge, we propose an automated method for selecting auxiliary tasks from a large pool, in order to obtain the most useful representation for the target task. We have developed an efficient algorithm that incorporates automatic auxiliary task selection within a nested-loop meta-learning process. We have applied this algorithm to the prediction of clinical outcomes using electronic medical records, learning from multiple self-supervised tasks related to forecasting patient trajectories. Our experiments on real clinical data demonstrate that our method outperforms direct supervised learning, naive pretraining, and simple multitask learning, especially in scenarios with limited data and few examples of the primary task. Through detailed analysis, we show that the selection rules are interpretable and can generalize to new target tasks with unseen data.