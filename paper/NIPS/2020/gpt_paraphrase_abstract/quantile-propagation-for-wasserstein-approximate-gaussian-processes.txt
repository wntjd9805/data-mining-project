Most existing approximate inference methods for Gaussian process models optimize the Kullback-Leibler (KL) divergence, which is convenient but lacks the necessary properties for accurate inference. In this study, we propose a new method called Quantile Propagation (QP) that minimizes the L2 Wasserstein distance (WD) instead of the KL divergence. Unlike expectation propagation (EP), QP matches quantile functions rather than moments and has a smaller variance update, reducing over-estimation of posterior variances. Despite the complexity of dealing with the WD, QP maintains the favorable locality property of EP, allowing for an efficient algorithm. Experimental results demonstrate that QP outperforms EP and variational Bayes in classification and Poisson regression tasks.