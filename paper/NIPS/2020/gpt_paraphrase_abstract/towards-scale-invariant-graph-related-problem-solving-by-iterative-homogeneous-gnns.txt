Current graph neural networks (GNNs) lack generalizability across different scales in solving graph analysis problems. To address this issue, we propose extensions in synthesizing graph theory programs. Firstly, we learn to adaptively terminate the message passing process in GNNs based on computation progress, inspired by the dependency of iteration number on graph size in common graph theory algorithms. Secondly, we introduce homogeneous transformation layers that act as universal homogeneous function approximators, making ordinary GNNs homogeneous in relation to graph weights. Experimental results demonstrate that our GNN can be trained on small-scale graphs yet perform well on large-scale graphs for various basic graph theory problems. Additionally, it exhibits generalizability in multi-body physical simulation and image-based navigation applications.