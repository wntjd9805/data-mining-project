A latent bandit problem involves a learning agent trying to determine an unknown discrete latent state based on known reward distributions of different options. The agent's objective is to accurately identify the latent state in order to make optimal decisions. This problem lies between online and offline learning, allowing for complex models to be learned offline while the agent identifies the latent state online. This is particularly relevant in recommender systems. In this study, we propose algorithms for latent bandits that incorporate both upper confidence bounds and Thompson sampling. These algorithms are contextual and take into account model uncertainty and misspecification. Through theoretical analysis, we demonstrate that our algorithms have lower regret compared to traditional bandit policies when the number of latent states is smaller than the number of actions. Our approach is further supported by a comprehensive empirical study showcasing its advantages.