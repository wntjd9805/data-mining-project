This paper introduces the integration of variational semantic memory into meta-learning for the purpose of acquiring long-term knowledge in few-shot learning. The variational semantic memory is responsible for storing and accumulating semantic information in a hierarchical Bayesian framework, allowing for probabilistic inference of class prototypes. This memory is built from scratch and gradually consolidated by incorporating information from encountered tasks, enabling the acquisition of long-term, general knowledge that facilitates learning new object concepts. The process of memory recall is formulated as the variational inference of a latent memory variable from addressed contents, providing a systematic approach to adapt knowledge to individual tasks. The proposed variational semantic memory serves as a new long-term memory module, offering principled mechanisms for recall and update that efficiently accumulate and adapt semantic information for few-shot learning. Experimental results demonstrate that the probabilistic modeling of prototypes yields a more informative representation of object classes compared to deterministic vectors. Furthermore, the consistent state-of-the-art performance achieved on four benchmarks highlights the advantages of incorporating variational semantic memory in enhancing few-shot recognition.