Advancements in deep reinforcement learning (RL) have made significant strides in 2-player zero-sum games like Go, Poker, and Starcraft. These games are advantageous for RL methods due to their purely adversarial nature. However, real-world scenarios involve multiple agents with complex interactions that combine both common-interest and competitive aspects. In this study, we focus on Diplomacy, a 7-player board game that magnifies the challenges resulting from many-agent interactions. Diplomacy poses difficulties for RL algorithms due to its large combinatorial action space and simultaneous moves. To address these challenges, we propose a straightforward but effective approximate best response operator capable of handling large combinatorial action spaces and simultaneous moves. Additionally, we introduce a set of policy iteration methods that approximate fictitious play. By employing these techniques, we successfully apply RL to Diplomacy, demonstrating that our agents outperform the previous state-of-the-art. Furthermore, game theoretic equilibrium analysis confirms that our new approach consistently leads to improvements.