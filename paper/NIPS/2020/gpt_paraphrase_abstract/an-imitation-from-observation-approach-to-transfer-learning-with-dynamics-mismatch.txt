The abstract discusses the challenge of transferring a policy learned in one environment to a different environment with varying dynamics. It emphasizes the importance of reducing interaction with the target environment during the learning process, especially in sim-to-real transfer scenarios where simulators do not perfectly model real-world dynamics. The paper explores the connection between grounded action transformation and imitation from observation (IfO) techniques, which involve learning behaviors that imitate observed behavior demonstrations. Based on this connection, the authors propose a new algorithm called generative adversarial reinforced action transformation (GARAT) by repurposing state-of-the-art IfO approaches. Experimental results demonstrate that agents trained with GARAT achieve higher returns in the target environment compared to existing black-box transfer methods.