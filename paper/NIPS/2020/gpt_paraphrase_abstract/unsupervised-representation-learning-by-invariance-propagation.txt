Unsupervised learning methods using contrastive learning have gained attention and achieved promising outcomes. While most methods focus on learning representations invariant to variations within instances, we propose Invariance Propagation to learn representations invariant to variations within categories. Our method recursively identifies semantically consistent samples within high-density regions in representation space. To maximize agreement between anchor and positive samples, we employ a hard sampling strategy that captures more intra-class variations and abstract invariance. Using a ResNet-50 backbone, our approach surpasses previous results with 71.3% top-1 accuracy on ImageNet linear classification and 78.2% top-5 accuracy on fine-tuning with only 1% labels. Additionally, we achieve state-of-the-art performance on other tasks such as linear classification on Places205 and Pascal VOC, as well as transfer learning on small-scale datasets.