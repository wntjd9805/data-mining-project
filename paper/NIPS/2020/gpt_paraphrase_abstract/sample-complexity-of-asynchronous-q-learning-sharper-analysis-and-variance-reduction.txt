Asynchronous Q-learning is a method used to learn the optimal action-value function in a Markov decision process (MDP) using a single trajectory of samples generated by a behavior policy. In this study, we focus on a γ-discounted MDP with state space S and action space A. We show that the sample complexity of classical asynchronous Q-learning, which refers to the number of samples needed to obtain an accurate estimate of the Q-function, can be bounded by 1µmin(1 − γ)5ε2 + tmixµmin(1 − γ), with a logarithmic factor included, when a suitable learning rate is used. Here, tmix represents the mixing time of the sample trajectory and µmin is the minimum state-action occupancy probability. The first term of this bound matches the complexity when samples are independently drawn from the stationary distribution of the trajectory. The second term accounts for the initial expense of reaching a steady state in the empirical distribution of the trajectory, which becomes amortized over time. Importantly, our bound improves upon the current state-of-the-art result by a factor of at least |S||A|. Additionally, the discount complexity can be further reduced through variance reduction techniques.