We present a new study that offers assurance of global convergence in competitive reinforcement learning scenarios with two agents. Our focus is on independent learning algorithms, also known as zero-sum stochastic games. In this study, we consider an episodic setup where each player selects a policy and only has access to their own actions, rewards, and the state. By running policy gradient methods simultaneously, we demonstrate that the policies of both players will converge to a min-max equilibrium of the game. The key requirement for convergence is that the learning rates of both players adhere to a two-timescale rule. This finding is significant as it represents the first finite-sample convergence result for independent policy gradient methods in competitive reinforcement learning. Previous research has primarily concentrated on centralized, coordinated approaches for calculating equilibrium.