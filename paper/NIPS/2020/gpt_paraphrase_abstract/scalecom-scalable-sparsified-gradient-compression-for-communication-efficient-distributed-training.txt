Large-scale distributed training of Deep Neural Networks (DNNs) on advanced platforms is expected to face communication limitations. To overcome this, many gradient compression techniques have been proposed with high compression ratios. However, most existing methods are not suitable for large-scale distributed systems and fail to evaluate model fidelity on large datasets. To address these issues, we propose a new compression technique called Scalable Sparsified Gradient Compression (ScaleCom). ScaleCom leverages the similarity in gradient distribution among learners to improve scalability. Theoretical analysis shows that ScaleCom guarantees favorable convergence and is compatible with gradient all-reduce techniques. Experimental results demonstrate that ScaleCom has low overhead, reduces gradient traffic, achieves high compression rates (65-400X), and exhibits excellent scalability (up to 64 learners and 8-12X larger batch sizes) across various applications (image, language, and speech) without significant accuracy loss.