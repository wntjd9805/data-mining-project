Current methods for estimating uncertainty in deep learning require multiple forward passes, which is impractical when computational resources are limited. To address this, we propose a method that performs probabilistic reasoning across different depths of neural networks. Each depth corresponds to a subnetwork with shared weights, and their predictions are combined through marginalization to obtain model uncertainty. By leveraging the sequential structure of feed-forward networks, we can achieve both training objective evaluation and prediction in a single forward pass. We demonstrate the effectiveness of our approach on real-world regression and image classification tasks. Our method offers calibrated uncertainty, resilience to dataset shift, and competitive accuracy compared to more computationally intensive methods.