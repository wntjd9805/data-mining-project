Decision trees are widely used in machine learning due to their simplicity and interpretability. However, understanding the factors that affect their generalization error is still a challenge. In this study, we focus on binary decision trees with real-valued features and analyze them from a data partition perspective. We introduce the concept of a partitioning function and examine its relationship with the growth function and VC dimension. By leveraging this new concept, we determine the exact VC dimension of decision stumps, which is the largest integer d satisfying a certain inequality involving the number of real-valued features. We establish a recursive expression to bound the partitioning functions, leading to an upper bound on the growth function of any decision tree structure. Consequently, we demonstrate that the VC dimension of a binary tree structure with N internal nodes scales approximately as N log(N). Additionally, we propose a pruning algorithm based on these findings, which outperforms the CART algorithm on various datasets without the need for cross-validation.