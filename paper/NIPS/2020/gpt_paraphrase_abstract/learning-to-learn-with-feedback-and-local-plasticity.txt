The interest in finding alternatives to backpropagation is driven by the goal of bridging the gap between deep learning and neuroscience, as well as addressing backpropagation's limitations in tasks like online learning. However, previous attempts at using local synaptic learning rules similar to those in the brain have not been as successful as backpropagation in deep networks. In this study, we use meta-learning to discover networks that learn through feedback connections and local, biologically inspired learning rules. Importantly, these feedback connections are not tied to the feedforward weights, which avoids unrealistic weight transport. Our experiments show that meta-trained networks effectively use feedback connections to assign credit in multi-layer architectures during online learning. Surprisingly, this approach performs as well as, or even better than, a state-of-the-art gradient-based online meta-learning algorithm in regression and classification tasks, particularly in continual learning. Analyzing the weight updates in these models reveals that they differ from gradient descent in a way that reduces interference between updates. This suggests that biologically plausible learning mechanisms not only match gradient descent-based learning, but also overcome its limitations.