We examine a multi-armed bandit problem where certain arms can be unavailable for periods of time and the rewards for each arm are given adversarially without following any distribution. This scenario represents situations where limited resources, such as arms, need to be allocated and can only be reused after certain time periods. We demonstrate that finding an optimal policy to maximize cumulative reward is strongly NP-hard in the case where blocking durations and rewards are known in advance, making it unlikely to find a fully polynomial-time approximation scheme (FPTAS) unless P = NP. However, we propose a greedy algorithm that selects the best available arm at each round, which provides an approximation guarantee based on the blocking durations and reward variability. In the bandit setting where blocking durations and rewards are unknown, we develop two algorithms, RGA and RGA-META, for bounded duration and path variation. RGA achieves a dynamic approximate regret of O((2D + K)BT) when the variation budget BT is known, while RGA-META achieves a regret of at most O((K + D)^(1/4)B^(1/2)T^(3/4)), where B represents the maximal path variation budget within each batch of RGA-META. We also prove that if either the variation budget or maximal blocking duration is unbounded, the approximate regret will be at least Î˜(T). Additionally, we establish that the regret upper bound of RGA is tight when the blocking durations are bounded above by O(1).