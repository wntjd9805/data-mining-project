We present a novel approach for designing first-order methods to solve unconstrained strongly convex problems. Instead of directly addressing the original objective, we create a shifted objective function that shares the same minimizer and incorporates the smoothness and strong convexity properties through an interpolation condition. We then propose an algorithmic framework to solve the shifted objective, which can take advantage of this condition. Using this framework, we develop several accelerated schemes for problems with different first-order oracles. We demonstrate that the interpolation condition simplifies and improves the analysis of these methods, resulting in faster worst-case convergence rates compared to existing methods. We conduct experiments on machine learning tasks to evaluate the effectiveness of our new methods.