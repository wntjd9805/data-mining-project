Many machine learning applications require optimizing higher-order derivatives, which can be computationally expensive. One such application is score matching in generative modeling, which involves optimizing the trace of a Hessian. To improve efficiency, we propose a method to approximate any-order directional derivative using finite differences. This approximation only requires function evaluations and no gradient computations, reducing computational cost and improving numerical stability. We demonstrate the effectiveness of our approach by reformulating variants of score matching objectives into the finite difference forms, resulting in comparable results to gradient-based methods but with significantly lower computational requirements.