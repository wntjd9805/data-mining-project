We present a framework for creating primal methods in decentralized optimization, specifically for situations where local functions are smooth and strongly convex. Our method involves solving a series of sub-problems using the accelerated augmented Lagrangian method, which allows for the derivation of various decentralized algorithms such as EXTRA and SSDA. By combining this framework with accelerated gradient descent, we develop a new primal algorithm that achieves an optimal convergence rate, matching recently established lower bounds. Experimental results confirm the effectiveness of our algorithm, particularly for challenging ill-conditioned problems.