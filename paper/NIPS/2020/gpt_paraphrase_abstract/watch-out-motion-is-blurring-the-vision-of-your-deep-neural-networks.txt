State-of-the-art deep neural networks (DNNs) are susceptible to adversarial examples with random noise-like perturbations. While these examples are rare in the physical world, the blurring effect caused by object motion is a common occurrence in practical scenarios such as object detection and tracking. This paper investigates the potential risks of blur effects on DNNs caused by object motion. A novel adversarial attack method called motion-based adversarial blur attack (ABBA) is proposed, which can generate visually natural motion-blurred adversarial examples. The attack is achieved by convolving an input image with kernels and tuning the kernel weights for misclassification capability. To make the examples more natural and plausible, a saliency-regularized adversarial kernel prediction is introduced, where the salient region acts as the moving object and the predicted kernel is regularized for natural visual effects. The attack is further enhanced by adaptively adjusting the translations of the object and background. Evaluation on the NeurIPS'17 adversarial competition dataset demonstrates the effectiveness of ABBA, considering various kernel sizes, translations, and regions. The study also shows that ABBA has a more effective penetrating capability compared to other blurring methods when tested against state-of-the-art GAN-based deblurring mechanisms. The code for ABBA is available at https://github.com/tsingqguo/ABBA.