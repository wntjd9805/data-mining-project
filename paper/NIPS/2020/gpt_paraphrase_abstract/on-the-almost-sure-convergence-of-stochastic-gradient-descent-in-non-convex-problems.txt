This paper examines the behavior of stochastic gradient descent (SGD) in non-convex problems to gain insights into its convergence properties. Initially, we demonstrate that the sequence of iterations produced by SGD remains bounded and converges with certainty for a wide range of step-size schedules. Furthermore, we go beyond existing guarantees and establish that SGD avoids strict saddle points/manifolds with certainty for all considered step-size policies. Lastly, we prove that when employing a step-size of Î˜(1/np), the algorithm converges to local minimizers with a positive-definite Hessian at a rate of O(1/np). This finding suggests that employing a cool-down phase with a diminishing step-size can potentially lead to faster convergence. We validate this heuristic by applying it to ResNet architectures on CIFAR datasets.