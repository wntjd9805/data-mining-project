Deep ensembles, which involve training neural network weights from different random initialization, have achieved high accuracy and calibration. Batch ensembles, a more parameter efficient alternative, have been introduced recently. In this study, we propose ensembles that consider both weights and hyperparameters to further enhance performance. Our method, called hyper-deep ensembles, involves a random search over hyperparameters, stratified across multiple random initializations. This approach demonstrates strong performance by combining models with diversity in both weights and hyperparameters. Additionally, we propose hyper-batch ensembles, a parameter efficient version that builds on the layer structure of batch ensembles and self-tuning networks. Our method has lower computational and memory costs compared to traditional ensembles. We conducted experiments on image classification tasks using various architectures, including MLP, LeNet, ResNet 20, and WideResNet 28-10. Our results show improvements over both deep and batch ensembles.