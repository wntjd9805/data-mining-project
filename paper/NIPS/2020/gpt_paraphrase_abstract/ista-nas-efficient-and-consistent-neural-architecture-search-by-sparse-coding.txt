Neural architecture search (NAS) is the process of finding the best sparse solution from a high-dimensional space. Existing gradient-based NAS methods do not consider sparsity during the search phase and instead apply post-processing to achieve sparsity. This approach leads to inefficiencies in training the dense super-net and a discrepancy with the final sparse architecture. In this paper, we propose a new approach that treats NAS as a sparse coding problem. We conduct a differentiable search in a compressed lower-dimensional space that preserves the validation loss of the original sparse solution space. By solving the sparse coding problem, we recover the architecture. The differentiable search and architecture recovery are optimized alternately, ensuring that the network for search satisfies the sparsity constraint and is efficient to train. Additionally, we address the depth and width gap by introducing a method that combines the search and evaluation stages under target-net settings. During training, the architecture variables are incorporated into the network weights, allowing us to obtain the searched architecture and optimized parameters in a single run. Experimental results show that our two-stage method achieves promising results on CIFAR-10 with minimal computational resources, while our one-stage method achieves state-of-the-art performance on both CIFAR-10 and ImageNet with only the evaluation time as additional cost.