We present a non-asymptotic convergence analysis of mirror-Langevin diffusions, addressing the challenge of sampling from ill-conditioned log-concave distributions. Building on the framework introduced in [Zha+20], we examine a specific type of diffusion called Newton-Langevin diffusions and demonstrate their exponential convergence to stationarity. Remarkably, this rate of convergence is not only dimension-free but also independent of the target distribution. To illustrate the practical significance of our findings, we apply them to the problem of sampling from the uniform distribution on a convex body, leveraging interior-point methods. Our methodology aligns with the recent trend of integrating sampling and optimization, highlighting the importance of the chi-squared divergence. Additionally, we provide novel insights into the convergence of the vanilla Langevin diffusion in Wasserstein distance.