This paper presents a straightforward method for learning a visual correspondence representation from raw video data. The approach involves predicting links in a space-time graph constructed from the video, where nodes represent patches from each frame. The nodes that are adjacent in time are connected by directed edges. The representation is learned by defining the transition probability of a random walk based on pairwise similarity. This allows for the computation of long-range correspondence by traversing the graph. The learning targets are generated using cycle-consistency, aiming to maximize the likelihood of returning to the initial node when walking along a graph constructed from a palindrome of frames. This approach implicitly supervises chains of intermediate comparisons. Experimental results demonstrate that the learned representation surpasses the current state-of-the-art self-supervised method in label propagation tasks involving objects, semantic parts, and pose. Additionally, the authors introduce a technique called edge dropout and self-supervised adaptation at test-time, which further enhance the transferability of the representation for object-centric correspondence.