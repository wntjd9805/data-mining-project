Discrete structures are crucial in fields like program language modeling and software engineering. While current approaches to predicting complex structures rely on autoregressive models for ease of use, they sacrifice flexibility. Energy-based models (EBMs), however, provide a more powerful and flexible approach to modeling such distributions. The drawback is that EBMs require partition function estimation. This paper introduces ALOE, a novel algorithm for learning conditional and unconditional EBMs for discrete structured data. A learned sampler that imitates local search is used to estimate parameter gradients. By employing a new variational form of power iteration, we efficiently train the energy function and sampler, achieving a better balance between flexibility and tractability. Experimental results demonstrate that learning local search leads to significant improvements in challenging application domains. Notably, an energy model guided fuzzer for software testing is presented, which performs similarly to well-engineered fuzzing engines like libfuzzer.