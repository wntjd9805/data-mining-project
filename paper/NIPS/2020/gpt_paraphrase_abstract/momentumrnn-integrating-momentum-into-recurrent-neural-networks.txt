Designing deep neural networks can be a complex task that involves searching through numerous potential architectures. This process can be time-consuming and expensive. However, for recurrent neural networks (RNNs), we have developed a method to overcome this challenge. By establishing a connection between the hidden state dynamics in an RNN and gradient descent, we have integrated momentum into this framework, resulting in a new family of RNNs called MomentumRNNs. Through theoretical proof and numerical demonstrations, we have shown that MomentumRNNs effectively address the issue of vanishing gradients during training. In particular, the Momentum long-short term memory (MomentumLSTM) outperforms its LSTM counterpart in terms of convergence speed and accuracy across various benchmarks. Furthermore, we have found that MomentumRNN can be applied to different types of recurrent cells, including those used in state-of-the-art orthogonal RNNs. Additionally, we have shown that incorporating other advanced momentum-based optimization methods, such as Adam and Nesterov accelerated gradients with a restart, into the MomentumRNN framework can lead to the design of new recurrent cells with even better performance.