Unsupervised pretraining followed by supervised fine-tuning is a method used for learning from few labeled examples and utilizing a large amount of unlabeled data. Unlike common approaches to semi-supervised learning for computer vision, this paradigm proves to be remarkably effective for semi-supervised learning on ImageNet. The key factor in this approach is the utilization of big networks during both pretraining and fine-tuning. It is observed that as the number of labeled examples decreases, the task-agnostic use of unlabeled data benefits more from larger networks. After fine-tuning, the big network can be further enhanced and distilled into a smaller one without significant loss in classification accuracy by utilizing unlabeled examples in a task-specific manner. The proposed semi-supervised learning algorithm consists of three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised fine-tuning with a few labeled examples, and distillation using unlabeled examples to refine and transfer task-specific knowledge. By employing this procedure, an impressive 73.9% ImageNet top-1 accuracy is achieved with just 1% of the labels (â‰¤13 labeled images per class) using ResNet-50. This represents a 10-fold improvement in label efficiency compared to the previous state-of-the-art. Furthermore, with 10% of the labels, ResNet-50 trained with this method achieves 77.5% top-1 accuracy, surpassing the performance of standard supervised training with all the labels.