The existing research on machine learning fairness assumes that protected attributes like race and sex are known and used to address fairness concerns. However, real-world factors like privacy and regulations often prevent the collection and use of such attributes, which limits the effectiveness of traditional fairness approaches. This study introduces Adversarially Reweighted Learning (ARL) to tackle this issue. ARL leverages non-protected attributes and task labels to co-train an adversarial reweighting method that improves fairness. Experimental results demonstrate that ARL enhances RawlsianMax-Min fairness, with significant improvements in AUC for worst-case protected groups across multiple datasets, surpassing current state-of-the-art methods.