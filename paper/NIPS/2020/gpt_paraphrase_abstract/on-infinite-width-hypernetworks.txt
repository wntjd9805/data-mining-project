Hypernetworks are architectures that generate the weights of a specific primary network. One popular use of hypernetworks is in learning to produce functional representations. These representations correspond to the weights of a shallow MLP that encodes shape or image information. While these representations have been successful in practice, they lack theoretical guarantees for standard architectures. This study focuses on wide over-parameterized hypernetworks and examines their convergence properties. It is found that infinitely wide hypernetworks do not guarantee convergence to a global minimum under gradient descent. However, by increasing the dimensionality of the hypernetwork's output to represent wide MLPs, convexity can be achieved. In the dually infinite-width regime, the functional priors of these architectures are identified by deriving their corresponding GP and NTK kernels, with the latter referred to as the hyperkernel. The study also contributes mathematically by deriving tight bounds on high order Taylor expansion terms of standard fully connected ReLU networks.