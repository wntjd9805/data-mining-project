Graph neural networks have shown promise in learning heuristics for combinatorial optimization problems. However, these methods have mostly focused on single-level optimization problems with one player. In this study, we extend these techniques to multilevel combinatorial optimization problems, which involve multiple players making sequential decisions. We propose a value-based method based on multi-agent reinforcement learning to solve multilevel budgeted combinatorial problems with two players in a zero-sum game. Our approach involves generating datasets of heuristically solved instances with increasing budgets to train the agent. Through this method, we achieve close to optimal results on graphs with up to 100 nodes. Additionally, we observe an average speedup of 185 times compared to the fastest known exact solver for the Multilevel Critical Node problem, which is a max-min-max trilevel problem known to be computationally challenging.