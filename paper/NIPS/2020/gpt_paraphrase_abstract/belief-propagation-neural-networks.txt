Learned neural solvers have been successful in solving combinatorial optimization and decision problems. However, counting variations of these problems are still predominantly solved using manually designed solvers. To bridge this gap, we propose belief propagation neural networks (BPNNs), which are a type of parameterized operators that work on factor graphs and generalize Belief Propagation (BP). A BPNN layer (BPNN-D) is an iterative operator that maintains many desirable properties of BP for any parameter choice. Empirically, we demonstrate that training BPNN-D allows it to outperform the original BP, converging 1.7 times faster on Ising models and providing tighter bounds. When applied to challenging model counting problems, BPNNs deliver estimates hundreds of times faster than existing handcrafted methods, while maintaining comparable quality of estimates.