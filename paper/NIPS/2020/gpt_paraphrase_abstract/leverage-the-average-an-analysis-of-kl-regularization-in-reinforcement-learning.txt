Recent reinforcement learning algorithms have achieved impressive results by using Kullback-Leibler regularization. However, the theoretical understanding of why this regularization method is effective is limited. In this study, we examine the role of KL regularization in an approximate value iteration scheme and discover that it implicitly averages q-values. Building on this finding, we develop a performance bound that has two desirable properties: a linear dependency on the horizon and an error propagation term that involves averaging the estimation errors. We also investigate the impact of incorporating an additional entropy regularizer into the framework, which encompasses various existing RL algorithms. Although some of our assumptions do not apply to neural networks, we supplement our theoretical analysis with an extensive empirical study.