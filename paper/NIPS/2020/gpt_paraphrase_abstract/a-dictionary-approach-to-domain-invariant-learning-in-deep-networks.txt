This paper explores the concept of domain-invariant deep learning by explicitly accounting for shifts in domains using a Convolutional Neural Network (CNN) with a minimal number of domain-specific parameters. The authors propose a method that decomposes a convolutional layer into a domain-specific atom layer and a domain-shared coefficient layer, effectively handling domain shifts. By convolving an input channel with domain-specific dictionary atoms to capture domain variations and then combining output channels using shared decomposition coefficients, the proposed framework promotes shared semantics across domains. The effectiveness of this approach is demonstrated through toy examples, thorough analysis, and real-world applications with diverse datasets and architectures. The proposed architecture requires only a small number of additional parameters, typically a few hundred, making it highly efficient for modeling additional domains.