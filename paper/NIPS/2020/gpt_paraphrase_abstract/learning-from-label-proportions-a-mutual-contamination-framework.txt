The learning from label proportions (LLP) approach is used for weakly supervised classification, where unlabeled training instances are organized into groups (bags) and each bag is annotated with the proportion of each class present. Previous studies on LLP have not established a consistent learning procedure or a theoretically justified training criterion. This research addresses these issues by framing LLP within the context of mutual contamination models (MCMs), which have proven successful in other weak supervision scenarios. The study introduces novel technical findings for MCMs, including unbiased losses and generalization error bounds for non-iid sampling plans. Additionally, the limitations of a commonly used experimental setting for LLP are identified, and a new setting based on the MCM framework is proposed.