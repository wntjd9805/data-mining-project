Effectively utilizing large datasets in reinforcement learning (RL) is a challenge for real-world applications. Offline RL algorithms aim to learn effective policies from static datasets without further interaction. However, offline RL faces challenges, and standard off-policy RL methods can fail due to overestimation of values caused by distributional shifts between the dataset and the learned policy, especially with complex and multi-modal data distributions. To address these limitations, we propose conservative Q-learning (CQL). CQL learns a conservative Q-function that ensures the expected value of a policy under this function is a lower bound of its true value. Theoretical analysis shows that CQL provides a lower bound on the policy value and can be incorporated into policy learning with improved guarantees. In practice, CQL adds a simple Q-value regularizer to the standard Bellman error objective, which can be easily implemented with existing deep Q-learning and actor-critic approaches. Experimental results on discrete and continuous control domains demonstrate that CQL significantly outperforms existing offline RL methods, achieving 2-5 times higher final returns, particularly with complex and multi-modal data distributions.