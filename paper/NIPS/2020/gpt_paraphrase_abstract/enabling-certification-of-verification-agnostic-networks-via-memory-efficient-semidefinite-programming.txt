Convex relaxations have emerged as a promising method for verifying desirable properties of neural networks, such as their ability to withstand adversarial perturbations. However, the widely used Linear Programming (LP) relaxations only work effectively when networks are trained specifically for verification purposes. This limits their applicability to networks that are not trained for verification (veriﬁcation-agnostic networks). On the other hand, semideﬁnite programming (SDP) relaxations have been successful in verifying veriﬁcation-agnostic networks, but their scalability is limited for larger networks due to inefficient time and space usage. To address this, we propose a novel ﬁrst-order dual SDP algorithm that offers several advantages. Firstly, it only requires memory linearly proportional to the total number of network activations. Secondly, it only requires a fixed number of forward/backward passes through the network per iteration. By leveraging iterative eigenvector methods, we are able to express all solver operations in terms of these passes through the network, allowing for efficient utilization of hardware resources like GPUs/TPUs. We demonstrate the effectiveness of our approach on two veriﬁcation-agnostic networks trained on MNIST and CIFAR-10 datasets, achieving significant improvements in veriﬁed robust accuracy from 1% to 40% and 6% to 88% respectively. Furthermore, we showcase the tight veriﬁcation of a quadratic stability speciﬁcation for the decoder of a variational autoencoder.