We investigate the online restless bandit problem, where each arm's state changes using a Markov chain, and the reward of pulling an arm depends on both the arm chosen and the current state of its Markov chain. We propose Restless-UCB, a learning policy that utilizes the explore-then-commit framework. Restless-UCB introduces a new method to construct offline instances, which requires only O(N) time complexity (N being the number of arms) and is significantly more efficient than existing learning policies. We prove that Restless-UCB achieves a regret upper bound of ËœO((N + M^3)T^(2/3)), where M represents the size of the Markov chain state space and T is the time horizon. Our approach eliminates the exponential factor (in M, N) in the regret upper bound compared to previous algorithms by exploiting the sparsity in transitions in general restless bandit problems. This technique can also be applied to improve the regret bounds of existing algorithms. Additionally, we conduct experiments using real-world data to compare Restless-UCB with state-of-the-art benchmarks. Results demonstrate that Restless-UCB outperforms existing algorithms in terms of regret and significantly reduces running time.