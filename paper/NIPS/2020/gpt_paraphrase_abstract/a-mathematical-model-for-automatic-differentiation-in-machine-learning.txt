Current implementations of automatic differentiation lack a mathematical model suitable for modern machine learning requirements. This study aims to establish the connection between differentiation of programs in practice and differentiation of nonsmooth functions. To achieve this, we introduce a basic set of functions called nonsmooth calculus and demonstrate their application in stochastic approximation techniques. Furthermore, we highlight the problem of artificial critical points generated by algorithmic differentiation and present conventional methods that effectively avoid these points with near certainty.