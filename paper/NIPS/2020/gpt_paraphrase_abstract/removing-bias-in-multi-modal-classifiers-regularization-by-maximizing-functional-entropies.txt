Recent datasets often contain multiple types of data, such as images, questions, and answers. When training deep neural network classifiers on these datasets, different types of data are exploited to varying degrees, resulting in a bias towards certain types of data. To address this issue, we propose a new regularization term based on functional entropy. This term encourages a balanced contribution from each type of data to the classification results. However, regularization with functional entropy is challenging. To overcome this, we develop a method based on the log-Sobolev inequality, which bounds the functional entropy with functional-Fisher-information. This maximizes the amount of information contributed by each type of data. On two challenging datasets, VQA-CPv2 and SocialIQ, we achieve state-of-the-art results while more evenly utilizing the different types of data. We also demonstrate the effectiveness of our method on the Colored MNIST dataset.