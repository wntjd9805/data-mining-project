Current interpretation methods for neural models in natural language processing focus on how information is encoded in hidden representations, but they do not determine if this information is actually used by the model. To address this, we propose a methodology based on causal mediation analysis to interpret which parts of a model are causally involved in its behavior. This approach allows us to examine the mechanisms that facilitate the flow of information from input to output through different model components called mediators. In a case study on gender bias in pre-trained Transformer language models, we apply this methodology to analyze the role of individual neurons and attention heads in mediating gender bias across three datasets. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that exhibit specialized behavior.