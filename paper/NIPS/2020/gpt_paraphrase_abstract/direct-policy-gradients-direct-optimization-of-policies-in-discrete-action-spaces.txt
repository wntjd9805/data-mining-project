Direct optimization is a framework that replaces integration with optimization for approximating gradients in models with discrete random variables. A? sampling is a framework for optimizing random objectives over large spaces. By combining these techniques, we propose a reinforcement learning algorithm called direct policy gradient (DirPG) algorithms. DirPG algorithms have the advantage of allowing the use of domain knowledge in the form of upper bounds on return-to-go during training, while still directly computing a policy gradient. We analyze the properties of DirPG algorithms and demonstrate that they have a higher probability of sampling informative gradients compared to REINFORCE. We also discover a built-in variance reduction technique and find that a previously viewed numerical approximation parameter can be interpreted as controlling risk sensitivity. Through empirical evaluation, we demonstrate the algorithm's effectiveness in illustrative domains compared to baselines.