This study examines the issue of credit assignment in reinforcement learning and delves into the optimal utilization of additional computation to improve an agent's predictions through planning with internal models. The benefits and unique aspects of utilizing forward models as proactive planning and backward models as reactive hindsight are explored. The study also evaluates the advantages, limitations, and complementary nature of both planning mechanisms in carefully designed scenarios. The selection of states for (re)-evaluation of predictions is investigated as a key aspect of effective planning using models. Additionally, the study addresses the challenge of model estimation and presents a range of methods, ranging from explicit environment-dynamics predictors to more abstract planner-aware models.