The most common methods for generating text using neural networks are fully autoregressive models and non-autoregressive models. This study proposes a new autoregressive model that can generate text more efficiently. By using conditional random fields with limited context, the researchers developed a cascaded decoding approach that produces high-quality output. They introduced a Markov transformer, a modified version of the fully autoregressive model, to parameterize this cascade and allow for decoding with specific autoregressive context cutoffs. This approach requires only a minor modification to the standard autoregressive training process and achieves a good balance between accuracy and speed compared to existing methods on five machine translation datasets.