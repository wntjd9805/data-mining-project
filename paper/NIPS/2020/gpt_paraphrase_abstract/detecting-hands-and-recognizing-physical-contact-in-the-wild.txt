We investigate the detection of hands and recognition of their physical contact state in challenging, unconstrained conditions. We propose a novel convolutional network, based on Mask-RCNN, that can learn to localize hands and predict their contact state. The network uses outputs from an object detector to locate objects in the scene, and combines this information with hand locations to determine the contact state using two attention mechanisms. We introduce a large-scale dataset, ContactHands, with annotated hand locations and contact states, to evaluate our method. The proposed network, including the attention modules, is trainable end-to-end and achieves a 7% improvement over a baseline network. Code and data are available at: https://github.com/cvlab-stonybrook/ContactHands.