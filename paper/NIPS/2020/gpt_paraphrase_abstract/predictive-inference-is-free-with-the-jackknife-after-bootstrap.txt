Ensemble learning is commonly used in complex decision problems to make accurate predictions. However, there is limited knowledge on how to perform valid and assumption-free inference on the output of these methods. This paper introduces a new procedure called jackknife+-after-bootstrap (J+aB) that constructs a predictive interval using bootstrapped samples and their fitted models. The J+aB procedure is cost-effective as it does not require additional model fitting. It provides a predictive coverage guarantee without assuming anything about the data distribution, the fitted model, or the aggregation of models. In the worst case scenario, the failure rate of the predictive interval is inflated by a maximum factor of 2. Numerical experiments on real data confirm the coverage and accuracy of the resulting predictive intervals.