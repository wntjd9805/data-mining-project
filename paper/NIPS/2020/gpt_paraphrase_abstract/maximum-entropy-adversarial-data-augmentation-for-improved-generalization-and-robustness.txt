This paper introduces a new regularization term for adversarial data augmentation, aimed at training deep neural networks to be more robust against unexpected data shifts or corruptions. The proposed regularization term is derived from the information bottleneck principle and encourages perturbations of the source distribution to increase predictive uncertainty in the model. This leads to the generation of "hard" adversarial perturbations that enhance the model's robustness during training. Experimental results on three standard benchmarks demonstrate that the proposed method consistently outperforms existing state-of-the-art techniques by a statistically significant margin. The code for this method is publicly available on GitHub.