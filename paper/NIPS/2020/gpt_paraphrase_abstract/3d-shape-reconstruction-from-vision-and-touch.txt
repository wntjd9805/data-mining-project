The fusion of visual and haptic modalities in 3D shape reconstruction is an underexplored area. This study introduces a chart-based approach to multi-modal shape understanding, inspired by how toddlers use touch and vision to understand new toys. A dataset of simulated touch and vision signals from a robotic hand interacting with various 3D objects is introduced. Results show that combining vision and touch signals consistently improves single-modality baselines. The proposed chart-based approach outperforms alternative fusion methods and benefits from the structure. The quality of reconstruction improves with the number of grasps provided, and touch information not only enhances the reconstruction at the touch site but also extrapolates to the surrounding area.