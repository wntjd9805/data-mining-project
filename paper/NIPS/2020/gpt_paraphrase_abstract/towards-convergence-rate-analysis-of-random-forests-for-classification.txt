Random forests are a popular ensemble algorithm in machine learning, where multiple random trees are created and their predictions are averaged. While most research has focused on the consistency of random forests for regression, this study examines their convergence rates for classification. The first finite-sample rate of convergence for pure random forests is determined to be O(n−1/(8d+2)), which can be improved to O(n−1/(3.87d+2)) by considering the midpoint splitting mechanism. Another variant of random forests is introduced, which follows Breiman's original approach but with different splitting mechanisms. This variant achieves a convergence rate of O(n−1/(d+2)(ln n)1/(d+2)), approaching the minimax rate of the optimal plug-in classifier under the L-Lipschitz assumption, except for a factor of (ln n)1/(d+2). Additionally, a tighter convergence rate of O((cid:112)ln n/n) is achieved under specific assumptions about structural data.