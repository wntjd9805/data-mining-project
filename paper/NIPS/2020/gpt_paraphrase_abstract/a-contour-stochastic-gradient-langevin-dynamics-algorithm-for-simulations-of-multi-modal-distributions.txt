We introduce a new algorithm called contour stochastic gradient Langevin dynamics (CSGLD) for Bayesian learning in big data statistics. CSGLD is a scalable dynamic importance sampler that flattens the target distribution, making simulation for multi-modal distributions easier. We prove the stability condition and asymptotic convergence of the self-adapting parameter, regardless of the non-convexity of the energy function. We also provide an error analysis for the weighted averaging estimators. Empirically, CSGLD outperforms existing algorithms when tested on benchmark datasets like CIFAR10 and CIFAR100 for training deep neural networks.