Recent research has shown that various models can be learned efficiently using neural network algorithms. However, these results typically require very large networks unless certain conditions are met. In this paper, we aim to achieve learnability results with network sizes that are close to optimal. To do this, we analyze the convergence rate of the Neural Tangent Kernel, which is a key tool in studying the behavior of neural networks. Based on this analysis, we demonstrate that stochastic gradient descent on depth two neural networks, initialized using a specific method, can effectively memorize samples, learn bounded-weight polynomials, and learn specific kernel spaces. This can be done using network sizes, sample complexity, and runtime that are close to optimal. Our findings show that a depth two network with approximately O(m*d) hidden neurons (and hence O(m) parameters) can successfully memorize m randomly labeled points in Sd 1.