Catastrophic forgetting is a problem in neural networks where they struggle to learn multiple tasks in sequence because they forget previous knowledge. This is due to neural networks being overly plastic and lacking stability. Many approaches have been proposed to address this issue, but there has been limited analysis on how different training methods, such as learning rate, batch size, and regularization, impact forgetting. Instead of changing the learning algorithm, this study focuses on the geometric properties of local minima in each task. The study examines the effects of dropout, learning rate decay, and batch size on widening the local minima and preventing catastrophic forgetting. The findings suggest that simple techniques can improve stability and outperform other approaches.