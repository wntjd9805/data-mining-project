Meta-learning algorithms have two components: a model that predicts task targets and a base learner that updates the model using examples from new tasks. However, this additional level of learning can lead to overfitting in either the model or the base learner. In this study, we explore both forms of overfitting in common meta-learning benchmarks. We propose an information-theoretic framework called meta-augmentation, which involves adding randomness to discourage the base learner and model from learning trivial solutions that do not generalize well to new tasks. Our experiments show that meta-augmentation provides significant benefits in addition to existing meta-regularization techniques.