This study investigates why ADAM-like adaptive gradient algorithms have worse generalization performance than SGD, despite their faster training speed. The researchers analyze the local convergence behaviors of these algorithms and find that there are heavy tails of gradient noise present. To understand these algorithms better, the researchers analyze them through their LÃ©vy-driven stochastic differential equations (SDEs), as the convergence behaviors of an algorithm and its SDE are similar. They establish the escaping time of these SDEs from a local basin and find that both SGD and ADAM's escaping time depends on the Radon measure of the basin and the heaviness of gradient noise. However, SGD has a smaller escaping time than ADAM for the same basin. This is because ADAM's geometry adaptation reduces the anisotropic structure in gradient noise, resulting in a larger Radon measure of the basin, and its exponential gradient average leads to lighter gradient noise tails compared to SGD. Therefore, SGD is more locally unstable than ADAM at sharp minima, and can better escape from them to flatter minima with a larger Radon measure. Since flatter minima often generalize better than sharp ones, this explains the better generalization performance of SGD over ADAM. Experimental results confirm the heavy-tailed gradient noise assumption and theoretical findings.