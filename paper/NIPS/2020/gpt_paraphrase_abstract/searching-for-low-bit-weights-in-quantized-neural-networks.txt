Quantized neural networks with low-bit weights and activations are appealing for AI accelerators. However, most existing quantization methods use non-differentiable functions, making it difficult to optimize quantized networks. This is because low-bit values have a smaller set of possibilities compared to full-precision parameters. To address this, we propose treating the discrete weights in a quantized neural network as searchable variables and using a differential method to accurately search them. Each weight is represented as a probability distribution over the discrete value set, and these probabilities are optimized during training. The values with the highest probability are selected to establish the desired quantized network. Experimental results on image classification and super-resolution tasks show that our method outperforms state-of-the-art techniques. The PyTorch code can be found at https://github.com/huawei-noah/Binary-Neural-Networks/tree/main/SLB, and the MindSpore code is available at https://www.mindspore.cn/resources/hub.