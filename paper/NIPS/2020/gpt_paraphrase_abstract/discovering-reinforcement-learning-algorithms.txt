Reinforcement learning (RL) algorithms update an agent's parameters based on manually discovered rules. Discovering update rules from data could lead to more efficient algorithms or algorithms better suited for specific environments. While previous attempts have been made to address this challenge, it remains uncertain if it is possible to find alternatives to fundamental RL concepts like value functions and temporal-difference learning. This paper presents a new meta-learning approach that learns an entire update rule, encompassing both "what to predict" (e.g., value functions) and "how to learn from it" (e.g., bootstrapping), by interacting with various environments. The result is a RL algorithm named Learned Policy Gradient (LPG). Experimental results demonstrate that LPG discovers its own alternative to value functions and a bootstrapping mechanism to utilize its predictions. Interestingly, when trained solely on simple environments, LPG can effectively generalize to complex Atari games and achieve significant performance. This highlights the potential of discovering general RL algorithms from data.