Cross-domain disentanglement refers to the problem of learning representations that are divided into domain-invariant and domain-specific representations. This is important for successful domain transfer and measuring semantic distance between domains. Instead of using adversarial training or gradient reversal layers, we propose a method based on information theory. We introduce multiple information constraints to simultaneously learn domain-invariant and domain-specific representations. We develop a tractable bound for this objective and propose a generative model called Interaction Information Auto-Encoder (IIAE). Our approach provides insights into the desired representation for cross-domain disentanglement and its relationship to Variational Auto-Encoder (VAE). We validate our model through image-to-image translation and cross-domain retrieval tasks. Additionally, we demonstrate that our model achieves state-of-the-art performance in zero-shot sketch-based image retrieval, even without external knowledge.