The study focuses on multi-task reinforcement learning (RL), which involves learning policies for multiple tasks simultaneously. Previous research has found that relabeling past experience with different reward functions can improve sample efficiency. Relabeling methods address the question of which task the experience was optimal for, in hindsight. Inverse RL provides an answer to this question. The paper demonstrates that inverse RL is a systematic approach for reusing experience across tasks. The authors extend goal-relabeling techniques from prior research to various types of reward functions. Experimental results show that relabeling data using inverse RL outperforms previous relabeling methods in goal-reaching tasks and accelerates learning in multi-task settings where previous methods are not applicable, such as domains with discrete sets of rewards and linear reward functions.