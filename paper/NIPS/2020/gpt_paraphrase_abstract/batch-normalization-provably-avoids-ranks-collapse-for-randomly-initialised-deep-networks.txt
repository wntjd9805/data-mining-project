The difficulty of training randomly initialized neural networks increases with depth, but can be mitigated by using architectural enhancements such as residual connections and batch normalization. This study investigates the relationship between random initialization in deep networks and spectral instabilities in products of random matrices. It is found that the rank of intermediate representations in unnormalized networks quickly collapses with depth. However, batch normalization prevents rank collapse in both linear and ReLU networks. By utilizing Markov chain theory, a lower rank bound is derived for deep linear networks. Empirical evidence shows that this rank robustness extends to ReLU networks as well. Extensive experiments on real-world datasets confirm the importance of rank stability for effectively training deep neural architectures.