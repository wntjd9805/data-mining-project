The study of vulnerabilities in deep neural networks (DNNs) has made rapid progress. Current attacks require either internal or external access to the victim model, which may not be feasible or costly in certain scenarios. We explore the concept of no-box adversarial examples, where the attacker has no access to the model or training set and cannot query the model. Instead, the attacker can only gather a small number of examples from the same problem domain as the victim model. This stronger threat model expands the applicability of adversarial attacks. We propose three mechanisms for training with a very small dataset and find that prototypical reconstruction is the most effective. Our experiments demonstrate that adversarial examples created using prototypical auto-encoding models transfer well to various image classification and face verification models. We successfully reduce the average prediction accuracy of a commercial celebrity recognition system to only 15.40%, comparable to attacks transferring adversarial examples from a pre-trained Arcface model. The code for our approach is publicly available at: https://github.com/qizhangli/nobox-attacks.