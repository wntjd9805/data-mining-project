We investigate an extended version of the stochastic multi-armed bandit problem where multiple plays and Markovian rewards are considered in the rested bandits setting. To address this problem, we propose an adaptive allocation rule that combines the sample means of all arms with the Kullback-Leibler upper confidence bound of a single arm chosen in a round-robin manner. We provide a finite-time upper bound for the regret incurred by this adaptive allocation rule when the rewards are generated from a one-parameter exponential family of Markov chains. Our analysis reveals that the regret has a logarithmic dependence on the time horizon and is asymptotically optimal. We also develop concentration results for Markov chains, including a maximal inequality, which may have independent interest. Moreover, we establish asymptotically optimal finite-time guarantees for the case of multiple plays and i.i.d. rewards drawn from a one-parameter exponential family of probability densities. We present simulation results that demonstrate the efficiency of calculating Kullback-Leibler upper confidence bounds in a round-robin manner compared to calculating them for each arm at every round, with both approaches yielding similar expected regrets.