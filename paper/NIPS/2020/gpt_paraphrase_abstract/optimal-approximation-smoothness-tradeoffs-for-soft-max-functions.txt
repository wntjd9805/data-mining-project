Different soft-max functions are evaluated based on their approximation and smoothness measures. The goal is to determine the optimal tradeoff between these measures for various applications. The commonly used exponential mechanism has the best tradeoff between approximation and smoothness, measured by expected additive approximation and R√©nyi Divergence, respectively. However, a new soft-max function called piecewise linear soft-max is introduced, which has the optimal tradeoff between worst-case additive approximation and smoothness measured by ‚Ñìùëû-norm. This function enforces sparsity in the output, which is important in Machine Learning applications. Additionally, the piecewise linear mechanism outperforms the exponential mechanism in Mechanism Design and Game Theory due to its ‚Ñìùëû-smoothness. Another soft-max function, the power mechanism, is examined, which has the optimal tradeoff between expected multiplicative approximation and smoothness measured by R√©nyi Divergence. This function provides improved results in differentially private submodular optimization.