The Information Bottleneck (IB) objective is commonly used in discriminative classification tasks to balance task-performance and robustness. However, it is unclear whether this objective can be applied to generative likelihood models like normalizing flows, as they are designed to preserve information rather than create a bottleneck. In this study, we introduce a new class of conditional normalizing flows called IB-INNs, where the IB objective is used to train invertible network architectures. By introducing controlled information loss, we are able to maintain the generative capabilities of the INNs while achieving an asymptotically exact formulation of the IB. Through experimentation, we explore the properties of these models when used as generative classifiers. Although traditional generative classifier solutions often suffer in classification accuracy, our findings show that the trade-off parameter in the IB controls a balance between generative capabilities and accuracy similar to standard classifiers. Furthermore, our uncertainty estimates in this mixed regime compare favorably to conventional generative and discriminative classifiers. The code for implementing IB-INNs can be found at github.com/VLL-HD/IB-INN.