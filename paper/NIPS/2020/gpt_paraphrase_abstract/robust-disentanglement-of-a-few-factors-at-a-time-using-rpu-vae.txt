Unsupervised learning is focused on disentanglement, as it improves generalization, interpretability, and performance in downstream tasks. However, current unsupervised approaches are not suitable for real-world datasets due to inconsistent performance and limited disentanglement compared to (semi-)supervised approaches. To address this, we propose population-based training (PBT) to enhance the consistency of training variational autoencoders (VAEs). In a supervised setting, we validate the effectiveness of PBT through PBT-VAE. Additionally, we introduce Unsupervised Disentanglement Ranking (UDR) as a heuristic for scoring models during PBT-VAE training. This scoring approach reveals that models trained using PBT tend to consistently disentangle only a subset of generative factors. Based on this observation, we propose the recursive rPU-VAE approach, where we train the model until convergence, remove the learned factors from the dataset, and repeat the process. By labeling subsets of the dataset with the learned factors and using these labels to train a single model, we achieve full disentanglement of the entire dataset. Our approach significantly improves unsupervised disentanglement performance and robustness across multiple datasets and metrics, surpassing the current state-of-the-art methods.