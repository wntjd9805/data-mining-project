This paper presents an efficient algorithm for online linear optimization with delayed bandit feedback. The framework of online linear optimization with bandit feedback is commonly used for decision-making problems with limited information. However, it assumes that feedback can be observed immediately after taking an action, which is not always the case in practical applications. To address this, we consider scenarios where feedback can only be observed d rounds after the action is taken. We propose an algorithm with an expected regret of ˜O( m(m + d)T ), disregarding logarithmic factors in m and T, where m represents the dimensionality of the action set and T is the number of rounds. This algorithm achieves nearly optimal performance, as we demonstrate that any other algorithm would suffer a regret of ⌦( m(m + d)T ) in the worst case. Our algorithm utilizes a technique called distribution truncation, which plays a crucial role in bounding the regret. Additionally, we extend our approach to cooperative bandits, building upon previous studies by Cesa-Bianchi et al. [18] and Bar-On and Mansour [12].