We present a new distributed second order optimization algorithm called COMRADE (COMunication-efÔ¨Åcient and Robust Approxi-mate Distributed nEwton). Unlike existing algorithms, such as GIANT and DINGO, COMRADE achieves communication efficiency by having worker machines communicate with the center machine only once per iteration. Additionally, we demonstrate that the worker machines can compress the local information before sending it to the center. We also implement a norm-based thresholding rule to filter out Byzantine worker machines. Our analysis shows that COMRADE converges at a linear-quadratic rate and has a low statistical error rate for convex loss functions. This is the first work to address the issue of Byzantine resilience in second order distributed optimization. We validate our theoretical findings through extensive experiments on synthetic and benchmark LIBSVM datasets, confirming convergence guarantees.