This paper introduces a general framework for designing and analyzing reinforcement learning algorithms that demonstrate optimism in the face of uncertainty. The framework is based on Lagrangian duality and shows that model-optimistic algorithms and value-optimistic algorithms can be equivalent. Traditionally, these two types of algorithms were considered distinct, with model-optimistic algorithms offering a cleaner probabilistic analysis and value-optimistic algorithms being easier to implement. However, the framework presented in this paper combines the advantages of both approaches, providing computationally efficient dynamic programming implementation and simple probabilistic analysis. The framework is not only applicable to tabular settings but also allows for addressing large-scale problems with realizable function approximation.