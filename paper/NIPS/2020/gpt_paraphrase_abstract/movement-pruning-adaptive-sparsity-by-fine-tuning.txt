Magnitude pruning is commonly used to reduce model size in supervised learning, but it is less effective in transfer learning for natural language processing. We propose movement pruning, a simple and deterministic weight pruning method that is better suited for fine-tuning pretrained models. We provide mathematical foundations for this method and compare it to existing pruning methods. Our experiments demonstrate that movement pruning significantly improves performance in high-sparsity scenarios when applied to large pretrained language models. When combined with distillation, this approach achieves minimal accuracy loss with as little as 3% of the model parameters.