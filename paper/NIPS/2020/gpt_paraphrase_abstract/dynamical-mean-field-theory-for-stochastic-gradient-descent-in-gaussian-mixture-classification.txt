We examine the learning process of stochastic gradient descent (SGD) for a neural network that classifies a high-dimensional Gaussian mixture. This problem showcases a non-convex loss landscape with varying behavior and a significant generalization gap. We introduce a stochastic process that extends SGD to a continuous-time limit known as stochastic gradient flow. When considering the entire dataset, we recover the standard gradient flow. Using dynamical mean field theory, we analyze the algorithm's dynamics in the high-dimensional limit using a self-consistent stochastic process. By studying the algorithm's performance with different control parameters, we gain insights into how it navigates the loss landscape.