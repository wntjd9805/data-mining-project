This study examines minimax optimization problems in which the objective function is strongly convex with respect to x and strongly concave with respect to y, and is also smooth. The authors propose a new algorithm that improves upon the previous best upper bound for the gradient complexity. The new bound achieves a linear convergence rate and has a tighter dependency on condition numbers, particularly when the interaction between x and y is weak. The improved bounds also apply to other types of minimax optimization problems. When the objective function is quadratic, the upper bound can be further improved to match the lower bound, with only a small sub-polynomial factor difference.