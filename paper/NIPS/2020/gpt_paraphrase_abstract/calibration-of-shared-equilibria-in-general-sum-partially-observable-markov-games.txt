This abstract discusses the training of multi-agent systems (MAS) to achieve realistic equilibria in order to understand and model real-world systems. The paper focuses on a specific scenario where agents of different types share a single policy network. The goal of the paper is twofold: i) to formally understand the equilibria reached by these agents, and ii) to match the emergent phenomena of these equilibria with real-world targets. The concept of Shared equilibrium is introduced as a symmetric pure Nash equilibrium of a certain Functional Form Game (FFG), and convergence to this equilibrium is proven for a certain class of games using self-play. Additionally, the paper addresses the need for these equilibria to satisfy certain constraints that align with real-world data. This is achieved through a dual-Reinforcement Learning based approach that fits the emergent behaviors of agents in a Shared equilibrium to externally-specified targets. The approach is applied to a n-player market example, where parameters governing distributions of agent types are calibrated instead of individual agents. This allows for behavior differentiation among agents and coherent scaling of the shared policy network to multiple agents.