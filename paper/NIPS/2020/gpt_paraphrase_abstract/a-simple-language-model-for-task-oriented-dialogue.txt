Task-oriented dialogue is typically divided into three tasks: understanding user input, deciding on actions, and generating a response. Although this division suggests the need for separate models for each sub-task, we have discovered that a simple and unified approach achieves state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a straightforward method for task-oriented dialogue that utilizes a single language model trained on all sub-tasks, treating them as a single sequence prediction problem. This enables SimpleTOD to effectively utilize transfer learning from pre-trained causal language models like GPT-2. SimpleTOD surpasses the previous state-of-the-art in accurately tracking dialogue state goals, and our analysis demonstrates its robustness to noisy annotations. Furthermore, SimpleTOD improves the primary metrics for evaluating action decisions and response generation in an end-to-end scenario, increasing the inform rate by 8.1 points, success rate by 9.7 points, and combined score by 7.2 points.