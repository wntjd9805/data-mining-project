Reinforcement learning has faced challenges in real-world applications due to the expensive exploration required to learn a good policy. To overcome this, imitation learning (IL) has been used with an oracle policy during training to accelerate the learning process. However, in practical scenarios, multiple suboptimal oracles may be available, leading to conflicting advice. The existing IL literature lacks a comprehensive approach for such situations. This paper proposes using the maximum value of the oracle policies as a baseline to resolve conflicting advice. The paper introduces a new IL algorithm called MAMBA, which optimizes policies using a gradient estimator similar to generalized advantage estimation. Theoretical analysis shows that MAMBA is robust and can outperform oracle policies by a larger margin compared to the state-of-the-art IL methods, even in the single-oracle case. Evaluation against standard policy gradient methods demonstrates MAMBA's ability to leverage demonstrations from both single and multiple weak oracles, resulting in significant speed improvements in policy optimization.