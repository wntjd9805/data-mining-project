Reinforcement learning (RL) has traditionally focused on simple representations of value functions. To make progress, researchers are now combining RL with advanced function approximators like kernel functions and deep neural networks. This has resulted in empirical successes in large-scale applications. However, developing a theory to support this approach is challenging due to the need to balance exploration and exploitation in RL, as well as the computational and statistical tradeoffs that arise with function approximation. In this study, we tackle these challenges by investigating an optimistic modification of the least-squares value iteration algorithm. We apply this algorithm to the action-value function represented by a kernel function or an overparameterized neural network. We demonstrate that this algorithm has both polynomial runtime complexity and polynomial sample complexity, without requiring additional assumptions on the data-generating model. Specifically, we prove that the algorithm incurs a regret of H^2pT, where H represents the length of each episode and T represents the total number of episodes. Importantly, our regret bounds are independent of the number of states, highlighting the advantages of function approximation in RL.