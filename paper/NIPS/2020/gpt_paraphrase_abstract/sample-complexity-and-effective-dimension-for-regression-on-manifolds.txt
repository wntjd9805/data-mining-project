We examine the use of reproducing kernel Hilbert space methods for regression on a manifold. Manifold models are commonly used in modern machine learning problems, and our objective is to gain insight into the effectiveness of various dimensionality-reduction techniques that exploit the structure of manifolds. Our first significant contribution is the establishment of a new nonasymptotic version of the Weyl law from differential geometry. This allows us to demonstrate that certain spaces of smooth functions on a manifold have an effectively finite dimension, which scales based on the manifold dimension rather than the data dimension. Furthermore, we demonstrate that a kernel regression estimator, derived from the spectral decomposition of the manifold, provides minimax-optimal error bounds when given function values (possibly with noise) uniformly sampled from the manifold. These error bounds are controlled by the effective dimension.