We examine the reliance of existing reinforcement learning (RL) algorithms, specifically Safe and Seldonian RL, on accurate training data. To evaluate susceptibility to perturbations in training data, we propose a new measure of security by creating an attacker model for worst-case analysis. Our analysis reveals that certain Seldonian RL methods are highly sensitive to even minor data corruptions. To address this issue, we present a more resilient algorithm that is less affected by data corruptions. We validate the effectiveness of this new algorithm through practical application in RL problems such as grid-world and diabetes treatment simulation.