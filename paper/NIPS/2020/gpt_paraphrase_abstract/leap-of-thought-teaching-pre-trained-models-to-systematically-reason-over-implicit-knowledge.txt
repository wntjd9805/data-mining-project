This study explores the extent to which neural networks can engage in systematic reasoning using symbolic facts. Previous evidence suggests that large pre-trained language models (LMs) possess some reasoning capabilities, although controlling these abilities is challenging. Recent research has shown that Transformer-based models excel at consistent reasoning with explicit symbolic facts in a "closed-world" context. However, in an open-domain scenario, it is desirable to leverage the implicit knowledge already embedded in the pre-trained LM parameters. This study demonstrates, for the first time, that LMs can be trained to systematically reason by combining both implicit pre-trained knowledge and explicit natural language statements. The researchers develop a method for automatically generating datasets that teach the model new reasoning skills and show that the models effectively perform inference using implicit taxonomic and world knowledge, chaining, and counting. Moreover, the models can generalize their reasoning skills beyond the training distribution, successfully employing multiple reasoning abilities in single examples. This research opens the door to open-domain systems that continuously enhance their performance through user interaction, where users can instantly correct the model by adding simple natural language statements.