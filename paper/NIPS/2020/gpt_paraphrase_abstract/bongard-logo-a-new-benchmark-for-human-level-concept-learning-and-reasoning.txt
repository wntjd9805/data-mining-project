Humans have the ability to learn new concepts with minimal examples and apply them to different scenarios. However, current machine learning models struggle to bridge the gap between pattern recognition at a machine-level and concept learning at a human-level. To address this, the Bongard problems (BPs) were introduced as a challenge for visual cognition in intelligent systems. Despite advancements in representation learning and learning to learn, BPs continue to pose a significant challenge for modern AI. In this study, we propose a new benchmark called BONGARD-LOGO, which aims to assess human-level concept learning and reasoning. We utilize a program-guided generation technique to create a large collection of visual cognition problems in the LOGO language that are interpretable by humans. Our benchmark captures three key aspects of human cognition: context-dependent perception, analogy-making perception, and perception with limited samples but infinite vocabulary. Experimental results demonstrate that state-of-the-art deep learning methods perform notably worse than human subjects on this benchmark, suggesting a failure to capture essential properties of human cognition. Finally, we discuss possible research directions for developing a general architecture for visual reasoning to tackle this benchmark.