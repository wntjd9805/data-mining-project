We tackle the issue of Federated Learning (FL) where users are distributed and divided into clusters. This scenario occurs when different user groups have their own learning objectives, but by combining their data with others in the same cluster, they can enhance the efficiency of Federated Learning. We introduce a new framework called the Iterative Federated Clustering Algorithm (IFCA), which iteratively estimates the cluster identities of users and optimizes model parameters for user clusters using gradient descent. We analyze the convergence rate of IFCA in linear models with squared loss and generic strongly convex and smooth loss functions. We demonstrate that with appropriate initialization, IFCA converges exponentially and discuss the optimality of the statistical error rate. When the clustering structure is uncertain, we propose combining IFCA with the weight sharing technique in multi-task learning to train the models. In our experiments, we demonstrate that our algorithm can succeed even with random initialization and multiple restarts. We also show that IFCA is efficient in non-convex problems like neural networks. We validate the advantages of IFCA over baselines using various clustered FL benchmarks.