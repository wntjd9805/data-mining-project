Spike neural networks (SNNs) are ideal for learning spatio-temporal patterns and can be implemented on energy-efficient neuromorphic processors. However, current SNN error backpropagation methods have limitations in dealing with spiking discontinuities and perform poorly compared to traditional artificial neural networks. These methods also require a large number of time steps, resulting in high latency and making spike-based computation unsuitable for deep architectures. To address these issues, we propose a new training method called Temporal Spike Sequence Learning Backpropagation (TSSL-BP). TSSL-BP breaks down error backpropagation into inter-neuron and intra-neuron dependencies, resulting in improved temporal learning accuracy. It captures inter-neuron dependencies by considering the all-or-none nature of firing activities, and intra-neuron dependencies by considering the evolution of each neuronal state over time. TSSL-BP effectively trains deep SNNs within a shorter temporal window, while also improving accuracy on various image classification datasets such as CIFAR10.