This study explores the explainability of neural networks by analyzing the characteristics of their decision boundaries. Drawing from the field of adversarial robustness, the researchers propose a novel perspective that connects dataset features to the distance of samples from the decision boundary. By manipulating the position of training samples, they investigate the impact on the boundaries of convolutional neural networks (CNNs) trained on large-scale vision datasets. The findings reveal interesting properties of CNNs, including their high invariance to non-discriminative features and the requirement for certain features to maintain decision boundaries. Additionally, the study highlights the sensitivity of decision boundary construction to small perturbations in training samples, with changes in specific directions leading to sudden invariances in orthogonal directions. This mechanism aligns with the approach of adversarial training for achieving robustness in neural networks.