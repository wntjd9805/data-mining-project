The current prevailing approach in sensorimotor control involves training policies directly in raw action spaces such as torque, joint angle, or end-effector position. However, this approach limits scalability to tasks that are continuous, high-dimensional, and long-horizon, as decisions need to be made at each point during training. In contrast, classical robotics research has utilized dynamical systems as a policy representation to learn robot behaviors through demonstrations. Although these techniques lack the flexibility and generalizability provided by deep learning or deep reinforcement learning, they have not been thoroughly explored in such settings. In this study, we aim to bridge this gap by incorporating dynamics structure into deep neural network-based policies through reparameterizing action spaces with differential equations. We introduce Neural Dynamic Policies (NDPs), which make predictions in trajectory distribution space instead of using raw control space for actions. This embedded structure allows for end-to-end policy learning in both reinforcement and imitation learning setups. Our results demonstrate that NDPs achieve performance comparable to or better than state-of-the-art approaches on various robotic control tasks using reward-based training and demonstrations. For further details and access to the project video and code, please visit: https://shikharbahl.github.io/neural-dynamic-policies/.