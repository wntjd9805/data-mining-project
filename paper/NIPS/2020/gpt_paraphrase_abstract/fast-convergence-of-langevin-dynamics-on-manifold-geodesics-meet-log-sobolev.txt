Sampling from a high dimensional distribution is a crucial task in Machine Learning. The Langevin Algorithm (LA) is an effective method to achieve this, even when dealing with non-convex functions. Recent studies have made significant progress in demonstrating the fast convergence of LA, particularly in cases where the function f is defined in Rn or has symmetries with manifold structure. Our research extends the findings of previous work by Vempala and Wibisono, focusing on functions defined on a manifold M instead of Rn. We establish that the KL divergence decreases at a geometric rate when the distribution eâˆ’f satisfies a log-Sobolev inequality on M.