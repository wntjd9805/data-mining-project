We investigate the problem of repeatedly learning linear classifiers against agents who strategically manipulate the classifiers. We measure the performance of our algorithms using the Stackelberg regret. We demonstrate that the concepts of Stackelberg and external regret are incompatible in the context of strategic classification. We propose a strategy-aware algorithm that minimizes the Stackelberg regret and provide upper and lower regret bounds. Additionally, we conduct simulations to support our theoretical findings. Our research expands the existing literature on learning from revealed preferences, which has primarily focused on simpler assumptions.