Zero-shot semantic segmentation (ZSS) is a technique that aims to classify pixels of new classes without any training examples. While most ZSS methods focus on learning the visual-semantic correspondence between seen and unseen classes, they often overlook the negative effects caused by noisy and outlying training samples from the seen classes. This paper identifies this challenge and presents a novel framework to address it. The framework utilizes Bayesian uncertainty estimation to discriminate noisy samples. The network outputs are modeled using Gaussian and Laplacian distributions, with variances representing observation noise and uncertainty of input samples. Learning objectives are then derived, with the estimated variances acting as adaptive attenuation for individual samples during training. As a result, the model learns more effectively from representative samples of seen classes while minimizing the impact of noisy and outlying samples. This leads to improved reliability and generalization towards unseen categories. The effectiveness of the framework is demonstrated through extensive experiments on multiple challenging benchmarks, showing significant accuracy improvement compared to previous approaches for large-scale open-set segmentation.