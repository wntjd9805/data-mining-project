Graph neural networks (GNNs) have become a powerful tool for learning software engineering tasks such as code completion, bug finding, and program repair. However, GNNs are not well-suited for tasks like program execution that require more sequential reasoning steps than the number of GNN propagation steps. On the other hand, recurrent neural networks (RNNs) are better at handling long sequential chains of reasoning but struggle to incorporate program structure and perform worse on the aforementioned tasks. To combine the strengths of both GNNs and RNNs, we introduce a novel GNN architecture called the InstructionPointer Attention Graph Neural Network (IPA-GNN). This model achieves improved systematic generalization in learning to execute programs using control flow graphs. The IPA-GNN can be seen as a continuous relaxation of the RNN model or as a GNN variant specifically designed for execution. We evaluate these models by testing systematic generalization on learning to execute using control flow graphs, which measures sequential reasoning and the utilization of program structure. Additionally, we evaluate these models on the task of learning to execute partial programs, which simulates the use of the model as a heuristic function in program synthesis. The results demonstrate that the IPA-GNN outperforms various RNN and GNN baselines in both tasks.