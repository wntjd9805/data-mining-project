This paper demonstrates that when using over-parametrization, common stochastic optimization algorithms can escape saddle-points and converge to local-minimizers more quickly. Over-parametrized models have the capability to interpolate training data, and assuming interpolation-like conditions are met, the Perturbed Stochastic Gradient Descent (PSGD) algorithm's complexity to reach an ✏-local-minimizer matches the deterministic rate of ˜ (1/✏2). The Stochastic Cubic-Regularized Newton (SCRN) algorithm's complexity to reach an ✏-local-minimizer under interpolation-like conditions is ˜ (1/✏2.5), which is better than both PSGD and SCRN without interpolation-like assumptions. However, it does not match the deterministic rate of ˜ (1/✏1.5) achieved by the Cubic-Regularized Newton method. Additional interpolation-like assumptions based on Hessian are likely needed to close this gap. The improved complexities in zeroth-order settings are also discussed.