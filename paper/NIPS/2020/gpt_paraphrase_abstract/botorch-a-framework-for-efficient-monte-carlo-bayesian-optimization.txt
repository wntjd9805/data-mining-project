BOTORCH is a modern programming framework for Bayesian optimization that is suitable for various applications such as automatic machine learning, engineering, physics, and experimental design. It incorporates Monte-Carlo acquisition functions, a new sample average approximation optimization method, auto-differentiation, and variance reduction techniques. The modular design of BOTORCH allows for flexible specification and optimization of probabilistic models written in PyTorch, making it easier to implement new acquisition functions. Our approach is supported by novel theoretical convergence results and is made practical through the use of fast predictive distributions, hardware acceleration, and deterministic optimization. Additionally, we introduce a unique "one-shot" formulation of the Knowledge Gradient, which is enabled by our theoretical and software contributions. Experimental results demonstrate that BOTORCH outperforms other popular libraries in terms of sample efficiency.