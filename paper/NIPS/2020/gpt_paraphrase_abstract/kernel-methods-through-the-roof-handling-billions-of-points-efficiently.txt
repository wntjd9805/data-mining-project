Kernel methods are a popular approach to nonparametric learning, but they have not been widely used for large scale problems due to poor scalability. Recent advancements have shown the benefits of combining optimization, numerical linear algebra, and random projections. In this study, we take these advancements further by developing and testing a solver that leverages GPU hardware. We designed a preconditioned gradient solver for kernel methods that utilizes GPU acceleration and parallelization with multiple GPUs. We also implemented out-of-core variants of common linear algebra operations to ensure optimal hardware utilization. Additionally, we optimized the numerical precision of different operations and maximized the efficiency of matrix-vector multiplications. Our experimental results demonstrate significant speedups on datasets with billions of points, while still maintaining state-of-the-art performance. Furthermore, we have made our software available as a user-friendly library.