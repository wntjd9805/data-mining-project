Recent advances in variational inference have contributed to a resurgence in the popularity of Bayesian neural networks. However, the commonly used fully factorized or mean-field distribution has limitations, particularly when dealing with posterior distributions that have highly correlated parameters. Existing algorithms that address this issue with a Gaussian approximating family are challenging to apply to large models due to computational costs and high variance of gradient updates. To overcome these challenges, we propose a new method that utilizes a reparametrization trick. This approach allows for computationally efficient variational inference with a Gaussian family that has a low-rank plus diagonal covariance structure. We successfully apply this method to deep feed-forward and convolutional architectures. Interestingly, we find that incorporating low-rank terms into the diagonal covariance does not enhance predictive performance, except for smaller networks. However, when added to a constant diagonal covariance, low-rank terms improve performance for networks of all sizes.