This study explores the presence of the mutual exclusivity (ME) bias in vanilla neural architectures and its implications for learning. The ME bias is used by children to differentiate how words are connected to objects, assuming that one object has only one label. The findings reveal that vanilla neural architectures do not possess this learning assumption, indicating a lack of ME bias. Additionally, the study shows that the inductive biases of these architectures do not align well with lifelong learning approaches to classification and translation. Consequently, there is a strong argument for developing task-general neural networks that incorporate mutual exclusivity as a learning mechanism, which remains an unresolved challenge.