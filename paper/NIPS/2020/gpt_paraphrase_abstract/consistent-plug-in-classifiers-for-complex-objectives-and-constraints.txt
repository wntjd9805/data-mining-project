We propose a unified algorithm for solving constrained classification problems by considering both the objective and constraints as functions of the confusion matrix. Our approach breaks down the problem into a series of classifier learning tasks, using an optimization method that combines achievable and feasible confusion matrices. By separating the constraint space, we can efficiently solve the problem using a Frank-Wolfe style optimization. Compared to a previous reduction-based algorithm, our method requires fewer calls to the plug-in subroutine for convex objective and constraint functions. Empirical results demonstrate that our algorithm performs competitively with existing methods and is more reliable when selecting hyperparameters.