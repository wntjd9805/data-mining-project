In recent years, there has been a focus on explaining the inner workings of black-box models, both at the individual instance level and the overall dataset level. However, less attention has been given to understanding these models at intermediate or group levels. This gap has been highlighted in studies on the challenges of implementing the General Data Protection Regulation (GDPR). To address this need, we propose a meta-method that can create a multilevel explanation tree using a local explainability method. The tree includes local explanations at the leaves, a global explanation at the root, and intermediate levels with explanations for groups of data points that are automatically clustered. Our method can also incorporate side information, allowing users to specify similarities in explanations for certain points. We argue that this multilevel structure can effectively communicate insights about the entire dataset by considering the appropriate level in the explanation tree. Additionally, we introduce a cost-efficient approach for obtaining explanations for new test points by associating them with the nearest training points. For generalized additive local explainability techniques like LIME and GAMs, we develop a fast approximate algorithm to build the multilevel tree and analyze its convergence behavior. Through experiments on public datasets and human studies involving both experts and non-expert users, we demonstrate the effectiveness of our technique in producing accurate and concise explanations.