We present new statistical guarantees for transfer learning through representation learning, where a shared feature representation is learned across multiple tasks. This allows for learning new tasks with less data compared to learning them individually. We consider a parameterized set of t + 1 tasks, each represented by functions of a certain form. We demonstrate that when training tasks are diverse, the sample complexity required to learn the shared representation for the first t tasks scales according to the complexity measure of the function class. Additionally, with an accurate estimate of the representation, the sample complexity needed to learn a new task scales only with the complexity measure. Our findings rely on a novel concept of task diversity that applies to models with different tasks, features, and losses, as well as a new chain rule for Gaussian complexities. We showcase the applicability of our framework in various important models discussed in the literature.