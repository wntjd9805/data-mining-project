This paper introduces the One-sample Guided Object Representation Disassembling (One-GORD) method, which aims to disassemble the features of objects and background in images. Existing methods either require a large amount of annotated samples or cannot handle complex backgrounds. One-GORD overcomes these limitations by only needing one annotated sample per object category to learn disassembled object representation. Data augmentation strategies are employed to generate synthetic samples from the annotated sample, guiding the disassembling process. For unannotated images, dual-swapping and fuzzy classification self-supervised mechanisms are introduced to disassemble object features from the background with the guidance of the annotated sample. Two metrics are devised to evaluate the disassembling performance in terms of representation and image quality. Experimental results demonstrate that One-GORD achieves competitive disassembling performance and can handle images with complicated backgrounds.