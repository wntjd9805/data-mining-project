We examine the sparsity of neurons in deep neural networks, motivated by the effectiveness of sparse representation in image restoration models. Our approach involves enforcing sparsity constraints on hidden neurons, which can be applied to convolution layers in different networks. By promoting sparsity in neurons, we can reduce computation by only considering non-zero elements while maintaining accuracy. Additionally, our method allows for increased representation dimensionality and model capacity without significant additional computational cost. Experimental results demonstrate the importance of sparse representation in various image restoration tasks such as super-resolution, denoising, and compression artifact removal. The code for our method is available at https://github.com/ychfan/nsr.