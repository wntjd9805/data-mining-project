We explore the problem of decentralized minimization in a network where smooth convex functions are stored across nodes. Recent research has established lower bounds for the minimum number of gradient computations and communication rounds needed to achieve a certain level of accuracy. In response, we present two new algorithms for this optimization problem, each with guaranteed complexity. Our first algorithm is optimal in terms of both communication rounds and gradient computations, and it does not require expensive evaluation of dual gradients like existing optimal algorithms. Our second algorithm is optimal in terms of communication rounds without a logarithmic factor. Both algorithms are based on an accelerated variant of the Forward Backward algorithm for solving monotone inclusions related to decentralized optimization. We validate the effectiveness of our methods through numerical experiments, comparing them to state-of-the-art algorithms.