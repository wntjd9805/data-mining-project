This paper presents a novel method called iterative hierarchical data augmentation (IHDA) that aims to enhance the generalization performance of deep neural networks (DNs). The motivation behind IHDA is based on three key insights: (1) DNs excel at learning multi-level representations from data, (2) augmenting data in the feature spaces learned by DNs can greatly enhance their performance, and (3) applying data augmentation in challenging regions of the feature space effectively improves generalization.  IHDA performs data augmentation in a deep feature space at a specific level (l) by transforming it into a distribution space and generating new samples using learned distributions for data points located in difficult-to-classify regions. These regions are identified by analyzing the neighborhood characteristics of each data point. The newly synthesized samples are then used to fine-tune the parameters of subsequent layers. This process is repeated for the feature space at level (l + 1). To prevent overfitting, IHDA incorporates the concept of dropout probability, gradually relaxing it as the method progresses towards higher-level feature spaces.  IHDA achieved state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet datasets for various DNs. It outperformed existing data augmentation approaches for the same networks on these datasets. Additionally, IHDA's domain-agnostic properties were demonstrated by showcasing its significant improvements in a non-image wearable sensor-based activity recognition benchmark for a deep neural network.