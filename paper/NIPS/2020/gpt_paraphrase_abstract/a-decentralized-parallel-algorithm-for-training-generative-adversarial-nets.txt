Generative Adversarial Networks (GANs) are powerful generative models in deep learning. Current large-scale GAN training relies on large models and distributed large-batch training strategies, implemented on centralized deep learning frameworks like TensorFlow and PyTorch. However, performance is significantly impacted when network bandwidth is low or latency is high. Decentralized algorithms for training deep neural networks have been explored, but it is unclear if GANs can be trained in a decentralized manner due to the challenges of nonconvex-nonconcave optimization and decentralized communication. In this paper, we propose a gradient-based decentralized parallel algorithm for GAN training that allows workers to communicate multiple times in one iteration and update the discriminator and generator simultaneously. Our algorithm can solve non-convex non-concave min-max problems and converge to a first-order stationary point. Experimental results demonstrate the effectiveness of our decentralized algorithm in training GANs.