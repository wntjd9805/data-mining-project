Self-supervised denoising frameworks that learn denoising models using individual noisy images have shown promising results. However, current frameworks are based on a theoretical foundation that requires the denoising models to be invariant, which may limit their performance. In this study, we propose Noise2Same, a new self-supervised denoising framework. Noise2Same introduces a new self-supervised loss by deriving an upper bound of the typical supervised loss. Unlike existing frameworks, Noise2Same does not require invariance or additional information about the noise model, making it applicable to a wider range of denoising tasks. The proposed framework is analyzed both theoretically and experimentally, and the results demonstrate that Noise2Same outperforms previous self-supervised denoising methods in terms of denoising performance and training efficiency.