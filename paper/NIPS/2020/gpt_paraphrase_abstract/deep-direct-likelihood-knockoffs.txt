Black box machine learning methods like deep neural networks are commonly used in predictive modeling to achieve high performance. However, in scientific domains, it is important to identify the important features that contribute to the predictions. This can lead to costly experiments, so it is crucial to minimize the error rate in these discoveries. Model-X knockoffs provide a solution by allowing the discovery of important features while controlling the false discovery rate. However, knockoffs require accurate generative models that adhere to the "swap" property. To address this, we introduce Deep Direct Likelihood Knockoffs (DDLK), which directly minimizes the KL divergence implied by the swap property. DDLK consists of two stages: maximizing the explicit likelihood of the features and minimizing the KL divergence between the joint distribution of features and knockoffs under any swap. To ensure the validity of the generated knockoffs, DDLK utilizes the Gumbel-Softmax trick to optimize the knockoff generator under the worst-case swap. Our experiments demonstrate that DDLK outperforms baseline methods in terms of power while effectively controlling the false discovery rate on various synthetic and real benchmarks, including a COVID-19 dataset.