Leverage score sampling, a technique derived from theoretical computer science, has proven to be a valuable tool in accelerating various fundamental questions such as linear regression, linear programming, and graph sparsification. Recent studies have also demonstrated its effectiveness in speeding up kernel methods. Building upon these findings, this research aims to extend the application of leverage score sampling to a wider range of kernels. Additionally, the study explores the connection between neural network initialization and approximating the neural tangent kernel using random features. The research also establishes the equivalence between regularized neural networks and neural tangent kernel ridge regression, specifically under the initialization of both classical random Gaussian and leverage score sampling techniques. Overall, this work contributes to the field of deep learning theory by integrating leverage score sampling into the study of neural networks.