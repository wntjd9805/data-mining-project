We introduce Probabilistic Regression (PR) trees, a generalized version of regression trees that can adapt to the smoothness of the prediction function, maintain interpretability, and handle noise. PR trees associate observations with regions through probability distributions, reflecting their proximity to each region. We demonstrate that PR trees are consistent, with their error approaching zero as the sample size increases, which is not the case for similar approaches like Soft trees and Smooth Transition Regression trees. We also discuss how PR trees can be incorporated into ensemble methods such as Random Forests and Gradient Boosted Trees. Through extensive experiments, we evaluate the performance of PR trees and highlight their advantages in terms of performance, interpretability, and robustness to noise.