This study investigates the ability of deep neural networks to approximate probability distributions. By considering a target distribution π and a source distribution pz both defined on Rd, the paper establishes that a deep neural network g : Rd→R with ReLU activation can be found, under certain assumptions, such that the push-forward measure (∇g)#pz of pz under the map ∇g can closely resemble the target measure π. The closeness is measured using three integral probability metrics: 1-Wasserstein distance, maximum mean distance (MMD), and kernelized Stein discrepancy (KSD). The paper provides upper bounds for the size (width and depth) of the deep neural network based on the dimension d and the approximation error ε with respect to the three discrepancies. Notably, when using 1-Wasserstein distance, the size of the neural network can exponentially grow in d, whereas for both MMD and KSD, the size of the neural network depends on d only polynomially. The proof relies on convergence estimates of empirical measures under the mentioned discrepancies and semi-discrete optimal transport.