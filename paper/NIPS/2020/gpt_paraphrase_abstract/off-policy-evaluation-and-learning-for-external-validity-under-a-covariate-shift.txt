The abstract discusses the concept of off-policy evaluation (OPE) and off-policy learning (OPL) in evaluating and training a new policy using historical data obtained from a different policy. While standard OPE and OPL methods assume the same distribution of covariate between historical and evaluation data, real-world applications often exhibit covariate shift where the distribution of covariate in historical data differs from that in evaluation data. The paper presents the efficiency bound of an OPE estimator under covariate shift and proposes doubly robust and efficient estimators for OPE and OPL by utilizing a nonparametric estimator of the density ratio between historical and evaluation data distributions. It also explores other potential estimators and compares their theoretical properties. Experimental results are provided to demonstrate the effectiveness of the proposed estimators.