The Spatial Description Resolution task involves locating a target in a panoramic street view using language descriptions. However, current methods lack the ability to explicitly characterize object-level relationships and distill spatial relationships, which are crucial for this task. To address this, we propose a novel approach called the spatial relationship induced (SIRI) network. Inspired by human navigation, our network sequentially processes spatial relationship words and objects from a first-person viewpoint to locate the target. We correlate visual features at an implicit object-level and distill them using each spatial relationship word, resulting in differently activated features representing each spatial relationship. To overcome the absence of positional information, we introduce global position priors for better positional reasoning. The linguistic and visual features are then combined to determine the target location. Experimental results on the Touchdown dataset demonstrate that our method outperforms the state-of-the-art by approximately 24% in terms of accuracy within an 80-pixel radius. Our method also generalizes well on an extended dataset. The code for our project is publicly available at the provided URL.