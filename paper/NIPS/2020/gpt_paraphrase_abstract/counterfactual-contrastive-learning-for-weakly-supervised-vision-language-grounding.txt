This study focuses on weakly-supervised vision-language grounding, which involves localizing a specific moment in a video or region in an image based on a given sentence query. Existing approaches for this task rely on either MIL-based or reconstruction-based paradigms, but these have limitations in terms of negative sample quality and direct optimization of visual-textual alignment score. To address these issues, the researchers propose a new method called Counterfactual Contrastive Learning (CCL). CCL involves training with contrast between counterfactual positive and negative results, achieved through robust and destructive counterfactual transformations. These transformations are applied at the feature, interaction, and relation levels, damaging visual features, confusing vision-language interaction, and destroying context clues in proposal relationships, respectively. The effectiveness of CCL is demonstrated through extensive experiments on five vision-language grounding datasets.