Recent advancements in speech synthesis have shown significant progress in the field of generative modeling. Traditional concatenative systems have been surpassed by likelihood-based autoregressive neural models. However, a drawback of these autoregressive models is their reliance on sequential operations, making them unsuitable for specialized deep learning hardware. To address this issue, we propose a novel learning method that enables the training of highly parallel speech models without the need for an analytical likelihood function. Our approach is based on a spectral energy distance, which measures the disparity between the distributions of generated and real audio. This distance is a reliable scoring rule that ensures statistical consistency when considering the magnitude-spectrograms of the generated waveform audio. It can be calculated from minibatches without bias and does not involve adversarial learning, resulting in a stable and consistent training method for implicit generative models. Through empirical evaluation, we demonstrate that our method achieves state-of-the-art generation quality compared to other implicit generative models, as assessed by the cFDSD metric. Additionally, when combined with adversarial techniques, our approach outperforms the GAN-TTS model in terms of Mean Opinion Score, as determined by trained human evaluators.