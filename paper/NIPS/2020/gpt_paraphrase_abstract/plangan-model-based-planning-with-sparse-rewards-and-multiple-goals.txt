Sparse rewards in reinforcement learning pose a significant challenge, particularly when training a policy to achieve multiple goals. Previous successful approaches have used model-free RL algorithms. This study introduces PlanGAN, a model-based algorithm designed for solving multi-goal tasks in sparse reward environments. The method utilizes the information from trajectories collected by the agent to train an ensemble of conditional generative models (GANs). These models generate plausible trajectories leading the agent towards specified goals. These imagined trajectories are then combined into a planning algorithm to efficiently achieve the desired goals. PlanGAN's performance was compared to various model-free RL baselines, including Hindsight Experience Replay, in robotic navigation/manipulation tasks. Results show that PlanGAN achieves comparable performance with 4-8 times higher sample efficiency.