This study examines the impact of adversarial perturbations on monocular depth prediction. We investigate the ability of small, imperceptible additive perturbations to selectively modify the perceived geometry of a scene. Our findings demonstrate that these perturbations can not only globally rescale the predicted distances from the camera, but also manipulate the prediction to match a different target scene. Moreover, we show that when provided with semantic or instance information, perturbations can deceive the network into altering the depth of specific categories or instances in the scene, and even remove them while preserving the rest of the scene. To comprehend the effects of targeted perturbations, we conduct experiments using state-of-the-art monocular depth prediction methods. These experiments uncover vulnerabilities in monocular depth prediction networks and provide insights into the biases and contextual understanding acquired by these networks.