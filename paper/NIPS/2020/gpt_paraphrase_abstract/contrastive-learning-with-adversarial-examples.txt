Contrastive learning (CL) is a widely used technique in self-supervised learning (SSL) for visual representations. It involves using pairs of augmented unlabeled training examples to create a classification task for pretext learning in deep embedding. However, previous studies have not addressed the selection of challenging negative pairs, as they treat images within a batch independently. This paper introduces a new approach called CLAE, which utilizes a new family of adversarial examples to address this problem. By incorporating adversarial training, CLAE generates more challenging positive pairs and harder negative pairs by considering all images in a batch during optimization. CLAE is compatible with various CL methods and has been shown to enhance the performance of existing CL baselines on multiple datasets.