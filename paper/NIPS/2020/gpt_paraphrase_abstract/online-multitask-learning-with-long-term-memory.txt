We present a new online multitask setting where tasks are divided into unknown segments, each associated with a hypothesis. Our algorithms are designed to take advantage of scenarios with many segments but fewer associated hypotheses. We prove that our algorithms guarantee regret bounds regardless of task segmentation or hypothesis association. In the single-task setting, our approach is equivalent to switching with long-term memory. For finite hypothesis classes, we propose an algorithm with linear prediction time. For infinite hypothesis classes, specifically reproducing kernel Hilbert spaces, we provide an algorithm with cubic per trial time complexity. This is the first efficient regret-bounded switching algorithm with long-term memory for a non-parametric hypothesis class in the single-task special case.