Self-supervised learning has gained significant attention for its success in various applications. In this study, we propose 3D versions of five self-supervised methods as proxy tasks to facilitate feature learning from unlabeled 3D images. Our aim is to reduce the need for expert annotation. The five methods we develop are 3D Contrastive Predictive Coding, 3D Rotation Prediction, 3D Jigsaw Puzzles, Relative 3D Patch Location, and 3D Exemplar Networks. Our experiments demonstrate that pretraining models with these 3D tasks result in more powerful semantic representations, leading to improved accuracy and efficiency in solving downstream tasks compared to training from scratch or pretraining on 2D slices. We validate the effectiveness of our methods on three medical imaging tasks: Brain Tumor Segmentation, Pancreas Tumor Segmentation, and Diabetic Retinopathy Detection. Our methods show gains in data efficiency, performance, and convergence speed. Additionally, we find that transferring the learned representations from a large unlabeled 3D dataset to a small specific dataset yields competitive results compared to state-of-the-art solutions, while significantly reducing computational expenses. To promote further research, we provide open-source implementations of our developed algorithms for both 3D and 2D versions.