Learning to plan for long horizons is a key challenge in episodic reinforcement learning. The difficulty of the problem as the horizon increases is a fundamental question. The sample complexity, measured by the number of episodes needed to discover a policy with a value close to the optimal value, is the natural measure. In a 2018 open problem, it was conjectured that tabular, episodic reinforcement learning has a lower bound on sample complexity that scales polynomially with the horizon. This work disproves the conjecture, showing that such learning can be achieved with a sample complexity that scales logarithmically with the horizon. The analysis introduces two ideas: the construction of a "net for near-optimal policies with a logarithmic scaling log-covering number, and the Online Trajectory Synthesis algorithm that evaluates policies in a given policy class with logarithmic sample complexity. Both ideas may have independent interest.