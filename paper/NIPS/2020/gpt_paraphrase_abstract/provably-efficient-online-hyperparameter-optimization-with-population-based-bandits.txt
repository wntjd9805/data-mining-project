Tuning hyperparameters is crucial in machine learning, especially in reinforcement learning where small changes can lead to failure. However, the current methods for hyperparameter tuning are expensive and inefficient. Population-Based Training (PBT) is a recent solution that updates both weights and hyperparameters in a single training run. Although PBT has been widely used in RL, it lacks theoretical guarantees and requires substantial computational resources. To address these limitations, we propose Population-Based Bandits (PB2), the first provably efficient PBT-style algorithm. PB2 utilizes a probabilistic model to guide the search, allowing for the discovery of high-performing hyperparameter configurations with fewer agents than PBT. We demonstrate through RL experiments that PB2 achieves high performance with a modest computational budget.