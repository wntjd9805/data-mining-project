The growing interest in developing powerful and versatile Reinforcement Learning (RL) agents has led to a focus on the single-agent setting, where agents aim to maximize a specific reward function. However, a crucial question arises when considering how these independent agents can effectively cooperate in a shared multi-agent environment. To address this, we propose equipping each RL agent with the ability to directly provide rewards to other agents using a learned incentive function. Each agent learns its own incentive function by considering its impact on the learning of recipients and, consequently, its own extrinsic objective. Our experiments demonstrate that these agents surpass standard RL and opponent-shaping agents in challenging multi-agent games, often achieving near-optimal division of labor. This research highlights the potential and difficulties of promoting the common good in a multi-agent future.