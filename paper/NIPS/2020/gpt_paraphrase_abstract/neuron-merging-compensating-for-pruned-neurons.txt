Network pruning is a common technique used to reduce the size and improve the speed of neural network models. However, structured network pruning often leads to a loss in accuracy because entire neurons or filters are discarded. In this study, we introduce a new concept called neuron merging that can be applied to both fully connected and convolution layers. Neuron merging involves decomposing the original weights into two matrices or tensors. One matrix becomes the new weights for the current layer, while the other matrix, called the scaling matrix, guides the combination of neurons. If the activation function is ReLU, the scaling matrix can be absorbed into the next layer, compensating for the removed neurons. We also propose a data-free and cost-effective method for decomposing the weights using cosine similarity between neurons. Compared to models that have undergone traditional pruning, our merged models better preserve the output feature map of the original model, thus maintaining accuracy without the need for fine-tuning. We demonstrate the effectiveness of our approach on various model architectures and datasets. For instance, using VGG-16 on CIFAR-10, we achieve an accuracy of 93.16% while reducing 64% of total parameters, without any fine-tuning. The code for our approach can be found at the following link: https://github.com/friendshipkim/neuron-merging.