We present a practical and efficient algorithm for predicting outcomes in linear dynamical systems (LDS) with unknown parameters and partial observations, considering stochastic noise. The Kalman filter is the optimal predictor when system parameters are known. However, existing predictive models perform poorly in unknown systems that are marginally stable and have long-term forecast memory. To address this issue, we bound the generalized Kolmogorov width of the Kalman filter coefficient set. This motivates the development of an algorithm called spectral LDS improper predictor (SLIP), which relaxes the Kalman predictive model through spectral methods. We provide a finite-sample analysis demonstrating that SLIP competes with the Kalman filter with only logarithmic regret. Our analysis relies on Mendelson's small-ball method, which yields sharp error bounds without concentration, boundedness, or exponential forgetting assumptions. Empirical evaluations show that SLIP outperforms state-of-the-art methods in LDS prediction. Our theoretical and experimental findings provide insights into the conditions necessary for efficient probably approximately correct (PAC) learning of the Kalman filter from partially observed data.