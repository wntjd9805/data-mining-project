The global context is essential for understanding 3D point cloud scenes. This study extends the contextual encoding layer, originally designed for 2D tasks, to 3D point cloud scenarios. The encoding layer learns code words in the feature space of the point cloud to capture global semantic context. Based on these code words, a global contextual descriptor is learned to adjust the feature maps. However, in 3D scenarios, data sparsity is a significant issue, and the performance of contextual encoding saturates as the number of code words increases. To address this problem, a group contextual encoding method is proposed, which divides the channel into groups and performs encoding on group-divided feature vectors. This facilitates learning global context in grouped subspaces for 3D point clouds. The method is evaluated on three well-known 3D point cloud tasks, demonstrating superior performance compared to existing methods. Remarkably, the proposed method outperforms VoteNet with a 3 mAP improvement on the SUN-RGBD benchmark and a 6.57 mAP improvement on ScanNet. Compared to the baseline PointNet++, the proposed method achieves 86% accuracy, surpassing the baseline by 1.5%.