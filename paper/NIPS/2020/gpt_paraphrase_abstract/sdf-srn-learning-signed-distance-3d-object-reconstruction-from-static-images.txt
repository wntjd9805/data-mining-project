Recent advancements in dense 3D object reconstruction from a single image have been remarkable. However, the process of creating paired image-shape datasets for supervising neural networks with ground-truth 3D shapes is laborious and impractical. To overcome this limitation, researchers have started exploring the use of annotated 2D silhouettes from RGB images to learn 3D reconstruction without 3D supervision. Although this approach reduces annotation efforts, it still requires multi-view annotations of the same object instance during training, making it impractical for real-world scenarios. This paper presents SDF-SRN, an approach that addresses this issue by only requiring a single view of objects at training time, making it more practical for real-world scenarios. SDF-SRN utilizes implicit 3D shape representations to handle arbitrary shape topologies present in the datasets. The authors introduce a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Experimental results demonstrate that our method outperforms existing techniques under challenging single-view supervision settings, both on synthetic and real-world datasets.