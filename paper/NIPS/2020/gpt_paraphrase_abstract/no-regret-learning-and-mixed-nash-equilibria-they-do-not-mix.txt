Understanding the behavior of no-regret dynamics in N-player games is a fundamental question in online learning and game theory. It is commonly believed that in finite games, the empirical frequency of play under no-regret learning converges to the game's set of coarse correlated equilibria. However, our understanding of how the day-to-day behavior of the dynamics relates to the game's Nash equilibria is limited, with only partial results for certain game classes. This study focuses on follow the regularized leader (FTRL), a well-studied class of no-regret dynamics, and presents a negative result. It shows that the concept of mixed Nash equilibrium is incompatible with no-regret learning. Specifically, any Nash equilibrium that is not strict cannot be stable and attracting under FTRL dynamics. This finding has significant implications for predicting the outcome of a learning process, as it demonstrates that only strict (pure) Nash equilibria can emerge as stable limit points.