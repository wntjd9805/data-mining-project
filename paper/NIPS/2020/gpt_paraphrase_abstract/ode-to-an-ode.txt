We introduce a novel approach called ODEtoODE for Neural ODE algorithms. In this approach, the time-dependent parameters of the main flow evolve based on a matrix flow on the orthogonal group O(d). This nested system of two flows, with the parameter flow constrained to a compact manifold, ensures stability and effectiveness in training. It also effectively addresses the problem of gradient vanishing or explosion that is commonly encountered in training deep neural network architectures like Neural ODEs. Our approach yields improved downstream models, as demonstrated in the training of reinforcement learning policies with evolution strategies and in supervised learning compared to previous state-of-the-art baselines. We also provide strong convergence results for our proposed mechanism, which hold irrespective of the network depth, validating our empirical findings. Our results establish an intriguing connection between the theory of deep neural networks and matrix flows on compact manifolds.