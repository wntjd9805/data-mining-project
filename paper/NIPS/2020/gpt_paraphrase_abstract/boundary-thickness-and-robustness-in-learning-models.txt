This paper introduces the concept of boundary thickness in machine learning classifiers and explores its relationship with model robustness. It is found that thicker decision boundaries improve performance and robustness, while thin boundaries lead to overfitting and lower robustness. The study demonstrates that thicker boundaries enhance robustness against adversarial examples and out-of-distribution transforms. Moreover, various regularization and data augmentation techniques are shown to increase boundary thickness. The paper establishes a connection between maximizing boundary thickness during training and mixup training. Additionally, it is shown that noise-augmentation in mixup training further increases boundary thickness, effectively countering vulnerability to adversarial attacks and out-of-distribution transforms. The study also reveals that recent advancements in performance are accompanied by thicker boundaries.