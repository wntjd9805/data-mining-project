Many algorithms have been developed to search for optimal channel numbers in convolutional neural networks (CNNs) based on recent advancements in neural architecture search. These algorithms often utilize parameter sharing to improve efficiency, allowing the reuse of parameters across different channel configurations. However, the impact of parameter sharing on the search process is not well understood. This paper aims to provide a comprehensive understanding and exploitation of parameter sharing in channel number search (CNS) algorithms. The authors propose affine parameter sharing (APS) as a general formulation to quantitatively analyze existing CNS algorithms. They find that parameter sharing allows weight updates from one architecture to benefit other candidates but also leads to less confidence in selecting good architectures. To address this issue, a new parameter sharing strategy is proposed to strike a better balance between training efficiency and architecture discrimination. Extensive analysis and experiments show that the proposed strategy outperforms state-of-the-art counterparts on benchmark datasets in terms of channel configuration.