This paper explores the issue of escaping saddle points and achieving second-order optimality in a decentralized setting where a group of agents collaborate to minimize their aggregate objective function. The authors present a non-asymptotic analysis, demonstrating that by employing perturbed gradient descent, it is possible to converge to a second-order stationary point in a number of iterations that is linearly dependent on dimension and polynomially dependent on the accuracy of the second-order stationary point. However, achieving this in a communication-efficient manner requires overcoming various challenges, such as identifying first-order stationary points in a distributed manner and adapting the perturbed gradient framework without excessive communication complexity. The proposed Perturbed Decentralized Gradient Tracking (PDGT) method consists of two main stages: a gradient-based step to find a first-order stationary point, and a perturbed gradient descent step to escape from a first-order stationary point if it is a saddle point with sufficient curvature. A notable benefit of the PDGT method is that, in the case where all saddle points are non-degenerate, it can find a local minimum of the decentralized optimization problem within a finite number of iterations.