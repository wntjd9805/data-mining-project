Gaussian processes (GPs) are commonly used to model functions, but their success relies on accurately choosing the kernel hyperparameters. Typically, the marginal likelihood of the hyperparameters needs to be evaluated, which can be computationally expensive. To address this issue, we propose an alternative approach that avoids the need for costly marginal likelihoods. Our method involves training a single neural network to estimate the kernel function based on regression data, which can be applied to various tasks. We utilize a hierarchical self-attention-based neural network that produces hyperparameter estimates that are invariant to the order of input data points and dimensions. By training this neural model on synthetic data, we found that it can generalize well to real-world GP applications. Our experiments demonstrate that the estimated hyperparameters are of comparable quality to those obtained through conventional model selection procedures, but with significantly faster computation times. This approach can greatly accelerate GP regression, Bayesian optimization, and Bayesian quadrature. The code and pre-trained model can be accessed at https://github.com/PrincetonLIPS/AHGP.