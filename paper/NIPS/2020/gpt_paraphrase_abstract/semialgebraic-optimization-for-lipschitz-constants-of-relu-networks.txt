We present a new method to estimate the global and local Lipschitz constant of a deep neural network using a semidefinite programming hierarchy. This method combines a polynomial lifting for ReLU function derivatives with a modified version of Putinar's positivity certificate. The approach can also be applied to other polynomial optimization problems in machine learning. Empirical results show that our method offers a trade-off compared to the current linear programming approach, providing better bounds in less time in certain cases.