Sampling-based methods in training Graph Convolutional Networks (GCNs) with stochastic gradient descent offer scalability improvements. However, these methods can lead to computational overheads in preprocessing and loading new samples, which negatively impact sampling performance. To address this, we propose LazyGCN, a framework that decouples the frequency of sampling from the sampling strategy. LazyGCN performs sampling periodically and recycles sampled nodes to mitigate data preparation overhead. The algorithm is theoretically analyzed and shown to achieve the same convergence rate as the underlying sampling method. Empirical evidence on large real-world graphs supports the effectiveness of LazyGCN in reducing the number of sampling steps and improving speed without sacrificing accuracy.