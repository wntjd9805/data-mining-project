Stochastic Gradient Descent (SGD) is widely used for optimizing deep neural networks with a large number of parameters. However, there is still a need to improve the efficiency of large-scale optimization. Recent research has shown that deep neural networks can be optimized in smaller randomly-projected subspaces. Although this approach is promising for more efficient optimization, it currently has limitations in terms of performance. In this study, we propose improvements to existing random subspace methods. Firstly, we find that keeping the random projection fixed throughout training is not beneficial and suggest redrawing the subspace at each step, which leads to significantly better performance. Additionally, we enhance the approximation by applying independent projections to different parts of the network, particularly as the network dimensionality increases. To conduct these experiments, we utilize hardware-accelerated pseudo-random number generation to generate the random projections on-demand, enabling distributed computation across multiple workers. This approach reduces memory requirements and is up to 10 times faster for the specific workloads studied.