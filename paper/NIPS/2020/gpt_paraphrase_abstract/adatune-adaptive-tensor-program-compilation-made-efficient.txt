Deep learning models are highly demanding in terms of computation, requiring optimization by experts or hardware vendors to be practical. Although the DL compiler and Learning-to-Compile have proven effective for optimizing tensor programs, they suffer from excessively long optimization times. To address this issue, we propose a new method called AdaTune, which significantly reduces the optimization time for high-performance deep learning inference. AdaTune introduces an adaptive evaluation method that terminates costly hardware measurements early without sacrificing accuracy. Additionally, we develop a surrogate model with uncertainty quantification to better adapt the optimization process to hardware and model variations. Moreover, we introduce a contextual optimizer that enhances the effectiveness of searching the transformation space by providing adaptive control of exploration and exploitation. By comparing AdaTune with AutoTVM, a state-of-the-art Learning-to-Compile technique, we observe that AdaTune achieves up to 115% higher GFLOPS than the baseline within the same optimization time budget. Furthermore, AdaTune achieves a 1.3–3.9× speedup in optimization time while maintaining the same optimization quality as the baseline across various models and hardware architectures.