This study focuses on domain adaptive object re-identification (re-ID), which involves transferring knowledge from a labeled source domain to an unlabeled target domain. While previous pseudo-label-based methods have achieved success, they have not fully utilized valuable information due to domain gaps and inadequate clustering performance. To address these issues, we propose a novel self-paced contrastive learning framework with hybrid memory. This framework generates supervisory signals at different levels (source-domain class, target-domain cluster, un-clustered instance) to learn feature representations. Unlike conventional contrastive learning strategies, our framework simultaneously distinguishes source-domain classes, target-domain clusters, and un-clustered instances. Importantly, our self-paced method progressively creates more reliable clusters to refine the hybrid memory and learning targets, leading to outstanding performance. Our method outperforms existing approaches in multiple domain adaptation tasks and even improves performance on the source domain without additional annotations. In unsupervised object re-ID, our generalized version achieves significant improvements of 16.7% and 7.9% on the Market-1501 and MSMT17 benchmarks respectively.