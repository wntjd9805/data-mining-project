Pruning deep neural networks is of great interest as it can save time, memory, and energy during training and testing. Previous studies have found that there are highly sparse trainable subnetworks, or "winning lottery tickets," at initialization. This raises the question of whether we can identify these subnetworks without training or looking at the data. Through theory-driven algorithm design, we provide an affirmative answer to this question. We explain the problem of layer-collapse, where an entire layer is pruned prematurely, making the network untrainable. We propose a novel pruning algorithm called Iterative Synaptic Flow Pruning (SynFlow) that avoids layer-collapse by preserving the total flow of synaptic strengths through the network at initialization. Remarkably, SynFlow performs competitively or better than existing pruning algorithms at initialization across various models, datasets, and sparsity constraints, without using any training data. This challenges the existing belief that data is necessary to determine the importance of synapses at initialization.