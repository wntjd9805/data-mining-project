We examine non-stationary stochastic optimization using squared error losses and noisy gradient feedback. We focus on the dynamic regret of an online learner compared to a changing comparator sequence. Inspired by non-parametric regression theory, we introduce a new variational constraint that limits the comparator sequence to a discrete kth order Total Variation ball with a radius of Cn. This constraint represents comparators with piecewise polynomial structure, which has practical applications. By connecting to wavelet-based non-parametric regression theory, we develop a polynomial time algorithm that achieves nearly optimal dynamic regret of ËœO(n). Our proposed policy is adaptable to the unknown radius Cn and is also minimax optimal for various other non-parametric families.