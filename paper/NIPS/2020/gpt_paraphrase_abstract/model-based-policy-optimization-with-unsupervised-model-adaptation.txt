Model-based reinforcement learning methods use real data from the environment to learn a dynamics model, which is then used to generate simulated data for agent training. However, there is often a mismatch between the distributions of the simulated and real data, leading to decreased performance. Existing methods have not effectively addressed this issue. This paper aims to bridge the gap between real and simulated data by improving model estimation for better policy optimization. The authors propose a novel framework called AMPO, which incorporates unsupervised model adaptation to minimize the distribution mismatch between real and simulated data. By using the integral probability metric (IPM) and specifically the Wasserstein-1 distance, the authors present a practical model-based approach. Experimental results demonstrate that their approach achieves superior performance in terms of sample efficiency on various continuous control benchmark tasks.