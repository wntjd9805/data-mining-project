We present and examine algorithms for the distributionally robust optimization of convex losses using conditional value at risk (CVaR) and χ2 divergence uncertainty sets. We demonstrate that our algorithms do not depend on the size of the training set or the number of parameters, making them suitable for large-scale applications. Our guarantees for χ2 uncertainty sets are the first of their kind in the literature, while our guarantees for CVaR scale linearly in the uncertainty level, unlike previous work which had quadratic scaling. We also provide lower bounds that prove the worst-case optimality of our algorithms for CVaR and a penalized version of the χ2 problem. Our main technical contributions include new bounds on the bias of batch robust risk estimation and the variance of a multilevel MonteCarlo gradient estimator. Experimental results on MNIST and ImageNet datasets confirm the theoretical scaling of our algorithms, which are 9-36 times more efficient than full-batch methods.