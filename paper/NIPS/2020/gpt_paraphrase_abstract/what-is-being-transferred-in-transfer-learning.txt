Transfer learning is a desired capability for machines, allowing them to apply knowledge gained from one domain to another with limited data. Despite its widespread use in deep learning, we still lack a comprehensive understanding of what factors contribute to successful transfer and which parts of the network are responsible for it. This paper introduces new tools and analyses to address these questions. By examining the transfer to block-shuffled images, we distinguish the impact of reusing features from learning the low-level statistics of the data. Our findings reveal that transfer learning benefits from both aspects. Additionally, we demonstrate that training with pre-trained weights leads the model to remain within the same basin in the loss landscape, resulting in similar feature space and proximity in parameter space among different instances of the model.