Pruning is a highly effective technique for compressing and speeding up modern neural networks. There are two main types of pruning methods: filter pruning (FP) and weight pruning (WP). FP is compatible with hardware but does not achieve as high compression ratios as WP. To combine the strengths of both methods, we propose a new approach called Stripe-Wise Pruning (SWP), where we treat filters as stripes and prune them individually instead of the whole filter. This allows for finer granularity while still being hardware-friendly. SWP involves using a learnable matrix called Filter Skeleton to represent the shape of each filter. We argue that the architecture of a single filter, including its shape, is important. Through extensive experiments, we demonstrate that SWP outperforms previous FP-based methods and achieves state-of-the-art compression ratios on CIFAR-10 and ImageNet datasets without significant loss in accuracy. The code for SWP is available at a specific URL.