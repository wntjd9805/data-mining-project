Meta-Reinforcement Learning (meta-RL) involves training an agent on a set of tasks to improve its learning speed and performance on new, unseen tasks. Typically, the training tasks are carefully chosen to represent the expected distribution of test tasks. However, we demonstrate that selecting the appropriate training tasks can lead to even faster and more effective learning. To achieve this, we propose an algorithm called Information-Theoretic Task Selection (ITTS) that optimizes the task set used for meta-RL training, regardless of how the tasks are generated. ITTS identifies training tasks that are sufficiently relevant to the test tasks and sufficiently distinct from each other. Through various meta-RL experiments, we validate that ITTS enhances the final performance in all cases.