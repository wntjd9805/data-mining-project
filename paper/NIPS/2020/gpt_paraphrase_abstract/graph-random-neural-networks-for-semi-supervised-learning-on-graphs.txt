We address the limitations of existing graph neural networks (GNNs) in semi-supervised learning on graphs, such as over-smoothing, non-robustness, and weak generalization when labeled nodes are scarce. To overcome these issues, we propose a framework called GRAPH RANDOM NEURAL NETWORKS (GRAND). GRAND incorporates a random propagation strategy for graph data augmentation and utilizes consistency regularization to optimize the prediction consistency of unlabeled nodes across different data augmentations. Our experiments on graph benchmark datasets demonstrate that GRAND outperforms state-of-the-art GNN baselines in semi-supervised node classification. Furthermore, GRAND mitigates over-smoothing and non-robustness problems and exhibits better generalization behavior compared to existing GNNs. The source code of GRAND is publicly available at https://github.com/Grand20/grand.