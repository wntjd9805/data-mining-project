This study introduces the Cooperative Heterogeneous Deep Reinforcement Learning (CHDRL) framework, which combines the strengths of different deep reinforcement learning agents. The framework categorizes agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from other agents, while local agents can effectively explore the local area and can be either on-policy agents or population-based evolutionary algorithms (EAs) agents. The framework uses global agents to guide the learning of local agents, allowing them to benefit from sample-efficient agents while maintaining their own advantages, such as stability. The experimental results on various continuous control tasks demonstrate that CHDRL outperforms state-of-the-art baselines.