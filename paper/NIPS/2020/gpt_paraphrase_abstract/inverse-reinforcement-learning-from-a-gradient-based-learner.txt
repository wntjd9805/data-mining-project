This study introduces a new algorithm for Inverse Reinforcement Learning (IRL) that focuses on recovering the reward function being optimized by an agent. The algorithm utilizes the expert's near-optimal behavior and observed learning process to infer the reward function. The approach is based on the assumption that the agent updates its policy parameters along the gradient direction. The algorithm is then extended to handle situations where only a dataset of learning trajectories is available. The performance of the algorithms is analyzed theoretically, and the approach is evaluated using simulated GridWorld and MuJoCo environments, comparing it against a state-of-the-art baseline.