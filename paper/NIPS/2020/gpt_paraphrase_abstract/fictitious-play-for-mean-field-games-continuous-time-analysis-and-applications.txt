This paper explores the continuous time Fictitious Play learning algorithm in various finite state Mean Field Game settings. It introduces an additional common noise and analyzes the convergence of the Fictitious Play process. The analysis shows that exploitability decreases at a rate of O(1/t), highlighting its relevance in evaluating convergence to a Nash equilibrium in Mean Field Games. Numerical experiments in both model-based and model-free settings support these theoretical findings. This study presents the first converging learning dynamics for Mean Field Games with common noise.