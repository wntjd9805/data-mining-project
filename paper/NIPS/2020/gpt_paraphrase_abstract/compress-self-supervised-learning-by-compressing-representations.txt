Self-supervised learning, which aims to learn useful representations from unlabeled data, has shown that larger models benefit more from this approach compared to smaller models. This has narrowed the gap between supervised and self-supervised learning for larger models. Instead of creating a new task for self-supervised learning, we propose a model compression technique to compress a deep self-supervised model (teacher) into a smaller one (student). The student model is trained to mimic the relative similarity between data points in the teacher's embedding space. Our method outperforms previous approaches, including fully supervised models, on ImageNet linear evaluation (59.0% compared to 56.5%) and nearest neighbor evaluation (50.7% compared to 41.4%) for AlexNet. To our knowledge, this is the first time a self-supervised AlexNet has surpassed a supervised model in ImageNet classification. The code for our method can be found at: https://github.com/UMBCvision/CompRess.