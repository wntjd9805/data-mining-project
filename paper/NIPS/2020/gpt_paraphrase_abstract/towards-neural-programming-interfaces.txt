Controlling the behavior of artificial neural networks, especially generative neural language models, is a challenging task. To address this, we propose a new approach that involves training a specialized neural network called a Neural Programming Interface (NPI) to interface with a pretrained language model. By manipulating the hidden activations of the pretrained model, the NPI can generate desired outputs without modifying the original model's weights. We also introduce a new algorithm and loss function inspired by GANs to construct a dataset and train the NPI models to control autoregressive transformers. Our experiments using OpenAI's GPT-2 model demonstrate the effectiveness of our methods in controlling noun selection, topic aversion, offensive speech filtering, and other language aspects while maintaining the controlled model's fluency.