Learning an optimal policy without trial and error is a challenging task in real-world decision-making. One approach is imitation learning, which mimics expert actions using available demonstrations. Another approach is learning in simulators to avoid real-world trials and errors. However, obtaining sufficient expert demonstrations or high-fidelity simulators is difficult. In this study, we investigate policy learning with a few expert demonstrations and a simulator with incorrect dynamics. We propose a method called Horizon-Adaptive Inverse Dynamics Imitation Learning (HIDIL) that matches simulator states with expert states within a certain horizon and accurately recovers actions based on inverse dynamics policies. HIDIL effectively adapts actions to the real environment. Experimental results in four MuJoCo locomotion environments show that HIDIL significantly improves performance and stability compared to other imitation learning and transfer methods in reinforcement learning.