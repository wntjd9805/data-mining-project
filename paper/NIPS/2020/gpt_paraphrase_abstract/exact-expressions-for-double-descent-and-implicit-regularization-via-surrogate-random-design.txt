The concept of double descent refers to a phase transition observed in unregularized learning models, where the generalization error varies with the ratio of parameters to training samples. While deep neural networks have shown success in over-parameterized machine learning models, this phenomenon has also been analyzed in classical models like linear regression, which can perform well in the over-parameterized regime. This study presents the first precise non-asymptotic expressions for double descent in the minimum norm linear estimator. To achieve this, a surrogate random design called a determinantal point process is constructed to replace the standard i.i.d. design of the training sample. This surrogate design allows for exact expressions of the mean squared error of the estimator while maintaining the key properties of the standard design. Additionally, an exact implicit regularization result is established for over-parameterized training samples. Specifically, it is shown that for the surrogate design, the implicit bias of the unregularized minimum norm estimator corresponds precisely to solving a ridge-regularized least squares problem on the population distribution. Furthermore, a new mathematical tool of independent interest, the class of random matrices for which determinant commutes with expectation, is introduced in the analysis.