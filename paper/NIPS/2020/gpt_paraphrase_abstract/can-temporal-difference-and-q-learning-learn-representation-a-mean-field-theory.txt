Temporal-difference and Q-learning are important in deep reinforcement learning, using nonlinear function approximators like neural networks. The success of these methods lies in the learned feature representation, which encodes semantic structures from rich observations. The evolution of this feature representation is crucial for convergence. When using a neural network as the function approximator, we investigate how the associated feature representation evolves and whether it converges to the optimal one. We prove that, with an overparameterized two-layer neural network, temporal-difference and Q-learning minimize the mean-squared projected Bellman error globally at a sublinear rate. Additionally, the feature representation converges to the optimal one, extending previous analysis in the neural tangent kernel regime. Our analysis utilizes a mean-field perspective, connecting the evolution of finite-dimensional parameters to their limiting counterparts in an infinite-dimensional Wasserstein space. This analysis also applies to soft Q-learning and is connected to policy gradient.