Machine learning algorithms typically require large amounts of data in a stable environment. However, many real-world applications involve nonstationary environments. The challenge lies in effectively adapting models to these changing environments. We propose a method for transferring knowledge from one domain to another using (cid:96)1 regularization in high dimension. Our method incorporates (cid:96)1 regularization to account for differences between parameters in the source and target domains, in addition to the usual (cid:96)1 regularization. This results in sparsity for both the parameter estimates themselves and their changes. Our method provides a precise estimation error bound in a stable environment, and the estimate remains unchanged from the source estimate when there are small residuals. Furthermore, our estimate remains consistent with the underlying function, even if the source estimate is incorrect due to nonstationarity. Empirical results show that our proposed method effectively balances stability and adaptability.