Domain Adaptation (DA) is the process of transferring a learning machine from a labeled source domain to an unlabeled target domain. Existing DA methods primarily focus on improving target accuracy at inference, but fail to address the crucial issue of estimating predictive uncertainty in DA models. This is particularly important in safety-critical scenarios. In this paper, we tackle the challenging problem of Calibration in DA, which is complicated by the presence of domain shift and the lack of target labels. We discover that DA models achieve higher accuracy at the expense of well-calibrated probabilities. Based on this observation, we propose Transferable Calibration (TransCal) as a solution that achieves more accurate calibration with lower bias and variance, without the need for hyperparameter optimization. TransCal can be easily applied to recalibrate existing DA methods as a general post-hoc calibration technique. Its effectiveness is supported by both theoretical analysis and empirical evaluation.