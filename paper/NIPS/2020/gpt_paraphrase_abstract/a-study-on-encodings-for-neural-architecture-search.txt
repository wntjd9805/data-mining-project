Neural architecture search (NAS) has been extensively studied recently. One common approach is to represent each neural architecture as a directed acyclic graph (DAG) and search through these graphs using hyperparameters that encode the adjacency matrix and list of operations. However, recent research has shown that even small changes to the encoding of architectures can greatly affect the performance of NAS algorithms. In this study, we provide a formal examination of the impact of architecture encodings on NAS. We define architecture encodings, analyze their scalability, and identify the encoding-dependent subroutines used in NAS algorithms. Through experiments, we determine the most effective encodings for popular algorithms, providing insights for future work and disentangling the contributions of algorithms and encodings. Our findings highlight the significance of NAS encodings as a crucial design decision that can significantly influence overall performance.