We demonstrate a method to train a neural topic model with discrete random variables, which explicitly represents the assigned topic for each word. Our approach utilizes neural variational inference instead of stochastic backpropagation to handle the discrete variables. By combining the strengths of neural methods for text sequence representation with the topic model's ability to capture thematic coherence, our model achieves improved perplexity and document understanding on multiple datasets. We investigate the impact of prior parameters on both the model and variational parameters, and show that our approach outperforms a widely used topic model implementation in terms of topic quality.