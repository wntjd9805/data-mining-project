Learning models of the environment is seen as a crucial part of developing intelligent reinforcement learning agents. Typically, the model is built separately from its use by accurately predicting state transitions. However, we propose that model-based RL agents can better utilize their limited resources by constructing models that directly aid in value-based planning. We introduce the concept of value equivalence, where two models are considered equivalent if they produce the same Bellman updates for a set of functions and policies. By formulating the model learning problem based on value equivalence, we explore how the choice of policies and functions affects the set of feasible solutions. We demonstrate that as we expand the set of considered policies and functions, the class of value equivalent models decreases until it collapses to a single model that perfectly describes the environment. In many cases, directly modeling state-to-state transitions is challenging and unnecessary. By leveraging the value equivalence principle, we can find simpler models that still perform well, saving computation and memory. Through experiments comparing value-equivalent model learning with traditional methods like maximum likelihood estimation, we showcase the benefits of our approach. Moreover, we argue that the principle of value equivalence underlies recent successful RL techniques such as Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and MuZero, providing a theoretical foundation for these results.