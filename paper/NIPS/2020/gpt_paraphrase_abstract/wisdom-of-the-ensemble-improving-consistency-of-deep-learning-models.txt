Deep learning classifiers play a crucial role in decision-making, and therefore, users' trust in these models is highly important. Trust is often based on consistent behavior, meaning that users expect the same output for the same input, particularly when the output is correct. This study focuses on the behavior of models that undergo periodic retraining, where successive generations may provide different labels for the same input. We define the terms "consistency" and "correct-consistency" to formally analyze model behavior. Through our research, we prove that the consistency and correct-consistency of an ensemble learner are equal to or greater than the average consistency and correct-consistency of individual learners. Additionally, we demonstrate that combining learners with accuracy equal to or greater than the average accuracy of ensemble component learners can improve correct-consistency with a certain probability. To validate our findings, we utilize three datasets and two advanced deep learning classifiers. We also propose an efficient dynamic snapshot ensemble method and provide the code for our algorithm on GitHub (https://github.com/christa60/dynens).