Goal-conditioned hierarchical reinforcement learning (HRL) is a promising method for scaling up reinforcement learning (RL), but it often suffers from slow training due to the large action space of the high-level goals. Searching in this large space is challenging for generating subgoals and learning low-level policies. To address this issue, we propose an adjacency constraint that restricts the high-level action space to a nearby region of the current state. We prove that this constraint preserves the optimal hierarchical policy in deterministic MDPs and propose training an adjacency network to implement it effectively. Experimental results demonstrate that incorporating this adjacency constraint improves the performance of state-of-the-art HRL approaches in both deterministic and stochastic environments.