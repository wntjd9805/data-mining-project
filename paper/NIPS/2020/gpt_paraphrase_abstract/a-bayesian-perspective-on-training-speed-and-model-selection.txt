The relationship between training speed and the marginal likelihood in linear models is examined from a Bayesian perspective. Two key findings emerge: firstly, the training speed of a model can be used to estimate its marginal likelihood; secondly, this measure can predict the relative weightings of models in linear model combinations aimed at minimizing regression loss, given certain conditions. The validity of these findings is confirmed through model selection tasks for linear models and deep neural networks in the infinite-width limit. Additionally, empirical evidence supports the notion that these insights also apply to deep neural networks trained with stochastic gradient descent. Consequently, this research offers a promising avenue for understanding the bias towards well-generalizing functions observed in neural networks trained with stochastic gradient descent.