Efforts have been made to train neural networks to replicate algorithmic reasoning, but they struggle to grasp the abstract concepts underlying these algorithms. This is evident in their failure to generalize to data distributions beyond their training sets, such as larger inputs and unseen data. Our research focuses on the generalization issues of numerical subroutines in common algorithms like sorting, shortest paths, and minimum spanning trees. We find that transformer-based sequence-to-sequence models can learn subroutines like sorting, but their performance declines as the length of lists exceeds the training set. This degradation is due to attention weights losing accuracy with longer sequences, especially when the input numbers are similar. To address this, we propose a learned conditional masking mechanism that enables the model to generalize well beyond its training range and achieve near-perfect accuracy on various algorithms. Additionally, we demonstrate that encoding numbers with a binary representation produces embeddings with rich structure after training on tasks like addition or multiplication. This allows the embeddings to handle missing data by accurately interpolating numbers that were not seen during training.