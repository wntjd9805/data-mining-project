We introduce VILLA, the first attempt at extensive adversarial training for vision-and-language representation learning. VILLA consists of two stages: (i) task-agnostic adversarial pre-training, followed by (ii) task-specific adversarial fine-tuning. Instead of perturbing image pixels and textual tokens, we propose conducting adversarial training in the embedding space of each modality. To facilitate large-scale training, we utilize the "free" adversarial training strategy and incorporate KL-divergence-based regularization to promote greater invariance in the embedding space. By applying VILLA to the current leading vision-and-language models, we achieve new state-of-the-art performance on various tasks such as Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.1. The output format only provides the abstract information.