Deep learning models are increasingly popular in various machine learning applications, but the training data often contains sensitive information. To ensure privacy, many learning systems now use differential privacy, incorporating (differentially) private SGD to train their models. One crucial step in private SGD is gradient clipping, which reduces the gradient of an individual example if its norm exceeds a certain threshold. This study demonstrates how gradient clipping can prevent SGD from reaching a stationary point and provides a theoretical analysis that quantifies the bias introduced by clipping on convergence. The empirical evaluation shows that the gradient distributions during private SGD exhibit a symmetric structure that promotes convergence. These findings explain why private SGD with gradient clipping remains effective in practice, despite the potential bias. Additionally, a new perturbation-based technique is developed to correct the clipping bias, even for instances with highly asymmetric gradient distributions.