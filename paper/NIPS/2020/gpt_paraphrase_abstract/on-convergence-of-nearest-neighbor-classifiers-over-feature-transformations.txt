The k-Nearest Neighbors (kNN) classifier is a popular non-parametric machine learning algorithm. However, it is well-known that it struggles with the curse of dimensionality, so it is often used in conjunction with a pre-trained feature transformation. Most theoretical analyses of the kNN classifier are based on the raw feature space, creating a gap between theoretical understanding and practical applications. This paper aims to bridge this gap by analyzing the convergence rates of a kNN classifier on transformed features. The analysis relies on understanding the properties of both the transformed space and the raw feature space, particularly in terms of safety and smoothness. The study demonstrates that certain feature transformations are more suitable for the kNN classifier based on these properties. Experimental validation is conducted on 30 feature transformations using 6 benchmark datasets from various domains.