To enable machine learning pipelines to make discrete decisions without breaking the back-propagation of computational graphs, we propose a method to transform optimizers into differentiable operations. This method involves using stochastically perturbed optimizers that can be easily integrated with existing solvers. The derivatives of these operations can be efficiently evaluated, and their smoothness can be adjusted by controlling the level of noise. We also connect this framework to a set of losses used in structured prediction and provide theoretical guarantees for their application in learning tasks. Experimental results confirm the effectiveness of our approach in various tasks.