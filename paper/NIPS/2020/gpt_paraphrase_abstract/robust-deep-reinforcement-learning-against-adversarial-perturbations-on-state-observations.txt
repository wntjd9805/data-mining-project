Current approaches to improving the robustness of deep reinforcement learning (DRL) agents have had limited success and lack theoretical principles. Adversarial attacks can mislead DRL agents into making suboptimal actions, but existing techniques like adversarial training are ineffective for many RL tasks. In this study, we introduce the state-adversarial Markov decision process (SA-MDP) to analyze this problem and propose a theoretically principled policy regularization method that can be applied to various DRL algorithms. We demonstrate significant improvements in the robustness of popular DRL algorithms, such as PPO, DDPG, and DQN, against strong white box adversarial attacks. Moreover, we find that a robust policy also enhances DRL performance in environments without adversaries. Our code is publicly available at https://github.com/chenhongge/StateAdvDRL.