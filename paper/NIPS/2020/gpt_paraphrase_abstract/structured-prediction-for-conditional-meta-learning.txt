Optimization-based meta-learning aims to find a single initialization that can be applied to a range of tasks in order to expedite the learning process. Conditional meta-learning, on the other hand, seeks task-specific initializations to better capture complex task distributions and enhance performance. However, current conditional methods often struggle to generalize and lack theoretical guarantees. This study introduces a fresh perspective on conditional meta-learning through structured prediction. The authors propose a framework called task-adaptive structured meta-learning (TASML), which generates task-specific objective functions by assigning weights to meta-training data based on the target tasks. TASML is a non-parametric approach that can be integrated with existing meta-learning methods to enable conditioning. Experimental results demonstrate that TASML enhances the performance of existing meta-learning models and surpasses the state-of-the-art on benchmark datasets.