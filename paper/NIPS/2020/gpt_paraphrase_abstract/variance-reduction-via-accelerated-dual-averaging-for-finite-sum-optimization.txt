This paper introduces a simplified and unified method called Variance Reduction via Accelerated Dual Averaging (VRADA) for optimizing finite-sum convex problems. In both general convex and strongly convex scenarios, VRADA achieves accurate solutions using fewer stochastic gradient evaluations compared to the best known method. Specifically, VRADA requires O(n log log n) evaluations, an improvement over the previous best of O(n log n), where n represents the number of samples. Additionally, VRADA matches the lower bound of the general convex setting up to a log log n factor and matches the lower bounds in both the general convex and strongly convex regimes. Furthermore, VRADA offers a more unified and simplified algorithmic implementation and convergence analysis for both convex settings. The paper also highlights the novel initialization strategy used in VRADA, which may be of independent interest. Experimental results on real datasets demonstrate the superior performance of VRADA compared to existing methods for large-scale machine learning problems.