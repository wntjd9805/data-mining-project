Self-imitation learning, inspired by lower-bound Q-learning, is an effective method for off-policy learning. This study proposes a generalized n-step lower bound that extends the original return-based lower-bound Q-learning and introduces a new family of self-imitation learning algorithms. By demonstrating a trade-off between fixed point bias and contraction rate, the n-step lower bound Q-learning provides a formal motivation for the potential performance gains of self-imitation learning. The study also shows that the n-step lower bound Q-learning is a more robust option compared to return-based self-imitation learning and uncorrected n-step Q-learning in various continuous control benchmark tasks. The implementation can be accessed at https://github.com/robintyh1/nstep-sil.