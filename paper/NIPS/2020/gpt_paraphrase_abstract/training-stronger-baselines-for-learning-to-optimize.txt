Learning to optimize (L2O) is gaining attention as traditional optimizers require manual design and tuning. However, existing L2O models have performance and practicality gaps. To address this, we propose improved training techniques for L2O models. Firstly, we introduce a curriculum-based training scheme that gradually increases the optimizer unroll length to mitigate the truncation bias versus gradient explosion dilemma. Secondly, we utilize off-policy imitation learning to guide L2O learning by imitating analytical optimizers. Our techniques enhance the performance of state-of-the-art L2O models without altering their structures. We demonstrate that even the simplest L2O model can surpass the latest and most complex models on various tasks. Our findings highlight the untapped potential of L2O and call for a reassessment of recent L2O model advancements. Our code is publicly available at: https://github.com/VITA-Group/L2O-Training-Techniques.