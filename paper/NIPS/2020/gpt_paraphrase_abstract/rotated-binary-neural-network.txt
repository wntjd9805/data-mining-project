The Binary Neural Network (BNN) is effective in reducing the complexity of deep neural networks, but it suffers from significant performance degradation. This is mainly due to a large quantization error between the full-precision weight vector and its binary counterpart. Previous studies have focused on compensating for the difference in magnitude, neglecting the impact of angular bias. In this paper, we investigate the influence of angular bias on the quantization error and propose a new approach called Rotated Binary Neural Network (RBNN). RBNN aims to align the angle between the full-precision weight vector and its binarized version. At the beginning of each training epoch, we rotate the full-precision weight vector to reduce the angular bias. To avoid the complexity of learning a large rotation matrix, we introduce a bi-rotation formulation that learns two smaller rotation matrices. During training, we use an adjustable rotated weight vector for binarization to avoid potential local optima. Our rotation approach leads to approximately 50% weight flips, maximizing information gain. Additionally, we propose a training-aware approximation of the sign function for gradient backward. Experimental results on CIFAR-10 and ImageNet datasets demonstrate that RBNN outperforms state-of-the-art methods. Our source code, experimental settings, training logs, and binary models are available at https://github.com/lmbxmu/RBNN.