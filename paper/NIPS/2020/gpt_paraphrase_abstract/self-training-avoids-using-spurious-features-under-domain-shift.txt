Current theory in unsupervised domain adaptation primarily focuses on scenarios where the source and target domains are similar. However, practical applications have shown that techniques such as conditional entropy minimization and pseudo-labeling can still be effective even when there are significant differences between the domains. This study specifically examines a situation where the domain shift is substantial, but these algorithms can still be proven to work. In this particular setting, certain irrelevant features in the source domain correlate with the label, but they are independent of the label in the target domain. The analysis focuses on linear classification, where the irrelevant features follow a Gaussian distribution and the relevant features follow a mixture of log-concave distributions. The study demonstrates that by minimizing entropy on unlabeled target data and starting with an adequately accurate source classifier, the algorithm can avoid using the irrelevant features. This is achieved despite the non-convex nature of the objective function, which contains multiple suboptimal solutions involving the irrelevant features. The theory is validated through experiments on spurious domain shift tasks using semi-synthetic Celeb-A and MNIST datasets. The findings suggest that practitioners should gather and self-train on large and diverse datasets to mitigate biases in classifiers, even if manual labeling is not feasible.