Value function approximation in reinforcement learning (RL) has been successful in practice, but there is still a lack of understanding regarding general function approximation schemes. This paper presents the first RL algorithm with provable efficiency using general value function approximation. The algorithm achieves a regret bound of F if the value functions can be approximated by a function class with complexity measure d, which depends on the eluder dimension and log-covering numbers, as well as the planning horizon H and the number of interactions with the environment T. This theory extends the linear MDP assumption to general function classes and provides a model-free algorithm that justifies the effectiveness of practical algorithms.