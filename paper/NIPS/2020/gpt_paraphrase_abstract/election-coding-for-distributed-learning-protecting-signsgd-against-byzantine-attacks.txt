This paper introduces ELECTION CODING, a framework that addresses the performance degradation of current distributed learning systems caused by Byzantine attacks. The framework focuses on guaranteeing Byzantine-robustness for distributed learning algorithms that utilize signed stochastic gradient descent (SignSGD) with minimal worker-master communication load. By exploring information-theoretic limits, the framework enables the identification of the majority opinion even when some workers are under attack, paving the way for the implementation of robust and communication-efficient distributed learning algorithms. The framework includes the construction of two types of codes, namely random Bernoulli codes and deterministic algebraic codes. These codes provide tolerance against Byzantine attacks by incorporating computational redundancy and ensuring convergence in general non-convex scenarios. For Bernoulli codes, an upper bound on the error probability in estimating the signs of the true gradients is provided, offering insights into code design for Byzantine tolerance. The proposed deterministic codes are proven to perfectly tolerate any arbitrary Byzantine attacks. Experiments conducted on real datasets validate the effectiveness of the suggested codes in improving the Byzantine tolerance of distributed learning systems that utilize SignSGD.