This study focuses on sign language translation (SLT) and the challenges of accurately interpreting sign video sequences into written language. Current SLT models often represent sign gestures frame by frame, avoiding the need for explicit video segmentation. However, this approach neglects the temporal information of the signs, leading to translation ambiguity. To address this, the researchers propose a novel sign video segment representation that considers multiple temporal granularities, eliminating the need for precise video segmentation. They then introduce a hierarchical sign video feature learning method, called TSPNet, which incorporates inter-scale and intra-scale attention mechanisms to enhance semantic consistency and resolve ambiguity. Experimental results demonstrate that TSPNet outperforms existing models, significantly improving BLEU and ROUGE scores on the largest SLT dataset. The implementation of TSPNet is available at https://github.com/verashira/TSPNet.