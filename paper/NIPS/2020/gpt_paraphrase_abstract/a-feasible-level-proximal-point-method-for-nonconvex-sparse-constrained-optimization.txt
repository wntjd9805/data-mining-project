Nonconvex sparse models have gained significant attention in high-dimensional machine learning. This paper introduces a novel model that combines general convex or nonconvex objectives with various continuous nonconvex sparsity-inducing constraints. To solve this constrained model, a new proximal point algorithm is proposed. This algorithm solves a sequence of convex subproblems with gradually relaxed constraint levels. Each subproblem, which includes a proximal point objective and a convex surrogate constraint, can be efficiently solved using a fast routine for projection onto the surrogate constraint. The proposed algorithm is proven to converge asymptotically to the Karush-Kuhn-Tucker (KKT) solutions. Additionally, new convergence complexities are established to achieve an approximate KKT solution for smooth/nonsmooth, deterministic/stochastic, and convex/nonconvex objectives. These complexities are comparable to those of gradient descent for unconstrained optimization problems. This study is the first to provide complexity guarantees for first-order methods in nonconvex sparse-constrained problems. Numerical experiments are conducted to demonstrate the effectiveness of the new model and the efficiency of the proposed algorithm when applied to large scale problems.