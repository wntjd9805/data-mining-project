This study examines the performance of Transformer language models (TLMs) in reasoning tasks by training them on knowledge encoded in natural language. The researchers focus on logical reasoning tasks that involve analyzing relationships between entities using first-order logical proofs. They use TLMs to generate natural language proofs and evaluate their logical consistency and accuracy. The study finds that TLMs face challenges in generalizing to longer sequences but improve after exposure to longer proofs. Moreover, the researchers discover that TLMs perform better with backward-chaining proofs and find it easier to generate forward-chaining proofs. Interestingly, models not specifically trained to generate proofs demonstrate better generalization with longer proofs, suggesting that Transformers employ efficient yet complex internal reasoning strategies. These findings highlight the systematic generalization behavior of TLMs in logical reasoning tasks and call for further investigation into their underlying reasoning strategies.