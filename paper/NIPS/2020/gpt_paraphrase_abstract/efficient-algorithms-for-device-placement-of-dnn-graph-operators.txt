As machine learning workloads utilize larger and more complex models, the need for executing these models on expensive devices is increasing. These devices are becoming more diverse, including specialized accelerators alongside CPUs. To handle this, workload distribution across multiple devices is necessary. Previous research has shown that significant improvements can be achieved through model parallelism, which involves partitioning a neural network's computational graph onto multiple devices. This approach assumes a pipeline of devices that process a stream of samples, resulting in high throughput for training and inference of deep neural networks.However, in order to effectively distribute the workload across devices in settings with large models and multiple heterogeneous devices, automated algorithms and toolchains are needed. This paper focuses on the optimization problem of device placement for DNN operators during both inference and training, particularly in modern pipelined settings. We present algorithms that can solve this problem optimally. The effectiveness and efficiency of our approaches are demonstrated using various contemporary DNN computation graphs.