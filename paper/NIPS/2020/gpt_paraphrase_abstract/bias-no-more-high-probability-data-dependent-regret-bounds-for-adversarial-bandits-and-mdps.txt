We propose a new method for achieving high probability regret bounds in online learning with bandit feedback against an adaptive adversary. Unlike existing approaches that require constructing optimistic and biased loss estimators, our method uses standard unbiased estimators and a simple increasing learning rate schedule. We also utilize logarithmically homogeneous self-concordant barriers and a strengthened version of Freedman's inequality. Our approach has several advantages. Firstly, the regret bounds obtained are data-dependent and can be much smaller than worst-case bounds, which addresses a problem posed by Neu. Secondly, our method provides the first general and efficient algorithm with a high probability regret bound for adversarial linear bandits, solving a problem posed by Bartlett et al. and Abernethy and Rakhlin. Previous methods either lack efficiency or are only applicable to specific action sets. Lastly, our approach can be applied to learning adversarial Markov Decision Processes and offers the first algorithm with a high probability small-loss bound for this problem.