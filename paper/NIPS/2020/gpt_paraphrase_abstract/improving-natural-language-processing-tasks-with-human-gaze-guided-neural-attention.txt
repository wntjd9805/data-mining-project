The lack of available data has hindered progress in incorporating human gaze data into neural attention mechanisms for natural language processing (NLP). To address this, we propose a new hybrid text saliency model (TSM) that combines a cognitive reading model with human gaze supervision in a single machine learning framework. We conduct experiments on four different datasets and show that our TSM accurately predicts the duration of human gaze. Additionally, we introduce a joint modeling approach that integrates TSM predictions into the attention layer of a network designed for a specific NLP task, without requiring task-specific human gaze data. Our joint model outperforms the current state of the art in paraphrase generation and achieves top performance in sentence compression tasks. Our work provides a practical solution for combining data-driven and cognitive models and presents a novel method for integrating human gaze-guided neural attention in NLP tasks.