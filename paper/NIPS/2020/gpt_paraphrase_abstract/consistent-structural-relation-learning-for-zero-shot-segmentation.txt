Zero-shot semantic segmentation aims to recognize the semantics of pixels from categories that have not been seen during training. Previous methods trained classifiers for unseen categories using visual features generated from semantic word embeddings. However, these methods only learned the generator on seen categories, resulting in poor generalization. In this study, we propose a Consistent Structural Relation Learning (CSRL) approach to constrain the generation of visual features for unseen categories by exploiting the structural relations between seen and unseen categories. We observe that categories often have similar relations in both semantic word embedding space and visual feature space. Based on this observation, we utilize the similarity of category-level relations in the semantic word embedding space to improve the visual feature generator. Specifically, we enforce the consistency of generated visual features with their counterparts in the semantic word embedding space by exploring pair-wise and list-wise structures. This transfer of relations between seen and unseen categories implicitly guides the generator to produce relation-consistent visual features for unseen categories. We conduct extensive experiments on Pascal-VOC and Pascal-Context benchmarks, and our proposed CSRL method outperforms existing state-of-the-art methods by a significant margin, achieving improvements of approximately 7-12% on Pascal-VOC and 2-5% on Pascal-Context.