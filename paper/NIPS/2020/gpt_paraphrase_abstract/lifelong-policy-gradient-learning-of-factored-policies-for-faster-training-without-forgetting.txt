Policy gradient methods have been successful in learning control policies for complex systems, but they require a significant amount of exploration before achieving high performance. In a lifelong learning scenario, where an agent faces multiple tasks over its lifetime, using information from previously encountered tasks can speed up the learning process for new tasks. We propose a new approach for lifelong policy gradient learning that directly trains lifelong function approximators using policy gradients. This allows the agent to benefit from accumulated knowledge throughout the entire training process. Through empirical evaluation, we demonstrate that our algorithm learns faster, converges to superior policies compared to single-task and lifelong learning methods, and avoids catastrophic forgetting in challenging domains.