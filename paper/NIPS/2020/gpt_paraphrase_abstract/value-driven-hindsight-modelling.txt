Learning value predictors is a crucial aspect of reinforcement learning (RL). The RL community has extensively studied how to effectively learn value predictors from data, with different approaches utilizing problem domain structure in different ways. Model learning can exploit the transition structure in observation sequences but is not sensitive to the reward function. On the other hand, model-free methods directly use future information but receive a potentially weak scalar signal. We propose a middle-ground approach for representation learning in RL, where we learn what to model to enhance value prediction. We determine which features of future trajectories are useful for predicting returns, providing relevant prediction targets for the task and accelerating value function learning. This approach can be understood as retrospectively reasoning about which aspects of future observations could aid past value prediction. We demonstrate the significant benefits of this approach in simple policy evaluation settings and scale it up to challenging domains, including 57 Atari 2600 games.