Bayesian optimization (BO) is a widely used method for optimizing black-box functions that are expensive to evaluate. With the increasing computational power of edge devices like mobile phones and concerns about privacy, federated learning (FL) has gained significant interest. FL focuses on collaborative training of deep neural networks (DNNs) using first-order optimization techniques. However, certain machine learning tasks, such as hyperparameter tuning of DNNs, lack access to gradients and require zeroth-order/black-box optimization. This suggests the potential extension of BO to the FL setting (FBO) for agents to collaborate in these black-box optimization tasks.  This paper introduces federated Thompson sampling (FTS), which addresses several challenges of FBO and FL in a principled manner. Firstly, we employ random Fourier features to approximate the Gaussian process surrogate model used in BO. This approximation naturally generates the parameters to be exchanged between agents. Secondly, we design FTS based on Thompson sampling, which significantly reduces the number of parameters that need to be exchanged. Lastly, we provide a theoretical convergence guarantee that is robust against heterogeneous agents, a major challenge in FL and FBO.   We conduct empirical experiments to demonstrate the effectiveness of FTS in terms of communication efficiency, computational efficiency, and practical performance.