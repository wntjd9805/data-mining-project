This paper introduces novel techniques and numerical formats that allow for the precise scaling of training systems from 8-bits to 4-bits. A new adaptive Gradient Scaling technique called GradScale is explored to address challenges related to limited range and resolution in quantized gradients, as well as to analyze the impact of quantization errors during model training. The role of bias in gradient quantization is theoretically analyzed, and solutions are proposed to mitigate its effect on model convergence. The techniques are tested on various deep learning models in computer vision, speech, and NLP. When combined with existing solutions for 4-bit quantization of weight and activation tensors, 4-bit training demonstrates minimal loss in accuracy across different application domains, while significantly improving hardware acceleration compared to state-of-the-art FP16 systems (over 7 times improvement).