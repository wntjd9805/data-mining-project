We present a novel approach called adaptive discretization for developing a highly efficient model-based episodic reinforcement learning algorithm in large state-action spaces. Our algorithm utilizes optimistic one-step value iteration and maintains an adaptive discretization of the space. Theoretical analysis reveals that our algorithm achieves competitive worst-case regret bounds compared to the current state-of-the-art model-based algorithms. Additionally, our proof technique allows for potential incorporation of additional problem structure. From an implementation perspective, our algorithm requires significantly less storage and computational resources by maintaining a more efficient partition of the state and action spaces. Experimental results on various control problems demonstrate that our algorithm outperforms fixed discretization methods in terms of faster convergence and lower memory usage. Intriguingly, we observe that while fixed discretization model-based algorithms outperform their model-free counterparts, our adaptive discretization approach achieves comparable performance to model-free methods.