Knowledge distillation is a successful method of compressing and transferring knowledge from a teacher network to a student network. However, its theoretical understanding is currently lacking. Recent research on neural tangent kernel allows us to approximate a wide neural network with a linear model. In this study, we analyze the knowledge distillation of a wide neural network theoretically. We present a transfer risk bound for the linearized model and introduce a metric called data inefficiency to measure the task's training difficulty. We demonstrate that a high ratio of teacher's soft labels can be beneficial when the teacher is perfect. Additionally, we find that hard labels can correct the teacher's wrong predictions, explaining the practice of using a combination of hard and soft labels.