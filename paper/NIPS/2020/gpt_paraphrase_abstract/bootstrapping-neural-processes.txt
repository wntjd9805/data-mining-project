Traditional statistical modeling requires users to manually specify a prior, while Neural Processes (NPs) use neural networks to implicitly define a wide range of stochastic processes. NPs learn the best stochastic process to describe a given data stream. However, NPs assume that uncertainty in stochastic processes is modeled by a single latent variable, which limits their flexibility. To address this limitation, we introduce the Bootstrapping Neural Process (BNP), an extension of NPs that utilizes the bootstrap technique for estimating uncertainty. BNP can learn the stochasticity in NPs without assuming a specific form. We demonstrate the effectiveness of BNP on diverse data types and its resilience in the face of model-data mismatch.