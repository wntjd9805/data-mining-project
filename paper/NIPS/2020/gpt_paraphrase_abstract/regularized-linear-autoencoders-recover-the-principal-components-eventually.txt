In recent years, there has been significant progress in understanding how neural networks learn input-output relationships. However, little is known about the convergence of the underlying representations, particularly in the case of linear autoencoders (LAEs). In this study, we demonstrate that LAEs can learn the optimal representation, which consists of ordered, axis-aligned principal components, when trained with appropriate regularization. We examine two regularization methods: non-uniform regularization and a deterministic variant of nested dropout. Although both regularization approaches converge to the optimal representation, the convergence is slow due to the worsening ill-conditioning with increasing latent dimension. However, we propose a modification to the gradient descent update that accelerates convergence significantly. Through empirical analysis, we show that the inefficiency of learning the optimal representation is not unavoidable.