The current dominance of feedforward networks in large-scale challenges raises the question of why recurrent connections, which are known to improve learning efficiency and generalization in vision models, are not more prevalent. This paper suggests that the limitation lies in the standard training algorithm for recurrent models, back-propagation through time (BPTT), which has high memory complexity. This restricts the design of recurrent models, forcing a choice between high capacity or complex dynamics. To address this issue, the authors propose a new learning algorithm called contractor recurrent back-propagation (C-RBP) that achieves constant memory complexity. They demonstrate that C-RBP-trained recurrent vision models outperform BPTT-trained models in detecting long-range spatial dependencies and in solving large-scale challenges, such as Panoptic SegmentationMS-COCO, with fewer parameters. C-RBP is a versatile learning algorithm that can benefit applications requiring recurrent dynamics. The code and data for C-RBP are available at https://github.com/c-rbp.