Deep reinforcement learning (RL) is a computationally intensive task that requires processing a large amount of data. Synchronous methods are more stable during training but have lower data throughput. On the other hand, asynchronous methods have higher throughput but suffer from stability issues and lower sample efficiency due to outdated policies. To combine the benefits of both methods, we propose a new approach called High-Throughput Synchronous Deep Reinforcement Learning (HTS-RL). In HTS-RL, we simultaneously perform learning and rollouts, employ a system design that avoids outdated policies, and ensure that actors interact with environment replicas in an asynchronous manner while maintaining determinism. We evaluate our approach on Atari games and the Google Research Football environment. Compared to synchronous baselines, HTS-RL achieves a speedup of 2 to 6 times. Compared to state-of-the-art asynchronous methods, HTS-RL has comparable throughput and consistently achieves higher average episode rewards.