Model-based reinforcement learning (RL) is a crucial aspect of RL that uses an empirical model to find the best policy. It is particularly suitable for multi-agent RL (MARL) because it separates the learning and planning phases, avoiding issues when all agents improve their policies simultaneously. However, the sample complexity of model-based MARL algorithms has been less explored. This paper aims to address the question of sample complexity in model-based MARL. The study focuses on a basic MARL scenario: two-player discounted zero-sum Markov games, where only a generative model of state transition is available. The research shows that model-based MARL achieves a sample complexity of (cid:101)O(|S||A||B|(1 −γ)−3(cid:15)−2) to find the Nash equilibrium (NE) value with a (cid:15) error, as well as the (cid:15)-NE policies. Here, γ represents the discount factor, and S, A, B denote the state space and action spaces for the two agents. It is also demonstrated that this method is nearly minimax optimal, with a tight dependence on 1 − γ and |S|, as evidenced by a lower bound of Ω(|S|(|A| + |B|)(1 − γ)−3(cid:15)−2). Overall, these findings support the effectiveness of the simple model-based approach in the multi-agent RL context.