Implicit neural representations parameterized by neural networks have become popular due to their ability to offer various advantages over traditional representations. However, current network architectures for these representations lack the ability to capture fine details in signals and accurately model spatial and temporal derivatives required for signals defined by differential equations. To address this, we propose using periodic activation functions in neural networks, called sinusoidal representation networks (SIRENs), which are highly effective in representing complex natural signals and their derivatives. We examine the statistics of SIREN activations to suggest a principled initialization method and showcase their capability in representing images, wavefields, videos, sounds, 3D shapes, and their derivatives. Additionally, we demonstrate how SIRENs can be used to solve challenging boundary value problems like Eikonal equations, Poisson equation, and Helmholtz and wave equations. Furthermore, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. For a comprehensive overview and demonstrations, please refer to the project website.