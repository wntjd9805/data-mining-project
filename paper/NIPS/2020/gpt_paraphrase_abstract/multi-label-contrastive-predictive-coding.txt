Variational mutual information (MI) estimators are commonly used in unsupervised representation learning methods like contrastive predictive coding (CPC). In these estimators, a lower bound on MI is obtained by using a multi-class classification problem, where a critic tries to distinguish a positive sample from the joint distribution from (m-1) negative samples drawn from a suitable proposal distribution. However, this approach can severely underestimate MI unless m is very large because the MI estimates are bounded above by log m. To address this issue, we propose a new estimator based on a multi-label classification problem, where the critic simultaneously identifies multiple positive samples. By using the same number of negative samples, our multi-label CPC estimator surpasses the log m bound while still ensuring a valid lower bound for mutual information. We demonstrate that our approach leads to more accurate mutual information estimation, improves unsupervised representation learning, and outperforms a state-of-the-art knowledge distillation method in 10 out of 13 tasks.