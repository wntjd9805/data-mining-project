The recent success of pre-trained language models relies on fine-tuning them with large amounts of labeled data, which can be costly or inaccessible for many applications. To address this issue, we explore self-training as a semi-supervised learning approach that utilizes unlabeled data for the target task. Traditionally, self-training randomly selects instances from the unlabeled data to create pseudo-labels and augment the labeled data. However, we propose an improved approach that incorporates uncertainty estimates from the neural network using Bayesian deep learning techniques. Specifically, we introduce acquisition functions that leverage Monte Carlo Dropout to select instances from the unlabeled data, and a learning mechanism that utilizes model confidence for self-training. We apply our approach to text classification using five benchmark datasets. Remarkably, our method achieves comparable performance to fully supervised pre-trained language models, even when trained with only 20-30 labeled samples per class. Our approach achieves an aggregate accuracy of 91% and improves baseline performance by up to 12%.