We propose a novel approach for Vision-and-Language Navigation (VLN) that focuses on capturing and utilizing the relationships between textual instructions and visual scenes. Our method involves creating a Language and Visual Entity Relationship Graph to model these inter-modal relationships, as well as the intra-modal relationships among visual entities. We use a message passing algorithm to propagate information between language elements and visual entities in the graph, enabling us to determine the next action to take. Experimental results demonstrate that leveraging these relationships improves performance compared to state-of-the-art methods. On the Room-to-Room (R2R) benchmark, our method achieves a success rate weighted by path length (SPL) of 52% on the test unseen split, setting a new best performance. On the Room-for-Room (R4R) dataset, our method significantly improves the previous best result from 13% to 34% on the success weighted by normalized dynamic time warping (SDTW). The code for our method is available at: https://github.com/YicongHong/Entity-Graph-VLN.