Various methods have been proposed to speed up the training of Graph Convolution Networks (GCNs) by using sampling algorithms with variance reduction. However, these algorithms are not optimal for GCNs and cannot be applied to more general graph neural networks (GNNs) like Graph Attention Networks (GAT). This is because the computation of the optimal sampling distribution is difficult, as the embeddings of neighbors or learned weights involved in the distribution change during training and are only partially observed when sampled. To address this, we present a solution that formulates the optimization of sampling variance as an adversary bandit problem. Our algorithm achieves an asymptotically optimal variance within a factor of 3 and proves to be efficient and effective on multiple datasets.