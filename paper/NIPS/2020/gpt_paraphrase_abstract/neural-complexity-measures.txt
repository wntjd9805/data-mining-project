Creating a suitable complexity measure to predict and explain generalization in deep neural networks has been difficult. In order to address this issue, we introduce Neural Complexity (NC), a meta-learning framework that predicts generalization. Our model learns a complexity measure through interactions with various tasks in a data-driven manner. The trained NC model can be used alongside the standard training loss to regulate any task learner in supervised learning. We compare NC's approach to manually-designed complexity measures and other meta-learning models, and demonstrate its effectiveness on multiple regression and classification tasks.