Reducing the dimensionality of large datasets by projecting them onto lower-dimensional subspaces is often desirable. Matrix sketching is an efficient technique used for this purpose. However, existing performance guarantees for sketching methods are typically different from what is observed in practice. To address this, we leverage recent advancements in spectral analysis of random matrices to develop new techniques. These techniques provide accurate expressions for the expected value of random projection matrices obtained through sketching. Our results can be applied to various machine learning tasks, such as low-rank approximation and iterative stochastic optimization. They allow for precise analysis of popular sketching methods, including Gaussian and Rademacher sketches, based on the spectral properties of the data. Empirical evidence demonstrates that our derived expressions accurately reflect the practical performance of these sketching methods, including lower-order effects and constant factors.