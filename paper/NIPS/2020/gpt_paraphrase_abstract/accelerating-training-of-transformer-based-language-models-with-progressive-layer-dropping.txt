Transformer-based language models have shown impressive performance in various NLP areas, but the pre-training step is computationally expensive. Existing methods to accelerate pre-training either depend on advanced hardware or are not suitable for language modeling. This study introduces a method called progressive layer dropping, which improves the efficiency of Transformer-based language models without excessive hardware requirements. Through experiments using BERT, the proposed method reduces training time by an average of 24% per sample and achieves 2.5 times faster pre-training compared to the baseline while maintaining similar accuracy on downstream tasks. Additionally, the pre-trained models exhibit strong knowledge transferability, achieving comparable or higher GLUE scores compared to the baseline when trained with the same number of samples.