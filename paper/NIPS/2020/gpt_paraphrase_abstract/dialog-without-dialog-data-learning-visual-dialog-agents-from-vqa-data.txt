Can we create dialog agents that can quickly adapt to new tasks while still maintaining their ability to communicate effectively with people? These agents could use a wider range of existing data to learn how to handle new tasks, reducing the need for costly data collection and annotation. In this study, we investigate a scenario called "Dialog without Dialog," where agents must develop dialog models based on visual cues alone, without any explicit language guidance. By separating intention and language, our model prevents significant changes in language after fine-tuning for new tasks. Our qualitative results, automated metrics, and human studies all demonstrate that our model successfully adapts to new tasks while preserving language quality. In comparison, other approaches either struggle to perform well on new tasks or experience significant changes in language, rendering them incomprehensible to humans. The code for our model is available at: https://github.com/mcogswell/dialog_without_dialog.