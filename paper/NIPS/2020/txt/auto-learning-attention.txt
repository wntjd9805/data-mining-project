Abstract
Attention modules have been demonstrated effective in strengthening the repre-sentation ability of a neural network via reweighting spatial or channel features or stacking both operations sequentially. However, designing the structures of different attention operations requires a bulk of computation and extensive ex-pertise. In this paper, we devise an Auto Learning Attention (AutoLA) method, which is the ﬁrst attempt on automatic attention design. Speciﬁcally, we deﬁne a novel attention module named high order group attention (HOGA) as a directed acyclic graph (DAG) where each group represents a node, and each edge rep-resents an operation of heterogeneous attentions. A typical HOGA architecture can be searched automatically via the differential AutoLA method within 1 GPU day using the ResNet-20 backbone on CIFAR10. Further, the searched attention module can generalize to various backbones as a plug-and-play component and outperforms popular manually designed channel and spatial attentions for many vision tasks, including image classiﬁcation on CIFAR100 and ImageNet, object detection and human keypoint detection on COCO dataset. Code is available at https://github.com/btma48/AutoLA. 1

Introduction
Attention learning has been increasingly incorporated into convolutional neural networks (CNNs) [1], aiming to compact the image representation and strengthen its discriminatory power [2, 3, 4, 5]. It has been widely recognized that attention learning is beneﬁcial for many computer vision tasks, such as image classiﬁcation, segmentation, and object detection.
There are two types of typical attention mechanisms. The channel attention is able to reinforce the informative channels and to suppress irrelevant channels of feature maps [2], while the spatial attention enables CNNs to dynamically concentrate processing resources at the location of interest, resulting in better and more effective processing of information [4]. Let either the channel attention or spatial attention be treated as the ﬁrst-order attention. The combination of both channel attention and spatial attention constitutes the second-order attention, which has been proven in benchmarks to produce better performance than either ﬁrst-order attention by modulating the feature maps in both channel-wise and spatial-wise [4]. Accordingly, we propose to extend attention modules from the ﬁrst- or second-order to a higher order, i.e., arranging more basic attention units structurally.
However, considering the highly variable structures and hyperparameters of basic attention units, exhaustively searching the architecture of high order attention module is an exponential explosion in complexity.
∗indicates equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Recent years have witnessed the unprecedented success of neural architecture search (NAS) in the automated design of neural network architectures, surpassing human designs on various tasks
[6, 7, 8, 9, 10, 11, 12]. We advocate the use of NAS to search the optimal architecture of high order attention module, which however is challenging for several reasons. First, there is no explicit off-the-shelf deﬁnition of the search space for attention modules, where various attention operations may be included [13]. Second, the sequential structure for arranging different attention operations should be computationally efﬁcient so that it can be searched within affordable computational budgets.
Third, how to search the attention module, e.g., given the backbone or together with the backbone cells, remains unclear. Fourth, the searched attention module is expected to generalize well to various backbones and tasks.
In this paper, we propose an Auto Learning Attention (AutoLA) method for automatically search-ing efﬁcient and effective plug-and-play attention modules for various well-established backbone networks. Speciﬁcally, we ﬁrst deﬁne a novel concept of attention module, i.e., high order group attention (HOGA), by exploring a ’split-reciprocate-aggregate’ strategy. Technically, each HOGA block receives feature tensor from each block in the backbone as input, which is divided into K groups along the channel dimension to reduce the computational complexity. Then, a directed acyclic graph (DAG) [14] is constructed, where each node is associated with a split group, and each edge represents a speciﬁc attention operation. The sequential connections between different nodes can represent different combinations of basic attention operations, resulting in various ﬁrst-order to Kth order attention modules, which indeed constitute a search space of HOGA. By customizing DARTS
[8] for our problem, the explicit HOGA structure can be searched efﬁciently within 1 GPU day on a modern GPU given a ﬁxed backbone network (e.g., ResNet-20) on CIFAR10. Extensive experiments demonstrate the obtained HOGA generalizes well on various backbones and outperforms previous hand-crafted attentions for many vision tasks, including image classiﬁcation on the CIFAR100 and
ImageNet datasets, object detection, and human keypoint detection on the COCO dataset.
To summarize, the contribution of our paper is three-fold. First, to the best of our knowledge,
AutoLA is the ﬁrst attempt to extend NAS to search plug-and-play attention modules beyond the backbone architecture. Second, we deﬁne a novel concept of attention module named HOGA that can represent high order attentions and the previous channel attention and spatial attention can be treated as its special cases. Third, we utilize a differentiable search method to search the optimal
HOGA module efﬁciently, which can generalize well on various backbones and outperform previous attention modules for many vision tasks. 2