Abstract
{ 2
˘
`
T 1
We consider the problem of online learning and its application to solving minimax games. For the online learning problem, Follow the Perturbed Leader (FTPL) is a widely studied algorithm which enjoys the optimal O worst case regret guarantee for both convex and nonconvex losses. In this work, we show that when the sequence of loss functions is predictable, a simple modiﬁcation of FTPL which incorporates optimism can achieve better regret guarantees, while retaining the optimal worst case regret guarantee for unpredictable sequences. A key challenge in obtaining these tighter regret bounds is the stochasticity and optimism in the algorithm, which requires different analysis techniques than those commonly used in the analysis of FTPL. The key ingredient we utilize in our analysis is the dual view of perturbation as regularization. While our algorithm has several applications, we consider the speciﬁc application of minimax games. For solving smooth convex-concave games, our algorithm only requires access to a linear optimization oracle.
For Lipschitz and smooth nonconvex-nonconcave games, our algorithm requires access to an optimization oracle which computes the perturbed best response. In both these settings, our algorithm solves the game up to an accuracy of O using T calls to the optimization oracle. An important feature of our algorithm
˘ is that it is highly parallelizable and requires only O iterations, with each iteration making O parallel calls to the optimization oracle.
T 1 p 1
T ´
T 1
` q 2 2 2
{
{
{ 1

Introduction
`
˘
In this work, we consider the problem of online learning, where in each iteration, the learner chooses an action and observes a loss function. The goal of the learner is to choose a sequence of actions which minimizes the cumulative loss suffered over the course of learning. The paradigm of online learning has many theoretical and practical applications and has been widely studied in a number of
ﬁelds, including game theory and machine learning. One of the popular applications of online learning is in solving minimax games arising in various contexts such as boosting [1], robust optimization [2],
Generative Adversarial Networks [3].
In recent years, a number of efﬁcient algorithms have been developed for regret minimization. These algorithms fall into two broad categories, namely, Follow the Regularized Leader (FTRL) [4] and
FTPL [5] style algorithms. When the sequence of loss functions encountered by the learner are convex, both these algorithms are known to achieve the optimal O worst case regret [6, 7]. While these algorithms have similar regret guarantees, they differ in computational aspects. Each iteration of FTRL involves optimization of a non-linear convex function over the action space (also called the projection step). In contrast, each step of FTPL involves solving a linear optimization problem, which can be implemented efﬁciently for many problems of interest [8, 9, 10]. For example, if the action space is an `p ball for some p
, then projecting onto this set is much more computationally expensive than performing linear optimization over this set. As another example, consider the 1, 2, 8u
T 1
R t
˘
` 2
{ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
scenario where the action space is the set of all positive semideﬁnite matrices. Then projecting onto this set requires performing expensive singular value decompositions. Whereas, linear optimization only requires computation of the leading eigenvector. This crucial difference between FTRL and
FTPL makes the latter algorithm more attractive in practice. Even in the more general nonconvex setting, where the loss functions encountered by the learner can potentially be nonconvex, FTPL algorithms are attractive. In this setting, FTPL requires access to an ofﬂine optimization oracle which computes the perturbed best response, and achieves O worst case regret [11]. Furthermore, these optimization oracles can be efﬁciently implemented for many problems by leveraging the rich body of work on global optimization [12].
T 1
˘
` 2
{
Despite its importance and popularity, FTPL has been mostly studied for the worst case setting, where the loss functions are assumed to be adversarially chosen. In a number of applications of online learning, the loss functions are actually benign and predictable [13]. In such scenarios, FTPL can not utilize the predictability of losses to achieve tighter regret bounds. While [11, 13] study variants of
FTPL which can make use of predictability, these works either consider restricted settings or provide sub-optimal regret guarantees (see Section 2 for more details). This is unlike FTRL, where optimistic variants that can utilize the predictability of loss functions have been well understood [13, 14] and have been shown to provide faster convergence rates in applications such as minimax games. In this work, we aim to bridge this gap and study a variant of FTPL called Optimistic FTPL (OFTPL), which can achieve better regret bounds, while retaining the optimal worst case regret guarantee for unpredictable sequences. The main challenge in obtaining these tighter regret bounds is handling the stochasticity and optimism in the algorithm, which requires different analysis techniques to those commonly used in the analysis of FTPL. In this work, we rely on the dual view of perturbation as regularization to derive regret bounds of OFTPL.
To demonstrate the usefulness of OFTPL, we consider the problem of solving minimax games.
A widely used approach for solving such games relies on online learning algorithms [6]. In this approach, both the minimization and the maximization players play a repeated game against each other and rely on online learning algorithms to choose their actions in each round of the game. In our algorithm for solving games, we let both the players use OFTPL to choose their actions. For solving smooth convex-concave games, our algorithm only requires access to a linear optimization oracle. For
Lipschitz and smooth nonconvex-nonconcave games, our algorithm requires access to an optimization oracle which computes the perturbed best response. In both these settings, our algorithm solves the using T calls to the optimization oracle. While there are prior game up to an accuracy of O algorithms that achieve these convergence rates [11, 15], an important feature of our algorithm is that 2 it is highly parallelizable and requires only O
{ parallel calls to the optimization oracle. We note that such parallelizable algorithms are especially
˘ useful in large-scale machine learning applications such as training of GANs, adversarial training, which often involve huge datasets such as ImageNet [16]. iterations, with each iteration making O
T 1 p 1
T ´
T 1
`
˘
` q 2 2
{
{ 2 Preliminaries and