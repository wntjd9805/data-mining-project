Abstract
Modern gradient boosting software frameworks, such as XGBoost and LightGBM, implement Newton descent in a functional space. At each boosting iteration, their goal is to ﬁnd the base hypothesis, selected from some base hypothesis class, that is closest to the Newton descent direction in a Euclidean sense. Typically, the base hypothesis class is ﬁxed to be all binary decision trees up to a given depth. In this work, we study a Heterogeneous Newton Boosting Machine (HNBM) in which the base hypothesis class may vary across boosting iterations. Speciﬁcally, at each boosting iteration, the base hypothesis class is chosen, from a ﬁxed set of subclasses, by sampling from a probability distribution. We derive a global linear convergence rate for the HNBM under certain assumptions, and show that it agrees with existing rates for Newton’s method when the Newton direction can be perfectly ﬁtted by the base hypothesis at each boosting iteration. We then describe a particular realization of a HNBM, SnapBoost, that, at each boosting iteration, randomly selects between either a decision tree of variable depth or a linear regressor with random Fourier features. We describe how SnapBoost is implemented, with a focus on the training complexity. Finally, we present experimental results, using OpenML and Kaggle datasets, that show that SnapBoost is able to achieve better generalization loss than competing boosting frameworks, without taking signiﬁcantly longer to tune. 1

Introduction
Boosted ensembles of decision trees are the dominant machine learning (ML) technique today in application domains where tabular data is abundant (e.g., competitive data science, ﬁnancial/retail industries). While these methods achieve best-in-class generalization, they also expose a large number of hyper-parameters. The fast training routines offered by modern boosting frameworks allow one to effectively tune these hyper-parameters and are an equally important factor in their success.
The idea of boosting, or building a strong learner from a sequence of weak learners, originated in the early 1990s [41], [21]. This discovery led to the widely-popular AdaBoost algorithm [22], which
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
iteratively trains a sequence of weak learners, whereby the training examples for the next learner are weighted according to the success of the previously-constructed learners. An alternative theoretical interpretation of AdaBoost was presented in [23], which showed that the algorithm is equivalent to minimizing an exponential loss function using gradient descent in a functional space. Moreover, the same paper showed that this idea can be applied to arbitrary differentiable loss functions.
The modern explosion of boosting can be attributed primarily to the rise of two software frameworks:
XGBoost [14] and LightGBM [27]. Both frameworks leverage the formulation of boosting as a functional gradient descent, to support a wide range of different loss functions, resulting in general-purpose ML solutions that can be applied to a wide range of problems. Furthermore, these frameworks place a high importance on training performance: employing a range of algorithmic optimizations to reduce complexity (e.g., splitting nodes using histogram summary statistics) as well as system-level optimizations to leverage both many-core CPUs and GPUs. One additional characteristic of these frameworks is that they use a second-order approximation of the loss function and perform an algorithm akin to Newton’s method for optimization. While this difference with traditional gradient boosting is often glossed over, in practice it is found to signiﬁcantly improve generalization [43].
From a theoretical perspective, boosting algorithms are not restricted to any particular class of weak learners. At each boosting iteration, a weak learner (from this point forward referred to as a base hypothesis) is chosen from some base hypothesis class. In both of the aforementioned frameworks, this class comprises all binary decision trees up to a ﬁxed maximum depth. Moreover, both frameworks are homogeneous: the hypothesis class is ﬁxed at each boosting iteration. Recently [18, 44, 17] have considered heterogeneous boosting, in which the hypothesis class may vary across boosting iterations. Promising results indicate that this approach may improve the generalization capability of the resulting ensembles, at the expense of signiﬁcantly more complex training procedures.
The goal of our work is to build upon the ideas of [18, 44, 17], and develop a heterogeneous boosting framework with theoretical convergence guarantees, that can achieve better generalization than both
XGBoost and LightGBM, without signiﬁcantly sacriﬁcing performance.
Contributions. The contributions of this work can be summarized as follows:
• We propose a Heterogeneous Newton Boosting Machine (HNBM), in which the base hypothesis class at each boosting iteration is selected at random, from a ﬁxed set of subclasses, according to an arbitrary probability mass function Φ.
• We derive a global linear convergence rate for the proposed HNBM for strongly convex loss functions with Lipschitz-continuous gradients. Our convergence rates agree with existing global rates in the special case when the base hypotheses are fully dense in the prediction space.
• We describe a particular realization of a HNBM, SnapBoost, that randomly selects between K different subclasses at each boosting iteration: (K − 1) of these subclasses correspond to binary decision trees (BDTs) of different maximum depths and one subclass corresponds to linear regressors with random Fourier features (LRFs). We provide details regarding how SnapBoost is implemented, with a focus on training complexity.
• We present experiments using OpenML [45] and Kaggle [7] datasets that demonstrate SnapBoost generalizes better than competing boosting frameworks, without compromising performance. 1.1