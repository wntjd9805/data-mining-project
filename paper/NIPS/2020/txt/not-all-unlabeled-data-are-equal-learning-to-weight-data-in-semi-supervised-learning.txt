Abstract
Existing semi-supervised learning (SSL) algorithms use a single weight to balance the loss of labeled and unlabeled examples, i.e., all unlabeled examples are equally weighted. But not all unlabeled data are equal. In this paper we study how to use a different weight for every unlabeled example. Manual tuning of all those weights – as done in prior work – is no longer possible. Instead, we adjust those weights via an algorithm based on the inﬂuence function, a measure of a model’s dependency on one training example. To make the approach efﬁcient, we propose a fast and effective approximation of the inﬂuence function. We demonstrate that this technique outperforms state-of-the-art methods on semi-supervised image and language classiﬁcation tasks. 1

Introduction
Unlabeled data helps to reduce the cost of supervised learning, particularly in ﬁelds where it is expensive to obtain annotations. For instance, labels for biomedical tasks need to be provided by domain experts, which are expensive to hire. Besides the hiring cost, labeling tasks are often labor intensive, e.g., dense labeling of video data requires to review many frames. Hence, a signiﬁcant amount of effort has been invested to develop novel semi-supervised learning (SSL) algorithms, i.e., algorithms which utilize both labeled and unlabeled data. See the seminal review (speciﬁcally
Sec. 1.1.2.) by Chapelle et al. [6] and references therein.
Classical semi-supervised techniques [26, 36, 38, 42] based on expectation-maximization [11, 16] iterate between (a) inferring a label-estimate for the unlabeled portion of the data using the current model and (b) using both labels and label-estimates to update the model. Methods for deep nets have also been explored [20, 29, 32, 33]. More recently, data augmentation techniques are combined with label-estimation for SSL. The key idea is to improve the model via consistency losses which encourage labels to remain identical after augmentation [4, 41].
Formally, the standard SSL setup consists of three datasets: a labeled training set, an unlabeled training set, and a validation set. In practice, SSL algorithms train the model parameters on both the labeled and unlabeled training sets and tune the hyperparameters manually based on the validation set performance. Speciﬁcally, a key hyperparameter adjusts the trade-off between labeled and unlabeled data. All aforementioned SSL methods use a single scalar for this, i.e., an identical weight is assigned to all unlabeled data points. To obtain good performance, in practice, this weight is carefully tuned using the validation set, and changes over the training iterations [4].
We think not all unlabeled data are equal. For instance, when the label-estimate of an unlabeled example is incorrect, training on that particular label-estimate hurts overall performance. In this case, using a single scalar to weight the labeled and unlabeled data loss term is suboptimal. To address this, we study use of an individual weight for each of the unlabeled examples. To facilitate such a
⇤Indicates equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Iteration 0
Iteration 10
Iteration 20
Figure 1: Decision boundaries across training iterations on linearly separable data. Labeled samples are shown in orange and unlabeled data in black/pink (shading depicts weight of each unlabeled point). Our approach (blue) with per example weights with Pseudo label SSL algorithm [20]. large number of hyperparameters, we automatically adjust the per-example weights by utilizing the inﬂuence function [7]. This inﬂuence function estimates the “importance” of each unlabeled example using the validation set performance.
In Fig. 1 we demo this idea on labeled and unlabeled, linearly separable data. The gray/pink color shade indicates the weight of the unlabeled data. We observe the proposed method to more closely mimic ground-truth compared to supervised training.
The performance gain does not come for free. The method involves adjusting per-example weights for each unlabeled example which is computationally expensive if implemented naively. Speciﬁcally, adjusting a per-example weight involves computing (a) a per-example gradient and (b) an inverse
Hessian vector product w.r.t. the model parameters. To address both challenges, we design an efﬁcient algorithm for computing per-example gradients, extending backpropagation. Moreover, we propose an effective and efﬁciently computable approximation speciﬁcally for the inﬂuence functions of deep nets. These improvements permit to scale the approach to recent SSL tasks and achieve compelling results on CIFAR-10, SVHN, and IMDb. 2