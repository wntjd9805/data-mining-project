Abstract
We introduce JAX MD, a software package for performing differentiable physics simulations with a focus on molecular dynamics. JAX MD includes a number of physics simulation environments, as well as interaction potentials and neural networks that can be integrated into these environments without writing any addi-tional code. Since the simulations themselves are differentiable functions, entire trajectories can be differentiated to perform meta-optimization. These features are built on primitive operations, such as spatial partitioning, that allow simulations to scale to hundreds-of-thousands of particles on a single GPU. These primitives are
ﬂexible enough that they can be used to scale up workloads outside of molecular dynamics. We present several examples that highlight the features of JAX MD including: integration of graph neural networks into traditional simulations, meta-optimization through minimization of particle packings, and a multi-agent ﬂocking simulation. JAX MD is available at www.github.com/google/jax-md. 1

Introduction
The past few years have seen an explosion of progress at the intersection of machine learning, au-tomatic differentiation, and the physical sciences [1, 2]. Deep neural network models of quantum mechanical energies have become ever more accurate [3–11], graph networks have been designed to simulate systems by observing their dynamics [12–15], and deep networks have improved upon classical approaches to protein folding [16]. Combining classical simulation environments with deep learning or optimizing them directly via automatic differentiation has led to signiﬁcant ad-vances including: end-to-end learning of protein structures [17, 18], the inverse design of photonic crystals [19], and structural optimization via reparameterization using convolutional networks [20].
Finally, signiﬁcant progress in developmental psychology has centered around combining intuitive physics simulations with probabilistic programming [21].
This ﬂurry of excellent research has highlighted a signiﬁcant source of inefﬁciency: we lack general purpose simulation environments that can easily be integrated with existing machine learning tools.
For example, despite the rapid progress developing neural network models of quantum mechanical systems, it is difﬁcult to incorporate these models into classical simulation environments such as
LAMMPS [22], HOOMD-Blue [23, 24], and OpenMM [25]. Currently, integrating machine learning models into existing simulations requires the construction of custom software “bridges” [26–31].
This is a signiﬁcant hindrance to the adoption of machine learning methods in practice. Moreover, most research into differentiable simulation environments are bespoke pieces of software written for a single application. As such, currently research at the intersection of simulation and machine learning requires practitioners to be experts, not only at machine learning, but also at writing the simulations themselves.
Here we introduce JAX MD, which is a software package that makes it easy to write physics simula-tions that naturally integrate with machine learning models in the ﬂourishing JAX [32, 33] ecosystem. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
At its core, JAX MD provides a collection of lightweight primitive operations that are useful in developing and analyzing a broad range of physics simulations. By composing these primitives, JAX
MD also provides standard molecular dynamics simulation environments and interaction potentials.
To help researchers analyze simulations, we include tools that can be used visualize particle trajec-tories within Jupyter notebooks [34]. Finally, we provide several state-of-the-art neural network architectures that can easily be incorporated into simulations no modiﬁcations of the simulation code.
We begin with a brief overview of JAX and JAX MD. We then provide benchmarks against classic simulation software before describing the structure of the library. Finally, we discuss several examples that illustrate the use of JAX MD to perform research at the intersection of simulation and machine learning including:
• Training and deployment of a neural network potential in simulation.
• Differentiation through simulation for structural optimization.
• A simulation of ﬂocking behavior.
Each example features an accompanying Colaboratory notebook that reproduces the results and includes more details about the use-case. While these examples are designed to be illustrative, they involve issues faced by researchers in practice. Moreover, each of these examples would be difﬁcult to implement using existing simulation infrastructure. 2