Abstract
A signiﬁcant effort has been made to train neural networks that replicate algorith-mic reasoning, but they often fail to learn the abstract concepts underlying these algorithms. This is evidenced by their inability to generalize to data distributions that are outside of their restricted training sets, namely larger inputs and unseen data. We study these generalization issues at the level of numerical subroutines that comprise common algorithms like sorting, shortest paths, and minimum spanning trees. First, we observe that transformer-based sequence-to-sequence models can learn subroutines like sorting a list of numbers, but their performance rapidly degrades as the length of lists grows beyond those found in the training set. We demonstrate that this is due to attention weights that lose ﬁdelity with longer se-quences, particularly when the input numbers are numerically similar. To address the issue, we propose a learned conditional masking mechanism, which enables the model to strongly generalize far outside of its training range with near-perfect accuracy on a variety of algorithms. Second, to generalize to unseen data, we show that encoding numbers with a binary representation leads to embeddings with rich structure once trained on downstream tasks like addition or multiplication. This allows the embedding to handle missing data by faithfully interpolating numbers not seen during training.

Introduction 1
Neural networks have become the preferred model for pattern recognition and prediction in perceptual tasks and natural language processing [13, 4] thanks to their ﬂexibility and their ability to learn complex solutions. Recently, researchers have turned their attention towards imbuing neural networks with the capability to perform algorithmic reasoning, thereby allowing them to go beyond pattern recognition and logically solve more complex problems [9, 12, 10, 14]. These are often inspired by concepts in conventional computer systems (e.g., pointers [29], external memory [22, 9]).
Unlike perceptual tasks, where the model is only expected to perform well on a speciﬁc distribution from which the training set is drawn, in algorithmic reasoning the goal is to learn a robust solution that performs the task regardless of the input distribution. This ability to generalize to arbitrary input distributions—as opposed to unseen instances from a ﬁxed data distribution—distinguishes the concept of strong generalization from ordinary generalization. To date, neural networks still have difﬁculty learning algorithmic tasks with strong generalization [28, 8, 12].
⇤Work completed during an internship at Google. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this work, we study this problem by learning to imitate the composable subroutines that form the basis of common algorithms, namely selection sort, merge sort, Dijkstra’s algorithm for shortest paths, and Prim’s algorithm to ﬁnd a minimum spanning tree. We choose to focus on subroutine imitation as: (1) it is a natural mechanism that is reminiscent of how human developers decompose problems (e.g., developers implement very different subroutines for merge sort vs. selection sort), (2) it supports introspection to understand how the network may fail to strongly generalize, and (3) it allows for providing additional supervision to the neural network if necessary (inputs, outputs, and intermediate state).
By testing a powerful sequence-to-sequence transformer model [26] within this context, we show that while it is able to learn subroutines for a given data distribution, it fails to strongly generalize as the test distribution deviates from the training distribution. Subroutines often operate on strict subsets of data, and further analysis of this failure case reveals that transformers have difﬁculty separating “what” to compute from “where” to compute, manifesting in attention weights whose entropy increases over longer sequence lengths than those seen in training. This, in turn, results in misprediction and compounding errors.
Our solution to this problem is to leverage the transformer mask. First, we have the transformer predict both a value and a pointer. These are used as the current output of the subroutine, and the pointer is also used as an input to a learned conditional masking mechanism that updates the encoder mask for subsequent computation. We call the resulting architecture a Neural Execution Engine (NEE), and show that NEEs achieve near-perfect generalization over a signiﬁcantly larger range of test values than existing models. We also ﬁnd that a NEE that is trained on one subroutine (e.g., comparison) can be used in a variety of algorithms (e.g., Dijkstra, Prim) as-is without retraining.
Another essential component of algorithmic reasoning is representing and manipulating numbers
[30]. To achieve strong generalization, the employed number system must work over large ranges and generalize outside of its training domain (as it is intractable to train the network on all integers).
In this work, we leverage binary numbers, as binary is a hierarchical representation that expands exponentially with the length of the bit string (e.g., 8-bit binary strings represent exponentially more data than 7-bit binary strings), thus making it possible to train and test on signiﬁcantly larger number ranges compared to prior work [8, 12]. We demonstrate that the binary embeddings trained on down-stream tasks (e.g., addition, multiplication) lead to well-structured and interpretable representations with natural interpolation capabilities. 2