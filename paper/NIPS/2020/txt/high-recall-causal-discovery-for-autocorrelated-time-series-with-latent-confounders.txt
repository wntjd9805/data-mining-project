Abstract
We present a new method for linear and nonlinear, lagged and contemporaneous constraint-based causal discovery from observational time series in the presence of latent confounders. We show that existing causal discovery methods such as
FCI and variants suffer from low recall in the autocorrelated time series case and identify low effect size of conditional independence tests as the main reason.
Information-theoretical arguments show that effect size can often be increased if causal parents are included in the conditioning sets. To identify parents early on, we suggest an iterative procedure that utilizes novel orientation rules to determine ancestral relationships already during the edge removal phase. We prove that the method is order-independent, and sound and complete in the oracle case. Extensive simulation studies for different numbers of variables, time lags, sample sizes, and further cases demonstrate that our method indeed achieves much higher recall than existing methods for the case of autocorrelated continuous variables while keeping false positives at the desired level. This performance gain grows with stronger autocorrelation. At github.com/jakobrunge/tigramite we provide Python code for all methods involved in the simulation studies. 1

Introduction
Observational causal discovery [Spirtes et al., 2000, Peters et al., 2017] from time series is a chal-lenge of high relevance to many ﬁelds of science and engineering if experimental interventions are in-feasible, expensive, or unethical. Causal knowledge of direct and indirect effects, interaction pathways, and time lags can help to understand and model physical systems and to predict the effect of interven-tions [Pearl, 2000]. Causal graphs can also guide interpretable variable selection for prediction and classiﬁcation tasks. Causal discovery from time series faces major challenges [Runge et al., 2019a] such as unobserved confounders, high-dimensionality, and nonlinear dependencies, to name a few.
Few frameworks can deal with these challenges and we here focus on constraint-based methods pioneered in the seminal works of Spirtes, Glymour, and Zhang [Spirtes et al., 2000, Zhang, 2008].
We demonstrate that existing latent causal discovery methods strongly suffer from low recall in the time series case where identifying lagged and contemporaneous causal links is the goal and autocor-relation is an added, ubiquitous challenge. Our main theoretical contributions lie in identifying low effect size as a major reason why current methods fail and in introducing a novel sound, complete, and order-independent causal discovery algorithm that yields strong gains in recall for autocorrelated continuous data. Our practical contributions lie in extensive numerical experiments that can serve as a future benchmark and in open-source Python implementations of our and major previous time series causal discovery algorithms. The paper is structured as follows: After brieﬂy introducing the problem and existing methods in Sec. 2, we describe our method and theoretical results in Sec. 3. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Section 4 provides numerical experiments followed by a discussion of strengths and weaknesses as well as an outlook in Sec. 6. The paper is accompanied by Supplementary Material (SM). 2 Time series causal discovery in the presence of latent confounders 2.1 Preliminaries t−1, . . .) for j = 1, . . . , ˜N that follow a stationary
We consider multivariate time series Vj = (V j t , V j discrete-time structural vector-autoregressive process described by the structural causal model (SCM) t = fj(pa(V j
V j t ), ηj t ) with j = 1, . . . , ˜N . (1) t , V 2
The measurable functions fj depend non-trivially on all their arguments, the noise variables ηj t are jointly independent, and the sets pa(V j t ) ⊆ (Vt, Vt−1, . . . , Vt−pts) deﬁne the causal parents of
V j t . Here, Vt = (V 1 t , . . .) and pts is the order of the time series. Due to stationarity the causal relationship of the pair of variables (V i t ), where τ ≥ 0 is known as lag, is the same as that of all time shifted pairs (V i t(cid:48) ). This is why below we always ﬁx one variable at time t. We assume that there are no cyclic causal relationships, which as a result of time order restricts the contemporaneous (τ = 0) interactions only. We allow for unobserved variables, i.e., we allow for observing only a subset X = {X1, . . . , XN } ⊆ V = {V1, V2, . . .} of time series with N ≤ ˜N . We further assume that there are no selection variables and assume the faithfulness [Spirtes et al., 2000] condition, which states that conditional independence (CI) in the observed distribution P (V) generated by the SCM implies d-separation in the associated time series graph G over variables V. t(cid:48)−τ , V j t−τ , V j
We assume the reader is familiar with the Fast Causal Inference (FCI) algorithm [Spirtes et al., 1995,
Spirtes et al., 2000, Zhang, 2008] and related graphical terminology, see Secs. S1 and S2 of the SM for a brief overview. Importantly, the MAGs (maximal ancestral graphs) considered in this paper can contain directed (→) and bidirected (↔) edges (interchangeably also called links). The associated
PAGs (partial ancestral graphs) may additionally have edges of the type ◦→ and ◦−◦. 2.2 Existing methods
The tsFCI algorithm [Entner and Hoyer, 2010] adapts the constraint-based FCI algorithm to time series.
It uses time order and stationarity to restrict conditioning sets and to apply additional edge orientations. SVAR-FCI [Malinsky and Spirtes, 2018] uses stationarity to also infer additional edge removals. There are no assumptions on the functional relationships or on the structure of confounding. Granger causality [Granger, 1969] is another common framework for inferring the causal structure of time series. It cannot deal with contemporaneous links (known as instantaneous effects in this context) and may draw wrong conclusions in the presence of latent confounders, see e.g. [Peters et al., 2017] for an overview. The ANLTSM method [Chu and Glymour, 2008] restricts contemporaneous interactions to be linear, and latent confounders to be linear and contemporaneous.
TS-LiNGAM [Hyvärinen et al., 2008] is based on LiNGAM [Shimizu et al., 2006] that is rooted in the structural causal model framework [Peters et al., 2017, Spirtes and Zhang, 2016]. It allows for contemporaneous effects, assumes linear interactions with additive non-Gaussian noise, and might fail in the presence of confounding. The TiMINo [Peters et al., 2013] method restricts interactions to an identiﬁable function class or requires an acyclic summary graph. Yet another approach are Bayesian score-based or hybrid methods [Chickering, 2002, Tsamardinos et al., 2006]. These often become computationally infeasible in the presence of unobserved variables, see [Jabbari et al., 2017] for a discussion, or make restrictive assumptions about functional dependencies or variable types.
In this paper we follow the constraint-based approach that allows for general functional relationships (both for lagged and contemporaneous interactions), general types of variables (discrete and continu-ous, univariate and multivariate), and that makes no assumption on the structure of confounding. The price of this generality is that we will not be able to distinguish all members of a Markov equivalence class (although time order and stationarity allow to exclude some members of the equivalence class).
Due to its additional use of stationarity we choose SVAR-FCI rather than tsFCI as a baseline and implement the method, restricted to no selection variables, in Python. As a second baseline we implement SVAR-RFCI, which is a time series adaption of RFCI along the lines of SVAR-FCI (also restricted to no selection variables). The RFCI algorithm [Colombo et al., 2012] is a modiﬁcation of
FCI that does not execute FCI’s potentially time consuming second edge removal phase. 2
Figure 1: Latent confounder example of the model in eq. (3) (Sec. 4) with linear ground truth links shown for the LPCMCI case (right panel). All auto-coefﬁcients are 0.9, all cross-coefﬁcients are 0.6 (colored links), false links or links with false orientations are grey. True and false adjacency detection rates shown as link width. Detection rates based on 500 realizations run at α = 0.01 for T = 500. 2.3 On maximum time lag, stationarity, soundness, and completeness
In time series causal discovery the assumption of stationarity and the length of the chosen time lag window t − τmax ≤ t(cid:48) ≤ t play an important role. In the causally sufﬁcient case (X = V) the causal graph stays the same for all τmax ≥ pts. Not so in the latent case: Let M(G)τmax be the MAG obtained by marginalizing over all unobserved variables and also all generally observed variables at times t(cid:48) < t − τmax. Then, increasing the considered time lag window by increasing
τmax may result in the removal of edges that are fully contained in the original window, even in the case of perfect statistical decisions. In other words, M(G)τmax,1 with τmax,1 < τmax,2 need not be a subgraph of M(G)τmax,2 . Hence, τmax may be regarded more as an analysis choice than as a tunable parameter. For the same reason stationarity also affects the deﬁnition of MAGs and PAGs that are being estimated. For example, SVAR-FCI uses stationarity to also remove edges whose separating set extends beyond the chosen time lag window. It does, therefore, in general not determine a PAG of M(G)τmax. To formalize this let i) M(G)τmax statA be the MAG obtained from M(G)τmax by enforcing repeating adjacencies, let ii) P(G)τmax statA be the maximally informative PAG for the
Markov equivalence class of M(G)τmax statA, which can be obtained from running the FCI orientation rules on M(G)τmax statAO be the PAG obtained when additionally enforcing time order and repeating orientations at each step of applying the orientation rules. Note that P(G)τmax statAO may have fewer circle marks, i.e., may be more informative than P(G)τmax statA. Our aim is to estimate
P(G)τmax statA, and complete if it returns P(G)τmax statAO for simplicity. statAO. We say an algorithm is sound if it returns a PAG for M(G)τmax statAO. Below we write M(G) = M(G)τmax statA and P(G) = P(G)τmax statA, and let iii) P(G)τmax 2.4 Motivational example
We illustrate the challenge posed by unobserved variables with the example of Fig. 1. SVAR-FCI with the partial correlation (ParCorr) CI test correctly identiﬁes the auto-links but misses the true lagged link Yt−1→Zt and returns a false link Yt−2→Zt instead. In most realizations the algorithm fails to detect the contemporaneous adjacency Xt↔Yt and, if detected, fails to orient it as bidirected.
The reason are wrong CI tests in its edge removal and orientation phases. When it iterates through conditioning sets of cardinality p = 0 in the edge removal phase, the correlation ρ(Xt; Yt) is non-signiﬁcant in many realizations since the high autocorrelation of both X and Y increases their variance and decreases their signal-to-noise ratio (the common signal due to the latent confounder).
Further, for p = 1 also the lagged correlation ρ(Yt−1; Zt|Yt−2) often is non-signiﬁcant and the true link Yt−1→Zt gets removed. Here conditioning away the autocorrelation of Yt−1 decreases the signal while the noise level in Zt is still high due to Z’s autocorrelation. This false negative has implications for further CI tests since Yt−1 won’t be used in subsequent conditioning sets: The path
Yt−2→Yt−1→Zt can then not be blocked anymore and the false positive Yt−2→Zt remains even after the next removal phase. In the orientation phase of SVAR-FCI rule R1 yields tails for all auto-links.
Even if the link Xt◦−◦Yt is detected, it is in most cases not oriented correctly. The reason again lies in wrong CI tests: In principle the collider rule R0 should identify Xt↔Yt since the middle node of the triple Xt−1◦→Xt◦−◦Yt does not lie in the separating set of Xt−1 and Yt (and similarly for X and
Y swapped). In practice R0 is implemented with the majority rule [Colombo and Maathuis, 2014] to avoid order-dependence, which involves further CI test given subsets of the adjacencies of Xt−1 and Yt. SVAR-FCI here ﬁnds independence given Yt−1 (correct) but also given Xt (wrong, due 3
to autocorrelation). Since the middle node Xt is in exactly half of the separating sets, the triple is marked as ambiguous and left unoriented. The same applies when X and Y are swapped.
Autocorrelation is only one manifestation of a more general problem we observe here: Low signal-to-noise ratio due to an ‘unfortunate’ choice of conditioning sets that leads to low effect size (here partial correlation) and, hence, low statistical power of CI tests. Wrong CI tests then lead to missing links, and these in turn to false positives and wrong orientations. In the following we analyze effect size more theoretically and suggest a general idea to overcome this issue. 3 Latent PCMCI 3.1 Effect size in causal discovery t−τ ∗→X j t−τ and B = X j t , where below we write A = X i
The detection power of a true link X i t to emphasize that the discussion also applies to the non-time series case, quantiﬁes the probability of the link not being erroneously removed due to a wrong CI test. It depends on i) the sample size (usually ﬁxed), ii) the CI tests’ signiﬁcance level α (ﬁxed by the researcher as the desired false positives level), iii) the CI tests’ estimation dimensions (kept at a minimum by SVAR-FCI’s design to preferentially test small conditioning sets), and iv) the effect size. We here deﬁne effect size as the minimum of the CI test statistic values I(A; B|S) taken over all conditioning sets S that are being tested (for ﬁxed A and B). As observed in the motivating example, this minimum can become very small and hence lead to low detection power. The central idea of our proposed method Latent
PCMCI (LPCMCI) is to increase effect size by a) restricting the conditioning sets S that need to be tested in order to remove all wrong links, and by b) extending those sets S that do need to be tested with so called default conditions Sdef that increase the CI test statistic values and at the same time do not induce spurious dependencies. Regarding a), Lemma S5 proves that it is sufﬁcient to only consider conditioning sets that consist of ancestors of A or B only. Regarding b), and well-ﬁtting with a), Lemma S4 proves that no spurious dependencies are introduced if Sdef consist of ancestors of A or B only. Further, the following theorem shows that taking Sdef as the union of the parents of
A and B (without A and B themselves) improves the effect size of LPCMCI over that of SVAR-FCI.
This generalizes the momentary conditional independence (MCI) idea that underlies the PCMCI and
PCMCI+ algorithms [Runge et al., 2019b, Runge, 2020] to causal discovery with latent confounders.
We state the theorem in an information theoretic framework, where I denotes (conditional) mutual information and I(A; B; C|D) ≡ I(A; B|D) − I(A; B|C ∪ D) the interaction information.
Theorem 1 (LPCMCI effect size). Let A∗→B (with A = X i t ) be a link (→ or ↔) in M(G). Consider the default conditions Sdef = pa({A, B}, M(G)) \ {A, B} and denote X∗ =
X \ Sdef . Let S = arg minS⊆X∗\{A,B} I(A; B|S ∪ Sdef ) be the set of sets that deﬁne LPCMCI’s effect size. If i) there is S ∗ ∈ S with S ∗ ⊆ adj(A, M(G)) \ Sdef or S ∗ ⊆ adj(B, M(G)) \ Sdef and ii) there is a proper subset Q ⊂ Sdef such that I(A; B; Sdef \ Q|S ∗ ∪ Q) < 0, then
I(A; B|S ∪ Sdef ) > t−τ and B = X j
I(A; B| ˜S) . (2) min
S⊆X∗\{A,B} min
˜S⊆X\{A,B}
If the assumptions are not fulﬁlled, then (trivially) "≥" holds in eq. (2).
The second assumption only requires that any subset Sdef \ Q of the parents contains information that increases the information between A and B. A sufﬁcient condition for this is detailed in Corollary S1.
These considerations lead to two design principles behind LPCMCI: First, when testing for conditional independence of A and B, discard conditioning sets that contain known non-ancestors of A and B.
Second, use known parents of A and B as default conditions. Unless the higher effect size is overly counteracted by the increased estimation dimension (due to conditioning sets of higher cardinality), this leads to higher detection power and hence higher recall of true links. While we do not claim that our choice of default conditions as further detailed in Sec. 3.4 is optimal, our numerical experiments in Sec. 4 and the SM indicate strong increases in recall for the case of continuous variables with autocorrelation. In [Runge et al., 2019b, Runge, 2020] it is discussed that, in addition to higher effect size, conditioning on the parents of both A and B also leads to better calibrated tests which in turn avoids inﬂated false positives. Another beneﬁt is that fewer conditioning sets need to be tested, which is also the motivation for a default conditioning on known parents in [Lee and Honavar, 2020].
The above design principles are only useful if some (non-)ancestorships are known before all CI test have been completed. LPCMCI achieves this by entangling the edge removal and edge orientation 4
phases, i.e., by learning ancestral relations before having removed all wrong links. For this purpose we below develop novel orientation rules. These are not necessary in the causally sufﬁcienct setting considered by PCMCI+ [Runge, 2020] because there the default conditions need not be limited to ancestors of A or B (although PCMCI+ tries to keep the number of default conditions low). While not considered here, background knowledge about (non-)ancestorships can easily be incorporated. 3.2
Introducing middle marks and LPCMCI-PAGs
To facilitate early orientation of edges we give an unambiguous causal interpretation to the graph at every step of the algorithm. This is achieved by augmenting edges with middle marks. Using generic variable names A, B, and C indicates that the discussion also applies to the non-time series case.
Middle marks are denoted above the link symbol and can be ‘?’, ‘L’, ‘R’, ‘!’, or ‘’ (empty). The
‘L’ (‘R’) on A∗−∗L B (A∗−∗R B) asserts that if A < B (B < A) then B /∈ an(A, G) or there is no
S ⊆ pa(A, M(G)) that m-separates A and B in M(G). Here < is any total order on the set of variables. Its choice is arbitrary and does not inﬂuence the causal information content, the sole purpose being to disambiguate A∗−∗L B from A∗−∗R B. Moreover, ‘∗’ is a wildcard that may stand for all three edge marks (tail, head, circle) that appear in PAGs. Further, the ‘!’ on A∗−∗! B asserts that both
A∗−∗L B and A∗−∗R B are true, and the empty middle mark on A∗−∗B says that A ∈ adj(B, M(G)).
Lastly, the ‘?’ on A∗−∗? B doesn’t promise anything. Non-circle edge marks (here potentially hidden by the ‘∗’ symbol) still convey their standard meaning of ancestorship and non-ancestorship, and the absence of an edge between A and B still asserts that A /∈ adj(B, M(G)). We call a PAG C(G) whose edges are extended with middle marks a LPCMCI-PAG for M(G), see Sec. S3 in the SM for a more formal deﬁnition. The ‘∗’ symbol is also used as a wildcard for the ﬁve middle marks.
Note that we are not changing the quantity we are trying to estimate, this is still the PAG P(G) as explained in Sec. 2.3. The notion of LPCMCI-PAGs is used in intermediate steps of LPCMCI and has two advantages. First, A∗−∗B is reserved for A ∈ adj(B, M(G)) and thus has an unambiguous meaning at every point of the algorithm, unlike for (SVAR-)FCI and (SVAR-)RFCI. In fact, even if LPCMCI is interrupted at any arbitrary point it still yields a graph with unambiguous and sound causal interpretation. Second, middle marks carry ﬁne-grained causal information that allows to determine deﬁnite adjacencies early on:
Lemma 1 (Ancestor-parent-rule). In LPCMCI-PAG C(G) one may replace 1.) A→! B by A→B, 2.)
A→L B for A > B by A→B, and 3.) A→R B for A < B by A→B.
When LPCMCI has converged all middle marks are empty and hence C(G) is a PAG. We choose a total order consistent with time order, namely X i t iff τ > 0 or τ = 0 and i < j. Lagged links can then be initialized with edges ◦→L (contemporaneous links as ◦−◦? ). t−τ < X j 3.3 Orientations rules for LPCMCI-PAGs
We now discuss rules for edge orientation in LPCMCI-PAGs. For this we need a deﬁnition:
Deﬁnition 1 (Weakly minimal separating sets). In MAG M(G) let A and B be m-separated by S.
The set S is a weakly minimal separating set of A and B if i) it decomposes as S = S1 ˙∪ S2 with
S1 ⊆ an({A, B}, M(G)) such that ii) if S (cid:48) = S1 ˙∪ S (cid:48) 2 ⊆ S2 m-separates A and B then
S (cid:48) 2 = S2. The pair (S1, S2) is called a weakly minimal decomposition of S. 2 with S (cid:48)
This generalizes the notion of minimal separating sets, for which additionally S1 = ∅. Since LPCMCI is designed to extend conditioning sets by known ancestors, the separating sets it ﬁnds are in general not minimal. However, they are still weakly minimal. The following Lemma, a generalization of the unshielded triple rule [Colombo et al., 2012], is central to orientations in LPCMCI-PAGs:
Lemma 2 (Strong unshielded triple rule). Let A∗−∗∗ B∗−∗∗ C be an unshielded triple in LPCMCI-PAG
C(G) and SAC the separating set of A and C. 1.) If i) B ∈ SAC and ii) SAC is weakly minimal, then B ∈ an({A, C}, G). 2.) Let TAB ⊆ an({A, B}, M(G)) and TCB ⊆ an({C, B}, M(G)) be arbitrary. If i) B /∈ SAC, ii) A and B are not m-separated by SAC ∪ TAB \ {A, B}, iii) C and B are not m-separated by SAC ∪ TCB \ {C, B}, then B /∈ an({A, C}, G). The conditioning sets in ii) and iii) may be intersected with the past and present of the later variable.
Part 2.) of this Lemma generalizes the FCI collider rule R0 to rule R0(cid:48) (of which there are several variations when restricting to particular middle marks), and part 1.) generalizes R1 to R1(cid:48). Rules R2 5
and R8 generalize trivially to triangles in C(G) with arbitrary middle marks, giving rise to R2(cid:48) and
R8(cid:48). Rules R3, R9 and R10 are generalized to R3(cid:48), R9(cid:48) and R10(cid:48) by adding the requirement that the middle variables of certain unshielded colliders are in the separating set of the two outer variables, and that these separating sets are weakly minimal. Since there are no selection variables, rules R5, R6 and R7 are not applicable. Rule R4(cid:48) generalizes the discriminating path rule [Colombo et al., 2012] of RFCI. These rules are complemented by the replacements speciﬁed in Lemma 1 and a rule for updating middle marks. Precise formulations of all rules are given in Sec. S4 of the SM.
We stress that these rules are applicable at every point of the algorithm and that they may be executed in any order. This is different from the (SVAR-)FCI orientation phase which requires that prior to orientation a PAG has been found. Also (SVAR-)RFCI orients links only once an RFCI-PAG has been determined, and both (SVAR-)FCI and (SVAR-)RFCI require that all colliders are oriented before applying their other orientation rules. 3.4 The LPCMCI algorithm t−τ , X j
LPCMCI is a constraint-based causal discovery algorithm that utilizes the ﬁndings of Sec. 3.1 to increase the effect size of CI tests. High-level pseudocode is given in Algorithm 1. After initializing
C(G) as a complete graph, the algorithm enters its preliminary phase in lines 2 to 4. This involves calls to Algorithm S2 (pseudocode in Sec. S5 of the SM), which removes many (but in general not all) false links and, while doing so, repeatedly applies the orientation rules introduced in the previous section. These rules identify a subset of the (non-)ancestorships in G and accordingly mark them by heads or tails on edges in C(G). This information is then used as prescribed by the two design principles of LPCMCI that were explained in Sec. 3.1: The non-ancestorships further constrain the conditioning sets S of subsequent CI tests, the ancestorships are used to extend these sets to
S ∪ Sdef where Sdef = pa({X i t }, C(G)) are the by then known parents of those variables whose independence is being tested. All parentships marked in C(G) after line 3 are remembered and carried over to an elsewise re-initialized C(G) before the next application of Alg. S2. Conditioning sets can then be extended with known parents already from the beginning. The purpose of this iterative process is to determine an accurate subset of the parentships in G. These are then passed on to the ﬁnal phase in lines 5 - 6, which starts with one ﬁnal application of Alg. S2. At this point there may still be false links because Alg. S2 may fail to remove a false link between variables
X i t if neither of the two is an ancestor of the other. This is the purpose of Algorithm S3 (pseudocode in Sec. S5 of the SM) that is called in line 6, which thus plays a similar role as the second removal phase in (SVAR-)FCI. Algorithm S3 repeatedly applies orientation rules and uses identiﬁed (non-)ancestorships in the same way as Alg. S2. As stated in the following theorems, LPCMCI will then have found the PAG P(G). Moreover, its output does not depend on the order of the N time series variables X j. The number k of iterations in the preliminary phase is a hyperparameter and we write LPCMCI(k = k0) when specifying k = k0. Stationarity is enforced at every step of the algorithm, i.e., whenever an edge is removed or oriented all equivalent time shifted edges (called
‘homologous’ in [Entner and Hoyer, 2010]) are removed too and oriented in the same way. t−τ and X j
Algorithm 1 LPCMCI
Require: Time series dataset X = {X1, . . . , XN }, maximal considered time lag τmax, signiﬁcance level α, CI test CI(X, Y, S), non-negative integer k t−τ ◦→L X j
Remove edges and apply orientations using Algorithm S2
Repeat line 1, orient edges as X i 1: Initialize C(G) as complete graph with X i 2: for 0 ≤ l ≤ k − 1 do 3: t−τ →∗ X j 4: 5: Remove edges and apply orientations using Algorithm S2 6: Remove edges and apply orientations using Algorithm S3 7: return PAG C(G) = P(G) = P(G)τmax t−τ →? X j t if X i statAO t (0 < τ ≤ τmax) and X i t−τ ◦−◦? X j t (τ = 0) t was in C(G) after line 3
Theorem 2 (LPCMCI is sound and complete). Assume that there is a process as in eq. (1) without causal cycles, which generates a distribution P that is faithful to its time series graph G. Further assume that there are no selection variables, and that we are given perfect statistical decisions about
CI of observed variables in P . Then LPCMCI is sound and complete, i.e., it returns the PAG P(G). 6
Theorem 3 (LPCMCI is order-independent). The output of LPCMCI does not depend on the order of the N time series variables X j (the j-indices may be permuted). 3.5 Back to the motivational example in Fig. 1
The ﬁrst iteration (l = 0) of LPCMCI also misses the links Yt−1→Zt and ﬁnds Xt∗−∗Yt in only few realizations (we here suppress middle marks for simpler notation), but orientations are already improved as compared to SVAR-FCI. Rule R1(cid:48) applied after p = 1 orients the auto-links Xt−1→Xt and Yt−1→Yt. This leads to the parents sets pa(Xt, C(G)) = {Xt−1} and pa(Yt, C(G)) = {Yt−1}, which are then used as default conditions in subsequent CI tests. This is relevant for orientation rule R0(cid:48) that tests whether the middle node of the unshielded triple Xt−1◦→Xt◦−◦Yt does not lie in the separating set of Xt−1 and Yt. Due to the extra conditions the relevant partial correlation
ρ(Xt−1; Yt|Xt, Xt−2, Yt−1) now correctly turns out signiﬁcant. This identiﬁes Xt as collider and (since the same applies with X and Y swapped) the bidirected edge Xt↔Yt is correctly found. The next iteration (l = 1) then uses the parents obtained in the l = 0 iteration, here the autodepen-dencies plus the (false) link Yt−2→Zt, as default conditions already from the beginning for p = 0.
While the correlation ρ(Xt; Yt) used by SVAR-FCI is often non-signiﬁcant, the partial correlation
ρ(Xt; Yt|Xt−1, Yt−1) is signiﬁcant since the autocorrelation noise was removed and effect size in-creased (indicated as link color in Fig. 1) in accord with Theorem 1. Also the lagged link is correctly detected because ρ(Yt−1; Zt|Yt−2, Zt−1) is larger than ρ(Yt−1; Zt|Yt−2). The false link Yt−2→Zt is now removed since the separating node Yt−1 was retained. This wrong parentship is then also not used for default conditioning anymore. Orientations of bidirected links are facilitated as before and
Yt−1→Zt is oriented by rule R1(cid:48). 4 Numerical experiments
We here compare LPCMCI to the SVAR-FCI and SVAR-RFCI baselines with CI tests based on linear partial correlation (ParCorr), for an overview of further experiments presented in the SM see the end of this section. To limit runtime we constrain the cardinality of conditioning sets to 3 in the second removal phase of SVAR-FCI and in Alg. S3 of LPCMCI (excluding the default conditions Sdef , i.e.,
|S| ≤ 3 but |S ∪ Sdef | > 3 is allowed). We generate datasets with this variant of the SCM in eq. (1): t = ajV j
V j t−1 + (cid:80) icifi(V i t−τi
) + ηj t for j ∈ {1, . . . , ˜N } (3)
Autocorrelations aj are drawn uniformly from [max(0, a − 0.3), a] for some a as indicated in Fig. 2.
For each model we in addition randomly choose L = ˜N linear (i.e., fi = id) cross-links with the corresponding non-zero coefﬁcients ci drawn uniformly from ±[0.2, 0.8]. 30% of these links are contemporaneous (i.e., τi = 0), the remaining τi are drawn from [1, pts = 3]. The noises j ) are iid with σj drawn from [0.5, 2]. We only consider stationary models. From the ˜N
ηj ∼ N (0, σ2 variables of each model we randomly choose N = (cid:100)(1 − λ) ˜N (cid:101) for λ = 0.3 as observed. As discussed in Sec. 2.3, the true PAG P(G) of each model depends on τmax. In Fig. 2 we show the relative average numbers of directed, bidirected, and (partially) unoriented links. For performance evaluation true positive (= recall) and false positive rates for adjacencies are distinguished between lagged cross-links (i (cid:54)= j), contemporaneous, and autodependency links. False positives instead of precision are shown to investigate whether methods can control these below the α-level. Orientation performance is evaluated based on edgemark recall and precision. In Fig. 2 we also show the average of minimum absolute ParCorr values as an estimate of effect size and the average maximum cardinality of all tested conditioning sets. All metrics are computed across all estimated graphs from 500 realizations of the model in eq. (3) at time series length T . The average and 90% range of runtime estimates were evaluated on Intel Xeon Platinum 8260.
In Fig. 2A we show LPCMCI for k = 0, . . . , 4 against increasing autocorrelation a. Note that a = 0 implies a different true PAG than a > 0. The largest gain, both in recall and precision, comes already from k = 0 to k = 1. For higher k LPCMCI maintains false positive control and orientation precision, and improves recall before converging at k = 4. The gain in recall is largely attributable to improved effect size. On the downside, larger k increase cardinality (estimation dimension) and runtime. However, the runtime increase is only marginal because later l-steps converge faster and the implementation caches CI test results. Fig. 2B shows a comparison of LPCMCI with SVAR-FCI and SVAR-RFCI against autocorrelation, depicting LPCMCI for k = 0 and k = 4. Already 7
Figure 2: Results of numerical experiments for (A) LPCMCI(k) for different k, LPCMCI compared to SVAR-FCI and SVAR-RFCI for (B) varying autocorrelation, for (C) number of variables N , and for (D) maximum time lag τmax (other parameters indicated in upper right of each panel).
LPCMCI(k = 0) has higher adjacency and orientation recall than SVAR-FCI and SVAR-RFCI for increasing autocorrelation while they are on par for a = 0. This comes at the price of precision, especially lagged orientation precision. LPCMCI(k = 4) has more than 0.4 higher contemporaneous orientation recall and still 0.1 higher lagged orientation recall than SVAR-FCI and SVAR-RFCI.
Lagged precision is higher for high autocorrelation and contemporaneous precision is slightly lower.
LPCMCI(k = 4) maintains high recall for increasing autocorrelation a ≥ 0.5 while SVAR-FCI and
SVAR-RFCI’s recall sharply drops. These results can be explained by improved effect size while the increased cardinality (≈ 5) of separating sets is still moderate compared to the sample size T = 500.
LPCMCI(k = 0) has similar low runtime as SVAR-RFCI, for LPCMCI(k = 4) it is comparable to that of SVAR-FCI. In Fig. 2C we show results for different numbers of variables N . As expected, all methods have decreasing adjacency and orientation recall for higher N , but LPCMCI starts at a much higher level. For N = 3 both SVAR-FCI and SVAR-RFCI cannot control false positives for lagged links while for larger N false positives become controlled. The reason is the interplay of ill-calibrated
CI tests for smaller N due to autocorrelation (inﬂating false positives) with sequential testing for larger N (reducing false positives), as has been discussed in [Runge et al., 2019b, Runge, 2020] for the similar PC algorithm [Spirtes and Glymour, 1991]. LPCMCI better controls false positives here, its decreasing recall can be explained by decreasing effect size and increasing cardinality. Runtime becomes slightly larger than that of SVAR-FCI for larger N . Fig. 2D shows results for different maximum time lags τmax. Note that these imply different true PAGs, especially since further lagged links appear for larger τmax. All methods show a decrease in lagged recall and precision, whereas contemporaneous recall and precision stay almost constant. For SVAR-FCI there is an explosion of runtime for higher τmax due to excessive searches of separating sets in its second removal phase. In
LPCMCI this is partially overcome since the sets that need to be searched through are more restricted.
In Sec. S9 in the SM we present further numerical experiments. This includes more combinations of model parameters N, a, λ, T , nonlinear models together with the nonparametric GPDC CI test
[Runge et al., 2019b], and a comparison to a residualization approach. In these cases the results are largely comparable to those above regarding relative performances. For non-time series models we
ﬁnd that, although all ﬁndings of Secs. 3.1 through 3.4 still apply, LPCMCI(k) is on par with the baselines for k = 0 while it shows inﬂated false positives for k = 4. Similarly, for models of discrete variables together with a G-test of conditional independence LPCMCI(k) performs comparable to 8
the baselines for k = 0 and gets worse with increasing k. A more detailed analysis of LPCMCI’s performance in these two cases, non-time series and discrete models, is subject to future research. 5 Application to real data
We here discuss an application of LPCMCI to average daily discharges of rivers in the upper
Danube basin, measurements of which are made available by the Bavarian Environmental Agency at https://www.gkd.bayern.de. We consider measurements from the Iller at Kempten (X), the Danube at
Dillingen (Y ), and the Isar at Lenggries (Z). While the Iller discharges into the Danube upstream of Dillingen with the water from Kempten reaching Dillingen within about a day, the Isar reaches the Danube downstream of Dillingen. We thus expect a contemporaneous link Xt→Yt and no direct causal relationships between the pairs X, Z and Y, Z. Since all variables may be confounded by rainfall or other weather conditions, this choice allows to test the ability of detecting and distinguishing directed and bidirected links. To keep the sample size comparable with those in the simulation studies we restrict to the records of the past three years (2017-2019). We set τmax = 2 and apply LPCMCI(k) for k = 0, . . . , 4 and α = 0.01 with ParCorr CI tests. Restricting the discussion to contemporaneous links, LPCMCI correctly ﬁnds Xt→Yt for k = 1, . . . , 4 and for k = 0 wrongly ﬁnds Xt↔Yt. For all k it infers the bidirected link Xt↔Zt, which is plausible due to confounding by weather. For k = 3, 4
LPCMCI wrongly ﬁnds the directed link Zt→Yt, which should either be absent or bidirected. The results are similar for α = 0.05, with the difference that LPCMCI then always correctly ﬁnds Xt→Yt but wrongly infers Zt→Yt also for k = 1, 2. In comparison, SVAR-FCI with ParCorr CI tests ﬁnds the contemporaneous adjacencies Yt◦−◦Xt◦−◦Zt for α = 0.01, 0.03, 0.05, 0.08, 0.1, 0.3, 0.5 and
Yt←◦Xt◦−◦Zt for α = 0.8. The estimated PAGs are shown in Sec. S10 of the SM.
We note that since the discharge values show extreme events caused by heavy rainfall, the assump-tion of stationarity is expected to be violated. For other analyses of the dataset of average daily discharges see [Asadi et al., 2015, Engelke and Hitz, 2020, Mhalla et al., 2020, Gnecco et al., 2020].
More detailed applications to and analyses of LPCMCI on real data are subject to future research. 6 Discussion and future work
Major strengths of LPCMCI lie in its signiﬁcantly improved recall as compared to the SVAR-FCI and SVAR-RFCI baselines for autocorrelated continuous variables, which grows with autocorrelation and is particularly strong for contemporaneous links. At the same time LPCMCI (for k > 0) has better calibrated CI test leading to better false positive control than the baselines. We cannot prove false positive control, but are not aware of any such proof for other constraint-based algorithms in the challenging latent, nonlinear, autocorrelated setting considered here. A general weakness, which also applies to (SVAR-)FCI and (SVAR-)RFCI, is the faithfulness assumption. If violated in practice this may lead to wrong conclusions. We did not attempt to only assume the weaker form of adjacency-faithfulness [Ramsey et al., 2006], which to our knowledge is however generally an open problem in the causally insufﬁcient case. Moreover, like all constraint-based methods, our method cannot distinguish all members of Markov equivalence classes like methods based on the SCM frame-work such as e.g. TS-LiNGAM [Hyvärinen et al., 2008] and TiMINo [Peters et al., 2013] do. These, however, restrict the type of dependencies. Concluding, this paper shows how causal discovery in au-tocorrelated time series beneﬁts from increasing the effect size of CI tests by including causal parents in conditioning sets. The LPCMCI algorithm introduced here implements this idea by entangling the removal and orientation of edges. As demonstrated in extensive simulation studies, LPCMCI achieves much higher recall than the SVAR-FCI and SVAR-RFCI baselines for autocorrelated continuous variables. We further presented novel orientation rules and an extension of graphical terminology by the notions of middle marks and weakly minimal separating sets. Code for all studied methods is provided as part of the tigramite Python package at https://github.com/jakobrunge/tigramite. In future work one may relax assumptions of LPCMCI to allow for selection bias and non-stationarity.