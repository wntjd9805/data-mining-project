Abstract
Matrix square roots and their inverses arise frequently in machine learning, e.g., when sampling from high-dimensional Gaussians N (0, K) or “whitening” a vector b against covariance matrix K. While existing methods typically require O(N 3) computation, we introduce a highly-efﬁcient quadratic-time algorithm for comput-ing K1/2b, K−1/2b, and their derivatives through matrix-vector multiplication (MVMs). Our method combines Krylov subspace methods with a rational approxi-mation and typically achieves 4 decimal places of accuracy with fewer than 100
MVMs. Moreover, the backward pass requires little additional computation. We demonstrate our method’s applicability on matrices as large as 50,000 × 50,000— well beyond traditional methods—with little approximation error. Applying this increased scalability to variational Gaussian processes, Bayesian optimization, and Gibbs sampling results in more powerful models with higher accuracy.
In particular, we perform variational GP inference with up to 10,000 inducing points and perform Gibbs sampling on a 25,000-dimensional problem. 1

Introduction
High-dimensional Gaussian distributions arise frequently in machine learning, especially in the context of Bayesian modeling. For example, the prior of Gaussian process models is given by a multivariate Gaussian distribution N (0, K) governed by an N × N symmetric positive deﬁnite kernel matrix K. Historically, O(N 3) computation and O(N 2) memory requirements have limited the tractability of inference for high-dimensional Gaussian latent variable models.
A growing line of research aims to reformulate many common covariance matrix operations—such as linear solves and log determinants—as iterative optimizations involving matrix-vector multiplications (MVMs) [e.g. 3, 11, 16, 29, 79]. MVM approaches have two primary advantages: 1) the covariance matrix need not be explicitly instantiated (so only O(N ) memory is required) [11, 16, 79]; and 2) MVMs utilize GPU acceleration better than direct methods like Cholesky [3, 29]. Thus MVM methods can be scaled to much larger covariance matrices.
In this paper, we propose an MVM method that addresses a common computational bottleneck for high-dimensional Gaussians: computing K±1/2b. This operation occurs frequently in Gaussian process models and inverse problems. For example, if b ∼ N (0, I), then K 1 2 b ∼ N (0, K). This operation appears frequently in Bayesian optimization [e.g. 23, 42, 74, 80] and Gibbs sampling [e.g.
∗This work was conducted while David Eriksson was at Uber AI. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
6, 8, 33]. K− 1 2 b can be used to project parameters into a “whitened” coordinate space [50, 54]—a transformation that accelerates the convergence of variational Gaussian process approximations. To make these computations more efﬁcient and scalable, we make the following contributions:
• We introduce a MVM approach for computing K±1/2b. The approach uses an insight from
Hale et al. [35] that expresses the matrix square root as a sum of shifted matrix inverses.
• To efﬁciently compute these shifted inverses, we leverage a modiﬁed version of the MINRES algorithm [59] that performs multiple shifted solves through a single iteration of MVMs. We demonstrate that, surprisingly, multi-shift MINRES (msMINRES) convergence can be accelerated with a single preconditioner despite the presence of multiple shifts. Moreover, msMINRES only requires O(N ) storage when used in conjunction with partitioned MVMs
[11, 79]. Achieving 4 or 5 decimal places of accuracy typically requires fewer than 100 matrix-vector multiplications, which can be highly accelerated through GPUs.
• We derive a scalable backward pass for K±1/2b that enables our approach to be used as part of learning and optimization.
• We apply our K−1/2b and K1/2b routines to three applications: 1) variational Gaussian processes with up to M = 104 inducing points (where we additionally introduce a O(M 2)
MVM-based natural gradient update); 2) sampling from Gaussian process posteriors in the context of Bayesian optimization with up to 50,000 candidate points; and 3) an image reconstruction task where we perform Gibbs sampling in 25,600 dimensions.
Code examples for the GPyTorch framework are available at bit.ly/ciq_variational and bit.ly/ciq_sampling. 2