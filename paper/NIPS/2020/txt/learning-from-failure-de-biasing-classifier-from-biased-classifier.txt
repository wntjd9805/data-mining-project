Abstract
Neural networks often learn to make predictions that overly rely on spurious corre-lation existing in the dataset, which causes the model to be biased. While previous work tackles this issue by using explicit labeling on the spuriously correlated attributes or presuming a particular bias type, we instead utilize a cheaper, yet generic form of human knowledge, which can be widely applicable to various types of bias. We ﬁrst observe that neural networks learn to rely on the spurious correlation only when it is “easier” to learn than the desired knowledge, and such reliance is most prominent during the early phase of training. Based on the obser-vations, we propose a failure-based debiasing scheme by training a pair of neural networks simultaneously. Our main idea is twofold; (a) we intentionally train the
ﬁrst network to be biased by repeatedly amplifying its “prejudice”, and (b) we debias the training of the second network by focusing on samples that go against the prejudice of the biased network in (a). Extensive experiments demonstrate that our method signiﬁcantly improves the training of network against various types of biases in both synthetic and real-world datasets. Surprisingly, our framework even occasionally outperforms the debiasing methods requiring explicit supervision of the spuriously correlated attributes. 1

Introduction
When trained on carefully curated datasets, deep neural networks achieve state-of-the-art perfor-mances on many tasks in artiﬁcial intelligence, including image classiﬁcation [11], object detection
[8], and speech recognition [9]. On the other hand, neural networks often dramatically fail when trained on a highly biased dataset, by learning the unintended decision rule that works well only on the dataset being trained on. For instance, it is widely known that object classiﬁcation datasets suffer from such misleading correlations [23, 29]. As an example, suppose that the “boat” is the only object category appearing in the images with the “water” background. When trained on a dataset with such bias, neural networks often learn to make predictions using the unintended decision rule based on the background of images, whereas a learner intended to learn the decision rule based on the object in the images.
To train a debiased model that captures the “intended correlation” from such biased datasets, recent approaches focus on how to utilize various types of human supervision effectively. One of the most popular forms of such supervision is an explicit label that indicates the misleadingly correlated attribute [14, 18, 24]. For instance, Kim et al. [14], Li and Vasconcelos [18] consider training a model to classify digits (instead of misleadingly correlated color) from the Colored MNIST dataset, under the setup where RGB values for coloring digits are given as side information. Another line of research focuses on developing algorithms tailored to a domain-speciﬁc type of bias in the target dataset, whose existence and characteristics are diagnosed by human experts [7, 25, 4, 2]. For instance, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Geirhos et al. [7] diagnose that ImageNet-trained classiﬁers are biased toward texture instead of a presumably more human-aligned notion of shapes, and use this takeaway to construct an augmented dataset to train shape-oriented classiﬁers.
Acquiring human supervision on the bias, however, is often a dauntingly laborious and expensive task. Gathering explicit labels of such misleadingly correlated attributes requires manual labeling by the workers that have a clear understanding of the underlying bias. Collecting expert knowledge of a human-perceived bias (e.g., texture bias) takes even more effort, as it requires a careful ablation study on the classiﬁers trained on biased datasets, e.g., Geirhos et al. [7] synthesize data via style transfer
[6] to discover the existence of texture bias. Hence, an approach to train a debiased classiﬁer without relying on such expensive supervision is warranted.
Contribution. In this paper, we propose a failure-based debiasing scheme, coined Learning from
Failure (LfF). Our scheme does not require expensive supervision on the bias, such as explicit labels of misleadingly correlated attributes, or bias-tailored training technique. Instead, our method utilizes a cheaper form of human knowledge, leveraging the following intriguing observations on neural networks that are being trained on biased datasets.
We ﬁrst observe that a biased dataset does not necessarily lead the model to learn the unintended decision rule; the bias negatively affects the model only when the bias attribute is “easier” to learn than the target attribute (Section 2.2). For the bias that negatively affects the model, we also observe that samples aligned with the bias show distinct loss trajectories in the training phase compared to samples conﬂicting with the bias. To be more speciﬁc, the classiﬁer learns to ﬁt samples aligned with the bias during the early stage of training and learns samples conﬂicting with the bias later (Section 2.3). The latter observation lines up with recent ﬁndings on training dynamics of deep neural networks [1, 21, 17]; networks tend to defer learning hard concepts, e.g., samples with random labels, to the later phase of training.
Based on the ﬁndings, we propose the following debiasing scheme, LfF. We simultaneously train two neural networks, one to be biased and the other to be debiased. Speciﬁcally, we train a “biased” neural network by amplifying its early-stage predictions. Here, we employ generalized cross entropy loss [28] for the biased model to focus on easy samples, which are expected to be samples aligned with bias. In parallel, we train a “debiased” neural network by focusing on samples that the biased model struggles to learn, which are expected to be samples conﬂicting with the bias. To this end, we re-weight training samples using the relative difﬁculty score based on the loss of the biased model and the debiased model (Section 3).
We show the effectiveness of LfF on various biased datasets, including Colored MNIST [14, 18] with color bias, Corrupted CIFAR-10 [12] with texture bias, and CelebA [19] with gender bias. In addition, we newly construct a real-world dataset, coined biased action recognition (BAR), to resolve the lack of realistic evaluation benchmark for debiasing schemes. In all of the experiments, our method succeeds in training a debiased classiﬁer. In particular, our method improves the accuracy of the unbiased evaluation set by 35.34% → 63.39%, 17.93% → 31.66%, for the Colored MNIST and Corrupted CIFAR-101 datasets, respectively, even when 99.5% of the training samples are bias-aligned (Section 4). 2 A closer look at training deep neural networks on biased datasets
In this section, we describe two empirical observations on training the neural networks with a biased dataset. These observations serve as a key intuition for designing and understanding our debiasing algorithm. We ﬁrst provide a formal description of biased datasets in Section 2.1. Then we provide our empirical observations in Section 2.2 and Section 2.3. 2.1 Setup
Consider a dataset D where each input x can be represented by a set of (possibly latent) attributes
{a1, . . . , ak} for ai ∈ Ai that describes the input. The goal is to train a predictor f that belongs to a set of intended decision rules Ft, consisting of decision rules that correctly predict the target attribute y = at ∈ At. We say that a dataset D is biased, if (a) there exists another attribute ab (cid:54)= y that is highly correlated to the target attribute y (i.e., H(y|ab) ≈ 0), and (b) one can settle an unintended decision rule gb /∈ Ft that correctly classiﬁes ab. We denote such an attribute ab by a bias attribute. 2
(a) Colored MNIST (b) Corrupted CIFAR-101
Figure 1: Illustration of bias-aligned samples for Colored MNIST, and Corrupted CIFAR-101 datasets.
Table 1: Accuracy of the unbiased evaluation set with varying choice of (target, bias) attribute pair for the Colored MNIST and Corrupted CIFAR-101,2 datasets. Accuracy and Accuracy∗ denotes the performance of the model trained on the biased and unbiased training set, respectively.
Dataset
Colored MNIST
Target
Color
Digit
Bias
Digit
Color
Corrupted CIFAR-101
Corrupted CIFAR-102
Corruption
Object
Object
Corruption
Corruption
Object
Object
Corruption
Accuracy Accuracy∗ Relative drop 99.97±0.04 50.34±0.16 98.34±0.26 22.72±0.87 98.64±0.20 21.07±0.29 100.0±0.00 96.41±0.07 99.62±0.03 80.00±0.01 99.80±0.01 79.65±0.11
-0.03%
-47.79%
-1.28%
-71.60%
-1.16%
-73.56%
In biased datasets with a bias attribute ab, we say that a sample is bias-aligned whenever it can be correctly classiﬁed by the unintended decision rule gb, and bias-conﬂicting whenever it cannot be correctly classiﬁed by gb.
Throughout the paper, we consider two types of evaluation datasets: the unbiased and bias-conﬂicting evaluation sets. We construct the unbiased evaluation set in a way that the target and bias attributes are uncorrelated. To this end, the unbiased evaluation set is constructed to have the same number of samples for every possible value of (at, ab). We simply construct the bias-conﬂicting evaluation set by excluding bias-aligned samples from the unbiased evaluation set.
Here, we illustrate the examples of biased datasets using the datasets considered for the experiment in Section 2.2 and 2.3, i.e., Colored MNIST and Corrupted CIFAR-10.
Colored MNIST. We inject color with random perturbation into the MNIST dataset [16] designed for digit classiﬁcation, resulting in a dataset with two attributes: Digit and Color. In the case of (at, ab) = (Digit, Color), a set of intended decision rules Ft consists of decision rules that correctly classify images based on the Digit of the images. Here, a decision rule based on other attributes, e.g.
Color, is considered as an unintended decision rule. Figure 1(a) illustrates examples of bias-aligned samples, which can be correctly classiﬁed by an unintended decision rule based on Color.
Corrupted CIFAR-10. This dataset is generated by corrupting the CIFAR-10 dataset [15] designed for object classiﬁcation, following the protocols proposed by Hendrycks and Dietterich [12]. The resulting dataset consists of two attributes, i.e., category of the Object and type of Corruption used. Similar to the Colored MNIST dataset, this results in two possible choices for the target and bias attribute. We use two sets of protocols for corruption to build two datasets, namely the Corrupted
CIFAR-101 and the Corrupted CIFAR-102 datasets. See Figure 1(b) for corruption-biased examples.
A detailed description of the datasets is provided in Appendix A. 2.2 Not all biases are malignant
Our ﬁrst observation is that training a classiﬁer with a biased dataset does not necessarily lead to learning an unintended decision rule. Instead, the bias in the dataset negatively affects the prediction only if the bias is easier to be captured by the learned classiﬁer. In Table 1, we report the accuracy of the classiﬁers for the unbiased evaluation set. We ﬁrst note the existence of benign bias, i.e., there are cases where the classiﬁer has not been affected by the bias. Particularly, when there is a degradation of accuracy with a certain choice of the target and bias attribute, the degradation does not occur with the choice made in reversed order. 3
(a) Colored MNIST, (Digit, Color) (b) Corrupted CIFAR-101, (Object, Corruption)
Figure 2: Illustration of neural network training on the biased datasets. For each dataset, the left and the right plots corresponds to training on the malignant bias and the benign bias. Our choice of (target, bias) attribute for the malignant bias is provided in brackets. For the benign bias, we choose the target and the bias attribute in reversed order.
From such an observation, we now deﬁne two types of bias: malignant and benign. For a biased dataset D with a target attribute y and a bias attribute ab, we say this bias is malignant if a model trained on D suffers performance degradation on unbiased evaluation set compared to one trained on another dataset which is not biased. In contrast, we say bias is benign if a model trained on D does not suffer such performance degradation. We interpret this observation as follows: the bias attribute inducing malignant bias is “easier” to learn than the target attribute, e.g., Color is easier to learn than Digit. To be speciﬁc, the classiﬁer establishes its decision rule by relying on either (a) the intended correlation from the target or (b) the spurious correlation from the bias attribute. If (a) is harder to leverage than (b), the bias becomes malignant since the classiﬁer learns unintended correlation. Otherwise, the classiﬁer learns the correct correlation, hence the bias becomes benign. 2.3 Malignant bias is learned ﬁrst
Next, we observe that the loss dynamics of bias-aligned samples and bias-conﬂicting samples during the training phase are in stark contrast, given that the bias is malignant. The loss of bias-aligned samples quickly declines to zero, but the loss of bias-conﬂicting samples ﬁrst increases and starts decreasing after the loss of bias-aligned samples reaches near zero.
In Figure 2, we observe a signiﬁcant difference between training dynamics on the malignant and the benign bias, consistently over the considered datasets. In the case of training under malignant bias, the training loss of the bias-conﬂicting samples is higher than the loss of the bias-aligned samples, and the gap is more signiﬁcant during the early stages. In contrast, they are almost indistinguishable when trained under a benign bias.
We make an interesting connection between our observation and the observation made by Arpit et al.
[1] on training neural networks on datasets with noisy labels. Arpit et al. [1] analyzed the training dynamics of the neural network on noisy datasets. They empirically demonstrate the preference of neural networks for easy concepts, e.g., common patterns in samples with correct labels, over the hard concepts, e.g., random patterns in samples with incorrect labels. One can interpret our results similarly; since the malignant bias attributes are easier to learn than the original task, the neural network tends to memorize it ﬁrst. 3 Debiasing by learning from failure (LfF)
Based on our ﬁndings in Section 2, we propose a debiasing algorithm, coined Learning from Failure (LfF), for training neural networks on a biased dataset. At a high level, our algorithm simultaneously trains a pair of neural networks (fB, fD) as follows: (a) intentionally training a model fB to be biased and (b) training a debiased model fD by focusing on the training samples that the biased model struggles to learn. In the rest of this section, we provide details on each component of our debiasing algorithm. We offer a full description of our debiasing scheme in Algorithm 1.
Training a biased model. We ﬁrst describe how we train the biased model. i.e., the model following the unintended decision rule. From our observation in Section 2, we intentionally strengthen the prediction of the model from the early stage of training to make it follow the unintended decision rule. To this end, we use the following generalized cross entropy (GCE) [28] loss to amplify the bias 4
Figure 3: Illustration of training two models (fD, fB) to be debiased and biased, respectively. The biased model optimizes generalized cross entropy (LGCE) loss to amplify bias. The debiased model trains with weighted cross entropy loss leveraging relative difﬁculty. It results in larger weights to bias-conﬂicting samples while training the debiased model.
Algorithm 1 Learning from Failure 1: Input: θB, θD, training set D, learning rate η, number of iterations T 2: Initialize two networks fB(x; θB) and fD(x; θD). 3: for t = 1, · · · , T do 4: 5: 6: 7: end for
Draw a mini-batch B = {(x(b), y(b))}B
Update fB(x; θB) by θB ← θB − η∇θB
Update fD(x; θD) by θD ← θD − η∇θD b=1 from D (cid:80) (cid:80) (x,y)∈B GCE(fB(x), y). (x,y)∈B W(x) · CE(fD(x), y). of the neural network:
GCE(p(x; θ), y) = 1 − py(x; θ)q q where p(x; θ) and py(x; θ) are softmax output of the neural network and its probability assigned to the target attribute of y, respectively. Here, q ∈ (0, 1] is a hyperparameter that controls the degree q = − log p, GCE becomes equivalent to standard of ampliﬁcation. For example, when limq→0 cross entropy (CE) loss. Compared to the CE loss, the gradient of the GCE loss up-weights the gradient of the CE loss for the samples with a high probability py of predicting the correct target attribute as follows: 1−pq
∂GCE(p, y)
∂θ
= pq y
∂CE(p, y)
∂θ
,
Therefore, GCE loss trains a model to be biased by emphasizing the “easier” samples with the strong agreement between softmax output of the neural network and the target, which ampliﬁes the
“prejudice” of the neural network compare to the network trained with CE.
Training a debiased model. While we train a biased model as described earlier, we also train a debiased model simultaneously with the samples using the CE loss re-weighted by the following relative difﬁculty score:
W(x) =
CE(fB(x), y)
CE(fB(x), y) + CE(fD(x), y)
, where fB(x), fD(x) are softmax outputs of the biased and debiased model, respectively. We use this score to indicate how much each sample is likely to be bias-conﬂicting to reﬂect our observation made in Section 2: for bias-aligned samples, biased model fB tends to have smaller loss compare to debiased model fD at the early stage of training, therefore having small weight for training debiased model. In other words, for bias-conﬂicting samples, biased model fB tends to have larger loss compare to debiased model fD, result in large weight (close to 1) for training debiased model. 5
Table 2: Accuracy evaluated on unbiased samples for the Colored MNIST and Corrupted CIFAR-101,2 datasets with varying ratio of bias-aligned samples. We denote bias supervision type by (no supervision), (explicit bias supervision). Best performing results are marked in bold. (bias-tailored supervision), and
Dataset
Ratio (%)
Vanilla
Ours
HEX
REPAIR
Group DRO
Colored
MNIST
Corrupted
CIFAR-101
Corrupted
CIFAR-102 95.0 98.0 99.0 99.5 95.0 98.0 99.0 99.5 95.0 98.0 99.0 99.5 77.63±0.44 62.29±1.47 50.34±0.16 35.34±0.13 45.24±0.22 30.21±0.82 22.72±0.87 17.93±0.66 41.27±0.98 28.29±0.62 20.71±0.29 17.37±0.31 85.39±0.94 80.48±0.45 74.01±2.21 63.39±1.97 59.95±0.16 49.43±0.78 41.37±2.34 31.66±1.18 58.57±1.18 48.75±1.68 41.29±2.08 34.11±2.39 70.44±1.41 62.03±0.24 51.99±1.09 41.38±1.31 21.74±0.27 17.81±0.29 16.62±0.80 15.39±0.13 19.25±0.81 15.55±0.84 14.42±0.51 13.63±0.42 82.51±0.59 72.86±1.47 67.28±1.69 56.40±3.74 48.74±0.71 37.89±0.22 32.42±0.35 26.26±1.06 54.05±1.01 44.22±0.84 38.40±0.26 31.03±0.42 84.50±0.46 76.30±1.53 71.33±1.76 59.67±2.73 53.15±0.53 40.19±0.23 32.11±0.83 29.26±0.11 57.92±0.31 46.12±1.11 39.57±1.04 34.25±0.74
Table 3: Accuracy evaluated on bias-conﬂicting samples for the Colored MNIST and Corrupted
CIFAR-101,2 datasets with varying ratio of bias-aligned samples. We denote bias supervision type by (explicit bias supervision). Best performing (bias-tailored supervision), and (no supervision), results are marked in bold.
Dataset
Ratio (%)
Vanilla
Ours
HEX
REPAIR
Group DRO
Colored
MNIST
Corrupted
CIFAR-101
Corrupted
CIFAR-102 95.0 98.0 99.0 99.5 95.0 98.0 99.0 99.5 95.0 98.0 99.0 99.5 75.17±0.51 58.13±1.63 44.83±0.18 28.15±1.44 39.42±0.20 22.65±0.95 14.24±1.03 10.50±0.71 34.97±1.06 20.52±0.73 12.11±0.29 10.01±0.01 85.77±0.66 80.67±0.56 74.19±1.94 63.49±1.94 59.62±0.03 48.69±0.70 39.55±2.56 28.61±1.25 58.64±1.04 48.99±1.61 40.84±2.06 32.03±2.51 67.75±1.49 58.80±0.28 46.96±1.20 35.05±1.46 14.09±0.31 9.34±0.41 8.37±0.56 6.38±0.08 10.79±0.90 6.60±7.23 5.11±0.59 4.22±0.43 83.26±0.42 73.42±1.42 68.26±1.52 57.27±3.92 49.99±0.92 38.94±0.20 33.05±0.36 26.52±0.94 54.46±1.02 44.63±0.75 38.81±0.20 31.45±0.28 83.11±0.41 74.28±1.93 69.58±1.66 57.07±3.60 49.00±0.45 35.10±0.49 28.04±1.18 24.40±0.28 54.60±0.11 42.71±1.24 37.07±1.02 30.92±0.86 4 Experiments
In this section, we demonstrate the effectiveness of our LfF algorithm proposed in Section 3, and introduce our newly constructed biased action recognition dataset, coined BAR. All experimental results in this section support that LfF successfully trains a debiased classiﬁer, with the knowledge that the bias attribute is learned earlier than the target attribute (instead of explicit supervision for the bias present in the dataset). The classiﬁers trained by LfF consistently outperforms the vanilla classiﬁers (trained without any debiasing procedure) on both the unbiased and bias-conﬂicting evaluation set.
For the experiments in this section, we use MLP with three hidden layers, ResNet-20, and ResNet-18
[11] for the Colored MNIST, Corrupted CIFAR-10, and {CelebA, BAR} datasets, respectively. All results reported in this section are averaged over three independent trials. We provide a detailed description of datasets we considered in Appendix A, B and experimental details in Appendix C. 4.1 Controlled experiments
Baselines. For controlled experiments, we compare the performance of our algorithm with other debiasing algorithms, either presuming a particular bias type or requiring access to additional labels 6
(a) Bias-{aligned, conﬂicting} Colored MNIST (b) Bias-{aligned, conﬂicting} Corrupted CIFAR-101
Figure 4: Learning curves of vanilla, biased, and debiased model for Colored MNIST and Corrupted
CIFAR-101. For each dataset, the left and the right plots correspond to curves for the bias-aligned samples and the bias-conﬂicting samples, respectively.
Table 4: Accuracy evaluated on the unbiased samples and bias-conﬂicting samples of the Colored
MNIST and Corrupted CIFAR-101,2 dataset for ablation of the proposed algorithm. We denote our algorithm using vanilla model as biased model by Ours†. Best performing results are marked in bold.
Dataset
Colored MNIST
Corrupted CIFAR-101
Corrupted CIFAR-102
Vanilla 50.34±0.16 22.72±0.87 20.71±0.29
Unbiased
Ours† 49.90±1.67 25.15±0.63 22.90±0.47
Ours 74.01±2.21 41.37±2.34 41.29±2.08
Vanilla 44.83±0.18 14.24±1.03 12.11±0.29
Bias-conﬂicting
Ours†
Ours 44.44±1.83 17.21±0.69 14.89±0.66 74.19±1.94 39.55±2.56 40.84±2.06 containing information about bias attributes. We consider HEX proposed by Wang et al. [25], which attempts to remove texture bias using the bias-speciﬁc knowledge. We also consider REPAIR and
Group DRO proposed by Li and Vasconcelos [18] and Sagawa et al. [24], requiring explicit labeling of the bias attributes. We provide a detailed description of the employed baselines in Appendix D.
Ratio of the bias-aligned samples. In the ﬁrst set of experiments, we vary the ratio of bias-aligned samples in the training dataset by selecting from {95.0%, 98.0%, 99.0%, 99.5%}. We experiment on Colored MNIST, and Corrupted CIFAR-101,2 datasets with (at, ab) = (Digit, Color), and (at, ab) = (Object, Corruption), respectively.
In Table 2, 3, we report the accuracy evaluated on unbiased samples and bias-conﬂicting samples.
We observe that the proposed method signiﬁcantly outperforms the baseline on all levels of ratio for bias-aligned samples. Most notably, LfF achieves 41.37% accuracy on the unbiased evaluation set for the Corrupted CIFAR-101 dataset with 99% bias-aligned samples, while the vanilla model only achieves 22.72%. In addition, we report the accuracy of unbiased evaluation set and bias-conﬂicting samples with varying levels of difﬁculty for the bias attributes in Appendix E.
Detailed analysis of failure-based debiasing. We further analyze the speciﬁc details of our algo-rithm. To be precise, we ﬁrst investigate the accuracies of the vanilla model and the biased, debiased models fB, fD trained by our algorithm for the bias-aligned and bias-conﬂicting samples. In Figure 4, we plot the training curves of each model. We observe both the vanilla model and the biased model easily achieve 100% accuracy for the bias-aligned samples. On the other hand, the vanilla model shows 50% accuracy for the bias-conﬂicting samples, and the biased model performs close to random guessing. From this result, we can say that our intentionally biased model only exploits the bias attribute without learning the target attribute.
To compare our debiased model to the vanilla model, we start by observing the accuracy gap between the bias-aligned and bias-conﬂicting samples. As described before, while the vanilla model easily achieves 100% accuracy for the bias-aligned samples, it cannot achieve similar performance for the bias-conﬂicting samples. In contrast, our debiased model shows consistent performance (about 80%) for both the bias-aligned and bias-conﬂicting samples. As a result, it indicates that our debiased model successfully learns the intended target attribute, while the predictions of the vanilla model heavily rely on the unintended bias attribute.
Contribution of GCE loss. We also test a variant of our algorithm, where we train the biased model with standard cross entropy instead of GCE. As observed in Figure 4, the model trained with standard cross entropy, denoted by Vanilla, not only exploits the bias attribute but also partially learns the target attribute. Therefore, one can expect that using such a CE-trained model as the biased model can hurt debiasing ability of our algorithm. In Table 4, we report the test performance of our debiased 7
Table 5: Accuracy evaluated on the unbiased samples and bias-conﬂicting samples for the CelebA dataset with Gender as the bias attribute.
Target attribute
Unbiased
Bias-conﬂicting
Vanilla
Ours
Group DRO
Vanilla
Ours
Group DRO
HairColor
HeavyMakeup 70.25±0.35 62.00±0.02 84.24±0.37 66.20±1.21 85.43±0.53 64.88±0.42 52.52±0.19 33.75±0.28 81.24±1.38 45.48±4.33 83.40±0.67 50.24±0.68
Table 6: Class-wise action recognition accuracy on BAR evaluation set. Best performing results are marked in bold.
Action
Climbing
Diving
Fishing
Racing
Throwing
Vaulting
Average
Vanilla
ReBias
Ours 59.05±17.48 77.78±8.32 79.36±4.79 16.56±1.58 51.57±4.54 34.59±2.26 62.69±3.64 54.76±2.38 75.39±3.63 77.27±2.62 80.56±2.19 83.08±1.90 28.62±2.95 28.63±2.71 33.72±0.68 66.92±7.25 65.14±7.21 71.75±3.32 51.85±5.92 59.74±1.49 62.98±2.76 model trained with the CE-trained biased model instead of the GCE-trained model, denoted as Ours†.
As expected, using the CE-trained model to compute relative difﬁculty does not help debiasing model. 4.2 Real-world experiments
CelebA. The CelebA dataset [19] is a multi-attribute dataset for face recognition, equipped with 40 types of attributes for each image. Among 40 attributes, we ﬁnd that Gender and HairColor attributes have a high correlation. Moreover, we observe the attribute Gender is used as a cue for predicting the attribute HairColor. Therefore, we use HairColor as the target and Gender as the bias attribute. Similarly, we do the same thing for HeavyMakeup as the target and Gender as the bias attribute.
In Table 5, we observe our algorithm consistently outperforms the vanilla model while the vanilla model suffers from gender bias existing in the real-world dataset. The accuracy gap between the vanilla model and our model is larger for the bias-conﬂicting samples, which indicates the vanilla model fails to learn the intended target attribute, instead exploits the biased statistic of the dataset.
Notably, our model is comparable to Group DRO, which requires explicit labeling for the bias attribute (unlike ours), and even outperforms on the unbiased evaluation set when the target attribute is HeavyMakeup. This is because Group DRO aims to maximize the worst-case group accuracy, not overall unbiased accuracy.
Biased action recognition dataset. To verify effectiveness of our proposed scheme in a realistic setting, we construct a place-biased action recognition (BAR) dataset with training and evaluation set.
We settle six typical action-place pairs by inspecting imSitu dataset [27], which provides action and place labels. We assign images describing these six typical action-place pairs to the training set and, otherwise, the evaluation set of BAR. BAR is publicly available1, and a detailed description of BAR is in Appendix B.
BAR aims to resolve the lack of realistic evaluation benchmark for debiasing schemes. Since previous debiasing schemes have tackled certain types of bias, they have assumed the correlation between the target and bias attribute to be tangible, which indeed is not available in the case of real-world settings. Such an assumption also makes it hard to verify whether one’s scheme can be applied to a wide range of realistic settings. BAR instead offers evaluation set which is constructed based on the intuition that “a majority of samples that do not match typical action-place pairs are bias-conﬂicting.”
To demonstrate, the evaluation set consists of samples not matching the settled six typical pairs. In the end, we can use this evaluation set, which would be similar to a set of bias-conﬂicting samples to verify our algorithm’s effectiveness.
Table 6 illustrates the test accuracy on the BAR evaluation set. LfF outperforms the vanilla classiﬁer for all action classes. This indicates that LfF encourages the model to train on relatively hard training samples, thereby leading to debiasing the model. In addition, LfF outperforms ReBias [2], which is also free from explicit labeling on the bias attribute, for most action classes except Diving. 1https://github.com/alinlab/BAR 8
(a) Climbing (b) Diving (c) Fishing (d) Racing (e) Throwing (f) Vaulting
Illustration of BAR images of six typical action-place pairs settled as (Climbing,
Figure 5:
RockWall), (Racing, APavedTrack), (Fishing, WaterSurface), (Throwing, PlayingField), and (Vaulting, Sky). The images with red border lines belong to BAR evaluation set, and others belong to BAR training set. (Diving, Underwater), 5