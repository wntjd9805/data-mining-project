Abstract
Task-oriented dialogue is often decomposed into three tasks: understanding user in-put, deciding actions, and generating a response. While such decomposition might suggest a dedicated model for each sub-task, we ﬁnd a simple, uniﬁed approach leads to state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a single, causal language model trained on all sub-tasks recast as a single sequence prediction problem. This allows
SimpleTOD to fully leverage transfer learning from pre-trained, open domain, causal language models such as GPT-2. SimpleTOD improves over the prior state-of-the-art in joint goal accuracy for dialogue state tracking, and our analysis reveals robustness to noisy annotations in this setting. SimpleTOD also improves the main metrics used to evaluate action decisions and response generation in an end-to-end setting: inform rate by 8.1 points, success rate by 9.7 points, and combined score by 7.2 points. 1

Introduction
Conversational AI has been a long-standing area of exploration in computer science, and has gained more attention recently in both academia and industries with the current advances of neural ap-proaches [15]. There are broadly two categories of dialogue. Open-domain dialogue systems focus on making chit-chat, open-ended conversations with humans more natural and engaging. They are usually trained end-to-end using large-scale data from social media [1, 42]. Task-oriented dialogue (TOD) systems accomplish a goal described by a user in natural language. They often use a pipeline approach [46, 58]. The pipeline requires natural language understanding (NLU) for belief state tracking, dialogue management (DM) for deciding which actions to take based on those beliefs, and natural language generation (NLG) for generating responses [50].
Traditionally, each component of task-oriented dialogue systems is trained independently with different supervision. The NLU module is trained on domain and intent labels. The DM module employs dialogue belief and dialogue act labels. The NLG module accesses templatized or natural responses. The modular dependencies of these components can lead to error propagation when information is not provided to subsequent modules in the pipeline [27]. For example, many systems do not consider the entire dialogue history at every turn, but rather rely on the NLU module to pass belief states reliably to following module components [60].
We propose recasting task-oriented dialogue as a simple, causal (unidirectional) language modeling task. We show that such an approach can solve all the sub-tasks in a uniﬁed way using multi-task 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: SimpleTOD is a simple approach to task-oriented dialogue that uses a single causal language model to generate all outputs given the dialogue context and retrieved database search results. The delexicalized response can then be lexicalized into a human-readable response by using information from the belief state and DB search results. maximum likelihood training. The proposed Simple Task-Oriented Dialogue (SimpleTOD) approach enables modeling of the inherent dependencies between the sub-tasks of task-oriented dialogue, by optimizing for all tasks in an end-to-end manner. SimpleTOD also opens the path towards fully leveraging large language models such as GPT-2 [39] for task-oriented dialogue. The success of
SimpleTOD demonstrates a strong connection between the implicit language understanding in the open domain required of high-quality causal language models and the kind of understanding required for a full task-oriented dialogue system.
Evaluation results demonstrate the advantages of SimpleTOD. It achieves 55.76 joint goal accuracy on MultiWOZ, which surpasses all prior work for the dialogue state tracking (i.e. belief state tracking) sub-task. In the setting closest to testing a full task-oriented dialogue system, in which belief states and action decisions are generated rather than retrieved from an oracle, SimpleTOD performance surpasses prior work on each individual action and response generation metric (+8.1 inform rate, +9.7 success rate).
The contributions of this work are summarized as follows:
• SimpleTOD – a state-of-the-art generative model for dialogue state tracking (DST).
• SimpleTOD is also the ﬁrst model to achieve state-of-the-art performance for dialogue state tracking, action decisions, and response generation metrics together in an end-to-end setting.
• Analysis showing SimpleTOD is a robust dialogue state tracker in the presence of noisy-labeled annotations.
• Ablations showing the importance of user/system and endof(segment) tokens.
• Ablations showing the importance of pre-training that also show larger versions of Simple-TOD are not always better for end-to-end MultiWOZ.
• A list of discovered noisy annotations in MultiWOZ 2.1 alongside a cleaned version of the test set, code for training and evaluation, are provided at https://github.com/ salesforce/simpletod 2
2