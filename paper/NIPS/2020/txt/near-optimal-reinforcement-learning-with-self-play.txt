Abstract
This paper considers the problem of designing optimal algorithms for reinforce-ment learning in two-player zero-sum games. We focus on self-play algorithms which learn the optimal policy by playing against itself without any direct super-vision. In a tabular episodic Markov game with S states, A max-player actions and B min-player actions, the best existing algorithm for ﬁnding an approximate
Nash equilibrium requires ˜O(S2AB) steps of game playing, when only highlight-ing the dependency on (S, A, B). In contrast, the best existing lower bound scales as Ω(S(A + B)) and has a signiﬁcant gap from the upper bound. This paper closes this gap for the ﬁrst time: we propose an optimistic variant of the Nash Q-learning algorithm with sample complexity ˜O(SAB), and a new Nash V-learning algorithm with sample complexity ˜O(S(A + B)). The latter result matches the information-theoretic lower bound in all problem-dependent parameters except for a polynomial factor of the length of each episode. In addition, we present a computational hardness result for learning the best responses against a ﬁxed op-ponent in Markov games—a learning objective different from ﬁnding the Nash equilibrium. 1

Introduction
A wide range of modern artiﬁcial intelligence challenges can be cast as a multi-agent reinforcement learning (multi-agent RL) problem, in which more than one agent performs sequential decision making in an interactive environment. Multi-agent RL has achieved signiﬁcant recent success on traditionally challenging tasks, for example in the game of GO [30, 31], Poker [6], real-time strategy games [33, 22], decentralized controls or multiagent robotics systems [5], autonomous driving [27], as well as complex social scenarios such as hide-and-seek [3]. In many scenarios, the learning agents even outperform the best human experts .
Despite the great empirical success, a major bottleneck for many existing RL algorithms is that they require a tremendous number of samples. For example, the biggest AlphaGo Zero model is trained on tens of millions of games and took more than a month to train [31]. While requiring such amount of samples may be acceptable in simulatable environments such as GO, it is not so in other sample-expensive real world settings such as robotics and autonomous driving. It is thus important for us to understand the sample complexity in RL—how can we design algorithms that ﬁnd a near optimal policy with a small number of samples, and what is the fundamental limit, i.e. the minimum number of samples required for any algorithm to ﬁnd a good policy.
Theoretical understandings on the sample complexity for multi-agent RL are rather limited, espe-cially when compared with single-agent settings. The standard model for a single-agent setting is an episodic Markov Decision Process (MDP) with S states, and A actions, and H steps per episode.
The best known algorithm can ﬁnd an (cid:15) near-optimal policy in ˜Θ(poly(H)SA/(cid:15)2) episodes, which matches the lower bound up to a single H factor [1, 8]. In contrast, in multi-agent settings, the opti-mal sample complexity remains open even in the basic setting of two-player tabular Markov games 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[28], where the agents are required to ﬁnd the solutions of the games—the Nash equilibria. The best known algorithm, VI-ULCB, ﬁnds an (cid:15)-approximate Nash equilibrium in ˜O(poly(H)S2AB/(cid:15)2) episodes [2], where B is the number of actions for the other player. The information theoretical lower bound is Ω(poly(H)S(A + B)/(cid:15)2). Speciﬁcally, the number of episodes required for the algorithm scales quadratically in both S and (A, B), and exhibits a gap from the linear dependency in the lower bound. This motivates the following question:
Can we design algorithms with near-optimal sample complexity for learning Markov games?
In this paper, we present the ﬁrst line of near-optimal algorithms for two-player Markov games that match the aforementioned lower bound up to a poly(H) factor. This closes the open problem for achieving the optimal sample complexity in all (S, A, B) dependency. Our algorithm learns by playing against itself without requiring any direct supervision, and is thus a self-play algorithm. 1.1 Our contributions
• We propose an optimistic variant of Nash Q-learning [11], and prove that it achieves sample complexity ˜O(H 5SAB/(cid:15)2) for ﬁnding an (cid:15)-approximate Nash equilibrium in two-player Markov games (Section 3). Our algorithm builds optimistic upper and lower estimates of Q-values, and computes the Coarse Correlated Equilibrium (CCE) over this pair of Q estimates as its execution policies for both players.
• We design a new algorithm—Nash V-learning—for ﬁnding approximate Nash equilibria, and show that it achieves sample complexity ˜O(H 6S(A + B)/(cid:15)2) (Section 4). This improves upon
Nash Q-learning in case min {A, B} > H. It is also the ﬁrst result that matches the minimax lower bound up to only a poly(H) factor. This algorithm builds optimistic upper and lower esti-mates of V -values, and features a novel combination of Follow-the-Regularized-Leader (FTRL) and standard Q-learning algorithm to determine its execution policies.
• Apart from ﬁnding Nash equilibria, we prove that learning the best responses of ﬁxed opponents in Markov games is as hard as learning parity with noise—a notoriously difﬁcult problem that is believed to be computationally hard (Section 5). As a corollary, this hardness result directly implies that achieving sublinear regret against adversarial opponents in Markov games is also computationally hard, a result that ﬁrst appeared in [25]. This in turn rules out the possibility of designing efﬁcient algorithms for ﬁnding Nash equilibria by running no-regret algorithms for each player separately.
In addition to above contributions, this paper also features a novel approach of extracting certi-ﬁed policies—from the estimates produced by reinforcement learning algorithms such as Nash Q-learning and Nash V-learning—that are certiﬁed to have similar performance as Nash equilibrium policies, even when facing against their best response (see Section 3 for more details). We believe this technique could be of broader interest to the community. 1.2