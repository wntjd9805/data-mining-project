Abstract
We study the regularisation induced in neural networks by Gaussian noise injec-tions (GNIs). Though such injections have been extensively studied when applied to data, there have been few studies on understanding the regularising effect they induce when applied to network activations. Here we derive the explicit regu-lariser of GNIs, obtained by marginalising out the injected noise, and show that it penalises functions with high-frequency components in the Fourier domain; par-ticularly in layers closer to a neural network’s output. We show analytically and empirically that such regularisation produces calibrated classiﬁers with large clas-siﬁcation margins. 1

Introduction
Noise injections are a family of methods that involve adding or multiplying samples from a noise distribution, typically an isotropic Gaussian, to the weights or activations of a neural network during training. The beneﬁts of such methods are well documented. Models trained with noise often generalise better to unseen data and are less prone to overﬁtting (Srivastava et al., 2014; Kingma et al., 2015; Poole et al., 2014).
Even though the regularisation conferred by Gaussian noise injections (GNIs) can be observed em-pirically, and the beneﬁts of noising data are well understood theoretically (Bishop, 1995; Cohen et al., 2019; Webb, 1994), there have been few studies on understanding the beneﬁts of methods that inject noise throughout a network. Here we study the explicit regularisation of such injections, which is a positive term added to the loss function obtained when we marginalise out the noise we have injected.
Concretely our contributions are:
• We derive an analytic form for an explicit regulariser that explains most of GNIs’ regular-ising effect.
• We show that this regulariser penalises networks that learn functions with high-frequency content in the Fourier domain and most heavily regularises neural network layers that are closer to the output. See Figure 1 for an illustration.
• Finally, we show analytically and empirically that this regularisation induces larger classi-ﬁcation margins and better calibration of models. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Here we illustrate the effect of GNIs injected throughout a network’s activations. Each coloured dot represents a neuron’s activations. We add GNIs, represented as circles, to each layer’s activations bar the output layer. GNIs induce a network for which each layer learns a progressively lower frequency function, represented as a sinusoid matching in colour to its corresponding layer. 2