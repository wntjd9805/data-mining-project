Abstract
Fine-tuning pre-trained deep networks on a small dataset is an important compo-nent in the deep learning pipeline. A critical problem in ﬁne-tuning is how to avoid over-ﬁtting when data are limited. Existing efforts work from two aspects: (1) impose regularization on parameters or features; (2) transfer prior knowledge to
ﬁne-tuning by reusing pre-trained parameters. In this paper, we take an alternative approach by refactoring the widely used Batch Normalization (BN) module to mit-igate over-ﬁtting. We propose a two-branch design with one branch normalized by mini-batch statistics and the other branch normalized by moving statistics. During training, two branches are stochastically selected to avoid over-depending on some sample statistics, resulting in a strong regularization effect, which we interpret as
“architecture regularization.” The resulting method is dubbed stochastic normal-ization (StochNorm). With the two-branch architecture, it naturally incorporates pre-trained moving statistics in BN layers during ﬁne-tuning, exploiting more prior knowledge of pre-trained networks. Extensive empirical experiments show that StochNorm is a powerful tool to avoid over-ﬁtting in ﬁne-tuning with small datasets. Besides, StochNorm is readily pluggable in modern CNN backbones. It is complementary to other ﬁne-tuning methods and can work together to achieve stronger regularization effect. 1

Introduction
Training deep networks (Szegedy et al., 2015; He et al., 2016b; Huang et al., 2017) from scratch requires large amounts of data. Nevertheless, data collecting is not easy. It took years to build the ImageNet dataset (Deng et al., 2009) with millions of images. For each new task at hand, it is unrealistic to collect a new dataset at the scale of ImageNet. Thanks to the release of pre-trained deep networks in PyTorch (Benoit et al., 2019) and TensorFlow (Abadi et al., 2016), practitioners can beneﬁt from deep learning (LeCun et al., 2015) even with a small amount of data. The practice of transferring pre-trained parameters, a.k.a. ﬁne-tuning, is prevalent in both computer vision (Jung et al., 2015) and natural language processing (Devlin et al., 2019).
Because the dataset used in ﬁne-tuning is typically very small, the universal approximation abil-ity (Zhang et al., 2017) of neural networks makes them prone to over-ﬁtting, which is a critical problem in ﬁne-tuning. In general, there are two ways to avoid over-ﬁtting during ﬁne-tuning: impose appropriate regularization to explicitly reduce over-ﬁtting, and transfer prior knowledge by reusing pre-trained networks as an initialization point for implicit regularization.
To avoid over-ﬁtting better, we choose to refactor the widely used Batch Normalization (BN) (Ioffe
& Szegedy, 2015) module and come up with a two-branch design: one branch is normalized by mini-batch statistics and the other branch is normalized by moving statistics. During training, the activations of each channel are normalized by either mini-batch statistics or moving statistics, which is determined stochastically in a dropout (Srivastava et al., 2014) style to avoid over-depending on
∗Equal contribution, in alphabetic order 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Comparing StochNorm and existing methods on regularization type and knowledge transfer. method
L2-SP
DELTA
BSS regularization layer type knowledge parameter convolutional layer weight & bias transferred
√
√
√ feature feature batch normalization weight & bias moving statistics × ( for StochNorm)
StochNorm architecture fully-connected layer weight & bias
× some sample statistics. The proposed stochastic normalization (StochNorm) brings a straightforward byproduct: the moving statistics branch can naturally inherit pre-trained moving statistics which are discarded by existing methods. Therefore, StochNorm transfers more pre-trained knowledge than existing methods to better combat over-ﬁtting.
Table 1(left) lists primary regularization techniques. Li et al. (2018) regularize the parameters near their pre-trained values, Li et al. (2019b) regularize the features near features computed by pre-trained networks, and Chen et al. (2019) penalize small eigenvalues of feature representations.
As an alternative to parameter regularization and feature regularization, the proposed StochNorm regularizes ﬁne-tuning by module design, which we interpret as “architecture regularization.”
Table 1(right) lists whether each type of knowledge is transferred during ﬁne-tuning, with a focus on commonly used ConvNets. Knowledge-free layers like max-pooling and ReLU function are omitted in the table. Usually ConvNets are constructed by stacking Conv-BN-ReLU blocks, followed by a task-speciﬁc fully-connected layer. It is a common belief that the knowledge in fully-connected layers is task-speciﬁc and cannot be transferred. Transferring learnable parameters (weight and bias) is as easy as just reusing them. Nevertheless, moving statistics in BN layers are simply discarded due to the characteristic behavior of BN (see Section 4.2). The proposed StochNorm also transfers moving statistics of pre-trained networks to exploit prior knowledge in pre-trained networks better.
In summary, we study the problem of avoiding over-ﬁtting during ﬁne-tuning, and propose StochNorm, a pluggable module that can be easily in place of BN layers. The novel two-branch architecture design and the stochastic selection mechanism facilitate explicit architecture regularization, while the transfer of pre-trained moving statistics brings implicit initialization regularization, both making
StochNorm a powerful tool for ﬁne-tuning with small data. We compare StochNorm with state-of-the-art ﬁne-tuning methods and empirically validate its efﬁcacy with limited data. StochNorm is also complementary to them and they can achieve better performance when combined together. 2