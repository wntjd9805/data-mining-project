Abstract
We propose a novel type of balanced clustering algorithm to approximate attention.
Attention complexity is reduced from O(N 2) to O(N log N ), where N is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by deﬁning new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining.
On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch.
We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report signiﬁcant memory and speed beneﬁts.
Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using 50% less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ. 1

Introduction
Attention layers enable long-range representation learning and are becoming indispensable in ar-chitectures for both Image Synthesis [1, 2, 3] and Natural Language Processing [4, 5, 6, 7, 8, 9].
Attention ﬁnds further uses in other domains like symbolic mathematics and music modeling as well [10, 11, 12]. Unfortunately, attention layers have high computational and memory cost which scales quadratically in the size of the input sequence. This constraint is so onerous that the canonical implementation of attention for image synthesis - Self-Attention GAN [2] - could only afford to use one self-attention layer. For NLP, modern transformer-based models can only be trained in large industry research labs with massive infrastructure investments. For instance, the recently published GPT-3 [13] model uses 96 attention layers trained on input sequences of 2048 tokens.
When ﬁne-tuning pre-trained attention models, NLP researchers usually truncate input sentences, limiting performance on datasets with longer inputs.
Recent research [14, 3] indicates that dense attention is statistically and computationally inefﬁ-cient [15, 16, 3]: it does not account for the locality inherent in many tasks. Alternatives have been proposed that are either more efﬁcient [12, 17, 18, 19, 20, 7, 21, 22] or that better accommodate locality [23, 3]. Most such alternatives have been sparse. Sparsity can be achieved by limiting 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
attention to pre-deﬁned positions [23, 3, 22, 12]. Recent work [17, 18, 19, 20] proposes data-driven sparsity, which allows for discovery of arbitrarily complex dependencies between input positions.
Despite this progress, new state-of-the-art models [8, 13, 9, 24, 25, 26] still use the original dense attention layers. There are three reasons for this: (i) alternative fast-attention mechanisms degrade the performance of the underlying model. For example, replacing dense attention layers in Transformers with memory efﬁcient local attention [23] increases perplexity from 41.57 to 44.23 [20]. (ii) some mechanisms work well, but make very strict assumptions. For example, in Star Transformer [22] all nodes attend to a relay node which summarizes the content of the entire input sequence, but this prevents the use of causal masking, so it can only be used for encoding. (iii) some alternatives are only efﬁcient in theory. For example, in some variants [17, 27] sparsiﬁcation of the attention map happens after instantiating the matrix, and so quadratic memory is still used before instantiation. Finally,
[12, 28] require highly specialized GPU-kernels and which prevents usage in several hardware settings (e.g. TPUs). The design of fast and efﬁcient attention layers remains a challenge.
Our Contributions: 1) We propose a novel type of balanced clustering to approximate attention. We call the underlying optimization problem Attention Biclustering and prove that ﬁnding an exact solution is computation-ally intractable. 2) We propose an algorithm for solving Attention Biclustering efﬁciently in practice. Our algorithm,
SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by deﬁning new Asymmetric trans-formations and an adaptive scheme that produces balanced clusters. 3) Our method, SMYRF, can handle different query and key vectors, just like normal dense attention.
As a result, SMYRF layers are drop-in replacements for pre-trained models, unlike previously pro-posed fast-attention mechanisms such as Sinkhorn [20], Reformer [18] and Routing Transformer [19]. 4) We show through numerous experiments that SMYRF attention layers are very effective in terms of performance, memory and speed, even without any training. We measure the memory-performance trade-off of applying SMYRF to state-of-the-art NLP and Computer Vision models, across more than a dozen tasks. For example, we are able to shrink the memory requirements of a pre-trained
BigGAN [1] by 50% while maintaining 98.2% of its Inception score without re-training. 5) We ﬁnetune SMYRF on GLUE [25] starting from a BERT (base) checkpoint. We demonstrate that
SMYRF-BERT outperforms BERT while using 50% less memory. We also show that with 75% less memory, SMYRF maintains 99% of BERT performance on GLUE. Due to SMYRF’s portability, we are also able to conduct experiments for various memory conﬁgurations with pre-trained BERT and
RoBERTa [9] models on IMDB. We show slight performance drops for great memory beneﬁts. 6) We show that SMYRF can be interchanged with dense layers before and after training. We report performance gains by using SMYRF in a back-and-forth manner: we replace dense with SMYRF during training (to earn in memory) and we replace SMYRF with dense attention during inference (to earn in performance). The interchangeability of SMYRF with dense attention is unique, as it has not been observed in previously proposed attention alternatives [18, 19, 20, 28, 3]. 7) We are able to scale the resolution of attention for GANs, due to our reduced memory footprint.
We train a BigGAN with an 128 × 128 SMYRF attention layer and show it outperforms the dense attention performance, decreasing FID from 26.06 to 25.03 in Celeba-HQ-128 [29]. Finally, we successfully train a BigGAN with attention at resolution 256 × 256 on a single v3-8 TPU. 8) We open-source our code and pre-trained models to encourage more related research: https://github.com/giannisdaras/smyrf. 2