Abstract
One of the basic primitives in the class of submodular optimization problems is the submodular maximization under a cardinality constraint. Here we are given a ground set V that is endowed with a monotone submodular function f : 2V → R+ and a parameter 0 < k ≤ n and the goal is to return an optimal set S ⊆ V of at most k elements, i.e., f (S) is maximum among all subsets of V of size at most k. This basic primitive has many applications in machine learning as well as combinatorial optimization. Example applications are agglomerative clustering, exemplar-based clustering, categorical feature compression, document and corpus summarization, recommender systems, search result diversiﬁcation, data subset selection, minimum spanning tree, max ﬂow, global minimum cut, maximum matching, traveling salesman problem, max clique, max cut, set cover and knapsack, among the others.
In this paper, we propose the ﬁrst dynamic algorithm for this problem. Given a stream of inserts and deletes of elements of an underlying ground set V , we develop a dynamic algorithm that with high probability, maintains a ( 1 2 − (cid:15))-approximation of a cardinality-constrained monotone submodular maximization for any sequence of z updates (inserts and deletes) in time O(k2z(cid:15)−3 · log5 n), where n is the maximum size of V at any time. That is, the amortized update time of our algorithm is O(k2(cid:15)−3 · log5 n). 1

Introduction
A general approach to solve machine learning problems as well as combinatorial optimization problems is to cast the problem at hand as a submodular optimization problem and then solve the submodular problem (approximately) using a rich toolkit of algorithmic techniques known for this class of problems. Such problems include agglomerative clustering, exemplar-based clustering
[DF07], categorical feature compression [BCE+19], document summarization [LB11, SSSJ12], recommender systems [EG11], search result diversiﬁcation [AGHI09], data subset selection [WIB15], social networks analysis [KKT15], minimum spanning tree, global minimum cut, maximum matching, traveling salesman problem, max clique, max cut, set cover and knapsack, among the others.
One of the basic primitives in the class of submodular optimization problems is the submodular maximization under a cardinality constraint. Here we are given a ground set V that is endowed with a monotone submodular function f : 2V → R+ and a parameter 0 < k ≤ n and the goal is to return an optimal set S ⊆ V of at most k elements, i.e., f (S) is maximum among all subsets of V of size at most k. In this paper, we propose the ﬁrst dynamic algorithm for this problem. We state our main result here:
Theorem 1 Suppose we start with an empty set V . Then, there exists a randomized dynamic algorithm that with probability at least 1 − 1 2 − (cid:15))-approximation of a cardinality-constrained monotone submodular maximization for any sequence of z updates (inserts and deletes) in O(k2(cid:15)−3 · log5 n) amortized update time, where n is the maximum size of V at any time. n2 maintains a ( 1 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We should mention that classical methods such as the celebrated greedy algorithm [NWF78] or its accelerated versions [BV14] and [MBK+15] require random access to the entire data, make multiple passes, and select elements sequentially in order to produce near optimal solutions. Naturally, such solutions cannot scale to large instances.
Probably the closest work to our work are two recent papers due to Mirzasoleiman, Karbasi and
Krause [MKK17] and Kazemi, Zadimoghaddam, and Karbasi [KZK18]. In [MKK17] the authors develop a dynamic streaming algorithm that given a stream of inserts and deletes of elements of an underlying ground set V , (1/2 − (cid:15))-approximates the submodular maximization under cardinality constraint using O(d2(k(cid:15)−1 log k)2) space and O(dk(cid:15)−1 log k) average update time, where d is an upper-bound for the number of deletes that are allowed. Thus, if the number of deletions is linear in terms of the maximum size of the ground set V , that is, at least Ω(n) deletions, it is in fact better to re-run the known greedy algorithms (say, [NWF78]) after every insertion and deletion.
The follow-up paper [KZK18] studies approximating submodular maximization under cardinality constraint in three models, (1) centralized model, (2) dynamic streaming where we are allowed to insert and delete (up to d) elements of an underlying ground set V , and (3) distributed (MapReduce) model. In order to develop a generic framework for all the three models, they develop a coreset for the submodular maximization under cardinality constraint. Their coreset has a size of O(k log k + d log2 k). Out of this coreset we can extract a set S of size at most k whose f (S) in expectation is at least 1 2 -approximation of the optimal solution. The time to extract such a set S from the coreset is
O(dk log2 k + d log3 k). Once again, if the number of deletions is Ω(n), where n is the maximum size of the ground set V at any time t, it is in fact better to re-run the known greedy algorithms (say
[NWF78]) after every insertion and deletion.
However, our algorithm in Theorem 1 has ˜O(k2(cid:15)−3 · log5 n) amortized update time which is inde-pendent of the number of deletions d. Very recently we learned that at NeurIPS 2020 there is another paper titled "Fully Dynamic Algorithm for Constrained Submodular Optimization" due to Lattanzi,
Mitrovic, Norouzi-Fard, Tarnawski, and Zadimoghaddam that consider the same problem that we study in this paper. The paper presents a dynamic algorithm whose amortized expected update time is O( log6(k)·log2(n)
). The amortized expected update time of our algorithm is O(k2(cid:15)−2 · log3 n).
Asymptotically the update time of this algorithm is better than our algorithm. However, in reality these two bounds are incomparable. As an example, for practical values of k, say k ≤ 220 and for an error bound of (cid:15) ≤ 0.05, the term log6(k) is approximately 252 while the term k2(cid:15)−2 in our update time is approximately 245 which is smaller than their update time for n even as large as 2100.
Moreover, our algorithm works with high probability and is much simpler than their algorithm. We think we can use our worst case framework in Section 3 to improve their update time from expected to a high probability bound. (cid:15)6 (cid:15)6
Here we mention the main difference between the streaming and the dynamic setting. In the streaming setting the main concern is the space complexity. We often compute a sketch of the input that is revealed in a streaming fashion. At the end of the stream we compute a solution using the sketch that we maintained in the course of stream. On the other hand, in the dynamic setting the main complexity is the time. The idea is that given the input that is revealed in a streaming fashion, we are interested in seeing the solution and the changes in the solution after every insert or delete. The main motivation is for learning highly dynamic and sensitive data (such as time series) that we need to take an action as soon as we see a shift in the function of the underlying data that we observe. Since we need to react to changes in the solution fast, we need to (re)-compute the solution as fast as we can. Indeed, we cannot wait till the end of the stream and take the corresponding action afterwards. The underlying assumption for the dynamic setting is that nowadays with existing machines that can easily have (SD)RAMs of GBs and soon TBs, the space constraint will not be a problem, but the time complexity is the main bottleneck. The results from [MKK17, KZK18] are streaming algorithms whose time complexities depend on the number of deletions (Theorem 1 of the second reference) which will be high if we want to (re)-compute a solution after each insertion or deletion.
Overview of Proof of Theorem 1. An interesting property of a submodular function f : 2V → R+ is that it satisﬁes f (A ∪ {e}) − f (A) ≥ f (A ∪ {e}) − f (A) for all A ⊆ B ⊆ V and e /∈ B. Our main idea is to combine this property with a logarithmic rate of sampling and then greedily collect the heavy items (whose marginal gain are above a threshold) and remove light items (whose marginal 2
gain are below a threshold) at each rate. Here we let ∆f (e|A) gain of adding an element e ∈ V to A where A ⊆ V and e ∈ V .
.
= f (A ∪ {e}) − f (A) be the marginal
Let us ﬁrst consider the ofﬂine scenario. We later show how to handle insertion and deletion of elements. Suppose we are given a ground set V of size n endowed with a monotone submodular function f : 2V → R+ under a cardinality constraint parameterized by 0 < k ≤ n. Let OP T = maxS⊆V :|S|≤k f (S) and let R0 = V and G0 = ∅. Let τ = OP T 2k .
We sample a set Si ⊆ Ri−1 of s = O((cid:15)−2 log n) elements uniformly at random. That is, we sample each element of Ri−1 with probability p = s
|Ri−1| . We let Gi = Gi−1. We then greedily ﬁnd those elements of Si whose marginal gain with respect to the set Gi is at least the threshold τ and add them to Gi. Next, we ﬁlter those elements of Ri−1\Si whose marginal gain with respect to the set Gi is below τ and let Ri be the rest of elements (i.e, those whose marginal gain with respect to the set Gi is at least τ ). We then recurse if Ri is not empty. Here the main idea is at each step i of this recursive sampling algorithm, with high probability, we either have |Gi| ≥ |Gi−1| + 1 or |Ri| ≤ |Ri−1|
. Thus, after i∗ = O(k log n) recursive sampling steps we either have |Gi∗ | ≥ k or we come up with an empty set Ri∗ . We let G be the ﬁnal set of elements whose marginal gain are above the threshold τ . 2
Next consider a dynamic scenario where elements are inserted to V or deleted from V . Once a new element e is inserted. We loop through steps of the recursive sampling and at each Step i, we sample e with probability p = s
|Ri−1| . If e is sampled, we re-iterate all steps of the recursive sampling from Step i going down to Step i∗. Each step i of the recursive sampling consists of one greedy and ﬁltering subroutines and it can be done in O(|Ri−1|k). Therefore, since i∗ = O(k log n), the expected computation that is associated to an insertion will be O((cid:15)−2k2 log3 n). The same is true when an element e ∈ V is deleted. We loop through steps of the recursive sampling and at each Step i, we check if e ∈ Si which happens with probability p = s
|Ri−1| . If e ∈ Si, we re-iterate all steps of the recursive sampling from Step i going down to Step i∗. Again, we can show that the expected computation that is associated to a deletion will be O((cid:15)−2k2 log3 n).
To have the dynamic algorithm that works with high probability we create O(log n) instances of this recursive sampling and run all of them in parallel. After any sequence of z insertions and deletions, we drop those instances whose computations are more than cz(cid:15)−2k2 log3 n for some constant c. We show that with high probability it remains at least one instance whose total computation is at most cz(cid:15)−2k2 log3 n. That is, the amortized update time of that instance is c(cid:15)−2k2 log3 n.
Finally we should mention that for the threshold τ we choose OP T 2k assuming we know OP T . In reality we do not know OP T . We can consider two scenarios. The ﬁrst scenario is when we are given a bound on the maximum element of V , that is, say maxe∈V f (e) = Θ(Γ). This is actually a fair assumption that we often make when we generalize the insertion-only streaming model to dynamic streaming models. For example, Frahling and Sohler in [FS05] show that we can ﬁnd coresets of small size for many clustering problems (a subset of submodular optimization problems) in dynamic geometric streams if we have an upper-bound on the maximum cost of the optimal clustering, something which is not possible if we do not have such an upper-bound. Since OP T ≤ ck · Γ for a reasonably large constant c, we run our recursive sampling algorithm for (cid:96) ∈ [0..(cid:15)−1 · log(ckΓ)] guesses (1 + (cid:15))(cid:96) of OP T and report the best solution of all guesses. This blows up the update time by a factor (cid:15)−1 log(kΓ) and the approximation factor would be (1/2 − (cid:15)).
If we are not given such a bound, we can keep a max heap of the elements that are inserted but not deleted at any time t. The insertion and deletion times of the max heap are logarithmic in terms of the number n of items that are stored in the max heap. Finding the maximum r elements stored in the heap can be done in O(r log n) time. We then do as follows. We sample a set S of O( nk) elements and let Γ = maxe∈V f (e) and run the algorithm as for the ﬁrst scenario. In parallel, at any time t, nk) elements from the max heap and run a simple greedy we extract the set T of maximum O( algorithm for T . At the end, we report the best solution of these two parallel runs. In this way, the update time increases to ˜O( nk) and the approximation factor would be (1/2 − (cid:15)). We elaborate on this second method in the supplementary material.
√
√
√ 3
1.1 Preliminaries
Suppose we have a ground set V . A function f : 2V → R+ is called submodular if it satisﬁes f (A ∪ {e}) − f (A) ≥ f (B ∪ {e}) − f (B) for all A ⊆ B ⊆ V and e /∈ B. When f satisﬁes the additional property f (A ∪ {e}) − f (A) ≥ 0 for all A and e /∈ A, we say f is monotone. We
.
= f (A ∪ {e}) − f (A) be the marginal gain of adding an element e ∈ V to A where let ∆f (e|A)
A ⊆ V and e ∈ V . The term ∆f (e|A) is a discrete derivative that quantiﬁes the increase in utility obtained when adding e to a set A. Observe that, the submodularity condition can be written as
∆f (e|A) ≥ ∆f (e|B) for all A ⊆ B.
Monotone submodular maximization under a cardinality constraint for a monotone function f is deﬁned as OP T = maxS⊆V :|S|≤k f (S). We denote by O an optimal subset of size at most k that achieves the optimal value OP T = f (S∗). Note that we can have more than one optimal set.
The seminal result by Nemhauser, Wolsey and Fisher [NWF78] shows that a simple greedy algorithm can approximate a cardinality constrained monotone submodular maximization problem to a factor of (1 − 1/e) of optimal. This greedy algorithm is as follows. In the beginning, we let S = ∅. We then take k passes over the set V and in each pass we ﬁnd an element e ∈ V that maximizes ∆f (e|S), add it to S and delete it from V . 1.2 Basic primitives
In this paper we frequently use two basic primitives. The ﬁrst one is a simple greedy algorithm parameterized by a threshold τ and a set size k. Given two sets S and G of elements, the greedy algorithm scrolls through the set S and adds those elements whose marginal gain with respect to the set G is at least τ provided that the size of G is less than k.
The second one is a simple ﬁltering algorithm parameterized by a threshold τ and a set size k. Given two sets R and G of elements, we iterate through the elements of the set R and drop those elements whose marginal gain with respect to the set G is less than k.
Basic Primitives
Greedy:
Input: Sets S and G and parameters τ, k. 1: for each e ∈ S do 2: 3:
Output: Return G if ∆f (e|G) ≥ τ and |G| < k then
G = G ∪ {e}.
Filtering:
Input: Sets R and G and parameters τ, k. 1: for each e ∈ R do 2: 3:
Output: Return R. if ∆f (e|G) < τ then
R = R\{e}.
Lemma 2 Given sets S and G and parameters τ, k, the query complexity (i.e., the number of times that we invoke function f to compute the marginal value) of Primitive Greedy is O(|S|). Similarly, given sets R and G and parameters τ, k, the query complexity of Primitive Filtering is O(|R|).
Proof : For each element e ∈ S and as long as |G| ≤ k, we check if the marginal value of e is greater than threshold τ . If this is the case, we add e to G. So, overall we invoke the function f for (cid:3)
O(|S|) times. The second part is proven similarly.
Dynamic model. Let S be a stream of insertions and deletions of elements of an underlying ground set V . Suppose we want to (approximately) compute a monotone submodular maximization under k cardinality constraint for a monotone function f which is deﬁned for the set V . We deﬁne the time t to be the tth update (i.e., insertion or deletion) of stream S. Let Gt be a solution of the underlying set
Vt where Vt is the set of elements that are inserted up to time t but not deleted. The update time of a dynamic algorithm A is the time that A needs to compute a solution Gt of the set Vt given a solution
Gt−1 of the set Vt−1.
Oblivious adversarial model. We work in the oblivious adversarial model as is common for analysis of randomized data structures such as universal hashing [CW77]. This model has been used 4
in a series of papers on dynamic maximal matching and dynamic connectivity problems: see for example [OR10, BGS15, NS13, KKM13]. The model allows the adversary to know all the elements in the set V and their arrival order, as well as the algorithm to be used. However, the adversary is not aware of the random bits used by the algorithm, and so cannot choose updates adaptively in response to the randomly guided choices of the algorithm. This effectively means that we can assume that the adversary prepares the full input (inserts and deletes) before the algorithm runs. 1.3