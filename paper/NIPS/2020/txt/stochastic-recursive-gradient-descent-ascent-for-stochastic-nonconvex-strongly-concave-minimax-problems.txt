Abstract
We consider nonconvex-concave minimax optimization problems of the form minx maxy∈Y f (x, y), where f is strongly-concave in y but possibly nonconvex in x and Y is a convex and compact set. We focus on the stochastic setting, where we can only access an unbiased stochastic gradient estimate of f at each iteration. This formulation includes many machine learning applications as special cases such as robust optimization and adversary training. We are interested in
ﬁnding an O(ε)-stationary point of the function Φ(·) = maxy∈Y f (·, y). The most popular algorithm to solve this problem is stochastic gradient decent ascent, which requires O(κ3ε−4) stochastic gradient evaluations, where κ is the condition number. In this paper, we propose a novel method called Stochastic Recursive gradiEnt Descent Ascent (SREDA), which estimates gradients more efﬁciently using variance reduction. This method achieves the best known stochastic gradient complexity of O(κ3ε−3), and its dependency on ε is optimal for this problem. 1

Introduction
This paper considers the following minimax optimization problem min x∈Rd max y∈Y f (x, y) (cid:44) E [F (x, y; ξ)] , (1) where the stochastic component F (x, y; ξ), indexed by some random vector ξ, is (cid:96)-gradient Lipschitz on average. This minimax optimization formulation includes many machine learning applications such as regularized empirical risk minimization [41, 52], AUC maximization [39, 48], robust op-timization [14, 46], adversarial training [16, 17, 40] and reinforcement learning [13, 43]. Many existing work [8, 9, 12, 13, 18, 28, 33, 34, 41, 45, 48, 50, 52] focused on the convex-concave case of problem (1), where f is convex in x and concave in y. For such problems, one can establish strong theoretical guarantees.
In this paper, we focus on a more general case of (1), where f (x, y) is µ-strongly-concave in y but possibly nonconvex in x. This case is referred to as stochastic nonconvex-strongly-concave minimax problems, and it is equivalent to the following problem (cid:26) min x∈Rd
Φ(x) (cid:44) max y∈Y (cid:27) f (x, y)
. (2)
Formulation (2) contains several interesting examples in machine learning such as robust optimiza-tion [14, 46] and adversarial training [17, 40].
Since Φ is possibly nonconvex, it is infeasible to ﬁnd the global minimum in general. One important task of the minimax problem is ﬁnding an approximate stationary point of Φ. A simple way to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
solve this problem is stochastic gradient descent with max-oracle (SGDmax) [19, 25]. The algorithm includes a nested loop to solve maxy∈Y f (x, y) and use the solution to run approximate stochastic gradient descent (SGD) on x. Lin et al. [25] showed that we can solve problem (2) by directly extending SGD to stochastic gradient descent ascent (SGDA). The iteration of SGDA is just using gradient descent on x and gradient descent on y. The complexity of SGDA to ﬁnd O(ε)-stationary point of Φ in expectation is O (cid:0)κ3ε−4(cid:1) stochastic gradient evaluations, where κ (cid:44) (cid:96)/µ is the condition number. SGDA is more efﬁcient than SGDmax whose complexity is O (cid:0)(κ3ε−4) log(1/ε)(cid:1).
One insight of SGDA is that the algorithm selects an appropriate ratio of learning rates for x and y.
Concretely, the learning rate for updating y is O(κ2) times that of x. Using this idea, it can be shown that the nested loop of SGDmax is unnecessary, and SGDA eliminates the logarithmic term in the complexity result. In addition, Raﬁque et al. [36] presented some nested-loop algorithms that also achieved O (cid:0)κ3ε−4(cid:1) complexity. Recently, Yan et al. [47] proposed Epoch-GDA which considered constraints on both two variables.
Lin et al. [26] proposed a deterministic algorithm called minimax proximal point algorithm (Minimax
PPA) to solve nonconvex-strongly-concave minimax problem whose complexity has square root dependence on κ. Barazandeh and Razaviyayn [7], Ostrovskii et al. [32], Thekumparampil et al. [42] also studied the non-convex-concave minimax problems, however, these methods do not cover the stochastic setting in this paper and only work for a special case of problem (2) when the stochastic variable ξ is ﬁnitely sampled from {ξ1, . . . , ξn} (a.k.a. ﬁnite-sum case). That is, f (x, y) (cid:44) 1 n
F (x, y; ξi). n (cid:88) (3) i=1
In this paper, we propose a novel algorithm called Stochastic Recursive gradiEnt Descent Ascent (SREDA) for stochastic nonconvex-strongly-concave minimax problems. Unlike SGDmax and
SGDA, which only iterate with current stochastic gradients, our SREDA updates the estimator recursively and reduces its variance.
The variance reduction techniques have been widely used in convex and nonconvex minimization problems [1–4, 11, 15, 20, 22, 23, 29, 30, 35, 37, 38, 44, 51, 53] and convex-concave saddle point problems [9, 12, 13, 28, 34]. However, the nonconvex-strongly-concave minimax problems have two variables x and y and their roles in the objective function are quite different. To apply the technique of variance reduction, SREDA employs a concave maximizer with multi-step iteration on y to simultaneously balance the learning rates, gradient batch sizes and iteration numbers of the two variables. We prove SREDA reduces the number of stochastic gradient evaluations to O(κ3ε−3), which is the best known upper bound complexity. The result gives optimal dependency on ε since the lower bound of stochastic ﬁrst order algorithms for general nonconvex optimization is O(ε−3) [6].
For ﬁnite-sum cases, the gradient cost of SREDA is O (cid:0)n log(κ/ε) + κ2n1/2ε−2(cid:1) when n ≥ κ2, and
O (cid:0)(κ2 + κn)ε−2(cid:1) when n ≤ κ2. This result is sharper than Minimax PPA [26] in the case of n is larger than κ2. We summarize the comparison of all algorithms in Table 1.
The paper is organized as follows. In Section 2, we present notations and preliminaries. In Section 3, we review the existing work for stochastic nonconvex-strongly-concave optimization and related techniques. In Section 4, we present the SREDA algorithm and the main theoretical result. In
Section 5, we give a brief overview of our convergence analysis. In Section 6, we demonstrate the effectiveness of our methods on robust optimization problem. We conclude this work in Section 7. 2 Notation and Preliminaries
We ﬁrst introduce the notations and preliminaries used in this paper. For a differentiable function f (x, y), we denote the partial gradient of f with respect to x and y at (x, y) as ∇xf (x, y) and
∇yf (x, y) respectively. We use (cid:107)·(cid:107)2 to denote the Euclidean norm of vectors. For a ﬁnite set S, we denote its cardinality as |S|. We assume that the minimax problem (2) satisﬁes the following assumptions.
Assumption 1. The function Φ(·) is lower bounded, i.e., we have Φ∗ = inf x∈Rd Φ(x) > −∞.
Assumption 2. The component function F has an average (cid:96)-Lipschitz gradient, i.e., there exists a constant (cid:96) > 0 such that E (cid:107)∇F (x, y; ξ) − ∇F (x(cid:48), y(cid:48); ξ)(cid:107)2 2) for any (x, y), (x(cid:48), y(cid:48)) and random vector ξ 2 ≤ (cid:96)2((cid:107)x − x(cid:48)(cid:107)2 2 + (cid:107)y − y(cid:48)(cid:107)2 2
Table 1: We present the comparison on stochastic gradient complexities of algorithms to solve stochastic problem (2) and ﬁnite-sum problem (3). We use notation ˜O(·) to hide logarithmic factors.
Some baseline algorithms solve problem (3) without considering the ﬁnite-sum structure and we regard the cost of full gradient evaluation is O(n).
Algorithm
Stochastic
˜O(κ3ε−4)
SGDmax (GDmax)
PGSMD / PGSVRG O(κ3ε−4)
MGDA / HiBSA –
Minimax PPA
SGDA (GDA) –
O(κ3ε−4)
SREDA
O(κ3ε−3)
Finite-sum
˜O(κ2nε−2)
O(κ2nε−2)
O(κ4nε−2)
˜O(κ1/2nε−2)
O(κ2nε−2) (cid:40) ˜O (cid:0)n + κ2n1/2ε−2(cid:1), n ≥ κ2 n ≤ κ2
O (cid:0)(κ2 + κn)ε−2(cid:1) ,
Reference
[19, 25]
[36]
[27, 31]
[26]
[25] this paper
Assumption 3. The component function F is concave in y. That is, for any x, y, y(cid:48) and random vector ξ, we have F (x, y; ξ) ≤ F (x, y(cid:48); ξ) + (cid:104)∇yF (x, y(cid:48); ξ), y − y(cid:48)(cid:105).
Assumption 4. The function f (x, y) is µ-strongly-concave in y. That is, there exists a constant µ > 0 2 (cid:107)y − y(cid:48)(cid:107)2 such that for any x, y and y(cid:48), we have f (x, y) ≤ f (x, y(cid:48)) + (cid:104)∇yf (x, y(cid:48)), y − y(cid:48)(cid:105) − µ 2.
Assumption 5. The gradient of each component function F (x, y; ξ) has bounded variance. That is, there exists a constant σ > 0 such that E (cid:107)∇F (x, y; ξ) − ∇f (x, y)(cid:107)2 2 ≤ σ2 < ∞ for any x, y and random vector ξ.
Under the assumptions of Lipschitz-gradient and strongly-concavity on f , we can show that Φ(·) also has Lipschitz-gradient.
Lemma 1 ([25, Lemma 4.3]). Under Assumptions 2 and 4, the function Φ(·) = maxy∈Y f (·, y) has ((cid:96) + κ(cid:96))-Lipschitz gradient. Additionally, the function y∗(·) = arg maxy f (·, y) is unique deﬁned and we have ∇Φ(·) = ∇xf (·, y∗(·)).
Since Φ is differentiable, we may deﬁne ε-stationary point based on its gradient. The goal of this paper is to establish a stochastic gradient algorithm that output an O(ε)-stationary point in expectation.
Deﬁnition 1. We call x an O(ε)-stationary point of Φ if (cid:107)∇Φ(x)(cid:107)2 ≤ O(ε).
We also need the notations of projection and gradient mapping to address the constraint on Y.
Deﬁnition 2. We deﬁne the projection of y on to convex set Y by ΠY (y) = arg minz∈Y (cid:107)z − y(cid:107)2.
Deﬁnition 3. We deﬁne the gradient mapping of f at (x(cid:48), y(cid:48)) with respect to y as follows
Gλ,y(x(cid:48), y(cid:48)) = 1
λ (y(cid:48) − ΠY (y(cid:48) + λ∇yf (x(cid:48), y(cid:48)))) , where λ > 0. 3