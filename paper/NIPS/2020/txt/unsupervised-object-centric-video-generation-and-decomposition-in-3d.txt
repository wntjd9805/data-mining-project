Abstract
A natural approach to generative modeling of videos is to represent them as a composition of moving objects. Recent works model a set of 2D sprites over a slowly-varying background, but without considering the underlying 3D scene that gives rise to them. We instead propose to model a video as the view seen while moving through a scene with multiple 3D objects and a 3D background. Our model is trained from monocular videos without any supervision, yet learns to generate coherent 3D scenes containing several moving objects. We conduct detailed experiments on two datasets, going beyond the visual complexity supported by state-of-the-art generative approaches. We evaluate our method on depth-prediction and 3D object detection—tasks which cannot be addressed by those earlier works— and show it out-performs them even on 2D instance segmentation and tracking. 1

Introduction
In recent years, there has been considerable interest in object-centric generative models of images and videos—that is, generative models whose latent structure explicitly represents their composition from multiple objects or regions (e.g. [12, 13, 15, 24, 29]). These methods allow segmentation of input videos or images into objects, and generation of new ones. This is a natural structure to adopt, as it mirrors the way humans understand the world [33], as well as capturing important aspects of the causal process by which images are formed. Existing approaches treat objects either as components of 2D spatial mixtures, or as 2.5D stacks of sprites, which reﬂects how they appear when imaged by a camera. Crucially however, it does not reﬂect the underlying physical structure of the world, which of course consists of solid 3D objects situated in 3D space.
In this work, we take the natural next step—we develop an object-centric generative model over videos that explicitly models the 3D scenes they show. Our model is trained purely from unannotated monocular videos, without any 3D data. Nonetheless, it learns to decompose videos into multiple 3D foreground objects and a 3D background, and learns to generate videos showing coherent scenes.
To achieve this, we design a novel generative model that represents videos as the view from a camera moving through a 3D scene composed of multiple objects and a background (Sec. 2). It has a single latent embedding that learns to capture the space of allowable scene structures, which is important to avoid sampling implausible layouts such as intersecting objects [12]. This embedding is interpreted by a structured decoder that processes objects independently and compositionally. Each object is endowed with an appearance embedding, and a temporally-varying 3D location and pose.
After decoding their appearance embeddings to 3D shapes and textures, the objects and background are differentiably rendered to give the ﬁnal frames. This lets us train the model like a variational autoencoder (VAE) [28], to reconstruct its training videos in terms of their latent embeddings (Sec. 3).
By decoding objects independently, and disentangling pose from appearance, we introduce inductive biases that allow a distribution of videos to be represented efﬁciently and compositionally [45, 46], without needing to separately model all possible combinations of objects, shapes, textures, and poses. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
shape and Dbg
Figure 1: Left: Our generative model of videos has a single latent variable z, which is decoded by
F obj to parameters of G objects. The contents of the green plate are replicated once per object; the contents of the blue plate are replicated once per frame. Each object has pose parameters and an appearance embedding ag, which is itself decoded (by Dobj voxels) to an explicit voxel representation of its 3D shape and color. Dbg tex map z to the shape and texture of a 3D background. The objects are rendered individually based on their pose and the camera parameters φ, then composed over the background to give the ﬁnal frames x. We add an encoder model (red) that predicts the latent variable corresponding to a given video ˜x. Right: Plan view of our 3D scene structure, viewed by a camera (green), which moves over time according to parameters φ. We deﬁne a grid of G candidate objects in 3D space (gray boxes), and a spherical shell for the background (gray dashed).
Presence indicators pg specify whether the object in each cell is present; here the blue ones are. Each is displaced by ∆g from the center of its cell (orange), and rotated by αg; the position and rotation then vary over time. The background is deformed (purple arrows) into the correct shape (red) to capture the surrounding environment
Moreover, by treating the objects and background as 3D, we automatically capture the subspace of appearance variation that arises due to viewpoint changes, which would otherwise need to be explicitly learnt by the model and encoded in its latent space. This is important as it eases the necessary trade-off between capacity and expressiveness—a generic model powerful enough to explain appearance variation due to viewpoint changes may more easily learn to model several objects together as one (an undesirable outcome). Factoring out this aspect of variability allows our approach to handle videos with signiﬁcantly higher visual complexity than previous methods [9, 24].
We conduct extensive experiments (Sec. 5) on two datasets of videos with both stationary and moving objects. We show that our method can:
• decompose a given video into its constituent objects and background, by predicting segmentation masks and tracking objects over time;
• determine its 3D structure, by predicting depth and 3D bounding boxes; and
• generate coherent videos showing objects moving in 3D space over a 3D background.
In short, our contribution is the ﬁrst object-centric generative model of videos that explicitly reasons in 3D space, learning in an unsupervised fashion to decompose scenes into 3D objects, and to generate new, plausible samples. 2 Generative model
Our generative process (Fig. 1) for a video x ∈ RL×H×W , of length L frames and size W × H pixels, begins by sampling a d-dimensional Gaussian latent variable z ∼ N (0, 1). This embeds the full content of the scene shown in the video, and is decoded to yield separate embeddings and poses for the different objects, and the shape and texture of the background. The object embeddings are decoded independently to 3D shapes, represented as voxels or meshes. Then, the objects and background are rendered into the ﬁnal video frames, and Gaussian pixel noise is added so the likelihood is well-deﬁned. The camera is assumed to be at the origin at frame zero, hence the model is ego-centric—it represents each scene in a frame of reference centered on the viewer. The camera parameters φ are treated as a known conditioning variable, a natural assumption in many applications such as robotics.
We ﬂatten the camera matrices to a single vector and encode this with a fully-connected network 2
F cam to give an embedding φ∗ ∈ Rc. In the following subsections, we describe each component of the decoder in detail.1 2.1