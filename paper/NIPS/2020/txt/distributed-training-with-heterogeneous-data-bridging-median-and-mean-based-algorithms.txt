Abstract
Recently, there is a growing interest in the study of median-based algorithms for distributed non-convex optimization. Two prominent examples include SIGNSGD with majority vote, an effective approach for communication reduction via 1-bit compression on the local gradients, and MEDIANSGD, an algorithm recently pro-posed to ensure robustness against Byzantine workers. The convergence analyses for these algorithms critically rely on the assumption that all the distributed data are drawn iid from the same distribution. However, in applications such as Federated
Learning, the data across different nodes or machines can be inherently hetero-geneous, which violates such an iid assumption. This work analyzes SIGNSGD and MEDIANSGD in distributed settings with heterogeneous data. We show that these algorithms are non-convergent whenever there is some disparity between the expected median and mean over the local gradients. To overcome this gap, we provide a novel gradient correction mechanism that perturbs the local gradients with noise, which we show can provably close the gap between mean and median of the gradients. The proposed methods largely preserve nice properties of these median-based algorithms, such as the low per-iteration communication complexity of SIGNSGD, and further enjoy global convergence to stationary solutions. Our perturbation technique can be of independent interest when one wishes to estimate mean through a median estimator. 1

Introduction
In the past few years, deep neural networks have achieved great success in many tasks including computer vision and natural language processing. For many tasks in these ﬁelds, it may take weeks or even months to train a model due to the size of the model and training dataset. One practical and promising way to reduce the training time of deep neural networks is using distributed training
[8]. A popular and practically successful paradigm for distributed training is the parameter server framework [16], where most of the computation is ofﬂoaded to workers in parallel and a parameter sever is used for coordinating the training process. Formally, the goal of such distributed optimization is to minimize the average of M different functions from M nodes, min x∈Rd f (x) (cid:44) 1
M
M
X i=1 fi(x), (1) where each node i can only access information of its local function fi(·), deﬁned by its local data.
Typically, such local objective takes the form of either a expected loss over local data distribution
∗equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(population risk), or a empirical average over loss functions evaluated over a ﬁnite number of data points (empirical risk). That is,
Z fi(x) = pi(ζ)l(x; ζ)dζ, or fi(x) = 1
Ki
KiX k=1 l(x; ζi,k) (2) where l(x; ζ) (resp. l(x; ζi,k)) is the cost evaluated at a given data point ζ (resp. ζi,k).
Similar to the parameter server paradigm, motivated by the use of machine learning models on mobile devices, a distributed training framework called Federated Learning has become popular [15, 18, 17].
In Federated Learning, the training data are distributed across personal devices and one wants to train a model without transmitting the users’ data due to privacy concerns. While many distributed algorithms proposed for parameter server are applicable to Federated Learning, Federated Learning posed many unique challenges, including the presence of heterogeneous data across the nodes, and the need to accommodate asynchronous updates, as well as very limited message exchange among the nodes and the servers. By heterogeneous data, we mean that either pi(ζ), or the empirical distribution formed by {ζi,k}K k=1 in (2), are signiﬁcantly different across the local nodes. Clearly, when the data is heterogeneous, we will have ∇fi(x) 6= ∇fj(x) and if local data are homogeneous, we will have
∇fi(x) = ∇fj(x) or ∇fi(x) ≈ ∇fj(x) when K is large.
Median-Based Methods. Under these distributed optimization frameworks, many algorithms based on stochastic gradient descent (SGD) have been proposed to solve (1). The basic idea is to perform updates based on the mean of the local stochastic directions. On the other hand, there are two prominent and interesting algorithms whose updates are not directly related to the mean of the local gradients. One is called SIGNSGD (with majority vote) (see Algorithm 1) [4], which updates the parameters based on a majority vote of sign of gradients to reduce communication overheads. The other one is called MEDIANGD (see its generalized stochastic version in Algorithm 2, which we refer to as MEDIANSGD [28]), which aims to ensure robustness against Byzantine workers by using coordinate-wise median of gradients to evaluate mean of gradients.
Algorithm 1 SIGNSGD (with M nodes) 1: Input: learning rate δ, current point xt 2: gt,i ← ∇fi(xt) + sampling noise 3: xt+1 ← xt − δ sign(PM i=1 sign(gt,i))
Algorithm 2 MEDIANSGD (with M nodes) 1: Input: learning rate δ, current point xt 2: gt,i ← ∇fi(xt) + sampling noise 3: xt+1 ← xt − δmedian({gt,i}M i=1)
It is clear that SIGNSGD and MEDIANSGD do not simply average their local gradients. At ﬁrst glance, their update rules also appear to be fundamentally different since they are tailored to different desiderata (that is, communication-efﬁciency versus robustness). Interestingly, in this work we made an observation that, SIGNSGD can be viewed as updating variables along signed median direction (sign(median({gt,i}M i=1))), uncovering its hidden connection to MEDIANSGD. This view provides a uniﬁed interpretation of these two algorithms as median-based distributed algorithms. We analyze these median-based methods in the heterogeneous regime.
Homogeneous v.s. Heterogeneous Data. While the median-based methods are increasingly popular, there has not been a good understanding about the convergence behavior of median-based methods.
The existing analyses of both SIGNSGD and MEDIANSGD rely on the assumption of homogeneous data. SIGNSGD is analyzed from the in-sample optimization perspective: it converges to stationary points if the stochastic gradients gt,i sampled from each worker follow the same distribution [4, 5].
That is, ∇fi(xt) = ∇fj(xt), ∀ xt, and the sampling noises follow the same distribution. On the other hand, MEDIANSGD is analyzed under the framework of population risk minimization: it converges with an optimal statistical rate, but again under the assumption that the data across the workers are iid [28].
However, in many modern distributed settings especially Federated Learning, data on different worker nodes can be inherently heterogeneous. For example, users’ data stored on different worker nodes might come from different geographic regions, which induce substantially different data distributions. In Federated Learning, the stochastic gradient gt,i from each device is effectively the full gradient ∇fi(xt) evaluated on the user’s data (due to the small size of local data), which violates the assumption of identical gradient distributions. Therefore, under these heterogeneous data settings, data aggregation and shufﬂing are often infeasible, and there is very little understanding of the behavior of both aforementioned algorithms. 2
From the ﬁxed-point perspective, median-based algorithms like SIGNSGD and MEDIANSGD drive the median of gradients to 0—that is, when the median of gradients reaches 0, the algorithms will not perform updates. When the median is close to the mean of gradients (the latter is the gradient of the target loss function), it follows that the true gradient is also approximately 0, and an approximate stationary solution is reached. The reason for assuming homogeneous data in existing literature
[4, 5, 28] is exactly to ensure that the median is close to mean. However, when the data from different workers are not drawn from the same distribution, the potential gap between the mean and median could prevent these algorithms from reducing the true gradient. i=1 fi(x) (cid:44) (x −
To illustrate this phenomenon, consider a simple one-dimensional example: 1 3 ai)2/2, with a1 = 1, a2 = 2, a3 = 10. If we run SIGNSGD and MEDIANSGD with step size
δ = 0.001 and initial point x0 = 0.0005, both algorithms will produce iterates with large disparity between the mean and median gradients. See Fig. 1 for the trajectories of gradient norms. Both algorithms drive the median of gradients to 0 (SIGNSGD ﬁnally converges to the level of step size due to the use of sign in its update rule), while the true gradient remains a constant. In Sec. 5, we provide further empirical evaluation to demonstrate that such disparity can severely hinder the training performance.
P3 (a) Trajectory of SIGNSGD (b) Trajectory of MEDIANSGD
Figure 1: Absolute value of mean and median of gradient vs iteration. (a) shows the trajectory of
SIGNSGD (b) shows the trajectory of MEDIANSGD
Our contribution. Motivated by the need to understand median-based algorithms under heteroge-neous data settings, we investigate two questions: 1) in a distributed training environment, under what conditions do SIGNSGD and MEDIANSGD work well? and 2) can we provide mechanisms to close the convergence gap in these algorithms? Speciﬁcally, we analyze the convergence rate of SIGNSGD and MEDIANSGD, with ﬁnite number of data samples per worker node, without assuming the data on different workers are from the same distribution. Our contributions are summarized as follows. (1) SIGNSGD as a median-based algorithm: We show that SIGNSGD updates along the direction of signed median of gradients, which connects SIGNSGD and MEDIANSGD. This fact is crucial for our subsequent analysis of SIGNSGD, and has not been recognized by existing works so far. (2) Bridging the gap between median and mean by adding controlled noise. We prove the follow-ing key result: given an arbitrary set of numbers, if one adds unimodal and symmetric noises with variance σ2, then the expected median of the resulting numbers will approach the expected mean of the original numbers, with a rate of O(1/σ). In addition, the distribution of the median will become increasingly symmetric as the variance of noise increases, with a rate of O(1/σ2). This result could be of independent interest. (3) Non-convergence due to the gap between median and mean. We prove that SIGNSGD and
MEDIANSGD only converge to solutions whose gradient sizes are proportional to the difference between the expected median and mean of gradients at different workers. Further, we show that the non-convergence is not an artifact of analysis by providing examples where SIGNSGD and
MEDIANSGD does not converge due to the gap between median and mean. (4) Convergence of noisy SIGNSGD and noisy MEDIANSGD. By using contribution (2), that the expected median converges to mean, and a sharp analysis on the probability density function of the noise on median, we prove that noisy SIGNSGD and noisy MEDIANSGD can both converge to stationary points. 3
Finally, we emphasize that the connection we established between SIGNSGD and median based method is mainly used to identify and resolve the non-convergence issue of SIGNSGD. The focus of this paper is not to analyze properties of median-based methods beyond convergence. 1.1