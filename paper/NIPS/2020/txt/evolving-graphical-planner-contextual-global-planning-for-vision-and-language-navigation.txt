Abstract
The ability to perform effective planning is crucial for building an instruction-following agent. When navigating through a new environment, an agent is chal-lenged with (1) connecting the natural language instructions with its progressively growing knowledge of the world; and (2) performing long-range planning and decision making in the form of effective exploration and error correction. Current methods are still limited on both fronts despite extensive efforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a model that performs global planning for navigation based on raw sensory input. The model dynamically constructs a graphical representation, generalizes the action space to allow for more ﬂexible decision making, and performs efﬁcient planning on a proxy graph representation. We evaluate our model on a challenging Vision-and-Language Nav-igation (VLN) task with photorealistic images, and achieve superior performance compared to previous navigation architectures. For instance, we achieve a 53% success rate on the test split of the Room-to-Room navigation task [1] through pure imitation learning, outperforming previous navigation architectures by up to 5%. 1

Introduction
Recent work has made remarkable progress towards building autonomous agents that navigate by following instructions [2, 3, 4, 5, 6, 7, 8, 9, 10] and constructing memory structures for maps
[11, 12, 13]. An important problem setting within this space is the paradigm of online navigation, where an agent needs to perform navigation based on goal descriptions in an unseen environment using a limited number of steps [14, 1].
In order to successfully navigate through an unseen environment, an agent needs to overcome two key challenges. First, the instructions given to the agent are natural language descriptions of the goal and the landmarks along the way; these descriptions need to be grounded onto the evolving visual world that the agent is observing. Second, the agent needs to perform non-trivial planning over a large action space, including: 1) deciding which step to take next to resolve ambiguities in the instructions through novel observations and 2) gaining a better understanding of the environment layout in order to progress towards the goal or recover from its prior mistakes. Notably this planning requires not only selecting from an increasingly large set of possible actions but also performing complex long-term reasoning.
Existing work tackles only one or two components of the above and may require additional pre-processing steps. Some are constrained to use local control policies [14, 2] or use rule-based algorithms such as beam or A∗ search [2, 15, 14] to perform localized path corrections. Others focus on processing long-range observations instead of actions [16] or employ ofﬂine pre-training schemes to learn topological structures [12, 13, 17]. This is challenging since accurate construction of graphs is non-trivial and requires special adaptation to work during real-time navigation [13, 17]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Under the guidance of natural language instruction, the autonomous agent needs to navigate through the environment from the start state to the target location (red ﬂag). Our proposed Evolving
Graphical Planner (EGP) constructs a dynamic representation and makes decisions in a global action space (right). With the EGP, the agent, currently in the orange node, maintains and reasons over the evolving graph to select the next node to visit (green) from possible choices (blue).
In this paper, we propose the Evolving Graphical Planner (EGP) (Figure 1), which 1) dynamically constructs a graphical map of the environment in an online fashion during exploration and 2) incorporates a global planning module for selecting actions. EGP can operate directly on raw sensory inputs in partially observable settings by building a structured representation of the geometric layout of the environment using discrete symbols to represent visited states and unexplored actions.
This expressive representation allows our agent to choose from a greater number of global actions conditioned on the text instruction and perform course corrections if needed.
Incorporating a global navigation module is challenging since we do not always have access to ground truth supervisions from the environment. Further, the ever-expanding size of the global graphs requires a scalable action selection module. To solve the ﬁrst challenge, we introduce a novel method for training our planning modules using imitation learning – this allows the agent to efﬁciently learn how to select global actions and backtrack when necessary. For the second, we introduce proxy graphs, which are local approximations of the entire map and allow for more scalable planning. Our entire model is end-to-end differentiable under a pure imitation learning framework.
We test EGP on two benchmarks for 3D navigation with instructions – Room-to-Room [1] and
Room-for-Room [18]. Our model outperforms several state-of-the-art backbone architectures on both datasets – e.g., on Room-to-Room, we achieve a 5% improvement in Success Rate over the Regretful agent [19]. We also perform a series of ablation studies on our model to jus-tify the design choices. Our implementation is available at https://github.com/Lucas2012/
EvolvingGraphicalPlanner. 2