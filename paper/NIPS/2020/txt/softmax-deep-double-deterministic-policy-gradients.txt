Abstract
A widely-used actor-critic reinforcement learning algorithm for continuous control,
Deep Deterministic Policy Gradients (DDPG), suffers from the overestimation problem, which can negatively affect the performance. Although the state-of-the-art Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm mitigates the overestimation issue, it can lead to a large underestimation bias. In this paper, we propose to use the Boltzmann softmax operator for value function estimation in continuous control. We ﬁrst theoretically analyze the softmax operator in continu-ous action space. Then, we uncover an important property of the softmax operator in actor-critic algorithms, i.e., it helps to smooth the optimization landscape, which sheds new light on the beneﬁts of the operator. We also design two new algorithms,
Softmax Deep Deterministic Policy Gradients (SD2) and Softmax Deep Double
Deterministic Policy Gradients (SD3), by building the softmax operator upon single and double estimators, which can effectively improve the overestimation and un-derestimation bias. We conduct extensive experiments on challenging continuous control tasks, and results show that SD3 outperforms state-of-the-art methods. 1

Introduction
Deep Deterministic Policy Gradients (DDPG) [24] is a widely-used reinforcement learning [26, 30, 24, 29] algorithm for continuous control, which learns a deterministic policy using the actor-critic method. In DDPG, the parameterized actor network learns to determine the best action with highest value estimates according to the critic network by policy gradient descent. However, as shown recently in [15], one of the dominant concerns for DDPG is that it suffers from the overestimation problem as in the value-based Q-learning [37] method, which can negatively affect the performance with function approximation [34]. Therefore, it is of vital importance to have good value estimates, as a better estimation of the value function for the critic can drive the actor to learn a better policy.
To address the problem of overestimation in actor-critic, Fujimoto et al. propose the Twin Delayed
Deep Deterministic Policy Gradient (TD3) method [15] leveraging double estimators [20] for the critic. However, directly applying the Double Q-learning [20] algorithm, though being a promising method for avoiding overestimation in value-based approaches, cannot fully alleviate the problem in actor-critic methods. A key component in TD3 [15] is the Clipped Double Q-learning algorithm, which takes the minimum of two Q-networks for value estimation. In this way, TD3 signiﬁcantly improves the performance of DDPG by reducing the overestimation. Nevertheless, TD3 can lead to a large underestimation bias, which also impacts performance [10].
The Boltzmann softmax distribution has been widely adopted in reinforcement learning. The softmax function can be used as a simple but effective action selection strategy, i.e., Boltzmann exploration
[33, 9], to trade-off exploration and exploitation. In fact, the optimal policy in entropy-regularized reinforcement learning [18, 19] is also in the form of softmax. Although it has been long believed that the softmax operator is not a non-expansion and can be problematic when used to update value 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
functions [25, 4], a recent work [32] shows that the difference between the value function induced by the softmax operator and the optimal one can be controlled in discrete action space. In [32], Song et al. also successfully apply the operator for value estimation in deep Q-networks [26], and show the promise of the operator in reducing overestimation. However, the proof technique in [32] does not always hold in the continuous action setting and is limited to discrete action space. Therefore, there still remains a theoretical challenge of whether the error bound can be controlled in the continuous action case. In addition, we ﬁnd that the softmax operator can also be beneﬁcial when there is no overestimation bias and with enough exploration noise, while previous works fail to understand the effectiveness of the operator in such cases.
In this paper, we investigate the use of the softmax operator in updating value functions in actor-critic methods for continuous control, and show that it has several advantages that makes it appealing.
Firstly, we theoretically analyze the properties of the softmax operator in continuous action space. We provide a new analysis showing that the error between the value function under the softmax operator and the optimal can be bounded. The result paves the way for the use of the softmax operator in deep reinforcement learning with continuous action space, despite that previous works have shown theoretical disadvantage of the operator [25, 4]. Then, we propose to incorporate the softmax operator into actor-critic for continuous control. We uncover a fundamental impact of the softmax operator, i.e., it can smooth the optimization landscape and thus helps learning empirically. Our ﬁnding sheds new light on the beneﬁts of the operator, and properly justiﬁes its use in continuous control.
We ﬁrst build the softmax operator upon single estimator, and develop the Softmax Deep Deterministic
Policy Gradient (SD2) algorithm. We demonstrate that SD2 can effectively reduce overestimation and outperforms DDPG. Next, we investigate the beneﬁts of the softmax operator in the case where there is underestimation bias on top of double estimators. It is worth noting that a direct combination of the operator with TD3 is ineffective and can only worsen the underestimation bias. Based on a novel use of the softmax operator, we propose the Softmax Deep Double Deterministic Policy Gradient (SD3) algorithm. We show that SD3 leads to a better value estimation than the state-of-the-art TD3 algorithm, where it can improve the underestimation bias, and results in better performance and higher sample efﬁciency.
We conduct extensive experiments in standard continuous control tasks from OpenAI Gym [6] to evaluate the SD3 algorithm. Results show that SD3 outperforms state-of-the-art methods including
TD3 and Soft Actor-Critic (SAC) [19] with minimal additional computation cost. 2 Preliminaries
The reinforcement learning problem can be formulated by a Markov decision process (MDP) deﬁned as a 5-tuple (S, A, r, p, γ), with S and A denoting the set of states and actions, r the reward function, p the transition probability, and γ the discount factor. We consider a continuous action space, and assume it is bounded. We also assume the reward function r is continuous and bounded, where the assumption is also required in [31]. In continuous action space, taking the max operator over A as in Q-learning [37] can be expensive. DDPG [24] extends Q-learning to continuous control based on the Deterministic Policy Gradient [31] algorithm, which learns a deterministic policy π(s; φ) parameterized by φ to maximize the Q-function to approximate the max operator. The objective is to maximize the expected long-term rewards J(π(·; φ)) = E[(cid:80)∞ k=0 γkr(sk, ak)|s0, a0, π(·; φ)].
Speciﬁcally, DDPG updates the policy by the deterministic policy gradient, i.e., (cid:3) , (cid:2)∇φ(π(s; φ))∇aQ(s, a; θ)|a=π(s;φ)
∇φJ(π(·; φ)) = Es (1) where Q(s, a; θ) is the Q-function parameterized by θ which approximates the true parameter
θtrue. We let T (s(cid:48)) denote the value estimation function, which is used to estimate the target
Q-value r + γT (s(cid:48)) for state s(cid:48). Then, we see that DDPG updates its critic according to θ(cid:48) =
θ + αEs,a∼ρ (r + γTDDPG(s(cid:48)) − Q(s, a; θ)) ∇θQ(s, a; θ), where TDDPG = Q(s(cid:48), π(s(cid:48); φ−); θ−),
ρ denotes the sample distribution from the replay buffer, α is the learning rate, and φ−, θ− denote parameters of the target networks for the actor and critic respectively. 3 Analysis of the Softmax Operator in Continuous Action Space
In this section, we theoretically analyze the softmax operator in continuous action space by studying the performance bound of value iteration under the operator. 2
The softmax operator (cid:82) exp(βQ(s,a)) in continuous action space is deﬁned by softmaxβ (Q(s, ·)) = a∈A (cid:82) a(cid:48) ∈A exp(βQ(s,a(cid:48)))da(cid:48) Q(s, a)da, where β is the parameter of the softmax operator.
In Theorem 1, we provide an O(1/β) upper bound for the difference between the max and softmax operators. The result is helpful for deriving the error bound in value iteration with the softmax operator in continuous action space. The proof of Theorem 1 is in Appendix A.1.
Theorem 1 Let C(Q, s, (cid:15)) = {a|a ∈ A, Q(s, a) ≥ maxa Q(s, a) − (cid:15)} and F (Q, s, (cid:15)) = (cid:82) a∈C(Q,s,(cid:15)) 1da for any (cid:15) > 0 and any state s. The difference between the max operator and the softmax operator is 0 ≤ maxa Q(s, a) − softmaxβ (Q(s, ·)) ≤ a∈A 1da−1−ln F (Q,s,(cid:15))
β
+ (cid:15). (cid:82)
Remark. A recent work [32] studies the distance between the two operators in the discrete setting.
However, the proof technique in [32] is limited to discrete action space. This is because applying the technique requires that for any state s, the set of the maximum actions at s with respect to the Q-function Q(s, a) covers a continuous set, which often only holds in special cases in the setting with continuous action. In this case, (cid:15) can be 0 and the bound still holds, which turns into (cid:82) a∈A 1da−1−ln F (Q,s,0)
. Note that when Q(s, a) is a constant function with respect to a, the upper
β bound in Theorem 1 will be 0, where the detailed discussion is in Appendix A.1.
Now, we formally deﬁne value iteration with the softmax operator by Qt+1(s, a) = rt(s, a) +
γEs(cid:48)∼p(·|s,a)[Vt(s(cid:48))], Vt+1(s) = softmaxβ (Qt+1(s, ·)), which updates the value function using the softmax operator iteratively. In Theorem 2, we provide an error bound between the value function under the softmax operator and the optimal in continuous action space, where the proof can be found in Appendix A.2.
Theorem 2 For any iteration t, the difference between the optimal value function V ∗ and the value function induced by softmax value iteration at the t-th iteration Vt satisﬁes:
||Vt − V ∗||∞ ≤ γt||V0(s) − V ∗(s)||∞ + 1 1 − γ
β(cid:15) + (cid:82) a∈A 1da − 1
β
− t (cid:88) k=1
γt−k mins ln F (Qk, s, (cid:15))
β
.
Therefore, for any (cid:15) > 0, the error between the value function induced by the softmax operator and the optimal can be bounded, which converges to (cid:15)/(1 − γ), and can be arbitrarily close to 0 as β approaches to inﬁnity. Theorem 2 paves the way for the use of the softmax operator for value function updates in continuous action space, as the error can be controlled in a reasonable scale. 4 The Softmax Operator in Actor-Critic
In this section, we propose to employ the softmax operator for value function estimation in standard actor-critic algorithms with single estimator and double estimators. We ﬁrst show that the softmax operator can smooth the optimization landscape and help learning empirically. Then, we show that it enables a better estimation of the value function, which effectively improves the overestimation and underestimation bias when built upon single and double estimators respectively. 4.1 The Softmax Operator Helps to Smooth the Optimization Landscape
We ﬁrst show that the softmax operator can help to smooth the optimization landscape. For simplicity, we showcase the smoothing effect based on a comparative study of DDPG and our new SD2 algorithm (to be introduced in Section 4.2). SD2 is a variant of DDPG that leverages the softmax operator to update the value function, which is the only difference between the two algorithms. We emphasize that the smoothing effect is attributed to the softmax operator, and also holds for our proposed SD3 algorithm (to be introduced in Section 4.3), which uses the operator to estimate value functions.1
We design a toy 1-dimensional, continuous state and action environment MoveCar (Figure 1(a)) to illustrate the effect. The car always starts at position x0 = 8, and can take actions ranging in
[−1.0, 1.0] to move left or right, where the left and right boundaries are 0 and 10. The rewards are 1See Appendix B.1 for the comparative study on the smoothing effect of TD3 and SD3. 3
+2, +1 in neighboring regions centered at x1 = 1 and x2 = 9, respectively, with the length of the neighboring region to be 1. In other positions, the reward is 0. The episode length is 100 steps. We run DDPG and SD2 on MoveCar for 100 independent runs. To exclude the effect of the exploration, we add a gaussian noise with the standard deviation to be high enough to the action during training, and both two algorithms collects diverse samples in the warm-up phase where actions are sampled from a uniform distribution. More details about the experimental setup are in Appendix B.2. The performance result is shown in Figure 1(b), where the shaded area denotes half a standard deviation for readability. As shown, SD2 outperforms DDPG in ﬁnal performance and sample efﬁciency. (a) MoveCar. (b) Performance. (c) Scatter plot. (d) Linear interpolation.
Figure 1: Analysis of smoothing effect in the MoveCar environment.
We investigate the optimization landscape based on the visualization technique proposed in [2].
According to Eq. (1), we take the loss function of the actor L(φ) = Es∼ρ [−Q(s, π(s; φ); θ)] as the objective function in our analysis. To understand the local geometry of the actor losses LDDPG and
LSD2, we randomly perturb the corresponding policy parameters φ0 and φ1 learned by DDPG and
SD2 during training from a same random initialization. Speciﬁcally, the key difference between the two parameters is that φ0 takes an action to move left in locations [0, 0.5], while φ1 determines to move right. Thus, φ1 are better parameters than φ0. The random perturbation is obtained by randomly sampling a batch of directions d from a unit ball, and then perturbing the policy parameters in positive and negative directions by φ ± αd for some value α. Then, we evaluate the difference between the perturbed loss functions and the original loss function, i.e., L(φ ± αd) − L(φ).
Figure 1(c) shows the scatter plot of random perturbation. For DDPG, the perturbations for its policy parameters φ0 are close to zero (blue circles around the origin). This implies that there is a ﬂat region in LDDPG, which can be difﬁcult for gradient-based methods to escape from [12]. Figure 2(a) shows that the policy of DDPG converges to always take action −1 at each location. On the other hand, as all perturbations around the policy parameters φ1 of SD2 with respect to its corresponding loss function LSD2 are positive (orange triangles), the point φ1 is a local minimum. Figure 2(b) conﬁrms that SD2 succeeds to learn an optimal policy to move the agent to high-reward region [0.5, 1.5]. To illustrate the critical effect of the softmax operator on the objective, we also evaluate the change of the loss function LSD2(φ0) of SD2 with respect to parameters φ0 from DDPG. Figure 1(c) shows that
φ0 is in an almost linear region under LSD2 (green squares), and the loss can be reduced following several directions, which demonstrates the beneﬁts of optimizing the softmax version of the objective. (a) Learned policy of DDPG. (b) Learned policy of SD2.
Figure 2: Policies of DDPG and SD2 during learning for each state (y-axis) at each step (x-axis).
To further analyze the difﬁculty of the optimization of LDDPG on a more global view, we linearly interpolate between the parameters of the policies from DDPG and SD2, i.e., αφ0 + (1 − α)φ1 (0 ≤ α ≤ 1) as in [16, 2]. Figure 1(d) illustrates the result with varying values of α. As shown, there exists at least a monotonically decreasing path in the actor loss of SD2 to a good solution. As a result, the smoothing effect of the softmax operator on the optimization landscape can help learning and reduce the number of local optima, and makes it less sensitive to different initialization. 4
4.2 Softmax Deep Deterministic Policy Gradients (SD2)
Here we present the design of SD2, which is short for Softmax Deep Deterministic Policy Gradients, where we build the softmax operator upon DDPG [31] with a single critic estimator.
Speciﬁcally, SD2 estimates the value function using the softmax operator, and the update of the critic of SD2 is deﬁned by Eq. (2), where the the actor aims to optimize a soft estimation of the return.
θ(cid:48) = θ + αEs,a∼ρ (r + γTSD2(s(cid:48)) − Q (s, a; θ)) ∇θQ(s, a; θ). (2)
In Eq. (2), TSD2(s(cid:48)) = softmaxβ(Q(s(cid:48), ·; θ−)). However, the softmax operator involves the integral, and is intractable in continuous action space. We express the Q-function induced by the softmax operator in expectation by importance sampling [18], and obtain an unbiased estimation by
Ea(cid:48)∼p (cid:20) exp(βQ(s(cid:48), a(cid:48); θ−))Q(s(cid:48), a(cid:48); θ−) p(a(cid:48)) (cid:21)
/Ea(cid:48)∼p (cid:20) exp(βQ(s(cid:48), a(cid:48); θ−)) p(a(cid:48)) (cid:21)
, (3) where p(a(cid:48)) denotes the probability density function of a Gaussian distribution. In practice, we sample actions obtained by adding noises which are sampled from a Gaussian distribution (cid:15) ∼ N (0, σ) to the target action π(s(cid:48); φ−), i.e., a(cid:48) = π(s(cid:48); φ−) + (cid:15). Here, each sampled noise is clipped to [−c, c] to ensure the sampled action is in Ac = [−c + π(s(cid:48); φ−), c + π(s(cid:48); φ−)]. This is because directly estimating TSD2(s(cid:48)) can incur large variance as 1/p(a(cid:48)) can be very large. Therefore, we limit the range of the action set to guarantee that actions are close to the original action, and that we obtain a robust estimate of the softmax Q-value. Due to space limitation, we put the full SD2 algorithm in
Appendix C.1. 4.2.1 SD2 Reduces the Overestimation Bias
Besides the smoothing effect on the optimization landscape, we show in Theorem 3 that SD2 enables a better value estimation by reducing the overestimation bias in DDPG, for which it is known that the critic estimate can cause signiﬁcant overestimation [15], where the proof is in Appendix C.2.
Theorem 3 Denote the bias of the value estimate and the true value induced by T as bias(T ) =
E [T (s(cid:48))] − E [Q(s(cid:48), π(s(cid:48); φ−); θtrue)]. Assume that the actor is a local maximizer with respect to the critic, then there exists noise clipping parameter c > 0 such that bias(TSD2) ≤ bias(TDDPG).
We validate the reduction effect in two MuJoCo [35] environments, Hopper-v2 and Walker2d-v2, where the experimental setting is the same as in Section 5.
Figure 3 shows the performance comparison between
DDPG and SD2, where the shaded area corresponds to standard deviation. The red horizontal line denotes the maximum return obtained by DDPG in evaluation during training, while the blue vertical lines show the number of steps for DDPG and SD2 to reach that score. As shown in Figure 3, SD2 signiﬁcantly outperforms DDPG in sample efﬁciency and ﬁnal performance. Estimation of value functions is shown in Figure 4(a), where value estimates are averaged over 1000 states sampled from the replay buffer at each timestep, and true values are estimated by averaging the discounted long-term rewards obtained by rolling out the current policy starting from the sampled states at each timestep. The bias of corresponding value estimates and true values is shown in Figure 4(b), where it can be observed that SD2 reduces overestimation and achieves a better estimation of value functions.
Figure 3: Performance comparison of DDPG and SD2, and Deterministic SAC.
Regarding the softmax operator in SD2, one may be interested in comparing it with the log-sum-exp operator applied in SAC [19]. To study the effect of different operators, we compare SD2 with a variant of SAC with deterministic policy and single critic for fair comparison. The performance of Deterministic SAC (with ﬁne-tuned parameter of log-sum-exp) is shown in Figure 3, which underperforms DDPG and SD2, where we also observe that its absolute bias is larger than that of
DDPG, and worsens the overestimation problem. Its value estimates can be found in Appendix C.3. 5
(a) Value estimates and true values. (b) Bias of value estimations.
Figure 4: Comparison of estimation of value functions of DDPG and SD2. 4.3 Softmax Deep Double Deterministic Policy Gradients (SD3)
In Section 4.2.1, we have analyzed the effect of the softmax operator in the aspect of value estimation based on DDPG which suffers from overestimation. We now investigate whether the softmax operator is still beneﬁcial when there is underestimation bias. We propose a novel method to leverage the softmax operator with double estimators, called Softmax Deep Double Deterministic Policy Gradients (SD3). We show that SD3 enables a better value estimation in comparison with the state-of-the-art
TD3 algorithm, which can suffer from a large underestimation bias.
TD3 [15] maintains a pair of critics as in Double Q-learning [20], which is a promising approach to al-leviate overestimation in the value-based Q-learning [37] method. However, directly applying Double
Q-learning still leads to overestimation in the actor-critic setting. To avoid the problem, Clipped Dou-ble Q-learning is proposed in TD3 [15], which clips the Q-value from the double estimator of the critic by the original Q-value itself. Speciﬁcally, TD3 estimates the value function by taking the minimum of value estimates from the two critics according to y1, y2 = r + γ mini=1,2 Qi(s(cid:48), π(s(cid:48); φ−); θ− i ).
Nevertheless, it may incur large underestimation bias, and can affect performance [23, 10].
We propose to use the softmax operator based on double estimators to address the problem. It is worth noting that a direct way to combine the softmax operator with TD3, i.e., apply the softmax operator to the Q-value from the double critic estimator and then clip it by the original Q-value, as in
Eq. (4) is ineffective. yi = r + γ min (cid:0)T −i i )(cid:1) ,
This is because according to Theorem 3, we have T −i
−i), then the value estimates result in even larger underestimation bias compared with TD3. To tackle the problem, we propose to estimate the target value for critic Qi by yi = r + γTSD3(s(cid:48)), where
SD2(s(cid:48)) = softmaxβ(Q−i(s(cid:48), ·; θ−
T −i
SD2(s(cid:48)) ≤ Q−i(s(cid:48), π(s(cid:48); φ−); θ−
SD2(s(cid:48)), Qi(s(cid:48), π(s(cid:48); φ−); θ−
−i)). (4)
TSD3(s(cid:48)) = softmaxβ (cid:17) (cid:16) ˆQi(s(cid:48), ·)
,
ˆQi(s(cid:48), a(cid:48)) = min (cid:0)Qi(s(cid:48), a(cid:48); θ− i ), Q−i(s(cid:48), a(cid:48); θ−
−i)(cid:1) . (5)
Here, target actions for computing the softmax Q-value are obtained by the same way as in the SD2 algorithm in Section 4.2. The full SD3 algorithm is shown in Algorithm 1. 4.3.1 SD3 Improves the Underestimation Bias
In Theorem 4, we present the relationship between the value estimation of SD3 and that of TD3, where the proof is in Appendix D.1.
Theorem 4 Denote TTD3, TSD3 the value estimation functions of TD3 and SD3 respectively, then we have bias(TSD3) ≥ bias(TTD3).
As illustrated in Theorem 4, the value estimation of SD3 is larger than that of TD3. As TD3 leads to an underestima-tion value estimate [15], we get that SD3 helps to improve the underestimation bias of TD3. Therefore, according to our SD2 and SD3 algorithms, we conclude that the soft-max operator can not only reduce the overestimation bias when built upon DDPG, but also improve the underestima-tion bias when built upon TD3. We empirically validate 6
Figure 5: Comparison of the bias of value estimations of TD3 and SD3.
Algorithm 1 SD3 1 ← φ1 1 ← θ1, θ− 2 ← θ2, φ− 1 ← φ1, φ− 1: Initialize critic networks Q1, Q2, and actor networks π1, π2 with random parameters θ1, θ2, φ1, φ2 2: Initialize target networks θ− 3: Initialzie replay buffer B 4: for t = 1 to T do 5: 6: 7: 8: 9: 10: 11: 12:
Select action a with exploration noise (cid:15) ∼ N (0, σ) based on π1 and π2
Execute action a, observe reward r, new state s(cid:48) and done d
Store transition tuple (s, a, r, s(cid:48), d) in B // d is the done ﬂag for i = 1, 2 do
Sample a mini-batch of N transitions {(s, a, r, s(cid:48), d)} from B
Sample K noises (cid:15) ∼ N (0, ¯σ)
ˆa(cid:48) ← πi(s(cid:48); φ−
ˆQ(s(cid:48), ˆa(cid:48)) ← minj=1,2 (cid:17) softmaxβ (cid:0)Qj(s(cid:48), ˆa(cid:48); θ−
← Eˆa(cid:48)∼p i ) + clip((cid:15), −c, c) (cid:16) ˆQ(s(cid:48), ·) (cid:105)
/Eˆa(cid:48)∼p (cid:104) exp(β ˆQ(s(cid:48),ˆa(cid:48))) p(ˆa(cid:48)) j )(cid:1) (cid:105) (cid:104) exp(β ˆQ(s(cid:48),ˆa(cid:48))) ˆQ(s(cid:48),ˆa(cid:48)) p(ˆa(cid:48)) (cid:16) ˆQ(s(cid:48), ·) (cid:17) yi ← r + γ(1 − d)softmaxβ
Update the critic θi according to Bellman loss: 1
N
Update actor φi by policy gradient: 1
N
Update target networks: θ− i ← τ θi + (1 − τ )θ− (cid:80) s (cid:80) s(Qi(s, a; θi) − yi)2 (cid:2)∇φi (π(s; φi))∇aQi(s, a; θi)|a=π(s;φi) i ← τ φi + (1 − τ )φ− i , φ− i (cid:3) 13: 14: 15: 16: 17: 18: 19: end for end for the theorem using the same two MuJoCo environments and estimation of value functions and true val-ues as in Section 4.2.1. Comparison of the bias of value estimates and true values is shown in Figure 5, where the performance comparison is in Figure 8. As shown, SD3 enables better value estimations as it achieves smaller absolute bias than TD3, while TD3 suffers from a large underestimation bias.
We also observe that the variance of value estimates of SD3 is smaller than that of TD3. 5 Experiments
In this section, we ﬁrst conduct an ablation study on SD3, from which we aim to obtain a better understanding of the effect of each component, and to further analyze the main driver of the perfor-mance improvement of SD3. Then, we extensively evaluate the SD3 algorithm on continuous control benchmarks and compare with state-of-the-art methods.
We conduct experiments on continuous control tasks from OpenAI Gym [6] simulated by MuJoCo
[35] and Box2d [8]. We compare SD3 with DDPG [24] and TD3 [15] using authors’ open-sourced implementation [14]. We also compare SD3 against Soft Actor-Critic (SAC) [19], a state-of-the-art method that also uses double critics. Each algorithm is run with 5 seeds, where the performance is evaluated for 10 times every 5000 timesteps. SD3 uses double actors and double critics based on the structure of Double Q-learning [20], with the same network conﬁguration as the default TD3 and DDPG baselines. For the softmax operator in SD3, the number of noises to sample K is 50, and the parameter β is mainly chosen from {10−3, 5 × 10−3, 10−2, 5 × 10−2, 10−1, 5 × 10−1} using grid search. All other hyperparameters of SD3 are set to be the same as the default setting for TD3 on all tasks except for Humanoid-v2, as TD3 with the default hyperparameters almost fails in Humanoid-v2. To better demonstrate the effectiveness of SD3, we therefore employ the
ﬁne-tuned hyperparameters provided by authors of TD3 [14] for Humanoid-v2 for DDPG, TD3 and
SD3. Details for hyperparameters are in Appendix E.1, and the implementation details are publicly available at https://github.com/ling-pan/SD3. 5.1 Ablation Study
We ﬁrst conduct an ablative study of SD3 in an MuJoCo environment HalfCheetah-v2 to study the effect of structure and important hyperparameters.
Structure. From Figure 6(a), we ﬁnd that for SD3 and TD3, using double actors outperforms its counterpart with a single actor. This is because using a single actor as in TD3 leads to a same training target for both critics, which can be close during training and may not fully utilize the double 7
(a) (b) (c) (d) (e)
Figure 6: Ablation study on HalfCheetah-v2 (mean ± standard deviation). (a) Structure. (b) Number of noises K. (c) Comparison with TD3-K. (d) Comparison with SD3 (averaged). (e) Parameter β. estimators. However, TD3 with double actors still largely underperforms SD3 (either with single or double actors).
The number of noises K. Figure 6(b) shows the performance of SD3 with varying number of noise samples K. The performance of all K values is competitive except for K = 2, where it fails to behave stable and also underperforms other values of K in sample efﬁciency. As SD3 is not sensitive to this parameter, we ﬁx K to be 50 in all environments as it performs best. Note that doing so does not incur much computation cost as setting K to be 50 only takes 3.28% more runtime on average compared with K = 1 (in this case the latter can be viewed as a variant of TD3 with double actors).
The effect of the softmax operator. It is also worth studying the performance of a variant of TD3 using K samples of actions to evaluate the Q-function (TD3-K). Speciﬁcally, TD3-K samples K actions by the same way as in SD3 to compute Q-values before taking the min operation (details are in Appendix E.2). As shown in Figure 6(c), TD3-K outperforms TD3 for some large values of K, but only by a small margin and still underperforms SD3. We also compare SD3 with its variant SD3 (averaged) that directly averages the K samples to compute the Q-function, which underperforms
SD3 by a large margin as shown in Figure 6(d). Results conﬁrm that the softmax operator is the key factor for the performance improvement for SD3 instead of other changes (multiple samples).
The parameter β. The parameter β of the softmax operator directly affects the estimation of value functions, and controls the bias of value estimations, which is a critical parameter for the performance.
A smaller β leads to lower variance while a larger β results in smaller bias. Indeed, there is an intermediate value that performs best that can best provide the trade-off as in Figure 6(e). 5.2 Performance Comparison
The performance comparison is shown in Figure 8, where we report the averaged performance as the solid line, and the shaded region denotes the standard deviation. As demonstrated, SD3 signiﬁcantly outperforms TD3, where it achieves a higher ﬁnal performance and is more stable due to the smoothing effect of the softmax operator on the optimization landscape and a better value estimation. Figure 7 shows the number of steps for TD3 and SD3 to reach the highest score of TD3 during training. We observe that SD3 learns much faster than TD3.
It is worth noting that SD3 outperforms SAC in most environments except for Humanoid-v2, where both algorithms are competitive. 6