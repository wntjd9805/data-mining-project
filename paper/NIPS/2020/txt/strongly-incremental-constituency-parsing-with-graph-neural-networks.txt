Abstract
Parsing sentences into syntax trees can beneﬁt downstream applications in NLP.
Transition-based parsers build trees by executing actions in a state transition system.
They are computationally efﬁcient, and can leverage machine learning to predict actions based on partial trees. However, existing transition-based parsers are predominantly based on the shift-reduce transition system, which does not align with how humans are known to parse sentences. Psycholinguistic research suggests that human parsing is strongly incremental—humans grow a single parse tree by adding exactly one token at each step.
In this paper, we propose a novel transition system called attach-juxtapose. It is strongly incremental; it represents a partial sentence using a single tree; each action adds exactly one token into the partial tree. Based on our transition system, we develop a strongly incremental parser. At each step, it encodes the partial tree using a graph neural network and predicts an action. We evaluate our parser on Penn Treebank (PTB) and
Chinese Treebank (CTB). On PTB, it outperforms existing parsers trained with only constituency trees; and it performs on par with state-of-the-art parsers that use dependency trees as additional training data. On CTB, our parser establishes a new state of the art. Code is available at https://github.com/princeton-vl/ attach-juxtapose-parser. 1

Introduction
Constituency parsing is a core task in natural language processing. It recovers the syntactic structures of sentences as trees (Fig. 1). State-of-the-art parsers are based on deep neural networks and typically consist of an encoder and a decoder. The encoder embeds input tokens into vectors, from which the decoder generates a parse tree. A main class of decoders builds trees by executing a sequence of actions in a state transition system [32, 12, 22]. These transition-based parsers achieve linear runtime in sentence length. More importantly, they construct partial trees during decoding, enabling the parser to leverage explicit structural information for predicting the next action.
Most existing transition-based parsers adopt the shift-reduce transition system [32, 12, 22]. They represent the partial sentence as a stack of subtrees. At each step, the parser either pushes a new token onto the stack (shift) or combines two existing subtrees in the stack (reduce).
Despite their empirical success, shift-reduce parsers appear to differ from how humans are known to perform parsing. Psycholinguistic research [25, 37, 35] has suggested that human parsing is strongly incremental: at each step, humans process exactly one token—no more, no less—and integrate it into a single parse tree for the partial sentence. In a shift-reduce system, however, only shift actions process new tokens, and the partial sentence is represented as a stack of disconnected subtrees rather than a single connected tree. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The constituency tree for “Arthur is King of the Britons.” Leaves are labeled with tokens, and internal nodes are labeled with syntactic categories, e.g., S for sentence, NP for noun phrase, VP for verb phrase, and PP for prepositional phrase.
This observation puts forward an intriguing question: can a strongly incremental transition system lead to a better parser? Intuitively, a strongly incremental system is more aligned with human processing; as a result, the sequence of actions may be easier to learn.
Attach-juxtapose transition system We propose a novel transition system named attach-juxtapose, which enables strongly incremental constituency parsing. For a sentence of length n, we start with an empty tree and execute n actions; each action integrates exactly one token into the current partial tree, deciding on where and how to integrate the new token. There are two types of actions: attach, which attaches the new token as a child to an existing node, and juxtapose, which juxtaposes the new token as a sibling to an existing node while also creating a shared parent node (Fig. 2). We can prove that any parse tree without unary chains can be constructed by a unique sequence of actions in this attach-juxtapose system.
Being strongly incremental, our system represents the state as a single tree rather than a stack of multiple subtrees. Not only is the single-tree representation more aligned with humans, but it also allows us to tap into a large inventory of model architectures for learning from graph data, such as
TreeLSTMs [39] and graph neural networks (GNNs) [19]. Further, the single-tree representation provides valid syntax trees for partial sentences, which is impossible in bottom-up shift-reduce systems [32]. Taking “Arthur is King of the Britons” as an example, we can produce a valid tree for the preﬁx “Arthur is King” (Fig. 2 Bottom). Whereas in bottom-up shift-reduce systems, you must complete the subtree for “is King of the Britons” before connecting it to “Arthur.” Therefore, our representation captures the complete syntactic structure of the partial sentence, and thus provides stronger guidance for action generation.
Our transition system can be understood as a refactorization of In-order Shift-reduce System (ISR) proposed by Liu and Zhang [22]. We prove that a sequence of actions in our system can be translated into a sequence of actions in ISR, but our sequence is shorter (Theorem 4). Speciﬁcally, to generate a parse tree with n leaves and m internal nodes (assuming no unary chains), our sequence has length n, whereas ISR has length n + 2m. On the other hand, each of our actions has a larger number of choices, resulting in a different trade-off between the sequence length and the number of choices per action. We hypothesize that our system achieves a trade-off more amenable to machine learning due to closer alignment with human processing.
Action generation with graph neural networks Based on the attach-juxtapose system, we de-velop a strongly incremental parser by training a deep neural network to generate actions. Speciﬁcally, we adopt the encoder in prior work [21, 49] and propose a novel graph-based decoder. It uses GNNs to learn node features in the partial tree, and uses attention to predict where and how to integrate the new token. To our knowledge, this is the ﬁrst time GNNs are applied to constituency parsing.
We evaluate our method on two standard benchmarks for constituency parsing: Penn Treebank (PTB) [24] and Chinese Treebank (CTB) [46]. On PTB, our method outperforms existing parsers trained with only constituency trees. And it performs competitively with state-of-the-art parsers that use dependency trees as additional training data. On CTB, we achieve an F1 score of 93.59—a signif-icant improvement of 0.95 upon previous best results. These results demonstrate the effectiveness of our strongly incremental parser. 2
Contributions Our contributions are threefold. First, we propose attach-juxtapose, a novel transi-tion system for constituency parsing. It is strongly incremental and motivated by psycholinguistics.
Second, we provide theoretical results characterizing its capability and its connections with an exist-ing shift-reduce system [22]. Third, we develop a parser by generating actions in the attach-juxtapose system. Our parser achieves state-of-the-art performance on two standard benchmarks. 2