Abstract
−
This work proposes a novel analysis of stochastic gradient descent (SGD) for non-convex and smooth optimization. Our analysis sheds light on the impact of the probability distribution of the gradient noise on the convergence rate of the norm of the gradient. In the case of sub-Gaussian and centered noise, we prove that, with
δ, the number of iterations to reach a precision ε for the squared probability 1 gradient norm is O(ε−2 ln(1/δ)). In the case of centered and integrable heavy-tailed noise, we show that, while the expectation of the iterates may be inﬁnite,
δ in O(ε−pδ−q) the squared gradient norm still converges with probability 1 iterations, where p, q > 2. This result shows that heavy-tailed noise on the gradient slows down the convergence of SGD without preventing it, proving that SGD is robust to gradient noise with unbounded variance, a setting of interest for Deep
Learning. In addition, it indicates that choosing a step size proportional to T −1/b where b is the tail-parameter of the noise and T is the number of iterations leads to the best convergence rates. Both results are simple corollaries of a uniﬁed analysis using the novel concept of biased expectations, a simple and intuitive mathematical tool to obtain concentration inequalities. Using this concept, we propose a new quantity to measure the amount of noise added to the gradient, and discuss its value in multiple scenarios.
− 1

Introduction
Stochastic Gradient Descent (SGD) and its variants (Adam [1], RMSProp [2], or Nesterov’s accel-erated gradient descent [3]) are used in a wide variety of tasks to train Machine Learning models.
Indeed, the scalability of many learning algorithms, such as support vector machines [4], logistic regression [5], Lasso [6] and more recently deep neural networks [7] essentially rely on the efﬁciency (and robustness) of stochastic optimization methods. However, one speciﬁcity of SGD is its inherent noise which originates either from the sampling of training points, the presence of noise in the gradients computation, or the shape of the target function [8, 9, 10]. While SGD is known to be robust in practice and its convergence behavior is well-understood in the convex setting [11, 12, 3, 13], many of its properties are not yet fully understood, and particularly in settings related to Deep Learning practice where gradients can be extremely noisy and the target function presents many local optima.
In particular, settings with unbounded variance noise were recently shown to appear [14, 15] when training NLP models such as BERT [16] over large corpora, and vision models such as AlexNet
[17] on Cifar10. As a result, SGD may present instabilities that are often solved by running the optimization multiple times, a technique refered to as multi-start. Recently, several authors explored these frameworks by adapting the tools developed in convex analysis to the non-convex setting in order to explain these phenomena [15, 18, 19, 20, 21, 22, 23, 24, 25]. However, none of these works proposed a uniﬁed framework able to handle both bounded and heavy-tailed noises.
This paper aims at ﬁlling this gap by providing a novel uniﬁed analysis of the convergence of SGD in a non-convex and noisy setting. With regards to the above mentioned works, our contribution is 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
threefold. First, we introduce a novel mathematical tool: biased expectation which allows to derive many results in stochastic analysis. Second, we show how to use this tool in the context of stochastic non-convex optimization to handle a large panel of noise assumptions. Third, the probabilistic bounds we obtain for SGD (i.e., on quantiles) provide novel insights over the previously known in-expectation bounds as they allow to consider heavy-tailed noise distributions with inﬁnite variance, explaining why multi-start methods work by showing that a small number of runs of SGD will exhibit good convergence and not be disrupted by extreme noise.
The rest of the paper is organized as follows. In Section 2, we introduce the biased expectation and its main properties. In Section 3, we introduce the optimization framework of the analysis and present our main result. In Section 4, we explicit the convergence rates for various noise assumptions. Finally, the results obtained in the paper are illustrated in an empirical assessment in Section 5. All proofs can be found in the Supplementary Material provided as a separate document.
≤ 2] (cid:107)∇ f (xt) (cid:107)