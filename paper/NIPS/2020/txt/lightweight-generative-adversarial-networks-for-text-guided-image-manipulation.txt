Abstract
We propose a novel lightweight generative adversarial network for efﬁcient image manipulation using natural language descriptions. To achieve this, a new word-level discriminator is proposed, which provides the generator with ﬁne-grained training feedback at word-level, to facilitate training a lightweight generator that has a small number of parameters, but can still correctly focus on speciﬁc visual attributes of an image, and then edit them without affecting other contents that are not described in the text. Furthermore, thanks to the explicit training signal related to each word, the discriminator can also be simpliﬁed to have a lightweight structure. Compared with the state of the art, our method has a much smaller number of parameters, but still achieves a competitive manipulation performance. Extensive experimental results demonstrate that our method can better disentangle different visual attributes, then correctly map them to corresponding semantic words, and thus achieve a more accurate image modiﬁcation using natural language descriptions. 1

Introduction
How to effectively edit a given image without tedious human operation is a challenging but mean-ingful task, which may potentially boost enormous applications in different areas, such as design, architecture, video games, and art. Recently, with the great progress on the development of deep learning, various applications in terms of image manipulation have been developed, including style transfer [5, 6, 9, 11], image colourisation [2, 13, 32], and image translation [3, 10, 20, 21, 26, 28].
Differently from above works, the goal of this paper is to provide a more user-friendly method, which can automatically edit a given image by simply using natural language descriptions. In particular, we aim to semantically modify parts of an image (e.g., colour, texture, and global style) according to user-provided text descriptions, where the descriptions contain desired visual attributes that the modiﬁed image should have. Meanwhile, the modiﬁed result should preserve text-irrelevant contents of the original image that are not required by the text.
Currently, only few studies [4, 15, 19] work on this task. Methods introduced in [4, 19] both fail to effectively modify text-required attributes, and results are also far from satisfactory (see Fig. 1).
Recently, Li et al. [15] proposed a new multi-stage network, which is able to produce more realistic images. However, as the model [15] is based on a multi-stage framework with multiple pairs of generator and discriminator, it very likely requires a large memory and needs a lot of time for training and inference, which is less practical to memory-limited devices, such as mobile phones.
This motivates us to investigate a lightweight architecture of the network. However, simply reducing the number of stages and parameters in the model [15] cannot achieve a satisfactory result, shown in
Fig. 1 (e). As we can see, compared with the original ManiGAN (d), the image quality of synthetic results drops signiﬁcantly as both images are obviously blurred. Also, the manipulation ability of this 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Examples of image manipulation using natural language descriptions. Our method has a lightweight architecture, but can still allow the input images to be manipulated accurately matching the descriptions. “ManiGAN*” denotes we reduce the number of stages and parameters in the model. lightweight ManiGAN becomes worse, because more regions (e.g., sky and branches) are coloured red. After a close investigation, we ﬁnd that the discriminator used in the model [15] fails to provide the generator with ﬁne-grained training feedback related to each word, because it only calculates the similarity between the whole text and image features. Due to the coarse supervisory feedback from discriminators, the network needs to have a heavy structure with a large number of parameters, to build an accurate relation between visual attributes and corresponding semantic words to achieve an effective manipulation, which greatly impedes the construction of a lightweight architecture.
Additionally, even in the original ManiGAN, this poor training feedback prevents the model from completely disentangling different visual attributes, causing an incorrect mapping between attributes and corresponding semantic words. Thus, some text-irrelevant contents are changed. For example, as shown in Fig. 1 (d), the branch is coloured red, and the background of the vase is modiﬁed as well.
Based on this, we propose a new word-level discriminator along with explicit word-level supervisory labels, which can provide the generator with detailed training feedback related to each word, to facilitate training a lightweight generator that has a small number of parameters but can still effectively disentangle different visual attributes, and then correctly map them to the corresponding semantic words. Besides, thanks to our powerful discriminator to provide explicit word-level training feedback, our network can be simpliﬁed to have only a generator network and a discriminator network, and we can even further reduce the number of parameters in the model without sacriﬁcing much image quality, which is more friendly to memory-limited devices.
To this end, we evaluate our model on the CUB bird [27] and more complicated COCO [17] datasets, which demonstrates that our method can accurately modify the given image using natural language descriptions with great efﬁciency. Also, extensive experiments on both datasets show the superiority of our method, in terms of both visual ﬁdelity and efﬁciency. The code will be available at https://github.com/mrlibw/Lightweight-Manipulation. 2