Abstract
One of the key reasons for the high sample complexity in reinforcement learning (RL) is the inability to transfer knowledge from one task to another. In standard multi-task RL settings, low-reward data collected while trying to solve one task provides little to no signal for solving that particular task and is hence effectively wasted. However, we argue that this data, which is uninformative for one task, is likely a rich source of information for other tasks. To leverage this insight and efﬁciently reuse data, we present Generalized Hindsight: an approximate inverse reinforcement learning technique for relabeling behaviors with the right tasks.
Intuitively, given a behavior generated under one task, Generalized Hindsight returns a different task that the behavior is better suited for. Then, the behavior is relabeled with this new task before being used by an off-policy RL optimizer.
Compared to standard relabeling techniques, Generalized Hindsight provides a substantially more efﬁcient re-use of samples, which we empirically demonstrate on a suite of multi-task navigation and manipulation tasks. (Website1) 1

Introduction
Model-free reinforcement learning (RL) combined with powerful function approximators has achieved remarkable success in games like Atari [43] and Go [64], and control tasks like walking [24] and
ﬂying [33]. However, a key limitation to these methods is their sample complexity. They often require millions of samples to learn simple locomotion skills, and sometimes even billions of samples to learn more complex game strategies. Creating general purpose agents will necessitate learning multiple such skills or strategies, which further exacerbates the inefﬁciency of these algorithms. On the other hand, humans (or biological agents) are not only able to learn a multitude of different skills, but from orders of magnitude fewer samples [32]. So, how do we endow RL agents with this ability to learn efﬁciently across multiple tasks?
One key hallmark of biological learning is the ability to learn from mistakes. In RL, mistakes made while solving a task are only used to guide the learning of that particular task. But data seen while making these mistakes often contain a lot more information. In fact, extracting and re-using this information lies at the heart of most efﬁcient RL algorithms. Model-based RL re-uses this information to learn a dynamics model of the environment. However for several domains, learning a robust model is often more difﬁcult than directly learning the policy [15], and addressing this challenge continues to remain an active area of research [46]. Another way to re-use low-reward data is off-policy RL, where in contrast to on-policy RL, data collected from an older policy is re-used while optimizing the new policy. But in the context of multi-task learning, this is still inefﬁcient since data generated 1Website: sites.google.com/view/generalized-hindsight 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Trajectories τ (zi), collected trying to maximize r(·|zi), may contain very little reward signal about how to solve their original tasks. Generalized Hindsight checks against randomly sampled “candidate tasks"
{vi}K i for which these trajectories can serve as “pseudo-demonstrations." Using off-policy RL, we can obtain stronger reward signal from these relabeled trajectories. i=1 to ﬁnd different tasks z(cid:48) from one task cannot effectively inform a different task. Towards solving this problem, work by
Andrychowicz et al. [2] focuses on extracting even more information through hindsight.
In goal-conditioned settings, where tasks are deﬁned by a sparse goal, Hindsight Experience Replay (HER) [2] relabels the desired goal, for which a trajectory was generated, to a state seen in that trajectory. Therefore, if the goal-conditioned policy erroneously reaches an incorrect goal instead of the desired goal, we can re-use this data to teach it how to reach this incorrect goal. Hence, a low-reward trajectory under one desired goal is converted to a high-reward trajectory for the unintended goal. This new relabeling provides a strong supervision and produces signiﬁcantly faster learning. However, a key assumption made in this framework is that goals are a sparse set of states that need to be reached. This allows for efﬁcient relabeling by simply setting the relabeled goals to the states visited by the policy. But for several real world problems like energy-efﬁcient transport, or robotic trajectory tracking, rewards are often complex combinations of desirables rather than sparse objectives. So how do we use hindsight for general families of reward functions?
In this paper, we build on the ideas of goal-conditioned hindsight and propose Generalized Hindsight.
Here, instead of performing hindsight on a task-family of sparse goals, we perform hindsight on a task-family of reward functions. Since dense reward functions can capture a richer task speciﬁcation, GH allows for better re-utilization of data. Note that this is done along with solving the task distribution induced by the family of reward functions. However, for relabeling, instead of simply setting visited states as goals, we now need to compute the reward functions that best explain the generated data. To do this, we draw connections from Inverse Reinforcement Learning (IRL), and propose an Approximate IRL Relabeling algorithm we call AIR. Concretely, AIR takes a new trajectory and compares it to K randomly sampled tasks from our distribution. It selects the task for which the trajectory is a “pseudo-demonstration," i.e. the trajectory achieves higher performance on that task than any of our previous trajectories. This “pseudo-demonstration" can then be used to quickly learn how to perform that new task. We illustrate the process in Figure 1. We test our algorithm on several multi-task control tasks, and ﬁnd that AIR consistently achieves higher asymptotic performance using as few as 20% of the environment interactions as our baselines. We also introduce a computationally more efﬁcient version, which relabels by comparing trajectory rewards to a learned baseline, that also achieves higher asymptotic performance than our baselines.
In summary, we present three key contributions in this paper: (a) we extend the ideas of hindsight to the generalized reward family setting; (b) we propose AIR, a relabeling algorithm using insights from IRL. This connection has been concurrently and independently studied in [17], with additional discussion in Section 4.5; (c) we demonstrate signiﬁcant improvements in multi-task RL on a suite of multi-task navigation and manipulation tasks. 2