Abstract
How meaning is represented in the brain is still one of the big open questions in neuroscience. Does a word (e.g., bird) always have the same representation, or does the task under which the word is processed alter its representation (answering
“can you eat it?” versus “can it ﬂy?”)? The brain activity of subjects who read the same word while performing different semantic tasks has been shown to differ across tasks. However, it is still not understood how the task itself contributes to this difference. In the current work, we study Magnetoencephalography (MEG) brain recordings of participants tasked with answering questions about concrete nouns. We investigate the effect of the task (i.e. the question being asked) on the processing of the concrete noun by predicting the millisecond-resolution MEG recordings as a function of both the semantics of the noun and the task. Using this approach, we test several hypotheses about the task-stimulus interactions by comparing the zero-shot predictions made by these hypotheses for novel tasks and nouns not seen during training. We ﬁnd that incorporating the task semantics signiﬁcantly improves the prediction of MEG recordings, across participants. The improvement occurs 475 − 550ms after the participants ﬁrst see the word, which corresponds to what is considered to be the ending time of semantic processing for a word. These results suggest that only the end of semantic processing of a word is task-dependent, and pose a challenge for future research to formulate new hypotheses for earlier task effects as a function of the task and stimuli. 1

Introduction
One of the central goals of artiﬁcial intelligence (AI) is to build intelligent systems that understand the meaning of concepts and use it to perform tasks in the real world. Despite the great strides in learning representations, there are still many problems that could beneﬁt from further improvements in understanding and representing meaning, such as symbol grounding, common-sense reasoning, and natural language understanding. While machines are limited in these areas, we do have one system that is capable of representing meaning and performing these tasks well: the human brain. Thus, looking to the brain for insights about how we represent and compose meaning may be beneﬁcial.
Studies of meaning representation in neuroscience have revealed that the brain accesses meaning differently depending on the demands of a task [1, 2, 3, 4, 5]. For instance, the recorded brain activity of a participant that observes the word “cat” differs according to whether the participant is asked to answer whether “cat” is an animal or a vegetable [6]. The difference is shown to occur between
∗
Equal contribution and joint lead authorship.
Code available at https://github.com/otiliastr/brain_task_effect. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
400 − 600ms after “cat” is presented to the participant, a period when it is believed that the brain processes the semantics of the perceived word [7], suggesting an interaction between the task and stimulus meaning. One hypothesis for the interaction that has received some experimental backing is that, in order to solve the task, the brain uses attention mechanisms to emphasize task-relevant information [8, 9, 10, 11, 12]. However, the computational principle behind this attention mechanism is poorly understood, as it can be due to several neural properties, such as an increased response gain, sharper tuning [13], or a tuning shift [11].
In this work, we propose the ﬁrst computational model that implements precise hypotheses for the interaction between the semantics of tasks and that of individual concepts in the brain, and tests their ability to explain brain activity. We posit that formulating such a computational model will be a helpful step towards specifying a full account of the task-stimulus interactions. Speciﬁcally, we study how tasks interact with the semantics of concepts by building models that predict recorded brain activity of people tasked with answering questions (e.g., “is it bigger than a microwave?”) about concrete nouns (e.g., “bear”). Importantly, the proposed model is able to generalize to previously unseen tasks and stimuli, allowing us to make zero-shot predictions of brain recordings.
Using this computational framework, we show that models that predict brain recordings as a function of the task semantics signiﬁcantly outperform ones that do not during time windows (475 − 550ms and 600 − 650ms) which largely coincide with the end of semantic processing of a word, typically thought to last until 600ms [7]. This result suggests that only the end of semantic processing of a word becomes task-dependent and that this effect is related to the meaning of the task. We believe that in addition to this result, neuroscientists will also be interested in the ability to computationally compare different hypotheses for the task-stimuli interactions, and we hope that our general problem formulation will beneﬁt future research attempting to study other forms of interaction not considered in this work. Additionally, our work may be of interest to the AI community. Further understanding task effects on concept meaning in the brain may provide insights into building AI models that learn how to combine representations speciﬁc to the task with task-invariant representations of concepts, as a step towards composing meaning that is both goal-oriented and more easily adaptable to new tasks.
Our main contributions can be summarized as follows:
• We propose a means of representing the semantics of the question task that shows a signiﬁcant relationship with the elicited brain response. We believe such an approach could be useful to future studies on question-answering in the brain.
• We provide the ﬁrst methodology that can predict brain recordings as a function of both the observed stimulus and question task. This is important because it will not only encourage neuroscientists to formulate mechanistic computational hypotheses about the effect of a question on the processing of a stimulus, but also enable neuroscientists to test these different hypotheses against each other by evaluating how well they can align with brain recordings. While we have implemented and compared several hypotheses for this effect, and have found some to be better than others, parts of the MEG recordings remain to be explained by future hypotheses. We hope neuroscientists will build on our method to formulate and test such future hypotheses. We make our code publicly available to facilitate this.
• We perform all learning in a zero-shot setting, in which neither the stimulus nor the question used to evaluate the learned models is seen during training (i.e. not just as the speciﬁc stimulus-question pair but also in combination with any other question/stimulus). Note that this is not the case in previous work that examines task effects, and we are the ﬁrst to demonstrate how zero-shot learning can be applied successfully to this question. This is important for scientiﬁc discovery because it can test the generalization of the results beyond the experimental stimuli and tasks.
• We show that models that integrate task and stimulus representations have signiﬁcantly higher prediction performance than models that do not account for the task semantics, and localize the effect of task semantics largely to time-windows in 475 − 650ms after the stimulus presentation. 2