Abstract
Deep reinforcement learning (RL) is computationally demanding and requires processing of many data points. Synchronous methods enjoy training stability while having lower data throughput. In contrast, asynchronous methods achieve high throughput but suffer from stability issues and lower sample efﬁciency due to ‘stale policies.’ To combine the advantages of both methods we propose High-Throughput Synchronous Deep Reinforcement Learning (HTS-RL). In HTS-RL, we perform learning and rollouts concurrently, devise a system design which avoids ‘stale policies’ and ensure that actors interact with environment replicas in an asynchronous manner while maintaining full determinism. We evaluate our approach on Atari games and the Google Research Football environment.
Compared to synchronous baselines, HTS-RL is 2 − 6× faster. Compared to state-of-the-art asynchronous methods, HTS-RL has competitive throughput and consistently achieves higher average episode rewards. 1

Introduction
Deep reinforcement learning (RL) has been impressively successful on a wide variety of tasks, including playing of video games [1, 6, 12, 13, 20, 21, 23–25, 32] and robotic control [10, 18, 22].
However, a long training time is a key challenge hindering deep RL to scale to even more complex tasks. To counter the often excessive training time, RL frameworks aim for two properties: (1) A high throughput which ensures that the framework collects data at very high rates. (2) A high sample efﬁciency which ensures that the framework learns the desired policy with fewer data. To achieve both, synchronous and asynchronous parallel actor-learners have been developed which accelerate
RL training [1, 6–8, 19, 25, 29, 34].
State-of-the-art synchronous methods, such as synchronous advantage actor critic (A2C) [6, 25] and related algorithms [26, 27, 35] are popular because of their data efﬁciency, training stability, full determinism, and reproducibility. However, synchronous methods suffer from idle time as all actors need to ﬁnish experience collection before trainable parameters are updated. This is particularly problematic when the time for an environment step varies signiﬁcantly. As a result, existing synchronous methods don’t scale to environments where the step time varies signiﬁcantly due to computationally intensive 3D-rendering and (physics) simulation.
Instead, asynchronous methods, e.g., asynchronous advantage actor-critic on a GPU (GA3C) [1] and importance weighted actor-learner architectures (IMPALA) [7], achieve high throughput. However, these methods suffer from a stale-policy issue [1, 7] as learning and data collecting are performed asynchronously. The policy which is used to collect data (behavior policy) is several updates behind the latest policy (target policy) used for parameter updates. This lag leads to noisy gradients and thus lower data efﬁciency. Consequently, asynchronous methods trade training stability for throughput.
We show that this trade-off is not necessary and propose High-Throughput Synchronous RL (HTS-RL), a technique, which achieves both high throughput and high training stability. HTS-RL can be applied to many off-the-shelf deep RL algorithms, such as A2C, proximal policy optimization 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Actor 1
Learner 1 
Actor 2
Learner 2  parameters
Actor 3
Learner 3  gradients
Shared
Model
Actor 4
Learner 4 
Actor 1
Actor 2
Actor 3
Actor 4 (a) A3C parameters
Actor 1
Actor 2 (s, a, r)
Actor 3
Actor 4
Data 
Storage 
Learner  parameters parameters (s, a, r)
Training
Queue 
Learner state
Prediction 
Queue 
Predictor  actions (b) GA3C actions action
Executor 1 state
Actor 1
Actor 2 (s, a, r)
Data Queue 
Actor 3
Actor 4
Learner 1 
Learner 2  (c) IMPALA parameters 1 2 3 4
Executor 2
Executor 3 1 2 3 4 polling
Actor 1 state
Actor 2
Action Buffer
Executor 4
State Buffer
Data 
Storage 1
Data 
Storage 2
Learner 1
Learner 2 (d) A2C (s, a, r) (e) HTS-RL
Figure 1: Structure and ﬂow of (a) A3C, (b) GA3C, (c) IMPALA, (d) A2C and (e) our HTS-RL.
Actor 1 forward 
Actor 2 forward 
Actor 3 forward 
Actor 4 forward 
Learner forward 
Learner backward
Env. step
Actor/learner 1
Actor/learner 2
Actor/learner 3
Actor/learner 4
Actor 1
Actor 2
Actor 3
Actor 4
Learner 1
Learner 2
Actor 1
Actor 2
Actor 3
Actor 4
Learner 
Executor 1
Executor 2
Executor 3
Executor 4
Learner 1
Learner 2 (a) A3C (b) GA3C / IMPALA (c) A2C (d) HTS-RL
Figure 2: Processing timeline of (a) A3C , (b) GA3C/IMPALA, (c) A2C and (d) our HTS-RL. (PPO) [27], and actor-critic using Kronecker-factored trust region (ACKTR) [35]. HTS-RL is particularly suitable for environments with large step time variance: It performs concurrent learning and data collection, which signiﬁcantly increases throughput. Additionally, actors interact with environment replicas in an asynchronous manner. The asynchronous interaction reduces the idle time. Lastly, by performing batched synchronization, HTS-RL ensures that the lag between target and behavior policy equals one. Thanks to this constant latency, HTS-RL uses a ‘one step delayed-gradient’ update which has the same convergence rate as a non-delayed version. As a result, HTS-RL maintains the advantages of synchronous RL, i.e., data efﬁciency, training stability, full determinism, and reproducibility, while achieving speedups, especially in environments where the step time varies.
We show that HTS-RL permits to speedup A2C and PPO on Atari environments [2, 3] and the Google
Research Football environment (GFootball) [15]. Following the evaluation protocol of Henderson et al. [11] and Colas et al. [4], we compare to the state-of-the-art, asynchronous method, IMPALA [7]: our approach has 3.7× higher average episode reward on Atari games, and 43% higher average game scores in the GFootball environment. When compared with synchronous A2C and PPO, our approach achieves a 2 − 6× speedup. Code is available at https://github.com/IouJenLiu/HTS-RL. 2