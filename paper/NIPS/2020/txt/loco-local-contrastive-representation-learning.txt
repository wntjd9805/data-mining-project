Abstract
Deep neural nets typically perform end-to-end backpropagation to learn the weights, a procedure that creates synchronization constraints in the weight update step across layers and is not biologically plausible. Recent advances in unsuper-vised contrastive representation learning invite the question of whether a learning algorithm can also be made local, that is, the updates of lower layers do not directly depend on the computation of upper layers. While Greedy InfoMax [39] separately learns each block with a local objective, we found that it consistently hurts readout accuracy in state-of-the-art unsupervised contrastive learning algorithms, possibly due to the greedy objective as well as gradient isolation. In this work, we discover that by overlapping local blocks stacking on top of each other, we effectively increase the decoder depth and allow upper blocks to implicitly send feedbacks to lower blocks. This simple design closes the performance gap between local learning and end-to-end contrastive learning algorithms for the ﬁrst time. Aside from stan-dard ImageNet experiments, we also show results on complex downstream tasks such as object detection and instance segmentation directly using readout features. 1

Introduction
Most deep learning algorithms nowadays are trained using backpropagation in an end-to-end fashion: training losses are computed at the top layer and weight updates are computed based on the gradient that ﬂows from the very top. Such an algorithm requires lower layers to “wait” for upper layers, a synchronization constraint that seems very unnatural in truly parallel distributed processing. Indeed, there are evidences that weight synapse updates in the human brain are achieved through local learning, without waiting for neurons in other parts of the brain to ﬁnish their jobs [8, 6]. In addition to biological plausibility aims, local learning algorithms can also signiﬁcantly reduce memory footprint during training, as they do not require saving the intermediate activations after each local module ﬁnish its calculation. With these synchronization constraints removed, one can further enable model parallelism in many deep network architectures [45] for faster parallel training and inference.
One main objection against local learning algorithms has always been the need for supervision from the top layer. This belief has recently been challenged by the success of numerous self-supervised contrastive learning algorithms [54, 22, 44, 11], some of which can achieve matching performance compared to supervised counterparts, meanwhile using zero class labels during the representation learning phase. Indeed, Löwe et al. [39] show that they can separately learn each block of layers using local contrastive learning by putting gradient stoppers in between blocks. While the authors show matching or even sometimes superior performance using local algorithms, we found that their gradient isolation blocks still result in degradation in accuracy in state-of-the-art self-supervised learning frameworks, such as SimCLR [11]. We hypothesize that, due to gradient isolation, lower layers are unaware of the existence of upper layers, and thus failing to deliver the full capacity of a deep network when evaluating on large scale datasets such as ImageNet [16].
Work done at Uber ATG 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To bridge the gradient isolation blocks and allow upper layers to inﬂuence lower layers while maintaining localism, we propose to group two blocks into one local unit and share the middle block simultaneously by two units. As shown in the right part of Fig. 1. Thus, the middle blocks will receive gradients from both the lower portion and the upper portion, acting like a gradient “bridge.” We found that such a simple scheme signiﬁcantly bridges the performance gap between Greedy InfoMax [39] and the original end-to-end algorithm [11].
On ImageNet unsupervised representation learning benchmark, we evaluate our new local learning algorithm, named LoCo, on both ResNet [25] and ShufﬂeNet [40] architectures and found the conclusion to be the same. Aside from ImageNet object classiﬁcation, we further validate the generalizability of locally learned features on other downstream tasks such as object detection and semantic segmentation, by only training the readout headers. On all benchmarks, our local learning algorithm once again closely matches the more costly end-to-end trained models.
We ﬁrst review related literature in local learning rules and unsupervised representation learning in
Section 2, and further elaborate the background and the two main baselines SimCLR [11] and Greedy
InfoMax [39] in Section 3.2. Section 4 describes our LoCo algorithm in detail. Finally, in Section 5, we present ImageNet-1K [16] results, followed by instance segmentation results on MS-COCO [37] and Cityscapes [15]. 2