Abstract
Recent research has established sufﬁcient conditions for ﬁnite mixture models to be identiﬁable from grouped observations. These conditions allow the mixture components to be nonparametric and have substantial (or even total) overlap. This work proposes an algorithm that consistently estimates any identiﬁable mixture model from grouped observations. Our analysis leverages an oracle inequality for weighted kernel density estimators of the distribution on groups, together with a general result showing that consistent estimation of the distribution on groups implies consistent estimation of mixture components. A practical implementation is provided for paired observations, and the approach is shown to outperform existing methods, especially when mixture components overlap signiﬁcantly. 1

Introduction
In statistics and machine learning, ﬁnite mixture models are often used to describe the distribution of subpopulations within a larger population. A ﬁnite mixture model can be written p =
M (cid:88) m=1 w∗ mp∗ m, (1) m=1 w∗ m > 0 are mixing weights such that (cid:80)M where w∗ m are probability densities.
Without additional assumptions, the mixture model p is not identiﬁable from iid data. Typically, identiﬁability is ensured by restricting the p∗ m to some family of parametric distributions. Restricting the p∗ m to be Gaussian yields the Gaussian mixture model (GMM) which is identiﬁable [1, 2].
Most work on estimating mixture models assumes an iid sampling scheme. In this work we examine an alternative sampling scheme where observations occur in iid groups. Each group is generated by sampling a component m ∈ [M ] according to w∗ m, and then drawing N iid observations from p∗ m. m = 1, and p∗
Recent work has shown that any ﬁnite mixture model is identiﬁable given grouped observations of sufﬁcient size [3]. In the worst case, any ﬁnite mixture model with M components is identiﬁable given groups of size N ≥ 2M − 1. It was also shown that, if the underlying components of the mixture model are jointly irreducible [4], then the mixture is identiﬁable given paired observations (N = 2). This framework provides a setting where the potential exists to recover nonparametric and highly overlapping mixture components. As of yet, however, no general theory or algorithms are known for this estimation problem.
This work makes the following contributions. We introduce a novel variant of the kernel density estimator that yields statistically consistent estimates of any identiﬁable nonparametric mixture
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
model (NoMM) from grouped observations. To prove this result, we establish an oracle inequality for weighted kernel density estimators. We also establish a general result showing that consistent estimation (with an estimator possessing a natural factored form) of the distribution on groups implies consistent estimates of the underlying components when the NoMM is identiﬁable. The only additional condition imposed by our theory is that the p∗ m be square integrable. In the case of N = 2, we offer an efﬁcient algorithm and demonstrate its effectiveness on several datasets.
We study two applications where paired observations naturally arise. The ﬁrst is nuclear source detection, where nuclear particles interact with a detector to produce some form of measurement.
A critical challenge in this application is to classify incoming particles as belonging to source or background. Because of changing environments, training data are typically not available, and these two classes also have substantial overlap. By positioning two detectors side-by-side, it is possible to simultaneously measure two particles from the same (unknown) class.
We also apply our method to topic modeling of Twitter data. Since tweets usually express a small set of very closely related ideas, words in tweets contain common underlying semantic information.
The pairing of words has the potential to encode this semantic information in a way that accounts for context. The proposed method, which operates on continuous word embeddings, allows for ﬂexible modeling of the distributions of topics over words using static word embeddings [5]. Furthermore our method does not require anchor words, allows for substantial overlap of topics without loss of identiﬁability, and can be trained using documents with as few as two words without any document aggregation [6, 7]. While other works have explored topic modeling with word embeddings, which we call continuous topic modeling, most either impose parametric assumptions or are not suited for very short texts. To our knowledge, this is the ﬁrst work to consider a nonparametric approach to continuous topic modeling of very short texts. 2