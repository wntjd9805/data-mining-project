Abstract
Invertible neural networks based on coupling ﬂows (CF-INNs) have various ma-chine learning applications such as image synthesis and representation learning.
However, their desirable characteristics such as analytic invertibility come at the cost of restricting the functional forms. This poses a question on their repre-sentation power: are CF-INNs universal approximators for invertible functions?
Without a universality, there could be a well-behaved invertible transformation that the CF-INN can never approximate, hence it would render the model class unreliable. We answer this question by showing a convenient criterion: a CF-INN is universal if its layers contain afﬁne coupling and invertible linear functions as special cases. As its corollary, we can afﬁrmatively resolve a previously unsolved problem: whether normalizing ﬂow models based on afﬁne coupling can be uni-versal distributional approximators. In the course of proving the universality, we prove a general theorem to show the equivalence of the universality for certain diffeomorphism classes, a theoretical insight that is of interest by itself. 1

Introduction
Invertible neural networks based on coupling ﬂows (CF-INNs) are neural network architectures with invertibility by design [1, 2]. Endowed with the analytic-form invertibility and the tractability of the
Jacobian, CF-INNs have demonstrated their usefulness in various machine learning tasks such as generative modeling [3–7], probabilistic inference [8–10], solving inverse problems [11], and feature extraction and manipulation [4, 12–14]. The attractive properties of CF-INNs come at the cost of potential restrictions on the set of functions that they can approximate because they rely on carefully designed network layers. To circumvent the potential drawback, a variety of layer designs have been proposed to construct CF-INNs with high representation power, e.g., the afﬁne coupling ﬂow [3, 4, 15–17], the neural autoregressive ﬂow [18–20], and the polynomial ﬂow [21], each demonstrating enhanced empirical performance.
Despite the diversity of layer designs [1, 2], the theoretical understanding of the representation power of CF-INNs has been limited. Indeed, the most basic property as a function approximator, namely the
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
universal approximation property (or universality for short) [22], has not been elucidated for CF-INNs.
The universality can be crucial when CF-INNs are used to learn an invertible transformation (e.g., feature extraction [12] or independent component analysis [14]) because, informally speaking, lack of universality implies that there exists an invertible transformation, even among well-behaved ones, that CF-INN can never approximate, and it would render the model class unreliable for the task of function approximation.
To elucidate the universality of CF-INNs, we ﬁrst prove a theorem to show the equivalence of the universality for certain diffeomorphism classes, which allows us to reduce the approximation of a general diffeomorphism to that of a much simpler one. By leveraging this problem reduction, we show that CF-INNs based on afﬁne coupling ﬂows (ACFs; see Section 2), one of the least expressive
ﬂow designs, are in fact universal approximators for a general class of diffeomorphisms. The result can be interpreted as a convenient means to check the universality of a CF-INN: if the ﬂow design can represent ACFs as special cases, then it is universal.
The difﬁculty in proving the universality of CF-INNs lies in two complications. (1) Only function composition can be leveraged to make complex approximators (e.g., a linear combination is not allowed). We overcome this complication by essentially decomposing a general diffeomorphism into much simpler ones, by using a structural theorem of differential geometry that elucidates the structure of a certain diffeomorphism group. Our equivalence theorem provides a way to take advantage of this technique implicitly. (2) The ﬂow layers tend to be inﬂexible due to the parametric restrictions.
As an extreme example, ACFs can only apply a uniform transformation along the transformed dimension, i.e., the parameter of the transformation cannot depend on the variable which undergoes the transformation. For ACFs, the reduction of the problem allows us to ﬁnd an approximator with a clear outlook by approximating a step function.
Our contributions. Our contributions are summarized as follows. 1. We present a theorem to show the equivalence of universal approximation properties for certain classes of functions. The result enables the reduction of the task of proving the universality for general diffeomorphisms to that for much simpler coordinate-wise ones. 2. We leverage the result to show that some ﬂow architectures, in particular even ACFs, can be used to construct a CF-INN with the universality for approximating a fairly general class of diffeomorphisms. This result can be seen as a convenient criterion to check the universality of a CF-INN: if the ﬂow designs can reproduce ACF as a special case, it is universal. 3. As a corollary, we give an afﬁrmative answer to a previously unsolved problem, namely the distributional universality [18, 21] of ACF-based CF-INNs.
Our result is an interesting application of a deep theorem in differential geometry to investigate the representation power of a neural network architecture. 2 Preliminary and goal
In this section, we describe the models analyzed in this study, the notion of universality, and the goal of this paper. We use R (resp. N) to represent the set of all real numbers (resp. positive integers). For a positive integer n, we deﬁne [n] as the set {1, 2, . . . , n}. 2.1
Invertible neural networks based on coupling ﬂows
Throughout the paper, we ﬁx d ∈ N and assume d ≥ 2. For a vector x ∈ Rd and k ∈ [d − 1], we deﬁne x≤k as the vector (x1, . . . , xk)(cid:62) ∈ Rk and x>k the vector (xk+1, . . . , xd)(cid:62) ∈ Rd−k.
[1] hk,τ,θ by hk,τ,θ(x≤k, x>k) =
Coupling ﬂows. We deﬁne a coupling ﬂow (CF) (x≤k, τ (x>k, θ(x≤k)), where k ∈ [d − 1], θ : Rk → Rl and τ : Rd−k × Rl → Rd−k are maps, and
τ (·, θ(y)) is an invertible map for any y ∈ Rk.
Afﬁne coupling ﬂows. One of the most standard types of CFs is afﬁne coupling ﬂows [3, 4, 16, 17].
We deﬁne an afﬁne coupling ﬂow Ψk,s,t : Rd → Rd by
Ψk,s,t(x≤k, x>k) = (x≤k, x>k (cid:12) exp(s(x≤k)) + t(x≤k)), 2
where k ∈ [d − 1], (cid:12) is the Hadamard product, exp is applied in an element-wise manner, and s, t : Rk → Rd−k are maps typically parametrized by neural networks.
Single-coordinate afﬁne coupling ﬂow. Let H be a set of functions from Rd−1 to R. We deﬁne
H-single-coordinate afﬁne coupling ﬂows by H-ACF := {Ψd−1,s,t : s, t ∈ H}, which is a subclass of ACFs. It is the least expressive ﬂow design appearing in this paper, but we show in Section 3.2 that it can form a CF-INN with universality. We specify the requirements on H later.
Invertible linear ﬂows. We deﬁne the set of all afﬁne transforms by Aﬀ := {x (cid:55)→ Ax + b : A ∈
GL, b ∈ Rd}, where GL denotes the set of all regular matrices on Rd.
We consider the invertible neural network architectures constructed by composing ﬂow layers:
Deﬁnition 1 (CF-INNs). Let G be a set consisting of invertible maps. We deﬁne the set of invertible neural networks based on G as
INNG := {W1 ◦ g1 ◦ · · · ◦ Wn ◦ gn : n ∈ N, gi ∈ G, Wi ∈ Aﬀ} .
When G can represent the addition of a constant vector, we can obtain the same set of maps by replacing Aﬀ with GL, which has been adopted by previous studies such as Kingma et al. [4]. In fact, it is possible to use only the symmetric group Sd that is the permutations of variables, instead of Aﬀ, when G contains H-ACF. For details, see Appendix H. 2.2 Goal: the notions of universality and their relations
Here, we clarify the notion of universality in this paper. First, we prepare some notation. Let p ∈ [1, ∞) and m, n ∈ N. For a measurable mapping f : Rm → Rn and a subset K ⊂ Rm, we deﬁne (cid:18)(cid:90) (cid:107)f (x)(cid:107)p dx (cid:19)1/p
, (cid:107)f (cid:107)p,K :=
K where (cid:107)·(cid:107) is the Euclidean norm of Rn. We also deﬁne (cid:107)f (cid:107)sup,K := supx∈K (cid:107)f (x)(cid:107).
Deﬁnition 2 (Lp-/sup-universality). Let M be a model which is a set of measurable mappings from
Rm to Rn. Let p ∈ [1, ∞), and let F be a set of measurable mappings f : Uf → Rn, where Uf is a measurable subset of Rm which may depend on f . We say that M is an Lp-universal approximator or has the Lp-universal approximation property for F if for any f ∈ F, any ε > 0, and any compact subset K ⊂ Uf , there exists g ∈ M such that (cid:107)f − g(cid:107)p,K < ε. We deﬁne the sup-universality analogously by replacing (cid:107)·(cid:107)p,K with (cid:107)·(cid:107)sup,K.
We also deﬁne the notion of distributional universality. Distributional universality has been used as a notion of theoretical guarantee in the literature of normalizing ﬂows, i.e., probability distribution models constructed using invertible neural networks [2].
Deﬁnition 3 (Distributional universality). Let M be a model which is a set of measurable mappings from Rm to Rn. We say that a model M is a distributional universal approximator or has the distributional universal approximation property if, for any absolutely continuous2 probability measure
µ on Rm and any probability measure ν on Rn, there exists a sequence {gi}∞ i=1 ⊂ M such that (gi)∗µ converges to ν in distribution as i → ∞, where (gi)∗µ := µ ◦ g−1
. i
If a model M has the distributional universal approximation property, then it implies M approxi-mately transforms a known distribution, for example, the uniform distribution on [0, 1]m, into any probability measure µ on Rn, not only absolutely continuous but singular one. There exists another convention that deﬁnes the distributional universality as a representation power for only absolutely continuous probability measures. However, since absolutely continuous probability measures are dense in the set of all the probability measures, that convention is equivalent to ours. We include a proof for this fact in Lemma 5 in Appendix A.
The different notions of universality are interrelated. Most importantly, the Lp-universality for a certain function class implies the distributional universality (see Lemma 1). Moreover, if a model
M is a sup-universal approximator for F, it is also an Lp-universal approximator for F for any p ∈ [1, ∞). 2In this paper, we say a measure on the Euclidean space is absolutely continuous when it is absolutely continuous with respect to the Lebesgue measure. 3
Our goal Our goal is to elucidate the representation power of the CF-INNs for some ﬂow archi-tectures G by proving the Lp-universality or sup-universality of INNG for a fairly large class of diffeomorphisms, i.e., smooth invertible functions. To prove universality, we need to construct a model g ∈ INNG that attains the approximation error ε for given f and K. 3 Main results
In this section, we present the main results of this paper on the universality of CF-INNs. The ﬁrst the-orem provides a general proof technique to simplify the problem of approximating diffeomorphisms, and the second theorem builds on the ﬁrst to show that the CF-INNs based on the afﬁne coupling are
Lp-universal approximators. 3.1 First main result: Equivalence of universal approximation properties
Our ﬁrst main theorem allows us to lift a universality result for a restricted set of diffeomorphisms to the universality for a fairly general class of diffeomorphisms by showing a certain equivalence of universalities. By using the result to reduce the approximation problem, we can essentially circumvent the major complication in proving the universality of CF-INNs, namely that only function composition can be leveraged to make complex approximators (e.g., a linear combination is not allowed).
First, we deﬁne the following classes of invertible functions. Our main theorem later reveals an equivalence of Lp-universality/sup-universality for these classes.
Deﬁnition 4 (C 2-diffeomorphisms: D2). We deﬁne D2 as the set of all C 2-diffeomorphisms f :
Uf → Im(f ) ⊂ Rd , where Uf ⊂ Rd is an open set C 2-diffeomorphic to Rd, which may depend on f .
Deﬁnition 5 (Triangular transformations: T ∞). We deﬁne T ∞ as the set of all C∞-increasing triangular mappings from Rd to Rd. Here, a mapping τ = (τ1, . . . , τd) : Rd → Rd is increasing triangular if each τk(x) depends only on x≤k and is strictly increasing with respect to xk.
Deﬁnition 6 (Single-coordinate transformations: S r c as the set of all compactly-supported C r-diffeomorphisms τ satisfying τ (x) = (x1, . . . , xd−1, τd(x)), i.e., those which alter c (⊂ T ∞). only the last coordinate. In this article, only r = 0, 2, ∞ appear, and we mainly focus on S∞
Here, a bijection τ : Rd → Rd is compactly supported if τ = Id outside some compact set. c ). We deﬁne S r
Among the above classes of invertible functions, D2 is our main approximation target, and it is a fairly large class: it contains any C 2-diffeomorphism deﬁned on the entire Rd, an open convex set, or more generally a star-shaped open set. The class T ∞ relates to the distributional universality as we will see in Lemma 1. The class S∞ is a much simpler class of diffeomorphisms that we use as a c stepladder for showing the universality for D2.
Now we are ready to state the ﬁrst main theorem. It reveals an equivalence among the universalities for D2, T ∞, and S∞ c , under mild regularity conditions. We can use the theorem to lift up the universality for S∞ to that for D2. c
Theorem 1 (Equivalence of Universality). Let p ∈ [1, ∞) and let G be a set of invertible functions. (A) If all elements of G are piecewise C 1-diffeomorphisms, then the Lp-universal approximation properties of INNG for D2, T ∞ and S∞ c are all equivalent. (B) If all elements of G are locally bounded, then the sup-universal approximation properties of
INNG for D2, T ∞ and S∞ c are all equivalent.
The proof is provided in Appendix B. For the deﬁnitions of the piecewise C 1-diffeomorphisms and the locally bounded maps, see Appendix E. The regularity conditions in (A) and (B) assure that function composition within G is compatible with approximations (see Appendix F for details), and they are usually satisﬁed, e.g., continuous maps are locally bounded.
If one of the two universality properties in Theorem 1 is satisﬁed, the model is also a distributional universal approximator. Let p ∈ [1, ∞), and we have the following.
Lemma 1. An Lp-universal approximator for T ∞ is a distributional universal approximator. 4
Table 1: CF-INN instances analyzed in this work (Model: the considered CF-INN architecture. Flow type: the ﬂow layer architecture. Universality (this): the universal approximation property that this work has shown. Universality (prev.): previously claimed universal approximation property. ) Our proof techniques are easy to apply to analyze the universality of various CF-INN architectures.
Model
Flow type
INNH-ACF Afﬁne coupling [3, 4, 16, 17]
INNDSF
INNSoS
Deep sigmoidal ﬂow [18]
Sum-of-squares polynomial ﬂow [21]
Universality (this) Universality (prev.)
Lp-universal sup-universal sup-universal
-Distributional [18]
Distributional [21]
Since sup-universality implies Lp-universality, Lemma 1 can be combined with both cases of (A) and (B) in Theorem 1. The proof is based on the existence of a triangular map connecting two absolutely continuous distributions [23]. See Appendix A for details. Note that the previous studies [18, 21] have discussed the distributional universality of some ﬂow architectures essentially via showing the sup-universality for T ∞. Lemma 1 clariﬁes that the weaker notion of Lp-universality is sufﬁcient for the distributional universality, which can also apply to the case (A) in Theorem 1.
Application to previously proposed CF-INN architectures. Theorem 1 can upgrade a previously known sup-universality for T ∞ of a CF-INN architecture to that for D2. As examples, deep sigmoidal
ﬂows (DSF; a version of neural autoregressive ﬂows [18]) and sum-of-squares polynomial ﬂows (SoS;
[21]) can both yield CF-INNs with the sup-universal approximation property for D2. We provide the proof in Appendix G. See Table 1 for a summary of the results. See Section 5.1 for a comparison with previous theoretical analyses on normalizing ﬂows. 3.2 Second main result: Lp-universal approximation property of INNH-ACF
Our second main theorem reveals the Lp-universality of INNH-ACF for S 0 can be combined with Theorem 1 to show its Lp-universality for D2. We deﬁne C∞ set of all compactly-supported C∞ maps from Rd−1 to R.
Theorem 2 (Lp-universality of INNH-ACF). Let p ∈ [1, ∞). Assume H is a sup-universal ap-c (Rd−1) and that it consists of piecewise C 1-functions. Then, INNH-ACF is an proximator for C∞
Lp-universal approximator for S 0 c . c ), which c (Rd−1) as the c (hence for S∞
We provide a proof in Appendix D. For the deﬁnition of piecewise C 1-functions, see Appendix E.
Theorem 2 can be combined with Theorem 1 to show that INNH-ACF is an Lp-universal approximator for D2. Examples of H satisfying the condition of Theorem 2 include multi-layer perceptron models with the rectiﬁer linear unit (ReLU) activation [24] and a linear-in-parameter model with smooth universal kernels [25]. The result can be interpreted as a convenient criterion to check the universality of a CF-INN: if the ﬂow architecture G contains ACFs (or even just H-ACF with sufﬁciently expressive H) as special cases, then INNG is an Lp-universal approximator for D2.
By combining Theorem 1, Theorem 2, and Lemma 1, we can afﬁrmatively answer a previously unsolved problem [1, p.13]: the distributional universality of CF-INN based on ACFs.
Theorem 3 (Distributional universality of INNH-ACF). Under the conditions of Theorem 2,
INNH-ACF is a distributional universal approximator.
Implications of Theorem 2 and Theorem 3. Theorem 2 implies that, if G contains H-ACF as special cases, then INNG is an Lp-universal approximator for D2. In light of Theorem 3, it is also a distributional universal approximator, hence we can conﬁrm the theoretical plausibility for using it for normalizing ﬂows. Such examples of G include the nonlinear squared ﬂow [26], Flow++ [20], the neural autoregressive ﬂow [18], and the sum-of-squares polynomial ﬂow [21]. The result may not immediately apply to the typical Glow [4] models for image data that use the 1x1 invertible convolution layers and convolutional neuralnetworks for the coupling layers. However, the Glow architecture for non-image data [11, 14] can be interpreted as INNG with ACF layers, hence it is both an Lp-universal approximator for D2 and a distributional universal approximator. 5
4 Proof outline
In this section, we outline the proof ideas of our main theorems to provide an intuition for the constructed approximator and derive reusable insight for future theoretical analyses. 4.1 Proof outline for Theorem 1
Here, we outline the equivalence proof of Theorem 1. For details, see Appendix B. Since we have
S∞ c ⊂ T ∞ ⊂ D2, it is sufﬁcient to prove that the universal approximation properties for S∞ implies that for D2. Note that the proofs do not change for Lp-universality and sup-universality.
Therefore, we focus on describing the reduction from D2 to S∞ can be reduced to that of S∞ reduction from D2 to S 2 c :
Theorem 4. For any element f ∈ D2 and compact subset K ⊂ Uf , there exist n ∈ N, W1, . . . , Wn ∈
Aﬀ, and τ1, . . . , τn ∈ S 2 c . Since the approximation of S 2 c c by a standard molliﬁcation argument (see Appendix B.2), we show a c such that f (x) = W1 ◦ τ1 ◦ · · · ◦ Wn ◦ τn(x) for all x ∈ K.
Behind the scenes, Theorem 4 reduces D2 to S 2 c in four steps:
D2 (cid:32) Diﬀ 2 c (cid:32) Flow endpoints (cid:32) nearly-Id (cid:32) S 2 c
Here, A (cid:32) B (A is reduced to B) indicates that the universality for A follows from that for B, and
Id denotes the identity map. We explain each reduction step in the below. c. We consider a special subset Diﬀ 2
From D2 to Diﬀ 2 c ⊂ D2, which is the group of compactly-supported C 2-diffeomorphisms on Rd whose group operation is functional composition. Here, a bijection f : Rd → Rd is compactly supported if f = Id outside some compact set. Proposition 1 below reduces the problem of the universality for D2 to that for Diﬀ 2 c.
Proposition 1. For any f ∈ D2 and any compact subset K ⊂ Uf , there exist h ∈ Diﬀ 2 such that for all x ∈ K, f (x) = W ◦ h(x). c, W ∈ Aﬀ, c to ﬂow endpoints.
From Diﬀ 2
In order to construct an approximation for the elements of D2, we devise its subset that we call the ﬂow endpoints. A ﬂow endpoint is an element of Diﬀ 2 c which can be represented as φ(1) using an “additive” continuous map φ : [0, 1] → Diﬀ 2 c with φ(0) = Id. Here,
“additivity” means φ(s) ◦ φ(t) = φ(s + t) for any s, t ∈ [0, 1] with s + t ∈ [0, 1]. This additivity will be later used to decompose a ﬂow endpoint into a composition of some mildly-behaved fragments of the ﬂow map. Note that we equip Diﬀ 2 c with the Whitney topology [27, Proposition 1.7.(9)] to deﬁne the continuity of the map φ. The importance of the ﬂow endpoints lies in the following lemma that we prove in Appendix C:
Lemma 2. Any element in Diﬀ 2 c can be represented as a ﬁnite composition of ﬂow endpoints.
Lemma 2 is essentially due to Fact 1, which is the following structure theorem in differential geometry attributed to Herman, Thurston [28], Epstein [29], and Mather [30, 31]:
Fact 1. The group Diﬀ 2 c is simple, i.e., any normal subgroup H ⊂ Diﬀ 2 c is either {Id} or Diﬀ 2 c. c can be decomposed into "nearly-Id" c by leveraging its additivity property, as in the following proposition. Let (cid:107)·(cid:107)op
From ﬂow endpoints to nearly-Id. The ﬂow endpoints in Diﬀ 2 elements in Diﬀ 2 denote the operator norm.
Proposition 2. For any f ∈ Diﬀ 2 g1 ◦ · · · ◦ gr and supx∈Rd (cid:107)Dgi(x) − I(cid:107)op < 1, where Dgi is the Jacobian of gi. c, there exist ﬁnite elements g1, . . . , gr ∈ Diﬀ 2 c such that f =
Proposition 2 leverages the continuity of the ﬂows with respect to the Whitney topology of Diﬀ 2 c:
φ(1/n) uniformly converges to the identity map both in its values and its Jacobian when n → ∞.
Thus, any ﬂow endpoint φ(1) can be represented by an n-time composition of φ(1/n) each of which is close to identity (nearly-Id) when n is sufﬁciently large. c . The nearly-Id elements, g ∈ Diﬀ 2 c in Proposition 2, can be decomposed into
From nearly-Id to S 2 elements of S 2 c and permutation matrices: 6
=
◦ f = (cid:18)a c (cid:19) b d
= f2 ◦ f1 (cid:19) (cid:18)x1 y2 f2
= (cid:18)ax1 + b( y2−cx1 y2 d (cid:19)
) (cid:19) (cid:18)x1 x2 f1
= (cid:18) x1 cx1 + dx2 (cid:19)
Figure 1: A nearly-Id transformation f can be decomposed into coordinate-wise ones (f1 and f2: realized by S 2 c and permutations). The arrows indicate the transportation of the positions. A general nonlinear f can be analogously decomposed by Proposition 3 when f satisﬁes certain conditions. f
ACF (Step 1)
ψ∗ n (Step 2)
ψ∗ n (Step 2) (x, y) (cid:55)→ (x, vn(y)) (Step 3)
∃g1, g2, g3 ∈ INNACF : g1 (cid:39) ψ∗
=⇒ f (cid:39) g3 ◦ g2 ◦ g1 n, g2 (cid:39) (x, vn(y)), g3 (cid:39) (ψ∗ n)−1 (Steps 4, 5)
Figure 2: Illustration of the proof technique for the Lp-universal approximation property of INNACF for S 0 c . The symbol (cid:39) indicates approximation to arbitrary precision.
Proposition 3. For any g ∈ Diﬀ 2
τ1, . . . , τd ∈ S 2 c and permutation matrices σ1, . . . , σd such that c with supx∈Rd (cid:107)Dg(x) − I(cid:107)op < 1, there exist d elements g = σ1 ◦ τ1 ◦ · · · ◦ σd ◦ τd.
The machinery of this decomposition is illustrated in Figure 1. 4.2 Proof outline for Theorem 2
Here, we give the proof outline of Theorem 2. For details, see Appendix D. The main difﬁculty in constructing the approximator is the restricted functional form of ACFs. However, the problem reduction by Theorem 1 allows us to construct an approximator by approximating a step function.
For illustration, we only describe the case for d = 2 and K ⊂ [0, 1]2. For complete proof of
Theorem 2, see Appendix D. Let f (x, y) = (x, u(x, y)) be the target function, where u(·, y) is a continuous function that is strictly increasing for each y (i.e., f ∈ S 0 c ). For the compact set
K ⊂ [0, 1]2 ⊂ R2, we ﬁnd g ∈ INNH-ACF arbitrarily approximating f on K as follows (Figure 2).
Step 1. Align the image into the square: First, without loss of generality, we may assume that the image f ([0, 1]2) is again [0, 1]2. Indeed, we can align the image so that u(x, 1) = 1 and u(x, 0) = 0 for all x ∈ [0, 1] by using only an ACF Ψ1,s,t with continuous s and t, which can be approximated by H-ACF. 7
Step 2. Slice the squares and stagger the pieces: We consider an imaginary ACF ψ∗ n := Ψ1,1,tn deﬁned using a discontinuous step function tn := (cid:80)n n splits
[0, 1]2 into pieces and staggers them so that a coordinate-wise independent transformation (e.g., vn in Step 3), which is uniform along the x-axis, can affect each piece separately. k=0 k1[k/n,(k+1)/n). The map ψ∗
Step 3. Express f by a coordinate-wise independent transformation: We construct a continuous increasing function vn : R → R such that for y ∈ [k, k + 1), vn(y) = u(k/n, y) + k (k = 0, . . . , n − 1). A direct computation shows that ˜fn := (ψ∗ n)−1 ◦ (·, vn(·)) ◦ ψ∗ n arbitrarily approximates f on [0, 1]2 if we increase n. We take a sufﬁciently large n.
Step 4. Approximate the coordinate-wise independent transformation vn: We ﬁnd an element of INNH-ACF sufﬁciently approximating (·, vn(·)) on [0, 1] × [0, n]. This is realized based on a lemma that we can construct an approximator for any element of S 0 c of the form (x, y) (cid:55)→ (x, v(y)) on any compact set in R2.
Step 5. Approximate ψ∗ n and combine the approximated constituents to approximate ˜fn: We can also approximate ψ∗ n and its inverse by ACFs based on the universality of H. Finally, composing the approximated constituents gives an approximation of f on [0, 1]2 with arbitrary precision (see Appendix F). 5