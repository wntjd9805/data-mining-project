Abstract
To handle vast amounts of data, it is natural and popular to compress vectors and matrices. When we compress a vector from size N down to size n ! N , it certainly makes it easier to store and transmit efﬁciently, but does it also make it easier to process?
In this paper we consider lossless compression schemes, and ask if we can run our computations on the compressed data as efﬁciently as if the original data was that small. That is, if an operation has time complexity T pinput-sizeq, can we perform it on the compressed representation in time T pnq rather than T pN q? We consider the most basic linear algebra operations: inner product, matrix-vector multiplication, and matrix multiplication.
In particular, given two compressed vectors, can we compute their inner product in time Opnq? Or perhaps we must decompress ﬁrst and then multiply, spending ΩpN q time?
The answer depends on the compression scheme. While for simple ones such as Run-Length-Encoding (RLE) the inner product can be done in Opnq time, we prove that this is impossible for compressions from a richer class: essentially n2 or even larger runtimes are needed in the worst case (under complexity assumptions).
This is the class of grammar-compressions containing most popular methods such as the Lempel-Ziv family. These schemes are more compressing than the simple
RLE, but alas, we prove that performing computations on them is much harder. 1

Introduction
The idea of using compression to speed up computations can be found in any domain that deals with large-scale data, and ML is no exception. By exploiting redundancies and various forms of structure in a piece of data, compression algorithms such as zip can reduce its size from N down to n, where n ! N . The data becomes cheaper to store, access, transmit, and perhaps also to analyze. Can we run our ML tools on the compressed data, without decompressing it ﬁrst, and make the computation times proportional to n rather than N ? Since most ML algorithms boil down to large amounts of basic algebraic operations such as multiplications of vectors and matrices, with inner product as the atomic operation, the most basic question in this context is: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Main Question. Given two N -dimensional vectors, each in a compressed form of size n ! N , can we compute their inner product in ˜Opnq time1 rather than OpN q?
The answer, of course, depends on the compression scheme that we use. There seems to be an inherent tension: more complex schemes have higher compression rates but are harder to analyze without decompression.
First, let us clarify that our interest is in exact computations and lossless compressions, even though lossy techniques such as dimensionality reduction [16] are widely used by the ML community. In many cases, e.g. when performing a basic algebraic operation within a larger pipeline, even a small amount of error could add up to make the ﬁnal result unintelligible. Recent years has seen a growing interest in exploring the potential of lossless compression for speeding up ML [35, 83, 59, 65]. An inspiring result was honorably mentioned as an outstanding paper at NeurIPS last year [65]: any
N ˆ d matrix A can be compressed down to a matrix of size d ˆ d such that the optimal solutions of
Least-Mean-Squares (LMS) instances are exactly the same on A and A1. This is an example where for a speciﬁc task (LMS solvers) a speciﬁc compression scheme (designed by the authors) leads to a solution in time T pnq rather than T pN q, giving a 100x speedup on benchmark data; it makes one wonder if this approach can work in a more general setting.
For rather simple compression methods, the answer to our question is positive. A recent Commu-nications of the ACM article [35] exhibits Compressed Linear Algebra [32, 33, 34] a compression scheme for vectors and matrices that uses simple techniques such as Run Length Encoding (RLE) and allows for fast computations on the compressed data with impressive experimental results when integrated into ML systems. The RLE encoding of a vector simply replaces runs of values by tuples indicating the value and the length of the run; e.g. the binary vector 00011111000 gets encoded as 031503. Given two vectors encoded in this way with size nRLE, a simple one-pass algorithm can compute their inner product in OpnRLEq time. Before that, there were many algorithms for exploit-ing succinct encodings of sparse vectors [78, 56, 52]; e.g. by simply listing the nonzero locations the binary vector 0100001000 gets encoded as p2, 7q. These encodings allow for a linear time inner product computation as well.
However, these simple methods are often not very compressing. At the other end of the spectrum, we have the heavy-duty and time-tested family of Grammar-Compressions [54] that includes the
Lempel-Ziv-family (LZ77, LZ78, LZW, etc.)
[58, 91, 86], Byte-Pair Encoding [82], dictionary methods, and others [69, 63]. These compressions are used in ubiquitous applications such as zip,
Snappy, GIF, PNG, the built-in Unix utility compress, and even in PDF. Their compression rates are often on a whole different level compared to RLE; e.g. the current draft of this paper reduces from 10KB to 4KB with zip but RLE has no effect. See Table 1 and [35, Table 1] for empirical data showing the quantitative potential of these methods for some standard ML datasets. What all these more elaborate compression techniques have in common is that they essentially (up to low order terms [76]) encode a string (or vector) by a Straight-Line Program (SLP): a restricted kind of a context-free grammar that can only produce one string. In more detail, an SLP is deﬁned over some alphabet Σ, say t0, 1u, and it is a set of replacement rules (or productions) of a very simple form: a rule is either a symbol in Σ or it is the concatenation of two previous rules (under some ﬁxed ordering of the rules). The last replacement rule is the sequence deﬁned by the SLP. For example, we can compress the sequence 01010101 with the rules S1 Ñ 0; S2 Ñ 1; S3 Ñ S1 S2; S4 Ñ
S3 S3; S5 Ñ S4 S4 and S5 corresponds to the sequence 01010101. See Figure 1. For some strings this can give an exponential compression, e.g. the sequence p01qN requires only Oplog N q rules; note that its RLE has size N . While ﬁnding the smallest SLP for a given string is NP-Hard, it can be approximated either by the above practical methods or provably up to logarithmic factors
[76, 20, 79, 48, 50].
Thus, the holy grail in this context is to perform algebraic operations in T pcompression-sizeq time even when the vectors are compressed with zip or one of the other heavy-duty grammar compres-sions; that is, without unzipping them ﬁrst.
Ideally, we would implement a “zip-inner-product” function that takes two zip ﬁles encoding vectors and computes the inner product in near-linear time (which may not even be enough time to unzip them). A recent paper titled “When LZW meets
ML” [59] makes partial progress towards this goal: the inner product can be computed efﬁciently on their tuple oriented coding where each coordinate is grammar-compressed separately, but not the 1We use the notation ˜Opnq “ n ¨ N op1q for near-linear time, hiding small terms such as log factors. 2
Table 1: The potential savings from grammar-compressed linear algebra: Compression rates on real datasets. We compare zip, a standard grammar-compression, with Run Length Encoding (RLE), a simple method that works well on repetitive or sparse data. For more such results, see [35, Table 1].
Dataset
Size
RLE (compression rate) zip (compression rate)
ISOLET [30]
US Census 1990 [30] 30.94 MB 342.26 MB 29.83 MB (0.96) 341.97 MB (0.99) 7.94 MB (0.26) 51.91 MB (0.15)
S5
S4
S4
S1 Ñ 0
S2 Ñ 1
S3 Ñ S1 S2
S4 Ñ S3 S3
S5 Ñ S4 S4 (a)
S3
S3
S3
S3
S1
S2
S1
S2
S1
S2
S1
S2 0 1 0 1 (b) 0 1 0 1
S5
S4
S3
S2
S1 1 0 (c)
Figure 1: (a) An SLP generating the sequence 01010101. (b) The corresponding parse tree. (c) The acyclic graph corresponding to the SLP. vector as a whole. This makes their method less compressing since, unlike with zip, the size of the encoding is always at least the dimensionality of the vectors.
Main Question (Restated). Given two N -dimensional vectors, each grammar-compressed down to size n ! N , can we compute their inner product in ˜Opnq time rather than OpN q?
While efﬁciently analyzing these grammars may seem like a daunting task, a large body of works over the last three decades has equipped us with an ingenious toolbox exactly for this pur-pose. It turns out that many important problems can indeed be solved surprisingly faster than the decompress-then-solve bound, e.g. in pattern matching [71, 53, 11, 36, 18, 61, 40, 45, 49]. This gives hope for a positive answer to our question and that many ML computations could be sped up by operating on grammar-compressions. These algorithms typically look at the parse trees that have N leaves but only n distinctly labelled internal nodes (see Figure 1), and traverse them starting from the root down, while attempting to only spend time proportional to the depth of the tree per distinct label. Using tricks that restructure the grammar to make the tree balanced, the depth can be upper bounded by Oplog N q, making the total time Opn log N q. To learn more about this subﬁeld of Algorithm Design, we refer the reader to the surveys [90, 57, 39, 81, 41, 73, 77, 64, 80]. 1.1 Our Results
Alas, our main result is a negative resolution to the main question above. We apply the tools of theoretical computer science, and the recently blossoming ﬁeld of ﬁne-grained complexity, in order to shed light into the mathematical foundations of Compressed Linear Algebra. We prove new hardness reductions showing cases where the time to compute the inner product must be large (under popular complexity assumptions) even when the vectors have very small grammar compressions.
For example, there are N -dimensional vectors with grammar-compressions of size n “ OpN 1{3q where the inner product must take ˜Ωpn2q time2 to compute. The consequences to other settings such as matrix-vector multiplication are further explained below. This creates a strong separation between grammar-compressions, where we prove an ˜Ωpn2q lower bound, and RLE, where an Opnq 2The more standard notation is n2´op1q which indicates an Ωpn1.9999q lower bound, no matter how close to 2 we go. That is, only mildly subquadratic algorithms are possible, e.g. by shaving log factors. 3
algorithm exists. This formally justiﬁes the use of simpler methods in ML systems and guides researchers away from searching for an ultra-efﬁcient “zip-inner-product” function.
Fine-Grained Complexity Negative results are paramount to the success of any scientiﬁc disci-pline. The most prominent framework for proving such results in computer science is the theory of
NP-Hardness, where one proves that a problem cannot be solved in polynomial time unless P “ N P which would imply breakthrough algorithms for famously-hard problems such as SAT and Subset
Sum. Without this theory, countless hours would have been wasted by algorithm designers trying to come up with provable, worst-case, polynomial time algorithms for NP-Hard problems. Due to the increase in data sizes of recent years, the ethos of this theory that “efﬁcient = polynomial” has become obsolete, and a more demanding attitude where “efﬁcient = linear” has arisen. By replac-ing the polynomial reductions of NP-Hardness with more efﬁcient ones (often linear), ﬁne-grained complexity can prove hardness results even for problems that have polynomial time algorithms.
Exemplary results show that linear or subquadratic algorithms for certain problems, which admit quadratic-time algorithms, would refute popular assumptions (conjectures that are similar to but stronger than P ‰ N P ) and have breakthrough consequences for famously hard problems. One of the central assumptions in this theory and in this paper is the 3SUM Conjecture: “No algorithm can decide, in subquadratic Opn2´εq time, if there are three numbers that sum to zero among a given set of n numbers”. A recent survey on ﬁne-grained complexity [89] cites dozens of papers, mainly in computational geometry [38] but also in other ﬁelds [72, 85, 7, 8, 21, 55, 10, 43], that prove 3SUM-Hardness results showing that their algorithms are optimal up to a refutation of this conjec-ture. In this paper, we prove the ﬁrst 3SUM-Hardness results in ML3 as far as we are aware. The 3SUM assumption and its generalizations that we use in the theorems below are formally deﬁned and discussed in Section 2.
Vector Inner Product Our ﬁrst and main result is a reduction from 3SUM to compressed inner product of two vectors, negatively resolving our main question.
Theorem 1.1. Assuming the 3SUM conjecture, the inner product of two N -dimensional vectors that are grammar-compressed to size n “ ΘpN 1 4 q cannot be computed in Opn2´εq time where ε ą 0.
Moreover, we strengthen and generalize this result in several ways. First, we address the dependence between n and N : could it be that for more or less compressed vectors the picture is different? Using a stronger variant of the 3SUM conjecture, the same lower bound of n2 holds even when n “ N 1{3, and therefore our result can be stated as an ˜ΩpN 2 3 q lower bound which is quite close to the trivial upper bound of OpN q. Moreover, by a (highly nontrivial) boosting of our reduction, in Section 3 we establish an ˜ΩpN 1 3 q lower bound with n “ N ε for any ε ď 1{3. That is, when the vectors are highly compressed even n10 time is not sufﬁcient4; this is in stark contrast to the case of RLE-compressed vectors where Opnq is always possible.
Matrix-Vector Multiplication Next, we consider the problem of computing the M ¨ v product of an N -dimensional vector v that is compressed to size n with an N ˆ N matrix M where each row is compressed to size Opnq. Perhaps computing these N inner products as a batch can be done faster than computing each separately. Alas, by another signiﬁcant boosting of our reduction we prove that this is also impossible. While if the encoding is with RLE the product can be computed in OpN nq time, which is linear in the representation size of the matrix and thus optimal, it turns out that for grammar compressions ˜ΩpN n2q is required. The proof is in Section 4.
Theorem 1.2. Assuming the 3SUM conjecture, the product of an N ˆ N -dimensional matrix, where each row is grammar-compressed to size n “ ΘpN 1 5 q, with an N -dimensional vector that is grammar-compressed to size n cannot be computed in OpN n2´εq time where ε ą 0. 3We remark that some complexity assumption is necessary for proving the kind of results we are interested, since unconditionally proving even very weak lower bounds on the time complexity such as Ωpn1`εq and even for NP-Hard problems like SAT (not to mention inner product) is far beyond current techniques [12]. 4Strictly speaking, such a conditional lower bound of Ωpn10q for highly compressible inputs can already be proven by combining a known #P-hardness reduction from SubsetSum [60] with a ﬁne-grained hardness of
SubsetSum under the Exponential Time Hypothesis (see, e.g. [47]). However, such an approach yields only a weak lower bound in terms of the uncompressed size N , namely a bound of ΩpN (cid:15)q for some non-explicit, possibly tiny (cid:15). Our lower bounds always give an explicit, reasonably large value for (cid:15). 4
Matrix Multiplication Finally, we consider matrix multiplication of compressed matrices C “
A ¨ B. There are multiple ways to compress an N ˆ N matrix: we might compress each row or each column, so that the compression size is N ¨ n, or treat the whole matrix as an N 2-dimensional vector and compress it to size n. Each way may lead to a different time complexity, but no matter which way we choose, the ﬁrst question to ask, and that will determine the time we can hope for, is: what is the output size? The na¨ıve answer is that the matrix C has size N ˆ N , but since A and B are compressed, shouldn’t we expect C to also be representable with a small grammar of size n ! N 2?
Unlike the above questions that deal with computation time, this is an information-theoretic ques-tion, and in Section 5 we give strong and unconditional negative answers: the matrix C cannot be grammar-compressed to size opN 2{ log2 N q even when A and B are strongly compressible. More-over, some of our results hold even when A and B have very small RLE encodings. Therefore, our results should be of interest to the compressed linear algebra project beyond grammar-compressions.
Technical Remarks While the tools for proving NP-Hardness results for grammar-compressed data are old [64], they only apply in the unrealistic setting where n “ log N , and we are interested in more ﬁne-grained results. Only recently, a FOCS paper by the authors [2] introduced the techniques for proving such lower bounds. This previous work focused on combinatorial pattern matching problems and the current work extends it to the setting of linear algebra. Our results establish the hardness even of the simplest setting of binary vectors and matrices over t0, 1u. This setting is particularly studied due to its connection to graphs, where grammar compressions have also received a lot of attention [66, 67]. Moreover, we show that even deciding if the inner product is 0 or ě 1 is hard, and so our lower bounds hold against any bounded approximation algorithms. Extending the lower bounds to other functions such as computing the (cid:96)2 distance between two vectors is also easy. Like almost all results in ﬁne-grained complexity [89], our lower bounds are against both deterministic and randomized algorithms.
Finally, we remark that our lower bounds are for the most basic setting of worst-case instances.
Extending them to average-case results, showing that instances that come from certain natural dis-tributions are also hard, is an open question. However, notice that even if the original vectors come from a natural distribution, the distribution of the grammar representations will be completely differ-ent (and probably far from natural). Therefore, exploiting the structure of non-worst-case instances seems far beyond current reach in this context. 1.2 Other