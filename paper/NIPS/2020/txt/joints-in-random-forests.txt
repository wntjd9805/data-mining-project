Abstract
Decision Trees (DTs) and Random Forests (RFs) are powerful discriminative learn-ers and tools of central importance to the everyday machine learning practitioner and data scientist. Due to their discriminative nature, however, they lack principled methods to process inputs with missing features or to detect outliers, which requires pairing them with imputation techniques or a separate generative model. In this paper, we demonstrate that DTs and RFs can naturally be interpreted as generative models, by drawing a connection to Probabilistic Circuits, a prominent class of tractable probabilistic models. This reinterpretation equips them with a full joint distribution over the feature space and leads to Generative Decision Trees (GeDTs) and Generative Forests (GeFs), a family of novel hybrid generative-discriminative models. This family of models retains the overall characteristics of DTs and RFs while additionally being able to handle missing features by means of marginalisa-tion. Under certain assumptions, frequently made for Bayes consistency results, we show that consistency in GeDTs and GeFs extend to any pattern of missing input features, if missing at random. Empirically, we show that our models often outperform common routines to treat missing data, such as K-nearest neighbour im-putation, and moreover, that our models can naturally detect outliers by monitoring the marginal probability of input features. 1

Introduction
Decision Trees (DTs) and Random Forests (RFs) are probably the most widely used non-linear machine learning models of today. While Deep Neural Networks are in the lead for image, video, audio, and text data—likely due to their beneﬁcial inductive bias for signal-like data—DTs and RFs are, by and large, the default predictive model for tabular, domain-agnostic datasets. Indeed, Kaggle’s 2019 report on the State of Data Science and Machine Learning [20] lists DTs and RFs as second most widely used techniques, right after linear and logistic regressions. Moreover, a study by Fernandez et al. [12] found that RFs performed best on 121 UCI datasets against 179 other classiﬁers. Thus, it is clear that DTs and RFs are of central importance for the current machine learning practitioner.
DTs and RFs are generally understood as discriminative models, that is, they are solely interpreted as predictive models, such as classiﬁers or regression functions, while attempts to additionally interpret them as generative models are scarce. In a nutshell, the difference between discriminative and generative models is that the former aim to capture the conditional distribution P (Y | X), while the latter aim to capture the whole joint distribution P (Y, X), where X are the input features and Y is the variable to be predicted—discrete for classiﬁcation and continuous for regression. In this paper, we focus on classiﬁcation, but the extension to regression is straightforward. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Generative and discriminative models are rather complementary in their strengths and use cases.
While discriminative models typically fare better in predictive performance, generative models allow to analyse and capture the structure present in the input space. They are also “all-round predictors”, that is, not restricted to a single prediction task but also capable of predicting any X given Y ∪ X \ X. Moreover, generative models have some crucial advantages on the prediction task
P (Y | X) a discriminative model has been trained on, as they naturally allow to detect outliers (by monitoring P (X)) and treat missing features (by marginalisation). A purely discriminative model does not have any “innate” mechanisms to deal with these problems, and needs to be supported with a generative model P (X) (to detect outliers) or imputation techniques (to handle missing features).
Ideally, we would like the best of both worlds: having the good predictive performance of discrimina-tive models and the advantages of generative models. In this paper, we show that this is achievable for DTs and RFs by relating them to Probabilistic Circuits (PCs) [50], a class of generative models based on computational graphs of sum nodes (mixtures), product nodes (factorisations), and leaf nodes (distribution functions). PCs subsume and represent a wide family of related models, such as arithmetic circuits [8], AND/OR-graphs [31], sum-product networks [38], cutset networks (CNets)
[44], and probabilistic sentential decision diagrams [24]. While many researchers are aware of the similarity between DTs and PCs—most notably, CNets [44] can be seen as a type of generative
DT—the connection to classical, discriminative DTs [40] and RFs [3] has not been studied so far.
We show that DTs and RFs can be naturally cast into the PC framework. For any given DT, we can construct a corresponding PC, a Generative Decision Tree (GeDT), representing a full joint distribution P (Y, X). This distribution gives rise to the predictor P (Y | X) = P (Y,X)/(cid:80) y P (y,X), which is identical to the original DT, if we impose certain constraints on the conversion from DT to
GeDT. Additionally, a GeDT also ﬁts the joint distribution P (X) to the training data, “upgrading” the DT to a fully generative model. For a completely observed sample X = x, the original DT and a corresponding GeDT agree entirely (yield the exact same predictions), and moreover, have the same computational complexity (a discussion on time complexity is deferred to the supp. material).
By converting each DT in an RF into an GeDT, we obtain an ensemble of GeDTs, which we call
Generative Forest (GeF). Clearly, if each GeDT in a GeF agrees with its original DT, then GeFs also agree with their corresponding RFs.
GeDTs and GeFs have a crucial advantage in the case of missing features, that is, assignments
Xo = xo for some subset Xo ⊂ X, while X¬o = X \ Xo are missing at random. In a GeDT, we can marginalise the missing features and yield the predictor
P (Y | Xo) = (cid:82) x¬o (cid:82) (cid:80) y x¬o
P (Y, Xo, x¬o)dx¬o
P (y, Xo, x¬o)dx¬o
. (1)
For GeFs, we yield a corresponding ensemble predictor for missing features, by applying marginali-sation to each GeDT. Using the true data generating distribution in Eq. (1) would deliver the Bayes optimal predictor for any subset Xo of observed features. Thus, since GeDTs are trained to approx-imate the true distribution, using the predictor of Eq. (1) under missing data is well justiﬁed. We show GeDTs are in fact consistent: they converge to the Bayes optimal classiﬁer as the number of data points goes to inﬁnity. Our proof requires similar assumptions to those of previous results for DTs [1, 4, 17] but is substantially more general: while consistency in DTs is shown only for a classiﬁer P (Y | X) using fully observed samples, our consistency result holds for all 2|X| classiﬁers
P (Y | Xo): one for each observation pattern Xo ⊆ X. While the high-dimensional integrals in
Eq. (1) seem prohibitive, they are in fact tractable, since a remarkable feature of PCs is that computing any marginal has the same complexity as evaluating the full joint, namely linear in the circuit size.
This ability of our models is desirable, as there is no clear consensus on how to deal with missing features in DTs at test time: The most common strategy is to use imputation, e.g. mean or k-nearest-neighbour (KNN) imputation, and subsequently feed the completed sample to the classiﬁer. DTs also have two “built-in” methods to deal with missing features that do not require external models.
These are the so-called surrogate splits [49] and an unnamed method proposed by Friedman in 1977
[14, 41]. Among these, KNN imputation seems to be the most widely used, and typically delivers good results on real-world data. However, we demonstrate it does not lead to a consistent predictor under missing data, even when assuming idealised settings. Moreover, in our experiments, we show that GeF classiﬁcation under missing inputs often outperforms standard RFs with KNN imputation. 2
Our generative interpretation can be easily incorporated in existing DT learners and does not require drastic changes in the learning and application practice for DTs and RFs. Essentially, any DT algorithm can be used to learn GeDTs, requiring only minor bookkeeping and some extra generative learning steps. There are de facto no model restrictions concerning the additional generative learning steps, representing a generic scheme to augment DTs and RFs to generative models. 2 Notation and