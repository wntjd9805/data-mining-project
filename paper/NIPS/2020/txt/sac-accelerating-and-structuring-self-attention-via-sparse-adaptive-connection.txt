Abstract
While the self-attention mechanism has been widely used in a wide variety of tasks, it has the unfortunate property of a quadratic cost with respect to the input length, which makes it difﬁcult to deal with long inputs. In this paper, we present a method for accelerating and structuring self-attentions: Sparse Adaptive Connection (SAC).
In SAC, we regard the input sequence as a graph and attention operations are performed between linked nodes. In contrast with previous self-attention models with pre-deﬁned structures (edges), the model learns to construct attention edges to improve task-speciﬁc performances. In this way, the model is able to select the most salient nodes and reduce the quadratic complexity regardless of the sequence length. Based on SAC, we show that previous variants of self-attention models are its special cases. Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classiﬁcation, we demonstrate SAC is competitive with state-of-the-art models while signiﬁcantly reducing memory cost. 1

Introduction
The self-attention mechanism has proved to beneﬁt a wide range of ﬁelds and tasks, such as natural language processing (Vaswani et al., 2017; Dai et al., 2019), computer vision (Xu et al., 2015;
Jaderberg et al., 2015; Bello et al., 2019; Parmar et al., 2019), video classiﬁcation (Wang et al., 2018) and graph modeling (Veliˇckovi´c et al., 2018; Lee et al., 2019). The attention mechanism allows the model to attend to different parts of the input and capture the most salient features, and provides with the ability to handle dependencies between elements across long distances.
Two conspicuous problems stand out with the self-attention mechanism: (1) the memory complexity is quadratic with respect to the input length, making it infeasible to handle long sequences; and (2) the self-attention operations are performed in a pre-deﬁned structure (usually fully-connected), which not only is costly but also lacks for the ability to learn the optimal attention structure optimized for the performance in the downstream tasks (Child et al., 2019; Sukhbaatar et al., 2019). These observations indicate that the fully-connected structure for self-attention operations can be replaced by a more sparse one where only speciﬁc edges are constructed for attention operations.
In this paper, we present Sparse Adaptive Connection (SAC), which replaces the fully-connected structure in self-attention with a sparse graph-like structure adapted to different tasks. In SAC, we regard the input as a graph where a node could be a token in the sequence or an ﬂattened feature map of an image, and an edge between a pair of nodes represents they can attend to each other. To select edges, we propose an Edge Predictor which utilizes an LSTM model to dynamically predict pairs of nodes that represent two ends of an edge. In contrast with previous self-attention models with pre-deﬁned attention structures (edges), SAC learns to construct attention edges adaptively, which are optimized to improve task-speciﬁc performances using reinforcement learning models. We 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
evaluate the proposed model on four tasks: neural machine translation, language modeling, graph representation learning and image classiﬁcation, and demonstrate that SAC learns to select the most salient nodes to attend to each other and is free from heavy memory cost while achieving strong performances. 2