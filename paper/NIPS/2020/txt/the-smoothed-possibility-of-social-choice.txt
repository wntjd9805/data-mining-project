Abstract leverages the smoothed complexity analysis
We develop a framework that by Spielman and Teng [60] to circumvent paradoxes and impossibility theorems in social choice, motivated by modern applications of social choice powered by
AI and ML. For Condrocet’s paradox, we prove that the smoothed likelihood of the paradox either vanishes at an exponential rate as the number of agents in-creases, or does not vanish at all. For the ANR impossibility on the non-existence of voting rules that simultaneously satisfy anonymity, neutrality, and resolvability, we characterize the rate for the impossibility to vanish, to be either polynomially fast or exponentially fast. We also propose a novel easy-to-compute tie-breaking mechanism that optimally preserves anonymity and neutrality for even number of alternatives in natural settings. Our results illustrate the smoothed possibility of social choice—even though the paradox and the impossibility theorem hold in the worst case, they may not be a big concern in practice. 1

Introduction
Dealing with paradoxes and impossibility theorems is a major challenge in social choice theory, because “the force and widespread presence of impossibility results generated a consolidated sense of pessimism, and this became a dominant theme in welfare economics and social choice theory in general”, as the eminent economist Amartya Sen commented in his Nobel prize lecture [57].
Many paradoxes and impossibility theorems in social choice are based on worst-case analysis. Take perhaps the earliest one, namely Condorcet’s (voting) paradox [15], for example. Condorcet’s para-dox states that, when there are at least three alternatives, it is impossible for pairwise majority aggregation to be transitive. The proof is done by explicitly constructing a worst-case scenario—a proﬁle P that contains a Condorcet cycle. For example, in P = {a (cid:31) b (cid:31) c, b (cid:31) c (cid:31) a, c (cid:31) a (cid:31) b}, there is a cycle a (cid:31) b, b (cid:31) c, and c (cid:31) a of pairwise majority. Condorcet’s paradox is closely related to the celebrated Arrow’s impossibility theorem: if Condorcet’s paradox can be avoided, then the pairwise majority rule can avoid Arrow’s impossibility theorem.
As another example, the ANR impossibility theorem (e.g. [49, Problem 1] and [53, 22, 12]) states that no voting rule r can simultaneously satisfy anonymity (r is insensitive to the identities of agents) and neutrality (r is insensitive to the identities of alternatives), and resolvability (r always chooses a single winner). The proof is done by analyzing a worst-case scenario P = {a (cid:31) b, b (cid:31) a}. Suppose for the sake of contradiction that a resolvable r satisﬁes anonymity and neutrality, and without loss of generality let r(P ) = a. After exchanging a and b, the winner ought to be b due to neutrality. But since the permuted proﬁle still contains one vote for a (cid:31) b and one vote for b (cid:31) a, the winner ought to be a due to anonymity, which is a contradiction.
There is an enormous literature in social choice on circumventing the impossibilities, most of which belongs to the following two approaches. (1) Domain restrictions, namely, agents’ reported preferences are assumed to come from a subset of all linear orders such as single-peaked prefer-ences [6, 2, 58, 48, 16, 8, 24]; and (2) likelihood analysis, where impossibility theorems are eval-uated by the likelihood of their occurrence in proﬁles randomly generated from a distribution such as the i.i.d. uniform distribution, a.k.a. Impartial Culture (IC) [28, 32]. Both approaches have been 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
criticized for making strong and unrealistic assumptions on the domain and on the probability dis-tributions, respectively. In particular, IC has received much criticism, see e.g. [19], yet no widely-accepted probabilisitic model exists to the best of our knowledge.
The worst-case nature behind the impossibility theorems might be desirable for high-stakes, less-frequent applications such as political elections, but it may not be appropriate for modern low-stakes, frequently-used applications of social choice, many of which are supported by AI systems that learn agents’ preferences to help them make group decisions [66]. While AI-powered social choice appears to be a promising solution to the long-standing turnout problem [57] and can therefore promote democracy to a larger scale and with a higher frequency, it questions the relevance of worst-case analysis in social choice theory. This motivates us to ask the following key question:
How serious are the impossibilities in frequently-used modern applications of social choice?
The frequently-used feature naturally leads to the analysis of average likelihood of the impossibility theorems. But in light of the criticism of the classical likelihood analysis approach discussed above, what is a realistic model to answer the key question?
Interestingly, computer science has encountered a similar challenge and has gone through a similar path in the analysis of practical performance of algorithms. Initially, the analysis mostly focused on the worst case, as in the spirit of big O notation and NP-hardness. Domain restrictions have been a popular approach beyond the worst-case analysis. For example, while SAT is NP-hard, its restriction 2-SAT is in P. Likelihood analysis, in particular average-case complexity analysis [7], has also been a popular approach, yet it suffers from the same criticism as its counterpart in social choice—the distribution used in the analysis may well be unrealistic [61].
The challenge was addressed by the smoothed (complexity) analysis introduced by Spielman and
Teng [60], which focuses on the “worst average-case” scenario that combines the worst-case analysis and the average-case analysis. The idea is based on the fact that the input (cid:126)x of an algorithm is often a noisy perception of the ground truth (cid:126)x∗. Therefore, the worst-case is analyzed by assuming that an adversary chooses a ground truth (cid:126)x∗ and then Nature adds a noise (cid:126)(cid:15) (e.g. a Gaussian noise) to it, such that the algorithm’s input becomes (cid:126)x = (cid:126)x∗ + (cid:126)(cid:15). The smoothed runtime of an algorithm is therefore sup(cid:126)x∗ E(cid:126)(cid:15) RunTime((cid:126)x∗ +(cid:126)(cid:15)), in contrast to the worst-case runtime sup(cid:126)x∗ RunTime((cid:126)x∗) and the average-case runtime E(cid:126)x∗∼πRunTime((cid:126)x∗), where π is a given distribution over data.
Our Contributions. We propose a framework that leverages the elegant smoothed complexity anal-ysis to answer the key question above. In social choice, the data is a proﬁle, which consists of agents’ reported preferences that are often represented by linear orders over a set A of m alternatives. Like in the smoothed complexity analysis, in our framework there is an adversary who controls agents’
“ground truth” preferences, which may be ordinal (as rankings over alternatives) or cardinal (as util-ities over alternatives). Then, Nature adds a “noise” to the ground truth preferences and outputs a preference proﬁle, which consists of linear orders over alternatives.
Following the convention in average-case complexity analysis [7], we use a statistical model to model Nature’s noising procedure. As in many smoothed-analysis approaches, we assume that noises in agents’ preferences are independently generated, yet agents’ ground truth preferences can be arbitrarily correlated, which constitutes the basis for the worst-case analysis. Using our smoothed analysis framework, we obtain the following two dichotomy theorems on the asymptotic smoothed likelihood of Condorcet’s paradox and the ANR impossibility under mild assumptions, when the number of alternatives m is ﬁxed and the number of agents n goes to inﬁnity.
THEOREM 1.
Condorcet’s Paradox either vanishes at an exponential rate, or does not vanish at all. (Smoothed Condorcet’s paradox, informally put). The smoothed likelihood of
THEOREM 2. (Smoothed ANR (im)possibility theorem, informally put). The theorem has two parts. The smoothed possibility part states that there exist resolute voting rules under which the impossibility theorem either vanishes at an exponential rate or at a polynomial rate. The smoothed impossibility part states that there does not exists a resolute voting rule under which the impossi-bility theorem vanishes faster than under the rules in the smoothed possibility part.
Both theorems are quite general and their formal statements also characterize conditions for each case. Such conditions in Theorem 1 tell us when Condorcet’s Paradox vanishes (at an exponential rate), which is positive. While the theorem may be expected at a high level and part of it is easy to 2
prove, for example the exponential-rate part can be proved by using a similar idea as in the proof of minimaxity/sample complexity of MLE under a large class of distance-based models [13], we are not aware of a previous work that provides a complete dichotomy that draws a clear line between paradoxes and non-paradoxes as Theorem 1 does. In addition, we view such expectedness posi-tive news, because it provides a theoretical conﬁrmation of well-believed hypotheses under natural settings, as smoothed complexity analysis did for the runtime of a simplex algorithm.
The smoothed possibility part of Theorem 2 is also positive because it states that the ANR impossi-bility vanishes as the number of agents n increases. The smoothed impossibility part of Theorem 2 is mildly negative, because it states that no voting rule can do better, though the impossibility may still vanish as n increases. Together, Theorem 1 and 2 illustrate the smoothed possibility of social choice—even though the paradox and the impossibility theorem hold in the worst case, they may not be a big concern in practice in some natural settings.
Our framework also allows us to develop a novel easy-to-compute tie-breaking mechanism called most popular singleton ranking (MPSR) tie-breaking, which tries to break ties using a linear order that uniquely occurs most often in the proﬁle (Deﬁnition 8). We prove that MPSR is better than the commonly-used lexicographic tie-breaking and ﬁxed-agent tie-breaking mechanisms w.r.t. the smoothed likelihood of the ANR impossibility—MPSR reduces the smoothed likelihood from n−0.5 to n− m! for many commonly-studied voting rules under natural assumptions (Proposition 1 and
Theorem 3), and is optimal for even number of alternatives m (Theorem 2 and Lemma 2). 4
Proof Techniques. Standard approximation techniques such as Berry-Esseen theorem and its high-dimensional counterparts, e.g. [64, 18, 21], due to their O(n−0.5) error terms, are too coarse for the (tight) bound in Theorem 2. To prove our theorems, we ﬁrst model various events of interest as systems of linear constraints. Then, we develop a technical tool (Lemma 1) to provide a dichotomy characterization for the Poisson Multinomial Variables (PMV) that corresponds to the histogram of a randomly generated proﬁle to satisfy the constraints. We further show in Appendix I that Lemma 1 is a general and useful tool for analyzing smoothed likelihood of many other commonly-studied events in social choice (Table 4), which are otherwise hard to analyze. 1.1