Abstract
Prioritized Experience Replay (PER) is a deep reinforcement learning technique in which agents learn from transitions sampled with non-uniform probability propor-tionate to their temporal-difference error. We show that any loss function evaluated with non-uniformly sampled data can be transformed into another uniformly sam-pled loss function with the same expected gradient. Surprisingly, we ﬁnd in some environments PER can be replaced entirely by this new loss function without impact to empirical performance. Furthermore, this relationship suggests a new branch of improvements to PER by correcting its uniformly sampled loss function equivalent. We demonstrate the effectiveness of our proposed modiﬁcations to PER and the equivalent loss function in several MuJoCo and Atari environments. 1

Introduction
The use of non-uniform sampling in deep reinforcement learning originates from a technique known as prioritized experience replay (PER) [1], in which high error transitions are sampled with increased probability, enabling faster propagation of rewards and accelerating learning by focusing on the most critical data. An ablation study over the many improvements to deep Q-learning [2] found
PER to be the most critical extension for overall performance. However, while the motivation for
PER is intuitive, this commonly used technique lacks a critical theoretical foundation. In this paper, we develop analysis enabling us to understand the beneﬁts of non-uniform sampling and suggest modiﬁcations to PER that both simplify and improve the empirical performance of the algorithm.
In deep reinforcement learning, value functions are approximated with deep neural networks, trained with a loss over the temporal-difference error of previously seen transitions. The expectation of this loss, over the sampling distribution of transitions, determines the effective gradient which is used to ﬁnd minima in the network. By biasing the sampling distribution, PER effectively changes the expected gradient. Our main theoretical contribution shows the expected gradient of a loss function minimized on data sampled with non-uniform probability, is equal to the expected gradient of another, distinct, loss function minimized on data sampled uniformly. This relationship can be used to transform any loss function into a prioritized sampling scheme with a new loss function or vice-versa. We can use this transformation to develop a concrete understanding about the beneﬁts of non-uniform sampling methods such as PER, as well as facilitate the design of novel prioritized methods. Our discoveries can be summarized as follows:
Loss function and prioritization should be tied. The key implication of this result is that the design of prioritized sampling methods should not be considered in isolation from the loss function. We can use this connection to analyze the correctness of methods which use non-uniform sampling by transforming the loss into the uniform-sampling equivalent and considering whether the new loss is in line with the target objective. In particular, we ﬁnd the PER objective can be unbiased, even without importance sampling, if the loss function is chosen correctly. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Variance reduction. This relationship in expected gradients brings us to a deeper understanding of the beneﬁts of prioritization. We show that the variance of the sampled gradient can be reduced by transforming a uniformly sampled loss function into a carefully chosen prioritization scheme and corresponding loss function. This means that non-uniform sampling is almost always favorable to uniform sampling, at the cost of additional algorithmic and computational complexity.
Empirically unnecessary. While non-uniform sampling is theoretically favorable, in many cases the variance reducing properties will be minimal. Perhaps most unexpectedly, we ﬁnd in a standard benchmark problem, prioritized sampling can be replaced with uniform sampling and a modiﬁed loss function, without affecting performance. This result suggests some of the beneﬁt of prioritized experience replay comes from the change in expected gradient, rather than the prioritization itself.
We introduce Loss-Adjusted Prioritized (LAP) experience replay and its uniformly sampled loss equivalent, Prioritized Approximation Loss (PAL). LAP simpliﬁes PER by removing unnecessary importance sampling ratios and setting the minimum priority to be 1, which reduces bias and the likelihood of dead transitions with near-zero sampling probability in a principled manner. On the other hand, our loss function PAL, which resembles a weighted variant of the Huber loss, computes the same expected gradient as LAP, and can be added to any deep reinforcement learning algorithm in only a few lines of code. We evaluate both LAP and PAL on the suite of MuJoCo environments [3] and a set of Atari games [4]. Across both domains, we ﬁnd both of our methods outperform the vanilla algorithms they modify. In the MuJoCo domain, we ﬁnd signiﬁcant gains over the state-of-the-art in the hardest task, Humanoid. All code is open-sourced (https://github.com/sfujim/LAP-PAL). 2