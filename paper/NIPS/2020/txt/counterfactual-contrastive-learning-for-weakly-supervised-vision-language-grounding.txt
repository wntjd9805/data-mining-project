Abstract
Weakly-supervised vision-language grounding aims to localize a target moment in a video or a speciﬁc region in an image according to the given sentence query, where only video-level or image-level sentence annotations are provided during training. Most existing approaches employ the MIL-based or reconstruction-based paradigms for the WSVLG task, but the former heavily depends on the quality of randomly-selected negative samples and the latter cannot directly optimize the visual-textual alignment score. In this paper, we propose a novel Counterfactual
Contrastive Learning (CCL) to develop sufﬁcient contrastive training between coun-terfactual positive and negative results, which are based on robust and destructive counterfactual transformations. Concretely, we design three counterfactual trans-formation strategies from the feature-, interaction- and relation-level, where the feature-level method damages the visual features of selected proposals, interaction-level approach confuses the vision-language interaction and relation-level strategy destroys the context clues in proposal relationships. Extensive experiments on ﬁve vision-language grounding datasets verify the effectiveness of our CCL paradigm. 1

Introduction
Vision-language grounding is a fundamental and crucial problem in multi-modal understanding.
Video grounding [14, 17] aims to identify the temporal boundaries of the target moment according to the given description. And image grounding [20, 28, 46] localizes a speciﬁc region described by a referring expression. Recently, to avoid expensive manual annotations, researchers begin to explore
Weakly-Supervised Vision-Language Grounding (WSVLG), which only needs the video-sentence or image-sentence pairs during training. Most existing approaches [33, 26, 11, 29, 15, 23, 8, 37] employ the MIL-based or reconstruction-based paradigm to train weakly-supervised grounding networks, but they both have some drawbacks. The MIL-based methods [11, 29, 15, 8] often deﬁne the original vision-language pairs as positive samples, construct the unmatched vision-language pairs as negative samples, and directly learn the latent visual-textual alignment by an inter-sample loss.
However, these approaches heavily depend on the quality of randomly-selected negative samples, which are often easy to distinguish and cannot provide strong supervision signals. On the other hand, reconstruction-based methods [33, 26, 23, 37] attempt to reconstruct the sentence query from visual contents during training and utilize intermediate results, such as attention weights, to localize the target proposal (i.e., region or moment) during inference. But these methods cannot directly optimize the visual-textual alignment scores which are applied for inference. Considering the proposals with
∗Zhou Zhao is the corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Examples of image grounding and video grounding. higher weights are not necessarily more relevant to the sentence query [21], the indirect optimization may restrict further performance improvement.
Recently, contrastive learning has greatly promoted unsupervised pretraining of visual representa-tions [16, 7], which develops contrastive training between positive and negative samples to learn expressive visual features. In this paper, we propose a novel Counterfactual Contrastive Learn-ing (CCL) paradigm for the WSVLG task, which constructs ﬁne-grained supervision signals from counterfactual results to directly optimize the visual-textual alignment. Speciﬁcally, we ﬁrst em-ploy a MIL-based pre-trained grounding network to estimate the given vision-language pairs to produce original results. By the gradient-based selection method, we then build a critical proposal set and an inessential proposal set. Next, we design Robust Counterfactual Transformations (RCT) based on the inessential set and devise Destructive Counterfactual Transformations (DCT) according to the critical set. After it, we apply the grounding network with constructed RCT and DCT to generate counterfactual results of the vision-language pairs, including positive and negative results corresponding to RCT and DCT, respectively. Finally, we develop a ranking loss to focus on the score-based difference between positive and negative results, and further devise the consistency loss to consider the distribution-based discrepancy between them. To make the contrastive training effective, the network with DCT needs to generate plausible negative results, where crucial visual contents corresponding to the sentence query are destroyed while unnecessary contents are retained.
On the contrary, the network with RCT should damage visual contents of inessential proposals and produce robust positive results. Thus, we design the counterfactual transformations from three levels: (1) the feature-level strategy damages the features (i.e. endogenous clues) of selected proposals by the memory-based replacement; (2) the interaction-level strategy confuses the vision-language interaction by destroying the multi-modal fusion; and (3) the relation-level strategy perturbs the context relations (i.e. exogenous clues) of chosen proposals by counterfactual relation construction.
The three strategies are applied to the intermediate process of network inference rather than the raw inputs, and produce ambiguous results for sufﬁcient contrastive learning.
The main contributions of this paper are summarized as follows:
• We propose a novel counterfactual contrastive learning for WSVLG, which develops sufﬁ-cient contrastive training between counterfactual positive and negative results.
• We design the feature-level, interaction-level and relation-level strategies for counterfactual transformations, which are applied to the intermediate process of network inference.
• Our CCL not only focuses on the score-based difference between the positive and negative results but also considers the distribution-based discrepancy between them.
• We conduct extensive experiments on ﬁve large-scale vision-language grounding datasets to verify the effectiveness of our proposed CCL paradigm. 2