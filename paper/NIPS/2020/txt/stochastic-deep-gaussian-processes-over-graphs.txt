Abstract
In this paper we propose Stochastic Deep Gaussian Processes over Graphs (DGPG), which are deep Gaussian models that learn the mappings between input and output signals in graph domains. The approximate posterior distributions of the latent variables are derived with variational inference, and the evidence lower bound is evaluated and optimized by the proposed recursive sampling scheme. The
Bayesian non-parametric natural of our model allows it to resist overﬁtting, while the expressive deep structure grants it the potential to learn complex relations.
Extensive experiments demonstrate that our method achieves superior performances in both small size (< 50) and large size (> 35,000) datasets. We show that DGPG outperforms another Gaussian-based approach, and is competitive to a state-of-the-art method in the challenging task of trafﬁc ﬂow prediction. Our model is also capable of capturing uncertainties in a mathematical principled way and automatically discovering which vertices and features are relevant to the prediction. 1

Introduction
Gaussian processes (GPs) [1] are a favourable choice in the machine learning arsenal, due to their distinguishing advantages in modeling uncertainties, the ability of introducing expert knowledge through the ﬂexible kernel design, and the data efﬁcient property which accounts for their success in small and medium datasets. GPs have been successfully applied in a variety of tasks, including computer vision [2], Bayesian optimization [3], active learning [4], multi-task learning [5] and reinforcement learning [6]. However standard GPs scale poorly as O(N 3), making it a challenge to apply them to large-scale datasets. Their expressiveness is also limited by the speciﬁc choice of kernel functions, which are difﬁcult to be manually decided for complex problems.
Recently a series of works about deep GPs were presented [7–9], which overcome the aforementioned disadvantages. All these methods can ﬁnd their roots in the seminal paper [10], which proposes to use a set of inducing points whose size is manageable to summarize all the information in the original large dataset - the inducing points and their latent function values can be inferred by variational inference. This sparse approximation technique enables deep GPs to handle extremely large datasets.
For instance [11] reports that their method achieves superior performance on a dataset with over one billion data points. The deep hierarchical structure has greatly improved the expressiveness of the models, resulting in state-of-the-art performances over various challenging tasks.
The last decade also witnesses the rapid development of machine learning algorithms over graph-structured datasets. A great amount of impacting research on graph neural networks (GNNs) has been presented, including [12–17], to name only a few of which. Hitherto most of the emerging literature on graph data analysis are based on neural networks. Though demonstrating satisfactory
∗Equal contributions.
†Corresponding author: Shu-Tao Xia (xiast@sz.tsinghua.edu.cn). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
performances over various graph learning tasks, their nature as parametric methods is associated with several inevitable drawbacks: they are insufﬁcient in modeling uncertainties; they are vulnerable to overﬁtting, especially on small datasets; the learning is difﬁcult and their success relies heavily on the choices of network structures and hyperparameters. A natural question to be asked is that can we analyze graph-structured data with non-parametric models such as Gaussian processes? A few recent publications show us that the answer is positive [18–20]. However, the investigations up to now are far from thorough and complete, and the development of deep Gaussian processes has also revealed us an alternative approach that is scarcely explored by far.
In this paper we propose Stochastic Deep Gaussian Processes over Graphs (DGPG), which is a method for modeling the relations between input and output signals over graph-structured domains.
Our work is closely related to [11]: both our methods are based on sparse approximation [10] and the variational inference framework [21]. Though the evidence lower bound is analytically intractable, it can be evaluated and optimized by the stochastic minibatch sampling technique.
We summarize the main contributions of this paper as follows: 1) We propose a novel Bayesian non-parametric method called Stochastic Deep Gaussian Processes over Graphs (DGPG), to model the relations of input/output signals over graphs; 2) It is rigorously proved that under some technical assumptions, the sampling variances of DGPG are strictly less then that of [11], implying that DGPG achieves faster convergence by considering graph information; 3) We performed experiments on a synthetic dataset. Numerical results conform with our theoretical analysis and support the claim that DGPG converges much faster and better by utilizing graph information; 4) Experiments on realistic datasets demonstrate that DGPG can be successfully applied to both small and large-scale datasets. We show that our method outperforms a recent GP-based graph learning algorithm, and is competitive to a state-of-the-art DNN method on the challenging task of trafﬁc ﬂow prediction; 5) We show that DGPG possesses several other desirable characteristics: it can model uncertainties with high accuracy, and the automatic relevance determination (ARD) kernel allows it to learn which neighbors and features are of greater importance for the prediction. 2