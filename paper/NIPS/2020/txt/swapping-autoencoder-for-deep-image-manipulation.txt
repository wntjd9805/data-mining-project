Abstract
Deep generative models have become increasingly effective at producing realistic images from randomly sampled seeds, but using such models for controllable manipulation of existing images remains challenging. We propose the Swapping
Autoencoder, a deep model designed speciﬁcally for image manipulation, rather than random sampling. The key idea is to encode an image into two independent components and enforce that any swapped combination maps to a realistic image.
In particular, we encourage the components to represent structure and texture, by enforcing one component to encode co-occurrent patch statistics across different parts of the image. As our method is trained with an encoder, ﬁnding the latent codes for a new input image becomes trivial, rather than cumbersome. As a result, our method enables us to manipulate real input images in various ways, including texture swapping, local and global editing, and latent code vector arithmetic. Experiments on multiple datasets show that our model produces better results and is substantially more efﬁcient compared to recent generative models.
Figure 1: Our Swapping Autoencoder learns to disentangle texture from structure for image editing tasks. One such task is texture swapping, shown here. Please see our project webpage for a demo video of our editing method. 1

Introduction
Traditional photo-editing tools, such as Photoshop, operate solely within the conﬁnes of the input image, i.e. they can only “recycle” the pixels that are already there. The promise of using machine learning for image manipulation has been to incorporate the generic visual knowledge drawn from external visual datasets into the editing process. The aim is to enable new class of editing operations, such as inpainting large image regions [60, 81, 55], synthesizing photorealistic images from layouts [33, 73, 59], replacing objects [88, 28], or changing the time photo is taken [41, 2].
However, learning-driven image manipulation brings in its own challenges. For image editing, there is a fundamental conﬂict: what information should be gleaned from the dataset versus information that must be retained from the input image? If the output image relies too much on the dataset, it will retain no resemblance to the input, so can hardly be called “editing”, whereas relying too much on the input lessens the value of the dataset. This conﬂict can be viewed as a disentanglement 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
problem. Starting from image pixels, one needs to factor out the visual information which is speciﬁc to a given image from information that is applicable across different images of the dataset. Indeed, many existing works on learning-based image manipulation, though not always explicitly framed as learning disentanglement, end up doing so, using paired supervision [70, 33, 73, 59], domain supervision [88, 30, 56, 2], or inductive bias of the model architecture [1, 21].
In our work, we aim to discover a disentanglement suitable for image editing in an unsupervised setting.
We argue that it is natural to explicitly factor out the visual patterns within the image that must change consistently with respect to each other. We operationalize this by learning an autoencoder with two modular latent codes, one to capture the within-image visual patterns, and another to capture the rest of the information. We enforce that any arbitrary combination of these codes map to a realistic image.
To disentangle these two factors, we ensure that all image patches with the same within-image code appear coherent with each other. Interestingly, this coincides with the classic deﬁnition of visual texture in a line of works started by Julesz [38, 40, 39, 64, 24, 17, 54]. The second code captures the remaining information, coinciding with structure. As such, we refer to the two codes as texture and structure codes.
A natural question to ask is: why not simply use unconditional GANs [19] that have been shown to disentangle style and content in unsupervised settings [43, 44, 21]? The short answer is that these methods do not work well for editing existing images. Unconditional GANs learn a mapping from an easy-to-sample (typically Gaussian) distribution. Some methods [4, 1, 44] have been suggested to retroﬁt pre-trained unconditional GAN models to ﬁnd the latent vector that reproduces the input image, but we show that these methods are inaccurate and magnitudes slower than our method. The conditional GAN models [33, 88, 30, 59] address this problem by starting with input images, but they require the task to be deﬁned a priori. In contrast, our model learns an embedding space that is useful for image manipulation in several downstream tasks, including synthesizing new image hybrids (see Figure 1), smooth manipulation of attributes or domain transfer by traversing latent directions (Figure 7), and local manipulation of the scene structure (Figure 8).
To show the effectiveness of our method, we evaluate it on multiple datasets, such as LSUN churches and bedrooms [80], FlickrFaces-HQ [43], and newly collected datasets of mountains and waterfalls, using both automatic metrics and human perceptual judgments. We also present an interactive UI (please see our video in the project webpage) that showcases the advantages of our method. 2