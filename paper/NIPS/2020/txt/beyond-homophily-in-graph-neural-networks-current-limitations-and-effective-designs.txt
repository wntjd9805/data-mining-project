Abstract
We investigate the representation power of graph neural networks in the semi-supervised node classiﬁcation task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Many popular GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer percep-trons). Motivated by this limitation, we identify a set of key designs—ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations—that boost learning from the graph structure under heterophily. We combine them into a graph neural network, H2GCN, which we use as the base method to empirically evaluate the effectiveness of the identiﬁed designs. Going beyond the traditional benchmarks with strong homophily, our empirical analysis shows that the identiﬁed designs increase the accuracy of GNNs by up to 40% and 27% over models without them on synthetic and real networks with heterophily, respectively, and yield competitive performance under homophily. 1

Introduction
We focus on the effectiveness of graph neural networks (GNNs) [42] in tackling the semi-supervised node classiﬁcation task in challenging settings: the goal of the task is to infer the unknown labels of the nodes by using the network structure [44], given partially labeled networks with node features (or attributes). Unlike most prior work that considers networks with strong homophily, we study the representation power of GNNs in settings with different levels of homophily or class label smoothness.
Homophily is a key principle of many real-world networks, whereby linked nodes often belong to the same class or have similar features (“birds of a feather ﬂock together”) [21]. For example, friends are likely to have similar political beliefs or age, and papers tend to cite papers from the same research area [23]. GNNs model the homophily principle by propagating features and aggregating them within various graph neighborhoods via different mechanisms (e.g., averaging, LSTM) [17, 11, 36].
However, in the real world, there are also settings where “opposites attract”, leading to networks with heterophily: linked nodes are likely from different classes or have dissimilar features. For instance, the majority of people tend to connect with people of the opposite gender in dating networks, different amino acid types are more likely to connect in protein structures, fraudsters are more likely to connect to accomplices than to other fraudsters in online purchasing networks [24].
Since many existing GNNs assume strong homophily, they fail to generalize to networks with heterophily (or low/medium level of homophily). In such cases, we ﬁnd that even models that ignore 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the graph structure altogether, such as multilayer perceptrons or MLPs, can outperform a number of existing GNNs. Motivated by this limitation, we make the following contributions:
• Current Limitations: We reveal the limitation of GNNs to learn over networks with heterophily, which is ignored in the literature due to evaluation on few benchmarks with similar properties. § 3
• Key Designs for Heterophily & New Model: We identify a set of key designs that can boost learn-ing from the graph structure in heterophily without trading off accuracy in homophily: (D1) ego-and neighbor-embedding separation, (D2) higher-order neighborhoods, and (D3) combination of intermediate representations. We justify the designs theoretically, and combine them into a model,
H2GCN, that effectively adapts to both heterophily and homophily. We compare it to prior GNN models, and make our code and data available at https://github.com/GemsLab/H2GCN. § 3-4
• Extensive Empirical Evaluation: We empirically analyze our model and competitive existing
GNN models on both synthetic and real networks covering the full spectrum of low-to-high homophily (besides the typically-used benchmarks with strong homophily only). In synthetic networks, our detailed ablation study of H2GCN (which is free of confounding designs) shows that the identiﬁed designs result in up to 40% performance gain in heterophily. In real networks, we observe that GNN models utilizing even a subset of our identiﬁed designs outperform popular models without them by up to 27% in heterophily, while being competitive in homophily. § 5 2 Notation and Preliminaries
We summarize our notation in Table A.1 (App. A). Let G = (V, E) be an undirected, unweighted graph with nodeset V and edgeset E. We denote a general neighborhood centered around v as N (v) (G may have self-loops), the corresponding neighborhood that does not include the ego (node v) as ¯N (v), and the general neighbors of node v at exactly i hops/steps away (minimum distance) as Ni(v). For example,
N1(v) = {u : (u, v) ∈ E} are the immediate neighbors of v. Other examples are shown in Fig. 1. We represent the graph by its adjacency
Figure 1: Neighborhoods. matrix A ∈ {0, 1}n×n and its node feature matrix X ∈ Rn×F , where the vector xv corresponds to the ego-feature of node v, and {xu : u ∈ ¯N (v)} to its neighbor-features.
We further assume a class label vector y, which for each node v contains a unique class label yv. The goal of semi-supervised node classiﬁcation is to learn a mapping (cid:96) : V → Y, where Y is the set of labels, given a set of labeled nodes TV = {(v1, y1), (v2, y2), ...} as training data.
Graph neural networks From a probabilistic perspective, most GNN models assume the following local Markov property on node features: for each node v ∈ V, there exists a neighborhood N (v) such that yv only depends on the ego-feature xv and neighbor-features {xu : u ∈ N (v)}. Most models derive the class label yv via the following representation learning approach: (cid:16) r(k) v = f r(k−1) v
, {r(k−1) u
: u ∈ N (v)} (cid:17)
, r(0) v = xv, and yv = arg max{softmax(r(K) v
)W}, (1) where the embedding function f is applied repeatedly in K total rounds, node v’s representation (or hidden state vector) at round k, r(k) v , is learned from its ego- and neighbor-representations in the previous round, and a softmax classiﬁer with learnable weight matrix W is applied to the ﬁnal representation of v. Most existing models differ in their deﬁnitions of neighborhoods N (v) and embedding function f . A typical deﬁnition of neighborhood is N1(v)—i.e., the 1-hop neighbors of v.
As for f , in graph convolutional networks (GCN) [17] each node repeatedly averages its own features and those of its neighbors to update its own feature representation. Using an attention mechanism,
GAT [36] models the inﬂuence of different neighbors more precisely as a weighted average of the ego- and neighbor-features. GraphSAGE [11] generalizes the aggregation beyond averaging, and models the ego-features distinctly from the neighbor-features in its subsampled neighborhood.
Homophily and heterophily
In this work, we focus on heterophily in class labels. We ﬁrst deﬁne the edge homophily ratio h as a measure of the graph homophily level, and use it to deﬁne graphs with strong homophily/heterophily:
Deﬁnition 1 The edge homophily ratio h = |{(u,v):(u,v)∈E∧yu=yv}| graph which connect nodes that have the same class label (i.e., intra-class edges). is the fraction of edges in a
|E|
Deﬁnition 2 Graphs with strong homophily have high edge homophily ratio h → 1, while graphs with strong heterophily (i.e., low/weak homophily) have small edge homophily ratio h → 0. 2
The edge homophily ratio in Dfn. 1 gives an overall trend for all the edges in the graph. The actual level of homophily may vary within different pairs of node classes, i.e., there is different tendency of connection between each pair of classes. In App. B, we give more details about capturing these more complex network characteristics via an empirical class compatibility matrix H, whose i, j-th entry is the fraction of outgoing edges to nodes in class j among all outgoing edges from nodes in class i.
Heterophily (cid:54)= Heterogeneity. We remark that heterophily, which we study in this work, is a distinct network concept from heterogeneity. Formally, a network is heterogeneous [34] if it has at least two types of nodes and different relationships between them (e.g., knowledge graphs), and homogeneous if it has a single type of nodes (e.g., users) and a single type of edges (e.g., friendship). The type of nodes in heterogeneous graphs does not necessarily match the class labels yv, therefore both homogeneous and heterogeneous networks may have different levels of homophily. 3 Learning Over Networks with Heterophily
Table 1: Example of a heterophily setting (h = 0.1) where existing GNNs fail to generalize, and a typical homophily setting (h = 0.7): mean accuracy and standard deviation over three runs (cf. App. G).
While many GNN models have been proposed, most of them are designed under the assumption of homophily, and are not capable of handling heterophily. As a moti-vating example, Table 1 shows the mean classiﬁcation accuracy for several leading GNN models on our syn-thetic benchmark syn-cora, where we can control the homophily/heterophily level (see App. G for details on the data and setup). Here we consider two homophily ratios, h = 0.1 and h = 0.7, one for high heterophily and one for high homophily. We observe that for het-erophily (h = 0.1) all existing methods fail to perform better than a Multilayer Perceptron (MLP) with 1 hidden layer, a graph-agnostic baseline that relies solely on the node features for classiﬁcation (differences in accuracy of MLP for different h are due to randomness). Especially, GCN [17] and GAT [36] show up to 42% worse performance than MLP, highlighting that methods that work well under high homophily (h = 0.7) may not be appropriate for networks with low/medium homophily.
GCN [17]
GAT [36]
GCN-Cheby [7]
GraphSAGE [11]
MixHop [1] 84.52±0.54 84.03±0.97 84.92±1.03 85.06±0.51 84.43±0.94 37.14±4.60 33.11±1.20 68.10±1.75 72.89±2.42 58.93±2.84
H2GCN (ours) 76.87±0.43 88.28±0.66 71.72±0.62 74.85±0.76 h = 0.1 h = 0.7
MLP
Motivated by this limitation, in the following subsections, we discuss and theoretically justify a set of key design choices that, when appropriately incorporated in a GNN framework, can improve the performance in the challenging heterophily settings. Then, we present H2GCN, a model that, thanks to these designs, adapts well to both homophily and heterophily (Table 1, last row). In Section 5, we provide a comprehensive empirical analysis on both synthetic and real data with varying homophily levels, and show that the identiﬁed designs signiﬁcantly improve the performance of GNNs (not limited to H2GCN) by effectively leveraging the graph structure in challenging heterophily settings, while maintaining competitive performance in homophily. 3.1 Effective Designs for Networks with Heterophily
We have identiﬁed three key designs that—when appropriately integrated—can help improve the performance of GNN models in heterophily settings: (D1) ego- and neighbor-embedding separation; (D2) higher-order neighborhoods; and (D3) combination of intermediate representations. While these designs have been utilized separately in some prior works [11, 7, 1, 38], we are the ﬁrst to discuss their importance under heterophily by providing novel theoretical justiﬁcations and an extensive empirical analysis on a variety of datasets. 3.1.1 (D1) Ego- and Neighbor-embedding Separation
The ﬁrst design entails encoding each ego-embedding (i.e., a node’s embedding) separately from the aggregated embeddings of its neighbors, since they are likely to be dissimilar in heterophily settings.
Formally, the representation (or hidden state vector) learned for each node v at round k is given as: (2) the neighborhood ¯N (v) does not include v (no self-loops), the AGGR function aggregates representa-tions only from the neighbors (in some way—e.g., average), and AGGR and COMBINE may be followed r(k) v = COMBINE
, AGGR({r(k−1)
: u ∈ ¯N (v) }) r(k−1) v (cid:16) (cid:17) u
, 3
by a non-linear transformation. For heterophily, after aggregating the neighbors’ representations, the deﬁnition of COMBINE (akin to ‘skip connection’ between layers) is critical: a simple way to combine the ego- and the aggregated neighbor-embeddings without ‘mixing’ them is with concatenation as in
GraphSAGE [11]—rather than averaging all of them as in the GCN model by Kipf and Welling [17].
Intuition. In heterophily settings, by deﬁnition (Dfn. 2), the class label yv and original features xv of a node and those of its neighboring nodes {(yu, xu) : u ∈ ¯N (v)} (esp. the direct neighbors
¯N1(v)) may be different. However, the typical GCN design that mixes the embeddings through an average [17] or weighted average [36] as the COMBINE function results in ﬁnal embeddings that are similar across neighboring nodes (especially within a community or cluster) for any set of original features [28]. While this may work well in the case of homophily, where neighbors likely belong to the same cluster and class, it poses severe challenges in the case of heterophily: it is not possible to distinguish neighbors from different classes based on the (similar) learned representations. Choosing a COMBINE function that separates the representations of each node v and its neighbors ¯N (v) allows for more expressiveness, where the skipped or non-aggregated representations can evolve separately over multiple rounds of propagation without becoming prohibitively similar.
Theoretical Justiﬁcation. We prove theoretically that, under some conditions, a GCN layer that co-embeds ego- and neighbor-features is less capable of generalizing to heterophily than a layer that embeds them separately. We measure its generalization ability by its robustness to test/train data deviations. We give the proof of the theorem in App. C.1. Though the theorem applies to speciﬁc conditions, our empirical analysis shows that it holds in more general cases (§ 5).
Theorem 1 Consider a graph G without self-loops (§ 2) with node features xv = onehot(yv) for each node v, and an equal number of nodes per class y ∈ Y in the training set TV . Also assume that all nodes in TV have degree d, and proportion h of their neighbors belong to the same class, while
|Y|−1 of them belong to any other class (uniformly). Then for h < 1−|Y|+2d proportion 1−h
, a simple
GCN layer formulated as (A + I)XW is less robust, i.e., misclassiﬁes a node for smaller train/test data deviations, than a AXW layer that separates the ego- and neighbor-embeddings. 2|Y|d
Observations. In Table 1, we observe that GCN, GAT, and MixHop, which ‘mix’ the ego- and neighbor-embeddings explicitly1, perform poorly in the heterophily setting. On the other hand,
GraphSAGE that separates the embeddings (e.g., it concatenates the two embeddings and then applies a non-linear transformation) achieves 33-40% better performance in this setting. 3.1.2 (D2) Higher-order Neighborhoods
The second design involves explicitly aggregating information from higher-order neighborhoods in each round k, beyond the immediate neighbors of each node: r(k) v = COMBINE (cid:16) r(k−1) v
, AGGR({r(k−1) u
: u ∈ N1(v) }), AGGR({r(k−1) u
: u ∈ N2(v) }), . . . (cid:17) (3) where Ni(v) denotes the neighbors of v at exactly i hops away, and the AGGR functions applied to different neighborhoods can be the same or different. This design—employed in GCN-Cheby [7] and
MixHop [1]—augments the implicit aggregation over higher-order neighborhoods that most GNN models achieve through multiple rounds of ﬁrst-order propagation based on variants of Eq. (2).
Intuition. To show why higher-order neighborhoods help in the heterophily settings, we ﬁrst deﬁne homophily-dominant and heterophily-dominant neighborhoods:
Deﬁnition 3 N (v) is expectedly homophily-dominant if P (yu = yv|yv) ≥ P (yu = y|yv), ∀u ∈
N (v) and y ∈ Y (cid:54)= yv. If the opposite inequality holds, N (v) is expectedly heterophily-dominant.
From this deﬁnition, we can see that expectedly homophily-dominant neighborhoods are more beneﬁcial for GNN layers, as in such neighborhoods the class label yv of each node v can in expectation be determined by the majority of the class labels in N (v). In the case of heterophily, we have seen empirically that although the immediate neighborhoods may be heterophily-dominant, the higher-order neighborhoods may be homophily-dominant and thus provide more relevant context.
This observation is also conﬁrmed by recent works [2, 6] in the context of binary attribute prediction. 1 These models consider self-loops, which turn each ego also into a neighbor, and thus mix the ego- and neighbor-representations. E.g., GCN and MixHop operate on the symmetric normalized adjacency matrix augmented with self-loops: ˆA = ˆD− 1 2 , where I is the identity and ˆD the degree matrix of A + I. 2 (A + I) ˆD− 1 4
Theoretical Justiﬁcation. Below we formalize the above observation for 2-hop neighborhoods under non-binary attributes (labels), and prove one case when they are homophily-dominant in App. C.2:
Theorem 2 Consider a graph G without self-loops (§ 2) with label set Y, where for each node v, its neighbors’ class labels {yu : u ∈ N (v)} are conditionally independent given yv, and P (yu = yv|yv) = h, P (yu = y|yv) = 1−h
|Y|−1 , ∀y (cid:54)= yv. Then, the 2-hop neighborhood N2(v) for a node v will always be homophily-dominant in expectation.
Observations. Under heterophily (h = 0.1), GCN-Cheby, which models different neighborhoods by combining Chebyshev polynomials to approximate a higher-order graph convolution operation [7], outperforms GCN and GAT, which aggregate over only the immediate neighbors N1, by up to +31% (Table 1). MixHop, which explicitly models 1-hop and 2-hop neighborhoods (though ‘mixes’ the ego- and neighbor-embeddings1, violating design D1), also outperforms these two models. 3.1.3 (D3) Combination of Intermediate Representations
The third design combines the intermediate representations of each node at the ﬁnal layer: r(ﬁnal) v
= COMBINE v , r(2) r(1) v , . . . , r(K) v (cid:18) (cid:19) (4) to explicitly capture local and global information via COMBINE functions that leverage each represen-tation separately–e.g., concatenation, LSTM-attention [38]. This design is introduced in jumping knowledge networks [38] and shown to increase the representation power of GCNs under homophily.
Intuition. Intuitively, each round collects information with different locality—earlier rounds are more local, while later rounds capture increasingly more global information (implicitly, via propagation).
Similar to D2 (which models explicit neighborhoods), this design models the distribution of neighbor representations in low-homophily networks more accurately. It also allows the class prediction to leverage different neighborhood ranges in different networks, adapting to their structural properties.
Theoretical Justiﬁcation. The beneﬁt of combining intermediate representations can be theoretically explained from the spectral perspective. Assuming a GCN-style layer—where propagation can be viewed as spectral ﬁltering—, the higher order polynomials of the normalized adjacency matrix
A is a low-pass ﬁlter [37], so intermediate outputs from earlier rounds contain higher-frequency components than outputs from later rounds. At the same time, the following theorem holds for graphs with heterophily, where we view class labels as graph signals (as in graph signal processing):
Theorem 3 Consider graph signals (label vectors) s, t ∈ {0, 1}|V| deﬁned on an undirected graph
G with edge homophily ratios hs and ht, respectively. If hs < ht, then signal s has higher energy (Dfn. 5) in high-frequency components than t in the spectrum of unnormalized graph Laplacian L.
In other words, in heterophily settings, the label distribution contains more information at higher than lower frequencies (see proof in App. C.3). Thus, by combining the intermediate outputs from different layers, this design captures both low- and high-frequency components in the ﬁnal representation, which is critical in heterophily settings, and allows for more expressiveness in the general setting.
Observations. By concatenating the intermediate representations from two rounds with the embedded ego-representation (following the jumping knowledge framework [38]), GCN’s accuracy increases to 58.93%±3.17 for h = 0.1, a 20% improvement over its counterpart without design D3 (Table 1).
Summary of designs To sum up, D1 models (at each layer) the ego- and neighbor-representations distinctly, D2 leverages (at each layer) representations of neighbors at different distances distinctly, and D3 leverages (at the ﬁnal layer) the learned ego-representations at previous layers distinctly. 3.2 H2GCN: A Framework for Networks with Homophily or Heterophily
We now describe H2GCN, which exempliﬁes how effectively combining designs D1-D3 can help better adapt to the whole spectrum of low-to-high homophily, while avoiding interference with other designs. It has three stages (Alg. 1, App. D): (S1) feature embedding, (S2) neighborhood aggregation, and (S3) classiﬁcation.
The feature embedding stage (S1) uses a graph-agnostic dense layer to generate for each node v the feature embedding r(0) v = σ(xvWe), where σ is an optional non-linear function, and We ∈ RF ×p is a learnable weight matrix. v ∈ Rp based on its ego-feature xv: r(0) 5
In the neighborhood aggregation stage (S2), the generated embeddings are aggregated and repeatedly updated within the node’s neighborhood for K rounds. Following designs D1 and D2, the neighbor-hood N (v) of our framework involves two sub-neighborhoods without the egos: the 1-hop graph neighbors ¯N1(v) and the 2-hop neighbors ¯N2(v), as shown in Fig. 1:
: u ∈ ¯N1(v)}, AGGR{r(k−1) (cid:16)
AGGR{r(k−1)
: u ∈ ¯N2(v)} r(k) v = COMBINE (5) (cid:17) u u
. (cid:16) (cid:17) v,2 r(k) v = v,1(cid:107)r(k) r(k)
We set COMBINE as concatenation (as to not mix different neighborhood ranges), and AGGR as a degree-normalized average of the neighbor-embeddings in sub-neighborhood ¯Ni(v): r(k) v,i = AGGR{r(k−1) (6) where dv,i = | ¯Ni(v)| is the i-hop degree of node v (i.e., number of nodes in its i-hop neighborhood).
Unlike Eq. (2), here we do not combine the ego-embedding of node v with the neighbor-embeddings.
We found that removing the usual nonlinear transformations per round, as in SGC [37], works better (App. D.2), in which case we only need to include the ego-embedding in the ﬁnal representation. By design D3, each node’s ﬁnal representation combines all its intermediate representations:
: u ∈ ¯Ni(v)} = (cid:80) u∈ ¯Ni(v) r(k−1) d−1/2 v,i d−1/2 and u,i u u
, r(ﬁnal) v
= COMBINE v , r(1) r(0) v , . . . , r(K) v
, (7) (cid:16) (cid:17) where we empirically ﬁnd concatenation works better than max-pooling [38] as the COMBINE function.
In the classiﬁcation stage (S3), the node is classiﬁed based on its ﬁnal embedding r(ﬁnal)
: v (8) where Wc ∈ R(2K+1−1)p×|Y| is a learnable weight matrix. We visualize our framework in App. D. yv = arg max{softmax(r(ﬁnal) v Wc)},
Time complexity The feature embedding stage (S1) takes O(nnz(X) p), where nnz(X) is the number of non-0s in feature matrix X ∈ Rn×F , and p is the dimension of the feature embeddings. The neighborhood aggregation stage (S2) takes O (|E|dmax) to derive the 2-hop neighborhoods via sparse-matrix multiplications, where dmax is the maximum degree of all nodes, and O (cid:0)2K(|E| + |E2|)p(cid:1) v∈V | ¯N2(v)|. We give a detailed analysis in App. D. for K rounds of aggregation, where |E2| = 1 2 (cid:80) 4 Other