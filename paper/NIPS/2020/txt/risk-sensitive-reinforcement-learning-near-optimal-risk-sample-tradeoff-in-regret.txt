Abstract
√
H 3S2AT (cid:1) regret, while RSQ attains an ˜O(cid:0)λ(|β|H 2)·
We study risk-sensitive reinforcement learning in episodic Markov decision processes with unknown transition kernels, where the goal is to optimize the total reward under the risk measure of exponential utility. We propose two provably efﬁcient model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ). These algorithms implement a form of risk-sensitive optimism in the face of uncertainty, which adapts to both risk-seeking and risk-averse modes of exploration. We prove that RSVI attains an
˜O(cid:0)λ(|β|H 2)·
H 4SAT (cid:1) regret, where λ(u) = (e3u−1)/u for u > 0. In the above, β is the risk parameter of the exponential utility function, S the number of states, A the number of actions, T the total number of timesteps, and H the episode length. On the ﬂip side, we estab-lish a regret lower bound showing that the exponential dependence on |β| and H is
√ unavoidable for any algorithm with an ˜O(
T ) regret (even when the risk objective is on the same scale as the original reward), thus certifying the near-optimality of the proposed algorithms. Our results demonstrate that incorporating risk awareness into reinforcement learning necessitates an exponential cost in |β| and H, which quantiﬁes the fundamental tradeoff between risk sensitivity (related to aleatoric uncertainty) and sample efﬁciency (related to epistemic uncertainty). To the best of our knowledge, this is the ﬁrst regret analysis of risk-sensitive reinforcement learning with the exponential utility.
√ 1

Introduction
Risk-sensitive reinforcement learning (RL) concerns learning to act in a dynamic environment while taking into account risks that arise during the learning process. Effective management of risks in RL is critical to many real-world applications such as autonomous driving [32], real-time strategy games
[56], ﬁnancial investment [44], etc. In neuroscience, risk-sensitive RL has been applied to model human behaviors in decision making [46, 52].
In this paper, we consider risk-sensitive RL with the exponential utility [34] under episodic Markov decision processes (MDPs) with unknown transition kernels. Informally, the agent aims to maximize a risk-sensitive objective function of the form
V = 1
β log (cid:8)EeβR(cid:9) , (1) where R is the total reward the agent receives, and β (cid:54)= 0 is a real-valued parameter that controls risk preference of the agent; see Equation (2) for a formal deﬁnition of V . The objective V admits the Taylor expansion V = E[R] + β 2 Var(R) + O(β2). It can be seen that for β > 0 the agent 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is risk-seeking (favoring high uncertainty in R), for β < 0 the agent is risk-averse (favoring low uncertainty in R), and a larger |β| implies higher risk-sensitivity. When β → 0, the agent tends to be risk-neutral and the objective reduces to the expected reward objective V = E[R] standard in RL.
Therefore, the risk-sensitive objective in (1) covers the entire spectrum of risk sensitivity by varying
β. In addition, the formulation (1) is closely related to RL with constraints. For example, a negative risk parameter β controls the tail of a risk distribution so as to mitigate the chance of receiving a total reward R that is excessively low. We refer to [42, Section 2.1] for an in-depth discussion of this connection.
The challenge of risk-sensitive RL lies both in the non-linearity of the objective function and in designing a risk-aware exploration mechanism. In particular, as we elaborate in Section 2.2, the non-linear objective function (1) induces a non-linear Bellman equation. Classical RL algorithms are inappropriate in this setting, as their design crucially relies on the linearity of Bellman equations. On the other hand, effective exploration has been well known to be crucial to RL algorithm design, yet it is not clear how to design an algorithm that efﬁciently explores uncertain environments while at the same time adapting to the risk-sensitive objective (1) of agents with different risk parameter β.
√
H 3S2AT (cid:1) regret, and RSQ achieves an ˜O(cid:0)λ(|β|H 2) ·
To address these difﬁculties, we propose two model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ). Speciﬁcally, RSVI is a batch algorithm and RSQ is an online algorithm; both families of batch and online algorithms see broad applications in practice.
We demonstrate in Section 3 that our proposed algorithms implement a form of risk-sensitive optimism for exploration. Importantly, the exact implementation of optimism depends on both the magnitude and the sign of the risk parameter, and therefore applies to both risk-seeking and risk-averse modes of learning. Letting λ(u) = (e3u − 1)/u for u > 0, we prove that RSVI attains an
H 4SAT (cid:1) regret. Here, S
˜O(cid:0)λ(|β|H 2) · and A are the numbers of states and actions, respectively, T is the total number of timesteps, and
H is the length of each episode. These regret bounds interpolate across different regimes of risk sensitivity and subsume existing results under the risk-neutral setting. Compared with risk-neutral
RL (corresponding to β → 0), our general regret bounds feature an exponential dependency on |β| and H, even though the risk-sensitive objective (1) is on the same scale as the total reward; see
Figure 1 for a plot of the exponential factor λ(|β|H 2). Complementarily, we prove a lower bound showing that such an exponential dependency is inevitable for any algorithm and thus certiﬁes the near-optimality of the proposed algorithms. To the best of our knowledge, our work provides the ﬁrst regret analysis of risk-sensitive RL with the exponential utility.
√
Our upper and lower bounds demonstrate the fundamental tradeoff between risk sensitivity and sample efﬁciency in RL.1 Broadly speaking, risk sensitivity is associated with aleatoric uncertainty, which originates from the inherent randomness of state transition, actions and rewards, whereas sample efﬁciency is associated with epistemic uncertainty, which arises from imperfect knowledge of the environment/system and can be reduced by more exploration [20, 24]. These two notions of uncertainty are usually decoupled in the regret analysis of risk-neutral RL—in particular, using the expected reward as the objective effectively suppresses the aleatoric uncertainty. In risk-sensitive
RL, we establish that there is a fundamental connection and tradeoff between these two forms of uncertainty: the risk-seeking and risk-averse regimes both incur an exponential cost in |β| and H on the regret, whereas the regret is polynomial in H in the risk-neutral regime.
Our contributions. The contributions of our work can be summarized as follows:
• We consider the problem of risk-sensitive RL with the exponential utility. We propose two provably efﬁcient model-free algorithms, namely RSVI and RSQ, that implement risk-sensitive optimism in the face of uncertainty;
• We provide regret analysis for both algorithms over the entire spectrum of risk parameter β.
As β → 0, we show that our results recover the existing regret bounds in the risk-neutral setting;
• We provide a lower bound result that certiﬁes the near-optimality of our upper bounds and reveals a fundamental tradeoff between risk sensitivity and sample complexity. 1By standard arguments, regret can be translated into sample complexity bounds and vice versa; see [38]. 2
Figure 1: Scaling of λ(|β|H 2) in risk sensitivity |β| for different values of episode length H.