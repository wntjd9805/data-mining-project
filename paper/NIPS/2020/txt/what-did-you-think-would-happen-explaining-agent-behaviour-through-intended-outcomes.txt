Abstract
We present a novel form of explanation for Reinforcement Learning, based around the notion of intended outcome. These explanations describe the outcome an agent is trying to achieve by its actions. We provide a simple proof that general methods for post-hoc explanations of this nature are impossible in traditional reinforcement learning. Rather, the information needed for the explanations must be collected in conjunction with training the agent. We derive approaches designed to extract local explanations based on intention for several variants of Q-function approximation and prove consistency between the explanations and the Q-values learned. We demonstrate our method on multiple reinforcement learning problems, and provide code1 to help researchers introspecting their RL environments and algorithms. 1

Introduction
Explaining the behaviour of machine learning algorithms or AI remains a key challenge in machine learning. With the guidelines of the European Union’s General Data Protection Regulation (GDPR)
[1] calling for explainable AI, it has come to the machine learning community’s attention that better understanding of black-box models is needed. Despite substantial research in explaining the behaviour of supervised machine-learning, it is unclear what should constitute an explanation for Reinforcement
Learning (RL). Current works in explainable reinforcement learning use similar techniques as those used to explain a supervised classiﬁer [22], as such their explanations highlight what in the current environment drives an agent to take an action, but not what the agent expects the action to achieve.
Frequently, the consequences of the agent’s actions are not immediate and a chain of many decisions all contribute to a single desired outcome.
This paper addresses this problem by asking what chain of events the agent intended to happen as a result of a particular action choice. The importance of such explanations based around intended outcome in day-to-day life is well-known in psychology with Malle [20] estimating that around 70% of these day-to-day explanations are intent-based. While the notion of intent makes little sense in the context of supervised classiﬁcation, it is directly applicable to agent-based reasoning, and it is perhaps surprising that we are the ﬁrst work in explainable RL to directly address this. Recent work [18] has called for these introspective abilities which they refer to as “Explainable Agency”.
We present a simple addition to standard value-based RL frameworks which allows us to obtain a projection of predicted future trajectories from a current observation and proposed action. Unlike existing approaches such as [33] that predict an agent’s future states by rerunning repeated forward simulations from the same initial state; we instead recover sum of the past events, weighted by the importance that the agent put on them when learning a Q-values, that lead to the agents current behaviour, and mathematically guarantee that this sum is consistent with the agent’s Q-values.
This allows for local interpretation of the agent’s intention based on its behavioural policy. We
∗Part of this work was done while at the Alan Turing Institute and University of Surrey. 1https://github.com/hmhyau/rl-intention 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Understanding an agent’s expected behaviour. These images show predictions of an agent’s behaviour in blackjack. All plots show an initial state where the dealer shows 5, and the player’s cards sum to 7 (no high aces), and the colour of each cell shows the expected number of times a state will be visited, if the agent hits. The two leftmost plots show the predicted behaviour, based on forward simulation [33], and ‘belief’ of a well-trained agent, while the two rightmost show the predicted behaviour and ‘belief’ of an underperforming agent trained on a small and ﬁxed replay buffer. The bottom two images show the ﬁnal outcomes the well-trained and underperforming agents believe will occur if they hit. This results in a conservative agent that sticks early owing to an increased belief that they will go bust from a hit. mathematically guarantee that this is consistent with the agent’s Q-values (see ﬁgure 1). We hope our method will help RL practitioners trying to understand agent behaviour for model debugging and diagnostics, and potentially lead to more reliable and trustworthy RL systems.
Mismatches between learnt Q-values and the forward simulations of [33] exist for a variety of reasons: stochasticity of the environment; mismatches between training and test environments2; systematic biases in the learning method [13] and the use of replay buffers (particularly hindsight experience replay (HER) [4]). Our representations highlight what is learnt by the agent at train time, rather than the behaviour of the agent at test time, allowing them to provide insights into some of these failures. 2