Abstract
Weighted sampling is a fundamental tool in data analysis and machine learning pipelines. Samples are used for efﬁcient estimation of statistics or as sparse representations of the data. When weight distributions are skewed, as is often the case in practice, without-replacement (WOR) sampling is much more effective than with-replacement (WR) sampling: it provides a broader representation and higher accuracy for the same number of samples. We design novel composable sketches for WOR (cid:96)p sampling, weighted sampling of keys according to a power p ∈ [0, 2] of their frequency (or for signed data, sum of updates). Our sketches have size that grows only linearly with the sample size. Our design is simple and practical, despite intricate analysis, and based on off-the-shelf use of widely implemented heavy hitters sketches such as CountSketch. Our method is the ﬁrst to provide
WOR sampling in the important regime of p > 1 and the ﬁrst to handle signed updates for p > 0. 1

Introduction
Weighted random sampling is a fundamental tool that is pervasive in machine learning and data analysis pipelines. A sample serves as a sparse summary of the data and provides efﬁcient estimation of statistics and aggregates.
We consider data E presented as elements in the form of key value pairs e = (e.key, e.val). We operate with respect to the aggregated form of keys and their frequencies νx := (cid:80) e|e.key=x e.val, deﬁned as the sum of values of elements with key x. Examples of such data sets are stochastic gradient updates (keys are parameters and element values are signed and the aggregated form is the combined gradient), search (keys are queries, elements have unit values, and the aggregated form are query-frequency pairs), or training examples for language models (keys are co-occurring terms).
The data is commonly distributed across servers or devices or is streamed and the number of distinct keys is very large.
In this scenario it is beneﬁcial to perform computations without explicitly producing a table of key-frequency pairs, as this requires storage or communication that grows linearly with the number of keys. Instead, we use composable sketches which are data structures that support (i) processing a new element e: Computing a sketch of E ∪ {e} from a sketch of E and e (ii) merging: Computing a sketch of E1 ∪ E2 from sketches of each Ei and (iii) are such that the desired output can be produced from the sketch. Composability facilitates parallel, distributed, or streaming computation. We aim to design sketches of small size, because the sketch size determines the storage and communication requirements. For sampling, we aim for the sketch size to be not much larger than the desired sample size. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The case for p’s: Aggregation and statistics of functions of the frequencies are essential for resource allocation, planning, and management of large scale systems across application areas. The need for efﬁciency prompted rich theoretical and applied work on streaming and sketching methods that spanned decades [57, 39, 3, 36, 41, 34, 33, 52, 51]. We study (cid:96)p sampling, which refers to weighted sampling of keys with respect to a power p of their frequency νp x. These samples support estimates of frequency statistics of the general form (cid:80) x f (νx)Lx for functions of frequency f and constitute sparse representations of the data. Low powers (p < 1) are used to mitigate frequent keys and obtain a better resolution of the tail whereas higher powers (p > 1) emphasize more frequent keys.
Moreover, recent work suggests that on realistic distributions, (cid:96)p samples for p ∈ [0, 2] provide accurate estimates for a surprisingly broad set of tasks [32].
Sampling is at the heart of stochastic optimization. When training data is distributed [54], sampling can facilitate efﬁcient example selection for training and efﬁcient communication of gradient updates of model parameters. Training examples are commonly weighted by a function of their frequency:
Language models [56, 62] use low powers p < 1 of frequency to mitigate the impact of frequent examples. More generally, the function of frequency can be adjusted in the course of training to shift focus to rarer and harder examples as training progresses [8]. A sample of examples can be used to produce stochastic gradients or evaluate loss on domains of examples (expressed as frequency statistics). In distributed learning, the communication of dense gradient updates can be a bottleneck, prompting the development of methods that sparsify communication while retaining accuracy [54, 1, 66, 45]. Weighted sampling by the p-th powers of magnitudes complements existing methods that sparsify using heavy hitters (or other methods, e.g., sparsify randomly), provides adjustable emphasis to larger magnitudes, and retains sparsity as updates are composed.
The case for WOR: Weighted sampling is classically considered with (WR) or without (WOR) replacement. We study here the WOR setting. The beneﬁts of WOR sampling were noted in very early work [42, 40, 67] and are becoming more apparent with modern applications and the typical skewed distributions of massive datasets. WOR sampling provides a broader representation and more accurate estimates, with tail norms replacing full norms in error bounds. Figure 1 illustrates these beneﬁts of WOR for Zipﬁan distributions with (cid:96)1 sampling (weighted by frequencies) and (cid:96)2 sampling (weighted by the squares of frequencies). We can see that WR samples have a smaller effective sample size than WOR (due to high multiplicity of heavy keys) and that while both WR and WOR well-approximate the frequency distribution on heavy keys, WOR provides a much better approximation of the tail.
Figure 1: WOR vs WR. Left and middle: Effective vs actual sample size Zipf[α = 1] and Zipf[α = 2], with each point reﬂecting a single sample. Right: Estimates of the frequency distribution Zipf[α = 2].