Abstract
In this paper, we propose a new accelerated stochastic ﬁrst-order method called clipped-SSTM for smooth convex stochastic optimization with heavy-tailed dis-tributed noise in stochastic gradients and derive the ﬁrst high-probability complexity bounds for this method closing the gap in the theory of stochastic optimization with heavy-tailed noise. Our method is based on a special variant of accelerated
Stochastic Gradient Descent (SGD) and clipping of stochastic gradients. We extend our method to the strongly convex case and prove new complexity bounds that out-perform state-of-the-art results in this case. Finally, we extend our proof technique and derive the ﬁrst non-trivial high-probability complexity bounds for SGD with clipping without light-tails assumption on the noise. 1

Introduction
In this paper we focus on the following problem min x∈Rn f (x), f (x) = Eξ [f (x, ξ)] , (1) where f (x) is a smooth convex function and the mathematical expectation in (1) is taken with respect to the random variable ξ deﬁned on the probability space (X , F, P) with some σ-algebra F and probability measure P. Such problems appear in various applications of machine learning [21, 61, 64] and mathematical statistics [66]. Perhaps, the most popular method to solve problems like (1) is
Stochastic Gradient Descent (SGD) [26, 50, 51, 59, 63]. There is a lot of literature on the convergence in expectation of SGD for (strongly) convex [20, 24, 25, 46, 48, 49, 55] and non-convex [6, 20, 34] problems under different assumptions on stochastic gradient. When the problem is good enough, i.e. when the distributions of stochastic gradients are light-tailed, this theory correlates well with the real behavior of trajectories of SGD in practice. Moreover, the existing high-probability bounds for
SGD [9, 11, 49] coincide with its counterpart from the theory of convergence in expectation up to logarithmical factors depending on the conﬁdence level.
However, there are a lot of important applications where the noise distribution in the stochastic gradient is signiﬁcantly heavy-tailed [65, 71]. For such problems SGD is often less robust and shows poor performance in practice. Furthermore, existing results for the convergence with high-probability for SGD are also much worse in the presence of heavy-tailed noise than its “light-tailed counterparts”.
In this case, rates of the convergence in expectation can be insufﬁcient to describe the behavior of the method.
To illustrate this phenomenon we consider a simple example of stochastic optimization problem and apply SGD with constant stepsize to solve it. After that, we present a natural and simple way to resolve the issue of SGD based on the clipping of stochastic gradients. However, we need to introduce some important notations and deﬁnitions before we start to discuss this example.
∗eduard.gorbunov@phystech.edu, eduardgorbunov.github.io
†danilovamarina15@gmail.com, marinadanya.github.io
‡gasnikov@yandex.ru 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1.1 Preliminaries
In this section we introduce the main part of notations, assumption and deﬁnitions. The rest is classical for optimization literature and stated in the appendix (see Section A). Throughout the paper we assume that at each point x ∈ Rn function f is accessible only via stochastic gradients ∇f (x, ξ) such that
Eξ[∇f (x, ξ)] = ∇f (x), Eξ (cid:104) (cid:107)∇f (x, ξ) − ∇f (x)(cid:107)2 2 (cid:105)
≤ σ2, (2) i.e. we have an access to the unbiased estimator of ∇f (x) with uniformly bounded by σ2 variance where σ is some non-negative number. These assumptions on the stochastic gradient are standard in the stochastic optimization literature [18, 20, 31, 38, 49]. Below we introduce one of the most important deﬁnitions in this paper.
Deﬁnition 1.1 (light-tailed random vector). We say that random vector η has a light-tailed dis-tribution, i.e. satisﬁes “light-tails” assumption, if there exist E[η] and P {(cid:107)η − E[η](cid:107)2 > b} ≤ 2 exp for all b > 0 (cid:16) (cid:17)
− b2 2σ2
Such distributions are often called sub-Gaussian ones (see [30] and references therein). One can show (see Lemma 2 from [30]) that this deﬁnition is equivalent to 2/σ2(cid:1)(cid:3) ≤ exp(1)
E (cid:2)exp (cid:0)(cid:107)η−E[η](cid:107)2 (3) up to absolute constant difference in σ. Due to Jensen’s inequality and convexity of exp(·) one can easily show that inequality (3) implies E[(cid:107)η − E[η](cid:107)2 2] ≤ σ2. However, the reverse implication does not hold in general. Therefore, in the rest of the paper by stochastic gradient with heavy-tailed distribution, we mean such a stochastic gradient that satisﬁes (2) but not necessarily (3). 1.2 Simple Motivational Example: Convergence in Expectation and Clipping
In this section we consider SGD xk+1 = xk − γ∇f (xk, ξk) applied to solve the problem (1) with 2/2 + (cid:104)ξ, x(cid:105), where ξ is a random vector with zero mean and the variance by σ2 (see f (x, ξ) = (cid:107)x(cid:107)2 the details in Section H.1). The state-of-the-art theory (e.g. [24, 25]) says that convergence properties in expectation of SGD in this case depend only on the stepsize γ, condition number of f , initial suboptimality f (x0) − f (x∗) and the variance σ, but does not depend on distribution of ξ. However, the trajectory of SGD signiﬁcantly depends on the distribution of ξ. To illustrate this we consider 3 different distributions of ξ with the same σ, i.e., Gaussian distribution, Weibull distribution [69] and
Burr Type XII distribution [3, 42] with proper shifts and scales to get needed mean and variance for
ξ (see the details in Section H.1). For each distribution, we run SGD several times from the same starting point, the same stepsize γ, and the same batchsize, see typical runs in Figure 1. This simple
Figure 1: Typical trajectories of SGD and clipped-SGD applied to solve (130) with ξ having Gaussian, Weibull, and Burr Type XII tails. example shows that SGD in all 3 cases rapidly reaches a neighborhood of the solution and then starts to oscillate there. However, these oscillations are signiﬁcantly larger for the second and the third cases where stochastic gradients are heavy-tailed. Unfortunately, guarantees for the convergence in expectation cannot express this phenomenon, since in expectation the convergence guarantees for all 3 cases are identical.
Moreover, in practice, e.g., in training big machine learning models, it is often used only a couple runs of SGD or another stochastic method. The training process can take hours or even days, so, it is extremely important to obtain good accuracy of the solution with high probability. However, 2
as our simple example shows, SGD fails to converge robustly if the noise in stochastic gradients is heavy-tailed which was also noticed for several real-world problems like training AlexNet [37] on
CIFAR10 [36] (see [65]) and training an attention model [68] via BERT [8] (see [71]).
Clearly, since the distributions of stochastic gradients in the second and the third cases are heavy tailed the probability of sampling too large ξ (in terms of the norm) and, as a consequence, too large ∇f (x, ξ) is high even if we are close to the solution. Once the current point xk is not too far from the solution and SGD gets a stochastic gradient with too large norm the method jumps far from the solution. Therefore, we see large oscillations. Since the reason of such oscillations is large norm of stochastic gradient it is natural to clip it, i.e., update xk+1 according to xk+1 = xk −
γ min{1, λ/(cid:107)∇f (xk,ξk)(cid:107)2}∇f (xk, ξk). The obtained method is known in literature as clipped-SGD (see [17, 21, 43, 44, 57, 70, 71] and references therein). Among the good properties of clipped-SGD we emphasize its robustness to the heavy-tailed noise in stochastic gradients (see also [71]). In our tests, trajectories of clipped-SGD oscillate not signiﬁcantly even for heavy-tailed distributions, and clipping does not spoil the rate of convergence. These two factors make clipped-SGD preferable than SGD when we deal with heavy-tailed distributed stochastic gradients (see further discussion in
Section B.2). 1.3