Abstract
The local Lipschitz constant of a neural network is a useful metric with applications in robustness, generalization, and fairness evaluation. We provide novel analytic results relating the local Lipschitz constant of nonsmooth vector-valued functions to a maximization over the norm of the generalized Jacobian. We present a sufﬁcient condition for which backpropagation always returns an element of the generalized Jacobian, and reframe the problem over this broad class of functions.
We show strong inapproximability results for estimating Lipschitz constants of
ReLU networks, and then formulate an algorithm to compute these quantities exactly. We leverage this algorithm to evaluate the tightness of competing Lipschitz estimators and the effects of regularized training on the Lipschitz constant. 1

Introduction
We are interested in computing the Lipschitz constant of neural networks with ReLU activations.
Formally, for a network f with multiple inputs and outputs, we are interested in the quantity
||f (x) − f (y)||β
||x − y||α
. sup x(cid:54)=y (1)
We allow the norm of the numerator and denominator to be arbitrary and further consider the case where x, y are constrained in an open subset of Rn leading to the more general problem of computing the local Lipschitz constant.
Estimating or bounding the Lipschitz constant of a neural network is an important and well-studied problem. For the Wasserstein GAN formulation [1] the discriminator is required to have a bounded
Lipschitz constant, and there are several techniques to enforce this [1–3]. For supervised learning
Bartlett et al. [4] have shown that classiﬁers with lower Lipschitz constants have better generalization properties. It has also been observed that networks with smaller gradient norms are more robust to adversarial attacks. Bounding the (local) Lipschitz constant has been used widely for certiﬁable robustness against targeted adversarial attacks [5–7]. Lipschitz bounds under fair metrics may also be used as a means to certify the individual fairness of a model [8, 9].
The Lipschitz constant of a function is fundamentally related to the supremal norm of its Jacobian matrix. Previous work has demonstrated the relationship between these two quantities for functions that are scalar-valued and smooth [10, 11]. However, neural networks used for multi-class classiﬁca-tion with ReLU activations do not meet either of these assumptions. We establish an analytical result that allows us to formulate the local Lipschitz constant of a vector-valued nonsmooth function as an optimization over the generalized Jacobian. We access the generalized Jacobian by means of the chain rule. As we discuss, the chain rule may produce incorrect results [12] for nonsmooth functions, even ReLU networks. To address this problem, we present a sufﬁcient condition over the parameters of a ReLU network such that the chain rule always returns an element of the generalized Jacobian, allowing us to solve the proposed optimization problem. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Exactly computing Lipschitz constants of scalar-valued neural networks under the (cid:96)2 norm was shown to be NP-hard [13]. In this paper we establish strong inapproximability results showing that it is hard to even approximate Lipschitz constants of scalar-valued ReLU networks, for (cid:96)1 and (cid:96)∞ norms.
A variety of algorithms exist that estimate Lipschitz constants for various norms. To the best of our knowledge, none of these techniques are exact: they are either upper bounds, or heuristic estimators with no provable guarantees. In this paper we present the ﬁrst technique to provably exactly compute
Lipschitz constants of ReLU networks under the (cid:96)1, (cid:96)∞ norms. Our method is called LipMIP and relies on Mixed-Integer Program (MIP) solvers. As expected from our hardness results, our algorithm runs in exponential time in the worst case. At any intermediate time our algorithm may be stopped early to yield valid upper bounds.
We demonstrate our algorithm on various applications. We evaluate a variety of Lipschitz estimation techniques to deﬁnitively evaluate their relative error compared to the true Lipschitz constant. We apply our algorithm to yield reliable empirical insights about how changes in architecture and various regularization schemes affect the Lipschitz constants of ReLU networks.
Our contributions are as follows:
• We present novel analytic results connecting the Lipschitz constant of an arbitrary, possibly nonsmooth, function to the supremal norm of generalized Jacobians.
• We present a sufﬁcient condition for which the chain rule will always yield an element of the generalized Jacobian of a ReLU network.
• We show that that it is provably hard to approximate the Lipschitz constant of a network to within a factor that scales almost linearly with input dimension.
• We present a Mixed-Integer Programming formulation (LipMIP) that is able to exactly compute the local Lipschitz constant of a scalar-valued ReLU network over a polyhedral domain.
• We analyze the efﬁciency and accuracy of LipMIP against other Lipschitz estimators. We provide experimental data demonstrating how Lipschitz constants change under training. 2 Gradient Norms and Lipschitz Constants
First we deﬁne the problem of interest. There have been several recent papers that leverage an analytical result relating the Lipschitz constant of a function to the maximal dual norm of its gradient
[6, 10, 14]. This analytical result is limited in two aspects: namely it only applies to functions that are both scalar-valued and continuously differentiable. Neural networks with ReLU nonlinearities are nonsmooth and for multi-class classiﬁcation or unsupervised learning settings, typically not scalar-valued. To remedy these issues, we will present a theorem relating the Lipschitz constant to the supremal norm of an element of the generalized Jacobian. We stress that this analytical result holds for all Lipschitz continuous functions, though we will only be applying this result to ReLU networks in the sequel.
The quantity we are interested in computing is deﬁned as follows:
Deﬁnition 1 The local (α, β)-Lipschitz constant of a function f : Rd → Rm over an open set
X ⊆ Rd is deﬁned as the following quantity:
L(α,β)(f, X ) := sup x,y∈X
||f (y) − f (x)||β
||x − y||α (x (cid:54)= y) (2)
And if L(α,β)(f, X ) exists and is ﬁnite, we say that f is (α, β)-locally Lipschitz over X .
If f is scalar-valued, then we denote the above quantity Lα(f, X ) where || · ||β = | · | is implicit. For smooth, scalar-valued f , it is well-known that
Lα(f, X ) = sup x∈X where ||z||α∗ := sup||y||α≤1 yT z is the dual norm of || · ||α [10, 11]. We seek to extend this result to be applicable to vector-valued nonsmooth Lipschitz continuous functions. As the Jacobian is not
||∇f (x)||α∗ , (3) 2
well-deﬁned everywhere for this class of functions, we recall the deﬁnition of Clarke’s generalized
Jacobian [15]:
Deﬁnition 2 The (Clarke) generalized Jacobian of f at x, denoted δf (x), is the convex hull of the set of limits of the form lim i=1 such that ∇f (xi) is well-deﬁned and i→∞ xi → x.
∇f (xi) for any sequence (xi)∞
Informally, δf (x) may be viewed as the convex hull of the Jacobian of nearby differentiable points.
We remark that for smooth functions, δf (x) = {∇f (x)} for all x, and for convex nonsmooth functions, δf (·) is the subdifferential operator.
The following theorem relates the norms of the generalized Jacobian to the local Lipschitz constant.
Theorem 1 Let || · ||α, || · ||β be arbitrary convex norms over Rd, Rm respectively, and let f : Rd →
Rm be (α, β)-Lipschitz continuous over an open set X . Then the following equality holds:
L(α,β)(f, X ) = sup
||GT ||α,β
G∈δf (X ) (4) where δf (X ) := {G ∈ δf (x) | x ∈ X } and ||M ||α,β := sup
||M v||β.
||v||α≤1
This result relies on the fact that Lipschitz continuous functions are differentiable almost everywhere (Rademacher’s Theorem). As desired our result recovers equation 3 for scalar-valued smooth functions. Developing techniques to optimize the right-hand-side of equation 4 will be the central algorithmic focus of this paper. 3 ReLU Networks and the Chain Rule
Theorem 1 relates the Lipschitz constant to an optimization over generalized Jacobians. Typically we access the Jacobian of a function through backpropagation, which is simply an efﬁcient imple-mentation of the familiar chain rule. However the chain rule is only provably correct for functions that are compositions of continuously differentiable functions, and hence does not apply to ReLU networks [12]. In this section we will provide a sufﬁcient condition over the parameters of a ReLU network such that any standard implementation of the chain rule will always yield an element of the generalized Jacobian.
The chain rule for nonsmooth functions: To motivate the discussion, we turn our attention to neural networks with ReLU nonlinearities. We say that a function is a ReLU network if it may be written as a composition of afﬁne operators and element-wise ReLU nonlinearities, which may be encoded by the following recursion: f (x) = cT σ(Zd(x))
Zi(x) = Wiσ(Zi−1(x)) + bi
Z0(x) := x (5) where σ(·) here is the ReLU operator applied element-wise. We present the following example where the chain rule yields a result not contained in the generalized Jacobian. The univariate identity function may be written as I(x) := 2x − σ(x) + σ(−x). Certainly at every point x, δI (x) = {1}.
However as Pytorch’s automatic differentiation package deﬁnes σ(cid:48)(0) = 0, Pytorch will compute
I (cid:48)(0) as 2 [16]. Indeed, this is exactly the case where naively replacing the feasible set δf (X ) in
Equation 4 by the set of Jacobians returned by the chain rule will yield an incorrect calculation of the Lipschitz constant. To correctly relate the set of generalized Jacobians to the set of elements returnable by an implementation of the chain rule, we introduce the following deﬁnition:
Deﬁnition 3 Consider any implementation of the chain rule which may arbitrarily assign any element of the generalized gradient δσ(0) for each required partial derivative σ(cid:48)(0). We deﬁne the set-valued function ∇#f (·) as the collection of answers yielded by any such chain rule.
The subdifferential of the ReLU function at zero is the closed interval [0, 1], so the chain rule as implemented in PyTorch and Tensorﬂow will yield an element contained in ∇#f (·). Our goal will be to demonstrate that, for a broad class of ReLU networks, the feasible set in Equation 4 may be replaced by the set {G ∈ ∇#f (x) | x ∈ X }. 3
General Position ReLU Networks: Taking inspiration from hyperplane arrangements, we refer to this sufﬁcient condition as general position. Letting f : Rd → Rm be a ReLU network with n neurons, we can deﬁne the function gi(x) : Rd → R for all i ∈ [n] as the input to the ith ReLU of f at x. Then we consider the set of inputs for which each gi is identically zero: we refer to the set
Ki := {x | gi(x) = 0} as the ith ReLU kernel of f . We say that a polytope P is k-dimensional if the afﬁne hull of P has dimension exactly k. Then we deﬁne general position ReLU networks as follows:
Deﬁnition 4 We say that a ReLU network with n neurons is in general position if, for every subset of neurons S ⊆ [n], the intersection ∩i∈SKi is a ﬁnite union of (d − |S|)-dimensional polytopes.
We emphasize that this deﬁnition requires that particular ReLU kernel is a ﬁnite union of (d − 1)-dimensional polytopes, i.e. the ‘bent hyperplanes’ referred to in [17]. For a general position neural net, no (d + 1) ReLU kernels may have a nonempty intersection. We now present our theorem on the correctness of chain rule for general position ReLU networks.
Theorem 2 Let f be a general position ReLU network, then for every x in the domain of f , the set of elements returned by the generalized chain rule is exactly the generalized Jacobian:
∇#f (x) = δf (x)
In particular this theorem implies that, for general position ReLU nets,
L(α,β)(f, X ) = sup
G∈∇#f (X )
||GT ||α,β (6) (7)
We will develop algorithms to solve this optimization problem predicated upon the assumption that a ReLU network is in general position. As shown by the following theorem, almost every ReLU network satisﬁes this condition.
Theorem 3 The set of ReLU networks not in general position has Lebesgue measure zero over the parameter space. 4
Inapproximability of the Local Lipschitz Constant
In general, we seek algorithms that yield estimates of the Lipschitz constant of ReLU networks with provable guarantees. In this section we will address the complexity of Lipschitz estimation of ReLU networks. We show that under mild complexity theoretic assumptions, no deterministic polynomial time algorithm can provably return a tight estimate of the Lipschitz constant of a ReLU network
Extant work discussing the complexity of Lipschitz estimation of ReLU networks has only shown that computing L2(f, Rd) is NP-hard [13]. This does not address the question of whether efﬁcient approximation algorithms exist. We relate this problem to the problem of approximating the maximum independent set of a graph. Maximum independent set is one of the hardest problems to approximate: if G is a graph with d vertices, then assuming the Exponential Time Hypothesis1, it is hard to approximate the maximum independent set of G with an approximation ratio of Ω(d1−c) for any constant c. Our result achieves the same inapproximability result, where d here refers to the encoding size of the ReLU network, which scales at least linearly with the input dimension and number of neurons.
Theorem 4 Let f be a scalar-valued ReLU network, not necessarily in general position, taking inputs in Rd. Then assuming the exponential time hypothesis, there does not exist a polynomial-time approximation algorithm with ratio Ω(d1−c) for computing L∞(f, X ) and L1(f, X ), for any constant c > 0. 5 Computing Local Lipschitz Constants With Mixed-Integer Programs
The results of the previous section indicate that one cannot develop any polynomial-time algorithm to estimate the local Lipschitz constant of ReLU network with nontrivial provable guarantees. Driven 1This states that 3SAT cannot be solved in sub-exponential time [18]. If true, this would imply P (cid:54)= N P . 4
by this negative result, we can instead develop algorithms that exactly compute this quantity but do not run in polynomial time in the worst-case. Namely we will use a mixed-integer programming (MIP) framework to formulate the optimization problem posed in Equation 7 for general position
ReLU networks. For ease of exposition, we will consider scalar-valued ReLU networks under the (cid:96)1, (cid:96)∞ norms, thereby using MIP to exactly compute L1(f, X ) and L∞(f, X ). Our formulation may be extended to vector-valued networks and a wider variety of norms, which we will discuss in the supplementary.
While mixed-integer programming requires exponential time in the worst-case, implementations of mixed-integer programming solvers typically have runtime that is signiﬁcantly lower than the worst-case. Our algorithm is unlikely to scale to massive state-of-the-art image classiﬁers, but we nevertheless argue the value of such an algorithm in two ways. First, it is important to provide a ground-truth as a frame of reference for evaluating the relative error of alternative Lipschitz estimation techniques. Second, an algorithm that provides provable guarantees for Lipschitz estimation allows one to make accurate claims about the properties of neural networks. We empirically demonstrate each of these use-cases in the experiments section.
We state the following theorem about the correctness of our MIP formulation and will spend the remainder of the section describing the construction yielding the proof.
Theorem 5 Let f : Rd → R be a general position ReLU network and let X be an open set that is the neighborhood of a bounded polytope in Rd. Then there exists an efﬁciently-encodable mixed-integer program whose optimal objective value is Lα(f, X ), where || · ||α is either the (cid:96)1 or (cid:96)∞ norm.
Mixed-Integer Programming: Mixed-integer programming may be viewed as the extension of linear programming where some variables are constrained to be integral. The feasible sets of mixed-integer programs, may be deﬁned as follows:
Deﬁnition 5 A mixed-integer polytope is a set M ⊆ Rn × {0, 1}m that satisﬁes a set of linear inequalities:
M := {(x, a) ⊆ Rn × {0, 1}m | Ax + Ba ≤ c} (8)
Mixed-integer programming then optimizes a linear function over a mixed-integer polytope.
From equation 7, our goal is to frame ∇#f (X ) as a mixed-integer polytope. More accurately, we aim to frame {||GT ||α | G ∈ ∇#f (X )} as a mixed-integer polytope. The key idea for how we do this is encapsulated in the following example. Suppose X is some set and we wish to solve the optimization problem maxx∈X (g ◦ f )(x). Letting Y := {f (x) | x ∈ X } and Z := {g(y) | y ∈ Y}, we see that (9) z max x∈X (g ◦ f )(x) = max y∈Y g(y) = max z∈Z
Thus, if X is a mixed-integer polytope, and f is such that f (X ) is also a mixed-integer polytope and similar for g, then the optimization problem may be solved under the MIP framework.
From the example above, it sufﬁces to show that ∇#f (·) is a composition of functions fi with the property that fi maps mixed-integer polytopes to mixed-integer polytopes without blowing up in encoding-size. We formalize this notion with the following deﬁnition:
Deﬁnition 6 We say that a function g is MIP-encodable if, for every mixed-integer polytope M , the image of M mapped through g is itself a mixed-integer polytope.
As an example, we show that the afﬁne function g(x) := Dx + e is MIP-encodable, where g is applied only to the continuous variables. Consider the canonical mixed-integer polytope M deﬁned in equation 8, then g(M ) is the mixed-integer polytope over the existing variables (x, a), with the dimension lifted to include the new continuous variable y and a new equality constraint: g(M ) := {(y, a) | (Ax + Ba ≤ c) ∧ (y = Dx + e)}. (10)
To represent {||GT ||α | x ∈ ∇#f (X )} as a mixed-integer polytope, there are two steps. First we must demonstrate a set of primitive functions such that ||∇#f (x)||α may be represented as a composition of these primitives, and then we must show that each of these primitives are MIP-encodable. In this sense, the following construction allows us to ‘unroll’ backpropagation into a mixed-integer polytope. 5
MIP-encodable components of ReLU networks: We introduce the following three primitive operators and show that ||∇#f ||α may be written as a composition of these primitive operators.
These operators are the afﬁne, conditional, and switch operators, deﬁned below:
Afﬁne operators: For some ﬁxed matrix W and vector b, A : Rn → Rm is an afﬁne operator if it is of the form A(x) := W x + b.
The conditional operator C : R → P({0, 1}) is deﬁned as
C(x) =



{1}
{0}
{0, 1} if x > 0 if x < 0 if x = 0.
The switch operator S : R × {0, 1} → R is deﬁned as
S(x, a) = x · a. (11) (12)
Then we have the two following lemmas which sufﬁce to show that ∇#f (·) is a MIP-encodable function:
Lemma 1 Let f be a scalar-valued general position ReLU network. Then f (x), ∇#f (x), || · ||1, and || · ||∞ may all be written as a composition of afﬁne, conditional and switch operators.
This is easy to see for f (x) by the recurrence in Equation 5; indeed this construction is used in the
MIP-formulation for evaluating robustness of neural networks [19–24]. For ∇#f , one can deﬁne the recurrence:
∇#f (x) = W T (13) where Λi(x) is the conditional operator applied to the input to the ith layer of f . Since Λi(x) takes values in {0, 1}∗, Diag(Λ(x))Yi+1(x) is equivalent to S(Yi+1(x), Λi(x)). i+1Diag(Λi(x))Yi+1(x) 1 Y1(x)
Yd+1(x) = c
Yi(x) = W T
Lemma 2 Let g be a composition of afﬁne, conditional and switch operators, where global lower and upper bounds are known for each input to each element of the composition. Then g is a
MIP-encodable function.
As we have seen, afﬁne operators are trivially MIP-encodable. For the conditional and switch operators, global lower and upper bounds are necessary for MIP-encodability. Provided that our original set X is bounded, there exist several efﬁcient schemes for propagating upper and lower bounds globally. Conditional and switch operators may be incorporated into the composition by adding only a constant number of new linear inequalities for each new variable. These constructions are described in full detail in the supplementary.
Formulating LipMIP: To put all the above components together, we summarize our algorithm.
Provided a bounded polytope P, we ﬁrst compute global lower and upper bounds to each conditional and switch operator in the composition that deﬁnes ||∇#f (·)||α by propagating the bounds of P. We then iteratively move components of the composition into the feasible set as in Equation 9 by lifting the dimension of the feasible set and incorporating new constraints and variables. This yields a valid mixed-integer program which can be optimized by off-the-shelf solvers to yield Lα(f, X ) for either the (cid:96)1 or (cid:96)∞ norms.
Extensions: While our results focus on evaluating the (cid:96)1 and (cid:96)∞ Lipschitz constants of scalar-valued
ReLU networks, we note that the above formulation is easily extensible to vector-valued networks over a variety of norms. We present this formulation, including an application to untargeted robustness veriﬁcation through the use of a novel norm in the supplementary. We also note that any convex relaxation of our formulation will yield a provable upper bound to the local Lipschitz constant.
Mixed-integer programming formulations have natural linear programming relaxations, by relaxing each integral constraint to a continuous constraint. We denote this linear programming relaxation as
LipLP. Most off-the-shelf MIP solvers may also be stopped early, yielding valid upper bounds for the
Lipschitz constant. 6