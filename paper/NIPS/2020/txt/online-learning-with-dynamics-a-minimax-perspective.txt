Abstract
We study the problem of online learning with dynamics, where a learner interacts with a stateful environment over multiple rounds. In each round of the interaction, the learner selects a policy to deploy and incurs a cost that depends on both the chosen policy and current state of the world. The state-evolution dynamics and the costs are allowed to be time-varying, in a possibly adversarial way. In this setting, we study the problem of minimizing policy regret and provide non-constructive upper bounds on the minimax rate for the problem.
Our main results provide sufﬁcient conditions for online learnability for this setup with corresponding rates. The rates are characterized by: 1) a complexity term capturing the expressiveness of the underlying policy class under the dynamics of state change, and 2) a dynamic stability term measuring the deviation of the instantaneous loss from a certain counterfactual loss. Further, we provide matching lower bounds which show that both the complexity terms are indeed necessary.
Our approach provides a unifying analysis that recovers regret bounds for several well studied problems including online learning with memory, online control of linear quadratic regulators, online Markov decision processes, and tracking adversarial targets. In addition, we show how our tools help obtain tight regret bounds for a new problems (with non-linear dynamics and non-convex losses) for which such bounds were not known prior to our work. 1

Introduction
Machine learning systems deployed in the real-world interact with people through their decision making. Such systems form a feedback loop with their environment: they learn to make decisions from real-world data and decisions made by these systems in turn affect the data that is collected. In addition, people often learn to adapt to such automated decision makers in an attempt to maximize their own utility rendering any assumption on the data generation process futile. Motivated by these aspects of decision making, we propose the problem of online learning with dynamics which involves repeated interaction between a learner and an environment with an underlying state. The decisions made by the learner affect this state of the environment which evolves as a dynamical system. Further, we place no distributional assumptions on the learning data and allow this to be adversarial.
Given such a setup, a natural question to ask is how does one measure the performance of the learner?
Classical online learning studies one such notion of performance known as regret. This measure compares the performance of the learner to that of a ﬁxed best policy in hindsight, when evaluated on the same states which were observed by the learner. Such a measure of performance clearly does not work for the above setup: if we would have deployed a different policy, we would have observed different states of the environment. To overcome this, we study a counterfactual notion of regret, called Policy Regret, where the comparator term is the performance of a policy on the states one would have observed if this policy was deployed from the beginning of time. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Such a notion of regret has been studied in the online learning literature for understanding memory based adversaries [20, 4, 5] and more recently, for the study of speciﬁc reinforcement learning models [11, 1, 10]. However, a vast majority of these works have focused on known and ﬁxed models of state evolution, often restricting the scope to linear dynamical systems. Further, these works have focused on simplistic policy classes as the comparators in their notion of policy regret. Contrast this with the vast literature on statistical learning [28, 6] and classical online learning [22] which study the question of learnability in full generality; for arbitrary losses and general function classes.
Our work is a step towards addressing this gap. We study the problem of learnability for a class of online learning problems with underlying states evolving as a dynamical system in its full gen-erality. Our main results provide sufﬁcient conditions (along with non-asymptotic upper bounds) on when such problems are learnable, that is, can have vanishing policy regret. Our approach is non-constructive and provides a complexity term that provides upper bounds on the minimax rates for these problems. Further, we provide lower bounds showing that for a large class of problems, our upper bounds are tight up to constant factors. By studying the problem in full generality, we show how several well-studied problems in the literature comprising online Markov decision processes [11], online adversarial tracking [1], online linear quadratic regulator [10], online control with adversarial noise [2], and online learning with memory [5, 4] can be seen as speciﬁc examples of our general framework. We recover the best known rates for a majority of these problems, often times even generalizing these setups. We also provide examples where, to the best of our knowledge, previous techniques are not able to obtain useful bounds on regret; however using our minimax tools, we are able to provide tight bounds on the policy regret for these examples.
Formally, we consider the setup where X denotes an arbitrary set of states, Π an arbitrary class of policies and Z an arbitrary instance space. Given this, the interaction between the learner and nature can be expressed as a T round protocol where on each round t ∈ [T ], the learner picks a policy πt ∈ Π, the adversary simultaneously picks instance (zt, ζt) ∈ Z. The learner suffers loss (cid:96)(πt, xt, zt) and the state of the system evolves1 as xt+1 ← Φ(xt, πt, ζt), where Φ is known to the learner. The goal of the learner is to minimize policy regret
Regpol
T =
T
∑ t=1 (cid:96)(πt, xt, zt) − inf
π∈Π
T
∑ t=1 (cid:96)(π, xt[π (t−1)
, ζ1∶t−1], zt) , where xt are the states of the system based on learners choices of policies and xt[π(t−1), ζ1∶t−1] represents the state of the system at time t if the policy π was used the previous t − 1 rounds. We refer to the loss (cid:96)(π, xt[π(t−1), ζ1∶t−1], zt) as the counterfactual loss of policy π. Notice that dynamics Φ being ﬁxed or known in advance to the learner is not really restrictive since an adversary can encode arbitrary state dynamics mapping in ζt’s and Φ can just be seen as an applicator of these mapping.
Our contributions. We are interested in the following question: for a given problem instance (Π, Z, Φ, (cid:96)), is the problem learnable, that is, does there exists a learning algorithm such that policy regret is such that Regpol
T = o(T ). Below we highlight some of the key contributions of this paper. 1. We show that the minimax policy regret for any problem speciﬁed by (Π, Z, Φ, (cid:96)) can be upper bounded by sum of two terms: i) a sequential Rademacher complexity like term for the class of counterfactual losses of the policy class, and ii) a term we refer to as dynamic stability term for the Empirical Risk Minimizer (ERM) (or regularized ERM) algorithm. 2. We analyze the problem in the dual game. While in most cases ERM does not even have low classical regret let alone policy regret, we show that ERM like strategy in the dual game can lead to the two term decomposition of minimax policy regret we mention above. 3. Ours is the ﬁrst work that studies arbitrary online dynamical systems, and provides an analysis for general policy classes and loss functions (possibly non-convex). 4. We provide lower bounds that show that our sufﬁcient conditions are tight for a large class of problem instances showing that both the terms in our upper bounds are indeed necessary. 5. We delineate a number of previously studied problems including online linear quadratic regulator, and online learning with memory for which we recover rates. More importantly, we provide examples of new non-convex and general online learning with dynamics problems and obtain tight regret bounds. For these examples, none of the previous methods are able to obtain any non-degenerate regret bounds. 1while we consider deterministic dynamics here, Section 2 considers general dynamics with stochastic noise 2