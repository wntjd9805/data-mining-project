Abstract
One of the main strengths of online algorithms is their ability to adapt to arbitrary data sequences. This is especially important in nonparametric settings, where performance is measured against rich classes of comparator functions that are able to ﬁt complex environments. Although such hard comparators and complex environments may exhibit local regularities, efﬁcient algorithms, which can prov-ably take advantage of these local patterns, are hardly known. We ﬁll this gap by introducing efﬁcient online algorithms (based on a single versatile master algo-rithm) each adapting to one of the following regularities: (i) local Lipschitzness of the competitor function, (ii) local metric dimension of the instance sequence, (iii) local performance of the predictor across different regions of the instance space. Extending previous approaches, we design algorithms that dynamically grow hierarchical ε-nets on the instance space whose prunings correspond to dif-ferent “locality proﬁles” for the problem at hand. Using a technique based on tree experts, we simultaneously and efﬁciently compete against all such prunings, and prove regret bounds each scaling with a quantity associated with a different type of local regularity. When competing against “simple” locality proﬁles, our technique delivers regret bounds that are signiﬁcantly better than those proven using the previous approach. On the other hand, the time dependence of our bounds is not worse than that obtained by ignoring any local regularities. 1

Introduction
In online convex optimization [34, 10], a learner interacts with an unknown environment in a sequence of rounds. In the speciﬁc setting considered in this paper, at each round t = 1, 2, . . . the learner observes an instance xt ∈ X ⊂ Rd and outputs a prediction (cid:98)yt for the label yt ∈ Y associated with the instance. After predicting, the learner incurs the loss (cid:96)t((cid:98)yt). We consider two basic learning problems: 2 (yt − (cid:98)yt)2, and binary classiﬁcation regression with square loss, where Y ≡ [0, 1] and (cid:96)t((cid:98)yt) = 1 with absolute loss, where Y ≡ {0, 1} and (cid:96)t((cid:98)yt) = |yt − (cid:98)yt| (or, equivalently, (cid:96)t((cid:98)yt) = P(yt (cid:54)= Yt) for randomized predictions Yt with P(Yt = 1) = (cid:98)yt). The performance of a learner is measured through the notion of regret, which is deﬁned as the amount by which the cumulative loss of the learner predicting with (cid:98)y1, (cid:98)y2, . . . exceeds the cumulative loss —on the same sequence of instances and labels— of any function f in a given reference class of functions F. Formally,
RT (f ) =
T (cid:88) (cid:16) t=1 (cid:96)t((cid:98)yt) − (cid:96)t (cid:0)f (xt)(cid:1)(cid:17)
∀f ∈ F . (1)
In order to capture complex environments, we focus on nonparametric classes F of Lipschitz functions f : X → Y. The speciﬁc approach adopted in this paper is inspired by the simple and versatile
∗Work partly done while at the University of Milan, Italy. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
algorithm from [11], henceforth denoted with HM, achieving a regret bound of the form 2 (cid:40)
RT (f ) O= (ln T )(cid:0)L T (cid:1) d d d+2 T d+1 d+2
L d+1 (square loss) (absolute loss)
∀f ∈ FL for any given L > 0. Here FL is the class of L-Lipschitz functions f : X → Y such that (cid:12)f (x) − f (x(cid:48))(cid:12) (cid:12) (cid:12) ≤ L (cid:107)x − x(cid:48)(cid:107) (2) (3) for all x, x(cid:48) ∈ X , where X , Y are compact.3 Although Lipschitzness is a standard assumption in nonparametric learning, a function in FL may alternate regions of low variation with regions of high variation. This implies that, if computed locally (i.e., on pairs x, x(cid:48) that belong to the same small region), the value of the smallest L satisfying (3) would change signiﬁcantly across these regions.
If we knew in advance the local Lipschitzness proﬁle, we could design algorithms that exploit this information to gain a better control on regret.
Although, for d ≥ 2, asymptotic rates T (d−1)/d improving on (2) can be obtained using different and more complicated algorithms [4], it is not clear whether these other algorithms can be made locally adaptive in a principled way as we do with HM.
Local Lipschitzness. Our ﬁrst contribution is an algorithm for regression with square loss that competes against all functions in FL. However, unlike the regret bound (2) achieved by HM, the regret RT (f ) of our algorithm depends in a detailed way on the local Lipschitzness proﬁle of f . Our algorithm operates by sequentially constructing a D-level hierarchical ε-net T of the instance space
X with balls whose radius ε decreases with each level of the hierarchy. The D levels are associated with local Lipschitz constants L1 < L2 < · · · < LD = L, all provided as an input parameter to the algorithm.
Figure 1: Matching functions to prunings. Proﬁles of local smoothness correspond to prunings so that smoother functions are matched to smaller prunings.
If we view the hierarchical net as a D-level tree whose nodes are the balls in the net at each level, then the local Lipschitzness proﬁle of a function f translates into a pruning of this tree (this is visually explained in Figure 1). By training a local predictor in each ball, we can use the leaves of a pruning
E to approximate a function whose local Lipschitz proﬁle “matches” E. Namely, a function that satisﬁes (3) with L = Lk for all observed instances x, x(cid:48) that belong to some leaf of E at level k, for all levels k (since E is a pruning of the hierarchical net T , there is a one-to-one mapping between instances xt and leaves of E). Because our algorithm is simultaneously competitive against all prunings, it is also competitive against all functions whose local Lipschitz proﬁle —with respect to the instance sequence— is matched by some pruning. More speciﬁcally, we prove that for any f ∈ FL and for any pruning E matching f on the sequence x1, . . . , xT of instances,
RT (f ) (cid:101)O= E (cid:105) (cid:104) d d+1
K
L d d+1 +
T
D (cid:88) k=1 (Lk TE,k) d d+1 (4) 2We use f O= g to denote f = O(g) and f (cid:101)O= g to denote f = (cid:101)O(g). 3The bound for the square loss, which is not contained in [11], can be proven with a straightforward extension of the analysis in that paper. 2
where, from now on, TE,k always denotes the total number of time steps t in which the current instance xt belongs to a leaf at level k of the pruning E. The expectation is with respect to the random variable K that takes value k with probability equal to the fraction of leaves of E at level k.
The ﬁrst term in the right-hand side of (4) bounds the estimation error, and is large when most of the leaves of E reside at deep levels (i.e., f has just a few regions of low variation). The second term bounds the approximation error, and is large whenever most of the instances xt belongs to leaves of
E at deep levels.
In order to compare this bound to (2), consider
Lk = 2k with L = LD = 2D. If f is matched by some pruning E such that most instances xt belong to shallow leaves of E, then our bound on RT (f ) becomes of order T d/(d+1), as opposed to the bound of (2) which is of or-der (2DT )d/(d+1). On the other hand, for any f ∈ FL we have at least a pruning matching the function: the one whose leaves are all at the deepest level of tree. In this case, our bound on
RT (f ) becomes of order (2DT )d/(d+1), which is asymptotically equivalent to (2). This shows that, up to log factors, our bound is never worse than (2), and can be much better in certain cases.
Figure 2 shows this empirically in a toy one-dimensional case.
Our locally adaptive approach can be general-ized beyond Lipschitzness. Next, we present two additional contributions where we show that variants of our algorithm can be made adaptive with respect to different local properties of the problem.
Figure 2: The ﬁrst row shows two target functions with different Lipschitz proﬁles. The second row shows the best pruning found by our algorithm, expressed using the depth of the largest weights along each tree-path. The last row show the re-gret of our algorithm (LA) compared to that of HM, which is given the true Lipschitz constant.
Local dimension.
It is well known that non-parametric regret bounds inevitably depend ex-ponentially on the metric dimension of the set of data points [11, 27]. Similarly to local Lips-chitzness, we want to take advantage of cases in which most of the data points live on manifolds that locally have a low metric dimension. In order to achieve a dependence on the “local dimension proﬁle” in the regret bound, we propose a slight modiﬁcation of our algorithm, where each level k of the hierarchical ε-net is associated with a local dimension bound dk such that d = d1 > · · · > dD.
Note that —unlike local Lipschitzness— the local dimension is decreasing as the tree gets deeper.
This happens because higher-dimensional balls occupy a larger volume than lower-dimensional ones with the same radius, and so they occur at shallower levels of the tree.
We say that a pruning of the tree associated with the hierarchical ε-net matches a sequence x1, . . . , xT of instances if the number of leaves of the pruning at each level k is O(cid:0)(L T )dk/(1+dk)(cid:1). For regression with square loss we can prove that, for any f ∈ FL and for any pruning E matching x1, . . . , xT , this modiﬁed algorithm achieves regret
RT (f ) (cid:101)O= E (cid:104) (L T ) (cid:105) dK 1+dK
+
D (cid:88) (L TE,k) dk 1+dk k=1 (5) where, as before, the expectation is with respect to the random variable K that takes value k with probability equal to the fraction of leaves of E at level k. If most xt lie in a low-dimensional manifold of X , so that x1, . . . , xT is matched by some pruning E with deeper leaves, we obtain a regret of order (L T )dD/(1+dD). This is nearly a parametric rate whenever dD (cid:28) d. In the worst case, when all instances are concentrated at the top level of the tree, we still recover (2).
Local loss bounds. Whereas the local Lipschitz proﬁle measures a property of a function with respect to an instance sequence, and the local dimension proﬁle measures a property of the instance 3
sequence, we now consider the local loss proﬁle, which measures a property of a base online learner with respect to a sequence of examples (xt, yt). The local loss proﬁle describes how the cumulative losses of the local learners at each node (which are instances of the base online learner) change across different regions of the instance space. To this end, we introduce the functions τk, which upper bound the total loss incurred by the local learners at level k. We can use the local learners on the leaves of a pruning E to predict a sequence of examples whose local loss proﬁle matches that of E. By matching we mean that the local learners run on the subsequence of examples (xt, yt) belonging to leaves at level k of E incur a total loss bounded by τk(TE,k), for all levels k. In order to take advantage of good local loss proﬁles, we focus on losses —such as the absolute loss— for which we can prove
“ﬁrst-order” regret bounds that scale with the loss of the expert against which the regret is measured.
For the absolute loss, the algorithm we consider attains regret (cid:118) (cid:117) (cid:117) (cid:116)E
RT (f ) O= E (L τk(TE,k)) (L τK(T )) (L τK(T ))
τk(TE,k) d+1 d+2 + (cid:105) D (cid:88)
D (cid:88) d d+2 d d+2 (6)
+ (cid:105) (cid:104) (cid:104) k=1 k=1 1 for any f ∈ FL, where —as before— the expectation is with respect to the random variable K that takes value k with probability equal to the fraction of leaves of E at level k. For concreteness, set
D−k+1 , so that deeper levels k correspond to loss rates that grow faster with time. When E
τk(n) = n has shallow leaves and TE,k is negligible for k > 1, the regret becomes of order (L T 1 d+2 , which d d+2 achieved by HM. Note that we always have has signiﬁcantly better dependence on T than L d+2 T a pruning matching all sequences: the one whose leaves are all at the deepest level of the tree. Indeed,
τD(n) = n is a trivial upper bound on the absolute loss of any online local learner. In this case, our bound on RT (f ) becomes of order (L T ) d+2 , which is asymptotically equivalent in T compared to (2). Note that our dependence on the Lipschitz constant is slightly worse than (2). This happens because we have to pay an extra constant term for the regret in each ball, which is unavoidable in any
ﬁrst-order regret bound.
D ) d+1 d+1 d+1
Intuition about the proof. HM greedily constructs a net on the instance space, where each node hosts a local online learner and the label for a new instance is predicted by the learner in the nearest node. Balls shrinking at polynomial rate are centered on each node, and a new node is created at an instance whenever that instance falls outside the union of all current balls. The algorithms we present here generalize this approach to a hierarchical construction of ε-nets at multiple levels. Each ball at a given level contains a lower-level ε-net using balls of smaller radius, and we view this nested structure of nets as a tree. Radii are now tuned not only with respect to time, but also with respect to the level k, where the dependence on k is characterized by the speciﬁc locality setting (i.e., local smoothness, local dimension, or local losses). The main novelty of our proof is the fact that we analyze HM in a level-wise manner, while simultaneously competing against the best pruning over the entire hierarchy. Our approach is adaptive because the regret now depends on both the number of leaves of the best pruning and on the number of observations made by the pruning at each level. In other words, if the best pruning has no leaves at a particular level, or is active for just a few time steps at that level, then the algorithm will seldom use the local learners hosted at that level.
Our main algorithmic technology is the sleeping experts framework from [8], where the local learner at each node is viewed as an expert, and active (non-sleeping) experts at a given time step are those along the root-to-leaf path associated with the current instance. For regression with square loss, we use exponential weights (up to re-normalization due to active experts). For classiﬁcation with absolute loss, we avoid the tuning problem by resorting to a parameter-free algorithm (speciﬁcally, we use AdaNormalHedge of [19] although other approaches could work as well). This makes our approach computationally efﬁcient: despite the exponential number of experts in the comparison class, we only pay in the regret a factor corresponding to the depth of the tree.
All omitted proofs can be found in the supplementary material. 2 Deﬁnitions
Throughout the paper, we assume instances xt have a bounded arbitrary norm, (cid:107)xt(cid:107) ≤ 1, so that X is the unit ball with center in 0. We use B(z, r) to denote the ball of center z ∈ Rd and radius r > 0, and we write B(r) instead of B(0, r). 4
Deﬁnition 1 (Coverings and packings). An ε-cover of a set X0 ⊆ X is a subset {x(cid:48) such that for each x ∈ X0 there exists i ∈ {1, . . . , n} such that (cid:107)x − x(cid:48)
X0 ⊆ X is a subset {x(cid:48) i − x(cid:48)
An ε-net of a set X0 ⊆ X is any set of points in X0 which is both an ε-cover and an ε-packing.
Deﬁnition 2 (Metric dimension). A set X has metric dimension d if there exists C > 0 such that, for all ε > 0, X has an ε-cover of size at most C ε−d. n} ⊂ X0 i(cid:107) ≤ ε. An ε-packing of a set j(cid:107) > ε. m} ⊂ X0 such that for any distinct i, j ∈ {1, . . . , m}, (cid:107)x(cid:48) 1, . . . , x(cid:48) 1, . . . , x(cid:48)
We consider the following online learning protocol with oblivious adversary. Given an unknown sequence (x1, y1), (x2, y2), . . . ∈ X × Y of instances and labels, for every round t = 1, 2, . . . 1. The environment reveals the instance xt ∈ X . 2. The learner selects an action (cid:98)yt ∈ Y and incurs the loss (cid:96)(cid:0) 3. The learner observes yt. (cid:98)yt, yt).
In the rest of the paper, we use (cid:96)t((cid:98)yt) as an abbreviation for (cid:96)(cid:0)
Hierarchical nets, trees, and prunings. A pruning of a rooted tree is the tree obtained after the application of zero or more replace operations, where each replace operation deletes the subtree rooted at an internal node without deleting the node itself (which becomes a leaf). (cid:98)yt, yt).
Recall that our algorithms work by sequentially building a hierarchical net of the instance sequence.
This tree-like structure is deﬁned as follows.
Deﬁnition 3 (Hierarchical net). A hierarchical net of depth D of an instance sequence σT = (x1, . . . , xT ) is a sequence of nonempty subsets4 S1 ⊂ · · · ⊂ SD ⊆ {1, . . . , T } and radii ε1 >
· · · > εD > 0 satisfying the following property: For each level k = 1, . . . , D, the set Sk is a εk-net
. of the elements of σT with balls {B(xs, εk)}s∈Sk
Any such hierarchical net can be viewed as a rooted tree T (conventionally, the root of the tree is the unit ball X , i.e., S0 = {0} , x0 = 0 and ε0 = 1) deﬁned by the parent function, where xs = PARENT(xt), if xt ∈ B(xs, εk) for s ∈ Sk (if there are more s such that xt ∈ B(xs, εk), then take the smallest one), while t ∈ Sk+1 and k = 0, 1, . . . , D − 1. Given an instance sequence σT , let
TD(σT ) be the family of all trees T of depth D generated from σT by choosing the εk-nets at each level in all possible ways given a ﬁxed sequence {εk}D
Given T and a pruning E of T , we use LEAVESk(T , E) to denote the subset of Sk containing the nodes of T that correspond to leaves of E. When T is clear from the context, we abbreviate
LEAVESk(T , E) with Ek. For any ﬁxed T ∈ TD(σT ) let also |E| = |E1| + · · · + |ED| be the number of leaves in E. k=1. 3