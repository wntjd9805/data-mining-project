Abstract
It is known that the current graph neural networks (GNNs) are difﬁcult to make themselves deep due to the problem known as over-smoothing. Multi-scale GNNs are a promising approach for mitigating the over-smoothing problem. However, there is little explanation of why it works empirically from the viewpoint of learning theory. In this study, we derive the optimization and generalization guarantees of transductive learning algorithms that include multi-scale GNNs. Using the boosting theory, we prove the convergence of the training error under weak learning-type conditions. By combining it with generalization gap bounds in terms of transductive
Rademacher complexity, we show that a test error bound of a speciﬁc type of multi-scale GNNs that decreases corresponding to the number of node aggregations under some conditions. Our results offer theoretical explanations for the effectiveness of the multi-scale structure against the over-smoothing problem. We apply boosting algorithms to the training of multi-scale GNNs for real-world node prediction tasks. We conﬁrm that its performance is comparable to existing GNNs, and the practical behaviors are consistent with theoretical observations. Code is available at https://github.com/delta2323/GB-GNN. 1

Introduction
Graph neural networks (GNNs) [27, 52] are an emerging deep learning model for analyzing graph structured-data. They have achieved state-of-the-art performances in node prediction tasks on a graph in various ﬁelds such as biochemistry [17], computer vision [69], and knowledge graph analysis [57].
While they are promising, the current design of GNNs has witnessed a challenge known as over-smoothing [38, 48]. Typically, a GNN iteratively aggregates and mixes node representations of a graph [26, 36, 64]. Although it can capture the subgraph information using local operations only, it smoothens the representations and makes them become indistinguishable (over-smoothen) between nodes as we stack too many layers, leading to underﬁtting of the model. Several studies suspected that this is the cause of the performance degradation of deep GNNs and devised methods to mitigate it [51, 70]. Among others, multi-scale GNNs [39, 45, 68] are a promising approach as a solution for the over-smoothing problem. These models are designed to combine the subgraph information at various scales, for example, by bypassing the output of the middle layers of a GNN to the ﬁnal layer.
Although multi-scale GNNs empirically have resolved the over-smoothing problem to some extent, little is known how it works theoretically. To justify the empirical performance from the viewpoint of statistical learning theory, we need to analyze two factors: generalization gap and optimization.
There are several studies to guarantee the generalization gaps [15, 24, 33, 53, 65]. However, to the best of our knowledge, few studies have provided optimization guarantees. The difﬁculty partly 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
originates owing to the inter-dependency of predictions. That is, the prediction for a node depends on the neighboring nodes, as well as its feature vector. It prevents us from extending the optimization theory for inductive learning settings to transductive ones.
In this study, we propose the analysis of multi-scale GNNs through the lens of the boosting theory [31, 46]. Our idea is to separate a model into two types of functions – aggregation functions G that mix the representations of nodes and transformation functions B, typically common to all nodes, that convert the representations to predictions. Accordingly, we can interpret a multi-scale GNN as an ensemble of supervised models and incorporate analysis tools of inductive settings. We ﬁrst consider our model in full generality and prove that as long as the model satisﬁes the weak learning condition (w.l.c.), which is a standard type of assumption in the boosting theory, it converges to the global optimum.
By combining it with the evaluation of the transductive version of Rademacher complexity [19], we give a sufﬁcient condition under which a particular type of multi-scale GNNs has the upper bound of test errors that decreases with respect to depth (the number of node aggregation operations) under the w.l.c. This is in contrast to usual GNNs suffering from the over-smoothing problem. Finally, we apply multi-scale GNNs trained with boosting algorithms, termed Gradient Boosting Graph
Neural Network (GB-GNN), to node prediction tasks on standard benchmark datasets. We conﬁrm that our algorithm can perform favorably compared with state-of-the-art GNNs, and our theoretical observations are consistent with the practical behaviors.
The contributions of this study can be summarized as follows:
• We propose the analysis of transductive learning models via the boosting theory and derive the optimization and generalization guarantees under the w.l.c. (Theorem 1, Proposition 2).
• As a special case, we give the test error bound of a particular type of multi-scale GNNs that monotonically decreases with respect to the number of node aggregations (Theorem 2).
• We apply GB-GNNs, GNNs trained with boosting algorithms, to node prediction tasks on real-world datasets. We conﬁrm that GB-GNNs perform favorably compared with state-of-the-art GNNs, and theoretical observations are consistent with the empirical behaviors. p := (cid:80)N
Notation N+ denotes the set of non-negative integers. For N ∈ N+, we deﬁne [N ] := {1, . . . , N }.
For a proposition P , 1{P } equals 1 when P is true and 0 otherwise. For a, b ∈ R, we denote a ∧ b := min(a, b) and a ∨ b := max(a, b). For a vector u, v ∈ RN and p ≥ 1, we denote the p-norm of v by (cid:107)v(cid:107)p n and the Kronecker product by u ⊗ v. All vectors are column vectors. For a matrix X, Y ∈ RN ×C and p ≥ 1, we deﬁne the inner product by (cid:104)X, Y (cid:105) := (cid:80)N c=1 XncYnc, (2, p)-norm of X by (cid:107)X(cid:107)p n=1 X 2
, the Frobenius norm by (cid:107)X(cid:107)F := (cid:107)X(cid:107)2,2, nc and the operator norm by (cid:107)X(cid:107)op := supv∈RC ,(cid:107)v(cid:107)2=1 (cid:107)Xv(cid:107)2. For y ∈ {0, 1}, we write y(cid:93) := 2y − 1 ∈ {±1}. For a ∈ R, we deﬁne sign(a) = 1 if a ≥ 0 and −1 otherwise. 2,p := (cid:80)C n=1 vp (cid:16)(cid:80)N (cid:80)C (cid:17) p n=1 c=1 2 2