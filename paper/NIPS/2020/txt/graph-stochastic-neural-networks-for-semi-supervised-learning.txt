Abstract
Graph Neural Networks (GNNs) have achieved remarkable performance in the task of the semi-supervised node classiﬁcation. However, most existing models learn a deterministic classiﬁcation function, which lack sufﬁcient ﬂexibility to explore better choices in the presence of kinds of imperfect observed data such as the scarce labeled nodes and noisy graph structure. To improve the rigidness and inﬂexibility of deterministic classiﬁcation functions, this paper proposes a novel framework named Graph Stochastic Neural Networks (GSNN), which aims to model the uncertainty of the classiﬁcation function by simultaneously learning a family of functions, i.e., a stochastic function. Speciﬁcally, we introduce a learnable graph neural network coupled with a high-dimensional latent variable to model the distribution of the classiﬁcation function, and further adopt the amortised variational inference to approximate the intractable joint posterior for missing labels and the latent variable. By maximizing the lower-bound of the likelihood for observed node labels, the instantiated models can be trained in an end-to-end manner effectively. Extensive experiments on three real-world datasets show that
GSNN achieves substantial performance gain in different scenarios compared with state-of-the-art baselines. 1

Introduction
Graphs are essential tools to represent complex relationships among entities in various domains, such as social networks, citation networks, biological networks and physical networks. Analyzing graph data has become one of the most important topics in the machine learning community. As an abstraction of many graph mining tasks, semi-supervised node classiﬁcation, which aims to predict the labels of unlabeled nodes given the graph structure, node features and labels of partial nodes, has received signiﬁcant attention in recent years. Graph Neural Networks (GNNs), in particular, have achieved impressive performance in the graph-based semi-supervised learning task [1, 2, 3, 4, 5].
Most existing GNN models are designed to learn a deterministic classiﬁcation function. This kind of design makes them look simple and artistic, but the other side of the coin is that the deterministic classiﬁcation function makes these GNN models lack sufﬁcient ﬂexibility to cater for kinds of imperfect observed data. For example, in many real situations, the ground-truth labels of nodes
∗Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
are often expensive or difﬁcult to obtain, which leads to the sparseness of the labeled nodes. The insufﬁcient supervision information can easily lead to the overﬁtting of the deterministic classiﬁcation functions, especially when there are no additional labeled nodes as the validation set for early-stopping. Another example is the noise in the graph structure. Arisen in nature or injected deliberately by attackers, noise is prone to affect the neighbor information aggregation and misleads the learning of deterministic classiﬁcation functions. The rigidness and inﬂexibility of deterministic classiﬁcation functions make it difﬁcult for them to bypass these similar issues and explore better choices.
In line of the aforementioned observations, this paper proposes a novel Graph Stochastic Neural
Network (GSNN for short) to model the uncertainty of GNN classiﬁcation functions. GSNN aims to learn simultaneously a family of classiﬁcation functions rather than ﬁtting a deterministic function.
This empowers GNNs the ﬂexibility to handle the imperfection or noise in graph data, and further bypass the traps caused by sparse labeled data and unreliable graph structure in real applications.
Speciﬁcally, we treat the classiﬁcation function to be learned as a stochastic function and integrate it into the process of label inference. To model the distribution of the stochastic function, we introduce a learnable neural network, which is coupled with a high-dimensional latent variable and takes the message-passing form. To infer the missing labels, we need to obtain the joint posterior distribution of labels for unlabeled nodes and the classiﬁcation function, of which the exact form is intractable in general. To solve the problem, we adopt the amortised variational inference [6, 7] to approximate the intractable posterior distribution with the other two types of neural networks. By maximizing the lower-bound of the likelihood for observed node labels, we could optimize all parameters effectively in an end-to-end manner. We conduct extensive experiments on three real-world datasets. The results show that compared with state-of-the-art baselines, GSNN not only achieves comparable or better performance in the standard experimental scenario with early-stopping, but also shows substantial performance gain when labeled nodes are scarce (no early-stopping) and there are deliberate edge perturbations in the graph structure. 2