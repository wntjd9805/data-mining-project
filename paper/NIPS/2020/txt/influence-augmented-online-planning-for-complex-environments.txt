Abstract
How can we plan efﬁciently in real time to control an agent in a complex environ-ment that may involve many other agents? While existing sample-based planners have enjoyed empirical success in large POMDPs, their performance heavily relies on a fast simulator. However, real-world scenarios are complex in nature and their simulators are often computationally demanding, which severely limits the performance of online planners. In this work, we propose inﬂuence-augmented online planning, a principled method to transform a factored simulator of the entire environment into a local simulator that samples only the state variables that are most relevant to the observation and reward of the planning agent and captures the incoming inﬂuence from the rest of the environment using machine learning methods. Our main experimental results show that planning on this less accurate but much faster local simulator with POMCP leads to higher real-time planning performance than planning on the simulator that models the entire environment. 1

Introduction
We consider the online planning setting where we control an agent in a complex environment that is partially observable and may involve many other agents. When the policies of other agents are known, the entire environment can be modeled as a Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998), and traditional online planning approaches can be applied. While sample-based planners like POMCP (Silver and Veness, 2010) have been shown effective for large
POMDPs, their performance relies heavily on a fast simulator to perform a vast number of Monte
Carlo simulations in a step. However, many real-world scenarios are complex in nature, making simulators that capture the dynamics of the entire environment extremely computationally demanding and hence preventing existing planners from being useful in practice. Towards effective planning in realistic scenarios, this work is motivated by the question: can we signiﬁcantly speed up a simulator by replacing the part of the environment that is less important with an approximate learned model?
We build on the multi-agent decision making literature that tries to identify compact representations of complex environments for an agent to make optimal decisions (Becker et al., 2003, 2004; Petrik and Zilberstein, 2009; Witwicki and Durfee, 2010). These methods exploit the fact that in many structured domains, only a small set of (state) variables, which we call local (state) factors, of the environment directly affects the observation and reward of the agent. The rest of the environment can only impact the agent indirectly through their inﬂuence on the local factors. For example, Figure 1a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b)
Figure 1: Left: Controlling a single agent in the Grab A Chair game with 4 other agents. Right:
Dynamic Bayesian Network for the inﬂuence-augmented local model. shows a game called Grab A Chair, in which there are N agents that, at every time step, need to decide whether they will try to grab the chair on their left or right side. An agent can only secure a chair if that chair is not targeted by the other neighboring agent. At the end of every step, each agent only observes whether it obtains the chair, without knowing the decisions of others. Additionally, there is a noise on observation, i.e., a chance that the agent gets an incorrect observation. In this game, it is clear that to the planning agent, whose goal is to obtain a chair at as many steps as possible, the decisions of neighboring agents 2 and 5 are more important than those of agents 3 and 4 as the former directly determine if the planning agent can secure a chair. In other words, only agents 2 and 5 directly inﬂuence agent 1’s local decision making, while agents 3 and 4 may only do so indirectly.
To utilize this fact, we propose inﬂuence-augmented online planning, a principled method that transforms a factored simulator of the entire environment, called global simulator, into a faster inﬂuence-augmented local simulator (IALS). The IALS simulates only the local factors, and concisely captures the inﬂuence of the external factors by predicting only the subset of them, called source factors, that directly affect the local factors. Using off-the-shelf supervised learning methods, the inﬂuence predictor is learned ofﬂine with data collected from the global simulator. Our intuition is that when planning with sample-based planners, the advantage that substantially more simulations can be performed in the IALS may outweigh the simulation inaccuracy caused by approximating the incoming inﬂuence. In this paper, we investigate this hypothesis, and show that this approach can indeed lead to improved online planning performance.
In detail, our planning experiments with POMCP show that, by replacing the global simulator with an IALS that learns the incoming inﬂuence with a recurrent neural network (RNN), we achieve matching performance while using much less time. More importantly, our real-time online planning experiments show that planning with the less accurate but much faster IALS yields better performance than planning with the global simulator in a complex environment, when the planning time per step is constrained. In addition, we ﬁnd that learning an accurate inﬂuence predictor is more important for good performance when the local planning problem is tightly coupled with the rest of the environment. 2