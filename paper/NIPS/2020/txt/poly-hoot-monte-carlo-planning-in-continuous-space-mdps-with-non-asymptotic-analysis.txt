Abstract
Monte-Carlo planning, as exempliﬁed by Monte-Carlo Tree Search (MCTS), has demonstrated remarkable performance in applications with ﬁnite spaces. In this paper, we consider Monte-Carlo planning in an environment with continuous state-action spaces, a much less understood problem with important applications in control and robotics. We introduce POLY-HOOT, an algorithm that augments
MCTS with a continuous armed bandit strategy named Hierarchical Optimistic
Optimization (HOO) (Bubeck et al., 2011). Speciﬁcally, we enhance HOO by using an appropriate polynomial, rather than logarithmic, bonus term in the upper conﬁ-dence bounds. Such a polynomial bonus is motivated by its empirical successes in AlphaGo Zero (Silver et al., 2017b), as well as its signiﬁcant role in achieving theoretical guarantees of ﬁnite space MCTS (Shah et al., 2019). We investigate, for the ﬁrst time, the regret of the enhanced HOO algorithm in non-stationary bandit problems. Using this result as a building block, we establish non-asymptotic con-vergence guarantees for POLY-HOOT: the value estimate converges to an arbitrarily small neighborhood of the optimal value function at a polynomial rate. We further provide experimental results that corroborate our theoretical ﬁndings. 1

Introduction
Monte-Carlo tree search (MCTS) has recently demonstrated remarkable success in deterministic games, especially in the game of Go (Silver et al., 2017b), Chess and Shogi (Silver et al., 2017a). It is also among the very few viable approaches to problems with partial observability, e.g., Poker (Rubin and Watson, 2011), and problems involving highly complicated strategies like real-time strategy games (Uriarte and Ontanón, 2014). However, most Monte-Carlo planning solutions only work well in ﬁnite state and action spaces, and are generally not compatible with continuous action spaces with enormous branching factors. Many important applications such as robotics and control require planning in a continuous state-action space, for which feasible solutions, especially those with theoretical guarantees, are scarce. In this paper, we aim to develop an MCTS method for continuous domains with non-asymptotic convergence guarantees.
Rigorous analysis of MCTS is highly non-trivial even in ﬁnite spaces. One crucial difﬁculty stems from the fact that the state-action value estimates in MCTS are non-stationary over multiple simula-tions, because the policies in the lower levels of the search tree are constantly changing. Due to the strong non-stationarity and interdependency of rewards, the reward concentration hypothesis made 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
in the seminal work of Kocsis and Szepesvári (2006)—which provides one of the ﬁrst theoretical analysis of bandit-based MCTS—turns out to be unrealistic. Hence, the convergence analysis given in Kocsis and Szepesvári (2006) is unlikely to hold in general. Recently a rigorous convergence result is established in Shah et al. (2019), based on further investigation of non-stationary multi-armed bandits (MABs).
Besides the non-stationarity issue inherent in MCTS analysis, an additional challenge for continuous domains lies in balancing the trade-off between generating ﬁne-grained samples across the entire continuous action domain to ensure optimality, and guaranteeing sufﬁcient exploitation of the sampled actions for accurate estimations. To tackle this challenge, a natural idea is to manually discretize the action space and then solve the resulting discrete problem using a discrete-space planning algorithm. However, this approach inevitably requires a hyper-parameter pre-specifying the level of discretization, which in turn leads to a fundamental trade-off between the computational complexity and the optimality of the planning solution: coarse discretization often fails to identify the optimal continuous action, yet ﬁne-grained discretization leads to a large action space and heavy computation.
In this paper, we consider Monte-Carlo planning in continuous space Markov Decision Processes (MDPs) without manually discretizing the action space. Our algorithm integrates MCTS with a continuous-armed bandit strategy, namely Hierarchical Optimistic Optimization (HOO) (Bubeck et al., 2011). Our algorithm adaptively partitions the action space and quickly identiﬁes the region of potentially optimal actions in the continuous space, which alleviates the inherent difﬁculties encountered by pre-speciﬁed discretization. The integration of MCTS with HOO has been empirically evaluated in Mansley et al. (2011), under the name of the Hierarchical Optimistic Optimization applied to Trees (HOOT) algorithm. HOOT directly replaces the UCB1 bandit algorithm (Auer et al., 2002) used in ﬁnite-space MCTS with the HOO strategy. However, this algorithm has a similar issue as that in Kocsis and Szepesvári (2006), as they both use a logarithmic bonus term for bandit exploration instead of a polynomial term. As pointed out in Shah et al. (2019) and mentioned above, convergence guarantees of these algorithms are generally unclear due to the lack of concentration of non-stationary rewards. In this work, we enhance the HOO strategy with a polynomial bonus term to account for the non-stationarity. As we will show in our theoretical results, our algorithm, Polynomial Hierarchical
Optimistic Optimization applied to Trees (POLY-HOOT), provably converges to an arbitrarily small neighborhood of the optimum at a polynomial rate.
Contributions. First, we enhance the continuous-armed bandit strategy HOO, and analyze its regret concentration rate in a non-stationary setting, which may also be of independent theoretical interest in the context of bandit problems. Second, we build on the enhanced HOO to design a Monte-Carlo planning algorithm POLY-HOOT for solving continuous space MDPs. Third, we generalize the recent analytical framework developed for ﬁnite-space MCTS (Shah et al., 2019) and prove that the value estimate of POLY-HOOT converges to an arbitrarily small neighborhood of the optimal value function at a polynomial rate. We note that HOOT is among the very few MCTS algorithms for continuous spaces and popular in practice. POLY-HOOT improves upon HOOT and provides theoretical justiﬁcations thereof. Finally, we present experimental results which corroborate our theoretical ﬁndings and demonstrate the superior performance of POLY-HOOT.