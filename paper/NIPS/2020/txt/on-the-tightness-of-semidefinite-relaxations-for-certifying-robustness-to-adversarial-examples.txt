Abstract
The robustness of a neural network to adversarial examples can be provably certiﬁed by solving a convex relaxation.
If the relaxation is loose, however, then the resulting certiﬁcate can be too conservative to be practically useful. Recently, a less conservative robustness certiﬁcate was proposed, based on a semideﬁnite programming (SDP) relaxation of the ReLU activation function. In this paper, we describe a geometric technique that determines whether this SDP certiﬁcate is exact, meaning whether it provides both a lower-bound on the size of the smallest adversarial perturbation, as well as a globally optimal perturbation that attains the lower-bound. Concretely, we show, for a least-squares restriction of the usual adversarial attack problem, that the SDP relaxation amounts to the nonconvex projection of a point onto a hyperbola. The resulting SDP certiﬁcate is exact if and only if the projection of the point lies on the major axis of the hyperbola.
Using this geometric technique, we prove that the certiﬁcate is exact over a single hidden layer under mild assumptions, and explain why it is usually conservative for several hidden layers. We experimentally conﬁrm our theoretical insights using a general-purpose interior-point method and a custom rank-2 Burer-Monteiro algorithm. 1

Introduction
It is now well-known that neural networks are vulnerable to adversarial examples: imperceptibly small changes to the input that result in large, possibly targeted change to the output [1–3]. Adversarial examples are particularly concerning for safety-critical applications like self-driving cars and smart grids, because they present a mechanism for erratic behavior and a vector for malicious attacks.
Methods for analyzing robustness to adversarial examples work by formulating the problem of ﬁnding the smallest perturbation needed to result in an adversarial outcome. For example, this could be the smallest change to an image of the digit “3” for a given model to misclassify it as an “8”. The size of this smallest change serves as a robustness margin: the model is robust if even the smallest adversarial change is still easily detectable.
Computing the robustness margin is a nonconvex optimization problem. In fact, methods that attack a model work by locally solving this optimization, usually using a variant of gradient descent [3–6].
A successful attack demonstrates vulnerability by explicitly stating a small—but not necessarily the smallest—adversarial perturbation. Of course, failed attacks do not prove robustness, as there is always the risk of being defeated by stronger attacks in the future. Instead, robustness can be certiﬁed by proving lower-bounds on the robustness margin [7–18]. Training against a robustness certiﬁcate (as an adversary) in turn produces models that are certiﬁably robust to adversarial examples [10, 19, 20]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The most useful robustness certiﬁcates are exact, meaning that they also explicitly state an adversarial perturbation whose size matches their lower-bound on the robustness margin, thereby proving global optimality [7–9]. Unfortunately, the robustness certiﬁcation problem is NP-hard in general, so most existing methods for exact certiﬁcation require worst-case time that scales exponentially with respect to the number of neurons. In contrast, conservative certiﬁcates are more scalable because the have polynomial worst-case time complexity [10–18]. Their usefulness is derived from their level of conservatism. The issue is that a pessimistic assessement for a model that is ostensibly robust can be attributed to either undue conservatism in the certiﬁcate, or an undiscovered vulnerability in the model. Also, training against an overly conservative certiﬁcate will result in an overly cautious model that is too willing to sacriﬁce performance for perceived safety.
Recently, Raghunathan et al. [21] proposed a less conservative certiﬁcate based on a semideﬁnite programming (SDP) relaxation of the rectiﬁed linear unit (ReLU) activation function. Their em-pirical results found it to be signiﬁcantly less conservative than competing approaches, based on linear programming or propagating Lipschitz constants. In other domains, ranging from integer programming [22, 23], polynomial optimization [24, 25], matrix completion [26, 27], to matrix sensing [28], the SDP relaxation is often tight, in the sense that it is formally equivalent to the original combinatorially hard problem. Within our context, tightness corresponds to exactness in the robustness certiﬁcate. Hence, the SDP relaxation is a good candidate for exact certiﬁcation in polynomial time, possibly over some restricted class of models or datasets.
This paper aims to understand when the SDP relaxation of the ReLU becomes tight, with the goal of characterizing conditions for exact robustness certiﬁcation. Our main contribution is a geometric technique for analyzing tightness, based on splitting a least-squares restriction of the adversarial attack problem into a sequence of projection problems. The ﬁnal problem projects a point onto a nonconvex hyperboloid (i.e. a high-dimensional hyperbola), and the SDP relaxation is tight if and only if this projection lies on the major axis of the hyperboloid. Using this geometric technique, we prove that the SDP certiﬁcate is generally exact for a single hidden layer. The certiﬁcate is usually conservative for several hidden layers; we use the same geometric technique to offer an explanation for why this is the case.
Notations. Denote vectors in boldface lower-case x, matrices in boldface upper-case X, and scalars in non-boldface x, X. The bracket denotes indexing x[i] starting from 1, and also concatenation, which is row-wise via the comma [a, b] and column-wise via the semicolon [a; b]. The i-th canonical basis vector ei satisﬁes ei[i] = 1 and ei[j] = 0 for all j (cid:54)= i. The usual inner product is (cid:104)a, b(cid:105) = (cid:80) ia[i]b[i], and the usual rectiﬁed linear unit activation function is ReLU(x) ≡ max{x, 0}. 2 Main results
Let f : Rn → Rm be a feedforward ReLU neural network classiﬁer with (cid:96) hidden layers f (x0) = x(cid:96) where xk+1 = ReLU(Wkxk + bk) (2.1) that takes an input ˆx ∈ Rn (say, an image of a hand-written single digit) labeled as belonging to the i-th of m classes (say, the 5-th of 10 possible classes of single digits), and outputs a prediction vector s = W(cid:96)f (ˆx) + b(cid:96) ∈ Rm whose i-th element is the largest, as in s[i] > s[j] for all j (cid:54)= i. Then, the problem of ﬁnding an adversarial example x that is similar to ˆx but causes an incorrect j-th class to be ranked over the i-th class can be posed for all k ∈ {0, 1, . . . , (cid:96) − 1}, (cid:107)x − ˆx(cid:107) subject to (2.1), (cid:104)w, f (x)(cid:105) + b ≤ 0, (A) where w = WT
ˆx over all incorrect classes gives a robustness margin d(cid:63) = minj(cid:54)=i dj for the neural network.
In practice, the SDP relaxation for problem (A) is often loose. To understand the underlying mechanism, we study a slight modiﬁcation that we call its least-squares restriction (cid:96) (ei − ej). In turn, the adversarial example x(cid:63) most similar to
L(cid:63) = min x∈Rn (cid:107)x − ˆx(cid:107) subject to (2.1), (cid:107)f (x) − ˆz(cid:107) ≤ ρ, (B) where ˆz ∈ Rm is the targeted output, and ρ > 0 is a radius parameter. Problem (A) is equivalent to problem (B) taken at the limit ρ → ∞, because a half-space is just an inﬁnite-sized ball (cid:107)z+w (cid:0)b/(cid:107)w(cid:107)2 + ρ/(cid:107)w(cid:107)(cid:1) (cid:107)2 ≤ ρ2 ⇐⇒ (cid:107)w(cid:107) 2ρ (cid:107)z+w (cid:0)b/(cid:107)w(cid:107)2(cid:1) (cid:107)2 +[(cid:104)w, z(cid:105)+b] ≤ 0 (2.2) 2 d(cid:63) j = min x∈Rn (cid:96) (ei − ej) and b = bT
with a center ˆz = −w (cid:0)b/(cid:107)w(cid:107)2 + ρ/(cid:107)w(cid:107)(cid:1) that tends to inﬁnity alongside the ball radius ρ. The SDP relaxation for problem (B) is often tight for ﬁnite values of the radius ρ. The resulting solution x is a strictly feasible (but suboptimal) attack for problem (A) that causes misclassiﬁcation (cid:104)w, f (x)(cid:105) + b < 0. The corresponding optimal value L(cid:63) gives an upper-bound dub ≡ L(cid:63) ≥ d(cid:63) that converges to an equality as ρ → ∞. (See Appendix E for details.)
In Section 5, we completely characterize the SDP relaxation for problem (B) over a single hidden neuron, by appealing to the underlying geometry of the relaxation. In Section 6, we extend these insights to partially characterize the case of a single hidden layer.
Theorem 2.1 (One hidden neuron). Consider the one-neuron version of problem (B), explicitly written
L(cid:63) = min x
|x − ˆx| subject to |ReLU(x) − ˆz| ≤ ρ. (2.3)
The SDP relaxation of (2.3) yields a tight lower-bound Llb = L(cid:63) and a globally optimal x(cid:63) satisfying |x(cid:63) − ˆx| = Llb if and only if one of the two conditions hold: (i) ρ ≥ |ˆz|; or (ii)
ρ < ˆz/(1 − min{0, ˆx/ˆz}).
Theorem 2.2 (One hidden layer). Consider the one-layer version of problem (B), explicitly written
L(cid:63) = min x∈Rn (cid:107)x − ˆx(cid:107) s.t. (cid:107)ReLU(Wx) − ˆz(cid:107) ≤ ρ (2.4)
The SDP relaxation of (2.3) yields a tight lower-bound Llb = L(cid:63) and a globally optimal x(cid:63) satisfying (cid:107)x(cid:63) − ˆx(cid:107) = Llb if one of the two conditions hold: (i) ρ ≥ (cid:107)ReLU(Wˆx) − ˆz(cid:107); or (ii) ρ < ˆzmin/2(1 +
κ) and (cid:107)Wˆx − ˆz(cid:107)∞ < ˆz2 min/(2ρκ) where ˆzmin = mini ˆzi and κ = (cid:107)W(cid:107)2(cid:107)(WWT )−1(cid:107)∞.
The lack of a weight term in (2.3) and a bias term in (2.3) and (2.4) is without loss of generality, as these can always be accommodated by shifting and scaling x and ˆx. Intuitively, Theorem 2.1 and
Theorem 2.2 say that the SDP relaxation tends to be tight if the output target ˆz is feasible, meaning that there exists some choice of u such that ˆz = f (u). (The condition ρ < ˆzmin/2(1 + κ) is sufﬁcient for feasibility.) Conversely, the SDP relaxation tends to be loose if the radius ρ > 0 lies within an intermediate band of “bad” values. For example, over a single neuron with a feasible ˆz = 1, the relaxation is loose if and only if ˆx ≤ 0 and 1/(1 + |ˆx|) ≤ ρ < 1. These two general trends are experimentally veriﬁed in Section 8.
In the case of multiple layers, the SDP relaxation is usually loose, with a notable exception being the trivial case with L(cid:63) = 0.
Corollary 2.3 (Multiple layers). If ρ ≥ (cid:107)f (ˆx) − ˆz(cid:107), then the SDP relaxation of problem (B) yields the tight lower-bound Llb = L(cid:63) = 0 and the globally optimal x(cid:63) = ˆx satisfying (cid:107)x(cid:63) − ˆx(cid:107) = 0.
The proof is given in Appendix E. In Section 7, we explain the looseness of the relaxation for multiple layers using the geometric insight developed for the single layer. As mentioned above, the general looseness of the SDP relaxation for problem (B) then immediately implies the general looseness for problem (A). 3