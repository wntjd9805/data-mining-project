Abstract
Normalizing ﬂows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are cur-rently outperformed by other models such as normalizing ﬂows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural archi-tectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierar-chical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that
NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the ﬁrst successful VAE applied to natural images as large as 256×256 pixels. The source code is available at https://github.com/NVlabs/NVAE. 1

Introduction
The majority of the research efforts on improving VAEs [1, 2] is dedicated to the statistical challenges, such as reducing the gap between approximate and true posterior distributions [3, 4, 5, 6, 7, 8, 9, 10], formulating tighter bounds [11, 12, 13, 14], reducing the gradient noise [15, 16], extending VAEs to discrete variables [17, 18, 19, 20, 21, 22, 23], or tackling posterior collapse [24, 25, 26, 27]. The role of neural network architectures for VAEs is somewhat overlooked, as most previous work borrows the architectures from classiﬁcation tasks.
Figure 1: 256×256-pixel samples generated by NVAE, trained on CelebA HQ [28].
However, VAEs can beneﬁt from designing special network architectures as they have fundamentally different requirements. First, VAEs maximize the mutual information between the input and latent variables [29, 30], requiring the networks to retain the information content of the input data as much 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
as possible. This is in contrast with classiﬁcation networks that discard information regarding the input [31]. Second, VAEs often respond differently to the over-parameterization in neural networks.
Since the marginal log-likelihood only depends on the generative model, overparameterizing the decoder network may hurt the test log-likelihood, whereas powerful encoders can yield better models because of reducing the amortization gap [6]. Wu et al. [32] observe that the marginal log-likelihood, estimated by non-encoder-based methods, is not sensitive to the encoder overﬁtting (see also Fig. 9 in [19]). Moreover, the neural networks for VAEs should model long-range correlations in data [33, 34, 35], requiring the networks to have large receptive ﬁelds. Finally, due to the unbounded
Kullback–Leibler (KL) divergence in the variational lower bound, training very deep hierarchical
VAEs is often unstable. The current state-of-the-art VAEs [4, 36] omit batch normalization (BN) [37] to combat the sources of randomness that could potentially amplify their instability.
In this paper, we aim to make VAEs great again by architecture design. We propose Nouveau VAE (NVAE), a deep hierarchical VAE with a carefully designed network architecture that produces high-quality images. NVAE obtains the state-of-the-art results among non-autoregressive likelihood-based generative models, reducing the gap with autoregressive models. The main building block of our network is depthwise convolutions [38, 39] that rapidly increase the receptive ﬁeld of the network without dramatically increasing the number of parameters.
In contrast to the previous work, we ﬁnd that BN is an important component of the success of deep
VAEs. We also observe that instability of training remains a major roadblock when the number of hierarchical groups is increased, independent of the presence of BN. To combat this, we propose a residual parameterization of the approximate posterior parameters to improve minimizing the KL term, and we show that spectral regularization is key to stabilizing VAE training.
In summary, we make the following contributions: i) We propose a novel deep hierarchical VAE, called NVAE, with depthwise convolutions in its generative model. ii) We propose a new residual parameterization of the approximate posteriors. iii) We stabilize training deep VAEs with spectral regularization. iv) We provide practical solutions to reduce the memory burden of VAEs. v) We show that deep hierarchical VAEs can obtain state-of-the-art results on several image datasets, and can produce high-quality samples even when trained with the original VAE objective. To the best of our knowledge, NVAE is the ﬁrst successful application of VAEs to images as large as 256×256 pixels.