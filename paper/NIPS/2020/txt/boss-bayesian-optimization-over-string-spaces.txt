Abstract
This article develops a Bayesian optimization (BO) method which acts directly over raw strings, proposing the ﬁrst uses of string kernels and genetic algorithms within
BO loops. Recent applications of BO over strings have been hindered by the need to map inputs into a smooth and unconstrained latent space. Learning this projection is computationally and data-intensive. Our approach instead builds a powerful
Gaussian process surrogate model based on string kernels, naturally supporting variable length inputs, and performs efﬁcient acquisition function maximization for spaces with syntactical constraints. Experiments demonstrate considerably improved optimization over existing approaches across a broad range of constraints, including the popular setting where syntax is governed by a context-free grammar. 1

Introduction
Many tasks in chemistry, biology and machine learning can be framed as optimization problems over spaces of strings. Examples include the design of synthetic genes [González et al., 2014, Tanaka and
Iwata, 2018] and chemical molecules [Grifﬁths and Hernández-Lobato, 2020, Gómez-Bombarelli et al., 2018], as well as problems in symbolic regression [Kusner et al., 2017] and kernel design
[Lu et al., 2018]. Common to these applications is the high cost of evaluating a particular input, for example requiring resource and labor-consuming wet lab tests. Consequently, most standard discrete optimization routines are unsuitable, as they require many evaluations.
Bayesian Optimization [Shahriari et al., 2015, BO] has recently risen as an effective strategy to address the applications above, due to its ability to ﬁnd good solutions within heavily restricted evaluation budgets. However, the vast majority of BO approaches assume a low dimensional, mostly continuous space; string inputs have to be converted to ﬁxed-size vectors such as bags-of-ngrams or latent representations learned through an unsupervised model, typically a variational autoencoder
[Kingma and Welling, 2014, VAE]. In this work, we remove this encoding step and propose a BO architecture that operates directly on raw strings through the lens of convolution kernels [Haussler, 1999]. In particular, we employ a Gaussian Process [Rasmussen, 2003, GP] with a string kernel
[Lodhi et al., 2002] as the surrogate model for the objective function, measuring the similarity between strings by examining shared non-contiguous sub-sequences. String kernels provide an easy and user-friendly way to deploy BO loops directly over strings, avoiding the expensive architecture tuning required to ﬁnd a useful VAE. At the same time, by using a kernel trick to work in much richer feature spaces than the bags-of-ngrams vectors, string kernels can encode the non-contiguity known to be informative when modeling genetic sequences [Vert, 2007] and SMILES [Anderson et al., 1987] representations of molecules [Cao et al., 2012](see Figure 1). We show that our string kernel’s two 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Similar molecules have SMILES strings with local differences (red) but common non-contiguous sub-sequences.
Figure 2: BO loop for molecule design using a string kernel sur-rogate model (a) and genetic algorithms for acquisition function maximization (b). parameters can be reliably ﬁne-tuned to model complex objective functions with just a handful of function evaluations, without needing the large collections of unlabeled data required to train VAEs.
Devising a BO framework directly over strings raises the question of how to maximize acquisition functions; heuristics used to select new evaluation points. Standard BO uses numerical methods to maximize these functions but these are not applicable when the inputs are discrete structures such as strings. To address this challenge, we employ a suite of genetic algorithms [Whitley, 1994] to provide efﬁcient exploration of string spaces under a range of syntactical constraints.
Our contributions can be summarized as follows:
• We introduce string kernels into BO, providing powerful GP surrogate models of complex objective functions with just two data-driven parameters (Figure 2.a).
• We propose a suite of genetic algorithms suitable for efﬁciently optimizing acquisition functions under a variety of syntactical constraints (Figure 2.b).
• We demonstrate that our framework out-performs established baselines across four scenarios encompassing a range of applications and diverse set of constraints. 2