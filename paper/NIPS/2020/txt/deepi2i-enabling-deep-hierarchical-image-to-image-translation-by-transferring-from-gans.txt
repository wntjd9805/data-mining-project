Abstract
Image-to-image translation has recently achieved remarkable results. But despite current success, it suffers from inferior performance when translations between classes require large shape changes. We attribute this to the high-resolution bottle-necks which are used by current state-of-the-art image-to-image methods. There-fore, in this work, we propose a novel deep hierarchical Image-to-Image Translation method, called DeepI2I. We learn a model by leveraging hierarchical features: (a) structural information contained in the shallow layers and (b) semantic information extracted from the deep layers. To enable the training of deep I2I models on small datasets, we propose a novel transfer learning method, that transfers knowledge from pre-trained GANs. Speciﬁcally, we leverage the discriminator of a pre-trained
GANs (i.e. BigGAN or StyleGAN) to initialize both the encoder and the dis-criminator and the pre-trained generator to initialize the generator of our model.
Applying knowledge transfer leads to an alignment problem between the encoder and generator. We introduce an adaptor network to address this. On many-class image-to-image translation on three datasets (Animal faces, Birds, and Foods) we decrease mFID by at least 35% when compared to the state-of-the-art. Furthermore, we qualitatively and quantitatively demonstrate that transfer learning signiﬁcantly improves the performance of I2I systems, especially for small datasets. Finally, we are the ﬁrst to perform I2I translations for domains with over 100 classes. Our code and models are made public at: https://github.com/yaxingwang/DeepI2I. 1

Introduction
Most image-to-image (I2I) networks have a high-resolution bottleneck [9, 21, 33, 34, 69, 36, 37, 67, 73, 74, 76]. These methods only apply two down-sampling blocks. Whereas such models are known to be successful for style transfer, it might be difﬁcult to extract abstract semantic information.
Therefore, we argue (and experimentally verify) that current I2I architectures have a limited capacity to translate between classes with signiﬁcant shape changes (e.g. from dog face to meerkat face). To successfully translate between those domains a semantic understanding of the image is required. This information is typically extracted in the deep low-resolution layers of a network.
The loss of spatial resolution, which occurs when adding multiple down-sampling layers, is one of the factors which complicates the usage of deep networks for I2I. For high-quality I2I both low-level style information, as well as high-level semantic information needs to be transferred from the encoder to the generator of the I2I system. Therefore, we propose a deep hierarchical I2I translation framework (see
Figure 1 right) that fuses feature representations at various levels of abstraction (depth). Leveraging the hierarchical framework, allows us to perform I2I translation between classes with signiﬁcant shape changes. The proposed method, DeepI2I, builds upon the state-of-the-art BigGAN model [7], extending it to I2I translation by adding an encoder with the same architecture as the discriminator.
An additional advantage of this architecture is that because it applies a latent class embedding, it 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is scalable to many-class domains. We are the ﬁrst to perform translations in a single architecture between over 100 classes, while current systems have only considered up to 40 classes maximum1.
Another factor, that complicates the extension to deep I2I, is that the increasing amount of network parameters limits its applicability to I2I problems with large datasets. To address this problem we propose a novel knowledge transfer method for I2I translation. Transferring knowledge from pre-trained networks has greatly beneﬁted the discriminative tasks [13], enabling the re-use of high-quality networks. However, it is not clear on what dataset a high-quality pre-trained I2I model should be trained, since translations between arbitrary classes might not make sense (e.g. translating from satellite images to dog faces does not make sense, while both classes have been used in separate
I2I problems [22]). Instead, we propose to initialize the weights of the I2I model with those of a pre-trained GANs. We leverage the discriminator of a pre-trained BigGAN (Figure 1 left) to initialize both the encoder and the discriminator of I2I translation model, and the pre-trained generator to initialize the generator of our I2I model. To address the misalignment between the different layers of the encoder and generator, we propose a dedicated adaptor network to align both the pre-initialized encoder and generator.
We perform experiments on multiple datasets, including complex datasets which have signiﬁcant shape changes such as animal faces [38] and foods [31]. We demonstrate the effects of the deep hierarchical
I2I translation framework, providing qualitative and quantitative results. We further evaluate the knowledge transfer, which largely accelerates convergence and outperforms DeepI2I trained from scratch. Additionally, leveraging the adaptor is able to tackle the misalignment, consistently improving the performance of I2I translation. 2