Abstract
Adversarial training is a popular method to give neural nets robustness against
In practice adversarial training leads to low robust adversarial perturbations. training loss. However, a rigorous explanation for why this happens under natural conditions is still missing. Recently a convergence theory for standard (non-adversarial) training was developed by various groups for very over-parametrized nets. It is unclear how to extend these results to adversarial training because of the min-max objective. Recently, a ﬁrst step towards this direction was made by [14] using tools from online learning, but they require the width of the net and the running time to be exponential in input dimension d, and they consider an activation function that is not used in practice. Our work proves convergence to low robust training loss for polynomial width and running time, instead of exponential, under natural assumptions and with ReLU activation. Key element of our proof is showing that ReLU networks near initialization can approximate the step function, which may be of independent interest. 1

Introduction
Deep neural networks trained by gradient based methods tend to change their answer (incorrectly) after small adversarial perturbations in inputs [25]. Much effort has been spent to make deep nets resistant to such perturbations but adversarial training with a natural min-max objective [20] stands out as one of the most effective approaches according to [9, 7].
One interpretation of the min-max formulation is a certain two-player game between a neural network learner and an adversary who is allowed to perturb the input within certain constraints. In each round, the adversary generates new adversarial examples against the current network, on which the learner takes a gradient step to decrease its prediction loss in response (see Algorithm 1).
It is empirically observed that, when the neural network is initialized randomly, this training algorithm is efﬁcient and computes a reasonably sized neural net that is robust on (at least) the training exam-ples [20]. We’re interested in theoretical understanding of this phenomenon: Why does adversarial
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
training efﬁciently ﬁnd a feasibly sized neural net to ﬁt training data robustly? In recent years, a convergence theory has been developed for non-adversarial training: it explains the ability of gradient descent to achieve small training loss, provided the neural nets are fairly over-parametrized. But it is quite unclear whether similar analysis can be applied to adversarial training setting where the inputs are perturbed. Furthermore, while the algorithm is reminiscent of well-studied no-regret dynamics for ﬁnding equilibria in two-player zero-sum convex/concave games [17], here the game value is training loss, and hence non-convex. Thus it is unclear if training leads to small robust training loss.
A study of such issues was initiated in [14]. For two-layer nets with quadratic ReLU activation2 they were able to show that if input is in Rd then training can achieve robust loss at most (cid:15) provided the net’s width is (1/(cid:15))Ω(d) (the number of required iterations is also that large)3. This is very extreme over-parametrization, and this curse of dimensionality is inherent to their argument. They left as an open problem the possibility to improve the width requirement, which is the theme of our paper.
Our contributions: Under a standard and natural assumption that training data are well-separated with respect to the magnitude of the adversarial perturbations (also veriﬁed for popular datasets in
Figure 1) we show the following:
• That there exists a two-layer ReLU neural network with width poly (d, n/(cid:15)) near Gaussian random initialization that achieves (cid:15) robust training loss.
• That starting from Gaussian random initialization, standard adversarial training (Algorithm 1) converges to such a network in poly (d, n/(cid:15)) iterations.
• New result in approximation theory, speciﬁcally the existence of a good approximation to the step function by a polynomially wide two-layer ReLU network with weights close to the standard gaussian initialization. Such approximation result may be of further use in the emerging theory of over-parameterized nets.
Paper structure. This paper is organized as follows. In section 2, we give an overview of the related works. In section 3, we present our notation, the adversarial training algorithm, the separability condition and we argue why the training examples being well-separated is a natural assumption. In section 4, we formally state our main result and in section 5 we give an overview of its proof. In section 6 we elaborate more on the core part of the proof, which is the existence of a net close to initialization that robustly ﬁts the training data. 2