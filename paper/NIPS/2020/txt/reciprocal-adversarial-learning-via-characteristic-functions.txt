Abstract
Generative adversarial nets (GANs) have become a preferred tool for tasks in-volving complicated distributions. To stabilise the training and reduce the mode collapse of GANs, one of their main variants employs the integral probability met-ric (IPM) as the loss function. This provides extensive IPM-GANs with theoretical support for basically comparing moments in an embedded domain of the critic.
We generalise this by comparing the distributions rather than their moments via a powerful tool, i.e., the characteristic function (CF), which uniquely and universally comprising all the information about a distribution. For rigour, we ﬁrst establish the physical meaning of the phase and amplitude in CF, and show that this provides a feasible way of balancing the accuracy and diversity of generation. We then develop an efﬁcient sampling strategy to calculate the CFs. Within this framework, we further prove an equivalence between the embedded and data domains when a reciprocal exists, where we naturally develop the GAN in an auto-encoder struc-ture, in a way of comparing everything in the embedded space (a semantically meaningful manifold). This efﬁcient structure uses only two modules, together with a simple training strategy, to achieve bi-directionally generating clear images, which is referred to as the reciprocal CF GAN (RCF-GAN). Experimental results demonstrate the superior performances of the proposed RCF-GAN in terms of both generation and reconstruction. 1

Introduction
Generative adversarial nets (GANs) owe their success to their powerful capability in capturing complicated data distributions [1]. In practical applications, however, their signiﬁcant potential still remains under-explored as GANs typically suffer from unstable training and mode collapse issues [2].
An effective yet elegant way to address these issues is to replace the Jensen-Shannon (JS) divergence in measuring the discrepancy in the original form of GANs [3] by another class of metrics called the integral probability metric (IPM) [4] given by, d(Pd, Pg) = sup f ∈F
|Ex∼Pd [f (x)] − Ex∼Pg [f (x)]|, (1) where the symbol F in IPMs represents a collection of (typically real) bounded functions, Pg denotes the generated distribution, and Pd is the real data distribution. Using IPMs to improve GANs has been justiﬁed by the fact that in real-world data distributions are typically embedded in low-dimensional manifolds, which is intuitive because data preserve semantic information instead of being a collection of rather random pixels. Thus, the divergence measure (“bin-to-bin” comparison) of the original GAN could easily max out, whereas the IPMs such as the Wasserstein distance (“cross-bin” comparison) can consistently yield a meaningful measure between the generated and real data distributions [3].
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The overall structure of the proposed RCF-GAN. The generator serves to minimise the CF loss between the embedded real and fake distributions. The critic serves to minimise the CF loss between the embedded real and the input noise distributions, whilst maximising the CF loss between the embedded fake and the input noise distributions. Moreover, an MSE loss between the embedded fake and the input noise distributions is regularised as the auto-encoder loss, which has not been shown in the ﬁgure. An optional t-net can be employed to optimally sample the CF loss.
Varying collections of F in (1), therefore, deﬁnes different IPM-GANs and the supremum supf ∈F is then typically achieved by the discriminator net, or more formally, the critic in the IPM-GANs.
The ﬁrst IPM-GAN was motivated by the Wasserstein GAN (W-GAN) [5], where F denotes all the 1-Lipschitz functions. However, it has been widely argued that the critic is not powerful enough to search within all the 1-Lipschitz function spaces, which leads to limited diversity of the generator due to an ill-posed equivalence measurement of Pd and Pg [6, 7]. Follow-up works have been proposed to improve the W-GAN by either enhancing it to satisfy the 1-Lipschitz condition (e.g., by gradient penalty [8] or spectral normalization [9]) or by employing easy-to-implement F for the critic. The latter, by virtue of relaxing the critic, typically leads to a stringent comparison on the embedded feature domain, i.e., by matching higher-order moments instead of the mean matching in the W-GAN.
This path includes many recent GANs which additionally consider the second-order moment (e.g.,
Fisher-GAN [10] and McGAN [11]), together with explicitly (e.g., Sphere GAN [12]) or implicitly (e.g., MMD-GAN [13, 14]) comparing higher-order moments. Furthermore, generalising (1) as moment matching problem has been justiﬁed as a natural and beneﬁcial way to understand IPM-GANs [15–17]. This also compensates for the deﬁciency where the critic may not transform the data distributions into unimodal distributions, for example, the Gaussian distribution that is solely determined by the ﬁrst- and second-order moments.
Moreover, it is more safe and elegant to compare the distributions because the equivalence in distributions ensures the equivalence in the moments; the inverse, however, does not necessarily hold. As a powerful tool of containing all the information relevant to a distribution, the characteristic function (CF) provides a universal way of comparing distributions, even when their probability density functions (pdfs) do not exist. The CF also has a one-to-one correspondence with the cumulative density function (cdf), which has also been veriﬁed to beneﬁt the design of GANs [18]. Compared to the moment generating function (mgf) that has been reﬂected in the MMD-GAN [13], the CF is unique and universally existent. More importantly, the CF is automatically aligned at 0; this means that even a simple “bin-to-bin” comparison between CFs can consistently provide a meaningful measure and thus avoid gradient vanishing that appears in the original GAN [5]. On the other hand, the weak convergence property of CFs ensures that the convergence in the CF also indicates the convergence in the distributions.
In this paper, we propose a reciprocal CF GAN (RCF-GAN) as a natural generalisation of the existing
IPM-GANs, with the overall structure shown in Fig. 1. It needs to be pointed out that incorporating the CF in a GAN is non-trivial because the CF is basically complex-valued and the comparison has to be performed on functions as well. To address these difﬁculties, we ﬁrst demystify the role of CFs by ﬁnding that its phase is closely related to the distribution centre, whereas the amplitude dominates the distribution scale. This provides a feasible way of balancing the accuracy and diversity of generation. Then, as for the comparison over functions, we prove that other than in the whole space of CFs, sampling within a small ball around 0 of CFs is sufﬁcient to compare two distributions, and also enables the proposed CF loss to be bounded and differentiable almost everywhere. We further optimise the sampling strategy by automatically adjusting sampling distributions under the umbrella of the scale mixture of normals [19]. 2
Beneﬁting from our powerful CF design in comparing distributions, we propose to purely compare in the embedded domain and prove its equivalence to the counterpart in the data domain when a reciprocal theory between the generator and the critic holds. This motivates us to incorporate an auto-encoder structure to satisfy this theoretical requirement. In this way, the critic in our RCF-GAN is further relaxed and only focuses on learning a fruitful embedding. Furthermore, different from many existing adversarial works with auto-encoders incorporating at least three modules2 [13, 14, 21–26], our RCF-GAN only requires two modules that already exist in a GAN; the critic is an encoder and the generator is a decoder as well, which is neat and reasonable as this comes without increasing computational complexity and complicated (unstable) training strategies, as well as without other requirements such as the Lipschitz continuity. More importantly, the framework of comparing everything in the embedded domain enables the CF-GAN to learn a semantic and meaningful latent space, and to also avoid the smoothing artefact that arises from the use of point-wise mean square error (MSE) employed in the data domain. This beneﬁts from both the auto-encoder and the GANs, i.e., bi-directionally generating clear images. Our experimental results show that our RCF-GAN achieves remarkable improvements on the generation, together with an additional capability in the reconstruction and interpolation3. 2 Characteristic Function Loss and Efﬁcient Sampling Strategy 2.1 Characteristic Function and Elliptical Distribution
The CF of a random variable, X ∈ Rm, represents the expectation of its complex unitary transform, given by
ΦX (t) = EX [ejtT x] = ejtT xdFX (x), (2) (cid:90) x where FX (x) is the cdf of X . We thus have ΦX (0) = 1 and |ΦX (t)| ≤ 1 for all t. This property ensures that CFs can be straightforwardly compared in a “bin-to-bin” manner, because all CFs are automatically aligned at t = 0. Moreover, when the pdf of X exists, the expression in (2) is equal to its inverse Fourier transform; this ensures that ΦX (t) is uniformly continuous. Another important property of the CF is that it uniquely and universally retains all the information regarding a random variable. In other words, a random variable does not necessarily need to possess a pdf (e.g., when it is an α-stable distribution), but its CF always exists.
As the cdf, FX (x), is unknown and is to be compared, we employ the empirical characteristic function (ECF) as an asymptotic approximation in the form of (cid:98)ΦX n (t) = (cid:80)n i=1 are n i.i.d. samples drawn from X . As a result of the Levy continuity theorem [28], the ECF converges weakly to the population CF [29]. More importantly, the uniqueness theorem guarantees that two random variables have the same distribution if and only if their CFs are identical [30]. Therefore, together with the weak convergence, the ECF provides a feasible and good proxy to the distribution, which has also been preliminarily applied in two sample test [31, 32]. Before proceeding further, we introduce an important class of distributions that will be used in this work. i=1 ejtT xi, where {xi}n
Example 1. Within unimodal distributions, one broad class of distributions is called the elliptical distribution, which is general enough to include various important distributions such as the Gaussian,
Laplace, Cauchy, Student-t, α-stable and logistic distributions. The elliptical distributions do not necessarily have pdfs, and we refer to [33] for more detail. The CF of an elliptical distribution, X , however, always exists and has the following form
ΦX (t) = ejtT µψ(tT Σt), (3) where µ denotes the distribution centre, Σ is the distribution scale, and ψ(·) is a real-valued function
R → R, for example, ψ(s) = e(−s/2) for the Gaussian distribution. By inspecting (3) we can see that the phase of the CF is solely related to the location of data centre and the amplitude is only governed by the distribution scale (diversity). 2To our best knowledge, the only exception is the AGE [20], which adopts two modules in an auto-encoder under a max-min problem and different losses. Please see the