Abstract
Reinforcement learning has shown great promise for synthesizing realistic human behaviors by learning humanoid control policies from motion capture data. How-ever, it is still very challenging to reproduce sophisticated human skills like ballet dance, or to stably imitate long-term human behaviors with complex transitions.
The main difﬁculty lies in the dynamics mismatch between the humanoid model and real humans. That is, motions of real humans may not be physically possible for the humanoid model. To overcome the dynamics mismatch, we propose a novel approach, residual force control (RFC), that augments a humanoid control policy by adding external residual forces into the action space. During training, the
RFC-based policy learns to apply residual forces to the humanoid to compensate for the dynamics mismatch and better imitate the reference motion. Experiments on a wide range of dynamic motions demonstrate that our approach outperforms state-of-the-art methods in terms of convergence speed and the quality of learned motions. Notably, we showcase a physics-based virtual character empowered by
RFC that can perform highly agile ballet dance moves such as pirouette, arabesque and jeté. Furthermore, we propose a dual-policy control framework, where a kinematic policy and an RFC-based policy work in tandem to synthesize multi-modal inﬁnite-horizon human motions without any task guidance or user input.
Our approach is the ﬁrst humanoid control method that successfully learns from a large-scale human motion dataset (Human3.6M) and generates diverse long-term motions. Code and videos are available at https://www.ye-yuan.com/rfc. 1

Introduction
Understanding human behaviors and creating virtual humans that act like real people has been a mesmerizing yet elusive goal in computer vision and graphics. One important step to achieve this goal is human motion synthesis which has broad applications in animation, gaming and virtual reality.
With advances in deep learning, data-driven approaches [12, 13, 41, 38, 3] have achieved remarkable progress in producing realistic motions learned from motion capture data. Among them are physics-based methods [41, 38, 3] empowered by reinforcement learning (RL), where a humanoid agent in simulation is trained to imitate reference motions. Physics-based methods have many advantages over their kinematics-based counterparts. For instance, the motions generated with physics are typically free from jitter, foot skidding or geometry penetration as they respect physical constraints. Moreover, the humanoid agent inside simulation can interact with the physical environment and adapt to various terrains and perturbations, generating diverse motion variations.
However, physics-based methods have their own challenges. In many cases, the humanoid agent fails to imitate highly agile motions like ballet dance or long-term motions that involve swift transitions between various locomotions. We attribute such difﬁculty to the dynamics mismatch between the humanoid model and real humans. Humans are very difﬁcult to model because they are very complex 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Top: A ballet dancer performing highly agile moves like jeté, arabesque and pirouette.
Bottom: A humanoid agent controlled by a policy augmented with the proposed residual forces (blue arrows) is able to dance like the performer. The motion is best viewed in our supplementary video. creatures with hundreds of bones and muscles. Although prior work has tried to improve the ﬁdelity of the humanoid model [21, 52], it is nonetheless safe to say that these models are not exact replicas of real humans and the dynamics mismatch still exists. The problem is further complicated when motion capture data comprises a variety of individuals with diverse body types. Due to the dynamics mismatch, motions produced by real humans may not be admissible by the humanoid model, which means no control policy of the humanoid is able to generate those motions.
To overcome the dynamics mismatch, we propose an approach termed Residual Force Control (RFC) which can be seamlessly integrated into existing RL-based humanoid control frameworks.
Speciﬁcally, RFC augments a control policy by introducing external residual forces into the action space. During RL training, the RFC-based policy learns to apply residual forces onto the humanoid to compensate for the dynamics mismatch and achieve better motion imitation. Intuitively, the residual forces can be interpreted as invisible forces that enhance the humanoid’s abilities to go beyond the physical limits imposed by the humanoid model. RFC generates a more expressive dynamics that admits a wider range of human motions since the residual forces serve as a learnable time-varying correction to the dynamics of the humanoid model. To validate our approach, we perform motion imitation experiments on a wide range of dynamic human motions including ballet dance and acrobatics. The results demonstrate that RFC outperforms state-of-the-art methods with faster convergence and better motion quality. Notably, we are able to showcase humanoid control policies that are capable of highly agile ballet dance moves like pirouette, arabesque and jeté (Fig. 1).
Another challenge facing physics-based methods is synthesizing multi-modal long-term human motions. Previous work has elicited long-term human motions with hierarchical RL [33, 32, 42] or user interactive control [3, 38]. However, these approaches still need to deﬁne high-level tasks of the agent or require human interaction. We argue that removing these requirements could be critical for applications like automated motion generation and large-scale character animation. Thus, we take a different approach to long-term human motion synthesis by leveraging the temporal dependence of human motion. In particular, we propose a dual-policy control framework where a kinematic policy learns to predict multi-modal future motions based on the past motion and a latent variable used to model human intent, while an RFC-based control policy learns to imitate the output motions of the kinematic policy to produce physically-plausible motions. Experiments on a large-scale human motion dataset, Human3.6M [14], show that our approach with RFC and dual policy control can synthesize stable long-term human motions without any task guidance or user input.
The main contributions of this work are as follows: (1) We address the dynamics mismatch in motion imitation by introducing the idea of RFC which can be readily integrated into RL-based humanoid control frameworks. (2) We propose a dual-policy control framework to synthesize multi-modal long-term human motions without the need for task guidance or user input. (3) Extensive experiments show that our approach outperforms state-of-the-art methods in terms of learning speed and motion quality. It also enables imitating highly agile motions like ballet dance that evade prior work. With
RFC and dual-policy control, we present the ﬁrst humanoid control method that successfully learns from a large-scale human motion dataset (Human3.6M) and generates diverse long-term motions. 2