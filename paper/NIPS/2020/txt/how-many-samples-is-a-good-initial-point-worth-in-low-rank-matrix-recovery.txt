Abstract
Given a sufﬁciently large amount of labeled data, the non-convex low-rank matrix recovery problem contains no spurious local minima, so a local optimization algorithm is guaranteed to converge to a global minimum starting from any initial guess. However, the actual amount of data needed by this theoretical guarantee is very pessimistic, as it must prevent spurious local minima from existing anywhere, including at adversarial locations. In contrast, prior work based on good initial guesses have more realistic data requirements, because they allow spurious local minima to exist outside of a neighborhood of the solution. In this paper, we quantify the relationship between the quality of the initial guess and the corresponding reduction in data requirements. Using the restricted isometry constant as a surrogate for sample complexity, we compute a sharp “threshold” number of samples needed to prevent each speciﬁc point on the optimization landscape from becoming a spurious local minimum. Optimizing the threshold over regions of the landscape, we see that for initial points around the ground truth, a linear improvement in the quality of the initial guess amounts to a constant factor improvement in the sample complexity. 1

Introduction
A perennial challenge in non-convex optimization is the possible existence of bad or spurious critical points and local minima, which can cause a local optimization algorithm like gradient descent to slow down or get stuck. Several recent lines of work showed that the effects of non-convexity can be tamed through a large amount of diverse and high quality training data [17, 1, 9, 3, 18, 12]. Concretely, these authors showed that, for classes of problems based on random sampling, spurious critical points and local minima become progressively less likely to exist with the addition of each new sample.
After a sufﬁciently large number of samples, all spurious local minima are eliminated, so any local optimization algorithm is guaranteed to converge to the globally optimal solution starting from an arbitrary, possibly random initial guess.
This notion of a global guarantee—one that is valid starting from any initial point—is considerably stronger than what is needed for empirical success to be observed [8]. For example, the existence of a spurious local minimum may not pose an issue if gradient descent does not converge towards it. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(1) (2)
However, a theoretical guarantee is no longer possible, as starting the algorithm from the spurious local minimum would result in failure [22]. As a consequence, these global guarantees tend to be pessimistic, because the number of samples must be sufﬁciently large to eliminate spurious local minima everywhere, even at adversarial locations. By contrast, the weaker notion of a local guarantee [11, 10, 15, 19, 5, 7, 20, 13]—one that is valid only for a speciﬁed set of initial points—is naturally less conservative, as it allows spurious local minima to exist outside of the speciﬁed set.
In this paper, we provide a unifying view between the notions of the global and local guarantees by quantifying the relationship between the sample complexity and the quality of the initial point.
We restrict our attention to the matrix sensing problem, which seeks to recover a rank-r positive semideﬁnite matrix M ∗ = ZZ T ∈ Rn×n with Z ∈ Rn×r from m sub-Gaussian linear measurements of the form
· · · by solving the following non-convex optimization problem: b ≡ A(ZZ T ) ≡ [(cid:104)A1, M ∗(cid:105) (cid:104)Am, M ∗(cid:105)]T min
X∈Rn×r fA(X) ≡ (cid:13) (cid:13)A (cid:0)XX T − ZZ T (cid:1)(cid:13) 2 (cid:13)
= m (cid:88) i=1 (cid:0)(cid:10)Ai, XX T (cid:11) − bi (cid:1)2
.
We characterize a sharp “threshold” on the number of samples m needed to prevent each speciﬁc point on the optimization landscape from becoming a spurious local minimum. While the threshold is difﬁcult to solve, we derive a lower-bound in closed-form based on spurious critical points, and show that it constitutes a sharp lower-bound on the original threshold of interest. The lower-bound reveals a simple geometric relationship: a point X is more likely to be a local minimum if the column spaces of X and Z are close to orthogonal. Optimizing the closed-form lower-bound over regions of the landscape, we show that for initial points close to the ground truth, a constant factor improvement of the initial point amounts to a constant factor reduction in the number of samples needed to guarantee recovery. 2