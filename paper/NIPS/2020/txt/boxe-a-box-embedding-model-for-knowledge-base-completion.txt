Abstract
Knowledge base completion (KBC) aims to automatically infer missing facts by exploiting information already present in a knowledge base (KB). A promising approach for KBC is to embed knowledge into latent spaces and make predictions from learned embeddings. However, existing embedding models are subject to at least one of the following limitations: (1) theoretical inexpressivity, (2) lack of support for prominent inference patterns (e.g., hierarchies), (3) lack of support for
KBC over higher-arity relations, and (4) lack of support for incorporating logical rules. Here, we propose a spatio-translational embedding model, called BoxE, that simultaneously addresses all these limitations. BoxE embeds entities as points, and relations as a set of hyper-rectangles (or boxes), which spatially characterize basic logical properties. This seemingly simple abstraction yields a fully expressive model offering a natural encoding for many desired logical properties. BoxE can both capture and inject rules from rich classes of rule languages, going well beyond individual inference patterns. By design, BoxE naturally applies to higher-arity
KBs. We conduct a detailed experimental analysis, and show that BoxE achieves state-of-the-art performance, both on benchmark knowledge graphs and on more general KBs, and we empirically show the power of integrating logical rules. 1

Introduction
Knowledge bases (KBs) are fundamental means for representing, storing, and processing information, and are widely used to enhance the reasoning and learning capabilities of modern information systems. KBs can be viewed as a collection of facts of the form r(e1, . . . , en), which represent a relation r between the entities e1, . . . , en, and knowledge graphs (KGs) as a special case, where all the relations are binary (i.e., composed of two entities). KBs such as YAGO [24], NELL [26],
Knowledge Vault [9], and Freebase [2] contain millions of facts, and are increasingly important in academia and industry, for applications such as question answering [3], recommender systems [39], information retrieval [44], and natural language processing [45].
KBs are, however, highly incomplete, which makes their downstream use more challenging. For instance, 71% of individuals in Freebase lack a connection to a place of birth [42]. Knowledge base completion (KBC), aiming at automatically inferring missing facts in a KB by exploiting the already present information, has thus become a focal point of research. One prominent approach for KBC is to learn embeddings for entities and relations in a latent space such that these embeddings, once learned from known facts, can be used to score the plausibility of unknown facts.
Currently, the main embedding approaches for KBC are translational models [4, 34], which score facts based on distances in the embedding space, bilinear models [36, 46, 1], which learn embeddings that factorize the truth tensor of a knowledge base, and neural models [7, 31, 27], which score facts using dedicated neural architectures. Each of these models suffer from limitations, most of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
which are well-known. Translational models, for instance, are theoretically inexpressive, i.e., cannot provably ﬁt an arbitrary KG. Furthermore, none of these models can capture simple sets of logical rules: even capturing a simple relational hierarchy goes beyond the current capabilities of most existing models [13]. This also makes it difﬁcult to inject background knowledge (i.e., schematic knowledge), in the form of logical rules, into the model to improve KBC performance. Additionally, existing KBC models are primarily designed for KGs, and thus do not naturally extend to KBs with higher-arity relations, involving 3 or more entities, e.g., DegreeFrom(Turing, PhD, Princeton)
[10], which hinders their applicability. Higher-arity relations are prevalent in modern KBs such as
Freebase [41], and cannot always be reduced to a KG without loss of information [10]. Despite the rich landscape for KBC, no existing model currently offers a solution to all these limitations.
In this paper, we address these problems by encoding relations as explicit regions in the embedding space, where logical properties such as relation subsumption and disjointness can naturally be analyzed and inferred. Speciﬁcally, we present BoxE, a spatio-translational box embedding model, which models relations as sets of d−dimensional boxes (corresponding to classes), and entities as d−dimensional points. Facts are scored based on the positions of entity embeddings with respect to relation boxes. Our contributions can be summarized as follows: – We introduce BoxE and show that this model achieves state-of-the-art performance on both knowledge graph completion and knowledge base completion tasks across multiple datasets. – We show that BoxE is fully expressive, a ﬁrst for translation-based models, to our knowledge. – We comprehensively analyze the inductive capacity of BoxE in terms of generalized inference patterns and rule languages, and show that BoxE can capture a rich rule language. – We prove that BoxE additionally supports injecting a rich language of logical rules, and empirically show on a subset of NELL [26], that this can signiﬁcantly improve KBC performance.
All proofs for theorems, as well as experimental details, can be found as an appendix in the long version of this paper. 2 Knowledge Base Completion: Problem, Properties, and Evaluation
In this section, we deﬁne knowledge bases and the problem of knowledge base completion (KBC).
We also give an overview of standard approaches for evaluating KBC models.
Consider a relational vocabulary, which consists of a ﬁnite set E of entities and a ﬁnite set R of relations. A fact (also called atom) is of the form r(e1, . . . , en), where r ∈ R is an n-ary relation, and ei ∈ E are entities. A knowledge base (KB) is a ﬁnite set of facts, and a knowledge graph (KG) is a KB with only binary relations. In KGs, facts are also known as triples, and are of the form r(eh, et), with a head entity eh and a tail entity et. Knowledge base completion (KBC) (resp., knowledge graph completion (KGC)) is the task of accurately predicting new facts from existing facts in a KB (resp.,
KG). KBC models are analyzed by means of (i) an experimental evaluation on existing benchmarks, (ii) their model expressiveness, and (iii) the set of inference patterns that they can capture.
Experimental evaluation. To evaluate KBC models empirically, true facts from the test set of a KB and corrupted facts, generated from the test set, are used. A corrupted fact is obtained by replacing one of the entities in a fact from the KB with a new entity: given a fact r(e1, . . . , ei, . . . , en) from the
KB, a corrupted fact is a fact r(e1, . . . , e(cid:48) i, . . . , en) that does not occur in the training, validation, or test set. KBC models deﬁne a scoring function over facts, and are optimized to score true facts higher than corrupted facts. KBC performance is evaluated using metrics [4] such as mean rank (MR), the average rank of facts against their corrupted counterparts, mean reciprocal rank (MRR), their average inverse rank (i.e., 1/rank), and Hits@K, the proportion of facts with rank at most K.
Expressiveness. A KBC model M is fully expressive if, for any given disjoint sets of true and false facts, there exists a parameter conﬁguration for M such that M accurately classiﬁes all the given facts. Intuitively, a fully expressive model can capture any knowledge base conﬁguration, but this does not necessarily correlate with inductive capacity: fully expressive models can merely memorize training data and generalize poorly. Conversely, a model that is not fully expressive can fail to ﬁt its training set properly, and thus can underﬁt. Hence, it is important to develop models that are jointly fully expressive and capture prominent and common inference patterns. 2
Inference patterns. Inference patterns are a common means to formally analyze the generalization ability of KBC systems. Brieﬂy, an inference pattern is a speciﬁcation of a logical property that may exist in a KB, which, if learned, enables further principled inferences from existing KB facts.
One well-known example inference pattern is symmetry, which speciﬁes that when a fact r(e1, e2) holds, then r(e2, e1) also holds. If a model learns a symmetry pattern for r, then it can automatically predict facts in the symmetric closure of r, thus providing a strong inductive bias. We present some prominent inference patterns in detail in Section 5, and also in Table 1. Intuitively, inference patterns captured by a model serve as an indication of its inductive capacity. 3