Abstract
We develop a new approach to obtaining high probability regret bounds for online learning with bandit feedback against an adaptive adversary. While existing ap-proaches all require carefully constructing optimistic and biased loss estimators, our approach uses standard unbiased estimators and relies on a simple increasing learning rate schedule, together with the help of logarithmically homogeneous self-concordant barriers and a strengthened Freedman’s inequality.
Besides its simplicity, our approach enjoys several advantages. First, the obtained high-probability regret bounds are data-dependent and could be much smaller than the worst-case bounds, which resolves an open problem asked by Neu [31].
Second, resolving another open problem of Bartlett et al. [12] and Abernethy and
Rakhlin [1], our approach leads to the ﬁrst general and efﬁcient algorithm with a high-probability regret bound for adversarial linear bandits, while previous methods are either inefﬁcient or only applicable to speciﬁc action sets. Finally, our approach can also be applied to learning adversarial Markov Decision Processes and provides the ﬁrst algorithm with a high-probability small-loss bound for this problem. 1

Introduction
Online learning with partial information in an adversarial environment, such as the non-stochastic
Multi-armed Bandit (MAB) problem [10], is by now a well-studied topic. However, the majority of work in this area has been focusing on obtaining algorithms with sublinear expected regret bounds, and these algorithms can in fact be highly instable and suffer a huge variance. For example, it is known that the classic EXP3 algorithm [10] for MAB suffers linear regret with a constant probability (over its internal randomness), despite having nearly optimal expected regret (see [26, Section 11.5,
Note 1]), making it a clearly undesirable choice in practice.
To address this issue, a few works develop algorithms with regret bounds that hold with high probability, including those for MAB [10, 8, 31], linear bandits [12, 1], and even adversarial Markov
Decision Processes (MDPs) [25]. Getting high-probability regret bounds is also the standard way of deriving guarantees against an adaptive adversary whose decisions can depend on learner’s previous actions. This is especially important for problems such as routing in wireless networks (modeled as linear bandits in [11]) where adversarial attacks can indeed adapt to algorithm’s decisions on the ﬂy.
As far as we know, all existing high-probability methods (listed above) are based on carefully constructing biased loss estimators that enjoy smaller variance compared to standard unbiased ones.
While this principle is widely applicable, the actual execution can be cumbersome; for example, the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
scheme proposed in [1] for linear bandits needs to satisfy seven conditions (see their Theorem 4), and other than two examples with speciﬁc action sets, no general algorithm satisfying these conditions was provided.
In this work, we develop a new and simple approach to obtaining high-probability regret bounds that works for a wide range of bandit problems with an adaptive adversary (including MAB, linear bandits, MDP, and more). Somewhat surprisingly, in contrast to all previous methods, our approach uses standard unbiased loss estimators. More speciﬁcally, our algorithms are based on Online Mirror
Descent with a self-concordant barrier regularizer [2], a standard approach with expected regret guarantees. The key difference is that we adopt an increasing learning rate schedule, inspired by several recent works using similar ideas for completely different purposes (e.g., [5]). At a high level, the effect of this schedule magically cancels the potentially large variance of the unbiased estimators.
Apart from its simplicity, there are several important advantages of our approach. First of all, our algorithms all enjoy data-dependent regret bounds, which could be much smaller than the majority of existing high-probability bounds in the form of ˜O(
T ) where T is the number of rounds. As a key example, we provide details for obtaining a particular kind of such bounds called “small-loss” bounds in the form ˜O(
L(cid:63)), where L(cid:63) ≤ T is the loss of the benchmark in the regret deﬁnition. For MAB and linear bandits, our approach also obtains bounds in terms of the variation of the environment in the vein of [23, 33, 37, 17], resolving an open problem asked by Neu [31].
√
√
Second, our approach provides the ﬁrst general and efﬁcient algorithm for adversarial linear bandits (also known as bandit linear optimization) with a high-probability regret guarantee. As mentioned,
Abernethy and Rakhlin [1] provide a general recipe for this task but in the end only show concrete examples for two speciﬁc action sets. The problem of obtaining a general and efﬁcient approach with regret ˜O(
T ) was left open since then. The work of [12] proposes an inefﬁcient but general approach, while the work of [22, 13] develop efﬁcient algorithms for polytopes but with ˜O(T 2/3) regret. We not only resolve this long-standing open problem, but also provide improved data-dependent bounds.
√
√
Third, our approach is also applicable to learning episodic MDPs with unknown transition, adversarial losses, and bandit feedback. The algorithm is largely based on a recent work [25] on the same problem where a high-probability ˜O(
T ) regret bound is obtained. We again develop the ﬁrst algorithm with a high-probability small-loss bound ˜O(
L(cid:63)) in this setting. The problem in fact shares great similarity with the simple MAB problem. However, none of the existing methods for obtaining small-loss bounds for MAB can be generalized to the MDP setting (at least not in a direct manner) as we argue in Section 4. Our approach, on the other hand, generalizes directly without much effort.
√
Techniques. Most new techniques of our work is in the algorithm for linear bandits (Section 3), which is based on the SCRIBLE algorithm from the seminal work [2, 3]. The ﬁrst difference is that we propose to lift the problem from Rd to Rd+1 (where d is the dimension of the problem) and use a logarithmically homogeneous self-concordant barrier of the conic hull of the action set (which always exists) as the regularizer for Online Mirror Descent. The nice properties of such a regularizer lead to a smaller variance of the loss estimators. Equivalently, this can be viewed as introducing a new sampling scheme for the original SCRIBLE algorithm in the space of Rd. The second difference is the aforementioned new learning rate schedule, where we increase the learning rate by a small factor whenever the Hessian of the regularizer at the current point is “large” in some sense.
In addition, we also provide a strengthened version of the Freedman’s concentration inequality for martingales [21], which is crucial to all of our analysis and might be of independent interest.