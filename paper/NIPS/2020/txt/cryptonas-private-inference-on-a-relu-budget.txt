Abstract
Machine learning as a service has given raise to privacy concerns surrounding clients’ data and providers’ models and has catalyzed research in private inference (PI): methods to process inferences without disclosing inputs. Recently, researchers have adapted cryptographic techniques to show PI is possible, however all solutions increase inference latency beyond practical limits. This paper makes the observa-tion that existing models are ill-suited for PI and proposes a novel NAS method, named CryptoNAS, for ﬁnding and tailoring models to the needs of PI. The key insight is that in PI operator latency costs are inverted: non-linear operations (e.g.,
ReLU) dominate latency, while linear layers become effectively free. We develop the idea of a ReLU budget as a proxy for inference latency and use CryptoNAS to build models that maximize accuracy within a given budget. CryptoNAS improves accuracy by 3.4% and latency by 2.4× over the state-of-the-art. 1

Introduction
User privacy has recently emerged as a ﬁrst-order constraint, sparking interest in private inference (PI) using deep learning models: techniques that preserve data and model conﬁdentiality without compromising user experience (i.e., inference accuracy). In response to this concern, highly-secure solutions for PI have been proposed using cryptographic primitives, namely with homomorphic encryption (HE) and secure multi-party computation (MPC). However, both solutions incur substantial slowdown to the point where even state-of-the-art techniques that combine HE and MPC cannot achieve practical inference latency. Most prior work has focused on developing new systems (e.g.,
Cheetah [1]) and security protocols (e.g., MiniONN[2]) for privacy, while little effort has been made to ﬁnd new network architectures tailored to the needs of PI. Realizing practical PI requires a better understanding of latency bottlenecks and new deep learning model optimizations to address them directly.
Prior work on PI [3, 2, 4] has developed cryptographic protocols optimized for common CNN operators. High-performance protocols take a hybrid approach to combine the strengths–and avoid the pitfalls–of different cryptographic methods. Most protocols today use one method for linear (e.g., secret sharing [5] for convolutions) and another for non-linear layers (e.g., Yao’s garbled circuit [6] for ReLUs). Figure 1 compares the cryptographic latency of ReLU and linear layers for the MiniONN
PI protocol [2] (details in Section 2). The data highlights how a layer’s “plaintext” latency has little bearing on its corresponding cryptographic latency: ReLU operations dominate latency, taking up to 10,000× longer to process than convolution layers. Given the inversion of operator latency costs, enabling efﬁcient PI requires developing new models that maximize accuracy while minimizing
ReLU operations, which we refer to as a model’s ReLU budget. This stands in stark contrast to existing neural architecture search (NAS) approaches that focus only on optimizing the number of
ﬂoating point operations (FLOPs).
Based on this insight we propose two optimization techniques: ReLU reduction and ReLU balanc-ing. ReLU reduction describes methods to reduce the ReLU counts starting from a ReLU-heavy baseline network, for which two solutions are developed: ReLU pruning and ReLU shufﬂing. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
ReLU pruning removes unnecessary ReLUs from ex-isting models, speciﬁcally from skip-connection and re-shaping layers of CNNs. ReLU shufﬂing moves the skip connection base such that the forwarded ac-tivations are post-ReLU, eliminating the redundancy.
We show ReLU reduction is effective and can be implemented with negligible accuracy impact, e.g.,
ReLU pruning reduces a model’s ReLU count by up to 5.8× and reduces latency proportionately. ReLU balancing scales the number of channels in the CNN to maximize model capacity constrained by a ReLU budget. We prove formally that the maximum model capacity under a ReLU budget is achieved when con-stant ReLUs per layer are used; in contrast, traditional channel scaling commonly assumes constant
FLOPs per layer [7, 8, 9]. ReLU balancing is most effective for smaller models, which beneﬁt the most from the additional parameters, providing an accuracy improvement of up to 3.75% on the
CIFAR-100 dataset.
Figure 1: PI latency of a convolution and following ReLU layer. The convolution layer has input size 8×W×W with W∈
{8, 16, 32, 64} and ﬁlter size 8×3×3.
Leveraging these optimizations, we propose CryptoNAS: a new and efﬁcient automated search methodology to ﬁnd models that maximize accuracy within a given ReLU budget. CryptoNAS searches over a space of feed-forward CNN architectures with skip connections (referred to as the macro-search space in NAS literature [9, 10]). First, to incorporate constrained ReLU budgets,
CryptoNAS uses ReLU reduction to make skip connections effectively “free” in terms of latency.
Next, the number of channels in the core network, deﬁned as the skip-free layers, are scaled based on
ReLU balancing, which keeps the number of ReLUs per layer constant. Finally, CryptoNAS uses an efﬁcient decoupled search procedure based on the observation that PI latency is dominated by the
ReLUs of the core network, while skips can be freely added to increase accuracy. CryptoNAS uses the ENAS [9] macro search to determine where to add skips, and separately scales the size of the core network to meet the ReLU budget.
This paper makes the following contributions:
• Develop the idea of a ReLU budget, showing how ReLUs are the primary latency bottleneck in PI to motivate the need for new research in ReLU-lean networks.
• Propose optimizations for ReLU-efﬁcient networks. ReLU reduction (ReLU pruning and shufﬂing) reduce network ReLU counts and ReLU balancing (provably) maximizes model capacity per ReLU. Our ReLU reduction technique reduces ReLU cost by over 5× with minimal accuracy impact, while ReLU balancing increases accuracy by 3.75% on a ReLU budget.
• We develop CryptoNAS to automatically and efﬁciently ﬁnd new models tailored for PI.
CryptoNAS decouples core and PI optimizations for search efﬁciency. Resulting networks along the accuracy-latency Pareto frontier outperform prior work with accuracy (latency) savings of 1.21% (2.2×) and 4.01% (1.3×) on CIFAR-10 and CIFAR-100 respectively. 2