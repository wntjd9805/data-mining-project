Abstract
Physical phenomena in the real world are often described by energy-based mod-eling theories, such as Hamiltonian mechanics or the Landau theory, which yield various physical laws. Recent developments in neural networks have enabled the mimicking of the energy conservation law by learning the underlying continuous-time differential equations. However, this may not be possible in discrete time, which is often the case in practical learning and computation. Moreover, other physical laws have been overlooked in the previous neural network models. In this study, we propose a deep energy-based physical model that admits a speciﬁc differ-ential geometric structure. From this structure, the conservation or dissipation law of energy and the mass conservation law follow naturally. To ensure the energetic behavior in discrete time, we also propose an automatic discrete differentiation algorithm that enables neural networks to employ the discrete gradient method. 1

Introduction
Deep neural networks have achieved signiﬁcant results for a variety of real-world tasks such as image processing [23, 46], natural language processing [13], and game playing [40]. Their successes depend on hard-coded prior knowledge, such as translation invariance in image recognition [28] and the manifold hypothesis in data modeling [36]. The prior knowledge guarantees a desirable property of the learned function. The Hamiltonian neural network (HNN) [19] implements the Hamiltonian structure on a neural network and thereby produces the energy conservation law in physics. After its great success, neural networks speciﬁcally designed for physical phenomena have received much attention. They have been intensively extended to various forms, such as the Hamiltonian systems with additional dissipative terms [44].
Meanwhile, most previous studies aimed to model continuous-time differential equations and em-ployed numerical integrators (typically, an explicit Runge–Kutta method) to integrate the neural network models for learning and computing the dynamics [7, 8, 19, 45]. Surprisingly, our numerical experiments reveal that a higher-order numerical integrator with adaptive time-stepping is quite often inferior in performance as compared to a quantitatively lower order but qualitatively superior numerical integrator. This is because higher-order integrators aim to reproduce continuous-time dynamics while practical learning and computation are in discrete time. In this case, the qualitative features that the integrators equipped with could be actually essential. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
From this point of view, this study proposes a deep energy-based discrete-time physical model, which combines neural networks and discrete-time energy-based modeling. The key ingredient is the structure-preserving integrators, in particular, the discrete gradient method along with the newly-developed automatic discrete differentiation. In addition, our framework uniﬁes and also extends the aforementioned previous studies. The main contributions include:
Applicable to general energy-based physical models. Our framework is applicable to general physical phenomena modeled by the energy-based theory, such as Hamiltonian mechanics, the Landau theory, and the phase ﬁeld modeling. Our target class includes a Hamiltonian system composed of position and momentum (a so-called natural system, such as a mass-spring system), a natural system with friction, a physical system derived from free-energy minimization (e.g., phase transitions), and a
Hamiltonian partial differential equation (PDE) (e.g., the Korteweg–de Vries (KdV) equation and the Maxwell equation). All equations can be written as a geometric equation. Most studies have focused on one of the ﬁrst two systems [19, 44, 45] under special conditions [8, 38, 41], or they are too general to model the conservation and dissipation laws [7, 34]. The details of the proposed framework along with the target class of the equations and the geometric aspects are described in
Section 3.1.
Equipping with the laws of physics in discrete time. Previous models inter-polate the discrete-time data using nu-merical integrators for learning and com-puting [8, 19, 38, 41, 44, 45]. The dis-cretization may destroy the geometrical structure from which the laws of physics follow (see the lower part of Fig. 1). Con-versely, our approach, in principle, learns a discrete-time model from the discrete-time data without the time-consuming interpolation and discretization error (see the upper part). Using the discrete gradient, our approach admits the important laws of physics, particularly the energy conservation or dissipation law and the mass conservation law in discrete time.
We demonstrate this property theoretically in Section 3.2 and experimentally in Section 4.
Figure 1: Modeling based on energy-based theories.
Easy-to-use. Our approach is based on the discrete gradient method [15, 18, 33]. Most discrete gradients require the explicit form of the function (see the middle part of Fig. 1); hence, they are unavailable for neural networks (see Appendix A for reference). We propose an automatic discrete differentiation algorithm, which automatically obtains the discrete gradient of the neural networks composed of linear and nonlinear operations. The proposed algorithm can be implemented in a similar way to the current automatic differentiation algorithm [20]; we provide it as a PyTorch library [31]1.
We introduce the detailed algorithm in Section 3.3. 2