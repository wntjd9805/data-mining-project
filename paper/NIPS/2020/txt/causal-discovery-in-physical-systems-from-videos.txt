Abstract
Causal discovery is at the core of human cognition. It enables us to reason about the environment and make counterfactual predictions about unseen scenarios that can vastly differ from our previous experiences. We consider the task of causal discov-ery from videos in an end-to-end fashion without supervision on the ground-truth graph structure. In particular, our goal is to discover the structural dependencies among environmental and object variables: inferring the type and strength of inter-actions that have a causal effect on the behavior of the dynamical system. Our model consists of (a) a perception module that extracts a semantically meaningful and temporally consistent keypoint representation from images, (b) an inference mod-ule for determining the graph distribution induced by the detected keypoints, and (c) a dynamics module that can predict the future by conditioning on the inferred graph. We assume access to different conﬁgurations and environmental conditions, i.e., data from unknown interventions on the underlying system; thus, we can hope to discover the correct underlying causal graph without explicit interventions. We evaluate our method in a planar multi-body interaction environment and scenarios involving fabrics of different shapes like shirts and pants. Experiments demonstrate that our model can correctly identify the interactions from a short sequence of images and make long-term future predictions. The causal structure assumed by the model also allows it to make counterfactual predictions and extrapolate to systems of unseen interaction graphs or graphs of various sizes. Please refer to our project page for additional results: https://yunzhuli.github.io/V-CDN/. 1

Introduction
Causal understanding of the world around us is part of the bedrock of intelligence. This ability enables counterfactual reasoning, which often distinguishes algorithmic models from intelligent behavior in humans. This ability to discover latent causal mechanisms from data poses an important technical question towards building intelligent and interactive systems [1–3]. For instance, Figure 1 shows an example of a multi-body system. While the images may convey the identity and position of balls, the structural causal mechanism is latent. Each pair of balls is connected to each other through an edge (say a spring, a rigid rod, or be free). Further, each edge may have a set of hidden confounders, like the rest length of a spring or the rigid rod, that causally affect the physical interaction behavior. The underlying causal structure and governing functional mechanism may not be apparent if observations, such as images, are implicit measurements of ground-truth variables [4]. Furthermore, they can also vary across different conﬁgurations and scenarios within a domain. Hence, we need few-shot causal discovery algorithms purely from image data.
In a special case, where the entities are all disconnected and the only interactions are of collision-type, there have been a number of models proposed to employ an object-centric formulation in recent 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Causal discovery in physical systems from videos. The left ﬁgure shows balls, connected by invisible physical relations (shown in grey), moving around. Hidden confounding variables like edge type and edge parameters have a causal effect on the behavior of the underlying system. We humans can observe balls, infer the existence and variables on the edges between the balls, and predict the future. Similarly, in the cloth environment shown on the right, we can ﬁnd a reduced-order representation by placing temporally consistent keypoints on the images and determining the causal relationships between them to reﬂect the cloth’s topology. literature to directly predict the future from images [5–7]. In such cases, model discovery may not even be necessary given these solutions. However, these associative models crumble in the face of more complex stationary underlying generative structures such as different types of latent edges and edge mechanisms [8]. Moreover, they are insufﬁcient to capture novel generative structures and make counterfactual predictions at test time.
In this work, we aim to discover the structural causal model (SCM) to predict the future and reason over counterfactuals. To recover an SCM only from images, we need to ﬁrst learn a compact state representation, infer a causal graph among these variables as well as identify hidden confounders,
ﬁnally learn the functional mechanism of dynamics. This is a particularly challenging task in that we only have images and do not have explicit knowledge of the node variables. Furthermore, we neither assume access to ground truth causal graph, nor the hidden confounders and the dynamics that characterize the effect of the physical interactions. In order to tackle this end-to-end causal discovery problem in an unsupervised manner, we learn from datasets that contain episodes generated from different causal graphs but with a shared dynamics model.
Summary of results. The main contributions of this work lie in the one-shot discovery of unseen causal mechanisms in new environments from partially observed visual data in a continuous state space. This entails jointly performing model class estimation, parameter inference, and thereby building a predictive model for new latent structures at test time in a meta-learning framework.
The proposed Visual Causal Discovery Network (V-CDN) consists of three modules for visual perception, structure inference, and dynamics prediction (Figure 2). Speciﬁcally, we train the perception module that extracts unsupervised keypoints from the images to enable node discovery, building upon [9]. The inference module then takes the predicted keypoints and infers the exogenous variables that govern the interactions between each pair of keypoints using graph neural networks.
Conditioned on the inferred graph, the dynamics module learns to predict the future movements of the keypoints. We consider a variety of conﬁgurations and scenarios, which gives us different combinations of variables, i.e., data from unknown interventions on the underlying system. Thus, we can hope to discover the correct underlying causal graph without explicit interventions.
Experiments show that our proposed model is robust to input noise and works well on multi-body interactions with varying degrees of complexity. Notably, our method can facilitate counterfactual predictions and extrapolate to cases with a variable number of objects and scenarios where the underlying interaction graphs are never seen before. Experiments in a fabric environment also demonstrate the generalization ability of our method, where the same model can handle fabrics of different types and shapes, accurately identifying the dependency structure and modeling the underlying dynamics even when state variables are a reduced-order keypoint-based representation of the original system. 2 Visual Causal Discovery in Physical Systems: V-CDN
In this section, we present the details of our model, which extracts structured representations from videos, discovers the causal relationships, infers the hidden confounding variables on the directed 2
Figure 2: Model overview. Visual Causal Discovery Network (V-CDN) consists of three components: (a) a perception module to process the images and extract unsupervised keypoints as the state representation, (b) an inference module that observes the movements of the keypoints and determines the existence of the causal relations as well as the associated hidden confounders, and (c) a dynamics module that predicts the future by conditioning on the current state and the inferred causal summary graph. edges, and then predicts the future. Our model directly learns from raw videos, which recovers the underlying causal graph without any ground truth supervision.
Problem formulation. We consider a dataset of M trajectories observed from a latent generative dynamical system, where each datapoint is generated with unknown interventions on both the underlying causal graph structure and parameters affecting the mechanism. The generative process of each episode follows a causal summary graph [2], Gm = (V 1:T m , Em), m = 1 . . . M , where
V 1:T m contains the subcomponents underlying the system at different time steps and Em, which we assume is invariant over time, denotes the causal relationships between the constituting components.
Speciﬁcally, for each directed edge (vm,i, vm,j) ∈ Em, there are both discrete and continuous hidden confounders denoting the type and parameters of the relationship that determines the computation of the underlying structural causal model (SCM) [10] and affects the behavior of the dynamical system.
We further assume that in the dynamical system, there are no instantaneous edges or edges that go back in time. Note that the causal summary graph may contain cycles, but when spanning over time, the derived causal full time graph is a directed acyclic graph (DAG), as shown in Figure 2.
In this work, we consider the case where we only have access to the data in the form of image sequences, Im = {I 1:T m }, without any knowledge of the ground truth causal structure and the intervention being applied, where I t m is an image of dimension H × W , denoting the data we received at time t of episode m. The goal is to perform a one-shot recovery of the causal summary graph from a short sequence of images and simultaneously learn a shared dynamics model that operates on the identiﬁed graph to make counterfactual predictions into the future. This is a particularly challenging task and our method serves as a ﬁrst step for tackling this problem in an end-to-end fashion using unsupervised intermediate keypoint representations.
Overview of Visual Causal Discovery Network (V-CDN). We aim to ﬁnd a temporally-consistent (and possibly reduced-order) keypoint-based representation from images using a perception module trained in an unsupervised way,
˜V t m = f V
θ (I t m), t = 1, . . . , T, (1)
θ , parameterized by θ, takes raw images as input and outputs a set of keypoints i=1, that reﬂect the constituting components in the
φ , parameterized by φ, that takes the sequence of m = {ot m,i|ot where the function f V in 2-D coordinates, ˜V t m,i ∈ R2}N system. Then, we use an inference module, f E detected keypoints as input and predicts the edge set, ˜Em,
φ ( ˜V 1:T
˜Em = f E m ), (2) where ˜Em = {(om,i, om,j, gm,ij)}. gm,ij includes gd m,ij, denoting the latent discrete and continuous confounders associated with the directed edge from j to i at episode m. ˜V 1:T m and ˜Em together constitute our discovered causal summary graph, conditioned on which, a dynamics module, f D
ψ , parameterized by ψ, aims to predict the state of the keypoints at time T + 1, m,ij and gc
ˆV T +1 m = f D
ψ ( ˜V 1:T m , ˜Em). (3)
By iteratively applying f D
ψ , we are able to make long-term future predictions. 3
θ , the inference module, f E
The perception module, f V
ψ , are shared among all episodes in the dataset consisting of various causal graphs with different discrete and continuous hidden confounders, which enables one-shot adaptation to an unseen graph at test time and allows counterfactual predictions by intervening on the identiﬁed graph and rolling into the future using the dynamics module.
φ , and the dynamics modules, f D
To train the system, we take an unsupervised keypoint detection algorithms [9] as our perception module and train it on the image set, I, for extracting temporally-consistent keypoints. The inference module and the dynamics module are trained together by minimizing the following objective: min
φ,ψ (cid:88) (cid:88) m t
L( ˜V t+1 m , f D
ψ ( ˜V 1:t m , ˜Em)) + λR( ˜Em), (4) where R(·) is a regularizer imposed on the identiﬁed graph, e.g., to encourage sparsity. 2.1 Unsupervised keypoint detection from videos
The perception module’s task is to transform the images into a keypoint representation in an unsuper-vised way. In this work, we leverage the technique developed by Kulkarni et al. [9]. In particular, we use reconstruction loss over the pixels to encourage the keypoints to disperse over the foreground of the image. During training, it takes in a source image I src and a target image I tgt sampled from the dataset, and passes them through a feature extractor f V
θ . The method then uses an operation called transport to construct a new feature map, Φ(I src, I tgt), using a set of local features indicated by the detected keypoints. A reﬁner network takes in the feature map and generates the reconstruction, ˆI tgt. The module optimizes the parameters in the feature extractor, keypoint detector and reﬁner by minimizing a pixel-wise L2 loss, Lrec = (cid:107)I tgt − ˆI tgt(cid:107), using stochastic gradient descent.
ω and a keypoint detector f V
By combining the keypoint-based bottleneck layer and the downstream reconstruction task, the model extracts temporally-consistent keypoints spreading over the foreground of the images. We denote the m), where ˜V t detected keypoints at time t as ˜V t m m,i ∈ R2}N m = {ot m,i|ot (cid:44) f V i=1.
θ (I t 2.2 Graph neural networks as the spatial encoder
We use graph neural networks as a building block to model the interactions between different keypoints and generate object- and relation-centric embeddings. Both the inference and the dynamics modules will have the graph neural networks as a submodule to capture the underlying inductive bias.
Speciﬁcally, for a set of N keypoints, we construct a directed graph G = (V, E), where vertices
V = {oi} represent the information on the keypoints and edges E = {(oi, oj, gij)} represent the directed relation pointing to i from j, where gij denotes the associated edge attributes.
We employ a graph neural network with a similar structure as the Interaction Networks (IN) [11] as our spatial encoder, denoted as φ, to generate the embeddings for the objects and the relations: ({hi}, {hij}) = φ(V, E). 2.3
Inferring the directed edge set of the Causal Summary Graph
After we obtain the keypoints from the images, we use an inference module to discover the edge set of the causal summary graph and infer the parameters associated with the directed edges. The inference module takes the detected keypoints over a small time window within the same episode as input and outputs a posterior distribution over the structure of the graph. More speciﬁcally, we denote the keypoint sequence as ˜V 1:T i=1. Our goal is to predict the distribution of the edge set conditioned on the keypoint sequence using the parameterized inference function, pφ( ˜Em| ˜V 1:T m = {o1:T m,i}N m ) (cid:44) f E
φ ( ˜V 1:T m ).
To achieve our goal, we ﬁrst use a graph neural network, as discussed in Section 2.2, to propagate information spatially for each frame, which gives us both node and edge embeddings for each keypoint at each frame. We then aggregate the embeddings over the temporal dimension for each node and edge using a 1-D convolutional neural network. Another graph neural network takes in the temporal aggregations and predicts a discrete distribution over the edge types, where the ﬁrst edge 4
type denotes “null edge”. Conditioned on a sample from the discrete distribution, the model will then predict the continuous edge parameters. The edge type and edge parameters together constitute the causal summary graph, which determines the existence and the actual mechanism of the interactions between different constituent components.
In particular, we ﬁrst propagate the information spatially by feeding the keypoints through a graph neural network φenc, which gives us node and edge embeddings at each time step, m,i}, {ht (5) where the edge set, ˜E fc, denotes a fully connected graph that contains an edge between each pair of keypoints with the edge attributes being zero. We then aggregate the information over the temporal dimension for each node and edge using 1-D convolutional neural networks (CNN): m,ij}) = φenc( ˜V t m, ˜E fc), ({ht
¯hm,i = CNNobj(h1:T which allows our model to handle input sequences of variable lengths.
Taking in the aggregated node and edge embeddings, we use another graph neural network, φd, that only makes predictions over the edges to predict the categorical distribution over the edge type:
¯hm,ij = CNNrel(h1:T m,ij), m,i), (6)
{gd m,ij} = φd( ¯Vm, ˜E d m), (7) i=1 and ˜E d m = {(¯hm,i, ¯hm,j, ¯hm,ij)|1 ≤ i, j ≤ N, i (cid:54)= j}. The output {gd where ¯Vm = {¯hm,i}N m,ij} represents the probabilistic distribution over the type of each edge. When an edge is classiﬁed as the
ﬁrst type, i.e., gd m,ij = 1 is true, which we denote as “null edge”, it will be removed in subsequent computation and no information will pass through it. Sampling from this discrete distribution is straightforward, but we cannot backpropagate the gradients through this operation. Instead, we employ the Gumbel-Softmax [12, 13] technique, a continuous approximation of the discrete distribution, to get the biased gradients, which makes end-to-end training possible.
Conditioned on the inferred edge type {gd rameter on each one of the edges. For this purpose, we construct another edge set ˜E c
{(¯hm,i, ¯hm,j, ¯hm,ij)|1 ≤ i, j ≤ N, i (cid:54)= j, gd
φc, to predict the continuous parameters:
{gc m,ij}, we would like to predict the continuous pa-m = m,ij (cid:54)= 1}, and use a new graph neural network, m,ij} = φc( ¯Vm, ˜E c m). (8)
We denote the resulting edge set as ˜Em = {(om,i, om,j, gm,ij)|1 ≤ i, j ≤ N, i (cid:54)= j, gd m,ij (cid:54)= 1}, where gm,ij = (gd m,ij), indicating the topology of the causal summary graph with both the type and the continuous parameter of the edge effect. The inferred causal summary graph is then represented as ˜Gm = ( ˜V 1:T m,ij, gc m , ˜Em). 2.4 Future prediction using the forward dynamics module
The dynamics module, f D the current state and the inferred causal graph: pψ( ˆV T +1
ψ as a graph recurrent network, φdy instantiate f D
ψ .
ψ , predicts the future movements of the keypoints by conditioning on m , ˜Em), where we m , ˜Em) (cid:44) f D m | ˜V 1:T
ψ ( ˜V 1:T
Since we are directly operating on the predicted keypoints from the perception module, the detected keypoints contain noise and introduce uncertainty on the actual locations. Hence, in practice, we represent the position in the future steps using a multivariate Gaussian distribution, where we predict both the mean and the covariance matrix of the next state for each keypoint. 2.5 Optimizing the model
The perception module is trained independently using the reconstruction loss, Lrec. To train the inference module and the dynamics module jointly, we instantiate the objective function shown in
Equation 4 by making an analogy to the ELBO objective [14]:
L = E pφ( ˜Em| ˜V 1:T m )[log pψ( ˆV T +1 (9)
For the prior pψ( ˜Em), we assume that each edge is independent and use a factorized distribution over the edge types as the prior, where pψ( ˜Em) = (cid:81) ij pψ( ˜Em,ij). The inference module and the dynamics module are then trained end-to-end using stochastic gradient descent to maximize the objective L. m , ˜Em)] − DKL(pφ( ˜Em| ˜V 1:T m )(cid:107)pψ( ˜Em)), m | ˜V 1:T 5
Figure 3: Unsupervised keypoint detection. The ﬁrst row shows the input images, and the second row shows an overlay between the predicted keypoints and the image. The perception module assigns keypoints over the foreground of the images and consistently tracks the objects over time across different frames. 3 Experiments
The goal of our experimental evaluation is to answer the following questions: (1) Can the model perform one-shot discovery of the causal summary graph and identify the hidden confounders, including both discrete and continuous variables? (2) How well can the model extrapolate to graphs of different sizes that are not seen during training? (3) How well can the learned model facilitate counterfactual prediction via intervening on the identiﬁed summary graph?
Environment. We study our model in two environments: one includes masses, connected by invisible physical constraints, moving around in a 2-D plane, and the other one contains a fabric of various shapes where we are applying forces to deform it over time (Figure 3).
• Multi-Body Interaction. There are 5 balls of different colors moving around. At the beginning of each episode, we sample the invisible physical relations between each pair of balls independently, giving us the ground truth Em that is ﬁxed throughout the episode. For each pair of balls, there is a one-third probability that they are not connected or linked by a rigid rod or a spring. We also sample the continuous parameters for each existing edge and ﬁx them within the episode, e.g., the length of the rigid relation or the rest length of the spring.
• Fabric Manipulation. We set up fabrics of three different types: a shirt, pants, and a towel, where we also vary the shape of the fabrics like the length of the pant leg or the height and width of the towel (Figure 5). We also apply forces on the contour of the fabric to deform and move it around.
Our goal is to produce one single model that can handle fabrics of different types and shapes, instead of training separate models for each one of them. 3.1 Results on unsupervised keypoint detection
We employ the same architecture and training procedure described in [9] to train our perception module, f V
θ . Figure 3 shows some qualitative results. Our perception module can spread the keypoints over the foreground of the image and consistently track the object. Please refer to our project page for video illustrations. 3.2 Discovery of the Causal Summary Graph and the hidden confounders
The inference module, f E
φ , takes in a short sequence of the detected keypoints and aims to discover whether there is a causal relation, i.e., a physical connection, between each pair of keypoints and identiﬁes the hidden confounders like the edge type and the edge parameters. The predicted graph will be conditioned by the dynamics module, f D
ψ , for future prediction. The optimization procedure does not require any supervision on the attributes associated with the edges, which allows us to infer the hidden confounders in an unsupervised way.
In the Multi-Body environment, the perception module accurately tracks the location of balls, which allows us to perform a systematic evaluation of the model’s performance by comparing its prediction with the ground truth causal summary graph used to generate the episodes. Because we are working in an unsupervised regime, where the predicted edge type is in a discrete latent space distinguishing between null edge, spring, and rigid relation, we need to ﬁnd a global one-on-one mapping between 6
Figure 4: Results on discovering the Causal Summary Graph. Shown in (a) and (b), the accuracy of edge-type classiﬁcation increases as the inference module observes more frames, which also effectively decreases the uncertainty, calculated as the entropy of the predicted distribution. As exhibited in (c) and (d), there is a strong correlation between the inferred continuous variable and the ground truth hidden confounder.
Figure 5: Qualitative results on predicting the Causal Summary Graph and the future. Our inference module observes a short sequence of images and performs one-shot discovery of the causal summary graph, which recovers the ground truth graph in the Multi-Body environment and captures the underlying connectivity structures in the Cloth environment. The unﬁlled circles in the right four columns indicate the model’s prediction into the future. We overlay the predicted future keypoints with the truth future for comparison. 7
Figure 6: Results on extrapolating to unseen graphs of different sizes. Our inference module and dynamics module are trained only in environments containing 5 bodies. Thanks to the inductive bias captured by the graph neural networks in our model, it automatically generalizes to scenarios with different numbers of bodies from training. The blue bars in the ﬁgures show the performance on the test set in the same distribution we trained on, and the orange bars illustrate results on extrapolation. Surprisingly, the model has a better performance in environments with 3 and 4 balls, even if the model has never seen them before. the prediction, {gd m}, and the ground truth. We pick the one that gives us the highest accuracy, with the constraint that the ﬁrst type, where there is no information passing through in the subsequent dynamics prediction, always corresponds to null edge. After the mapping, we evaluate the model’s ability to predict the continuous confounder, {gc m}, by computing its correlation with the ground truth physical parameters like rest length of the spring connection.
The results are shown in Figure 4. As the model observes more frames, the classiﬁcation accuracy increases, and the uncertainty decreases, which correlates with our intuition that as we obtain more observations from the environment, we have a better estimate of the exogenous variables that govern the behavior of the system. We also show the comparison with a baseline that is the same as our method except that it does not have the inference module. Our model signiﬁcantly outperforms the baseline, indicating the importance of the correct modeling of the causal mechanism (Figure 6 (d)).
Figure 5 shows some qualitative results, where we include side-by-side comparisons between the identiﬁed causal summary graph and the ground truth.
For the cloth environment, the keypoints on the fabrics act as a reduced-order representation of the original system, where we do not know the ground truth causal summary graph. We encode the action as a 6-dimensional vector: the ﬁrst three are the coordinates of the dragged point, and the other three indicate the movement, which will then be concatenated with the embedding of every keypoint.
As shown in Figure 5, the same inference module produces different causal graphs for different types of fabrics that reﬂect the underlying connectivity patterns, which illustrates the model’s ability to recognize the underlying dependency structure. 3.3 Extrapolation to unseen causal graphs of different sizes
To evaluate our model’s performance on extrapolation, we also create another 4 test sets in the
Multi-Body environment, including 3, 4, 6, and 7 bodies, respectively, for which we need to train separate perception modules to reﬂect the number of moving components. However, the inference module and the dynamics module do not require retraining; instead, they can directly generalize to systems of different numbers of bodies. As shown in Figure 6, the blue bar shows the performance on the test set that has the same number of balls as the training set, while the other bars illustrate the model’s ability to perform extrapolation. Interestingly, for environments with fewer balls, e.g., 3 or 4 balls, even if the model is not directly trained on these scenarios, the performance is yet better. 3.4 Counterfactual prediction and extrapolation on parameter change
In our experiment, we make counterfactual predictions by intervening on the estimated hidden confounders and evaluate how well the model predicts the future by making the same intervention on the ground truth simulator. The estimated confounders are in the latent space, which requires a mapping function to get the corresponding parameters in the original simulator. We use the same mapping as described in Section 3.2 to ﬁnd the corresponding discrete variables, and train a simple linear regressor for transforming the continuous variable. Figure 7 shows the performance on 8
Figure 7: Results on counterfactual prediction. We make counterfactual predictions by intervening on the identiﬁed causal summary graph and evaluate the performance by comparing the predicted future with the original simulator undergoing the same intervention at T + 30. The modeling of the causal mechanism allows it to extrapolate to parameter ranges outside the training distribution. counterfactual predictions, which illustrates our model’s ability to answer “what if” questions and extrapolate to parameter ranges that are outside the training distribution. 4