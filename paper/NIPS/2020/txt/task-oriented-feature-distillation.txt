Abstract
Feature distillation, a primary method in knowledge distillation, always leads to signiﬁcant accuracy improvements. Most existing methods distill features in the teacher network through a manually designed transformation. In this paper, we propose a novel distillation method named task-oriented feature distillation (TOFD) where the transformation is convolutional layers that are trained in a data-driven manner by task loss. As a result, the task-oriented information in the features can be captured and distilled to students. Moreover, an orthogonal loss is applied to the feature resizing layer in TOFD to improve the performance of knowledge distillation. Experiments show that TOFD outperforms other distillation methods by a large margin on both image classiﬁcation and 3D classiﬁcation tasks. Codes have been released in Github3. 1

Introduction
Recently, remarkable achievements have been attained with deep neural networks in all kinds of applications such as nature lan-guage processing [3, 13, 8, 60] and com-puter vision [54, 52, 51]. However, the suc-cess in neural networks is always accom-panied by explosive growth of model pa-rameters and computation, which has lim-ited the deployment of neural networks on edge devices such as mobile phones and em-bedded devices. Various techniques have been proposed to tackle this issue, includ-ing pruning [15, 41, 72, 67, 39], quantiza-tion [35, 46, 11, 50], lightweight model de-sign [23, 49, 27], and knowledge distillation (KD) [22, 53, 70].
Figure 1: Comparison between previous feature distil-lation and task-oriented feature distillation.
Hinton et al. ﬁrst propose the concept of dis-tillation, where a lightweight student model is trained to mimic the softmax outputs (i.e. logit) of an over-parameterized teacher model [22]. Later, abundant feature distillation methods are proposed to encourage the student models to mimic the features of teacher models [53, 62, 66, 7, 20].
Since the features of teacher models have more information than logit, feature distillation enables student models to learn richer information and always leads to more accuracy improvements. As shown in Figure 1, instead of directly learning all the features of the teacher models, most of the feature distillation methods ﬁrst apply a transformation function to the features to convert them
∗Equal contribution
†Corresponding authors 3https://github.com/ArchipLab-LinfengZhang/Task-Oriented-Feature-Distillation 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: A survey of previous feature distillation methods and task-oriented feature distillation.
Method
Transformation
Lost Information
AT [70]
FSP [68]
Jacobian [58]
SVD [26]
Heo et al. [20]
Channelwise Pooling
FSP Matrix
Gradients
SVD Decomposition
Margin ReLU
Channel dims
Spatial dims
Channel dims
Spatial dims
Negative Feature
Task-Oriented
Feature Distillation
Convolutional Layers Non-Task-Oriented
Trained by Task Loss features into an easy-to-distill form and then distill them to students. In this progress, some unimportant information are ﬁltered, as shown in Table 1. However, what still remains unknown is which form of information is the best to distill and which kind of transformation function can extract this form of information.
In this paper, we assume that the task-oriented information is the information which is the most essential to distillation. Based on this assumption, we propose a novel knowledge distillation method named task-oriented feature distillation (short as TOFD). Different from previous feature distillation methods whose transformation functions are manually designed, the transformation function in TOFD is convolutional layers which are trained in a data-driven manner by both distillation loss and the task loss. In the training period of TOFD, several auxiliary classiﬁers are attached at different depths to the backbone layers. Each auxiliary classiﬁer consists of several convolutional layers, a pooling layer and a fully connected layer. They are trained to perform the same task as the whole neural network does. As a result, the auxiliary classiﬁers help to capture the task-oriented information from the whole features in the backbone layers, leading to high-efﬁciency knowledge distillation.
In most situations of knowledge distillation, the features of students and teachers have different widths, heights and channels. Usually, a convolutional layer or a fully connected layer is applied to match their sizes. However, this leads to one problem that some useful information of teachers may be lost in the progress of feature resizing. To address this problem, an orthogonal loss has been introduced in TOFD to regularize the weights of the feature resizing layer. With the property of orthogonality, more supervision from teachers can be exploited in students training.
Sufﬁcient experiments demonstrate that the proposed TOFD achieves consistent and signiﬁcant accuracy boost in various neural networks and datasets. Experiments in ten kinds of neural networks on ﬁve datasets show that TOFD outperforms the state-of-the-art distillation method by a large margin on both images classiﬁcation and 3D classiﬁcation. On average, 5.46%, 1.71%, 1.18%, 1.25% and 0.82% accuracy boost can be observed on CIFAR100, CIFAR10, ImageNet, ModelNet10,
ModelNet40 datasets, respectively. Besides, ablation study and hyper-parameters sensitivity study are also conducted to show the effectiveness and stability of TOFD.
To sum up, the contribution of this paper can be summarized as follows:
• A novel knowledge distillation method named TOFD is proposed to distill the task-oriented information from teachers to students. Auxiliary classiﬁers are utilized to capture the task-oriented information from all features of teachers and students.
• An orthogonal loss is proposed to avoid the information loss of teacher’s supervision in the feature resizing layers.
• Sufﬁcient experiments on ten neural networks and ﬁve datasets are conducted to show the effectiveness of TOFD. Five kinds of knowledge distillation methods are utilized as the comparison experiments. 2