Abstract
Large-scale distributed training of Deep Neural Networks (DNNs) on state-of-the-art platforms is expected to be severely communication constrained.
To overcome this limitation, numerous gradient compression techniques have been proposed and have demonstrated high compression ratios. However, most existing methods do not scale well to large scale distributed systems (due to gradient build-up) and/or fail to evaluate model ﬁdelity (test accuracy) on large datasets. To mitigate these issues, we propose a new compression technique,
Scalable Sparsiﬁed Gradient Compression (ScaleCom), that leverages similarity in the gradient distribution amongst learners to provide signiﬁcantly improved scalability. Using theoretical analysis, we show that ScaleCom provides favorable convergence guarantees and is compatible with gradient all-reduce techniques.
Furthermore, we experimentally demonstrate that ScaleCom has small overheads, directly reduces gradient trafﬁc and provides high compression rates (65-400X) and excellent scalability (up to 64 learners and 8-12X larger batch sizes over standard training) across a wide range of applications (image, language, and speech) without signiﬁcant accuracy loss. 1

Introduction
Over the past decade, DNNs have surpassed traditional Machine Learning models on a wide range of applications including computer vision [1][2], speech [3], and natural language processing (NLP)
[4][5]. As models and datasets have grown in complexity, training times have increased signiﬁcantly
[2][4]. To tackle this challenge, data-parallelism approaches are widely used to accelerate the training of DNN models [6]. In order to scale data-parallelism techniques to more workers while preserving the computational efﬁciency in each worker, it is important to increase the overall batch size proportionally with the number of workers. However, increasing the batch size often leads to a signiﬁcant loss in test accuracy–remedied by a number of recent ideas including increasing the learning rate during the training process as well as a learning rate warm-up procedure [7][8][9]. Using these techniques, large batch size training has been successfully applied to state-of-the-art distributed systems [10][11]. However, increasing evidence seems to suggest that there is a maximum mini-batch size beyond which the number of iterations required to converge increases [12]. Furthermore, driven by recent advances in low-precision arithmetic [13][14][15], there has been a renaissance in the computational capability of deep learning training hardware resulting in accelerator throughputs exceeding 100s of TeraOps/s [16][17][18][19]. This dramatic increase in throughput can cause an imbalance between computation and communication, resulting in large scale training platforms that are severely communication constrained. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To mitigate these communication bottlenecks in DNN training, several gradient compression techniques have been proposed [20][21][22][23]. Most of these techniques exploit error feedback or ‘local memory’ (preserving gradient residues from compression) to demonstrate signiﬁcant compression rates and good convergence properties. However, current error-feedback gradient compression techniques cannot be directly applied to large-scale distributed training. There are two primary challenges. (a) Gradient build-up: As addressed in [24][25][26][27], compressed data can be gathered, but not reduced. This results in a dramatically decreased compression rate as the number of workers increases. (b) Large batch size with scaled learning rate: As shown in [28], for a convex problem, the noise term in the error-feedback gradient increases as the cube of the learning rate (↵3). [29] also shows that the increased learning rate could add large noise for error-feedback gradient compression in non-convex and distributed settings. Thus, scaled learning rates needed for large batch-sized training can signiﬁcantly increase gradient noise and cause performance degradation (or even divergence), particularly for complex models and datasets.
In this paper, we propose a new gradient compression algorithm, ScaleCom, that provides solutions to both of these challenges. ScaleCom provides signiﬁcant compression rates (65-400X) while enabling convergence in large-scale distributed training (64 workers). To the best of our knowledge, this is the
ﬁrst compression algorithm that has been extensively evaluated in large datasets and batch sizes and shown to be fully compatible with conventional all-reduce schemes, as shown in Table 1.
Table 1: Comparing different compressors for error-feedback SGD
Compressor scalability overhead (FLOPs/element) compr. rate empirical exp.
LBe (n) (n) (n) (log(n)) (log(n)) (log p) (sort)a 4 (quasi-sort) (1) (sample based-sort)
Top K[21][30] no no
AdaComp[22] no
DGC[23] yes
PowerSGD[26] gTop-k[27] no no
SketchSGD[24]
ScaleCom (ours) yes a p is mode size. bunless explicit assumption is made. cH(.) is hash function computation and r is rows of sketch table. d include a wide range of applications with large datasets. e large batch size training/scaled learning rate.
O
⇠
O low-rank approximation local top-k merge 2 broadly tested broadly tested broadly tested small datasets up to 6% degrad. transformer broadly testedd
>100X 40-200X 270-600X 40-128X
>100X 40X 65-400X
H(.)
⇤ 3 (chunk-wise sort)
⇠
O
O
O
O
O constant constant r (sketch table)c
⇤ convergence not guaranteedb not guaranteed not guaranteed not guaranteed not guaranteed guaranteed guaranteed 1.1 Challenges and