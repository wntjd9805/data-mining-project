Abstract
The state-of-the-art deep neural networks (DNNs) are vulnerable to adversarial examples with additive random noise-like perturbations. While such examples are hardly found in the physical world, the image blurring effect caused by ob-ject motion, on the other hand, commonly occurs in practice, making the study of which greatly important especially for the widely adopted real-time image processing tasks (e.g., object detection, tracking). In this paper, we initiate the
ﬁrst step to comprehensively investigate the potential hazards of blur effect for
DNN, caused by object motion. We propose a novel adversarial attack method that can generate visually natural motion-blurred adversarial examples, named motion-based adversarial blur attack (ABBA). To this end, we ﬁrst formulate the kernel-prediction-based attack where an input image is convolved with kernels in a pixel-wise way, and the misclassiﬁcation capability is achieved by tuning the kernel weights. To generate visually more natural and plausible examples, we further propose the saliency-regularized adversarial kernel prediction, where the salient region serves as a moving object, and the predicted kernel is regularized to achieve visual effects that are natural. Besides, the attack is further enhanced by adaptively tuning the translations of object and background. A comprehensive evaluation on the NeurIPS’17 adversarial competition dataset demonstrates the effectiveness of
ABBA by considering various kernel sizes, translations, and regions. The in-depth study further conﬁrms that our method shows more effective penetrating capability to the state-of-the-art GAN-based deblurring mechanisms compared with other blurring methods. We release the code to https://github.com/tsingqguo/ABBA.

Introduction 1
Deep neural networks (DNN) have been widely applied in various vision perception tasks (e.g., object recognition, segmentation, scene understanding), permeating many aspects of our daily life, such as autonomous driving, robotics, video surveillance, photo taking, etc. However, the state-of-the-art DNNs are still vulnerable to adversarial examples. Extensive previous works are proposed (e.g., FGSM [1], BIM [2], MI-FGSM [3], C&W [4]), to mislead the DNN through additive noise perturbations that could be obtained by optimizing the adversarial objectives. To be imperceptible to human, Lp-norm plays an important role in such attacks, conﬁning the perturbation noise to be small.
However, the random noise-like perturbation often does not pose imminent threats to the camera systems, which does not usually occur in natural environment. Thus, some recent attempts [2] were made to physically fashion adversarial examples such as by putting up stickers or printed patterns
∗Xiaofei Xie and Lei Ma are corresponding authors (xfxie@ntu.edu.sg, malei@ait.kyushu-u.ac.jp). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
on the physical stop sign, etc. Again, these artifacts are often intentionally prepared for adversarial attacks, which are not ‘naturally’ found in the real-world environment either.
While the blurring effect caused by object motion commonly occurs in practical image perception systems, the potential hazards of motion blur effect to the DNN are largely untouched so far. Motion blur naturally happens during the exposure time of image capturing. When an object moves at a relatively high speed, all information of the object during the image capture process is integrated, constituting a blur-like image along a relative moving direction. Compared with other kinds of image blur (e.g., defocus blur caused by using unsuitable camera focus), motion blur is directly related to the motion of object and camera, whose effect cannot be easily removed by adjusting the camera’s setting. As a result, motion blur almost coexists with the camera and potentially posts serious effects on DNN perception-based systems. However, up to present, there are limited studies discuss how motion blur affects the DNN perception tasks. It is not even clear whether and what kinds of motion blur can systematically mislead a DNN.
In this paper, we initiate the ﬁrst step to comprehen-sively investigate the blur effects to DNNs from the adversarial attack perspective, where systematic mo-tion blur-based adversarial example discovery would be an important step towards further DNN enhance-ment. In particular, we propose a new type of adversar-ial attack, termed motion-based adversarial blur attack (ABBA), which can generate visually natural and plau-sible motion-blurred adversarial examples. We ﬁrst for-mulate the kernel-prediction-based attack where an in-put image is convolved with kernels in a pixel-wise way, and the misclassiﬁcation ability is guided by system-atically tuning the kernel weight. In order to generate more natural motion blurred examples, we also propose the saliency-regularized adversarial kernel prediction, where the salient region serves as a moving object, and the predicted kernel is regularized to achieve visually natural effects. Besides, our method could easily adjust the blur effects of different exposure time during the image capturing in the real world, by adaptively tuning translations of object and background.
Figure 1: Four adversarial examples of MIFGSM [3], TIM-IFGSM [5] and our ABBA. MIFGSM and TIMIFGSM pro-duces apparent noise on all four cases. Our ABBA generates visually natural motion blur. All adversarial images fool the
Inception-v3 model.
We perform comprehensive evaluation on the effectiveness of the proposed ABBA, benchmarked against various noise-based attacks on both attack success rates and transferability. The main contributions of this work can be summarized as follows. (cid:182) To the best of our knowledge, we make the very ﬁrst attempt to investigate kernel-based adversarial attack. (cid:183) We propose a motion-based adversarial blur attack as a new type of attack mode, to be added to the adversarial attack family. (cid:184) In order to produce more visually plausible blur attack, we introduce a saliency regularizer that forces consistent blur patterns within the boundary of the objects (or background in some cases). (cid:185)
Compared with the state-of-the-art (SOTA) additive-noise based adversarial attacks and common blur techniques, our proposed method achieves better attack success rate and transferability. (cid:186)
Furthermore, our proposed method has demonstrated higher penetration capability against the SOTA
GAN-based deblur mechanism, compared to normal image motion blur.