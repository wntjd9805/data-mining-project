Abstract
Ensemble learning is widely used in applications to make predictions in complex decision problems—for example, averaging models ﬁtted to a sequence of samples bootstrapped from the available training data. While such methods offer more accurate, stable, and robust predictions and model estimates, much less is known about how to perform valid, assumption-lean inference on the output of these types of procedures. In this paper, we propose the jackknife+-after-bootstrap (J+aB), a procedure for constructing a predictive interval, which uses only the available bootstrapped samples and their corresponding ﬁtted models, and is therefore “free” in terms of the cost of model ﬁtting. The J+aB offers a predictive coverage guarantee that holds with no assumptions on the distribution of the data, the nature of the ﬁtted model, or the way in which the ensemble of models are aggregated—at worst, the failure rate of the predictive interval is inﬂated by a factor of 2. Our numerical experiments verify the coverage and accuracy of the resulting predictive intervals on real data. 1

Introduction
Ensemble learning is a popular technique for enhancing the performance of machine learning algorithms. It is used to capture a complex model space with simple hypotheses which are often signiﬁcantly easier to learn, or to increase the accuracy of an otherwise unstable procedure [see 14, 27, 29, and references therein].
While ensembling can provide substantially more stable and accurate estimates, relatively little is known about how to perform provably valid inference on the resulting output. Particular challenges arise when the data distribution is unknown, or when the base learner is difﬁcult to analyze. To consider a motivating example, suppose that each observation consists of a vector of features X ∈ Rp and a real-valued response Y ∈ R. Even in an idealized scenario where we might be certain that the data follow a linear model, it is still not clear how we might perform inference on a bagged prediction obtained by, say, averaging the Lasso predictions on multiple bootstrapped samples of the data.
To address the problem of valid statistical inference for ensemble predictions, we propose a method for constructing a predictive conﬁdence interval for a new observation that can be wrapped around existing ensemble prediction methods. Our method integrates ensemble learning with the recently proposed jackknife+ [2]. It is implemented by tweaking how the ensemble aggregates the learned predictions. This makes the resulting integrated algorithm to output an interval-valued prediction that, when run at a target predictive coverage level of 1 − α, provably covers the new response value at least 1 − 2α proportion of the time in the worst case, with no assumptions on the data beyond independent and identically distributed samples.
∗Department of Statistics, The University of Chicago, Chicago, IL 60637, USA byolkim@uchicago.edu
†H. Milton Stewart School of Industrial & Systems Engineering, Georgia Institute of Technology, Atlanta,
GA 30332, USA cxu310@gatech.edu
‡Department of Statistics, The University of Chicago, Chicago, IL 60637, USA rina@uchicago.edu 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our main contributions are as follows.
• We propose the jackknife+-after-bootstrap (J+aB), a method for constructing predictive conﬁdence intervals that can be efﬁciently wrapped around an ensemble learning algorithm chosen by the user.
• We prove that the coverage of a J+aB interval is at worst 1 − 2α for the assumption-free theory. This lower bound is non-asymptotic, and holds for any sample size and any distribution of the data.
• We verify that the empirical coverage of a J+aB interval is actually close to 1 − α. 2