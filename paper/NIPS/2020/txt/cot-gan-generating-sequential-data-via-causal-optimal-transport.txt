Abstract
We introduce COT-GAN, an adversarial algorithm to train implicit generative mod-els optimized for producing sequential data. The loss function of this algorithm is formulated using ideas from Causal Optimal Transport (COT), which combines classic optimal transport methods with an additional temporal causality constraint.
Remarkably, we ﬁnd that this causality condition provides a natural framework to parameterize the cost function that is learned by the discriminator as a robust (worst-case) distance, and an ideal mechanism for learning time dependent data distributions. Following Genevay et al. (2018), we also include an entropic penal-ization term which allows for the use of the Sinkhorn algorithm when computing the optimal transport cost. Our experiments show effectiveness and stability of
COT-GAN when generating both low- and high-dimensional time series data. The success of the algorithm also relies on a new, improved version of the Sinkhorn divergence which demonstrates less bias in learning. 1

Introduction
Dynamical data are ubiquitous in the world, including natural scenes such as video and audio data, and temporal recordings such as physiological and ﬁnancial traces. Being able to synthesize realistic dynamical data is a challenging unsupervised learning problem and has wide scientiﬁc and practical applications. In recent years, training implicit generative models (IGMs) has proven to be a promising approach to data synthesis, driven by the work on generative adversarial networks (GANs) [23].
Nonetheless, training IGMs on dynamical data poses an interesting yet difﬁcult challenge. On one hand, learning complex spatial structures of static images has already received signiﬁcant effort within the research community. On the other hand, temporal dependencies are no less complicated since the dynamical features are strongly correlated with spatial features. Recent works, including
[16, 36, 39, 41, 44], often tackle this problem by separating the model or loss into static and dynamic components.
In this paper, we examine training dynamic IGMs for sequential data. We introduce a new adversarial objective that builds on optimal transport (OT) theory, and constrains the transport plans to respect causality: the probability mass moved to the target sequence at time t can only depend on the source sequence up to time t, see [2, 8]. A reformulation of the causality constraint leads to a new adversarial training objective, in the spirit of [21] but tailored to sequential data. In addition, we demonstrate that optimizing the original Sinkhorn divergence over mini-batches causes biased parameter estimation, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
and propose the mixed Sinkhorn divergence which mitigates this problem. Our new framework,
Causal Optimal Transport GAN (COT-GAN), outperforms existing methods on a wide range of datasets from traditional time series to high dimensional videos. 2