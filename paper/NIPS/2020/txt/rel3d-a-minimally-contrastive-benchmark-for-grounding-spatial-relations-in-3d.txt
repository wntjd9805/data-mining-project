Abstract
Understanding spatial relations (e.g., “laptop on table”) in visual input is important for both humans and robots. Existing datasets are insufﬁcient as they lack large-scale, high-quality 3D ground truth information, which is critical for learning spatial relations. In this paper, we ﬁll this gap by constructing Rel3D: the ﬁrst large-scale, human-annotated dataset for grounding spatial relations in 3D. Rel3D enables quantifying the effectiveness of 3D information in predicting spatial relations on large-scale human data. Moreover, we propose minimally contrastive data collection—a novel crowdsourcing method for reducing dataset bias. The 3D scenes in our dataset come in minimally contrastive pairs: two scenes in a pair are almost identical, but a spatial relation holds in one and fails in the other. We empirically validate that minimally contrastive examples can diagnose issues with current relation detection models as well as lead to sample-efﬁcient training. Code and data are available at https://github.com/princeton-vl/Rel3D. 1

Introduction
Spatial relations such as “laptop on table” are ubiquitous in our environment, and understanding them is vital for both humans and intelligent agents like robots. As humans, we use spatial relations for perceiving and building knowledge of the surrounding environment and supporting our daily activities such as moving around and ﬁnding objects [1, 2]. Spatial relations play an important role in communication for describing to others where objects are located [3–6].
Likewise, for robots, understanding spatial relations is necessary for navigation [7], object manipu-lation [8, 9], and human-robot interaction [10, 11]. For a robot to complete a task such as “put the bottle in the box”, it is necessary to ﬁrst understand the relation “bottle in the box”.
A spatial relation is deﬁned as a subject-predicate-object triplet, where predicate describes the spatial conﬁguration between subject and object, such as painting-over-bed. Understanding spatial relations may seem an easy task at ﬁrst glance, and a plausible model could be a set of hand-crafted rules for each predicate based on the spatial properties of subject and object, like their relative position [12–15].
However, just like many other rule-based systems, this approach works for a small set of carefully curated examples, but fails for wider real-world examples consistent with human judgment [16].
The failure results from the rich and complex semantics of spatial predicates, which depend on various factors beyond relative positions. For example, they depend on frames of reference (Is “left to the car” relative to the observer or relative to the car?), object properties (For “in front of the house”, what is the frontal side of a house? Is there still a frontal side if the object were a tree?), and also commonsense (“painting over bed” is not touching while “blanket over bed” is). With all these subtleties, any set of hand-crafted rules is likely to be inadequate, so researchers have applied data-driven approaches to learn spatial relations from visual data [17, 18, 11, 7, 8, 19, 20]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Some samples from Rel3D. Rel3D contains pairs of minimally contrastive scenes: two scenes in a pair are almost identical, but a spatial relation holds in one while fails in other.
Benchmarking spatial relations in 3D. Benchmark datasets have been proposed for training and evaluating a system’s understanding of spatial relations [6, 19, 20]. However, existing datasets either have limited scale and variety [6, 19] or contain human-annotated relations only on 2D images [19, 20].
Prior research suggests that 3D information may play a critical role in spatial relations [20, 21, 11, 9].
With only 2D images, the model has to implicitly learn the mapping to 3D, which is itself an unsolved problem. Instead of developing an accurate 3D understanding of spatial relations, models tend to utilize superﬁcial 2D cues [20]. More importantly, in robotics, 3D information is often readily available, making it valuable to study spatial relation recognition using 3D information.
In this work, we propose Rel3D—the ﬁrst large-scale dataset of human-annotated spatial relations in 3D (Fig. 1). It consists of spatial relations situated in synthetic 3D scenes, making it possible to extract rich and accurate geometric and semantic information, including depth, segmentation mask, object positions, poses, and scales. The scenes in Rel3D are created by crowd workers on Amazon
Mechanical Turk (Fig. 2). Workers manipulate objects according to instructions, and independent workers are asked to verify whether a given spatial relation holds in the 3D scene. We choose to use synthetic scenes because they give us complete control over various factors, e.g., objects, positions, camera poses, etc. Such ﬂexibility is important for datasets specializing in spatial relations, enabling us to study the grounding of spatial relations with respect to these factors.
Rel3D is the ﬁrst to provide rich geometric and semantic information in 3D for the task of spa-tial relation understanding. It enables the exploration of problems that were out of reach before.
Speciﬁcally, we study how ground truth object 3D positions, scales, and poses (including frontal and upright orientation) can be used to train neural networks to predict spatial relations with high accuracy. Further, our experiments suggest that estimating 3D conﬁgurations is a promising step towards understanding spatial relations in 2D images.
Reducing dataset bias through minimally contrastive examples. Besides promoting 3D under-standing of spatial relations, Rel3D also addresses a fundamental issue with existing datasets—biases in language and 2D spatial cues. Despite prior efforts in mitigating bias [22, 23, 20], state-of-the-art models achieve superﬁcially high performance by exploiting bias, without much understanding [20].
We propose minimally contrastive data collection for spatial relations—a novel crowdsourcing method that signiﬁcantly reduces dataset bias. Rel3D consists of minimally contrastive scenes: pairs of scenes with minimal differences, so that the spatial relation holds in one but fails in the other (Fig. 1). The task for the model is to classify whether the given relation holds. This minimally contrastive construction makes it unlikely for a model to exploit bias, including language bias (“cup on table” is more likely than not) and other spurious correlations with factors like the color of the background or the texture of an object. If a model attempts to associate the background with a relation, it cannot succeed in both instances of a minimally contrastive pair with identical backgrounds.
Through our experiments, we demonstrate how Rel3D can be used as an effective tool for diagnosing models that rely heavily on 2D bias as well as Language bias for making predictions. We show that a simple 2D baseline outperforms more sophisticated models, implying that these models lack 3D understanding for recognizing spatial relations. Further, we empirically demonstrate that training models on minimally contrastive examples leads to better sample efﬁciency.
Our contributions are as follows:
• We construct Rel3D: the ﬁrst large-scale dataset of human-annotated spatial relations in 3D
• Rel3D is the ﬁrst benchmark for spatial relation understanding that contains minimally contrastive examples, alleviating bias and leading to sample-efﬁcient training 2
• With Rel3D, we demonstrate how 3D positions, scales, and poses of objects can be used to predict spatial relations with high accuracy 2