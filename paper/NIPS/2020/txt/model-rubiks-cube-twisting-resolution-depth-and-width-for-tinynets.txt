Abstract
To obtain excellent deep neural architectures, a series of techniques are carefully designed in EfﬁcientNets. The giant formula for simultaneously enlarging the resolution, depth and width provides us a Rubik’s cube for neural networks. So that we can ﬁnd networks with high efﬁciency and excellent performance by twisting the three dimensions. This paper aims to explore the twisting rules for obtaining deep neural networks with minimum model sizes and computational costs.
Different from the network enlarging, we observe that resolution and depth are more important than width for tiny networks. Therefore, the original method, i.e. the compound scaling in EfﬁcientNet is no longer suitable. To this end, we summarize a tiny formula for downsizing neural architectures through a series of smaller models derived from the EfﬁcientNet-B0 with the FLOPs constraint. Experimental results on the ImageNet benchmark illustrate that our TinyNet performs much better than the smaller version of EfﬁcientNets using the inversed giant formula. For instance, our TinyNet-E achieves a 59.9% Top-1 accuracy with only 24M FLOPs, which is about 1.9% higher than that of the previous best MobileNetV3 with similar computational cost. Code will be available at https://github.com/huawei-noah/ ghostnet/tree/master/tinynet_pytorch, and https://gitee.com/mindspore/ mindspore/tree/master/model_zoo/research/cv/tinynet. 1

Introduction
Deep convolutional neural networks (CNNs) have achieved great success in many visual tasks, such as image recognition [21, 12, 9], object detection [36, 28, 8], and super-resolution [20, 46, 40]. In the past few decades, the evolution of neural architectures has greatly increased the performance of deep learning models. From LeNet [22] and AlexNet [21] to modern ResNet [12] and EfﬁcientNet [43], there are a number of novel components including shortcuts and depth-wise convolution. Neural architecture search [61, 27, 52, 54, 44] also provides more possibility of network architectures. These various architectures have provided candidates for a large variety of real-world applications.
To deploy the networks on mobile devices, the depth, the width and the image resolution are continuously adjusted to reduce memory and latency. For example, ResNet [12] provides models with different number of layers, and MobileNet [15, 38] changes the number of channels (i.e. the width of neural network) and image resolution for different FLOPs. Most of existing works only scale one of the three dimensions – resolution, depth, and width (denoted as r, d, and w). Tan and
Le explore the EfﬁcientNet [43], which enlarges CNNs with a compound scaling method. The great
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
success made by EfﬁcientNets bring a Rubik’s cube to the deep learning community, i.e. we can twist it for better neural architectures using some pre-deﬁned formulas. For example, the EfﬁcientNet-B7 is derivate from the B0 version by uniformly increasing these three dimensions. Nevertheless, the original EfﬁcientNet and some improved versions [2, 50] only discuss the giant formula, the rules for effectively downsize the baseline model has not been fully investigated.
The straightforward way for designing tiny networks is to apply the experience used in Efﬁcient-Net [43]. For example, we can obtain an EfﬁcientNet-B−1 with a 200M FLOPs (ﬂoating-point operations). Since the giant formula is explored for enlarging networks, this naive strategy could not perfectly ﬁnd a network with the highest performance. To this end, we randomly generate 100 models by twisting the three dimensions (r, d, w) from the baseline EfﬁcientNet-B0. FLOPs of these models are less than or equal to that of the baseline. It can be found in Figure 1, the performance of best models is about 2.5% higher than that of models obtained using the inversed giant formula of
EfﬁcientNet (green line) with different FLOPs.
In this paper, we study the relationship between the accuracy and the three dimensions (r, d, w) and explore a tiny formula for the model Ru-bik’s cube. Firstly, we ﬁnd that resolution and depth are more important than width for retain-ing the performance of a smaller neural architec-ture. We then point out that the inversed giant formula, i.e. the compound scaling method in
EfﬁcientNets is no longer suitable for design-ing portable networks for mobile devices, due to the reduction on the resolution is relatively large. Therefore, we explore a tiny formula for the cube through massive experiments and ob-servations. In contrast to the giant formula in
EfﬁcientNet that is handcrafted, the proposed scheme twists the three dimensions based on the observation of frontier models. Speciﬁcally, for the given upper limit of FLOPs, we calculate the optimal resolution and depth exploiting the tiny formula, i.e. the Gaussian process regression on frontier models. The width of the resulting model is then determined according to the FLOPs constraint and previously obtained r and d. The proposed tiny formula for establishing TinyNets is simple yet effective. For instance, TinyNet-A achieves a 76.8% Top-1 accuracy with about 339M
FLOPs but the EfﬁcientNet-B0 with the similar performance needs about 387M FLOPs. In addition,
TinyNet-E achieves a 59.9% Top-1 accuracy with only 24M FLOPs, being 1.9% higher than the previous best MobileNetV3 with similar FLOPs. To our best knowledge, we are the ﬁrst to study how to generate tiny neural networks via simultaneously twisting resolution, depth and width. Besides the validations on EfﬁcientNet, our tiny formula can be directly applied on ResNet architectures to obtain small but effective neural networks.
Figure 1: Accuracy v.s. FLOPs of the models randomly donwsized from EfﬁcientNet-B0. Five models using the inversed giant formula (green) and the frontier models (red) with both higher per-formance and lower FLOPs are highlighted. 2