Abstract
Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies, either at the image or the feature level, improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e. the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing memory requirements, however, leads to diminishing returns in terms of performance.
We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning.
Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-ﬂy with a minimal computational overhead. We exhaustively ablate our approach on linear classiﬁcation, object detection, and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.
Project page: https://europe.naverlabs.com/mochi 1

Introduction
Contrastive learning was recently shown to be a highly effective way of learning visual repre-sentations in a self-supervised manner [8, 21].
Pushing the embeddings of two transformed ver-sions of the same image (forming the positive pair) close to each other and further apart from the embedding of any other image (negatives) using a contrastive loss, leads to powerful and transferable representations. A number of recent studies [10, 17, 39] show that carefully hand-crafting the set of data augmentations applied to images is instrumental in learning such represen-Figure 1: MoCHi generates synthetic hard nega-tives for each positive (query). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
tations. We suspect that the right set of transformations provides more diverse, i.e. more challenging, copies of the same image to the model and makes the self-supervised (proxy) task harder. At the same time, data mixing techniques operating at either the pixel [41, 49, 50] or the feature level [40] help models learn more robust features that improve both supervised and semi-supervised learning on subsequent (target) tasks.
In most recent contrastive self-supervised learning approaches, the negative samples come from either the current batch or a memory bank. Because the number of negatives directly affects the contrastive loss, current top contrastive approaches either substantially increase the batch size [8], or keep large memory banks. Approaches like [31, 46] use memories that contain the whole training set, while the recent Momentum Contrast (or MoCo) approach of He et al. [21] keeps a queue with features of the last few batches as memory. The MoCo approach with the modiﬁcations presented in [10] (named MoCo-v2) currently holds the state-of-the-art performance on a number of target tasks used to evaluate the quality of visual representations learned in an unsupervised way. It is however shown [8, 21] that increasing the memory/batch size leads to diminishing returns in terms of performance: more negative samples does not necessarily mean hard negative samples.
In this paper, we argue that an important aspect of contrastive learning, i.e. the effect of hard negatives, has so far been neglected in the context of self-supervised representation learning. We delve deeper into learning with a momentum encoder [21] and show evidence that harder negatives are required to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing approaches, we propose hard negative mixing, i.e. feature-level mixing for hard negative samples, that can be computed on-the-ﬂy with a minimal computational overhead. We refer to the proposed approach as MoCHi, that stands for "(M)ixing (o)f (C)ontrastive (H)ard negat(i)ves".
A toy example of the proposed hard negative mixing strategy is presented in Figure 1; it shows a t-SNE [29] plot after running MoCHi on 32-dimensional random embeddings on the unit hypersphere.
We see that for each positive query (red square), the memory (gray marks) contains many easy negatives and few hard ones, i.e. many of the negatives are too far to contribute to the contrastive loss.
We propose to mix only the hardest negatives (based on their similarity to the query) and synthesize new, hopefully also hard but more diverse, negative points (blue triangles).
Contributions. a) We delve deeper into a top-performing contrastive self-supervised learning method [21] and observe the need for harder negatives; b) We propose hard negative mixing, i.e. to synthesize hard negatives directly in the embedding space, on-the-ﬂy, and adapted to each positive query. We propose to both mix pairs of the hardest existing negatives, as well as mixing the hardest negatives with the query itself; c) We exhaustively ablate our approach and show that employing hard negative mixing improves both the generalization of the visual representations learned (measured via their transfer learning performance), as well as the utilization of the embedding space, for a wide range of hyperparameters; d) We report competitive results for linear classiﬁcation, object detection and instance segmentation, and further show that our gains over a state-of-the-art method are higher when pre-training for fewer epochs, i.e. MoCHi learns transferable representations faster. 2