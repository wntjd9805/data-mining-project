Abstract
Direct optimization [25, 37] is an appealing framework that replaces integration with optimization of a random objective for approximating gradients in models with discrete random variables [21]. A? sampling [24] is a framework for opti-mizing such random objectives over large spaces. We show how to combine these techniques to yield a reinforcement learning algorithm that approximates a policy gradient by ﬁnding trajectories that optimize a random objective. We call the resulting algorithms direct policy gradient (DirPG) algorithms. A main beneﬁt of
DirPG algorithms is that they allow the insertion of domain knowledge in the form of upper bounds on return-to-go at training time, like is used in heuristic search, while still directly computing a policy gradient. We further analyze their properties, showing there are cases where DirPG has an exponentially larger probability of sampling informative gradients compared to REINFORCE. We also show that there is a built-in variance reduction technique and that a parameter that was previously viewed as a numerical approximation can be interpreted as controlling risk sensi-tivity. Empirically, we evaluate the effect of key degrees of freedom and show that the algorithm performs well in illustrative domains compared to baselines. 1

Introduction
Many problems in machine learning reduce to learning a probability distribution (or policy) over sequences of discrete actions so as to maximize a downstream utility function. Examples include generating text sequences to maximize a task-speciﬁc metric like BLEU and generating action sequences in reinforcement learning (RL) to maximize expected return. A main challenge is that evaluating the objective requires integrating over all possible sequences, which is intractable, and thus approximations like REINFORCE are needed [40] to learn these policies.
A line of work has emerged in recent years that allows replacing integration and sampling with optimization of noisy objective functions [29, 12, 38, 24, 21]. While this does not immediately remove the intractability of the integration problem, casting the problem in terms of optimization gives access to a different toolbox of ideas, which can provide new perspectives and methods for these hard problems. For example, Maddison et al. provide a new way of leveraging bounds from convex duality for use in sampling from continuous probability distributions [24]. Our aim in this work is the analog for reinforcement learning: we will replace the integral that is typically approximated by
REINFORCE with an alternative that requires only optimization over a noisy objective function. The beneﬁt is that this opens up techniques from heuristic search for use in reinforcement learning (e.g., variants of A? search) and provides an opportunity to express domain knowledge, all while retaining the conceptual simplicity that comes from optimizing a standard expected return objective function. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(1) (2)
The resulting algorithm is quite different from standard approaches to computing a policy gradient, but it estimates the same quantity up to one ﬁnite difference approximation. We provide a comprehensive analysis of the new algorithm from both theoretical and empirical perspectives. In total, this work provides a new perspective on computing a policy gradient and expands the toolbox of techniques and domain knowledge that can be used to tackle this fundamental problem. 2