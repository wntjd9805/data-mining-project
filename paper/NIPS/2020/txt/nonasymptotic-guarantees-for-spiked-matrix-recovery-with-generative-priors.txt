Abstract
Many problems in statistics and machine learning require the reconstruction of a rank-one signal matrix from noisy data. Enforcing additional prior information on the rank-one component is often key to guaranteeing good recovery performance.
One such prior on the low-rank component is sparsity, giving rise to the sparse principal component analysis problem. Unfortunately, there is strong evidence that this problem suffers from a computational-to-statistical gap, which may be
In this work, we study an alternative prior where the low-rank fundamental. component is in the range of a trained generative network. We provide a non-asymptotic analysis with optimal sample complexity, up to logarithmic factors, for rank-one matrix recovery under an expansive-Gaussian network prior. Speciﬁcally, we establish a favorable global optimization landscape for a nonlinear least squares objective, provided the number of samples is on the order of the dimensionality of the input to the generative model. This result suggests that generative priors have no computational-to-statistical gap for structured rank-one matrix recovery in the
ﬁnite data, nonasymptotic regime. We present this analysis in the case of both the
Wishart and Wigner spiked matrix models. 1

Introduction
In this paper we study the problem of estimating a spike vector y(cid:63) ∈ Rn from data Y consisting of a rank-1 matrix perturbed with random noise. In particular, the following random models for Y will be considered.
• The Spiked Wishart Model in which Y ∈ RN ×n is given by:
Y = u y(cid:63) where σ > 0, u ∼ N (0, In) and Z are independent and Zij are i.i.d. from N (0, 1). (cid:124) + σZ, (1)
• The Spiked Wigner Model in which Y ∈ Rn×n is given by: (2) where ν > 0, H ∈ Rn×n is drawn from a Gaussian Orthogonal Ensemble GOE(n), i.e.
Hii ∼ N (0, 2/n) for all 1 ≤ i ≤ n and Hij = Hji ∼ N (0, 1/n) for 1 ≤ j < i ≤ n.
Y = y(cid:63)y(cid:63) (cid:124) + νH 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Spiked random matrices have been extensively studied in recent years as they serve as a mathematical model for many statistical inverse problems such as PCA [34, 2, 21, 58], synchronization over graphs [1, 8, 33] and community detection [43, 20, 47]. They are, moreover, connected to the rank-1 case of other linear inverse problems such as matrix sensing and matrix completion under RIP-like assumptions on the measurements operator [12, 65].
In the high-dimensional/low signal-to-noise ratio regimes, it is fundamental to leverage additional prior information on the low-rank component in order to obtain consistent estimates of y(cid:63). Recent works, however, have discovered that some priors give rise to gaps between what is statistically-theoretically optimal and can be achieved with unbounded computational resources, and what instead can be achieved with polynomial-time algorithms. A prominent example is represented by the Sparse
PCA problem in which the vector y(cid:63) in (1) is taken to be sparse (see next section and [9, 37] for surveys of recent approaches).
In this paper we study the spiked random matrix models (1) and (2), where the prior information on the planted signal y(cid:63) comes from a learned generative network. In particular, we assume that a generative neural network G : Rk → Rn with k < n, has been trained on a data set of spikes, and the unknown spike y(cid:63) ∈ Rn lies on the range of G, i.e. we can write y(cid:63) = G(x(cid:63)) for some x(cid:63) ∈ Rk.
As a mathematical model for the trained G, we consider a d-layer feed forward network of the form:
G(x) = relu(Wd . . . relu(W2relu(W1x)) . . . ) (3) with weight matrices Wi ∈ Rni×ni−1 and relu(x) = max(x, 0) is applied entrywise. We furthermore assume that the network is expansive, i.e. n = nd > nd−1 > · · · > n0 = k, and the weights have Gaussian entries. This modeling assumption was introduced in [29], and additionally it and its variants were used in [31, 26, 41, 25, 57]. See Section 1.1 for justiﬁcations of this model.
Generative priors have been shown to close a computational-to-statistical gap in the Compressive
Phase Retrieval problem. With a sparsity prior the information-theoretically optimal sample complex-ity is proportional to the sparsity level s of the signal, on the other hand the best known algorithms (convex methods [28, 39, 48], iterative thresholding [15, 60, 64], etc.) require a sample complexity proportional to s2 for stable recovery, a barrier which might not be resolvable by polynomial-time algorithms [10]. Under the generative prior (3), [26] has shown that, compressive phase retrieval is possible via gradient descent over a nonlinear objective with sample complexity proportional (up to log factors) to the underlying signal dimensionality k. This result suggests that it may be possible to use generative priors to close other computational-to-statistical gaps such as for models (1) and (2). Indeed, recently [7] considered these low-rank models and the generative network prior (3) and shows that in the asymptotic limit k, n, N → ∞ with n/k = O(1) and N/n = O(1), an
Approximate-Message Passing algorithm achieves the statistical information-theoretic lower bound and no computational-to-statistical gap is present.
This paper analyzes the low-rank matrix models (1) and (2) under the generative network prior (3).
The contributions of this paper are as follows. We analyze the global landscape of a natural least-square loss over the range of the generative network demonstrating its benign optimization geometry.
Our result provide further evidences for the claim that rank-one matrix recovery does not have computational-to-statistical gaps when enforcing a generative prior in the non-asymptotic ﬁnite-data regime. This provides a second problem for which generative priors have closed such gaps in a non-asymptotic case. We further corroborate these ﬁndings by proposing a (sub)gradient algorithm which, as shown by our numerical experiments, is able to recover the sought spike with optimal sample complexity. This paper, therefore, strengthens the case for generative networks as priors for statistical inverse problems, not only because of their ability to learn natural signal priors, but also because of their capacity to lead to statistically optimal polynomial-time algorithms and zero computational-to-statistical gaps. 1.1 Problem formulation and main results
We consider the rank-one matrix recovery problem under a deep generative prior. We assume that the signal spike lies in the range of the generative prior y(cid:63) = G(x(cid:63)). To estimate y(cid:63), we propose to ﬁrst
ﬁnd an estimate ˆx of the latent variable x(cid:63) and then use G(ˆx) ≈ y(cid:63). We thus consider the following 2
minimization problem1: where: min x∈Rk f (x) := 1 4 (cid:107)G(x)G(x)(cid:124) − M (cid:107)2
F . (4)
• for the Wishart model (1) we take M = ΣN − σ2In with ΣN = Y (cid:124)Y /N .
• for the Wigner model (2) we take M = Y .
Despite the objective function (4) being nonconvex and nonsmooth, we show that it enjoys a favorable global optimization geometry for Gaussian weight matrices {Wi}d i=1. The informal version of our main results for the two spiked models is given below.
Theorem 1 (Informal). Let y(cid:63) = G(x(cid:63)) for a given a generative network G : Rk → Rn as in (3). Assume that each layer is sufﬁciently expansive, i.e. ni+1 = Ω(ni log ni), and the weights are
Gaussian. Consider the minimization problem (4) and assume that up to factors dependent on the number of layers d:
• for the Wishart model: (cid:112)k log n /N (cid:46) 1,
• for the Wigner model: ν(cid:112)k log n /n (cid:46) 1.
With high probability:
A. for any nonzero point x ∈ Rk outside two small neighborhoods of x(cid:63) and −ρdx(cid:63) with 0 < ρd ≤ 1, the objective function (4) has a direction of strict descent given almost everywhere by the gradient of f ;
B. the objective function values near −ρdx(cid:63) are larger than those near x(cid:63), while x = 0 is a local maximum;
C. for any point x in the small neighborhood around of x(cid:63), up to polynomials in d:
• for the Wishart model:
• for the Wigner model: (cid:107)G(x) − y(cid:63)(cid:107)2 (cid:46) (cid:114) k log n
N
, (cid:107)G(x) − y(cid:63)(cid:107)2 (cid:46) ν (cid:114) k log n n
. (5) (6)
Our main result characterizes the global optimization geometry of the problem (4) for a network with an expansive architecture and Gaussian weights. Even though the objective function in (4) is a piecewise-quartic polynomial, we show that outside two small neighborhoods around x(cid:63) and a negative multiple of it, there are no other spurious local minima or saddles, and every nonzero point has a strict linear descent direction. The point x = 0 is a local maximum and a neighborhood around x(cid:63) contains the global minimum of f .
We note, moreover, that for any point x in the “benign neighborhood” of x(cid:63), the reconstruction error (cid:107)G(x) − y(cid:63)(cid:107) has information-theoretically optimal rates (5) and (6) corresponding (up to log factors) to the best achievable even in the simple case of a k-dimensional subspace prior. This implies that for the Wishart model the number of samples required to estimate y(cid:63) scales like the latent dimension k which corresponds to the intrinsic degrees of freedom of the signal y(cid:63). Similarly for the Wigner model this implies that enforcing the generative network prior leads to a reduction of the noise by a factor of k/n.
Furthermore we observe that the direction of descent guaranteed by the theorem are almost everywhere given by the gradient of the objective function f . Our result, therefore, suggests that spiked matrix recovery with a deep (random) generative network prior can be solved rate-optimally by simply 1Under the deterministic conditions on the generative network (see below for details), it was shown in [29] that G is invertible and therefore there exists a unique x(cid:63) that satisﬁes y(cid:63) = G(x). 3
minimizing over the range of the network via simple and computationally tractable algorithms such as gradient descent methods. For small enough step sizes, the iterates of these methods would converge to one of the two neighborhoods where the gradients are small (identiﬁed in Theorem 1A), and avoiding the bad neighborhood of −ρdx(cid:63) can be done by exploiting the knowledge of the properties of the loss function (described in Theorem 1B) as done in Algorithm 1 below and shown in the numerical experiments. Finally, proving a convexity-like property of the “benign neighborhood” around x(cid:63) would ensure that the iterates will remain in this neighborhood and gradient descent will converge to a point with optimal error-rates (5) and (6). Formally proving the optimality and polynomial runtime of a gradient method for spiked matrix recovery is left for future work.
Regarding the Gaussian weight assumption, we observe that there is empirical evidence that the distribution of the weights of deep neural networks have properties consistent with those of Gaussian matrices [4]. Moreover, these observations have been used in advancing the theoretical understanding of deep network trained in supervised setting and in particular their ability to preserve the metric structure of the data [24]. The randomness assumption has been further used by [3] to show that autoencoders with random weights can be learned in polynomial time. More recently, a series of works (see for example [40, 23, 51, 44, 17]) have been dedicated to theoretical guarantees for training deep neural networks in the close-to-random regime of the Neural Tangent Kernel [32]. Finally, as for the case of compressed sensing in which the analysis of the random setting has led to considerable understanding of the problem as well as tangible practical innovations, we hope that the analysis of the random setting for deep generative networks will provide insights and generate novel developments in the ﬁeld of statistical inverse problems.
We ﬁnally observe that signal recovery problems where multiple signal structures hold simultaneously, e.g. low-rankness and sparsity, have been notoriously difﬁcult, leading to no tractable algorithms at optimal sample complexity (see the next section for further details). Consequently, one might expect that enforcing low-rankness and generative priors would be comparably hard. In this work, we show instead that this combination of structural priors is not inherently difﬁcult. This would motivate practitioners to invest in building and using generative priors, as those studied in this paper, in contexts where other priors have been traditionally used with suboptimal theoretical guarantees or empirical performance. 2