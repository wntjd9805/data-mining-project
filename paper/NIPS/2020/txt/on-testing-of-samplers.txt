Abstract
Given a set of items F and a weight function wt : F (cid:55)→ (0, 1), the problem of sam-pling seeks to sample an item proportional to its weight. Sampling is a fundamental problem in machine learning. The daunting computational complexity of sampling with formal guarantees leads designers to propose heuristics-based techniques for which no rigorous theoretical analysis exists to quantify the quality of generated distributions. This poses a challenge in designing a testing methodology to test whether a sampler under test generates samples according to a given distribution.
Only recently, Chakraborty and Meel (2019) designed the ﬁrst scalable veriﬁer, called Barbarik, for samplers in the special case when the weight function wt is constant, that is, when the sampler is supposed to sample uniformly from F . The techniques in Barbarik, however, fail to handle general weight functions.
The primary contribution of this paper is an afﬁrmative answer to the above chal-lenge: motivated by Barbarik, but using different techniques and analysis, we design Barbarik2, an algorithm to test whether the distribution generated by a sampler is ε-close or η-far from any target distribution.
In contrast to black-box sampling techniques that require a number of samples proportional to |F| ,
Barbarik2 requires only ˜O(tilt(wt, ϕ)2/η(η − 6ε)3) samples, where the tilt is the maximum ratio of weights of two satisfying assignments. Barbarik2 can handle any arbitrary weight function. We present a prototype implementation of Barbarik2 and use it to test three state-of-the-art samplers. 1

Introduction
Motivated by the success of statistical techniques, automated decision-making systems are increas-ingly employed in critical domains such as medical [19], aeronautics [33], criminal sentencing [20], and military [2]. The potential long-term impact of the ensuing decisions has led to research in the correct-by-construction design of AI-based decision systems. There has been a call for the design of randomized and quantitative formal methods [35] to verify the basic building blocks of the modern
AI systems. In this work, we focus on one such core building block: constrained sampling.
Given a set of constraints ϕ over a set of variables X and a weight function wt over assignments to
X, the problem of constrained sampling is to sample a satisfying assignment σ of ϕ with probability proportional to wt(σ). Constrained sampling is a fundamental problem that encapsulates a wide range of sampling formulations [24, 23, 12, 30, 14]. For example, wt can be used to capture a given
∗The accompanying tool, available open source, can be found at https://github.com/meelgroup/barbarik. The
Appendix is available in the accompanying supplementary material.
†The authors decided to forgo the old convention of alphabetical ordering of authors in favor of a randomized ordering, denoted by r(cid:13). The publicly veriﬁable record of the randomization is available at https://www.aeaweb.org/journals/policies/random-author-order/search with conﬁrmation code: GH8VZdz4mQIh. For citation of the work, authors request that the citation guidelines by AEA for random author ordering be followed. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
prior distribution often represented implicitly through probabilistic models, and ϕ can be used to capture the evidence arising from the observed data, then the problem of constrained sampling models the problem of sampling from the resulting posterior distribution.
The problem of constrained sampling is computationally hard and has witnessed a sustained interest from theoreticians and practitioners, resulting in the proposal of several approximation techniques. Of these, Monte Carlo Markov Chain(MCMC)-based methods form the backbone of modern sampling techniques [3, 7]. The runtime of these techniques depends on the length of the random walk, and the
Markov chains that require polynomial walks are called rapidly mixing Markov chains. Unfortunately, for most distributions of practical interest, it is infeasible to design rapidly mixing Markov chains [26], and the practical implementations of such techniques have to resort to the usage of heuristics that violate theoretical guarantees. The developers of such techniques, often and rightly so, strive to demonstrate their effectiveness via empirical behavior in practice [6].
The need for the usage of heuristics to achieve scalability is not restricted to just MCMC methods but is widely observed for other methods such as simulated annealing [29], variational methods [18], and hashing-based techniques [12, 23, 13, 32]. Consequently, a fundamental problem for the designers of sampling techniques is: how can one efﬁciently test whether a given technique samples from the desired distribution? Most of the existing approaches rely on the computations of statistical metrics such as variation distance and KL-divergence by drawing samples and perform hypothesis testing with a preset p-value. Sound computations of statistical metrics require a large number of samples that is proportional to the support of the posterior distribution [4, 36], which is prohibitively large; it is not uncommon for the distribution support to be signiﬁcantly larger than 270. Consequently, the existing approaches tend to estimate the desired quantities using a fraction of the required samples, and such estimates are often without the required conﬁdence. The usage of unsound metrics may lead to unsound conclusions, as demonstrated by a recent study where the usage of unsound metric would lead one to conclude that two samplers were indistinguishable (it is worth mentioning that the authors of the study clearly warn the reader about the unsoundness of the underlying metrics) [21].
The researchers in the sub-ﬁeld of property testing within theoretical computer science have analyzed the sample complexity of testing under different models of samplers and computation. The resulting frameworks have not witnessed widespread adoption to practice due to a lack of samplers that can precisely ﬁt the models under which results are obtained. In recent work, Chakraborty and Meel [10], building on the concepts developed in the condition sampling model (rf. [1]), designed the ﬁrst practical algorithmic procedure, called Barbarik, that can rigorously test whether a given sampler samples from the uniform distribution using a constant number of samples, assuming that the given sampler is subquery-consistent (see Deﬁnition 9). Empirically, Barbarik was shown to be able to distinguish samplers that were indistinguishable in prior studies based on unsound metrics. While
Barbarik made signiﬁcant progress, it is marred by its ability to handle only the uniform distribution.
Therefore, one wonders: Can we design an algorithmic framework to test whether the distribution generated by a given sampler is close to a desired (but arbitrary) posterior distribution of interest?
This paper’s primary contribution is the ﬁrst efﬁcient algorithmic framework, Barbarik2, to test whether the distribution generated by a sampler is ε-close or η-far from the desired distribution speciﬁed by the set of constraints ϕ and a weight function wt. In contrast to the statistical techniques that require an exponential or sub-exponential number of samples for samplers whose support can be represented by n bits, the number of samples required by Barbarik2 depends on the tilt of the distribution, where tilt is deﬁned as the maximum ratio of non-zero weights of two solutions of ϕ.
Like Barbarik, the key technical idea of Barbarik2 sits at the intersection of property testing and formal methods and uses ideas from conditional sampling and employs chain formulas. However, the key algorithmic framework of Barbarik2 differs signiﬁcantly from Barbarik, and, as demonstrated, the proof of its correctness and sample complexity requires an entirely new set of technical arguments.
Given access to an ideal sampler A, Barbarik2 accepts every sampler that is ε-close to A while its ability to reject a sampler that is η-far from A assumes that the sampler under test is subquery consistent. Since Barbarik2 assumes access to an ideal sampler, one might wonder if a tester such as Barbarik2 is needed when we already have access to an ideal sampler. Since sampling is computationally intractable, it is almost always the case that an ideal sampler A is quite slow and one would prefer to use some other efﬁcient sampler G instead of A, if G can be certiﬁed to be close to A.
To demonstrate the practical efﬁciency of Barbarik2, we developed a prototype implementation in
Python and performed an experimental evaluation with several samplers. While our framework 2
does not put a restriction on the representation of wt, we perform empirical validation with weight distributions corresponding to log-linear models, a widely used class of distributions. Our empirical evaluation shows that Barbarik2 returns ACCEPT for the samplers with formal guarantees but returns
REJECT for other samplers that are without formal guarantees. Our ability to reject samplers provides evidence in support of our assumption of subquery consistency of samplers. We believe our formal-ization of testing of samplers and the design of the algorithmic procedure, Barbarik2, contributes to the design of randomized formal methods for veriﬁed AI, a principle argued by Seshia et al [35]. 2 Notations and Preliminaries
A Boolean variable is denoted by a lowercase letter. For a Boolean formula ϕ, the set of variables appearing in ϕ, called the support of ϕ, is denoted by Supp(ϕ). An assignment σ ∈ {0, 1}|Supp(ϕ)| to the variables of ϕ is a satisfying assignment or witness if it makes ϕ evaluate to 1. We denote the set of all satisfying assignments of ϕ as Rϕ. For S ⊆ Supp(ϕ), we use σ↓S to indicate the projection of σ over the set of variables in S. And we denote by Rϕ↓S the set {σ↓S | σ ∈ Rϕ}.
Deﬁnition 1 (Weight Function). For a set S of Boolean variables, a weight function wt : {0, 1}|S| → (0, 1) maps each assignment to some weight.
Deﬁnition 2 (Sampler). A sampler G(ϕ, S, wt, τ ) is a randomized algorithm that takes in a Boolean formula ϕ, a weight function wt, a set S ⊆ Supp(ϕ) and a positive integer τ and outputs τ independent samples from Rϕ↓S . For brevity of notation we will omit arguments ϕ, S, wt, τ , whenever may sometimes refer to a sampler as G(ϕ) or simply, G.
For any σ ∈ {0, 1}|S| the probability of the sampler G outputting σ is denoted by pG(ϕ, S, σ) (or pG(ϕ, σ) when the set S in question is clear from the context).
We use DG(ϕ,S) to represent the distribution induced by G(ϕ, S) on Rϕ↓S . When the set S is understood from the context we will denote DG(ϕ,S) by DG(ϕ).
Deﬁnition 3 (Ideal Sampler). For a weight function wt, a sampler A(ϕ, S, τ ) is called an ideal wt(σ(cid:48)) . In the rest sampler w.r.t. weight function wt if for all σ ∈ Rϕ↓S : pA(ϕ, S, wt, σ) = of the paper, A(·, ·, ·, ·) denotes the ideal sampler. When wt(σ) = 1
|Rϕ| then the ideal sampler is called a uniform sampler. wt(σ)
σ(cid:48)∈Rϕ↓S (cid:80)
Deﬁnition 4 (Tilt). For a Boolean formula ϕ and weight function wt, we deﬁne tilt(wt, ϕ) = max
σ1,σ2∈Rϕ wt(σ1) wt(σ2) .
Our goal is to design a program that can test the quality of a sampler with respect to an ideal sampler.
We use two different notions of distance of the sampler from the ideal sampler.
Deﬁnition 5 (ε-closeness and η-farness). A sampler G is ε-multiplicative-close (or simply ε-close) to an ideal sampler A, if for all ϕ and all σ ∈ Rϕ, we have (1 − ε)pA(ϕ, σ) ≤ pG(ϕ, σ) ≤ (1 + ε)pA(ϕ, σ). For a formula ϕ, a sampler G(ϕ) is η-(cid:96)1-far (or simply η-far) from the ideal sampler A(ϕ), if (cid:80)
|pA(ϕ, σ) − pG(ϕ, σ)| ≥ η
σ∈Rϕ
It is worth emphasising that the asymmetry in the notions of ε-close and η-far stems from the availability of practical samplers. Since the available off-the-shelf solvers with theoretical guarantees provide the guarantee of ε-closeness, we are interested in accepting a sampler that is ε-close [24, 23, 12, 14]. On the other hand, we would like to be more forgiving to the samplers without guarantees and would like to reject only if they are η-far in (cid:96)1 distance, a notion more relaxed than multiplicative closeness.
Deﬁnition 6 ((ε, η, δ)-tester for samplers). A (ε, η, δ)-tester for samplers is a randomized algorithm that takes a sampler G, an ideal sampler A, a tolerance parameter ε, an intolerance parameter η, a guarantee parameter δ and a CNF formula ϕ such that (1) If G(ϕ) is ε-close to A(ϕ), then the tester returns ACCEPT with probability at least (1 − δ), and (2) If G(ϕ) is η-far from A(ϕ) then the tester returns REJECT with probability at least (1 − δ). 3
2.1 Chain Formula
A crucial component in our algorithm is the chain formula. Chain formulas, introduced in [15], are a special class of Boolean formulas. Given a positive integer k and m, chain formulas provide an efﬁcient construction of a Boolean formula ψk,m with exactly k satisfying assignments with (cid:100)log(k)(cid:101) ≤ m variables. We employ chain formulas for inverse transform sampling and in the subroutine Barbarik2Kernel.
[15] Let c1c2 · · · cm be the m-bit binary representation of k, where cm is the least
Deﬁnition 7. signiﬁcant bit. We then construct a chain formula ϕk,m(·) on m variables a1, . . . am as follows. For every j in {1, . . . m − 1}, let Cj be the connector “∨” if cj = 1, and the connector “∧” if cj = 0.
Deﬁne
ϕk,m(a1, · · · am) = a1 C1 (a2 C2(· · · (am−1 Cm−1 am) · · · ))
For example, consider k = 11 and m = 4. The binary representation of 11 using 4 bits is 1011.
Therefore, ϕ5,4(a1, a2, a3, a4) = a1 ∨ (a2 ∧ (a3 ∨ a4)).
Lemma 1. [15] Let m > 0 be a natural number, k < 2m , and ϕk,m as deﬁned above. Then |ϕk,m| is linear in m and ϕk,m has exactly k satisfying assignments. Every chain formula ψ on n variables is equivalent to a CNF formula ψCN F having at most n clauses. In addition, |ψCN F | is in O(n2). 2.2 Barbarik2Kernel and the Subquery Consistency Assumption
Barbarik2Kernel is a crucial subroutine that we use in our algorithm to help us draw conditional samples from Rϕ↓S . This is similar to the subroutine Kernel used by the Barbarik in [10]. We will now deﬁne a collection of functions KernelFamily.
Deﬁnition 8. KernelFamily is family of functions that take a Boolean formula ϕ, a set of variables
S ⊆ Supp(ϕ), and two assignments σ1, σ2 ∈ Rϕ↓S, and return ˆϕ such that R ˆϕ↓S = {σ1, σ2}.
[10] introduced the notion of non-adversarial assumption, which was crucial in their analysis. We rename the notion of subquery consistency to better capture its intended properties, deﬁned below.
Deﬁnition 9. Let Barbarik2Kernel ∈ KernelFamily. A sampler G is subquery consistent w.r.t. a particular Barbarik2Kernel for ϕ if for all S ⊆ Supp(ϕ), σ1, σ2 ∈ Rϕ↓S , let ˆϕ ←
Barbarik2Kernel(ϕ, S, σ1, σ2) then the output of G( ˆϕ, wt, S, τ ) is τ independent samples from the conditional distribution DG(ϕ)|T , where T = {σ1, σ2}.
Similar to the usage of non-adversarial assumption in the correctness analysis of Barbarik [10], the notion of subquery consistency would play a crucial role in our analysis. Since each subquery can be viewed as conditioning and given that conditioning is a fundamental operation, one would expect that off the shelf samplers would be subquery consistent. At the same time, in contrast to practical applications, the set T is arbitrarily chosen, and therefore, it is possible that certain samplers do not satisfy the property of subquery consistency. It is, however, not known how to test whether a sampler is subquery consistent w.r.t a particular Barbarik2Kernel. While our empirical evaluation provides weak evidence to our claim that off the shelf samplers are subquery consistent, we believe checking whether a sampler is subquery consistent is an interesting and important problem for future work. 3