Abstract
Deﬁning and reliably ﬁnding a canonical orientation for 3D surfaces is key to many
Computer Vision and Robotics applications. This task is commonly addressed by handcrafted algorithms exploiting geometric cues deemed as distinctive and robust by the designer. Yet, one might conjecture that humans learn the notion of the inherent orientation of 3D objects from experience and that machines may do so alike. In this work, we show the feasibility of learning a robust canonical orientation for surfaces represented as point clouds. Based on the observation that the quintessential property of a canonical orientation is equivariance to 3D rotations, we propose to employ Spherical CNNs, a recently introduced machinery that can learn equivariant representations deﬁned on the Special Orthogonal group
SO(3). Speciﬁcally, spherical correlations compute feature maps whose elements deﬁne 3D rotations. Our method learns such feature maps from raw data by a self-supervised training procedure and robustly selects a rotation to transform the input point cloud into a learned canonical orientation. Thereby, we realize the ﬁrst end-to-end learning approach to deﬁne and extract the canonical orientation of 3D shapes, which we aptly dub Compass. Experiments on several public datasets prove its effectiveness at orienting local surface patches as well as whole objects. 1

Introduction
Humans naturally develop the ability to mentally portray and reason about objects in what we perceive as their neutral, canonical orientation, and this ability is key for correctly recognizing and manipulating objects as well as reasoning about the environment. Indeed, mental rotation abilities have been extensively studied and linked with motor and spatial visualization abilities since the 70s in the experimental psychology literature [36, 40, 16].
Robotic and computer vision systems similarly require neutralizing variations w.r.t. rotations when processing 3D data and images in many important applications such as grasping, navigation, surface matching, augmented reality, shape classiﬁcation and detection, among the others. In these domains, two main approaches have been pursued so to deﬁne rotation-invariant methods to process 3D data: rotation-invariant operators and canonical orientation estimation. Pioneering works applying deep learning to point clouds, such as PointNet [31, 32] achieved invariance to rotation by means of a transformation network used to predict a canonical orientation to apply direclty to the coordinates 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Canonical orientations in humans and machines. Randomly rotated mugs are depicted in (a). To achieve rotation-invariant processing, e.g. to check if they are the same mug, humans mentally neutralize rotation variations preferring an upright canonical orientation, as illustrated in (b). A machine may instead use any canonical reference orientation, even unnatural to humans, e.g. like in (c). of the input point cloud. Despite being trained by sampling the range of all possible rotations at training time through data augmentation, this approach, however, does not generalize to rotations not seen during training. Hence, invariant operators like rotation-invariant convolutions were introduced, allowing to train on a reduced set of rotations (ideally one, the unmodiﬁed data) and test on the full spectrum of rotations [23, 10, 43, 46, 33, 45]. Canonical orientation estimation, instead, follows more closely the human path to invariance and exploits the geometry of the surface to estimate an intrinsic 3D reference frame which rotates with the surface. Transforming the input data by the inverse of the 3D orientation of such reference frame brings the surface in an orientation-neutral, canonical coordinate system wherein rotation invariant processing and reasoning can happen. While humans have a preference for a canonical orientation matching one of the usual orientations in which they encounter an object in everyday life, in machines this paradigm does not need to favour any actual reference orientation over others: as illustrated in Figure 1, an arbitrary one is ﬁne as long as it can be repeatably estimated from the input data.
Despite mental rotation tasks being solved by a set of unconscious abilities that humans learn through experience, and despite the huge successes achieved by deep neural networks in addressing analogous unconscious tasks in vision and robotics, the problem of estimating a canonical orientation is still solved solely by handcrafted proposals [35, 28, 24, 12, 42, 11, 1]. This may be due to convnets, the standard architectures for vision applications, reliance on the convolution operator in Euclidean domains, which possesses only the property of equivariance to translations of the input signal.
However, the essential property of a canonical orientation estimation algorithm is equivariance with respect to 3D rotations because, upon a 3D rotation, the 3D reference frame which establishes the canonical orientation of an object should undergo the same rotation as the object. We also point out that, although, in principle, estimation of a canonical reference frame is suitable to pursue orientation neutralization for whole shapes, in past literature it has been intensively studied mainly to achieve rotation-invariant description of local surface patches.
In this work, we explore the feasibility of using deep neural networks to learn to pursue rotation-invariance by estimating the canonical orientation of a 3D surface, be it either a whole shape or a local patch. Purposely, we propose to leverage Spherical CNNs [7, 10], a recently introduced variant of convnets which possesses the property of equivariance w.r.t. 3D rotations by design, in order to build
Compass, a self-supervised methodology that learns to orient 3D shapes. As the proposed method computes feature maps living in SO(3), i.e. feature map coordinates deﬁne 3D rotations, and does so by rotation-equivariant operators, any salient element in a feature map, e.g. its arg max, may readily be used to bring the input point cloud into a canonical reference frame. However, due to discretization artifacts, Spherical CNNs turn out to be not perfectly rotation-equivariant [7]. Moreover, the input data may be noisy and, in case of 2.5D views sensed from 3D scenes, affected by self-occlusions and missing parts. To overcome these issues, we propose a robust end-to-end training pipeline which mimics sensor nuisances by data augmentation and allows the calculation of gradients with respect to feature maps coordinates. The effectiveness and general applicability of Compass is established by achieving state-of-the art results in two challenging applications: robust local reference frame estimation for local surface patches and rotation-invariant global shape classiﬁcation. 2
2