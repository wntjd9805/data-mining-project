Abstract 2K
T√ for one dimension and at least T√
K
We study the problem of switching-constrained online convex optimization (OCO), where the player has a limited number of opportunities to change her action.
While the discrete analog of this online learning task has been studied extensively, previous work in the continuous setting has neither established the minimax rate nor algorithmically achieved it. In this paper, we show that T -round switching-constrained OCO with fewer than K switches has a minimax regret of Θ( T√
).
K
In particular, it is at least for higher dimensions. The lower bound in higher dimensions is attained by an orthogonal subspace argument. In one dimension, a novel adversarial strategy yields the lower bound of O( T√
), but a precise minimax analysis including constants is
K more involved. To establish the tighter one-dimensional result, we introduce the fugal game relaxation, whose minimax regret lower bounds that of switching-constrained OCO. We show that the minimax regret of the fugal game is at least
T√ and thereby establish the optimal minimax lower bound in one dimension.
To establish the dimension-independent upper bound, we next show that a mini-batching algorithm provides an O( T√
) upper bound, and therefore conclude that
K the minimax regret of switching-constrained OCO is Θ( T√
) for any K. This is
K in sharp contrast to its discrete counterpart, the switching-constrained prediction-from-experts problem, which exhibits a phase transition in minimax regret between the low-switching and high-switching regimes. 2K 1

Introduction
Online learning provides a versatile framework for studying a wide range of dynamic optimization problems, with manifold applications in portfolio selection [18], packet routing [7], hyperparameter optimization [19], and spam ﬁltering [23]. The fundamental problem is typically formulated as a repeated game between a player and an adversary. In the tth round, the player ﬁrst chooses an action xt from the set of all possible actions D; the adversary then responds by revealing the penalty for that action, a function ft : D → R. The player’s goal is to minimize the total penalties she receives, while the adversary’s goal is to maximize the penalties she assigns to the player’s action. Explicitly, the standard benchmark for success is regret, the difference between the player’s accumulated penalty and that of the best ﬁxed action in hindsight: R = (cid:80)T
Several variants of this general learning setting have been studied. When D is a discrete set of actions, the game is called either “prediction from experts" (PFE), if the player is allowed knowledge of the complete function ft(·) on each round, or “multi-armed bandit" (MAB), if only ft(xt) is revealed on each round. Crucially, the adversary is not strongly adaptive and picks ft based solely on prior i=1 fi(xi) − inf x∈D i=1 fi(x) . (cid:80)T
∗First two authors contributed equally. Correspondence to: Lin Chen <lin.chen@berkeley.edu>. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
knowledge of the player’s randomized strategy and x1, . . . , xt−1, and not xt; otherwise, she could always force linear regret in T [12, 24]. In this paper, we instead consider the continuous analog of prediction from experts, termed online convex optimization. Here, D is a continuum of possible actions, but the entirety of ft(·) is revealed after it has been played. Surprisingly, the continuity of D means that a player can guarantee sublinear regret against a strongly adaptive adversary, i.e., one who may choose ft even after observing xt.
In many real-world applications, however, we desire an online algorithm to have greater continuity in its actions over the course of many rounds. In caching, for example, erratic online decisions may induce cache misses, and thus costly memory access procedures [10]. More explicitly, the number of times that the player can switch her action between rounds may be strictly constrained. For example, suppose that the player makes predictions based on expert advice. If she would like to hire a new expert, she has to terminate the current contract, pay an early termination fee, and hire and pay a new expert. If hiring a new expert costs $1000 in total and her budget is $10000 dollars, the number of her switches must be less than 10. This setting is called switching-constrained or switching-budgeted online learning [3]. In these settings, it is necessary to assume an oblivious adversary: an adaptive adversary can force an algorithm with fewer than K switches to incur linear regret by assigning 0 to a switched action between rounds, and 1 to a repeated action [3].
Previous work has established the minimax regret of the switching-constrained multi-armed bandit and prediction from experts problems, but the minimax rate of switching-constrained online convex optimization was neither known nor algorithmically achieved. In this paper, we establish the minimax regret of switching-constrained online convex optimization (OCO) against the strongest adaptive adversary, and in doing so, present a simple mini-batching algorithm that achieves this optimal rate.
We assume that D, the action set of the player, is a compact, convex subset of Rn. Let F be a family of differentiable convex functions from D to R from which the adversary selects each round’s loss function, ft. In the full-information setting (OCO), we assume that the player observes the loss function ft after the adversary decides on it. The key ingredient, differentiating our setting from typical OCO, is a limit on the player’s number of switches. Formally, given a sequence of points x1, . . . , xT , let c(x1, . . . , xT ) = (cid:80)T −1 1[xi+1 (cid:54)= xi] denote the number of switches. The player’s action sequence must satisfy c(x1, . . . , xT ) < K for some natural number K.2
Given the player’s action sequence x1, . . . , xT and the adversary’s loss sequence f1, . . . , fT , the usual regret is deﬁned by the total accumulated loss incurred by the player, minus the total loss of the best possible single action in hindsight. We add an additional term and an outermost supremum that drives the regret of any player’s sequence that violates the switch limit to inﬁnity: i=1
R({xi}, {fi}) = sup
λ>0 (cid:32) T (cid:88) i=1 fi(xi) − inf x∈D
T (cid:88) i=1 fi(x) + λ1[c(x1, . . . , xT ) ≥ K]
, (cid:33) where 1[·] is the statement function whose value is 1 if the proposition inside the brackets holds and is 0 otherwise. In the following sections, we denote the switching-constrained minimax regret by
R(T, K) = inf x1 sup f1
. . . inf xT sup fT
R({xi}, {fi}), where it will be clear from context from which sets xi and fi may be drawn. 2