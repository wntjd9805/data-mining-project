Abstract
Current graph neural networks (GNNs) lack generalizability with respect to scales (graph sizes, graph diameters, edge weights, etc..) when solving many graph analy-sis problems. Taking the perspective of synthesizing graph theory programs, we propose several extensions to address the issue. First, inspired by the dependency of the iteration number of common graph theory algorithms on graph size, we learn to terminate the message passing process in GNNs adaptively according to the computation progress. Second, inspired by the fact that many graph theory algo-rithms are homogeneous with respect to graph weights, we introduce homogeneous transformation layers that are universal homogeneous function approximators, to convert ordinary GNNs to be homogeneous. Experimentally, we show that our
GNN can be trained from small-scale graphs but generalize well to large-scale graphs for a number of basic graph theory problems. It also shows generalizability for applications of multi-body physical simulation and image-based navigation problems. 1

Introduction
Graph, as a powerful data representation, arises in many real-world applications [1, 2, 3, 4, 5, 6]. On the other hand, the ﬂexibility of graphs, including the different representations of isomorphic graphs, the unlimited degree distributions [7, 8], and the boundless graph scales [9, 10], also presents many challenges to their analysis. Recently, Graph Neural Networks (GNNs) have attracted broad attention in solving graph analysis problems. They are permutation-invariant/equivariant by design and have shown superior performance on various graph-based applications [11, 12, 13, 14, 15].
However, investigation into the generalizability of GNNs with respect to the graph scale is still limited.
Speciﬁcally, we are interested in GNNs that can learn from small graphs and perform well on new graphs of arbitrary scales. Existing GNNs [11, 12, 13, 15] are either ineffective or inefﬁcient under this setting. In fact, even ignoring the optimization process of network training, the representation power of existing GNNs is yet too limited to achieve graph scale generalizability. There are at least two issues: 1) By using a pre-deﬁned layer number [16, 17, 18], these GNNs are not able to approximate graph algorithms whose complexity depends on graph size (most graph algorithms in textbooks are of this kind). The reason is easy to see: For most GNNs, each node only uses information of the 1-hop neighborhoods to update features by message passing, and it is impossible for k-layer GNNs to send messages between nodes whose distance is larger than k. More formally, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Loukas [19] proves that GNNs, which fall within the message passing framework, lose a signiﬁcant portion of their power for solving many graph problems when their width and depth are restricted; and 2) a not-so-obvious observation is that, the range of numbers to be encoded by the internal representation may deviate greatly for graphs of different scales. For example, if we train a GNN to solve the shortest path problem on small graphs of diameter k with weight in the range of [0, 1], the internal representation could only need to build the encoding for the path length within [0, k]; but if we test this GNN on a large graph of diameter K (cid:29) k with the same weight range, then it has to use and transform the encoding for [0, K]. The performance of classical neural network modules (e.g. the multilayer perceptron in GNNs) are usually highly degraded on those out-of-range inputs.
To address the pre-deﬁned layer number issue, we take a program synthesis perspective, to design
GNNs that have stronger representation power by mimicking the control ﬂow of classical graph algorithms. Typical graph algorithm, such as Dijkstra’s algorithm for shortest path computation, are iterative. They often consist of two sub-modules: an iteration body to solve the sub-problem (e.g., update the distance for the neighborhood of a node as in Dijkstra), and a termination condition to control the loop out of the iteration body. By adjusting the iteration numbers, an iterative algorithm can handle arbitrary large-scale problems. We, therefore, introduce our novel Iterative GNN (IterGNN) that equips ordinary GNN with an adaptive and differentiable stopping criterion to let GNN iterate by itself, as shown in Figure 1. Our stopping condition is adaptive to the inputs, supports arbitrarily large iteration numbers, and, interestingly, is able to be trained in an end-to-end fashion without any direct supervision.
We also give a partial solution to address the issue of out-of-range number encoding, if the underlying graph algorithm is in a speciﬁc hypothesis class. More concretely, the solutions to many graph problems, such as the shortest path problem and TSP problem, are homogeneous with respect to the input graph weights, i.e., the solution scales linearly with the magnitudes of the input weights.
To build GNNs with representation power to approximate the solution to such graph problems, we further introduce the homogeneous inductive-bias. By assuming the message processing functions are homogeneous, the knowledge that neural networks learn at one scale can be generalized to different scales. We build HomoMLP and HomoGNN as powerful approximates of homogeneous functions over vectors and graphs, respectively.
We summarize our contributions as follows: (1) We propose IterGNN to approximate iterative algorithms, which avoids ﬁxed computation steps in previous graph neural networks, and provides the potential for solving arbitrary large-scale problems. (2) The homogeneous prior is further introduced as a powerful inductive bias for solving many graph-related problems. (3) We prove the universal approximation theorem of HomoMLP for homogeneous functions and also prove the generalization error bounds of homogeneous neural networks under proper conditions. (4) In experiments, we demonstrate that our methods can generalize on various tasks and have outperformed baselines. 2