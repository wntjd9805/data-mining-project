Abstract
Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and ofﬂine training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through mixing rehearsal with knowledge distillation and regularization; our simple baseline, Dark Experience Replay, matches the network’s logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on both standard benchmarks and a novel GCL evaluation setting (MNIST-360), we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. We further explore the generalization capabilities of our objec-tive, showing its regularization being beneﬁcial beyond mere performance.
Code is available at https://github.com/aimagelab/mammoth. 1

Introduction
Practical applications of neural networks may require to go beyond the classical setting where all data are available at once: when new classes or tasks emerge, such models should acquire new knowledge on-the-ﬂy, incorporating it with the current one. However, if the learning focuses on the current set of examples solely, a sudden performance deterioration occurs on the old data, referred to as catastrophic forgetting [29]. As a trivial workaround, one could store all incoming examples and re-train from scratch when needed, but this is impracticable in terms of required resources. Continual
Learning (CL) methods aim at training a neural network from a stream of non i.i.d. samples, relieving catastrophic forgetting while limiting computational costs and memory footprint [32].
It is not always easy to have a clear picture of the merits of these works: due to subtle differences in the way methods are evaluated, many state-of-the-art approaches only stand out in the setting where they were originally conceived. Several recent papers [10, 11, 17, 39] address this issue and conduct a critical review of existing evaluation settings, leading to the formalization of three main experimental settings [17, 39]. By conducting an extensive comparison on them, we surprisingly observe that a simple Experience Replay baseline (i.e. interleaving old examples with ones from the current task) consistently outperforms cutting-edge methods in the considered settings.
Also, the majority of the compared methods are unsuited for real-world applications, where memory is bounded and tasks intertwine and overlap. Recently, [10] introduced a series of guidelines that CL methods should realize to be applicable in practice: i) no task boundaries: do not rely on boundaries between tasks during training; ii) no test time oracle: do not require task identiﬁers at inference time; iii) constant memory: have a bounded memory footprint throughout the entire training phase. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Methods
Constant memory
No task boundaries
No test time oracle
PNN PackNet HAT
[28]
[35]
ER MER GSS GEM A-GEM HAL iCaRL FDR LwF SI oEWC DER DER++ (ours)
[32]
[24] [42] (ours)
[20]
[27]
[33]
[9]
[4]
[1]
[8]
[37] [31, 33] – – – – – – – – – (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) – (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) – (cid:51) (cid:51) – (cid:51) (cid:51) – (cid:51) (cid:51) – – (cid:51) – (cid:51) (cid:51) – (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)
Table 1: Continual learning approaches and their compatibility with the General Continual Learning major requirements [10]. For an exhaustive discussion, please refer to supplementary materials.
These requirements outline the General Continual Learning (GCL), of which Continual Learning is a relaxation. As reported in Table 1, ER also stands out being one of the few methods that are fully compliant with GCL. MER [33] and GSS [1] fulﬁll the requirements as well, but they suffer from a very long running time which hinders their applicability to non-trivial datasets.
In this work, we propose a novel CL baseline that improves on ER while maintaining a very simple formulation. We call it Dark Experience Replay (DER) as it relies on dark knowledge [15] for distilling past experiences, sampled over the entire training trajectory. Our proposal satisﬁes the GCL guidelines and outperforms the current state-of-the-art approaches in the standard CL experiments we conduct. With respect to ER, we empirically show that our baseline exhibits remarkable qualities: it converges to ﬂatter minima, achieves better model calibration at the cost of a limited memory and training time overhead. Eventually, we propose a novel GCL setting (MNIST-360); it displays MNIST digits sequentially and subject to a smooth increasing rotation, thus generating both sudden and gradual changes in their distribution. By evaluating the few GCL-compatible methods on MNIST-360, we show that DER also qualiﬁes as a state-of-the-art baseline for future studies on this setting. 2