Abstract
Distribution alignment has many applications in deep learning, including domain adaptation and unsupervised image-to-image translation. Most prior work on unsu-pervised distribution alignment relies either on minimizing simple non-parametric statistical distances such as maximum mean discrepancy or on adversarial align-ment. However, the former fails to capture the structure of complex real-world distributions, while the latter is difﬁcult to train and does not provide any universal convergence guarantees or automatic quantitative validation procedures. In this paper, we propose a new distribution alignment method based on a log-likelihood ratio statistic and normalizing ﬂows. We show that, under certain assumptions, this combination yields a deep neural likelihood-based minimization objective that attains a known lower bound upon convergence. We experimentally verify that minimizing the resulting objective results in domain alignment that preserves the local structure of input domains. 1

Introduction
The goal of unsupervised domain alignment is to ﬁnd a transformation of one dataset that makes it similar to another dataset while preserving the structure of the original. The majority of modern neural approaches to domain alignment directly search for a transformation of the dataset that minimizes an empirical estimate of some statistical distance - a non-negative quantity that takes lower values as datasets become more similar. The variability of what “similar” means in this context, which transformations are allowed, and whether data points themselves or their feature representations are aligned, leads to a variety of domain alignment methods. Unfortunately, existing estimators of statistical distances either restrict the notion of similarity to enable closed-form estimation [24], or rely on adversarial (min-max) training [27] that makes it very difﬁcult to quantitatively reason about the performance of such methods [3; 5; 25]. In particular, the value of the optimized adversarial objective conveys very little about the quality of the alignment, which makes it difﬁcult to perform automatic model selection on a new dataset pair. On the other hand, Normalizing Flows [20] are an emerging class of deep neural density models that do not rely on adversarial training. They model a given dataset as a random variable with a simple known distribution transformed by an unknown invertible transformation parameterized using a deep neural network. Recent work on normalizing ﬂows for maximum likelihood density estimation made great strides in deﬁning new rich parameterizations for these invertible transforms [8; 11; 14], but little work focused on ﬂow-based density alignment [12; 29].
In this paper, we present the Log-likelihood Ratio Minimizing Flow (LRMF), a new non-adversarial approach for aligning distributions in a way that makes them indistinguishable for a given family of density models M . We consider datasets A and B indistinguishable with respect to the family M if there is a single density model in M that is optimal for both A and B individually since in this case 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: To align input datasets A and B, we look for a transformation T that makes T (A) and B “indistin-guishable”. (a) We propose the log-likelihood ratio distance dΛ(T (A), B) that compares likelihoods of density models θAT ﬁtted to T (A) and θB to B independently with the likelihood of θS optimal for the combined dataset
T (A) ∪ B. This problem is adversarial, but we show how to reduce it to minimization if T is a normalizing ﬂow. (b) Colored contours represent level sets of models for B (orange), T (A) (green), and θS (purple), contour sizes corresponds to entropies of these models. Only θAT and θS (dashed) change during training. The proposed objective can be viewed as maximizing the entropy of the transformed dataset, while minimizing the combined entropy of T (A) ∪ B, i.e. expanding green contour while squeezing the purple contour around green and orange contours. At equilibrium, θS and θAT model the same distribution as θB, i.e. shapes of purple and green contours match and tightly envelope the orange contour. Best viewed in color. there is no way of telling which of two datasets was used for training it. For example, two different distributions with the same means and covariances are indistinguishable for the Gaussian family M since we can not tell which of two datasets was used by examining the model ﬁtted to either one of them. For a general M , we can quantitatively measure whether two datasets are indistinguishable by models from M by comparing average log-likelihoods of two “private” density models each ﬁt independently to A and B, to the average log-likelihood of the “shared” model ﬁt to both datasets at the same time. We observe that, if datasets are sufﬁciently large, the maximum likelihood of the
“shared” model would reach the likelihoods of two “private” models on respective datasets only if the shared model is optimal for both of them individually, and consequently datasets are equivalent with respect to M . Then a density model optimal for A is guaranteed to be optimal for B and vice versa.
We want to ﬁnd a transformation T (x) that transforms dataset A in a way that makes the transformed dataset T (A) equivalent to B for the given family M . We do that by minimizing the aforementioned gap between average log-likelihood scores of “shared” and “private” models. In this paper, we show that, generally, such T (x) can be found only by solving a min-max optimization problem, but if T (x, φ) is a family of normalizing ﬂows, then the ﬂow T (x, φ∗) that makes T (A, φ∗) and B equivalent with respect to M can be found by minimizing a single objective that attains zero upon convergence. This enables automatic model validation and hyperparameter tuning on the held-out set.
To sum up, the novel non-adversarial data alignment method presented in this paper combines the clear convergence criteria found in non-parametric and simple parametric approaches and the power of deep neural discriminators used in adversarial models. Our method ﬁnds a transformation of one dataset that makes it “equivalent” to another dataset with respect to the speciﬁed family of density models. We show that if that transformation is restricted to a normalizing ﬂow, the resulting problem can be solved by minimizing a single simple objective that attains zero only if two domains are correctly aligned. We experimentally verify this claim and show that the proposed method preserves the local structure of the transformed distribution and that it is robust to model misspeciﬁcation by both over- and under-parameterization. We show that minimizing the proposed objective is equivalent to training a particular adversarial network, but in contrast with adversarial methods, the performance of our model can be inferred from the objective value alone. We also characterize the the vanishing of generator gradient mode that our model shares with its adversarial counterparts, and principal ways of detecting it. 2 Log-Likelihood Ratio Minimizing Flow
In this section, we formally deﬁne the proposed method for aligning distributions. We assume that
M (θ) is a family of densities parameterized by a real-valued vector θ, and we ﬁt models from M to data by maximizing its likelihood across models from M . Intuitively, if we ﬁt two models θA and
θB to datasets A and B independently, and also ﬁt a single shared model θS to the combined dataset
A ∪ B, then the log-likelihood ratio distance would equal the difference between the log-likelihood of that optimal “shared” and the two optimal “private” models (Deﬁnition 2.1). Next, we consider the problem of ﬁnding a transformation that would minimize this distance. In general, this would require solving an adversarial optimization problem (1), but we show that if the transformation is restricted to the family of normalizing ﬂows, then the optimal one can be found by minimizing a simple non-adversarial objective (Theorem 2.3). We also illustrate this result with an example that can be solved analytically: we show that minimizing the proposed distance between two random variables 2
with respect to the normal density family is equivalent to directly matching their ﬁrst two moments (Example 2.1). Finally, we show the relation between the proposed objective and Jensen-Shannon divergence and show that minimizing the proposed objective is equivalent to training a generative adversarial network with a particular choice of the discriminator family.
Notation. Let log PM (X; θ) := Ex∼PX log PM (x; θ) denote the negative cross-entropy between the distribution PX of the dataset X deﬁned over X ⊂ Rn, and a member PM (x; θ) of the parametric family of distributions M (θ) deﬁned over the same domain, i.e. the likelihood of X given PM (x; θ).
Deﬁnition 2.1 (LRD). Let us deﬁne the log-likelihood ratio distance dΛ between datasets A and B from X with respect to the family of densities M , as the difference between log-likelihoods of A and
B given optimal models with “private” parameters θA and θB, and “shared” parameters θS: dΛ(A, B; M ) = max
θA;θB (cid:104) (cid:105) log PM (A; θA) + log PM (B; θB) (cid:104)
− max
θS log PM (A; θS) + log PM (B; θS) (cid:105)
= min
θS max
θA;θB (cid:104)(cid:0) log PM (A; θA) − log PM (A; θS)(cid:1) + (cid:0) log PM (B; θB) − log PM (B; θS)(cid:1)(cid:105)
.
The expression above is also the log-likelihood ratio test statistic log Λn for the null hypothesis
H0 : θA = θB for the model described by the likelihood function P (A, B | θA, θB) = (cid:2)PM (A; θA) ·
PM (B; θB)(cid:3) and intuitively equals to the amount of likelihood we “lose” by forcing θA = θB onto the model ﬁtted to approximate A and B independently. Figure 1 illustrates that, in terms of average likelihood, the shared model (purple) is always inferior to two private models from the same class, unless two datasets are in fact just different samples from the same distribution.
Lemma 2.1. The log-likelihood ratio distance is non-negative, and the equals zero only if there exists a single “shared” model that approximates datasets as well as their “private” optimal models: dΛ(A, B; M ) = 0 ⇔ ∃ θS : log PM (A; θS) = max
θ log PM (A; θ) ∧ log P (B; θS) = max
θ log PM (B; θ).
Proof. Follows from the fact that the shared part in the Deﬁnition 2.1 is identical to the private part but over a smaller feasibility set {θA = θB}. See the supplementary Section 8.6 for the formal proof.
Adversarial formulation. If we introduce the parametric family of transformations T (x, φ) and try to ﬁnd φ that minimizes the log-likelihood ratio distance minφ dΛ(T (A; φ), B; M ), an adversarial problem arises. Note that for a ﬁxed dataset B, only the ﬁrst term is adversarial, and only w.r.t. θAT : (cid:104) min
φ,θS max
θAT ;θB log PM (T (A; φ); θAT ) + log PM (B; θB) − log PM (T (A; φ); θS) − log PM (B; θS) (cid:105) (1)
Figure 1b illustrates that minimizing this objective (1) over θS while maximizing it over θAT corresponds to minimizing entropy (“squeezing”) of the combination of T (A) and B while maximizing entropy of (“expanding”) transformed dataset T (A) as much as possible.
Non-adversarial formulation. The adversarial objective (1) requires ﬁnding a new optimal model
θAT for each new value of φ to ﬁnd the maximal likelihood of the transformed dataset T (A), but
Figure 1a illustrates that the likelihood of the transformed dataset can be often estimated from the parameters of the transformation T alone. For example, if T uniformly squeezes the dataset by a factor of two, the average maximum likelihood of the transformed dataset maxθ log PM (T (A); θ) doubles compared to the likelihood of the original A. In general, the likelihood of the transformed dataset is inversely proportional to the Jacobian of the determinant of the applied transformation. The lemma presented below formalizes this relation taking into account the limited capacity of M , and leads us to our main contribution: the optimal transformation can be found by simply minimizing a modiﬁed version of the objective (1) using an iterative method of one’s choice.
Lemma 2.2. If T (x; φ) is a normalizing ﬂow, then the ﬁrst term in the objective (1) can be bounded in closed form as a function of φ up to an approximation error Ebias. The equality in (2) holds when the approximation term vanishes, i.e. if M approximates both A and T (A; φ) equally well; PA is the true distribution of A and T [PA, φ] is the push-forward distribution of the transformed dataset. max
θAT log PM (T (A; φ); θAT ) ≤ max
θA (cid:20) log PM (A; θA) − log det |∇xT (A; φ)| + Ebias(A, T, M ) (2) (cid:21)
Ebias(A, T, M ) (cid:44) max
φ min
θ
DKL(PA; M (θ)) − min
θ
DKL(T [PA, φ]; M (θ)) 3
Proof. We expand likelihoods of combined and shared datasets given best models from M into respective “true” negative entropies and the approximation errors due to the choice of M (KL-divergence between true distributions and their KL-projections onto M ). Then we replace the entropy of the transformed dataset with the entropy of the original and the log-determinant of the Jacobian of the applied transformation, noting that log det |∇xT −1(T (A, φ), φ)| = log det |∇xT (A, φ)|. We refer readers to the Section 8.6 of the supplementary for the full proof.
By applying this lemma to the objective (1) and grouping together terms that do not depend on θS and φ, we ﬁnally obtain the ﬁnal objective.
Deﬁnition 2.2 (LMRF). Let us deﬁne the log-likelihood ratio minimizing ﬂow (LRMF) for a pair of datasets A and B on X , the family of densities M (θ) on X , and the parametric family of normalizing ﬂows T (x; φ) from X onto itself, as the ﬂow T (x; φ∗) that minimizes LLRMF (3), where the constant c(A, B) does not depend on θS and φ, and can be precomputed in advance.
LLRMF(A, B, φ, θS) = − log det |∇xT (A; φ)| − log PM (T (A; φ); θS) − log PM (B; θS) + c(A, B), (3) c(A, B) = max
θA log PM (A; θA) + max
θB log PM (B; θB)
Theorem 2.3. If T (x, φ) is a normalizing ﬂow, then the adversarial log-likelihood ratio distance (1) between the transformed source and target datasets can be bounded via the non-adversarial
LRMF objective (3), and therefore the parameters of the normalizing ﬂow φ that make T (A, φ) and
B equivalent with respect to M can be found by minimizing the LRMF objective (3) using gradient descent iterations with known convergence guarantees. 0 ≤ dΛ(T (A, φ), B; M ) ≤ minθLLRMF(A, B, φ, θ) + Ebias. (4)
This theorem follows from the deﬁnition of dΛ and two lemmas provided above that show that the optimization over θAT can be (up to the error term) replaced by a closed-form expression for the likelihood of the transformed dataset if the transformation is a normalizing ﬂow. Intuitively, the
LRMF loss (3) encourages the transformation T to draw all points from A towards the mode of the shared model P (x, θS) via the second term, while simultaneously encouraging T to expand as much as possible via the ﬁrst term as illustrated in Figure 1b. The delicate balance is attained only when two distributions are aligned, as shown in Lemma 2.1. The inequality (4) is tight (equality holds) only when the bias term is zero, and the shared model is optimal.
The example below shows that the afﬁne log-likelihood ratio minimizing ﬂow between two univariate random variables with respect to the normal density family M corresponds to shifting and scaling one variable to match two ﬁrst moments of the other, which agrees with our intuitive understanding of what it means to make two distributions “indistinguishable” for the Gaussian family.
A, σ2
Example 2.1. Let us consider two univariate normal random variables A, B with moments
µA, µB, σ2
B, restrict M to normal densities, and the transform T (x; φ) to the afﬁne fam-θ = (µ, σ) and φ = (a, b). Using the expression for the ily: T (x; a, b) = ax + b, i.e. maximum log-likelihood (negative entropy) of the normal distribution, and the expression for variance of the equal mixture, we can solve the optimization over θS = (µS, σS) analytically: 1 2 (cid:105)
− log PM (T (A; φ); θS) − log PM (B; θS)
EX log P (X; µ, σ) = − min
µ,σ (cid:104) min
θS log(2πeσ2
X ) = − log σX + C
= log (cid:16) 1 2 (a2σ2
A + σ2
B) − (µA + b − µB)2(cid:17)
− 2C. 1 4
Combining expressions above gives us the ﬁnal objective that can be solved analytically by setting the derivatives with respect to a and b to zero: log det |∇xT (A; φ)| = log a and c(A, B) = − log σA − log σB + 2C,
LLRMF = − log a + log (cid:16) 1 2
B) − (µA + b − µB)2(cid:17) 1 4
− log σA − log σB (a2σ2 a∗ =
A + σ2
σB
σA
, b∗ = µB − µA.
The error term Ebias equals zero because any afﬁne transformation of a Gaussian is still a Gaussian. 4
Relation to Jensen-Shannon divergence and GANs. From the same expansion as in the proof of
Lemma 2.2 and the information-theoretic deﬁnition of the Jensen-Shannon divergence (JSD) as the difference between entropies of individual distributions and their equal mixture, it follows that the likelihood-ratio distance (and consequently LRMF) can be viewed as biased estimates of JSD. dΛ(A, B) = 2 · JSD(A, B) − DKL(A, M ) − DKL(B, M ) + 2 · DKL((A + B)/2, M )
Also, if the density family M is “convex”, in a sense that for any two densities from M their equal mixture also lies in M , then by rearranging the terms in the deﬁnition of the likelihood-ratio distance, and noticing that the optimal shared model is the equal mixture of two densities, it becomes evident that the LRMF objective is equivalent to the GAN objective with the appropriate choice of the discriminator family: min
T dΛ(T (A), B, M ) = min
T max
θAT ,θB min
θS (cid:20) log
PM (T (A); θAT )
PM (T (A); θS)
+ log (cid:21)
PM (B; θB)
PM (B; θS)
= min
T max
θAT ,θB (cid:20) log
PM (T (A); θAT )
PM (T (A); θAT ) + PM (T (A); θB)
+ log
= min
T max
D∈H (cid:2) log D(T (A)) + log (1 − D(B)) + log 4(cid:3), H(θ, θ(cid:48)) = (cid:21)
PM (B; θB)
PM (B; θAT ) + PM (B; θB)
PM (x; θ)
PM (x; θ) + PM (x; θ(cid:48)) (cid:26)
+ log 4 (cid:27)
.
B)/(cid:0)PM (x; θ∗
Since M is not “convex” in most cases, minimizing the LRMF objective is equivalent to adversarially aligning two datasets against a regularized discriminator. From the adversarial network perspective, the reason why LLRMF manages to solve this min-max problem using plain minimization is because for any ﬂow transformation parameter φ the optimal discriminator between T (A; φ) and B is deﬁned
A) det |∇xT −1(x; φ)|(cid:1). in closed form: D∗(x, φ) = PM (x; θ∗
Vanishing of generator gradients. The relation presented above suggests that the analysis performed by Arjovsky and Bottou [1] for GANs (Theorem 2.4, page 6) applies to LRMF as well, meaning that gradients of the LRMF objective w.r.t. the learned transformation parameters might vanish in higher dimensions. This implies that while the inequality (4) always holds, the model produces a useful alignment only when a sufﬁciently “deep” minimum of the LRMF loss (3) is found, otherwise the method fails, and the loss value should be indicative of this. An example presented below shows that reaching this deep minimum becomes exponentially more difﬁcult as the initial distance between distributions grows, which is often the case in higher dimensions.
B) + PM (T −1(x; φ); θ∗ s , µ(2) (cid:16)
N (x|µ(1) s ) and PM (x | θ) = 1 2 0) and Bµ sampled from M (µ + δ, µ − δ, σ2
Example 2.2. Consider M (θ) that parameterizes all equal mixtures of two univariate Gaussians with (cid:17) equal variances, i.e. θ = (µ(1) s , σ2 s , σ2
. s )
Consider A sampled from M (δ, −δ, σ2 0) for some
ﬁxed δ, µ and σ0. Let transformations be restricted to shifts T (x; b) = x + b, so φ = b, and log det |∇xT (x; φ)| = 0, and Ebias = 0 since M can approximate both A and T (A; b) perfectly for any b. For a sufﬁciently large µ, optimal shared model parameters can be found in closed form: 0 + δ2). This way the LRMF loss can be computed in closed form up to the cross-entropy:
θ∗ = (b, µ, σ2 0 + δ2)] + C. A sim-L(b, µ) := minθ LLRMF(A, Bµ, b, θ) = −2H[M (µ + δ, µ − δ, σ2 ulation provided in the supplementary Section 8.8 shows that the norm of the gradient of the LRMF objective decays exponentially as a function of µ: (cid:107)[∂L(b, µ)/∂µ](0, µ)(cid:107) ∝ exp(−µ2), meaning that as A and Bµ become further, the objective quickly becomes ﬂat w.r.t φ near the initial φt=0 = 0. s ) + N (x|µ(2) 0), M (b, µ, σ2 s , σ2
Model complexity. We propose the following intuition: 1) chose the family M (θ) that gives highest validation likelihood on B, since at optimum the shared model has to approximate the true underlying
PB well; 2) chose the family T (x; φ) that has fewer degrees of freedom then M , since otherwise the problem becomes underspeciﬁed. For example, consider M containing all univariate Gaussians parameterized by two parameters (µ, σ) aligned using polynomial transformations of the form
T (x; a0, a1, a2) = a2x2 + a1x + a0. In Example 2.1 we showed that Gaussian LRMF is equivalent to moment matching for two ﬁrst moments, but with this choice of T , there exist inﬁnitely many solutions for φ that all produce the desired mean and variance of the transformed dataset. 5
3