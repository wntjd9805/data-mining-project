Abstract
We study an online learning problem subject to the constraint of individual fairness, which requires that similar individuals are treated similarly. Unlike prior work on individual fairness, we do not assume the similarity measure among individuals is known, nor do we assume that such measure takes a certain parametric form.
Instead, we leverage the existence of an auditor who detects fairness violations without enunciating the quantitative measure. In each round, the auditor examines the learner’s decisions and attempts to identify a pair of individuals that are treated unfairly by the learner. We provide a general reduction framework that reduces online classiﬁcation in our model to standard online classiﬁcation, which allows us to leverage existing online learning algorithms to achieve sub-linear regret and number of fairness violations. Surprisingly, in the stochastic setting where the data are drawn independently from a distribution, we are also able to establish
PAC-style fairness and accuracy generalization guarantees (Rothblum and Yona (2018)), despite only having access to a very restricted form of fairness feedback.
Our fairness generalization bound qualitatively matches the uniform convergence bound of Rothblum and Yona (2018), while also providing a meaningful accuracy generalization guarantee. Our results resolve an open question by Gillen et al. (2018) by showing that online learning under an unknown individual fairness constraint is possible even without assuming a strong parametric form of the underlying similarity measure. 1

Introduction
As machine learning increasingly permeates many critical aspects of society, including education, healthcare, criminal justice, and lending, there is by now a vast literature that studies how to make machine learning algorithms fair (see, e.g., Chouldechova and Roth (2018); Podesta et al. (2014);
Corbett-Davies and Goel (2018)). Most of the work in this literature tackles the problem by taking the statistical group fairness approach that ﬁrst ﬁxes a small collection of high-level groups deﬁned by protected attributes (e.g., race or gender) and then asks for approximate parity of some statistic of the predictor, such as positive classiﬁcation rate or false positive rate, across these groups (see, e.g., Hardt et al. (2016); Chouldechova (2017); Kleinberg et al. (2017); Agarwal et al. (2018)). While notions of group fairness are easy to operationalize, they are aggregate in nature without fairness guarantees for
ﬁner subgroups or individuals (Dwork et al., 2012; Hébert-Johnson et al., 2018; Kearns et al., 2018).
In contrast, the individual fairness approach aims to address this limitation by asking for explicit fairness criteria at an individual level. In particular, the compelling notion of individual fairness proposed in the seminal work of Dwork et al. (2012) requires that similar people are treated similarly.
The original formulation of individual fairness assumes that the algorithm designer has access to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
a task-speciﬁc fairness metric that captures how similar two individuals are in the context of the speciﬁc classiﬁcation task at hand. In practice, however, such a fairness metric is rarely speciﬁed, and the lack of metrics has been a major obstacle for the wide adoption of individual fairness. There has been recent work on learning the fairness metric based on different forms of human feedback.
For example, Ilvento (2019) provides an algorithm for learning the metric by presenting human arbiters with queries concerning the distance between individuals, and Gillen et al. (2018) provide an online learning algorithm that can eventually learn a Mahalanobis metric based on identiﬁed fairness violations. While these results are encouraging, they are still bound by several limitations.
In particular, it might be difﬁcult for humans to enunciate a precise quantitative similarity measure between individuals. Moreover, their similarity measure across individuals may not be consistent with any metric (e.g., it may not satisfy the triangle inequality) and is unlikely to be given by a simple parametric function (e.g., the Mahalanobis metric function).
To tackle these issues, this paper studies metric-free online learning algorithms for individual fairness that rely on a weaker form of interactive human feedback and minimal assumptions on the similarity measure across individuals. Similar to the prior work of Gillen et al. (2018), we do not assume a pre-speciﬁed metric, but instead assume access to an auditor, who observes the learner’s decisions over a group of individuals that show up in each round and attempts to identify a fairness violation—a pair of individuals in the group that should have been treated more similarly by the learner. Since the auditor only needs to identify such unfairly treated pairs, there is no need for them to enunciate a quantitative measure – to specify the distance between the identiﬁed pairs. Moreover, we do not impose any parametric assumption on the underlying similarity measure, nor do we assume that it is actually a metric since we do not require that similarity measure to satisfy the triangle inequality. Under this model, we provide a general reduction framework that can take any online classiﬁcation algorithm (without fairness constraint) as a black-box and obtain a learning algorithm that can simultaneously minimize cumulative classiﬁcation loss and the number of fairness violations. Our results in particular remove many strong assumptions in Gillen et al. (2018), including their parametric assumptions on linear rewards and Mahalanobis distances, and thus answer several questions left open in their work. 1.1 Overview of Model and Results k ∈ {0, 1} of each individual with a “soft” predictor πt that predicts πt(xt
τ ) − yt
We study an online classiﬁcation problem: over rounds t = 1, . . . , T , a learner observes a small
τ )k set of k individuals with their feature vectors (xt
τ =1 in space X . The learner tries to predict the label yt
τ ) ∈ [0, 1] on each xt
τ and incurs classiﬁcation loss |πt(xt
τ |. Then an auditor will investigate if the learner has violated the individual fairness constraint on any pair of individuals within this round, that is, if there
) + α, where d : X × X → R+ is an exists (τ1, τ2) ∈ [k]2 such that |πt(xt
τ1 unknown distance function and α denotes the auditor’s tolerance. If this violation has occurred on any number of pairs, the auditor will identify one of such pairs and incur a fairness loss of 1; otherwise, the fairness loss is 0. Then the learner will update the predictive policy based on the observed labels and the received fairness feedback. Under this model, our results include:
)| > d(xt
τ1
) − πt(xt
τ2
, xt
τ2
A Reduction from Fair Online Classiﬁcation to Standard Online Classiﬁcation. Our reduction-based algorithm can take any no-regret online (batch) classiﬁcation learner as a black-box and achieve sub-linear cumulative fairness loss and sub-linear regret on mis-classiﬁcation loss compared to the most accurate policy that is fair on every round. In particular, our framework can leverage the generic exponential weights method (Freund and Schapire, 1997; Cesa-Bianchi et al., 1997; Arora et al., 2012) and also oracle-efﬁcient methods, including variants of Follow-the-Perturbed-Leader (FTPL) (e.g., Syrgkanis et al. (2016); Suggala and Netrapalli (2019)), that further reduces online learning to standard supervised learning or optimization problems. We instantiate our framework using two online learning algorithms (exponential weights and CONTEXT-FTPL), both of which obtain a ˜O(
T ) on misclassiﬁcation regret and cumulative fairness loss.
√
Fairness and Accuracy Generalization Guarantees. While our algorithmic results hold under adversarial arrivals of the individuals, in the stochastic arrivals setting we show that the uniform average policy over time is probably approximate correct and fair (PACF) (Rothblum and Yona, 2018)– that is, the policy is approximately fair on almost all random pairs drawn from the distribution and nearly matches the accuracy gurantee of the best fair policy. In particular, we show that the average policy πavg with high probability satisﬁes Prx,x(cid:48)[|πavg(x)−πavg(x(cid:48))| > α+1/T 1/4] ≤ O(1/T 1/4), 2
which qualitatively achieves similar PACF uniform convergence sample complexity as Rothblum and
Yona (2018).1 However, we establish our generalization guarantee through fundamentally different techniques. While their work assumes a fully speciﬁed metric and i.i.d. data, the learner in our setting can only access the similarity measure through an auditor’s limited fairness violations feedback. The main challenge we need to overcome is that the fairness feedback is inherently adaptive–that is, the auditor only provides feedback for the sequence of deployed policies, which are updated adaptively over rounds. In comparison, a fully known metric allows the learner to evaluate the fairness guarantee of all policies simultaneously. As a result, we cannot rely on their uniform convergence result to bound the fairness generalization error, but instead we leverage a probabilistic argument that relates the learner’s regret to the distributional fairness guarantee. 2