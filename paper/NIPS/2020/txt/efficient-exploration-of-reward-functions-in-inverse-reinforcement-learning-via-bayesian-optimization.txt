Abstract
The problem of inverse reinforcement learning (IRL) is relevant to a variety of tasks including value alignment and robot learning from demonstration. Despite signiﬁcant algorithmic contributions in recent years, IRL remains an ill-posed problem at its core; multiple reward functions coincide with the observed behavior and the actual reward function is not identiﬁable without prior knowledge or supplementary information. This paper presents an IRL framework called Bayesian optimization-IRL (BO-IRL) which identiﬁes multiple solutions that are consistent with the expert demonstrations by efﬁciently exploring the reward function space.
BO-IRL achieves this by utilizing Bayesian Optimization along with our newly proposed kernel that (a) projects the parameters of policy invariant reward functions to a single point in a latent space and (b) ensures nearby points in the latent space correspond to reward functions yielding similar likelihoods. This projection allows the use of standard stationary kernels in the latent space to capture the correlations present across the reward function space. Empirical results on synthetic and real-world environments (model-free and model-based) show that BO-IRL discovers multiple reward functions while minimizing the number of expensive exact policy optimizations. 1

Introduction
Inverse reinforcement learning (IRL) is the problem of inferring the reward function of a reinforcement learning (RL) agent from its observed behavior [1]. Despite wide-spread application (e.g., [1, 4, 5, 27]), IRL remains a challenging problem. A key difﬁculty is that IRL is ill-posed; typically, there exist many solutions (reward functions) for which a given behavior is optimal [2, 3, 29] and it is not possible to infer the true reward function from among these alternatives without additional information, such as prior knowledge or more informative demonstrations [9, 15].
Given the ill-posed nature of IRL, we adopt the perspective that an IRL algorithm should characterize the space of solutions rather than output a single answer. Indeed, there is often no one correct solution.
Although this approach differs from traditional gradient-based IRL methods [38] and modern deep incarnations that converge to speciﬁc solutions in the reward function space (e.g., [12, 14]), it is not entirely unconventional. Previous approaches, notably Bayesian IRL (BIRL) [32], share this view and return a posterior distribution over possible reward functions. However, BIRL and other similar methods [25] are computationally expensive (often due to exact policy optimization steps) or suffer from issues such as overﬁtting [8].
In this paper, we pursue a novel approach to IRL by using Bayesian optimization (BO) [26] to minimize the negative log-likelihood (NLL) of the expert demonstrations with respect to reward functions. BO is speciﬁcally designed for optimizing expensive functions by strategically picking inputs to evaluate and appears to be a natural ﬁt for this task. In addition to the samples procured, the
Gaussian process (GP) regression used in BO returns additional information about the discovered 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Our BO-IRL framework makes use of the ρ-projection that maps reward functions into a space where covariances can be ascertained using a standard stationary kernel. (a) Our running example of a 6 × 6 Gridworld example where the goal is to collect as many coins as possible. The reward function is modeled by a translated logistic function Rθ(s) = 10/(1 + exp(−θ1 × (ψ(s) −
θ0))) + θ2 where ψ(s) indicates the number of coins present in state s. (b) shows the NLL value of 50 expert demonstrations for {θ0, θ1} with no translation while (c) shows the same for translation by a value of 2. (d) θa and θb are policy invariant and map to the same point in the projected space. θc and θd have a similar likelihood and are mapped to nearby positions. reward functions in the form of a GP posterior. Uncertainty estimates of the NLL for each reward function enable downstream analysis and existing methods such as active learning [23] and active teaching [9] can be used to further narrow down these solutions. Given the beneﬁts above, it may appear surprising that BO has not yet been applied to IRL, considering its application to many different domains [35]. A possible reason may be that BO does not work “out-of-the-box” for IRL despite its apparent suitability. Indeed, our initial naïve application of BO to IRL failed to produce good results.
Further investigation revealed that standard kernels were unsuitable for representing the covariance structure in the space of reward functions. In particular, they ignore policy invariance [3] where a reward function maintains its optimal policy under certain operations such as linear translation.
Leveraging on this insight, we contribute a novel ρ-projection that remedies this problem. Brieﬂy, the
ρ-projection maps policy invariant reward functions to a single point in a new representation space where nearby points share similar NLL; Fig. 1 illustrates this key idea on a Gridworld environment.1
With the ρ-projection in hand, standard stationary kernels (such as the popular RBF) can be applied in a straightforward manner. We provide theoretical support for this property and experiments on a variety of environments (both discrete and continuous, with model-based and model-free settings) show that our BO-IRL algorithm (with ρ-projection) efﬁciently captures the correlation structure of the reward space and outperforms representative state-of-the-art methods. 2 Preliminaries and