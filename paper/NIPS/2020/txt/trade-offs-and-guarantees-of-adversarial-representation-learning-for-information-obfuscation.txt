Abstract
Crowdsourced data used in machine learning services might carry sensitive infor-mation about attributes that users do not want to share. Various methods have been proposed to minimize the potential information leakage of sensitive attributes while maximizing the task accuracy. However, little is known about the theory behind these methods. In light of this gap, we develop a novel theoretical framework for attribute obfuscation. Under our framework, we propose a minimax optimization formulation to protect the given attribute and analyze its inference guarantees against worst-case adversaries. Meanwhile, it is clear that in general there is a tension between minimizing information leakage and maximizing task accuracy.
To understand this, we prove an information-theoretic lower bound to precisely characterize the fundamental trade-off between accuracy and information leakage.
We conduct experiments on two real-world datasets to corroborate the inference guarantees and validate this trade-off. Our results indicate that, among several alternatives, the adversarial learning approach achieves the best trade-off in terms of attribute obfuscation and accuracy maximization. 1

Introduction
With the growing demand for machine learning systems provided as services, a massive amount of data containing sensitive information, such as race, income level, age, etc., are generated and collected from local users. This poses a substantial challenge and it has become an imperative object of study in machine learning [18], computer vision [6, 34], healthcare [2, 3], speech recognition [30], and many other domains. In this paper, we consider a practical scenario where the prediction vendor requests crowdsourced data for a target task, e.g, scientiﬁc modeling. The data owner agrees on the data usage for the target task while she does not want her other sensitive information (e.g., age, race) to be leaked. The goal in this context is then to obfuscate sensitive attributes of the sanitized data released by data owner from potential attribute inference attacks from a malicious adversary.
For example, in an online advertising scenario, while the user (data owner) may agree to share her historical purchasing events, she also wants to protect her age information so that no malicious adversary can infer her age range from the shared data. Note that simply removing age attribute
⇤The ﬁrst two authors contributed equally to this work. Work done while HZ was at Carnegie Mellon
University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
from the shared data is insufﬁcient for this purpose, due to the redundant encoding in data, i.e., other attributes may have a high correlation with age.
Under this scenario, a line of work [4, 14, 19, 22, 24, 25, 32–34] aims to address the problem in the framework of (constrained) minimax problem. However, the theory behind these methods is little known. Such a gap between theory and practice calls for an important and appealing challenge:
Can we prevent the information leakage of the sensitive attribute while still maxi-mizing the task accuracy? Furthermore, what is the fundamental trade-off between attribute obfuscation and accuracy maximization in the minimax problem?
Under the setting of attribute obfuscation, the notion of information conﬁdentiality should be attribute-speciﬁc: the goal is to protect speciﬁc attributes from being inferred by malicious adversaries as much as possible. Note that this is in sharp contrast with differential privacy (we systematically compare the related notions in Sec. 5