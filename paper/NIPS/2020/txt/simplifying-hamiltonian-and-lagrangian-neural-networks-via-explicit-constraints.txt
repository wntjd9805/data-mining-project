Abstract
Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works im-prove generalization for predicting trajectories by learning the Hamiltonian or La-grangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the con-straints explicitly with Lagrange multipliers dramatically simpliﬁes the learning problem. We introduce a series of challenging chaotic and extended-body sys-tems, including systems with N -pendulums, spring coupling, magnetic ﬁelds, rigid rotors, and gyroscopes, to push the limits of current approaches. Our ex-periments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efﬁciency.
Figure 1: By using Cartesian coordinates with explicit constraints, we simplify the Hamiltonians and La-grangians that our models learn, resulting in better long term predictions and data-efﬁciency than Neural ODEs and Hamiltonian Neural Networks (HNNs). Left: a spinning gyroscope with the ground truth trajectory and predictions of each model. Predicted trajectories by our model (CHNN) overlaps almost exactly with the ground truth (black). Middle: Geometric mean of the relative error over 100 timesteps as a function of number of training trajectories. On the gyroscope system, our model can be 100 times more data efﬁcient or 260 times more accurate. Right: The Hamiltonian expressed in Cartesian coordinates is simpler and easier to learn than when expressed in angular coordinates. 1

Introduction
Although the behavior of physical systems can be complex, they can be derived from more abstract functions that succinctly summarize the underlying physics. For example, the trajectory of a physical system can be found by solving the system’s differential equation for the state as a function of time.
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 2: A visualization of how abstracting the physical system reduces the complexity that our model must learn. For systems like the 3-Pendulum, the trajectory is so complex that there is no closed form solution.
Although the dynamics ˙z do have a closed form, they require a long description. The Hamiltonian H of the system is simpler, and modeling at this higher level of abstraction reduces the burden of learning. Separating out the constraints from the learning problem, the Hamiltonian for CHNNs is even more succinct.
For many systems, these differential equations can in turn be derived from even more fundamental functions known as Hamiltonians and Lagrangians. We visualize this hierarchy of abstraction in
Figure 2. Recent work has shown that we can model physical systems by learning their Hamiltonians and Lagrangians from data [9, 14, 20]. However, these models still struggle to learn the behavior of sophisticated constrained systems [2, 3, 7–9, 22]. This raises the question of whether it is possible to improve model performance by further abstracting out the complexity to make learning easier.
Constraints in physical systems are typically enforced by generalized coordinates, which are coordinates formed from any set of variables that describe the complete state of the sys-the 2D 2-pendulum in Figure 3 can be described by two angles rel-For example, tem. ative to the vertical axis, instead of the Cartesian coordinates x of the masses. By expressing functions in generalized coordinates, we ensure that con-like the distances from each pendulum mass to its pivot, are always satisﬁed im-straints, if we have a mechanism to explicitly enforce constraints, we can in-plicitly. However, stead use Cartesian coordinates, which more naturally describe our three dimensional world. labelled as q = (q1, q2),
In this paper, we show that generalized coordi-nates make the Hamiltonian and the Lagrangian of a system difﬁcult to learn. Instead, we pro-pose to separate the dual purposes played by generalized coordinates into independent enti-ties: a state represented entirely in Cartesian coordinates x, and a set of constraints Φ(x) that are enforced explicitly via Lagrange multipli-ers λ. Our approach simpliﬁes the functional form of the Hamiltonian and Lagrangian and allows us to learn complicated behavior more accurately, as shown in Figure 1.
In particular, our paper makes the follow-(1) We demonstrate ana-ing contributions. lytically that embedding problems in Carte-sian coordinates simpliﬁes the Hamiltonian and the Lagrangian that must be learned, result-ing in systems that can be accurately mod-elled by neural networks with 100 times less (2) We show how to learn Hamiltoni-data. ans and Lagrangians in Cartesian coordinates via explicit constraints using networks that we term Constrained Hamiltonian Neural Net-works (CHNNs) and Constrained Lagrangian
Neural Networks (CLNNs). (3) We show how to apply our method to arbitrary rigid extended-body systems by showing how such systems can be embedded purely into Cartesian coordinates. (4) We introduce a series of complex phys-Figure 3: A 2D 2-pendulum expressed in terms of gen-eralized coordinates q and Cartesian coordinates x with explicit constraints Φ(x) = 0 for the Lagrangian for-malism and the constrained Lagrangian formalism. L is the Lagrangian, a scalar function that summarizes the entire behavior of the system, entries of λ are the La-grange multipliers, and S is a functional that is mini-mized by the system’s true trajectory. 2
ical systems, including chaotic and 3D extended-body systems, that challenge current approaches to learning Hamiltonians and Lagrangians. On these systems, our explicitly-constrained CHNNs and CLNNs are 10 to 100 times more accurate than HNNs [9] and DeLaNs [14], which are implicitly-constrained models, and more data-efﬁcient. Code for our experiments can be found at: https://github.com/mfinzi/constrained-hamiltonian-neural-networks. 2