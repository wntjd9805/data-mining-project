Abstract
It has been recently demonstrated that multi-generational self-distillation can im-prove generalization [11]. Despite this intriguing observation, reasons for the enhancement remain poorly understood. In this paper, we ﬁrst demonstrate experi-mentally that the improved performance of multi-generational self-distillation is in part associated with the increasing diversity in teacher predictions. With this in mind, we offer a new interpretation for teacher-student training as amortized MAP estimation, such that teacher predictions enable instance-speciﬁc regularization.
Our framework allows us to theoretically relate self-distillation to label smoothing, a commonly used technique that regularizes predictive uncertainty, and suggests the importance of predictive diversity in addition to predictive uncertainty. We present experimental results using multiple datasets and neural network architectures that, overall, demonstrate the utility of predictive diversity. Finally, we propose a novel instance-speciﬁc label smoothing technique that promotes predictive diversity with-out the need for a separately trained teacher model. We provide an empirical evaluation of the proposed method, which, we ﬁnd, often outperforms classical label smoothing. 1

Introduction
First introduced as a simple method to compress high-capacity neural networks into a low-capacity counterpart for computational efﬁciency, knowledge distillation [15] has since gained much popularity across various application domains ranging from computer vision to natural language processing [19, 22, 28, 39, 41] as an effective method to transfer knowledge or features learned from a teacher network to a student network. This empirical success is often justiﬁed with the intuition that deeper teacher networks learn better representation with greater model complexity, and the "dark knowledge" that teacher networks provide facilitates student networks to learn better representations and hence enhanced generalization performance. Nevertheless, it still remains an open question as to how exactly student networks beneﬁt from this dark knowledge. The problem is made further puzzling by the recent observation that even self-distillation, a special case of the teacher-student training framework in which the teacher and student networks have identical architectures, can lead to better generalization performance [11]. It was also demonstrated that repeated self-distillation process with multiple generations can further improve classiﬁcation accuracy.
In this work, we aim to shed some light on self-distillation. We start off by revisiting the multi-generational self-distillation strategy, and experimentally demonstrate that the performance improve-ment observed in multi-generational self-distillation is correlated with increasing diversity in teacher predictions. Inspired by this, we view self-distillation as instance-speciﬁc regularization on the neural network softmax outputs, and cast the teacher-student training procedure as performing amortized maximum a posteriori (MAP) estimation of the softmax probability outputs. The proposed frame-work provides us with a new interpretation of the teacher predictions as instance-speciﬁc priors conditioned on the inputs. This interpretation allows us to theoretically relate distillation to label smoothing, a commonly used technique to regularize predictive uncertainty of NNs, and suggests 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
that regularization on the softmax probability simplex space in addition to the regularization on predictive uncertainty can be the key to better generalization. To verify the claim, we systematically design experiments to compare teacher-student training against label smoothing. Lastly, to further demonstrate the potential gain from regularization on the probability simplex space, we also design a new regularization procedure based on label smoothing that we term “Beta smoothing.”
Our contributions can be summarized as follows: 1. We provide a plausible explanation for recent ﬁndings on multi-generational self-distillation. 2. We offer an amortized MAP interpretation of the teacher-student training strategy. 3. We attribute the success of distillation to regularization on both the label space and the softmax probability simplex space, and verify the importance of the latter with systematically designed experiments on several benchmark datasets. 4. We propose a new regularization technique termed “Beta smoothing” that improves upon classical label smoothing at little extra cost. 5. We demonstrate self-distillation can improve calibration. 2