Abstract
This work addresses efﬁcient inference and learning in switching Gaussian linear dynamical systems using a Rao-Blackwellised particle ﬁlter and a corresponding
Monte Carlo objective. To improve the forecasting capabilities, we extend this classical model by conditionally linear state-to-switch dynamics, while leaving the partial tractability of the conditional Gaussian linear part intact. Furthermore, we use an auxiliary variable approach with a decoder-type neural network that allows for more complex non-linear emission models and multivariate observations.
We propose a Monte Carlo objective that leverages the conditional linearity by computing the corresponding conditional expectations in closed-form and a suitable proposal distribution that is factorised similarly to the optimal proposal distribution.
We evaluate our approach on several popular time series forecasting datasets as well as image streams of simulated physical systems. Our results show improved forecasting performance compared to other deep state-space model approaches. 1

Introduction
The Gaussian linear dynamical system (GLS) [4, 38, 31] is one of the most well-studied dynamical models with wide-ranging applications in many domains, including control, navigation, and time-series forecasting. This state-space model (SSM) is described by a (typically Markovian) latent linear process that generates a sequence of observations. The assumption of Gaussian noise and linear state transitions and measurements allows for exact inference of the latent variables—such as ﬁltering and smoothing—and computation of the marginal likelihood for system identiﬁcation (learning).
However, most systems of practical interest are non-linear, requiring more complex models.
Many approximate inference methods have been developed for non-linear dynamical systems: Deter-ministic methods approximate the ﬁltering and smoothing distributions e.g. by using a Taylor series expansion of the non-linear functions (known as extended Kalman ﬁlter (EKF) and second-order
EKF) or by directly approximating these marginal distributions by a Gaussian using moment matching techniques [26, 27, 2, 3, 36]. Stochastic methods such as particle ﬁlters or smoothers approximate the ﬁltering and smoothing distributions by a set of weighted samples (particles) using sequential
Monte Carlo (SMC) [13, 16]. Furthermore, system identiﬁcation with deep neural networks has been proposed using stochastic variational inference [17, 15] and variational SMC [23, 29, 20].
A common challenge with both types of approximations is that predictions/forecasts over long forecast horizons suffer from accumulated errors resulting from insufﬁcient approximations at every time step.
Switching Gaussian linear systems (SGLS) [1, 12]—which use additional latent variables to switch between different linear dynamics—provide a way to alleviate this problem: the conditional linearity can be exploited by approximate inference algorithms to reduce approximation errors. Unfortunately, this comes at the cost of reduced modelling ﬂexibility compared to more general non-linear dynamical systems. Speciﬁcally, we identify the following two weaknesses of the SGLS: i) the switch transition
∗Correspondence to richard.kurle@tum.de. †Work done while at AWS AI Labs. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
model is assumed independent of the GLS state and observation; ii) conditionally-linear emissions are insufﬁcient for modelling complex multivariate data streams such as video data, while more suitable emission models (e.g. using convolutional neural networks) exist. The ﬁrst problem has been addressed in [21] through augmentation with a Polya-gamma-distributed variable and a stick-breaking process. However, this approach uses Gibbs sampling to infer model parameters and thus does not scale well to large datasets. The second problem is addressed in [11] using an auxiliary variable between GLS states and emissions and stochastic variational inference to obtain a tractable objective function for learning. Yet, this model predicts the GLS parameters deterministically using an LSTM with an auto-regressive component, resulting in poor long-term forecasts.
We propose auxiliary variable recurrent switching Gaussian linear systems (ARSGLS), an extension of the SGLS to address both weaknesses by building on ideas from [21] and [11]. ARSGLS improves upon the SGLS by incorporating a conditionally linear state-to-switch dependence, which is crucial for accurate long-term out-of-sample predictions (forecasting), and a decoder-type neural network that allows modelling multivariate time-series data with a non-linear emission/measurement process.
As in the SGLS, a crucial feature of ARSGLS is that approximate inference and likelihood estimation can be Rao-Blackwellised, that is, expectations involving the conditionally linear part of the model can be computed in closed-form, and only the expectations wrt. the switch variables need to be approximated. We leverage this feature and propose a Rao-Blackwellized ﬁltering objective function and a suitable proposal distribution for this model class.
We evaluate our model with two different instantiations of the GLS: in the ﬁrst scenario, the GLS is implemented with a constrained structure that models time-series patterns such as level, trend and seasonality [14, 34]; the second scenario considers a general, unconstrained conditional GLS. We compare our approach to closely related methods such as Deep State Space Models (DeepState)
[30] and Kalman Variational Auto-Encoders (KVAE) [11] on 5 popular forecasting datasets and multivariate (image) data streams generated from a physics engine as used in [11]. Our results show that the proposed model achieves improved performance for univariate and multivariate forecasting. 2