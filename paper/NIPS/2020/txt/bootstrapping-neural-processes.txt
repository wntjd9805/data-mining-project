Abstract
Unlike in the traditional statistical modeling for which a user typically hand-specify a prior, Neural Processs (NPs) implicitly deﬁne a broad class of stochastic processes with neural networks. Given a data stream, NP learns a stochastic process that best describes the data. While this “data-driven” way of learning stochastic processes has proven to handle various types of data, NPs still rely on an assumption that uncertainty in stochastic processes is modeled by a single latent variable, which potentially limits the ﬂexibility. To this end, we propose the Bootstrapping Neural
Process (BNP), a novel extension of the NP family using the bootstrap. The bootstrap is a classical data-driven technique for estimating uncertainty, which allows BNP to learn the stochasticity in NPs without assuming a particular form. We demonstrate the efﬁcacy of BNP on various types of data and its robustness in the presence of model-data mismatch. 1

Introduction
Neural Process (NP) [8] is a class of stochastic processes deﬁned by parametric neural networks.
Traditional stochastic processes such as Gaussian Process (GP) [19] are usually derived from mathe-matical objects based on certain prior beliefs on data (e.g., smoothness of functions quantiﬁed by
Gaussian distributions). On the other hand, given a stream of data, NP learns to construct a stochastic process that might describe the data well. In that sense, NP may be considered as a data-driven way of deﬁning stochastic processes. When appropriately trained, NP can deﬁne a ﬂexible class of stochastic processes well suited for highly non-trivial functions that are not easily represented by existing stochastic processes.
Like other stochastic processes, NP induces stochasticity in function realizations. More speciﬁcally,
NP deﬁnes a function value y for a point x as a conditional distribution p(y|x, . . . ) to model point-wise uncertainty. Additionally, NP further introduces a global latent variable capturing functional uncertainty - a global uncertainty in the overal structure of the function. The global latent variable modeling functional uncertainty is empirically demonstrated to improve the predictive performance and diversity in function realizations [14].
Although it is clear both intuitively and empirically that adding functional uncertainty helps, it remains unclear whether modeling it with a single Gaussian latent variable is optimal. For instance, [16] pointed out that the global latent variable acts as a bottleneck. One could introduce more complex architectures to better capture the functional uncertainty, but that would typically come with an architectural overhead. Moreover, it contradicts the philosophy behind NP to use minimal modeling assumptions and let the model learn from data.
∗ Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
This paper introduces a novel way of introducing functional uncertainty to the family of NP models.
We revisit the bootstrap [6], a classic frequentist technique to model uncertainty in parameter estimation by simulating population distribution via resampling. The bootstrap is a simple yet effective way of modelling uncertainty in a data-driven way, making it well-suited for our purpose of giving uncertainty to NP with minimal modeling assumptions. To this end, we propose BNP, an extension of NP using bootstrap to induce functional uncertainty. BNP utilizes bootstrap to construct multiple resampled datasets and combines the predictions computed from them. The functional uncertainty is then naturally induced by the uncertainty in the bootstrap procedure.
BNP can be deﬁned for any existing NP variants with minimal additional parameters and provides several beneﬁts over existing models. One important aspect is its robustness under the presence of model-data mismatch, where test data come from distributions different from the one used to train the model. An ensemble of bootstrap is well known to enhance the stability and accuracy [1]. Recently,
[11] showed that ensembling Bayesian posteriors from multiple bootstrap samples dramatically improves the robustness under model-data mismatch. We show that our extension of NP with bootstrap also enjoys this property. Using various data ranging from simple synthetic data to challenging real-world data, we demonstrate that BNP is much more robust than the existing NP with global latent variables. This tendency was particularly strong under model-data mismatch, where the test data is signiﬁcantly different from the datasets used to train the model. 2