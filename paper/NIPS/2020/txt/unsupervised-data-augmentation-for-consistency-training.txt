Abstract
Semi-supervised learning lately has shown much promise in improving deep learn-ing models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, speciﬁcally those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAug-ment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework.
On the IMDb text classiﬁcation dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark,
CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when ﬁnetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used.1 1

Introduction
A fundamental weakness of deep learning is that it typically requires a lot of labeled data to work well. Semi-supervised learning (SSL) [1] is one of the most promising paradigms of leveraging unlabeled data to address this weakness. The recent works in SSL are diverse but those that are based on consistency training [2, 3, 4, 5] have shown to work well on many benchmarks.
In a nutshell, consistency training methods simply regularize model predictions to be invariant to small noise applied to either input examples [6, 7, 8] or hidden states [2, 4]. This framework makes sense intuitively because a good model should be robust to any small change in an input example or hidden states. Under this framework, different methods in this category differ mostly in how and where the noise injection is applied. Typical noise injection methods are additive Gaussian noise, dropout noise or adversarial noise.
In this work, we investigate the role of noise injection in consistency training and observe that advanced data augmentation methods, speciﬁcally those work best in supervised learning [9, 10, 11, 12], also perform well in semi-supervised learning. There is indeed a strong correlation between the performance of data augmentation operations in supervised learning and their performance in consistency training. We, hence, propose to substitute the traditional noise injection methods with high quality data augmentation methods in order to improve consistency training. To emphasize the 1Code is available at https://github.com/google-research/uda. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
use of better data augmentation in consistency training, we name our method Unsupervised Data
Augmentation or UDA.
We evaluate UDA on a wide variety of language and vision tasks. On six text classiﬁcation tasks, our method achieves signiﬁcant improvements over state-of-the-art models. Notably, on IMDb, UDA with 20 labeled examples outperforms the state-of-the-art model trained on 1250x more labeled data.
On standard semi-supervised learning benchmarks CIFAR-10 and SVHN, UDA outperforms all existing semi-supervised learning methods by signiﬁcant margins and achieves an error rate of 5.43 and 2.72 with 250 labeled examples respectively. Finally, we also ﬁnd UDA to be beneﬁcial when there is a large amount of supervised data. For instance, on ImageNet, UDA leads to improvements of top-1 accuracy from 58.84 to 68.78 with 10% of the labeled set and from 78.43 to 79.05 when we use the full labeled set and an external dataset with 1.3M unlabeled examples.
Our key contributions and ﬁndings can be summarized as follows:
•
•
•
•
First, we show that state-of-the-art data augmentations found in supervised learning can also serve as a superior source of noise under the consistency enforcing semi-supervised framework. See results in Table 1 and Table 2.
Second, we show that UDA can match and even outperform purely supervised learning that uses orders of magnitude more labeled data. See results in Table 4 and Figure 4.
State-of-the-art results for both vision and language tasks are reported in Table 3 and 4. The effectiveness of UDA across different training data sizes are highlighted in Figure 4 and 7.
Third, we show that UDA combines well with transfer learning, e.g., when ﬁne-tuning from BERT (see Table 4), and is effective at high-data regime, e.g. on ImageNet (see Table 5).
Lastly, we also provide a theoretical analysis of how UDA improves the classiﬁcation performance and the corresponding role of the state-of-the-art augmentation in Section 3. 2 Unsupervised Data Augmentation (UDA)
In this section, we ﬁrst formulate our task and then present the key method and insights behind UDA.
Throughout this paper, we focus on classiﬁcation problems and will use x to denote the input and y⇤ to denote its ground-truth prediction target. We are interested in learning a model p✓(y x) to predict y⇤ based on the input x, where ✓ denotes the model parameters. Finally, we will use pL(x) and pU (x) to denote the distributions of labeled and unlabeled examples respectively and use f ⇤ to denote the perfect classiﬁer that we hope to learn.
| 2.1