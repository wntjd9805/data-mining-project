Abstract
As classiﬁcations datasets progressively get larger in terms of label space and number of examples, annotating them with all labels becomes non-trivial and expensive task. For example, annotating the entire OpenImage test set [32] can cost $6.5M. Hence, in current large-scale benchmarks such as OpenImages [32] and LVIS [16], less than 1% of the labels are annotated across all images. Standard classiﬁcation models are trained in a manner where these un-annotated labels are ignored. Ignoring these un-annotated labels result in loss of supervisory signal which reduces the performance of the classiﬁcation models. Instead, in this paper, we exploit relationships among images and labels to derive more supervisory signal from the un-annotated labels. We study the effectiveness of our approach across several multi-label computer vision benchmarks, such as CIFAR100 [31], MS-COCO panoptic segmentation [27], OpenImage [32] and LVIS [16] datasets. Our approach can outperform baselines by a margin of 2-10% across all the datasets on mean average precision (mAP) and mean F1 metrics. 1

Introduction
In the past decade, deep networks have shown promising results on a wide variety of applications and tasks, such as image classiﬁcation [19, 30, 52], image segmentation [17, 18, 40, 43, 50], object detection [9, 14, 18, 49] and vision language tasks, e.g. image captioning [26, 60, 65], visual question answering [1, 2, 15, 37, 51], etc. As the number of applications increase, the number of visual concepts that are needed to be recognized have also increased. The models need to reason across a wide breadth as well as depth of visual concepts. In Fig. 1, we can see the diversity and ﬁne-graininess of the label categories that can be deﬁned across the images of a dataset. Annotating such large-scale benchmarks is an arduous task [12, 29] since this process needs veriﬁcation of all possible labels across all images. It is expensive and practically infeasible. For example, using the
Amazon Mechanical Turk service to verify all 9.6K labels of the entire OpenImage test set [32] of 116K images (not a particularly large number of images for computer vision tasks) would cost $6.5M.
For specialized domains such as chest radiographs, annotations are ambiguous and must come from high-skilled annotators (aka, doctors in this case) [22, 24].
To overcome this challenge, large scale datasets such as OpenImages [32] and LVIS [16] have adopted the concept of federated datasets [38], where a single dataset is formed by the union of a large number of smaller constituent datasets for a single category. For each category, c, there exists two disjoint subsets of the entire dataset, Pc and Nc, which contain the positive and negative images associated with the class label, c respectively. For OpenImages and LVIS, the sum of positive and negative images per category (on average) is 2-3 orders of magnitude less than the total number of images.
Such a process can not only reduce the annotation cost, but also speed up the effort of adding new labels to the dataset. This strategy has been used to fast track annotation of COVID-19 cases [10]
∗Primary correspondence 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
s e g a m
I s o
P g e
N
Man, Human head, Human arm, Human hair,
Clothing, Girl, Human mouth, Cosplay,
Woman, Person, Costume, Human face,
Superhero, Boy
Real tennis
Comics, Convention, Anime
Tennis polo
Real tennis, Racketlon, Tennis skirt, Person,
Woman, Sports equipment, Clothing, Ball,
Strings, Tennis Equipment, Tennis player,
Tennis, Soft tennis, Racquet sport, Human face, Racket
Hockey, Girl, Rackets, Soccer, Tennis polo,
Statue, Basketball (Sport), Elbow, Baseball,
Bowling (c) (a) (b)
Figure 1: Examples of images from OpenImages v6 dataset alongside existing X-ray imaging datasets such as [24, 62]. In Fig. 1, we show some examples of images and their corresponding positive and negative labels from the OpenImages [32] dataset. It can been seen that similar images can have a large variation in the labels that are annotated. For example, human head is present as a positive label in Fig. 1 (a), but not in (b) or (c). While this design principle of enables the computer vision industry and the research community to construct and evaluate on large-scale benchmarks efﬁciently, it also forces the community to reconsider how to train deep networks on such datasets. Traditional methods to train the classiﬁcation models on partially annotated datasets simply ignore the un-annotated labels during training but that leaves valuable training signal unused.
In this paper, we explore relationships among the images and labels to derive supervisory signal from the un-annotated label categories. For example, in both Fig. 1 (b) and (c), there is a person with a tennis racket. If the visual representations of these images are close by, we can use the annotations from each other. Thus the person label in Fig. 1 (c) can be considered as a positive label for the image in Fig. 1 (b). We can also exploit label relationships in a similar fashion. In Fig. 1 (c), we have person as a positive label, but not human arm or human head. If the representation of human arm and human head is close to that of the person, we can consider them as positive labels as well. Such relationships have been explored in the domain adapation [33] and few shot learning literature [48, 59], where such relationships have been reasoned across multiple datasets, but here we reason about these relationships within the same dataset.
Recently, [13] introduced the problem of training deep neural networks on partially annotated datasets.
It works by ﬁrst training a classiﬁcation network while ignoring the un-annotated labels. Then, it uses a graph convolutional network to bootstrap on the un-annotated labels and hence expand the annotated labels. In this paper, we introduce a simple, yet effective baseline, where all un-annotated labels are considered as negative. Similar approaches have been attempted for noisy label classiﬁcation, where the number of un-annotated labels or incorrect labels are signiﬁcantly less compared to the partial label scenario. While this may not be intuitive for the partial labeling scenario, we observe that this baseline with a simple class-weighting scheme can sometimes out-perform or perform similar to the training approaches where the un-annotated labels are ignored [13, 21]. We hypothesize that this might be because the number of positive labels are orders of magnitude smaller than the negative labels [35, 32, 16]. Hence, this noisy, negative supervisory signal results in learning better representations compared to the scenario where we use a clean, but signiﬁcantly less supervision signal. While this baseline improves classiﬁer performance by exposing the model to signiﬁcantly more data, the assumption that all un-annotated labels are negative is clearly incorrect. This form of noise cannot be modeled trivially since it depends on complex factors involving annotation budget and ad hoc choices involved during the annotation process. To overcome this deﬁciency, we exploit image and label relationships to soften the noise from our new baseline. Instead of ignoring the un-annotated labels or assigning them as strict negative labels, we use image and label relationships to consider the un-annotated labels in a “soft” manner. We model this softness using knowledge distillation [20]. We evaluate the performance of our multi-label classiﬁcation models on challenging datasets such as CIFAR100 [31], MS COCO detection [35], MS COCO panoptic segmentation [27], 2
OpenImage [32] and LVIS [16]. Our code will be made open source upon acceptance. In summary our contributions are:
• We introduce a simple baseline that outperforms previous methods on the task of training from partially labeled datasets.
• We present a novel method based on image and label relationships to soften the strict negative supervisory signal imposed by our baseline.
• We show the effectiveness of both our baseline and novel approach on 5 challenging, large-scale vision datasets.
• We release all code to reproduce the results presented in this paper. 2