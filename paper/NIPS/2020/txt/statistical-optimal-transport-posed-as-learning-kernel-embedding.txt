Abstract
The objective in statistical Optimal Transport (OT) is to consistently estimate the optimal transport plan/map solely using samples from the given source and target marginal distributions. This work takes the novel approach of posing statistical
OT as that of learning the transport plan’s kernel mean embedding from sample based estimates of marginal embeddings. The proposed estimator controls over-ﬁtting by employing maximum mean discrepancy based regularization, which is complementary to φ-divergence (entropy) based regularization popularly employed in existing estimators. A key result is that, under very mild conditions, (cid:15)-optimal recovery of the transport plan as well as the Barycentric-projection based transport map is possible with a sample complexity that is completely dimension-free. More-over, the implicit smoothing in the kernel mean embeddings enables out-of-sample estimation. An appropriate representer theorem is proved leading to a kernelized convex formulation for the estimator, which can then be potentially used to perform
OT even in non-standard domains. Empirical results illustrate the efﬁcacy of the proposed approach. 1

Introduction
Optimal Transport is proving to be an increasingly successful tool in solving diverse machine learning problems. Recent research shows that variants of Optimal Transport (OT) achieve state-of-the-art performance in various machine learning (ML) applications such as data alignment/integration [2, 24, 42, 19], domain adaptation [7, 34], model interpolation/combination [38, 35, 9], natural language processing [43, 44] etc. It is also shown that OT based (Wasserstein) metrics serve as good loss functions in both supervised [13, 20] and unsupervised [16] learning.
Given two marginal distributions over source and target domains, and a cost function between elements of the domains, the classical OT problem (Kantorovich’s formulation) is that of ﬁnding the joint distribution whose marginals are equal to the given marginals, and which minimizes the expected cost with respect to this joint distribution [22]. This joint distribution is known as the (optimal) transport plan or the optimal coupling. A related object of interest for ML applications is the so-called Barycentric-projection based transport map corresponding to a transport plan (e.g., refer Equation (11) in [37]). Though OT techniques already improve state-of-the-art in many ML applications, there are two main bottlenecks that seem to limit OT’s success in ML settings:
• while continuous distributions are ubiquitous, algorithms for ﬁnding the transport plan/map over continuous domains are very scarce [15]. The situation is worse in case of non-standard domains, which are not uncommon in ML.
• the marginal distributions are never available, and merely samples from them are given. The variant of OT where the transport plan/map needs to be estimated merely using samples from the marginals is known as the statistical OT problem. Unfortunately, this estimation problem is plagued with the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
curse of dimensionality: the sample complexity of O(m−1/d), where m is number of samples, and d is the dimensionality of data, cannot be improved without further assumptions [29].
Though several works alleviated the curse of dimensionality [14, 29, 18], none of them completely remove the adversarial dependence on dimensionality. Further, authors in [15, 14, 12, 4] comment that estimators that are free from the curse of dimensionality are important, yet not well-studied. The concluding report from a recent workshop on OT (refer section 2 in [4]) summarizes that one of the major open problems in this area is to design estimators in context of continuous statistical OT whose sample complexity is not a strong function of the dimension (ideally dimension-free).
Our work focuses on this challenging and important problem of statistical OT over continuous domains, and seeks consistent estimators for (cid:15)-optimal transport plan/map, whose sample complexity is dimension-free. To this end, we take the novel approach of equivalently re-formulating the statistical OT problem solely in terms of the relevant kernel mean embeddings [26]. More speciﬁcally, our formulation ﬁnds the (characterizing) kernel mean embedding of a joint distribution with least expected cost, and whose marginal embeddings are close to the given-sample based estimates of the marginal embeddings. There are several advantages of this new approach: 1. because the samples based estimates of the kernel mean embeddings of the marginals are known to have sample complexities that are dimension-free, it is expected that the sample complexity remains dimension-free even for the proposed estimator of the transport plan embedding. 2. kernel embeddings provide implicit smoothness, as controlled by the kernel. Appropriate smooth-ness not only improves the quality of estimation, but also enable out-of-sample estimation. 3. since Maximum Mean Discrepancy (MMD) is the natural notion of distance in the kernel mean em-bedding space, this reformulation facilitates MMD based regularization for controlling overﬁtting.
Such regularizers are complementary to the φ-divergence (or entropy) based regularizers popularly employed in existing estimators. [40] observe that MMD and φ-divergence based regularization exhibit complementary properties and hence both are interesting to study.
A key result from this work is that, under very mild conditions, the proposed methodology can recover an (cid:15)-optimal transport plan and corresponding (Barycentric-projection based) transport map with a sample complexity, O(m−1/2), which is completely1 dimension-free. Another contribution is an appropriate representer theorem that guarantees ﬁnite characterization for the transport plan embedding, leading to a fully kernelized and convex formulation for the estimation. Thus the same formulation can potentially be used for obtaining estimators with all variants of OT: continuous, semi-discrete, and discrete, merely by switching the kernel between the Kronecker delta and the
Gaussian kernels. More importantly, the same can be used to solve OT problems in non-standard domains using appropriate universal kernels [5]. Finally, we discuss special cases where the proposed convex formulation can be solved efﬁciently using ADMM based solver [3]. Empirical results on synthetic and real-world datasets illustrate the efﬁcacy of the proposed approach. The proofs of all the theorems discussed in this paper are provided in the technical report [28]. 2