Abstract
In this paper, we propose a number of novel techniques and numerical representa-tion formats that enable, for the very ﬁrst time, the precision of training systems to be aggressively scaled from 8-bits to 4-bits. To enable this advance, we ex-plore a novel adaptive Gradient Scaling technique (GradScale) that addresses the challenges of insufﬁcient range and resolution in quantized gradients as well as explores the impact of quantization errors observed during model training. We theoretically analyze the role of bias in gradient quantization and propose solutions that mitigate the impact of this bias on model convergence. Finally, we examine our techniques on a spectrum of deep learning models in computer vision, speech and
NLP. In combination with previously proposed solutions for 4-bit quantization of weight and activation tensors, 4-bit training shows non-signiﬁcant loss in accuracy across application domains while enabling signiﬁcant hardware acceleration (>7× over state of the art FP16 systems). 1

Introduction
Over the past decade, Deep Neural Networks (DNNs) have surpassed traditional Machine Learning techniques on a wide spectrum of application domains including speech [1–3], computer vision
[4–10], and language [11, 12]. As models and datasets have grown in complexity, DNN training times have increased signiﬁcantly—limiting the pace of innovation in model training and deployment life cycles.
Driven by a number of key advances in low-precision arithmetic [13–16] and the quadratic dependency of throughput on precision, reduced-precision training has become the de facto technique to boost the performance and power efﬁciency of deep learning hardware. 16-bit ﬂoating point training formats have been integrated into multiple accelerator products [17–19] and shown to provide 4-8 times the performance of 32-bit hardware designs. Recently, new insights into low-precision accumulation errors, batchnorm statistics and novel 8-bit ﬂoating point formats have resulted in successful demonstrations of 8-bit training for a wide range of deep learning tasks—resulting in a further >2-4× boost [15] in training system performance over 16-bit systems. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Meanwhile, there has been tremendous progress in the use of ultra-low precision formats (2-4 bits) for inference [20–27]. 4-bit inference [21] (where weights and activations are quantized in 4-bit integer formats) has been shown to fully preserve model accuracy and provide signiﬁcant acceleration in comparison to 8-bit integer systems [28] used widely for inference [17]. Unfortunately, since approximately 2/3rd of the training time and operations are spent on the backward GEMM and update GEMM phases, acceleration of the entire training workload will necessitate (a) representing the gradients in 4-bits formats, and (b) computing the backward and update phase entirely in 4-bits representation. To the best of our knowledge, no studies so far have demonstrated deep learning model convergence using 4-bits for all tensors (weights, activations and gradients).
While 8-bit ﬂoating point formats appear to be sufﬁcient for training [14, 19], 4-bit gradient repre-sentations appear challenging from a quantization error (rounding), precision and dynamic range perspective. This results in signiﬁcant model optimization and generalization difﬁculties. Further-more, tensors used in forward and backpropagation may require dramatically different numerical representations and range. Indeed, [15] demonstrated that forward ﬂoating-point tensors require a higher resolution and a lower range in comparison to backward gradients. This issue is further exacerbated if low precision (4-bit) integers are used to represent weights and activations for inference and in general for training proposals that utilize the same number formats for all tensors (including 8-bit ﬁxed-point[16] or the 5-bit logarithmic format[29]). These results point to a direction where successful 4-bit training will likely necessitate different numerical formats for representing different tensors in the forward and backward phases of training.
In this work we demonstrate, for the very ﬁrst time, an end-to-end solution that uses 4-bits for the vast majority of the computations needed during DNN training. We propose a novel 4-bit ﬂoating point format, rounding schemes, as well as new gradient scaling techniques that minimize gradient representation, precision and range challenges. These methods enable model convergence with negligible accuracy loss for a spectrum of deep learning benchmarks. In addition, instead of using the same 4-bit gradient format to represent weights and activations, we integrate state-of-the-art 4-bit inference methodologies alongside our techniques. Furthermore, we note that the design of our 4-bit training solution is guided by generations of deep learning hardware design expertise as well as compiler driven performance optimizations. 2