Abstract
Coreset is usually a small weighted subset of n input points in Rd, that provably approximates their loss function for a given set of queries (models, classiﬁers, etc.). Coresets become increasingly common in machine learning since existing heuristics or inefﬁcient algorithms may be improved by running them possibly many times on the small coreset that can be maintained for streaming distributed data. Coresets can be obtained by sensitivity (importance) sampling, where its size is proportional to the total sum of sensitivities. Unfortunately, computing the sensitivity of each point is problem dependent and may be harder to compute than the original optimization problem at hand. We suggest a generic framework for computing sensitivities (and thus coresets) for wide family of loss functions which we call near-convex functions. This is by suggesting the f -SVD factorization that generalizes the SVD factorization of matrices to functions. Example applications include coresets that are either new or signiﬁcantly improves previous results, such as SVM, Logistic regression, M-estimators, and (cid:96)z-regression. Experimental results and open source are also provided. 1

Introduction
In common machine learning problems, we are given a set of input points P ⊆ Rd (training data), and a loss function f : P × Rd → [0, ∞), where the goal is to solve the optimization problem of
ﬁnding a query (model, classiﬁers, centers) x∗ that minimizes the sum of ﬁtting errors (cid:80) p∈P f (p, x) over every query x in a given (usually inﬁnite) set. For example, in k-median (or k-mean) clustering, each query is a set of k centers and the loss function is the distance (or squared distance) of a point to its nearest center. In linear regression or SVM, every input point includes a label, and the loss function is the ﬁtting error between the classiﬁcation of p via a given query to the actual label of p.
Empirical risk minimization (ERM) may be used to generalize the result from train to test data.
Modern machine learning. In practice, many of these optimization or learning problems are usually hard even to approximate. Instead, practical heuristics with no provable guarantees may be used to solve them. Even for well understood problems, which have close optimal solution, such as linear regression or classes of convex optimization, in the era of big data we may wish to maintain the solution in other computation models such as: streaming input data (“on-the-ﬂy") that provably uses small memory, parallel computations on distributed data (on the cloud, network or GPUs) as well as deletion of points, constrained optimization (e.g. sparse classiﬁers). Cross validation [34] or hyper-parameter tuning techniques such as AutoML [30, 32] need to evaluate many queries for different subsets of the data, and different constraints.
Coresets. One approach is to redesign existing machine learning algorithms for faster, approximate solutions and these new computation models. A different approach that is to use data summarization techniques. Coresets in particular were ﬁrst used to solve problems in computational geometry [1] and 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
got increasing attention [3, 4, 5, 6, 8, 20, 26, 27, 38] over the recent years; see surveys in [23, 47, 50].
Informally, coreset is a small weighted subset of the input points (unlike e.g. sketches, or dimension-reduction techniques) that approximates the loss of the input set P for every feasible query x, up to a provable bound of 1 ± ε for a given error parameter ε ∈ (0, 1). The size of the coreset is usually polynomial in 1/ε but independent or near-logarithmic in the size of the input. Since such a coreset approximates every query (and not just the optimal one), it supports constraint optimization, and the above computation models using merge-and-reduce trees; see details in [23]. Moreover, coresets may be computed in time that is near-linear in the input, even for NP-hard optimization problems.
Existing heuristic or inefﬁcient algorithms may then be applied many times on the small coreset to obtain improved or faster models in such cases.
Example coresets in machine learning include SVM [33, 57, 58, 59, 60], (cid:96)z-regression [18, 21, 54], clustering [2, 16, 24, 31, 37, 42, 53], logistic regression [35, 47], LMS solvers and SVD [28, 44, 45, 52], where all of these works have been dedicated to suggest a coreset for a speciﬁc problem.
A generic framework for constructing coresets was suggested in [25, 40].
It states that, with high probability, non-uniform sampling from the input set yields a coreset. Each point should be sampled i.i.d. with a probability that is proportional to its importance or sensitivity, and assigned a multiplicative weight which is inverse proportional to this probability, so that the expected original sum of losses over all the points will be preserved. Here, the sensitivity of an input point p ∈ P is deﬁned to be the maximum of its relative ﬁtting loss s(p) = f (p, x)/ (cid:80) q∈P f (q, x) over every possible query x. The size of the coreset is near-linear in the total (sum) t of these sensitivities; see
Theorem 3 for details. It turns out in the recent years that many classical and hard machine learning problems [7, 43, 55] have total sensitivity that is near-logarithmic or independent of the input size
|P | which implies small coresets via sensitivity sampling.
Paper per problem. The main disadvantage of this framework is that the sensitivity s(p), as de-ﬁned above, is problem dependent: namely on the loss function f and the feasible set of queries.
Moreover, maximizing s(p) = f (p, x)/ (cid:80) q∈P f (q, x) is equivalent to minimizing the inverse (cid:80) q∈P f (q, x)/f (p, x). Unfortunately, minimizing the enumerator is usually the original optimiza-tion problem which motivated the coreset in the ﬁrst place. The denominator may make the problem harder, in addition to the fact that now we need to solve this optimization problem for each and every input point in P . While approximations of the sensitivities usually sufﬁce, sophisticated and different approximation techniques are frequently tailored in papers of recent machine learning conferences for each and every problem. 1.1 Problem Statement
To this end, the goal of this paper is to suggest a framework for sensitivity bounding of a family of functions, and not for a speciﬁc optimization problem. This approach is inspired by convex optimization: while we do not have a single algorithm to solve any convex optimization, we do have generic solutions for family of convex functions. E.g., linear programming, Semi-deﬁnite programming, and so on.
We choose the following family of near-convex loss functions, with example supervised and unsuper-vised applications that include support vector machines, logistic regression, (cid:96)z-regression for any z ∈ (0, ∞), and functions that are robust to outliers. In the Supplementary Material we suggest a more generalized version that handles a bigger family of functions; see Deﬁnition 13, and hope that this paper will inspire the research of more and larger families.
Deﬁnition 1 (Near-convex functions). Let P ⊆ Rd be a set of n points, and let f : P × Rd → [0, ∞) be a loss function. We call f a near-convex function if there are a convex loss function g : P × Rd →
[0, ∞) (see Deﬁnition 12 at Supplementary Material), a function h : P × Rd → [0, ∞), and a scalar z > 0 satisfying: (i) There exist c1, c2 > 0 such that for every p ∈ P , and x ∈ Rd, c1 (g(p, x)z + h(p, x)z) ≤ f (p, x) ≤ c2 (g(p, x)z + h(p, x)z) . (ii) For every p ∈ P , x ∈ Rd and b > 0, we have g(p, bx) = b · g(p, x). (iii) For every p ∈ P and x ∈ Rd, we have (cid:80) h(p,x)z q∈P h(q,x)z ≤ 2 n . 2
x ∈ Rd(cid:12) (cid:110) (cid:12) (cid:12) (cid:80) (iv) The set Xg = is centrally symmetric, i.e., for every x ∈ Xg we have −x ∈ Xg, and there exist R, r ∈ (0, ∞) such that B(0d, r) ⊂ Xg ⊂
B(0d, R), where B(0d, y) denotes a ball of radius y > 0, centered at 0d. p∈P g(p, x)max{1,z} ≤ 1 (cid:111)
We denote by F, the union of all functions f with the above properties.
The intuition behind Deﬁnition 1. Properties (i)-(iii) are used to reduce the problem to dealing with a “simpler” pair of functions where the ﬁrst is a convex function “g” that is linear in its argument x and the second function “h” being independent of the input points. Property (iv) ensures that the ellipsoid which encloses the level set of g (the convex function) exists and is centered at the origin to avoid dealing with the center. By combining the properties associated with the level set of g (the convex function) and Properties (i)-(iv), we manage to bound the loss function from above and below by the mahalanobis distance with respect to the enclosing ellipsoid. This is due to the fact that the level set encloses a contracted version of the ellipsoid which encloses the level set of g.
We are interested in a generic algorithm that would get a set of input points, and a loss function as above, and compute a sensitivity for each point, based on the parameters of the given loss function.
In addition, we wish to use worst-case analysis and prove that for every input the total sensitivity (and thus size of coreset) would be small, depending on the “hardness" of the loss function that is encapsulated in the above parameters z, R, etc. 2