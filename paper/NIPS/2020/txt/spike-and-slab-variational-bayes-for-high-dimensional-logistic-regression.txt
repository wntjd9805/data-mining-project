Abstract
Variational Bayes (VB) is a popular scalable alternative to Markov chain Monte
Carlo for Bayesian inference. We study a mean-ﬁeld spike and slab VB approxima-tion of widely used Bayesian model selection priors in sparse high-dimensional logistic regression. We provide non-asymptotic theoretical guarantees for the VB posterior in both (cid:96)2 and prediction loss for a sparse truth, giving optimal (minimax) convergence rates. Since the VB algorithm does not depend on the unknown truth to achieve optimality, our results shed light on effective prior choices. We conﬁrm the improved performance of our VB algorithm over common sparse VB approaches in a numerical study. 1

Introduction
Let x ∈ Rp denote a feature vector and Y ∈ {0, 1} an associated binary label to be predicted. In logistic regression, one of the most widely used methods in classiﬁcation, we model
P (Y = 1|X = x) = 1 − P (Y = 0|X = x) = Ψ(xT θ) = exT θ 1 + exT θ
, (1) where θ ∈ Rp is an unknown regression parameter and Ψ(t) = et/(1 + et) is the logistic function.
Suppose we observe n training examples {(x1, y1), . . . , (xn, yn)}.
We study the sparse high-dimensional setting, where n ≤ p and typically n (cid:28) p, and many of the coefﬁcients of θ are (close to) zero. This setting has been studied by many authors, notably using (cid:96)1-regularized M -estimators (e.g. the LASSO), see for instance [3, 22, 30, 31] and the references therein. In Bayesian logistic regression, one assigns a prior distribution to θ, giving a probabilistic model. An especially natural Bayesian way to model sparsity is via a model selection prior, which assigns probabilistic weights to every potential model, i.e. every subset of {1, . . . , p} corresponding to selecting the non-zero coordinates of θ ∈ Rp. This is a widely used Bayesian approach and includes the hugely popular spike and slab prior [16, 28].
Such priors work well in many settings for estimation and prediction [2, 12, 14], uncertainty quan-tiﬁcation [37, 13] and multiple hypothesis testing [11], see [4] for a recent review. An especially attractive property is their interpretability, particularly for variable selection, compared to many other black-box machine learning methods. For example, such methods provide posterior inclusion probabilities of particular features, and their credible sets, which are often important in practice.
However, the discrete nature of such priors makes scalable computation hugely challenging. Under a model selection prior, posterior exploration typically involves searching over all 2p possible models, making standard Markov chain Monte Carlo (MCMC) methods infeasible for moderate p unless the
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
feature vectors {x1, . . . , xn} satisfy strong structural conditions like orthogonality [14, 46]. There has been recent progress on adapting MCMC methods to sparse high-dimensional logistic regression
[29], while another common alternative is to instead use continuous shrinkage-type priors [10, 52].
A popular scalable alternative is variational Bayes (VB), which approximates the posterior by solving an optimization problem. One proposes an approximating family of tractable distributions, called the variational family, and ﬁnds the member of this family that is closest to the computationally intractable posterior in Kullback-Leibler (KL) sense. This member is taken as a substitute for the posterior. An especially popular family consists of factorizable distributions, called mean-ﬁeld variational Bayes.
VB scales to large data sets and works empirically well in many models, see [7] for a recent review.
In this work, we study the theoretical properties of a mean-ﬁeld VB approach to sparsity-inducing priors with variational family the set of factorizable spike and slab distributions. This is a natural approximation since it keeps the discrete model selection aspect and many of the interpretable features of the original posterior, while reducing the full O(2p) model complexity to a much more manageable
O(p). The procedure is adaptive in that it does not depend on the typically unknown sparsity level, which avoids delicate issues about tuning hyperparameters. This sparse variational family has been employed in various settings [20, 25, 33, 38, 44], including logistic regression [9, 56]. VB is natural in model (1) since in even the simplest low-dimensional setting (p (cid:28) n) using Gaussian priors, the posterior is intractable and VB is widely used [6, 21, 34, 43, 49].
However, VB generally comes with few theoretical guarantees, with none currently available in high-dimensional logistic regression. Our main contribution is to show that the sparse VB posterior converges to the true sparse vector at the optimal (minimax) rate in both (cid:96)2 and prediction loss. We prove this under the same conditions for which the true (computationally infeasible) posterior is known to converge [2], showing that one does not necessarily need to sacriﬁce theoretical guarantees when using sparse VB, at least for estimation and prediction. Our convergence bounds are non-asymptotic and thus reﬂect relevant ﬁnite-sample situations.
Our results also provide practical insights on effective prior and VB calibrations, in particular the choice of prior slab distribution. Many existing works employ Gaussian slabs for the underlying prior, even though these cause excessive shrinkage and suboptimal parameter recovery in benchmark models
[14]. Our theoretical results show that optimal parameter recovery is possible if the VB posterior is based on a prior with heavier-tailed Laplace slabs, corroborating ﬁndings in linear regression that one should use prior slabs with exponential or heavier tails [12, 14], including for VB [38]. We conﬁrm in simulations that using Laplace prior slabs, as our theory suggests, indeed empirically outperforms the usual VB choice of Gaussian prior slabs, demonstrating the practical importance of the prior slab choice. We further demonstrate that our VB algorithm is empirically competitive with other state-of-the-art Bayesian sparse variable selection methods for logistic regression.
Lastly, we provide conditions on the design matrix under which sparse VB can be expected to work well. Together, these provide theoretical backing for using VB for estimation and prediction in the widely used sparse high-dimensional Bayesian logistic regression model (1).