Abstract
The notion of implicit bias, or implicit regularization, has been suggested as a means to explain the surprising generalization ability of modern-days overparameterized learning algorithms. This notion refers to the tendency of the optimization algorithm towards a certain structured solution that often generalizes well. Recently, several papers have studied implicit regularization and were able to identify this phenomenon in various scenarios. We revisit this paradigm in arguably the simplest non-trivial setup, and study the implicit bias of Stochastic Gradient Descent (SGD) in the context of Stochastic Convex Optimization. As a Ô¨Årst step, we provide a simple construction that rules out the existence of a distribution-independent implicit regularizer that governs the generalization ability of SGD. We then demonstrate a learning problem that rules out a very general class of distribution-dependent implicit regularizers from explaining generalization, which includes strongly convex regularizers as well as non-degenerate norm-based regularizations. Certain aspects of our constructions point out to signiÔ¨Åcant diÔ¨Éculties in providing a comprehensive explanation of an algorithm‚Äôs generalization performance by solely arguing about its implicit regularization properties. 1

Introduction
One of the great mysteries of contemporary machine learning is the impressive success of unregularized and overparameterized learning algorithms. In detail, current machine learning practice is to train models with far more parameters than samples and let the algorithm Ô¨Åt the data, oftentimes without any type of regularization.
In fact, these algorithms are so overcapacitated that they can even memorize and Ô¨Åt random data. Yet, when trained on real-life data, these algorithms show remarkable performance in generalizing to unseen samples [18, 30].
This phenomenon is often attributed to what is described as the implicit-regularization of an algorithm [18]. Implicit regularization roughly refers to the learner‚Äôs preference to implicitly choosing certain structured solutions as if some explicit regularization term appeared in its objective. As a canonical example, in linear optimization one can show that various forms of gradient descent, an apriori unregularized algorithm, behaves identically as regularized risk minimization penalized with the squared Euclidean norm on the parameters [24]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Understanding implicit regularization poses several interesting challenges. For example: how can we
Ô¨Ånd the implicit bias of a given learning algorithm? what is the rate of convergence towards the biased solution? how (and if) does it govern the generalization of an algorithm? and, when and what types of regularizations can account for and explain the generalization in modern-days machine learning?
Towards answering these questions we revisit a fundamental setting that was extensively studied in recent years: Stochastic Convex Optimization (SCO), focusing on the SGD optimization algorithm.
In contrast to most previous work, we do not attempt to identify the implicit bias in speciÔ¨Åc problems.
Instead, we study these questions in the general case, and we construct examples which rule out the existence of potential regularizers in general. To some extent, these constructions demonstrate a behavior that might seem counter-intuitive or contradictory to the implicit-bias point of view.
Besides being a well-studied and well-understood model for learning, an important trait of SCO which makes it suitable for our investigation is that learning cannot in general be performed by naive
Empirical Risk Minimization (ERM). In detail, the work of Shalev-Shwartz et al. [23] showed the existence of SCO instances where naive-ERM fails but regularized-ERM succeeds. Thus, we view
SCO as a natural test-bed for exploring the role of regularization and its relation to generalization.
Compellingly, the generalization of SGD in SCO is well-established, and we are left with the question of how well can we account for generalization through an investigation of its bias. 1.1 Contributions
Implicit distribution-independent bias. We begin with a simple construction which demonstrates that SGD does not have any distribution-independent implicit bias. To show that, we construct a case where SGD does not converge to a Pareto-eÔ¨Écient (not even approximately) solution with respect to the empirical loss and a given regularization penalty. In fact, this result is also true for
Gradient Descent over smooth functions. In other words, our construction here involves a distribution supported on a single smooth convex function.
Our result is general and rules out any (reasonable) regularizer from being the implicit bias of SGD in this distribution-independent setting. Since the Euclidean-norm distance is the immediate suspect for the implicit regularization of SGD, the Ô¨Årst step towards achieving the result is to rule out that
Euclidean norm is the implicit bias of SGD. We thus construct an example of a function with a plateau of minimizers where SGD does not converge to the closest point in Euclidean-norm sense. While the result might not seem surprising, it is the technical engine behind the further constructions we provide.
Previous to this work, Suggala et al. [27] showed that gradient descent with an inÔ¨Ånitely small step size (that is, gradient Ô¨Çow), might diverge from the closest point, and we provide a complementary construction combined with a full rigorous analysis for Ô¨Åxed step-size gradient descent.
Implicit distribution-dependent bias. Having ruled out the possibility of a problem-independent regularizer, we proceed to study the more compelling distribution-dependent implicit regularization.
The question here is whether for every distribution over convex functions, we can associate a regularizer
ùëü such that SGD tries to (approximately) Ô¨Ånd a Pareto-eÔ¨Écient solution with respect to ùëü and the empirical loss (notice that we allow the regularizer to depend on the distribution, but not on the speciÔ¨Åc sample received by SGD.)
We Ô¨Årst show that we can rule out the eÔ¨Äect of strongly-convex regularizers in the relevant regime of learning (where the dimension and the number of training examples are of roughly the same order). In fact, we rule out a more general class of regularizers that have large range on sets with large diameters.
Namely, in any ball with large diameter the regularizer shows preference towards a certain point.
We then continue and demonstrate a distribution where, given an input sample, there is a very large set of possible solutions that share the same empirical loss and the same regularization penalty, and yet, SGD chooses its solution arbitrarily within this set. Here, by ‚Äúvery large‚Äù we mean from a learning-theoretical point of view; namely, this set is large enough so that, in general, empirical risk minimization restricted to the set will fail (and yet, it appears that this is exactly what SGD does). In other words, no regularizer ùëü is suÔ¨Écient for narrowing down the set of possible SGD solutions to the point where non-trivial generalization can be deduced without appealing to other properties of the speciÔ¨Åc problem. 2
Implicit bias in constant dimension. Several of our constructions are given in high dimension, namely the number of parameters is larger than the number of examples. One could argue that this is the interesting regime, nevertheless it is still worthy to understand the role of implicit bias when the dimension of the problem is smaller than number of examples. Here we cannot rule out the role of implicit bias in a similar fashion to before - namely, due to uniform convergence, any algorithm that is constrained to the unit ball will generalize and this implicit bias is indeed the explanation to that. It is interesting though to understand the existence of speciÔ¨Åc regularizers (such as, e.g., strongly convex regularizers).
While we do not provide an answer to this question, we make an intermediate step. Our Ô¨Ånal construction is in a slightly relaxed model, where the instances are non-convex, but the expected loss function is convex. While this result may be limited, because of the non-convexity, we stress that the learning guarantees of SGD are completely applicable to this setting: namely, SGD does learn the problem (as it is convex in expectation). We show that for any strictly quasi-convex regularizer, namely a regularizer that has preference for a single point in any convex regime, the algorithm will not converge to the optimal solution with optimal regularization penalty (even though it converges to a convex domain where seemingly it can improve its parameter choice towards the regularized solution). 1.2