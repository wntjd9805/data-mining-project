Abstract
Recently, Transformer-based language models have demonstrated remarkable performance across many NLP domains. However, the unsupervised pre-training step of these models suffers from unbearable overall computational expenses.
Current methods for accelerating the pre-training either rely on massive parallelism with advanced hardware or are not applicable to language modeling.
In this work, we propose a method based on progressive layer dropping that speeds the training of Transformer-based language models, not at the cost of excessive hardware resources but from model architecture change and training technique boosted efﬁciency. Extensive experiments on BERT show that the proposed method achieves a 24% time reduction on average per sample and allows the pre-training to be 2.5× faster than the baseline to get a similar accuracy on downstream tasks.
While being faster, our pre-trained models are equipped with strong knowledge transferability, achieving comparable and sometimes higher GLUE score than the baseline when pre-trained with the same number of samples. 1

Introduction
Natural language processing (NLP) tasks, such as natural language inference [1, 2] and question answering [3–5], have achieved great success with the development of neural networks. It has been demonstrated recently that Transformer-based networks have obtained superior performance in many NLP tasks (e.g., the GLUE benchmark [6] and the challenging multi-hop reasoning task [7]) than recurrent neural networks or convolutional neural networks. BERT trains a deep bidirectional
Transformer and obtains outstanding results with transfer learning [3]. RoBERTa [2], which is a robustly optimized version of BERT trained with more steps and larger corpora, achieves state-of-the-art results on 9 GLUE tasks. Megatron-LM [8] further advances the state-of-the-art in NLP by signiﬁcantly increasing the size of BERT model. Finally, there are multiple research proposing different enhanced versions of Transformer-based networks, such as GPT-2/3 [9, 10], XLNet [1],
SpanBERT [11], BioBERT [12], UniLM [13], Turing-NLG [14], and T5 [15]. Due to the exciting prospect, pre-training Transformer networks with a large corpus of text followed by ﬁne-tuning on speciﬁc tasks has become a new paradigm for natural language processing.
Despite great success, a big challenge of Transformer networks comes from the training efﬁciency – even with self-attention and parallelizable recurrence [16], and extremely high performance hard-ware [17], the pre-training step still takes a signiﬁcant amount of time. To address this challenge, mixed-precision training is explored [8, 18], where the forward pass and backward pass are computed in half-precision and parameter update is in single precision. However, it requires Tensor Cores [19], which do not exist in all hardware. Some work resort to distributed training [20, 21, 8]. However, distributed training uses large mini-batch sizes to increase the parallelism, where the training often converges to sharp local minima with poor generalizability even with signiﬁcant hyperparameter tuning [22]. Subsequently, Yang et al. propose a layer-wise adaptive large batch optimizer called
LAMB [23], allowing to train BERT with 32K batch size on 1024 TPU chips. However, this type of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
approach often requires dedicated clusters with hundreds or even thousands of GPUs and sophisti-cated system techniques at managing and tuning distributed training, not to mention that the amount of computational resources is intractable for most research labs or individual practitioners.
In this paper, we speedup pre-training Transformer networks by exploring architectural change and training techniques, not at the cost of excessive hardware resources. Given that the training cost grows linearly with the number of Transformer layers, one straightforward idea to reduce the computation cost is to reduce the depth of the Transformer networks. However, this is restrictive as it often results in lower accuracy in downstream tasks compared to full model pre-training, presumably because of having smaller model capacities [24, 25]. Techniques such as Stochastic Depth have been demonstrated to be useful in accelerating supervised training in the image recognition domain [26].
However, we observe that stochastically removing Transformer layers destabilizes the performance and easily results in severe consequences such as model divergence or convergence to bad/suspicious local optima. Why are Transformer networks difﬁcult to train with stochastic depth? Moreover, can we speed up the (unsupervised) pre-training of Transformer networks without hurting downstream performance?
To address the above challenges, we propose to accelerate pre-training of Transformer networks by making the following contributions. (i) We conduct a comprehensive analysis to answer the question: what makes Transformer networks difﬁcult to train with stochastic depth. We ﬁnd that both the choice of Transformer architecture as well as training dynamics would have a big impact on layer dropping. (ii) We propose a new architecture unit, called the Switchable-Transformer (ST) block, that not only allows switching on/off a Transformer layer for only a set portion of the training schedule, excluding them from both forward and backward pass but also stabilizes Transformer network training. (iii) We further propose a progressive schedule to add extra-stableness for pre-training Transformer networks with layer dropping – our schedule smoothly increases the layer dropping rate for each mini-batch as training evolves by adapting in time the parameter of the Bernoulli distribution used for sampling.
Within each gradient update, we distribute a global layer dropping rate across all the ST blocks to favor different layers. (iv) We use BERT as an example, and we conduct extensive experiments to show that the proposed method not only allows to train BERT 24% faster than the baseline under the same number of samples but also allows the pre-training to be 2.5× faster to get similar accuracy on downstream tasks. Furthermore, we evaluate the generalizability of models pre-trained with the same number of samples as the baseline, and we observe that while faster to train, our approach achieves a 1.1% higher GLUE score than the baseline, indicating a strong knowledge transferability. 2