Abstract
Learning from Observations (LfO) is a practical reinforcement learning scenario from which many applications can beneﬁt through the reuse of incomplete re-sources. Compared to conventional imitation learning (IL), LfO is more chal-lenging because of the lack of expert action guidance. In both conventional IL and LfO, distribution matching is at the heart of their foundation. Traditional distribution matching approaches are sample-costly which depend on on-policy transitions for policy learning. Towards sample-efﬁciency, some off-policy solu-tions have been proposed, which, however, either lack comprehensive theoretical justiﬁcations or depend on the guidance of expert actions. In this work, we pro-pose a sample-efﬁcient LfO approach which enables off-policy optimization in a principled manner. To further accelerate the learning procedure, we regulate the policy update with an inverse action model, which assists distribution matching from the perspective of mode-covering. Extensive empirical results on challenging locomotion tasks indicate that our approach is comparable with state-of-the-art in terms of both sample-efﬁciency and asymptotic performance.

Introduction 1
Imitation Learning (IL) has been widely studied in the reinforcement learning (RL) domain to assist in learning complex tasks by leveraging the experience from expertise [1, 2, 3, 4, 5]. Unlike conventional
RL that depends on environment reward feedbacks, IL can purely learn from expert guidance, and is therefore crucial for realizing robotic intelligence in practical applications, where demonstrations are usually easier to access than a delicate reward function [6, 7].
Classical IL, or more concretely, Learning from Demonstrations (LfD), assumes that both states and actions are available as expert demonstrations [8, 2, 3]. Although expert actions can beneﬁt IL by providing elaborated guidance, requiring such information for IL may not always accord with the real-world. Actually, collecting demonstrated actions can sometimes be costly or impractical, whereas observations without actions are more accessible resources, such as camera or sensory logs.
Consequently, Learning from Observations (LfO) has been proposed to address the scenario without expert actions [9, 10, 11]. On one hand, LfO is more challenging compared with conventional IL, due to missing ﬁner-grained guidance from actions. On the other hand, LfO is a more practical setting for IL, not only because it capitalizes previously unusable resources, but also because it reveals the potential to realize advanced artiﬁcial intelligence. In fact, learning without action guidance is an inherent ability for human being. For instance, a novice game player can improve his skill purely by watching video records of an expert, without knowing what actions have been taken [12].
Among popular LfD and LfO approaches, distribution matching has served as a principled solution [2, 3, 9, 10, 13], which works by interactively estimating and minimizing the discrepancy between two 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
stationary distributions: one generated by the expert, and the other generated by the learning agent. To correctly estimate the distribution discrepancy, traditional approaches require on-policy interactions with the environment whenever the agent policy gets updated. This inefﬁcient sampling strategy impedes wide applications of IL to scenarios where accessing transitions are expensive [14, 15]. The same challenge is aggravated in LfO, as more explorations by the agent are needed to cope with the lack of action guidance.
Towards sample-efﬁciency, some off-policy IL solutions have been proposed to leverage transitions cached in a replay buffer. Mostly designed for LfD, these methods either lack theoretical guarantee by ignoring a potential distribution drift [4, 16, 17], or hinge on the knowledge of expert actions to enable off-policy distribution matching [3], which makes their approach inapplicable to LfO.
To address the aforementioned limitations, in this work, we propose a LfO approach that improves sample-efﬁciency in a principled manner. Speciﬁcally, we derive an upper-bound of the LfO objective which dispenses with the need of knowing expert actions and can be fully optimized with off-policy learning. To further accelerate the learning procedure, we combine our objective with a regularization term, which is validated to pursue distribution matching between the expert and the agent from a mode-covering perspective. Under a mild assumption of a deterministic environment, we show that the regularization can be enforced by learning an inverse action model. We call our approach OPOLO (Off POlicy Learning from Observations). Extensive experiments on popular benchmarks show that
OPOLO achieves state-of-the-art in terms of both asymptotic performance and sample-efﬁciency. 2