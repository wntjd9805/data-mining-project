Abstract
Machine learning transparency calls for interpretable explanations of how inputs re-late to predictions. Feature attribution is a way to analyze the impact of features on predictions. Feature interactions are the contextual dependence between features that jointly impact predictions. There are a number of methods that extract feature interactions in prediction models; however, the methods that assign attributions to interactions are either uninterpretable, model-speciﬁc, or non-axiomatic. We propose an interaction attribution and detection framework called Archipelago which addresses these problems and is also scalable in real-world settings. Our ex-periments on standard annotation labels indicate our approach provides signiﬁcantly more interpretable explanations than comparable methods, which is important for analyzing the impact of interactions on predictions. We also provide accompanying visualizations of our approach that give new insights into deep neural networks. 1

Introduction
The success of state-of-the-art prediction models such as neural networks is driven by their capability to learn complex feature interactions. When such models are used to make predictions for users, we may want to know how they personalize to us. Such model behavior can be explained via interaction detection and attribution, i.e. if features inﬂuence each other and how these interactions contribute to predictions, respectively. Interaction explanations are useful for applications such as sentiment analysis [36], image classiﬁcation [48], and recommendation tasks [21, 48].
Relevant methods for attributing predictions to feature interactions are black-box explanation methods based on axioms (or principles), but these methods lack interpretability. One of the core issues is that an interaction’s importance is not the same as its attribution. Techniques like Shapley Taylor
Interaction Index (STI) [14] and Integrated Hessians (IH) [25] combine these concepts in order to be axiomatic. Speciﬁcally, they base an interaction’s attribution on non-additivity, i.e. the degree that features non-additively affect an outcome. While non-additivity can be used for interaction detection, it is not interpretable as an attribution measure as we see in Fig. 1. In addition, neither STI nor IH is tractable for higher-order feature interactions [14, 46]. Hence, there is a need for interpretable, axiomatic, and scalable methods for interaction attribution and corresponding interaction detection.
To this end, we propose a novel framework called Archipelago1, which consists of an interaction attribution method, ArchAttribute, and a corresponding interaction detector, ArchDetect, to address the challenges of being interpretable, axiomatic, and scalable. Archipelago is named after its ability to provide explanations by isolating feature interactions, or feature “islands”. The inputs to
Archipelago are a black-box model f and data instance x(cid:63), and its outputs are a set of interactions and individual features {I} as well as an attribution score φ(I) for each of the feature sets I. 1Code is available at: https://github.com/mtsang/archipelago 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Integrated Hessians [25]
Shapley Taylor
Interaction Index [14]
Our Method
Figure 1: Our explanation for the sentiment analysis example of [25]. Colors indicate sentiment, and arrows indicate interactions. Compared to other axiomatic interaction explainers, only our work corroborates our intuition by showing negative attribution among top-ranked interactions.
ArchAttribute satisﬁes attribution axioms by making relatively mild assumptions: a) disjointness of interaction sets, which is easily obtainable, and b) the availability of a generalized additive function which is a good approximator to any function [49–51]. On the other hand, ArchDetect circumvents intractability issues of higher-order interaction detection by removing certain uninterpretable higher-order interactions and leveraging a property of feature interactions that allows pairwise interactions to merge for disjoint arbitrary-order interaction detection. In practice, where any assumptions may not hold in real-world settings, Archipelago still performs well. In particular, Archipelago effectively detects relevant interactions and is more interpretable than state-of-the-art methods [14, 20, 25, 26, 47, 51] when evaluated on annotation labels in sentiment analysis and image classiﬁcation. We visualize Archipelago explanations on sentiment analysis, coronavirus detection on chest X-rays, and ad-recommendation, and we demonstrate an interactive visualization of Archipelago.
Our main contributions are summarized below.
• Interaction Attribution: We propose ArchAttribute, a feature attribution measure which leverages feature interactions. It has advantages of being model-agnostic, interpretable, and runtime-efﬁcient as compared to other state-of-the-art interaction attribution methods.
• Principled Attribution: ArchAttribute obeys standard attribution axioms [47] that are generalized to work for feature sets, and we also propose a new axiom for interaction attribution to respect the additive structure of a function.
• Interaction Detection: We propose a complementary feature interaction detector,
ArchDetect, which is also model-agnostic and O(p2)-efﬁcient for pairwise and disjoint arbitrary-order interaction detection (p is number of features).
Archipelago satisﬁes other desirable qualities of interpretability [34]: coherence, simplicity, gener-ality, and truthfulness. We achieve coherence by separating interaction attribution from detection, simplicity by merging feature sets, generality by isolating attributions from contexts, and truthful-ness by trying to explain the true cause of predictions via interactions. Our empirical studies on
Archipelago demonstrate its superior properties as compared to state-of-the-art methods. 2 Preliminaries
We ﬁrst introduce preliminaries that serve as a basis for our discussions.
Notations: We use boldface lowercase symbols, such as x, to represent vectors. The i-th entry of a vector x is denoted by xi. For a set S, its cardinality is denoted by |S|, and the operation \S means all except S. For p features in a dataset, let I be a subset of feature indices: I ⊆ {1, 2, . . . , p}. For a vector x ∈ Rp, let xI ∈ Rp be deﬁned element-wise in (1). In our discussions, a context means x\I. (xI)i = (cid:26) xi, 0 if i ∈ I otherwise (1)
Problem Setup: Let f denote a black-box model with scalar output. For multi-class classiﬁcation, f is assumed to be a class logit. We use an input vector x(cid:63) ∈ Rp to denote the data instance where we 2
(a) Additive (linear) function (b) Non-additive (ReLU) function (c) δ vs. φ on a text example (Fig. 1)
Figure 2: Non-additive interaction for p = 2 features: The corner points are used to determine if x1 and x2 interact based on their non-additivity on f , i.e. they interact if δ ∝ (f (a) − f (b)) − (f (c) − f (d)) (cid:54)= 0 (§4.1). In (c), the attribution of (bad, awful) should be negative via φ (2), but Shapley
Taylor Interaction Index uses the positive δ. Note that φ depends on a and d whereas δ depends on a, b, c, and d. Also, Integrated Hessians is not relevant here since it does not apply to ReLU functions. wish to explain f , and x(cid:48) ∈ Rp to denote a neutral baseline. Here, the baseline is a reference vector for x(cid:63) and conveys an “absence of signal” as per [47]. These vectors form the space of X ⊂ Rp, where each element comes from either x(cid:63) i, i.e. X = {(x1, . . . , xp) | xi ∈ {x(cid:63) i}, ∀i = 1, . . . , p}. i or x(cid:48) i , x(cid:48)
Interpretability: One of the motivations of this work is to provide coherent interpretability, meaning that it needs to be consistent with human prior belief [34]. Coherence is important so that users can agree with model interpretations and don’t inadvertently think a model fails when it actually works, as we motivate in Fig. 1. Coherent interpretability leads to more trustworthy explanations [26]. In our experiments, we leverage standard annotation labels to measure coherence.
Feature Interaction: The deﬁnition of the feature interaction of interest is formalized as follows.
Deﬁnition 1 (Statistical Non-Additive Interaction). A function f contains a statistical non-additive interaction of multiple features indexed in set I if and only if f cannot be decomposed into a sum of
|I| subfunctions fi , each excluding the i-th interaction variable: f (x) (cid:54)= (cid:80) i∈I fi(x\{i}).
Def. 1 identiﬁes a non-additive effect among all features I on the output of function f [18, 46, 49].
For example, this means that the function ReLU(x1 + x2) creates a feature interaction because it cannot be represented as an addition of univariate functions, i.e. ReLU(x1 + x2) (cid:54)= f1(x2) + f2(x1) (Fig. 2b). We refer to individual feature effects which do not interact with other features as main effects. Higher-order feature interactions are captured by |I| > 2, i.e. interactions larger than pairs.
Additionally, if a higher-order interaction exists, all of its subsets also exist as interactions [46, 49]. 3 Archipelago Interaction Attribution
We begin by presenting our feature attribution measure. Our feature attribution analyzes and assigns scores to detected feature interactions. Our corresponding interaction detector is presented in §4. 3.1 ArchAttribute
Let I be the set of feature indices that correspond to a desired attribution score. Our proposed attribution measure, called ArchAttribute, is given by
φ(I) = f (x(cid:63)
I + x(cid:48)
\I) − f (x(cid:48)). (2) i }i∈I do not speciﬁcally interact with the baseline features (cid:8)x(cid:48)
I from the surrounding baseline context x(cid:48)
ArchAttribute essentially isolates the attribution of x(cid:63)
\I while also satisfying axioms (§3.2). We call this isolation an “island effect”, where the input features
{x(cid:63)
. For example, consider sentiment analysis on a phrase x(cid:63) = “not very bad” with a baseline x(cid:48) = “_ _ _” . Suppose that we want to examine the attribution of an interaction I that corresponds to {very, bad} in isolation. Here, the contextual word “not” also interacts with I, which becomes apparent when small perturbations to the word “not” causes large changes to prediction probabilities. However, as we move further away from the word “not” towards the empty-word “_” in the word-embedding space, small perturbations no longer result in large prediction changes, meaning that the “_” context does not speciﬁcally j∈\I (cid:9) j 3
interact with {very, bad}. This intuition motivates our use of the baseline context x(cid:48)
\I in (2). Note that
ArchAttribute is independent of input context and thus carries generality over input data instances. 3.2 Axioms
We now show how ArchAttribute obeys standard feature attribution axioms [47]. Since
ArchAttribute operates on feature sets, we generalize the notion of standard axioms to feature sets.
To this end, we also propose a new axiom, Set Attribution, which allows us to work with feature sets.
Let S = {Ii}s where we take the union of overlapping sets in S. Later in §4, we explain how to obtain S. i=1 be all s feature interactions and main effects of f in the space X (deﬁned in §2),
Completeness: We consider a generalization of the completeness axiom for which the sum of all attributions equals f (x(cid:63)) − f (x(cid:48)). The axiom tells us how much feature(s) impact a prediction.
Lemma 2 (Completeness on S). The sum of all attributions by ArchAttribute for the disjoint sets in S equals the difference of f between x(cid:63) and the baseline x(cid:48): f (x(cid:63)) − f (x(cid:48)).
The proof is in Appendix C. We can easily see ArchAttribute satisfying this axiom in the limiting case where s = 1, I1 = {i}p i=1 because (2) directly becomes f (x(cid:63)) − f (x(cid:48)). Existing interaction / group attribution methods: Sampling Contextual Decomposition (SCD) [26], its variant (CD) [36,43],
Sampling Occlusion (SOC) [26], and Shapley Interaction Index (SI) [20] do not satisfy completeness, whereas Integrated Hessians (IH) [25] and Shapley Taylor Interaction Index (STI) [14] do.
Set Attribution: We propose an axiom for interaction attribution called Set Attribution to work with feature sets as opposed to individual features and follow the additive structure of a function.
Axiom 3 (Set Attribution). If f : Rp → R is a function in the form of f (x) = (cid:80)s
{Ii}s admits an attribution for feature set Ii as ϕi(xIi) ∀i = 1, . . . , s. i=1 ϕi(xIi) where i=1 have roots, then an interaction attribution method i=1 are disjoint and functions {ϕi(·)}s
For example, if we consider a function y = x1x2 + x3; it makes sense for the attribution of the x1x2 interaction to be the value of x1x2 and the attribution for the x3 main effect to be the value of x3.
Lemma 4 (Set Attribution on S). For x = x(cid:63) and a baseline x(cid:48) such that ϕi(x(cid:48)
Ii
ArchAttribute satisﬁes the Set Attribution axiom and provides attribution ϕi(xIi) for set Ii ∀i.
) = 0 ∀i = 1, . . . , s,
The proof is in Appendix E, which follows from Lemma 2. Neither SCD, CD, SOC, SI, IH, nor STI satisfy Set Attribution (shown in Appendix E.1). We can enable Integrated Gradients (IG) [47] to satisfy our axiom by summing its attributions within each feature set of S. ArchAttribute differs from IG by its “island effect” (§3.1) and model-agnostic properties.
Other Axioms: ArchAttribute also satisﬁes the remaining axioms: Sensitivity, Implementation
Invariance, Linearity, and Symmetry-Preserving, which we show via Lemmas 7-11 in Appendix F.
Discussion: Several axioms required disjoint interaction and main effect sets in S. Though in-teractions are not necessarily disjoint by deﬁnition (Def. 1), it is reasonable to merge overlapping interactions to obtain compact and simpler visualizations, as shown in Fig. 1 and our experiments later in §5.3. The disjoint sets also allow ArchAttribute to yield identiﬁable non-additive attributions in the sense that it can identify the attribution given a feature set in S. This contrasts with Model-Agnostic Hierarchical Explanations (MAHE) [51], which yields unidentiﬁable attributions [57]. 4 Archipelago Interaction Detection
Our axiomatic analysis of ArchAttribute relied on S, which contains interaction sets of f on the space X (deﬁned in §2). To develop an interaction detection method that works in tandem with
ArchAttribute, we draw inspiration from the discrete interpretation of mixed partial derivatives. 4.1 Discrete Interpretation of Mixed Partial Derivatives
Consider the plots in Fig. 2, which consist of points a, b, c, and d that each contain two feature values.
From a top-down view of each plot, the points form the corners of a rectangle, whose side lengths are h1 = |a1 − b1| = |c1 − d1| and h2 = |a2 − c2| = |b2 − d2|. When h1 and h2 are small, the mixed partial derivative w.r.t variables x1 and x2 is computed as follows. First,
∂f (a)
∂x1
≈ 1 h1 (f (a) − f (b)) 4
and
∂f (c)
∂x1
≈ 1 h1 (f (c) − f (d)). Similarly, the mixed partial derivative is approximated as:
∂2f
∂x1x2
≈ 1 h2 (cid:18) ∂f (a)
∂x1
−
∂f (c)
∂x1 (cid:19)
≈ 1 h1h2 ((f (a) − f (b)) − (f (c) − f (d))) . (3)
When h1 and h2 become large, (3) tells us if a plane can ﬁt through all four points a,b,c, d (Fig. 2a), which occurs when (3) is zero. Here, a plane in the linear form f (x) = w1x1 + w2x2 + b is functionally equivalent to all functions of the form f (x) = f1(x1) + f2(x2) + b since x1 and x2 only take two possible values each, so any deviation from the plane (e.g. Fig. 2b) becomes non-additive. Consequently, a non-zero value of (3) identiﬁes a non-additive interaction by the deﬁnition of statistical interaction (Def. 1). What’s more, the magnitude of (3) tells us the degree of deviation from the plane, or the degree of non-additivity. (Additional details in Appendix G) 4.2 ArchDetect
Leveraging these insights about mixed partial derivatives, we now discuss the two components of our proposed interaction detection technique – ArchDetect. 4.2.1 Handling Context: As deﬁned in §3.2 and §4, our problem is how to identify interactions of p features in X for our input data instance x(cid:63) and baseline x(cid:48). If p = 2, then we can almost directly use (3), where a = (x(cid:63) 2). However if p > 2, all possible combinations of features in X would need to be examined to thoroughly identify just one pairwise interaction. To see this, we ﬁrst rewrite (3) to accommodate p features, and square the result to measure interaction strength and be consistent with previous interaction detectors [18, 19]. The interaction strength between features i and j for a context x\{i,j} is then deﬁned as 2), and d = (x(cid:48) 2), c = (x(cid:63) 2), b = (x(cid:48) 1, x(cid:63) 1, x(cid:63) 1, x(cid:48) 1, x(cid:48)
ωi,j(x) = (cid:16) (cid:18) 1 hihj f (x(cid:63)
{i,j} + x\{i,j}) − f (x(cid:48)
{i} + x(cid:63)
{j} + x\{i,j})
− f (x(cid:63)
{i} + x(cid:48)
{j} + x\{i,j}) + f (x(cid:48)
{i,j} + x\{i,j}) (cid:17)(cid:19)2 (4)
, i − x(cid:48) i| and hj = (cid:12) (cid:12) where hi = |x(cid:63) (cid:12). The thorough way to identify the {i, j} feature interaction is given by ¯ωi,j = Ex∈X [ωi,j(x)], where each element of x\{i,j} is Bernoulli (0.5). This expectation is intractable because X has an exponential search space, so we propose the ﬁrst component of
ArchDetect for efﬁcient pairwise interaction detection: j − x(cid:48) j (cid:12)x(cid:63)
¯ωi,j = 1 2 (ωi,j(x(cid:63)) + ωi,j(x(cid:48))) . (5)
Here, we estimate the expectation by leveraging the physical meaning of the interactions and
ArchAttribute’s axioms via the different contexts of x in (5) as follows:
• Context of x(cid:63): An important interaction is one due to multiple x(cid:63) features. As a concrete example, consider an image representation of a cat which acts as our input data instance.
The following higher-order interaction, if xear = x(cid:63) nose and xf ur = x(cid:63) f ur then f (x) = high cat probability, is responsible for classifying “cat”. We can detect
\{i,j} using ωi,j(x(cid:63)). any pairwise subset {i, j} of this interaction by setting the context as x(cid:63)
\{i,j} to detect interactions via ωi,j(x(cid:48)), which helps us establish ArchAttribute’s completeness (Lemma 2). This also separates out effects of any higher-order baseline interactions from f (x(cid:48)) in (8) (Appendix C) and recombine their effects in (11). From an interpretability standpoint, the x(cid:48)
\{i,j} context ranks pairwise interactions w.r.t. a standard baseline. This context is also used by ArchAttribute (2).
• Context of x(cid:48): Next, we consider x(cid:48) ear and xnose = x(cid:63)
• Other Contexts: The ﬁrst two contexts accounted for any-order interactions created by either input or baseline features and a few interactions created by a mix of input and baseline features. The remaining interactions speciﬁcally require a mix of > 3 input and baseline features. This case is unlikely and is excluded, as we discuss next.
The following assumption formalizes our intuition for the Other Contexts setting where there is a mix of higher-order (> 3) input and baseline feature interactions. 5
Table 1: Comparison of interaction detectors (b) on synthetic ground truth in (a). (a) Functions with Ground Truth Interactions j=21 xixj + (cid:80)40 (cid:80)30 i=11) + (cid:80)40 i }30 j=1 xj i=11) + (cid:80)40 i }30 j=1 xj 3}) + (cid:86)(x; {x(cid:63) i=11) + (cid:80)40 i }30 (cid:80)10 i=1 (cid:86)(x; {x(cid:63) (cid:86)(x; {x(cid:48) 1, x(cid:63) i=1) + (cid:86)(x; {x(cid:63) i=1) + (cid:86)(x; {x(cid:63)
F1(x) = (cid:80)10
F2(x) =
F3(x) =
F4(x) = (cid:86)(x; {x(cid:63) i }20 i}20 2} ∪ {x(cid:48) j=1 xixj + (cid:80)20 i=11 k=1 xk j=1 xj (b) Pairwise Interaction Ranking AUC. The baseline methods often fail to detect interactions suited for the desired contexts in §4.2.1.
Method
Two-way ANOVA
Integrated Hessians
Neural Interaction Detection
Shapley Interaction Index
Shapley Taylor Interaction Index
ArchDetect (this work)
F1 1.0 1.0 0.94 1.0 1.0 1.0
F4
F3
F2 0.51 0.55 0.51
N/A N/A N/A 0.56 0.54 0.54 0.51 0.50 0.50 0.55 0.78 0.55 1.0 1.0 1.0
Figure 3: Interaction detection over-lap (redundancy) with added contexts to (5). “ﬁxed” at n = 2 (ArchDetect) already shows good stability.
Assumption 5 (Higher-Order Mixed-Interaction). For any feature set I where |I| > 3 and any pair of non-empty disjoint sets A and B where A ∪ B = I, the instances x ∈ X such that xi = x(cid:63) i ∀i ∈ A and xj = x(cid:48) j ∀j ∈ B do not cause a higher-order interaction of all features {xk}k∈I via f .
Assumption 5 has a similar intuition as ArchAttribute in §3.1 that input feature values do not speciﬁcally interact with baseline feature values. To understand this assumption, consider the original sentiment analysis example in Fig. 1 simpliﬁed as x(cid:63) = “bad terrible awful horrible movie” where x(cid:48) = “_ _ _ _ _”. It is reasonable to assume that there is no special interaction created by token sets such as {bad, terrible, _ , horrible} or {_ , _ , _ , horrible} due to the meaningless nature of the “_” token. In practice, it makes more sense to detect interactions out of sets like {bad, terrible, horrible} so we can avoid spurious interactions with the “_” token.
Efﬁciency: In (5), ArchDetect attains interaction detection over all pairs {i, j} in O(p2) calls of f .
Note that in (4), most function calls are reusable during pairwise interaction detection. 4.2.2 Detecting Disjoint Interaction Sets:
In this section, the aim here is to recover arbitrary size and disjoint non-additive feature sets S = {Ii} (not just pairs). ArchDetect looks at the union of overlapping pairwise interactions to obtain disjoint feature sets. Merging these pairwise interactions captures any existing higher-order interactions automatically since the existence of a higher-order interaction automatically means all its subset interactions exist (§2). In addition, ArchDetect merges these overlapped pairwise interactions with all individual feature effects to account for all features.
Note that to visualize ArchDetect, an initial threshold on the pairwise interaction ranking of (5) may be needed prior to merging, to show smaller size interactions for explanation simplicity. 5 Experiments 5.1 Setup
We conduct experiments ﬁrst on ArchDetect in §5.2 then on ArchAttribute in §5.3. We then visualize their combined form as Archipelago in §5.3. Throughout our experiments, we commonly study BERT [13, 56] on text-based sentiment analysis and ResNet152 [24] on image classiﬁcation.
BERT was ﬁne-tuned on the SST dataset [44], and ResNet152 was pretrained on ImageNet [12].
For sentiment analysis, we set the baseline vector x(cid:48) to be the tokens “_”, in place of each word-token from x(cid:63). For image classiﬁcation, we set x(cid:48) to be an all-zero image, and use the Quickshift superpixel segmenter [53] as per the need for input dimensionality reduction [48] (details in Appendix B). We set hi = hj = 1 for both domains. Several methods we compare to are common across experiments, in particular IG, IH, (disjoint) MAHE, SI, STI, and Difference, deﬁned as φd(I) = f (x(cid:63))−f (x(cid:48)
\I).
I +x(cid:63) 5.2 ArchDetect
We validate ArchDetect’s performance via synthetic ground truth and redundancy experiments. 6
Table 2: Comparison of attribution methods on BERT for sentiment analysis and ResNet152 for image classiﬁcation. Performance is measured by the correlation (ρ) or AUC of the top and bottom 10% of attributions for each method with respect to reference scores deﬁned in §5.3.
Method
BERT
Sentiment Analysis
ResNet152
Image Classiﬁcation
Word ρ
Phrase ρ †
Segment AUC †
Difference
Integrated Gradients (IG)
Integrated Hessians (IH)
Model-Agnostic Hierarchical Explanations (MAHE)
Shapley Interaction Index (SI)
Shapley Taylor Interaction Index (STI)
*Sampling Contextual Decomposition (SCD)
*Sampling Occlusion (SOC)
ArchAttribute (this work) 0.333 0.473
N/A 0.570 0.160 0.657 0.622 0.670 0.745 0.639 0.737 0.128 0.702
−0.018 0.286 0.742 0.794 0.836 0.705 0.786
N/A 0.712 0.530 0.626
N/A
N/A 0.919
† Methods that cannot tractably run for arbitrary feature set sizes are only run for pairwise feature sets.
* SCD and SOC are speciﬁcally for sequence models and contiguous words.
Synthetic Validation: We set x(cid:63) = [1, 1, . . . , 1] ∈ R40 and x(cid:48) = [−1, −1, . . . , −1] ∈ R40. Let z[·] be a key-value pair function such that z[i] = xi for key i ∈ z.keys and value xi, so we can deﬁne (cid:94) (x; z) := (cid:26) 1,
−1 if xi = z[i] ∀i ∈ z.keys for all other cases.
Table 1a shows functions with ground truth interactions suited for the desired contexts in §4.2.1.
Table 1b shows interaction detection AUC on these functions by ArchDetect, IH, SI, STI, Two-way
ANOVA [16] and the state-of-the-art Neural Interaction Detection [49]. On F2, F3, & F4, the baseline methods often fail because they are not designed to detect the desired interactions from §4.2.1.
Interaction Redundancy: The purpose of the next experiment is to see if ArchDetect can omit certain higher-order interactions. We study the form of (5) by examining the redundancy of inter-actions as new contexts are added to (5), which we now write as ¯ωi,j(C) = 1 c=1 ωi,j(xc). Let
C n be the number of contexts considered, and k be the number of top pairwise interactions selected after running pairwise interaction detection via ¯ωi,j for all {i, j} pairs. Interaction redundancy is the overlap ratio of two sets of top-k pairwise interactions, one generated via ¯ωi,j(n) and the other one via ¯ωi,j(n − 1) for some integer n ≥ 2. We generally expect the redundancy to increase as n increases, which we initially observe in Fig. 3. Here, “ﬁxed” and “random” correspond to different context sequences x1, x2, . . . , xN . The “random” sequence uses random samples from X for all
{xi}N i=1, whereas the “ﬁxed” sequence is ﬁxed in the sense that x1 = x(cid:63), x2 = x(cid:48), and the remaining
{xi}N i=3 are random samples. Experiments are done on the SST test set for BERT and 100 random test images in ImageNet for ResNet152. Notably, the “ﬁxed” setting has very low redundancy at n = 2 (ArchDetect) versus “random”. As soon as n = 3, the redundancy jumps and stabilizes quickly. These experiments support Assumption 5 and (5) to omit speciﬁed higher-order interactions. (cid:80)C 5.3 ArchAttribute & Archipelago
We study the coherent interpretability of ArchAttribute by comparing its attribution scores to ground truth annotations on subsets of features. For fair comparison, we look at extreme attributions (top and bottom 10%) for each baseline method. We then visualize the combined Archipelago framework. Additional comparisons on attributions and visualizations are shown in Appendices I, J.
Sentiment Analysis: For this task, we compare ArchAttribute to other explanation methods on two metrics: phrase correlation (Phrase ρ) and word correlation (Word ρ) on the SST test set. Phrase
ρ is the Pearson correlation between estimated phrase attributions and SST phrase labels (excluding prediction labels) on a 5-point sentiment scale [26]. Word ρ is the same as Phrase ρ but only for single words that consist of a single token. In addition to the aforementioned baseline methods in §5.1, we include the state-of-the-art SCD and SOC methods for sequence models [26] in our evaluation. In Table 2, ArchAttribute compares favorably to all methods where we consider the top and bottom 10% of the attribution scores for each method. We obtain similar performance across all other percentiles in Appendix I.
We visualize Archipelago explanations on S generated by top-3 pairwise interactions (§4.2.2) in
Fig. 4. The sentence examples are randomly selected from the SST test set. The visualizations 7
Figure 4: Our BERT visualizations on random test sentences from SST under BERT tokenization.
Arrows indicate interactions, and colors indicate attribution scores. fcls is the sentiment classiﬁcation.
The interactions point to salient and sometimes long-range sets of words, and the colors are sensible.
Figure 5: Our explanations of a COVID-19 classiﬁer (COVID-Net) [54] on randomly selected test
X-rays [9, 10] classiﬁed as COVID positive. COVID-Net accurately distinguishes COVID from pneumonia and normal X-rays. Colored outlines indicate detected feature sets with positive attribution.
The interactions consistently focus on the “great vessel” region outlined in green. show interactions and individual feature effects which all have reasonable polarity and intensity.
Interestingly, some of the interactions, e.g. between “lou-sy” and “un”, are long range.
Image Classiﬁcation: On image classiﬁcation, we compare ArchAttribute to relevant baseline methods on a “Segment AUC” metric, which computes the agreement between the estimated at-tribution of an image segment and that segment’s label. We obtain segment labels from the MS
COCO dataset [29] and match them to the label space of ImageNet. All explanation attributions are computed relative to ResNet152’s top-classiﬁcation in the joint label space. The segment label thus becomes whether or not the segment belongs to the same class as the top-classiﬁcation. Evaluation is conducted on all segments with valid labels in the MS COCO dev set. ArchAttribute performs especially well on extreme attributions in Table 2, as well as all attributions (in Appendix I).
Fig. 5 visualizes Archipelago on an accurate coronavirus (COVID-19) classiﬁer for chest X-rays [54], where S is gener-ated by top-5 pairwise interactions (§4.2.2). Shown is a random selection of test X-rays [9,10] that are classiﬁed COVID-positive.
The explanations tend to detect the “great vessels” near the heart.
Recommendation Task: Fig. 6 shows Archipelago’s result for this task using a state-of-the-art AutoInt model [45] for ad-recommendation. Here, our approach ﬁnds a positive interaction between“device_id” and “banner_pos” in the Avazu dataset [1], meaning that the online advertisement model decides the banner position based on user device_id. Note that for this task, there are no ground truth annotations. 5.4
Interactive Visualization
Figure 6: Online ad-targeting:
“banner_pos” is used to target ads to a user per their “device_id”.
While our visualizations have used a ﬁxed threshold on pairwise interaction strength, we can inter-actively visualize Archipelago explanations by varying the threshold. For example, a slider user interface offers this interactivity and allows users to perform in-depth analysis. Fig. 7 illustrates the interface, where moving the slider tells us when interactions appear and allows us to better judge 8
Figure 7: Interactive visualization of Archipelago. When moving the slider, the initial negativity of
“predictable” and “but” turns positive after interacting with the positive phrase “jumps with style”. model quality. Note that our interactive visualization is fast since interaction detection only runs once, and the additional ArchAttribute and interaction-merge steps are fast. 5.5 Runtime
Fig. 8 shows a serial runtime comparison of explainer methods for (a) BERT sentiment analysis on SST and (b) ResNet152 image classiﬁcation on ImageNet. Run-times correspond to static explanations and are averaged across 100 random data samples from respective test sets.
Archipelago outperforms the state-of-the-art. These exper-iments were done on a server with 32 Intel Xeon E5-2640 v2 CPUs @ 2.00GHz and 2 Nvidia 1080 Ti GPUs. 6