Abstract
In this work we consider partially observable environments with sparse rewards.
We present a self-supervised representation learning method for image-based obser-vations, which arranges embeddings respecting temporal distance of observations.
This representation is empirically robust to stochasticity and suitable for novelty detection from the error of a predictive forward model. We consider episodic and life-long uncertainties to guide the exploration. We propose to estimate the missing information about the environment with the world model, which operates in the learned latent space. As a motivation of the method, we analyse the exploration problem in a tabular Partially Observable Labyrinth. We demonstrate the method on image-based hard exploration environments from the Atari benchmark and report signiﬁcant improvement with respect to prior work. The source code of the method and all the experiments is available at https://github.com/htdt/lwm. 1

Introduction
A sparse reward signal is one of the challenging problems in reinforcement learning (RL). In this case, random exploration is inefﬁcient, the number of randomly performed steps grows exponentially with the number of distinct visited states, the agent is unlikely to stumble on the rewarding state. The intrinsic motivation, or curiosity, approach aims at this problem [4]. This method provides an intrinsic reward signal to encourage an agent to seek novel or rare states. There are several approaches to detect such states and calculate the reward: one class of methods estimates the novelty proportionally to an error of future state prediction (predictive forward models) [31, 25, 6], another class approximately counts visited states and estimates the novelty in an inverse ratio [32, 4, 10, 24, 26, 33]. Most of the methods estimate novelty in a life-long time frame, states are considered novel only if the agent did not observe them during the whole training. Some methods use an episodic time frame comparing only the states inside a speciﬁc episode [26, 2].
Often RL environments are only partially observable and the agent must maintain a belief state, i.e., a sufﬁcient statistic of the past, required for action selection. For instance, in the Atari Pong game, the state should include at least two consecutive frames of the environment, making it possible to determine the direction of the ball movement. This is a common situation, when the environment is partially observable and the reward is sparse at the same time. For instance, consider a labyrinth composed of rooms similar to each other. The agent observes only the room in which it is currently located, the reward is given only when the agent reaches the goal. Our experiments (Tab. 1) show that even the 16 room labyrinth is a challenging task for a common RL algorithm. Novelty estimation, based on current observation, is also inefﬁcient: each room looks similar even during one episode.
Visiting states with high uncertainty can be beneﬁcial for exploration. We distinguish three types of uncertainty by sources: from partial observability, from novelty (epistemic) and from stochasticity (aleatoric). The latter type is irrelevant to the exploration strategy, whereas the ﬁrst and the second 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
indicate states required to explore. We use a world model to estimate the missing information, i.e., a Recurrent Neural Network (RNN) trained to predict the next state given a current state and action, where a high prediction error indicates interesting states that may improve knowledge of the environment. The model maintains a belief state [11] allowing to detect missing information related to partial observability, e.g. prediction error changes before and after visiting a room during one episode. Being a predictive forward model, it allows to detect novel states in general, since the prediction error changes before and after the model is trained on the observation.
The desired properties of the world model for exploration are the insensitivity to stochasticity and the ability to extrapolate the state dynamics, such that the prediction error can be a measurement for novelty. We hypothesize that the temporal distance between observations indicates how different (and possibly novel) they are. We propose a self-supervised representation learning method based on minimization of Euclidean distance between representations of temporally close observations and the feature whitening to prevent degenerate solutions. The method produces a low-dimensional latent representation which is empirically robust to stochastic elements and is arranged respecting the temporal distance of observations. The proposed world model operates with latent representations, and the reconstruction error is calculated for low-dimensional latent states during the training, without relying on generative decoding of observation images.
To learn the exploration policy, we sum the environment (extrinsic) reward with the intrinsic reward proportional to the prediction error of the world model. Even if the initial environment was fully observable, the updated reward assignment transforms it into partially observable, i.e., the reward of a speciﬁc state may change during the episode. To address partial observability in our experiments we use a RNN architecture combined with a DQN (recurrent DQN) [13, 18]. Being off-policy, the algorithm maintains a replay buffer and reuses the past experience for training. We propose to recalculate the intrinsic reward for each sampling from the buffer, thus the estimation is always up-to-date, improving the sample efﬁciency of the algorithm. The experience from the buffer is also used to train the self-supervised representation and the world model.
Our contribution is as follows. We propose a method to estimate missing information and novelty in the environment with the world model, and we demonstrate it in the tabular Partially Observable
Labyrinth. We introduce the self-supervised representation learning method that scales up the world model to environments with image observations. We compare our method with other common representation learning methods, demonstrating arrangement properties. We report the scores on several challenging Atari environments [3] with sparse rewards, improving results with respect to prior exploration methods. 2