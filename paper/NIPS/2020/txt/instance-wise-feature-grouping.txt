Abstract
In many learning problems, the domain scientist is often interested in discover-ing the groups of features that are redundant and are important for classiﬁcation.
Moreover, the features that belong to each group, and the important feature groups may vary per sample. But what do we mean by feature redundancy? In this paper, we formally deﬁne two types of redundancies using information theory: Repre-sentation and Relevant redundancies. We leverage these redundancies to design a formulation for instance-wise feature group discovery and reveal a theoretical guideline to help discover the appropriate number of groups. We approximate mutual information via a variational lower bound and learn the feature group and selector indicators with Gumbel-Softmax in optimizing our formulation. Experi-ments on synthetic data validate our theoretical claims. Experiments on MNIST,
Fashion MNIST, and gene expression datasets show that our method discovers feature groups with high classiﬁcation accuracies. 1

Introduction
Data samples are typically represented by features that domain experts assume to be important for a learning problem; however, not all features are important. The goal of feature selection is to select which features are needed to improve learning performance. Moreover, knowing which features are important helps in understanding learning algorithms.
Traditionally, Feature Selection algorithms ﬁnd a global set of features for the entire data [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. While knowing the most important global features are useful, feature importance may vary across the entire population. For example in images, while one set of pixels may help us identify a shoe, a vastly different set of pixels would be required to identify a shirt. From this observation, there is an additional need for Feature Selection to be on a case-by-case basis, an approach also known as Instance-wise Feature Selection. A novel concept that has only been recently investigated in the context of explaining black-box models [11, 12, 13, 14]. Learning saliency maps [15] in some ways also provide some form of instance-wise feature importance by highlighting (weighting) important pixels in an image.
While Instance-wise Feature Selection focuses on each feature’s relationship to its labels, it ignores the interaction among features. Multiple features may be equally important and yet redundant in relation to each other. Traditional feature selection algorithms (such as LASSO [16]) tend to select just one of these redundant features. However, in some domains such as gene expression applications, we are interested not only in which genes (features) are important but also in which genes interact together for disease prediction. Therefore, in addition to Instance-wise Feature Selection, we wish to also group the features based on their relationship with each other and to the label. There exist
∗Signiﬁes equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
methods like group Lasso (GLasso) [17] that selects which feature groups are important given a predeﬁned grouping. Yet, in many applications the feature groups are unknown. Thus, methods that learn feature groups have been proposed [18, 19, 20, 21]. While these methods perform group feature selection, the groups are global and not instance-wise. In contrast to these approaches, this paper introduces instance-wise methods that can learn the feature group structure and identify its importance for prediction from an information theory perspective. We refer to this approach as
Instance-wise Feature Grouping.
Our Contribution. We introduce a novel method for learning instance-wise feature grouping, the group Interpreter (gI). Our formulation is made possible by our theoretical contribution of deﬁning the concept of redundancy in this setting. Leveraging mutual information’s ability to measure dependency, we formally deﬁne two types: Representation Redundancy captures the dependency between features while Relevant Redundancy captures the dependency between features and its corresponding labels.
We prove how these redundancies can be captured and describe the mechanisms by which information is preserved. Our analysis leads to a lower bound to identify the number of groups for each sample.
Moreover, we provide a practical algorithm that approximates mutual information (MI) through a variational lower bound. The algorithm also learns a mapping function that identiﬁes the most important feature groups on a sample by sample basis. Finally, our theories are experimentally veriﬁed on both synthetic and real data from ongoing research. Indeed, our method reveals the difference in gene expression based on smoking status. We make the source code publicly available at https://github.com/ariahimself/Instance-wise-Feature-Grouping.