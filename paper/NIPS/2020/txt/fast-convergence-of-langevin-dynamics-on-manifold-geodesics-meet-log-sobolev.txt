Abstract
Sampling is a fundamental and arguably very important task with numerous appli-cations in Machine Learning. One approach to sample from a high dimensional distribution e−f for some function f is the Langevin Algorithm (LA). Recently, there has been a lot of progress in showing fast convergence of LA even in cases where f is non-convex, notably Vempala and Wibisono [2019], Moitra and Risteski
[2020] in which the former paper focuses on functions f deﬁned in Rn and the latter paper focuses on functions with symmetries (like matrix completion type objectives) with manifold structure. Our work generalizes the results of Vempala and Wibisono [2019] where f is deﬁned on a manifold M rather than Rn. From technical point of view, we show that KL decreases in a geometric rate whenever the distribution e−f satisﬁes a log-Sobolev inequality on M . 1

Introduction
We focus on the problem of sampling from a distribution e−f (x) supported on a Riemannian manifold
M with standard volume measure. Sampling is a fundamental and arguably very important task with numerous applications in machine learning and Langevin dynamics is a quite standard approach.
There is a growing interest in Langevin algorithms, e.g. Welling and Teh [2011], Wibisono [2018],
Dalalyan [2017a], due to its simple structure and the good empirical behavior. The classic Riemannian
Langevin algorithm, e.g. Girolami and Calderhead [2011], Patterson and Teh [2013], Zhang et al.
[2020], is used to sample from distributions supported on Rn (or a subset D) by endowing Rn (or
D) a Riemannian structure. Beyond the classic application of Riemannian Langevin Algorithm (RLA), recent progress in Domingo-Enrich et al. [2020], Moitra and Risteski [2020], Li and Erdogdu
[2020] shows that sampling from a distribution on a manifold has application in matrix factorization, principal component analysis, matrix completion, solving SDP, mean ﬁeld and continuous games and GANs. Formally, a game with ﬁnite number of agents is called continuous if the strategy spaces are continuous, either a ﬁnite dimensional differential manifold or an inﬁnite dimensional Banach manifold Ratliff et al. [2013, 2016], Domingo-Enrich et al. [2020]. The mixed strategy is then a probability distribution on the strategy manifold and mixed Nash equilibria can be approximated by
Langevin dynamics.
In order to sample from a distribution on M , geodesic
Geodesic Langevin Algorithm (GLA). based algorithms (e.g. Geodesic Monte Carlo and Geodesic MCMC) are considered in Byrne and
Girolami [2013], Liu et al. [2016], where a geodesic integrator is used in the implementation. We propose a Geodesic Langevin Algorithm (GLA) as a natural generalization of unadjusted Langevin algorithm (ULA) from the Euclidean space to manifold M . The beneﬁt of GLA is to leverage sufﬁciently the geometric information (curvature, geodesic distance, isoperimetry) of M while keeping the structure of the algorithm simple enough, so that we can obtain a non-asymptotic convergence guarantee of the algorithm. In local coordinate systems, the Riemannian metric is represented by a matrix g = {gij}, see Deﬁnition 3.3. We denote gij the ij-th entry of the inverse 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
matrix g−1 of g, and |g| = det(gij), the determinant of the matrix {gij}. Then GLA is the stochastic process on M that is deﬁned by xk+1 = Expxk ((cid:15)F + (cid:112) 2(cid:15)g−1ξ0) where F = (F1, ..., Fn) with
Fi = − (cid:88) j gij ∂f
∂xj
+ 1 (cid:112)|g| (cid:88) j
∂
∂xj (cid:16)(cid:112)|g|gij(cid:17)
, (1) (2) (cid:15) > 0 is the stepsize, ξ0 ∼ N (0, I) is the standard Gaussian noise, and Expx(·) : TxM → M is the exponential map (Deﬁnition 3.5). Clearly GLA is a two-step discretization scheme of the Riemannian
Langevin equation dXt = F (Xt)dt + 2g−1dBt (cid:112) where F is given by (2). Suppose the position at time k is xk, then the next position xk+1 can be obtained by the following tangent-geodesic composition: 1. Tangent step: Take a local coordinate chart ϕ : Uxk → Rn at xk, this map induces the expression of gij and gij, then compute the vector v = (cid:15)F + (cid:112)2(cid:15)g−1ξ0 in tangent space
Txk M ; 2. Geodesic step: Solve the geodesic equation (a second order ODE) whose solution is a curve
γ(t) ⊂ ϕ(Uxk ), such that the initial conditions satisfy γ(0) = ϕ(xk) and γ(cid:48)(0) = v. Then let xk+1 = γ(1) be the updated point.
The exponential map and ODE solver for geodesic equations is commonly used in sampling algorithms on manifold, e.g. Vempala and Lee [2017], Byrne and Girolami [2013], Liu et al. [2016]. We will discuss on other approximations of the exponential map without solving ODEs through illustrations in a later section. Figure 1 gives an intuition of GLA on the unit sphere where the exponential map is
Expx(v) = cos((cid:107)v(cid:107) )x + sin((cid:107)v(cid:107) ) v (cid:107)v(cid:107) .
Figure 1: f (x1, x2, x3) = x2
Iterations: 100k. 1 + 3.05x2 2 − 0.9x2 3 + 1.1x1x2 + −1.02x2x3 + 2.1x3x1, (cid:15) = 0.1,
The main result on convergence is stated as follows.
Theorem 1.1 (Informal). Let M be a closed n-dimensional manifold (Deﬁnition 3.2). Suppose that
ν = e−f (x) is a distribution on M with α > 0 the log-Sobolev constant. Then there exists a real number K2, K3, K4, C, such that by choosing stepsize (cid:15) properly based on the Lipschitz constant of the Riemannian gradient of f , log-Sobolev constant of the target distribution ν, dimension and curvature of M , the KL divergence H(ρk|ν) decreases along the GLA iterations rapidly in the sense that
H(pk|ν) ≤ e−αk(cid:15)H(p0|ν) + (2nL2 + 2n3K2C + nK3K4). 16(cid:15) 3α 2
The same as unadjusted Langevin algorithm (ULA) in Euclidean space, GLA is a biased algorithm that converges to a distribution different from e−f (x). Practically we need a lower bound estimate for α. With additional condition on Ricci curvature, this lower bound can be chosen based on the diameter of M by Theorem ??.
Our main technical contributions are:
• A non-asymptotic convergence guarantee for Geodesic Langevin algorithm on closed mani-fold is provided, with the help of log-Sobolev inequality.
• The framework of this paper serves as the ﬁrst step understanding to the rate of convergence in sampling from distributions on manifold with log-Sobolev inequality, and can be general-ized to prove non-asymptotic convergence results for more general settings and more subtle algorithms, i.e., for open manifolds and unbiased algorithms.
Comparison to literarture The typical difference between algorithm (1) and the classic RLA is the use of exponential map. As (cid:15) → 0, both GLA and RLA boil down to the same continuous time
Langevin equation in the local coordinate system: dXt = F (Xt)dt + (cid:112) 2g−1dBt where F is given by (2) and Bt is the standard Brownian motion in Rn. The direct Euler-Maryuyama discretization iterates in the way that xk+1 = xt +(cid:15)F (xk)+(cid:112)2(cid:15)g−1(xk)ξ0. However, by adding the vector (cid:15)F (xk) + (cid:112)2(cid:15)g−1(xk)ξ0 that is in the tangent space to a point xk that is on the manifold M has no intrinsic geometric meaning, since the resulted point xk+1 is indeed not in M . The exponential map just gives a way to pull xk+1 back to M . On the other hand, since RLA is ﬁrstly used to sample from distributions on Rn (or its domain) with a Riemannian structure, Roberts and Stramer [2002],
Girolami and Calderhead [2011], Patterson and Teh [2013], Smith et al. [2018], this requires a global coordinate system of M , i.e. M is covered by a single coordinate chart and the iterations do not transit between different charts. But this makes it difﬁcult to use RLA when there are inevitably multiple coordinate charts on M . More sophisticated algorithms like Geodesic MCMC Liu et al.
[2016] is used to transit between different coordinate charts, but to the best knowledge of the authors, the rate of convergence is missing in the literature. Li and Erdogdu Li and Erdogdu [2020] generalize the result of Vempala and Wibisono [2019] by implementing the Riemannian Langevin algorithm in two steps (gradient+Riemannian Brownian motion). 2