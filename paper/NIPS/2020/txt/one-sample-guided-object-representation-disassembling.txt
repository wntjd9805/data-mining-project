Abstract
The ability to disassemble the features of objects and background is crucial for many machine learning tasks, including image classiﬁcation, image editing, visual concepts learning, and so on. However, existing (semi-)supervised methods all need a large amount of annotated samples, while unsupervised methods can’t handle real-world images with complicated backgrounds. In this paper, we introduce the One-sample Guided Object Representation Disassembling (One-GORD) method, which only requires one annotated sample for each object category to learn disassembled object representation from unannotated images. For the annotated one-sample, we
ﬁrst adopt some data augmentation strategies to generate some synthetic samples, which can guide the disassembling of the object features and background features.
For the unannotated images, two self-supervised mechanisms: dual-swapping and fuzzy classiﬁcation are introduced to disassemble object features from the background with the guidance of annotated one-sample. What’s more, we devise two metrics to evaluate the disassembling performance from the perspective of representation and image, respectively. Experiments demonstrate that the One-GORD achieves competitive dissembling performance and can handle natural scenes with complicated backgrounds. 1

Introduction
Learning disassembled object representation is a vital step in many machine learning tasks, including image editing, image classiﬁcation, few/zero-shot learning, and visual concepts learning. For example, many image editing [4, 21, 26] for objects typically rely on image segmentation techniques and human labor, which only handles object in image level. The existing classiﬁcation works [15, 18] usually train classiﬁers with large amounts of annotated samples to extract speciﬁc object features and identify them, which also has a serious cost of labor, time, and memory. For the few/zero-shot learning problem, most of works [2, 22, 27, 30] adopt representations extracted by pre-trained deep models as the features of speciﬁc objects. However, the representations extracted by pre-trained models usually contain many irrelevant features, which will disturb the performance of models. So, an object representation learning method that can learn the pure and entire features of the speciﬁc object with a few annotated data is desperately needed.
∗Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Until now, most object representation learning methods [6, 8, 12, 13, 23] are proposed to handle simple scenes with multiple objects in an unsupervised manner. However, those methods can’t handle real-world images with complicated backgrounds, which limits their application in many machine learning tasks. On the other hand, the existing supervised object representation learning methods are rarer. Some (semi-)supervised disentangling methods [11, 25] can be transferred to learn disassembled object representation through annotating the object information as labels. However, it still requires many annotated samples. Another line of works [14, 28] is concerned with obtaining the segmentation of objects and does not learn structured object representations.
In this paper, we propose the One-sample Guided Object Representation Disassembling (One-GORD) method, which only requires one annotated sample for each object category to learn disassembled object representation from a large number of unannotated images. The proposed One-GORD is composed of two modules: the augmented one-sample supervision module and the guided self-supervision module. In the one-sample guided module, we ﬁrst generate some synthetic sample pairs with data augmentation strategies. Then, following the “encoding-swapping-decoding" architecture, we swap the parts of their representations to reconstruct the synthetic ground-truth pairs, which guides the features of objects and backgrounds to be decoded into different parts of the representations.
In the guided self-supervision module, we introduce two self-supervised mechanisms: fuzzy classiﬁ-cation and dual swapping. For the dual swapping, given a pair of samples that are composed of the annotated one-sample and an unannotated image, we swap the ﬁrst halves of their representations to reconstruct the hybrid images. Then, the ﬁrst halves of the hybrids’ representations are swap back to reconstruct the original pair samples, which formats the self-supervision loss for the disassembling of unannotated images with the guidance of annotated one-sample. Meanwhile, the fuzzy classiﬁcation supervises the ﬁrst and latter halves of representation to extract features of any object category and background, respectively.
Furthermore, to verify the effectiveness of the proposed method, we devise two metrics to evaluate the modularity of representations and the integrity of images. The former measures the modularity and portability of the latent representations, while the latter evaluates the visual completeness of the reconstructed images. As will be demonstrated in our experiments, the proposed One-GORD achieves truly promising performance.
Our contribution is the proposed One-GORD, which only requires one annotated sample for learning disassembled object representation. Two self-supervised mechanisms format self-supervised losses for the disassembling of unannotated image representations with the guidance of annotated one-sample.
Meanwhile, We also introduce two disassembling metrics, upon which the proposed One-GORD achieves truly encouraging results. 2