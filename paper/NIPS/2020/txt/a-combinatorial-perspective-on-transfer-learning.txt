Abstract
Human intelligence is characterized not only by the capacity to learn complex skills, but the ability to rapidly adapt and acquire new skills within an ever-changing environment. In this work we study how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data. Our main postulate is that the combination of task segmentation, modular learning and memory-based ensembling can give rise to generalization on an exponentially growing number of unseen tasks. We provide a concrete instantiation of this idea using a combination of: (1) the Forget-Me-Not Process, for task segmentation and memory based ensembling; and (2) Gated Linear Networks, which in contrast to contemporary deep learning techniques use a modular and local learning mechanism. We demonstrate that this system exhibits a number of desirable continual learning properties: robustness to catastrophic forgetting, no negative transfer and increasing levels of positive transfer as more tasks are seen. We show competitive performance against both ofﬂine and online methods on standard continual learning benchmarks. 1

Introduction
Humans learn new tasks from a single temporal stream (online learning) by efﬁciently transferring experience of previously encountered tasks (continual learning). Contemporary machine learning algorithms struggle in both of these settings, and few attempts have been made to solve challenges at their intersection. Despite obvious computational inefﬁciencies, the dominant machine learning paradigm involves i.i.d. sampling of data at massive scale to reduce gradient variance and stabilize training via back-propagation. In the case of continual learning, the batch i.i.d. paradigm is often further extended to sample from a memory of experiences from all previous tasks. This is a popular method of overcoming “catastrophic forgetting" [CG88, MC89, Rob95], whereby a neural network trained on a new target task rapidly loses in its ability to solve previous source tasks.
Instead of considering “online" and “continual" learning as inconvenient constraints to avoid, in this paper we describe a framework that leverages them as desirable properties to enable effective, data-efﬁcient transfer of previously acquired skills. Core to this framework is the ability to ensemble task-speciﬁc neural networks at the level of individual nodes. This leads naturally to a desirable property we call combinatorial transfer, where a network of m nodes trained on h tasks can generalize to hm “pseudo-tasks". Although the distribution of tasks and pseudo-tasks may differ, we show that this method works very well in practice across a range of online continual learning benchmarks.
The ability to meaningfully ensemble individual neurons is not a property of contemporary deep neural networks, owing to a lack of explicit modularity in the distributed and tightly coupled feature representations learnt via backpropagation [BDR+19, CFB+19, PKRCS17]. To concretely instantiate our learning framework we instead borrow two complementary algorithms from recent literature: the
Gated Linear Network (GLN) and the Forget-Me-Not Process (FMN). We demonstrate that properties of both models are complementary and give rise to a system suitable for online continual learning. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2