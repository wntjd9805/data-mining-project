Abstract
Finding approximate Nash equilibria in zero-sum imperfect-information games is challenging when the number of information states is large. Policy Space Response
Oracles (PSRO) is a deep reinforcement learning algorithm grounded in game theory that is guaranteed to converge to an approximate Nash equilibrium. However,
PSRO requires training a reinforcement learning policy at each iteration, making it too slow for large games. We show through counterexamples and experiments that DCH and Rectiﬁed PSRO, two existing approaches to scaling up PSRO, fail to converge even in small games. We introduce Pipeline PSRO (P2SRO), the ﬁrst scalable PSRO-based method for ﬁnding approximate Nash equilibria in large zero-sum imperfect-information games. P2SRO is able to parallelize PSRO with convergence guarantees by maintaining a hierarchical pipeline of reinforcement learning workers, each training against the policies generated by lower levels in the hierarchy. We show that unlike existing methods, P2SRO converges to an approximate Nash equilibrium, and does so faster as the number of parallel workers increases, across a variety of imperfect information games. We also introduce an open-source environment for Barrage Stratego, a variant of Stratego with an approximate game tree complexity of 1050. P2SRO is able to achieve state-of-the-art performance on Barrage Stratego and beats all existing bots. Experiment code is available at https://github.com/JBLanier/pipeline-psro. 1

Introduction
A long-standing goal in artiﬁcial intelligence and algorithmic game theory has been to develop a general algorithm which is capable of ﬁnding approximate Nash equilibria in large imperfect-information two-player zero-sum games. AlphaStar [Vinyals et al., 2019] and OpenAI Five [Berner et al., 2019] were able to demonstrate that variants of self-play reinforcement learning are capable of achieving expert-level performance in large imperfect-information video games. However, these methods are not principled from a game-theoretic point of view and are not guaranteed to converge to an approximate Nash equilibrium. Policy Space Response Oracles (PSRO) [Lanctot et al., 2017]
∗Authors contributed equally 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is a game-theoretic reinforcement learning algorithm based on the Double Oracle algorithm and is guaranteed to converge to an approximate Nash equilibrium.
PSRO is a general, principled method for ﬁnding approximate Nash equilibria, but it may not scale to large games because it is a sequential algorithm that uses reinforcement learning to train a full best response at every iteration. Two existing approaches parallelize PSRO: Deep Cognitive
Hierarchies (DCH) [Lanctot et al., 2017] and Rectiﬁed PSRO [Balduzzi et al., 2019], but both have counterexamples on which they fail to converge to an approximate Nash equilibrium, and as we show in our experiments, neither reliably converges in random normal form games.
Although DCH approximates PSRO, it has two main limitations. First, DCH needs the same number of parallel workers as the number of best response iterations that PSRO takes. For large games, this requires a very large number of parallel reinforcement learning workers. This also requires guessing how many iterations the algorithm will need before training starts. Second, DCH keeps training policies even after they have plateaued. This introduces variance by allowing the best responses of early levels to change each iteration, causing a ripple effect of instability. We ﬁnd that, in random normal form games, DCH rarely converges to an approximate Nash equilibrium even with a large number of parallel workers, unless their learning rate is carefully annealed.
Rectiﬁed PSRO is a variant of PSRO in which each learner only plays against other learners that it already beats. We prove by counterexample that Rectiﬁed PSRO is not guaranteed to converge to a Nash equilibrium. We also show that Rectiﬁed PSRO rarely converges in random normal form games.
In this paper we introduce Pipeline PSRO (P2SRO), the ﬁrst scalable PSRO-based method for ﬁnding approximate Nash equilibria in large zero-sum imperfect-information games. P2SRO is able to scale up PSRO with convergence guarantees by maintaining a hierarchical pipeline of reinforcement learning workers, each training against the policies generated by lower levels in the hierarchy. P2SRO has two classes of policies: ﬁxed and active. Active policies are trained in parallel while ﬁxed policies are not trained anymore. Each parallel reinforcement learning worker trains an active policy in a hierarchical pipeline, training against the meta Nash equilibrium of both the ﬁxed policies and the active policies on lower levels in the pipeline. Once the performance increase of the lowest-level active worker in the pipeline does not improve past a given threshold in a given amount of time, the policy becomes ﬁxed, and a new active policy is added to the pipeline. P2SRO is guaranteed to converge to an approximate Nash equilibrium. Unlike Rectiﬁed PSRO and DCH, P2SRO converges to an approximate Nash equilibrium across a variety of imperfect information games such as Leduc poker and random normal form games.
We also introduce an open-source environment for Barrage Stratego, a variant of Stratego. Barrage
Stratego is a large two-player zero sum imperfect information board game with an approximate game tree complexity of 1050. We demonstrate that P2SRO is able to achieve state-of-the-art performance on Barrage Stratego, beating all existing bots.
To summarize, in this paper we provide the following contributions:
• We develop a method for parallelizing PSRO which is guaranteed to converge to an approxi-mate Nash equilibrium, and show that this method outperforms existing methods on random normal form games and Leduc poker.
• We present theory analyzing the performance of PSRO as well as a counterexample where
Rectiﬁed PSRO does not converge to an approximate Nash equilibrium.
• We introduce an open-source environment for Stratego and Barrage Stratego, and demon-strate state-of-the-art performance of P2SRO on Barrage Stratego. 2