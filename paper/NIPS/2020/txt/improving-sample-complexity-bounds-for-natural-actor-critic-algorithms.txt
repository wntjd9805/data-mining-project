Abstract
The actor-critic (AC) algorithm is a popular method to ﬁnd an optimal policy in rein-forcement learning. In the inﬁnite horizon scenario, the ﬁnite-sample convergence rate for the AC and natural actor-critic (NAC) algorithms has been established recently, but under independent and identically distributed (i.i.d.) sampling and single-sample update at each iteration. In contrast, this paper characterizes the con-vergence rate and sample complexity of AC and NAC under Markovian sampling, with mini-batch data for each iteration, and with actor having general policy class approximation. We show that the overall sample complexity for a mini-batch AC to attain an (cid:15)-accurate stationary point improves the best known sample complexity of AC by an order of O((cid:15)−1 log(1/(cid:15))), and the overall sample complexity for a mini-batch NAC to attain an (cid:15)-accurate globally optimal point improves the exist-ing sample complexity of NAC by an order of O((cid:15)−2/ log(1/(cid:15))). Moreover, the sample complexity of AC and NAC characterized in this work outperforms that of policy gradient (PG) and natural policy gradient (NPG) by a factor of O((1 − γ)−3) and O((1 − γ)−4(cid:15)−2/ log(1/(cid:15))), respectively. This is the ﬁrst theoretical study establishing that AC and NAC attain orderwise performance improvement over PG and NPG under inﬁnite horizon due to the incorporation of critic. 1

Introduction
The goal of reinforcement learning (RL) [39] is to maximize the expected total reward by taking actions according to a policy in a stochastic environment, which is modelled as a Markov decision process (MDP) [4]. To obtain an optimal policy, one popular method is the direct maximization of the expected total reward via gradient ascent, which is referred to as the policy gradient (PG) method [40, 47]. In practice, PG methods often suffer from large variance and high sampling cost caused by Monte Carlo rollouts to acquire the value function for estimating the policy gradient, which substantially slow down the convergence. To address such an issue, the actor-critic (AC) type of algorithms have been proposed [22, 23], in which critic tracks the value function and actor updates the policy using the return of critic. The usage of critic effectively reduces the variance of the policy update and the sampling cost, and signiﬁcantly speeds up the convergence.
The ﬁrst AC algorithm was proposed by [23], in which actor’s updates adopt the simple stochastic policy gradient ascent step. This algorithm was later extended to the natural actor-critic (NAC) algorithm in [33, 9], in which actor’s updates adopt the natural policy gradient (NPG) algorithm [18].
The asymptotic convergence of AC and NAC algorithms under both independent and identically distributed (i.i.d.) sampling and Markovian sampling have been established in [18, 21, 7, 9, 8].
The non-asymptotic convergence rate (i.e., the ﬁnite-sample analysis) of AC and NAC has recently been studied. More speciﬁcally, [54] studied the sample complexity of AC with linear function approximation in the linear quadratic regulator (LQR) problem. For general MDP with possibly 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Comparison of sample complexity of AC and NAC algorithms1,2
Algorithm
Reference
Actor-Critic
Natural Actor-Critic (Wang et al., 2019) [46] (Kumar et al., 2019) [24] (Qiu et al., 2019) [35]
This paper (Wang et al., 2019) [46]
This paper
Actor i.i.d. i.i.d. i.i.d.
Sampling
Critic i.i.d. i.i.d.
Total complexity3,4
O((cid:15)−4)
O((cid:15)−4)
Markovian O((cid:15)−3 log2(1/(cid:15)))
Markovian Markovian O((cid:15)−2 log(1/(cid:15))) i.i.d. i.i.d.
O((cid:15)−4)
Markovian Markovian O((cid:15)−2 log(1/(cid:15))) 1 The table includes all previous studies on ﬁnite-sample analysis of AC and NAC under inﬁnite-horizon
MDP and policy function approximation, to our best knowledge. 2 For comparison between our results of AC and NAC and the best known results of PG [49] and NPG
[1], please refer to the discussion after Theorem 2 and Theorem 3. 3 Total complexity of AC is measured to attain an ((cid:15) + error)-accurate stationary point ¯w, i.e., 2 < (cid:15) + error. Total complexity of NAC is measured to attain an ((cid:15) + error)-accurate (cid:107)∇wJ( ¯w)(cid:107)2 global optimum ¯w, i.e., J(π∗) − J( ¯w) < (cid:15) + error. 4 We do not include the dependence on 1 − γ into the complexity because most studies do not capture such dependence and it is difﬁcult to make a fair comparison. Our results do capture such dependence as speciﬁed in our theorems. inﬁnity state space, [46] studied AC and NAC with both actor and critic utilize overparameterized neural networks as approximation functions, [24] studied AC with general nonlinear policy class and linear function approximation for critic, but with the requirement that the true value function is in the linear function class of critic. [35] studied a similar problem as [24] with weaker assumptions.
Although having progressed signiﬁcantly, existing ﬁnite-sample analysis of AC and NAC have several limitations. They all assume that algorithms have access to the stationary distribution to generate i.i.d. samples, which can hardly be satisﬁed in practice. Moreover, existing studies focused on single-sample estimator for each update of actor and critic, which may not be overall sample-efﬁcient.
• In this paper, we consider the discounted MDP with inﬁnite horizon and possibly inﬁnite state and action space, and with the policy taking a general nonlinear function approximation. We study the online AC and NAC algorithms, which has the entire execution based on a single sample path and each update based on a Markovian mini-batch of samples taken from such a sample path. We characterize the convergence rate for both AC and NAC, and show that mini-batch AC improves the best known sample complexity of AC [35] by a factor of O((cid:15)−1 log(1/(cid:15))) to attain an (cid:15)-accurate stationary point, and mini-batch NAC improves the existing sample complexity [46] by a factor of O((cid:15)−2/ log(1/(cid:15))) to attain an (cid:15)-accurate globally optimal point. Table 1 includes the detailed comparison among AC and NAC algorithms.
Second, the sample complexity of AC and NAC characterized in the existing studies is no better (in fact often worse) than that of PG and NPG under inﬁnite horizon MDP. Speciﬁcally, the best known sample complexity O((cid:15)−3 log2(1/(cid:15))) [35] of AC is worse than that of PG O((cid:15)−2) in [55, 49], and the best known complexity O((cid:15)−4) [46] of NAC is the same as that of NPG [1]. Clearly, these theoretical studies of AC and NAC did not capture their performance advantage over PG and NPG due to the incorporation of critic. Furthermore, the existing studies of AC and NAC with discounted reward did not capture the dependence of the sample complexity on 1 − γ, and hence did not capture one important aspect of the comparison to PG and NPG.
• In this paper, for both AC and NAC, our characterization of the sample complexity is orderwisely better than the best known results for PG and NPG, respectively. Speciﬁcally, we show that AC improves the best known complexity O((1 − γ)−5(cid:15)−2) of PG in [49] by a factor of O((1 − γ)−3).
We further show that NAC improves signiﬁcantly upon the complexity O((1 − γ)−8(cid:15)−4) of NPG in [1] by a factor of O((1 − γ)−4(cid:15)−2/ log(1/(cid:15))). This is the ﬁrst time that AC and NAC are shown to have better convergence rate than PG and NPG in theory.
We develop the following new techniques in our analysis. To obtain the convergence rate for critic, we develop a new technique to handle the bias error caused by mini-batch Markovian sampling in the linear stochastic approximation (SA) setting, which is different in nature from how existing studies handle single-sample bias [6]. Our result shows that Markovian mini-batch linear SA outperforms 2
single-sample linear SA in terms of the total sample complexity by a factor of log(1/(cid:15)) [6, 38, 16].
For actor’s update in AC, we develop a new technique to bound the bias error caused by mini-batch
Markovian sampling in the nonlinear SA setting, which is different from the bias error of linear SA in critic’s update. We show that the Markovian minibatch update allows a constant stepsize for actor’s update, which yields a faster convergence rate and hence improves the total sample complexity by a factor of O((cid:15)−1) compared with previous study on AC [35]. For actor’s update in NAC, we discover that the variance of actor’s update is self-reduced under the Markovian mini-batch update, which yields an improved complexity by a factor of O((cid:15)−2) compared with previous study on NAC [46]. 1.1