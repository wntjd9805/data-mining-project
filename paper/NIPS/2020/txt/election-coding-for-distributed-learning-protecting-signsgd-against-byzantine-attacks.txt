Abstract
Current distributed learning systems suffer from serious performance degradation under Byzantine attacks. This paper proposes ELECTION CODING, a coding-theoretic framework to guarantee Byzantine-robustness for distributed learning algorithms based on signed stochastic gradient descent (SignSGD) that minimizes the worker-master communication load. The suggested framework explores new information-theoretic limits of ﬁnding the majority opinion when some workers could be attacked by adversary, and paves the road to implement robust and communication-efﬁcient distributed learning algorithms. Under this framework, we construct two types of codes, random Bernoulli codes and deterministic algebraic codes, that tolerate Byzantine attacks with a controlled amount of computational redundancy and guarantee convergence in general non-convex scenarios. For the
Bernoulli codes, we provide an upper bound on the error probability in estimating the signs of the true gradients, which gives useful insights into code design for
Byzantine tolerance. The proposed deterministic codes are proven to perfectly tolerate arbitrary Byzantine attacks. Experiments on real datasets conﬁrm that the suggested codes provide substantial improvement in Byzantine tolerance of distributed learning systems employing SignSGD. 1

Introduction
The modern machine learning paradigm is moving toward parallelization and decentralization [4, 12, 18] to speed up the training and provide reliable solutions to time-sensitive real-world problems.
There has been extensive work on developing distributed learning algorithms [1, 16, 21, 22, 24, 25] to exploit large-scale computing units. These distributed algorithms are usually implemented in parameter-server (PS) framework [17], where a central PS (or master) aggregates the computational results (e.g., gradient vectors minimizing empirical losses) of distributed workers to update the shared model parameters. In recent years, two issues have emerged as major drawbacks that limit the performance of distributed learning: Byzantine attacks and communication burden.
Nodes affected by Byzantine attacks send arbitrary messages to PS, which would mislead the model updating process and severely degrade learning capability. To counter the threat of Byzantine attacks, much attention has been focused on robust solutions [2, 13, 14]. Motivated by the fact that a naive linear aggregation at PS cannot even tolerate Byzantine attack on a single node, the authors of [7, 10, 31] considered median-based aggregation methods. However, as data volume and the number of workers increase, computing the median involves a large cost [7] far greater than the cost 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Conventional Setting vs Suggested ELECTION CODING (b) Suggested scheme applied to SIGNSGD
Figure 1: Use of coding to protect the majority vote. (a) The conventional scheme (without coding) is vulnerable to Byzantine attacks, while the suggested scheme is not. In the suggested scheme, each polling station gathers the votes of a subset of voters, and sends the majority value to the master. (b) In the real setup, voters are like data partitions Di while polling stations represent workers. for batch gradient computations. Thus, recent works [9, 23] instead suggested redundant gradient computation that tolerates Byzantine attacks.
Another issue is the high communication burden caused by transmitting gradient vectors between
PS and workers for updating network models. Regarding this issue, the authors of [3, 5, 6, 15, 19, 27–29] considered quantization of real-valued gradient vectors. The signed stochastic gradient descent method (SIGNSGD) suggested in [5] compresses a real-valued gradient vector g into a binary vector sign(g), and updates the model using the 1-bit compressed gradients. This scheme minimizes the communication load from PS to each worker for transmitting the aggregated gradient.
A further variation called SIGNSGD WITH MAJORITY VOTE (SIGNSGD-MV) [5, 6] also applies 1-bit quantization on gradients communicated from each worker to PS in achieving minimum master-worker communication in both directions. These schemes have been shown to minimize the communication load while maintaining the SGD-level convergence speed in general non-convex problems. A major issue that remains is the lack of Byzantine-robust solutions suitable for such communication-efﬁcient learning algorithms.
In this paper, we propose ELECTION CODING, a coding-theoretic framework to make SIGNSGD-MV [5] highly robust to Byzantine attacks. In particular, we focus on estimating the next step for model update used in [5], i.e., the majority voting on the signed gradients extracted from n data partitions, under the scenario where b of the n worker nodes are under Byzantine attacks.
Let us illustrate the concept of the suggested framework as a voting scenario where n people vote for either one candidate (+1) or the other (−1). Suppose that each individual must send her vote directly to the central election commission, or a master. Assume that frauds can happen during the transmittal of votes, possibly ﬂipping the result of a closely contested election, as a single fraud did in the ﬁrst example of Fig. 1a. A simple strategy can effectively combat this type of voting fraud. First set up multiple polling stations and let each person go to multiple stations to cast her ballots. Each station
ﬁnds the majority vote of its poll and sends it to the central commission. Again a fraud can happen as the transmissions begin. However, as seen in the second example of Fig. 1a, the single fraud was not able to change the election result, thanks to the built-in redundancy in the voting process. In this example, coding amounts to telling each voter to go to which polling stations. In the context of
SignSGD, the individual votes are like the locally computed gradient signs that must be sent to the
PS, and through some ideal redundant allocation of data partitions we wish to protect the integrity of the gathered gradient computation results under Byzantine attacks on locally computed gradients.
Main contributions: Under this suggested hierarchical voting framework, we construct two ELEC-TION coding schemes: random Bernoulli codes and deterministic algebraic codes. Regarding the random Bernoulli codes, which are based on arbitrarily assigning data partitions to each node with probability p, we obtain an upper bound on the error probability in estimating the sign of the true gradient. Given p = Θ( log(n)/n), the estimation error vanishes to zero for arbitrary b, under the asymptotic regime of large n. Moreover, the convergence of Bernoulli coded systems are proven in general non-convex optimization scenarios. As for the deterministic codes, we ﬁrst obtain the necessary and sufﬁcient condition on the data allocation rule, in order to accurately estimate the majority vote under Byzantine attacks. Afterwards, we suggest an explicit coding scheme which achieves perfect Byzantine tolerance for arbitrary n, b.
Finally, the mathematical results are conﬁrmed by simulations on well-known machine learning architectures. We implement the suggested coded distributed learning algorithms in PyTorch, and deploy them on Amazon EC2 using Python with MPI4py package. We trained RESNET-18 using
CIFAR-10 dataset as well as a logistic regression model using Amazon Employee Access dataset. (cid:112) 2
Figure 2: A formal description of the suggested ELECTION CODING framework for estimating the majority opinion µ. This framework is applied for each coordinate of the model parameter w ∈ Rd in a parallel manner.
The experimental results conﬁrm that the suggested coded algorithm has signiﬁcant advantages in tolerating Byzantines compared to the conventional uncoded method, under various attack scenarios.