Abstract
Feature attribution methods, which explain an individual prediction made by a model as a sum of attributions for each input feature, are an essential tool for understanding the behavior of complex deep learning models. However, ensuring that models produce meaningful explanations, rather than ones that rely on noise, is not straightforward. Exacerbating this problem is the fact that attribution methods do not provide insight as to why features are assigned their attribution values, leading to explanations that are difﬁcult to interpret. In real-world problems we often have sets of additional information for each feature that are predictive of that feature’s importance to the task at hand. Here, we propose the deep attribution prior (DAPr) framework to exploit such information to overcome the limitations of attribution methods. Our framework jointly learns a relationship between prior information and feature importance, as well as biases models to have explanations that rely on features predicted to be important. We ﬁnd that our framework both results in networks that generalize better to out of sample data and admits new methods for interpreting model behavior. 1

Introduction
Recent advances in machine learning have come in the form of complex models that humans struggle to interpret. In response to the black-box nature of these models, a variety of recent work has focused on model interpretability [11]. One particular line of work that has gained much attention is that of feature attribution methods [22, 37, 28, 3, 33]. Given a model and a speciﬁc prediction made by that model, these methods assign a numeric value to each input feature, indicating how important that feature was for the given prediction (Figure 1a). A variety of such methods have been proposed, and previous work has focused on how such methods can be used to gain insight into model behavior in applications where model trust is critical [43, 31]
Given a set of attributions, a natural question is why a feature was assigned a speciﬁc attribution value. In some settings it is easy for a human to evaluate the sensibility of attributions; for example, in image classiﬁcation problems we can overlay attribution values on the original image. However, in many other domains we do not have the ability to assess the validity of attribution values so 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
easily. Recent work [30, 7, 29] has attempted to address this problem by regularizing model training so that both model predictions and explanations agree with prior human knowledge. While such regularization methods do lead to noticeably different attributions, they suffer from two shortcomings.
First, they require a human expert with prior knowledge about a given problem domain to construct a domain-speciﬁc regularization function. Second, after a set of feature attribution values is produced for a given prediction, we still do not have a human-interpretable way to understand why that set of values was produced beyond trusting in the regularization procedure.
At the same time, the overparameterization of deep learning models makes them prone to overﬁtting to noise. This limitation has stalled the adoption of deep models in domains where data acquisition is difﬁcult, such as many areas of medicine [5]. As such, methods that encourage deep models to learn more generalizable representations, even when sample sizes are small, are in high demand.
In many real-world tasks we have access to sets of information about each feature that characterize the feature. We refer to such sets of information as meta-features. Meta-features can potentially encode information on a feature’s relative importance across all samples to the task at hand. For example, we can consider a task where we seek to predict a user’s rating for a new movie based on their previous ratings for other movies. In this case we would expect ratings for movies with similar meta-feature values to be more relevant than those for dissimilar ﬁlms. Potential meta-features to capture the similarity between features in this task could include movie genre and director, among others. By using meta-features to bias models to focus on more relevant features when making predictions, we may be able to achieve both greater model accuracy and interpretability. For a given problem, while a practitioner may have an idea of potential meta-features that are relevant to the problem, it is far less likely that they know a priori a speciﬁc relationship between these meta-features and global feature importance. As a ﬁrst step towards mitigating this problem, we propose the deep attribution prior (DAPr) framework for training deep neural networks that simultaneously learns a relationship between meta-features and global feature importance values, and biases the prediction model to have local explanations that broadly agree with this learned relationship (Figure 1b). For clarity, in the remainder of the text we use feature importance speciﬁcally to refer to a feature’s global relevance to a task across samples, and attribution value to refer to its local relevance for a speciﬁc prediction.
We apply our method to a synthetic dataset and two real-world biological datasets. Empirically we ﬁnd that models trained using the DAPr framework achieve better performance on tasks where training data is limited. Furthermore, we demonstrate that such learned meta-feature to feature importance relationships can be leveraged to obtain more insight into the behavior of deep models.
Figure 1: (a) An attribution method Φ is used to explain the decision of an arbitrary black-box model. (b) Overview of the DAPr framework. In addition to training a prediction model, we train a second prior model to use meta-features to predict global feature importance values. We also add a penalty term to our prediction model’s loss function that encourages the values of local attributions to be similar to predicted global importance values. 2
2