Abstract
Alon et al. [4] and Bun et al. [10] recently showed that online learnability and private PAC learnability are equivalent in binary classiﬁcation. We investigate whether this equivalence extends to multi-class classiﬁcation and regression. First, we show that private learnability implies online learnability in both settings. Our extension involves studying a novel variant of the Littlestone dimension that de-pends on a tolerance parameter and on an appropriate generalization of the concept of threshold functions beyond binary classiﬁcation. Second, we show that while online learnability continues to imply private learnability in multi-class classiﬁ-cation, current proof techniques encounter signiﬁcant hurdles in the regression setting. While the equivalence for regression remains open, we provide non-trivial sufﬁcient conditions for an online learnable class to also be privately learnable. 1

Introduction
Online learning and differentially-private (DP) learning have been well-studied in the machine learning literature. While these two subjects are seemingly unrelated, recent papers have revealed a strong connection between online and private learnability via the notion of stability [2, 3, 17]. The notion of differential privacy is, at its core, less about privacy and more about algorithmic stability since the output distribution of a DP algorithm should be robust to small changes in the input. Stability also plays a key role in developing online learning algorithms such as follow-the-perturbed-leader (FTPL) and follow-the-regularized-leader (FTRL) [1].
Recently Alon et al. [4] and Bun et al. [10] showed that online learnability and private PAC learnability are equivalent in binary classiﬁcation. Alon et al. [4] showed that private PAC learnability implies
ﬁnite Littlestone dimension (Ldim) in two steps; (i) every approximately DP learner for a class with
Ldim d requires Ω(log∗ d) thresholds (see Section 2.4 for the deﬁnition of log∗), and (ii) the class of thresholds over N cannot be learned in a private manner. Bun et al. [10] proved the converse statement via a notion of algorithmic stability, called global stability. They showed (i) every class with ﬁnite
Ldim can be learned by a globally-stable learning algorithm and (ii) they use global stability to derive a DP algorithm. In this work, we investigate whether this equivalence extends to multi-class classiﬁcation (MC) and regression, which is one of open questions raised by Bun et al. [10].
In general, online learning and private learning for MC and regression have been less studied. In binary classiﬁcation without considering privacy, the Vapnik-Chervonenkis dimension (VCdim) of hypothesis classes yields tight sample complexity bounds in the batch learning setting, and Littlestone
[20] deﬁned Ldim as a combinatorial parameter that was later shown to fully characterize hypothesis classes that are learnable in the online setting [8]. Until recently, however, it was unknown what
∗Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
complexity measures for MC or regression classes characterize online or private learnability. Daniely et al. [11] extended the Ldim to the MC setting, and Rakhlin et al. [22] proposed the sequential fat-shattering dimension, an online counterpart of the fat-shattering dimension in the batch setting [6]. 1.1