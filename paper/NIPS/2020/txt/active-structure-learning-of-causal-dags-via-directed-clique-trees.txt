Abstract
A growing body of work has begun to study intervention design for efﬁcient structure learning of causal directed acyclic graphs (DAGs). A typical setting is a causally sufﬁcient setting, i.e. a system with no latent confounders, selection bias, or feedback, when the essential graph of the observational equivalence class (EC) is given as an input and interventions are assumed to be noiseless. Most existing works focus on worst-case or average-case lower bounds for the number of interventions required to orient a DAG. These worst-case lower bounds only establish that the largest clique in the essential graph could make it difﬁcult to learn the true DAG. In this work, we develop a universal lower bound for single-node interventions that establishes that the largest clique is always a fundamental impediment to structure learning. Speciﬁcally, we present a decomposition of a
DAG into independently orientable components through directed clique trees and use it to prove that the number of single-node interventions necessary to orient any
DAG in an EC is at least the sum of half the size of the largest cliques in each chain component of the essential graph. Moreover, we present a two-phase intervention design algorithm that, under certain conditions on the chordal skeleton, matches the optimal number of interventions up to a multiplicative logarithmic factor in the number of maximal cliques. We show via synthetic experiments that our algorithm can scale to much larger graphs than most of the related work and achieves better worst-case performance than other scalable approaches. 1 1

Introduction
Causal modeling is an important tool in medicine, biology and econometrics, allowing practitioners to predict the effect of actions on a system and the behavior of a system if its causal mechanisms change due to external factors (Pearl, 2009; Spirtes et al., 2000; Peters et al., 2017). A commonly-used model 1A code base to recreate these results can be found at https://github.com/csquires/dct-policy. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is the directed acyclic graph (DAG), which is capable of modeling causally sufﬁcient systems, i.e. systems with no latent confounders, selection bias, or feedback. However, even in this favorable setup, a causal model cannot (in general) be fully identiﬁed from observational data alone; in these cases experimental (“interventional”) data is necessary to resolve ambiguities about causal relationships.
In many real-world applications, interventions may be time-consuming or expensive, e.g. randomized controlled trials or gene knockout experiments. These settings crucially rely on intervention design, i.e. ﬁnding a cost-optimal set of interventions that can fully identify a causal model. Recently, many methods have been developed for intervention design under different assumptions (He & Geng, 2008;
Hyttinen et al., 2013; Shanmugam et al., 2015; Kocaoglu et al., 2017; Lindgren et al., 2018).
In this work we extend the Central Node algorithm of Greenewald et al. (2019) to learn the structure of causal graphs in a causally sufﬁcient setting from interventions on single variables for both noiseless and noisy interventions. Noiseless interventions are able to deterministically orient a set of edges, while noisy interventions result in a posterior update over a set of compatible graphs. We also focus only on interventions with a single target variable, i.e. single-node interventions, but as opposed to (Greenewald et al., 2019) which focuses on limited types of graphs, we allow for general DAGs but only consider noiseless interventions. In particular, we focus on adaptive intervention design, also known as sequential or active (He & Geng, 2008), where the result of each intervention is incorporated into the decision-making process for later interventions. This contrasts with passive intervention design, for which all interventions are decided beforehand.
Universal lower bound. Our key contribution is to show that the problem of fully orienting a DAG with single-node interventions is equivalent to fully orienting special induced subgraphs of the DAG, called residuals (Theorem 1 below). Given this decomposition, we prove a universal lower bound on the minimum number of single-node interventions necessary to fully orient any DAG in a given
Markov Equivalence Class (MEC), the set of graphs that ﬁt the observational distribution. This lower bound is equal to the sum of half the size of the largest cliques in each chain component of the essential graph (Theorem 2). This result has a surprising consequence: the largest clique is always a fundamental impediment to structure learning. In comparison, prior work (Hauser & Bühlmann, 2014; Shanmugam et al., 2015) established worst-case lower bounds based on the maximum clique size, which only implied that the largest clique in each chain component of the essential graph could make it difﬁcult to learn the true DAG.
Intervention policy. We also propose a novel two-phase single-node intervention policy. The ﬁrst phase, based on the Central Node algorithm, uses properties of directed clique trees (Deﬁnition 2) to reduce the identiﬁcation problem to identiﬁcation within the (DAG dependent) residuals. The second phase then completes the orientations within each residual. We cover the condition of intersection-incomparability for the chordal skeleton of a DAG (Kumar & Madhavan (2002) introduce this condition in the context of graph theory) . We show that under this condition, our policy uses at most
O(log Cmax) times as many interventions as are used by the (DAG dependent) optimal intervention set, where Cmax is the greatest number of maximal cliques in any chain component (Theorem 3).
Finally, we evaluate our policy on general synthetic DAGs. We ﬁnd that our intervention policy performs comparably to intervention policies in previous work, while being much more scalable than most policies and adapting more effectively to the difﬁculty of the underlying identiﬁcation problem. 2 Preliminaries
We brieﬂy review our notation and terminology for graphs. A mixed graph G is a tuple of vertices
V (G), directed edges D(G), bidirected edges B(G), and undirected edges U (G). Directed, bidi-rected, and undirected edges between vertices i and j in G are denoted i →G j, i ↔G j, and i −G j, respectively. We use asterisks as wildcards for edge endpoints, e.g., i ∗→Gj denotes either i →G j or i ↔G j. A directed cycle in a mixed graph is a sequence of edges i ∗→G . . . ∗→G i with at least one directed edge. A mixed graph is a chain graph if it has no directed cycles and B(G) = ∅, and a chain graph is called a directed acyclic graph (DAG) if we also have U (G) = ∅. An undirected graph is a mixed graph with B(G) = ∅ and D(G) = ∅.
DAGs and (I-)Markov equivalence. DAGs are used to represent causal models (Pearl, 2009). Each vertex i is associated with a random variable Xi. The skeleton of graph D, skel(D), is the undirected graph with the same vertices and adjacencies as D. A distribution f is Markov w.r.t. a DAG D if it 2
factors as f (X) = (cid:81) i∈V (D) f (Xi | XpaD(i)). Two DAGs D1 and D2 are called Markov equivalent if all positive distributions f which are Markov to D1 are also Markov to D2 and vice versa. The set of DAGs that are Markov equivalent to D is the Markov equivalence class (MEC), denoted as [D].
[D] is represented by a chain graph called the essential graph E(D), which has the same skeleton as D, with directed edges i →E(D) j if i →D(cid:48) j for all D(cid:48) ∈ [D], and undirected edges otherwise.
Given an intervention I ⊆ V (D), the distributions (f obs, f I ) are I-Markov to D if f obs is Markov to
D and f I factors as f I (X) = (cid:89) i(cid:54)∈I f obs(Xi | XpaD(i)) (cid:89) i∈I f I (Xi | XpaD(i)) where paD(i) represents the set of parents of vertex i in the DAG D. Given a list of interventions I =
[I1, . . . , IM ], the set of distributions {f obs, f I1, . . . , f IM } is I-Markov to a DAG D if (f obs, f Im) is Im-Markov to D for ∀m = 1 . . . M . The I-Markov equivalence class of D (I-MEC), denoted as [D]I, can be represented by the I-essential graph EI(D) with the same adjacencies as D and i →EI (D) j if i →D(cid:48) j for all D(cid:48) ∈ [D]I.
The edges which are undirected in the essential graph E(D), but directed in the I-essential graph
EI(D), are the edges which are learned from performing the interventions in I. In the special case of a single-node intervention, the edges learned are all of those incident to the intervened node, along with any edges learned via the set of logical constraints known as Meek rules Appendix A.
Structure of essential graphs. We now report a known result that proves that any intervention policy can split essential graphs in components that can be oriented independently. The chain components of a chain graph G, denoted CC(G), are the connected components of the graph after removing its directed edges. These chain components are then clearly undirected graphs. An undirected graph is chordal if every cycle of length greater than 3 has a chord, i.e., an edge between two non-consecutive vertices in the cycle.
Lemma 1 (Hauser & Bühlmann (2014)). Every I-essential graph is a chain graph with chordal chain components. Orientations in one chain component do not affect orientations in other components.
Deﬁnition 1. A DAG whose essential graph has a single chain component is called a moral DAG.
In many of the following results we will consider moral DAGs, since once we can orient moral DAGs we can easily generalize to general DAGs through these results.
Intervention Policies. An intervention policy π is a (possibly randomized) map from (I-)essential graphs to interventions. An intervention policy is adaptive if each intervention Im is decided based on information gained from previous interventions, and passive if the whole set of interventions I is decided prior to any interventions being performed. An intervention is noiseless if the intervention set I collapses the set of compatible graphs exactly to the I-MEC, while noisy interventions simply induce a posterior update on the distribution over compatible graphs. Most policies assume that the
MEC is known (e.g., it has been estimated from observational data) and interventions are noiseless; this is true of our policy too. Moreover, we focus only on interventions on a single target variable, i.e. single-node interventions. We discuss previous work on intervention policies in Section 6. 3 Universal lower-bound in the number of single-node interventions
In this section we prove a lower-bound on any possible single-node policy (Theorem 2) by decompos-ing the complete orientation of a DAG in terms of the complete orientation of smaller independent subgraphs, called residuals (Theorem 1), deﬁned on a novel graphical structure, directed clique trees (DCTs). We provide all proofs in the Appendix.
First, we review the standard deﬁnitions of clique trees and clique graphs for undirected chordal graphs (see also (Galinier et al., 1995)). A clique C ⊆ V (G) is a subset of the nodes with an edge between each pair of nodes. A clique C is maximal if C ∪ {v} is not a clique for any v ∈ V (G) \ C.
The set of maximal cliques of G is denoted C(G). The clique number of G is ω(G) = maxC∈C |C|.
A clique tree (aka a junction tree) TG of a chordal graph is a tree with vertices C(G) that satisﬁes the induced subtree property, i.e., for any v ∈ V (G), the induced subgraph on the set of cliques containing v is a tree. A chordal graph can have multiple clique trees, so we denote the set of all clique trees of G as T (G). A clique graph ΓG is the graph union of all clique trees, i.e. the undirected graph 3
Figure 1: A moral DAG (a), one of its clique trees (b), its two DCTs (c-d) and the DCG (e).
Figure 2: Examples of edge orientations.
Figure 3: A DAG and its CDCTs with (using only edge a) and without arrow-meets (edge b). with V (ΓG) = C(G) and U (ΓG) = ∪T ∈T (G)U (T ). A useful characterization of the clique trees of
G are as the max-weight spanning trees of the weighted clique graph WG (Koller & Friedman, 2009), which is a complete graph over vertices C(G), with the edge C1 −WG C2 having weight |C1 ∩ C2|.
Given a moral DAG D, we can trivially deﬁne its clique trees T (D) as the clique trees of its skeleton
G = skel(D), i.e. T (G). For example, in Fig. 1 (a) we show a DAG, where we have chosen a color for each of the cliques, while in Fig. 1 (b) we show one of its clique trees. We now deﬁne a directed counterpart to clique trees based on the orientations in the underlying DAG:
Deﬁnition 2. A directed clique tree TD of a moral DAG D has the same vertices and adjacencies as a clique tree TG of G = skel(D). For each ordered pair of adjacent cliques C1 ∗−∗ C2 we orient the edge mark of C2 as:
• C1 ∗→C2, if ∀v12 ∈ C1 ∩ C2 and ∀v2 ∈ C2 \ C1, we have v12 →D v2 in the DAG D;
• C1 ∗−C2 otherwise, i.e. if there exists at least one incoming edge from C2 \ C1 into C1 ∩ C2, where we recall that ∗ denotes a wildcard for an edge. Thus, the above conditions only decide the presence or absence of an arrowhead at C2; the presence or absence of an arrowhead at C1 is decided when considering the reversed order.
A DAG can have multiple directed clique trees (DCTs), as shown in Fig. 1 (c) and (d). In ﬁgures, we annotate edges with the intersection between cliques. Fig. 1 (c) represents the directed clique tree corresponding to the standard clique tree in Fig. 1 (b). In Fig. 2 we show in detail the orientations for two of the directed clique edges following Deﬁnition 2, the red edges are outcoming from the clique intersection, while the blue edge is incoming in the intersection. Deﬁnition 2 also implies each edge that is shared between two different clique trees has a unique orientation (since it is based on the underlying DAG), so we can deﬁne the directed clique graph (DCG) ΓD of a moral DAG D as the graph union of all directed clique trees of D. We show an example of a DCG in Fig. 1(e). As can be seen in the examples in Fig. 1, DCTs can contain directed and bidirected edges, and, as we prove in Appendix C, no undirected edgees. We deﬁne the bidirected components of a DCT as:
Deﬁnition 3. The bidirected components of TD, B(TD), are the connected components of TD after removing directed edges.
Another structure that can happen in a DCT is when two arrows meet at the same clique. To avoid confusing associations with colliders in DAGs, we call these structures in DCTs arrow-meets. Arrow-meets will prove to be challenging for our algorithms, so we introduce intersection incomparability and prove that in case it holds there can be no arrow-meets: 4
Figure 4: A DAG, its CDCT and its residuals.
Figure 5: DAGs in the same MEC with m(D1) (cid:54)= m(D2).
Deﬁnition 4. A pair of edges C1 −TG C2 and C2 −TG C3 are intersection comparable if C1 ∩ C2 ⊆
C2 ∩ C3 or C1 ∩ C2 ⊇ C2 ∩ C3. Otherwise they are intersection incomparable.
For example, in Fig. 1 (e), the edges {2, 5, 6}↔{2, 3, 4} and {2, 3, 4}←{1, 2, 3} are intersection comparable, since {2} ⊂ {1, 2}, while {2, 5, 6}↔{2, 3, 4} and {2, 5, 6}→{5, 7} are intersection incomparable, since {2} (cid:54)⊆ {5} and {5} (cid:54)⊆ {2}.
Proposition 1. Suppose C1 ∗→TD C2 and C2 ←∗ TD C3 in TD. Then these edges are intersection comparable. Equivalently in the contrapositve, if C1 ∗→TD C2 and C2 ∗−∗ TD C4 are intersection incomparable, we can immediately deduce that C2 →TD C4.
Bidirected components do not have a clear ordering, so we contract them into single nodes in a contracted DCT (CDCT), and prove we can always construct a tree-like CDCT for any moral DAG:
Deﬁnition 5. The contracted directed clique tree (CDCT) ˜TD of a DCT TD is a graph on the vertex set B1, B2 . . . BK ∈ B(TD) with B1 → ˜TD
B2 if C1 →TD C2 for any clique C1 ∈ B1 and C2 ∈ B2.
Lemma 2. For any moral DAG D, one can always construct a CDCT with no arrow-meets.
In particular, one can adapt Kruskal’s algorithm for ﬁnding a max-weight spanning tree to construct a DCT from the weighted clique graph and then contract it, as shown in detail in Algorithm 3 in
Appendix D. In Fig. 3 we show an example of a CDCT with arrow-meets (represented by the black edge and the edge labelled “a”) and its equivalent no arrow-meets version (represented by the black edge and the edge “b”) . Since we can always construct a CDCT with no arrow-meets, we assume w.l.o.g. that the CDCT is a tree. The CDCT allows us to deﬁne a decomposition of a moral DAG into independently orientable components. We call these components residuals, since they extend the notion of residuals in rooted, undirected clique trees (Vandenberghe et al., 2015). Formally:
Deﬁnition 6. For a tree-like CDCT ˜TD of a moral DAG D, the residual of its node B is deﬁned as (B) = D|B\P , where P is its parent in ˜TD (or if there is none, P = ∅) and D|B\P is the
Res ˜TD induced subgraph of D over the subset of V (D) that are assigned to B but not to P . We denote the set of all residuals of ˜TD by R( ˜TD).
Intuitively this describes the subgraphs in which we cut all edges that are captured in the CDCT, as shown in Fig. 4. We now generalize our results from a moral DAG to a general DAG. Surprisingly, we show that orienting all of the residuals for all chain components in the essential graph is both necessary and sufﬁcient to completely orient any DAG. We start by introducing a VIS:
Deﬁnition 7. Given a general DAG D, a verifying intervention set (VIS) is a set of single-node interventions I that fully orients the DAG starting from an essential graph, i.e. EI(D) = D. A minimal VIS (MVIS) is a VIS of minimal size. We denote the size of the minimal VIS for D as m(D).
For each DAG there are many possible VISes. A trivial VIS for any DAG is just the set of all of its nodes. In general, we are more interested in MVISes, which are also not necessarily unique for a
DAG. For example, the DAG in Fig. 4 has four MVISes: {1, 3, 5}, {1, 3, 6}, {2, 4, 5}, and {2, 4, 6}.
We now show that ﬁnding a VIS for any DAG D can be decomposed twice: ﬁrst we can create a separate task of ﬁnding a VIS for each of the chain components G of its essential graph E(D), and then for each G we can create a tree-like CDCT and ﬁnd independently a VIS for each of its residuals:
Theorem 1. A single-node intervention set is a VIS for any general DAG D iff it contains VISes for each residual R ∈ R( ˜TG) for all chain components G ∈ CC(E(D)) of its essential graph E(D). 5
An MVIS of D will then contain only the MVISes of each residual of each chain component. An algorithm using this decomposition to compute an MVIS is given in Appendix F. In general, the size of an MVIS of D cannot be calculated from just its essential graph, as shown by the two graphs in
Fig. 5. Instead, we propose a universal lower bound that holds for all DAGs in the same MEC:
Theorem 2. Let D be any DAG. Then m(D) ≥ (cid:80) largest clique in each of the chain components G of the essential graph E(D).
, where ω(G) is the size of the (cid:106) ω(G) 2
G∈CC(E(D)) (cid:107)
We reiterate how this bound is different from previous work. For a ﬁxed MEC [D] with essential graph E, it is easy to construct D∗ ∈ [D] such that m(D∗) ≥ (cid:80) by picking the largest clique in each chain component to be the upstream-most clique. The bound in Theorem 2 gives a much stronger result: any choice of DAG in the MEC requires this many single-node interventions. (cid:106) ω(G) 2
G∈CC(E(D)) (cid:107) 4 A two-phase intervention policy based on DCTs
While in the previous section we started from a known DAG D to construct a CDCT and then proved an universal lower bound on m(D), in this section we focus on intervention design to learn the orientations of an unknown DAG starting from its observational essential graph. Theorem 1 proves that to orient a DAG D, we only need to orient the residuals for each of its essential graph chain components. The deﬁnition of residuals requires the knowledge of a tree-like CDCT for each component, which can be easily derived from the directed clique graph (DCG) (e.g. through
Algorithm 3 in Appendix D). So, we propose a two phase policy, in which the ﬁrst phase uses interventions to identify the DCG of each chain components, while the second phase uses interventions to orient each of the residuals, as described in Algorithm 1. We now focus on describing the ﬁrst phase of the algorithm and start by introducing two types of abstract, higher-level interventions.
Deﬁnition 8. A clique-intervention on a clique C is a series of single-node interventions that sufﬁces to learn the orientation of all edges in ΓD that are incident on C. An edge-intervention on an edge
C1 −TG C2 is a series of single-node interventions that sufﬁces to learn the orientation of C1 −TD C2.
A trivial clique-intervention is intervening on all of C, and a trivial edge-intervention is intervening on all of C1 ∩ C2. The clique- and edge- interventions we use in practice are outlined in Appendix H.
Algorithm 1 DCT POLICY 1: Input: essential graph E(D) 2: for component G in CC(E(D)) do 3: 4: 5: 6: 7: 8: create clique graph ΓG
ΓD = FINDDCG(ΓG) convert ΓD to a CDCT ˜TD for clique C in ˜TD do
Let R = Res ˜TD (C)
Intervene on nodes in V (R) un-til R is fully oriented end for 9: 10: end for 11: return completely oriented D 5: 6: 7: 8: 9: 10: 11:
Algorithm 2 FINDDCG 1: Input: clique graph ΓG 2: let ΓD = ΓG 3: while ΓD has undirected edges do 4: let T be a maximum-weight spanning tree of the undirected component of ΓD let C be a central node of T perform a clique-intervention on C let Pup(C) = IDENTIFYUPSTREAM(C) let S = V (BC:Pup(C)
) while ΓD has unoriented incident to C \ S do
T propagate edges in ΓD perform an edge-intervention on an edge C1 −ΓG
C2 with C1 ∈ C \ S end while 12: 13: end while 14: return ΓD
The ﬁrst phase of our algorithm, described in Algorithm 2, is inspired by the Central Node algorithm (Greenewald et al., 2019). This algorithm operates over a tree, so we will have to use a spanning tree:
Deﬁnition 9. (Greenewald et al., 2019) Given a tree T and a node v ∈ V (T ), we divide T into branches w.r.t. v. For a node w adjacent to v, the branch B(v:w) is the connected component of
T − {v} that contains w. A central node c is a node for which ∀w adjacent to c : |B(c:w)
| ≤ | V (T )
|.
T
T 2 6
While our algorithm works for general graphs, it will help our intuition to ﬁrst assume that ΓG is intersection-incomparable. In this case, there are no arrow-meets in ΓD by Prop. 1, nor in any of the directed clique trees. Thus, after each clique-intervention on a central node C, there will be only one parent clique upstream and the algorithm will orient at least half of the remaining unoriented edges by repeated application of Prop. 1. For the intersection-comparable case, two steps can go wrong. First, after a clique-intervention on C, we may ﬁnd that C has multiple parents in ΓD (i.e. C is at an arrows-meet). We can prove that even in this case, there is always a single “upstream” branch, identiﬁed via the IdentifyUpstream procedure, described in Appendix I, which performs edge-interventions on a subset of the parents. A second step which may go wrong is in the propagation of orientations along the downstream branches, which halts when encountering intersection-incomparable edges. In this case, we simply kickstart further propagation by performing an edge-intervention.
The size of the problem is cut in half after each clique-intervention, so that we use at most (cid:80)
G∈CC(E(D))(cid:100)log2(|C(G)|)(cid:101) clique-interventions, where C(G) is the set of maximal cliques for
G. Furthermore, if ΓG is intersection-incomparable we use no edge-interventions (see Lemma 8 in Appendix J). The second phase of the algorithm then orients the residuals and uses at most (cid:80) (C)| − 1 single-node interventions (see Lemma 9 in Appendix J). (cid:80)
C∈C(G) | Res ˜TG
G∈CC(E(D))
Theorem 3. Assuming ΓG is intersection-incomparable, Algorithm 1 uses at most (3(cid:100)log2 Cmax(cid:101) + 2)m(D) single-node interventions, where Cmax = maxG∈CC(E(D)) |C(G)|.
In the extreme case in which the essential graph is a tree, a single intervention on the root node can orient the tree, so m(D) = 1, and |C| = |V (D)| − 1, so Theorem 3 says that Algorithm 1 uses
O(log(p)) interventions, which is the scaling of the Bayes-optimal policy for the uniform prior as discussed in Greenewald et al. (2019).
Remark on intersection-incomparability. Intersection-incomparable chordal graphs were intro-duced as “uniquely representable chordal graphs” in Kumar & Madhavan (2002). This class was shown to include familiar classes of graphs such as proper interval graphs. While the assumption of intersection-incomparability is necessary for our analysis of the DCT policy, the policy still performs well on intersection-comparable graphs as demonstrated in Section 5. This suggests that the restriction may be an artifact of our analysis, and the result of Theorem 3 may hold more generally. 5 Experimental Results
We evaluate our policy on synthetic graphs of varying size. To evaluate the performance of a policy on a speciﬁc DAG D, relative to m(D), the size of its smallest VIS (MVIS), we adapt the notion of competitive ratio from online algorithms (Borodin et al., 1992; Daniely & Mansour, 2019). We use ιD(π) to denote the expected size of the VIS found by policy π for the DAG D, and deﬁne our evaluation metric as:
Deﬁnition 10. The instance-wise competitive ratio (ic-ratio) of an intervention policy π on D is
R(π, D) = ιD(cid:48) (π) m(D(cid:48)) . The competitive ratio on an MEC [D] is R(π) = maxD(cid:48)∈[D]
ιD(cid:48) (π) m(D(cid:48)) .
The instance-wise competitive ratio of a policy on a DAG D simply measures the number of interventions used by the policy relative to the number of interventions used by the best policy for that DAG, i.e., the policy which guesses that D is the true DAG and uses exactly a MVIS of D to verify this guess. Thus, a lower ic-ratio is better, and an ic-ratio of 1 is the best possible. In order to compute the ic-ratio on D, we must compute m(D), the size of a MVIS for D. In our experiments, we use our DCT characterization of VIS’s from Theorem 1 to decompose the DAG into its residuals, each of whose MVIS’s can be computed efﬁciently. We describe this procedure in Appendix F.
Smaller graphs. For our evaluation on smaller graphs, we generate random connected moral DAGs using the following procedure, which is a modiﬁcation of Erdös-Rényi sampling that guarantees that the graph is connected. We ﬁrst generate a random ordering σ over vertices. Then, for the n-th node in the order, we set its indegree to be Xn = max(1, Bin(n − 1, ρ)), and sample Xn parents uniformly from the nodes earlier in the ordering. Finally, we chordalize the graph by running the elimination algorithm (Koller & Friedman, 2009) with elimination ordering equal to the reverse of σ.
We compare the OptSingle policy (Hauser & Bühlmann, 2014), the Minmax and Entropy strategy of He & Geng (2008), called MinmaxMEC and MinmaxEntropy, respectively, and the coloring-based strategy of Shanmugam et al. (2015), called Coloring. We also introduce a baseline that picks 7
(a) Average ic-Ratio (small graphs) (b) Average Time (small graphs) (c) Average ic-Ratio (larger graphs) (d) Maximum ic-Ratio (larger graphs)
Figure 6: Comparison of intervention policies over 100 synthetic DAGs. randomly among non-dominated2 nodes in the I-essential graph, called the non-dominated random (ND-Random) strategy. As the name suggests, dominated nodes are easily proven to be non-optimal interventions, so ND-Random is a more fair baseline than simply picking randomly amongst nodes.
In Fig. 6a and Fig. 6b, we show the average ic-ratio and the average run-time for each of the algorithms.
In terms of average ic-ratio, all algorithms aside from ND-Random perform comparably, using on average 1.4-1.7x more interventions than the smallest MVIS. However, the computation time grows quite quickly for GreedyMEC, GreedyEntropy, and OptSingle. This is because, when scoring a node as a potential intervention target, each of these algorithms iterates over all possible parent sets of the node. Moreover, the GreedyMEC and GreedyEntropy policies then compute the sizes of the resulting interventional MECs, which can grow superexponentially in the number of nodes (Gillispie
& Perlman, 2013). In Appendix K, we show that in the same setting, OptSingle takes >10 seconds per graph for just 25 nodes, whereas Coloring, DCT, and Random remain under .1 seconds per graph.
Larger graphs. For our evaluation on large tree-like graphs, we create random moral DAGs of n = 100, . . . , 300 nodes using the following procedure. We generate a complete directed 4-ary tree on n nodes. Then, we sample an integer R ∼ U (2, 5) and add R edges to the tree. Finally, we ﬁnd a topological order of the graph by DFS and triangulate the graph using that order. This ensures that the graph retains a nearly tree-like structure, making m(D) small compared to the overall number of nodes. In Fig. 6c and Fig. 6d, we show the average and maximum competitive ratio (computation time is given in Appendix K). For the average graph, our DCT policy and the Coloring policy use only 2-3 times as many interventions as the theoretical lower bound. Moreover, the worst competitive ratio experienced by the DCT algorithm is signiﬁcantly smaller than the worst ratio experienced by the Coloring policy, which suggests that our policy is more adaptive to the underlying difﬁculty of the identiﬁcation problem. 2A node is dominated if all incident edges are directed, or if it has only a single incident edge to a neighbor with more than one incident undirected edges 8
6