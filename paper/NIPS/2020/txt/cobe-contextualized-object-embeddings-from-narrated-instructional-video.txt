Abstract
Many objects in the real world undergo dramatic variations in visual appearance.
For example, a tomato may be red or green, sliced or chopped, fresh or fried, liquid or solid. Training a single detector to accurately recognize tomatoes in all these different states is challenging. On the other hand, contextual cues (e.g., the presence of a knife, a cutting board, a strainer or a pan) are often strongly indicative of how the object appears in the scene. Recognizing such contextual cues is useful not only to improve the accuracy of object detection or to determine the state of the object, but also to understand its functional properties and to infer ongoing or upcoming human-object interactions. A fully-supervised approach to recognizing object states and their contexts in the real-world is unfortunately marred by the long-tailed, open-ended distribution of the data, which would effectively require massive amounts of annotations to capture the appearance of objects in all their different forms. Instead of relying on manually-labeled data for this task, we propose a new framework for learning Contextualized OBject Embeddings (COBE) from automatically-transcribed narrations of instructional videos. We leverage the semantic and compositional structure of language by training a visual detector to predict a contextualized word embedding of the object and its associated narration. This enables the learning of an object representation where concepts relate according to a semantic language metric. Our experiments show that our detector learns to predict a rich variety of contextual object information, and that it is highly effective in the settings of few-shot and zero-shot learning. 1

Introduction
In recent years, the ﬁeld of object detection has witnessed dramatic progress in the domain of both images [1, 2, 3, 4] and videos [5, 6, 7, 8, 9]. To a large extent these advances have been driven by the introduction of increasingly bigger labeled datasets [10, 11, 12, 13], which have enabled the training of progressively more complex and deeper models. However, even the largest datasets in this ﬁeld [10, 11, 12] still deﬁne objects at a very coarse level, with label spaces expressed in terms of nouns, e.g., tomato, ﬁsh, plant or ﬂower. Such noun-centric ontologies fail to represent the dramatic variations in appearance caused by changes in object “states” for many of these classes. For example, a tomato may be fresh or fried, large or small, red or green. It may also frequently appear in conjunction with other objects such as a knife, a cutting board, or a cucumber. Furthermore, it may undergo drastic appearance changes caused by human actions (e.g., slicing, chopping, or squeezing).
The goal of this work is to design a model that can recognize a rich variety of contextual object cues and states in the visual world (see Figure 1 for a few illustrative examples). A detection system capable to do so is useful not only for understanding the object functional properties, but also for inferring present or future human-object interactions. Unfortunately, due to the long-tailed, open-ended distribution of the data, implementing a fully-supervised approach to this problem is challenging as it requires collecting large amounts of annotations capturing the visual appearance of objects in all their different forms. Instead of relying on manually-labeled data, we propose a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Many objects in the real-world undergo state changes that dramatically alter their appearance, such as in the case of the onions illustrated in this ﬁgure. In this work, we aim at training a detector that recognizes the appearances, states and contexts of objects using narrated instructional video. novel paradigm that learns Contextualized OBject Embeddings (COBE) from instructional videos with automatically-transcribed narrations [14]. The underlying assumption is that, due to the tutorial properties of instructional video, the accompanying narration often provides a rich and detailed description of an object in the scene in terms of not only its name (noun) but also its appearance (adjective or verb), and contextual objects (other nouns) that tend to occur with it. We refer collectively to these ancillary narrated properties as the “contextualized object embedding” in reference to the contextualized word representation [15] which we use to automatically annotate objects in a video.
Speciﬁcally, we propose to train a visual detector to map each object instance appearing in a video frame into its contextualized word representation obtained from the contextual narration. Because the word representation is contextualized, the word embedding associated to the object will capture not only the category of the object but also the words that were used to describe it, e.g., its size, color, and function, other objects in the vicinity, and/or actions being applied to it. Consider for example the frame illustrated in Figure 2. Its accompanying narration is “... break an egg into a bowl...” The contextualized word embedding of the egg represented in the frame will be a vector that encodes primarily the word “egg” but also the contextual action of “breaking” and the contextual object “bowl.” By training our model to predict the contextualized embedding of an object, we force it to recognize ﬁne-grained contextual information beyond the coarse categorical labels. In addition to the ability of representing object concepts at a ﬁner grain, such a representation is also advantageous because it leverages the compositionality and the semantic structure of the language, which enable generalization of states and contexts across categories.
We train COBE on the instructional videos of HowTo100M dataset [14], and then test it on the evaluation sets of HowTo100M, EPIC-Kitchens [16], and YouCook2 [17] datasets. Our experiments show that on all three of these datasets, COBE outperforms the state-of-the-art Faster R-CNN detector. Furthermore, we demonstrate that despite a substantial semantic and appearance gap between HowTo100M and EPIC-Kitchens datasets, COBE successfully generalizes to this setting where it outperforms several highly competitive baselines trained on large-scale manually-labeled data. Lastly, our additional experiments in the context of zero-shot and few-shot learning indicate that
COBE can generalize concepts even to novel classes, not seen during training, or from few examples. 2