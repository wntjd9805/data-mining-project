Abstract
We introduce a new category of generative autoencoders called automodulators.
These networks can faithfully reproduce individual real-world input images like regular autoencoders, but also generate a fused sample from an arbitrary combina-tion of several such images, allowing instantaneous ‘style-mixing’ and other new applications. An automodulator decouples the data ﬂow of decoder operations from statistical properties thereof and uses the latent vector to modulate the former by the latter, with a principled approach for mutual disentanglement of decoder layers.
Prior work has explored similar decoder architecture with GANs, but their focus has been on random sampling. A corresponding autoencoder could operate on real input images. For the ﬁrst time, we show how to train such a general-purpose model with sharp outputs in high resolution, using novel training techniques, demonstrated on four image data sets. Besides style-mixing, we show state-of-the-art results in autoencoder comparison, and visual image quality nearly indistinguishable from state-of-the-art GANs. We expect the automodulator variants to become a useful building block for image applications and other data domains. 1

Introduction
This paper introduces a new category of generative autoencoders for learning representations of image data sets, capable of not only reconstructing real-world input images, but also of arbitrarily combining their latent codes to generate fused images. Fig. 1 illustrates the rationale: The same model can encode input images (far-left), mix their features (middle), generate novel ones (middle), and sample new variants of an image (conditional sampling, far-right). Without discriminator networks, training such an autoencoder for sharp high resolution images is challenging. For the ﬁrst time, we show a way to achieve this.
Recently, impressive results have been achieved in random image generation (e.g., by GANs [5, 14, 25]). However, in order to manipulate a real input image, an ‘encoder’ must ﬁrst infer the correct representation of it. This means simultaneously requiring sufﬁcient output image quality and the ability for reconstruction and feature extraction, which then allow semantic editing. Deep generative autoencoders provide a principled approach for this. Building on the PIONEER autoencoder [18], we proceed to show that modulation of decoder layers by leveraging adaptive instance normalization (AdaIn, [12, 23, 44]) further improves these capabilities. It also yields representations that are less entangled, a property here broadly deﬁned as something that allows for ﬁne and independent control of one semantic (image) sample attribute at a time. Here, the inductive bias is to assume each such attribute to only affect certain scales, allowing disentanglement [33]. Unlike [23], previous
GAN-based works on AdaIn [6, 25] have no built-in encoder for new input images.
In a typical autoencoder, input images are encoded into a latent space, and the information of the latent variables is then passed through successive layers of decoding until a reconstructed image has been formed. In our model, the latent vector independently modulates the statistics of each layer of the decoder so that the output of layer n is no longer solely determined by the input from layer n − 1. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Input
Reconstruction
Modulated output
Real input images is modulated by is modulated by
Modulated output
‘Coarse’ features modulated
Input
Reconstruction
Random sample
Conditional samples given ‘coarse’ and ‘ﬁne’ features from input
Figure 1: Illustration of some automodulator capabilities. The model can directly encode real (unseen) input images (left). Inputs can be mixed by modulating one with another or with a randomly drawn sample, at desired scales (center); e.g., ‘coarse’ scales affect pose and gender etc. Finally, taking random modulations for certain scales produces novel samples conditioned on the input image (right).
A key idea in our work is to reduce the mutual entanglement of decoder layers. For robustness, the samples once encoded and reconstructed by the autoencoder could be re-introduced to the encoder, repeating the process, and we could require consistency between the passes. In comparison to stochastic models such as VAEs [28, 40], our deterministic model is better suited to take advantage of this. We can take the latent codes of two separate samples, drive certain layers (scales) of the decoder with one and the rest with the other, and then separately measure whether the information contained in each latent is conserved during the full decode–encode cycle. This enforces disentanglement of layer-speciﬁc properties, because we can ensure that the latent code introduced to affect only certain scales on the 1st pass should not affect the other layers on the 2nd pass, either.
In comparison to implicit (GAN) methods, regular image autoencoders such as VAEs tend to have poor output image quality. In contrast, our model simultaneously balances sharp image outputs with the capability to encode and arbitrarily mix latent representations of real input images.
The contributions of this paper are as follows. (i) We provide techniques for stable fully unsupervised training of a high-resolution automodulator, a new form of an autoencoder with powerful properties not found in regular autoencoders, including scale-speciﬁc style transfer [13]. In contrast to architec-turally similar ‘style’-based GANs, the automodulator can directly encode and manipulate new inputs. (ii) We shift the way of thinking about autoencoders by presenting a novel disentanglement loss that further helps to learn more disentangled representations than regular autoencoders, a principled approach for incorporating scale-speciﬁc prior information in training, and a clean scale-speciﬁc approach to attribute modiﬁcation. (iii) We demonstrate promising qualitative and quantitative performance and applications on FFHQ, CELEBA-HQ, and LSUN Bedrooms and Cars data sets. 2