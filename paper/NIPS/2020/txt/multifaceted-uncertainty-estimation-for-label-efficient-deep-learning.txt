Abstract
We present a novel multi-source uncertainty prediction approach that enables deep learning (DL) models to be actively trained with much less labeled data. By leveraging the second-order uncertainty representation provided by subjective logic (SL), we conduct evidence-based theoretical analysis and formally decompose the predicted entropy over multiple classes into two distinct sources of uncertainty: vacuity and dissonance, caused by lack of evidence and conﬂict of strong evidence, respectively. The evidence based entropy decomposition provides deeper insights on the nature of uncertainty, which can help effectively explore a large and high-dimensional unlabeled data space. We develop a novel loss function that augments
DL based evidence prediction with uncertainty anchor sample identiﬁcation. The accurately estimated multiple sources of uncertainty are systematically integrated and dynamically balanced using a data sampling function for label-efﬁcient active deep learning (ADL). Experiments conducted over both synthetic and real data and comparison with competitive AL methods demonstrate the effectiveness of the proposed ADL model. 1

Introduction
Deep learning (DL) models establish dominating status among other types of supervised learning models by achieving the state-of-the-art performance in various application domains. However, such an advantage only emerges when a huge amount of labeled training data is available. This limitation slows down the pace of DL, especially when being applied to knowledge-rich domains, such as medicine and biology, where large-scale labeled samples are too expensive to obtain from well-trained experts. Meanwhile, active learning (AL) has demonstrated great success by showing that for many supervised models, training samples are not equally important in terms of improving the model performance [1]. As a result, a carefully selected smaller training set can achieve a model equally well or even better than a randomly selected large training set.
An interesting question arises, which is whether DL models can be actively trained using much less labeled data. Recent efforts show promising results in this direction through Bayesian modeling [2] and batch model sampling [3]. However, as DL models are commonly applied to high dimensional data such as images and videos, a fundamental challenge still remains, which is how to most effectively explore the exponentially growing feature space to select the most useful data samples for active model training. Existing AL models usually leverage the model provided information, such as estimated decision boundaries or predicted entropy for data sampling. However, the deep structure and the large number of parameters of DL models make model overﬁtting almost inevitable especially in the early stage of AL when only very limited training data is available. As a result, the model may provide misleading information that makes data sampling from a high-dimensional search space even more difﬁcult. Besides a high dimensionality, complex data may contain a large number of classes
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b) (c) (d)
Figure 1: A dataset consists of three mixtures of Gaussian’s (shown in red, blue, and yellow), each of which has a large and small clusters of data samples. (a) Softmax predicted entropy; (b) ADL predicted entropy; (c) ADL predicted vacuity; (d) ADL predicted dissonance. and data samples from certain classes may be completely missing. Such situations are quite common for domains, such as scientiﬁc discovery (e.g., gene function prediction) and anomaly detection. AL models should be able to effectively discover these out of distribution (OOD) samples for labeling in order to achieve an overall good prediction performance.
Uncertainty sampling has been one of the most commonly used pool-based AL models. In particular, a model chooses the data sample that it is least certain about. Thus, once the sample is labeled, model uncertainty can be signiﬁcantly reduced. As an information-theoretic measure, entropy provides a general criterion for uncertainty sampling. Some commonly used sampling methods, including least conﬁdent and margin based strategies, are equivalent to entropy-based sampling in binary classiﬁcation [1]. It is also straightforward to generalize to multi-class problems.
A key challenge of entropy-based sampling for AL is that the predicted entropy may be highly inaccurate, especially in the early state of the AL. Such an issue may become more severe when training a neural network (NN)/DL active learner due to model overﬁtting as described above.
Figure 1(a) shows the predicted entropy by an NN active learner trained using nine labeled data samples, which are in black color and evenly distributed in three classes. The standard softmax layer is used in the output layer to generate class probabilities over three classes, each of which is a mixture of two Gaussian’s. It turns out that all the data samples in the three small clusters located in the top left, top right, and bottom center, are wrongly predicted with high conﬁdence, as indicated by the low entropy. As a result, data samples from these three clusters are less likely to be selected for labeling.
In contrast, the data samples that are close to the center of the three major clusters are more likely to be selected. However, labeling these samples will have the effect of ﬁne-tuning a wrongly predicted decision boundary, leading to a much higher (but less effective) labeling cost.
Figure 1(b) shows the result from the proposed active deep learning (ADL) model. While the samples from the small clusters are still wrongly predicted due to lack of training data, they are predicted with a much lower conﬁdence as indicated by the high entropy. However, even with a more accurately predicted entropy, the active learner may still sample from the center of the three major clusters as it is also assigned a high entropy along with the areas that cover the three smaller clusters. By leveraging the evidenced based probabilistic constructs developed under the subjective logic (SL) framework [4], we formally decompose entropy into two distinct sources of uncertainty: vacuity and dissonance, which are caused by lack of evidence and conﬂict of strong evidence, respectively. When putting the vacuity and dissonance as shown in Figures 1(c) and (d) together, it is interesting to see that we recover the entropy as shown in Figure 1(b), which empirically veriﬁes our theoretical results.
Entropy decomposition provides further insights on the sources on uncertainty, which is instrumental to guide the data sampling process. Intuitively, given the dataset in Figure 1, an effective sampling strategy will ﬁrst choose samples from the three small clusters according to vacuity in the early stage of AL to properly establish the shape of the decision boundary. It can then ﬁne-tune the decision boundary by sampling according to dissonance. Such an uncertainty-aware sampling strategy will be critical for a high-dimensional space with multiple competing classes where data samples are scarcely distributed and the decision boundary becomes more complicated.
Our major contribution is threefold: (1) theoretical decomposition of entropy into evidence-based second-order uncertainties, including belief vacuity and belief dissonance; (2) a multi-source uncer-tainty prediction model that accurately quantiﬁes different sources of uncertainty; (3) an active deep learning model that systematically integrates different types of uncertainty for effective data sampling in a high-dimensional space. Extensive experiments are conducted over both synthetic and real-world data to demonstrate the effectiveness of the proposed ADL model. 2
2