Abstract
Wasserstein gradient ﬂows are continuous time dynamics that deﬁne curves of steepest descent to minimize an objective function over the space of probability measures (i.e., the Wasserstein space). This objective is typically a divergence w.r.t. a ﬁxed target distribution. In recent years, these continuous time dynamics have been used to study the convergence of machine learning algorithms aiming at approximating a probability distribution. However, the discrete-time behavior of these algorithms might differ from the continuous time dynamics. Besides, although discretized gradient ﬂows have been proposed in the literature, little is known about their minimization power. In this work, we propose a Forward
Backward (FB) discretization scheme that can tackle the case where the objective function is the sum of a smooth and a nonsmooth geodesically convex terms. Using techniques from convex optimization and optimal transport, we analyze the FB scheme as a minimization algorithm on the Wasserstein space. More precisely, we show under mild assumptions that the FB scheme has convergence guarantees similar to the proximal gradient algorithm in Euclidean spaces. 1

Introduction
The task of transporting an initial distribution µ0 to a target distribution µ(cid:63) is common in machine learning. This task can be reformulated as the minimization of a cost functional deﬁned over the set of probability distributions. Wasserstein gradient ﬂows [2] are suitable continuous time dynamics to minimize such cost functionals. These ﬂows have found applications in various ﬁelds of machine learning such as reinforcement learning [32,40], sampling [5,12,16,38] and neural networks optimization [14, 26]. Indeed, Wasserstein gradient ﬂows can be seen as the continuous limit of several discrete time machine learning algorithms. The analysis of continuous time dynamics is often easier than the analysis of their discrete time counterparts. Therefore, many works focus solely on continuous time analyses of machine learning algorithms such as variants of gradient descent
[7, 14, 15, 26, 34, 37]. Besides, although discretized Wasserstein gradient ﬂows have been proposed in the literature [2, 8, 11, 19, 24, 38], most of them have not been studied as minimization algorithms.
In this paper, we focus on the resolution, by a discrete time algorithm, of a minimization problem deﬁned on the set P2(X ) of probability measures µ over X = Rd such that (cid:82) (cid:107)x(cid:107)2dµ(x) < ∞.
More precisely, µ(cid:63) is deﬁned as a solution to min
µ∈P2(X )
G(µ) := EF (µ) + H(µ), (1) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
where EF is a potential energy EF (µ) = (cid:82) F (x)dµ(x) tied to a smooth convex function F : Rd → R, and H is a nonsmooth term convex along the generalized geodesics deﬁned by the Wasserstein distance. The potential EF plays the role of a data ﬁtting term whereas H can be seen as a regularizer.
Motivation for studying the template problem (1). Many free energy minimization problems can be cast as Problem (1), see [1, Proposition 7.7] or more generally [2, Section 9]. For instance,
H can be an internal energy [2, Example 9.3.6]. In particular, if H is the negative entropy, then G boils down to the Kullback Leibler divergence (up to an additive constant) w.r.t. the Gibbs measure
µ(cid:63) ∝ exp(−F ). This remark has been used by several authors to study sampling tasks as minimizing
Problem (1) [5, 12, 16, 20, 38]. Another example where H is an internal energy is the case where H is a higher order entropy. In this case, µ(cid:63) follows a Barenblatt proﬁle. Moreover, in the context of optimization of inﬁnitely wide two layers neural networks [14,26], µ(cid:63) denotes the optimal distribution over the parameters of the network. In this setting, F (x) = −2 (cid:82) k(x, y)dµ(cid:63)(y) is non convex and
H(µ) = (cid:82) k(x, y)dµ(x)dµ(y) is an interaction energy [2, Example 9.3.4], with k depending on the activation functions of the network. Moreover, G boils down to a Maximum Mean Discrepancy w.r.t.
µ(cid:63) [3] under a well-posedness condition. Alternatively, F can be a regularizer of the distribution on the parameters of the network (see Appendix).