Abstract
This paper introduces MDP homomorphic networks for deep reinforcement learn-ing. MDP homomorphic networks are neural networks that are equivariant under symmetries in the joint state-action space of an MDP. Current approaches to deep reinforcement learning do not usually exploit knowledge about such structure. By building this prior knowledge into policy and value networks using an equivariance constraint, we can reduce the size of the solution space. We speciﬁcally focus on group-structured symmetries (invertible transformations). Additionally, we introduce an easy method for constructing equivariant network layers numerically, so the system designer need not solve the constraints by hand, as is typically done.
We construct MDP homomorphic MLPs and CNNs that are equivariant under either a group of reﬂections or rotations. We show that such networks converge faster than unstructured baselines on CartPole, a grid world and Pong. 1

Introduction
This paper considers learning decision-making systems that exploit symmetries in the structure of the world. Deep reinforcement learning (DRL) is concerned with learning neural function approximators for decision making strategies. While DRL algorithms have been shown to solve complex, high-dimensional problems [35, 34, 26, 25], they are often used in problems with large state-action spaces, and thus require many samples before convergence. Many tasks exhibit symmetries, easily recognized by a designer of a reinforcement learning system. Consider the classic control task of balancing a pole on a cart. Balancing a pole that falls to the right requires an equivalent, but mirrored, strategy to one that falls to the left. See Figure 1. In this paper, we exploit knowledge of such symmetries in the state-action space of Markov decision processes (MDPs) to reduce the size of the solution space.
We use the notion of MDP homomorphisms [32, 30] to formalize these symmetries. Intuitively, an
MDP homomorphism is a map between MDPs, preserving the essential structure of the original
MDP, while removing redundancies in the problem description, i.e., equivalent state-action pairs. The removal of these redundancies results in a smaller state-action space, upon which we may more easily build a policy. While earlier work has been concerned with discovering an MDP homomorphism for a given MDP [32, 30, 27, 31, 6, 39], we are instead concerned with how to construct deep policies, satisfying the MDP homomorphism. We call these models MDP homomorphic networks. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
MDP homomorphic networks use experience from one state-action pair to improve the policy for all ‘equivalent’ pairs. See Section 2.1 for a deﬁnition. They do this by ty-ing the weights for two states if they are equivalent under a transformation chosen by the designer, such as s and L[s] in Figure 1. Such weight-tying follows a similar principle to the use of convolutional networks [18], which are equiv-ariant to translations of the input [11]. In particular, when equivalent state-action pairs can be related by an invert-ible transformation, which we refer to as group-structured, we show that the policy network belongs to the class of group-equivariant neural networks [11, 46].Equivariant neural networks are a class of neural network, which have built-in symmetries [11, 12, 46, 43, 41]. They are a generalization of convolutional neural networks—which exhibit translation symmetry—to transformation groups (group-structured equivariance) and transformation semi-groups [47] (semigroup-structured equivariance). They have been shown to reduce sample complexity for classi-ﬁcation tasks [46, 44] and also to be universal approxima-tors of symmetric functions1 [48]. We borrow from the literature on group equivariant networks to design policies that tie weights for state-action pairs given their equiv-alence classes, with the goal of reducing the number of samples needed to ﬁnd good policies. Furthermore, we can use the MDP homomorphism property to design not just policy networks, but also value networks and even environment models. MDP homomorphic networks are agnostic to the type of model-free
DRL algorithm, as long as an appropriate transformation on the output is given. In this paper we focus on equivariant policy and invariant value networks. See Figure 1 for an example policy.
Figure 1: Example state-action space symmetry. Pairs (s, ←) and (L[s], →) (and by extension (s, →) and (L[s], ←)) are symmetric under a horizontal ﬂip.
Constraining the set of policies to those where π(s, ←) = π(L[s], →) reduces the size of the solution space.
An additional contribution of this paper is a novel numerical way of ﬁnding equivariant layers for arbitrary transformation groups. The design of equivariant networks imposes a system of linear constraint equations on the linear/convolutional layers [12, 11, 46, 43]. Solving these equations has typically been done analytically by hand, which is a time-consuming and intricate process, barring rapid prototyping. Rather than requiring analytical derivation, our method only requires that the system designer specify input and output transformation groups of the form {state transformation, policy transformation}. We provide Pytorch [29] implementations of our equivariant network layers, and implementations of the transformations used in this paper. We also experimentally demonstrate that exploiting equivalences in MDPs leads to faster learning of policies for DRL.
Our contributions are two-fold:
• We draw a connection between MDP homomorphisms and group equivariant networks, proposing MDP homomorphic networks to exploit symmetries in decision-making problems;
• We introduce a numerical algorithm for the automated construction of equivariant layers. 2