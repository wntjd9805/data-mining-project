Abstract
Primate vision depends on recurrent processing for reliable perception [1–3]. A growing body of literature also suggests that recurrent connections improve the learning efﬁciency and generalization of vision models on classic computer vision challenges. Why then, are current large-scale challenges dominated by feedforward networks? We posit that the effectiveness of recurrent vision models is bottlenecked by the standard algorithm used for training them, “back-propagation through time” (BPTT), which has O(N ) memory-complexity for training an N step model. Thus, recurrent vision model design is bounded by memory constraints, forcing a choice between rivaling the enormous capacity of leading feedforward models or trying to compensate for this deﬁcit through granular and complex dynamics. Here, we de-velop a new learning algorithm, “contractor recurrent back-propagation” (C-RBP), which alleviates these issues by achieving constant O(1) memory-complexity with steps of recurrent processing. We demonstrate that recurrent vision models trained with C-RBP can detect long-range spatial dependencies in a synthetic contour tracing task that BPTT-trained models cannot. We further show that recurrent vision models trained with C-RBP to solve the large-scale Panoptic Segmentation
MS-COCO challenge outperform the leading feedforward approach, with fewer free parameters. C-RBP is a general-purpose learning algorithm for any application that can beneﬁt from expansive recurrent dynamics. Code and data are available at https://github.com/c-rbp. 1

Introduction
Ullman (1984) famously theorized that humans reason about the visual world by composing sequences of elemental computations into “visual routines” [2]. It has been found that many of these visual routines, from perceptual grouping [4] to object categorization [5], depend on local and long-range recurrent circuits of the visual cortex [1, 3, 6]. Convolutional neural networks (CNNs) with recurrent connections – recurrent CNNs – also seem to learn visual routines that standard feedforward CNNs do not [7–14]. For example, consider the Pathﬁnder challenge in Fig. 1a, which asks observers to trace the contour extending from the white dot. Although Pathﬁnder is visually simple, clutter and variations in path shape make it difﬁcult for feedforward CNNs to solve, even very deep residual networks [7,8]. By contrast, a one-layer recurrent CNN can learn to solve Pathﬁnder by incrementally grouping paths from one end to the other, reminiscent of Gestalt-like visual routines used by human observers [7, 8, 15]. Others have found that the visual routines learned by recurrent CNNs on small computer vision datasets lead to better sample efﬁciency and out-of-distribution generalization than feedforward CNNs [6, 16, 17]. There is also evidence that primate visual decisions and neural
*These authors contributed equally to this work. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
responses elicited by natural images are best explained by recurrent CNNs [17–23]. Nevertheless, the great promise of recurrent CNNs has yet to translate into improvements on large-scale computer vision challenges like MS-COCO [24], which are dominated by feedforward CNNs.
A well known limitation of recurrent CNNs is a memory bottleneck imposed by the standard learning algorithm, “back-propagation through time” (BPTT; [25]). The memory requirement of BPTT-trained models scales linearly with steps of processing, since optimization involves propagating error through the full latent trajectory. This makes it difﬁcult to develop recurrent CNNs that can rival the massive capacity of leading feedforward CNNs, which is critical for performance on challenges [26], while also simulating enough steps of processing to learn robust human-like visual routines.
Contributions. We develop a solution to the recurrent CNN memory bottleneck introduced by
BPTT. Our work is inspired by recent successful efforts in memory-efﬁcient approximations to
BPTT for sequence modeling [27, 28]. Of particular interest is recurrent back-propagation (RBP), which exploits the stability of convergent dynamical systems to achieve constant memory complexity w.r.t. steps of processing [27, 29, 30]. This approach depends on models with stable dynamics that converge to a task-optimized steady state. However, we ﬁnd that leading recurrent CNNs violate this assumption and “forget” task information as they approach steady state. While this pathology can be mitigated with hyperparameters that guarantee stability, these choices hurt model performance, or “expressivity”. Thus, we make the observation that recurrent CNNs face a fundamental trade-off between stable dynamics and model expressivity that must be addressed before they can adopt efﬁcient learning algorithms and compete on large-scale computer vision challenges.
• We derive a constraint for training recurrent CNNs to become both stable and expressive. We refer to this as the Lipschitz-Constant Penalty (LCP).
• We combine LCP with RBP to introduce “contractor-RBP” (C-RBP), a learning algorithm for recurrent CNNs with constant memory complexity w.r.t. steps of processing.
• Recurrent CNNs trained with C-RBP learn difﬁcult versions of Pathﬁnder that BPTT-trained models cannot due to memory constraints, generalize better to out-of-distribution exemplars, and need a fraction of the parameters of BPTT-trained models to reach high performance.
• C-RBP alleviates the memory bottleneck faced by recurrent CNNs on large-scale computer vision challenges. Our C-RBP trained recurrent model outperforms the leading feedforward approach to the MS-COCO Panoptic Segmentation challenge with nearly 800K fewer parameters, and without exceeding the memory capacity of a standard NVIDIA Titan X GPU. 2