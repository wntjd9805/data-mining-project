Abstract
The accuracy of deep convolutional neural networks (CNNs) generally improves when fueled with high resolution images. However, this often comes at a high computational cost and high memory footprint.
Inspired by the fact that not all regions in an image are task-relevant, we propose a novel framework that performs efﬁcient image classiﬁcation by processing a sequence of relatively small inputs, which are strategically selected from the original image with reinforcement learning. Such a dynamic decision process naturally facilitates adaptive inference at test time, i.e., it can be terminated once the model is sufﬁciently conﬁdent about its prediction and thus avoids further redundant computation. Notably, our framework is general and ﬂexible as it is compatible with most of the state-of-the-art light-weighted CNNs (such as MobileNets, EfﬁcientNets and RegNets), which can be conveniently deployed as the backbone feature extractor. Experiments on
ImageNet show that our method consistently improves the computational efﬁciency of a wide variety of deep models. For example, it further reduces the average latency of the highly efﬁcient MobileNet-V3 on an iPhone XS Max by 20% without sacriﬁcing accuracy. Code and pre-trained models are available at https:
//github.com/blackfeather-wang/GFNet-Pytorch. 1

Introduction
Modern convolutional networks (CNNs) are shown to beneﬁt from training and inferring on high-resolution images. For example, state-of-the-art CNNs have achieved super-human-level performance 320 images [47, 14, 61, 18, 22]. on the competitive ILSVRC [9] competition with 224
Recent works [50, 23] scale up the image resolution to 480 480 or even larger for higher accuracy.
However, large images usually come at a high computational cost and high memory footprint, both of which grow quadratically with respect to the image height (or width) [38]. In real-world applications like content-based image search [54] or autonomous vehicles [3], computation usually translates into latency and power consumption, which should be minimized for both safety and economical reasons
[17, 21, 43]. 224 or 320
×
×
×
In this paper, we seek to reduce the computational cost introduced by high-resolution inputs in image classiﬁcation tasks. Our motivation is that considerable spatial redundancy exists in the process of recognizing an image. In fact, CNNs are shown to be able to produce correct classiﬁcation results with only a few class-discriminative patches, such as the head of a dog or the wings of a bird [38, 12, 8]. These regions are typically smaller than the whole image, and thus require much less computational resources. Therefore, if we can dynamically identify the “class-discriminative” regions of each individual image, and perform efﬁcient inference only on these small inputs, then the
∗Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
spatial redundancy can be signiﬁcantly reduced without sacriﬁcing accuracy. To implement this idea, we need to address two challenges: 1) how to efﬁciently identify class-discriminative regions; and 2) how to adaptively allocate computation to each individual image, given that the number/size of discriminative regions may differ across different inputs.
×
×
In this paper, we present a two-stage framework, named glance and focus, to address the aforementioned issues.
Speciﬁcally, the region selection operation is formulated as a sequential decision process, where at each step our model processes a relatively small input, producing a clas-siﬁcation prediction with a conﬁdence value as well as a region proposal for the next step. Each step can be done efﬁciently due to the reduced image size. For example, 96 image patch the computational cost of inferring a 96 is only 18% of that of processing the original 224 224 input. The whole sequential process starts with processing the full image in a down-sampled scale (e.g., 96 96), serving as the initial step. We call it the glance step, at which the model produces a quick prediction of the input image using the global information. In practice, we ﬁnd that a large portion of images with discriminative features can already be correctly classiﬁed with high conﬁdence at the glance step, which is inline with the observation in
[64]. When the glance step fails to produce sufﬁciently high conﬁdence about its prediction, it will output a region proposal of the most discriminative region for the subse-quent step to process. As the proposed region is usually a small patch of the original image with full resolution, we call these subsequent steps the focus stage. This stage proceeds progressively with iteratively localizing and processing the class-discriminative image regions, facilitating early termination in an adaptive manner, i.e., the decision process can be interrupted dynamically conditioned on each input image. As shown in Figure 1, our method allocates computation unevenly across different images at test time, leading to a signiﬁcant improvement of the overall efﬁciency. We refer to our method as
Glance and Focus Network (GFNet).
Figure 1: for GFNet.
Examples
“FLOPs” refers to the proportion of the computation required by GFNet (with 96 96 image patches) versus process-ing the entire 224 224 image.
×
×
×
It is worth noting that the proposed GFNet is designed as a general framework, where the classiﬁer and the region proposal network are treated as two independent modules. Therefore, most of the state-of-the-art light-weighted CNNs, such as MobileNets [17, 43, 16], CondenseNet [21], ShufﬂeNets
[66, 36] and EfﬁcientNet [50], can be deployed for higher efﬁciency. This differentiates our method from early recurrent attention methods [38] which adopt pure recurrent models. In addition, we focus on improving the computational efﬁciency under the adaptive inference setting, while most existing works aim to improve accuracy with ﬁxed sequence length.
Besides its high computational efﬁciency, the proposed GFNet is appealing in several other aspects.
For example, the memory consumption can be signiﬁcantly reduced, and it is independent of the original image resolution as long as we ﬁx the size of the region. Moreover, the computational cost of GFNet can be adjusted online without additional training (by simply adjusting the termination criterion). This enables GFNet to make full use of all available computational resources ﬂexibly or achieve the required performance with minimal power consumption – a practical requirement of many real-world applications such as search engines and mobile apps.
We evaluate the performance of GFNet on ImageNet [9] with various efﬁcient CNNs (e.g., MobileNet-V3 [16], RegNet [40], EfﬁcientNet [50], etc.) in the budgeted batch classiﬁcation setting [20], where the test set comes with a given computational budget, and the anytime prediction setting [13, 20], where the network can be forced to output a prediction at any given point in time. We also benchmark the average latency of GFNet on an iPhone XS Max. Experimental results show that GFNet effectively improves the efﬁciency of state-of-the-art networks both theoretically and empirically. For example, and when the MobileNets-V3 and ResNets are used as the backbone network, GFNet has up to 1.4 less Multiply-Add operations compared to the original models when achieving the same level 3
× of accuracy, respectively. Notably, the actual speedup on an iPhone XS Max (measured by average latency) is 1.3
, respectively. and 2.9
×
×
× 2
2