Abstract
We present an optimal transport framework for learning topics from textual data. While the celebrated Latent Dirichlet allocation (LDA) topic model and its variants have been applied to many disciplines, they mainly focus on word-occurrences and neglect to incorporate semantic regularities in language. Even though recent works have tried to exploit the semantic relationship between words to bridge this gap, they, however, these models which are usually extensions of
LDA or Dirichlet Multinomial mixture (DMM) are tailored to deal effectively with either regular or short documents. The optimal transport distance provides an appealing tool to incorporate the geometry of word semantics into it. More-over, recent developments on efﬁcient computation of optimal transport distance also promote its application in topic modeling. In this paper we ground on optimal transport theory to naturally exploit the geometric structures of semantically rela-ted words in embedding spaces which leads to more interpretable learned topics.
Comprehensive experiments illustrate that the proposed framework outperforms competitive approaches in terms of topic coherence on assorted text corpora which include both long and short documents. The representation of learned topic also leads to better accuracy on classiﬁcation downstream tasks, which is considered as an extrinsic evaluation. 1

Introduction
Topic models such as Latent Dirichlet Allocation (LDA) [3] and its extensions have been success-fully applied to various domains such as science publication, social science, and machine translation
[4]. Fundamentally, topic models are probabilistic models that infer a set of latent topics from a corpus using word co-occurrences within each document. However, when there are a small number of documents or the corpus contains short documents, topic models will tend to infer poor quality topics from little evidence of co-occurrences. Moreover, infrequently occurring words in the corpus might be grouped into irrelevant topics although there are signiﬁcant statistics of synonyms of those words in the corpus.
Several existing studies have targeted to incorporate synonyms or semantic relationship between words into topic models to improve the topic representations. The source of semantic regularities may come from thesauri or knowledge base [20, 25] or distributional similarity [25, 28, 19]. Dis-tributional representation of word semantics, aka word embeddings, has been widely utilized to improve the performance as well as the robustness of learned topics. Much research focuses on adapting existing frameworks to integrate semantic relationships of words [19, 28, 1, 7], while other authors developed non-conjugate models for topic modeling with word embedding awareness and inference with using deep neural networks [8, 14]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Another notable strand developed recently is the optimal transport theory which has been employed in formulating loss functions to numerous machine learning problems, particularly in language mo-deling such as learning document distances [13, 11], generative models [17]. Optimal transport geometry was also applied to quantify the ﬁt between the observed matrix and its reconstruction in the nonlinear dictionary learning [23, 22, 26]. When the observed matrix is the normalized word count matrix, the dictionary learning problem is closely related to topic modeling [9]. The optimal transport distance becomes appealing to machine learning problems due to its nature of integrating the geometry of the data space of the distributions. Recent developments on efﬁcient computation of optimal transport distance [2, 5, 6] also promote its application in machine learning.
In this paper, we aim to apply optimal transport theory to semantically modeling document topics with geometry awareness of word embedding space. The beneﬁt of using optimal transport dis-tance is to naturally exploit the geometric structures of semantically related words in embedding spaces which leads to more interpretable learned topics. Under the convex geometric perspective of the Latent Dirichlet Allocation (LDA) or probabilistic latent semantic indexing (PLSI) [10, 27], our proposed framework is a generalized methodology of LDA/PLSI in which the loss of squared
Euclidean distance between a document empirical distribution and its topic mixture is substituted by regularized optimal transport distance (aka Wasserstein distance). Our model also distinguishes from nonlinear dictionary learning models [23, 22, 26] in the aspect that we take document length into account which allows to effectively model heterogeneous corpora of long and short documents.
In summary, our contributions in this paper are: (i) we provide a novel representation which ge-neralizes the geometric loss function of LDA/PLSI via discrete optimal transport framework. The framework naturally allows us to incorporate underlying geometry of word semantics in embedding spaces; (ii) our proposed formulation leads to an efﬁcient learning algorithm using alternating opti-mization borrowed from discrete optimal transport optimization techniques; (iii) our proposed model achieves signiﬁcantly better performance in the comparison with state-of-the-art topic models; (iv) last but not least, with a strong ﬂavor of geometry and efﬁcient optimization, our framework has implications to study richer classes of topic models such as with multilevel, hierarchical or temporal structures.
Notations. We denote a corpus of N documents of vocabulary size V by D = {di}N i=1. Each document contains ni (repeated) word counts and is represented as a normalized empirical distribu-tion on the support of V vocabulary: di = 1 v=1 nivδwv , where nvi is the number of word v in ni document i. We also denote the normalized word count of document i as ¯ni, a (sparse) vector of V dimensions. The collection of K learned topics is denoted as B = {βk}K k=1 where k-th topic be-longs to the simplex ΣV −1 of RV . Regularized optimal transport distance between two distributions p and q is written as OTγ (p, q).
PV 2 Related background
In this section, together with related work, we review key results on optimal transport, distance, and barycenter, as well as the geometric view of the notable LDA model that we are using in subsequent sections for developing our proposed framework. We also review literature related to our work. 2.1 Optimal transport distance and LDA geometry interpretation
Let p and q ∈ P (X) be two discrete probability distributions on the arbitrary space X ⊆ Rn endowed with cost function d between two points x, y ∈ X. Suppose that p and q share the ﬁxed number of supports V which means p = PV v=1 cvδxv where u, v ∈ ΣV , the simplex of RV . v=1 rvδxv and q = PV
P v tuv = ru and PV u,v tuvduv, such that PV
Optimal transport distance between p and q is deﬁned as the optimization problem OT (p, q) = u tuv = cv. Here, duv = d (xu, xv) and T is a minT
V ×V matrix called transportation plan in which tuv is an element at row u, column v. As computing the distance has the cubic time complexity, Cuturi [5] suggested using the entropic regularization,
H = − P
Optimal transport barycenter is a notion of Fréchet mean of a set of discrete probability dis-tributions {p1, . . . pm} which is deﬁned as the minimizer of the following optimization problem uv tuv ln tuv, to relax the problem and lead to fast computation. 2
Pm i=1 λiOT (q, pi) where λi > 0 and P argmin q∈P(X) problem of ﬁnding the barycenter needs a high complexity algorithm to compute. However, relaxing this problem with entropic regularization also leads to a smooth problem with efﬁcient algorithms to solve [2, 6]. i λi = 1. Similar to optimal transport distance, the
Optimal transport (Wasserstein) dictionary learning [23, 22] is an extension of non-negative ma-trix factorization in which the loss function l2 of reconstruction error is substituted by Wasserstein distance. Let D = {di}N i=1 be normalized bag-of-word of documents where di in V -dimensional simplex. Dictionary learning aims to learn to factorize D into K dictionary elements B = {βk}K k=1 of the same dimension V and matrix mixture weights Λ = {λi}N i=1. The objective of learning is to solve problems of form minB,Λ L (di, P λikβk). When loss function L is squared Euclidean distance the problem becomes non-negative matrix factorization (NMF). Otherwise, if L is Kull-back–Leibler divergence, it becomes PLSI [9]. In Wasserstein dictionary learning, authors dedicated to using Wasserstein (optimal transport) distance for loss function L. Dictionary elements B can be interpreted as the topics while Λ can be considered as document topic proportions.
Convex geometry of topics interpretation of LDA was introduced in [27, 3, 10] in which learning topics from documents in LDA model is equivalent to estimate the convex hull of the K topics
Ω = Conv (ω1, . . . , ωK) from noisy observation of documents. Here ωk represents a topic to be learned in LDA. The authors surrogate the LDA’s likelihood with the geometric loss function 2, where ¯ni is normalized word counts in document i. Ba-as minΩ sed on this interpretation, we propose replacing squared l2 loss with optimal transport distance to incorporate the underlying geometry of word embedding space. i=1 ni minωi∈Ω kωi − ¯nik2
PN 2.2