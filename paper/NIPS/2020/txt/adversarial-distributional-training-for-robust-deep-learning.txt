Abstract
Adversarial training (AT) is among the most effective techniques to improve model robustness by augmenting training data with adversarial examples. However, most existing AT methods adopt a speciﬁc attack to craft adversarial examples, leading to the unreliable robustness against other unseen attacks. Besides, a single attack algorithm could be insufﬁcient to explore the space of perturbations. In this paper, we introduce adversarial distributional training (ADT), a novel framework for learn-ing robust models. ADT is formulated as a minimax optimization problem, where the inner maximization aims to learn an adversarial distribution to characterize the potential adversarial examples around a natural one under an entropic regularizer, and the outer minimization aims to train robust models by minimizing the expected loss over the worst-case adversarial distributions. Through a theoretical analysis, we develop a general algorithm for solving ADT, and present three approaches for parameterizing the adversarial distributions, ranging from the typical Gaussian distributions to the ﬂexible implicit ones. Empirical results on several benchmarks validate the effectiveness of ADT compared with the state-of-the-art AT methods. 1

Introduction
While recent breakthroughs in deep neural networks (DNNs) have led to substantial success in a wide range of ﬁelds [21], DNNs also exhibit adversarial vulnerability to small perturbations around the input [60, 22]. Due to the security threat, considerable efforts have been devoted to improving the adversarial robustness of DNNs [22, 36, 38, 42, 68, 46, 72, 47, 78]. Among them, adversarial training (AT) is one of the most effective techniques [2, 16]. AT can be formulated as a minimax optimization problem [42], where the inner maximization aims to ﬁnd an adversarial example that maximizes the classiﬁcation loss for a natural one, while the outer minimization aims to train a robust classiﬁer using the generated adversarial examples. To solve the non-concave and typically intractable inner maximization problem approximately, several adversarial attack methods can be adopted, such as fast gradient sign method (FGSM) [22] and projected gradient descent (PGD) method [42].
However, existing AT methods usually solve the inner maximization problem based on a speciﬁc attack algorithm, some of which can result in poor generalization for other unseen attacks under the same threat model [58]1. For example, defenses trained on the FGSM adversarial examples, without random initialization or early stopping [69], are vulnerable to multi-step attacks [36, 63]. Afterwards, recent methods [77, 70] can achieve the state-of-the-art robustness against the commonly used attacks (e.g., PGD), but they can still be defeated by others [39, 64]. It indicates that these defenses probably cause gradient masking [63, 2, 66], and can be fooled by stronger or adaptive attacks.
∗Equal contribution. † Corresponding author. 1It should be noted that we consider the generalization problem across attacks under the same threat model, rather than studying the generalization ability across different threat models [24, 17, 62]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Moreover, a single attack algorithm could be insufﬁcient to explore the space of possible perturbations.
PGD addresses this issue by using random initialization, however the adversarial examples crafted by
PGD with random restarts probably lie together and lose diversity [61]. As one key to the success of
AT is how to solve the inner maximization problem, other methods perform training against multiple adversaries [63, 28], which can be seen as more exhaustive approximations of the inner problem [42].
Nevertheless, there still lacks a formal characterization of multiple, diverse adversaries.
To mitigate the aforementioned issues and improve the model robustness against a wide range of adversarial attacks, in this paper we present adversarial distributional training (ADT), a novel framework that explicitly models the adversarial examples around a natural input using a distribu-tion. Subsuming AT as a special case, ADT is formulated as a minimax problem, where the inner maximization aims to ﬁnd an adversarial distribution for each natural example by maximizing the expected loss over this distribution, while the outer minimization aims to learn a robust classiﬁer by minimizing the expected loss over the worst-case adversarial distributions. To keep the adversarial distribution from collapsing into a Delta one, we explicitly add an entropic regularization term into the objective, making the distribution capable of characterizing heterogeneous adversarial examples.
Through a theoretical analysis, we show that the minimax problem of ADT can be solved sequentially similar to AT [42]. We implement ADT by parameterizing the adversarial distributions with trainable parameters, with three concrete examples ranging from the classical Gaussian distributions to the very ﬂexible implicit density models. Extensive experiments on the CIFAR-10 [34], CIFAR-100 [34], and SVHN [44] datasets validate the effectiveness of our proposed methods on building robust deep learning models, compared with the alternative state-of-the-art AT methods. 2 Proposed method
In this section, we ﬁrst introduce the background of adversarial training (AT), then detail adversarial distributional training (ADT) framework, and ﬁnally provide a general algorithm for solving ADT. 2.1 Adversarial training
Adversarial training has been widely studied to improve the adversarial robustness of DNNs. Given a i=1 of n training samples with xi ∈ Rd and yi ∈ {1, ..., C} being the natural dataset D = {(xi, yi)}n example and the true label, AT can be formulated as a minimax optimization problem [42] as min
θ 1 n n (cid:88) i=1 max
δi∈S
L(fθ(xi + δi), yi), (1) where fθ is the DNN model with parameters θ that outputs predicted probabilities over all classes, L is a loss function (e.g., cross-entropy loss), and S = {δ : (cid:107)δ(cid:107)∞ ≤ (cid:15)} is a perturbation set with (cid:15) > 0.
This is the (cid:96)∞ threat model widely studied before and what we consider in this paper. Our method can also be extended to other threat models (e.g., (cid:96)2 norm), which we leave to future work.
This minimax problem is usually solved sequentially, i.e., adversarial examples are crafted by solving the inner maximization ﬁrst, and then the model parameters are optimized based on the generated adversarial examples. Several attack methods can be used to solve the inner maximization problem approximately, such as FGSM [22] or PGD [42]. For example, PGD takes multiple gradient steps as i + α · sign(∇xL(fθ(xi + δt i is the adversarial perturbation at the t-th step, Π(·) is the projection function, and α is a
δt+1 i = ΠS (2) i), yi))(cid:1), (cid:0)δt i is initialized uniformly in S. δt i will converge to a local maximum eventually. where δt small step size. δ0 2.2 Adversarial distributional training
As we discussed, though effective, AT is not problemless. AT with a speciﬁc attack possibly leads to overﬁtting on the attack pattern [36, 63, 77, 70], which hinders the trained models from defending against other attacks. And a single attack algorithm may be unable to explore all possible perturbations in the high-dimensional space, which could result in unsatisfactory robustness performance [63, 28].
To alleviate these problems, we propose to capture the distribution of adversarial perturbations around each input instead of only ﬁnding a locally most adversarial point for more generalizable adversarial 2
Figure 1: Visualization of the adversarial exam-ples generated by PGD with random restarts (blue) and those sampled from the adversarial distribution learned by ADTEXP (orange). Each subﬁgure corre-sponds to one randomly selected data point.
Figure 2: Visualization of loss surfaces in the vicinity of an input along the gradient direction (dg) and a random direction (dr) for various models in (a)-(e). (f) reports the dominant eigenvalue of the Hessian matrix of the classiﬁ-cation loss w.r.t. the input. Full details are in Sec. 5.3. training, called adversarial distributional training (ADT). In particular, we model the adversarial perturbations around each natural example xi by a distribution p(δi), whose support is contained in
S. Based on this, ADT is formulated as a distribution-based minimax optimization problem as min
θ 1 n n (cid:88) i=1 max p(δi)∈P
Ep(δi) (cid:2)L(fθ(xi + δi), yi)(cid:3), (3) where P = {p : supp(p) ⊆ S} is a set of distributions with support contained in S. As can be seen in
Eq. (3), the inner maximization aims to learn an adversarial distribution, such that a point drawn from it is likely an adversarial example. And the objective of the outer minimization is to adversarially train the model parameters by minimizing the expected loss over the worst-case adversarial distributions induced by the inner problem. It is noteworthy that AT is a special case of ADT, by specifying the distribution family P to contain Delta distributions only.
Regularizing adversarial distributions. For the inner maximization of ADT, we can easily see that max p(δi)∈P
Ep(δi) (cid:2)L(fθ(xi + δi), yi)(cid:3) ≤ max
δi∈S
L(fθ(xi + δi), yi). (4)
It indicates that the optimal distribution by solving the inner problem of ADT will degenerate into a
Dirac one. Hence the adversarial distribution cannot cover a diverse set of adversarial examples, and
ADT becomes AT. To solve this issue, we add an entropic regularization term into the objective as min
θ 1 n n (cid:88) i=1 max p(δi)∈P
J (cid:0)p(δi), θ(cid:1), with J (cid:0)p(δi), θ(cid:1) = Ep(δi) (cid:2)L(fθ(xi+δi), yi)(cid:3)+λH(p(δi)), (5) where H(p(δi)) = −Ep(δi)[log p(δi)] is the entropy of p(δi), λ is a balancing hyperparameter, and
J (cid:0)p(δi), θ(cid:1) denotes the overall loss function for notation simplicity. Note that the entropy maximiza-tion is a common technique to increase the support of a distribution in generative modeling [12, 13] or reinforcement learning [23]. We next discuss why ADT is superior to AT. 2.2.1 Discussion on the superiority of ADT
The major difference between AT and ADT is that for each natural input xi, AT ﬁnds a worst-case adversarial example, while ADT learns a worst-case adversarial distribution comprising a variety of adversarial examples. Because adversarial examples can be generated by various attacks, we expect that those adversarial examples probably lie in the region where the adversarial distribution assigns high probabilities, such that minimizing the expected loss over this distribution can naturally lead to a better generalization ability of the trained classiﬁer across attacks under the same threat model.
Furthermore, as we add an entropic regularizer into the objective (5), the adversarial distribution is able to better explore the space of possible perturbations and characterize more diverse adversarial examples compared with a single attack method (e.g., PGD). To show this, for each data we generate a set of adversarial examples by PGD with random restarts and sample another set of adversarial examples from the adversarial distribution learned by ADTEXP (a variant of ADT detailed in Sec. 3.1), 3
Algorithm 1 The general algorithm for ADT
Input: Training data D, objective function J (cid:0)p(δi), θ(cid:1), the set of perturbation distributions P, training epochs
N , and learning rate η. 1: Initialize θ; 2: for epoch = 1 to N do 3: 4: 5: 6: 7: end for end for for each minibatch B ⊂ D do
Obtain p∗(δi) for each input (xi, yi) ∈ B by solving p∗(δi) = arg maxp(δi)∈P J (cid:0)p(δi), θ(cid:1);
Update θ with stochastic gradient descent θ ← θ − η · E(xi,yi)∈B (cid:2)∇θJ (cid:0)p∗(δi), θ(cid:1)(cid:3); targeted at a standard trained model. Then we can visualize these adversarial examples by projecting them onto the 2D space spanned by the ﬁrst two eigenvectors given by PCA [30]. The visualization results of some randomly selected data points in Fig. 1 show that adversarial examples sampled from the adversarial distribution are scattered while those crafted by PGD concentrate together. We further evaluate the diversity of adversarial examples by quantitatively measuring their average pairwise distances. The average (cid:96)2 distance of adversarial examples sampled from the adversarial distribution over 100 test images is 1.95, which is 1.56 for PGD. Although the adversarial distributions can characterize more diverse adversarial examples, they have a similar attack power compared with PGD, as later shown in Table 4. Minimizing the loss on such diverse adversarial examples can consequently help to learn a smoother and more ﬂattened loss surface around the natural examples in the input space, as shown in Fig. 2. Therefore, ADT can improve the overall robustness compared with AT. 2.3 A general algorithm for ADT
To solve minimax problems, Danskin’s theorem [14] states how the maximizers of the inner problem can be used to deﬁne the gradients for the outer problem, which is also the theoretical foundation of
AT [42]. However, it is problematic to directly apply Danskin’s theorem for solving ADT since the search space P may not be compact, which is one assumption of this theorem. As it is non-trivial to perform a theoretical analysis on how to solve ADT, we ﬁrst lay out the following assumptions.
Assumption 1. The loss function J (cid:0)p(δi), θ(cid:1) is continuously differentiable w.r.t. θ.
Assumption 1 is also made in [42] for AT. Although the loss function is not completely continuously differentiable due to the ReLU layers, the set of discontinuous points has measure zero, such that it is assumed not to be an issue in practice [42].
Assumption 2. Probability density functions of distributions in P are bounded and equicontinuous.
Assumption 2 puts a restriction on the set of distributions P. We show that the explicit adversarial distributions proposed in Sec. 3.1 satisfy this assumption (in Appendix B.1).
Theorem 1. Suppose Assumptions 1 and 2 hold. We deﬁne ρ(θ) = maxp(δi)∈P J (cid:0)p(δi), θ(cid:1), and
P ∗(θ) = {p(δi) ∈ P : J (cid:0)p(δi), θ(cid:1) = ρ(θ)}. Then ρ(θ) is directionally differentiable, and its directional derivative along the direction v satisﬁes
ρ(cid:48)(θ; v) = sup p(δi)∈P ∗(θ) v(cid:62)∇θJ (cid:0)p(δi), θ(cid:1). (6)
Particularly, when P ∗(θ) = {p∗(δi)} only contains one maximizer, ρ(θ) is differentiable at θ and
∇θρ(θ) = ∇θJ (cid:0)p∗(δi), θ(cid:1). (7)
The complete proof of Theorem 1 is deferred to Appendix B.1. Theorem 1 provides us a general principle for training ADT, by ﬁrst solving the inner problem and then updating the model parameters along the gradient direction of the loss function at the global maximizer of the inner problem, in a sequential manner similar to AT [42]. We provide the general algorithm for ADT in Alg. 1. Analogous to AT, the global maximizer of the inner problem cannot be solved analytically. Therefore, we propose three different approaches to obtain approximate solutions, as introduced in Sec. 3. Although we cannot reach the global maximizer of the inner problem, our experiments suggest that we can reliably solve the minimax problem (5) by our algorithm. 4
Figure 3: An illustration of the three different approaches to parameterize the distributions of adversarial perturbations. (a) ADTEXP: the explicit adversarial distribution pφi (δi) is deﬁned by transforming N (µi, diag(σ2 i )) via tanh fol-lowed by a multiplication with (cid:15). (b) ADTEXP-AM: we amor-tize the explicit adversarial distributions by a neural network gφ taking xi as input. (c) ADTIMP-AM: we deﬁne the im-plicit adversarial distributions by inputting an additional random variable z ∼ U(−1, 1) to the network gφ. 3 Parameterizing adversarial distributions
At the core of ADT lie the solutions of the inner maximization problem of Eq. (5). The basic idea is to parameterize the adversarial distributions with trainable parameters φi. With the parameterized pφi(δi), the inner problem is converted into maximizing the expected loss w.r.t. φi. In the following, we present the parametrizations and learning strategies of three different approaches, respectively.
We provide an overview of these approaches in Fig. 3. 3.1 ADTEXP: explicit modeling of adversarial perturbations
A natural way to model adversarial perturbations around an input data is using a distribution with an explicit density function. We name ADT with EXPlicit adversarial distributions as ADTEXP. To deﬁne a proper distribution pφi(δi) on S, we take the transformation of random variable approach as
δi = (cid:15) · tanh(ui), ui ∼ N (µi, diag(σ2 (8) where ui is sampled from a diagonal Gaussian distribution with µi, σi ∈ Rd as the mean and standard deviation. ui is transformed by a tanh function and then multiplied by (cid:15) to get δi. We let
φi = (µi, σi) denote the parameters to be learned. We sample ui from a diagonal Gaussian mainly for the sake of computational simplicity. But our method is fully compatible with more expressive distributions, such as matrix-variate Gaussians [40] or multiplicative normalizing ﬂows [41], and we leave using them to future work. Given Eq. (8), the inner problem of Eq. (5) becomes i )), (cid:110)
Epφi (δi) max
φi (cid:2)L(fθ(xi + δi), yi)(cid:3) + λH(pφi(δi)) (cid:111)
. (9)
To solve this, we need to estimate the gradient of the expected loss w.r.t. the parameters φi. A com-monly used method is the low-variance reparameterization trick [33, 5], which replaces the sampling process of the random variable of interest with the corresponding differentiable transformation. With this technique, the gradient can be back-propagated from the samples to the distribution parameters directly. In our case, we reparameterize δi by δi = (cid:15) · tanh(ui) = (cid:15) · tanh(µi + σir), where r is an auxiliary noise variable following the standard Gaussian distribution N (0, I). Therefore, we can estimate the gradient of φi via
Er∼N (0,I)∇φi (cid:0)xi + (cid:15) · tanh(µi + σir)(cid:1), yi (cid:0)(cid:15) · tanh(µi + σir)(cid:1)(cid:105) (cid:1) − λ log pφi
L(cid:0)fθ (10) (cid:104)
.
The ﬁrst term inside is the classiﬁcation loss with the sampled noise, and the second is the negative log density (i.e., estimation of entropy). It can be calculated analytically (proof in Appendix B.2) as d (cid:88) j=1 (cid:16) 1 2 (r(j))2 + log 2π 2
+ log σ(j) i + log (cid:0)1 − tanh(µ(j) i + σ(j) i r(j))2(cid:1) + log (cid:15) (cid:17)
, (11) where the superscript j denotes the j-th element of a vector.
In practice, we approximate the expectation in Eq. (10) with k Monte Carlo (MC) samples, and perform T steps of gradient ascent on φi to solve the inner problem. After obtaining the optimal parameters φ∗ i , we use the adversarial distribution pφ∗ (δi) to update model parameters θ. i 3.2 ADTEXP-AM: amortizing the explicit adversarial distributions
Although the aforementioned method in Sec. 3.1 provides a simple way to learn explicit adversarial distributions for ADT, it needs to learn the distribution parameters for each input and then brings 5
prohibitive computational cost. Compared with PGD-based AT which constructs adversarial examples by T steps PGD [42], ADTEXP is approximately k times slower since the gradient of φi is estimated by k MC samples in each step. In this subsection, we propose to amortize the inner optimization of
ADTEXP, to develop a more feasible and scalable training method. We name ADT with the AMortized version of EXPlicit adversarial distributions as ADTEXP-AM.
Instead of learning the distribution parameters for each data xi, we opt to learn a mapping gφ : Rd →
P, which deﬁnes the adversarial distribution for each input in a conditional manner pφ(δi|xi). We instantiate gφ by a conditional generator network. It takes a natural example xi as input, and outputs the parameters (µi, σi) of its corresponding explicit adversarial distribution, which is also deﬁned by
Eq. (8). The advantage of this method is that the generator network can potentially learn common structures of the adversarial perturbations, which can generalize to other training samples [3, 49]. It means that we do not need to optimize φ excessively on each data xi, which can accelerate training. 3.3 ADTIMP-AM: implicit modeling of adversarial perturbations
Since the underlying distributions of adversarial perturbations have not been ﬁgured out yet and could be different across samples, it is hard to specify a proper explicit distribution of adversarial examples, which may lead to the underﬁtting problem. To bypass this, we resort to implicit distributions (i.e., distributions without tractable probability density functions but can still be sampled from), which have shown promising results recently [20, 55, 56], particularly in modeling complex high-dimensional data [51, 27]. The major advantage of implicit distributions is that they are not conﬁned to provide explicit densities, which improves the ﬂexibility inside the sampling process.
Based on this, we propose to use the implicit distributions to characterize the adversarial perturbations.
Considering the priority of amortized optimization, we learn a generator gφ : Rdz × Rd → Rd which implicitly deﬁnes a conditional distribution pφ(δi|xi) by δi = gφ(z; xi), where xi is a natural input and z ∈ Rdz is a random noise vector. Typically, z is sampled from a prior p(z) such as the standard
Gaussian or uniform distributions as in the generative adversarial networks (GANs) [20]. In this work, we sample z from a uniform distribution U(−1, 1). We refer to this approach as ADTIMP-AM.
A practical problem remains unaddressed is that the entropy of the implicit distributions cannot be estimated exactly as we have no access to the density pφ(δi|xi). We instead maximize the variational lower bound of the entropy [12] for its simplicity and success in GANs [13]. We provide full technical details of ADTEXP-AM and ADTIMP-AM, and training algorithms of the three methods in Appendix A. 4