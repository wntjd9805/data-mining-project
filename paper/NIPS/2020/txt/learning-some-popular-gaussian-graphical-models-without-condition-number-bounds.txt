Abstract
Gaussian Graphical Models (GGMs) have wide-ranging applications in machine learning and the natural and social sciences. In most of the settings in which they are applied, the number of observed samples is much smaller than the dimension and they are assumed to be sparse. While there are a variety of algorithms (e.g.
Graphical Lasso, CLIME) that provably recover the graph structure with a log-arithmic number of samples, to do so they require various assumptions on the well-conditioning of the precision matrix which preclude long-range dependen-cies, present in many settings of interest.
Here we give the ﬁrst ﬁxed polynomial-time algorithms for learning attractive
GGMs and walk-summable GGMs with a logarithmic number of samples and without any such assumptions. In particular, our algorithms can tolerate strong dependencies among the variables. Our result for structure recovery in walk-summable GGMs is derived from a more general result for efﬁcient sparse lin-ear regression in walk-summable models without any norm dependencies. We complement our results with experiments showing that many existing algorithms fail even in some simple settings where there are long dependency chains. Our algorithms do not. 1

Introduction
A Gaussian Graphical Model (GGM) in n dimensions is a probability distribution with density pX (x) = 1 (cid:112)(2π)n det Σ exp (cid:0) − (x − µ)T Σ−1(x − µ)/2(cid:1) where µ is the mean and Σ is the covariance matrix. In other words, it is just a multivariate Gaussian.
What makes the multivariate Gaussian so interesting as a graphical model is that its conditional independence structure is completely encoded in the precision matrix, Θ = Σ−1. For a given
Θ, if we let G be the graph with vertex set [n] and an edge between vertices i, j iff Θij (cid:54)= 0, then X ∼ N (µ, Σ) exhibits the following remarkable Markov property with respect to G: for any set S ⊂ [n], if removing the nodes S from G disconnects nodes i and j, then Xi and Xj are conditionally independent given XS. This is a very special and useful algebraic property, because
∗Department of Mathematics, Massachusetts Institute of Technology. Email: kelner@mit.edu. This work was partially supported by NSF Award CCF-1565235
†Department of Mathematics, Massachusetts Institute of Technology. Email: fkoehler@mit.edu. This work was supported in part by Ankur Moitra’s ONR Young Investigator Award.
‡Department of Computer Science, UCLA. Email: raghum@cs.ucla.edu. This work was supported by NSF CAREER Award CCF-1553605.
§Department of Mathematics, Massachusetts Institute of Technology. Email: moitra@mit.edu. This work was supported in part by NSF CAREER Award CCF-1453261, NSF Large CCF-1565235, a David and
Lucile Packard Fellowship and an ONR Young Investigator Award. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
it means that if we can just estimate the support of the matrix Θ accurately, we can potentially discover a lot of valuable information about the random variables X1, . . . , Xn. It was for this reason that Dempster [1] initiated the study of learning GGMs in the 1970s.
GGMs have wide-ranging applications in machine learning and the natural and social sciences where they are one of the most popular ways to model statistical relationships between observed variables.
For example, they are used to infer the structure of gene regulatory networks (see e.g. [2, 3, 4, 5]) and to learn functional brain connectivity networks [6, 7]. In causal inference, GGM routines are often used as a basic subroutine when attempting to learn causal models (see e.g. [8]), because it corresponds to looking for the moral graph of the directed model, which (1) greatly constrains the possible set of causal structures which need to be considered, and (2) can be learned from purely observational data, unlike the original causal structure. It is important to note that in most of the settings in which GGMs are applied, the number of observed samples is small compared to the dimensionality of the data. This means that in practice, it is only possible to learn the GGM in a meaningful sense under some sort of sparsity assumption. We will make the assumption that the rows and columns of Θ are d-sparse – i.e., the case where the dependency graph G has maximum degree at most d.
From a theoretical standpoint, there is vast literature on learning sparse GGMs under various as-sumptions. Many approaches focus on sparsistency – where the goal is to learn the sparsity pattern of Θ assuming some sort of lower bound on the strength of non-zero interactions. This is a natural objective because once the sparsity pattern is known, estimating the entries of Θ is straightforward (e.g. one can use ordinary least squares); because of this, the problems of learning GGMs and sparse linear regression are very closely related. A popular approach to learning GGMs is the Graphical
Lasso5 [9] which solves the following convex program: (cid:16) log det(Θ) − (cid:104)(cid:98)Σ, Θ(cid:105) − λ(cid:107)Θ(cid:107)1 (cid:17) max
Θ(cid:31)0 where (cid:98)Σ is the empirical covariance matrix and (cid:107)Θ(cid:107)1 is the (cid:96)1 norm of the matrix as a vector.
It is known that if Θ satisﬁes various conditions, which typically include an assumption similar to or stronger than the restricted eigenvalue (RE) condition (a condition which, in particular, lower bounds the smallest eigenvalue of any 2d × 2d principal submatrix of Σ) then Graphical Lasso and related (cid:96)1 methods can succeed in recovering the graph structure (see e.g. [10, 11]). For the Graphical
Lasso itself, under some incoherence assumptions on the precision matrix (stronger than RE), it has been shown [12] that the sparsity pattern of the precision matrix can be accurately recovered from O((1/α2)d2 log(n)) samples; here α is an incoherence parameter and we are omitting the dependence on some additional terms. We emphasize that this is only the best known theoretical guarantee — the performance in real life often seems better than this pessimistic bound.
Another popular approach to learning GGMs is the CLIME estimator which solves the following linear program: min
Θ (cid:107)Θ(cid:107)1 s.t. (cid:107)(cid:98)ΣΘ − I(cid:107)∞ ≤ λ
The analysis of CLIME assumes a bound M on the maximum (cid:96)1-norm of any row of the inverse covariance (given that the Xi’s are standardized to unit variance). This is also a type of condition number assumption, although with respect to a different geometry than RE: more precisely, since
M = max (cid:107)u(cid:107)∞≤1 (cid:107)Θu(cid:107)∞ it can be thought of as the condition number of Σ when viewed as an operator mapping (cid:96)∞ → (cid:96)∞; this can be smaller than the normal Euclidean condition number. CLIME succeeds at structure recovery when given m (cid:38) CM 4 log n samples (here for simplicity we are assuming the entries Θij are either zero or bounded away from zero by an absolute constant c so that M = Ω(d)).
While these works show that sparse GGMs can be estimated when the number of samples is loga-rithmic in the dimension, there is an important caveat in their guarantees. They all need to assume 5We note that [9] did not introduce this objective (see discussion there), but rather an optimization procedure used to maximize it, and Graphical Lasso technically refers to this speciﬁc optimization procedure. 2
that Θ is well-conditioned, and differ mainly in the strength of their assumption: roughly speaking, one of the stronger assumptions6 used in this literature is that Θ is well-conditioned in the usual sense, and the weakest is the (cid:96)∞ → (cid:96)∞ condition number bound assumed by CLIME. This is of-ten heuristically justiﬁed by the belief that a small condition number is information-theoretically required for structure recovery to be possible. However, and as we will discuss later, recent works have pointed out that this is actually not the case — the correct information-theoretic condition is signiﬁcantly weaker than even the assumption which CLIME makes. Indeed, the fact that bounded condition number is not the right assumption for structure recovery is hinted at by the fact that it does not behave nicely under benign operations like rescaling individual variables.
In the high-dimensional setting, bounded condition number can be a somewhat strong condition: in particular, this assumption is violated by simple and natural models (e.g. a graphical model on a path such as a time series), where these bounds turn out to be polynomial in the dimension.
In this paper, we study some fundamental classes of GGMs and show how to learn them efﬁciently in the low-sample regime under the correct information-theoretic assumption, even when they are ill-conditioned. We also complement our results with examples that break both previous algorithms and our own algorithms for learning general sparse GGMs. This leaves open the interesting ques-tion (raised in [14], and closely related to similar questions about sparse linear regression [15]) of whether some sparse GGMs may be computationally hard to learn with so few samples. Finally, we show experimentally that popular approaches, like the Graphical Lasso and CLIME, do in fact need a polynomial in n number of samples even in some relatively benign examples (and where our algorithm does succeed).
Our work was motivated by a recent paper of Misra, Vuffray and Lokhov [14] which studied the question of how many samples are needed information-theoretically to learn sparse GGMs in the ill-conditioned case. They required only the following natural non-degeneracy condition (which also appeared in [16, 17]): that for every i, j with Θij (cid:54)= 0, we have a lower bound on the conditional partial correlation7 below:
κ ≤
|Θij| (cid:112)ΘiiΘjj
=
|Cov(Xi, Xj | X∼i,j)| (cid:112)Var(Xi|X∼i)Var(Xj|X∼j)
.
Intuitively, this assumption means that if we have already observed all of the coordinates of X except for Xi and Xj, then the remaining randomness over Xi and Xj has a correlation coefﬁcient of at least κ. This condition is the correct one because: (1) the absence of an edge in the GGM exactly corresponds to zero partial correlation in the above sense, (2) it has the correct symmetries — it is not affected by rescaling of any coordinate, and (3) it is the same condition which is needed in the classical, low-dimensional OLS regression t-test [18] to successfully reject the null hypothesis that the true coefﬁcient of Xj is zero when regressing Xi off of X∼i.
Crucially, this assumption could be much weaker than any condition number bound, because it allows for the random variables to be strongly correlated. Here is a basic example (from [14]): suppose we have three Gaussians X1, X2 and X3 where X1 is heavily correlated with X2. In this case, the condition number of Σ will explode as X1 and X2 become more correlated. Nevertheless, it remains possible to test if there is a κ-nondegenerate edge between X1 and X3, as long as we correctly adjust for the effect of X2. In contrast, if we were unaware of the value of X2, it would be very difﬁcult to test for the same edge between X1 and X3, because the X1 and X2 edge contributes a very large amount of variance to X1.
The work of [14] exhibited an algorithm achieving this requirement — more precisely, they showed that it is always possible to estimate the graph structure with m ≥ C d
κ2 log n samples without requiring any additional assumptions, clarifying that further condition number as-sumptions are indeed unnecessary. On the other hand, the result of [17] gives an information-6Indeed, there are even stronger assumptions such as quantitative versions of faithfulness which we do not discuss but are needed to prove the correctness of the popular PC algorithm [13]. 7Here X∼i (resp. X∼i,j) denotes the random vector formed by deleting coordinate i (resp. please see Preliminaries for further details and formal deﬁnitions of conditional variances, covariances. i, j) of X; 3
theoretic lower bound8 of Ω((1/κ2) log n) on the sample complexity for structure recovery. To summarize, the upper bound of [14] differs from the lower bound of [17] by exactly a factor of d (it is unknown what the optimal dependence is) and otherwise is optimal.
However, the algorithm of [14] runs in time nO(d), making it impossible to run except for small instances. This is because their algorithm is based on a reduction to a sequence of sparse linear re-gression problems that can all be ill-conditioned. It is believed that such problems exhibit wide gaps between what is possible information theoretically and what is possible efﬁciently. For instance, it is known that the general sparse linear regression problem under ﬁxed design is NP-hard9 [19, 15].
Misra et al. solve the sparse linear regression problems using exhaustive search over d-size neigh-borhoods (hence the nO(d) time). This leads to the main question we study:
Can we get efﬁcient and practical algorithms for learning GGMs (run-time (cid:28) no(d)) in some natural, but still ill-conditioned, cases? 2 Our Results and Technical Overview
We show that for some popular and widely-used classes of GGMs—attractive GGMs and walk-summable GGMs — it is possible to achieve both logarithmic sample complexity (the truly high-dimensional setting) and computational efﬁciency, even when Θ is ill-conditioned.
Attractive GGMs
First we study the class of attractive GGMs, in which the off-diagonal entries of Θ are non-positive.
In terms of the correlation structure, this means that all partial correlations are nonnegative. There are several practical motivations for studying attractive GGMs: in phylogenetic applications, ob-served variables are often positively dependent because of shared ancestry [20]; in various copula models that are popular in ﬁnance, we posit a latent global market variable that also leads to positive dependence [21]; see also [22] for more discussion.
A well-studied special case (which essentially captures all attractive GGMs — see Lemma 15) is the discrete Gaussian Free Field (GFF), in which case Θ is the generalized Laplacian associated to a weighted graph. This is a natural model because the Laplacian encourages “smoothness” with respect to the graph structure: see e.g. [23]; for this reason, the GFF is an important modeling tool in active and semi-supervised learning (see [24, 25, 26]); the GFF also arises in nature from a number of diverse phenomena in random walks, statistical physics, and random surfaces [27, 28, 23].
In the GFF setting, Θ will be ill-conditioned, even in the weak (cid:96)∞ → (cid:96)∞ sense, whenever some pair of vertices have large effective resistance between them (e.g., paths, rectangular grids, etc.,); informally, it happens when the graph has many sparse cuts.
We show experimentally (in Appendix I) that simple examples, like the union of a long path and some small cliques, do indeed foil the Graphical Lasso and other popular methods. Intuitively, this is because GFFs on a path exhibit long-range correlations that violate the assumptions used in current works — our examples show that the assumptions made in the literature are to some extent necessary for these algorithms. This analysis reveals a blind spot of the Graphical Lasso: It performs poorly in the presence of long dependency chains, which could lead to missing some important statistical relationships in applications.
We propose the following simple algorithm and show that it succeeds in learning the graph structure of attractive GGMs. This algorithm, called GREEDYANDPRUNE, does the following to learn the neighborhood of node i: 1. Set S = ∅ and let ν > 0 be a thresholding parameter. 2. (Greedy/OMP step) Repeat the following T times: set j to be the the minimizer of (cid:100)Var(Xi|XS, Xj) and add j to S. 8A subtle point arises when interpreting this bound, because d and κ are closely related quantities (see e.g.
Lemma 6 below). In the lower bound constructions of [17] they have d = O(1/κ) and the term dominating their bound depends only on κ. 9For proper learning, where the algorithm is required to output a d-sparse estimator. 4
3. (Pruning step) For each j ∈ S: if (cid:100)Var(Xi|XS) > (1 − ν)(cid:100)Var(Xi|XS\{j}), remove j from
S. 4. Return S as the neighborhood of node i. where (cid:100)Var indicates the variance is estimated from sample, using Ordinary Least Squares. A more detailed description of the algorithm is given in the Appendix.
In the literature, this is called a forward-backward method [29].
Theorem 1 (Informal version of Theorem 7). Fix a κ-nondegenerate attractive GGM. The
GREEDYANDPRUNE algorithm runs in polynomial time and returns the true neighborhood of every node i with high probability with m ≥ C(d/κ2) log(1/κ) log(n) samples, where C is a universal constant.
Our algorithm matches the sample complexity of the previous best (inefﬁcient) algorithms for this setting [16, 14] and obtains, up to log factors, the optimal dependence on κ for ﬁxed d.
Analysis for Attractive GGMs The main intuition behind the algorithm and the crux of our anal-ysis is the following: For attractive GGMs the conditional variance of a variable Xi when we con-dition on a set XS is a monotonically decreasing and supermodular function of S. This fact was previously observed in the GFF setting (independently in [26, 30]) with relatively involved proofs; we give a new, short proof of this fact using just basic linear algebra. Other works such as [31, 32] have considered supermodularity in somewhat related regression settings, but with important differ-ences (see Further Discussion).
Given the supermodularity result, we next need to address the issue that we don’t have access to actual conditional variances, but only their empirical estimates. To achieve the efﬁcient sam-ple complexity of Theorem 1 we carefully analyze the alignment between the true decrement of conditional variance in one step, Var(Xi|XS) − Var(Xi|XS∪{j}) and the noisy empirical decre-ment (cid:100)Var(Xi|XS) − (cid:100)Var(Xi|XS∪{j}). A subtle obstacle is that we need to control the differences (cid:100)Var(Xi|XS)−(cid:100)Var(Xi|XS∪{j}) without assuming too much accuracy on the estimates (cid:100)Var(Xi|XS) themselves. Fortunately, this can be shown using matrix concentration, combined with some tools from classical low-dimensional regression tests [18].
To complete the analysis, we need a new structural result for attractive GGMs which bounds the conditional variance after the ﬁrst step of greedy, so that only a bounded number of iterations of greedy are required to learn a superset of the neighborhood. We prove this by reducing to the setting of discrete GFFs, where we can use an electrical argument based on effective resistances. Formally, we prove the following new structural result for walk-summable GGMs:
Lemma 1 (Lemma 9 of the Appendix). Suppose that i is a node with d ≥ 1 neighbors in an attractive or walk-summable GGM. Then there exists a neighbor j such that
Var(Xi|Xj) ≤ 4d
Θii
= 4d · Var(Xi|X∼i).
Previous work on Learning Attractive GGMs. Some prior work on learning attractive GGMs have focused on the Maximum Likelihood Estimator (MLE). This was shown to exist and be unique using connections to total positivity in [33, 34], but we are not aware of any sample complexity guarantees in the context of structure learning. It also is likely broken by the same examples (see
Section I) as the Graphical Lasso (since the constrained MLE is the same as the Graphical Lasso with zero regularization and a non-negativity constraint). Finally, the recent work [22] studied adaptive estimators for learning GGMs, but only for the case where the model is well-conditioned.
Optimal Information-Theoretic Bounds. The previous literature leaves open the information-theoretically optimal sample complexity for learning attractive GGMs. We resolve this question: a simple estimator based on (cid:96)0-constrained least squares, which we refer to as SEARCHANDVALI-DATE, achieves sample complexity matching the information-theoretic lower bounds of [17] (whose instances can easily be made attractive) up to constants:
Theorem 2 (Informal version of Theorem 11). In a κ-nondegenerate attractive GGM, as long as m = Ω((1/κ2) log(n)), with high probability Algorithm SEARCHANDVALIDATE returns the true neighborhood of every node i. This algorithm runs in time O(nd+1). 5
The results of [17] imply that Ω((1/κ2) log(n)) samples are required even to distinguish the empty graph from a graph with a single κ-nondegenerate edge in an unknown location. This bound does not depend on d, which may appear surprising. This is possible because d ≤ 1/κ2 in κ-nondegenerate attractive GGMs — see Lemma 6. We also give a version of the above result for general models with sample complexity O(d log(n)/κ2) and time complexity O(nd+1), giving a faster alternative to [14] with the same sample complexity guarantee.
Theorem 2 is proved by a careful analysis of the signal-vs-entropy tradeoff between choosing the correct support (which is best in expectation) and an incorrect support with k disagreements for each k. To do this we again need to study structural properties of the GGM; we establish something similar to a “margin condition” in empirical process theory [35]. Precisely analyzing the differences in empirical risk again builds upon some classical ideas in regression testing [18].
This result also identiﬁes an important barrier to improving the information theoretic lower bound of [17], as their lower-bound instances can easily be made attractive. If this bound is not tight for general GGMs, it appears signiﬁcantly new ideas will be needed to separate the sample complex-ity of learning attractive and non-attractive GGMs — they must rely upon the ability of negative correlations to create nontrivial cancellations.
Walk-Summable GGMs
While attractive GGMs are natural in some contexts, in others they are not. For example, in Genome
Wide Association Schemes (GWASs), genes typically have inhibitory effects too. This leads us to another popular and well-studied class of GGMs: the walk-summable models. These were origi-nally introduced by Maliutov, Johnson, and Willsky [36] to explain the convergence properties of
Gaussian Belief Propagation observed in practice (see also [37]).
All attractive GGMs are walk-summable, as are other important classes of GGMs like pairwise nor-malizable and non-frustrated models [36]. A number of equivalent deﬁnitions are known for walk-summability. The following deﬁnition is perhaps the easiest to work with: Θ is walk-summable if making all off-diagonal entries of Θ negative preserves the fact that Θ is positive deﬁnite. Per-haps less well known, walk-summable models are exactly those GGMs with Symmetric Diagonally
Dominant (SDD, see Preliminaries) precision matrices under a rescaling of the coordinates — see e.g. [38, 39]. In the linear algebra literature [38], a walk-summable matrix Θ is referred to as a symmetric H-matrix with nonnegative diagonal.
Analysis for learning Walk-Summable GGMs. The analysis of learning walk-summable models is considerably different from the attractive case, because supermodularity (and even weak super-modularity [32]) of the conditional variance fails to hold – see Section H.1. Regardless, we are still able to prove that GREEDYANDPRUNE learns all walk-summable models with sample complex-ity that scales logarithmically with n. We also propose a variant HYBRIDMB that achieves better sample complexity.
The key idea in this analysis is that a single greedy step can serve as a kind of sparse weak precondi-tioner, roughly in terms of the (cid:96)∞ → (cid:96)∞ geometry considered in CLIME. More precisely, we show that after a single step of greedy, the unknown sparse regression vector has small (cid:96)1-norm (inde-pendent of n and scaling correctly with the noise level). This is shown in the proof of Theorem 16, based on effective resistance arguments relatd to Lemma 1. The (cid:96)1-norm bound not only implies that greedy works, but also that appropriate invovations of (cid:96)1-based methods (like the Lasso) can now be guaranteed to work. We emphasize that such bounds do not hold without our “weak precon-ditioning” step.
Concretely, we propose an algorithm called HYBRIDMB based on this idea and show that it learns walk-summable GGMs without any condition number dependence. This algorithm does the follow-ing to learn the neighborhood of node i, where some technical details are left to the full algorithm description given in the Appendix: 1. (Greedy step) Set j to be the minimizer of (cid:100)Var(Xi|Xj). 6
2. (Lasso with implicit weak preconditioning) Solve for w, a in

 min w,a:(cid:107)w(cid:107)1≤λ
ˆE


Xi − (cid:88) wk k /∈{i,j} (cid:113)
Xk (cid:100)Var(Xk|Xj) 2

− aXj


 .
We detail the selection of λ in the full version of the algorithm — see the Appendix. 3. (Pruning step) We perform a pruning step similar to GREEDYANDPRUNE to zero out some of the entries of w, and to test if j is an actual neighbor. 4. Return j (if it passed the test) and the remaining support of w as the neighborhood of i.
The analysis of HYBRIDMB uses the aforementioned structural results for walk-summable models and a statistical analysis for the regression problem arising after the greedy step. The regression anal-ysis is similar in spirit to the usual generalization bounds for (cid:96)1-constrained regression but slightly more subtle. The key insight is that the output of the algorithm is the same if we replace Xk by
Xk − E[Xk|Xj]; this change of basis is unknown to the algorithm, but the analysis is much easier because Xj becomes independent of the other regressors.
Theorem 3 (Informal version of Theorem 17). Fix a walk-summable, κ-nondegenerate GGM. Al-gorithm HYBRIDMB runs in polynomial time and returns the true neighborhood of every node i with high probability given m ≥ C(d/κ4) log(n) samples, where C is a universal constant.
We can also prove a similar (but slightly weaker) guarantee for Algorithm GREEDYANDPRUNE
— see Theorem 18. For context, we note that prior to our work, Anandkumar, Tan, Huang and
Willsky [16] gave an inefﬁcient nO(d) time algorithm for learning walk-summable models with similar guarantees and requiring some additional assumptions.
The above structure learning result requires κ-nondegeneracy and sparsity of the entire model. How-ever, it is proved using the following general result for sparse linear regression, which requires only a joint walk-summability assumption:
Theorem 4 (Informal version of Theorem 16). Suppose that Y = w · X + ξ where w is d-sparse,
ξ ∼ N (0, σ2) is independent of multivariate Gaussian r.v. X ∼ N (0, Σ), and suppose that the joint distribution of (X1, . . . , Xn, Y ) is a walk-summable GGM. Given m samples from this model,
WS-REGRESSION runs in polynomial time and returns ˆw such that
E[(w · X − ˆw · X)2] = O(σ2(cid:112)d log(n)/m) with high probability.
Although this result gives a “slow rate” of (cid:112)1/m, it is quite different from the usual slow rate guarantee for the Lasso. The latter guarantees an upper bound on the prediction error of the form
O(σRW (cid:112)log(n)/m + RW log(n)/m) where R is an (cid:96)1 norm bound on w and W is an (cid:96)∞ bound on X, see e.g. [40, 41]. To interpret this, we can rescale the problem so that R, W = Θ(1). Then
Theorem 16 guarantees error on the order of the noise level σ2 using O(d log(n)) samples – in comparison, the standard slow rate result only guarantees error on the order of σ plus an additional term. This difference is the key to achieving structure recovery from O(log n) samples: σ can be orders of magnitude smaller compared to RW in our applications. Compared to (cid:96)0-constrained least squares, which requires runtime O(nd), the above result is computationally efﬁcient and still has the correct dependence on d and σ2.
General Models. There do exist some well-conditioned GGMs which are not walk-summable.
However, our analysis actually shows that our methods (GREEDYANDPRUNE, HYBRIDMB) also recover similar sample complexity bounds to [42] under their assumptions (the aforementioned (cid:96)∞ → (cid:96)∞ condition number bound) — see Theorem 19. Therefore, our results are a strict ex-tension of the situation considered in prior work.
Non-Gaussian Models.
It’s well-known that many results for Gaussian Graphical Models can be generalized to other distributions in the following sense: if we can learn a GGM with precision matrix Θ = Σ−1, then the result will generally extend to estimating Θ = Σ−1 for X with sufﬁciently strong concentration assumptions. The reason is that for any result which depends only on the ﬁrst 7
two moments of X (i.e. any quantity deﬁnable in terms of Σ, µ), we can generalize it to such an
X by considering the Gaussian with matching ﬁrst and second moments, and higher moments are generally needed only for concentration purposes.
We brieﬂy note that the guarantees for our algorithms will also extend in this sense if, for any w ∈ Rn, the sub-Gaussian constant of w · X is upper bounded by CVar(w · X) for a ﬁxed constant
C, as the needed concentration estimates generalize [43]. On the other hand, for non-Gaussian distributions the connection between Θ and conditional independence will not generally hold. 2.1 Further Discussion
GGMs vs Ising Models. There exist parallels but also surprisingly signiﬁcant differences between learning GGMs and Ising models. For Ising models, Bresler [31] gave a greedy algorithm that builds a superset of the neighborhood around each node and then prunes to learn the true graph structure using O(f (d) log n) samples and under some relatively mild assumptions. This greedy algorithm is able to perform structure learning in Ising models even when they exhibit long range correlations, which was previously considered a difﬁcult case to analyze. However in our setting, and unlike the previously described situation for Ising models, variables have real values and can have arbitrarily small or large variance. It turns out this changes the problem dramatically, as it means that the inter-node ﬂuctuations in the random ﬁeld (which contribute to Var(Xi)) may be orders of magnitude larger than the per-node ﬂuctuations (corresponding to Var(Xi|X∼i)). This is exactly the setting
σ (cid:28) RW discussed in the context of sparse linear regression.
As a result of this difference, greedy methods fail to learn general GGMs from O(polylog(n)) samples (see Appendix J), so any analysis of greedy methods must rely on structural results for a subclass of models. The same issue comes up when learning the model directly from (cid:96)1-constrained regression guarantees as in [44, 45] — in fact, we will see in Section I that natural methods based only on (cid:96)1 regularization fail even in some relatively simple attractive GGMs (where greedy works).
Sparse Linear Regression and Submodularity. As previously mentioned, Das and Kempe [32] studied the problem of sparse regression without assuming the restricted eigenvalue condition.
While in sparse regression, in order to learn the parameters accurately (in additive error) some bound on the condition number is needed, they studied the problem of selecting a subset of columns that maximizes squared multiple correlation (a.k.a. minimizes mean squared error). They then gave approximation guarantees for greedy algorithms under an approximate submodularity condition and assuming access to the true joint covariance matrix (in other words, they studied this as a purely algorithmic problem while ignoring sample complexity).
Our algorithm for attractive models follows the same supermodularity-based strategy, but has no knowledge of the true model except for the samples it sees. Therefore it requires a careful analysis of the interaction between the greedy iteration and noise. In the more general setting of walk-summable
GGMs, we show the conditional variance does not satisfy an approximate supermodularity condition with any constant submodularity ratio. (See Remark 7.)
Some other