Abstract
Recently, different machine learning methods have been introduced to tackle the challenging few-shot learning scenario that is, learning from a small labeled dataset related to a speciﬁc task. Common approaches have taken the form of meta-learning: learning to learn on the new problem given the old. Following the recognition that meta-learning is implementing learning in a multi-level model, we present a
Bayesian treatment for the meta-learning inner loop through the use of deep kernels.
As a result we can learn a kernel that transfers to new tasks; we call this Deep
Kernel Transfer (DKT). This approach has many advantages: is straightforward to implement as a single optimizer, provides uncertainty quantiﬁcation, and does not require estimation of task-speciﬁc parameters. We empirically demonstrate that DKT outperforms several state-of-the-art algorithms in few-shot classiﬁcation, and is the state of the art for cross-domain adaptation and regression. We conclude that complex meta-learning routines can be replaced by a simpler Bayesian model without loss of accuracy. 1

Introduction
One of the key differences between state-of-the-art machine learning methods, such as deep learning (LeCun et al., 2015; Schmidhuber, 2015), and human learning is that the former needs a large amount of data in order to ﬁnd relevant patterns across samples, whereas the latter acquires rich structural information from a handful of examples. Moreover, deep learning methods struggle in providing a measure of uncertainty, which is a crucial requirement when dealing with scarce data, whereas humans can effectively weigh up different alternatives given limited evidence. In this regard, some authors have suggested that the human ability for few-shot inductive reasoning could derive from a Bayesian inference mechanism (Steyvers et al., 2006; Tenenbaum et al., 2011). Accordingly, we argue that the natural interpretation of meta-learning as implementing learning in a hierarchical model, leads to a Bayesian equivalent through the use of deep kernel methods.
Deep kernels combine neural networks with kernels to provide scalable and expressive closed-form covariance functions (Hinton and Salakhutdinov, 2008; Wilson et al., 2016). If one has a large number of small but related tasks, as in few-shot learning, it is possible to deﬁne a common prior that induces knowledge transfer. This prior can be a deep kernel with parameters shared across tasks, so that given a new unseen task it is possible to effectively estimate the posterior distribution over a query set conditioned on a small support set. In a meta-learning framework (Hospedales et al., 2020) this 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
corresponds to a Bayesian treatment for the inner loop cycle. This is our proposed approach, which we refer to as deep kernel learning with transfer, or Deep Kernel Transfer (DKT) for short.
We derive two versions of DKT for both the regression and the classiﬁcation setting, comparing it against recent methods on a standardized benchmark environment; the code is released with an open-source license1. DKT has several advantages over other few-shot methods, which can be summarized as follows: 1. Simplicity and efﬁciency: it does not require any complex meta-learning optimization routines, it is straightforward to implement as a single optimizer as the inner loop is replaced by an analytic marginal likelihood computation, and it is efﬁcient in the low-data regime. 2. Flexibility: it can be used in a variety of settings such as regression, cross-domain and within-domain classiﬁcation, with state-of-the-art performance. 3. Robustness: it provides a measure of uncertainty with respect to new instances, that is crucial for a decision maker in the few-shot setting.
Main contributions: (i) a novel approach to deal with the few-shot learning problem through the use of deep kernels, (ii) an effective Bayesian treatment for the meta-learning inner-loop, and (iii) empirical evidence that complex meta-learning routines for few-shot learning can be replaced by a simpler hierarchical Bayesian model without loss of accuracy. 1.1 Motivation
The Bayesian meta-learning approach to the few-shot setting has predominantly followed the route of hierarchical modeling and multi-task learning (Finn et al., 2018; Gordon et al., 2019; Yoon et al., 2018).
The underlying directed graphical model distinguishes between a set of shared parameters θ, common to all tasks, and a set of N task-speciﬁc parameters ρt. Given a train dataset of tasks D = {Tt}N t=1, each one containing input-output pairs T = {(xl, yl)}L l=1, and given a test point x∗ from a new task T∗, learning consists of ﬁnding an estimate of θ, forming the posterior distribution over the task-speciﬁc parameters p(ρt|x∗, D, θ), and then computing the posterior predictive distribution p(y∗|x∗, θ). This approach is principled from a probabilistic perspective, but is problematic, as it requires managing two levels of inference via amortized distributions or sampling, often requiring cumbersome architectures.
In recent differentiable meta-learning methods, the two sets of parameters are learned by maximum likelihood estimation, by iteratively updating θ in an outer loop, and ρt in a inner loop (Finn et al., 2017). This case has various issues, since learning is destabilized by the joint optimization of two sets of parameters, and by the need to estimate higher-order derivatives (gradient of the gradient) for updating the weights (Antoniou et al., 2019).
To avoid these drawbacks we propose a simpler solution, that is marginalizing ρt over the data of a speciﬁc task. This marginalization is analytic and leads to a closed form marginal likelihood, which measures the expectedness of the data under the given set of parameters. By ﬁnding the parameters of a deep kernel we can maximize the marginal likelihood. Following our approach there is no need to estimate the posterior distribution over the task-speciﬁc parameters, meaning that it is possible to directly compute the posterior predictive distribution, skipping an intermediate inference step.
We argue that this approach can be very effective in the few-shot setting, signiﬁcantly reducing the complexity of the model with respect to meta-learning approaches, while retaining the advantages of
Bayesian methods (e.g. uncertainty estimation) with state-of-the-art performances. 2