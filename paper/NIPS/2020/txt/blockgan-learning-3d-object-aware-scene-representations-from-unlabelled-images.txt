Abstract
We present BlockGAN, an image generative model that learns object-aware 3D scene representations directly from unlabelled 2D images. Current work on scene representation learning either ignores scene background or treats the whole scene as one object. Meanwhile, work that considers scene compositionality treats scene objects only as image patches or 2D layers with alpha maps. Inspired by the computer graphics pipeline, we design BlockGAN to learn to ﬁrst generate 3D features of background and foreground objects, then combine them into 3D features for the whole scene, and ﬁnally render them into realistic images. This allows BlockGAN to reason over occlusion and interaction between objects’ appearance, such as shadow and lighting, and provides control over each object’s 3D pose and identity, while maintaining image realism. BlockGAN is trained end-to-end, using only unlabelled single images, without the need for 3D geometry, pose labels, object masks, or multiple views of the same scene. Our experiments show that using explicit 3D features to represent objects allows BlockGAN to learn disentangled representations both in terms of objects (foreground and background) and their properties (pose and identity). Our code is available at https://github.com/thunguyenphuoc/BlockGAN. 1

Introduction
The computer graphics pipeline has achieved impressive results in generating high-quality images, while offering users a great level of freedom and controllability over the generated images. This has many applications in creating and editing content for the creative industries, such as ﬁlms, games, scientiﬁc visualisation, and more recently, in generating training data for computer vision tasks. However, the current pipeline, ranging from generating 3D geometry and textures, rendering, compositing and image post-processing, can be very expensive in terms of labour, time, and costs.
Recent image generative models, in particular generative adversarial networks [GANs; 14], have greatly improved the visual ﬁdelity and resolution of generated images [5, 23, 24]. Conditional GANs
[36] allow users to manipulate images, but require labels during training. Recent work on unsupervised disentangled representations using GANs [9, 24, 38] relaxes this need for labels. The ability to produce high-quality, controllable images has made GANs an increasingly attractive alternative to the traditional graphics pipeline for content generation. However, most work focuses on property disentanglement, such as shape, pose and appearance, without considering the compositionality of the images, i.e., scenes being made up of multiple objects. Therefore, they do not offer control over individual objects in a way that respects the interaction of objects, such as consistent lighting and shadows. This is a major limitation of current image generative models, compared to the graphics pipeline, where 3D objects are modelled individually in terms of geometry and appearance, and combined into 3D scenes with consistent lighting. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Even when considering object compositionality, most approaches treat objects as 2D layers combined using alpha compositing [12, 50, 53]. Moreover, they also assume that each object’s appearance is inde-pendent [3, 6, 12]. While this layering approach has led to good results in terms of object separation and visual ﬁdelity, it is fundamentally limited by the choice of 2D representation. Firstly, it is hard to manipu-late properties that require 3D understanding, such as pose or perspective. Secondly, object layers tend to bake in appearance and cannot adequately represent view-speciﬁc appearance, such as shadows or mate-rial highlights changing as objects move around in the scene. Finally, it is non-trivial to model the appear-ance interactions between objects, such as scene lighting that affects objects’ shadows on a background.
We introduce BlockGAN, a generative adversarial network that learns 3D object-oriented scene repre-sentations directly from unlabelled 2D images. Instead of learning 2D layers of objects and combining them with alpha compositing, BlockGAN learns to generate 3D object features and to combine them into deep 3D scene features that are projected and rendered as 2D images. This process closely re-sembles the computer graphics pipeline where scenes are modelled in 3D, enabling reasoning over occlusion and interaction between object appearance, such as shadows or highlights. During test time, each object’s pose can be manipulated using 3D transforms directly applied to the object’s deep 3D fea-tures. We can also add new objects and remove existing objects in the generated image by changing the number of 3D object features in the 3D scene features at inference time. This shows that BlockGAN has learnt a non-trivial representation of objects and their interaction, instead of merely memorizing images.
BlockGAN is trained end-to-end in an unsupervised manner directly from unlabelled 2D images, without any multi-view images, paired images, pose labels, or 3D shapes. We experiment with
BlockGAN on a variety of synthetic and natural image datasets. In summary, our main contributions are:
• BlockGAN, an unsupervised image generative model that learns an object-aware 3D scene representation directly from unlabelled 2D images, disentangling both between objects and individual object properties (pose and identity);
• showing that BlockGAN can learn to separate objects even from cluttered backgrounds; and
• demonstrating that BlockGAN’s object features can be added, removed and manipulated to create novel scenes that are not observed during training. 2