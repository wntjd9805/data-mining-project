Abstract
We study the problem of efﬁcient adversarial attacks on tree based ensembles such as gradient boosting decision trees (GBDTs) and random forests (RFs). Since these models are non-continuous step functions and gradient does not exist, most existing efﬁcient adversarial attacks are not applicable. Although decision-based black-box attacks can be applied, they cannot utilize the special structure of trees. In our work, we transform the attack problem into a discrete search problem specially designed for tree ensembles, where the goal is to ﬁnd a valid “leaf tuple” that leads to mis-classiﬁcation while having the shortest distance to the original input.
With this formulation, we show that a simple yet effective greedy algorithm can be applied to iteratively optimize the adversarial example by moving the leaf tuple to its neighborhood within hamming distance 1. Experimental results on several large GBDT and RF models with up to hundreds of trees demonstrate that our method can be thousands of times faster than the previous mixed-integer linear programming (MILP) based approach, while also providing smaller (better) adversarial examples than decision-based black-box attacks on general (cid:96)p (p = 1, 2, ∞) norm perturbations. Our code is available at https://github.com/ chong-z/tree-ensemble-attack. 1

Introduction
It has been widely studied that machine learning models are vulnerable to adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2015; Athalye et al., 2018), where a small imperceptible perturbation on the input can easily alter the prediction of a model. A series of adversarial attack methods have been proposed on continuous models such as neural networks, which can be generally split into two types. The gradient based methods formulate the attack into an optimization problem on a specially designed loss function for attacks, where the gradient can be acquired through either back-propagation in the white-box setting (Carlini, Wagner, 2017; Madry et al., 2018), or numerical estimation in the soft-label black-box setting (Chen et al., 2017; Tu et al., 2018; Ilyas et al., 2018).
The decision based (or hard-label black-box) methods only have access to the output label, which usually starts with an initial adversarial example and minimizes the perturbation along the decision boundary (Brendel et al., 2018; Brunner et al., 2018; Cheng et al., 2019, 2020; Chen et al., 2019c).
In this paper we study the problem of efﬁcient adversarial attack on tree based ensembles such as gradient boosting decision trees (GBDT) and random forests (RFs), which have been widely used in practice (Chen, Guestrin, 2016; Ke et al., 2017; Zhang et al., 2017; Prokhorenkova et al., 2018).
We minimize the perturbation to ﬁnd the smallest possible attack, to uncover the true weakness of a model. Different from neural networks, tree based ensembles are non-continuous step functions and existing gradient based methods are not applicable. Decision based methods can be applied but they usually require a large number of queries and may easily fall into local optimum due to rugged decision boundary. In general, ﬁnding the exact minimal adversarial perturbation for tree ensembles is NP-complete (Kantchelian et al., 2015), and a feasible approximation solution is necessary to evaluate the robustness of large ensembles. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The major difﬁculty of attacking tree ensembles is that the prediction remains unchanged within regions on the input space, where the region could be large and makes continuous updates inef-ﬁcient. To overcome this difﬁculty, we transform the continuous Rd input space into a discrete
{1, 2, . . . , N }K “leaf tuple” space, where N is the number of leaves per tree and K is the number of trees. On the leaf tuple space we deﬁne the distance between two input examples to be the number of trees that have different prediction leaves (i.e., hamming distance), and deﬁne the neighborhood of a tuple to be all valid tuples within a small hamming distance. In practice, we propose the attack that iteratively optimizes the adversarial leaf tuple by moving it to the best adversarial tuple within the neighborhood of distance 1. Intuitively we could reach a far away adversarial tuple through a series of smaller updates, based on the fact that each tree makes prediction independently.
In experiments, we compare (cid:96)1,2,∞ norm perturbation metrics across 10 datasets, and show that our method is thousands of times faster than MILP (Kantchelian et al., 2015) on most of the large ensembles, and 3∼72x faster than decision based and empirical attacks on all datasets while achieving a smaller distortion. For instance, with the standard (natural) GBDT on the MNIST dataset with 10 classes and 200 trees per class, our method ﬁnds the adversarial example with only 2.07 times larger (cid:96)∞ perturbation than the optimal solution produced by MILP and only uses 0.237 seconds per test example, whereas MILP requires 375 seconds. As for other approximate attacks, SignOPT (Cheng et al., 2020) ﬁnds a 13.93 times larger (cid:96)∞ perturbation (compared to MILP) using 3.7 seconds,
HSJA (Chen et al., 2019c) achieves a 8.36 times larger (cid:96)∞ perturbation using 1.8 seconds, and
Cube (Andriushchenko, Hein, 2019) achieves a 4 times larger (cid:96)∞ perturbation using 4.42 seconds.
Additionally, although (cid:96)p distance is widely used in previous attacks and a small (cid:96)p perturbation is usually invisible, our method is general and can also be adapted to other distance metrics. 2