Abstract
Recent approaches for modelling dynamics of physical systems with neural net-works enforce Lagrangian or Hamiltonian structure to improve prediction and generalization. However, when coordinates are embedded in high-dimensional data such as images, these approaches either lose interpretability or can only be applied to one particular example. We introduce a new unsupervised neural network model that learns Lagrangian dynamics from images, with interpretability that beneﬁts prediction and control. The model infers Lagrangian dynamics on generalized coordinates that are simultaneously learned with a coordinate-aware variational autoencoder (VAE). The VAE is designed to account for the geometry of physical systems composed of multiple rigid bodies in the plane. By inferring interpretable
Lagrangian dynamics, the model learns physical system properties, such as kinetic and potential energy, which enables long-term prediction of dynamics in the image space and synthesis of energy-based controllers. 1

Introduction
Humans can learn to predict the trajectories of mechanical systems, e.g., a basketball or a drone, from high-dimensional visual input, and learn to control the system, e.g., catch a ball or maneuver a drone, after a small number of interactions with those systems. We hypothesize that humans use domain-speciﬁc knowledge, e.g., physics laws, to achieve efﬁcient learning. Motivated by this hypothesis, in this work, we propose incorporation of physics priors to learn and control dynamics from image data, aiming to gain interpretability and data efﬁciency. Speciﬁcally, we incorporate Lagrangian dynamics as the physic prior, which enables us to represent a broad class of physical systems. Recently, an increasing number of works [1, 2, 3, 4, 5] have incorporated Lagrangian/Hamiltonian dynamics into learning dynamical systems from coordinate data, to improve prediction and generalization.
These approaches, however, require coordinate data, which are not always available in real-world applications. Hamiltonian Neural Network (HNN) [3] provides a single experiment with image observations, which requires a modiﬁcation in the model. This modiﬁcation is hard to generalize to systems with multiple rigid bodies. Hamiltonian Generative Network (HGN) [6] learns Hamiltonian dynamics from image sequences. However, the dimension of latent generalized coordinates is 4 × 4 × 16 = 256, making interpretation difﬁcult. Moreover, both HNN and HGN focus on prediction and have no design of control. Another class of approaches learn physical models from images, by either learning the map from images to coordinates with supervision on coordinate data [7] or learning the coordinates in an unsupervised way but only with translational coordinates [8, 9]. The unsupervised learning of rotational coordinates such as angles are under-explored in the literature.
In this work, we propose an unsupervised neural network model that learns coordinates and La-grangian dynamics on those coordinates from images of physical systems in motion in the plane. The latent dynamical model enforces Lagrangian dynamics, which beneﬁts long term prediction of the system. As Lagrangian dynamics commonly involve rotational coordinates to describe the changing 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
conﬁgurations of objects in the system, we propose a coordinate-aware variational autoencoder (VAE) that can infer interpretable rotational and translational coordinates from images without supervision.
The interpretable coordinates together with the interpretable Lagrangian dynamics pave the way for introducing energy-based controllers of the learned dynamics. 1.1