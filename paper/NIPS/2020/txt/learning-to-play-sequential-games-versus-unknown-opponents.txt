Abstract
We consider a repeated sequential game between a learner, who plays ﬁrst, and an opponent who responds to the chosen action. We seek to design strategies for the learner to successfully interact with the opponent. While most previous approaches consider known opponent models, we focus on the setting in which the opponent’s model is unknown. To this end, we use kernel-based regularity assumptions to capture and exploit the structure in the opponent’s response. We propose a novel algorithm for the learner when playing against an adversarial sequence of opponents. The algorithm combines ideas from bilevel optimization and online learning to effectively balance between exploration (learning about the opponent’s model) and exploitation (selecting highly rewarding actions for the learner). Our results include algorithm’s regret guarantees that depend on the regularity of the opponent’s response and scale sublinearly with the number of game rounds. More-over, we specialize our approach to repeated Stackelberg games, and empirically demonstrate its effectiveness in a trafﬁc routing and wildlife conservation task. 1

Introduction
Several important real-world problems involve sequential interactions between two parties. These problems can often be modeled as two-player games, where the ﬁrst player chooses a strategy and the second player responds to it. For example, in trafﬁc networks, trafﬁc operators plan routes for a subset of network vehicles (e.g., public transport), while the remaining vehicles (e.g., private cars) can choose their routes in response to that. The goal of the ﬁrst player in these games is to ﬁnd the optimal strategy (e.g., trafﬁc operators seek the routing strategy that minimizes the overall network’s congestion, cf., [20]). Several algorithms have been previously proposed, successfully deployed, and used in domains such as urban roads [17], airport security [29], wildlife protection [40], and markets
[15], to name a few.
In many applications, complete knowledge of the game is not available and, thus, ﬁnding a good strategy for the ﬁrst player becomes more challenging. The response function of the second player, that is, how the second player responds to strategies of the ﬁrst player, is typically unknown and can only be inferred by repeatedly playing and observing the responses and game outcomes [22, 6].
Consequently, we refer to the ﬁrst and second players as learner and opponent, respectively. An additional challenge for the learner in such repeated games lies in facing a potentially different type of opponent at every game round. In various domains (e.g., in security applications), the learner can even face an adversarially chosen sequence of opponent/attacker types [4].
Motivated by these important considerations, we study a repeated sequential game against an un-known opponent with multiple types. We propose a novel algorithm for the learner when facing an adversarially chosen sequence of types. No-regret guarantees of our algorithm in these settings ensure that the learner’s performance converges to the optimal one in hindsight (i.e., the idealized scenario in which the types’ sequence and opponent’s response function are known ahead of time). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To that end, our algorithm learns the opponent’s response function online, and gradually improves the learner’s strategy throughout the game.