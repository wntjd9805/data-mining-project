Abstract
Consider an oracle which takes a point x and returns the minimizer of a convex function f in an (cid:96)2 ball of radius r around x. It is straightforward to show that roughly r−1 log 1 (cid:15) calls to the oracle sufﬁce to ﬁnd an (cid:15)-approximate minimizer of f in an (cid:96)2 unit ball. Perhaps surprisingly, this is not optimal: we design an accelerated algorithm which attains an (cid:15)-approximate minimizer with roughly r−2/3 log 1 (cid:15) oracle queries, and give a matching lower bound. Further, we implement ball optimization oracles for functions with locally stable Hessians using a variant of
Newton’s method and, in certain cases, stochastic ﬁrst-order methods. The resulting algorithm applies to a number of problems of practical and theoretical import, improving upon previous results for logistic and (cid:96)∞ regression and achieving guarantees comparable to the state-of-the-art for (cid:96)p regression. 1

Introduction
We study unconstrained minimization of a smooth convex objective f : Rd → R, which we access through a ball optimization oracle Oball, that when queried at any point x, returns the minimizer of f restricted a ball of radius r around x, i.e.,1
Oball(x) = arg min x(cid:48) s.t. (cid:107)x(cid:48)−x(cid:107)≤r f (x(cid:48)).
Such oracles underlie trust-region methods [15] and, as we demonstrate via applications, encapsulate problems with local stability. Iterating xk+1 ← Oball(xk) minimizes f in (cid:101)O(R/r) iterations (see Ap-pendix A), where R is the initial distance to the minimizer, x∗, and (cid:101)O(·) hides polylogarithmic factors in problem parameters, including the desired accuracy.
Given the fundamental geometric nature of the ball optimization abstraction, the central question motivating our work is whether it is possible to improve upon this (cid:101)O(R/r) query complexity. It is natural to conjecture that the answer is negative: we require R/r oracle calls to observe the entire line from x0 to the optimum, and therefore ﬁnding a solution using less queries would require jumping into completely unobserved regions. Nevertheless, we prove that the optimal query complexity scales as (R/r)2/3. This result has positive implications for the complexity for several key regression tasks, for which we can efﬁciently implement the ball optimization oracles. 1.1 Our contributions
We overview our main contributions: accelerating ball optimization oracles (with a matching lower bound), implementing them under Hessian stability, and applying our results to regression problems.
∗Stanford University, {yairc,jmblpati,qjiang2,yujiajin,sidford,kjtian}@stanford.edu.
†Tel Aviv University, ycarmon@cs.tau.ac.il.
‡University of Washington, yintat@uw.edu. 1In the introduction we discuss exact oracles for simplicity, but our results account for inexactness. Our x(cid:62)Mx for M (cid:23) 0, which we sometimes
√ results hold for any weighted Euclidean (semi)norm, i.e., (cid:107)x(cid:107) = write explicitly as (cid:107)x(cid:107)M. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Monteiro-Svaiter (MS) oracles via ball optimization. Our starting point is an acceleration frame-work due to Monteiro and Svaiter [25]. It relies on access to an oracle that when queried with x, v ∈ Rd and A > 0, returns points x+, y ∈ Rd and λ > 0 such that y =
A
A + aλ x+ ≈ arg min x(cid:48)∈Rd x + (cid:26) aλ
A + aλ 1 2λ f (x(cid:48)) + v, and (cid:107)x(cid:48) − y(cid:107)2 (cid:27)
, (1) (2)
√ 2 (λ + where aλ = 1
λ2 + 4Aλ). Basic calculus shows that for any z, the radius-r oracle response
Oball(z) solves the proximal point problem (2) for y = z and some λ = λ(cid:63) r(z) ≥ 0 which depends on r and z. Therefore, to implement the MS oracle with a ball optimization oracle, given query (x, v, A) we need to ﬁnd λ that solves the implicit equation λ = λ(cid:63) r(y(λ)), with y(λ) as in (1). We solve this equation to sufﬁcient accuracy via binary search over λ, resulting in an accelerated scheme that makes (cid:101)O(1) queries to Oball(·) per iteration (each iteration also requires a gradient evaluation).
The main challenge lies in proving that our MS oracle implementation guarantees rapid convergence.
We do so by a careful analysis which relates convergence to the distance between the MS oracle outputs y and x+. Speciﬁcally, letting {yk, xk+1} be the sequence of these points, we prove that f (xK) − f (x∗) f (x0) − f (x∗) (cid:26)
≤ exp
−Ω(K) min k<K (cid:107)xk+1 − yk(cid:107)2/3
R2/3 (cid:27)
.
Since Oball guarantees (cid:107)xk+1 − yk(cid:107) = r for all k except possibly the last, our result follows.
Matching lower bound. We give a distribution over functions with domain of size R for which any algorithm interacting with a ball optimization oracle of radius r requires Ω((R/r)2/3) queries to ﬁnd an approximate solution with O(r1/3) additive error. Our lower bound in fact holds for an even more powerful r-local oracle, which reveals all values of f in a ball of radius r around the query point. We prove our lower bounds using well-established techniques and Nemirovski’s function, a canonical hard instance in convex optimization [26, 30, 12, 17, 9]. Here, our primary contribution is to show that appropriately scaling this construction makes it hard even against r-local oracles with a ﬁxed radius r, as opposed to the more standard notion of local oracles that reveal the instance only in an arbitrarily small neighborhood around the query.
Ball optimization oracle implementation. Trust-region methods [15] solve a sequence of subprob-lems of the form minimize
δ∈Rd s.t. (cid:107)δ(cid:107)≤r (cid:110)
δ(cid:62)g +
δ(cid:62)Hδ (cid:111)
. 1 2
When g = ∇f (x) and H = ∇2f (x), the trust-region subproblem minimizes a second-order Taylor expansion of f around x, implementing an approximate ball optimization oracle. We show how to implement a ball optimization oracle for f to high accuracy for functions satisfying a local Hessian stability property. Speciﬁcally, we use a notion of Hessian stability similar to that of Karimireddy et al. [22], requiring 1 c ∇2f (x) (cid:22) ∇2f (y) (cid:22) c∇2f (x) for every y in a ball of radius r around x for some c > 1. We analyze Nesterov’s accelerated gradient method in a Euclidean norm weighted by the Hessian at x, which we can also view as accelerated Newton steps, and show that it implements the oracle in (cid:101)O(c) linear system solutions, improving upon the c2 dependence of more naive methods.
This improvement is not necessary for our applications where we take c to be a constant, but we include it for completeness. For certain objectives (e.g., softmax), we show that a ﬁrst-order oracle implementation (e.g., computing the Newton steps with accelerated SVRG) allows us to further exploit the problem structure, and improve state-of-the-art runtimes guarantees in some regimes.
Applications. We apply our implementation and acceleration of ball optimization oracles to problems of the form f (Ax − b) for data matrix A ∈ Rn×d. For logistic regression, where f (z) = (cid:80) i∈[n] log(1 + e−zi), Hessian stability implies [4] that our algorithm solves the problem with (cid:101)O((cid:107)x0 − x∗(cid:107)2/3
A(cid:62)A) linear system solves of the form A(cid:62)DAx = z for diagonal D. This improves upon the previous best linearly-convergent condition-free algorithm due to Karimireddy et al. [22], which requires (cid:101)O((cid:107)x0 − x∗(cid:107)A(cid:62)A) system solves. Our improvement is precisely the power 2/3 factor that comes from acceleration using the ball optimization oracle.
For (cid:96)∞ regression, we take f to be the log-sum-exp (softmax) function and establish that it too has a stable Hessian. By appropriately scaling softmax to approximate (cid:96)∞ regression to (cid:15) additive 2
error and taking r = (cid:15), our method solves (cid:96)∞ to additive error (cid:15) in (cid:101)O((cid:107)x0 − x∗(cid:107)2/3
A(cid:62)A(cid:15)−2/3) linear system solves of the same form as above. This improves upon the algorithm of Bullins and Peng
[11] in terms of (cid:15) scaling (from (cid:15)−4/5 to (cid:15)−2/3) and the algorithm of Ene and Vladu [18] in terms of distance scaling (from n1/3(cid:107)A(x0 − x∗)(cid:107)2/3
). We also give a runtime guarantee improving over the state-of-the-art ﬁrst-order method of Carmon et al. [13] whenever d ≥ ( maxi(cid:107)ai(cid:107)2R n
)2/3 ≥ d where R is the (cid:96)2 distance between an initial point and the optimizer, by using a ﬁrst-order oracle implementation based on [5].
∞ to (cid:107)A(x0 − x∗)(cid:107)2/3 2 (cid:15)
Finally, we leverage our framework to obtain high accuracy solutions to (cid:96)p norm regression, where f (z) = (cid:80) i∈[n] |zi|p, via minimizing a sequence of proximal problems with geometrically shrinking regularization. The result is an algorithm that solves (cid:101)O(poly(p)n1/3) linear systems. For p = ω(1), this matches the state-of-the-art n dependence [1] but obtains worse dependence on p. Nevertheless, we provide a straightforward alternative approach to prior work and our results leave room for further reﬁnements which we believe may result in stronger guarantees. 1.2