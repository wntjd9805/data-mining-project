Abstract
We study the online restless bandit problem, where the state of each arm evolves according to a Markov chain, and the reward of pulling an arm depends on both the pulled arm and the current state of the corresponding Markov chain. In this paper, we propose Restless-UCB, a learning policy that follows the explore-then-commit framework. In Restless-UCB, we present a novel method to construct ofﬂine instances, which only requires O(N ) time-complexity (N is the number of arms) and is exponentially better than the complexity of existing learning policy. We also prove that Restless-UCB achieves a regret upper bound of ˜O((N + M 3)T 2 3 ), where M is the Markov chain state space size and T is the time horizon. Compared to existing algorithms, our result eliminates the exponential factor (in M, N ) in the regret upper bound, due to a novel exploitation of the sparsity in transitions in general restless bandit problems. As a result, our analysis technique can also be adopted to tighten the regret bounds of existing algorithms. Finally, we conduct experiments based on real-world dataset, to compare the Restless-UCB policy with state-of-the-art benchmarks. Our results show that Restless-UCB outperforms existing algorithms in regret, and signiﬁcantly reduces the running time. 1

Introduction
The restless bandit problem is a time slotted game between a player and the environment [50]. In this problem, there are N arms (or actions), and the state of each arm i evolves according to a Markov chain Mi, which makes one transition per time slot during the game (regardless of being pulled or not). At each time slot t, the player chooses one arm to pull. If he pulls arm i, he observes the current state si(t) of Mi, and receives a random reward xi(t) that depends on i and si(t), i.e.,
E[xi(t)] = r(i, si(t)) for some function r. The goal of the player is to maximize his expected cumulative reward during the time horizon T , i.e., E[(cid:80)T t=1 xa(t)(t)], where a(t) ∈ [N ] denotes the pulled arm at time step t.
Restless bandit can model many important applications. For instance, in a job allocation problem, an operator allocates jobs to N different servers. The state of each server, i.e., the number of background jobs currently running at the server, can be modeled by a Markov chain, and it changes every time slot according to an underlying transition matrix [32, 20]. At each time slot, the operator allocates a job to one server, and receives the reward from that server, i.e., whether the job is completed, which depends on the current state of the server. At the same time, the operator can determine the current state of the chosen server based on its feedback. For servers that are not assigned jobs at the current 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
time slot, however, the operator does not observe their current state or transitions. The operator’s objective is to maximize his cumulative reward in the T time slots.
Another application of the restless bandit model is in wireless communication. In this scenario, a base-station (player) transmits packets over N distinct channels to users. Each channel may be in
“good” or “bad” state due to the channel fading condition, which evolves according to a two-state
Markov chain [37, 40]. Every time, the player chooses one channel for packet transmission. If the transmission is successful, the player gets a reward of 1. The player also learns about the state of the channel based on receiver feedback. The goal of the player is to maximize his cumulative reward, i.e., deliver as many packets as possible, within the given period of time.
Most existing works on restless bandit focus on the ofﬂine setting, i.e., all parameters of the game are known to the player, e.g., [50, 48, 31, 8, 44]. In this setting, the objective is to search for the best policy of the player. In practice, one often cannot have the full system information beforehand. Thus, traditional solutions instead choose to solve the ofﬂine problem using empirical parameter values.
However, due to the increasing sizes of the problem instances in practice, a small error on parameter estimation can lead to a large error, i.e., regret.
The online restless bandit setting, where parameters have to be learned online, has been gaining attention, e.g., [42, 13, 26, 23, 22, 33]. However, many challenges remain unsolved. First of all, existing policies may not perform close to the optimal ofﬂine one, e.g., [26] only considers the best policy that constantly pulls one arm. Second, for the class of Thompson Sampling based algorithms, e.g., [23, 22], theoretical guarantees are often established in the Bayesian setting, where the update methods can be computationally expensive when the likelihood functions are complex, especially for prior distributions with continuous support. Third, the existing policy with theoretical guarantee of a sublinear regret upper bound, i.e., colored-UCRL2 [33], suffers from an exponential computation complexity and a regret bound that is exponential in the numbers of arms and states, as it requires solving a set Bellman equations with an exponentially large space set.
In this paper, we aim to tackle the high computation complexity and exponential factor in the regret bounds for online restless bandit. Speciﬁcally, we consider a class of restless bandit problems with birth-death state Markov chains, and develop online algorithms to achieve a regret bound that is only polynomial in the numbers of arms and states. We emphasize that, birth-death Markov chains have been widely used to model real-world applications, e.g., queueing systems [24] and wireless communication [46], and are generalization of the two-state Markov chain assumption often made in prior works on restless bandits, e.g., [17, 28]. Our model can also be applied to many important applications, e.g., communications [3, 2], recommendation systems [30] and queueing systems [4].
The main contributions of this paper are summarized as follows:
• We consider a general class of online restless bandit problems with birth-death Markov structures and propose the Restless-UCB policy. Restless-UCB contains a novel method for constructing ofﬂine instances in guiding action selection, and only has an O(N ) complexity (N is the number of arms), which is exponentially better than that of colored-UCRL2, the state-of-the-art policy with theoretical guarantee [33] for online restless bandits.
• We devise a novel analysis and prove that Restless-UCB achieves an ˜O((N + M 3)T 2 3 ) regret, where M is the Markov chain state space size and T is the time horizon. Our bound improves upon existing regret bounds in [33, 22], which are exponential in N, M . The novelty of our analysis lies in the exploitation of the sparsity in general restless bandit problems, i.e., each belief state can only transit to M other ones. This approach can also be combined with the analysis in [33, 22] to reduce the exponential factors in the regret bound to polynomial values (complexity remains exponential) in online restless bandit problems.
Thus, our analysis can be of independent interest in online restless bandit analysis.
• We show that Restless-UCB can be combined with an efﬁcient ofﬂine approximation oracle to guarantee O(N ) time-complexity and an ˜O(T 2 3 ) approximation regret upper bound.
Note that existing algorithms suffer from either an exponential complexity or no theoretical guarantee even with an efﬁcient approximation oracle.
• We conduct experiments based on real-world datasets, and compare our policy with existing benchmarks. Our results show that Restless-UCB outperforms existing algorithms in both regret and running time. 2
1.1