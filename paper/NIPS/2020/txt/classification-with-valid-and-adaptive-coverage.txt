Abstract
Conformal inference, cross-validation+, and the jackknife+ are hold-out methods that can be combined with virtually any machine learning algorithm to construct prediction sets with guaranteed marginal coverage. In this paper, we develop specialized versions of these techniques for categorical and unordered response labels that, in addition to providing marginal coverage, are also fully adaptive to complex data distributions, in the sense that they perform favorably in terms of approximate conditional coverage compared to alternative methods. The heart of our contribution is a novel conformity score, which we explicitly demonstrate to be powerful and intuitive for classiﬁcation problems, but whose underlying principle is potentially far more general. Experiments on synthetic and real data demonstrate the practical value of our theoretical guarantees, as well as the statistical advantages of the proposed methods over the existing alternatives. 1

Introduction i=1 with features Xi ∈ Rp and a discrete label Yi ∈
Imagine we have n data samples {(Xi, Yi)}n
Y = {1, 2, . . . , C}. The samples are drawn exchangeably (e.g., i.i.d., although exchangeability alone is sufﬁcient) from some unknown distribution PXY . Given such data and a desired coverage level 1 − α ∈ (0, 1), we seek to construct a prediction set ˆCn,α ⊆ Y for the unseen label of a new data point (Xn+1, Yn+1), also drawn exchangeably from PXY , achieving marginal coverage; that is, obeying
P (cid:104)
Yn+1 ∈ ˆCn,α(Xn+1) (cid:105)
≥ 1 − α. (1)
The probability above is taken over all n + 1 data points, and we ask that (1) holds for any ﬁxed
α, n, and PXY . While marginal coverage has the advantage of being both desirable and practically achievable, it unfortunately does not imply the stronger notion of conditional coverage: (cid:104)
P
Yn+1 ∈ ˆCn,α(x) | Xn+1 = x (cid:105)
≥ 1 − α. (2)
The latter asks for valid coverage conditional on a speciﬁc observed value of the features X. It is already known that conditional coverage cannot be achieved in theory without strong modeling
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
assumptions [1, 23], which we are not willing to make in this paper. That said, it is undeniable that conditional coverage would be preferable. We thus seek to develop classiﬁcation methods that are provably valid in the marginal sense (1) and also attempt to sensibly approximate conditional coverage (16). At the same time, we want powerful predictions, in the sense that the cardinality of ˆC should be as small as possible. 1.1 The oracle classiﬁer
Imagine we have an oracle with perfect knowledge of the conditional distribution PY |X of Y given
X. This would of course give the problem away; to be sure, we would deﬁne optimal prediction sets (Xn+1) with conditional coverage as follows: for any x ∈ Rp, set πy(x) = P[Y = y | X = x]
Coracle
α for each y ∈ Y. Denote by π(1)(x) ≥ π(2)(x) ≥ . . . ≥ π(C)(x) the order statistics for πy(x). For simplicity, let us assume for now that there are no ties; we will relax this assumption shortly. For any
τ ∈ [0, 1], deﬁne the generalized conditional quantile function2
L(x; π, τ ) = min{c ∈ {1, . . . , C} : π(1)(x) + π(2)(x) + . . . + π(c)(x) ≥ τ }, and the prediction set:
C oracle+
α (x) = {‘y’ indices of the L(x; π, 1 − α) largest πy(x)} . (3) (4)
Hence, (4) is the smallest deterministic set that contains a response with feature values X = x with probability at least 1 − α. For example, if π1(x) = 0.3, π2(x) = 0.6, and π3(x) = 0.1, we have
π(1)(x) = 0.6, π(2)(x) = 0.3, and π(3)(x) = 0.1, with L(x, 0.9) = 2, C oracle 0.1 (x) = {1, 2}, and
L(x, 0.5) = 1, C oracle 0.5 (x) = {2}. Furthermore, deﬁne a function S with input x, u ∈ [0, 1], π, and τ , which computes the set of most likely labels up to (but possibly excluding) the one identiﬁed by (3):
S(x, u; π, τ ) = (cid:26) ‘y’ indices of the L(x; π, τ ) − 1 largest πy(x),
‘y’ indices of the L(x; π, τ ) largest πy(x), if u ≤ V (x; π, τ ), otherwise, (5) where
V (x; π, τ ) = 1
π(L(x;π,τ ))(x)
L(x;π,τ ) (cid:88)



π(c)(x) − τ
 . c=1
With this in place, by letting u be the realization of a uniform random variable, we can see that the oracle has access to tighter randomized prediction sets, namely,
C oracle
α (x) = S(x, U ; π, 1 − α). (6)
Above, U ∼ Uniform(0, 1) is independent of everything else. It is easy to verify that the sets in (6) are the smallest randomized prediction sets with conditional coverage at level 1 − α. In the above example, we would have C oracle 0.5 (x) = {2} otherwise. Finally, if there are any ties among the class probabilities, the oracle could simply break them at random. Of course, we do not have access to such an oracle since PY |X is unknown. 0.5 (x) = ∅ with probability (0.6 − 0.5)/0.6 = 1/6 and C oracle 1.2 Preview of our methods
This paper uses classiﬁers trained on the available data to approximate the unknown conditional distribution of Y | X. A key strength of the proposed methods is their ability to work with any black-box predictive model, including neural networks, random forests, support vector classiﬁers, or any other currently existing or possible future alternatives. The only restriction on the training algorithm is that it should treat all samples exchangeably; i.e., it should be invariant to their order.
Most off-the-shelf tools offer such suitable probability estimates ˆπy(x) that we can exploit, regardless of whether they are well-calibrated, by imputing them into an algorithm inspired by the oracle from
Section 1.1 in order to obtain prediction sets with guaranteed coverage—as we shall see.
Our reader will understand that naively substituting πy(x) with ˆπy(x) into the oracle procedure would yield predictions lacking any statistical guarantees because ˆπy(x) may be a poor approximation of
πy(x). Fortunately, we can automatically account for errors in ˆπy(x) by adaptively choosing the 2Recall that the conditional quantiles for continuous responses are: inf{y ∈ R : P[Y ≤ y | X = x] ≥ τ }. 2
threshold τ in (3) in such a way as to guarantee ﬁnite-sample coverage on future test points. The intuition is that setting τ = 1 − α may not necessarily guarantee coverage at level 1 − α for future test points, if ˆπ (cid:54)= π. However, we can compute the empirical coverage on hold-out data as a function of τ , and then select the smallest value of τ that leads to the desired 1 − α coverage. Below, we will show that this adaptive tuning rigorously yields tight coverage. 1.3