Abstract
We propose PLLay, a novel topological layer for general deep learning models based on persistence landscapes, in which we can efﬁciently exploit the underlying topological features of the input data structure. In this work, we show differentia-bility with respect to layer inputs, for a general persistent homology with arbitrary
ﬁltration. Thus, our proposed layer can be placed anywhere in the network and feed critical information on the topological features of input data into subsequent layers to improve the learnability of the networks toward a given task. A task-optimal structure of PLLay is learned during training via backpropagation, without requiring any input featurization or data preprocessing. We provide a novel adap-tation for the DTM function-based ﬁltration, and show that the proposed layer is robust against noise and outliers through a stability analysis. We demonstrate the effectiveness of our approach by classiﬁcation experiments on various datasets. 1

Introduction
With its strong generalizability, deep learning has been pervasively applied in machine learning. To improve the learnability of deep learning models, various techniques have been proposed. Some of them have achieved an efﬁcient data processing method through specialized layer structures; for instance, inserting a convolutional layer greatly improves visual object recognition and other tasks in computer vision [e.g., Krizhevsky et al., 2012, LeCun et al., 2016]. On the other hand, a large body of recent work focuses on optimal architecture of deep network [Simonyan and Zisserman, 2015, He et al., 2016, Szegedy et al., 2015, Albelwi and Mahmood, 2016].
In this paper, we explore an alternative way to enhance the learnability of deep learning models by developing a novel topological layer which feeds the signiﬁcant topological features of the underlying data structure in an arbitrary network. The power of topology lies in its capacity which differentiates sets in topological spaces in a robust and meaningful geometric way [Carlsson, 2009, Ghrist, 2008].
It provides important insights into the global "shape" of the data structure via persistent homology 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of PLLay, a novel topological layer based on weighted persistence landscapes.
Information in the persistence diagram is ﬁrst encoded into persistence landscapes as a form of vectorized function, and then a deep learning model determines which components of the landscape (e.g., particular hills or valleys) are important for a given task during training. PLLay can be placed anywhere in the network.
[Zomorodian and Carlsson, 2005]. The use of topological methods in data analysis has been limited by the difﬁculty of combining the main tool of the subject, persistent homology, with statistics and machine learning. Nonetheless, a series of recent studies have reported notable successes in utilizing topological methods in data analysis [e.g., Zhu, 2013, Dindin et al., 2020, Nanda and Sazdanovi´c, 2014, Tralie and Perea, 2018, Seversky et al., 2016, Gamble and Heo, 2010, Pereira and de Mello, 2015, Umeda, 2017, Liu et al., 2016, Venkataraman et al., 2016, Emrani et al., 2014]
There are at least three beneﬁts of utilizing the topological layer in deep learning; 1) we can efﬁciently extract robust global features of input data that otherwise would not be readily accessible via traditional feature maps, 2) an optimal structure of the layer for a given task can be easily embodied via backpropagation during training, and 3) with proper ﬁltrations it can be applied to arbitrarily complicated data structure even without any data preprocessing.