Abstract
Regularization and transfer learning are two popular techniques to enhance model generalization on unseen data, which is a fundamental problem of machine learning.
Regularization techniques are versatile, as they are task- and architecture-agnostic, but they do not exploit a large amount of data available. Transfer learning methods learn to transfer knowledge from one domain to another, but may not generalize across tasks and architectures, and may introduce new training cost for adapting to the target task. To bridge the gap between the two, we propose a transferable perturbation, MetaPerturb, which is meta-learned to improve generalization per-formance on unseen data. MetaPerturb is implemented as a set-based lightweight network that is agnostic to the size and the order of the input, which is shared across the layers. Then, we propose a meta-learning framework, to jointly train the perturbation function over heterogeneous tasks in parallel. As MetaPerturb is a set-function trained over diverse distributions across layers and tasks, it can generalize to heterogeneous tasks and architectures. We validate the efﬁcacy and generality of MetaPerturb trained on a speciﬁc source domain and architecture, by applying it to the training of diverse neural architectures on heterogeneous target datasets against various regularizers and ﬁne-tuning. The results show that the networks trained with MetaPerturb signiﬁcantly outperform the baselines on most of the tasks and architectures, with a negligible increase in the parameter size and no hyperparameters to tune. 1

Introduction
The success of Deep Neural Networks (DNNs) largely owes to their ability to accurately represent arbitrarily complex functions. However, at the same time, the excessive number of parameters, which enables such expressive power, renders them susceptible to overﬁtting especially when we do not have a sufﬁcient amount of data to ensure generalization. There are two popular techniques that can help with generalization of deep neural networks: transfer learning and regularization.
Transfer learning [39] methods aim to overcome this data scarcity problem by transferring knowledge obtained from a source dataset to effectively guide the learning on the target task. Whereas the existing transfer learning methods have been proven to be very effective, there also exist some limitations. Firstly, their performance gain highly depends on the similarity between source and target domains, and knowledge transfer across different domains may not be effective or even degenerate the performance on the target task. Secondly, many transfer learning methods require the neural architectures for the source and the target tasks to be the same, as in the case of ﬁne-tuning. Moreover, transfer learning methods usually require additional memory and computational cost for knowledge transfer. Many require to store the entire set of parameters for the source network (e.g. ﬁne-tuning,
LwF [21], attention transfer [48]), and some methods require extra training to transfer the source
∗: Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Concepts. We learn our perturbation function at meta-training stage and use it to solve diverse meta-testing tasks that come with diverse network architectures. knowledge to the target task [15]. Such restriction makes transfer learning unappealing, and thus not many of them are used in practice except for simple ﬁne-tuning of the networks pre-trained on large datasets (e.g. convolutional networks pretrained on ImageNet [33], BERT [8] trained on Wikipedia).
On the other hand, regularization techniques, which leverage human prior knowledge on the learning tasks to help with generalization, are more versatile as they are domain- and architecture- agnostic.
Penalizing the (cid:96)p-norm of the weights [28], dropping out random units or ﬁlters [38, 11], normalizing the distribution of latent features at each input [14, 41, 45], randomly mixing or perturbing sam-ples [50, 42], are instances of such domain-agnostic regularizations. They are more favored in practice over transfer learning since they can work with any architectures and do not incur extra memory or computational overhead, which is often costly with many advanced transfer learning techniques.
However, regularization techniques are limited in that they do not exploit the rich information in the large amount of data available.
These limitations of transfer learning and regularization techniques motivate us to come up with a transferable regularization technique that can bridge the gap between the two different approaches for enhancing generalization. Such a transferable regularizer should learn useful knowledge from the source task for regularization, while generalizing across different domains and architectures, with minimal extra cost. A recent work [19] propose to meta-learn a noise generator for few-shot learning, to improve the generalization on unseen tasks. Yet, the proposed gradient-based meta-learning scheme cannot scale to standard learning settings which require large amount of steps to converge to good solutions and is inapplicable to architectures that are different from the source network architecture.
To overcome these difﬁculties, we propose a novel lightweight, scalable perturbation function that is meta-learned to improve generalization on unseen tasks and architectures for standard training (See Figure 1 for the concept). Our model generates regularizing perturbations to latent features, given the set of original latent features at each layer. Since it is implemented as an order-equivariant set function, it can be shared across layers and networks learned with different initializations. We meta-learn our perturbation function by a simple joint training over multiple subsets of the source dataset in parallel, which largely reduces the computational cost of meta-learning.
We validate the efﬁcacy and efﬁciency of our transferable regularizer MetaPerturb by training it on a speciﬁc source dataset and applying the learned function to the training of heterogeneous architectures on a large number of datasets with varying degrees of task similarity. The results show that networks trained with our meta regularizer outperforms recent regularization techniques and ﬁne-tuning, and obtains signiﬁcantly improved performances even on largely different tasks on which ﬁne-tuning fails.
Also, since the optimal amount of perturbation is automatically learned at each layer, MetaPerturb does not have any hyperparameters unlike most of the existing regularizers. Such effectiveness, efﬁciency, and versatility of our method makes it an appealing transferable regularization technique that can replace or accompany ﬁne-tuning and conventional regularization techniques.
The contribution of this paper is threefold:
• We propose a lightweight and versatile perturbation function that can transfer the knowledge of a source task to heterogeneous target tasks and architectures.
• We propose a novel meta-learning framework in the form of joint training, which allows to efﬁciently perform meta-learning on large-scale datasets in the standard learning framework.
• We validate our perturbation function on a large number of datasets and architectures, on which it successfully outperforms existing regularizers and ﬁnetuning. 2
2