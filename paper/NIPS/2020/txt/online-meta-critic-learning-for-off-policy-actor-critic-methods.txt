Abstract
Off-Policy Actor-Critic (OffP-AC) methods have proven successful in a variety of continuous control tasks. Normally, the critic’s action-value function is updated using temporal-difference, and the critic in turn provides a loss for the actor that trains it to take actions with higher expected return. In this paper, we introduce a
ﬂexible meta-critic framework based on observing the learning process and meta-learning an additional loss for the actor that accelerates and improves actor-critic learning. Compared to existing meta-learning algorithms, meta-critic is rapidly learned online for a single task, rather than slowly over a family of tasks. Crucially, our meta-critic is designed for off-policy based learners, which currently provide state-of-the-art reinforcement learning sample efﬁciency. We demonstrate that online meta-critic learning beneﬁts to a variety of continuous control tasks when combined with contemporary OffP-AC methods DDPG, TD3 and SAC. 1

Introduction
Off-policy Actor-Critic (OffP-AC) methods are currently central in deep reinforcement learning (RL) research due to their greater sample efﬁciency compared to on-policy alternatives. On-policy learning requires new trajectories to be collected for each update to the policy, and is expensive as the number of gradient steps and samples per step increases with task-complexity even for contemporary TRPO
[33], PPO [34] and A3C [27] algorithms.
Off-policy methods, such as DDPG [20], TD3 [9] and SAC [13] achieve greater sample efﬁciency as they can learn from randomly sampled historical transitions without a time sequence requirement, making better use of past experience. The critic estimates action-value (Q-value) function using a differentiable function approximator, and the actor updates its policy parameters in the direction of the approximate action-value gradient. Brieﬂy, the critic provides a loss to guide the actor, and is trained in turn to estimate the environmental action-value under the current policy via temporal-difference learning [38]. In all these cases the learning objective function is hand-crafted and ﬁxed.
Recently, meta-learning [14] has become topical as a paradigm to accelerate RL by learning aspects of the learning strategy, for example, learning fast adaptation strategies [7, 30, 31], losses [3, 15, 17, 36], optimisation strategies [6], exploration strategies [11], hyperparameters [40, 42], and intrinsic rewards
[44]. However, most of these works perform meta-learning on a family of tasks or environments and amortize this huge cost by deploying the trained strategy for fast learning on a new task.
∗Contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper we introduce a meta-critic network to enhance OffP-AC learning methods. The meta-critic augments the vanilla critic to provide an additional loss to guide the actor’s learning. However, compared to the vanilla critic, the meta-critic is explicitly (meta)-trained to accelerate the learning process rather than merely estimate the action-value function. Overall, the actor is trained by both critic and meta-critic provided losses, the critic is trained by temporal-difference as usual, and crucially the meta-critic is trained to generate maximum learning progress in the actor. Both the critic and meta-critic use randomly sampled transitions for effective OffP-AC learning, providing superior sample efﬁciency compared to existing on-policy meta-learners. We emphasize that meta-critic can be successfully learned online within a single task. This is in contrast to the currently widely used meta-learning paradigm – where entire task families are required to provide enough data for meta-learning, and to provide new tasks to amortize the huge cost of meta-learning.
Our framework augments vanilla AC learning with an additional meta-learned critic, which can be seen as providing intrinsic motivation towards optimum actor learning progress [28]. As analogously observed in recent meta-learning studies [8], our loss-learning can be formalized as bi-level optimi-sation with the upper level being meta-critic learning, and lower level being conventional learning.
We solve this joint optimisation by iteratively updating the meta-critic and base learner online in a single task. Our strategy is related to the meta-loss learning in EPG [15], but learned online rather than ofﬂine, and integrated with OffP-AC rather than their on-policy policy-gradient learning. The most related prior work is LIRPG [44], which meta-learns an intrinsic reward online. However, their intrinsic reward just provides a helpful scalar offset to the environmental reward for on-policy trajectory optimisation via policy-gradient [37]. In contrast our meta-critic provides a loss for direct actor optimisation using sampled transitions, and achieves dramatically better sample efﬁciency than
LIRPG reward learning. We evaluate several continuous control benchmarks and show that online meta-critic learning can improve contemporary OffP-AC algorithms including DDPG, TD3 and SAC. 2