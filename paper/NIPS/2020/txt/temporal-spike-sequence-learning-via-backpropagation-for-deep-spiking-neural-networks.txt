Abstract
Spiking neural networks (SNNs) are well suited for spatio-temporal learning and implementations on energy-efﬁcient event-driven neuromorphic processors. How-ever, existing SNN error backpropagation (BP) methods lack proper handling of spiking discontinuities and suffer from low performance compared with the BP methods for traditional artiﬁcial neural networks. In addition, a large number of time steps are typically required to achieve decent performance, leading to high la-tency and rendering spike based computation unscalable to deep architectures. We present a novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision. It captures inter-neuron dependencies through presy-naptic ﬁring times by considering the all-or-none characteristics of ﬁring activities, and captures intra-neuron dependencies by handling the internal evolution of each neuronal state in time. TSSL-BP efﬁciently trains deep SNNs within a much short-ened temporal window of a few steps while improving the accuracy for various image classiﬁcation datasets including CIFAR10. 1

Introduction
Spiking neural networks (SNNs), a brain-inspired computational model, have gathered signiﬁcant interests [9]. The spike-based operational principles of SNNs not only allow information coding based on efﬁcient temporal codes and give rise to promising spatiotemporal computing power but also render energy-efﬁcient VLSI neuromorphic chips such as IBM’s TrueNorth [1] and Intel’s
Loihi [6]. Despite the recent progress in SNNs and neuromorphic processor designs, fully leveraging the theoretical computing advantages of SNNs over traditional artiﬁcial neural networks (ANNs) [17] to achieve competitive performance in wide ranges of challenging real-world tasks remains difﬁcult.
Inspired by the success of error backpropagation (BP) and its variants in training conventional deep neural networks (DNNs), various SNNs BP methods have emerged, aiming at attaining the same level of performance [4, 16, 24, 13, 21, 28, 11, 2, 3, 29]. Although many appealing results are achieved by these methods, developing SNNs BP training methods that are on a par with the mature BP tools widely available for training ANNs today is a nontrivial problem [22].
Training of SNNs via BP are challenged by two fundamental issues. First, from an algorithmic perspective, the complex neural dynamics in both spatial and temporal domains make the BP process obscure. Moreover, the errors are hard to be precisely backpropagated due to the non-differentiability of discrete spike events. Second, a large number of time steps are typically required for emulating
SNNs in time to achieve decent performance, leading to high latency and rendering spike based computation unscalable to deep architectures. It is desirable to demonstrate the success of BP in training deeper SNNs achieving satisfactory performances on more challenging datasets. We propose a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
new SNNs BP method, called temporal spike sequence learning via BP (TSSL-BP), to learn any target output temporal spiking sequences. TSSL-BP acts as a universal training method for any employed spike codes (rate, temporal, and combinations thereof). To tackle the above difﬁculties, TSSL-BP breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies, leading to improved temporal learning precision. It captures the inter-neuron dependencies within an
SNN by considering the all-or-none characteristics of ﬁring activities through presynaptic ﬁring times; it considers the internal evolution of each neuronal state through time, capturing how the intra-neuron dependencies between different ﬁring times of the same presynaptic neuron impact its postsynaptic neuron. The efﬁcacy and precision of TSSL-BP allows it to successfully train SNNs over a very short temporal window, e.g. over 5-10 time steps, enabling ultra-low latency spike computation. As shown in Section 4, TSSL-BP signﬁcantly improves accuracy and runtime efﬁciency of BP training on several well-known image datasets of MNIST [15], NMNIST [19], FashionMNIST [26], and
CIFAR10 [14]. Speciﬁcally, it achieves up to 3.98% accuracy improvement over the previously reported SNN work on CIFAR10, a challenging dataset for all prior SNNs BP methods. 2