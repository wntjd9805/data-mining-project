Abstract
The performance of generative adversarial networks (GANs) heavily deteriorates given a limited amount of training data. This is mainly because the discriminator is memorizing the exact training set. To combat it, we propose Differentiable
Augmentation (DiffAugment), a simple method that improves the data efﬁciency of
GANs by imposing various types of differentiable augmentations on both real and fake samples. Previous attempts to directly augment the training data manipulate the distribution of real images, yielding little beneﬁt; DiffAugment enables us to adopt the differentiable augmentation for the generated samples, effectively stabi-lizes training, and leads to better convergence. Experiments demonstrate consistent gains of our method over a variety of GAN architectures and loss functions for both unconditional and class-conditional generation. With DiffAugment, we achieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128×128 and 2-4× reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore, with only 20% training data, we can match the top performance on CIFAR-10 and CIFAR-100.
Finally, our method can generate high-ﬁdelity images using only 100 images with-out pre-training, while being on par with existing transfer learning algorithms. Code is available at https://github.com/mit-han-lab/data-efficient-gans. 1

Introduction
Big data has enabled deep learning algorithms achieve rapid advancements. In particular, state-of-the-art generative adversarial networks (GANs) [11] are able to generate high-ﬁdelity natural images of diverse categories [2, 18]. Many computer vision and graphics applications have been enabled [32, 43, 53]. However, this success comes at the cost of a tremendous amount of computation and data. Recently, researchers have proposed promising techniques to improve the computational efﬁciency of model inference [22,36], while the data efﬁciency remains to be a fundamental challenge.
GANs heavily rely on vast quantities of diverse and high-quality training examples. To name a few, the FFHQ dataset [17] contains 70,000 selective post-processed high-resolution images of human faces; the ImageNet dataset [6] annotates more than a million of images with various object categories.
Collecting such large-scale datasets requires months or even years of considerable human efforts along with prohibitive annotation costs. In some cases, it is not even possible to have that many examples, e.g., images of rare species or photos of a speciﬁc person or landmark. Thus, it is of critical importance to eliminate the need of immense datasets for GAN training. However, reducing the amount of training data results in drastic degradation in the performance. For example in Figure 1, given only 10% or 20% of the CIFAR-10 data, the training accuracy of the discriminator saturates quickly (to nearly 100%); however, its validation accuracy keeps decreasing (to lower than 30%), suggesting that the discriminator is simply memorizing the entire training set. This severe over-ﬁtting problem disrupts the training dynamics and leads to degraded image quality.
A widely-used strategy to reduce overﬁtting in image classiﬁcation is data augmentation [20, 38, 42], which can increase the diversity of training data without collecting new samples. Transformations such 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: BigGAN heavily deteriorates given a limited amount of data. left: With 10% of CIFAR-10 data, FID increases shortly after the training starts, and the model then collapses (red curve). middle: the training accuracy of the discriminator D quickly saturates. right: the validation accuracy of D dramatically falls, indicating that D has memorized the exact training set and fails to generalize.
Figure 2: Unconditional generation results on CIFAR-10. StyleGAN2’s performance drastically degrades given less training data. With DiffAugment, we are able to roughly match its FID and outperform its Inception Score (IS) using only 20% training data. FID and IS are measured using 10k samples; the validation set is used as the reference distribution for FID calculation. as cropping, ﬂipping, scaling, color jittering [20], and region masking (Cutout) [8] are commonly-used augmentations for vision models. However, applying data augmentation to GANs is fundamentally different. If the transformation is only added to the real images, the generator would be encouraged to match the distribution of the augmented images. As a consequence, the outputs suffer from distribution shift and the introduced artifacts (e.g., a region being masked, unnatural color, see
Figure 5a). Alternatively, we can augment both the real and generated images when training the discriminator; however, this would break the subtle balance between the generator and discriminator, leading to poor convergence as they are optimizing completely different objectives (see Figure 5b).
To combat it, we introduce a simple but effective method, DiffAugment, which applies the same differentiable augmentation to both real and fake images for both generator and discriminator training.
It enables the gradients to be propagated through the augmentation back to the generator, regularizes the discriminator without manipulating the target distribution, and maintains the balance of training dynamics. Experiments on a variety of GAN architectures and datasets consistently demonstrate the effectiveness of our method. With DiffAugment, we improve BigGAN and achieve a Fr´echet
Inception Distance (FID) of 6.80 with an Inception Score (IS) of 100.8 on ImageNet 128×128 without the truncation trick [2] and reduce the StyleGAN2 baseline’s FID by 2-4× given 1,000 images on the FFHQ and LSUN datasets. We also match the top performance on CIFAR-10 and CIFAR-100 using only 20% training data (see Figure 2). Furthermore, our method can generate high-quality images with only 100 examples (see Figure 3). Without any pre-training, we achieve competitive performance with existing transfer learning algorithms that used to require tens of thousands of training images. 2