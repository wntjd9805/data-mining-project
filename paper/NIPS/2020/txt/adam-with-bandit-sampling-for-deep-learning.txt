Abstract
Adam is a widely used optimization method for training deep learning models. It computes individual adaptive learning rates for different parameters. In this paper, we propose a generalization of Adam, called ADAMBS, that allows us to also adapt to different training examples based on their importance in the model’s convergence.
To achieve this, we maintain a distribution over all examples, selecting a mini-batch in each iteration by sampling according to this distribution, which we update using a multi-armed bandit algorithm. This ensures that examples that are more beneﬁcial to the model training are sampled with higher probabilities. We theoretically show
T ) instead of
T ) in some cases. Experiments on various models and datasets demonstrate that ADAMBS improves the convergence rate of Adam—O(
O((cid:112) n
ADAMBS’s fast convergence in practice. (cid:113) log n 1

Introduction
Stochastic gradient descent (SGD) is a popular optimization method, which iteratively updates the model parameters by moving them in the direction of the negative gradient of the loss evaluated on a mini-batch. However, standard SGD does not have the ability to use past gradients or adapt to individual parameters (a.k.a. features). Some variants of SGD, such as AdaGrad [16], RMSprop [34],
AdaDelta [36], or Nadam [15] can exploit past gradients or adapt to individual features. Adam [19] combines the advantages of these SGD variants: it uses momentum on past gradients, but also computes adaptive learning rates for each individual parameter by estimating the ﬁrst and second moments of the gradients. This adaptive behavior is quite beneﬁcial as different parameters might be of different importance in terms of the convergence of the model training. In fact, by adapting to different parameters, Adam has shown to outperform its competitors in various applications [19], and as such, has gained signiﬁcant popularity.
However, another form of adaptivity that has proven beneﬁcial in the context of basic SGD variants is adaptivity with respect to different examples in the training set [18, 6, 8, 38, 13]. For the ﬁrst time, to the best of our knowledge, in this paper we show that accounting for the varying importance of training examples can even improve Adam’s performance and convergence rate. In other words,
Adam has only considered the varying importance among parameters but not among the training examples. Although some prior work exploits importance sampling to improve SGD’s convergence rate [18, 6, 8, 38, 13], they often rely on some special properties of different models to estimate a sampling distribution over examples in advance. As a more general approach, we focus on learning the distribution during the training procedure, which is more adaptive. This is key to achieving faster convergence rate, because the sampling distribution typically changes from one iteration to the next, depending on the relationship between training examples and model parameters at each iteration.
In this paper, we propose a new general optimization method based on bandit sampling, called
ADAMBS (Adam with Bandit Sampling), that endows Adam with the ability to adapt to different examples. To achieve this goal, we maintain a distribution over all examples, representing their 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Adam with Bandit Sampling. Our method ADAMBS generalizes Adam with the ability to adapt to different examples by using a bandit method to maintain a distribution p over all training examples. relative importance to the overall convergence. In Adam, at each iteration, a mini-batch is selected uniformly at random from training examples. In contrast, we select our mini-batch according to our maintained distribution. We then use this mini-batch to compute the ﬁrst and second moments of the gradient and update the model parameters, in the same way as Adam. While seemingly simple, this process introduces another challenge: how to efﬁciently obtain this distribution and update it at each iteration? Ideally, to obtain the optimal distribution, at each iteration one would need to compute the gradients for all training examples. However, in Adam, we only have access to gradients of the examples in the selected mini-batch. Since we only have access to partial feedback, we use a multi-armed bandit method to address this challenge. Our idea is illustrated in Figure 1.
Speciﬁcally, we cast the process of learning an optimal distribution as an adversarial multi-armed bandit problem. We use a multi-armed bandit method to update the distribution over all of the training examples, based on the partial information gained from the mini-batch at each iteration. We use the
EXP3 method [3], but extend it to allow for sampling multiple actions at each step. The original
EXP3 method only samples one action at a time, and collects the partial feedback by observing the loss incurred by that single action. In contrast, in optimization frameworks such as Adam, we need to sample a mini-batch, which typically contains more than one example. We thus need a bandit method that samples multiple actions and observes the loss incurred by each of those actions.
In this paper, we extend EXP3 to use feedback from multiple actions and update its distribution accordingly. Although ideas similar to bandit sampling have been applied to some SGD variants and coordinate descent methods [30, 29, 24], extending this idea to Adam is not straightforward, as
Adam is considerably more complex, due to its momentum mechanism and parameter adaptivity. To the best of our knowledge, we are the ﬁrst to propose and analyze the improvement of using bandit sampling for Adam. Maintaining and updating distribution over all training examples incur some computational overhead. We will show an efﬁcient way to update this distribution, which has time complexity logarithmic with respect to the total number of training examples. With this efﬁcient update, the per-iteration cost is dominated by gradient computation, whose time complexity depends on the mini-batch size.
To endow Adam with the adaptive ability to different examples while keeping its original structure, we interleave our bandit method with Adam’s original parameter update, except that we select the mini-batch according to our maintained distribution. ADAMBS therefore adapts to both different parameters and different examples. We provide a theoretical analysis of this new method showing that our bandit sampling does indeed improve Adam’s convergence rate. Through an extensive empirical study across various optimization tasks and datasets, we also show that this new method yields signiﬁcant speedups in practice as well. 2