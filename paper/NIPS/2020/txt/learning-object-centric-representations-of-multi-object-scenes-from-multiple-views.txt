Abstract
Learning object-centric representations of multi-object scenes is a promising ap-proach towards machine intelligence, facilitating high-level reasoning and control from visual sensory data. However, current approaches for unsupervised object-centric scene representation are incapable of aggregating information from multiple observations of a scene. As a result, these “single-view” methods form their repre-sentations of a 3D scene based only on a single 2D observation (view). Naturally, this leads to several inaccuracies, with these methods falling victim to single-view spatial ambiguities. To address this, we propose The Multi-View and Multi-Object
Network (MulMON)1—a method for learning accurate, object-centric represen-tations of multi-object scenes by leveraging multiple views. In order to sidestep the main technical difﬁculty of the multi-object-multi-view scenario—maintaining object correspondences across views—MulMON iteratively updates the latent ob-ject representations for a scene over multiple views. To ensure that these iterative updates do indeed aggregate spatial information to form a complete 3D scene understanding, MulMON is asked to predict the appearance of the scene from novel viewpoints during training. Through experiments we show that MulMON better-resolves spatial ambiguities than single-view methods—learning more accu-rate and disentangled object representations—and also achieves new functionality in predicting object segmentations for novel viewpoints. 1

Introduction
Traditional VAEs[15] use “single-object” or “ﬂat” vector representations that fail to capture the compositional structure of natural scenes, i.e. the existence of interchangeable objects with common properties. As a result, “multi-object” or object-centric representations have emerged as a promising approach to scene understanding, improving sample-efﬁciency and generalization for many down-stream applications like relational reasoning and control [5, 20, 3]. However, recent progress in unsupervised object-centric scene representation has been limited to “single-view” methods which form their representations of 3D scenes based only on a single 2D observation (view). As a result, these methods form inaccurate representations that fall victim to single-view spatial ambiguities (e.g. occluded or partially occluded objects) and fail to capture 3D spatial structures. 1Code available at https://github.com/NanboLi/MulMON 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Left: Multi-object-multi-view setup. vq denotes the query viewpoint, while zk denotes
“slot” k, i.e. the latent object representation of a scene object. Right: MulMON overview. Starting with a standard normal prior, MulMON iteratively reﬁnes z over multiple views, each time reducing its uncertainty about the scene—–as illustrated by the darkening, white-to-blue arrow. Within-view
“inner loop” iterations are depicted by the green arrows and boxes. Cross-view “outer-loop” iterations are depicted by the white-to-blue arrows and boxes. At the bottom, we have visualised MulMON’s reduction in uncertainty about z in image space, where each image shows the per-pixel variance of MulMON’s predicted observation from query viewpoint vq. MulMON’s ﬁnal predictions for vq (observation and segmentation) are shown to the right of the vertical dotted line.
To address this, we present MulMON (Multi-View and Multi-Object Network)—an unsupervised method for learning object-centric scene representations from multiple views. Using a spatial mixture model [10] and iterative amortized inference [19], MulMON sidesteps the main technical difﬁculty of the multi-object-multi-view scenario—maintaining object correspondence across views— by iteratively updating the latent object representations for a scene over multiple views, each time using the previous iterations posterior as the new prior. To ensure that these iterative updates do indeed aggregate spatial information, rather than simply overwrite, MulMON is asked to predict the appearance of the scene from novel viewpoints during training. Given images of a static scene from several viewpoints, MulMON forms an object-centric representation, then uses this representation to predict the appearance and object segmentations of that scene from unobserved viewpoints. Through experiments we demonstrate that:
• MulMON better-resolves spatial ambiguities than single-view methods like IODINE [9], while providing all the beneﬁts of object-based representations that “single-object” methods like GQN [8] lack, e.g. object segmentations and manipulations (see Section 5).
• MulMON accurately captures 3D scene information (rotation along the vertical axis) by integrating spatial information from multiple views (see Section 5.3).
• MulMON achieves both inter- and intra-object disentanglement—enabling both single-object and single-object-property scene manipulations (see Section 5.3).
• MulMON represents the ﬁrst feasible solution to the multi-object-multi-view problem, permitting new functionality like viewpoint-queried object-segmentation (see Section 5.2). 2