Abstract
This paper studies a gradient temporal difference (GTD) algorithm using neural network (NN) function approximators to minimize the mean squared Bellman error (MSBE). For off-policy learning, we show that the minimum MSBE problem can be recast into a min-max optimization involving a pair of over-parameterized primal-dual NNs. The resultant formulation can then be tackled using a neural
GTD algorithm. We analyze the convergence of the proposed algorithm with a 2-layer ReLU NN architecture using m neurons and prove that it computes an approximate optimal solution to the minimum MSBE problem as m
.
! 1 1

Introduction
Policy evaluation is a key problem in reinforcement learning (RL) whose goal is to estimate the value function of a given policy, i.e., the expected total reward of a discounted Markov decision process (MDP) starting from a given state. Among others, the temporal difference (TD) learning algorithm
[Sutton, 1988] has been used to minimize the mean squared (projected) Bellman error (MSBE). For off-policy learning where the behavior policy differs from the target policy, gradient-based TD (GTD) learning algorithms [Sutton et al., 2009a,b] have been proposed with guaranteed convergence. A growing trend is to employ nonlinear approximation such as neural network (NN) functions [e.g.,
Chung et al., 2019, Haarnoja et al., 2018, Lillicrap et al., 2015, Mnih et al., 2016, Silver, 2012].
The TD/GTD learning algorithms have been analyzed with linear function approximation [Bhandari et al., 2018, Dalal et al., 2017]. Meanwhile, nonlinear TD/GTD learning algorithms (as well as other related problems such as Q-learning) are studied in [Bhatnagar et al., 2009, Brandfonbrener and Bruna, 2019, Chung et al., 2019, Dai et al., 2018, Wai et al., 2019], also see [Bertsekas, 2019].
However, these algorithms lack theoretical guarantees related to minimizing the MSBE or MSPBE objective as they may get stuck in a local optimum. Furthermore, these algorithms are designed for arbitrary nonlinear function approximation, whose actual implementations involve computationally intensive steps such as computing the Hessians for the nonlinear functions.
In this paper, we analyze the efﬁciency of an off-policy GTD learning algorithm with NN function approximation. We consider a simpliﬁed setting employing a pair of over-parameterized 2-layer
ReLU NNs. A key result proven is that the proposed neural GTD algorithm is guaranteed to converge globally to a minimizer of the MSBE problem, despite the corresponding optimization problem is non-convex. Our main contributions are: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
•
•
•
We derive a new formulation for MSBE minimization with a primal NN and a dual NN as approximators. A neural GTD algorithm, which obeys similar update rules as the classical GTD2 algorithm, is proposed for the resultant min-max optimization problem.
Under an off-policy learning setting, we analyze the convergence rates of the neural GTD algorithm with various sampling techniques, including population update, stochastic updates with i.i.d. samples and Markov samples.
We focus on the 2-layer ReLU NN architecture. We show that when the width of the NN employed goes to inﬁnity and the TD error function lies in the NN function class, the proposed neural
GTD algorithm is guaranteed to ﬁnd a global minimizer of the MSBE problem. Importantly, the convergence rates measured with the functional distance are independent of NN’s width.