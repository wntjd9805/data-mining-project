Abstract
Variational Bayesian phylogenetic inference (VBPI) provides a promising general variational framework for efﬁcient estimation of phylogenetic posteriors. However, the current diagonal Lognormal branch length approximation would signiﬁcantly restrict the quality of the approximating distributions. In this paper, we propose a new type of VBPI, VBPI-NF, as a ﬁrst step to empower phylogenetic posterior estimation with deep learning techniques. By handling the non-Euclidean branch length space of phylogenetic models with carefully designed permutation equivari-ant transformations, VBPI-NF uses normalizing ﬂows to provide a rich family of
ﬂexible branch length distributions that generalize across different tree topologies.
We show that VBPI-NF signiﬁcantly improves upon the vanilla VBPI on a bench-mark of challenging real data Bayesian phylogenetic inference problems. Further investigation also reveals that the structured parameterization in those permutation equivariant transformations can provide additional amortization beneﬁt. 1

Introduction
As a powerful statistical tool that has revolutionized modern molecular evolutionary analysis, Bayesian phylogenetic inference has been widely used for tasks ranging from genomic epidemiology (Neher and Bedford, 2015; Sun et al., 2020) to conservation genetics (DeSalle and Amato, 2004). Given properly aligned sequence data (e.g., DNA, RNA or protein sequences) and a model of evolution,
Bayesian phylogenetics provides principled approaches to quantify the uncertainty of the evolutionary process in terms of the posterior probabilities of phylogenetic trees (Huelsenbeck et al., 2001). A commonly used Bayesian phylogenetic inference method is random-walk Markov chain Monte
Carlo (MCMC), which was introduced to the community in the late 1990’s (Yang and Rannala, 1997; Mau et al., 1999; Huelsenbeck and Ronquist, 2001). However, random-walk MCMC has been fundamentally limited as it often exhibits low exploration efﬁciency and requires long runs to deliver accurate posterior estimates due to the complexity of tree space. Although many advanced methods for posterior sampling have been proposed recently, including Hamiltonian Monte Carlo (Duane et al., 1987; Neal, 2011), it is not straightforward to extend these methods to phylogenetic models due to the composite structure of tree space, i.e., a combination of discrete variables (e.g., tree topologies) and continuous variables (e.g., branch lengths) (Dinh et al., 2017b).
Variational inference (VI) (Jordan et al., 1999; Wainwright and Jordan, 2008; Blei et al., 2017) is an alternative approximate Bayesian inference method that is growing in popularity. Unlike MCMC methods, VI seeks the best approximation to the true posterior from a family of tractable distributions.
By transforming the inference problem into an optimization problem, VI tends to be faster and easier to scale to large data (Blei et al., 2017). In recent years, many efforts have been made to harness
VI for phylogenetic inference (Zhang and Matsen IV, 2019; Dang and Kishino, 2019; Fourment and Darling, 2019), among which variational Bayesian phylogenetic inference (VBPI) proposed by 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Zhang and Matsen IV (2019) provides a promising general framework that allows joint learning of phylogenetic trees with branch lengths. At the core of VBPI lie subsplit Bayesian networks (SBNs) (Zhang and Matsen IV, 2018), an expressive probabilistic graphical model for distributions over the tree topology space, and a structured amortization of the branch lengths over different tree topologies.
With guided exploration in the tree space (enabled by SBNs) and joint learning of the branch length distributions across tree topologies (via amortization), VBPI provides competitive performance to
MCMC with much less computation. However, the diagonal Lognormal branch length distribution currently used in VBPI might not be ﬂexible enough to resemble the true posterior distributions.
A powerful framework for building ﬂexible approximating distributions is normalizing ﬂows (NFs) (Rezende and Mohamed, 2015; Dinh et al., 2017a; Kingma et al., 2016; Papamakarios et al., 2019).
Starting from a simple base distribution with a tractable probability density function, NFs apply a sequence of invertible transformations, often parameterized by neural networks, to obtain a more
ﬂexible distribution. These ﬂow-based approximating distributions enjoy many advantages such as efﬁcient sampling, exact likelihood evaluation, and low-variance Monte Carlo gradient estimates when the base distribution is reparameterizible, making them ideal for variational inference. While efﬁcient, current NFs are primarily designed for distributions on Euclidean space, and as a result, these approaches are ill-equipped for phylogenetic models where the tree topology and the branch lengths are intertwined in a rather complex and non-Euclidean fashion.
In this paper, we propose a new type of VBPI, VBPI-NF (Normalizing Flows), which incorporates normalizing ﬂows for more expressive branch length approximations. More speciﬁcally, we develop permutation equivariant normalizing ﬂows to deal with the non-Euclidean branch length space across different tree topologies, with structured parameterization based on the local topologies of trees.
Inference using VBPI-NF provides tighter lower bounds and can be performed the same way as in
Zhang and Matsen IV (2019). Experiments on a benchmark of challenging real data Bayesian phylo-genetic inference problems demonstrate the signiﬁcant improvement of VBPI-NF over the vanilla
VBPI. Further investigation also shows that the transformations in these permutation equivariant normalizing ﬂows can provide additional amortization beneﬁt while improving approximation. 2