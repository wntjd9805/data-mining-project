Abstract
Nonconvex sparse models have received signiﬁcant attention in high-dimensional machine learning. In this paper, we study a new model consisting of a general convex or nonconvex objectives and a variety of continuous nonconvex sparsity-inducing constraints. For this constrained model, we propose a novel proximal point algorithm that solves a sequence of convex subproblems with gradually relaxed constraint levels. Each subproblem, having a proximal point objective and a convex surrogate constraint, can be efﬁciently solved based on a fast routine for projection onto the surrogate constraint. We establish the asymptotic convergence of the proposed algorithm to the Karush-Kuhn-Tucker (KKT) solutions. We also establish new convergence complexities to achieve an approximate KKT solution when the objective can be smooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity that is on a par with gradient descent for unconstrained optimization problems in respective cases. To the best of our knowledge, this is the ﬁrst study of the ﬁrst-order methods with complexity guarantee for nonconvex sparse-constrained problems. We perform numerical experiments to demonstrate the effectiveness of our new model and efﬁciency of the proposed algorithm for large scale problems. 1

Introduction
Recent years have witnessed a great deal of work on the sparse optimization arising from machine learning, statistics and signal processing. A fundamental challenge in this area lies in ﬁnding the best set of size k out of a total of d (k ă d) features to form a parsimonious ﬁt to the data: min ψpxq, subject to (cid:107)x(cid:107)0 ď k, x P Rd. (1)
However, due to the discontinuity of (cid:107)¨(cid:107)0 norm2, the above problem is intractable when there is no other assumptions. To bypass this difﬁculty, a popular approach is to replace the (cid:96)0-norm by the (cid:96)1-norm, giving rise to an (cid:96)1-constrained or (cid:96)1-regularized problem. A notable example is the Lasso
˚Work done when author was at Georgia Tech 2Note that }¨}0 is not a norm in mathematical sense. Indeed, }x}0 “ }tx}0 for any nonzero t. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
([31]) approach for linear regression and its regularized variant min (cid:107)b ´ Ax(cid:107)2 2, min (cid:107)b ´ Ax(cid:107)2 2 ` λ(cid:107)x(cid:107)1. subject to (cid:107)x(cid:107)1 ď τ, x P Rd; (2) (3)
Due to the Lagrange duality theory, problem (2) and (3) are equivalent in the sense that there is a one-to-one mapping between the parameters τ and λ. A substantial amount of literature already exists for understanding the statistical properties of (cid:96)1 models ([41, 32, 7, 39, 19]) as well as for the development efﬁcient algorithms when such models are employed ([11, 1, 22, 34, 19]).
In spite of their success, (cid:96)1 models suffer from the issue of biased estimation of large coefﬁcients
[12] and empirical merits of using nonconvex approximations were shown in [26]. Due to these observations, a large body of recent research looked at replacing the (cid:96)1-penalty in (3) by a nonconvex function gpxq to obtain sharper approximation of the (cid:96)0-norm: min ψpxq ` βgpxq, (4) where , throughout the paper, gpxq is a nonsmooth nonconvex function of the form gpxq “ λ }x}1 ´ hpxq.
Here hpxq is a convex and continuously differentiable function, giving gpxq a DC form. This class of constraints already covers many important nonconvex sparsity inducing functions in the literature (see Table 2).
Despite the favorable statistical properties ([12, 38, 8, 40]), nonconvex models have posed a great challenge for optimization algorithms and has been increasingly an important issue ([36, 16, 17, 29]).
While most of these works studied the regularized version, it is often favorable to consider the following constrained form: min ψpxq, subject to gpxq ď η, x P Rd, (5) since sparsity of solutions is imperative in many applications of statistical learning and constrained form in (5) explicitly imposes such a requirement. In contrast, (4) imposes sparsity implicitly using penalty parameter β. However, unlike the convex problems, large values of β do not necessarily imply small value of the nonconvex penalty gpxq.
Therefore, it is natural to ask whether we can provide an efﬁcient algorithm for problem (5). The continuous nonconvex relaxation (5) of the (cid:96)0-norm in (1), albeit a straightforward one, was not studied in the literature. We suspect that to be the case due to the difﬁculty in handling nonconvex constraints algorithmically. There are two theoretical challenges: First, since the regularized form (4) and the constrained form (5) are not equivalent due to the nonconvexity of gpxq, we cannot bypass (5) by solving problem (4) instead. Second, the nonconvex function gpxq can be nonsmooth especially for the sparsity applications, presenting a substantial challenge for classic nonlinear programming methods, e.g., augmented Lagrangian methods and penalty methods (see [2]) which assumes that functions are continuously differentiable.
Our contributions
In this paper, we study the newly proposed nonconvex constrained model (5). In particular, we present a novel level-constrained proximal point (LCPP) method for problem (5) where the objective ψ can be either deterministic/stochastic, smooth/nonsmooth and convex/nonconvex and the constraint tgpxq ď ηu models a variety of sparsity inducing nonconvex constraints proposed in the literature. The key idea is to translate problem (5) into a sequence of convex subproblems where
ψpxq is convexiﬁed using a proximal point quadratic term and gpxq is majorized by a convex function rgpxqrě gpxqs. Note that trgpxq ď ηu is a convex subset of the nonconvex set tgpxq ď ηu.
We show that starting from a strict feasible point3, LCPP traces a feasible solution path with respect to the set tgpxq ď ηu. We also show that LCPP generates convex subproblems for which bounds on the optimal Lagrange multiplier (or the optimal dual) can be provided under a mild and a well-known constraint qualiﬁcation. This bound on the dual and the proximal point update in the objective allows us to prove asymptotic convergence to the KKT points of the problem (5).
While deriving the complexity, we consider the inexact LCPP method that solves convex subproblems approximately. We show that the constraint, rgpxq ď η, has an efﬁcient projection algorithm. 3Origin is always strictly feasible for sparsity inducing constraints and can be chosen as a starting point. 2
Table 1: Iteration complexities of LCPP for problem (5) when the objective can be either convex or nonconvex, smooth or nonsmooth and deterministic or stochastic
Cases
Convex (5)
Smooth Nonsmooth
Deterministic Op1{εq
Op1{ε2q
Stochastic
Op1{ε2q
Op1{ε2q
Nonconvex (5)
Smooth Nonsmooth
Op1{εq
Op1{ε2q
Op1{ε2q
Op1{ε2q
Hence, each convex subproblem can be solved by projection-based ﬁrst-order methods. This allows us to be feasible even when the solution reaches arbitrarily close to the boundary of the set tgpxq ď ηu which entails that the bound on the dual mentioned earlier works in the inexact case too. Moreover, efﬁcient projection-based ﬁrst-order method for solving the subproblem helps us get an accelerated convergence complexity of Op 1
ε2 qs gradient [stochastic gradient] in order to obtain an ε-KKT point. In particular, refer to Table 1. We see that in the case where objective is smooth and deterministic, we obtain convergence rate of Op1{εq whereas for nonsmooth and/or stochastic objective we obtain convergence rate of Op1{ε2q. This complexity is nearly the same as that of the gradient [stochastic gradient] descent for the regularized problem (4) of the respective type. Remarkably, this convergence rate is better than black-box nonconvex function constrained optimization methods proposed in the literature recently ([5, 21]). See related work section for more detailed discussion. Note that the convergence of gradient descent does not ensure a bound on the infeasibility of the constraint g, whereas the KKT criterion requires feasibility on top of stationarity.
Moreover, such a bound cannot be ensured theoretically due to the absence of duality. Hence, our algorithm provides additional guarantees without paying much in the complexity.
ε qrOp 1
We perform numerical experiments to measure the efﬁciency of our LCPP method and the effectiveness of the new constrained model (5). First, we show that our algorithm has competitive running time performance against open-source solvers, e.g., DCCP [27]. Second, we also compare the effectiveness of our constrained model with respect to the existing convex and nonconvex regularization models in the literature. Our numerical experiments show promising results compared to (cid:96)1-regularization model 3 and has competitive performance with respect to recently developed algorithm for nonconvex regularization model 4 (see [16]). Given that this is the ﬁrst study in the development of algorithms for the constrained model, we believe empirical study of even more efﬁcient algorithms solving problem (5) may be of independent interest and can be pursued in the future.