Abstract
As deep learning models become tasked with more and more decisions that impact human lives, such as criminal recidivism, loan repayment, and face recognition for law enforcement, bias is becoming a growing concern. Debiasing algorithms are typically split into three paradigms: pre-processing, in-processing, and post-processing. However, in computer vision or natural language applications, it is common to start with a large generic model and then ﬁne-tune to a speciﬁc use-case. Pre- or in-processing methods would require retraining the entire model from scratch, while post-processing methods only have black-box access to the model, so they do not leverage the weights of the trained model. Creating debiasing algorithms speciﬁcally for this ﬁne-tuning use-case has largely been neglected.
In this work, we initiate the study of a new paradigm in debiasing research, intra-processing, which sits between in-processing and post-processing methods. Intra-processing methods are designed speciﬁcally to debias large models which have been trained on a generic dataset, and ﬁne-tuned on a more speciﬁc task. We show how to repurpose existing in-processing methods for this use-case, and we also propose three baseline algorithms: random perturbation, layerwise optimization, and adversarial debiasing. We evaluate these methods across three popular datasets from the AIF360 toolkit, as well as on the CelebA faces dataset. 1 1

Introduction
The last decade has seen a huge increase in applications of machine learning in a wide variety of domains such as credit scoring, fraud detection, hiring decisions, criminal recidivism, loan repayment, face recognition, and so on [43, 7, 45, 3, 36]. The outcome of these algorithms are impacting the lives of people more than ever. There are clear advantages in the automation of classiﬁcation tasks, as machines can quickly process thousands of datapoints with many features.
However, algorithms are susceptible to bias towards individuals or groups of people from a variety of sources [49, 46, 47, 60, 8, 28, 59].
For example, facial recognition algorithms are currently being used by the US government to match application photos from people applying for visas and immigration beneﬁts, to match mugshots, and to match photos as people cross the border into the USA [24]. However, recent studies showed that many of these algorithms exhibit bias based on race and gender [23]. For example, some of the algorithms were 10 or 100 times more likely to have false positives for Asian or Black people, compared to white people. When used for law enforcement, it means that a Black or Asian person is more likely to be arrested and detained for a crime they didn’t commit [1]. 1See the full-length paper here: https://arxiv.org/abs/2006.08564. Our code is available at https:
//github.com/abacusai/intraprocessing_debiasing. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Motivated by the discovery of biased models in real-life applications, the last few years has seen a huge growth in the area of fairness in machine learning. Dozens of formal deﬁnitions of fairness have been proposed [44], and many algorithmic techniques have been developed for debiasing according to these deﬁnitions [58]. Many debiasing algorithms ﬁt into one of three categories: pre-processing, in-processing, or post-processing [14, 5]. Pre-processing techniques make changes to the data itself, in-processing techniques are methods for training machine learning models tailored to making fairer models, and post-processing techniques modify the ﬁnal predictions outputted by a (biased) model.
However, as datasets become larger and training becomes more computationally intensive, especially in the case of computer vision and natural language processing, it is becoming increasingly more common in applications to start with a very large pretrained model, and then ﬁne-tune for the speciﬁc use-case [50, 30, 10, 57]. In fact, PyTorch offers several pretrained models, all of which have been trained for dozens of GPU hours on ImageNet [51]. Pre-, in-, and post-processing debiasing methdos are of little help here: pre- and in-processing methods would require retraining the entire model from scratch, and post-processing methods would not make use of the full power of the model.
In this work, we initiate the study of intra-processing methods for debiasing neural networks. An intra-processing method is deﬁned as an algorithm which has access to a trained model and a dataset (which typically differs from the original training dataset), and outputs a new model which gives debiased predictions on the target task (typically by updating or augmenting the weights of the original model). We propose three different intra-processing baseline algorithms, and we also show how to repurpose a popular in-processing algorithm [61] to the intra-processing setting. All of the algorithms we study work for any group fairness measure and any objective which trades off accuracy with bias.
Our ﬁrst baseline is a simple random perturbation algorithm, which iteratively adds multiplicative noise to the weights of the neural network and then picks the perturbation which maximizes the chosen objective. Our next baseline optimizes the weights of each layer using GBRT [20]. Finally, we propose adversarial methods for ﬁne-tuning. Adversarial training was recently used as an in-processing method for debiasing [61], by training a critic model to predict the protected attribute of datapoints, to ensure that the predictions are not correlated with the protected attribute. We modify this approach to be an intra-processing, and we also propose a new, more direct approach which trains a critic to directly measure the bias of the model weights, which gives us a differentiable proxy for bias, enabling the use of gradient descent for debiasing.
We compare the above techniques with one in-processing algorithm and three post-processing algorithms from prior work: adversarial debiasing [61], reject option classiﬁcation [31], equalized odds post-processing [26], and calibrated equalized odds post-processing [52]. We run experiments with three fairness datasets from AIF360 [5], as well as the CelebA dataset [39], with three popular fairness deﬁnitions. We show that intra-processing is much more effective than post-processing for the ﬁne-tuning use case. We also show that the difﬁculty of post-hoc debiasing is highly dependent on the initial conditions of the original model. In particular, given a neural network trained to optimize accuracy, the variance in the amount of bias of the trained model is much higher than the variance in the accuracy, with respect to the random seed used for initializing the weights of the original model.
Fairness research (and machine learning research as a whole) has seen a huge increase in popularity, and recent papers have highlighted the need for fair and reproducible results [55, 5]. To facilitate best practices, we run our experiments on the AIF360 toolkit [5] and open source all of our code.
Our contributions. We summarize our main contributions below.
• We initiate the study of intra-processing algorithms for debiasing ML models. This framework sits in between in-processing and post-processing, and is realistic for many ﬁne-tuning use cases.
• We study the nature of intra-processing techniques for debiasing neural networks, showing that the problem is sensitive to the initial conditions of the original model.
• We propose three baseline intra-processing algorithms, and we show how to repurpose popular in-processing algorithms into the intra-processing setting. We compare all algorithms across a variety of group fairness constraints and datasets. 2