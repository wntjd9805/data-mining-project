Abstract
Recent theoretical and experimental results suggest that the dopamine system im-plements distributional temporal difference backups, allowing learning of the entire distributions of the long-run values of states rather than just their expected values.
However, the distributional codes explored so far rely on a complex imputation step which crucially relies on spatial non-locality: in order to compute reward prediction errors, units must know not only their own state but also the states of the other units. It is far from clear how these steps could be implemented in realistic neural circuits. Here, we introduce the Laplace code: a local temporal difference code for distributional reinforcement learning that is representationally powerful and computationally straightforward. The code decomposes value distributions and prediction errors across three separated dimensions: reward magnitude (related to distributional quantiles), temporal discounting (related to the Laplace transform of future rewards) and time horizon (related to eligibility traces). Besides lending itself to a local learning rule, the decomposition recovers the temporal evolution of the immediate reward distribution, indicating all possible rewards at all future times. This increases representational capacity and allows for temporally-ﬂexible computations that immediately adjust to changing horizons or discount factors. 1

Introduction
In the traditional Reinforcement Learning (RL) framework, agents make decisions by learning and maximizing the scalar values of states, which quantify the expected sums of discounted future rewards that will be encountered from those states [1]. Recently, several results have suggested that humans and animals keep track of richer information about the distribution of future rewards, besides just its expectation [2–5]. Indeed, recent machine learning advances suggest that, besides providing more ﬂexibility for decision-making, requiring the entire value distribution to be learned leads to representations that support improved average performance, since the agent needs to represent separately states with the same expected value but different value statistics [6–8].
These results naturally pose the question of how such distributional estimators are learned and repre-sented by neural systems. A distributional RL algorithm called Expectile temporal difference (TD) learning [9] has been recently proposed as a neurally plausible method that extends the conventional temporal difference reward prediction error (RPE) theory of dopamine activity [10]. Expectile TD algorithms learn a set of estimators that converge to the expectiles of the value distribution. Impor-tantly, as we explain in the next section, the Expectile algorithms are non-local, which is a critical problem when considering neurally-plausible implementations of distributional RL.
Here, we show that the non-locality of distributional TD algorithms is not required to learn the value distribution. We show that an ensemble of independent units performing traditional and local TD
∗Corresponding author: Pablo.TanoRetamales@unige.ch 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
backups can recover the value distribution. Units in the ensemble have selectivities that vary along three dimensions: reward magnitude, temporal discount factors, and an explicit memory about past outcomes. In addition to the value distribution, it is possible to recover the temporal evolution of the immediate reward distribution from our code, by taking an inverse Laplace operator. The temporal evolution indicates the probability of obtaining different reward magnitudes at every timestep in the future. This additional information increases representational capacity and allows for a computation of expected and distributional value that immediately adapts to changes in the temporal horizon of the task. Finally, we illustrate a strong connection between our code and predictive representations, by showing that our code can be linearly computed from an ensemble of successor representations (SR) with different temporal discounts, a model recently proposed for the hippocampus [11]. 2