Abstract
This paper presents one-bit supervision, a novel setting of learning from incomplete annotations, in the scenario of image classiﬁcation. Instead of training a model upon the accurate label of each sample, our setting requires the model to query with a predicted label of each sample and learn from the answer whether the guess is correct. This provides one bit (yes or no) of information, and more importantly, annotating each sample becomes much easier than ﬁnding the accurate label from many candidate classes. There are two keys to training a model upon one-bit supervision: improving the guess accuracy and making use of incorrect guesses.
For these purposes, we propose a multi-stage training paradigm which incorporates negative label suppression into an off-the-shelf semi-supervised learning algorithm.
In three popular image classiﬁcation benchmarks, our approach claims higher efﬁciency in utilizing the limited amount of annotations. 1

Introduction
In the deep learning era [18], training deep neural networks is a standard methodology for solving computer vision problems. Yet, it is a major burden to collect annotations for training data. In particular, when the dataset contains a large number of object categories (e.g., ImageNet [3]), it is even difﬁcult for human to memorize all categories [30, 41]. In these scenarios, the annotation job would be much easier if the worker is asked whether an image belongs to a speciﬁed class, rather than being asked to ﬁnd the accurate class label from a large amount of candidates.
This paper investigates this setting, which we refer to as one-bit supervision because the labeler provides one bit of information by answering the yes-or-no question. In comparison, each accurate label provides log2 C bits of information where C is the number of classes, though we point out that the actual cost of accurate annotation is often much higher than log2 C× that of one-bit annotation.
One-bit supervision is a new challenge of learning from incomplete annotation. We expect the learning efﬁciency, in terms of accuracy under the same amount of supervision bits, to be superior to that of semi-supervised learning. For example, in a dataset with 100 classes, we can choose to accurately annotate 10K samples which give 10K × log2 100 = 66.4K bits of information, or accurately annotate 5K samples which give 33.2K bits of information, and leave the remainder to answering 33.2K yes-or-no questions raised by the neural network. To verify its superiority, we asked three labelers to annotate 100 images (50 correctly labeled and 50 wrongly labeled) from ImageNet in one-bit setting. The average annotation time is 2.72 seconds per image (with a precision of 92.3%).
According to [30], the average time for a full-bit annotation is around 1 minute, much higher than 10× of the one-bit cost. This validates our motivation in a many-class dataset.
∗Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We notice that one-bit supervision has higher uncertainty compared to the conventional setting, because the supervision mostly comes from guessing the label of each sample. If a guess is correct, we obtain the accurate label of the sample, otherwise only one class is eliminated from the possibilities.
To learn from this setting efﬁciently, two keys should be ensured: (i) trying to improve the accuracy of each guess so as to obtain more positive labels, and (ii) making full use of the failure guesses so that the negative labels, though weak, are not wasted. This motivates us to propose a multi-stage training framework. It starts with a small number of accurate labels and makes use of off-the-shelf semi-supervised learning approaches to obtain a reasonable initial model. In each of the following stages, we allow the model to use up part of the supervision quota by querying, with its prediction, on a random subset of the unlabeled images. The correct guesses are added to the set of fully-supervised samples, and the wrong guesses compose of a set of negative labels, and we learn from it by forcing the semi-supervised algorithm to predict a very low probability on the eliminated class. After each stage, the model becomes stronger than the previous one and thus is expected to achieve higher guess accuracy in the next stage. Hence, the information obtained by one-bit supervision is signiﬁcantly enriched.
We evaluate our setting and approach on three image classiﬁcation benchmarks, namely, CIFAR100,
Mini-ImageNet and ImageNet. We choose the mean-teachers model [33] as a semi-supervised learning baseline as well as the method used for the ﬁrst training stage, and compare the accuracy under different numbers of accurate labels. Results demonstrate the superiority of one-bit supervision, and, with diagnostic experiments, we verify that the beneﬁts come from a more efﬁcient way of utilizing the information of incomplete supervision.
The remaining part of this paper is organized as follows. Section 2 illustrates the one-bit supervision setting and our solution, and Section 3 shows experiments on image classiﬁcation benchmarks. Based on these results, we discuss the relationship to prior work and future research directions in Section 4, and ﬁnally conclude our work in 5. 2 One-bit Supervision 2.1 Problem Statement
Conventional semi-supervised learning starts with a dataset of D = {xn}N n=1, where N is the total number of training samples, xn is the image data of the n-th sample. Let y(cid:63) n be the ground-truth class label2 of xn, but in our setting, y(cid:63) n is often unknown to the training algorithm. Speciﬁcally, there is a small fraction containing L samples for which y(cid:63) n is provided, and L is often much smaller than
N , e.g., as in Section 3.1, researchers often use 20% of labels on the CIFAR100 and Mini-ImageNet datasets, and only 10% of labels on the ImageNet dataset. That being said, D is partitioned into two subsets, DS and DU, where the superscripts stand for ‘supervised’ and ‘unsupervised’, respectively.
The key insight of our research is that, when the number of classes is large, it becomes very challenging to assign an accurate label for each image. According to early user studies on ImageNet [30, 41], it is even difﬁcult for a testee to memorize all the categories. This largely increases the burden of data annotation. In comparison, the cost will become much smaller if we ask the labeler ‘Does the image belong to a speciﬁc class?’ rather than ‘What is the accurate class of the image?’
To verify our motivation that the new setting indeed improves the efﬁciency of annotation, we invite three labelers who are moderately familiar with the ImageNet-1K dataset [30]. We run a pre-trained
ResNet-50 model on the test set and randomly sample 100 images, 50 correctly labeled and 50 wrongly labeled, for the three labelers to judge if the network prediction is correct. The above conﬁguration maximally approximates the scenario that a labeler can encounter in a real-world annotation process, meanwhile we avoid the labeler to bias towards either positive or negative samples by using a half-half data mix. The three labelers report an average precision of 92.3% and the average annotation time for each image is 2.72 seconds. The accuracy is acceptable provided that an experienced labeler can achieve a top-5 accuracy of ∼ 95%, yet a full annotation takes around 1 minute for each image [30], higher than 10× of our cost (log2 1000 ≈ 10). Hence, we validate the beneﬁt in learning from a large-scale dataset. 2Throughout this paper, we study the problem of image classiﬁcation, yet extending one-bit supervision to other vision tasks (e.g., object detection and semantic segmentation) may require non-trivial efforts. 2
Figure 1: The training procedure with one-bit supervision (best viewed in color). At the beginning, only a small set of training samples (blue triangles) are provided with ground-truth labels and the remaining part (black circles) remains unlabeled. We initialize the model using an off-the-shelf semi-supervised learning algorithm. In each of the following iterations (we show two iterations but there can be more), part of unlabeled data are sent into the current model for prediction, and the labeler is asked to judge if the prediction is correct. Some of these samples obtain positive labels (red circles) while some obtain negative labels (green circles). This process continues until the quota of supervision is used up as scheduled.
Motivated by the above, we formulate the problem into a new setting that incorporates semi-supervised learning and weakly-supervised learning. The dataset is partitioned into three parts, namely, D =
DS ∪ DO ∪ DU, where DO is a ﬁxed set3 for one-bit supervision. For each sample in DO, the labeler is provided with the image and a predicted label, and the task is to distinguish if the image belongs to the speciﬁc label. Note that the guess is only allowed once – if the guess is correct, the image is assigned with a positive (true) label, y(cid:63) n, otherwise, it is assigned with a negative label, denoted as y− n and no further supervision of this image can be obtained.
From the perspective of information theory, the labeler provides 1 bit of supervision to the system by answering the yes-or-no question, yet to obtain the accurate label, the average bits of required supervision is log2 C. Therefore, the burden of annotating a single image is alleviated, therefore, with the same cost, one can obtain much more one-bit annotations than full-bit annotations. We use the CIFAR100 dataset as an example. A common semi-supervised setting annotates 10K out of 50K training images which requires 10K × log2 100 = 66.4K bits of supervision. Alternately, we can annotate 5K images in full-bit and as many as 33.2K images in one-bit, resulting in the same amount of supervision, but with higher learning efﬁciency4. 3Since DO is ﬁxed, our setting stands out from the active learning paradigm in which the algorithm mines hard examples from the unlabeled set. We will show later that one-bit supervision has different behaviors, so that aggressively mining the hardest examples does not lead to best learning efﬁciency. 4Here is a disclaimer. By providing ‘one-bit’ of supervision, the system should allow the algorithm to ask a generalized question in the form of ‘Does this image belong to any class in a speciﬁc set?’ This setting is favorable to the learning algorithm especially for a weak initial model – in particular, we can start without any accurate labels. However, asking such questions can largely increase the workload of labelers, so we have constrained the query to be a single class rather than an arbitrary set of classes. From another perspective, it takes a labeler much more efforts to provide the accurate label of an image (more than log2 C× that of making a one-bit annotation). Therefore, the ‘actual’ ratio of supervision between full-supervision and one-bit supervision is larger than log2 C : 1 – in other words, under the same supervision bits, our approach actually receives a smaller amount of information. 3
2.2 A Multi-Stage Training Paradigm
There are two important factors to one-bit supervision, namely, (i) making high-quality guesses in
DO; (ii) making use of the negative (wrongly predicted) labels. We elaborate our solution for the ﬁrst factor in this subsection, and leave the second to the next subsection.
Intuitively, the accuracy of a model is improved when it sees more (fully or weakly) labeled training samples. Considering that each image in DO can be guessed only once, the straightforward strategy is to partition the training procedure into several stages, each of which makes prediction on a part of
DO and then enhances the model based on the results. This offers the generalized training algorithm as illustrated in Figure 1. The initial model, M0, is trained with a semi-supervised learning process in which DS is the labeled training set and DO ∪ DU is the unlabeled reference set. We use Mean-Teacher [33], an off-the-shelf semi-supervised learning algorithm to utilize the knowledge in the reference set. This provides us with a reasonable model to make prediction on DO.
The remaining part of training is a scheduled procedure which is composed of T iterations. Let DR t−1 denote the set of samples without accurate labels before the t-th iteration, and DR 0 . We also maintain two sets, DO+ and DO−, for the correctly and wrongly guessed samples. Both of these sets are initialized as ∅. In the t-th iteration, we ﬁrst randomly sample DO t−1, and use the previous model, Mt−1, for receiving one-bit supervision. The strategy of sampling DO t will be further discussed in Section 3.3. Then, we make use of Mt−1 to predict the labels of images in DO t and check the ground-truth. The correctly predicted samples are added to DO+ and the others added to DO−. Hence, the entire training set is split into three parts: DS ∪ DO+ (has full labels), DO− (has negative labels) and DU t . To continue iteration with a stronger model, we update Mt−1 into Mt with the currently available supervision. The fully-labeled and unlabeled parts contribute as in the semi-supervised baseline, and we concentrate on making use of the negative labels in DO−, which we will elaborate our solution in the following part. t (has no labels), and ﬁnally, DR t , a subset of DR t = DO− t ∪ DU 0 ≡ DU 2.3 Negative Label Suppression
To make use of the negative labels, we recall our baseline, the Mean-Teacher algorithm [33], that maintains two models, a teacher and a student, and trains them by assuming that they produce similar outputs – note that this does not require labels. Mathematically, given a training image, x ∈ D, we
ﬁrst compute the cross-entropy loss term if x ∈ DS ∪ DO+. In addition, no matter whether x has an accurate label, we compute the difference between the predictions of the teacher and student models as an extra loss term. Let f (x; θ) be the mathematical function of the student model where θ denotes the learnable parameters. Correspondingly, the teacher model is denoted by f (cid:0)x; θ(cid:48)(cid:1) where θ(cid:48) is the moving average of θ. The loss function is written as:
L(θ) = Ex∈DS∪DO+ (cid:96)(y(cid:63) n, f (x; θ)) + λ · Ex∈D where λ is the balancing coefﬁcient and (cid:96)(·, ·) is the cross-entropy loss. For simplicity, we have omitted the explicit notation for the individual noise added to the teacher and student model. That is being said, the model’s output on a fully-supervised training sample is constrained by both the cross-entropy and prediction consistency terms (the idea is borrowed from knowledge distillation [11, 36]).
However, for a training sample with a negative label, the ﬁrst term is unavailable, so we inject the negative label into the second term by modifying f(x; θ(cid:48)) accordingly, so that the score of the negative class is suppressed to zero5. We name this method negative label suppression (NLS). While there may exist other ways to utilize the negative labels, we believe that NLS is a straightforward and effective one that takes advantages of both the teacher model and newly added negative labels. As we shall see in experiments (Section 3.2), while being naive and easy to implement, negative label suppression brings signiﬁcant accuracy gain to the one-bit supervision procedure. (cid:12)f (cid:0)x; θ(cid:48)(cid:1) − f (x; θ)(cid:12) (cid:12) (cid:12) (1) 2
,
From a generalized perspective, NLS is a practical solution to integrate negative (weak) labels into the framework of teacher-student optimization. It is complementary to the conventional methods that used cross-entropy to absorb the information from positive (strong) labels, but instead impact the consistency loss. This is a new ﬁnding under the setting of one-bit supervision, and intuitively, as more negative labels are available (i.e., the situation is closer to full supervision), NLS may conﬂict with the design of teacher-student optimization and weakened cross-entropy may take the lead again. 5To guarantee the correctness of normalization (i.e., the scores of all classes sum to 1), in practice, we set the logit (before softmax) of the negative class to a large negative value. 4
3 Experiments 3.1 Datasets and Implementation Details
We conduct experiments on three popular image classiﬁcation benchmarks, namely, CIFAR100,
Mini-Imagenet, and Imagenet. CIFAR100 [16] contains 50K training images and 10K testing images, all of which are 32 × 32 RGB images and uniformly distributed over 100 classes. For Mini-ImageNet in which the image resolution is 84 × 84, we use the training/testing split created in [28] which consists of 100 classes, 500 training images, and 100 testing images per class. For ImageNet [3], we use the commonly used competition subset [30] which contains 1K classes, 1.3M training images, and 50K testing images, and each class has roughly the same number of samples. All images are of high resolution and pre-processed into 224 × 224 as network inputs. (cid:12)DS(cid:12) (cid:12)DS(cid:48)(cid:12) (cid:12)DO(cid:12)
We follow Mean-Teacher [33], a previous semi-supervised learning approach, to build our baseline. It assumes that a small subset, DS(cid:48), has been labeled. (cid:12) (cid:12) is 20% of the training set for CIFAR100 and
Mini-ImageNet, and 10% for ImageNet. We reschedule the assignment by allowing part of the annota-(cid:12) ≈ (cid:12) tion to be one-bit, resulting in two subsets, (cid:12) (cid:12), satisfying (cid:12) (cid:12) and (cid:12) (cid:12) / log2 C.
The detailed conﬁguration for the three datasets are shown in Table 1. Following [33], we use a 26-layer deep residual network [10] with Shake-Shake regularization [7] for CIFAR100, and a 50-layer residual network for Mini-ImageNet and ImageNet. The number of training epochs is 180 for CIFAR100 and Mini-ImageNet, and 60 for ImageNet. The consistency loss, as in [33], is computed using the mean square error in each stage for all three datasets. The consistency parameter is 1,000 for CIFAR100, and 100 for Mini-ImageNet and ImageNet. Other hyper-parameters simply follow the original implementation, except that the batch size is adjusted to ﬁt our hardware (e.g., eight NVIDIA Tesla-V100 GPUs for ImageNet experiments). (cid:12)DS(cid:48)(cid:12) (cid:12)DO(cid:12) (cid:12)DS(cid:12) (cid:12) + (cid:12) 3.2 Comparison to Full-bit Semi-supervised Supervision
We ﬁrst compare our approach to Mean-Teacher [33], a semi-supervised learning baseline in which all annotations are full-bit. Results are summarized in Table 2. One can observe that, the one-bit supervision baseline (with two stages, without utilizing the knowledge in negative labels) produces inferior performance to full-bit supervision. Note that on CIFAR100, our approach actually obtains more accurate labels via one-bit supervision (the number of accurate labels is 3K + 25.3K, while the baseline setting only has 10K), but the correct guesses are prone to the easy samples that do not contribute much to model training. The deﬁcit becomes more signiﬁcant in Mini-ImageNet on which the baseline accuracy is lower, and in ImageNet on which the number of incorrectly guessed images is larger. The reason is straightforward, namely, these samples contribute little new knowledge to the learning process because (i) the model has already learned how to classify these samples; and (ii) these samples are relatively easy compared to the incorrectly predicted ones. Therefore, making use of the negative labels is crucial for one-bit supervision.
We next investigate negative label suppression, the basic method to extract knowledge from incorrect guesses. Results are listed in Table 2. One can observe signiﬁcant improvement brought by simply suppressing the score of the incorrect class for each element in DO−. Compared to the two-stage baseline, this brings 4.37%, 5.86%, and 4.76% accuracy gains on CIFAR100, Mini-ImageNet, and
ImageNet, respectively. That being said, the negative labels, though only ﬁltering out one out of 100 or 1,000 classes, can help substantially in the scenario of semi-supervised learning, and the key contribution is to avoid the teacher and student models from arriving in a wrong consensus.
Table 1: The data split for semi-supervised and one-bit-supervised learning, where the total numbers of supervision bits are comparable. We will investigate other data splits in Section 3.3.
Dataset
CIFAR100
Mini-ImageNet
ImageNet
C 100 100 1,000 log2 C 6.6439 6.6439 9.9658
|D|
# of bits 66.9K 66.9K 1281K 128K 1276K 30K 977K 1276K one-bit-supervised (cid:12)DO(cid:12) (cid:12)DS(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 47K 3K 47K 3K semi-supervised (cid:12)DS(cid:12) (cid:12)
# of bits (cid:12) 66.4K 10K 66.4K 10K 50K 50K 5
In summary, with two-stage training and negative label suppression, our approach achieves favorable performance in one-bit supervision. In particular, under the same bits of supervision, the accuracy gain over the baseline, Mean-Teacher, is 4.00%, 4.48%, and 2.24% on CIFAR100, Mini-ImageNet, and ImageNet, respectively. Provided that the current question (querying whether the image belongs to one speciﬁc class) actually obtains less information than 1 bit (the true one-bit supervision should allow the querying subset to be arbitrary, not limited within one class), the effectiveness of the learning framework, as well as our multi-stage training algorithm, becomes clearer. Though we have only tested on top of the Mean-Teacher algorithm, we believe that the pipeline is generalized to other semi-supervised approaches as well as other network backbones.
Table 2 also lists three recently published methods using different backbones. We hope to deliver two messages. First, one-bit supervision is friendly to stronger backbones since it often leads to a better base model. Second, being a new learning pipeline, one-bit supervision with multi-stage training can be freely integrated into these methods towards better performance. 3.3 Number of Stages and Guessing Strategies
Now, we ablate the proposed algorithm by altering the guessing strategy, namely, the number of stages used, the number of fully supervised samples, the strategy of sampling DO, etc.
First, we compare one-stage training with two-stage training. To be speciﬁc, the former option uses up the quota of one-bit supervision all at once, and the latter uses part of them for training an intermediate model and iterates again to obtain the ﬁnal model. Results on the three image classiﬁcation benchmarks are shown in Table 2. The advantage of using two-stage training is clear.
This mainly owes to that the intermediate model is strengthened by one-bit supervision and thus can
ﬁnd more positive labels than the initial model. On CIFAR100, Mini-ImageNet and ImageNet, the numbers of correct guesses using one-stage training are around 23.2K, 9.8K, and 470K, while using two-stage training, these numbers become 25.3K, 12.2K, and 475K, respectively. Consequently, from one-stage to two-stage training the accuracy of the ﬁnal model is boosted by 2.63%, 7.24%, and 3.46% on the three datasets, respectively.
Table 3: Accuracy (%) of using different partitions of quota in the two-stage training process.
As a follow-up study, we investigate (i) the split of quota between two stages and (ii) using more training stages. For (i), Table 3 shows four op-tions of assigning the 47K quota to two stages.
One can observe the importance of making a bal-anced schedule, i.e., the accuracy drops consis-tently when the ﬁrst-stage uses either too many or too few quota. Intuitively, either case will push the training paradigm towards one-stage training which was veriﬁed less efﬁcient in one-bit supervision. For (ii), experiments are performed on CIFAR100. We use three training stages 1st-stage quota CIFAR100 Mini-ImageNet 73.36 74.10 73.76 73.33 45.30 45.45 45.54 44.15 10K 20K 27K 37K
Table 2: Comparison of accuracy (%) to our baseline, Mean-Teacher [33], and some state-of-the-art semi-supervised learning methods. On all the datasets, we report the top-1 accuracy. In our multi-stage training process, we report the accuracy after initialization (using Mean-Teacher for semi-supervised learning) as well as after each one-bit supervision stage. The discussion of using one or two stages is in Section 3.3. NLS indicates negative label suppression (see Section 2.3).
Method
Π-Model [17]
DCT [26]
LPDSSL [14]
Mean Teacher [33]
Ours (1-stage base)
+NLS
CIFAR100 56.57 (ConvNet-13) 61.23 (ConvNet-13) 64.08 (ConvNet-13) 69.76 (ResNet-26) 51.47 → 66.26 51.47 → 71.13
Mini-ImageNet
--42.65 (ResNet-18) 41.06 (ResNet-50) 22.36 → 35.88 22.36 → 38.30
ImageNet
-53.50 (ResNet-18)
-58.16 (ResNet-50) 47.83 → 54.46 47.83 → 58.52
Ours (2-stage base) 51.47 → 64.83 → 69.39 22.36 → 33.97 → 39.68 47.83 → 54.04 → 55.64 51.47 → 67.82 → 73.76 22.36 → 37.92 → 45.54 47.83 → 57.44 → 60.40
+NLS 6
and, following the conclusions of (i), split the quota uniformly into the three stages. From the ﬁrst to the last, each stage has 15K, 17K, and 15K guesses, respectively. The ﬁnal test accuracy is 74.72%, comparable to 73.76% obtained by two-stage training. Though three-stage training brings a considerable accuracy gain (around 1%) over two-stage training, we point out that the gain is much smaller than 2.63% (two-stage training over one-stage training). Considering the tradeoff between accuracy and computational costs, we use two-stage training with balanced quota over two stages.
Table 4: Accuracy (%) of using different numbers of labeled samples for the three datasets.
Second, we study the impact of different sizes of DS, i.e., the labeled set in the beginning. On
CIFAR100 and Mini-ImageNet, we adjust the size to 1K, 3K (as in main experiments), 5K, and 10K (the baseline setting with no one-bit supervision); on ImageNet, the corresponding numbers are 10K, 30K (as in main experiments), 50K, and 128K (the baseline setting), respec-tively. As shown in the right table, the optimal solution is to keep a proper amount (e.g., 30%–50%) of full-bit supervision and exchange the re-maining quota to one-bit supervision. When the portion of full-bit supervision is too small, the initial model may be too weak to extract positive labels via prediction; when the portion is too large, the advantage of one-bit supervision becomes small and the algorithm degenerates to a regular semi-supervised learning process. This conclusion partly overlaps with the previous one, showing the advantage of making a balanced schedule of using supervision, including assigning the quota between the same or different types of supervision forms. 30 65.06 73.76 73.90 34.85 45.54 45.64 55.42 60.40 61.03
# labels per class
CIFAR100
Mini-ImageNet
ImageNet 10 50
Third, we investigate the strategy of selecting samples for querying. We still use the two-stage training procedure. Differently, each time when we need to sample from S O, we no longer assign all unlabeled samples with a uniform probability but instead measure the difﬁculty of each sample using the top-ranked score after the softmax computation. Here are two strategies, namely, taking out the easiest samples (with the highest scores) and the hardest samples (with the lowest scores), respectively. Note that both strategies can impact heavily on the guess accuracy, e.g., on CIFAR100, the easy selection and hard selection strategies lead to a total of 30.9K and 18.0K correct guesses, signiﬁcantly different from 25.3K of random sampling. Correspondingly, the ﬁnal accuracy is slightly changed from 73.76% to 74.23% and 74.96%. However, on Mini-ImageNet, the same operation causes the accuracy to drop from 45.54% to 44.22% and 42.57% respectively. That being said, though the easy selection strategy produces more positive labels, most of them are easy samples and do not deliver much knowledge to the model; in comparison, the hard selection strategy obtains supervision from the challenging cases, but the number of positive labels may be largely reduced.
Therefore, if the dataset is relatively easy or the model has achieved a relatively high accuracy, the hard selection strategy can beneﬁt from hard example mining, e.g., the accuracy is boosted by over 1% on CIFAR100. But, when the model is not that strong compared to the dataset, this strategy can incur performance drop, e.g., on Mini-ImageNet, the accuracy drops about 3%.
In summary, in the setting of one-bit supervision, it is important to design an efﬁcient sampling strategy so that maximal information can be extracted from the ﬁxed quota of querying. We have presented some heuristic strategies including using multiple stages and performing uniform sampling.
Though consistent accuracy gain is obtained on all three classiﬁcation benchmarks, we believe that more efﬁcient strategies exist, and may be strongly required to generalize one-bit supervision to a wider range of learning tasks and/or network backbones. 4 Discussions: Past and Future 4.1 Past: