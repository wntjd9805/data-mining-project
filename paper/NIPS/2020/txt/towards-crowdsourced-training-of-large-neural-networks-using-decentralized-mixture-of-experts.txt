Abstract
Many recent breakthroughs in deep learning were achieved by training increas-ingly larger models on massive datasets. However, training such models can be prohibitively expensive. For instance, the cluster used to train GPT-3 costs over
$250 million2. As a result, most researchers cannot afford to train state of the art models and contribute to their development. Hypothetically, a researcher could crowdsource the training of large neural networks with thousands of regular PCs provided by volunteers. The raw computing power of a hundred thousand $2500 desktops dwarfs that of a $250M server pod, but one cannot utilize that power efﬁciently with conventional distributed training methods. In this work, we propose
Learning@home: a novel neural network training paradigm designed to handle large amounts of poorly connected participants. We analyze the performance, reliability, and architectural constraints of this paradigm and compare it against existing distributed training techniques. 1

Introduction
Our investigation begins with a thought experiment. Imagine a deep neural network with capacity 1000 times greater than today’s most powerful architectures: for example, a language model trained on all digitally available texts or a generative model for all images ever uploaded to the Internet. How can we train such a model?
Viewed from a historical perspective, the 1000-fold increase in capacity is not unrealistic. Over the past decade, the deep learning community has made remarkable progress by training large models on abundant data, and the scale of those models keeps growing. Since the advent of the ImageNet challenge [1] with 1.3M labeled images, the typical size of convolutional neural networks increased from a few megabytes to hundreds of megabytes [2, 3, 4]. Recent studies report even larger models for datasets with hundreds of millions of images [5, 6].
Another trend from natural language processing is to train large Transformer-like language models [7, 8, 9]. The data for this task is nearly unlimited, allowing researchers to train models with tens or even hundreds of gigabytes of parameters [10, 11, 12, 13]. While we may not need the 1000-fold increase at the moment, planning for it will prepare us for the next big leap in model capacity.
To be speciﬁc, let us focus on training large Transformer networks for the language modeling task. At the time of writing, the largest conventional model for that task is GPT-3 with 175 billion parameters.
Scaling it up 1000 times gives us 175 trillion; depending on whether you use single or half-precision, this requires 300–600 terabytes of memory just to store the model. No modern mass-produced hardware accelerator is up to such task. Even high-end servers with 16x V100 accelerators can store only 0.15% of that model in combined GPU memory, let alone train it.
∗Corresponding author. 2A conservative estimate based on https://blogs.microsoft.com/ai/openai-azure-supercomputer 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The dominant way of growing neural network size has so far been to scale up: deploy more powerful computational accelerators in specialized tightly interconnected clusters. However, this approach will only work up to a point. Models such as T-NLG [13] and Megatron-LM [11] were already trained on
DGX-SuperPOD — a supercomputer with hundreds of Tesla V100 GPUs spread over tens of servers.
As for GPT-3 [10], a single training run was estimated to cost 4.6 – 12 million dollars [14, 15].
Even today, the need for costly hardware weighs heavily on the research community. Most researchers cannot contribute to the development of large neural networks because conducting the necessary experiments would be too expensive for them. If we continue to increase the model size by scaling up, eventually the only labs that can conduct competitive research will be those with massive budgets.
However, there is another solution: to scale out. Instead of using a supercomputer, researchers could crowdsource the computation from volunteers with regular PCs. This paradigm is known as volunteer computing and was successfully applied to solve problems in biology [16], high energy physics [17] and other subject areas. While a single volunteer PC may be slow and unreliable, the combined
ﬂoating-point performance of such projects is on par with largest supercomputers [18].
The main challenge of volunteer computing is how to utilize this performance. Unlike server pods, consumer-grade PCs communicate over the Internet, which is signiﬁcantly slower, especially in terms of latency. They are also more prone to failures as they lack many reliability features of their server-grade counterparts. Therefore, volunteer computing was traditionally used for tasks that have high computation to communication ratio and can recover from individual node failures.
Unfortunately, existing paradigms of distributed training require nodes to continuously transfer large amounts of intermediate data [19, 20], making them unsuitable for volunteer computing. In this work, we take a different approach. Instead of adopting the existing distributed training strategies, we identify the advantages of volunteer computing and design a new strategy that capitalizes on them.
We summarize the contributions of our paper as follows:
• We propose Decentralized Mixture of Experts (DMoE) — a layer designed for training with vast amounts of unreliable consumer-grade hardware;
• We describe a framework for training large neural networks composed of DMoE layers;
• We conﬁrm the efﬁciency and reliability of this ap-proach using formal guarantees and experiments;
• The PyTorch source code that can be used to repro-duce our results is available online3. 2