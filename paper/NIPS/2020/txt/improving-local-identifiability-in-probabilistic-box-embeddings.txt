Abstract
Geometric embeddings have recently received attention for their natural ability to represent transitive asymmetric relations via containment. Box embeddings, where objects are represented by n-dimensional hyperrectangles, are a particularly promising example of such an embedding as they are closed under intersection and their volume can be calculated easily, allowing them to naturally represent calibrated probability distributions. The beneﬁts of geometric embeddings also introduce a problem of local identiﬁability, however, where whole neighborhoods of parameters result in equivalent loss which impedes learning. Prior work addressed some of these issues by using an approximation to Gaussian convolution over the box parameters, however this intersection operation also increases the sparsity of the gradient. In this work we model the box parameters with min and max
Gumbel distributions, which were chosen such that the space is still closed under the operation of intersection. The calculation of the expected intersection volume involves all parameters, and we demonstrate experimentally that this drastically improves the ability of such models to learn. 1

Introduction
Geometric embedding models have recently been explored for their ability to learn hierarchies, transitive relations, and partial order structures. Rather than representing objects with vectors, geometric representation models associate domain elements, such as knowledge base queries, images, sentences, concepts, or graph nodes, with objects whose geometry is more suited to expressing relationships in the domain.
Geometric embedding models have used Gaussian densities [25, 1], convex cones, as in order embeddings and entailment cones [24, 10, 5], and axis-aligned hyperrectangles, as in box embeddings
∗Equal Contributions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Disjoint (b) Fully Contained (c) Contained (per dimension) (d) Disjoint / Contained (per dimension)
Figure 1: Settings of parameters which lack local identiﬁability. For example, any local perturbation of a preserves zero joint probability. For b and c independent local translations of the boxes (for example) preserve the joint probability. Prior work [13] improves only the ﬁrst case, and even then only to a degree, as it still suffers a lack of local identiﬁability for settings such as d.
[26, 22, 13] and query2box [21]. Recent work also explores extensions to non-Euclidean spaces such as Poincaré embeddings [18] and hyperbolic entailment cones [5].
These representations can provide a much more natural basis for transitive relational data, where e.g. entailment can be represented as inclusion among cones or boxes. Additionally, these methods allow for intrinsic notions of an objects breadth of scope or marginal probability, as well as the ability to accurately represent inherently multimodal or ambiguous concepts.
In this work, we focus on the probabilistic box embedding model, which represents the probabilities of binary random variables in terms of volumes of axis-aligned hyperrectangles. While this model
ﬂexibly allows for the expression of positively and negatively correlated random variables with complex latent dependency structures, it can be difﬁcult to train due to a lack of local identiﬁability in the parameter space. That is, large neighborhoods of parameters can give equivalent probabilities to the same events, which leads to gradient sparsity during training (see ﬁgure 1). Previous work partially addresses some settings of parameters which lack local identiﬁability that arise when training box embeddings using Gaussian convolutions [13], but many still remain, leading to a variety of pathological parameter settings, impeding learning.
We generalize the probabilistic box embedding model to a random process model over parametric families of box embeddings, which we term the Gumbel-box processes. Our approximation to the marginal likelihood of this model has an intuitively pleasing closed form. The resulting model replaces the sparse gradients of the base model’s volume calculations with effectively log-smoothed approximations having support on the entire parameter space.
We apply our model to a variety of synthetic problems and demonstrate that the new objective function correctly solves cases that are not solvable by any previous approaches to ﬁtting box models.
In real-world data, we demonstrate improved performance on a WordNet completion task and a
MovieLens density estimation task.2. 2