Abstract
Over-parameterized models, such as DeepNets and ConvNets, form a class of models that are routinely adopted in a wide variety of applications, and for which
Bayesian inference is desirable but extremely challenging. Variational inference offers the tools to tackle this challenge in a scalable way and with some degree of
ﬂexibility on the approximation, but for over-parameterized models this is challeng-ing due to the over-regularization property of the variational objective. Inspired by the literature on kernel methods, and in particular on structured approxima-tions of distributions of random matrices, this paper proposes Walsh-Hadamard
Variational Inference (WHVI), which uses Walsh-Hadamard-based factorization strategies to reduce the parameterization and accelerate computations, thus avoid-ing over-regularization issues with the variational objective. Extensive theoretical and empirical analyses demonstrate that WHVI yields considerable speedups and model reductions compared to other techniques to carry out approximate inference for over-parameterized models, and ultimately show how advances in kernel meth-ods can be translated into advances in approximate Bayesian inference for Deep
Learning. 1

Introduction
Since its inception, Variational Inference (VI, [25]) has continuously gained popularity as a scalable and ﬂexible approximate inference scheme for a variety of models for which exact Bayesian inference is intractable. Bayesian neural networks [35, 38] represent a good example of models for which inference is intractable, and for which VI– and approximate inference in general – is challenging due to the nontrivial form of the posterior distribution and the large dimensionality of the parameter space [17, 14]. Recent advances in VI allow one to effectively deal with these issues in various ways.
For instance, a ﬂexible class of posterior approximations can be constructed using, e.g., normalizing
ﬂows [46], whereas the need to operate with large parameter spaces has pushed the research in the direction of Bayesian compression [34, 36].
Employing VI is notoriously challenging for over-parameterized statistical models. In this paper, we focus in particular on Bayesian Deep Neural Networks (DNNs) and Bayesian Convolutional Neural
Networks (CNNs) as typical examples of over-parameterized models. Let’s consider a supervised 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
∗Equal contribution
learning task with N input vectors and corresponding labels collected in X = {x1, . . . , xN } and Y = {y1, . . . , yN }, respectively; furthermore, let’s consider DNNs with weight matrices
W = (cid:8)W (1), . . . , W (L)(cid:9), likelihood p(Y |X, W), and prior p(W). Following standard variational arguments, after introducing an approximation q(W) to the posterior p(W|X, Y ) it is possible to obtain a lower bound to the log-marginal likelihood log [p(Y |X)] as follows: log [p(Y |X)] ≥ Eq(W)[log p(Y |X, W)] − KL{q(W)(cid:107)p(W)} . (1)
The ﬁrst term acts as a model ﬁtting term, whereas the second one acts as a regularizer, penalizing solutions where the posterior is far away from the prior. It is easy to verify that the KL term can be the dominant one in the objective for over-parameterized models. For example, a mean ﬁeld posterior approximation turns the KL term into a sum of as many KL terms as the number of model parameters, say Q, which can dominate the overall objective when Q (cid:29) N . As a result, the optimization focuses on keeping the approximate posterior close to the prior, disregarding the rather important model
ﬁtting term. This issue has been observed in a variety of deep models [3], where it was proposed to gradually include the KL term throughout the optimization [3, 50] to scale up the model ﬁtting term [58, 57] or to improve the initialization of variational parameters [47]. Alternatively, other approximate inference methods for deep models with connections to VI have been proposed, notably
Monte Carlo Dropout [MCD; 14] and Noisy Natural Gradients [NNG; 62].
In this paper, we propose a novel strategy to cope with model over-parameterization when using variational inference, which is inspired by the literature on kernel methods. Our proposal is to repa-rameterize the variational posterior over model parameters by means of a structured decomposition based on random matrix theory [54], which has inspired a number of fundamental contributions in the literature on approximations for kernel methods, such as FASTFOOD [31] and Orthogonal Random
Features (ORF, [60]). The key operation within our proposal is the Walsh-Hadamard transform, and this is why we name our proposal Walsh-Hadamard Variational Inference (WHVI).
Without loss of generality, consider Bayesian DNNs with weight matrices W (l) of size D × D.
Compared with mean ﬁeld VI, WHVI has a number of attractive properties. The number of parameters is reduced from O(D2) to O(D), thus reducing the over-regularization effect of the KL term in the variational objective. We derive expressions for the reparameterization and the local reparameteri-zation tricks, showing that, the computational complexity is reduced from O(D2) to O(D log D).
Finally, unlike mean ﬁeld VI, WHVI induces a matrix-variate distribution to approximate the posterior over the weights, thus increasing ﬂexibility at a log-linear cost in D instead of linear.
We can think of our proposal as a speciﬁc factorization of the weight matrix, so we can speculate that other tensor factorizations [42] of the weight matrix could equally yield such beneﬁts. Our comparison against various matrix factorization alternatives, however, shows that WHVI is superior to other parameterizations that have the same complexity. Furthermore, while matrix-variate posterior approximations have been proposed in the literature of VI [32], this comes at the expense of increasing the complexity, while our proposal keeps the complexity to log-linear in D.
Through a wide range of experiments on DNNs and CNNs, we demonstrate that our approach enables the possibility to run variational inference on complex over-parameterized models, while being competitive with state-of-the-art alternatives. Ultimately, our proposal shows how advances in kernel methods can be instrumental in improving VI, much like previous works showed how kernel methods can improve, e.g., Markov chain Monte Carlo sampling [48, 52] and statistical testing [18, 19, 61]. 2 Walsh-Hadamard Variational Inference 2.1