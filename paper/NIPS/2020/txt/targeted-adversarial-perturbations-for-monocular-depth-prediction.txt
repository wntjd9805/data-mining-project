Abstract
We study the effect of adversarial perturbations on the task of monocular depth prediction. Speciﬁcally, we explore the ability of small, imperceptible additive perturbations to selectively alter the perceived geometry of the scene. We show that such perturbations can not only globally re-scale the predicted distances from the camera, but also alter the prediction to match a different target scene. We also show that, when given semantic or instance information, perturbations can fool the network to alter the depth of speciﬁc categories or instances in the scene, and even remove them while preserving the rest of the scene. To understand the effect of targeted perturbations, we conduct experiments on state-of-the-art monocular depth prediction methods. Our experiments reveal vulnerabilities in monocular depth prediction networks, and shed light on the biases and context learned by them.
Figure 1: Altering the predicted scene with adversarial perturbations. Top to bottom: input image; adversarial perturbations with upper norm of 2 × 10−2; predicted scene visualized as disparity.
Left to right: original image and predicted scene; overall scene altered to be 10% closer; all vehicles altered to be 10% closer; vehicle in the center of the road is removed by perturbations. 1

Introduction
Consider the image shown in the top-left of Fig. 1, captured from a moving car. The corresponding depth of the scene, inferred by a deep neural network and visualized as disparity, is shown underneath.
Can adding a small perturbation cause the perceived vehicle in front of us disappear? Indeed, this is shown on the rightmost panel of the same ﬁgure: The perturbed image, shown on the top-right, is indistinguishable from the original. Yet, the perturbation, ampliﬁed and shown in the center row, causes the depth map to be altered in a way that makes the car in front of us disappear.
Adversarial perturbations are small signals that, when added to images, are imperceptible yet can cause the output of a deep neural network to change catastrophically [37]. We know that they can fool a network to mistake a tree for a peacock [27]. But, as autonomous vehicles are increasingly employing learned perception modules, mistaking a stop sign for a speed limit [8] or causing obstacles to disappear is not just an interesting academic exercise. We explore the possibility that small perturbations can alter not just the class label associated to an image, but the inferred depth map, for instance to make the entire scene appear closer or farther, or portions of the scene, like speciﬁc objects, to become invisible or be perceived as being elsewhere in the scene. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
When semantic segmentation is available, perturbations can target a speciﬁc category in the predicted scene. Some categories (e.g. trafﬁc lights, humans) are harder to attack than others (e.g. roads, nature). When instance segmentation is available, perturbations can manipulate individual objects, for instance make a car disappear or move it to another location. We call these phenomena collectively as stereopagnosia, as the solid geometric analogue of prosopagnosia [4].
Stereopagnosia sheds light on the role of context in the representation of geometry with deep networks.
When attacking a speciﬁc category or instance, while most of the perturbations are localized, some are distributed throughout the scene, far from the object of interest. Even when the target effect is localized (e.g., make a car disappear), the perturbations are non-local, indicating that the network exploits non-local context, which represents a vulnerability. Could one perturb regions in the image, for instance displaying billboards, thus making cars seemingly disappear?
We note that, although the adversarial perturbations we consider are not universal, that is, they are tailored to a speciﬁc scene and its corresponding image, they are somewhat robust. Blurring the image after applying the perturbations reduces, but does not eliminate, stereopagnosia. To understand generalizability of adversarial perturbations, we examine the transferability of the perturbations between monocular depth prediction models with different architectures and losses. 2