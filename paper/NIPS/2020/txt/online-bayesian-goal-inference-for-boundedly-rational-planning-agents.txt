Abstract
People routinely infer the goals of others by observing their actions over time.
Remarkably, we can do so even when those actions lead to failure, enabling us to assist others when we detect that they might not achieve their goals. How might we endow machines with similar capabilities? Here we present an architecture capable of inferring an agent’s goals online from both optimal and non-optimal sequences of actions. Our architecture models agents as boundedly-rational planners that interleave search with execution by replanning, thereby accounting for sub-optimal behavior. These models are speciﬁed as probabilistic programs, allowing us to rep-resent and perform efﬁcient Bayesian inference over an agent’s goals and internal planning processes. To perform such inference, we develop Sequential Inverse
Plan Search (SIPS), a sequential Monte Carlo algorithm that exploits the online replanning assumption of these models, limiting computation by incrementally extending inferred plans as new actions are observed. We present experiments showing that this modeling and inference architecture outperforms Bayesian inverse reinforcement learning baselines, accurately inferring goals from both optimal and non-optimal trajectories involving failure and back-tracking, while generalizing across domains with compositional structure and sparse rewards. 1

Introduction
Everyday experience tells us that it is impossible to plan ahead for everything. Yet, not only do humans still manage to achieve our goals by piecing together partial and approximate plans, we also appear to account for this cognitive strategy when inferring the goals of others, understanding that they might plan and act sub-optimally, or even fail to achieve their goals. Indeed, even 18-month old infants seem capable of such inferences, offering their assistance to adults after observing them execute failed plans [1]. How might we understand this ability to infer goals from such plans? And how might we endow machines with this capacity, so they might assist us when our plans fail?
While there has been considerable work on inferring the goals and desires of agents, much of this work has assumed that agents act optimally to achieve their goals. Even when this assumption is relaxed, the forms of sub-optimality considered are often highly simpliﬁed. In inverse reinforcement learning, for example, agents are assumed to either act optimally [2] or to exhibit Boltzmann-rational action noise [3], while in the plan recognition literature, longer plans are assigned exponentially decreasing probability [4]. None of these approaches account for the difﬁculty of planning itself, which may lead agents to produce sub-optimal or failed plans. This not only makes them ill-equipped to infer goals from such plans, but also saddles them with a cognitively implausible burden: If inferring an agent’s goals requires knowing the optimal solution to reach each goal, then an observer would need to compute the optimal plan or policy for all of those goals in advance [5]. Outside of the simplest problems and domains, this is deeply intractable. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Our architecture performing online Bayesian goal inference via Sequential Inverse Plan
Search. In (a), an agent exhibits a sub-optimal plan to acquire the blue gem, backtracking to pick up the key required for the second door. In (b), an agent exhibits a failed plan to acquire the blue gem, myopically using up its ﬁrst key to get closer to the gem instead of realizing that it needs to collect the bottom two keys. In both cases, our method not only manages to infer the correct goal by the end, but also captures sharp human-like shifts in its inferences at key points, such as (a.ii) when the agent picks up a key unnecessary for the red gem, (a.ii) when the agent starts to backtrack, (b.iii) when the agent ignores the door to the red gem, or (b.iv) when the agent unlocks the ﬁrst door to the blue gem.
In this paper, we present a uniﬁed modeling and inference architecture (Figure 2) that addresses both of these limitations. In contrast to prior work that models agents as actors that are noisily rational, we model agents as planners that are boundedly rational with respect to how much they plan, interleaving resource-limited plan search with plan execution. This allows us to perform online Bayesian inference of plans and goals even from highly sub-optimal trajectories involving backtracking or irreversible failure (Figure 1). We do so by modeling agents as probabilistic programs (Figure 3), comprised of goal priors and domain-general planning algorithms (Figure 2i), and interacting with a symbolic environment model (Figure 2ii). Inference is then performed via Sequential Inverse Plan Search (SIPS), a sequential Monte Carlo (SMC) algorithm that exploits the replanning assumption of our agent models, incrementally inferring partial plans while limiting computational cost (Figure 2iii).
Our architecture delivers both accuracy and speed by being built in Gen, a general-purpose prob-abilistic programming system that supports customized inference using data-driven proposals and involutive rejuvenation kernels [6, 7, 8], alongside an embedding of the Planning Domain Deﬁnition
Language [9, 10], enabling the use of fast general-purpose planners [11] as modeling components.
We evaluate our approach against a Bayesian inverse reinforcement learning baseline [12] on a wide variety of planning domains that exhibit compositional task structure and sparse rewards (e.g. Figure 1), achieving high accuracy on many domains, often with orders of magnitude less computation. 2
Figure 2: Our modeling and inference architecture is comprised of: (i) A programmatic model of a boundedly rational planning agent, implemented in the Gen probabilistic programming system; (ii)
An environment model speciﬁed in the Planning Domain Deﬁnition Language (PDDL), facilitating support for a wide variety of planning domains and state-of-the-art symbolic planners; (iii) Sequential
Inverse Plan Search (SIPS), a novel SMC algorithm that exploits the replanning assumption of our agent model to reduce computation, extending hypothesized plans only as new observations arrive. 2