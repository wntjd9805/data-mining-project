Abstract
This work develops central limit theorems for cross-validation and consistent esti-mators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact conﬁ-dence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the ﬁrst of their kind for the popular choice of leave-one-out cross-validation.
In our real-data experiments with diverse learning algorithms, the resulting inter-vals and tests outperform the most popular alternative methods from the literature. 1

Introduction
Cross-validation (CV) [49, 26] is a de facto standard for estimating the test error of a prediction rule. By partitioning a dataset into k equal-sized validation sets, ﬁtting a prediction rule with each validation set held out, evaluating each prediction rule on its corresponding held-out set, and aver-aging the k error estimates, CV produces an unbiased estimate of the test error with lower variance than a single train-validation split could provide. However, these properties alone are insufﬁcient for high-stakes applications in which the uncertainty of an error estimate impacts decision-making.
In predictive cancer prognosis and mortality prediction for instance, scientists and clinicians rely on test error conﬁdence intervals (CIs) based on CV and other repeated sample splitting estimators to avoid spurious ﬁndings and improve reproducibility [42, 45]. Unfortunately, the CIs most often used have no correctness guarantees and can be severely misleading [30]. The difﬁculty comes from the dependence across the k averaged error estimates: if the estimates were independent, one could derive an asymptotically-exact CI (i.e., a CI with coverage converging exactly to the target level) for test error using a standard central limit theorem. However, the error estimates are seldom inde-pendent, due to the overlap amongst training sets and between different training and validation sets.
Thus, new tools are needed to develop valid, informative CIs based on CV.
The same uncertainty considerations are relevant when comparing two machine learning methods: before selecting a prediction rule for deployment, one would like to be conﬁdent that its test error is better than a baseline or an available alternative. The standard practice amongst both method developers and consumers is to conduct a formal hypothesis test for a difference in test error between two prediction rules [22, 38, 43, 13, 19]. Unfortunately, the most popular tests from the literature like the cross-validated t-test [22], the repeated train-validation t-test [43], and the 5 × 2 CV test
[22] have no correctness guarantees and hence can produce misleading conclusions. The difﬁculty parallels that of the conﬁdence interval setting: standard tests assume independence and do not
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
appropriately account for the dependencies across CV error estimates. Therefore, new tools are also needed to develop valid, powerful tests for test error improvement based on CV.
Our contributions To meet these needs, we characterize the asymptotic distribution of CV error and develop consistent estimates of its variance under weak stability conditions on the learning al-gorithm. Together, these results provide practical, asymptotically-exact conﬁdence intervals for test error as well as valid and powerful hypothesis tests of whether one learning algorithm has smaller test error than another. In more detail, we prove in Sec. 2 that k-fold CV error is asymptotically normal around its test error under an abstract asymptotic linearity condition. We then give in Sec. 3 two different stability conditions that hold for large classes of learning algorithms and losses and that individually imply the asymptotic linearity condition. In Sec. 4, we propose two estimators of the asymptotic variance of CV and prove them to be consistent under similar stability conditions; our second estimator accommodates any choice of k and appears to be the ﬁrst consistent variance estimator for leave-one-out CV. To validate our theory in Sec. 5, we apply our intervals and tests to a diverse collection of classiﬁcation and regression methods on particle physics and ﬂight delay data and observe consistent improvements in width and power over the most popular alternative methods from the literature.