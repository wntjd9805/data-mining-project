Abstract
Score function-based natural language generation (NLG) approaches such as RE-INFORCE, in general, suffer from low sample efﬁciency and training instability problems. This is mainly due to the non-differentiable nature of the discrete space sampling and thus these methods have to treat the discriminator as a black box and ignore the gradient information. To improve the sample efﬁciency and reduce the variance of REINFORCE, we propose a novel approach, TaylorGAN, which augments the gradient estimation by off-policy update and the ﬁrst-order Taylor ex-pansion. This approach enables us to train NLG models from scratch with smaller batch size — without maximum likelihood pre-training, and outperforms existing
GAN-based methods on multiple metrics of quality and diversity.1 1

Introduction
Generative adversarial networks (GAN) [14] have advanced many applications such as image gen-eration and unsupervised style transfer [19, 28]. Unsurprisingly, much effort has been devoted to adopting the GAN framework for unsupervised text generation [5, 9, 11, 17, 25, 32, 33]. However, in natural language generation (NLG), more challenges are concerned, such as passing discrete tokens through a non-differentiable operation, which prohibits backpropagating the gradient signal to the generator.
To address the issue about non-differentiablity, researchers and practitioners used score function-based gradient estimators such as REINFORCE to train GANs for NLG, where the discriminator is cast as a reward function for the generator. These methods suffer from poor sample efﬁciency, high variance, and credit assignment problems. We argue that it is disadvantageous to utilize the discriminator as a simple reward function when it is known that gradient-based backpropagation is more effective for optimization.
In this paper, we propose a novel unsupervised NLG technique, TaylorGAN, where the approximated reward of sequences improves the efﬁciency and accuracy of the estimator. Our contributions are 3-fold:
•
•
•
This paper proposes a novel update formula for the generator to incorporate gradient information from the discriminator during training.
The experiments demonstrate that TaylorGAN achieves state-of-the-art performance without maximum likelihood pre-training.
Our model does not require additional variance reduction techniques used in other
REINFORCE-based counterparts such as large batch size [9], value estimation model
[17] and Monte-Carlo rollouts [32]. 1The source code and data are available at https://github.com/MiuLab/TaylorGAN/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2