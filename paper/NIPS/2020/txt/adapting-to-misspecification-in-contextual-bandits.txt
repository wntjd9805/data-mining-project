Abstract
A major research direction in contextual bandits is to develop algorithms that are computationally efﬁcient, yet support ﬂexible, general-purpose function approx-imation. Algorithms based on modeling rewards have shown strong empirical performance, yet typically require a well-speciﬁed model, and can fail when this assumption does not hold. Can we design algorithms that are efﬁcient and ﬂexible, yet degrade gracefully in the face of model misspeciﬁcation? We introduce a new family of oracle-efﬁcient algorithms for ε-misspeciﬁed contextual bandits that adapt to unknown model misspeciﬁcation—both for ﬁnite and inﬁnite action settings. Given access to an online oracle for square loss regression, our algorithm attains optimal regret and—in particular—optimal dependence on the misspeciﬁ-cation level, with no prior knowledge. Specializing to linear contextual bandits with inﬁnite actions in d dimensions, we obtain the ﬁrst algorithm that achieves the optimal ˜O(d dT ) regret bound for unknown ε.
On a conceptual level, our results are enabled by a new optimization-based per-spective on the regression oracle reduction framework of Foster and Rakhlin [20], which we believe will be useful more broadly.
T + ε
√
√ 1

Introduction
The contextual bandit (CB) problem is an extension of the standard multi-armed bandit problem that is relevant to a variety of applications in practice, including health services [42], online advertisement
[34, 4] and recommendation systems [8]. In the contextual bandit setting, at each round, the learner observes a feature vector (or context) and an action set. The learner must select an action out of that set and only observes the reward of that action. To make its selection, the learner has access to a family of hypotheses (or policies), which map contexts to actions. The objective of the learner is to achieve a cumulative reward that is close to that of the best hypothesis in hindsight for that speciﬁc sequence of contexts and action sets.
A common approach to the contextual bandit problem consists of reducing it to a supervised learning task such as classiﬁcation or regression [32, 19, 6, 7, 41, 8, 35]. Recently, Foster and Rakhlin [20] proposed SquareCB, an efﬁcient reduction from K-armed contextual bandits to square loss regression under realizability assumptions. One open question that comes up after this work is whether their approach can be generalized to action spaces with many (or inﬁnite) actions in d-dimensions. Another open question is whether one can seamlessly shift from realizability to misspeciﬁed models without requiring prior knowledge of the amount of misspeciﬁcation. This is precisely the setup we study here, where the action set is large or inﬁnite, but where the learner has a ‘good’ feature representation available up to some unknown amount of misspeciﬁcation.
Adequately handling misspeciﬁcation has been a subject of intense recent interest even for the simple special case of linear contextual bandits. Du et al. [18] questioned whether “good” is indeed enough, (cid:63)Massachusetts Institute of Technology.
†Google Research.
‡Courant Institute of Mathematical Sciences. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
that is, whether we can learn efﬁciently even without realizability. Lattimore et al. [33] gave a positive answer to that question, provided the misspeciﬁcation level ε is known in advance, and showed that the price of misspeciﬁcation (for regret) is roughly ε dT , where d is the dimension and T is the time horizon. However, they left the adapting to unknown ε as an open question.
√
Our results. We provide an afﬁrmative answer to all of these questions. We generalize SquareCB to inﬁnite action sets, and use this strategy to adapt to unknown misspeciﬁcation ε by combining it with a bandit model selection procedure akin to the one proposed by Agarwal et al. [9]. Our algorithm is oracle-efﬁcient, and adapts to misspeciﬁcation efﬁciently and optimally whenever it has access to an online oracle for square loss regression. When specialized to linear contextual bandits, it answers the question of Lattimore et al. [33].
An important conceptual contribution of our work is to show that one can view the action selection scheme used by SquareCB as an approximation to a log-barrier regularized optimization problem, which paves the way for a generalization to inﬁnite action spaces. Another by-product of our results is a generalization of the original CORRAL algorithm [9] for combining bandit algorithms, which is simpler, ﬂexible, and enjoys improved logarithmic factors. 1.1