Abstract
Gradient clipping is commonly used in training deep neural networks partly due to its practicability in relieving the exploding gradient problem. Recently, Zhang et al. [2020a] show that clipped (stochastic) Gradient Descent (GD) converges faster than vanilla GD/SGD via introducing a new assumption called (L0, L1)-smoothness, which characterizes the violent ﬂuctuation of gradients typically en-countered in deep neural networks. However, their iteration complexities on the problem-dependent parameters are rather pessimistic, and theoretical justiﬁcation of clipping combined with other crucial techniques, e.g. momentum acceleration, are still lacking. In this paper, we bridge the gap by presenting a general frame-work to study the clipping algorithms, which also takes momentum methods into consideration. We provide convergence analysis of the framework in both deter-ministic and stochastic setting, and demonstrate the tightness of our results by comparing them with existing lower bounds. Our results imply that the efﬁciency of clipping methods will not degenerate even in highly non-smooth regions of the landscape. Experiments conﬁrm the superiority of clipping-based methods in deep learning tasks. 1

Introduction
The problem of the central interest in this paper is to minimize a general non-convex function pre-sented below: min x∈Rd
F (x), (1) where F (x) can be potentially stochastic, i.e.
F (x) = Eξ∼D [f (x, ξ)] .
For non-convex optimization problems in form of (1), since obtaining the global minimum is
NP-hard in general, this paper takes the concern on a reasonable relaxed criteria: ﬁnding an ε-approximate ﬁrst-order stationary point such that ∥∇F (x)∥ ≤ ε.
∗Equal Contributions.
†Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b) (c)
Figure 1: (a) Some simple examples of (L0, L1)-smooth functions that are not L-smooth. (b) The magnitude of gradient norm ∥∇F (x)∥ w.r.t the local smoothness ∥∇2F (x)∥ on some sample points for a polynomial F (x, y) = x2 + (y − 3x + 2)4. We use log-scale axis. The local smoothness strongly correlates to the gradient. (c) Gradient and smoothness in the process of LSTM training, taken from Zhang et al. [2020a].
We consider gradient-based algorithms to solve (1) and separately study two cases: i) the gradient of
F given a point x is accessible; ii) only a stochastic estimator is accessible. We shall refer the former as the deterministic setting and the latter as the stochastic setting, and we analyze the (stochastic) gradient complexities to search an approximate ﬁrst-order stationary point for Problem (1).
Gradient clipping [Pascanu et al., 2012] is a simple and commonly used trick in algorithms that adap-tively choose step sizes to make optimization stable. For the task of training deep neural networks (especially for language processing tasks), it is often a standard practice and is believed to be efﬁcient in relieving the exploding gradient problem from empirical studies [Pascanu et al., 2013]. More re-cently, Zhang et al. [2020a] proposed an inspiring theoretical justiﬁcation on the clipping technique via introducing the (L0, L1)-smoothness assumption. The concept of (L0, L1)-smoothness is de-ﬁned as follows.
Deﬁnition 1.1 We say that a twice differentiable function F (x) is (L0, L1)-smooth, if for all x ∈ Rd we have ∥∇2F (x)∥ ≤ L0 + L1∥∇F (x)∥.
This assumption can be further relaxed such that twice differentiability is not required (see Remark 2.3). Therefore the standard L-smoothness assumption (i.e. the gradient of f is L-Lipschitz contin-uous) is stronger than the (L0, L1)-smoothness one in the sense that the latter allows ∥∇2F (x)∥ to have a linear growth with respect to ∥∇F (x)∥. (L0, L1)-smoothness is more realistic than L-smoothness. Firstly, it includes a variety of simple and important functions which, unfortunately, do not satisfy L-smoothness. For example, all univariate polynomials (which can possibly be non-convex) are (L0, L1)-smooth for L1 = 1, while a simple function x4 is not globally L-smooth for any L. Moreover, (L0, L1)-smoothness also encompasses all functions that belongs to the so-called exponential family. Figure 1(a) presents some simple examples and Figure 1(b) shows that the local smoothness of (L0, L1)-smooth functions strongly correlates to the gradient norm.
Secondly, Zhang et al. [2020a] performed experiments to show that (L0, L1)-smoothness is a pre-ciser characterization of the landscapes for objective functions in many real-world tasks, especially for training a deep neural network model. It was observed that the local Lipschitz constant L0 near the stationary point is thousands of times smaller than the global one L in the LSTM training (see
Figure 1(c) taken from Zhang et al. [2020a]).
Seeing this, it is desirable to give a comprehensive and deep analysis on iteration complexities for (L0, L1)-smooth objectives. How fast can we achieve to ﬁnd a ﬁrst-order stationary point for (L0, L1)-smooth functions? What are simple algorithms that provably achieve such a convergence rate? In this paper, we give afﬁrmative answers to the above questions. In fact, due to the violent
ﬂuctuation of gradients, the efﬁciency of (stochastic) Gradient Descent with a constant step size degenerates, whereas we will show in this paper that by simply combining the clipping technique, a wide range of algorithms can achieve much better convergence rate for (L0, L1)-smooth func-tions. In fact, when ε is small, the complexities (i.e. the number of gradient queries required) are 2
(
) (
)
∆L0ε−2 for the deterministic setting and O
O for the stochastic setting (see Section 3 for details), which are both independent of L1. Compared with Zhang et al. [2020a] who only stud-ied clipped (stochastic) gradient descent, we consider proposing a uniﬁed framework which contains a variety of clipping-based algorithms and achieve much sharper complexities. The main technique for our proof is by introducing a novel Lyapunov function which does not appear in existing studies.
We believe that our work provides better understandings for the clipping technique in training deep neural networks. We summarize the contributions of the paper in the following.
∆L0σ2ε−4
• We provide a general framework to analyze the clipping technique for optimizing (L0, L1)-smooth functions. It contains a variety of clipping algorithms, including gradient clipping and momentum clipping as special cases.
• We provide convergence analysis for the general framework we propose. We show that our bounds are tight by comparing with existing lower bounds. For gradient clipping, a special case in our framework, our result is much sharper than that proposed by Zhang et al. [2020a].
• We conduct experiments on a variety of different tasks, and observe that the clipping algo-rithms consistently perform better than vanilla ones.
Notations. For a vector x ∈ Rd, we denote ∥x∥ as the l2-norm of x. For a matrix A ∈ Rm×n, let
∥A∥ be the spectral norm of A. Given functions f, g : X → [0, ∞) where X is any set, we say f = O(g) if there exists a constant c > 0 such that f (x) ≤ cg(x) for all x ∈ X , and f = Ω(g) if there exists a constant c > 0 such that f (x) ≥ cg(x) for all x ∈ X . We say f = Θ(g) if f = O(g) and f = Ω(g). 1.1