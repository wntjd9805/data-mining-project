Abstract
Over the last decade, a single algorithm has changed many facets of our lives -Stochastic Gradient Descent (SGD). In the era of ever decreasing loss functions,
SGD and its various offspring have become the go-to optimization tool in machine learning and are a key component of the success of deep neural networks (DNNs).
While SGD is guaranteed to converge to a local optimum (under loose assumptions), in some cases it may matter which local optimum is found, and this is often context-dependent. Examples frequently arise in machine learning, from shape-versus-texture-features to ensemble methods and zero-shot coordination. In these settings, there are desired solutions which SGD on ‘standard’ loss functions will not ﬁnd, since it instead converges to the ‘easy’ solutions. In this paper, we present a different approach. Rather than following the gradient, which corresponds to a locally greedy direction, we instead follow the eigenvectors of the Hessian, which we call “ridges”. By iteratively following and branching amongst the ridges, we effectively span the loss surface to ﬁnd qualitatively different solutions. We show both theoretically and experimentally that our method, called Ridge Rider (RR), offers a promising direction for a variety of challenging problems. 1

Introduction
Deep Neural Networks (DNNs) are extremely popular in many applications of machine learning ranging from vision [23, 49] to reinforcement learning [47]. Optimizing them is a non-convex problem and so the use of gradient methods (e.g. stochastic gradient descent, SGD) leads to ﬁnding local minima. While recent evidence suggests [9] that in supervised problems these local minima obtain loss values close to the global minimum of the loss, there are a number of problem settings where optima with the same value can have very different properties. For example, in Reinforcement
Learning (RL), two very different policies might obtain the same reward on a given task, but one of them might be more robust to perturbations. Similarility, it is known that in supervised settings some minima generalize far better than others [28, 24]. Thus, being able to ﬁnd a speciﬁc type or class of minimum is an important problem.
At this point it is natural to ask what the beneﬁt of ﬁnding diverse solutions is? Why not optimize the property we care about directly? The answer is Goodhart’s law: “When a measure becomes a target, it ceases to be a good measure.” [48]. Generalization and zero-shot coordination are two examples of these type of objectives, whose very deﬁnition prohibits direct optimization.
⇤Equal contribution. Correspondence to jackph@robots.ox.ac.uk , jnf@fb.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To provide a speciﬁc example, in computer vision it has been shown that solutions which use shape features are known to generalize much better than those relying on textures [18]. However, they are also more difﬁcult to ﬁnd [16]. In reinforcement learning (RL), recent work focuses on constructing agents that can coordinate with humans in the cooperative card game Hanabi [4]. Agents trained with self-play ﬁnd easy to learn, but highly arbitrary, strategies which are impossible to play with for a novel partner (including human). To avoid these undesirable minima previous methods need access to the symmetries of the problem to make them inaccessible during training. The resulting agents can then coordinate with novel partners, including humans [26]. Importantly, in both of these two cases, standard SGD-based methods do not ﬁnd these ‘good’ minima easily and problem-speciﬁc hand tuning is required by designers to prevent SGD from converging to ‘bad’ minima.
Our primary contribution is to take a step towards addressing such issues in a general way that is applicable across modalities. One might imagine a plausible approach to ﬁnding different minima of the loss landscape would be to initialize gradient descent near a saddle point of the loss in multiple replicates, and hope that it descends the loss surface in different directions of negative curvature.
Unfortunately, from any position near a saddle point, gradient descent will always curve towards the direction of most negative curvature (see Appendix D.1), and there may be many symmetric directions of high curvature.
Instead, we start at a saddle point and force different replicates to follow distinct, orthogonal directions of negative curvature by iteratively following each of the eigenvectors of the Hessian until we can no longer re-duce the loss, at which point we repeat the branching process. Repeating this process hopefully leads the replicates to minima corresponding to distinct convex subspaces of the parameter space, essentially convert-ing an optimization problem into search [30]. We re-fer to our procedure as Ridge Rider (RR). RR is less
‘greedy’ than standard SGD methods, with Empirical
Risk Minimization (ERM, [51]), and thus can be used in a variety of situations to ﬁnd diverse minima. This greediness in SGD stems from it following the path with highest expected local reduction in the loss. Conse-quently, some minima, which might actually represent the solutions we seek, are never found.
Figure 1: Comparison of gradient descent (GD, hollow circles) and RR (RR, solid circles) on a two-dimensional loss surface. GD starting near the origin only ﬁnds the two local minima whose basin have large gradient near the origin. RR starts at the maximum and explores along four paths based on the two eigenvectors of
. Two paths (blue and green) ﬁnd the local minima while the other two explore the lower-curvature ridge and ﬁnd global minima. Following the eigenvectors leads RR around a local minimum (brown), while causing it to halt at a local max-imum (orange) where either a second ride (dot-ted) or GD may ﬁnd a minimum.
In the next section we introduce notation and formalize the problem motivation. In Section 3 we introduce RR, both as an exact method, and as an approximate scalable algorithm. In both cases we underpin our approach with theoretical guarantees. Finally, we present extensions of RR which are able to solve a variety of challenging machine learning problems, such as ﬁnding diverse solutions reinforcement learning, learning successful strategies for zero-shot coordination and generalizing to out of distribution data in supervised learning. We test RR in each of these settings in Section 4.
Our results suggest a conceptual connection between these previously unrelated problems.
L 2