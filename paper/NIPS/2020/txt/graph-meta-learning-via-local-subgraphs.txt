Abstract
Prevailing methods for graphs require abundant label and edge information for learning. When data for a new task are scarce, meta learning can learn from prior experiences and form much-needed inductive biases for fast adaption to new tasks. Here, we introduce G-META, a novel meta-learning algorithm for graphs.
G-META uses local subgraphs to transfer subgraph-speciﬁc information and learn transferable knowledge faster via meta gradients. G-META learns how to quickly adapt to a new task using only a handful of nodes or edges in the new task and does so by learning from data points in other graphs or related, albeit disjoint, label sets.
G-META is theoretically justiﬁed as we show that the evidence for a prediction can be found in the local subgraph surrounding the target node or edge. Experiments on seven datasets and nine baseline methods show that G-META outperforms existing methods by up to 16.3%. Unlike previous methods, G-META successfully learns in challenging, few-shot learning settings that require generalization to completely new graphs and never-before-seen labels. Finally, G-META scales to large graphs, which we demonstrate on a new Tree-of-Life dataset comprising 1,840 graphs, a two-orders of magnitude increase in the number of graphs used in prior work. 1

Introduction
Graph Neural Networks (GNNs) have achieved remarkable results in domains such as recommender systems [56], molecular biology [65, 19], and knowledge graphs [49, 18]. Performance is typically evaluated after extensive training on datasets where majority of labels are available [52, 54]. In contrast, many problems require rapid learning from only a few labeled nodes or edges in the graph. Such ﬂexible adaptation, known as meta learning, has been extensively studied for images and language, e.g., [39, 51, 27]. However, meta learning on graphs has received considerably less research attention and has remained a problem beyond the reach of prevailing GNN models.
Meta learning on graphs generally refers to a scenario in which a model learns at two levels. In the
ﬁrst level, rapid learning occurs within a task. For example, when a GNN learns to classify nodes in a particular graph accurately. In the second level, this learning is guided by knowledge accumulated gradually across tasks to capture how the task structure changes across target domains [35, 4, 34]. A powerful GNN trained to meta-learn can quickly learn never-before-seen labels and relations using only a handful of labeled data points. As an example, a key problem in biology is to translate insights from non-human organisms (such as yeast, zebraﬁsh, and mouse) to humans [66]. How to train a
GNN to effectively meta-learn on a large number of incomplete and scarcely labeled protein-protein interaction (PPI) graphs from various organisms, transfer the accrued knowledge to humans, and use it to predict the roles of protein nodes in the human PPI graph? While this is a hard task, it is only an instance of a particular graph meta-learning problem (Figure 1C). In this problem, the GNN needs to learn on a large number of graphs, each scarcely labeled with a unique label set. Then, it needs to quickly adapt to a never-before-seen graph (e.g., a PPI graph from a new organism) and never-before-seen labels (e.g., newly discovered roles of proteins). Current methods are specialized techniques speciﬁcally designed for a particular problem and a particular task [63, 3, 24, 7, 53]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
A
Single graph & disjoint labels
Meta-Training
G<latexit sha1_base64="7MkpROuO7clFAb9gxo7FQwZ8p0g=">AAAB8nicbVBNTwIxEJ3FL8Qv1KOXRjDxRHbxoEeiBz1iIoKBDemWLjS03bXtmpANf8KLB70Yr/4ab/4by7IHBV/S5uW9mczMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcP7nWUKEJbJOKR6gRYU84kbRlmOO3EimIRcNoOxlczv/1ElWaRvDOTmPoCDyULGcHGSp1qTzOBrqv9csWtuRnQMvFyUoEczX75qzeISCKoNIRjrbueGxs/xcowwum01Es0jTEZ4yHtWiqxoNpPs32n6MQqAxRGyj5pUKb+7kix0HoiAlspsBnpRW8m/ud1ExNe+CmTcWKoJPNBYcKRidDseDRgihLDJ5ZgopjdFZERVpgYG1HJhuAtnrxMHuo176xmv/ptvdK4zBMpwhEcwyl4cA4NuIEmtIAAh2d4hTfn0Xlx3p2PeWnByXsO4Q+czx+Hx4+9</latexit> a
⇠ a b
?
Meta-Testing
G<latexit sha1_base64="7MkpROuO7clFAb9gxo7FQwZ8p0g=">AAAB8nicbVBNTwIxEJ3FL8Qv1KOXRjDxRHbxoEeiBz1iIoKBDemWLjS03bXtmpANf8KLB70Yr/4ab/4by7IHBV/S5uW9mczMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcP7nWUKEJbJOKR6gRYU84kbRlmOO3EimIRcNoOxlczv/1ElWaRvDOTmPoCDyULGcHGSp1qTzOBrqv9csWtuRnQMvFyUoEczX75qzeISCKoNIRjrbueGxs/xcowwum01Es0jTEZ4yHtWiqxoNpPs32n6MQqAxRGyj5pUKb+7kix0HoiAlspsBnpRW8m/ud1ExNe+CmTcWKoJPNBYcKRidDseDRgihLDJ5ZgopjdFZERVpgYG1HJhuAtnrxMHuo176xmv/ptvdK4zBMpwhEcwyl4cA4NuIEmtIAAh2d4hTfn0Xlx3p2PeWnByXsO4Q+czx+Hx4+9</latexit> b
⇠
Meta
Learner
?
Label set      :
Y
⇤<latexit sha1_base64="IdLaBQDFTRC0qf7ta1Txa1LNWpU=">AAAB+HicbVA9TwJBEJ3zE/ELtbS5CCbGgtxhoSXRxhITEQicZG/Zgw17e5fdOZVc+B82FtoYW3+Knf/GBa5Q8CUzeXlvJjv7/FhwjY7zbS0tr6yurec28ptb2zu7hb39Ox0lirI6jUSkmj7RTHDJ6shRsGasGAl9wRr+8GriNx6Y0jyStziKmReSvuQBpwSNdF/qIHtCTdPWuHta6haKTtmZwl4kbkaKkKHWLXx1ehFNQiaRCqJ123Vi9FKikFPBxvlOollM6JD0WdtQSUKmvXR69dg+NkrPDiJlSqI9VX9vpCTUehT6ZjIkONDz3kT8z2snGFx4KZdxgkzS2UNBImyM7EkEdo8rRlGMDCFUcXOrTQdEEYomqLwJwZ3/8iJpVcruWdm0yk2lWL3MEsnBIRzBCbhwDlW4hhrUgYKCZ3iFN+vRerHerY/Z6JKV7RzAH1ifP7qHkr4=</latexit>
Label set     :
Y<latexit sha1_base64="mOIZZxfzXWp27xOXpXsF+NTPxz0=">AAAB9nicbVBNT8JAEN3iF+IX6tFLI5h4Ii0e9Ej04hETEQhtyHaZwobtttmdGknD3/DiQS/Gq7/Fm//GBXpQ8CUzeXlvJjv7gkRwjY7zbRXW1jc2t4rbpZ3dvf2D8uHRg45TxaDFYhGrTkA1CC6hhRwFdBIFNAoEtIPxzcxvP4LSPJb3OEnAj+hQ8pAzikbyqh7CE2qWdafVfrni1Jw57FXi5qRCcjT75S9vELM0AolMUK17rpOgn1GFnAmYlrxUQ0LZmA6hZ6ikEWg/m988tc+MMrDDWJmSaM/V3xsZjbSeRIGZjCiO9LI3E//zeimGV37GZZIiSLZ4KEyFjbE9C8AecAUMxcQQyhQ3t9psRBVlaGIqmRDc5S+vkm695l7UTKvf1SuN6zyRIjkhp+ScuOSSNMgtaZIWYSQhz+SVvFmp9WK9Wx+L0YKV7xyTP7A+fwCVfZIh</latexit>
B
Multiple graphs & shared labels
C
Multiple graphs & disjoint labels
…  a b
Meta-Training
Gi a
<latexit sha1_base64="WDJSagmDk0AN3s0dnbnErgZBcBw=">AAAB9HicbVBNTwIxEJ3FL8Qv1KOXRjDxRHbxoEeiBz1iIoLChnRLFxra7qbtmpAN/8KLB70Yr/4Yb/4by7IHBV/S5uW9mczMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcP7nWUKEJbJOKR6gRYU84kbRlmOO3EimIRcNoOxlczv/1ElWaRvDOTmPoCDyULGcHGSo/VnmYCXfdZtV+uuDU3A1omXk4qkKPZL3/1BhFJBJWGcKx113Nj46dYGUY4nZZ6iaYxJmM8pF1LJRZU+2m28RSdWGWAwkjZJw3K1N8dKRZaT0RgKwU2I73ozcT/vG5iwgs/ZTJODJVkPihMODIRmp2PBkxRYvjEEkwUs7siMsIKE2NDKtkQvMWTl8lDvead1exXv61XGpd5IkU4gmM4BQ/OoQE30IQWEJDwDK/w5mjnxXl3PualBSfvOYQ/cD5/AAjQkJk=</latexit>
⇠
Meta-Training
Gi a
<latexit sha1_base64="WDJSagmDk0AN3s0dnbnErgZBcBw=">AAAB9HicbVBNTwIxEJ3FL8Qv1KOXRjDxRHbxoEeiBz1iIoLChnRLFxra7qbtmpAN/8KLB70Yr/4Yb/4by7IHBV/S5uW9mczMC2LOtHHdb6ewsrq2vlHcLG1t7+zulfcP7nWUKEJbJOKR6gRYU84kbRlmOO3EimIRcNoOxlczv/1ElWaRvDOTmPoCDyULGcHGSo/VnmYCXfdZtV+uuDU3A1omXk4qkKPZL3/1BhFJBJWGcKx113Nj46dYGUY4nZZ6iaYxJmM8pF1LJRZU+2m28RSdWGWAwkjZJw3K1N8dKRZaT0RgKwU2I73ozcT/vG5iwgs/ZTJODJVkPihMODIRmp2PBkxRYvjEEkwUs7siMsIKE2NDKtkQvMWTl8lDvead1exXv61XGpd5IkU4gmM4BQ/OoQE30IQWEJDwDK/w5mjnxXl3PualBSfvOYQ/cD5/AAjQkJk=</latexit>
⇠
?
Meta-Testing
G b
<latexit sha1_base64="jidHU9LwmshblPN5+LNrrPb6BdU=">AAAB9HicbVBNTwIxEJ3FL8Qv1KOXRjAxHsguHPRI9KBHTERQ2JBu6UJD2920XROy4V948aAX49Uf481/Y4E9KPiSNi/vzWRmXhBzpo3rfju5ldW19Y38ZmFre2d3r7h/cK+jRBHaJBGPVDvAmnImadMww2k7VhSLgNNWMLqa+q0nqjSL5J0Zx9QXeCBZyAg2VnosdzUT6Lp3Vu4VS27FnQEtEy8jJcjQ6BW/uv2IJIJKQzjWuuO5sfFTrAwjnE4K3UTTGJMRHtCOpRILqv10tvEEnVilj8JI2ScNmqm/O1IstB6LwFYKbIZ60ZuK/3mdxIQXfspknBgqyXxQmHBkIjQ9H/WZosTwsSWYKGZ3RWSIFSbGhlSwIXiLJy+Th2rFq1XsV72tluqXWSJ5OIJjOAUPzqEON9CAJhCQ8Ayv8OZo58V5dz7mpTkn6zmEP3A+fwCoSZBa</latexit>
⇠
Meta
Learner
⇤ a b
?
Meta-Testing b
Meta
Learner
?
?
Y<latexit sha1_base64="mOIZZxfzXWp27xOXpXsF+NTPxz0=">AAAB9nicbVBNT8JAEN3iF+IX6tFLI5h4Ii0e9Ej04hETEQhtyHaZwobtttmdGknD3/DiQS/Gq7/Fm//GBXpQ8CUzeXlvJjv7gkRwjY7zbRXW1jc2t4rbpZ3dvf2D8uHRg45TxaDFYhGrTkA1CC6hhRwFdBIFNAoEtIPxzcxvP4LSPJb3OEnAj+hQ8pAzikbyqh7CE2qWdafVfrni1Jw57FXi5qRCcjT75S9vELM0AolMUK17rpOgn1GFnAmYlrxUQ0LZmA6hZ6ikEWg/m988tc+MMrDDWJmSaM/V3xsZjbSeRIGZjCiO9LI3E//zeimGV37GZZIiSLZ4KEyFjbE9C8AecAUMxcQQyhQ3t9psRBVlaGIqmRDc5S+vkm695l7UTKvf1SuN6zyRIjkhp+ScuOSSNMgtaZIWYSQhz+SVvFmp9WK9Wx+L0YKV7xyTP7A+fwCVfZIh</latexit>
Label set     :
Y<latexit sha1_base64="mOIZZxfzXWp27xOXpXsF+NTPxz0=">AAAB9nicbVBNT8JAEN3iF+IX6tFLI5h4Ii0e9Ej04hETEQhtyHaZwobtttmdGknD3/DiQS/Gq7/Fm//GBXpQ8CUzeXlvJjv7gkRwjY7zbRXW1jc2t4rbpZ3dvf2D8uHRg45TxaDFYhGrTkA1CC6hhRwFdBIFNAoEtIPxzcxvP4LSPJb3OEnAj+hQ8pAzikbyqh7CE2qWdafVfrni1Jw57FXi5qRCcjT75S9vELM0AolMUK17rpOgn1GFnAmYlrxUQ0LZmA6hZ6ikEWg/m988tc+MMrDDWJmSaM/V3xsZjbSeRIGZjCiO9LI3E//zeimGV37GZZIiSLZ4KEyFjbE9C8AecAUMxcQQyhQ3t9psRBVlaGIqmRDc5S+vkm695l7UTKvf1SuN6zyRIjkhp+ScuOSSNMgtaZIWYSQhz+SVvFmp9WK9Wx+L0YKV7xyTP7A+fwCVfZIh</latexit>
Label set     :
Y
Label set      :
⇤<latexit sha1_base64="IdLaBQDFTRC0qf7ta1Txa1LNWpU=">AAAB+HicbVA9TwJBEJ3zE/ELtbS5CCbGgtxhoSXRxhITEQicZG/Zgw17e5fdOZVc+B82FtoYW3+Knf/GBa5Q8CUzeXlvJjv7/FhwjY7zbS0tr6yurec28ptb2zu7hb39Ox0lirI6jUSkmj7RTHDJ6shRsGasGAl9wRr+8GriNx6Y0jyStziKmReSvuQBpwSNdF/qIHtCTdPWuHta6haKTtmZwl4kbkaKkKHWLXx1ehFNQiaRCqJ123Vi9FKikFPBxvlOollM6JD0WdtQSUKmvXR69dg+NkrPDiJlSqI9VX9vpCTUehT6ZjIkONDz3kT8z2snGFx4KZdxgkzS2UNBImyM7EkEdo8rRlGMDCFUcXOrTQdEEYomqLwJwZ3/8iJpVcruWdm0yk2lWL3MEsnBIRzBCbhwDlW4hhrUgYKCZ3iFN+vRerHerY/Z6JKV7RzAH1ifP7qHkr4=</latexit>
Figure 1: Graph meta-learning problems. A. Meta-learner classiﬁes unseen label set by observing other label sets in the same graph. B. Meta-learner learns unseen graph by learning from other graphs with the same label set. C. Meta-learner classiﬁes unseen label set by learning from other label sets across multiple graphs. Unlike existing methods, G-META solves all three problems and also works for link prediction (see Section 6.1).
While these methods provide a promising approach to meta learning in GNNs, their speciﬁc strategy does not scale well nor extend to other problems (Figure 1).
Present work. We introduce G-META,1 an approach for meta learning on graphs (Figure 1). The core principle of G-META is to represent every node with a local subgraph and use subgraphs to train
GNNs to meta-learn. Our theoretical analysis (Section 4) suggests that the evidence for a prediction can be found in the subgraph surrounding the target node or edge when using GNNs. In contrast to
G-META, earlier techniques are trained to meta-learn on entire graphs. As we show theoretically and empirically, such methods are unlikely to succeed in few-shot learning settings when the labels are scarce and scattered around multiple graphs. Furthermore, previous methods capture the overall graph structure but at the loss of ﬁner local structures. Besides, G-META’s construction of local subgraphs gives local structural representations that enable direct structural similarity comparison using GNNs based on its connection to Weisfeiler-Lehman test [54, 60]. Further, structural similarity enables
G-META to form the much-needed inductive bias via a metric-learning algorithm [37]. Moreover, local subgraphs also allow for effective feature propagation and label smoothing within a GNN. (1) G-META is a general approach for a variety of meta learning problems on graph-structured data.
While previous methods [63, 3] apply only to one graph meta-learning problem (Figure 1), G-META works for all of them (Appendix B). (2) G-META yields accurate predictors. We demonstrate G-META’s performance on seven datasets and compare it against nine baselines. G-META considerably outperforms baselines by up to 16.3%. (3) G-META is scalable. By operating on subgraphs, G-META needs to examine only small graph neighborhoods. We show how G-META scales to large graphs by applying it to our new Tree-of-Life dataset comprising 1,840 graphs, a two-orders of magnitude increase in the number of graphs used in prior work. 2