Abstract
We consider the framework of non-stationary stochastic optimization [Besbes et al., 2015] with squared error losses and noisy gradient feedback where the dynamic regret of an online learner against a time varying comparator sequence is studied. Motivated from the theory of non-parametric regression, we introduce a new variational constraint that enforces the comparator sequence to belong to a discrete kth order Total Variation ball of radius Cn. This variational constraint models comparators that have piecewise polynomial structure which has many relevant practical applications [Tibshirani, 2014]. By establishing connections to the theory of wavelet based non-parametric regression, we design a polynomial time algorithm that achieves the nearly optimal dynamic regret of ˜O(n
).
The proposed policy is adaptive to the unknown radius Cn. Further, we show that the same policy is minimax optimal for several other non-parametric families of interest. 1 2k+3 C 2 2k+3 n 1

Introduction
In time series analysis, estimating and removing the trend are often the ﬁrst steps taken to make the sequence “stationary”. The non-parametric assumption that the underlying trend is a piecewise polynomial or a spline [de Boor, 1978], is one of the most popular choices, especially when we do not know where the “change points” are and how many of them are appropriate. The higher order Total
Variation (see Assumption A3) of the trend can capture in some sense both the sparsity and intensity of changes in underlying dynamics. A non-parametric regression method that penalizes this quantity
— trend ﬁltering [Tibshirani, 2014] — enjoys a superior local adaptivity over traditional methods such as the Hodrick-Prescott Filter [Hodrick and Prescott, 1997]. However, Trend Filtering is an ofﬂine algorithm which limits its applicability for the inherently online time series forecasting problem. In this paper, we are interested in designing an online forecasting strategy that can essentially match the performance of the ofﬂine methods for trend estimation, hence allowing us to apply time series models forecasting on-the-ﬂy. In particular, our problem setup (see Figure 1) and algorithm are applicable to all online variants of trend ﬁltering problem such as predicting stock prices, server payloads, sales etc.
Let’s describe the notations that will be used throughout the paper. All vectors and matrices will be written in bold face letters. For a vector x ∈ Rm, x[i] or xi denotes its value at the ith coordinate. x[a : b] or xa:b is the vector [x[a], . . . , x[b]]. (cid:107)·(cid:107)p denotes ﬁnite dimensional Lp norms. (cid:107)x(cid:107)0 is the number of non-zero coordinates of a vector x. [n] represents the set {1, . . . , n}. Di ∈ R(n−i)×n denotes the discrete difference operator of order i deﬁned as in [Tibshirani, 2014] and reproduced 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
below.
D1 =




−1 1 0 −1
... 0 0 0 . . .
. . . 1 0 0 0 . . . −1
 0 0


 1
∈ R(n−1)×n, and Di = ˜D 1
· Di−1 ∀i ≥ 2 where ˜D 1 is the (n − i) × (n − i + 1) truncation of D1.
The theme of this paper builds on the non-parametric online forecasting model developed in [Baby and Wang, 2019]. We consider a sequential n step interaction process between an agent and an adversary as shown in Figure 1. 1. Fix a time horizon n. 2. Agent declares a forecasting strategy S 3. Adversary chooses a sequence θ1:n 4. For t = 1, . . . , n: (a) Agent outputs a prediction S(t). (b) Adversary reveals yt = θ1:n[t] + (cid:15)t 5. After n steps, agent suffers a cumulative loss (cid:80)n i=1 (S(i) − θ1:n[i])2.
Figure 1: Interaction protocol
A forecasting strategy S is deﬁned as an algorithm that outputs a prediction S(t) at time t only based on the information available after the completion of time t − 1. Random variables (cid:15)t for t ∈ [n] are independent and subgaussian with parameter σ2. This sequential game can be regarded as an online version of the non-parametric regression setup well studied in statistics community.
In this paper, we consider the problem of forecasting sequences that obey nk(cid:107)Dk+1θ1:n(cid:107)1≤ Cn, k ≥ 0 and (cid:107)θ1:n(cid:107)∞≤ B. The constraint nk(cid:107)Dk+1θ1:n(cid:107)1≤ Cn has been widely used in the rich literature of non-parametric regression. For example, the ofﬂine problem of estimating sequences obeying such higher order difference constraint from noisy labels under squared error loss is studied in [Mammen and van de Geer, 1997, Donoho et al., 1998, Tibshirani, 2014, Wang et al., 2016,
Sadhanala et al., 2016, Guntuboyina et al., 2017] to cite a few. We aim to design forecasters whose predictions are only based on past history and still perform as good as a batch estimator that sees the entire observations ahead of time.
Scaling of nk. The family {θ1:n | nk(cid:107)Dk+1θ1:n(cid:107)1≤ Cn} may appear to be alarmingly restrictive for a constant Cn due to the scaling factor nk, but let us argue why this is actually a natural construct.
The continuous T V k distance of a function f : [0, 1] → R is deﬁned as (cid:82) 1 0 |f (k+1)(x)|dx, where f (k+1) is the (k + 1)th order (weak) derivative. A sequence can be obtained by sampling the function at xi = i/n, i ∈ [n]. Discretizing the integral yields the T V k distance of this sequence to be nk(cid:107)Dk+1θ1:n(cid:107)1. Thus, the nk(cid:107)Dk+1θ1:n(cid:107)1 term can be interpreted as the discrete approximation to continuous higher order TV distance of a function. See Figure 2 for an illustration for the case k = 1.
Non-stationary Stochastic Optimization. The setting above can also be viewed under the frame-work of non-stationary stochastic optimization as studied in [Besbes et al., 2015, Chen et al., 2018b] with squared error loss and noisy gradient feedback. At each time step, the adversary chooses a loss function ft(x) = (x − θt)2. Since ∇ft(x) = 2(x − θt), the feedback ˜∇ft(x) = 2(x − yt) constitutes an unbiased estimate of the gradient ∇ft(x). [Besbes et al., 2015, Chen et al., 2018b] quantiﬁes the performance of a forecasting strategy S in terms of dynamic regret as follows. (cid:35) (cid:35)
Rdynamic(S, θ1:n) := E ft (S(t))
− (S(t) − θ1:n[t])2
, (1) n (cid:88) ft(xt), = E inf xt (cid:34) n (cid:88) t=1 where the last equality follows from the fact that when ft(x) = (x−θ1:n[t])2, inf x(x−θ1:n[t])2 = 0.
The expectation above is taken over the randomness in the noisy gradient feedback and that of the agent’s forecasting strategy. It is impossible to achieve sublinear dynamic regret against arbitrary t=1 t=1 (cid:34) n (cid:88) 2
ground truth sequences. However if the sequence of minimizers of loss functions ft(x) = (x − θt)2 obey a path variational constraint, then we can parameterize the dynamic regret as a function of the path length, which could be sublinear when the path-length is sublinear. Typical variational constraints considered in the existing work includes (cid:80) p)1/q
[see Baby and Wang, 2019, for a review]. These are all useful in their respective contexts, but do not capture higher order smoothness. t|θt − θt−1|2, ((cid:80) t|θt − θt−1|, (cid:80) t(cid:107)ft − ft−1(cid:107)q
The purpose of this work is to connect ideas from batch non-parametric regression to the framework of online stochastic optimization and deﬁne a natural family of higher order variational functionals of the form (cid:107)Dk+1θ1:n(cid:107)1 to track a comparator sequence with piecewise polynomial structure. To the best of our knowledge such higher order path variationals for k ≥ 1 are vastly unexplored in the domain of non-stationary stochastic optimization. In this work, we take the ﬁrst steps in introducing such variational constraints to online non-stationary stochastic optimization and exploiting them to get sub-linear dynamic regret.
Figure 2: A T V 1 bounded comparator sequence θ1:n can be obtained by sampling the continuous piecewise linear function on the left at points i/n, i ∈ [n]. On the right, we plot the T V 1 distance (which is equal to n(cid:107)D2θ1:n(cid:107)1 by deﬁnition) of the generated sequence for various sequence lengths n. As n increases the discrete T V 1 distance converges to a constant value given by the continous
T V 1 distance of the function on left panel. 2 Summary of results
In this section, we summarize the assumptions and main results of the paper.
Assumptions. We start by listing the assumptions made and provide justiﬁcations for them. (A1) The time horizon is known to be n. (A2) The parameter σ2 of subgaussian noise in the observations is a known ﬁxed positive constant. (A3) The ground truth denoted by θ1:n has its kth order total variation bounded by some positive
Cn, i.e., we consider ground truth sequences that belongs to the class
TVk(Cn) := {θ1:n ∈ Rn : nk(cid:107)Dk+1θ1:n(cid:107)1≤ Cn}
We refer to nk(cid:107)Dk+1θ1:n(cid:107)1 as T V k distance of the sequence θ1:n. To avoid trivial cases, we assume Cn = Ω(1). (A4) The TV order k is a known ﬁxed positive constant. (A5) (cid:107)θ1:n(cid:107)∞≤ B for a known ﬁxed positive constant B.
Though we require the time horizon to be known in advance in assumption (A1), this can be easily lifted using standard doubling trick arguments. The knowledge of time horizon helps us to present the policy in a most transparent way. If standard deviation of sub-gaussian noise is unknown, contrary to assumption (A2), then it can be robustly estimated by a Median Absolute Deviation estimator using ﬁrst few observations, see for eg. Johnstone [2017]. This is indeed facilitated by the sparsity of wavelet coefﬁcients of T V k bounded sequences. Assumption (A3) characterizes the ground truth 3
sequences whose forecasting is the main theme of this paper. The TVk(Cn) class features a rich family of sequences that can potentially exhibit spatially non-homogeneous smoothness. For example it can capture sequences that are piecewise polynomials of degree at most k. This poses a challenge to design forecasters that are locally adaptive and can efﬁciently detect and make predictions under the presence of the non-homogeneous trends. Though knowledge of the TV order k is required in assumption (A4), most of the practical interest is often limited to the lower orders k = 0, 1, 2, 3, see for eg. [Kim et al., 2009, Tibshirani, 2014] and we present (in Appendix D) a meta-policy based on exponential weighted averages [Cesa-Bianchi and Lugosi, 2006] to adapt to these lower orders.
Finally assumption (A5) is standard in the online learning literature.
Our contributions. We summarize our main results below.
• When the revealed labels are noisy realizations of sequences that belong to T V k(Cn) we propose a polynomial time policy called Ada-VAW (Adaptive Vovk Azoury Warmuth forecaster) that achieves the nearly minimax optimal rate of ˜O 1 2k+3 C for Rdynamic 2 2k+3 n (cid:18) (cid:19) n with high probability. The proposed policy optimally adapts to the unknown radius Cn.
• We show that the proposed policy achieves optimal Rdynamic when revealed labels are noisy realizations of sequences residing in higher order discrete Holder and discrete Sobolev classes.
• When the revealed labels are noisy realizations of sequences that obey (cid:107)Dkθ1:n(cid:107)0≤
Jn, (cid:107)θ1:n(cid:107)∞≤ B, we show that the same policy achieves the minimax optimal ˜O(Jn) rate for for Rdynamic with high probability. The policy optimally adapts to unknown Jn.
Notes on key novelties. It is known that the VAW forecaster is an optimal algorithm for online polynomial regression with squared error losses [Cesa-Bianchi and Lugosi, 2006]. With the side information of change points where the underlying ground truth switches from one polynomial to another, we can run a VAW forecaster on each of the stable polynomial sections to control the cumulative squared error of the policy. We use the machinery of wavelets to mimic an oracle that can provide side information of the change points. For detecting change points, a restart rule is formulated by exploiting connections between wavelet coefﬁcients and locally adaptive regression splines. This is a more general strategy than that used in [Baby and Wang, 2019]. To the best of our knowledge, this is the ﬁrst time an interplay between VAW forecaster and theory of wavelets along with its adaptive minimaxity [Donoho et al., 1998] has been used in the literature.
Wavelet computations require the length of underlying data whose wavelet transform needs to be computed has to be a power of 2. In practice this is achieved by a padding strategy in cases where original data length is not a power of 2. We show that most commonly used padding strategies – eg. zero padding as in [Baby and Wang, 2019] – are not useful for the current problem and propose a novel packing strategy that alleviates the need to pad. This will be useful to many applications that use wavelets which can be well beyond the scope of the current paper.
Our proof techniques for bounding regret use properties of the CDJV wavelet construction [Cohen et al., 1993]. To the best of our knowledge, this is the ﬁrst time we witness the ideas from a general
CDJV construction scheme implying useful results in an online learning paradigm. Optimally controlling the bias of VAW demands to carefully bound the (cid:96)2 norm of coefﬁcients computed by polynomial regression. This is done by using ideas from number theory and symbolic determinant evaluation of polynomial matrices. This could be of independent interest in both ofﬂine and online polynomial regression. 3