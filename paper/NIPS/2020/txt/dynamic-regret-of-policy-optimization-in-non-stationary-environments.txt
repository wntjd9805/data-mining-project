Abstract
We consider reinforcement learning (RL) in episodic MDPs with adversarial full-information reward feedback and unknown ﬁxed transition kernels. We propose two model-free policy optimization algorithms, POWER and POWER++, and establish guarantees for their dynamic regret. Compared with the classical notion of static regret, dynamic regret is a stronger notion as it explicitly accounts for the non-stationarity of environments. The dynamic regret attained by the proposed algorithms interpolates between different regimes of non-stationarity, and moreover satisﬁes a notion of adaptive (near-)optimality, in the sense that it matches the (near-)optimal static regret under slow-changing environments. The dynamic regret bound features two components, one arising from exploration, which deals with the uncertainty of transition kernels, and the other arising from adaptation, which deals with non-stationary environments. Speciﬁcally, we show that POWER++ improves over POWER on the second component of the dynamic regret by actively adapting to non-stationarity through prediction. To the best of our knowledge, our work is the ﬁrst dynamic regret analysis of model-free RL algorithms in non-stationary environments. 1

Introduction
Classical reinforcement learning (RL) literature often evaluates an algorithm by comparing its performance with that of the best ﬁxed (i.e., stationary) policy in hindsight, where the difference is commonly known as regret. Such evaluation metric implicitly assumes that the environment is static so that it is appropriate to compare an algorithm to a single best policy. However, as we advance towards modern and practical RL problems, we face challenges arising in dynamic and non-stationary environments for which comparing against a single policy is no longer sufﬁcient.
Two of the most prominent examples of RL for non-stationary environments are continual RL [30] and meta RL [16, 51] (and more broadly meta learning [20, 21]), which are central topics in the study of generalizability of RL algorithms. In these settings, an agent encounters a stream of tasks throughout time and aims to solve each task with knowledge accrued via solving previous tasks.
The tasks can be very different in nature from each other, with potentially increasing difﬁculties. In particular, the reward mechanism may vary across tasks, and therefore requires the agent to adapt to the change of tasks. Another example of RL under non-stationary environments is human-machine interaction [23, 41]. This line of research studies how humans and machines (or robots) should interact or collaborate to accomplish certain goals. In one scenario, a human teaches a robot to complete a task by assigning appropriate rewards to the robot but without intervening its dynamics. The rewards from the human can depend on the stage of the learning process and the rate of improvement in the robot’s behaviors. Therefore, the robot has to adjust its policy over time to maximize the rewards it receives. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In the above examples, it is uninformative to compare an algorithm with a ﬁxed stationary policy, which itself may not perform well given the rapidly changing nature of environments. It is also unclear whether existing algorithms, designed for static environments and evaluated by the standard notion of regret, are sufﬁcient for tackling non-stationary problems.
We aim to address these challenges in this paper. We consider the setting of episodic Markov decision processes (MDPs) with adversarial full-information reward feedback and unknown ﬁxed transition kernels. We are interested in the notion of dynamic regret, the performance difference between an algorithm and the set of policies optimal for individual episodes in hindsight. For non-stationary RL, dynamic regret is a signiﬁcantly stronger and more appropriate notion of performance measure than the standard (static) regret, but on the other hand more challenging for algorithm design and analysis.
We propose two efﬁcient, model-free policy optimization algorithms, POWER and POWER++. Under a mild regularity condition of MDPs, we provide dynamic regret analysis for both algorithms and we show that the regret bounds interpolate bewteen different regimes of non-stationarity. In particular, the bounds are of order ˜O(T 1/2) when the underlying model is nearly stationary, matching with existing near-optimal static regret bounds. In that sense, our algorithms are adaptively near-optimal in slow-varying environments. To the best of our knowledge, we provide the ﬁrst dynamic regret analysis for model-free RL algorithms under non-stationary environments.
Our dynamic regret bounds naturally decompose into two terms, one due to maintaining optimism and encouraging exploration in the face of uncertainty associated with the transition kernel, and the other due to the changing nature of reward functions. This decomposition highlights the two main components an RL algorithm needs in order to perform well in non-stationary environments: effective exploration under uncertainty and self-stabilization under drifting reward signals. Our second algorithm, POWER++, takes advantage of active prediction and improves over POWER in terms of the second term in the dynamic regret bounds.
Our contributions. The contributions of our work can be summarized as follows:
• We propose two model-free policy optimization algorithms, POWER and POWER++, for non-stationary RL with adversarial rewards;
• We provide dynamic regret analysis for both algorithms, and the regret bounds are applicable across all regimes of non-stationarity of the underlying model;
• When the environment is nearly stationary, our dynamic regret bounds are of order ˜O(T 1/2) and match the near-optimal static regret bounds, thereby demonstrating the adaptive near-optimality of our algorithms in slow-changing environments.