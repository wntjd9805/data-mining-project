Abstract
Training regimes based on Maximum Likelihood Estimation (MLE) suffer from known limitations, often leading to poorly generated text sequences. At the root of these limitations is the mismatch between training and inference, i.e. the so-called exposure bias, exacerbated by considering only the reference texts as correct, while in practice several alternative formulations could be as good. Generative Adversar-ial Networks (GANs) can mitigate those limitations but the discrete nature of text has hindered their application to language generation: the approaches proposed so far, based on Reinforcement Learning, have been shown to underperform MLE.
Departing from previous works, we analyze the exploration step in GANs applied to text generation, and show how classical sampling results in unstable training.
We propose to consider alternative exploration strategies in a GAN framework that we name ColdGAN s, where we force the sampling to be close to the distri-bution modes to get smoother learning dynamics. For the ﬁrst time, to the best of our knowledge, the proposed language GANs compare favorably to MLE, and obtain improvements over the state-of-the-art on three generative tasks, namely unconditional text generation, question generation, and abstractive summarization. 1

Introduction
Deep learning approaches have paved the way for signiﬁcant achievements in Natural Language
Generation (NLG). Under the most popular paradigm, sequence to sequence models [40] are trained with Maximum Likelihood Estimation (MLE) via Teacher Forcing [50]. Training neural networks under MLE does not succeed in modeling sequence probabilities [48], since, at inference, the model is conditioned on sequences that may have never been observed at training time. Indeed, generated texts using this approach are often degenerate [16], e.g. prone to repetition.
Nonetheless, these same architectures, when used as discriminators, are able to distinguish human from machine-generated text with a disconcerting efﬁciency: reported values are around 97% for long article generation [53] or abstractive summarization [37]. In the generative architectures, the encoder part can reach such performances, supporting the hypothesis that generation failures are mostly due to the decoding step: under MLE training regimes, the decoding suffers from exposure bias [33, 1] and lacks a sequence-level loss to optimize [26].
To mitigate MLE limitations, Reinforcement Learning (RL) has been applied to text generation tasks [33, 29], considering sequence level metrics such as BLEU or ROUGE as the reward. However, such metrics, based on n-grams similarity, are known to poorly correlate with human judgments [27], and do not preserve meaning [39]. Hence, when reinforced on them, models yield to poorer genera-34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
tions and higher degradation compared to their MLE counterparts [29]. To overcome these drawbacks, better rewards are thus necessary [29].
To this end, Ziegler et al. [60] proposed to directly reward systems using human judgment. Although this approach performs very well and approximates the best possible reward, it is obviously not a viable solution in practice. However, it attests that, with perfect rewards, one can achieve excellent levels of performance. A natural alternative, not requiring human judgments, is to frame the problem under the Generative Adversarial Network (GAN) paradigm [13], which has been used successfully for image generation [2]. For text, modeled as a sequence of discrete symbols, a naive computation of the gradients is however intractable. Hence, Language GANs are based on gradient estimation via
RL-based techniques [52].
However, the reward in this case can be extremely sparse (as discussed in Section 3.2), yielding to high-variance gradient estimation, which is known to be challenging for optimization [56]. Most previous works have focused on this aspect, and proposed denser rewards [19, 22]. Unfortunately, these attempts to apply GANs to text generation obtained limited success [4] and have been found to underperform MLE [38, 42, 22].
Although known to be crucial [41], exploration is surprisingly understudied when RL is applied to text generation. In this work, we propose a new exploration method that aims at sampling more structured rewards and that better suits the GANs’ training dynamics, allowing for the ﬁrst time to successfully train Language GANs. Our main contributions can be summarized as: 1. We study the discriminators’ behavior and show that their degree of specialization has important implications on the exploration to stabilize the training process. In particular, we ﬁnd that reducing the exploration space is essential to successfully train discrete GANs. 2. Based on these observations, we propose ColdGAN s, a GAN architecture using alternative sampling strategies that force the sampling to remain closer to the distribution modes. 3. Finally, we apply our proposed methods on three tasks. We report positive results compared to previous works, including GANs and MLE-based models. 2