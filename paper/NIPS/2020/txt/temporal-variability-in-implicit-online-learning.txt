Abstract
In the setting of online learning, Implicit algorithms turn out to be highly suc-cessful from a practical standpoint. However, the tightest regret analyses only show marginal improvements over Online Mirror Descent. In this work, we shed light on this behavior carrying out a careful regret analysis. We prove a novel static regret bound that depends on the temporal variability of the sequence of loss functions, a quantity which is often encountered when considering dynamic competitors. We show, for example, that the regret can be constant if the tempo-ral variability is constant and the learning rate is tuned appropriately, without the need of smooth losses. Moreover, we present an adaptive algorithm that achieves this regret bound without prior knowledge of the temporal variability and prove a matching lower bound. Finally, we validate our theoretical ﬁndings on classiﬁca-tion and regression datasets. 1

Introduction
The online learning paradigm is a powerful tool to model common scenarios in the real world when the data comes in a streaming fashion, for example in the case of time series. In the last two decades there has been a tremendous amount of progress in this ﬁeld (see, e.g., [30, 13, 24], for an introduction), which also led to advances in seemingly unrelated areas of machine learning and computer science.
In this setting, a learning agent faces the environment in a game played sequentially. The protocol is the following: given a time horizon T , in every round t = 1, . . . , T the agent chooses a model xt from a convex set V . Then, a convex loss function (cid:96)t is revealed by the environment and the agent pays a loss (cid:96)t(xt). As usual in this setting, we do not make assumptions about the environment, but allow it to be adversarial. The agent’s goal is to minimize her regret against any decision maker, i.e., the cumulative sum of her losses compared to the losses of an agent which always commits to the same choice u. So, formally the regret against any u ∈ V is deﬁned as
T (cid:88)
T (cid:88)
RT (u) (cid:44) (cid:96)t(xt) − (cid:96)t(u) . t=1 t=1
Much of the progress in this ﬁeld is driven by the strictly related model of Online Linear Opti-mization (OLO): exploiting the assumption that the loss functions are convex, we can linearize them using a ﬁrst-order approximation through its (sub)gradient and subsequently minimize the linearized regret. For example, the well-known Online Gradient Descent (OGD) [38] simply uses the direction of the negative (sub)gradient of the loss function to update its model, multiplied by a given learning rate. Usually, a properly tuned learning rate gives a regret bound of O(
T ), which is also optimal.
On the other hand, we can choose to not use any approximation to the loss function and instead up-date our model using directly the loss function rather than its subgradient [17]. This type of update is known as Implicit and algorithms designed in this way are known to have practical advantages [18].
Unfortunately, their theoretical understanding is still limited at this point.
√
∗Work done while visiting the OPTIMAL Lab at Boston University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our ﬁrst contribution (Section 5) in this paper is a reﬁned analysis of Implicit algorithms in the framework of Online Mirror Descent (OMD). Doing this allows us to understand why Implicit algo-rithms might practically work better compared to algorithms which use (sub)gradients in the update.
In particular, we describe how these algorithms can potentially incur only a constant regret if the sequence of loss functions does not vary with time. In particular, we measure the hardness of the sequence of loss functions with its temporal variability, which is deﬁned as
VT (cid:44)
T (cid:88) t=2 max x∈V (cid:96)t(x) − (cid:96)t−1(x) . (1)
√
Our second contribution (Section 6) is a new adaptive Implicit algorithm, AdaImplicit, which retains the worst-case O(
T ) regret bound but takes advantage of a slow varying sequence of loss functions and achieve a regret of O(VT + 1). Also, we prove a lower bound which shows that our algorithm is optimal. Finally, in order to show the beneﬁts of using Implicit algorithms in practice, in Section 7 we conduct an empirical analysis on real-world datasets in both classiﬁcation and regression tasks. 2