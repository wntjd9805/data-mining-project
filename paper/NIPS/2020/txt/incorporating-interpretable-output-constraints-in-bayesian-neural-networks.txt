Abstract
Domains where supervised models are deployed often come with task-speciﬁc con-straints, such as prior expert knowledge on the ground-truth function, or desiderata like safety and fairness. We introduce a novel probabilistic framework for rea-soning with such constraints and formulate a prior that enables us to effectively incorporate them into Bayesian neural networks (BNNs), including a variant that can be amortized over tasks. The resulting Output-Constrained BNN (OC-BNN) is fully consistent with the Bayesian framework for uncertainty quantiﬁcation and is amenable to black-box inference. Unlike typical BNN inference in uninterpretable parameter space, OC-BNNs widen the range of functional knowledge that can be incorporated, especially for model users without expertise in machine learning. We demonstrate the efﬁcacy of OC-BNNs on real-world datasets, spanning multiple domains such as healthcare, criminal justice, and credit scoring. 1

Introduction
In domains where predictive errors are prohibitively costly, we desire models that can both capture predictive uncertainty (to inform downstream decision-making) as well as enforce prior human expertise or knowledge (to induce appropriate model biases). Performing Bayesian inference on deep neural networks, which are universal approximators [11] with substantial model capacity, results in
BNNs — models that combine high representation power with quantiﬁable uncertainty estimates
[21, 20] 1. The ability to encode informative functional beliefs in BNN priors can signiﬁcantly reduce the bias and uncertainty of the posterior predictive, especially in regions of input space sparsely covered by training data [27]. Unfortunately, the trade-off for their versatility is that BNN priors, deﬁned in high-dimensional parameter space, are uninterpretable. A general approach for incorporating functional knowledge (that human experts might possess) is therefore intractable.
Recent work has addressed the challenge of incorporating richer functional knowledge into BNNs, such as preventing miscalibrated model predictions out-of-distribution [9], enforcing smoothness constraints [2] or specifying priors induced by covariance structures in the dataset (cf. Gaussian processes) [25, 19]. In this paper 2, we take a different direction by tackling functional knowledge expressed as output constraints — the set of values y is constrained to hold for any given x. Unlike other types of functional beliefs, output constraints are intuitive, interpretable and easily speciﬁed,
∗Work done while at Harvard University. 1See Appendix A for a technical overview of BNN inference and acronyms used throughout this paper. 2Our code is publicly available at: https://github.com/dtak/ocbnn-public. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
even by domain experts without technical understanding of machine learning methods. Examples include ground-truth human expertise (e.g. known input-output relationship, expressed as scientiﬁc formulae or clinical rules) or critical desiderata that the model should enforce (e.g. output should be restricted to the permissible set of safe or fair actions for any given input scenario).
We propose a sampling-based prior that assigns probability mass to BNN parameters based on how well the BNN output obeys constraints on drawn samples. The resulting Output-Constrained BNN (OC-BNN) allows the user to specify any constraint directly in its functional form, and is amenable to all black-box BNN inference algorithms since the prior is ultimately evaluated in parameter space.
Our contributions are: (a) we present a formal framework that lays out what it means to learn from output constraints in the probabilistic setting that BNNs operate in, (b) we formulate a prior that enforces output constraint satisfaction on the resulting posterior predictive, including a variant that can be amortized across multiple tasks, (c) we demonstrate proof-of-concepts on toy simulations and apply OC-BNNs to three real-world, high-dimensional datasets: (i) enforcing physiologically feasible interventions on a clinical action prediction task, (ii) enforcing a racial fairness constraint on a recidivism prediction task where the training data is biased, and (iii) enforcing recourse on a credit scoring task where a subpopulation is poorly represented by data. 2