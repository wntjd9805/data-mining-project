Abstract
Randomly initialized neural networks are known to become harder to train with increasing depth, unless architectural enhancements like residual connections and batch normalization are used. We here investigate this phenomenon by revisiting the connection between random initialization in deep networks and spectral instabilities in products of random matrices. Given the rich literature on random matrices, it is not surprising to ﬁnd that the rank of the intermediate representations in unnormalized networks collapses quickly with depth. In this work we highlight the fact that batch normalization is an effective strategy to avoid rank collapse for both linear and ReLU networks. Leveraging tools from Markov chain theory, we derive a meaningful lower rank bound in deep linear networks. Empirically, we also demonstrate that this rank robustness generalizes to ReLU nets. Finally, we conduct an extensive set of experiments on real-world data sets, which conﬁrm that rank stability is indeed a crucial condition for training modern-day deep neural architectures. 1

Introduction and related work
Depth is known to play an important role in the expressive power of neural networks [28]. Yet, increased depth typically leads to a drastic slow down of learning with gradient-based methods, which is commonly attributed to unstable gradient norms in deep networks [15]. One key aspect of the training process concerns the way the layer weights are initialized. When training contemporary neural networks, both practitioners and theoreticians advocate the use of randomly initialized layer weights with i.i.d. entries from a zero mean (Gaussian or uniform) distribution. This initialization strategy is commonly scaled such that the variance of the layer activation stays constant across layers
[13, 14]. However, this approach can not avoid spectral instabilities as the depth of the network increases. For example, [26] observes that for linear neural networks, such initialization lets all but one singular values of the last layers activation collapse towards zero as the depth increases.
Nevertheless, recent advances in neural architectures have allowed the training of very deep neural networks with standard i.i.d. initialization schemes despite the above mentioned shortcomings.
∗Shared ﬁrst authorship 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Among these, both residual connections and normalization layers have proven particularly effective and are thus in widespread use (see [17, 24, 14] to name just a few). Our goal here is to bridge the explanatory gap between these two observations by studying the effect of architectural enhancements on the spectral properties of randomly initialized neural networks. We also provide evidence for a strong link of the latter with the performance of gradient-based optimization algorithms.
One particularly interesting architectural component of modern day neural networks is Batch Nor-malization (BN) [17]. This simple heuristics that normalizes the pre-activation of hidden units across a mini-batch, has proven tremendously effective when training deep neural networks with gradient-based methods. Yet, despite of its ubiquitous use and strong empirical beneﬁts, the research community has not yet reached a broad consensus, when it comes to a theoretical explanation for its practical success. Recently, several alternatives to the original “internal covariate shift” hypothesis
[17] have appeared in the literature: decoupling optimization of direction and length of the parame-ters [20], auto-tuning of the learning rate for stochastic gradient descent [3], widening the learning rate range [7], alleviating sharpness of the Fisher information matrix [18], and smoothing the opti-mization landscape [25]. Yet, most of these candidate justiﬁcations are still actively debated within the community. For example, [25] ﬁrst made a strong empirical case against the original internal covariate shift hypothesis. Secondly, they argued that batch normalization simpliﬁes optimization by smoothing the loss landscape. However, their analysis is on a per-layer basis and treats only the largest eigenvalue. Furthermore, even more recent empirical studies again dispute these ﬁndings, by observing the exact opposite behaviour of BN on a ResNet20 network [34]. 1.1 On random initialization and gradient based training
In light of the above discussion, we take a step back – namely to the beginning of training – to ﬁnd an interesting property that is provably present in batch normalized networks and can serve as a solid basis for a more complete theoretical understanding.
The difﬁculty of training randomly initialized, un-normalized deep networks with gradient methods is a long-known fact, that is commonly attributed to the so-called vanishing gradient effect, i.e., a decreasing gradient norm as the networks grow in depth (see, e.g., [27]). A more recent line of research tries to explain this effect by the condition number of the input-output Jacobian (see, e.g.,
[32, 33, 23, 7]). Here, we study the spectral properties of the above introduced initialization with a particular focus on the rank of the hidden layer activations over a batch of samples. The question at hand is whether or not the network preserves a diverse data representation which is necessary to disentangle the input in the ﬁnal classiﬁcation layer.
As a motivation, consider the results of Fig. 1, which plots accuracy and output rank when training batch-normalized and un-normalized neural networks of growing depth on the Fashion-MNIST dataset [31]. As can be seen, the rank in the last hidden layer of the vanilla networks collapses with depth and they are essentially unable to learn (in a limited number of epochs) as soon as the number of layers is above 10. The rank collapse indicates that the direction of the output vector has become independent of the actual input. In other words, the randomly initialized network no longer preserves information about the input. Batch-normalized networks, however, preserve a high rank across all network sizes and their training accuracy drops only very mildly as the networks reach depth 32.
The above example shows that both rank and optimization of even moderately-sized, unnormalized networks scale poorly with depth. Batch-normalization, however, stabilizes the rank in this setting and the obvious question is whether this effect is just a slow-down or even simply a numerical phenomenon, or whether it actually generalizes to networks of inﬁnite depth.
In this work we make a strong case for the latter option by showing a remarkable stationarity aspect of BN. Consider for example the case of passing N samples xi ∈ Rd arranged column-wise in an input matrix X ∈ Rd×N through a very deep network with fully-connected layers. Ideally, from an information propagation perspective, the network should be able to differentiate between individual samples, regardless of its depth [27]. However, as can be seen in Fig. 2, the hidden representation of X collapses to a rank one matrix in vanilla networks, thus mapping all xi to the same line in Rd.
Hence, the hidden layer activations and along with it the individual gradient directions become 3Computed using torch.matrix_rank(), which regards singular values below σmax × d × 10−7 as zero.
This is consistent with both Matlab and Numpy. 2
Figure 1: Effect of depth on rank and learning, on the Fashion-MNIST dataset with ReLU multilayer perceptrons (MLPs) of depth 1-32 and width 128 hidden units. Left: Rank3 after random initialization as in
PyTorch [22]. Right: Training accuracy after training 75 epochs with SGD, batch size 128 and grid-searched learning rate. Mean and 95% conﬁdence interval of 5 independent runs. independent from the input xi as depth goes to inﬁnity. We call this effect “directional” gradient vanishing (see Section 3 for a more thorough explanation).
Interestingly this effect does not happen in batch-normalized networks, which yield – as we shall prove in Theorem 2 – a stable rank for any depth, thereby preserving a disentangled representation of the input and hence allowing the training of very deep networks. These results substantiate earlier empirical observations made by [7] for random BN-nets, and also validates the claim that BN helps with deep information propagation [27].
Figure 2: Rank comparison of last hidden activation: Log(rank) of the last hidden layer’s activation over total number of layers (blue for BN- and orange for vanilla-networks) for Gaussian inputs.
Networks are MLPs of width d = 32. (Left) Linear activations, (Right) ReLU activations. Mean and 95% conﬁdence interval of 10 independent runs. While the rank quickly drops in depth for both networks, BN stabilizes the rank above
√ d. 1.2 Contributions
In summary, the work at hand makes the following two key contributions: (i) We theoretically prove that BN indeed avoids rank collapse for deep linear neural nets under standard initialization and for any depth. In particular, we show that BN can be seen as a computa-tionally cheap rank preservation operator, which may not yield hidden matrices with full rank but d), still preserves sufﬁcient modes of variation in the data to achieve a scaling of the rank with Ω( where d is the width of the network. Subsequently, we leverage existing results from random matrix theory [9] to complete the picture with a simple proof of the above observed rank collapse for linear vanilla networks, which interestingly holds regardless of the presence of residual connections (Lemma 3). Finally, we connect the rank to difﬁculties in gradient based training of deep nets by showing that rank collapse makes the directional component of the gradients independent of the input.
√ (ii) We empirically show that the rank is indeed a crucial quantity for gradient-based learning. In particular, we show that both the rank and the ﬁnal training accuracy quickly diminish in depth unless 3
BN layers are incorporated in both simple feed-forward and convolutional neural nets. To take this reasoning beyond mere correlations, we actively intervene with the rank of networks before training and show that (a) one can break the training stability of BN by initializing in a way that reduces its rank-preserving properties, and (b) a rank-increasing pre-training procedure for vanilla networks can recover their training ability even for large depth. Interestingly, our pre-training method allows vanilla
SGD to outperform BN on very deep MLPs. In all of our experiments, we ﬁnd that SGD updates preserve the order of the initial rank throughout optimization, which underscores the importance of the rank at initialization for the entire convergence behavior. 2