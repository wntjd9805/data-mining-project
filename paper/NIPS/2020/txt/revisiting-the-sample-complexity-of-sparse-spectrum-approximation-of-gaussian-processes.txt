Abstract
We introduce a new scalable approximation for Gaussian processes with provable guarantees which hold simultaneously over its entire parameter space. Our ap-proximation is obtained from an improved sample complexity analysis for sparse spectrum Gaussian processes (SSGPs). In particular, our analysis shows that under a certain data disentangling condition, an SSGP’s prediction and model evidence (for training) can well-approximate those of a full GP with low sample complexity.
We also develop a new auto-encoding algorithm that ﬁnds a latent space to dis-entangle latent input coordinates into well-separated clusters, which is amenable to our sample complexity analysis. We validate our proposed method on several benchmarks with promising results supporting our theoretical analysis. 1

Introduction
A Gaussian process (GP) [36] is a popular probabilistic kernel method for regression which has found applications across many scientiﬁc disciplines. Examples of such applications include meteorological forecasting, such as precipitation and sea-level pressure prediction [2]; sensing and monitoring of ocean and freshwater phenomena such as temperature and plankton bloom [7, 12]; trafﬁc ﬂow and mobility demand predictions over urban road networks [9, 10, 29]; ﬂight delay predictions [15, 19, 20]; and persistent robotics tasks such as localization and ﬁltering [43]. The broad applicability of GPs is in part due to their expressive Bayesian non-parametric nature which provides a closed-form prediction [36] in the form of a Gaussian distribution with formal measures of predictive uncertainty, such as entropy and mutual information criteria [27, 39, 44]. Such expressiveness makes GPs not only useful as predictive methods but also a go-to representation for active learning applications
[24, 23, 27, 44] or Bayesian optimization [38, 45, 22, 16] that need to optimize for information gain while collecting training data.
Unfortunately, the expressive power of a GP comes at a cost of poor scalability (i.e., cubic time [36]) in the size of the training data (see Section 2.1 below), hence limiting its use to small datasets. This prevents GPs from being applied more broadly to modern settings with increasingly growing volumes of data [15, 19, 20]. To sidestep this limitation, a prevalent research trend is to impose sparse structural assumptions [33, 34] on the GP’s kernel matrix to reduce its multiplication and inversion cost, which comprises the main bulk of the training and inference complexity. This results in a broad family of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
sparse Gaussian processes [15, 17, 19, 28, 37, 40] that are not only computationally efﬁcient but also amenable to various forms of parallelism [8, 29] and distributed computation [1, 13, 20, 21, 18], further increasing their efﬁciency.
Despite such advantages, the sparsiﬁcation components at the core of these methods are heuristically designed and do not come with provable guarantees that explicitly characterize the interplay between approximation quality and computational complexity. This motivates us to develop a more robust, theoretically-grounded approximation scheme for GPs that is both provable and amenable to the many fast computation schemes mentioned above. More speciﬁcally, our contributions include: 1. An analysis of a new approximation scheme that generates a sparse spectrum approximation of a GP with provable bounds on its sample complexity, which practically becomes signiﬁcantly small when the input data exhibits a certain clustering structure. Furthermore, the impact of the approximation on the resulting training and inference qualities is also formally analyzed (Section 3.1). 2. A data partitioning algorithm inspired from the above analysis, which learns a cluster embedding that reorients the input distribution while ensuring reconstructability of the original distribution (Section 3.3). We show that using sparse spectrum Gaussian processes (SSGP) [17, 28] on the embedded space requires fewer samples to achieve the same level of approximation quality. This also induces a linear feature map which enables efﬁcient training and inference of GPs. 3. An empirical study on benchmarks that demonstrates the efﬁciency of the proposed method over existing works in terms of its approximation quality versus computational efﬁciency (Section 4). 2