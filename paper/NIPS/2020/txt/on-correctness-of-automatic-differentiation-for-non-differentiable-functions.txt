Abstract
Differentiation lies at the core of many machine-learning algorithms, and is well-supported by popular autodiff systems, such as TensorFlow and PyTorch. Orig-inally, these systems have been developed to compute derivatives of differen-tiable functions, but in practice, they are commonly applied to functions with non-differentiabilities. For instance, neural networks using ReLU deﬁne non-differentiable functions in general, but the gradients of losses involving those functions are computed using autodiff systems in practice. This status quo raises a natural question: are autodiff systems correct in any formal sense when they are applied to such non-differentiable functions? In this paper, we provide a positive answer to this question. Using counterexamples, we ﬁrst point out ﬂaws in often-used informal arguments, such as: non-differentiabilities arising in deep learning do not cause any issues because they form a measure-zero set. We then investigate a class of functions, called PAP functions, that includes nearly all (possibly non-differentiable) functions in deep learning nowadays. For these PAP functions, we propose a new type of derivatives, called intensional derivatives, and prove that these derivatives always exist and coincide with standard derivatives for almost all inputs. We also show that these intensional derivatives are what most autodiff systems compute or try to compute essentially. In this way, we formally establish the correctness of autodiff systems applied to non-differentiable functions. 1

Introduction
Automatic differentiation or autodiff is one of the key technologies behind the dramatic progress of deep learning in recent years [4, 26, 38, 39]. It refers to the idea of developing and using a generic tool that can differentiate any function expressed as a program in a general-purpose programming language
[20, 34]. Effective autodiff systems have been developed for popular programming languages
[2, 3, 5, 9, 22, 28, 33, 35, 42, 44, 45]. They have enabled the development of sophisticated models and algorithms in machine learning that, in particular, involve deep neural networks [19].
This paper is concerned with one seeming contradiction of these autodiff systems: the systems have originally been developed to compute derivatives of differentiable functions, but in practice, they are commonly applied to functions with non-differentiabilities. For instance, neural networks using
ReLU deﬁne non-differentiable functions in general, but the derivatives of losses involving those functions are computed using autodiff systems in practice. This status quo raises a natural question: are autodiff systems correct in any formal sense when applied to such non-differentiable functions?
A common reaction to the question is: non-differentiabilities arising in deep learning (e.g., from
ReLU) do not cause any issues because they occur rarely (i.e., they form a Lebesgue-measure-zero set). In the paper, we ﬁrst show that this reaction needs to be carefully re-examined at least. Using counterexamples, we point out ﬂaws in three often-used arguments derived from this reaction. We 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
then present our answer. It is also positive, but based on a class of functions that satisfy a condition called piecewise analyticity under analytic partition (in short, PAP). These PAP functions include nearly all (possibly non-differentiable) functions in deep learning nowadays. For these PAP functions, we propose a new type of derivatives, called intensional derivatives, and prove that these derivatives always exist and coincide with standard derivatives for almost all inputs. These intensional derivatives behave almost as well as, and sometimes even better than, usual derivatives for differentiable functions.
For instance, they always satisfy a chain rule even if functions are non-differentiable. Using these properties of intensional derivatives, we show that the intensional derivatives are what most autodiff systems compute or try to compute essentially. In this way, we formally establish the correctness of autodiff systems that compute derivatives of non-differentiable functions.
We use (a, b), (a, b], [a, b), and [a, b] to denote intervals in R, and (cid:104)a1, . . . , an(cid:105) to denote tuples. For n ∈ (Z>0 ∪ {∞}), [n] means the set {1, 2, . . . , n}. We call Lebesgue measure simply by measure.
The detailed statements and missing proofs of our results can be found in the appendix. 2 Challenges
As mentioned in the introduction, practitioners frequently apply autodiff systems to functions with non-differentiabilities, and justify these out-of-scope use cases with plausible yet heuristic arguments.
In this section, we analyse these arguments. We go through three claims that are often used in the arguments implicitly, and show that although looking innocent at the outset, the claims have serious
ﬂaws; they are wrong, and we provide counterexamples.
Recall a notion of correctness for an autodiff system covering non-differentiable functions [6, 20, 24]:
Deﬁnition 1 (Correctness of Autodiff). We say that an autodiff system is correct if the following condition holds: for every measurable function f : X → Rm deﬁned on an open X ⊆ Rn and implemented by a program, if f is differentiable almost everywhere (i.e., the set of inputs making f non-differentiable is contained in a measure-zero set), then for almost every x ∈ X , the autodiff system applied to (the program of) f and the input x computes the derivative of f at x.
The deﬁnition permits a non-differentiable function as an input to an autodiff system, as long as its non-differentiability occurs rarely (i.e., at a measure-zero subset of the input domain). For such a function, it may be impossible to compute the correct derivative for all inputs, simply because the derivative does not exist for inputs where the function is non-differentiable. Thus, the deﬁnition just requires that the system should compute the correct derivative for most inputs instead (i.e., for a subset of the input domain whose complement inside the domain is contained in a measure-zero set).
Proving the correctness of an autodiff system is surprisingly difﬁcult. Nearly every autodiff system is based on a chain rule for computing the derivative of function composition, but when the component functions are non-differentiable, designing a correct version of the rule is challenging. To help the reader see this challenge, we analyse three plausible yet ﬂawed claims about the derivative of function composition, which are sometimes used implicitly in heuristic justiﬁcations of autodiff systems.
Let f : X → Y and g : Y → Rl be measurable functions deﬁned over open sets X ⊆ Rn and
Y ⊆ Rm. Here is the ﬁrst claim:
Claim 1 If f and g are differentiable almost everywhere and continuous, then g ◦ f should be differentiable almost everywhere.
A rationale for the claim goes as follows. In order for g ◦ f to be non-differentiable at x0, the function f has to be non-differentiable at x0, or it should map x0 to a non-differentiable input to g and be able to vary enough in a neighbourhood of x0. The claim says that such an x0 is rare (from the perspective of measure). Of course, the ﬁrst case that f is non-differentiable at x0 occurs rarely by assumption. The second case seems to happen rarely as well, because the non-differentiable inputs to g are rare and f is continuous: because of continuity, if f maps many x0’s (i.e., all the x0 in some non-measure-zero set) to those rare non-differentiable inputs of g, it should behave as a constant function in the neighbourhoods of most of those x0’s.
The rationale has a ﬂaw, and the claim is false. The inputs x0 falling into the second case are not necessarily rare. Although f is continuous, it is possible that f maps many x0’s to some of those rare non-differentiable inputs of g without acting as a constant function in a neighbourhood of each of those x0’s. The precise result is summarised in the following proposition: 2
Figure 1: The graphs of φ1, f , and g that are described in the proof of Proposition 1.
Proposition 1. There exist functions f : (0, 1) → (0, 1) and g : (0, 1) → [0, 1] such that f and g are differentiable almost everywhere and continuous, but g ◦f fails to be almost-everywhere differentiable.
Proof. Before our proof, we review a generalised version of the Cantor set and Cantor function [11, 14] deﬁned over the open interval (0, 1): the λ-Cantor set Cλ and the λ-Cantor function φλ for
λ ∈ (0, 1], which reduce to the original Cantor set and function when λ = 1. The Cλ and φλ will serve as building blocks of counterexamples presented in this paper. The set Cλ ⊂ (0, 1) consists of all the real numbers not removed during the following procedure: at step k = 0, start with a closed interval [0, 1] ⊂ R; at each step k > 0, remove every open interval of length λ/3k which locates exactly at the middle of each remaining closed interval; and after all these steps, remove 0 and 1.
Note that what remains after the step k are 2k closed intervals. The function φλ : (0, 1) → (0, 1) is deﬁned as follows, based on the construction of Cλ. At each step k > 0, deﬁne φλ over the i-th open interval to be removed as φλ(x) = (2i − 1)/2k (i ∈ [2k−1]). After all these steps, φλ is deﬁned over some dense open subset S of [0, 1] (in fact, S = (0, 1) \ C1). Since φλ : S → [0, 1] is uniformly continuous with S being dense in [0, 1], it has the unique continuous extension φλ : [0, 1] → [0, 1]
[37, Exercise 4.13]. The λ-Cantor function refers to its restricted version φλ : (0, 1) → (0, 1). 2 (φ1(x) + x) [8, Example 2.3.1].
Returning to the proof, let f be the inverse of the homeomorphism F : (0, 1) → (0, 1) deﬁned by
F (x) = 1
It is known that f (C1/2) = C1 [8, Example 2.3.2].
Construct g : (0, 1) → [0, 1] based on the construction of C1 as follows, similarly to [18, Ex-ample 8.18]: at each step k > 0, deﬁne g over the i-th open interval (li, ri) to be removed, as g(y) = 2−k · [1 − (y − (ri + li)/2)2/((ri − li)/2)2] (i ∈ [2k−1]); and deﬁne g over C1 as g(y) = 0.
See Figure 1 for the graphs of φ1, f , and g constructed so far. Clearly, f is continuous. Also, g is continuous, since the height of the parabolas deﬁned at the k-th step of g’s construction converges to 0 as k → ∞, and g(C1) = {0} (see Appendix A for the details). Hence, g ◦ f is continuous. Note that f is even Lipschitz continuous. To prove this, observe that f can be constructed similarly to
φ1/2, and repeat the proof of the Lipschitz continuity of φ1/2 [14].
We now show that f and g are differentiable almost everywhere, but g ◦ f is not. First, since f is
Lipschitz continuous, it is differentiable almost everywhere by Rademacher’s theorem [41, Theorem 2.2.4]. Next, since g is differentiable on (0, 1) \ C1 by its construction, it is differentiable almost everywhere (as C1 has measure 0). Lastly, g ◦ f is non-differentiable on C1/2, which has measure 1/2, due to that: the parabolas deﬁned at the k-th step of g’s construction get sharper as k → ∞; g(C1) = {0}; and f is a homeomorphism with f (C1/2) = C1 (see Appendix A for the details).
The second claim is about a chain rule.
Claim 2 If f , g, and g ◦ f are differentiable almost everywhere and continuous, then the standard chain rule for g ◦ f should hold almost everywhere. In particular, if f, g : (0, 1) → (0, 1), then (g ◦ f )(cid:48)(x0) = g(cid:48)(f (x0)) · f (cid:48)(x0) for almost all x0 ∈ (0, 1).
Note that all of f , g, and g ◦ f in the claim are assumed to be differentiable almost everywhere. The claim comes from heuristic reasoning that if we just avoid those rare non-differentiable inputs of g ◦ f , we should be able to use the standard result for differentiation, including the chain rule.
The second claim is also wrong. The ﬂaw in the heuristic reasoning from above is that the almost-everywhere differentiability of f , g, and g ◦ f does not stop g(cid:48)(f (x0)) from being undeﬁned for many x0’s in (0, 1). This is related to the ﬂaw in the justiﬁcation for the ﬁrst claim that we explained. The next proposition and its proof provide a concrete example for this phenomenon:
Proposition 2. There exist functions f, g : (0, 1) → (0, 1) such that f , g, and g ◦ f are differentiable almost everywhere and continuous, but it is not that g(cid:48)(f (x)) is deﬁned for almost all x ∈ (0, 1). 3
Proof. Let f (x) = 1/2 and g(y) = ReLU(y − 1/2) + 1/2. Then, (g ◦ f )(x) = 1/2. Certainly, f , g, and g ◦ f are differentiable almost everywhere and Lipschitz continuous. But, g is not differentiable at f (x) = 1/2 for all x ∈ (0, 1). So, it is not that g(cid:48)(f (x)) is deﬁned for almost all x ∈ (0, 1).
The third claim is a natural reaction to the failure of the second claim. It implements the strategy of making the chain rule in the second claim more permissive such that the counter argument of
Proposition 2 no longer applies. The claim expresses a weaker version of the rule that allows one to set the derivatives of f and g to arbitrary values wherever f and g are not differentiable.
Claim 3 Let f, g : (0, 1) → (0, 1). If f , g, and g ◦ f are differentiable almost everywhere and continuous, then there should exist df , dg : (0, 1) → R such that df (x0) = f (cid:48)(x0), dg(y0) = g(cid:48)(y0), and (g ◦ f )(cid:48)(x0) = dg(f (x0)) · df (x0) for almost all x0, y0 ∈ (0, 1).
The functions df and dg in the claim are the extensions of f (cid:48) and g(cid:48) that set df (x) and dg(y) to arbitrary values whenever f (cid:48)(x) and g(cid:48)(y) are undeﬁned. The chain rule in the claim is phrased in terms of these extensions df and dg, so that it does not suffer from the problem pointed out in
Proposition 2. However, this new rule is still ﬂawed as shown in the next proposition:
Proposition 3. There exist functions f, g : (0, 1) → (0, 1) such that f , g, and g ◦ f are differentiable almost everywhere and continuous, but for some measurable subset A ⊆ (0, 1) with non-zero measure, they satisfy the following property: f (cid:48)(x) = 0 and (g ◦ f )(cid:48)(x) (cid:54)= 0 for all x ∈ A.
Proof. Consider the function f in the proof of Proposition 1. Let g be the 1-Cantor function φ1. Then, g ◦ f is the (1/2)-Cantor function φ1/2. We already showed f is differentiable almost everywhere and even Lipschitz continuous. Since g and g ◦ f are monotone on (0, 1), they are differentiable almost everywhere by the monotone differentiation theorem [41, Theorem 1.6.25]; and they are clearly continuous. We now show there exists A ⊆ C1/2 with the desired properties. Since C1/2 has measure 1/2, it sufﬁces to prove f (cid:48)(x) = 0 and (g ◦ f )(cid:48)(x) = 2 for almost all x ∈ C1/2. The claim indeed holds due to the following: f and g ◦ f are Lipschitz, so absolutely continuous; f (cid:48)(x) = 2 and (g ◦ f )(cid:48)(x) = 0 for all x /∈ C1/2; f (cid:48)(x) ≥ 0 and (g ◦ f )(cid:48)(x) ≤ 2 for all x ∈ C1/2 whenever these derivatives exist; and C1/2 has measure 1/2. For the details, see [8, Example 2.3.2] and [14].
The proposition implies the third claim is doomed. The claim says that df (x0) = f (cid:48)(x0) and (g ◦ f )(cid:48)(x0) = dg(f (x0)) · df (x0) for almost all x0 ∈ A. But both equalities cannot hold simultaneously: if they do, by Proposition 3, (g ◦ f )(cid:48)(x0) = dg(f (x0)) · df (x0) = dg(f (x0)) · f (cid:48)(x0) = dg(f (x0)) · 0 = 0, but the same proposition also entails (g ◦ f )(cid:48)(x0) (cid:54)= 0, leading to a contradiction.
A lesson from these ﬂawed claims is that although the notion of correctness in Deﬁnition 1 only refers to almost-everywhere differentiability, we need a condition stronger than it, which behaves better in handling function composition and gives rise to a chain rule. We describe such a condition next. 3 PAP Function and Intensional Derivative
Our justiﬁcation of autodiff systems relies on two key concepts: piecewise analyticity under analytic partition, and intensional derivative. The ﬁrst is a (strictly) stronger property about functions than almost-everywhere differentiability, and yet it is satisﬁed by practically all programs targeted at by existing autodiff systems, as we will show in §4. Functions with this new property, called PAP functions, have an unusual type of derivatives, called intensional derivatives, which form the second concept. Intensional derivatives of PAP functions are deﬁned everywhere and satisfy a chain rule, while still agreeing with standard derivatives for almost all inputs. In fact, the PAP functions have not just ﬁrst-order but also all higher-order intensional derivatives. In §4, we will show that most autodiff systems compute intensional derivatives when applied to functions with non-differentiabilities.
To expand the overview of the two concepts just given, we need a notion of piecewise representation:
Deﬁnition 2 (Piecewise Representation). Let X ⊆ Rn and Y ⊆ Rm. A set of pairs γ =
{(cid:104)Ai, f i(cid:105)}i∈[I] is a piecewise representation of a function from X to Y or simply representa-tion if and only if (i) I ∈ Z>0 ∪ {∞}; (ii) {Ai}i∈[I] is a partition of X ; and (iii) each f i : X i → Y is a function on an open domain X i ⊆ Rn with Ai ⊆ X i. The evaluation of a representation
γ = {(cid:104)Ai, f i(cid:105)}i∈[I] is the map (cid:104)(cid:104)γ(cid:105)(cid:105) : X → Y deﬁned by: (cid:104)(cid:104)γ(cid:105)(cid:105)(x) = f i(x) for all i ∈ [I] and x ∈ Ai. 4
A representation γ = {(cid:104)Ai, f i(cid:105)}i∈[I] describes a function f in terms of a ﬁnite or countable number of component functions f i and their scopes Ai of application. For each input x, the value of f at x is determined by some component f i, chosen based on which piece of the input partition {Ai}i∈[I], the x belongs to. Our notation for the described f is (cid:104)(cid:104)γ(cid:105)(cid:105). Note that the domain X i of each f i has to be open, and it may be larger than Ai, the set of inputs where f i is used. As a result, a function can be described by an inﬁnite number of representations. For instance, ReLU has at least two representations:
{(cid:104)R<0, x (cid:55)−→ 0(cid:105), (cid:104)R≥0, x (cid:55)−→ x(cid:105)} and {(cid:104)R<0, x (cid:55)−→ 0(cid:105), (cid:104){0}, x (cid:55)−→ 2x(cid:105), (cid:104)R>0, x (cid:55)−→ x(cid:105)}.
Having a piecewise representation of a function, instead of just the function, has several advantages.
One of them is that for many properties of functions, we can derive their piecewise variants using those representations. We apply this advantage to analyticity. Recall that a real-valued function is analytic if and only if it is inﬁnitely differentiable and is equal to its Taylor expansion. For Rm-valued function g, the analyticity means the coordinate-wise analyticity: for all j ∈ [m], the composition
πj ◦ g of the j-th projection πj and g is analytic. Analytic functions are favoured by autodiff systems, because they are inﬁnitely differentiable and become zero only at a measure-zero set of inputs (unless they are zero everywhere or their domains are disconnected). Let X ⊆ Rn and Y ⊆ Rm be any sets.
Deﬁnition 3 (Analytic Partition). A set A ⊆ Rn is analytic if and only if for some J, L ∈ Z>0, there l ⊆ Rn (j ∈ [J], l → R over open domains X + are analytic functions g+ j (x) > 0) ∧ ((cid:86) l ∈ [L]) such that A = {x ∈ Rn | ((cid:86) l ∧ g− j ∧ g+ l (x) ≤ 0)}.
A partition {Ai}i∈[I] of X is analytic if and only if all Ai are analytic.
Deﬁnition 4 (PAP Representation). A representation γ = {(cid:104)Ai, f i(cid:105)}i∈[I] from X to Y is piecewise analytic under analytic partition (in short, PAP) if and only if {Ai}i∈[I] is an analytic partition of
X and f i is analytic over its domain X i for all i ∈ [I].
Deﬁnition 5 (PAP Function). A function f : X → Y is piecewise analytic under analytic partition (in short, PAP) if f = (cid:104)(cid:104)γ(cid:105)(cid:105) for some PAP representation γ. j , X − l∈[L] x ∈ X −
: X − j∈[J] x ∈ X + j → R and g− j : X + l
The deﬁnitions identify PAP representations and PAP functions as those built by the two-step process: we ﬁrst split the input domain such that boundaries of the split regions are expressed by the zero sets of analytic functions, and next choose an appropriate analytic function for each piece of the split. Note the use of analytic functions in both steps. Thus, just like the standard analyticity, the PAP property implies almost-everywhere differentiability (Proposition 4), but not vice versa (Proposition 5).
Proposition 4 ([47]). All PAP functions are differentiable almost everywhere.
Proof. The proof extends the one for a similar result in [47]. The key idea is to use the fact that the zero set of a non-constant analytic function over a connected open domain has measure zero [32]. To prove the proposition, we show that for each PAP function f , there exist countably many non-constant analytic functions {gj}j over connected open domains such that if f is non-differentiable at x ∈ X , then x belongs to the zero set of some gj. For the details, see Appendix B.3.
Proposition 5. There is a continuous almost-everywhere differentiable yet non-PAP function.
Proof. We have two sufﬁcient conditions for a function f to be non-PAP: (i) the k-th order derivative of f is undeﬁned on a set of positive measure for some k ≥ 1 (Proposition 8); and (ii) the k-th order derivative of f is undeﬁned on an uncountable set for some k ≥ 1, and f is deﬁned on a subset of
R (Appendix B.5). The following functions satisfy (i) with k = 2: the λ-Cantor function for all
λ ∈ (0, 1), f in the proof of Proposition 1, and Volterra’s function V : (0, 1) → R [18, Example 8.35]. The following functions satisfy (ii) with k = 1: the 1-Cantor function and g in the proof of Proposition 1. All these functions are known (or already proven above) to be continuous and almost-everywhere differentiable. Hence, all of them are desired functions.
We now deﬁne intensional derivatives of PAP representations and PAP functions. Let γ =
{(cid:104)Ai, f i(cid:105)}i∈[I] be a PAP representation from X to Y for some X ⊆ Rn and Y ⊆ Rm.
Deﬁnition 6 (Intensional Derivative of PAP Representation). An intensional derivative of γ is the set Dγ = {(cid:104)Ai, Df i(cid:105)}i∈[I], where Df i is the standard derivative of f i viewed as a function from the domain X i of f i to Rm×n.
Proposition 6. The intensional derivative Dγ is a PAP representation from X to Rm×n.
Proof. Nearly all the requirements for Dγ to be a PAP representation directly follow from the fact that γ is PAP. The only exception is the analyticity of Df i. There we use the fact that the operation of taking a (standard) partial derivative of a function preserves analyticity [25, Proposition 2.2.3]. 5
Deﬁnition 7 (First-Order Intensional Derivative). Let f : X → Y be a PAP function. Deﬁne ∂•f to be the following set of functions: ∂•f = {(cid:104)(cid:104)Dγ(cid:105)(cid:105) | γ is a PAP representation of f }. Each df ∈ ∂•f is called a (ﬁrst-order) intensional derivative of f .
By Proposition 6, only PAP functions live in ∂•f . Thus, we can also take intensional derivatives of functions in ∂•f . We push this observation further and deﬁne higher-order intensional derivatives:
Deﬁnition 8 (Higher-Order Intensional Derivative). Let f : X → Y be a PAP function. For each
• f = {df k ∈ ∂•(df k−1) | k ∈ Z≥0, inductively deﬁne the set ∂k df k−1 ∈ ∂k−1
• f is called a k-th order intensional derivative of f . f } for k ≥ 1. Each df k ∈ ∂k
• f of functions by: ∂0
•f = {f }, and ∂k
•
•
• f of the k-th order intensional derivatives is not empty.
.
A function f : X → Y may have zero, one, or more than one intensional derivatives. Having at least one intensional derivative corresponds to f being differentiable in the standard sense. The next propositions show that every PAP function is inﬁnitely differentiable in the new sense (Proposition 7), and that these standard and new notions of differentiability coincide if we ignore a measure-zero set of inputs (Proposition 8). Let f : X → Y be a PAP function for some X ⊆ Rn and Y ⊆ Rm.
Proposition 7. For all k ∈ Z≥0, the set ∂k
Furthermore, its elements are again PAP functions from X to Rm×nk
Proof. The proof is by induction on k. First, for the case k = 0, ∂k
• f is a singleton set and its unique element f is PAP. Hence, the proposition holds. Next, consider the case k > 0. By induction hypothesis, ∂k−1 f is not empty and consists of PAP functions only. Thus, the set {γ | γ is a PAP representation of g for some g ∈ ∂k−1 f } is not empty. This and Proposition 6 imply that ∂k
• f (obtained by applying (cid:104)(cid:104)D−(cid:105)(cid:105) on the above set) is nonempty and contains only PAP functions.
Proposition 8. For all k ∈ Z≥0, f has the k-th order standard derivative almost everywhere, and this derivative agrees with any k-th order intensional derivative df k ∈ ∂k
In the proposition, we view the k-th order standard derivative of f as a function of type X → Rm×nk
For instance, when k = 1, the derivative maps an input to the Jacobian matrix of f .
Proof of Proposition 8. The ﬁrst claim is proven similarly to Proposition 4, except that we additionally use the following: an analytic function is inﬁnitely differentiable. As in Proposition 4, we prove a stronger statement: there exist countably many non-constant analytic functions {gj}j over connected open domains such that for all k, if the k-th order standard derivative of f is not deﬁned at x ∈ X , then x is in the zero set of some gj. Next, consider the second claim. Its current form is not strong enough to enable inductive proofs. We instead prove a stronger statement by induction on k: for each df k ∈ ∂k
• f , there exist countably many non-constant analytic functions {hl}l over connected open domains such that the k-th order standard derivative of f is well-deﬁned, and agrees with df k, at all those inputs not in the zero sets of {hl}l. For the details, see Appendices B.3 and B.4.
• f almost everywhere.
•
.
Intensional derivatives behave better than standard derivatives. First, the intensional derivatives of a PAP function f are total functions, i.e., functions deﬁned for all inputs (Proposition 7). Contrast this with the fact that the standard derivative of f is a partial function in general. The intensional derivatives can be understood as extensions of this standard derivative of f to those problematic inputs that make f non-differentiable (Proposition 8). The totality simpliﬁes the reasoning about intensional derivatives. Second, the intensional derivatives satisfy a chain rule for all PAP functions (Proposition 10). Let f : X → Y and g : Y → Rl be PAP functions for some X ⊆ Rn and Y ⊆ Rm.
Proposition 9. g ◦ f is a PAP function.
Proof. Since f and g are PAP, they have PAP representations γf = {(cid:104)Ai, f i(cid:105)}i∈[I] and γg =
{(cid:104)Bj, gj(cid:105)}j∈[J]. Deﬁne their composition as follows: γg ◦ γf = {(cid:104)C (cid:104)i,j(cid:105), gj ◦ f i(cid:105)}(cid:104)i,j(cid:105)∈[I]×[J] where
C (cid:104)i,j(cid:105) = {x ∈ X | x ∈ Ai ∧ f i(x) ∈ Bj}. Then, γg ◦ γf is a representation of g ◦ f . Also, it is PAP as the composition of analytic functions is analytic [25, Proposition 2.2.8]. Thus, g ◦ f is PAP.
Proposition 10 (Chain Rule for Intensional Derivatives). Let df : X → Rm×n and dg : Y → Rl×m be intensional derivatives of f and g (i.e., df ∈ ∂•f and dg ∈ ∂•g). Let h = g◦f . Then, the following function dh : X → Rl×n is an intensional derivative of h (i.e., dh ∈ ∂•h): dh(x) = dg(f (x))·df (x) for all x ∈ X , where dg(f (x)) · df (x) is the multiplication of matrices dg(f (x)) and df (x).
Proof. By the deﬁnition of intensional derivative, df = (cid:104)(cid:104)Dγf (cid:105)(cid:105) and dg = (cid:104)(cid:104)Dγg(cid:105)(cid:105) for some PAP representations γf = {(cid:104)Ai, f i(cid:105)}i∈[I] and γg = {(cid:104)Bj, gj(cid:105)}j∈[J] of f and g. Let γg ◦ γf be the composed representation deﬁned in the proof of Proposition 9. Then, γg ◦ γf is a PAP representation 6
of g ◦ f . So, (cid:104)(cid:104)D(γg ◦ γf )(cid:105)(cid:105) is an intensional derivative of g ◦ f . Also, for all x ∈ X , if we let (cid:104)i, j(cid:105) ∈ [I] × [J] with x ∈ Ai and f i(x) ∈ Bj, then (cid:104)(cid:104)D(γg ◦ γf )(cid:105)(cid:105)(x) = D(gj ◦ f i)(x) =
D(gj)(f i(x)) · D(f i)(x) = (cid:104)(cid:104)Dγg(cid:105)(cid:105)(f (x)) · (cid:104)(cid:104)Dγf (cid:105)(cid:105)(x). The ﬁrst and last equalities follow from the deﬁnition of intensional derivative. The second equality uses the chain rule for standard derivatives: the rule holds here because f i and gj are analytic in neighbourhoods of x and f i(x), respectively.
We next use these properties to show that existing autodiff systems compute intensional derivatives. 4 Correctness of Autodiff Systems
Consider a simple programming language that assumes real-valued input variables x1, . . . , xN and has the following syntax for programs: e ::= c | xi | f(e1, . . . , en) | if (e1 > 0) e2 e3.
A program e in the language describes a real-valued computation. It is a real number c, an input variable xi, the application of a primitive function f to arguments e1, . . . , en, or a conditional expression. In the third case, the applied function is required to be a PAP function of the right type (i.e., Rn → R in this case). We remark that practically all primitive functions supported by autodiff systems are indeed PAP; we are unaware of any non-PAP primitive function used in practice. If this requirement is met, all programs e mean PAP functions of type RN → R. More precisely, we
: RN → R inductively, as shown below: for all v ∈ RN , interpret each program e as a function v = f( e1 v = c, (cid:74) (cid:75) (cid:75) v else e2 (cid:75) v > 0) then e1 (cid:75) (cid:74) where vi is the i-th component of the vector v, and f : Rn → R is the PAP function denoted by the function symbol f in the language. Then, the deﬁned
Proposition 11. For every program e, its denotation is always PAP. is a PAP function from RN to R. f(e1, . . . , en) (cid:74) v = if ( xi (cid:74) if (e1 > 0) e2 e3 e (cid:74) v = vi, v, . . . , e3 c (cid:75) v), (cid:74) v. en (cid:75) (cid:75) (cid:75) (cid:74) (cid:75) (cid:74) (cid:74) (cid:74) (cid:75) e (cid:74) e (cid:74) (cid:75) (cid:75) (cid:75) (cid:74) (cid:75) (cid:75) (cid:75) (cid:75) (cid:75) (cid:74) (cid:74) (cid:74) (cid:75) (cid:74) (cid:74) e e1 (cid:75) e1 c (cid:75) e1 (cid:74) en (cid:74)
∇v; . . . ; v) · [ (cid:74)
∇v], e3
∇v = (cid:126)01×N , en (cid:74) (cid:75)
∇v else e1 (cid:75) v>0) then v, . . . , en
∇v = if ( (cid:74)
∇v = ( (cid:101)Df)( e1 (cid:75) (cid:75) if (e1>0) e2 e3 f(e1, . . . , en) (cid:74) (cid:74)
∇v = [(cid:126)0(i−1)×1;(cid:126)11×1;(cid:126)0(N −i)×1](cid:62),
We show that if an autodiff system for the language satisﬁes two requirements to be described shortly, it essentially computes an intensional derivative, even when the input function is not differentiable.
The ﬁrst requirement is that for every primitive operation f of type Rn → R, the system should come with a function (cid:101)Df : Rn → R1×n that satisﬁes (cid:101)Df ∈ ∂•f and serves as a “derivative” of
∇ : RN → R1×N , e f. To describe the second requirement, we inductively deﬁne the function formalising symbolic differentiation of a program e, as follows: for all v ∈ Rn, (cid:74) (cid:75)
∇v; . . . ; e2 v, . . . , (cid:74) (cid:75)
∇v] that is constructed by concatenating n matrices (cid:74)
∇v computes a “Jacobian” matrix of (cid:75)
∇v. xi
Here (cid:126)ci×j denotes the i × j matrix containing only the constant c, and the RHS of the second v) and the n × N matrix en equation means the multiplication of the 1 × n matrix ( (cid:101)Df)( (cid:75)
∇v of size
∇v, . . . ,
[ en e1 (cid:74) (cid:74) (cid:75) (which is a 1 × N matrix in this 1 × N . The deﬁnition of case) in the usual way of symbolic differentiation: run the program e under the input v, and calculate the “Jacobian” matrix of the encountered operations using the chain rule with the “derivatives” (cid:101)Df of those operations f [1, 4, 23, 46]. Our second requirement is that given a program e and an input v ∈ RN , if the system performs forward-mode (or reverse-mode) autodiff with a tangent vector w ∈ RN (or a cotangent vector u ∈ R), it should output the Jacobian-vector product (
∇v) · w ∈ R (cid:75) (cid:74)
∇v) ∈ RN ) [20, 27, 31]. Intuitively, the second requirement (or the vector-Jacobian product u(cid:62)· ( (cid:74) says that the output of an autodiff system should coincide with that of symbolic differentiation. Note that the requirement does not ﬁx the choice of an autodiff algorithm, and has two separate conditions for forward-mode and reverse-mode algorithms. Our next results show that these requirements ensure the correctness of an autodiff system.
Theorem 12. If (cid:101)Df ∈ ∂•f for all primitive functions f, then
Corollary 13. Assume that an autodiff system for the language in this section satisﬁes the two requirements. Then, for each program e, there exists an intensional derivative df of such that if the system performs forward-mode (or reverse-mode) autodiff with a tangent vector w ∈ RN (or a cotangent vector u ∈ R), it computes the Jacobian-vector product df (v) · w ∈ R (or the vector-Jacobian product u(cid:62)· df (v) ∈ RN ) for every input v ∈ RN . Furthermore, the computed entity is the corresponding Jacobian-vector product (or vector-Jacobian product) with the standard derivative of for almost all inputs v ∈ RN . for all programs e.
∇ ∈ ∂• (cid:75) e (cid:75) e e e e e e (cid:75) (cid:75) (cid:74) (cid:74) (cid:74) (cid:75) (cid:75) (cid:74) (cid:74) (cid:74) (cid:75) 7
Remark 1 (Intensional Derivatives in Practice). We brieﬂy discuss whether the ﬁrst requirement mentioned earlier is indeed met by autodiff systems. TensorFlow [2] and PyTorch [33], two popular autodiff systems, support a wide range of primitive functions that are not differentiable for some inputs. A well-known example is relu : R → R (i.e., the map x (cid:55)−→ max(0, x)). This function is not differentiable at 0, but when these autodiff systems are applied to differentiate relu at 0, they return 0 instead of an error (i.e., ( (cid:101)Drelu)(0) = 0). It means that the systems compute the intensional derivative (cid:104)(cid:104)Dγ(cid:105)(cid:105) of relu for γ = {(cid:104)R>0, x (cid:55)−→ x(cid:105), (cid:104)R≤0, x (cid:55)−→ 0(cid:105)}. Thus, they fulﬁl the ﬁrst requirement on relu. Note that setting the derivative at the input 0 to any c ∈ R is an acceptable option; it can be justiﬁed by a different PAP representation γ(cid:48) of relu where γ(cid:48) = {(cid:104)R>0, x (cid:55)−→ x(cid:105), (cid:104){0}, x (cid:55)−→ c·x(cid:105), (cid:104)R<0, x (cid:55)−→ 0(cid:105)}. Another example is reciprocal_no_nan from TensorFlow.
It means the map f (x) = 1/x if x (cid:54)= 0, and 0 if x = 0. Differentiating this map by TensorFlow gives the function ( (cid:101)Df )(x) = −1/x2 if x (cid:54)= 0, and 0 if x = 0. The (cid:101)Df is an intensional derivative of f , because γ = {(cid:104)R \ {0}, x (cid:55)−→ 1/x(cid:105), (cid:104){0}, x (cid:55)−→ 0(cid:105)} is a PAP representation of f and (cid:104)(cid:104)Dγ(cid:105)(cid:105) coincides with (cid:101)Df . Hence, TensorFlow meets the ﬁrst requirement on reciprocal_no_nan.
However, for some functions, TensorFlow and PyTorch behave more conservatively than our theory of PAP functions permits. A good example is sqrt. The domain of sqrt is R≥0, but differentiating sqrt at 0 using these autodiff systems returns +inf, which means ∞ (i.e., ( (cid:101)Dsqrt)(0) = ∞). Thus, the ﬁrst requirement on sqrt is violated by these systems. One consequence of this is that when the systems are applied to differentiate sqrt(mult(x, 0)), they end up with computing 0*(+inf) that evaluates to NaN (meaning an undeﬁned number in ﬂoating point arithmetic) for all x ∈ R. Note that sqrt(mult(x, 0)) is constantly zero and has zero as its derivative. This conservative behaviour could have been avoided if TensorFlow and PyTorch had recognised that sqrt is a PAP function and had x) if x > 0, used its intensional derivative instead. For instance, the function (dsqrt)(x) = 1/(2 and 0 if x = 0, is an intensional derivative of sqrt, since γ = {(cid:104)R>0, x (cid:55)−→ x(cid:105), (cid:104){0}, x (cid:55)−→ 0(cid:105)} is a PAP representation of sqrt. If TensorFlow and PyTorch used dsqrt as a derivative of sqrt, they would be able to differentiate sqrt(mult(x, 0)) correctly for all x ∈ R.
√
√ 5