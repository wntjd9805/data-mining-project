Abstract
We propose average Localisation-Recall-Precision (aLRP), a uniﬁed, bounded, balanced and ranking-based loss function for both classiﬁcation and localisation tasks in object detection. aLRP extends the Localisation-Recall-Precision (LRP) performance metric (Oksuz et al., 2018) inspired from how Average Precision (AP)
Loss extends precision to a ranking-based loss function for classiﬁcation (Chen et al., 2020). aLRP has the following distinct advantages: (i) aLRP is the ﬁrst ranking-based loss function for both classiﬁcation and localisation tasks. (ii) Thanks to using ranking for both tasks, aLRP naturally enforces high-quality localisation for high-precision classiﬁcation. (iii) aLRP provides provable balance between positives and negatives. (iv) Compared to on average ∼6 hyperparameters in the loss functions of state-of-the-art detectors, aLRP Loss has only one hyperparameter, which we did not tune in practice. On the COCO dataset, aLRP Loss improves its ranking-based predecessor, AP Loss, up to around 5 AP points, achieves 48.9
AP without test time augmentation and outperforms all one-stage detectors. Code available at: https://github.com/kemaloksuz/aLRPLoss. 1

Introduction
Object detection requires jointly optimizing a classiﬁcation objective (Lc) and a localisation objective (Lr) combined conventionally with a balancing hyperparameter (wr) as follows:
L = Lc + wrLr. (1)
Optimizing L in this manner has three critical drawbacks: (D1) It does not correlate the two tasks, and hence, does not guarantee high-quality localisation for high-precision examples (Fig. 1). (D2) It requires a careful tuning of wr [8, 26, 33], which is prohibitive since a single training may last on the order of days, and ends up with a sub-optimal constant wr [4, 11]. (D3) It is adversely impeded by the positive-negative imbalance in Lc and inlier-outlier imbalance in Lr, thus it requires sampling strategies [13, 14] or specialized loss functions [9, 22], introducing more hyperparameters (Table 1).
A recent solution for D3 is to directly maximize Average Precision (AP) with a loss function called
AP Loss [7]. AP Loss is a ranking-based loss function to optimize the ranking of the classiﬁcation outputs and provides balanced training between positives and negatives.
In this paper, we extend AP Loss to address all three drawbacks (D1-D3) with one, uniﬁed loss function called average Localisation Recall Precision (aLRP) Loss. In analogy with the link between precision and AP Loss, we formulate aLRP Loss as the average of LRP values [19] over the positive examples on the Recall-Precision (RP) curve. aLRP has the following beneﬁts: (i) It exploits ranking for both classiﬁcation and localisation, enforcing high-precision detections to have high-quality
∗Equal contribution for senior authorship. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: aLRP Loss enforces high-precision detections to have high-IoUs, while others do not. (a) Classiﬁcation and three possible localisation outputs for 10 anchors and the rankings of the positive anchors with respect to (wrt) the scores (for C) and IoUs (for R1, R2 and R3). Since the regressor is only trained by positive anchors, “–” is assigned for negative anchors. (b,c) Performance and loss assignment comparison of R1, R2 and R3 when combined with C. When correlation between the rankings of classiﬁer and regressor outputs decreases, performance degrades up to 17 AP (b). While any combination of Lc and Lr cannot distinguish them, aLRP Loss penalizes the outputs accordingly (c). The details of the calculations are presented in the Supp.Mat.
Table 1: State-of-the-art loss functions have several hyperparameters (6.4 on avg.). aLRP Loss has only one for step-function approximation (Sec. 2.1). See the Supp. Mat. for descriptions of the required hyperparameters. FL: Focal Loss, CE: Cross Entropy, SL1: Smooth L1, H: Hinge Loss.
Method
L
AP Loss+α SL1
FL+ α SL1
FL+α IoU+β CE
DR Loss+α SL1
α log(max(eCE × eβSL1))+γ FL
AP Loss [7]
Focal Loss [14]
FCOS [28]
DR Loss [24]
FreeAnchor [33]
Faster R-CNN [25] CE+α SL1+βCE+γ SL1
Center Net [8]
Ours
FL+FL+α L2+β H+γ (SL1+SL1) aLRP Loss
Number of hyperparameters 3 4 4 5 8 9 10 1 localisation (Fig. 1). (ii) aLRP has a single hyperparameter (which we did not need to tune) as opposed to ∼6 in state-of-the-art loss functions (Table 1). (iii) The network is trained by a single loss function that provides provable balance between positives and negatives.
Our contributions are: (1) We develop a generalized framework to optimize non-differentiable ranking-based functions by extending the error-driven optimization of AP Loss. (2) We prove that ranking-based loss functions conforming to this generalized form provide a natural balance between positive and negative samples. (3) We introduce aLRP Loss (and its gradients) as a special case of this generalized formulation. Replacing AP and SmoothL1 losses by aLRP Loss for training RetinaNet improves the performance by up to 5.4AP, and our best model reaches 48.9AP without test time augmentation, outperforming all existing one-stage detectors with signiﬁcant margin. 1.1