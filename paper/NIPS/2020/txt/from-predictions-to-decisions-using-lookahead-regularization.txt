Abstract
Machine learning is a powerful tool for predicting human-related outcomes, from creditworthiness to heart attack risks. But when deployed transparently, learned models also affect how users act in order to improve outcomes. The standard approach to learning predictive models is agnostic to induced user actions and provides no guarantees as to the effect of actions. We provide a framework for learning predictors that are accurate, while also considering interactions between the learned model and user decisions. For this, we introduce look-ahead regular-ization which, by anticipating user actions, encourages predictive models to also induce actions that improve outcomes. This regularization carefully tailors the uncertainty estimates that govern conﬁdence in this improvement to the distribu-tion of model-induced actions. We report the results of experiments on real and synthetic data that show the effectiveness of this approach. 1

Introduction
Machine learning is increasingly being used in domains that have considerable impact on people, ranging from healthcare [7], to banking [45], to manufacturing [52]. In many of these domains, fairness and safety concerns promote the deployment of fully transparent predictive models. An unavoidable consequence of this transparency is that end-users are prone to use models prescriptively: if a user (wrongly) views a predictive model as a description of the real world phenomena it models (e.g., heart attack risk), then she may look to the model for how to adapt her features in order to improve future outcomes (e.g., reduce her risk). But predictive models optimized for accuracy cannot in general be assumed to faithfully reﬂect post-modiﬁcation outcomes, and model-guided actions can prove to be detrimental. Our goal in this paper is to present a learning framework for organizations seeking to deploy learned models in a way that is transparent and responsible, a setting we believe applies widely. Consider a medical center who would like to publish an online tool to allow users to estimate their heart attack risk, while keeping in mind that users may also infer how lifestyle changes will affect future risk.1 Consider a lender who would like to be transparent about their
ﬁrst-time mortgage approval process, while knowing that this will suggest to applicants how they may alter their credit proﬁles. Consider a wine reseller, who would like to provide demand guidance 1For example, the Mayo Clinic, a leading medical center in the U.S., provides such a calculator [8].
MDCalc.com is an example of a site that provides medical many risk assessment calculators to the public. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to producers through an interpretable model while considering that producers may use the same guidance to modify future vintages. Each of these organizations must be cognizant of the actions their classiﬁers promote, and seek to emphasize features that promote safe adaptations as well as predictive accuracy.
It is well understood that correlation and causation need not go hand-in-hand [32, 39]. What is novel about this work is that we seek models that serve the dual purpose of achieving predictive accuracy and providing high conﬁdence that decisions made with respect to the model are safe. That is, we care foremost about the utility that comes from having a predictive tool, but recognize that these tools may also drive decisions.
To illustrate the potential pitfalls of a naïve predictive approach, consider a patient who seeks to understand his or her heart attack risk. If the patient consults a linear predictive model (as is often the case for medical models, see [51]), then a negative coefﬁcient for alcohol consumption may lead the patient to infer that a daily glass of red wine would improve his or her prognosis. Is this decision justiﬁed? Perhaps not, although this recommendation has often been made based on correlative evidence and despite a clear lack of experimental support [18, 40].
Our main insight is that controlling the tradeoff between accuracy and decision quality, where it exists, can be cast as a problem of model selection. For instance, there may be multiple models with similar predictive performance but different coefﬁcients, that therefore induce different decisions [5].
To achieve this tradeoff, we introduce lookahead regularization, which balances accuracy and the improvement associated with induced decisions. This is achieved by modeling how users will act, and penalizeing a model unless there is high conﬁdence that decisions will improve outcomes.
Formally, these decisions induce a target distribution p(cid:48) on covariates that may differ from the distribution of data at training, p. In particular, a decision will map an individual with covariates x to new covariates x(cid:48). For a prespeciﬁed conﬁdence level τ , we want to guarantee improvement for at least a τ -fraction of the population, comparing outcomes under p(cid:48) in relation to outcomes in p (under an invariance assumption on p(y|x)). The technical challenge is that p(cid:48) may differ considerably from p, resulting in uncertainty in estimating the effect of decisions. To solve this, lookahead regularization makes use of an uncertainty model that provides conﬁdence intervals around decision outcomes.
A discriminative uncertainty model is trained through importance weighting [14, 44, 48] to handle covariate shift, and is designed to estimate accurate intervals for p(cid:48).
Lookahead regularization has stages that alternate between optimizing the different components of our framework: the predictive model (under the lookahead regularization term), the uncertainty model (used within the regularization term), and the propensity model (used for covariate shift adjustment). If the uncertainty model is differentiable and the predictive model is twice-differentiable, then gradients can pass through the entire pipeline and gradient-based optimization can be applied.
We run three experiments. One experiment uses synthetic data and illustrates the approach, helping to understand what is needed for it to succeed. The second experiment considers an application to wine quality prediction, and shows that even simple tasks lead to interesting tradeoffs between accuracy and improved decisions. The third experiment focuses on predicting diabetes progression and includes a demonstration of the framework in a setting with individualized actions. 1.1