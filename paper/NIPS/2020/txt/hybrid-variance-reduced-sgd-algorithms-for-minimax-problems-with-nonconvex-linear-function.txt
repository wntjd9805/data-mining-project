Abstract
We develop a novel and single-loop variance-reduced algorithm to solve a class of stochastic nonconvex-convex minimax problems involving a nonconvex-linear objective function, which has various applications in different ﬁelds such as ma-chine learning and robust optimization. This problem class has several compu-tational challenges due to its nonsmoothness, nonconvexity, nonlinearity, and non-separability of the objective functions. Our approach relies on a new combi-nation of recent ideas, including smoothing and hybrid biased variance-reduced techniques. Our algorithm and its variants can achieve O(T −2/3)-convergence rate and the best known oracle complexity under standard assumptions, where T is the iteration counter. They have several computational advantages compared to exist-ing methods such as simple to implement and less parameter tuning requirements.
They can also work with both single sample or mini-batch on derivative estimators, and with constant or diminishing step-sizes. We demonstrate the beneﬁts of our algorithms over existing methods through two numerical examples, including a nonsmooth and nonconvex-non-strongly concave minimax model. 1

Introduction
We study the following stochastic minimax problem with nonconvex-linear objective function, which covers various practical problems in different ﬁelds, see, e.g., [4, 10, 12]: min x∈Rp max y∈Rn (cid:110)
Ψ(x, y) := R(x) + Eξ (cid:111) (cid:2)(cid:104)Ky, F(x, ξ)(cid:105)(cid:3) − ψ(y)
, (1) where F : Rp × Ω → Rq is a stochastic vector function deﬁned on a probability space (Ω, P), K ∈
Rq×n is a given matrix, (cid:104)·, ·(cid:105) is an inner product, and ψ : Rn → R∪{+∞} and R : Rp → R∪{+∞} are proper, closed, and convex functions [3]. Problem (1) is a special case of the nonconvex-concave minimax problem, where H(x, y) := Eξ
Due to the linearity of H w.r.t. y, (1) can be reformulated into a general stochastic compositional nonconvex problem of the form: (cid:2)(cid:104)Ky, F(x, ξ)(cid:105)(cid:3) is nonconvex in x and linear in y. (cid:110)
Ψ0(x) := φ0(F (x)) + R(x) ≡ φ0 (cid:0)Eξ (cid:111) (cid:2)F(x, ξ)(cid:3)(cid:1) + R(x)
, min x∈Rp where φ0 is a convex, but possibly nonsmooth function, deﬁned as
φ0(u) := max y∈Rn (cid:8)(cid:104)K (cid:62)u, y(cid:105) − ψ(y)(cid:9) ≡ ψ∗(K (cid:62)u), (2) (3) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
with ψ∗ being the Fenchel conjugate of ψ [3], and we deﬁne Φ0(x) := φ0(F (x)). Note that problem (2) is completely different from existing models such as [8, 9], where the expectation is inside the outer function φ0, i.e., φ0 (cid:2)F(x, ξ)(cid:3)(cid:1). We refer to this setting as a “non-separable" model. (cid:0)Eξ
Challenges: Developing numerical methods for solving (1) or (2) faces several challenges. First, it is often nonconvex, i.e., F is not afﬁne. Many recent papers consider special cases of (2) when Ψ0 in (2) is convex by imposing restrictive conditions, which are unfortunately not realistic in applications.
Second, the max-form φ0 in (3) is often nonsmooth if ψ is not strongly convex. This prevents the use of gradient-based methods. Third, since the expectation is inside φ0, it is very challenging to form an unbiased estimate for [sub]gradients of Φ0, making classical stochastic gradient-based methods inapplicable. Finally, prox-linear operator-based methods as in [8, 9, 34, 45] require large mini-batch evaluations of both function value F and its Jacobian F(cid:48), see [34, 43, 45], instead of single sample or small mini-batch, making them less ﬂexible and more expensive than gradient-based methods.