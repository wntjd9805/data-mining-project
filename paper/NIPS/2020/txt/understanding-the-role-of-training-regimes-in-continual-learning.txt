Abstract
Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks.
This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with dif-ferent degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes – learning rate, batch size, regularization method– can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks’ local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.1 1

Introduction
We study the continual learning problem, where a neural network model should learn a sequence of tasks rather than a single one. A signiﬁcant challenge in continual learning (CL) is that during training on each task, the data from previous ones are unavailable. One consequence of applying typical learning algorithms under such a scenario is that as the model learns newer tasks, the performance of the model on older ones degrades. This phenomenon is known as “catastrophic forgetting” [52].
This forgetting problem is closely related to the “stability-plasticity dilemma” [53], which is a common challenge for both biological and artiﬁcial neural networks. Ideally, a model needs plasticity to obtain new knowledge and adapt to new environments, while it also requires stability to prevent forgetting the knowledge from previous environments. If the model is very plastic but not stable, it can learn fast, but it also forgets quickly. Without further modiﬁcations in training, a naively trained neural network tends to be plastic but not stable. Note that plasticity in this scenario does not necessarily imply that neural nets can learn new tasks efﬁciently. In fact, they tend to be extremely data inefﬁcient. By being plastic, we mean a single update can change the function considerably. 1The code is available at: https://github.com/imirzadeh/stable-continual-learning 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
With the recent advances in the deep learning ﬁeld, continual learning has gained more attention since the catastrophic forgetting problem poses a critical challenge for various applications [43, 37].
A growing body of research has attempted to tackle this problem in recent years [58, 72, 57, 29].
Despite the tangible improvements in the continual learning ﬁeld, the core problem of catas-In particular, a variety of neural network models and trophic forgetting is still under-studied. training approaches have been proposed, however, to the best of our knowledge, there has been little work on systematically understanding the effect of common training regimes created by varying dropout regularization, batch size, and learning rate on overcoming catastrophic forget-ting2. Fig. 1 shows how signiﬁcantly these techniques can overcome catastrophic forgetting.
In this work, we explore the catastrophic forgetting prob-lem from an optimization and loss landscape perspective (Section 3) and hypothesize that the geometry of the local minima found for the different learned tasks correlates with the ability of the model to not catastrophically forget.
Empirically we show how a few well-known techniques, such as dropout and large learning rate with decay and shrinking batch size, can create a training regime to af-fect the stability of neural networks (Section 4). Some of them, like dropout, had been previously proposed to help continual learning [22, 54]. However, in this work, we provide an alternative justiﬁcation of why these tech-niques are effective. Crucially, we empirically show that jointly with a carefully tuned learning rate schedule and batch size, these simple techniques can outperform con-siderably more complex algorithms meant to deal with continual learning (Section 5). Our analysis can be ap-plied to any other training technique that widens the tasks’ local minima or shrinks the distance between them.
Figure 1: For the same architecture and dataset (Rotation MNIST) and only changing the training regime, the forget-ting is reduced signiﬁcantly at the cost of a relatively small accuracy drop on the current task. Refer to appendix C for details.
Our work shows that plain neural networks can be much stronger baselines for continual learning than previously thought, provided that we use the right hyperparameters. Moreover, the choice for the hyperparameters is orthogonal to other continual learning methods and can be integrated with these methods, as we show in Appendix C.8. 2