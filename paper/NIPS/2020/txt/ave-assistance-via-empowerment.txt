Abstract
One difﬁculty in using artiﬁcial agents for human-assistive applications lies in the challenge of accurately assisting with a person’s goal(s). Existing methods tend to rely on inferring the human’s goal, which is challenging when there are many potential goals or when the set of candidate goals is difﬁcult to identify.
We propose a new paradigm for assistance by instead increasing the human’s ability to control their environment, and formalize this approach by augmenting reinforcement learning with human empowerment. This task-agnostic objective preserves the person’s autonomy and ability to achieve any eventual state. We test our approach against assistance based on goal inference, highlighting scenarios where our method overcomes failure modes stemming from goal ambiguity or misspeciﬁcation. As existing methods for estimating empowerment in continuous domains are computationally hard, precluding its use in real time learned assistance, we also propose an efﬁcient empowerment-inspired proxy metric. Using this, we are able to successfully demonstrate our method in a shared autonomy user study for a challenging simulated teleoperation task with human-in-the-loop training. 1

Introduction
We aim to enable artiﬁcial agents, whether physical or virtual, to assist humans in a broad array of tasks. However, training an agent to provide assistance is challenging when the human’s goal is unknown because that makes it unclear what the agent should do. Assistance games [18] formally capture this as the problem of working together with a human to maximize a common reward function whose parameters are only known to the human and not to the agent. Naturally, approaches to assistance in both shared workspace [32, 14, 13, 33, 31, 25] and shared autonomy [21, 20, 11, 15, 32] settings have focused on inferring the human’s goal (or, more broadly, the hidden reward parameters) from their ongoing actions, building on tools from Bayesian inference [5] and Inverse Reinforcement
Learning [30, 37, 2, 4, 45, 19, 34, 12, 26]. However, goal inference can fail when the human model is misspeciﬁed, e.g. because people are not acting noisy-rationally [27, 36], or because the set of candidate goals the agent is considering is incorrect [7]. In such cases, the agent can infer an incorrect goal, causing its assistance (along with the human’s success) to suffer, as it does in Figure 1.
Even in scenarios where the agent correctly infers the human’s goal, we encounter further questions about the nature of collaborations between humans and assistive agents: what roles do each of them play in achieving the shared goal? One can imagine a scenario where a human is attempting to traverse down a hallway blocked by heavy objects. Here, there are a range of assistive behaviours: a robot could move the objects and create a path so the human is still the main actor, or a robot could physically carry the person down the hallway, making the human passive. Depending on the context, either solution may be more or less appropriate. Speciﬁcally, the boundary between assisting humans 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Toy scenario where a robot operates multiple doors. On the left, the robot attempts to infer the human’s intended goal, C, but mistakenly infers B. On the right, the robot assesses that the human’s empowerment would increase with doors B and C open. Naively opening all doors will not increase empowerment as A is too small for the person and D leads to the same location as C. in tackling challenging tasks and solving these tasks in the place of humans is not clearly deﬁned.
As AI improves at ‘human’ jobs, it is crucial to consider how the technology can complement and amplify human abilities, rather than replace them [43, 42].
Our key insight is that agents can assist humans without inferring their goals or limiting their autonomy by instead increasing the human’s controllability of their environment – in other words, their ability to affect the environment through actions. We capture this via empowerment, an information-theoretic quantity that is a measure of the controllability of a state through calculating the logarithm of the number of possible distinguishable future states that are reachable from the initial state [39]. In our method, Assistance via Empowerment (AvE), we formalize the learning of assistive agents as an augmentation of reinforcement learning with a measure of human empowerment. The intuition behind our method is that by prioritizing agent actions that increase the human’s empowerment, we are enabling the human to more easily reach whichever goal they want. Thus, we are assisting the human without information about their goal – the agent does not carry the human to the goal, but instead clears a path so they can get there on their own. Without any information or prior assumptions about the human’s goals or intentions, our agents can still learn to assist humans.
We test our insight across different environments by investigating whether having the agent’s be-haviour take into account human empowerment during learning will lead to agents that are able to assist humans in reaching their goal, despite having no information about what the goal truly is. Our proposed method is the ﬁrst one, to our knowledge, that successfully uses the concept behind empow-erment with real human users in a human-agent assistance scenario. Our experiments suggest that while goal inference is preferable when the goal set is correctly speciﬁed and small, empowerment can signiﬁcantly increase the human’s success rate when the goal set is large or misspeciﬁed. This does come at some penalty in terms of how quickly the human reaches their goal when successful, pointing to an interesting future work direction in hybrid methods that try to get the best of both worlds. As existing methods for computing empowerment are computationally intensive, we also propose an efﬁcient empowerment-inspired proxy metric that avoids the challenges of computing empowerment while preserving the intuition behind its usefulness in assistance. We demonstrate the success of this algorithm and our method in a user study on shared autonomy for controlling a simulated dynamical system. We ﬁnd that the strategy of stabilizing the system naturally emerges out of our method, which in turn leads to higher user success rate. While our method cannot outperform an oracle method that has knowledge of the human’s goal, we ﬁnd that increasing human empowerment provides a novel step towards generalized assistance, including in situations where the human’s goals cannot be easily inferred.
Main contributions:
• We formalize learning for human-agent assistance via the empowerment method.
• We directly compare our method against a goal inference approach and conﬁrm where our method is able to overcome potential pitfalls of inaccurate goal inference in assistance.
• We propose a computationally efﬁcient proxy for empowerment in continuous domains, enabling human-in-the-loop experiments of learned assistance in a challenging simulated teleoperation task. 2
2 Empowerment Preliminary