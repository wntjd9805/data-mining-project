Abstract
In ofﬂine reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment.
The ability to train RL policies ofﬂine would greatly expand where RL can be applied, its data efﬁciency, and its experimental velocity. Prior work in ofﬂine
RL has been conﬁned almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based ofﬂine RL.
This framework consists of two steps: (a) learning a pessimistic MDP (P-MDP) using the ofﬂine dataset; (b) learning a near-optimal policy in this P-MDP. The learned P-MDP has the property that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the P-MDP.
This enables it to serve as a good surrogate for purposes of policy evaluation and learning, and overcome common pitfalls of model-based RL like model exploitation.
Theoretically, we show that MOReL enjoys strong performance guarantees for ofﬂine
RL. Through experiments, we show that MOReL matches or exceeds state-of-the-art results in widely studied ofﬂine RL benchmarks. Moreover, the modular design of MOReL enables future advances in its components (e.g., in model learning, planning etc.) to directly translate into improvements for ofﬂine RL. Project webpage: https://sites.google.com/view/morel 1

Introduction
The ﬁelds of computer vision and NLP have seen tremendous advances by utilizing large-scale ofﬂine datasets [1, 2, 3] for training and deploying deep learning models [4, 5, 6, 7]. In contrast, reinforcement learning (RL) [8] is typically viewed as an online learning process. The RL agent iteratively collects data through interactions with the environment while learning the policy. Unfor-tunately, a direct embodiment of this trial and error learning is often inefﬁcient and feasible only with a simulator [9, 10, 11]. Similar to progress in other ﬁelds of AI, the ability to learn from ofﬂine datasets may hold the key to unlocking the sample efﬁciency and widespread use of RL agents.
Ofﬂine RL, also known as batch RL [12], involves learning a highly rewarding policy using only a static ofﬂine dataset collected by one or more data logging (behavior) policies. Since the data has already been collected, ofﬂine RL abstracts away data collection or exploration, and allows prime focus on data-driven learning of policies. This abstraction is suitable for safety sensitive applications
∗Equal Contributions. Correspond to rkidambi@cornell.edu and aravraj@cs.washington.edu. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (a) Illustration of the ofﬂine RL paradigm. (b) Illustration of our framework, MOReL, which learns a pessimistic MDP (P-MDP) from the dataset and uses it for policy search. (c) Illustration of the P-MDP, which partitions the state-action space into known (green) and unknown (orange) regions, and also forces a transition to a low reward absorbing state (HALT) from unknown regions. Blue dots denote the support in the dataset. See algorithm 1 for more details. like healthcare and industrial automation where careful oversight by a domain expert is necessary for taking exploratory actions or deploying new policies [13, 14]. Additionally, large historical datasets are readily available in domains like autonomous driving and recommendation systems, where ofﬂine
RL may be used to improve upon currently deployed policies.
Due to use of static dataset, ofﬂine RL faces unique challenges. Over the course of learning, the agent has to evaluate and reason about various candidate policy updates. This ofﬂine policy evaluation is particularly challenging due to deviation between the state visitation distribution of the candidate policy and the logging policy. Furthermore, this difﬁculty is exacerbated over the course of learning as the candidate policies increasingly deviate from the logging policy. This change in distribution, as a result of policy updates, is typically called distribution shift and constitutes a major challenge in ofﬂine RL. Recent studies show that directly using off-policy RL algorithms with an ofﬂine dataset yields poor results due to distribution shift and function approximation errors [15, 16, 17].
To overcome this, prior works have proposed modiﬁcations like Q-network ensembles [15, 18] and regularization towards the data logging policy [19, 16, 18]. Most notably, prior work in ofﬂine RL has been conﬁned almost exclusively to model-free methods [20, 15, 16, 19, 17, 18, 21].
Model-based RL (MBRL) presents an alternate set of approaches involving the learning of approxi-mate dynamics models which can subsequently be used for policy search. MBRL enables the use of generic priors like smoothness and physics [22] for model learning, and a wide variety of planning algorithms [23, 24, 25, 26, 27]. As a result, MBRL algorithms have been highly sample efﬁcient for online RL [28, 29]. However, direct use of MBRL algorithms with ofﬂine datasets can prove challenging, again due to the distribution shift issue. In particular, since the dataset may not span the entire state-action space, the learned model is unlikely to be globally accurate. As a result, planning using a learned model without any safeguards against model inaccuracy can result in “model exploitation” [30, 31, 29, 28], yielding poor results [32]. In this context, we study the pertinent question of how to effectively regularize and adapt model-based methods for ofﬂine RL.
Our Contributions: The principal contribution of our work is the development of MOReL (Model-based Ofﬂine Reinforcement Learning), a novel model-based framework for ofﬂine RL (see ﬁgure 1 for an overview). MOReL enjoys rigorous theoretical guarantees, enables transparent algorithm design, and offers state of the art (SOTA) results on widely studied ofﬂine RL benchmarks.
• MOReL consists of two modular steps: (a) learning a pessimistic MDP (P-MDP) using the ofﬂine dataset; and (b) learning a near-optimal policy for the P-MDP. For any policy, the performance in the true MDP (environment) is approximately lower bounded by the performance in the P-MDP, making it a suitable surrogate for purposes of policy evaluation and learning. This also guards against model exploitation, which often plagues MBRL.
• The P-MDP partitions the state space into “known” and “unknown” regions, and uses a large negative reward for unknown regions. This provides a regularizing effect during policy learning by heavily penalizing policies that visit unknown states. Such a regularization in the space of state visitations, afforded by a model-based approach, is particularly well suited for ofﬂine RL. In contrast, model-free algorithms [16, 18] are forced to regularize the policies directly towards the data logging policy, which can be overly conservative. 2
• Theoretically, we establish upper bounds for the sub-optimality of a policy learned with MOReL, and a worst case lower-bound for the sub-optimality of a policy learnable by any ofﬂine RL algorithm.
We ﬁnd that these bounds match upto log factors, suggesting that the performance guarantee of MOReL is nearly optimal in terms of discount factor and support mismatch between optimal and data collecting policies (see Corollary 3 and Proposition 4).
• We evaluate MOReL on standard benchmark tasks used for ofﬂine RL. MOReL obtains SOTA results in 12 out of 20 environment-dataset conﬁgurations, and performs competitively in the rest. In contrast, the best prior algorithm [18] obtains SOTA results in only 5 (out of 20) conﬁgurations. 2