Abstract
Few-shot image generation seeks to generate more data of a given domain, with only few available training examples. As it is unreasonable to expect to fully infer the distribution from just a few observations (e.g., emojis), we seek to leverage a large, related source domain as pretraining (e.g., human faces). Thus, we wish to preserve the diversity of the source domain, while adapting to the appearance of the target. We adapt a pretrained model, without introducing any additional parameters, to the few examples of the target domain. Crucially, we regularize the changes of the weights during this adaptation, in order to best preserve the “information” of the source dataset, while ﬁtting the target. We demonstrate the effectiveness of our algorithm by generating high-quality results of different target domains, including those with extremely few examples (e.g., ≤10). We also analyze the performance of our method with respect to some important factors, such as the number of examples and the dissimilarity between the source and target domain. 1

Introduction
The success of generative adversarial networks (GANs) [8] has typically been illustrated with large amounts of training data, for example, 70,000 images for just a speciﬁc domain (aligned faces) [16] or 1.3M images across different classes [34]. However, many practical use cases provide limited data.
For example, in the artistic domain, it is at best cumbersome, and at times prohibitive, to hire artists to make thousands of creations. While generative models currently struggle in this low-data regime, our goal is to generalize from a few, new examples.
A key component to this is the ability to leverage prior experience. For example, we can use our knowledge of variations in the appearance of natural faces to easily imagine variations of a speciﬁc, given cartoon face. In this work, we aim to give generative models the same ability, as shown in Figure 1. More formally, we study the problem of few-shot image generation in a continuous learning framework – training an algorithm to generate more data of a target domain, given only a few examples. An underlying assumption with this setup is that the source and target domains share some latent factors, with some differences related to their distinct difference in appearance. For example, when transferring from real natural faces to emojis, variations in pose and expression can be naturally extended to the target domain.
To achieve this goal, we propose a straightforward and effective adaptation technique. That is, we adapt the pretrained model’s weights, without introducing additional parameters. Fixing the architecture implies that tedious manual designs on new parameters (e.g., number of parameters, their position, etc.) are not necessary. Instead, the challenge is how to adapt the weights to ﬁt the appearance of the limited target domain data, while retaining as much transferred knowledge, or diversity from the source. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Pipeline of our few-shot image generation. We ﬁrst pretrain a generative model on the source domain (e.g., real faces) with a lot of data. We then adapt it to the target domain (e.g., Moïse
Kisling faces [44]) with just a few examples to generate more data in target domain (all images are of size 256×256).
A key property to note is that weights have different levels of importance; thus, each parameter should not be treated equally in the adaptation, or tuning process. We propose to quantify the “importance” of each parameter, emphasizing preservation of important parameters during the tuning process. In the discriminative modeling setting, Kirkpatrick et al. [17] propose Elastic Weight Consolidation (EWC), which evaluates the importance of each parameter by estimating its Fisher Information relative to the objective likelihood. A key difference is in the generative setting, the training objective is not ﬁxed. Nonetheless, we demonstrate that the Fisher Information can be estimated from a proxy objective (a frozen discriminator) and are able to generate high-quality results of different target domains, even with extremely few examples (≤10).
In addition, we consider there will always be an inherent trade-off between preserving information from the source and adapting to the target domain. We conduct an in-depth analysis on the perfor-mance of our method, with respect to important factors, such as the number of target examples and the dissimilarity between the source and target domain.
The main contributions of this work are summarized as follows:
• We propose to adapt a pretrained generative model to a new target domain without introducing additional parameters, producing diverse generations even with limited data.
• We demonstrate the effectiveness of the proposed method in artistic domains, where practical use cases often have limited data.
• We evaluate our method on several cross-domain source/target pairs, in contrast to previous methods which mostly focus on the photo domain. 2