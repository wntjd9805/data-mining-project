Abstract
We present REVEL, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efﬁcient veriﬁcation. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit veriﬁcation of neural networks. Our empirical results show that REVEL enforces safe exploration in many scenarios in which Constrained
Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to veriﬁed exploration. 1

Introduction
Guaranteeing that an agent behaves safely during exploration is a fundamental problem in reinforce-ment learning (RL) [13, 1]. Most approaches to the problem are based on stochastic deﬁnitions of safety [23, 7, 1, 7], requiring the agent to satisfy a safety constraint with high probability or in expectation. However, in applications such as autonomous robotics, unsafe agent actions — no matter how improbable — can lead to cascading failures with high human and ﬁnancial costs. As a result, it can be important to ensure that the agent behaves safely even on worst-case inputs.
A number of recent efforts [3, 11] use formal methods to offer such worst-case guarantees during exploration. Broadly, these methods construct a space of provably safe policies before the learning process starts. Then, during exploration, a safety monitor observes the learner, forbidding all actions that cannot result from one of these safe policies. If the learner is about to take a forbidden action, a safe policy (a safety shield) is executed instead.
So far, these methods have only been used to discover policies over simple, ﬁnite action spaces.
Using them in more complex settings — in particular, continuous action spaces — is much more challenging. A key problem is that these safety monitors are constructed a priori and are blind to the internal state of the learner. As we experimentally show later in this paper, such a “one-size-ﬁts-all” strategy can unnecessarily limit exploration and impede learner performance. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we improve this state of the art through an RL framework, called REVEL1 that allows learning over continuous state and action spaces, supports (partially) neural policy representations and contemporary policy gradient methods for learning, while also ensuring that every intermediate policy that the learner constructs during exploration is safe on worst-case inputs. Like previous efforts, REVEL uses monitoring and shielding. However, unlike in prior work, the monitor and the shield are updated as learning progresses.
A key feature of our approach is that we repeatedly invoke a formal veriﬁer from within the learning loop. Doing this is challenging because of the high computational cost of verifying neural networks.
We overcome this challenge using a neurosymbolic policy representation in which the shield and the monitor are expressed in an easily-veriﬁable symbolic form, whereas the normal-mode policy is given by a neural network. Overall, this representation admits efﬁcient gradient-based learning as well as efﬁcient updates to both the shield and monitor.
To learn such neurosymbolic policies, we build on PROPEL [29], a recent RL framework in which policies are represented in compact symbolic forms (albeit without consideration of safety), and design a learning algorithm that performs a functional mirror descent in the space of neurosymbolic policies. The algorithm views the set of shields as being obtained by imposing a constraint on the general policy space. Starting with a safe but suboptimal shield, it alternates between: (i) safely lifting the current shield into the unconstrained policy space by adding a neural component; (ii) safely updating this neurosymbolic policy using approximate gradients; and (iii) using a form of imitation learning to project the updated policy back into the constrained space of shields. Importantly, none of these steps requires direct veriﬁcation of neural networks.
Our empirical evaluation, on a suite of continuous control problems, shows that REVEL enforces safe exploration in many scenarios where established RL algorithms (including CPO [1], which is motivated by safe RL) do not, while discovering policies that outperform policies based on static shields. Also, building on results for PROPEL, we develop a theoretical analysis of REVEL.
In summary, the contributions of the paper are threefold. First, we introduce the ﬁrst RL approach to use deep policy representations and policy gradient methods while guaranteeing formally veriﬁed exploration. Second, we propose a new solution to this problem that combines ideas from RL and formal methods, and we show that our method has convergence guarantees. Third, we present promising experimental results for our method in the continuous control setting. 2 Preliminaries
A
S × A → is a set of agent actions; P (s�
Safe Exploration. We formulate our problem in terms of a Markov Decision Process (MDP) that has the standard probabilistic dynamics, as well as a worst-case dynamics that is used for veriﬁcation. Formally, such an MDP is a structure M = (
, is a set
S s, a) is a probabilistic transition function; of environment states;
|
R is a state-action cost function; 0 < γ < 1 is a discount factor; p0(s) is an initial state c : 2S is a deterministic function that deﬁnes worst-case distribution with support
SU is a designated set of unsafe states that the learner must bounds on the environment behavior; and always avoid. Because our focus is on continuous domains, we assume that are real vector spaces. The function P # is assumed to be available in closed form to the learner; because it captures
P #(s, a) for all s, a. In general, the worst-case dynamics, we require that supp(P (s� method for obtaining P # will depend on the problem under consideration. In Section 4 we explain how we generate these worst case bounds for our experimental environments.
, P, c, γ, p0, P #,
SU ). Here,
S0; P # :
S × A → s, a)) and
A
A
⊆
S
S
|
A policy for M is a (stochastic) map π : that determines which action the agent should take in a given state. Each policy π induces a probability distribution on the cost ci at each time step i.
The aggregate cost of a policy π is J(π) = E[ i γici], where ci is the cost at the i-th time step.
S → A
, we deﬁne the set of states reachi(π, S) that are reachable from S in i steps under
�
For a set S worst-case dynamics:
⊆ S reach1(π, S) = s
The policy π is safe if (
�
S,a
∈ supp(π(
∈ i reachi(π,
·| s)) P #(s, a)
S0)) reachi+1(π, S) = reach1(π, reachi(π, S)).
∩ SU = ∅. If π is safe, we write Safe(π). 1REVEL stands for Reinforcement learning with veriﬁed exploration. The current implementation is available
� at https://github.com/gavlegoat/safe-learning. 2
∈ G
& Neural Policy Class
G
, with the guarantee φ0
Algorithm 1 Reinforcement Learning with Formally Veriﬁed Exploration (REVEL) 1: Input: Symbolic Policy Class 2: Input: Initial g0 3: Deﬁne neurosymbolic policy class 4: for t = 1, . . . , T do 5: 6:
F 7:
← 8: end for 9: Return: Policy hT
.
F
Safe(g0) for some φ0 if P #(s, f (s))
//lifting the new symbolic policy and proof into the blended space
//policy gradient in neural policy space with learning rate η ht ht (gt+1, φt+1) (ht, η)
PROJECTΠ(ht)
φ then f (s) else g(s)
LIFT
UPDATE (gt, φt)
� h(s)
←
←
H
≡
⊆
=
}
{
H
//synthesis of safe symbolic policy and corresponding invariant
= π0, π1, . . . , πm. We assume that the
We deﬁne a learning process as a sequence of policies initial policy π0 in this sequence is worst-case safe. Our algorithmic objective is to discover a learning process such that the ﬁnal policy πm is safe and optimal, and every intermediate policy is safe:
L
L
πm = arg min
J(π)
π s.t. Safe(π) 0
∀
≤ i
≤ m : Safe(πi). (1) (2)
�
Formal Veriﬁcation. Our learning algorithm relies on an oracle for formal veriﬁcation of policies.
Given a policy π, such a veriﬁer tries to construct an inductive proof of the property Safe(π). Such a proof takes the form of an inductive invariant, deﬁned as a set of states φ such that: (i) φ includes the
φ; initial states, i.e., and (iii) φ does not overlap with the unsafe states, i.e., φ
. Intuitively, states s in φ are such
∅ that even under worst-case dynamics, MDP trajectories from s can never encounter an unsafe state.
We use the notation φ
φ; (ii) φ is closed under the worst-case transition relation, i.e., reach1(φ)
∩ SU =
π to indicate that policy π can be proven safe using inductive invariant φ.
S0 ⊆
⊆
Inductive invariants can be constructed in many ways. Our implementation uses abstract interpre-tation [9], which maintains some abstract state that approximates the concrete states which the system can reach. For example, the abstract state might be a hyperinterval in the state space of the program that deﬁnes independent bounds on each state variable. Critically, this abstract state is an overapproximation, meaning that while the abstract state may include states which are not actually reachable, it will always include at least every reachable state. By starting with an abstraction of the initial states and using abstract interpretation to propagate this abstract state through the environment transitions and the policy, we can obtain an abstract state which includes all of the reachable states of the system (that is, we compute approximations of reachi(
S0) for increasing i). Then if this abstract state does not include any unsafe states, we can be sure that none of the unsafe states are reachable by any concrete trajectory of the system either. 3 Learning Algorithm
Our learning method is a functional mirror descent in policy space, based on approximate gradients, similar to PROPEL [29]. The algorithm relies on two policy classes
, with and
.
G
H
G ⊆ H
G
The class comprises the policies that we use as shields. These policies are safe and can be efﬁciently certiﬁed as such. Because automatic veriﬁcation works better on functions that belong to certain restricted classes and are represented in compact, symbolic forms, we assume some syntactic restrictions on our shields. The speciﬁc restrictions depend on the power of the veriﬁcation oracle; we describe the choices made in our implementation in Section 3.1.
The larger class
We assume that each shield in are of the form:
H
G consists of neurosymbolic policies. Let be a predeﬁned class of neural policies. can also be expressed as a policy in
F
, i.e.,
F
G ⊆ F
. Policies h
∈ H h(s) = if (P #(s, f (s))
φ) then f (s) else g(s)
⊆ where g a policy h as above by the notation (g, φ, f ).
∈ F
∈ G
, f
, and φ is an inductive invariant that establishes Safe(g). We commonly denote
The “true” branch in the deﬁnition of h represents the normal mode of the policy. The condition
P #(s, f (s))
φ is the safety monitor. If this condition holds, then the action f (s) is safe, as it can
⊆ 3
G g
←
Algorithm 2 Implementation of PROJECT 1: Input: A neurosymbolic policy h = (g, φ, f ) where g = [(g1, χ1), . . . , (gn, χn)] 2: g∗ 3: for t = 1, . . . , T do 4: 5: 6: 7: g∗ 8: 9: end if 10: end for 11: φ∗
← 12: return (g∗, φ∗)
ψ
CUTTINGPLANE(χi) for heuristically selected i
← g1
IMITATESAFELY(f, gi, χi i ←
SPLIT(g, i, (g1 g�
← if D(g�, h) < D(g∗, h) then
IMITATESAFELY(f, gi, χi
ψ);
∧
ψ), (g2 i , χi g2 i ←
ψ))
SAFESPACE(g∗) i , χi
∧ ¬
← g�
∧
ψ)
∧ ¬ only lead to states in φ (which does not overlap with the unsafe states). If the condition does not hold, then f can violate safety, and the shield g is executed in its place. In either case, h is safe.
As for updates to h, we do not assume that the policy gradient exists, and approximate it by the gradient
J(h) in the space of neural policies.
J(h) in the space
∇H
H
∇F
F
. This step takes as input a shield g and a corresponding invariant φ0, then iteratively performs the following steps.
We sketch our learning procedure in Algorithm 1. The algorithm starts with a (manually constructed) shield g0 ∈ G and its accompanying invariant φ, and constructs the
LIFT
H
. Note that the neural component of this policy is just the input shield g (in a policy (g, φ, g) neural representation). In practice, to construct this component, we can train a randomly initialized neural network to imitate g, using an algorithm such as DAGGER [26]. Because the safety of any policy (g, φ, f �) only depends on g and φ, this step is safe.
∈ H
∈ G
F
. This procedure performs a series of gradient updates to a neurosymbolic policy h =
UPDATE (g, φ, f ). As mentioned earlier, this step uses the approximate policy gradient
J(h). This means that after an update, the new policy is (g, φ, f
J(h)), for a suitable learning rate η. As the
η update does not change g and φ, the new policy is provably safe. Also, we show later that, under certain assumptions, the regret introduced by our approximation of the gradient is bounded.
∇F
∇F
−
G
PROJECT neurosymbolic policy h = (g, φ, f ), the procedure computes a policy g� arg ming��∈G invariant φ� such that φ�
. This procedure implements the projection operation of mirror descent. Given a that satisﬁes g� =
D(g��, (g, φ, f )) for some Bregman divergence D. Along with g�, we compute an
Safe(g�).
∈ G
The computation of g� can be naturally cast as an imitation learning task with respect to the demon-stration oracle (g, φ, f ). Prior work [29, 30] has given heuristic solutions to this problem for the case when g�� obeys certain syntactic constraints. In our setting, we have an additional semantic requirement: g�� must be provably safe. How to solve this problem depends on the precise deﬁnition of
. Section 3.1 sketches the approach to this problem used in our implementation. the class of shields
�
G 3.1 Instantiation with Piecewise Linear Shields
Any attempt to implement REVEL must start by choosing a class should
G be sufﬁciently expressive to allow for good learning performance but also facilitate veriﬁcation. In to comprise deterministic, piecewise linear policies of the form: our implementation, we choose of shields. Policies in
G
G g1(s) g2(s)
. . . gn(s) if χ1(s) if χ2(s) if χn(s)
∧ ¬ (
∧
χ1(s)
χi(s)), 1 i<n ¬
≤ g(s) = 

 where χ1, . . . , χn are linear predicates that partition the state space, and each gi is a linear function.
We represent g(s) as a list of pairs (gi, χi). We refer to the subpart of the state space deﬁned by
χj as the region for linear policy gi and denote this region by Region(gi).
χi ∧ i 1
− j=1 ¬
�
� 4
Now we sketch our implementation of Algorithm 1. Since the LIFT are agnostic to the choice of imitation distance D(g, h) from a given h
, we focus on PROJECT
G
G
. procedures
, which seeks to ﬁnd a shield g at minimum and UPDATE
H
F
∈ H i , h) within the region χj
Our implementation of this operation is the iterative procedure in Algorithm 2. Here, we start with an input policy h = (g, φ, f ). In each iteration, we identify a component gi with region χi, then perform the following steps: (i) Sample a cutting plane that creates a more ﬁne-grained partitioning of the safe i . (ii) For each new region χj region, by splitting the region χi into two new regions χ1 i and χ2 i , use a subroutine IMITATESAFELY to construct a safe linear policy gj i (and a corresponding invariant) that minimizes D(gj i . (iii) Replace (gi, χi) by the two new components, leading to the creation of a new, reﬁned shield g�. The procedure ends by returning the most optimal shield g� (and an invariant obtained by combining the invariants of the gj i -s) constructed through this process.
Now we sketch IMITATESAFELY, which constructs safe and imitation-loss-minimizing linear policies.
By collecting state-action pairs using DAGGER [26], we reduce the procedure’s objective to a series of constrained supervised learning problems. Each of these problems is solved using a projected gradient descent (PGD) that alternates between gradient updates to a linear policy and projections into the set of safe linear policies. Critically, the constraint imposed on each of these optimization problems is constructed such that (i) the resulting policy is provably safe and (ii) the projection for the PGD algorithm is easy to compute. In our implementation these constraints take the form of a hyperinterval in the parameter space of the linear policies. We can then use abstract interpretation [9], a common framework for program veriﬁcation, to prove that every controller within a particular hyperinterval behaves safely. For more details on IMITATESAFELY, see the supplementary material. 3.2 Theoretical Analysis
The REVEL approach introduces two new sources of error over standard mirror descent. First, we approximate the gradient
, which introduces bias. Second, our projection step may be inexact. Prior work [29] has studied methods for implementing the projection step with bounded error. Here, we bound the bias in the gradient approximation under some simplifying assumptions, and use this result to prove a regret bound on the ﬁnal shield that our method converges on. We deﬁne a safety indicator Z which is zero whenever the shield is invoked and one otherwise. We assume:
∇H
∇F by
H
, and is a vector space equipped with an inner product 1. 2. J is convex in 3.
H 4. E[1 5. the bias introduced in the sampling process is bounded by β, i.e.,
·�
J is LJ -Lipschitz continuous on h
<
ζ, i.e., the probability that the shield is invoked is bounded above by ζ, is bounded (i.e., sup and induced norm
∇
{�
∈ H} h, h�
� |
Z]
∞ h�
H
H
≤
−
−
�·
),
,
,
�
= h
�
� h, h
,
�
�
E[
�
∇F | h]
− ∇F
J(h)
� ≤
β, where 6. for s is the estimated gradient
, and policy h
, a
∇F
∈ S
�
∈ A
, if h(a
| s) > 0 then h(a
| s) > δ for some ﬁxed δ > 0.
�
∈ H
Intuitively, this last assumption amounts to cutting of the tails of the distribution so that no action can be arbitrarily unlikely. Now, let the variance of the gradient estimates be bounded by σ2, and
� where g∗t is the exact projection of a neurosymbolic policy assume the projection error and gt is the computed projection. Let R be an α-strongly convex and LR-strongly smooth onto regularizer. Then the bias of our gradient estimate is bounded by Lemma 1 and the expected regret bound is given by Theorem 1.
Lemma 1. Let γ be the diameter of by approximating h, h�
H
{�
J(h) and sampling is bounded by
. Then the bias incurred
, i.e., γ = sup g∗t � ≤
J(h) with gt −
∈ H}
� | h�
−
G h
�
∇H
∇F
∇t | h
E
− ∇H
J(h)
= O(β + Ljζ).
�
Theorem 1. Let g1, . . . , gT be a sequence of shields in
�
� optimal programmatic policy. Choosing a learning rate η = regret over T iterations:
�
�
�
G
�
�
� returned by REVEL and let g∗ be the we have the expected 1
T + � 1
σ2
�
�
� 1
T
E
�
T i=1
�
J(gi)
� −
J(g∗) = O 1
T
σ
�
�
+ � + β + LJ ζ
� 5
This theorem matches the expectation that when a blended policy h = (g, φ, f ) is allowed to take more actions without the shield intervening (i.e., ζ decreases), the regret bound is decreased.
Intuitively, this is because when we use the shield, the action we take does not depend on the neural network f , so the learner does not learn anything useful. However if h is using f to choose actions, then we have unbiased gradient information as in standard RL. 4 Experiments
Now we present our empirical evaluation of REVEL. We investigate two questions: (1) How much safer are REVEL policies compared to state-of-the-art RL techniques that lack worst-case safety guarantees? What is the performance penalty for this increased safety? (2) Does REVEL offer signiﬁcant performance gains over prior veriﬁed exploration approaches based on static shields[11, 3]?
To answer these questions, we compared REVEL against three baselines: (1) Deep Deterministic
Policy Gradients (DDPG) [22]; (2) Constrained policy optimization (CPO) [1]; and (3) a variant of
REVEL that never updates the user-provided shield. Of these, CPO is designed for safe exploration and takes into account a safety cost function. For DDPG, we engineered a reward function that has a penalty for safety violations. Details of hyperparameters that we used appear in the Appendix.
Our experiments used 10 benchmarks that include classic control problems, robotics applications, and benchmarks from prior work [11]. For each of these environments, we hand-constructed a worst-case, piecewise linear model of the dynamics. These models are based on the physics of the environment and use non-determinism to approximate nonlinear functions. For example, some of our benchmarks include trigonometric functions which cannot be represented linearly. In these cases, we deﬁne piecewise linear upper and lower bounds to the trigonometric functions. These linear approximations are necessary to make veriﬁcation feasible. Each benchmark also includes a bounded-time safety property which should hold at all times during training.
Performance. First, we compare the policies learned using REVEL against policies learned using the baselines in terms of their cost (lower is better). Figures 1 and 2 show the cost over time of the policies during training. The results suggest that:
•
•
•
The performance of REVEL is competitive with (or even better than) DDPG for 7 out of the 10 benchmarks. REVEL achieves signiﬁcantly better reward than DDPG in the “car-racing” benchmark, and reward is only slightly worse for 2 benchmarks.
REVEL has better performance than CPO on 4 out of the 10 benchmarks and only performs slightly worse on 2. Furthermore, the cost incurred by CPO is signiﬁcantly worse on 2 benchmarks (noisy-road and car-racing).
REVEL outperforms the static shielding approach on 4 out of 10 benchmarks. Furthermore, the difference is very substantial on two of these benchmarks (noisy-road and mountain-car).
REVEL does induce substantial overhead in terms of computational cost. The cost of the network updates and shield updates for each benchmark are shown in Table 1 along with the percentage of the total time spent in shield synthesis. The “acc” and “pendulum” benchmarks stand out as having very fast shield updates. For these two benchmarks the safety properties are relatively simple, so the veriﬁcation engine is able to come up with safe shields more quickly. Otherwise, REVEL spends the majority of its time (87% on average) on shield synthesis.
Safety. To validate whether the safety guarantee provided by REVEL is useful, we consider how
DDPG and CPO behave during training. Speciﬁcally, Table 2 shows the average number of safety violations per run for DDPG and CPO. As we can see from this table, DDPG and CPO both exhibit safety violations in 8 out of the 10 benchmarks. In Figure 3, we show how the number of violations varies throughout the training process for a few of the benchmarks. The remaining plots are left to the supplementary material.
Qualitative Assessment. To gain intuition about the difference between policies that REVEL and the baselines compute, we consider trajectories from the trained policies for two of our benchmarks that are easy to visualize. Figure 4 shows the trajectories taken by each of the policies for the obstacle2 benchmark. In this environment, the policy starts in the lower left corner, and the goal is to move to the green circle in the upper right. However, the red box in the middle is unsafe. As we can see from Figure 4, all of the policies have learned to go around the unsafe region in the center. However 6
(a) mountain-car (b) road (c) road-2d (d) noisy-road (e) noisy-road-2d (f) obstacle (g) obstacle2 (h) pendulum
Figure 1: Training performance comparison on our benchmarks. The y-axis represents the Cost J(π) and the x-axis gives the number of training episodes.
DDPG has not reinforced this behavior enough and still enters the unsafe region at the corner. By contrast, the statically shielded policy manages to avoid the region, but there is a very clear bend in its trajectory where the shield has to step in. Revel avoids the unsafe region while maintaining a smooth trajectory throughout. In this case, CPO also learns to avoid the unsafe region and go to the goal. (Because the environment is symmetrical, there is no signiﬁcance to the CPO curve going up
ﬁrst and then right.) 7
(a) acc (b) car-racing
Figure 2: Training performance comparison (continued). The y-axis represents the Cost J(π) and the x-axis gives the number of training episodes.
Table 1: Training time in seconds for network and shield updates.
Benchmark mountain-car road road-2d noisy-road noisy-road-2d obstacle obstacle2 pendulum acc car-racing
Network update (s) 1900 954 1015 962 935 4332 4365 1292 1097 4361
Shield update (s) 5315 9401 19492 12793 25514 27818 21661 113 56 15892
Shield percentage 73.7% 90.8% 95.1% 93.0% 96.5% 86.5% 83.2% 8.0% 4.9% 78.5%
Table 2: Safety violations.
Benchmark mountain-car road road-2d noisy-road noisy-road-2d obstacle obstacle2 pendulum acc car-racing
DDPG 0 0 113.4 1130.4 107.4 12.4 96 92.4 4 4956.2
CPO 3.6 0 70.8 8526.4 0 1.0 118.6 9906 673 22.4
Figure 5 shows trajectories for “acc”, which models an adaptive cruise control system where the goal is to follow a lead car as closely as possible without crashing into it. The lead car can apply an acceleration to itself at any time. The x-axis shows the distance to the lead car while the y-axis shows the relative velocities of the two cars. Here, all three trajectories start by accelerating to close the gap to the lead car before slowing down again. The statically shielded (and most conservative) policy is the ﬁrst to slow down. The DDPG and CPO policies fail to slow down soon enough or quickly enough and crash into the lead car (the red region on the right side of the ﬁgure). In contrast, the REVEL policy can more quickly close the gap to the lead car and slow down later while still avoiding a crash. (a) pendulum (b) acc (c) car-racing
Figure 3: Cumulative safety violations during training. 8
Figure 4: Trajectories for obstacle2.
Figure 5: Trajectories for acc. 5