Abstract
The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence.
Traditional tuning algorithms only consider the ﬁnal performance of hyperparam-eters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efﬁcient hyperparameter tuning. We propose to learn an evaluation function compress-ing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the beneﬁt of assessing a hyperparameter setting over additional train-ing steps against their computation cost. We further increase model efﬁciency by selectively including scores from different training steps for any evaluated hyper-parameter set. We demonstrate the efﬁciency of our algorithm by tuning hyperpa-rameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time. 1

Introduction
Deep learning (DL) and deep reinforcement learning (DRL) have led to impressive breakthroughs in a broad range of applications such as game play [26, 36], motor control [43], and image recognition
[20]. To maintain general applicability, these algorithms expose sets of hyperparameters to adapt their behavior to any particular task at hand. This ﬂexibility comes at the price of having to tune an additional set of parameters – poor settings lead to drastic performance losses [11, 30, 37]. On top of being notoriously sensitive to these choices, deep (reinforcement) learning systems often have high training costs, in computational resources and time. For example, a single training run on the
Atari Breakout game took approximately 75 hours on a GPU cluster [26]. Tuning DRL parameters is further complicated as only noisy evaluations of an agent’s ﬁnal performance are obtainable.
Bayesian optimization (BO) [12, 28, 35] has recently achieved considerable success in optimizing these hyperparameters. This approach casts the tuning process as a global optimization problem based on noisy evaluations of a black-box function f . BO constructs a surrogate model typically using a Gaussian process (GP) [31], over this unknown function. This GP surrogate is used to build an acquisition function [13, 44] which suggests the next hyperparameter to evaluate.
In modern machine learning (ML) algorithms [15], the training process is often conducted in an iterative manner. A natural example is given by deep learning where training is usually based on stochastic gradient descent and other iterative procedures. Similarly, the training of reinforcement learning agents is mostly carried out using multiple episodes. The knowledge accumulated during these training iterations can be useful to inform BO. However, most existing BO approaches [35]
∗These authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
deﬁne the objective function as the average performance over the ﬁnal training iterations. In doing so, they ignore the information contained in the preceding training steps.
In this paper, we present a Bayesian optimization approach for tuning algorithms where iterative learning is available – the cases of deep learning and deep reinforcement learning. First, we consider the joint space of input hyperparameters and number of training iterations to capture the learning progress at different time steps in the training process. We then propose to transform the whole training curve into a numeric score according to user preference. To learn across the joint space efﬁciently, we introduce a data augmentation technique leveraging intermediate information from the iterative process. By exploiting the iterative structure of training procedures, we encourage our algorithm to consider running a larger number of cheap (but high-utility) experiments, when cost-ignorant algorithms would only be able to run a few expensive ones. We demonstrate the efﬁciency of our algorithm on training DRL agents on several well-known benchmarks as well as the training of convolutional neural networks.
In particular, our algorithm outperforms existing baselines in
ﬁnding the best hyperparameter in terms of wall-clock time. Our main contributions are:
• an algorithm to optimize the learning curve of a ML algorithm by using training curve compression, instead of averaged ﬁnal performance;
• an approach to learn the compression curve from the data and a data augmentation tech-nique for increased sample-efﬁciency;
• demonstration on tuning DRL and convolutional neural networks. 2