Abstract
Magnitude pruning is a widely used strategy for reducing model size in pure supervised learning; however, it is less effective in the transfer learning regime that has become standard for state-of-the-art natural language processing applications.
We propose the use of movement pruning, a simple, deterministic ﬁrst-order weight pruning method that is more adaptive to pretrained model ﬁne-tuning. We give mathematical foundations to the method and compare it to existing zeroth- and
ﬁrst-order pruning methods. Experiments show that when pruning large pretrained language models, movement pruning shows signiﬁcant improvements in high-sparsity regimes. When combined with distillation, the approach achieves minimal accuracy loss with down to only 3% of the model parameters. 1

Introduction
Large-scale transfer learning has become ubiquitous in deep learning and achieves state-of-the-art performance in applications in natural language processing and related ﬁelds. In this setup, a large model pretrained on a massive generic dataset is then ﬁne-tuned on a smaller annotated dataset to perform a speciﬁc end-task. Model accuracy has been shown to scale with the pretrained model and dataset size [Raffel et al., 2019]. However, signiﬁcant resources are required to ship and deploy these large models, and training the models have high environmental costs [Strubell et al., 2019].
Sparsity induction is a widely used approach to reduce the memory footprint of neural networks at only a small cost of accuracy. Pruning methods, which remove weights based on their importance, are a particularly simple and effective method for compressing models. Smaller models are easier to sent on edge devices such as mobile phones but are also signiﬁcantly less energy greedy: the majority of the energy consumption comes from fetching the model parameters from the long term storage of the mobile device to its volatile memory [Han et al., 2016a, Horowitz, 2014].
Magnitude pruning [Han et al., 2015, 2016b], which preserves weights with high absolute values, is the most widely used method for weight pruning. It has been applied to a large variety of architectures in computer vision [Guo et al., 2016], in language processing [Gale et al., 2019], and more recently has been leveraged as a core component in the lottery ticket hypothesis [Frankle et al., 2020].
While magnitude pruning is highly effective for standard supervised learning, it is inherently less useful in the transfer learning regime. In supervised learning, weight values are primarily determined by the end-task training data. In transfer learning, weight values are mostly predetermined by the original model and are only ﬁne-tuned on the end task. This prevents these methods from learning to prune based on the ﬁne-tuning step, or “ﬁne-pruning.”
In this work, we argue that to effectively reduce the size of models for transfer learning, one should instead use movement pruning, i.e., pruning approaches that consider the changes in weights during
ﬁne-tuning. Movement pruning differs from magnitude pruning in that both weights with low and high values can be pruned if they shrink during training. This strategy moves the selection criteria 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
from the 0th to the 1st-order and facilitates greater pruning based on the ﬁne-tuning objective. To test this approach, we introduce a particularly simple, deterministic version of movement pruning utilizing the straight-through estimator [Bengio et al., 2013].
We apply movement pruning to pretrained language representations (BERT) [Devlin et al., 2019,
Vaswani et al., 2017] on a diverse set of ﬁne-tuning tasks. In highly sparse regimes (less than 15% of remaining weights), we observe signiﬁcant improvements over magnitude pruning and other 1st-order methods such as L0 regularization [Louizos et al., 2017]. Our models reach 95% of the original
BERT performance with only 5% of the encoder’s weight on natural language inference (MNLI)
[Williams et al., 2018] and question answering (SQuAD v1.1) [Rajpurkar et al., 2016]. Analysis of the differences between magnitude pruning and movement pruning shows that the two methods lead to radically different pruned models with movement pruning showing greater ability to adapt to the end-task. 2