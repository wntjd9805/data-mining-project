Abstract
Robustness to adversarial attacks is an important concern due to the fragility of deep neural networks to small perturbations and has received an abundance of attention in recent years. Distributional Robust Optimization (DRO), a particularly promising way of addressing this challenge, studies robustness via divergence-based uncertainty sets and has provided valuable insights into robustiﬁcation strategies such as regularisation. In the context of machine learning, majority of existing results have chosen f -divergences, Wasserstein distances and more recently, the Maximum Mean Discrepancy (MMD) to construct uncertainty sets.
We extend this line of work for the purposes of understanding robustness via regularization by studying uncertainty sets constructed with Integral Probability
Metrics (IPMs) - a large family of divergences including the MMD, Total Variation and Wasserstein distances. Our main result shows that DRO under any choice of
IPM corresponds to a family of regularization penalties, which recover and improve upon existing results in the setting of MMD and Wasserstein distances. Due to the generality of our result, we show that other choices of IPMs correspond to other commonly used penalties in machine learning. Furthermore, we extend our results to shed light on adversarial generative modelling via f -GANs, constituting the ﬁrst study of distributional robustness for the f -GAN objective. Our results unveil the inductive properties of the discriminator set with regards to robustness, allowing us to give positive comments for a number of existing penalty-based
GAN methods such as Wasserstein-, MMD- and Sobolev-GANs. In summary, our results intimately link GANs to distributional robustness, extend previous results on DRO and contribute to our understanding of the link between regularization and robustness at large. 1

Introduction
Robustness to adversarial attacks is an important concern due to the fragility of deep neural net-works to small perturbations and has received an abundance of attention in recent years [21, 50, 31].
Distributionally Robust Optimization (DRO), a particularly promising way of addressing this chal-lenge, studies robustness via divergence-based uncertainty sets and considers robustness against shifts in distributions. To see this more clearly, for some space Ω, model h : Ω → R and training data ˆP with empirical loss E x∼ ˆP [lf ], DRO when applied to machine learning studies the objec-tive supQ∈U for a given divergence d and ε > 0 that characterize the adversary. Work along this line has shown that this objective is upper bounded by the empirical loss E x∼ ˆP [lf ] plus a penalty term that plays the role of a regularizer, consequently providing formal connections and valuable insights into regularization as a robustiﬁcation strategy
[22, 27, 36, 5, 14, 11].
Ex∼Q[lf ] where U =
Q : d(Q, ˆP ) ≤ ε (cid:110) (cid:111) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The choice of d is crucial as it highlights the strength and nature of robustness we desire, and different choices yield differing penalties. It has been shown that minimizing the distributionally robust objective when d is chosen to be an f -divergence is roughly equivalent to variance regularization
[22, 27, 36]. However, there is a problem with this choice of d, as highlighted in [48]: every distribution in the uncertainty set is required to be absolutely continuous with respect to P . This is particularly problematic in the case when P is empirical since every distribution in U will be ﬁnitely supported, meaning that the population distribution will not be contained as it is typically continuous.
Choosing the Wasserstein distance as d is a typical antidote for this problem, and much work has been invested in this direction, explicating connections to Lipschitz regularization [20, 10, 44, 42, 11].
More recently, uncertainty sets based on the kernel Maximum Mean Discrepancy (MMD) were investigated to address concerns with the f -divergence and discovered links to regularization with
Hilbert space norms. Both the Wasserstein distance and MMD are part of a larger family of divergences referred to as Integral Probability Metrics (IPM) [35], which are characterized by a set of functions F, and include other metrics such as the Total Variation distance and the Dudley Metric
[47].
In this work, we generalize these results and study DRO for uncertainty sets induced by the Integral
Probability Metric (IPM) for any set of functions F. We present an identity which links distributional robustness under these uncertainty sets UF , to regularization under a new penalty ΛF . Our identity takes the form (cid:90)
Ω sup
Q∈UF hdQ = (cid:90)
Ω hdP + ΛF (h) (1)
The appeal of this result is that it reduces the inﬁnite-dimensional optimization on the left-hand side into a penalty-based regularization problem on the right-hand side. We study properties of this penalty and show that it can be upper bounded by another term, ΘF , which recovers and improves upon existing penalties when F is chosen to coincide with the MMD and Wasserstein distances. Our result, however, holds in much more generality, allowing us to derive new penalties by considering other
IPMs such as the Total Variation, Fisher IPM [33], and Sobelov IPM [32]. We ﬁnd that these new penalties are related to existing penalties in regularized critic losses [51] and manifold regularization
[4], permitting us to provide untried robustness perspectives for existing regularization schemes.
Furthermore, most work in this direction takes the form of upper bounds, and although working with ΘF reduces (1) into an inequality, we present a necessary and sufﬁcient condition such that
ΛF coincides with ΘF , yielding equality. This condition reveals an intimate connection between distributional robustness and regularized binary classiﬁcation.
We then apply our result to understanding the distributional robustness of Generative Adversarial
Networks (GANs), a popular method for modelling distributions that learn a model Q by utilizing a set of discriminators D that try to distinguish Q from P (the training data). This is particularly relevant for the robustness community since lines of work [53, 9, 58, 57, 28, 26, 39, 45, 46, 24, 55, 40] implement GANs as a robustifying mechanism by training a binary classiﬁer on the learned GAN distribution. Our analysis applies to the f -GAN objective [37] - a loss that subsumes many existing
GAN losses. This is, to the best of our knowledge, the ﬁrst analysis of robustness for f -GANs with respect to divergence-based uncertainty sets. The main insight of our result is the advocation of regularized discriminators when training GANs. In particular, we show that the generative distribution learned using regularized discriminators gives guarantees on the worst-case perturbed distribution (robustness). Our ﬁndings complement existing empirical beneﬁts of regularized discriminators such as the MMD-GAN [29, 2, 6], Wasserstein-GAN [3, 23], Sobelov-GAN [32], Fisher-GAN [33] and other penalty-based GANs [51].
Our contributions come in three Theorems, where the ﬁrst two concern DRO with IPMs (Section 3) and the third is an extension to understanding GANs (Section 4): (cid:46) (Theorem 1) An identity for distributional robustness using uncertainty sets induced by any IPM.
Our result tells us that this is exactly equal to regularization with a penalty ΛF . We show that this penalty can be upper bounded by another penalty ΘF which recovers existing work when the IPM is set to the MMD and Wasserstein distance, tightening these results. Since our result holds in much more generality, we derive penalties for other IPMs such as the Total Variation, Fisher IPM, and
Sobelov IPM, and draw connections to existing methods. (cid:46) (Theorem 2) A necessary and sufﬁcient condition under which the penalties ΛF and ΘF coincide.
It turns out this condition is linked to regularized binary classiﬁcation and is related to critic losses 2
appearing in penalty-based GANs. This allows us to give positive results for work in this direction, along with drawing a link between regularized binary classiﬁcation and distributional robustness. (cid:46) (Theorem 3) A result that characterizes the distributional robustness of the f -GAN objective showing that the discriminator set plays an important part for the robustness of a GAN. This is, to the best of our knowledge, the ﬁrst result on divergence-based distributional robustness of f -GANs. Our result allows us to provide a novel perspective for several existing penalty-based GAN methods such as Wasserstein-, MMD-, and Sobelov-GANs. 2 Preliminaries 2.1 Notation
We will use Ω to denote a compact Polish space and denote Σ as the standard Borel σ-algebra on Ω and R will denote the real numbers. We use F (Ω, R) to denote the set of all bounded and measurable functions mapping from Ω into R with respect to Σ, B(Ω) to be the set of ﬁnite signed measures and the set P(Ω) ⊂ B(Ω) will denote the set of probability measures. For any additive monoid X, a function f : X → R is subadditive if f (x + x(cid:48)) ≤ f (x) + f (x(cid:48)) and the inﬁmal convolution between two functions f : X → R and g : X → R is another function given by (f (cid:63) g)(x) = inf x(cid:48)∈X (f (x(cid:48)) + g(x − x(cid:48))). For any proposition I , the inversion bracket is
= 1 if I is true and 0 otherwise. We say a set of functions F is even if h ∈ F implies −h ∈ F.
For a function h ∈ F (Ω, R) and metric c : Ω × Ω → R, the Lipschitz constant of h (w.r.t c) is Lipc(h) = supω,ω(cid:48)∈Ω |h(ω) − h(ω(cid:48))| /c(ω, ω(cid:48)) and (cid:107)h(cid:107)∞ := supω∈Ω |h(ω)|. For any set of functions F ⊆ F (Ω, R), we use co (F) to denote the closed convex hull of F. For a function h ∈ F (Ω, R) and measure µ ∈ P(Ω), we use Varµ(h) = Eµ[h2] − Eµ[h]2 to denote the variance of h under µ.
I (cid:75) (cid:74) 2.2