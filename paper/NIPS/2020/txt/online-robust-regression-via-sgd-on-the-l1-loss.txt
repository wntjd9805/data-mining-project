Abstract
We consider the robust linear regression problem in the online setting where we have access to the data in a streaming manner, one data point after the other. More speciﬁcally, for a true parameter θ∗, we consider the corrupted Gaussian linear model y = (cid:104)x, θ∗(cid:105) + ε + b where the adversarial noise b can take any value with probability η and equals zero otherwise. We consider this adversary to be oblivious (i.e., b independent of the data) since this is the only contamination model under which consistency is possible. Current algorithms rely on having the whole data at hand in order to identify and remove the outliers. In contrast, we show in this work that stochastic gradient descent on the (cid:96)1 loss converges to the true parameter vector at a ˜O(1/(1 − η)2n) rate which is independent of the values of the contaminated measurements. Our proof relies on the elegant smoothing of the non-smooth (cid:96)1 loss by the Gaussian data and a classical non-asymptotic analysis of Polyak-Ruppert averaged SGD. In addition, we provide experimental evidence of the efﬁciency of this simple and highly scalable algorithm. 1

Introduction
Robust learning is a critical ﬁeld that seeks to develop efﬁcient algorithms that can recover an underlying model despite possibly malicious corruptions in the data. In recent decades, being able to deal with corrupted measurements has become of crucial importance. The applications are considerable, to name a few settings: computer vision [87, 90, 5], economics [85, 73, 92], astronomy [72], biology [89, 76] and above all, safety-critical systems [15, 40, 33].
Linear regression being one of the most fundamental statistical model, the robust regression problem has naturally drawn substantial attention. In this problem, we wish to recover a signal from noisy linear measurements where an unknown proportion η has been arbitrarily perturbed. Various models have been proposed to illustrate such contaminations. The broadest is to consider that the adversary is adaptive and is allowed to inspect the samples before changing a fraction η. In this general framework, exact model recovery is not possible and several robust algorithms have been proposed [19, 49, 22, 68, 77, 17, 55, 54]. The information-theoretic optimal recovery guarantee has recently been reached by [28]. Another model is to consider an oblivious adversary, in this simpler context it is possible to consistently recover the model parameter and several algorithms have been proposed [8, 79].
However, none of these algorithms are suitable for online or large-scale problems [58, 36]. Indeed, all of the suggested algorithms require handling the complete dataset, which is simply unrealistic in such settings. This is a considerable problem when we know that modern problems involve colossal datasets and that current machine learning methods are limited by the computing time rather than the amount of data [12]. Such considerations advocate the necessity of proposing practical, online and highly scalable robust algorithms, hence we ask the following question:
Can we design an efﬁcient online algorithm for the robust regression problem ? 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper we answer by the afﬁrmative for the online oblivious response corruption model where we are given a stream of i.i.d. observation (xi, yi)i∈N from the following generative model: y = (cid:104)x, θ∗(cid:105) + ε + b, where θ∗ ∈ Rd is the true parameter we wish to recover, x is the Gaussian feature, ε is the Gaussian noise of variance σ2 and b is an adversarial ’sparse’ noise assumed independent of the data (x, ε) such that P(b (cid:54)= 0) = η. In order to recover the parameter θ∗, we perform averaged SGD on the expected (cid:96)1 loss E [|y − (cid:104)x, θ(cid:105)|]. Though this algorithm is very simple, we show that it successively handles the outliers in an online manner and recovers the true parameter at the optimal non-asymptotic convergence rate ˜O(1/n) for any outlier proportion η < 1. Such an algorithm is useful for abundant practical applications such as : (a) detection of irrelevant measurements and systematic labelling errors [52], (b) real time detection of system attacks such as frauds by click bots [41] or malware recommendation rating-frauds [95], and (c) online regression with heavy-tailed noise [79].
The minimisation problem minθ∈Rd E [|y − (cid:104)x, θ(cid:105)|] is certainly not new and is also known as the
Least Absolute Deviations (LAD) problem. While originally suggested by Boscovich in the mid-eighteenth century [10], it ﬁrst appears in the work of Edgeworth [32]. In contrast with least-squares, there is no closed form solution to the problem and, in addition, the non-differentiability of the (cid:96)1 loss prevents the use of fast optimisation solvers for large-scale applications [88]. However, if successively dealt with, the LAD problem has many advantages. Indeed, the (cid:96)1 loss is well known for its robustness properties [48] and, unlike the Huber loss [46], it is parameter free which makes it more convenient in practice. In this context, using the SGD algorithm in order to solve the LAD problem is a very natural approach. We show in our analysis that, though the (cid:96)1 loss is not strongly convex, n) averaged SGD recovers a remarkable O(1/n) convergence rate instead of the classical O(1/ which is ordinary in the non-strongly-convex framework
With a convergence rate depending on the variance as O(σ2d/(1 − ˜η)2n), the proposed algorithm has several major beneﬁts: a) it is highly scalable and statistically optimal, b) it depends on the outlier contamination through an effective outlier proportion ˜η strictly smaller than η, which makes it adaptive to the difﬁculty of the adversary, c) it is relatively insensitive to the ill conditioning of the features and d) it is almost parameter free since it only requires in practice an upper-bound on the covariates’ norm. Though the algorithm is simple, its analysis is not and requires several technical manipulations based on recent advances in stochastic approximation [44]. Indeed, in the classical n) and not non-strongly-convex framework which we are in, the usual convergence rate is O(1/
O(1/n) as we obtain. Overall our analysis relies on the smoothing of the (cid:96)1 loss by the Gaussian data. This smoothing enables the retrieval of a fast O(1/n) rate thanks to the local strong convexity around θ∗ and to Polyak-Ruppert averaging.
√
√
Our paper is organised as follows. We deﬁne the problem which we consider in Section 2. We then describe the particular structure our function f (θ) := E[|y − (cid:104)x, θ(cid:105)|] enjoys in Section 3. Our main convergence guarantee result is given Section 4 followed by the sketch of proof in Section 5. Finally, in Section 6, we provide experimental validation of the performances of our method. 1.1