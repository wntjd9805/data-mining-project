Abstract
Deep learning methods for graphs achieve remarkable performance on many node-level and graph-level prediction tasks. However, despite the proliferation of the methods and their success, prevailing Graph Neural Networks (GNNs) neglect subgraphs, rendering subgraph prediction tasks challenging to tackle in many im-pactful applications. Further, subgraph prediction tasks present several unique challenges: subgraphs can have non-trivial internal topology, but also carry a notion of position and external connectivity information relative to the underlying graph in which they exist. Here, we introduce SUBGNN, a subgraph neural network to learn disentangled subgraph representations. We propose a novel subgraph routing mechanism that propagates neural messages between the subgraph’s components and randomly sampled anchor patches from the underlying graph, yielding highly accurate subgraph representations. SUBGNN speciﬁes three channels, each de-signed to capture a distinct aspect of subgraph topology, and we provide empirical evidence that the channels encode their intended properties. We design a series of new synthetic and real-world subgraph datasets. Empirical results for subgraph classiﬁcation on eight datasets show that SUBGNN achieves considerable perfor-mance gains, outperforming strong baseline methods, including node-level and graph-level methods, by 19.8% over the strongest baseline. SUBGNN performs ex-ceptionally well on challenging biomedical datasets where subgraphs have complex topology and even comprise multiple disconnected components. 1

Introduction
Deep learning on graphs and Graph Neural Networks (GNNs), in particular, have emerged as the dominant paradigm for learning compact representations of interconnected data [66, 81, 23]. The methods condense graph neighborhood connectivity patterns into low-dimensional embeddings that can be used for a variety of downstream prediction tasks. While graph representation learning has made tremendous progress in recent years [20, 84], prevailing methods focus on learning useful representations for nodes [25, 68], edges [21, 37] or entire graphs [6, 27].
Graph-level representations provide an overarching view of the graphs but at the loss of some ﬁner local structure. In contrast, node-level representations focus instead on the preservation of the local topological structure (potentially to the detriment of the big picture). It is unclear if the methods can generate powerful representations for subgraphs and effectively capture the unique topology of subgraphs. Despite the popularity and importance of subgraphs for machine learning [77, 13, 78], there is still limited research on subgraph prediction [41], i.e., to predict if a particular subgraph has
⇤Contributed Equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
=
Figure 1: a. Shown is graph G in which nodes explicitly state their group memberships, resulting in
. Subgraphs S2, S3 and S5 comprise single connected
S1, . . . , S5} intricate subgraph structures,
{
S components in G whereas subgraphs S1 and S4 each form two isolated components. Colors indicate
. b. We investigate the problem of predicting subgraph properties
C1, C2, C3} subgraph labels, by learning subgraph representations that recognize and disentangle the heterogeneous properties
C of subgraphs (i.e., neighborhood, structure, and position) and how they relate to underlying G (i.e., internal connectivity and border structure of the edge volume that points outside of the subgraph).
=
C
{ a particular property of interest (Figure 1a). This can be, in part, because subgraphs are incredibly challenging structures from the topological standpoint (Figure 1b). (1) Subgraphs require that we make joint predictions over larger structures of varying sizes—the challenge is how to represent subgraphs that do not coincide with enclosing k-hop neighborhoods of nodes and can even comprise multiple disparate components that are far apart from each other in the graph. (2) Subgraphs contain rich higher-order connectivity patterns, both internally among member nodes as well as externally through interactions between the member nodes and the rest of the graph—the challenge is how to inject information about border and external subgraph structure into the GNN’s neural message passing. (3) Subgraphs can be localized and reside in one region of the graph or can be distributed across multiple local neighborhoods—the challenge is how to effectively learn about subgraph positions within the underlying graph in which they are deﬁned. (4) Finally, subgraph datasets give rise to unavoidable dependencies that emerge from subgraphs sharing edges and non-edges—the challenge is how to incorporate these dependencies into the model architecture while still being able to take feature information into account and facilitate inductive reasoning.
Present work. Here, we introduce SUBGNN2 (Figure 2), a novel graph neural network for subgraph prediction that addresses all of the challenges above. Unlike current prediction problems that are deﬁned on individual nodes, pairwise relations, or entire graphs, our task here is to make predictions for subgraphs. To the best of our knowledge, SUBGNN is the only representation learning method designed for general subgraph classiﬁcation. While [41] performs prediction on small (3-4 nodes),
ﬁxed-size subgraphs, SUBGNN operates on larger subgraphs with varying sizes and multiple disparate components. SUBGNN’s core principle is to propagate messages at the subgraph level via three property-aware channels that capture position, neighborhood, and structure. Further, SUBGNN is inductive: it can operate on new subgraphs in unseen portions of the graph.
Experiments on eight datasets show that SUBGNN outperforms baselines by an average of 77.4% on synthetic datasets and 125.2% on real-world datasets. Further, SUBGNN improves the strongest baseline by 19.8%. As an example, on a network of phenotypes where subgraphs represent symptoms of a disease, our method is able to distinguish between 10 subcategories of monogenic neurological disorders. We also ﬁnd that several naive generalizations of node-level and graph-level GNNs have poor performance. This ﬁnding is especially relevant as these generalizations are popular in practical applications, yet they cannot fully realize the potential of neural message-passing for subgraph prediction, as evidenced by our experiments. For example, we ﬁnd that a popular node-level approach, which represents subgraphs with virtual nodes (i.e., meta nodes) and then uses an attention-based
GNN (e.g., GAT [62]) to train a node classiﬁer, performs poorly in a variety of settings.
Finally, we design a suite of synthetic subgraph benchmarks that are uniquely suited for evaluating aspects of subgraph topology. We also provide four new and original real-world datasets from 2Code and datasets are available at https://github.com/mims-harvard/SubGNN. 2
biological, medical, and social domains to practically test the new algorithms. Each dataset consists of an underlying graph overlaid with a large number of subgraphs whose labels we need to predict. 2