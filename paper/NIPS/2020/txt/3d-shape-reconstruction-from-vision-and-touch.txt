Abstract
When a toddler is presented a new toy, their instinctual behaviour is to pick it up and inspect it with their hand and eyes in tandem, clearly searching over its surface to properly understand what they are playing with. At any instance here, touch provides high ﬁdelity localized information while vision provides complementary global context. However, in 3D shape reconstruction, the complementary fusion of visual and haptic modalities remains largely unexplored. In this paper, we study this problem and present an effective chart-based approach to multi-modal shape understanding which encourages a similar fusion vision and touch information.
To do so, we introduce a dataset of simulated touch and vision signals from the interaction between a robotic hand and a large array of 3D objects. Our results show that (1) leveraging both vision and touch signals consistently improves single-modality baselines; (2) our approach outperforms alternative modality fusion methods and strongly beneﬁts from the proposed chart-based structure; (3) the reconstruction quality increases with the number of grasps provided; and (4) the touch information not only enhances the reconstruction at the touch site but also extrapolates to its local neighborhood. 1

Introduction
From an early age children clearly and often loudly demonstrate that they need to both look and touch any new object that has peaked their interest. The instinctual behavior of inspecting with both their eyes and hands in tandem demonstrates the importance of fusing vision and touch information for 3D object understanding. Through machine learning techniques, 3D models of both objects and environments have been built by independent leveraging a variety of perception-based sensors, such as those for vision (e.g. a single RGB image) [58, 19] and touch [72, 66]. However, vision and touch possess a clear complementary nature. On one hand, vision provides a global context for object understanding, but is hindered by occlusions introduced by the object itself and from other objects in the scene. Moreover, vision is also affected by bas-relief [39] and scale/distance ambiguities, as well as slant/tilt angles [3]. On the other hand, touch provides localized 3D shape information, including the point of contact in space as well as high spatial resolution of the shape, but fails quickly when extrapolating without global context or strong priors. Hence, combining both modalities should lead to richer information and better models for 3D understanding. An overview of 3D shape reconstruction from vision and touch is displayed in Figure 1.
Visual and haptic modalities have been combined in the literature [2] to learn multi-modal repre-sentations of the 3D world, and improve upon subsequent 3D understanding tasks such as object manipulation [41] or any-modal conditional generation [42]. Tactile information has also been used
∗Correspondence to: ejsmith@fb.com and edward.smith@mail.mcgill.ca 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: 3D shape understanding from vision and touch includes: (1) shape sensing with a camera and touch sensor, as well as (2) reconstruction algorithm that fuses vision and touch readings. In this paper, we introduce a dataset that captures object sensing and propose a chart-based fusion model for 3D shape prediction from multi-modal inputs. For touch, we realistically simulate an existing vision-based tactile sensor [40]. to improve 3D reconstructions in real environments. In particular, [66] leverages vision and touch sequentially, by ﬁrst using vision to learn 3D object shape priors on simulated data, and subsequently using touch to reﬁne the vision reconstructions when performing sim2real transfer. However, to the best of our knowledge, the complementary fusion of vision (in particular, RGB images) and touch in 3D shape reconstruction remains largely unexplored.
In this paper, we focus on this unexplored space, and present an approach that effectively fuses the global and local information provided by visual and haptic modalities to perform 3D shape reconstruction. Inspired by the papier-mâché technique of AtlasNet and its similar works [21, 71, 14] and leveraging recent advances in graph convolutional networks (GCN) [38], we aim to represent a 3D object with a collection of disjoint mesh surface elements, called charts [33], where some charts are reserved for tactile signals and others are used to represent visual information. More precisely, given an RGB image of an object and high spatial resolution tactile (mimicking a DIGIT tactile sensor [40]) and pose information of a grasp, the approach predicts a high ﬁdelity local chart at each touch site and then uses the corresponding vision information to predict global charts which close the surface around them, in a ﬁll-in-the-blank type procedure. As learning from real world robot interactions is resource and time intensive, we have designed a simulator to produce a multi-modal dataset of interactions between a robotic hand and four classes of objects, that can be used to benchmark approaches to 3D shape reconstructions from vision and touch, and help advance the ﬁeld. Our dataset contains ground truth 3D objects as well as recordings from vision and tactile sensors, such as RGB images and touch readings. Results on the proposed dataset show that by combining visual and tactile cues, we are able to outperform single modality touch and vision baselines. We demonstrate the intuitive property that learning from touch exclusively translates into decreased performance, as the 3D shape reconstruction suffers from poor global context while learning from vision exclusively suffers from occlusions and leads to lower local reconstruction accuracy. However, when combining both modalities, we observe a systematic improvement, suggesting that the proposed approach effectively beneﬁts from vision and touch signals, and surpasses alternative fusion strategies. Moreover, when increasing the number of grasps provided, we are able to further boost the 3D shape reconstruction quality. Finally, due to our model design, the touch readings not only enhance the reconstruction at the touch site but also reduce the error in the neighborhood of touch sensor position. Our main contributions can be summarized as: (1) we introduce a chart-based approach to 3D object reconstruction, leveraging GCNs to combine visual and haptic signals; (2) we build a dataset of simulated haptic object interactions to benchmark 3D shape reconstructions algorithms in this setting; and (3) through an extensive evaluation, we highlight the beneﬁts of the proposed approach, which effectively exploits the complementarity of both modalities. Code for our system is publicly available on a GitHub repository, to ensure reproducible experimental comparison.2 2