Abstract
Priority dispatching rule (PDR) is widely used for solving real-world Job-shop scheduling problem (JSSP). However, the design of effective PDRs is a tedious task, requiring a myriad of specialized knowledge and often delivering limited per-formance. In this paper, we propose to automatically learn PDRs via an end-to-end deep reinforcement learning agent. We exploit the disjunctive graph representa-tion of JSSP, and propose a Graph Neural Network based scheme to embed the states encountered during solving. The resulting policy network is size-agnostic, effectively enabling generalization on large-scale instances. Experiments show that the agent can learn high-quality PDRs from scratch with elementary raw features, and demonstrates strong performance against the best existing PDRs. The learned policies also perform well on much larger instances that are unseen in training. 1

Introduction
Job-shop scheduling problem (JSSP) is a well-known combinatorial optimization problem in computer science and operations research, and is ubiquitous in many industries such as manufacturing and transportation [1, 2]. In JSSP, a number of jobs with predeﬁned processing constraints (e.g. the operations are processed in order by their eligible machines) are assigned to a set of heterogeneous machines, to achieve the desired objective such as minimizing the makespan, ﬂowtime, or tardiness.
Due to its NP-hardness, ﬁnding exact solutions to JSSP is often impractical [3, 4], while efﬁciency in practice usually relies on heuristics [5, 6] or approximate methods [7].
Priority dispatching rule (PDR) [6] is a heuristic method that is widely used in real-world scheduling systems. Compared with complicated optimization methods such as mathematical programming and metaheuristics, PDR is computationally fast, intuitive and easy to implement, and naturally capable of handling uncertainties that are ubiquitous in practice [8]. Motivated by these advantages, a large number of PDRs for JSSP have been proposed in the literature [9]. However, it is commonly accepted that designing an effective PDR is very costly and time-consuming, requiring substantial domain knowledge and trial-and-error especially for complex JSSP. Moreover, performance of a
PDR often varies drastically on different instances [10]. Therefore, a natural question to ask is: can we automate the process of designing PDR, such that it performs well on a class of JSSP instances sharing common characteristics? A number of recent works on learning algorithms for other types of combinatorial optimization problems (COPs) (see [11] for a survey) show that deep reinforcement
∗Both authors contributed equally.
†Corresponding Author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
learning (DRL) could be an ideal technique for this purpose. However, for complex scheduling problems such as JSSP which differs structurally from other COPs and received much less attention, existing methods cannot apply [11], and it remains challenging to design effective representation and learning mechanism.
In this paper, we propose a novel DRL based method to automatically learn strong and robust PDRs for solving JSSP. Speciﬁcally, we ﬁrst present a Markov Decision Process (MDP) formulation of PDR based scheduling, where the states are captured by leveraging the disjunctive graph representation of JSSP. Such representation effectively integrates the operation dependencies and machine status, and provides critical information for scheduling decisions. Then, we propose a Graph Neural
Network (GNN) based scheme with an efﬁcient computation strategy to encode the nodes in the disjunctive graphs to ﬁxed dimensional embeddings. Based on this scheme, we design a size-agnostic policy network that can process JSSP instances with arbitrary size, which effectively enables training on small-sized instances and generalizing to large-scale ones. We train the network using a policy gradient algorithm to obtain high-quality PDRs, without the need of supervision. Extensive experiments on generated instances and standard benchmarks show that, the PDRs trained by our policy signiﬁcantly outperform existing manually designed ones, and generalize reasonably well to instances that are much larger than those used in training. 2