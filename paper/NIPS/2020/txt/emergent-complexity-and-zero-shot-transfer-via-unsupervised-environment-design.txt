Abstract
A wide range of reinforcement learning (RL) problems — including robustness, transfer learning, unsupervised RL, and emergent complexity — require speci-fying a distribution of tasks or environments in which a policy will be trained.
However, creating a useful distribution of environments is error prone, and takes a signiﬁcant amount of developer time and effort. We propose Unsupervised
Environment Design (UED) as an alternative paradigm, where developers pro-vide environments with unknown parameters, and these parameters are used to automatically produce a distribution over valid, solvable environments. Existing approaches to automatically generating environments suffer from common failure modes: domain randomization cannot generate structure or adapt the difﬁculty of the environment to the agent’s learning progress, and minimax adversarial training leads to worst-case environments that are often unsolvable. To generate struc-tured, solvable environments for our protagonist agent, we introduce a second, antagonist agent that is allied with the environment-generating adversary. The adversary is motivated to generate environments which maximize regret, deﬁned as the difference between the protagonist and antagonist agent’s return. We call our technique Protagonist Antagonist Induced Regret Environment Design (PAIRED).
Our experiments demonstrate that PAIRED produces a natural curriculum of in-creasingly complex environments, and PAIRED agents achieve higher zero-shot transfer performance when tested in highly novel environments. 1

Introduction
Many reinforcement learning problems require designing a distribution of tasks and environments that can be used to evaluate and train effective policies. This is true for a diverse array of methods including transfer learning (e.g., [2, 48, 32, 41]), robust RL (e.g., [4, 16, 28]), unsupervised RL (e.g.,
[15]), and emergent complexity (e.g., [38, 45, 46]). For example, suppose we wish to train a robot in simulation to pick up objects from a bin in a real-world warehouse. There are many possible conﬁgurations of objects, including objects being stacked on top of each other. We may not know a priori the typical arrangement of the objects, but can naturally describe the simulated environment as having a distribution over the object positions.
However, designing an appropriate distribution of environments is challenging. The real world is complicated, and correctly enumerating all of the edge cases relevant to an application could be impractical or impossible. Even if the developer of the RL method knew every edge case, specifying this distribution could take a signiﬁcant amount of time and effort. We want to automate this process.
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Domain Randomization (b) Minimax Adversarial (c) PAIRED (ours) (d) Transfer task
Figure 1: An agent learns to navigate an environment where the position of the goal and obstacles is an underspeciﬁed parameter. If trained using domain randomization to randomly choose the obstacle locations (a), the agent will fail to generalize to a speciﬁc or complex conﬁguration of obstacles, such as a maze (d). Minimax adversarial training encourages the adversary to create impossible environments, as shown in (b). In contrast, Protagonist Antagonist Induced Regret Environment
Design (PAIRED), trains the adversary based on the difference between the reward of the agent (protagonist) and the reward of a second, antagonist agent. Because the two agents are learning together, the adversary is motivated to create a curriculum of difﬁcult but achievable environments tailored to the agents’ current level of performance (c). PAIRED facilitates learning complex behaviors and policies that perform well under zero-shot transfer to challenging new environments at test time.
In order for this automation to be useful, there must be a way of specifying the domain of environments in which the policy should be trained, without needing to fully specify the distribution. In our approach, developers only need to supply an underspeciﬁed environment: an environment which has free parameters which control its features and behavior. For instance, the developer could give a navigation environment in which the agent’s objective is to navigate to the goal and the free parameters are the positions of obstacles. Our method will then construct distributions of environments by providing a distribution of settings of these free parameters; in this case, positions for those blocks. We call this problem of taking the underspeciﬁed environment and a policy, and producing an interesting distribution of fully speciﬁed environments in which that policy can be further trained Unsupervised Environment Design (UED). We formalize unsupervised environment design in Section 3, providing a characterization of the space of possible approaches which subsumes prior work. After training a policy in a distribution of environments generated by UED, we arrive at an updated policy and can use UED to generate more environments in which the updated policy can be trained. In this way, an approach for UED naturally gives rise to an approach for Unsupervised
Curriculum Design (UCD). This method can be used to generate capable policies through increasingly complex environments targeted at the policy’s current abilities.
Two prior approaches to UED are domain randomization, which generates fully speciﬁed environ-ments uniformly randomly regardless of the current policy (e.g., [17, 32, 41]), and adversarially generating environments to minimize the reward of the current policy; i.e. minimax training (e.g.,
[28, 25, 43, 18]). While each of these approaches have their place, they can each fail to generate any interesting environments. In Figure 1 we show examples of maze navigation environments generated by each of these techniques. Uniformly random environments will often fail to generate interesting structures; in the maze example, it will be unlikely to generate walls (Figure 1a). On the other extreme, a minimax adversary is incentivized to make the environments completely unsolvable, generating mazes with unreachable goals (Figure 1b). In many environments, both of these methods fail to generate structured and solvable environments. We present a middle ground, generating environments which maximize regret, which produces difﬁcult but solvable tasks (Figure 1c). Our results show that optimizing regret results in agents that are able to perform difﬁcult transfer task (Figure 1d), which are not possible using the other two techniques.
We propose a novel adversarial training technique which naturally solves the problem of the adversary generating unsolvable environments by introducing an antagonist which is allied with the environment-generating adversary. For the sake of clarity, we refer to the primary agent we are trying to train as the protagonist. The environment adversary’s goal is to design environments in which the antagonist achieves high reward and the protagonist receives low reward. If the adversary generates unsolvable environments, the antagonist and protagonist would perform the same and the adversary would get a score of zero, but if the adversary ﬁnds environments the antagonist solves and the protagonist does not solve, the adversary achieves a positive score. Thus, the environment adversary is incentivized to 2
create challenging but feasible environments, in which the antagonist can outperform the protagonist.
Moreover, as the protagonist learns to solves the simple environments, the antagonist must generate more complex environments to make the protagonist fail, increasing the complexity of the generated tasks and leading to automatic curriculum generation.
We show that when both teams reach a Nash equilibrium, the generated environments have the highest regret, in which the performance of the protagonist’s policy most differs from that of the optimal policy. Thus, we call our approach Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our results demonstrate that compared to domain randomization, minimax adversarial training, and population-based minimax training (as in Wang et al. [45]), PAIRED agents learn more complex behaviors, and have higher zero-shot transfer performance in challenging, novel environments.
In summary, our main contributions are: a) formalizing the problem of Unsupervised Environment
Design (UED) (Section 3), b) introducing the PAIRED Algorithm (Section 4), c) demonstrating
PAIRED leads to more complex behavior and more effective zero-shot transfer to new environments than existing approaches to environment design (Section 5), and d) characterizing solutions to UED and connecting the framework to classical decision theory (Section 3). 2