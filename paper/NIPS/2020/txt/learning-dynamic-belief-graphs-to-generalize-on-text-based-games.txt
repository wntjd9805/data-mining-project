Abstract
Playing text-based games requires skills in processing natural language and sequen-tial decision making. Achieving human-level performance on text-based games remains an open challenge, and prior research has largely relied on hand-crafted structured representations and heuristics. In this work, we investigate how an agent can plan and generalize in text-based games using graph-structured representations learned end-to-end from raw text. We propose a novel graph-aided transformer agent (GATA) that infers and updates latent belief graphs during planning to enable effective action selection by capturing the underlying game dynamics. GATA is trained using a combination of reinforcement and self-supervised learning. Our work demonstrates that the learned graph-based representations help agents con-verge to better policies than their text-only counterparts and facilitate effective generalization across game conﬁgurations. Experiments on 500+ unique games from the TextWorld suite show that our best agent outperforms text-based baselines by an average of 24.2%. 1

Introduction
Text-based games are complex, interactive simulations in which the game state is described with text and players act using simple text commands (e.g., light torch with match). They serve as a proxy for studying how agents can exploit language to comprehend and interact with the environment.
Text-based games are a useful challenge in the pursuit of intelligent agents that communicate with humans (e.g., in customer service systems).
Solving text-based games requires a combination of reinforcement learning (RL) and natural language processing (NLP) techniques. However, inherent challenges like partial observability, long-term dependencies, sparse rewards, and combinatorial action spaces make these games very difﬁcult.2 For instance, Hausknecht et al. [16] show that a state-of-the-art model achieves a mere 2.56% of the total possible score on a curated set of text-based games for human players [5]. On the other hand, while text-based games exhibit many of the same difﬁculties as linguistic tasks like open-ended dialogue, they are more structured and constrained.
To design successful agents for text-based games, previous works have relied largely on heuristics that exploit games’ inherent structure. For example, several works have proposed rule-based components
∗
Equal contribution. 2We challenge readers to solve this representative game: https://aka.ms/textworld-tryit. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: GATA playing a text-based game by updating its belief graph. In response to action At−1, the environment returns text observation Ot. Based on Ot and Gt−1, the agent updates Gt and selects a new action At. In the ﬁgure, blue box with squares is the game engine, green box with diamonds is the graph updater, red box with slashes is the action selector. that prune the action space or shape the rewards according to a priori knowledge of the game dynamics [50, 24, 1, 48]. More recent approaches take advantage of the graph-like structure of text-based games by building knowledge graph (KG) representations of the game state: Ammanabrolu and
Riedl [4], Ammanabrolu and Hausknecht [3], for example, use hand-crafted heuristics to populate a KG that feeds into a deep neural agent to inform its policy. Despite progress along this line, we expect more general, effective representations for text-based games to arise in agents that learn and scale more automatically, which replace heuristics with learning [37].
This work investigates how we can learn graph-structured state representations for text-based games in an entirely data-driven manner. We propose the graph aided transformer agent (GATA)3 that, in lieu of heuristics, learns to construct and update graph-structured beliefs4 and use them to further optimize rewards. We introduce two self-supervised learning strategies—based on text reconstruction and mutual information maximization—which enable our agent to learn latent graph representations without direct supervision or hand-crafted heuristics.
We benchmark GATA on 500+ unique games generated by TextWorld [9], evaluating performance in a setting that requires generalization across different game conﬁgurations. We show that GATA outperforms strong baselines, including text-based models with recurrent policies. In addition, we compare GATA to agents with access to ground-truth graph representations of the game state. We show that GATA achieves competitive performance against these baselines even though it receives only partial text observations of the state. Our ﬁndings suggest, promisingly, that graph-structured representations provide a useful inductive bias for learning and generalizing in text-based games, and act as a memory enabling agents to optimize rewards in a partially observed setting. 2