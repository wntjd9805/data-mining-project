Abstract
Despite the widespread use of gradient-based algorithms for optimizing high-dimensional non-convex functions, understanding their ability of ﬁnding good minima instead of being trapped in spurious ones remains to a large extent an open problem. Here we focus on gradient ﬂow dynamics for phase retrieval from random measurements. When the ratio of the number of measurements over the input dimension is small the dynamics remains trapped in spurious minima with large basins of attraction. We ﬁnd analytically that above a critical ratio those critical points become unstable developing a negative direction toward the signal.
By numerical experiments we show that in this regime the gradient ﬂow algorithm is not trapped; it drifts away from the spurious critical points along the unstable direction and succeeds in ﬁnding the global minimum. Using tools from statistical physics we characterize this phenomenon, which is related to a BBP-type transition in the Hessian of the spurious minima. 1

Introduction
In many machine learning applications one optimizes a non-convex loss function; this is often achieved using simple descending algorithms such as gradient descent or its stochastic variations.
The positive results obtained in practice are often hard to justify from the theoretical point of view, and this apparent contradiction between non-convex landscapes and good performance of simple algorithms is a recurrent problem in machine learning.
A successful line of research has studied the geometrical properties of the loss landscape, distinguish-ing between good minima - that lead to good generalization error - and spurious minima - associated with bad generalization error. The results showed that in some regimes, for several problems from matrix completion [1] to wide neural networks [2, 3], spurious minima disappear and consequently under weak assumptions [4] gradient descent will converge to good minima. However, these results do not justify numerous other results showing that good and spurious minima are present, but sys-tematically gradient descent works [5, 6]. In [7] it was theoretically shown that in a toy model - the spiked matrix-tensor model - it is possible to ﬁnd good minima with high probability in a regime where exponentially many spurious minima are provably present. In [8] it was shown that this is due to the presence of the so-called threshold states in the landscape, that play a key role in the 1. Université Paris-Saclay, CNRS, CEA, Institut de physique théorique, 91191, Gif-sur-Yvette,France. 2. Laboratoire de Physique de l’Ecole normale supérieure ENS, Université PSL, CNRS, Sorbonne Université,
Université Paris-Diderot, Sorbonne Paris Cité Paris, France 3. Dipartimento di Fisica, Sapienza Università di Roma, P.le A. Moro 5, 00185 Rome, Italy 4. Department of Mathematics, King’s College London, Strand London WC2R 2LS, UK 5. IdePHICS laboratory, EPFL, Switzerland 6. SPOC laboratory, EPFL, Switzerland 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
dynamics of the gradient ﬂow [9, 10]: at ﬁrst attracting it, and successively triggering the converge towards lower minima under certain conditions [11, 12]. However, the spiked matrix-tensor model is an unsupervised learning model and it remained open whether the picture put forward in [7, 8] happens also in learning with neural networks.
In this work we thus study learning with a simple single-layer neural network on data stemming from the well-known phase retrieval problem - that consists of reconstructing a hidden vector having access only to the absolute value of its projection onto random directions. The problem emerges naturally in a variety of imaging applications where the intensity is easier to access than the phase [13–16] but it appears also in acoustics [17] and quantum mechanics [18]. The phase retrieval problem considered here leads to a high-dimensional and non-convex optimization problem deﬁned as follows.
Phase retrieval. Consider αN N -dimensional sensing vectors xxxm with unitary norm and generated m with ym = (cid:104)xxxm, WWW ∗(cid:105) and WWW ∗ according to a centered Gaussian distribution, and the true labels y2
√ an unknown teacher-vector (the signal in the phase retrieval literature) from SN −1(
N ). Given the dataset, the goal is to build an estimator WWW of WWW ∗ by minimizing the loss function
L(WWW ) = 1 2
αN (cid:88) m=1 (cid:0)(cid:104)xxxm, WWW (cid:105)2 − (cid:104)xxxm, WWW ∗(cid:105)2(cid:1)2
= 1 2
αN (cid:88) m=1 (cid:96)(ˆym, ym), (1) with ˆym = (cid:104)xxxm, WWW (cid:105) and (cid:96)(ˆy, y) = (ˆy2 − y2)2 is a modiﬁed square loss commonly used in the literature [19, 20] that ensures a smoother landscape compared to the square loss with the absolute values. The loss function, Eq. (1), is minimized using gradient-descent ﬂow on the sphere starting from random initialization
˙WWW (t) = −∇WWW L(t) + µ(t)WWW (t),
Wi(t = 0) ∼ N (0, 1) ∀ i, (2) with µ(t) the Lagrange multiplier that enforces the spherical constraint during the dynamics. The value of µ(t) can be readily evaluated by taking the scalar product of the gradient of the loss with WWW and dividing by N
µ(t) = 1 2N
αN (cid:88) m=1
∂
∂ ˆym (cid:96)(ˆym(t), ym)ˆym(t).
We can ﬁnally deﬁne the Hessian of the problem, denoting δi,j the Kronecker delta, it reads
Hi,j(WWW ) = 1 2
αN (cid:88) m=1
∂2
∂ ˆy2 m (cid:96)(ˆym, ym)xm,ixm,j − µδi,j. (3) (4)