Abstract
Scaling up the convolutional neural network (CNN) size (e.g., width, depth, etc.) is known to effectively improve model accuracy. However, the large model size impedes training on resource-constrained edge devices. For instance, federated learning (FL) may place undue burden on the compute capability of edge nodes, even though there is a strong practical need for FL due to its privacy and conﬁden-tiality properties. To address the resource-constrained reality of edge devices, we reformulate FL as a group knowledge transfer training algorithm, called FedGKT.
FedGKT designs a variant of the alternating minimization approach to train small
CNNs on edge nodes and periodically transfer their knowledge by knowledge dis-tillation to a large server-side CNN. FedGKT consolidates several advantages into a single framework: reduced demand for edge computation, lower communication bandwidth for large CNNs, and asynchronous training, all while maintaining model accuracy comparable to FedAvg. We train CNNs designed based on ResNet-56 and
ResNet-110 using three distinct datasets (CIFAR-10, CIFAR-100, and CINIC-10) and their non-I.I.D. variants. Our results show that FedGKT can obtain comparable or even slightly higher accuracy than FedAvg. More importantly, FedGKT makes edge training affordable. Compared to the edge training using FedAvg, FedGKT demands 9 to 17 times less computational power (FLOPs) on edge devices and requires 54 to 105 times fewer parameters in the edge CNN. Our source code is released at FedML (https://fedml.ai). 1

Introduction
The size of convolutional neural networks (CNN) matters. As seen in both manually designed neural architectures (ResNet [1]) and automated architectures discovered by neural architecture search (DARTS [2], MiLeNAS [3], EfﬁcientNets [4]), scaling up CNN size (e.g., width, depth, etc.) is known to be an effective approach for improving model accuracy. Unfortunately, training large
CNNs is challenging for resource-constrained edge devices (e.g., smartphones, IoT devices, and edge servers). The demand for edge-based training is increasing as evinced by a recent surge of interest in Federated Learning (FL) [5]. FL is a distributed learning paradigm that can collaboratively train a global model for many edge devices without centralizing any device’s dataset [6, 7, 8]. FL can boost model accuracy in situations when a single organization or user does not have sufﬁcient or relevant data. Consequently, many FL services have been deployed commercially. For instance,
Google has improved the accuracy of item ranking and language models on Android smartphones by using FL [9]. FL is also a promising solution when data centralization is undesirable or infeasible due to privacy and regulatory constraints [5]. However, one signiﬁcant impediment in edge training is the gap between the computational demand of large CNNs and the meager computational power on edge devices. FL approaches, such as FedAvg [6] can reduce communication frequency by local
SGD and model averaging [10], but they only evaluate the convergence property on small CNNs, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Alternating and periodical knowledge transfer (b) CNN architectures on the edge and server
Figure 1: Reformulation of Federated Learning: Group Knowledge Transfer or assume the client has enough computational power with GPUs to train large CNNs, which is unrealistic in a real-world system. To tackle the computational limitation of edge nodes, model parallelism-based split learning (SL) [11, 12] partitions a large model and ofﬂoads some portion of the neural architecture to the cloud, but SL has a severe straggler problem because a single mini-batch iteration requires multiple rounds of communication between the server and edges.
In this paper, we propose Group Knowledge Transfer (FedGKT), an efﬁcient federated learning framework for resource-constrained edge devices. FedGKT aims to incorporate beneﬁts from both
FedAvg [6] and SL [11, 12] by training using local SGD as in FedAvg but also placing low compute demand at the edge as in SL. FedGKT can transfer knowledge from many compact CNNs trained at the edge to a large CNN trained at a cloud server. The essence of FedGKT is that it reformulates FL as an alternating minimization (AM) approach [13, 14, 15, 16, 17, 18], which optimizes two random variables (the edge model and the server model) by alternatively ﬁxing one and optimizing another.
Under this reformulation, FedGKT not only boosts training CNNs at the edge but also contributes to the development of a new knowledge distillation (KD) paradigm, group knowledge transfer, to boost the performance of the server model. Fig. 1(a) provides an overview of FedGKT. The compact
CNN on the edge device consists of a lightweight feature extractor and classiﬁer that can be trained efﬁciently using its private data (1 - local training). After local training, all the edge nodes agree to generate exactly the same tensor dimensions as an output from the feature extractor. The larger server model is trained by taking features extracted from the edge-side model as inputs to the model, and then uses KD-based loss function that can minimize the gap between the ground truth and soft label (probabilistic prediction in KD [19, 20, 21, 22]) predicted from the edge-side model (2 - periodic transfer). To boost the edge model’s performance, the server sends its predicted soft labels to the edge, then the edge also trains its local dataset with a KD-based loss function using server-side soft labels (3 - transfer back). Thus, the server’s performance is essentially boosted by knowledge transferred from the edge models and vice-versa. Once the training is complete, the ﬁnal model is a combination of its local feature extractor and shared server model (4 - edge-sided model). The primary trade-off is that FedGKT shifts the computing burden from edge devices to the powerful server.
FedGKT uniﬁes multiple advantages into a single framework: 1. FedGKT is memory and computation efﬁcient, similar to SL; 2. FedGKT can train in a local SGD manner like FedAvg to reduce the communication frequency; 3. Exchanging hidden features as in SL, as opposed to exchanging the entire model as in FedAvg, reduces the communication bandwidth requirement. 4. FedGKT naturally supports asynchronous training, which circumvents the severe synchronization issue in SL. The server model can immediately start training when it receives inputs from any client. We develop FedGKT based on the FedML research library [23] and comprehensively evaluate FedGKT using edge and server CNNs designed based on ResNet [1] (as shown in Fig. 1(b)). We train on three datasets with varying training difﬁculties (CIFAR-10 [24], CIFAR-100 [24], and CINIC-10 [25]) and their non-I.I.D. (non identical and independent distribution) variants. As for the model accuracy, our results on both I.I.D. and non-I.I.D. datasets show that FedGKT can obtain accuracy comparable to FedAvg
[6]. More importantly, FedGKT makes edge training affordable. Compared to FedAvg, FedGKT demands 9 to 17 times less computational power (FLOPs) on edge devices and requires 54 to 105 times fewer parameters in the edge CNN. To understand FedGKT comprehensively, asynchronous training and ablation studies are performed. Some limitations are also discussed. 2
2