Abstract
Natural Gradient Descent (NGD) helps to accelerate the convergence of gradient de-scent dynamics, but it requires approximations in large-scale deep neural networks because of its high computational cost. Empirical studies have conﬁrmed that some
NGD methods with approximate Fisher information converge sufﬁciently fast in practice. Nevertheless, it remains unclear from the theoretical perspective why and under what conditions such heuristic approximations work well. In this work, we reveal that, under speciﬁc conditions, NGD with approximate Fisher information achieves the same fast convergence to global minima as exact NGD. We consider deep neural networks in the inﬁnite-width limit, and analyze the asymptotic training dynamics of NGD in function space via the neural tangent kernel. In the function space, the training dynamics with the approximate Fisher information are identical to those with the exact Fisher information, and they converge quickly. The fast convergence holds in layer-wise approximations; for instance, in block diagonal approximation where each block corresponds to a layer as well as in block tri-diagonal and K-FAC approximations. We also ﬁnd that a unit-wise approximation achieves the same fast convergence under some assumptions. All of these different approximations have an isotropic gradient in the function space, and this plays a fundamental role in achieving the same convergence properties in training. Thus, the current study gives a novel and uniﬁed theoretical foundation with which to understand NGD methods in deep learning. 1

Introduction
Natural gradient descent (NGD) was developed to speed up the convergence of the gradient method
[1]. The main drawback of the natural gradient is its high computational cost to compute the inverse of the Fisher information matrix (FIM). Numerous studies have proposed approximation methods to reduce the computational cost so that NGD can be used in large-scale models with many parameters, especially, in deep neural networks (DNNs). For instance, to compute the inverse efﬁciently, some studies have proposed layer-wise block diagonal approximations of the FIM [2–4], where each block matrix corresponds to a layer of the DNN. This approach is usually combined with the Kronecker-factored approximate curvature (K-FAC) to further reduce the computational cost. Others have proposed unit-wise approximations, where each block matrix corresponds to a unit [5–7].
Although empirical experiments have conﬁrmed that these approximations make the convergence faster than the conventional ﬁrst-order gradient descent, their conclusions are rather heuristic and present few theoretical guarantees on how fast the approximate NGD converges. It is important for both theory and practice to answer the question of how well approximate NGD preserves the performance of the original NGD. The lack of theoretical evidence is mostly caused by the difﬁculty 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of analyzing the training dynamics of neural networks. Recently, however, researchers have developed a theoretical framework, known as neural tangent kernel (NTK), to analyze the training dynamics of the conventional gradient descent (GD) in DNNs with sufﬁciently large widths [8–10]. The NTK determines the gradient dynamics in function space. It enables us to prove the global convergence of gradient descent and, furthermore, to explain generalization performance by using the equivalence between the trained model and a Gaussian process.
In this paper, we extend the asymptotic analysis of GD dynamics in inﬁnitely-wide deep neural networks to NGD and investigate the dynamics of NGD with the approximate FIMs developed in practice. We ﬁnd that, surprisingly, they achieve the same fast convergence of training to global minima as the NGD with the exact FIM. we show this is true for layer-wise block diagonal approxi-mation of the FIM, block tri-diagonal approximation, K-FAC, and unit-wise approximation under speciﬁc conditions. Each algorithm requires an appropriately scaled learning rate depending on the network size or sample size for convergence. In function space, the exact NGD algorithm and these different approximations give the same dynamics on training samples. We clarify that they become independent of the NTK matrix and isotropic in the function space, which leads to fast convergence.
We also discuss some results with the goal of increasing our understanding of approximate NGD.
First, the dynamics of approximate NGD methods on training samples are the same in the function space, but they are different in the parameter space and converge to different global minima. Their predictions on test samples also vary from one algorithm to another. Our numerical experiments demonstrate that the predictions of a model trained by the approximate methods are comparable to those of exact NGD. Second, we empirically show that the isotropic condition holds in the layer-wise and unit-wise approximations but not in entry-wise diagonal approximations of the FIM. In this way, we give a systematic understanding of NGD with approximate Fisher information for deep learning. 2