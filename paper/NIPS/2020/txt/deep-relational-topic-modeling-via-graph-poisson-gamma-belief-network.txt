Abstract
To analyze a collection of interconnected documents, relational topic models (RTMs) have been developed to describe both the link structure and document con-tent, exploring their underlying relationships via a single-layer latent representation with limited expressive capability. To better utilize the document network, we ﬁrst propose graph Poisson factor analysis (GPFA) that constructs a probabilistic model for interconnected documents and also provides closed-form Gibbs sampling up-date equations, moving beyond sophisticated approximate assumptions of existing
RTMs. Extending GPFA, we develop a novel hierarchical RTM named graph Pois-son gamma belief network (GPGBN), and further introduce two different Weibull distribution based variational graph auto-encoders for efﬁcient model inference and effective network information aggregation. Experimental results demonstrate that our models extract high-quality hierarchical latent document representations, leading to improved performance over baselines on various graph analytic tasks. 1

Introduction
A wide variety of network data, such as citation networks [1], chemical molecular structures [2], and social networks [3], can be represented as a graph composed of a set of objects (nodes), each of which is characterized by a set of node features (attributes), and their relationships (edges). The node features and edges are usually characterized as count, binary, or positive variables. In many graph analytic applications, such as link prediction, modeling uncertainty [4] in the latent space rather than only providing deterministic node embeddings, is of crucial importance [5–7] and can be realized via probabilistic generative models (PGMs).
Various network decomposition methods have been proposed to discover the underlying relationships of the nodes from the link structure [8–11]. These methods, however, often ignore the information provided by the node features. Inspired by the efﬁciency of latent Dirichlet allocation (LDA) [12] in exploring the hidden structure of count-valued data, a series of relational topic models (RTMs)
[3, 13–17] were introduced to explore the relationships between the nodes and edges in a latent space.
∗Equal Contribution
†Corresponding Author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Though achieving appealing performance, these RTMs are limited by their shallow structures that only employ a single-layer latent representation. Although there is a recent trend to develop deep probabilistic topic models [18–20] replacing LDA to provide multi-layer semantic representations, how to construct a hierarchical RTM to capture multi-layer semantic connections and use them together with graph learning remains an open research problem.
From another perspective, variational autoencoder (VAE) [21, 22] was extended for modeling graph-structured data, resulting in a variational graph autoencoder (VGAE) [23] parameterized by graph convolutional networks (GCNs) [24]. Further, focused on task-speciﬁc applications, several VGAE-based variations [2, 25–27] are introduced for various graph analytic tasks. To move beyond the naive choice of a Gaussian prior in combination with the inner product decoder in VGAE, some methods [4, 28] attempt to learn non-Gaussian latent representations and have achieved promising performance. However, these VGAEs mentioned only capture single-layer semantic representations and ignore the reconstruction information of node features, which may hurt the performance on node clustering or classiﬁcation.
Inspired by both the advantages of existing RTMs and VGAEs and to move beyond their constraints, we ﬁrst construct an interpretable hierarchical (deep) RTM, which is further developed as two different non-Gaussian VGAEs with multiple stochastic layers. The main contributions of this paper are:
• A novel RTM named graph Poisson factor analysis (GPFA), equipped with analytic conditional posteriors for efﬁcient Gibbs sampling, is proposed to account for both node features and link structure by sharing their latent representations (topic proportions).
• To explore hierarchical latent representations of the nodes and reveal their relationships at different semantic levels, GPFA is extended to a deep generative model, referred to as graph Poisson gamma belief network (GPGBN). To the best of our knowledge, GPGBN is the ﬁrst unsupervised deep
RTM for analyzing network data.
• To move beyond Gaussian-based VGAEs, which often fail to well approximate sparse, nonnegative, and skewed document latent representations, and generalize GPGBN to different tasks, we combine
GPGBN (decoder) with two Weibull distribution-based graph variational inference networks (encoder), resulting in two different Weibull graph autoencoders.
• Besides achieving state-of-the-art or comparable performance on various graph analytic tasks, our models provide a potential solution to explore multi-layer interpretable network relationships. 2