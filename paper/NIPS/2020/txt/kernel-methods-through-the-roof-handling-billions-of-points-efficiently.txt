Abstract
Kernel methods provide an elegant and principled approach to nonparametric learning, but so far could hardly be used in large scale problems, since naïve imple-mentations scale poorly with data size. Recent advances have shown the beneﬁts of a number of algorithmic ideas, for example combining optimization, numerical linear algebra and random projections. Here, we push these efforts further to develop and test a solver that takes full advantage of GPU hardware. Towards this end, we designed a preconditioned gradient solver for kernel methods exploiting both GPU acceleration and parallelization with multiple GPUs, implementing out-of-core variants of common linear algebra operations to guarantee optimal hardware utilization. Further, we optimize the numerical precision of different operations and maximize efﬁciency of matrix-vector multiplications. As a result we can experimentally show dramatic speedups on datasets with billions of points, while still guaranteeing state of the art performance. Additionally, we make our software available as an easy to use library1. 1

Introduction
Kernel methods provide non-linear/non-parametric extensions of many classical linear models in machine learning and statistics [45, 49]. The data are embedded via a non-linear map into a high dimensional feature space, so that linear models in such a space effectively deﬁne non-linear models in the original space. This approach is appealing, since it naturally extends to models with inﬁnitely many features, as long as the inner product in the feature space can be computed. In this case, the inner product is replaced by a positive deﬁnite kernel, and inﬁnite dimensional models are reduced to
ﬁnite dimensional problems. The mathematics of kernel methods has its foundation in the rich theory of reproducing kernel Hilbert spaces [47], and the connection to linear models provides a gateway to deriving sharp statistical results [53, 10, 54, 6, 4, 56]. Further, kernel methods are tightly connected to
Gaussian processes [40], and have recently being used to understand the properties of deep learning models [23, 29]. It is not a surprise that kernel methods are among the most theoretically studied models. From a numerical point of view, they reduce to convex optimization problems that can be solved with strong guarantees. The corresponding algorithms provide excellent results on a variety of data-sets, but most implementations are limited to problems of small/medium size, see 1https://github.com/FalkonML/falkon 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Benchmarks of kernel solvers on large scale datasets with millions and billions points (see
Section 4). Our approach (red and yellow lines) consistently achieves state of the art accuracy in minutes. discussion in [52], Chapter 11. Most methods require handling a kernel matrix quadratic in the sample size. Hence, dealing with datasets of size 104 to 105 is challenging, while larger datasets are typically out of reach. A number of approaches have been considered to alleviate these computational bottlenecks. Among others, random features [38, 39, 66, 26, 12, 11] and the Nyström method are often used [61, 50], see also [14, 25, 18, 3, 67, 9]. While different, both these approaches consider random projections to reduce the problem size and hence computational costs. Renewed interest in approximate kernel methods was also spurred by recent theoretical results proving that computational gains can possibly be achieved with no loss of accuracy, see e.g. [27, 55, 41, 4, 42, 5].
In this paper, we investigate the practical consequences of this line of work, developing and testing large scale kernel methods that can run efﬁciently on billions of points. Following [43] we use a
Nyström approach to reduce the problem size and also to derive a preconditioned gradient solver for kernel methods. Indeed, we focus on smooth loss functions where such approaches are natural.
Making these algorithmic ideas practical and capable of exploiting the GPU, requires developing a number of computational solutions, borrowing ideas not only from optimization and numerical analysis but also from scientiﬁc and high performance computing [28, 2, 7]. Indeed, we design preconditioned conjugate gradient solvers that take full advantage of both GPU acceleration and parallelization with multiple GPUs, implementing out-of-core variants of common linear algebra operations to guarantee optimal hardware utilization. We further optimize the numerical precision of different operations and investigate ways to perform matrix-vector multiplications most efﬁciently.
The corresponding implementation is then tested extensively on a number of datasets ranging from millions to billions of points. For comparison, we focused on other available large scale kernel implementations that do not require data splitting, or multiple machines. In particular, we consider Eigenpro [30] which is an approach similar to the one we propose, and GPyTorch [16] and
GPﬂow [58] which come from the Gaussian process literature. While these latter solutions allow also for uncertainty quantiﬁcation, we limit the comparison to prediction. We perform a systematic empirical evaluation running an extensive series of tests. Empirical results show that indeed our approach can process huge datasets in minutes and obtain state of the art performances, comparing favorably to other solutions, both in terms of efﬁciency and accuracy. More broadly, these results conﬁrm and extend the observations made in [29, 30], that kernel methods can now be seamlessly and effectively deployed on large scale problems. To make these new solutions readily available, the corresponding code is distributed as an easy to use library developed on top of PyTorch [36].
The rest of the paper is organized as follows. In Section 2, we provide some background on the considered approaches. In Section 3, we detail the main algorithmic solutions in our implementation, whereas the last section is devoted to assessing the practical advantages. 2
2