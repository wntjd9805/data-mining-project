Abstract
The recently proposed distribution correction estimation (DICE) family of estima-tors has advanced the state of the art in off-policy evaluation from behavior-agnostic data. While these estimators all perform some form of stationary distribution cor-rection, they arise from different derivations and objective functions. In this paper, we unify these estimators as regularized Lagrangians of the same linear program.
The uniﬁcation allows us to expand the space of DICE estimators to new alterna-tives that demonstrate improved performance. More importantly, by analyzing the expanded space of estimators both mathematically and empirically we ﬁnd that dual solutions offer greater ﬂexibility in navigating the tradeoff between optimization stability and estimation bias, and generally provide superior estimates in practice. 1

Introduction
One of the most fundamental problems in reinforcement learning (RL) is policy evaluation, where we seek to estimate the expected long-term payoff of a given target policy in a decision making environment. An important variant of this problem, off-policy evaluation (OPE) [23], is motivated by applications where deploying a policy in a live environment entails signiﬁcant cost or risk [20, 27].
To circumvent these issues, OPE attempts to estimate the value of a target policy by referring only to a dataset of experience previously gathered by other policies in the environment. Often, such logging or behavior policies are not known explicitly (e.g., the experience may come from human actors), which necessitates the use of behavior-agnostic OPE methods [21].
While behavior-agnostic OPE appears to be a daunting problem, a number of estimators have recently been developed for this scenario [21, 28, 30, 31], demonstrating impressive empirical results. Such estimators, known collectively as the “DICE” family for DIstribution Correction Estimation, model the ratio between the propensity of the target policy to visit particular state-action pairs relative to their likelihood of appearing in the logged data. A distribution corrector of this form can then be directly used to estimate the value of the target policy.
Although there are many commonalities between the various DICE estimators, their derivations are distinct and seemingly incompatible. For example, DualDICE [21] is derived by a particular change-of-variables technique, whereas GenDICE [30] observes that the substitution strategy cannot work in the average reward setting, and proposes a distinct derivation based on distribution matching.
GradientDICE [31] notes that GenDICE exacerbates optimization difﬁculties, and proposes a variant designed for limited sampling capabilities. Despite these apparent differences in these methods, the algorithms all involve a minimax optimization that has a strikingly similar form, which suggests that there is a common connection that underlies the alternative derivations.
We show that the previous DICE formulations are all in fact equivalent to regularized Lagrangians of the same linear program (LP). This LP shares an intimate relationship with the policy evaluation problem, and has a primal form we refer to as the Q-LP and a dual form we refer to as the d-LP. The primal form has been concurrently identiﬁed and studied in the context of policy optimization [22],
∗Indicates equal contribution. Email: {sherryy, ofirnachum, bodai}@google.com. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
but we focus on the d-LP formulation for off-policy evaluation here, which we ﬁnd to have a more succinct and revealing form for this purpose. Using the d-LP, we identify a number of key choices in translating it into a stable minimax optimization problem – i.e. whether to include redundant constraints, whether to regularize the primal or dual variables – in addition to choices in how to translate an optimized solution into an asymptotically unbiased estimate of the policy value.2 We use this characterization to show that the known members of the DICE family are a small subset of speciﬁc choices made within a much larger, unexplored set of potential OPE methods.
To understand the consequences of the various choices, we provide a comprehensive study. First, we theoretically investigate which conﬁgurations lead to bias in the primal or dual solutions, and when this affects the ﬁnal estimates. Our analysis shows that the dual solutions offer greater ﬂexibility in stabilizing the optimization while preserving asymptotic unbiasedness, versus primal solutions. We also perform an extensive empirical evaluation of the various choices across different domains and function approximators, and identify novel conﬁgurations that improve the observed outcomes. 2