Abstract
√
√
We study the structure of regret-minimizing policies in the many-armed Bayesian multi-armed bandit problem: in particular, with k the number of arms and T the
T . We ﬁrst show that subsampling time horizon, we consider the case where k ≥ is a critical step for designing optimal policies. In particular, the standard UCB algorithm leads to sub-optimal regret bounds in the many-armed regime. However,
T ) arms and executes UCB a subsampled UCB (SS-UCB), which samples Θ( only on that subset, is rate-optimal. Despite theoretically optimal regret, even
SS-UCB performs poorly due to excessive exploration of suboptimal arms. In particular, in numerical experiments SS-UCB performs worse than a simple greedy algorithm (and its subsampled version) that pulls the current empirical best arm at every time period. We show that these insights hold even in a contextual setting, using real-world data. These empirical results suggest a novel form of free exploration in the many-armed regime that beneﬁts greedy algorithms. We theoretically study this new source of free exploration and ﬁnd that it is deeply connected to the distribution of a certain tail event for the prior distribution of arm rewards. This is a fundamentally distinct phenomenon from free exploration as discussed in the recent literature on contextual bandits, where free exploration arises due to variation in contexts. We use this insight to prove that the subsampled greedy algorithm is rate-optimal for Bernoulli bandits when k >
T , and achieves sublinear regret with more general distributions. This is a case where theoretical rate optimality does not tell the whole story: when complemented by the empirical observations of our paper, the power of greedy algorithms becomes quite evident.
Taken together, from a practical standpoint, our results suggest that in applications it may be preferable to use a variant of the greedy algorithm in the many-armed regime.
√ 1

Introduction
We consider the standard stochastic multi-armed bandit (MAB) problem, in which a decision-maker takes actions sequentially over T time periods (the horizon). At each time period, the decision-maker chooses one of k arms, and receives an uncertain reward. The goal is to maximize cumulative rewards attained over the horizon. Crucially, in the typical formulation of this problem, the set of arms k is assumed to be “small” relative to the time horizon T ; in particular, in standard asymptotic analysis
∗The majority of this work was completed while Khashayar Khosravi was afﬁliated with Stanford University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of the MAB setting, the horizon T scales to inﬁnity while k remains constant. In practice, however, there are many situations where the number of arms is large relative to the time horizon of interest.
For example, drug development typically considers many combinations of basic substances; thus
MABs for adaptive drug design inherently involve a large set of arms. Similarly, when MABs are used in recommendation engines for online platforms, the number of choices available to users is enormous: this is the case in e-commerce (many products available); media platforms (many content options); online labor markets (wide variety of jobs or workers available); dating markets (many possible partners); etc.
Formally, we say that an MAB instance is in the many-armed regime where k ≥
T . In our
T is in fact the correct point of transition to the theoretical results, we show that the threshold many-armed regime, at which behavior of the MAB problem becomes qualitatively different than the
T . Throughout our paper, we consider a Bayesian framework 2, i.e., where the regime where k < arms’ reward distributions are drawn from a prior.
√
√
√
√
In §3, we ﬁrst use straightforward arguments to establish a fundamental lower bound of Ω(
T ) on Bayesian regret in the many-armed regime. We note that prior Bayesian lower bounds for the stochastic MAB problem require k to be ﬁxed while T → ∞ (see, e.g., 16, 18, 19), and hence, are not applicable in the many-armed regime.
Our ﬁrst main insight (see §4) is the importance of subsampling. The standard UCB algorithm can perform quite poorly in the many-armed regime, because it over-explores arms: even trying every arm
T ) bound is achieved (up to logarithmic once leads to a regret of Ω(k). Instead, we show that the Ω(
T factors) by a subsampled upper conﬁdence bound (SS-UCB) algorithm, where we ﬁrst select arms uniformly at random, and then run a standard UCB algorithm [17, 5] with just these arms.
√
√
However, numerical investigation reveals interesting behaviors. In Figure 1, we simulate several different algorithms over 400 simulations, for two pairs of T, k in the many-armed regime. 3Notably, the greedy algorithm (Greedy) — i.e., an algorithm that pulls each arm once, and thereafter pulls the empirically best arm for all remaining times – performs extremely well. This is despite the well-known fact that Greedy can suffer linear regret in the standard MAB problem, as it can ﬁxate too early on a suboptimal arm. Observe that in line with our ﬁrst insight above, subsampling improves the performance of all algorithms, including UCB, Thompson sampling (TS), and Greedy. In particular, the subsampled greedy algorithm (SS-Greedy) outperforms all other algorithms.
The right panel in Figure 1 shows that Greedy and SS-Greedy beneﬁt from a novel form of free exploration, that arises due to the availability of a large number of near-optimal arms. This free exploration helps the greedy algorithms to quickly discard sub-optimal arms that are substantially over-explored by algorithms with “active exploration” (i.e., UCB, TS, and their subsampled versions).
We emphasize that this source of free exploration is distinct from that observed in recent literature on contextual bandits (see, e.g., 6, 15, 20, 14), where free exploration arises due to diversity in the context distribution. Our extensive simulations in Section 6 and in the longer version of paper [7] show that these insights are robust to varying rewards and prior distributions. Indeed, similar results are obtained with Bernoulli rewards and general beta priors. We refer the interested reader to this longer version of the paper. Further, using simulations, we also observe that the same phenomenon arises in the contextual MAB setting, via simulations with synthetic and real-world data.
Motivated by these observations, in §5 and §7 we embark on a theoretical analysis of Greedy in the many-armed regime to complement our empirical investigation. We show that with high probability, one of the arms on which Greedy concentrates attention is likely to have a high mean reward (as also observed in the right panel of Figure 1). Our proof technique uses the Lundberg inequality to relate the probability of this event to distribution of the ruin event of a random walk, and may be of independent interest in studying the performance of greedy algorithms in other settings. Using this result we show that for Bernoulli rewards, the regret of Greedy is ˜O(max(k, T /k)); in particular, for k ≥
T
T , Greedy is optimal). For more general reward distributions
SS-Greedy is optimal (and for k = we show that, under a mild condition, an upper bound on the regret of Greedy is ˜O(max[k, T / k]).
Thus theoretically, for general reward distributions, in the many-armed regime Greedy achieves sublinear, though not optimal, regret.
√
√
√ 2While our focus is on the Bayesian setting, our analysis can be extended to the frequentist setting. 3Our code is available at http://github.com/khashayarkhv/many-armed-bandit. 2
Figure 1: Distribution of the per-instance regret (on left) and proﬁle of pulls in logarithmic scale based on arms index (on right). Rewards are generated according to N (µi, 1), with µi ∼ U[0, 1]. The list of algorithms included is as follows. (1) UCB: Algorithm 1, (2) SS-UCB: Algorithm 2 with m =
T , (3) Greedy: Algorithm 3, (4) SS-Greedy: Algorithm 4 with m = T 2/3 (see Theorem 5), (5) UCB-F: UCB-F algorithm of [27] with the choice of conﬁdence set Et = 2 log(10 log t), (6) TS: Thompson Sampling algorithm [26, 22, 2], and (7) SS-TS: subsampled TS with m =
T .
√
√
Our theoretical results illuminate why Greedy and SS-Greedy perform well in our numerical experi-ments, due to the novel form of free exploration we identify in the many-armed regime. Although our theoretical results do not establish universal rate optimality of SS-Greedy, this is clearly a case where regret bounds do not tell the whole story. Indeed, given the robust empirical performance of Greedy and SS-Greedy, from a practical standpoint the combination of our empirical and theoretical insights suggests that in applications it may be preferable to use greedy algorithms in the many-armed regime.
This advice is only ampliﬁed when one considers that in contextual settings, such algorithms are likely to beneﬁt from free exploration due to context diversity as well (as noted above).
Details of the proofs are all deferred to the longer version of the paper [7]. 1.1