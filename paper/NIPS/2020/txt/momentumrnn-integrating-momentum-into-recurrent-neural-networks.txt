Abstract
Designing deep neural networks is an art that often involves an expensive search over candidate architectures. To overcome this for recurrent neural nets (RNNs), we establish a connection between the hidden state dynamics in an RNN and gradient descent (GD). We then integrate momentum into this framework and propose a new family of RNNs, called MomentumRNNs. We theoretically prove and numerically demonstrate that MomentumRNNs alleviate the vanishing gradient issue in training
RNNs. We study the momentum long-short term memory (MomentumLSTM) and verify its advantages in convergence speed and accuracy over its LSTM counterpart across a variety of benchmarks. We also demonstrate that MomentumRNN is applicable to many types of recurrent cells, including those in the state-of-the-art orthogonal RNNs. Finally, we show that other advanced momentum-based optimization methods, such as Adam and Nesterov accelerated gradients with a restart, can be easily incorporated into the MomentumRNN framework for designing new recurrent cells with even better performance. 1

Introduction
Mathematically principled recurrent neural nets (RNNs) facilitate the network design process and reduce the cost of searching over many candidate architectures. A particular advancement in RNNs is the long short-term memory (LSTM) model [24] which has achieved state-of-the-art results in many applications, including speech recognition [15], acoustic modeling [53, 51], and language modeling [46]. There have been many efforts in improving LSTM: [19] introduces a forget gate into the original LSTM cell, which can forget information selectively; [18] further adds peephole connections to the LSTM cell to inspect its current internal states[17]; to reduce the computational cost, a gated recurrent unit (GRU) [11] uses a single update gate to replace the forget and input gates in LSTM. Phased LSTM [42] adds a new time gate to the LSTM cell and achieves faster convergence than the regular LSTM on learning long sequences. In addition, [52] and [50] introduce a biological cell state and working memory into LSTM, respectively. Nevertheless, most of RNNs, including
LSTMs, are biologically informed or even ad-hoc instead of being guided by mathematical principles.
∗Please correspond to: wangbaonj@gmail.com or mn15@rice.edu 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1.1 Recap on RNNs and LSTM 1 and xt in a recurrent cell can be written as
Recurrent cells are the building blocks of RNNs. A recurrent cell employs a cyclic connection to update the current hidden state (ht) using the past hidden state (ht 1) and the current input data (xt)
[14]; the dependence of ht on ht ht = σ(Uht h, W (1)
, T, where U
) is a nonlinear activation
· function, e.g., sigmoid or hyperbolic tangent. Error backpropagation through time is used to train
RNN, but it tends to result in exploding or vanishing gradients [4]. Thus RNNs may fail to learn long term dependencies. Several approaches exist to improve RNNs’ performance, including enforcing unitary weight matrices [1, 62, 25, 60, 38, 22], leveraging LSTM cells, and others [35, 30]. 1 + Wxt + b), xt ∈
Rh d, and b
Rh are trainable parameters; σ(
Rd, and ht
−
Rh, t = 1, 2, 1, ht ∈
Rh
· · ·
∈
∈
∈
−
×
×
−
−
LSTM cells augment the recurrent cell with “gates” [24] and can be formulated as 1 + Wixxt + bi), 1 + W(cid:101)cxxt + b (cid:101)c), 1 + Woxxt + bo),
− ct,
− it = σ(Uihht ct = tanh (U(cid:101)chht 1 + it (cid:12) ct = ct
− (cid:101) ot = σ(Uohht ht = ot (cid:12) h, W
∗ ∈
×
− (cid:101) tanh ct,
Rh d, and b (it : input gate) ct : cell input) ( (ct : cell state) (cid:101) (ot : output gate) (ht : hidden state) (2)
×
Rh
∗ ∈ where U denotes the
Hadamard product. The input gate decides what new information to be stored in the cell state, and the output gate decides what information to output based on the cell state value. The gating mechanism in LSTMs can lead to the issue of saturation [59, 8].
Rh are learnable parameters, and
∗ ∈ (cid:12) 1.2 Our Contributions
In this paper, we develop a gradient descent (GD) analogy of the recurrent cell. In particular, the hidden state update in a recurrent cell is associated with a gradient descent step towards the optimal representation of the hidden state. We then propose to integrate momentum that used for accelerating gradient dynamics into the recurrent cell, which results in the momentum cell. At the core of the momentum cell is the use of momentum to accelerate the hidden state learning in RNNs. The architectures of the standard recurrent cell and our momentum cell are illustrated in Fig. 1. We provide the design principle and detailed derivation of the momentum cell in Sections 2.2 and 2.4.
We call the RNN that consists of momentum cells the MomentumRNN. The major advantages of
MomentumRNN are fourfold:
•
•
•
•
MomentumRNN can alleviate the vanishing gradient problem in training RNN.
MomentumRNN accelerates training and improves the test accuracy of the baseline RNN.
MomentumRNN is universally applicable to many existing RNNs. It can be easily implemented by changing a few lines of the baseline RNN code.
MomentumRNN is principled with theoretical guarantees provided by the momentum-accelerated dynamical system for optimization and sampling. The design principle can be generalized to other advanced momentum-based optimization methods, including Adam [28] and Nesterov accelerated gradients with a restart [44, 61]. 1.3