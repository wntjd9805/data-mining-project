Abstract
Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets. 1

Introduction
The internet age relies on lossy compression algorithms that transmit information at low bitrates.
These algorithms are typically analysed through the rate-distortion trade-off, originally posited by Shannon [33]. When performing lossy compression at extremely low bit rates, obtaining low distortions often results in reconstructions of very low perceptual quality [5, 6, 38]. For modern lossy compression, high perceptual quality of reconstructions is often more desirable than low distortions.
This work investigates good performance on this rate-perception tradeoff as opposed to more standard rate-distortion trade offs, with a focus on the low-rate regime.
At low bitrates it is desirable to communicate only high-level concepts and ofﬂoad the ‘ﬁlling in’ of details to a powerful decoder [38]. Neural Networks present a promising avenue since they are ﬂexible enough to learn the complex transformations required to both capture such high-level concepts and reconstruct in a convincing way that avoids artifacts [32, 10, 14].
Variational Autoencoders (VAEs [15]) are latent variable Neural Network models that have made signiﬁcant strides in lossy image compression [35, 1]. However, due to a combination of a poor likelihood function and a sub-optimal variational posterior [31, 43], reconstructions can look blurred and unrealistic [44, 11]. There have been many attempts to construct hierarchical forms of both VAEs and Vector Quantized Variational Autoencoders (VQ-VAEs), however perceptual quality is frequently sacriﬁced at low-rates, and has only recently been made viable with methods that require large autoregressive decoders [8, 30]. Solutions to this problem then take two forms: either augmenting the likelihood model, for instance, by using adversarial methods [38] or improving the structure of the posterior/latent space [43, 3]. However, at low rates both solutions struggle to match the realism of implicit generative models [9].
*Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: CelebA interpolations of the HQA encoder output ze in the 9 bit 8x8 latent space. The original 64x64 images are shown on the left and right. The center images are the resulting decodes when using 8 linearly interpolated points between the ze of the original images. Compression is from 98,304 to 576 bits (171x compression).
To address these issues, we build from previous work on heirarchical VQ-VAEs and introduce1 the ‘Hierarchical Quantized Autoencoder’ (HQA). Our system implicitly gives rise to many of the qualities of explicit perceptual losses and furnishes the practitioner with a repeatable operation of learned-compression that can be trained greedily.
Our key contributions are as follows:
• We introduce new analysis as to why probabilistic quantized hierarchies are particularly well-suited to optimising the perception-rate tradeoff when performing extreme lossy compression.
• We propose a new scheme (HQA) for extreme lossy compression. HQA exploits probabilistic forms of VQ-VAE’s commitment and codebook losses and uses a novel objective for training hierarchical VQ-VAEs. This objective leads to higher layers implicitly reconstructing the full posterior of the layer below, as opposed to samples from this posterior.
• We show that HQA can produce reconstructions of high perceptual quality at very low rates using only simple feedforward decoders, where as related methods require autoregressive decoders. 2