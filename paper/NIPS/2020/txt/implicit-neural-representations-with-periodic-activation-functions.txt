Abstract
Implicitly deﬁned, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible beneﬁts over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with ﬁne detail. They also fail to accurately model spatial and temporal derivatives, which is necessary to represent signals deﬁned implicitly by differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or
SIRENs, are ideally suited for representing complex natural signals and their deriva-tives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, waveﬁelds, video, sound, three-dimensional shapes, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular
Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications. 1

Introduction
We are interested in a class of functions Φ that satisfy equations of the form:
C (cid:0)x, Φ, ∇xΦ, ∇2 xΦ, . . .(cid:1) = 0, Φ : x (cid:55)→ Φ(x). (1)
In this implicit problem formulation, a functional C takes as input the spatial or spatio-temporal coordinates x ∈ Rm and, optionally, derivatives of Φ with respect to these coordinates. Our goal is then to learn a neural network that parameterizes Φ to map x to some quantity of interest while satisfying the constraint presented in Equation (1). Thus, Φ is implicitly deﬁned by the relation modeled by C and we refer to neural networks that parameterize such implicitly deﬁned functions as implicit neural representations. As we show in this paper, a surprisingly wide variety of problems across scientiﬁc ﬁelds fall into this form, such as modeling many different types of discrete signals in image, video, and audio processing using a continuous and differentiable representation, learning 3D shape representations via signed distance functions [1–4], and, more generally, solving boundary value problems, such as the Poisson, Helmholtz, or wave equations.
∗These authors contributed equally to this work. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
A continuous parameterization offers several beneﬁts over alternatives, such as discrete grid-based representations. For example, due to the fact that Φ is deﬁned on the continuous domain of x, it can be signiﬁcantly more memory efﬁcient than a discrete representation, allowing it to model ﬁne detail that is not limited by the grid resolution but by the capacity of the underlying network architecture.
As an example, we show how our SIREN architecture can represent complex 3D shapes with networks using only a few hundred kilobytes whereas naive mesh representations of the same datasets require hundreds of megabytes. Being differentiable implies that gradients and higher-order derivatives can be computed analytically, for example using automatic differentiation, which again makes these models independent of conventional grid resolutions. Finally, with well-behaved derivatives, implicit neural representations may offer a new toolbox for solving inverse problems, such as differential equations.
For these reasons, implicit neural representations have seen signiﬁcant research interest over the last year (Sec. 2). Most of these recent representations build on ReLU-based multilayer perceptrons (MLPs). While promising, these architectures lack the capacity to represent ﬁne details in the underlying signals, and they typically do not represent the derivatives of a target signal well. This is partly due to the fact that ReLU networks are piecewise linear, their second derivative is zero everywhere, and they are thus incapable of modeling information contained in higher-order derivatives of natural signals. While alternative activations, such as tanh or softplus, are capable of representing higher-order derivatives, we demonstrate that their derivatives are often not well behaved and also fail to represent ﬁne details.
To address these limitations, we leverage MLPs with periodic activation functions for implicit neural representations. We demonstrate that this approach is not only capable of representing details in the signals better than ReLU-MLPs, or positional encoding strategies proposed in concurrent work [5], but that these properties also uniquely apply to the derivatives, which is critical for many applications we explore in this paper.
To summarize, the contributions of our work include:
• A continuous implicit neural representation using periodic activation functions that ﬁts complicated signals, such as natural images and 3D shapes, and their derivatives robustly.
• An initialization scheme for training these representations and validation that distributions of these representations can be learned using hypernetworks.
• Demonstration of applications in image, video, and audio representation; 3D shape recon-struction; solving ﬁrst-order differential equations to estimate a signal from its gradients; and solving second-order differential equations. 2