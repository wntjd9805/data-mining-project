Abstract
Message-passing has proved to be an effective way to design graph neural networks, as it is able to leverage both permutation equivariance and an inductive bias towards learning local structures in order to achieve good generalization. However, current message-passing architectures have a limited representation power and fail to learn basic topological properties of graphs. We address this problem and propose a powerful and equivariant message-passing framework based on two ideas: ﬁrst, we propagate a one-hot encoding of the nodes, in addition to the features, in order to learn a local context matrix around each node. This matrix contains rich local information about both features and topology and can eventually be pooled to build node representations. Second, we propose methods for the parametrization of the message and update functions that ensure permutation equivariance. Having a representation that is independent of the speciﬁc choice of the one-hot encoding permits inductive reasoning and leads to better generalization properties. Experi-mentally, our model can predict various graph topological properties on synthetic data more accurately than previous methods and achieves state-of-the-art results on molecular graph regression on the ZINC dataset. 1

Introduction
Graph neural networks have recently emerged as a popular way to process and analyze graph-structured data. Among the numerous architectures that have been proposed, the class of message-passing neural networks (MPNNs) [1–3] has been by far the most widely adopted. In addition to being able to efﬁciently exploit the sparsity of graphs, MPNNs exhibit an inherent tendency to learn relationships between nearby nodes. This inductive bias is generally considered as a good ﬁt for problems that require relational reasoning [4], such as tractable relational inference [5, 6], problems in combinatorial optimization [7–10] or the simulation of physical interactions between objects [11, 12].
A second key factor to the success of MPNNs is their equivariance properties. Since neural networks can ultimately only process tensors, in order to use a graph as input, it is necessary to order its nodes and build an adjacency list or matrix. Non-equivariant networks tend to exhibit poor sample efﬁciency as they need to explicitly learn that all representations of a graph in the (enormous) symmetry group of possible orderings actually correspond to the same object. On the contrary, permutation equivariant networks, such as MPNNs, are better equipped to generalize as they already implement the prior knowledge that any ordering is arbitrary.
Despite their success, equivariant MPNNs possess limited expressive power [13, 14]. For example, they cannot learn whether a graph is connected, what is the local clustering coefﬁcient of a node, or if a given pattern such as a cycle is present in a graph [15]. For tasks where the graph structure is important, such as the prediction of chemical properties of molecules [16, 17] and the solution to combinatorial optimization problems, more powerful graph neural networks are necessary. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Aiming to address these challenges, this work puts forth structural message-passing (SMP)—a new type of graph neural network that is strictly more powerful than MPNNs, while also sharing the attractive inductive bias of message-passing architectures. SMP inherits its power from its ability to manipulate node identiﬁers. However, in contrast to previous studies that relied on identiﬁers [18, 19], it does so in a permutation equivariant way without introducing new sources of randomness. As a result, SMP can be powerful without sacriﬁcing its ability to generalize to unseen data. In particular, we show that if SMP is built out of powerful layers, the resulting model is computationally universal over the space of equivariant functions.
Concretely, SMP maintains at each node a matrix called “local context” (instead of a feature vector as in MPNNs) that is initialized with a one-hot encoding of the nodes and the node features. These local contexts are then propagated in such a way that a permutation of the nodes or a change in the one-hot encoding will reorder the lines of each context without changing their content, which is key to efﬁcient learning and good generalization.
We evaluate SMP on a diverse set of structural tasks that are known to be difﬁcult for message-passing architectures, such as cycle detection, connectivity testing, diameter and shortest path distance computation. In all cases, our approach compares favorably to previous methods: for example, SMP solves cycle detection in all evaluated conﬁgurations, whereas other powerful networks struggle when the graphs become larger, and MPNNs do not manage to solve the task completely.
Finally, we evaluate our method on the ZINC chemistry dataset and achieve state-of-the-art perfor-mance among methods that do not use expert features. It shows that SMP is able to successfully learn both from the features and from topological information, which is essential in chemistry applications.
Overall, our method is able to overcome a major limitation of MPNNs, while retaining their ability to process features with a bias towards locality.
Notation.
In the following, we consider the problem of representation learning on one or several graphs of possibly varying sizes. Each graph G = (V, E) has an adjacency matrix A ∈ Rn×n, and potentially node attributes X = (x1, ..., xn)T ∈ Rn×cX and edge attributes yij ∈ RcY for every (vi, vj) ∈ E. These attributes are aggregated into a 3-d tensor Y ∈ Rn×n×cY . We consider the edge weights of weighted graphs as edge attributes and view A as a binary adjacency matrix. The set of neighbors of a node vi ∈ V is written as Ni. 2