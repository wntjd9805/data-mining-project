Abstract
Reward shaping is an effective technique for incorporating domain knowledge into reinforcement learning (RL). Existing approaches such as potential-based reward shaping normally make full use of a given shaping reward function. However, since the transformation of human knowledge into numeric reward values is often imperfect due to reasons such as human cognitive bias, completely utilizing the shaping reward function may fail to improve the performance of RL algorithms. In this paper, we consider the problem of adaptively utilizing a given shaping reward function. We formulate the utilization of shaping rewards as a bi-level optimiza-tion problem, where the lower level is to optimize policy using the shaping rewards and the upper level is to optimize a parameterized shaping weight function for true reward maximization. We formally derive the gradient of the expected true reward with respect to the shaping weight function parameters and accordingly propose three learning algorithms based on different assumptions. Experiments in sparse-reward cartpole and MuJoCo environments show that our algorithms can fully exploit beneﬁcial shaping rewards, and meanwhile ignore unbeneﬁcial shaping rewards or even transform them into beneﬁcial ones. 1

Introduction
A common way for addressing the sample efﬁciency issue of RL is to transform possible domain knowledge into additional rewards and guide learning algorithms to learn faster and better with the combination of the original and new rewards, which is known as reward shaping (RS). Early work of reward shaping can be dated back to the attempt of using hand-crafted reward function for robot behavior learning [3] and bicycle driving [15]. The most well-known work in the reward shaping domain is the potential-based reward shaping (PBRS) method [12], which is the ﬁrst to show that policy invariance can be guaranteed if the shaping reward function is in the form of the difference of potential values. Recently, reward shaping has also been successfully applied in more complex problems such as Doom [8, 18] and Dota 2 [13].
Existing reward shaping approaches such as PBRS and its variants [2, 5, 6, 24] mainly focus on the way of generating shaping rewards (e.g., using potential values) and normally assume that the shaping rewards transformed from prior knowledge are completely helpful. However, such an as-sumption is not practical since the transformation of human knowledge (e.g., rules) into numeric
∗Corresponding Author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
values (e.g., rewards or potentials) inevitably involves human operations, which are often subjective and may introduce human cognitive bias. For example, for training a Doom agent, designers should set appropriate rewards for key events such as object pickup, shooting, losing health, and losing ammo [8]. However, there are actually no instructions indicating what speciﬁc reward values are appropriate and the designers most probably have to try many versions of reward functions before getting a well-performed agent. Furthermore, the prior knowledge provided may also be unreliable if the reward designers are not experts of Doom.
Instead of studying how to generate useful shaping rewards, in this paper, we consider how to adap-tively utilize a given shaping reward function. The term adaptively utilize stands for utilizing the beneﬁcial part of the given shaping reward function as much as possible and meanwhile ignoring the unbeneﬁcial shaping rewards. The main contributions of this paper are as follows. Firstly, we deﬁne the utilization of a shaping reward function as a bi-level optimization problem, where the lower level is to optimize policy for shaping rewards maximization and the upper level is to optimize a param-eterized shaping weight function for maximizing the expected accumulative true reward. Secondly, we provide formal results for computing the gradient of the expected true reward with respect to the weight function parameters and propose three learning algorithms for solving the bi-level optimiza-tion problem. Lastly, extensive experiments are conducted in cart-pole and MuJoCo environments.
The results show that our algorithms can identify the quality of given shaping rewards and adap-tively make use of them. In some tests, our algorithms even transform harmful shaping rewards into beneﬁcial ones and help to optimize the policy better and faster. 2