Abstract
Combining different models is a widely used paradigm in machine learning appli-cations. While the most common approach is to form an ensemble of models and average their individual predictions, this approach is often rendered infeasible by given resource constraints in terms of memory and computation, which grow lin-early with the number of models. We present a layer-wise model fusion algorithm for neural networks that utilizes optimal transport to (soft-) align neurons across the models before averaging their associated parameters.
We show that this can successfully yield “one-shot” knowledge transfer (i.e, without requiring any retraining) between neural networks trained on heteroge-neous non-i.i.d. data. In both i.i.d. and non-i.i.d. settings, we illustrate that our approach signiﬁcantly outperforms vanilla averaging, as well as how it can serve as an efﬁcient replacement for the ensemble with moderate ﬁne-tuning, for standard convolutional networks (like VGG11), residual networks (like RESNET18), and multi-layer perceptrons on CIFAR10, CIFAR100, and MNIST. Finally, our ap-proach also provides a principled way to combine the parameters of neural networks with different widths, and we explore its application for model compression. The code is available at the following link, https://github.com/sidak/otfusion. 1

Introduction
If two neural networks had a child, what would be its weights? In this work, we study the fusion of two parent neural networks—which were trained differently but have the same number of layers—into a single child network. We further focus on performing this operation in a one-shot manner, based on the network weights only, so as to minimize the need of any retraining.
This fundamental operation of merging several neural networks into one contrasts other widely used techniques for combining machine learning models:
Ensemble methods have a very long history. They combine the outputs of several different models as a way to improve the prediction performance and robustness. However, this requires maintaining the K trained models and running each of them at test time (say, in order to average their outputs).
This approach thus quickly becomes infeasible for many applications with limited computational resources, especially in view of the ever-growing size of modern deep learning models.
The simplest way to fuse several parent networks into a single network of the same size is direct weight averaging, which we refer to as vanilla averaging; here for simplicity, we assume that all network architectures are identical. Unfortunately, neural networks are typically highly redundant in their parameterizations, so that there is no one-to-one correspondence between the weights of two different neural networks, even if they would describe the same function of the input. In practice, vanilla averaging is known to perform very poorly on trained networks whose weights differ non-trivially.
Finally, a third way to combine two models is distillation, where one network is retrained on its training data, while jointly using the output predictions of the other ‘teacher’ network on those
∗Work done while at EPFL. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
samples. Such a scenario is considered infeasible in our setting, as we aim for approaches not requiring the sharing of training data.This requirement is particularly crucial if the training data is to be kept private, like in federated learning applications, or is unavailable due to e.g. legal reasons.
Contributions. We propose a novel layer-wise approach of aligning the neurons and weights of several differently trained models, for fusing them into a single model of the same architecture.
Our method relies on optimal transport (OT) [1, 2], to minimize the transportation cost of neurons present in the layers of individual models, measured by the similarity of activations or incoming weights. The resulting layer-wise averaging scheme can be interpreted as computing the Wasserstein barycenter [3, 4] of the probability measures deﬁned at the corresponding layers of the parent models.
We empirically demonstrate that our method succeeds in the one-shot merging of networks of different weights, and in all scenarios signiﬁcantly outperforms vanilla averaging. More surprisingly, we also show that our method succeeds in merging two networks that were trained for slightly different tasks (such as using a different set of labels). The method is able to “inherit” abilities unique to one of the parent networks, while outperforming the same parent network on the task associated with the other network. Further, we illustrate how it can serve as a data-free and algorithm independent post-processing tool for structured pruning. Finally, we show that OT fusion, with mild ﬁne-tuning, can act as efﬁcient proxy for the ensemble, whereas vanilla averaging fails for more than two models.
Extensions and Applications.
The method serves as a new building block for enabling several use-cases: (1) The adaptation of a global model to personal training data. (2) Fusing the parameters of a bigger model into a smaller sized model and vice versa. (3) Federated or decentralized learning applications, where training data can not be shared due to privacy reasons or simply due to its large size.
In general, improved model fusion techniques such as ours have strong potential towards encouraging model exchange as opposed to data exchange, to improve privacy & reduce communication costs. 2