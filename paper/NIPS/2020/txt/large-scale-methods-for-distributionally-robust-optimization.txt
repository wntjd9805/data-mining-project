Abstract
We propose and analyze algorithms for distributionally robust optimization of convex losses with conditional value at risk (CVaR) and χ2 divergence uncertainty sets. We prove that our algorithms require a number of gradient evaluations independent of training set size and number of parameters, making them suitable for large-scale applications. For χ2 uncertainty sets these are the ﬁrst such guarantees in the literature, and for CVaR our guarantees scale linearly in the uncertainty level rather than quadratically as in previous work. We also provide lower bounds proving the worst-case optimality of our algorithms for CVaR and a penalized version of the χ2 problem. Our primary technical contributions are novel bounds on the bias of batch robust risk estimation and the variance of a multilevel Monte
Carlo gradient estimator due to Blanchet and Glynn [8]. Experiments on MNIST and ImageNet conﬁrm the theoretical scaling of our algorithms, which are 9–36 times more efﬁcient than full-batch methods. 1

Introduction
The growing role of machine learning in high-stakes decision-making raises the need to train reliable models that perform robustly across subpopulations and environments [11, 29, 70, 58, 36, 53, 39].
Distributionally robust optimization (DRO) [3, 66] shows promise as a way to address this challenge, with recent interest in both the machine learning community [68, 74, 22, 69, 34, 55] and in operations research [20, 3, 5, 27]. Yet while DRO has had substantial impact in operations research, a lack of scalable optimization methods has hindered its adoption in common machine learning practice.
In contrast to empirical risk minimization (ERM), which minimizes an expected loss ES∼P0 (cid:96)(x; S)
Rd with respect to a training distribution P0, DRO minimizes the expected loss with over x respect to the worst distribution in an uncertainty set (P0), that is, its goal is to solve
∈ X ⊂ minimize x∈X
L (x; P0) := sup
Q∈U (P0)
U
ES∼Q (cid:96)(x; S). (1)
The literature considers several uncertainty sets [3, 5, 7, 27], and we focus on two particular choices: (a) the set of distributions with bounded likelihood ratio to P0, so that becomes the conditional value at risk (CVaR) [59, 67], and (b) the set of distributions with bounded χ2 divergence to P0 [3, 16].
Some of our results extend to more general φ-divergence (or Rényi divergence) balls [72]. Minimizers of these objectives enjoy favorable statistical properties [22, 34], but ﬁnding them is more challenging than standard ERM. More speciﬁcally, stochastic gradient methods solve ERM with a number of (cid:96) computations independent of both N , the support size of P0 (i.e., number of data points), and
∇ d, the dimension of x (i.e., number of parameters). These guarantees do not directly apply to DRO because the supremum over Q in (1) makes cheap sampling-based gradient estimates biased. As a consequence, existing techniques for minimizing the χ2 objective [2, 20, 3, 5, 47, 22] have (cid:96)
∇
∗Equal contribution. Code is available on GitHub at https://github.com/daniellevy/fast-dro/.
L 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Objective (x; P0) =
L
Subgradient method
Dual SGM [Appendix A.3]
Subsampling [22]
Stoch. primal-dual [17, 47]
Ours
Lower Bound
CVaR at level α q(cid:62)(cid:96)(x) sup (cid:107)q(cid:107)∞≤ 1
αN
N (cid:15)−2
α−2(cid:15)−2
-N (cid:15)−2
α−1(cid:15)−2 (Thm. 2)
α−1(cid:15)−2 (Thm. 3)
χ2 constraint ρ q(cid:62)(cid:96)(x) sup
Dχ2 (q)≤ρ
N (cid:15)−2
-ρ2d(cid:15)−4
N ρ(cid:15)−2
ρ(cid:15)−3 (Thm. 4)
ρ(cid:15)−2 [22]
χ2 penalty λ q(cid:62)(cid:96)(x)
−
λDχ2(q) sup q∈∆N
N (cid:15)−2
λ−2(cid:15)−2
--λ−1(cid:15)−2 (Thm. 2)
λ−1(cid:15)−2 (Thm. 3)
Table 1. Number of ∇(cid:96) evaluations to obtain E[L(x; P0)]−inf x(cid:48)∈X L(x(cid:48); P0) ≤ (cid:15) when P0 is uniform on N training points. For simplicity we omit the Lipschitz constant of (cid:96), the size of the domain X , and logarithmic factors. We deﬁne (cid:96)i(x) := (cid:96)(x; Si) and Dχ2 (q) := N 2. The suprema are over q in the simplex. 2 (cid:107)q − 1
N 1(cid:107)2 evaluation complexity scaling linearly (or worse) in either N or d, which is prohibitive in large-scale applications.
In this paper, we consider the setting in which (cid:96) is a Lipschitz convex loss, a prototype case for stochastic optimization and machine learning [75, 49], and we propose methods for solving the problem (1) with (cid:96) complexity independent of sample size N and dimension d, and with optimal (linear) dependence on the uncertainty set size. In Table 1 we summarize their complexities and compare them to previous work. Each entry of the table shows the number of (sub)gradient evaluations to obtain a point with optimality gap (cid:15); for reference, recall that for ERM the stochastic subgradient method requires order (cid:15)−2 evaluations, independent of d and N . We discuss related work further in
Section 1.1 after outlining our approach.
∇ (x; n) = E
We begin our development in Section 3 by considering the surrogate objective (x; (cid:98)Pn) corresponding to the average empirical robust objective over random batches of size n sampled from
P0. In contrast to (1), it is straightforward to obtain unbiased gradient estimates for
—using the mini-(x; (cid:98)Pn)—and to optimize it efﬁciently with stochastic gradient methods. To obtain batch estimator
∇L guarantees for the true objective
. (x; n)
|
For CVaR we prove a bound scaling as 1/√n and extend it to other uncertainty sets, including
χ2 balls, via the Kusuoka representation [42]. Notably, for the penalty version of the χ2 objective (Table 1 right column) we prove a stronger bound scaling as 1/n. This analysis implies that, for large enough batch size n, an (cid:15)/2-minimizer of
. Furthermore, for CVaR and χ2 penalty we show that the variance of the gradient estimator decreases as 1/n, and we use
Nesterov acceleration to decrease the required number of gradient steps.
, we establish uniform bounds on the error is also an (cid:15)-minimizer of (x; P0)
− L
|L
L
L
L
L
L
L
To obtain stronger guarantees, in Section 4 we present a theoretically more efﬁcient multi-level Monte
Carlo (MLMC) [31, 32] gradient estimator which is a slight modiﬁcation of the general technique of Blanchet and Glynn [8]. The resulting estimator is unbiased for (x; n) but requires only a logarithmic number of samples in n. For CVaR and χ2 penalty we control the second moment of the gradient estimator, resulting in complexity bounds scaling with (cid:15)−2. We further prove that these rates are worst-case optimal up to logarithmic factors.
Unfortunately, direct application of the MLMC estimator for the χ2 uncertainty set (Table 1 center column) demonstrably fails for certain inputs. Instead, in Appendix E we optimize its Lagrange dual—the χ2 penalty—with respect to x and Lagrange multiplier λ. Using a doubling scheme on the
λ domain, we obtain a complexity guarantee scaling as (cid:15)−3.
∇L
Section 5 presents experiments where we use DRO to train linear models for digit classiﬁcation (on a mixture between MNIST [44] and typed digits [19]), and ImageNet [60]. To the best of our knowledge, the latter is the largest DRO problem solved to date. In both experiments DRO provides generalization improvements over ERM, and we show that our stochastic gradient estimators require far less
— than full-batch methods. Our experiments also reveal two facts that our theory only hints at. First, using the mini-batch gradient estimator the error (x; P0) becomes negligible even for batch sizes as
ﬂoor due to the difference between (cid:96) computations—between 9 (x; n) and and 40
∇
×
×
L
L 2
small as 10. Second, while the MLMC estimator avoids these error ﬂoors altogether, its increased variance makes it practically inferior to the mini-batch estimator with properly tuned batch size and learning rate. Our code implements our gradient estimators in PyTorch [56] and combines them seamlessly with the framework’s optimizers; we show an example code snippet in Appendix F.3. 1.1