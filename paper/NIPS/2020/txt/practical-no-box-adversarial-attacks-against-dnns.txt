Abstract
The study of adversarial vulnerabilities of deep neural networks (DNNs) has pro-gressed rapidly. Existing attacks require either internal access (to the architecture, parameters, or training set of the victim model) or external access (to query the model). However, both the access may be infeasible or expensive in many scenarios.
We investigate no-box adversarial examples, where the attacker can neither access the model information or the training set nor query the model. Instead, the attacker can only gather a small number of examples from the same problem domain as that of the victim model. Such a stronger threat model greatly expands the applicability of adversarial attacks. We propose three mechanisms for training with a very small dataset (on the order of tens of examples) and ﬁnd that prototypical reconstruction is the most effective. Our experiments show that adversarial examples crafted on prototypical auto-encoding models transfer well to a variety of image classiﬁcation and face veriﬁcation models. On a commercial celebrity recognition system held by clarifai.com, our approach signiﬁcantly diminishes the average prediction accuracy of the system to only 15.40%, which is on par with the attack that trans-fers adversarial examples from a pre-trained Arcface model. Our code is publicly available at: https://github.com/qizhangli/nobox-attacks. 1

Introduction
The adversarial vulnerability of deep neural networks (DNNs) have been extensively studied over the past few years [46, 12, 35, 30, 3, 29, 1]. It has been demonstrated that an attacker is able to generate small, human-imperceptible perturbations to fool advanced DNN models to make incorrect decisions.
These attacks pose great threats to security-critical systems where DNNs are deployed and lead to increasing concerns about the robustness of DNNs.
Based on how much information the attacker knows, we can divide existing attacks into white-box and black-box settings. In black-box attacks, the attacker cannot access the architecture, the parameters, or the training data of the victim model. Early attempts for black-box adversarial attacks [33, 34] relied on the transferability of adversarial examples. They trained substitute architectures by querying the victim models. Recent progress considered gradient estimation [5, 20, 50, 21, 13, 6, 4] and boundary tracing [2].
Black-box attacks rely on querying the victim models, a.k.a., “oracles”. However, in many scenarios, such queries are either infeasible (e.g., the model API is inaccessible to the attacker) or are expensive in time or money. To overcome this limitation, we consider a stronger threat model where the attacker makes no query to the victim model (and also has no access to the model parameters or its training data). This was coined as the “no-box” threat model [5] but no practical attack has been studied to the best of our knowledge. We investigate such no-box attacks against DNNs on computer vision models.
Similar to some strong black-box attacks [34], we assume that the attacker can access neither a large
∗Work done during an internship at ByteDance
†Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
scale training data nor pre-trained models on it. Instead, she or he can collect only a small number of auxiliary examples (on the order of tens or less) from the same problem domain. Given this small sample size, it is challenging to obtain a substitute model using conventional supervised training.
Inspired by recent advances in unsupervised representation learning and distribution modeling [48, 41], we developed auto-encoders that can learn discriminative features given very little data, e.g., 20 images from 2 classes. We investigated three training mechanisms by (a) estimating the front view of each rotated image, (b) estimating the best ﬁt of each possible jigsaw puzzle, and (c) constructing prototypical images, respectively. They entail discriminative and, more importantly, generalizable feature representations. We evaluated our approach on two computer vision tasks: image classiﬁcation and face veriﬁcation. Our experiments show that adversarial examples crafted on such auto-encoding models transfer well to a variety of open-source victim models, and their effectiveness is sometimes even on par with those crafted using pre-trained models trained on the same large-scale data set as the victim models. On a celebrity recognition system hosted by clarifai.com, our approach reduced the prediction accuracy of the system from 100.00% to only 15.40%, using only 10 facial images for training and crafting each adversarial example. We also studied the quality of the generated adversarial example in such a way, and we showed in the supplementary materials that the generated no-box adversarial examples were intrinsically different from the adversarial examples generated in the white-box/black-box settings, which probably worth further exploring. 2