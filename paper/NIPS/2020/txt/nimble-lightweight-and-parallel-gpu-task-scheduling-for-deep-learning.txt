Abstract
Deep learning (DL) frameworks take advantage of GPUs to improve the speed of DL inference and training. Ideally, DL frameworks should be able to fully uti-lize the computation power of GPUs such that the running time depends on the amount of computation assigned to GPUs. Yet, we observe that in scheduling GPU tasks, existing DL frameworks suffer from inefﬁciencies such as large scheduling overhead and unnecessary serial execution. To this end, we propose Nimble, a DL execution engine that runs GPU tasks in parallel with minimal scheduling over-head. Nimble introduces a novel technique called ahead-of-time (AoT) scheduling.
Here, the scheduling procedure ﬁnishes before executing the GPU kernel, thereby removing most of the scheduling overhead during run time. Furthermore, Nimble automatically parallelizes the execution of GPU tasks by exploiting multiple GPU streams in a single GPU. Evaluation on a variety of neural networks shows that compared to PyTorch, Nimble speeds up inference and training by up to 22.34 and 3.61 systems, TensorRT and TVM, by up to 2.81
×
, respectively. Moreover, Nimble outperforms state-of-the-art inference
, respectively. and 1.70
×
×
× 1

Introduction
In recent years, growing demands for deep learning (DL) have facilitated the advance of DL frame-works such as Caffe2 [23], MXNet [13], PyTorch [27], and TensorFlow [9]. These frameworks provide implementations of GPU-based neural network computations along with high-level APIs, with which users can express the semantics of neural networks as usual Python programs. Further-more, such frameworks allow users to describe the training and inference procedure of their networks without the need to control GPUs directly. DL frameworks then automatically handle GPU intricacies such as copying neural network weights to GPUs and launching DL operators on GPUs. Operators indicate numerical computations, like convolution and batch normalization, and consist of one or more GPU tasks (i.e., GPU kernels and GPU memory operations).
Before a GPU processes a task, DL frameworks must ﬁrst go through a series of preparation steps (GPU task scheduling), and then submit the task to the GPU (GPU task submission). We note that current DL frameworks conduct GPU task scheduling during run time. For instance, TensorFlow,
Caffe2, and MXNet represent a neural network as a computation graph of DL operators, and schedule the GPU tasks of an operator at run time once the operator’s dependencies are met. Meanwhile, for PyTorch and TensorFlow Eager [10], GPU tasks are scheduled at run time as Python code is interpreted line by line.
∗First two authors have equal contribution
†Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
While under ideal circumstances the running time of neural networks mostly depends on the amount of computation assigned to GPUs, in reality we ﬁnd otherwise. We point out two important problems in run-time task scheduling that may signiﬁcantly limit framework performance. First, the time spent on scheduling, which we call scheduling overhead, can take a substantial portion of the overall running time. Although the scheduling overhead is negligible when the running time of a GPU task is sufﬁciently long enough to hide the overhead, we ﬁnd that this does not hold in many cases, especially when inference and training of a neural network consist of small and short GPU tasks.
Modern GPUs [2, 4] have thousands of computation units along with specialized processors like
Tensor Core [5], and use high bandwidth memory [16] to avoid bottlenecks from memory bandwidth.
While the time spent on running GPU tasks can dramatically be reduced by such GPUs, we observe that the scheduling overhead is constantly imposed by every GPU task, and often dominates the running time of DL inference and training.
Another problem DL frameworks face is that serial execution of GPU tasks misses the opportunity to further improve performance by parallelizing task execution. Recent neural networks exhibit inter-operator level parallelism. For example, topologies of the neural networks obtained by neural architecture search (NAS) [11, 12, 25, 28, 29, 39] are directed acyclic graphs (DAGs) with multiple branches rather than linear chains. In addition, recent works have proposed new types of layers that consist of smaller operators arranged in parallel, such as MixConv [35] and Split-Attention [38] blocks. Leveraging inter-operator parallelism can lead to performance improvements in executing such neural networks, especially in the case of inference. However, existing DL frameworks [9, 13, 27] are designed and optimized to schedule GPU tasks to be executed one at a time, and thus hardly exploit inter-operator parallelism.
To address the above limitations, we present Nimble, a new DL execution engine that schedules
GPU tasks to run in parallel with minimal scheduling overhead. The key observation that drives the design of Nimble is that for static neural networks the behavior of a network is predetermined by its architecture. For both inference and training, DL frameworks run the exact same computation graph with the same shapes of inputs over and over again. Thus, we can leverage detailed information about the computation graph and the input shape to optimize the scheduling of GPU tasks.
To avoid the scheduling overhead, Nimble introduces a novel ahead-of-time (AoT) scheduling tech-nique. Nimble schedules GPU tasks for a given neural network execution ahead of time; later when
Nimble is given an input, Nimble skips scheduling and proceeds immediately to task submission.
Since the preparation steps of GPU tasks are invariant to each neural network execution (i.e., inde-pendent of the input values), we only need to perform task scheduling once. While Nimble’s AoT scheduler performs GPU task scheduling, it records a trace of GPU tasks and GPU memory requests, and generates a task schedule. The task schedule contains all information and resources (i.e., result of the scheduling) required for the execution of the neural network, including the submission order between GPU tasks, function arguments for the GPU tasks, and how to run GPU tasks in parallel. At run time, Nimble substitutes the high-overhead scheduling procedure by the raw submission of GPU tasks based on the task schedule, dramatically reducing the scheduling overhead.
To execute multiple GPU tasks in parallel on a GPU, Nimble employs automatic multi-stream execution. Although the CUDA programming interface provides Stream API for concurrent kernel execution [1], assigning neural network operators to appropriate streams is a difﬁcult task for users.
Nimble automates the stream assignment and synchronization process. Before AoT scheduling,
Nimble analyzes dependency relationships between operators and ﬁnds an optimal stream assignment that guarantees the smallest number of synchronizations across streams while parallelizing as many operators as possible. Given the operator-to-stream mapping, Nimble rewrites the computation graph of the given neural network to run the GPU tasks of the operators on their corresponding streams with proper synchronizations. The modiﬁed graph is then used as an input to the AoT scheduler, which in turn embeds the information about the stream mapping and synchronization in the task schedule.
Nimble is built on top of PyTorch and supports both inference and training of neural networks.
Users can seamlessly apply Nimble to their PyTorch programs by wrapping DL model instances in
Nimble objects. Our evaluation on a variety of deep neural networks shows that Nimble improves the speed of inference and training by up to 22.34 compared to PyTorch, respectively.
×
Moreover, Nimble outperforms state-of-the-art inference systems, TensorRT [3] and TVM [14], by
, respectively. Nimble is publicly available at https://github.com/ up to 2.81 snuspl/nimble. and 1.70 and 3.61
×
×
× 2
Figure 1: GPU task scheduling in DL frameworks that build a computation graph for DL execution. 2