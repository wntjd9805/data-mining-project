Abstract
Our understanding of learning input-output relationships with neural nets has improved rapidly in recent years, but little is known about the convergence of the underlying representations, even in the simple case of linear autoencoders (LAEs). We show that when trained with proper regularization, LAEs can directly learn the optimal representation – ordered, axis-aligned principal components. We analyze two such regularization schemes: non-uniform (cid:96)2 regularization and a deterministic variant of nested dropout [24]. Though both regularization schemes converge to the optimal representation, we show that this convergence is slow due to ill-conditioning that worsens with increasing latent dimension. We show that the inefﬁciency of learning the optimal representation is not inevitable – we present a simple modiﬁcation to the gradient descent update that greatly speeds up convergence empirically.1 1

Introduction
While there has been rapid progress in understanding the learning dynamics of neural networks, most such work focuses on the networks’ ability to ﬁt input-output relationships. However, many machine learning problems require learning representations with general utility. For example, the representations of a pre-trained neural network that successfully classiﬁes the ImageNet dataset [6] may be reused for other tasks. It is difﬁcult in general to analyze the dynamics of learning represen-tations, as metrics such as training and validation accuracy reveal little about them. Furthermore, analysis through the Neural Tangent Kernel shows that in some settings, neural networks can learn input-output mappings without ﬁnding meaningful representations [11].
In some special cases, the optimal representations are known, allowing us to analyze representation learning exactly. In this paper, we focus on linear autoencoders (LAE). With specially chosen regularizers or update rules, their optimal weight representations consist of ordered, axis-aligned principal directions of the input data.
It is well known that the unregularized LAE ﬁnds solutions in the principal component spanning subspace [3], but in general, the individual components and corresponding eigenvalues cannot be recovered. This is because any invertible linear transformation and its inverse can be inserted between the encoder and the decoder without changing the loss. Kunin et al. [17] showed that applying (cid:96)2 regularization on the encoder and decoder reduces the symmetry of the stationary point solutions to the group of orthogonal transformations. The individual principal directions can then be recovered by applying the singular value decomposition (SVD) to the learned decoder weights.
We investigate how, with appropriate regularization, gradient-based optimization can further break the symmetry, and directly learn the individual principal directions. We analyze two such regularization schemes: non-uniform (cid:96)2 regularization and a deterministic variant of nested dropout [24]. 1The code is available at https://github.com/XuchanBao/linear-ae 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The ﬁrst regularization scheme we analyze applies non-uniform (cid:96)2 regularization on the weights connected to different latent dimen-sions. We show that at any global minimum, an LAE with non-uniform (cid:96)2 regularization di-rectly recovers the ordered, axis-aligned princi-pal components. We analyze the loss landscape and show that all local minima are global min-ima. The second scheme is nested dropout [24], which is already known to recover the individual principal components in the linear case.
After establishing two viable models, we ask: how fast can a gradient-based optimizer, such as gradient descent, ﬁnd the correct representa-tion? In principle, this ought to be a simple task once the PCA subspace is found, as an SVD on this low dimensional latent space can recover the correct alignment of the principal directions.
However, we ﬁnd that gradient descent applied to either aforementioned regularization scheme converges very slowly to the correct represen-tation, even though the reconstruction error quickly decreases. To understand this phenomenon, we analyze the curvature of both objectives at their respective global minima, and show that these objectives cause ill-conditioning that worsens as the latent dimension is increased. Furthermore, we note that this ill-conditioning is nearly invisible in the training or validation loss, analogous to the general difﬁculty of measuring representation learning for practical nonlinear neural networks. The ill-conditioned loss landscape for non-uniform (cid:96)2 regularization is illustrated in Figure 1.
Figure 1: Visualization of the loss surface of an
LAE with non-uniform (cid:96)2 regularization, plotted for a 2D subspace that includes a global optimal so-lution. The narrow valley along the rotation direc-tion causes slow convergence. Detailed discussion can be found in Section 4.3.
While the above results might suggest that gradient-based optimization is ill-suited for efﬁciently recovering the principal components, we show that this is not the case. We propose a simple iterative learning rule that recovers the principal components much faster than the previous methods. The gradient is augmented with a term that explicitly accounts for “rotation” of the latent space, and thus achieves a much stronger notion of symmetry breaking than the regularized objectives.
Our main contributions are as follows. 1) We characterize all stationary points of the non-uniform (cid:96)2 regularized objective, and prove it recovers the optimal representation at global minima (Sec-tion 4.1, 4.2). 2) Through analysis of Hessian conditioning, we explain the slow convergence of the non-uniform (cid:96)2 regularized LAE to the optimal representation (Section 4.3). 3) We derive a determin-istic variant of nested dropout and explain its slow convergence with similar Hessian conditioning analysis (Section 5). 4) We propose an update rule that directly accounts for latent space rotation (Section 6). We prove that the gradient augmentation term globally drives the representation to be axis-aligned, and the update rule has local linear convergence to the optimal representation. We empirically show that this update rule accelerates learning the optimal representation. 2 Preliminaries
We consider linear models consisting of two weight matrices: an encoder W1 ∈ Rk×m and decoder
W2 ∈ Rm×k (with k < m). The model learns a low-dimensional embedding of the data X ∈ Rm×n (which we assume is zero-centered without loss of generality) by minimizing the objective,
L(W1, W2; X) = 1 n
||X − W2W1X||2
F (1) 1 > · · · > σ2 k > 0 are the k largest eigenvalues of 1
We will assume σ2 n XX (cid:62). The assumption that the σ1, . . . , σk are positive and distinct ensures identiﬁability of the principal components, and is common in this setting [17]. Let S = diag(σ1, . . . , σk). The corresponding eigenvectors are the columns of U ∈ Rm×k. Principal Component Analysis (PCA) [22] provides a unique optimal solution to this problem that can be interpreted as the projection of the data along columns of U , up to sign changes to the projection directions. However, the minima of (1) are not unique in general
[17]. In fact, the objective is invariant under the transformation (W1, W2) (cid:55)→ (AW1, W2A−1), for any invertible matrix A ∈ Rk×k. 2
Regularized linear autoencoders. Kunin et al. [17] provide a theoretical analysis of (cid:96)2-regularized linear autoencoders, where the objective is as follows,
Lλ(W1, W2; X) = L(W1, W2; X) + λ(cid:107)W1(cid:107)2
F + λ(cid:107)W2(cid:107)2
F . (2)
Kunin et al. [17] proved that the set of globally optimal solutions to objective 2 exhibit only an orthogonal symmetry through the mapping: (W1, W2) (cid:55)→ (OW1, W2O(cid:62)), for orthogonal matrix O. 3