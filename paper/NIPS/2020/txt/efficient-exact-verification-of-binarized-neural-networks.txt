Abstract
Concerned with the reliability of neural networks, researchers have developed veri-ﬁcation techniques to prove their robustness. Most veriﬁers work with real-valued networks. Unfortunately, the exact (complete and sound) veriﬁers face scalability challenges and provide no correctness guarantees due to ﬂoating point errors. We argue that Binarized Neural Networks (BNNs) provide comparable robustness and allow exact and signiﬁcantly more efﬁcient veriﬁcation. We present a new system, EEV, for efﬁcient and exact veriﬁcation of BNNs. EEV consists of two parts: (i) a novel SAT solver that speeds up BNN veriﬁcation by natively handling the reiﬁed cardinality constraints arising in BNN encodings; and (ii) strategies to train solver-friendly robust BNNs by inducing balanced layer-wise sparsity and low cardinality bounds, and adaptively cancelling the gradients. We demonstrate the ef-fectiveness of EEV by presenting the ﬁrst exact veriﬁcation results for (cid:96)∞-bounded adversarial robustness of nontrivial convolutional BNNs on the MNIST and CI-FAR10 datasets. Compared to exact veriﬁcation of real-valued networks of the same architectures on the same tasks, EEV veriﬁes BNNs hundreds to thousands of times faster, while delivering comparable veriﬁable accuracy in most cases. 1

Introduction
Deep Neural Networks (DNNs) have achieved impressive success in many applications including image understanding, speech recognition, natural language processing, and game playing [26]. Unfor-tunately, DNNs exhibit unexpected or potentially dangerous behavior due to limited robustness [56].
In response, researchers have developed techniques that attempt to verify that a DNN satisﬁes a robustness speciﬁcation [34, 20, 57]. However, scalability is a formidable challenge for exact veriﬁers [10]. For example, MIPVerify [57] needs hundreds of seconds to verify the robustness of a network whose inference on CPU takes only a few milliseconds, even though the network is trained to be solver-friendly [61]. Additionally, ﬂoating point errors make any correctness guarantees effectively unobtainable for exact veriﬁcation of nontrivial real-valued neural networks [32].
Binarized Neural Networks (BNNs) comprise an attractive alternative to real-valued neural networks.
BNNs exhibit signiﬁcant speed gain and energy savings during inference [29, 47, 42] while achieving competitive accuracy on challenging datasets [7]. Because of the absence of ﬂoating point arithmetic,
BNNs can also support exact veriﬁcation [13, 43]. To date, however, BNN veriﬁcation has exhibited even worse scalability than real-valued neural network veriﬁcation [44].
We present a new system, EEV, for exact and efﬁcient veriﬁcation of BNNs. EEV incorporates a novel SAT solver tailored for BNN veriﬁcation and new training strategies that enhance the robustness and veriﬁcation efﬁciency of the trained BNNs. We use EEV to verify the robustness of BNNs against input perturbations bounded by the (cid:96)∞ norm. Our experimental results show that, compared to exact veriﬁcation of robustly trained real-valued networks with the same architectures, EEV delivers several 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
orders of magnitude faster veriﬁcation of BNNs with comparable veriﬁable accuracy in most cases.
This paper makes the following contributions: 1. We incorporate native support for reiﬁed cardinality constraints into a SAT solver, which improves the performance of BNN veriﬁcation by more than a factor of one hundred compared to an unmodiﬁed SAT solver (Section 4.2). 2. We identify that sparse weights induced by ternary quantization [44] cause unbalanced spar-sity between layers of convolutional networks, which leads to high veriﬁcation complexity despite sufﬁcient overall sparsity. We propose a new strategy, BinMask, which produces more balanced sparsity. Our system veriﬁes BNNs trained with BinMask two to ﬁve orders of magnitude faster than BNNs trained with ternary quantization (Section 5.1). 3. We further reduce the veriﬁcation complexity of BNNs by introducing a regularizer that induces lower cardinality bounds, which leads to an additional speedup of up to thousands of times (Section 5.2). 4. We ﬁnd that directly applying the PGD training algorithm [38] does not induce veriﬁably robust BNNs. We propose adaptive gradient cancelling to train robust BNNs (Section 6). 5. We present the ﬁrst exact veriﬁcation of robustness against (cid:96)∞-norm bounded input per-turbations of convolutional BNNs on MNIST and CIFAR10, and compare the results with real-valued networks. EEV veriﬁes exact robustness of BNNs between 338.28 to 2440.65 times faster than a state-of-the-art exact veriﬁer on real-valued networks with the same architecture, while delivering comparable veriﬁable accuracy in most cases (Table 4). 2