Abstract
Large-scale Markov decision processes (MDPs) require planning algorithms with runtime independent of the number of states of the MDP. We consider the planning problem in MDPs using linear value function approximation with only weak requirements: low approximation error for the optimal value function, and a small set of â€œcoreâ€ states whose features span those of other states. In particular, we make no assumptions about the representability of policies or value functions of non-optimal policies. Our algorithm produces almost-optimal actions for any state using a generative oracle (simulator) for the MDP, while its computation time scales polynomially with the number of features, core states, and actions and the eï¬€ective horizon. 1

Introduction
Markov decision processes (MDPs) are commonly used to model sequential decision making under uncertainty and have a wide range of applications [see 42, 33, 7, for example]. We consider planning in large-scale, expected discounted total reward MDPs. Computing an optimal policy in the discounted setting is known to require â€œreadingâ€ all states at least once [6]. As the state space for most interesting applications is intractably large if not inï¬nite (â€œBellmanâ€™s curse of dimensionalityâ€), it is common to consider restrictions to the problem that can allow eï¬ƒcient calculation of near-optimal actions.
One such relaxation is online planning â€” the MDP can be accessed through a simulator and we ask only for a good action at a given state [25, 29]. While in this problem the complexity of computing a â€œgood actionâ€ can be independent of the number of states, the complexity is exponential in the planning horizon [25]. An alternative idea, which can be traced back to at least the work of Schweitzer and Seidmann [35], is to assume that one has access to a feature representation (that is, a vector of features for each state) and the planner needs to work well for those MDPs where the optimal value function of the MDP can be uniformly well approximated over all states by some appropriate weighted combination of the features. Since an accurate approximation of the optimal value function is known to be suï¬ƒcient to generate near-optimal behavior,1 the problem simpliï¬es to producing a good estimate of the unknown feature weights with a computation cost that is independent of the number of states.
In this paper we consider the intersection of these two problem formulations. More precisely, our goal is to construct planning algorithms that produce an action for any given input state, using black-box access to the MDP through a simulator which takes a state and an action as input and produces a random next state and immediate reward. The planner can also access the feature representation of any state as a ğ‘‘-dimensional feature vector. Assume that the optimal value function of the MDP can be uniformly well approximated â€” to an accuracy of ğœ€approx â€” as a linear combination of the features with ï¬xed, but unknown, coeï¬ƒcients. Our goal is a randomized planning algorithm that interacts with 1See Proposition 1; or, for example, SzepesvÃ¡ri [39, Lemma 5.17], Kearns et al. [25, Lemma 5], Kallenberg
[24, Theorem 3.7]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the simulator poly(1/ğœ€, ğ‘‘, ğ», ğ´, . . . ) times to produce an action, such that following this action in every state results in an ğ‘‚ (ğœ€ + ğ‘ğœ€approx)-optimal policy. Here ğ» = 1/(1 âˆ’ ğ›¾) is the eï¬€ective planning horizon for the discount factor 0 â‰¤ ğ›¾ < 1, which is used in the deï¬nition of the values of policies; ğ´ is the number of actions; and ğ‘ > 0 is an error inï¬‚ation factor that may depend on ğ›¾, ğ‘‘, and ğ´.
We call the features â€œweakâ€ as we only require the optimal value function to be accurately representable by their linear combinations, in contrast to â€œstrongâ€ features that can accurately represent the value functions of all policies; in this latter case the problem of eï¬ƒcient planning in the presence of a simulator is known to have a solution, both in the episodic and discounted settings [43, 28]. As pointed out by Du et al. [12], with only weak features, the problem of eï¬ƒcient planning has not yet been solved.
Our Contributions We design a randomized algorithm that positively answers the challenge posed above under one extra assumption â€” that the feature vectors of all states lie within the convex hull of the feature vectors of a few selected â€œcore statesâ€ that the algorithm is given. In particular, we show that the query-complexity and runtime of our algorithm is polynomial in the relevant quantities and the number of core states, providing a partial positive answer to the previously open problem of eï¬ƒcient planning in the presence of weak features.
To achieve our result, we start from the approximate linear programming (ALP) approach where the value function is approximated using the feature vectors. Following Lakshminarayanan et al.
[27], we construct a relaxed ALP that drops all constraints except at the core states. In their work,
Lakshminarayanan et al. gave bounds on the error of the value function that is obtained from solving this relaxed ALP. The authors also suggested a way to turn this error bound into an eï¬ƒcient planning method, though without a detailed analysis. The main contribution of the present work is to ï¬ll this gap, in addition to simplifying, strengthening and streamlining the earlier results. In particular, we propose using a randomized saddle-point solver that substantially reduces the computational requirements compared to the procedure hinted at by [27].
Paper Organization The rest of the paper is organized as follows: Sections 1.1 and 1.2 give background on MDPs and introduce the linear programming (LP) approach to planning. Section 2 formally deï¬nes the problem. Then, in Section 3, we present the linear program that we start with and give our ï¬rst results, bounding the value loss of the policy that can be read out from optimal solutions of the linear program. Section 4 gives the eï¬ƒcient algorithm to solve the linear program and our main result. Section 5 discusses related work. The paper is concluded in Section 6. The proofs of the results are moved to Appendix A in the Supplementary Material.
Notation The set of real numbers is denoted by R, whereas R+ = [0, âˆ). Rğ‘‘ denotes the vectors with ğ‘‘ dimensions, while the ğ‘š Ã— ğ‘› matrices are Rğ‘šÃ—ğ‘›. We use bold letters for vectors (ğ’“) and bold capitals for matrices (ğ‘·); their elements are written as ğ‘Ÿğ‘– and ğ‘ƒğ‘–, ğ‘— and matrix rows are ğ‘·ğ‘–. For vectors of identical dimension, ğ’™ â‰¤ ğ’š means element-wise comparison: ğ‘¥ğ‘– â‰¤ ğ‘¦ğ‘– for each index ğ‘–. The standard basis vector ğ’†ğ‘– has ğ‘’ğ‘–,ğ‘– = 1 and ğ‘’ğ‘–, ğ‘— = 0 for ğ‘– â‰  ğ‘—, and the constant 0 or 1 vector is denoted by 0, 1 âˆˆ Rğ‘‘; their dimension depends on the context. All vectors are considered column vectors by default. The probability simplex over any ï¬nite set A is denoted Î”A â‰” { ğ’‘ âˆˆ R | A |
| (cid:107) ğ‘(cid:107)1 = 1}. For a
ï¬nite set S with cardinality ğ‘† = |S|, we will think of functions ğ‘£ : S â†’ R or ğœ‘ : S â†’ Rğ‘‘ as vectors or matrices, respectively, and use both notations: for example, ğ’— âˆˆ Rğ‘†, ğ‘£ğ‘ , or ğ‘£(ğ‘ ); and ğš½ âˆˆ Rğ‘†Ã—ğ‘‘,
ğ‹ğ‘ , or ğ‹(ğ‘ ) where ğ‘  âˆˆ S. When the domain takes the form S Ã— A with respective cardinalities ğ‘† and
ğ´, we use intuitive double indices of the form ğ‘ ğ‘, e.g., with ğ‘Ÿ : S Ã— A â†’ R we index components of
ğ’“ using the notation ğ‘Ÿğ‘ ğ‘ (i.e., ğ‘Ÿğ‘ ğ‘ = ğ‘Ÿ (ğ‘ , ğ‘)). In this case we also write ğ’“ âˆˆ Rğ‘† ğ´.
For convenience, an Index of Notation section is included in the Supplementary Material.
+ 1.1