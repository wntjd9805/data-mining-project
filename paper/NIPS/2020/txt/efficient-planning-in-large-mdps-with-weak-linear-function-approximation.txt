Abstract
Large-scale Markov decision processes (MDPs) require planning algorithms with runtime independent of the number of states of the MDP. We consider the planning problem in MDPs using linear value function approximation with only weak requirements: low approximation error for the optimal value function, and a small set of “core” states whose features span those of other states. In particular, we make no assumptions about the representability of policies or value functions of non-optimal policies. Our algorithm produces almost-optimal actions for any state using a generative oracle (simulator) for the MDP, while its computation time scales polynomially with the number of features, core states, and actions and the eﬀective horizon. 1

Introduction
Markov decision processes (MDPs) are commonly used to model sequential decision making under uncertainty and have a wide range of applications [see 42, 33, 7, for example]. We consider planning in large-scale, expected discounted total reward MDPs. Computing an optimal policy in the discounted setting is known to require “reading” all states at least once [6]. As the state space for most interesting applications is intractably large if not inﬁnite (“Bellman’s curse of dimensionality”), it is common to consider restrictions to the problem that can allow eﬃcient calculation of near-optimal actions.
One such relaxation is online planning — the MDP can be accessed through a simulator and we ask only for a good action at a given state [25, 29]. While in this problem the complexity of computing a “good action” can be independent of the number of states, the complexity is exponential in the planning horizon [25]. An alternative idea, which can be traced back to at least the work of Schweitzer and Seidmann [35], is to assume that one has access to a feature representation (that is, a vector of features for each state) and the planner needs to work well for those MDPs where the optimal value function of the MDP can be uniformly well approximated over all states by some appropriate weighted combination of the features. Since an accurate approximation of the optimal value function is known to be suﬃcient to generate near-optimal behavior,1 the problem simpliﬁes to producing a good estimate of the unknown feature weights with a computation cost that is independent of the number of states.
In this paper we consider the intersection of these two problem formulations. More precisely, our goal is to construct planning algorithms that produce an action for any given input state, using black-box access to the MDP through a simulator which takes a state and an action as input and produces a random next state and immediate reward. The planner can also access the feature representation of any state as a 𝑑-dimensional feature vector. Assume that the optimal value function of the MDP can be uniformly well approximated — to an accuracy of 𝜀approx — as a linear combination of the features with ﬁxed, but unknown, coeﬃcients. Our goal is a randomized planning algorithm that interacts with 1See Proposition 1; or, for example, Szepesvári [39, Lemma 5.17], Kearns et al. [25, Lemma 5], Kallenberg
[24, Theorem 3.7]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the simulator poly(1/𝜀, 𝑑, 𝐻, 𝐴, . . . ) times to produce an action, such that following this action in every state results in an 𝑂 (𝜀 + 𝑐𝜀approx)-optimal policy. Here 𝐻 = 1/(1 − 𝛾) is the eﬀective planning horizon for the discount factor 0 ≤ 𝛾 < 1, which is used in the deﬁnition of the values of policies; 𝐴 is the number of actions; and 𝑐 > 0 is an error inﬂation factor that may depend on 𝛾, 𝑑, and 𝐴.
We call the features “weak” as we only require the optimal value function to be accurately representable by their linear combinations, in contrast to “strong” features that can accurately represent the value functions of all policies; in this latter case the problem of eﬃcient planning in the presence of a simulator is known to have a solution, both in the episodic and discounted settings [43, 28]. As pointed out by Du et al. [12], with only weak features, the problem of eﬃcient planning has not yet been solved.
Our Contributions We design a randomized algorithm that positively answers the challenge posed above under one extra assumption — that the feature vectors of all states lie within the convex hull of the feature vectors of a few selected “core states” that the algorithm is given. In particular, we show that the query-complexity and runtime of our algorithm is polynomial in the relevant quantities and the number of core states, providing a partial positive answer to the previously open problem of eﬃcient planning in the presence of weak features.
To achieve our result, we start from the approximate linear programming (ALP) approach where the value function is approximated using the feature vectors. Following Lakshminarayanan et al.
[27], we construct a relaxed ALP that drops all constraints except at the core states. In their work,
Lakshminarayanan et al. gave bounds on the error of the value function that is obtained from solving this relaxed ALP. The authors also suggested a way to turn this error bound into an eﬃcient planning method, though without a detailed analysis. The main contribution of the present work is to ﬁll this gap, in addition to simplifying, strengthening and streamlining the earlier results. In particular, we propose using a randomized saddle-point solver that substantially reduces the computational requirements compared to the procedure hinted at by [27].
Paper Organization The rest of the paper is organized as follows: Sections 1.1 and 1.2 give background on MDPs and introduce the linear programming (LP) approach to planning. Section 2 formally deﬁnes the problem. Then, in Section 3, we present the linear program that we start with and give our ﬁrst results, bounding the value loss of the policy that can be read out from optimal solutions of the linear program. Section 4 gives the eﬃcient algorithm to solve the linear program and our main result. Section 5 discusses related work. The paper is concluded in Section 6. The proofs of the results are moved to Appendix A in the Supplementary Material.
Notation The set of real numbers is denoted by R, whereas R+ = [0, ∞). R𝑑 denotes the vectors with 𝑑 dimensions, while the 𝑚 × 𝑛 matrices are R𝑚×𝑛. We use bold letters for vectors (𝒓) and bold capitals for matrices (𝑷); their elements are written as 𝑟𝑖 and 𝑃𝑖, 𝑗 and matrix rows are 𝑷𝑖. For vectors of identical dimension, 𝒙 ≤ 𝒚 means element-wise comparison: 𝑥𝑖 ≤ 𝑦𝑖 for each index 𝑖. The standard basis vector 𝒆𝑖 has 𝑒𝑖,𝑖 = 1 and 𝑒𝑖, 𝑗 = 0 for 𝑖 ≠ 𝑗, and the constant 0 or 1 vector is denoted by 0, 1 ∈ R𝑑; their dimension depends on the context. All vectors are considered column vectors by default. The probability simplex over any ﬁnite set A is denoted ΔA ≔ { 𝒑 ∈ R | A |
| (cid:107) 𝑝(cid:107)1 = 1}. For a
ﬁnite set S with cardinality 𝑆 = |S|, we will think of functions 𝑣 : S → R or 𝜑 : S → R𝑑 as vectors or matrices, respectively, and use both notations: for example, 𝒗 ∈ R𝑆, 𝑣𝑠, or 𝑣(𝑠); and 𝚽 ∈ R𝑆×𝑑,
𝝋𝑠, or 𝝋(𝑠) where 𝑠 ∈ S. When the domain takes the form S × A with respective cardinalities 𝑆 and
𝐴, we use intuitive double indices of the form 𝑠𝑎, e.g., with 𝑟 : S × A → R we index components of
𝒓 using the notation 𝑟𝑠𝑎 (i.e., 𝑟𝑠𝑎 = 𝑟 (𝑠, 𝑎)). In this case we also write 𝒓 ∈ R𝑆 𝐴.
For convenience, an Index of Notation section is included in the Supplementary Material.
+ 1.1