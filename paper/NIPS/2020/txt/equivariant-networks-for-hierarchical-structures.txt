Abstract
While using invariant and equivariant maps, it is possible to apply deep learning to a range of primitive data structures, a formalism for dealing with hierarchy is lacking.
This is a signiﬁcant issue because many practical structures are hierarchies of simple building blocks; some examples include sequences of sets, graphs of graphs, or multiresolution images. Observing that the symmetry of a hierarchical structure is the “wreath product” of symmetries of the building blocks, we express the equivariant map for the hierarchy using an intuitive combination of the equivariant linear layers of the building blocks. More generally, we show that any equivariant map for the hierarchy has this form. To demonstrate the effectiveness of this approach to model design, we consider its application in the semantic segmentation of point-cloud data. By voxelizing the point cloud, we impose a hierarchy of translation and permutation symmetries on the data and report state-of-the-art on SEMANTIC3D, S3DIS, and VKITTI, that include some of the largest real-world point-cloud benchmarks. 1

Introduction
In designing deep models for structured data, equivariance (invariance) of the model to transformation groups has proven to be a powerful inductive bias, which enables sample efﬁcient learning. A widely used family of equivariant deep models constrain the feed-forward layer so that speciﬁc transformations of the input lead to the corresponding transformations of the output. A canonical example is the convolution layer, in which the constrained MLP is equivariant to translation operations.
Many recent works have extended this idea to design equivariant networks for more exotic structures such as sets, exchangeable tensors and graphs, as well as relational and geometric structures.
This paper considers a nested hierarchy of such structures, or more generally, any hierarchical composition of transformation symmetries. These hierarchies naturally appear in many settings: for example, the interaction between nodes in a social graph may be a sequence or a set of events. Or in diffusion tensor imaging of the brain, each subject may be modeled as a set of sequences, where each sequence is a ﬁbre bundle in the brain. The application we consider in this paper models point clouds as 3D images, where each voxel is a set of points with coordinates relative to the center of that voxel.
To get an intuition for a hierarchy of symmetry transformations, consider the example of a sequence of sequences – e.g., a text document can be viewed as a sequence of sentences, where each sentence is itself a sequence of words. Here, each inner sequence as well as the outer sequence is assumed to possess an “independent” translation symmetry. Contrast this with symmetries of an image (2D translation), where all inner sequences (say row pixels) translate together, so we have a total of two translations. This is the key difference between the wreath product of two translation groups (former) and their direct product (latter). It is the wreath product that often appears in nested structures. As is evident from this example, the wreath product results in a signiﬁcantly larger set of transformations, and as we elaborate later, it provides a stronger inductive bias.
∗Currently at Borealis AI. Work done while at UBC. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Wreath product can express the symmetries of hierarchical structures: wreath product of three groups (cid:85) ≀ (cid:75) ≀ (cid:72) acting on the set of elements (cid:80) × (cid:81) × (cid:82), can be seen as independent copies of groups, (cid:72), (cid:75) and (cid:85) at different level of hierarchy acting ↻ on copies of (cid:80), (cid:81) and (cid:82). Intuitively, a linear map
W(cid:85)≀(cid:75)≀(cid:72) ∶ RP QR → RP QR equivariant to (cid:85) ≀ (cid:75) ≀ (cid:72), performs pooling over leaves under each inner node
, applies equivariant map for each inner structure (i.e., W(cid:85), W(cid:75) and W(cid:72) respectively), and broadcasting the output back to the leaves. A (cid:85) ≀ (cid:75) ≀ (cid:72)-equivariant map can be constructed as the sum of these three contributions, from equivariant maps at each level of hierarchy.
We are interested in application of equivariant/invariant deep learning to this type of nested structure.
The building blocks of equivariant and invariant MLPs are equivariant linear maps of the feedforward layer. We show that any equivariant linear map for the hierarchical structure is built using equivariant maps for the individual symmetry group at different levels of the hierarchy. Our construction only uses additional pooling and broadcasting operations; see Fig. 1.
In the following, after discussing related works in Section 2, we give a short background on equivariant
MLPs in Section 3. Section 4 starts by giving the closed form of equivariant maps for direct product of groups before moving to the more difﬁcult case of wreath product in Section 4.2. Finally, Section 5 applies this idea to impose a hierarchical structure on 3D point clouds. We show that the equivariant map for this hierarchical structure achieves state-of-the-art performance on the largest benchmark datasets for 3D semantic segmentation. 2