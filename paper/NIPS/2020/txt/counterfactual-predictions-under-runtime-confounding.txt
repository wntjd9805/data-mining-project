Abstract
Algorithms are commonly used to predict outcomes under a particular decision or intervention, such as predicting likelihood of default if a loan is approved. Gen-erally, to learn such counterfactual prediction models from observational data on historical decisions and corresponding outcomes, one must measure all factors that jointly affect the outcome and the decision taken. Motivated by decision support applications, we study the counterfactual prediction task in the setting where all relevant factors are captured in the historical data, but it is infeasible, undesirable, or impermissible to use some such factors in the prediction model. We refer to this setting as runtime confounding. We propose a doubly-robust procedure for learning counterfactual prediction models in this setting. Our theoretical analysis and experimental results suggest that our method often outperforms competing approaches. We also present a validation procedure for evaluating the performance of counterfactual prediction methods. 1

Introduction
Algorithmic tools are increasingly prevalent in domains such as health care, education, lending, criminal justice, and child welfare [4, 38, 18, 15, 9]. In many cases, the tools are not intended to replace human decision-making, but rather to distill rich case information into a simpler form, such as a risk score, to inform human decision makers [3, 11]. The type of information that these tools need to convey is often counterfactual in nature. Decision-makers need to know what is likely to happen if they choose to take a particular action. For instance, an undergraduate program advisor determining which students to recommend for a personalized case management program might wish to know the likelihood that a given student will graduate if enrolled in the program. In child welfare, case workers and their supervisors may wish to know the likelihood of positive outcomes for a family under different possible types of supportive service offerings.
A common challenge to developing valid counterfactual prediction models is that all the data available for training and evaluation is observational: the data reﬂects historical decisions and outcomes under those decisions rather than randomized trials intended to assess outcomes under different policies. If the data is confounded—that is, if there are factors not captured in the data that inﬂuenced both the outcome of interest and historical decisions—valid counterfactual prediction may not be possible. In this paper we consider the setting where all relevant factors are captured in the data, and so historical 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
decisions and outcomes are unconfounded, but where it is infeasible, undesirable, or impermissible to use some such factors in the prediction model. We refer to this setting as runtime confounding.
Runtime confounding naturally arises in a number of different settings. First, relevant factors may not yet be available at the desired runtime. For instance, in child welfare screening, call workers decide which allegations coming in to the child abuse hotline should be investigated based on the information in the call and historical administrative data [9]. The call worker’s decision-making process can be informed by a risk assessment if the call worker can access the risk score in real-time. Since existing case management software cannot run speech/NLP models in realtime, the call information (although recorded) is not available at runtime, thereby leading to runtime confounding.
Second, runtime confounding arises when historical decisions and outcomes have been affected by sensitive or protected attributes which for legal or ethical reasons are deemed ineligible as inputs to algorithmic predictions. We may for instance be concerned that call workers implicitly relied on race in their decisions, but it would not be permissible to include race as a model input. Third, runtime confounding may result from interpretability or simplicity requirements. For example, a university may require algorithmic tools used for case management to be interpretable. While information conveyed during student-advisor meetings is likely informative both of case management decisions and student outcomes, natural language processing models are not classically interpretable, and thus the university may wish instead to only use structured information like GPA in their tools.
In practice, when it is undesirable or impermissible to use particular features as model inputs at runtime, it is common to discard the ineligible features from the training process. This can induce considerable bias in the resulting prediction model when the discarded features are signiﬁcant confounders. To our knowledge, the problem of learning valid counterfactual prediction models under runtime confounding has not been considered in the prior literature, leaving practitioners without the tools to properly incorporate runtime-ineligible confounding features into the training process.
Contributions: Drawing upon techniques used in low-dimensional treatment effect estimation
[46, 52, 8], we propose a procedure for the full pipeline of learning and evaluating prediction models under runtime confounding. We (1) formalize the problem of counterfactual prediction with runtime confounding [§ 2]; (2) propose a solution based on doubly-robust techniques that has desirable theoretical properties [§ 3.3]; (3) theoretically and empirically compare this solution to an alternative counterfactually valid approach as well as the standard practice, describing the conditions under which we expect each to perform well [§ 3 & 5]; and (4) provide an evaluation procedure to assess performance of the methods in the real-world [§ 4]. Proofs, code and results of additional experiments are presented in the Supplement. 1.1