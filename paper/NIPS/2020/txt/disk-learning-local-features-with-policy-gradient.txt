Abstract
Local feature frameworks are difﬁcult to learn in an end-to-end fashion, due to the discreteness inherent to the selection and matching of sparse keypoints.
We introduce DISK (DIScrete Keypoints), a novel method that overcomes these obstacles by leveraging principles from Reinforcement Learning (RL), optimizing end-to-end for a high number of correct feature matches. Our simple yet expressive probabilistic model lets us keep the training and inference regimes close, while maintaining good enough convergence properties to reliably train from scratch. Our features can be extracted very densely while remaining discriminative, challenging commonly held assumptions about what constitutes a good keypoint, as showcased in Fig. 1, and deliver state-of-the-art results on three public benchmarks. 1

Introduction
Local features have been a key computer vision technology since the introduction of SIFT [20], en-abling applications such as Structure-from-Motion (SfM) [1, 15, 36], SLAM [27], re-localization [23], and many others. While not immune to the deep learning “revolution”, 3D reconstruction is one of the last bastions where sparse, hand-crafted solutions remain competitive with or outperform their dense, learned counterparts [37, 34, 16]. This is due to the difﬁculty of designing end-to-end methods with a differentiable training objective that corresponds well enough with the downstream task.
While patch descriptors can be easily learned on predeﬁned keypoints [38, 39, 25, 40, 13], joint detection and matching is harder to relax in a differentiable manner, due to its computational complexity. Given two images A and B with feature sets FA and FB, matching them is O(|FA|·|FB|).
As each image pixel may become a feature, the problem quickly becomes intractable. Moreover, the
“quality” of a given feature depends on the rest, because a feature that is very similar to others is less distinctive, and therefore less useful. This is hard to account for during training.
We address this issue by bridging the gap between training and inference to fully leverage the expressive power of CNNs. Our backbone is a network that takes images as input and outputs keypoint ‘heatmaps’ and dense descriptors. Discrete keypoints are sampled from the heatmap, and the descriptors at those locations are used to build a distribution over feature matches across images.
We then use geometric ground truth to assign positive or negative rewards to each match, and perform gradient descent to maximize the expected reward E (cid:80) r(i ↔ j), where MA↔B is the set of matches and r is per-match reward. In effect, this is a policy gradient method [44]. (i,j)∈MA↔B
Probabilistic relaxation is powerful for discrete tasks, but its applicability is limited by the fact that the expected reward and its gradients usually cannot be computed exactly. Therefore, noisy Monte
Carlo approximations have to be used instead, which harms convergence. We overcome this difﬁculty by careful modeling that yields analytical expressions for the gradients. As a result, we can beneﬁt from the expressiveness of policy gradient, narrowing the gap between training and inference and ultimately outperforming state-of-the-art methods, while still being able to train models from scratch. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Upright Root-SIFT [20, 2] 179k landmarks, 22.3 observations/landmark (b) DISK (ours) 190k landmarks, 30.0 observations/landmark
Figure 1: SIFT vs. DISK in SfM. We reconstruct “Sacre Coeur” from 1179 images [16] with
COLMAP. For Upright Root-SIFT (left) and DISK (right) we show a point cloud and one image with its keypoints. Landmarks, and their respective keypoints, are drawn in blue. Keypoints which do not create landmarks are drawn in red. Our features can be extracted (and create associations) on seemingly textureless regions where SIFT fails to, producing more landmarks with more observations.
Our contribution therefore is a novel, end-to-end-trainable approach to learning local features that relies on policy gradient. It yields considerably more accurate matches than earlier methods, and this results in better performance on downstream tasks, as illustrated in Fig. 1 and Sec. 4. 2