Abstract
A recent breakthrough in deep learning theory shows that the training of over-parameterized deep neural networks can be characterized by a kernel function called neural tangent kernel (NTK). However, it is known that this type of results does not perfectly match the practice, as NTK-based analysis requires the network weights to stay very close to their initialization throughout training, and cannot handle regularizers or gradient noises. In this paper, we provide a generalized neural tangent kernel analysis and show that noisy gradient descent with weight decay can still exhibit a “kernel-like” behavior. This implies that the training loss converges linearly up to a certain accuracy. We also establish a novel generalization error bound for two-layer neural networks trained by noisy gradient descent with weight decay. 1

Introduction
Deep learning has achieved tremendous practical success in a wide range of machine learning tasks
[21, 19, 34]. However, due to the nonconvex and over-parameterized nature of modern neural networks, the success of deep learning cannot be fully explained by conventional optimization and machine learning theory.
A recent line of work studies the learning of over-parameterized neural networks in the so-called
“neural tangent kernel (NTK) regime” [20]. It has been shown that the training of over-parameterized deep neural networks can be characterized by the training dynamics of kernel regression with the neural tangent kernel (NTK). Based on this, fast convergence rates can be proved for over-parameterized neural networks trained with randomly initialized (stochastic) gradient descent [16, 2, 15, 39, 40]. Moreover, it has also been shown that target functions in the NTK-induced reproducing kernel Hilbert space (RKHS) can be learned by wide enough neural networks with good generalization error [3, 4, 10].
Despite having beautiful theoretical results, the NTK-based results are known to have their limitations, for not perfectly matching the empirical observations in many aspects. Speciﬁcally, NTK-based analysis requires that the network weights stay very close to their initialization in the “node-wise” (cid:96)2 distance throughout the training. Moreover, due to this requirement, NTK-based analysis cannot handle regularizers such as weight decay, or large additive noises in the noisy gradient descent. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Given the advantages and disadvantages of the existing NTK-based results, a natural question is:
Is it possible to establish the NTK-type results under more general settings?
In this paper, we give an afﬁrmative answer to this question by utilizing a mean-ﬁeld analysis
[12, 28, 27, 37, 17] to study neural tangent kernel. We show that with appropriate scaling, two-layer neural networks trained with noisy gradient descent and weight decay can still enjoy the nice theoretical guarantees.
We summarize the contributions of our paper as follows:
• Our analysis demonstrates that neural network training with noisy gradient and appropriate regular-izers can still exhibit similar training dynamics as kernel methods, which is considered intractable in the neural tangent kernel literature, as the regularizer can easily push the network parameters far away from the initialization. Our analysis overcomes this technical barrier by relaxing the requirement on the closeness in the parameter space to the closeness in the distribution space. A direct consequence of our analysis is the linear convergence of noisy gradient descent up to certain accuracy for regularized neural network training. 1
• We establish generalization bounds for the neural networks trained with noisy gradient descent with weight decay regularization. Our result shows that the inﬁnitely wide neural networks trained by noisy gradient descent with weight decay can learn a class of functions that are deﬁned based on a bounded χ2-divergence to initialization distribution. Different from standard NTK-type generalization bounds [1, 3, 10], our result can handle explicit regularization. Moreover, our proof is based on an extension of the proof technique in Meir and Zhang [29] from discrete distributions to continuous distributions, which may be of independent interest.
Notation We use lower case letters to denote scalars, and use lower and upper case bold face letters to denote vectors and matrices respectively. For a vector x = (x1, . . . , xd)(cid:62) ∈ Rd, and any positive integer p, we denote the (cid:96)p norm of x as (cid:107)x(cid:107)p = (cid:0) (cid:80)d
. For a matrix
A = (Aij) ∈ Rm×n, we denote by (cid:107)A(cid:107)2 and (cid:107)A(cid:107)F its spectral and Frobenius norms respectively.
We also deﬁne (cid:107)A(cid:107)∞,∞ = max{|Aij| : 1 ≤ i ≤ m, 1 ≤ j ≤ n}. For a positive semi-deﬁnite matrix
A, we use λmin(A) to denote its smallest eigenvalue. i=1 |xi|p(cid:1)1/p
For a positive integer n, we denote [n] = {1, . . . , n}. We also use the following asymptotic notations.
For two sequences {an} and {bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤ Cbn. We also introduce (cid:101)O(·) to hide the logarithmic terms in the Big-O notations.
At last, for two distributions p and p(cid:48), we deﬁne the Kullback–Leibler divergence (KL-divergence) and χ2-divergence between p and p(cid:48) as follows:
DKL(p||p(cid:48)) = (cid:90) p(z) log p(z) p(cid:48)(z) dz, Dχ2(p||p(cid:48)) = (cid:90) (cid:18) p(z) p(cid:48)(z) (cid:19)2
− 1 p(cid:48)(z)dz. 2