Abstract
Multi-Source Domain Adaptation (MSDA) deals with the transfer of task knowl-edge from multiple labeled source domains to an unlabeled target domain, under a domain-shift. Existing methods aim to minimize this domain-shift using auxiliary distribution alignment objectives. In this work, we present a different perspective to MSDA wherein deep models are observed to implicitly align the domains under label supervision. Thus, we aim to utilize implicit alignment without additional training objectives to perform adaptation. To this end, we use pseudo-labeled target samples and enforce a classiﬁer agreement on the pseudo-labels, a process called
Self-supervised Implicit Alignment (SImpAl). We ﬁnd that SImpAl readily works even under category-shift among the source domains. Further, we propose classiﬁer agreement as a cue to determine the training convergence, resulting in a simple training algorithm. We provide a thorough evaluation of our approach on ﬁve benchmarks, along with detailed insights into each component of our approach. 1

Introduction
The task of supervised learning for classiﬁcation is based on the assumption that the training data and the testing data are sampled from the same distributions. Thus, supervised learning methods achieve state-of-the-art results when evaluated on popular benchmarks such as ImageNet [46]. However, when such models are deployed in real-world, they yield sub-optimal results due to the inherent distribution-shift (domain-shift [55]) between the training data and the real-world environment (a.k.a. the target domain). While it is possible to obtain unlabeled samples from the target domain in most cases, the huge costs of data annotation prohibit the creation of a reliable labeled training dataset. To this end, Unsupervised Domain Adaptation (DA) methods have been proposed that aim to transfer knowledge from a labeled "source" dataset to an unlabeled "target" dataset under a domain-shift.
A popular strategy in Unsupervised DA is to learn the task-speciﬁc knowledge using supervision from the labeled source dataset, while learning a domain-invariant latent space where the features across the source and the target domains align. Such an alignment is enforced using statistical discrepancy minimization schemes [1, 12, 39, 43, 54] or via an adversarial objective [11, 30, 57, 61, 66], or by employing domain-speciﬁc transformations [6, 26, 44]. This alignment minimizes the domain-shift in the latent space, and improves the target generalization. However, the performance of Single-Source
Domain Adaptation (SSDA) methods is usually determined by the choice of the source dataset [24].
Recently, Multi-Source Domain Adaptation (MSDA) [35, 67] has garnered interest wherein multiple labeled source domains are used to transfer the task knowledge to the unlabeled target domain. A common approach [15, 43, 61] is to learn a shared feature extractor, along with domain-speciﬁc classiﬁer modules (Fig. 1a), which yield an ensemble prediction for the target samples. However, an additional challenge in MSDA is to tackle the domain-shift and category-shift [61] between each pair of source-domains (Fig. 1b). To this end, auxiliary losses are enforced encouraging the model to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: An illustration of the proposed concept for two-class (c1, c2) classiﬁcation, with two labeled source domains (s1, s2) and one unlabeled target domain (t). Best viewed in color. (a) Architecture. Several works employ a shared feature extractor (f ), and source-speciﬁc classiﬁer modules. (b) Classical MSDA. Prior works employ source-speciﬁc classiﬁers that learn distinct (domain-speciﬁc) decision boundaries (denoted as DBi).
This results in a large discrepancy in the classiﬁer predictions (region shaded in yellow). Thus, an auxiliary feature alignment loss is required for improving the classiﬁer predictions. (c) Our approach. We enforce classiﬁer agreement on source-domain samples leading to an implicit alignment of latent features. Further, imposing an agreement on the pseudo-labeled target samples improves the generalization to the target domain. learn domain invariant but class-discriminative representations. Ultimately, an appropriate alignment of all the domains in the latent space [43] improves the generalization on the target domain (Fig. 1b).
In this work, we approach the MSDA problem from a different perspective. Since deep models are known to capture rich transferable representations [29, 38, 62], we ask, is an auxiliary feature alignment loss really necessary? The motivation stems from the observation that deep models exhibit a strong inductive bias to implicitly align the latent features under supervision. This is demonstrated in Fig. 2. Following the prior approaches [43, 61], we train domain-speciﬁc classiﬁers (Fig. 1b) and observe that the domains do not align in the latent space (Fig. 2a), which calls for an explicit feature alignment loss. However, when we enforce a classiﬁer agreement on the class label for each input instance (Fig. 2b), we ﬁnd that the domains tend to align, without requiring an explicit alignment loss.
This motivates us to further explore implicit alignment of latent features for MSDA. We aim to leverage the labeled data from multiple source domains, and the multi-classiﬁer setup (Fig. 1a) employed in MSDA to perform alignment, without incorporating auxiliary components such as a domain discriminator [61, 66]. In contrast to learning domain-speciﬁc classiﬁer modules, we enforce an agreement among the classiﬁers (Fig. 1c) to align the domains in the latent space.
Since the target domain is unlabeled, we resort to the class labels predicted by the model being trained (a.k.a. pseudo-labels [25]). The adaptation step encourages the classiﬁers to agree upon these pseudo-labels which enables alignment of the target features with the source features that have classiﬁer agreement owing to label supervision. Accordingly, we name the approach as Self-supervised
Implicit Alignment, abbreviated as SImpAl (pronounced "simple"). We observe that even under category-shift, implicit alignment can be leveraged to align the shared categories, without requiring additional components (e.g. ﬁne-grained alignment [5, 22, 42], adversarial discriminator [61]) or cumbersome training strategies (e.g. to handle arbitrary category-shifts [23, 61, 63]). We also ﬁnd that classiﬁer agreement can be leveraged as a cue to determine adaptation convergence.
To summarize, we demonstrate successful MSDA by leveraging implicit alignment exhibited by deep classiﬁers, corroborating the potential for designing simple and effective adaptation algorithms. We conduct extensive evaluation of our approach over ﬁve benchmark datasets, with two popular CNN backbone models (ResNet-50, ResNet-101 [16]) and derive insights from the empirical analysis. 2