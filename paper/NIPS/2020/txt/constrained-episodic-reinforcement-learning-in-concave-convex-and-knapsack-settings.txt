Abstract
We propose an algorithm for tabular episodic reinforcement learning (RL) with constraints. We provide a modular analysis with strong theoretical guarantees for two general settings. First is the convex-concave setting: maximization of a concave reward function subject to constraints that expected values of some vector quantities (such as the use of unsafe actions) lie in a convex set. Second is the knapsack setting: maximization of reward subject to the constraint that the total consumption of any of the speciﬁed resources does not exceed speciﬁed levels during the whole learning process. Previous work in constrained RL is limited to linear expectation constraints (a special case of convex-concave setting), or focuses on feasibility question, or on single-episode settings. Our experiments demonstrate that the proposed algorithm signiﬁcantly outperforms these approaches in constrained episodic benchmarks. 1

Introduction
Standard reinforcement learning (RL) approaches seek to maximize a scalar reward (Sutton and
Barto, 1998, 2018; Schulman et al., 2015; Mnih et al., 2015), but in many settings this is insufﬁcient, because the desired properties of the agent behavior are better described using constraints. For example, an autonomous vehicle should not only get to the destination, but should also respect safety, fuel efﬁciency, and human comfort constraints along the way (Le et al., 2019); a robot should not only fulﬁll its task, but should also control its wear and tear, for example, by limiting the torque exerted on its motors (Tessler et al., 2019). Moreover, in many settings, we wish to satisfy such constraints already during training and not only during the deployment. For example, a power grid, an autonomous vehicle, or a real robotic hardware should avoid costly failures, where the hardware is damaged or humans are harmed, already during training (Leike et al., 2017; Ray et al., 2020).
Constraints are also key in additional sequential decision making applications, such as dynamic pricing with limited supply (e.g., Besbes and Zeevi, 2009; Babaioff et al., 2015), scheduling of resources on a computer cluster (Mao et al., 2016), and imitation learning, where the goal is to stay close to an expert behavior (Syed and Schapire, 2007; Ziebart et al., 2008; Sun et al., 2019).
In this paper we study constrained episodic reinforcement learning, which encompasses all of these applications. An important characteristic of our approach, distinguishing it from previous work (e.g.,
Altman, 1999; Achiam et al., 2017; Tessler et al., 2019; Miryooseﬁ et al., 2019; Ray et al., 2020), is our focus on efﬁcient exploration, leading to reduced sample complexity. Notably, the modularity of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
our approach enables extensions to more complex settings such as (i) maximizing concave objectives under convex constraints, and (ii) reinforcement learning under hard constraints, where the learner has to stop when some constraint is violated (e.g., a car runs out of gas). For these extensions, which we refer to as concave-convex setting and knapsack setting, we provide the ﬁrst regret guarantees in the episodic setting (see related work below for a detailed comparison). Moreover, our guarantees are anytime, meaning that the constraint violations are bounded at any point during learning, even if the learning process is interrupted. This is important for those applications where the system continues to learn after it is deployed.
Our approach relies on the principle of optimism under uncertainty to efﬁciently explore. Our learning algorithms optimize their actions with respect to a model based on the empirical statistics, while optimistically overestimating rewards and underestimating the resource consumption (i.e., overestimating the distance from the constraint). This idea was previously introduced in multi-armed bandits (Agrawal and Devanur, 2014); extending it to episodic reinforcement learning poses additional challenges since the policy space is exponential in the episode horizon. Circumventing these challenges, we provide a modular way to analyze this approach in the basic setting where both rewards and constraints are linear (Section 3) and then transfer this result to the more complicated concave-convex and knapsack settings (Sections 4 and 5). We empirically compare our approach with the only previous works that can handle convex constraints and show that our algorithmic innovations lead to signiﬁcant empirical improvements (Section 6).