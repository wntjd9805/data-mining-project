Abstract
The idea of slicing divergences has been proven to be successful when comparing two probability measures in various machine learning applications including gener-ative modeling, and consists in computing the expected value of a ‘base divergence’ between one-dimensional random projections of the two measures. However, the topological, statistical, and computational consequences of this technique have not yet been well-established. In this paper, we aim at bridging this gap and derive various theoretical properties of sliced probability divergences. First, we show that slicing preserves the metric axioms and the weak continuity of the divergence, implying that the sliced divergence will share similar topological properties. We then precise the results in the case where the base divergence belongs to the class of integral probability metrics. On the other hand, we establish that, under mild condi-tions, the sample complexity of a sliced divergence does not depend on the problem dimension. We ﬁnally apply our general results to several base divergences, and illustrate our theory on both synthetic and real data experiments. 1

Introduction
Most inference methods in implicit generative modeling (IGM), such as generative adversarial networks [1] and variational auto-encoders [2], rely on the use of a particular divergence in order to be able to discriminate probability distributions. Recent advances in this ﬁeld have illustrated that the choice of this divergence is of crucial importance and can lead to very different practical and theoretical properties [3, 4, 5, 6, 7]. In this context, ‘sliced’ probability divergences, such as
Sliced-Wasserstein [8], or Sliced-Cramér [9], have become increasingly popular.
This slicing strategy has been essentially motivated by two main purposes. The ﬁrst purpose is that some probability divergences are only deﬁned to compare measures supported on one-dimensional spaces (e.g., Cramér distance, [10]); hence, the slicing operation allows the use of such divergences to multivariate distributions [11, 9]. The second purpose arises when the computational complexity of a divergence becomes excessive when comparing measures on high-dimensional spaces, but can efﬁciently be computed in the univariate case (e.g., the Wasserstein distance between one-dimensional distributions admits a closed-form analytical which can easily be approximated). The slicing operation then leverages these advantages originally available in one dimension to deﬁne divergences achieving computational efﬁciency on multivariate settings [8, 12, 13, 14, 15, 16].
∗Corresponding author: kimia.nadjahi@telecom-paris.fr 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Even though various sliced divergences have successfully been deployed in practical applications, their theoretical properties have not yet been well-understood. Existing results are largely restricted to the speciﬁc case of the Sliced-Wasserstein (SW) distance: it has been shown that SW satisﬁes the metric axioms [17], the convergence in SW is equivalent to the convergence in Wasserstein distance [18, 19], and the estimators obtained by minimizing SW are consistent [18]. Besides, some properties of SW have only been characterized for speciﬁc settings, in particular its statistical beneﬁts observed in practice [20, 12]. In this paper, we aim to bridge this gap by investigating the theoretical properties of sliced probability divergences from a general point of view: since such divergences are all characterized via the same slicing operation, we explore in depth the topological and statistical implications of this operation. Speciﬁcally, we consider a generic base divergence ∆ between one-dimensional probability measures, and deﬁne its sliced version, denoted by S∆, which operates on multivariate settings.
We ﬁrst establish several topological properties of S∆. Thanks to our general approach, our ﬁndings can directly be applied to any instance of sliced divergence, including those motivated by the two aforementioned purposes. Speciﬁcally, we show that slicing preserves the metric properties: if ∆ is a metric, so is S∆ (Proposition 1). We then focus on ﬁner topological properties of S∆ and show in
Theorem 1 that, if the convergence in ∆ implies the weak convergence of measures (or conversely), then slicing preserves this property, i.e. the convergence in S∆ implies the weak convergence of measures (or conversely). We also consider the case when ∆ is an integral probability metric [21] and identify sufﬁcient conditions for S∆ to be upper-bounded by ∆, which implies that S∆ induces a weaker topology (Theorem 2). Similarly, we identify sufﬁcient conditions such that ∆ and S∆ are strongly equivalent (Corollary 1), meaning that ∆ is upper- and lower-bounded by S∆.
Then, we derive the following statistical properties of S∆: we prove that the ‘sample complexity’ of S∆ is proportional to the sample complexity of ∆ for one-dimensional measures, and does not depend on the dimension d (Theorems 4, 5). This property explains why any S∆ motivated by the second purpose offers statistical beneﬁts when the original divergence suffers from the curse of dimensionality. However, this comes with a caveat: we show that, if one approximates the expectation over the random projections that appears in S∆ with a Monte Carlo average, which is the most common practice, then an additional variance term appears in the sample complexity and can limit the performance of S∆ in high dimensions (Theorem 6). Our results agree with the recent empirical observations reported in [12, 15] and provide a better understanding for them.
We illustrate all our theoretical ﬁndings on various examples, which demonstrate their applicability. In particular, our general topological analysis allows us to establish a novel result for the Sliced-Cramér distance. We also derive a sample complexity result for SW which has never been shown before, under different assumptions on the measures to be compared. We then consider Sinkhorn divergences
[22], whose sample complexity is known to have an exponential dependence on the dimension d and regularization parameter ε [23], and introduce its sliced version, referred to as the Sliced-Sinkhorn divergence. We prove that this new divergence has several merits: we derive its sample complexity by combining our general results with recent work [23, 24], and obtain rates that do not depend on d nor on ε. We also show that this divergence improves the worst-case computational complexity bounds of Sinkhorn divergences in Rd. Finally, we support our theory with numerical experiments on synthetic and real data. 2 Preliminaries and Technical