Abstract
Learning models of the environment from data is often viewed as an essential com-ponent to building intelligent reinforcement learning (RL) agents. The common practice is to separate the learning of the model from its use, by constructing a model of the environment’s dynamics that correctly predicts the observed state transitions. In this paper we argue that the limited representational resources of model-based RL agents are better used to build models that are directly useful for value-based planning. As our main contribution, we introduce the principle of value equivalence: two models are value equivalent with respect to a set of functions and policies if they yield the same Bellman updates. We propose a for-mulation of the model learning problem based on the value equivalence principle and analyze how the set of feasible solutions is impacted by the choice of policies and functions. Speciﬁcally, we show that, as we augment the set of policies and functions considered, the class of value equivalent models shrinks, until eventu-ally collapsing to a single point corresponding to a model that perfectly describes the environment. In many problems, directly modelling state-to-state transitions may be both difﬁcult and unnecessary. By leveraging the value-equivalence prin-ciple one may ﬁnd simpler models without compromising performance, saving computation and memory. We illustrate the beneﬁts of value-equivalent model learning with experiments comparing it against more traditional counterparts like maximum likelihood estimation. More generally, we argue that the principle of value equivalence underlies a number of recent empirical successes in RL, such as
Value Iteration Networks, the Predictron, Value Prediction Networks, TreeQN, and
MuZero, and provides a ﬁrst theoretical underpinning of those results. 1

Introduction
Reinforcement learning (RL) provides a conceptual framework to tackle a fundamental challenge in artiﬁcial intelligence: how to design agents that learn while interacting with the environment [36]. It has been argued that truly general agents should be able to learn a model of the environment that allows for fast re-planning and counterfactual reasoning [32]. Although this is not a particularly contentious statement, the question of how to learn such a model is far from being resolved. The common practice in model-based RL is to conceptually separate the learning of the model from its use. In this paper we argue that the limited representational capacity of model-based RL agents is better allocated if the future use of the model (e.g., value-based planning) is also taken into account during its construction [22, 15, 13].
Our primary contribution is to formalize and analyze a clear principle that underlies this new approach to model-based RL. Speciﬁcally, we show that, when the model is to be used for value-based planning, requirements on the model can be naturally captured by an equivalence relation induced by a set 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of policies and functions. This leads to the principle of value equivalence: two models are value equivalent with respect to a set of functions and a set of policies if they yield the same updates under corresponding Bellman operators. The policies and functions then become the mechanism through which one incorporates information about the intended use of the model during its construction.
We propose a formulation of the model learning problem based on the value equivalence principle and analyze how the set of feasible solutions is impacted by the choice of policies and functions.
Speciﬁcally, we show that, as we augment the set of policies and functions considered, the class of value equivalent models shrinks, until eventually collapsing to a single point corresponding to a model that perfectly describes the environment.
We also discuss cases in which one can meaningfully restrict the class of policies and functions used to tailor the model. One common case is when the construction of an optimal policy through value-based planning only requires that a model predicts a subset of value functions. We show that in this case the resulting value equivalent models can perform well under much more restrictive conditions than their traditional counterparts. Another common case is when the agent has limited representational capacity. We show that in this scenario it sufﬁces for a model to be value equivalent with respect to appropriately-deﬁned bases of the spaces of representable policies and functions.
This allows models to be found with less memory or computation than conventional model-based approaches that aim at predicting all state transitions, such as maximum likelihood estimation. We illustrate the beneﬁts of value-equivalent model learning in experiments that compare it against more conventional counterparts. More generally, we argue that the principle of value equivalence underlies a number of recent empirical successes in RL and provides a ﬁrst theoretical underpinning of those results [40, 34, 26, 16, 33]. 2