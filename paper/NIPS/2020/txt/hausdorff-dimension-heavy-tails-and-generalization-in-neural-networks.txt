Abstract
Despite its success in a wide range of applications, characterizing the generaliza-tion properties of stochastic gradient descent (SGD) in non-convex deep learning problems is still an important challenge. While modeling the trajectories of SGD via stochastic differential equations (SDE) under heavy-tailed gradient noise has re-cently shed light over several peculiar characteristics of SGD, a rigorous treatment of the generalization properties of such SDEs in a learning theoretical framework is still missing. Aiming to bridge this gap, in this paper, we prove generalization bounds for SGD under the assumption that its trajectories can be well-approximated by a Feller process, which deﬁnes a rich class of Markov processes that include several recent SDE representations (both Brownian or heavy-tailed) as its special case. We show that the generalization error can be controlled by the Hausdorff dimension of the trajectories, which is intimately linked to the tail behavior of the driving process. Our results imply that heavier-tailed processes should achieve better generalization; hence, the tail-index of the process can be used as a notion of
“capacity metric”. We support our theory with experiments on deep neural networks illustrating that the proposed capacity metric accurately estimates the generaliza-tion error, and it does not necessarily grow with the number of parameters unlike the existing capacity metrics in the literature. 1

Introduction
Many important tasks in deep learning can be represented by the following optimization problem, (cid:110) (cid:88)n min w∈Rd f (w) := 1 n where w ∈ Rd denotes the network weights, n denotes the number of training data points, f denotes a non-convex cost function, and f (i) denotes the cost incurred by a single data point. Gradient-based optimization algorithms, perhaps Stochastic gradient descent (SGD) being the most popular one, have been the primary algorithmic choice for attacking such optimization problems. Given an initial point w0, the SGD algorithm is based on the following recursion, f (i)(w) (1) i=1 (cid:111)
, wk+1 = wk − η∇ ˜fk(wk) with ∇ ˜fk(w) := 1
B (cid:88) i∈ ˜Bk
∇f (i)(w), (2) where η is the step-size, and ∇ ˜fk is the unbiased stochastic gradient with batch size B = | ˜Bk| for a random subset ˜Bk of {1, . . . , n} for all k ∈ N, | · | denoting cardinality.
In contrast to convex optimization setting where the behavior of SGD is fairly well-understood (see e.g. [DDB19, SSBD14]), the generalization properties of SGD in non-convex deep learning problems is an active area of research [PBL19, AZL19, AZLL19]. In the last decade, there has been considerable progress around this topic, where several generalization bounds have been proven in 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
different mathematical setups [NTS15, MWZZ17, Lon17, DR17, KL17, RRT17, ZLZ19, AZLL19,
NHD+19]. While these bounds are useful at capturing the generalization behavior of SGD in certain cases, they typically grow with dimension d, which contradicts empirical observations [NBMS17].
An important initial step towards developing a concrete generalization theory for the SGD algorithm in deep learning problems, is to characterize the statistical properties of the weights {wk}k∈N, as they might provide guidance for identifying the constituents that determine the performance of SGD. A popular approach for analyzing the dynamics of SGD, mainly borrowed from statistical physics, is based on viewing it as a discretization of a continuous-time stochastic process that can be described by a stochastic differential equation (SDE). For instance, if we assume that the gradient noise, i.e.,
∇ ˜fk(w) − ∇f (w) can be well-approximated with a Gaussian random vector, we can represent (2) as the Euler-Maruyama discretization of the following SDE, dWt = −∇f (Wt)dt + Σ(Wt)dBt, (3) where Bt denotes the standard Brownian motion in Rd, and Σ : Rd (cid:55)→ Rd×d is called the diffu-sion coefﬁcient. This approach has been adopted by several studies [MHB16, JKA+17, HLLL17,
CS18, ZWY+19]. In particular, based on the ‘ﬂat minima’ argument (cf. [HS97]), Jastrzebski et al.
[JKA+17] illustrated that the performance of SGD on unseen data correlates well with the ratio η/B.
More recently, Gaussian approximation for the gradient noise has been taken under investiga-tion. While Gaussian noise can accurately characterize the behavior of SGD for very large batch sizes [PSGN19], Simsekli et. al. [SSG19] empirically demonstrated that the gradient noise in fully connected and convolutional neural networks can exhibit heavy-tailed behavior in practical settings.
This characteristic was also observed in recurrent neural networks [ZKV+19]. Favaro et al. [FFP20] illustrated that the iterates themselves can exhibit heavy-tails and investigated the corresponding asymptotic behavior in the inﬁnite-width limit. Similarly, Martin and Mahoney [MM19] observed that the eigenspectra of the weight matrices in individual layers of a neural network can exhibit heavy-tails; hence, they proposed a layer-wise heavy-tailed model for the SGD iterates. By invoking results from heavy-tailed random matrix theory, they proposed a capacity metric based on a quantiﬁcation of the heavy-tails, which correlated well with the performance of the network on unseen data. Further, they empirically demonstrated that this capacity metric does not necessarily grow with dimension d.
Based on the argument that the observed heavy-tailed behavior of SGD1 in practice cannot be accurately represented by an SDE driven by a Brownian motion, Simsekli et al. [SSG19] proposed modeling SGD with an SDE driven by a heavy-tailed process, so-called the α-stable Lévy motion
[Sat99]. By using this framework and invoking metastability results proven in statistical physics
[IP06, Pav07], SGD is shown to spend more time around ‘wider minima’, and the time spent around those minima is linked to the tail properties of the driving process [SSG19, N ¸SGR19].
Even though the SDE representations of SGD have provided many insights on several distinguishing characteristics of this algorithm in deep learning problems, a rigorous treatment of their generalization properties in a statistical learning theoretical framework is still missing. In this paper, we aim to take a ﬁrst step in this direction and prove novel generalization bounds in the case where the trajectories of the optimization algorithm (including but not limited to SGD) can be well-approximated by a
Feller process [Sch16], which form a broad class of Markov processes that includes many important stochastic processes as a special case. More precisely, as a proxy to SGD, we consider the Feller process that is expressed by the following SDE: dWt = −∇f (Wt)dt + Σ1(Wt)dBt + Σ2(Wt)dLα(Wt) t
, (4) where Σ1, Σ2 are d × d matrix-valued functions, and Lα(·) denotes the state-dependent α-stable
Lévy motion, which will be deﬁned in detail in Section 2. Informally, Lα(·) can be seen as a heavy-tailed generalization of the Brownian motion, where α : Rd (cid:55)→ (0, 2]d denotes its state-dependent tail-indices. In the case αi(w) = 2 for all i and w, Lα(·) 2Bt whereas if αi gets smaller than 2, the process becomes heavier-tailed in the i-th component, whose tails asymptotically obey a power-law decay with exponent αi. The SDEs in [MHB16, JKA+17, HLLL17, CS18, ZWY+19] reduces to
√ t t t 1Very recently, Gurbuzbalaban et al. [GSZ20] and Hodgkinson and Mahoney [HM20] have simultaneously shown that the law of the SGD iterates (2) can indeed converge to a heavy-tailed stationary distribution with inﬁnite variance when the step-size η is large and/or the batch-size B is small. These results form a theoretical basis for the origins of the observed heavy-tailed behavior of SGD in practice. 2
all appear as a special case of (4) with Σ2 = 0, and the SDE proposed in [SSG19] corresponds to the isotropic setting: Σ2(w) is diagonal and αi(w) = α ∈ (0, 2] for all i, w. In (4), we allow each coordinate of Lα(·) to have a different tail-index which can also depend on the state Wt. We believe that Lα(·) provides a more realistic model based on the empirical results of [ ¸SGN+19], suggesting t that the tail index can have different values at each coordinate and evolve over time. t
At the core of our approach lies the fact that the sample paths of Markov processes often exhibit a fractal-like structure [Xia03], and the generalization error over the sample paths is intimately related to the ‘roughness’ of the random fractal generated by the driving Markov process, as measured by a notion called the Hausdorff dimension. Our main contributions are as follows. (i) We introduce a novel notion of complexity for the trajectories of a stochastic learning algorithm, which we coin as ‘uniform Hausdorff dimension’. Building on [Sch98], we show that the sample paths of Feller processes admit a uniform Hausdorff dimension, which is closely related to the tail properties of the process. (ii) By using tools from geometric measure theory, we prove that the generalization error can be controlled by the Hausdorff dimension of the process, which can be signiﬁcantly smaller than the standard Euclidean dimension. In this sense, the Hausdorff dimension acts as an ‘intrinsic dimension’ of the problem, mimicking the role of Vapnik-Chervonenkis (VC) dimension in classical generalization bounds.
These two contributions collectively show that heavier-tailed processes achieve smaller generalization error, implying that the heavy-tails of SGD incur an implicit regularization. Our results also provide a theoretical justiﬁcation to the observations reported in [MM19] and [SSG19]. Besides, a remarkable feature of the Hausdorff dimension is that it solely depends on the tail behavior of the process; hence, contrary to existing capacity metrics, it does not necessarily grow with the number of parameters d.
Furthermore, we provide an efﬁcient approach to estimate the Hausdorff dimension by making use of existing tail index estimators, and empirically demonstrate the validity of our theory on various neural networks. Experiments on both synthetic and real data verify that our bounds do not grow with the problem dimension, providing an accurate characterization of the generalization performance. 2 Technical