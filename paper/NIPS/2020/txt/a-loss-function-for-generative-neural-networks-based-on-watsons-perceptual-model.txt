Abstract
To train Variational Autoencoders (VAEs) to generate realistic imagery requires a loss function that reﬂects human perception of image similarity. We propose such a loss function based on Watson’s perceptual model, which computes a weighted distance in frequency space and accounts for luminance and contrast masking. We extend the model to color images, increase its robustness to translation by using the Fourier Transform, remove artifacts due to splitting the image into blocks, and make it differentiable. In experiments, VAEs trained with the new loss function generated realistic, high-quality image samples. Compared to using the Euclidean distance and the Structural Similarity Index, the images were less blurry; compared to deep neural network based losses, the new approach required less computational resources and generated images with less artifacts. 1

Introduction
Variational Autoencoders (VAEs) [11] are generative neural networks that learn a probability distri-bution over X from training data D = {x0, ..., xn} ⊂ X . New samples are generated by drawing a latent variable z ∈ Z from a distribution p(z) and using z to sample x ∈ X from a conditional decoder distribution p(x|z). The distribution of p(x|z) induces a similarity measure on X . A generic choice is a normal distribution p(x|z) = N (µx(z), σ2) with a ﬁxed variance σ2. In this case the 2σ2 kx − x′k2. Thus, the model assumes that for two underlying energy-function is L(x, x′) = 1 samples which are sufﬁciently close to each other (as measured by σ2), the similarity measure can be well approximated by the squared loss. The choice of L is crucial for the generative model. For image generation, traditional pixel-by-pixel loss metrics such as the squared loss are popular because of their simplicity, ease of use and efﬁciency [5]. However, they perform poorly at modeling the human perception of image similarity [30]. Most VAEs trained with such losses produce images that look blurred [3, 5]. Accordingly, perceptual loss functions for VAEs are an active research area. These loss functions fall into two broad categories, namely explicit models, as exempliﬁed by the Structural
Similarity Index Model (SSIM) [25], and learned models. The latter include models based on deep feature embeddings extracted from image classiﬁcation networks [5, 30, 8] as well as combinations of VAEs with discriminator networks of Generative Adversarial Networks (GANs) [4, 13, 18].
Perceptual loss functions based on deep neural networks have produced promising results. However, features optimized for one task need not be a good choice for a different task. Our experimental results suggest that powerful metrics optimized on speciﬁc datasets may not generalize to broader categories of images. We argue that using features from networks pre-trained for image classiﬁcation in loss functions for training VAEs for image generation may be problematic, because invariance properties beneﬁcial for classiﬁcation make it difﬁcult to capture details required to generate realistic images.
Code and experiments are available at github.com/SteffenCzolbe/PerceptualSimilarity 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Reference
Dist.	1
Dist.	2
Reference
Dist.	1
Dist.	2
Reference
Dist.	1
Dist.	2
Human
Watson-DFT
Baselines
Deep	Models
Figure 1: Similarity judgement of tested metrics on selected images (see 4.1 for details). The proposed Watson-DFT metric can model spatial variations, yet punishes image degradation through noise and graphic artifacts. Deep neural network based metrics pre-trained on classiﬁcation tasks are invariant to the image quality, leading to more artifacts when employed in generation tasks. We refer to Supplement F for additional random examples.
In this work, we introduce a loss function based on Watson’s visual perception model [27], an explicit perceptual model used in image compression and digital watermarking [15]. The model accounts for the perceptual phenomena of sensitivity, luminance masking, and contrast masking. It computes the loss as a weighted distance in frequency space based on a Discrete Cosine Transform (DCT). We optimize the Watson model for image generation by (i) replacing the DCT with the discrete Fourier
Transform (DFT) to improve robustness against translational shifts, (ii) extending the model to color images, (iii) replacing the ﬁxed grid in the block-wise computations by a randomized grid to avoid artifacts, and (iv) replacing the max operator to make the loss function differentiable. We trained the free parameters of our model and several competitors using human similarity judgement data ([30], see Figure 1 for examples). We applied the trained similarity measures to image generation of numerals and celebrity faces. The modiﬁed Watson model generalized well to the different image domains and resulted in imagery exhibiting less blur and far fewer artifacts compared to alternative approaches. 2