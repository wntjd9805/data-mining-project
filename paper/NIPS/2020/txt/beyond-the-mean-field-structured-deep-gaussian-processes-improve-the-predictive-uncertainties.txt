Abstract
Deep Gaussian Processes learn probabilistic data representations for supervised learning by cascading multiple Gaussian Processes. While this model family promises ﬂexible predictive distributions, exact inference is not tractable. Ap-proximate inference techniques trade off the ability to closely resemble the pos-terior distribution against speed of convergence and computational efﬁciency. We propose a novel Gaussian variational family that allows for retaining covariances between latent processes while achieving fast convergence by marginalising out all global latent variables. After providing a proof of how this marginalisation can be done for general covariances, we restrict them to the ones we empirically found to be most important in order to also achieve computational efﬁciency. We provide an efﬁcient implementation of our new approach and apply it to several benchmark datasets. It yields excellent results and strikes a better balance between accuracy and calibrated uncertainty estimates than its state-of-the-art alternatives. 1

Introduction
Gaussian Processes (GPs) provide a non-parametric framework for learning distributions over un-known functions from data [21]: As the posterior distribution can be computed in closed-form, they return well-calibrated uncertainty estimates, making them particularly useful in safety critical ap-plications [3, 22], Bayesian optimisation [10, 30], active learning [37] or under covariate shift [31].
However, the analytical tractability of GPs comes at the price of reduced ﬂexibility: Standard kernel functions make strong assumptions such as stationarity or smoothness. To make GPs more ﬂexible, a practitioner would have to come up with hand-crafted features or kernel functions. Both alternatives require expert knowledge and are prone to overﬁtting.
Deep Gaussian Processes (DGPs) offer a compelling alternative since they learn non-linear feature representations in a fully probabilistic manner via GP cascades [6]. The gained ﬂexibility has the drawback that inference can no longer be carried out in closed-form, but must be performed via
Monte Carlo sampling [9], or approximate inference techniques [5, 6, 24]. The most popular ap-proximation, variational inference, searches for the best approximate posterior within a pre-deﬁned class of distributions: the variational family [4]. For GPs, variational approximations often build on the inducing point framework where a small set of global latent variables acts as pseudo datapoints summarising the training data [29, 32]. For DGPs, each latent GP is governed by its own set of inducing variables, which, in general, need not be independent from those of other latent GPs. Here, we offer a new class of variational families for DGPs taking the following two requirements into ac-count: (i) all global latent variables, i.e., inducing outputs, can be marginalised out, (ii) correlations between latent GP models can be captured. Satisfying (i) reduces the variance in the estimators and is needed for fast convergence [16] while (ii) leads to better calibrated uncertainty estimates [33]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
By using a fully-parameterised Gaussian variational posterior over the global latent variables, we automat-ically fulﬁl (ii), and we show in Sec. 3.1, via a proof by induction, that (i) can still be achieved. The proof is constructive, resulting in a novel inference scheme for variational families that allow for correlations within and across layers. The proposed scheme is general and can be used for arbitrarily structured covariances allow-ing the user to easily adapt it to application-speciﬁc covariances, depending on the desired DGP model ar-chitecture and on the system requirements with respect to speed, memory and accuracy. One particular case, in which the variational family is chain-structured, has also been considered in a recent work [34], in which the compositional uncertainty in deep GP models is studied.
Covariance matrices for
Figure 1: variational posteriors.
We used a
DGP with 2 hidden layers (L1, L2) of 5 latent GPs each and a single GP in the output layer (L3). The complexity of the variational approximation is in-creased by allowing for additional de-pendencies within and across layers in a
Gaussian variational family (left: mean-ﬁeld [24], middle: stripes-and-arrow, right: fully-coupled). Plotted are natural logarithms of the absolute values of the variational covariance matrices over the inducing outputs.
In Fig. 1 (right) we depict exemplary inferred covari-ances between the latent GPs for a standard deep GP architecture. In addition to the diagonal blocks, the co-variance matrix has visible diagonal stripes in the off-diagonal blocks and an arrow structure. These diago-nal stripes point towards strong dependencies between successive latent GPs, while the arrow structure reﬂects dependencies between all hidden layers and the output layer.
In Sec. 3.2, we further propose a scalable approximation to this variational family, which only takes these stronger correlations into account (Fig. 1, middle). We provide efﬁcient implementations for both variational families, where we particularly exploit the sparsity and structure of the covariance matrix of the variational poste-rior. In Sec. 4, we show experimentally that the new algorithm works well in practice. Our approach obtains a better balance between accurate predictions and calibrated uncertainty estimates than its competitors, as we showcase by varying the distance of the test from the training points. 2