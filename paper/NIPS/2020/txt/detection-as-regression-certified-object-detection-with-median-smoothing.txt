Abstract
Despite the vulnerability of object detectors to adversarial attacks, very few de-fenses are known to date. While adversarial training can improve the empirical robustness of image classiﬁers, a direct extension to object detection is very ex-pensive. This work is motivated by recent progress on certiﬁed classiﬁcation by randomized smoothing. We start by presenting a reduction from object detection to a regression problem. Then, to enable certiﬁed regression, where standard mean smoothing fails, we propose median smoothing, which is of independent interest.
We obtain the ﬁrst model-agnostic, training-free, and certiﬁed defense for object detection against (cid:96)2-bounded attacks. 1

Introduction
Adversarial examples are seemingly innocuous neural network inputs that have been deliberately modiﬁed to produce unexpected or malicious outputs. Early work on adversarial examples was highly focused on image classiﬁers, which assign a single label to an entire image [13, 6, 25, 8]. A large literature has rapidly emerged on defenses against classiﬁer attacks, which includes both heuristic defenses [23] and certiﬁed methods with theoretical guarantees of robustness [9, 36, 2, 24, 26].
However, most realistic vision systems crucially rely on object detectors, rather than image classiﬁers, to identify and localize multiple objects within an image [29, 28, 21].
Over time, attacks on object detection have become more sophisticated, as has been successfully demonstrated both digitally and in the physical world using a range of perturbation techniques, as well as attacks that break both the object localization and classiﬁcation parts of the detection pipeline
[22, 32, 37, 15, 19]. As of this writing, we are only aware of one recent paper on the adversarial robustness of object detectors [39]. This lack of defenses is likely because (i) the complexity of the multi-stage detection pipeline [29] is difﬁcult to analyze, and (ii) detectors are far more expensive to train than classiﬁers. Furthermore, (iii) object detectors output bounding-box coordinates, and are thus regression networks to which many standard defenses for classiﬁer networks cannot be readily applied.
In this paper, we present, to the best of our knowledge, the ﬁrst certiﬁed defense against adversarial attacks on object detectors. To avoid the difﬁculties discussed above, we treat the complex detection pipeline as a black box without requiring specialized re-training schemes. In particular, we present a reduction from object detection to a single regression problem which envelopes the proposal, classiﬁcation, and non-maximum suppression stages of the detection pipeline. Then, we endow this regression with certiﬁed robustness using the Gaussian smoothing approach [4], originally proposed 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Samples of object detection certiﬁcates using the proposed method. Dotted lines represent the farthest a bounding box could move under an adversarial perturbation δ of bounded (cid:96)2-norm. If the predicted bounding box can be made to disappear, or if the label can be made to change, after a perturbation with (cid:107)δ(cid:107)2 < 0.36, then we annotate the bounding box with a red X. for the defense of classiﬁers. To this end, we develop a new variant of smoothing speciﬁcally for regression based on the medians of the smoothed predictions, rather than their mean values. The proposed median smoothing approach enjoys a number of useful properties, and we expect it to
ﬁnd further applications in certiﬁed robustness. Finally, we implement our method to obtain a certiﬁably-robust wrapper of the YOLOv3 [28], Mask RCNN [10], and Faster RCNN [29] object detectors. We use the MS-COCO dataset [20] to test the resulting detector, obtaining the ﬁrst detector to achieve non-trivial (cid:96)2-norm certiﬁed average precision on large scale image dataset. 2