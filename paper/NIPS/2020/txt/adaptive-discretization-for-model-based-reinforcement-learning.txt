Abstract
We introduce the technique of adaptive discretization to design an efﬁcient model-based episodic reinforcement learning algorithm in large (potentially continuous) state-action spaces. Our algorithm is based on optimistic one-step value iteration extended to maintain an adaptive discretization of the space. From a theoretical perspective we provide worst-case regret bounds for our algorithm which are competitive compared to the state-of-the-art model-based algorithms. Moreover, our bounds are obtained via a modular proof technique which can potentially extend to incorporate additional structure on the problem.
From an implementation standpoint, our algorithm has much lower storage and computational requirements due to maintaining a more efﬁcient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs signiﬁcantly better than ﬁxed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while ﬁxed discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization. 1 1

Introduction
Reinforcement learning (RL) is a paradigm modeling an agent’s interactions with an unknown environment with the goal of maximizing their cumulative reward throughout the trajectory [43]. In online settings the dynamics of the system are unknown and the agent must learn the optimal policy only through interacting with the environment. This requires the agent to navigate the exploration exploitation trade-off, between exploring unseen parts of the system and exploiting historical high-reward decisions. Most algorithms for learning the optimal policy in these online settings can be classiﬁed as either model-free or model-based. Model-free algorithms construct estimates for the
Q-function of the optimal policy, the expected sum of rewards obtained from playing a speciﬁc action and following the optimal policy thereafter, and create upper-conﬁdence bounds on this quantity
[38, 16]. In contrast, model-based algorithms instead estimate unknown system parameters, namely the average reward function and the dynamics of the system, and use this to learn the optimal policy based on full or one-step planning [5, 13]. 1The are
AdaptiveQLearning. A full report is available at https://arxiv.org/abs/2007.00717. experiments available https://github.com/seanrsinclair/ code the for at 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Algorithm
ADAMB (Alg. 1) (dS > 2) H 1+ 1 (dS ≤ 2) H 1+ 1
ADAPTIVE Q-LEARNING [38]
KERNEL UCBVI [11]
NET-BASED Q-LEARNING [42]
LOWER-BOUNDS [40] d+dS 1 d+dS +2
Regret d+1 K 1− 1 d+1 K 1−
H 5/2K 1− 1
H 3 K 1− 1
H 5/2K 1− 1
H K 1− 1 d+2 d+2 d+2 2d+1 d+dS
Time Complexity
HK 1+ dS
HK 1+ dS d+dS +2
HK logd(K)
HAK 2
HK 2
Space Complexity
HK
HK 1− 2 d+dS +2 d+2
HK 1− 2
HK
HK
N/A
N/A
Table 1: Comparison of our bounds with several state-of-the-art bounds for RL in continuous settings.
Here, d is the covering dimension of the state-action space, dS is the covering dimension of the state space, H is the horizon of the MDP, and K is the total number of episodes. Implementing KERNEL
UCBVI [11] is unclear under general action spaces, so we specialize the time complexity under a
ﬁnite set of actions of size A. As running UCBVI with a ﬁxed discretization is a natural approach to this problem, we include a short discussion of this algorithm in Appendix G.1. Since the results are informal, we do not include them in the table here. We include ‘N/A’ under the time and space complexity lower bound as there is no prior work in this domain to our knowledge.
RL has received a lot of interest in the design of algorithms for large-scale systems using parametric models and function approximation. For example, the AlphaGo Zero algorithm that mastered Chess and Go from scratch trained their algorithm over 72 hours using 4 TPUs and 64 GPUs [36]. These results show the intrinsic power of RL in learning complex control policies, but are computationally infeasible for applying algorithms to RL tasks in computing systems or operations research. The limiting factor is implementing regression oracles or gradient steps on computing hardware. For example, RL approaches have received much interest in designing controllers for memory systems [1] or resource allocation in cloud-based computing [15]. Common to these examples are computation and storage limitations on the devices used for the controller, requiring algorithms to compete on three major facets: efﬁcient learning, low computation, and low storage requirements.
Motivated by these requirements we consider discretization techniques which map the continuous problem to a discrete one as these algorithms are based on simple primitives easy to implement in hardware (and has been tested heuristically in practice [32, 22]). A challenge is picking a discretization to manage the trade-off between the discretization error and the errors accumulated from solving the discrete problem. As a ﬁxed discretization wastes computation and memory by forcing the algorithm to explore unnecessary parts of the space, we develop an adaptive discretization of the space, where the discretization is only reﬁned on an as-needed basis. This approach reduces unnecessary exploration, computation, and memory by only keeping a ﬁne-discretization across important parts of the space [38].
Adaptive discretization techniques have been successfully applied to multi-armed bandits [40] and model-free RL [38]. The key idea is to maintain a non-uniform partition of the space which is reﬁned based on the density of samples. These techniques do not, however, directly extend to model-based
RL, where the main additional ingredient lies in maintaining transition probability estimates and incorporating these in decision-making. Doing so is easy in tabular RL and (cid:15)-net based policies, as simple transition counts concentrate well enough to get good regret. This is much less straightforward, though, when the underlying discretization changes in an online, data-dependent way.
Our Contributions. We design and analyze a model-based RL algorithm, ADAMB, that discretizes the state-action space in a data-driven way so as to minimize regret. ADAMB requires the underlying state and action spaces to be embedded in compact metric spaces, and the reward function and transition kernel to be Lipschitz continuous with respect to this metric. This encompasses discrete and continuous state-action spaces with mild assumptions on the transition kernel, and deterministic systems with Lipschitz continuous transitions. Our algorithm only requires access to the metric, unlike prior algorithms which require access to simulation oracles [18], strong parametric assumptions [17], or impose additional assumptions on the action space to be computationally efﬁcient [11].
Our policy achieves near-optimal dependence of the regret on the covering dimension of the metric space when compared to other model-based algorithms. In particular, we show that for a H-step 2
MDP played over K episodes, our algorithm achieves a regret bound
R(K) (cid:46) (cid:40)
H 1+ 1
H 1+ 1 d+1 K d+1 K d+dS −1 d+dS d+dS +1 d+dS +2 dS > 2 dS ≤ 2 where dS and dA are the covering dimensions of the state and action space respectively, and d = dS + dA. As Table 1 illustrates, our bounds are uniformly better (in terms of dependence on K and
H, in all dimensions) than the best existing bounds for model-based RL in continuous-spaces [21, 11].
In addition to having lower regret, ADAMB is also simple and practical to implement, with low query complexity and storage requirements (see Table 1) compared to other model-based techniques.
To highlight this, we complement our theory with experiments comparing model-free and model-based algorithms, using both ﬁxed and adaptive discretization. Our experiments show that with a ﬁxed discretization, model-based algorithms outperform model-free ones; however, when using an adaptive partition of the space, model-based and model-free algorithms perform similarly. This provides an interesting contrast between practice (where model-based algorithms are thought to perform much better) and theory (where regret bounds in continuous settings are currently worse for model-based compared to model-free algorithms), and suggests more investigation is required for ranking the two approaches.