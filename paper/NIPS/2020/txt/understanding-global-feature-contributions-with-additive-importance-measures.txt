Abstract
Understanding the inner workings of complex machine learning models is a long-standing problem and most recent research has focused on local interpretability.
To assess the role of individual input features in a global sense, we explore the perspective of deﬁning feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which uniﬁes numerous methods in the literature. We then propose
SAGE, a model-agnostic method that quantiﬁes predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efﬁciently and that it assigns more accurate importance values than other methods. 1

Introduction
Our limited understanding of the inner workings of complex models is a long-standing problem that impedes machine learning adoption in many domains. Most recent research has addressed this by focusing on local interpretability, which explains a model’s individual predictions (e.g., the role of each feature in a patient’s diagnosis) [25, 30, 34, 38]. However, in some cases users require knowledge of a feature’s global importance to understand its role across an entire dataset.
In this work we seek to understand how much models rely on each feature overall, which is often referred to as the problem of global feature importance. The problem is open to many interpretations
[5, 16, 22, 24, 27], and we present the idea of deﬁning feature importance as the amount of predictive power that a feature contributes. This raises the challenge of handling feature interactions, because features contribute different amounts of information when introduced in isolation versus into a larger set of features. We aim to provide a solution that accounts for these complex interactions.
We begin by presenting the framework of additive importance measures, a view that uniﬁes numerous methods that deﬁne feature importance in terms of predictive power (Section 2). We then present a new tool for calculating feature importance, SAGE,1 a model-agnostic approach to summarizing a model’s dependence on each feature while accounting for complex interactions (Section 3). Our work makes the following contributions: 1. We derive SAGE by applying the Shapley value to a function that represents the predictive power contained in subsets of features. Many desirable properties result from this approach, including invariance to invertible feature transformations and a relationship with SHAP [25]. 2. We introduce a framework of additive importance measures that lets us unify many existing methods in the literature. We show that these methods all deﬁne feature importance in terms of predictive power, but that only SAGE does so while properly accounting for feature interactions. 1http://github.com/iancovert/sage/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
3. To manage tractability challenges with SAGE, we propose an efﬁcient sampling-based approxima-tion that is signiﬁcantly faster than a naive calculation via local SHAP values. Our approach also provides uncertainty estimates and permits automatic convergence detection.
Our experiments compare SAGE to several baselines and demonstrate that SAGE’s feature importance values are more representative of the predictive power associated with each feature. We also show that when a model’s performance is unexpectedly poor, SAGE can help identify corrupted features. 2 Unifying Global Feature Importance Methods
We introduce two notions of predictive power for subsets of features and then discuss our unifying framework of additive importance measures. 2.1 Predictive Power of Feature Subsets
Consider a supervised learning task where a model f is used to predict the response variable Y given an input X, where X consists of individual features (X1, X2, . . . , Xd). We use uppercase symbols (e.g., X) to denote random variables and lowercase symbols (e.g., x) to denote values.
The notion of feature importance is open to different interpretations, but we take the perspective that a feature’s importance should correspond to how much predictive power it provides to the model.
Although f is trained using all the features, we can examine its performance when it is given access to subsets of features XS ≡ {Xi | i ∈ S} for different S ⊆ D, where D ≡ {1, . . . , d}. We can then deﬁne “important” features as those whose absence degrades f ’s performance. As a convention for evaluating f when deprived of the features ¯S ≡ D \ S, we deﬁne the restricted model fS as fS(xS) = E(cid:2)f (X) (cid:12) (cid:12) XS = xS (cid:3), (1) so that missing features X ¯S are marginalized out using the conditional distribution p(X ¯S|XS = xs).
Two special cases are S = ∅ and S = D, which respectively correspond to the mean prediction f∅(x∅) = E[f (X)] and the full model prediction fD(x) = f (x). This approach is common in recent work [1, 25] and is necessary for later connections with mutual information (Supplement A).
Using this convention for accommodating subsets of features, we can now measure how much f ’s performance degrades when features are removed. Given a loss function (cid:96), the population risk for fS is deﬁned as E(cid:2)(cid:96)(cid:0)fS(XS), Y (cid:1)(cid:3) where the expectation is taken over the data distribution p(X, Y ). To deﬁne predictive power as a quantity that increases with model accuracy, we consider the reduction in risk over the mean prediction and deﬁne the function vf : P(D) (cid:55)→ R as follows: (cid:104) vf (S) = E (cid:124) (cid:96)(cid:0)f∅(X∅), Y (cid:1)(cid:105) (cid:123)(cid:122) (cid:125)
Mean prediction (cid:96)(cid:0)fS(XS), Y (cid:1)(cid:105) (cid:104)
− E (cid:125) (cid:123)(cid:122) (cid:124)
Using features XS
. (2)
The domain is the power set P(D), the left term is the loss achieved with the mean prediction
E[f (X)], and the right term is the loss achieved using the features XS. The function vf (S) quantiﬁes the amount of predictive power f derives from the features XS, and we generally expect that including more features in S will make vf (S) larger. While vf provides a model-based notion of predictive power, we also introduce a notion of universal predictive power. For this, we deﬁne the function v : P(D) (cid:55)→ R as the reduction in risk from XS when using an optimal model: v(S) = min (cid:104)
E (cid:96)(cid:0)ˆy, Y (cid:1)(cid:105) (cid:124) (cid:125) (cid:123)(cid:122)
Optimal constant ˆy
ˆy
− min g (cid:96)(cid:0)g(XS), Y (cid:1)(cid:105) (cid:104)
E (cid:125) (cid:123)(cid:122) (cid:124)
Optimal model using XS
. (3)
The left term is the loss from an optimal constant prediction ˆy and the right term is the loss for an optimal model g from the class of all functions (e.g., the Bayes classiﬁer). Intuitively, v represents the maximum predictive power that could hypothetically be derived from XS. Since f is typically trained using empirical risk minimization [40], the model-based predictive power vf provides an approximation to v and the two coincide in certain cases where f is optimal (Supplement B). 2
2.2 Additive Importance Measures
In certain very simple cases, features contribute predictive power in an additive manner. This means that we have v(S ∪ {i}) − v(S) = v(T ∪ {i}) − v(T ) for all subsets S, T such that i /∈ S, T . In these situations, we can deﬁne Xi’s importance as the predictive power it contributes, or φi = v({i})−v(∅).
However, a feature’s contribution is generally not additive because it depends on which features XS are already present.
We therefore propose a class of additive importance measures that includes any method whose scores
φ1, . . . , φd can be understood as performance gains associated with each feature. This framework lets us unify numerous methods that either explicitly or implicitly deﬁne feature importance in terms of predictive power. The class of methods is deﬁned as follows.
Deﬁnition 1. Additive importance measures are methods that assign importance scores φi ∈ R to features i = 1, . . . , d and for which there exists a constant φ0 ∈ R so that the additive function u(S) = φ0 + (cid:88) i∈S
φi provides a proxy for the predictive power of feature subsets, i.e., u(S) ≈ v(S).
For methods in this class, u(S) approximates v(S) up to a constant value φ0 by summing the values
φi for each included feature i ∈ S. Each φi can be viewed as the performance gain associated with
Xi, which provides a measure of its importance for the prediction task. Although our deﬁnition focuses on methods that approximate the universal predictive power v, we also include methods that approximate model-based predictive power vf because these implicitly approximate v.
The function v exhibits non-additive behavior for most prediction problems, so the proxy u often cannot perfectly represent each feature’s contribution to the predictive power. Although a crude approximation may provide users with some insight, closer approximations give a more accurate sense of each feature’s importance. Next, we show that several existing methods manage this challenge by providing high quality approximations in speciﬁc regions of the domain P(D). 2.3 Existing Additive Importance Measures
Our framework of additive importance measures uniﬁes numerous methods in the feature importance literature. These methods can be divided into three categories, representing the parts of the domain
P(D) in which the additive proxy u models v or vf most accurately. Supplement G provides a table that summarizes the methods in each category.
The ﬁrst category of methods characterize predictive power when no more than one feature is excluded, and they provide an additive function u that accurately approximates v or vf in the subdomain (cid:0)(cid:8)D(cid:9) ∪ (cid:8)D \ {i} | i ∈ D(cid:9)(cid:1) ⊂ P(D). The canonical method for this is a feature ablation study where, in addition to a model f trained on all features, separate models f1, f2, . . . , fd are trained with individual features excluded [2, 15, 20]. Importance values φ1, . . . , φd are then assigned based on the degradation in performance, according to the formula
φi = E(cid:2)(cid:96)(cid:0)fi(XD\{i}), Y (cid:1)(cid:3) − E(cid:2)(cid:96)(cid:0)f (X), Y (cid:1)(cid:3). (4)
A natural choice for φ0 in this case is φ0 = minˆy E[(cid:96)(ˆy, Y )] − E[(cid:96)(f (X), Y )] − (cid:80) then u(D) and u(D \ {i}) approximate v(D) and v(D \ {i}), respectively. i∈D φi, because
Several other methods provide similar notions of feature importance using a single model f . Permu-tation tests measure performance degradation when each column of the data is permuted [5]. Since permutation tests break feature dependencies, one variation suggests using a conditional permutation scheme [36]. Finally, in a method we refer to as “mean importance,” performance degradation is measured after mean imputing individual features [32]. Although these methods take slightly different approaches, they all approximate either v or vf when at most one feature is excluded.
The second category of methods describe v when no more than one feature is included, providing an additive function u that accurately describes v in the subdomain (cid:0)(cid:8)∅(cid:9) ∪ (cid:8){i} | i ∈ D(cid:9)(cid:1) ⊂ P(D). 3
Methods in this category model the bivariate association between Xi and Y to quantify Xi’s stand-alone predictive power. Bivariate association is commonly studied in ﬁelds such as computational biology (e.g., [23]) and is widely used to identify sensitive features [31]. As an example, the squared correlation Corr(Xi, Y )2 is equivalent to the variance explained by a univariate linear model. More generally, one can measure the performance of univariate models trained to predict Y given Xi [13].
Given a model gi for each feature i ∈ D, the importance values are calculated with the formula
φi = min
ˆy
E(cid:2)(cid:96)(cid:0)ˆy, Y (cid:1)(cid:3) − E(cid:2)(cid:96)(cid:0)gi(Xi), Y (cid:1)(cid:3). (5)
A natural choice for the constant φ0 is φ0 = 0 in this case, because with these scores we see that u(∅) = v(∅) = 0 and that u({i}) approximates v({i}).
The two previous categories of methods provide imperfect notions of feature importance because they do not account for feature interactions. For example, two perfectly correlated features with signiﬁcant predictive power would both be deemed unimportant by a feature ablation study, and two complementary features would have their importance underestimated by univariate models. The third category of methods addresses these issues by considering all feature subsets S ⊆ D.
Methods in the third category account for complex feature interactions by modeling v across its entire domain P(D). These methods therefore supersede the two other categories, which either exclude or include individual features. Our method, SAGE, belongs to this category, and we show that SAGE assigns scores by modeling vf optimally via a weighted least squares objective (Section 3.2). 3 Shapley Additive Global Importance
We now introduce our method, Shapley additive global importance (SAGE), for quantifying a model’s dependence on each feature. We present SAGE as an application of the game-theoretic Shapley value to vf and then examine its properties, including how it can be understood as an additive importance measure. Finally, we propose a practical sampling-based approximation algorithm. 3.1 Shapley Values for Credit Allocation
Recall that the function vf describes the amount of predictive power that a model f derives from subsets of features S ⊆ D. We deﬁne feature importance via vf to quantify how critical each feature
Xi is for f to make accurate predictions. It is natural to view vf as a cooperative game, representing the proﬁt (predictive power) when each player (feature) participates (is available to the model).
Research in game theory has extensively analyzed credit allocation for cooperative games, so we apply a game theoretic solution known as the Shapley value [33].
Shapley values are the unique credit allocation scheme that satisﬁes a set of fairness axioms. For any cooperative game w : P(D) (cid:55)→ R (such as v or vf ) we may want the scores φi(w) assigned to each player to satisfy the following desirable properties: 1. (Efﬁciency) They sum to the total improvement over the empty set, (cid:80)d i=1 φi(w) = w(D) − w(∅). 2. (Symmetry) If two players always make equal contributions, or w(S ∪ {i}) = w(S ∪ {j}) for all
S, then φi(w) = φj(w). 3. (Dummy) If a player makes zero contribution, or w(S) = w(S ∪ {i}) for all S, then φi(w) = 0. 4. (Monotonicity) If for two games w and w(cid:48) a player always make greater contributions to w than w(cid:48), or w(S ∪ {i}) − w(S) ≥ w(cid:48)(S ∪ {i}) − w(cid:48)(S) for all S, then φi(w) ≥ φi(w(cid:48)). 5. (Linearity) The game w(S) = (cid:80)n k=1 ckwk(S), which is a linear combination of multiple games (w1, . . . , wn), has scores given by φi(w) = (cid:80)n k=1 ckφi(wk).
The Shapley values φi(w) are the unique credit allocation scheme that satisﬁes properties 1-5 [33], and they are given by the expression:
φi(w) = 1 d (cid:88)
S⊆D\{i} (cid:19)−1(cid:16) (cid:18)d − 1
|S| w(S ∪ {i}) − w(S) (cid:17)
. (6) 4
The expression above shows that each Shapley value φi(w) is a weighted average of the incremental changes from adding i to subsets S ⊆ D \ {i}. For SAGE, we propose assigning feature importance using the Shapley values of our model-based predictive power, or φi(vf ) for i = 1, 2, . . . , d, which we refer to as SAGE values.
SAGE values satisfy many intuitive and desirable properties, arising both from the properties of
Shapley values and from how vf is deﬁned. These properties include: 1. Due to the efﬁciency property, SAGE values sum to X’s total predictive power. In other words, we have (cid:80)d i=1 φi(vf ) = vf (D). 2. Due to the symmetry property, pairs of features (Xi, Xj) with a deterministic relationship (e.g., perfect correlation) always have equal importance. To see this, remark that fS∪{i}(xS∪{i}) = fS∪{j}(xS∪{j}) for all (S, x), so that vf (S ∪ {i}) = vf (S ∪ {j}). 3. Due to the dummy property, we have φi(vf ) = 0 if Xi is conditionally independent of f (X) given all possible subsets of features XS. However, features may receive non-zero importance even if they are not used by f (which in some cases may be desirable). 4. Due to the monotonicity property, if we have two response variables (Y, Y (cid:48)) with models (f, f (cid:48)) and we have vf (S ∪ {i}) − vf (S) ≥ vf (cid:48)(S ∪ {i}) − vf (cid:48)(S) for all S, so that Xi contributes more predictive power for Y than for Y (cid:48), then the SAGE values satisfy φi(vf ) ≥ φi(vf (cid:48)). 5. Due to the linearity property, SAGE values are the expectation of per-instance SHAP values applied to the model loss [24]. By this, we mean the Shapley values φi(vf,x,y) of the game vf,x,y(S) = (cid:96)(cid:0)f∅(x∅), y(cid:1) − (cid:96)(cid:0)fS(xS), y(cid:1).
In other words, the SAGE values are given by φi(vf ) = EXY [φi(vf,X,Y )]. These values were used in prior work, but they were not analyzed in depth and are costly to calculate via many local explanations [24]. 6. Due to our deﬁnition of vf , SAGE values are invariant to invertible mappings of the features.
More precisely, if we apply an invertible function h to a feature Xi and deﬁne Zi = h(Xi), and we then use a model f (cid:48) that applies the inverse h−1 to Zi before f , then the SAGE values of the new model under the new data distribution are unchanged. For example, SAGE values do not depend on whether a model uses gene counts or log gene counts.
Finally, SAGE values have an elegant interpretation when the loss function is cross entropy or mean squared error (MSE) and the model f is optimal. With cross entropy loss, the optimal model (the
Bayes classiﬁer) predicts the conditional distribution f ∗(x) = p(Y |X = x) and the predictive power is vf ∗ (S) = I(Y ; XS), where I denotes mutual information. The SAGE values are then given by:
φi(vf ∗ ) = 1 d (cid:88)
S⊆D\{i} (cid:19)−1 (cid:18)d − 1
|S|
I(Y ; Xi | XS). (7)
The expression above represents a weighted average of the conditional mutual information, i.e., the reduction in uncertainty for Y when incorporating Xi into different subsets XS. An analogous result arises in the MSE case, and in both cases the SAGE values satisfy φi(vf ∗ ) ≥ 0 (Supplement C).
Through this we see that although SAGE is a tool for model interpretation, it can also provide insight into intrinsic relationships in the data when applied with optimal models. 3.2 SAGE as an Additive Importance Measure
Though it not immediately obvious, SAGE is an additive importance measure (Section 2). Prior work has shown that Shapley values (Eq. 6) can be understood as the solution to a weighted least squares problem [6, 25]. From these ﬁndings, we see that SAGE provides an additive approximation to vf with u(S) = (cid:80) i∈S φi, where φ1, . . . , φd are optimal coefﬁcients for the following problem: min
φ1,...,φd (cid:88)
S⊆D d − 1 (cid:0) d
|S| (cid:1)|S|(d − |S|) (cid:16) (cid:88) i∈S
φi − vf (S) (cid:17)2
. (8) 5
Understanding SAGE values in this way reveals that SAGE attempts to represent vf across its entire domain, modeling it optimally in a weighted least squares sense. Although the weights are perhaps not intuitive, this is the unique weighting scheme that leads to SAGE’s desirable properties.
Using this interpretation of Shapley values, we observe that two more existing methods can be categorized as additive importance measures. The mean SHAP value of the loss [24] and Shapley Net
Effects for linear models [22] are both similar to SAGE, but they involve more expensive calculations (explaining every individual prediction, or ﬁtting an exponential number of linear models). 3.3 Practical SAGE Approximation
We now consider how to calculate SAGE values φi(vf ) efﬁciently. Obtaining these values is challenging because (i) there are an exponential number of subsets S ⊆ D and (ii) evaluating a restricted model fS requires a Monte Carlo estimate with X ¯S sampled from p(X ¯S|XS = xS). Like previous methods that use Shapley values, we sidestep the inherent exponential complexity using an approximation algorithm [9, 18, 25, 37].
We address the ﬁrst challenge by sampling random subsets of features S ⊆ D, and the second by sampling X ¯S from its marginal distribution. To efﬁciently generate subsets of features from the appropriate distribution, we enumerate over random permutations of the indices D = {1, . . . , d}, similar to Štrumbelj and Kononenko [18]. Prior work has used sampling from the marginal distribution in a similar manner [25], but doing so alters some of SAGE’s properties to align less with the information value of each feature and more with the model’s mechanism. Supplement D describes the SAGE sampling algorithm (Algorithm 1) and the changes to its properties in more detail.
In Theorems 1 and 2, we make two claims regarding the estimates from our sampling algorithm (with proofs provided in Supplement E). The ﬁrst result shows that the estimates converge to the correct values when run under appropriate conditions, and the second result shows that the estimates have variance that reduces at a linear rate.
Theorem 1. The SAGE value estimates ˆφi(vf ) from Algorithm 1 converge to the correct values
φi(vf ) when run with n → ∞, m → ∞, with an arbitrarily large dataset (cid:8)(xi, yi)(cid:9)N i=1, and with sampling from the correct conditional distribution p(X ¯S|XS = xS).
Theorem 2. The SAGE value estimates ˆφi(vf ) from Algorithm 1 have variance that reduces at the rate of O( 1 n ).
In practice the algorithm must run for a ﬁnite number of iterations, so we propose an approach to monitor the estimates’ uncertainty and detect convergence. Theorem 2 implies that for each feature i such that the estimate ˆφi(vf ) after n iterations has variance given by
Xi there exists a constant σ2
Var(cid:0) ˆφi(vf )(cid:1) ≈ σ2 i /n. This quantity can be estimated while running the sampling algorithm and can then be used to provide conﬁdence intervals on the estimated values. Finally, the algorithm may be considered converged when the largest standard deviation is a sufﬁciently low proportion t (e.g., t = 0.01) of the range in the estimated values, or when the following criterion is satisﬁed: max i
σi√ n (cid:16)
< t max i
ˆφi(vf ) − min i
ˆφi(vf ) (cid:17)
. 4