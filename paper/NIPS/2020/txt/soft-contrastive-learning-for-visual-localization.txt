Abstract
Localization by image retrieval is inexpensive and scalable due to its simple map-ping and matching techniques. The localization accuracy, however, depends on the quality of the underlying image features, often obtained using contrastive learning.
Most contrastive learning strategies learn features that distinguish between different classes. In the context of localization, however, there is no natural deﬁnition of classes. Therefore, images are artiﬁcially separated into positive/negative classes with respect to the chosen anchor images, based on some geometric proximity measure. In this paper, we show why such divisions are problematic for learning localization features. We argue that any artiﬁcial division based on a proximity measure is undesirable due to the inherently ambiguous supervision for images near the proximity threshold. To avoid this problem, we propose a novel technique that uses soft positive/negative assignments of images for contrastive learning. Our soft assignment makes a gradual distinction between close and far images in both geometric and feature space. Experiments on four large-scale benchmark datasets demonstrate the superiority of our soft contrastive learning over the state-of-the-art method for retrieval-based visual localization. 1

Introduction
A high quality, view aware1 image often captures sufﬁcient information to uniquely represent a location. Therefore, it is not surprising that we use vision as the primary source of information for localization, navigation, and exploration in our environments. Vision-based localization has become an effective solution for several important applications ranging from robotics to augmented reality.
However, the application of such solutions in large scale environments is limited primarily because of the high computational demand. This limitation is usually alleviated using additional sensory systems such as GPS and beacons. In the absence of such sensors—either the device is not equipped with them or the environment denies the usage (e.g. indoors)—image retrieval-based localization is an appealing alternative. The problem of retrieval-based localization equates to matching one or more query images, taken at some unknown location, to a set of geo-tagged reference images.
Traditionally, vision-based localization [1] is tackled either with structure-based methods, such as Structure-from-Motion (SfM) [2, 3, 4, 5, 6, 7] and Simultaneous Localization and Mapping (SLAM) [8, 9, 10, 11, 12], or with retrieval-based approaches [13, 14, 15, 16, 17, 18, 19, 20].
Structure-based methods usually focus on accurate relative pose estimation, while retrieval-based approaches prioritize absolute re-localization. Localization by image retrieval (or simply retrieval-based localization) is inexpensive due to its simple mapping and matching possibilities, which scale well to large spaces [16, 4, 21]. Many structure-based approaches use retrieval for initialization [4].
Recent developments in learning image features for object and place recognition [13, 14, 15, 16] have made image retrieval a viable method for localization. 1Images captured with an intention to localize with a wide view of the surrounding. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Almost all current methods for learning image features for localization are based on learning for classiﬁcation. Discriminative approaches based on contrastive learning in the feature space have recently shown great promise [13, 22] for classiﬁcation using human supervision. In the absence of such supervision, an effective visual representation can still be learned using the framework of contrastive learning by artiﬁcially dividing images into similar and dissimilar categories [23, 24], also known as positives and negatives. When the visual task itself is of contrastive nature, such as face recognition, it is natural to learn features by dividing images into categories. Many notable works [25, 26, 27, 28] in this context, have laid a foundation for learning powerful features when images can be meaningfully divided into discrete categories. The state-of-the-art localization features [16, 29, 30, 31] are learned by building upon this foundation, where images are divided into discrete categories of positives and negatives, with respect to a chosen anchor image, using some geometric proximity measure to that anchor.
In this paper, we show why dividing images into discrete categories of positive/negative is problematic in the context of learning features for visual localization. Our argument stems from the fact that there is no natural deﬁnition of discrete categories of the continuous world. Any artiﬁcial division based on some proximity measure immediately becomes undesirable due to the inherently ambiguous supervision for images close to the proximity threshold. To avoid this problem, we propose a novel technique that uses soft positive/negative assignments of images for contrastive learning. We, however, acknowledge that features for place/landmark recognition [16, 14, 29]—which are shown to somewhat generalize for localization—do not necessarily suffer from the same problem, as the landmarks can be discrete. Methods for place recognition primarily aim to distinguish between prominent landmarks—where images do not necessarily have to be geo-tagged2. Therefore, place recognition features offer only a vague promise for accurate localization. In this regard, the task of learning localization speciﬁc features has received little to no attention in the literature, with the exception of Thoma et al. [31]. Although [31] beneﬁts from softly treating the positives, the used positive/negative division directly conﬂicts with the theoretical argument of our work.
Contributions. Our major contributions are threefold. (i) We propose a formal theoretical framework, in contrast to the common practice, for learning localization features. (ii) Within the proposed framework, we formulate a novel loss function while offering other possibilities to tackle the original multi-objective problem. (iii) Using four large-scale datasets, we demonstrate a clear superiority of our method over the state of the art. 2