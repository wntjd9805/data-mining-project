Abstract
Most compilers for machine learning (ML) frameworks need to solve many cor-related optimization problems to generate efﬁcient machine code. Current ML compilers rely on heuristics based algorithms to solve these optimization problems one at a time. However, this approach is not only hard to maintain but often leads to sub-optimal solutions especially for newer model architectures. Existing learning based approaches in the literature are sample inefﬁcient, tackle a single optimization problem, and do not generalize to unseen graphs making them in-feasible to be deployed in practice. To address these limitations, we propose an end-to-end, transferable deep reinforcement learning method for computational graph optimization (GO), based on a scalable sequential attention mechanism over an inductive graph neural network. GO generates decisions on the entire graph rather than on each individual node autoregressively, drastically speeding up the search compared to prior methods. Moreover, we propose recurrent attention layers to jointly optimize dependent graph optimization tasks and demonstrate 33%-60% speedup on three graph optimization tasks compared to TensorFlow default op-timization. On a diverse set of representative graphs consisting of up to 80,000 nodes, including Inception-v3, Transformer-XL, and WaveNet, GO achieves on average 21% improvement over human experts and 18% improvement over the prior state of the art with 15× faster convergence, on a device placement task evaluated in real systems. 1

Introduction
Increasingly, many applications are driven by large and complex neural network models [12, 28, 15, 18, 24]. The high computation requirements of training such models requires efﬁcient use of
ML accelerator(like GPUs and TPUs). However, the effective use of such accelerators is largely determined by device-speciﬁc optimization by compilers, like TensorFlow XLA, Glow, MLIR, and
AutoTVM [10, 26, 17, 4], which map the high-level computational graph to operations executable on the device.
In mapping a computational graph to machine code that executes on a collection of devices, ML compilers need to solve many optimization problems including graph rewriting, assignment of operations on devices, operation fusion, layout and tiling of tensors, and scheduling. ML compilers usually apply heuristics to solve these problems individually, which suffers from two key limitations. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
First, these heuristics often lead to sub-optimal conﬁgurations especially for previously unseen model architectures. Second, by solving these problems in isolation, the compiler misses out on opportunities for joint optimizations across tasks. To overcome these limitations of current approaches,
ML practitioners often rely on their domain knowledge and use manual hints (for example, explicit device assignments) to guide the compiler’s decisions.
Prior RL-based approaches [21, 19, 9] outperform both human experts as well as heuristic algorithms.
However, they often require substantial computational resources to train and do not generalize well to new graphs. Furthermore, most prior learning based approaches are targeted towards solving a single optimization problem in the stack without any knowledge sharing across tasks. Many of the graph optimization problems in the compiler stack are inherently coupled. For example, a seemingly well optimized graph partitioning and device placement can lead to poor run time due to bad scheduling decisions that induces a near-sequential execution. For making learned solutions practically viable and competitive, we need designs that are not only resource efﬁcient and fast but also are able to jointly solve tightly coupled optimization problems in the stack.
In this paper, we propose an end-to-end deep RL method (GO) for ML compiler graph optimizations where the learned policy is generalizable to new graphs and transferable across multiple tasks. Specif-ically, GO consists of an inductive graph-embedding network that encodes operation features and dependencies in a trainable graph representation, followed by a policy network of segmented recur-rent attention layers. The policy network transforms the graph representations into an optimization decision with soft attention. These two networks can be jointly trained in an end-to-end fashion using a supervised reward. To generalize to arbitrary and held-out graphs, GO is trained jointly over a set of computation graphs (instead of one at a time) and then ﬁne-tuned on new graphs. By transferring the learned graph embeddings and optimization policies, GO converges faster using less resources. We also use super-positioning, a feature conditioning mechanism based on the input graph embeddings, to effectively orchestrate the optimization dynamics of a batch containing graphs with drastically different sizes. To jointly optimize multiple dependent graph optimization tasks, we propose a novel recurrent attention, without introducing additional parameters or training overhead. 2