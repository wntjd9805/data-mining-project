Abstract
Randomized smoothing is a recently proposed defense against adversarial attacks that has achieved state-of-the-art provable robustness against (cid:96)2 perturbations. A number of publications have extended the guarantees to other metrics, such as (cid:96)1 or (cid:96)∞, by using different smoothing measures. Although the current framework has been shown to yield near-optimal (cid:96)p radii, the total safety region certiﬁed by the current framework can be arbitrarily small compared to the optimal.
In this work, we propose a framework to improve the certiﬁed safety region for these smoothed classiﬁers without changing the underlying smoothing scheme.
The theoretical contributions are as follows: 1) We generalize the certiﬁcation for randomized smoothing by reformulating certiﬁed radius calculation as a nested optimization problem over a class of functions. 2) We provide a method to calcu-late the certiﬁed safety region using zeroth-order and ﬁrst-order information for
Gaussian-smoothed classiﬁers. We also provide a framework that generalizes the calculation for certiﬁcation using higher-order information. 3) We design efﬁcient, high-conﬁdence estimators for the relevant statistics of the ﬁrst-order information.
Combining the theoretical contribution 2) and 3) allows us to certify safety region that are signiﬁcantly larger than the ones provided by the current methods. On
CIFAR10 and Imagenet datasets, the new regions certiﬁed by our approach achieve signiﬁcant improvements on general (cid:96)1 certiﬁed radii and on the (cid:96)2 certiﬁed radii for color-space attacks ((cid:96)2 perturbation restricted to only one color/channel) while also achieving smaller improvements on the general (cid:96)2 certiﬁed radii.
As discussed in the future works section, our framework can also provide a way to circumvent the current impossibility results on achieving higher magnitudes of certiﬁed radii without requiring the use of data-dependent smoothing techniques. 1

Introduction
Deep neural networks (DNNs) can be highly sensitive, i.e., small imperceptible input perturbations can lead to mis-classiﬁcation [1, 2]. This poses a big problem for the deployment of DNNs in safety critical applications including aircraft control systems, video surveillance and self-driving cars, which require near-zero tolerance to lack of robustness. Thus, it is important to provide guarantees for the robustness of deep neural network models against multiple worst-case perturbations. Popular threat models are the (cid:96)p-norm-based attacks, where possible perturbations are constrained in an (cid:96)p-ball with respect to a given input x. To that end recent research efforts have focused on attack-agnostic robustness certiﬁcation which, given an input x, provides a safety region within which the model is guaranteed not to change its prediction.
Randomized smoothing is a recently-proposed defense [3, 4, 5] that has achieved state-of-the-art robustness guarantees. Given any classiﬁer f , denoted as a base classiﬁer, randomized smoothing 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
predicts the class that is “most likely” to be returned when noise is added to the input x. Thus, randomized smoothing acts as an operator that given a base classiﬁer and a noise model, denoted as smoothing measure, produces a new classiﬁer, denoted as the smoothed classiﬁer. The smoothed classiﬁers thus produced are easily-certiﬁable with strong (even near-optimal for (cid:96)2) robustness guarantees under various (cid:96)p norm threat models [3, 6, 7]).
However, for any given threat model, the state-of-the-art robustness guarantees are achieved only for the smoothed classiﬁers obtained using very speciﬁc smoothing measures. Thus, the classiﬁers that attain state-of-the-art guarantees under one threat model might perform poorly under another (Gaussian-smoothed classiﬁers are optimal under (cid:96)2 but perform poorly under (cid:96)1; uniform noise-smoothed classiﬁers are state-of-the-art under (cid:96)1 but have poor performance under (cid:96)2). Moreover, some of the recent works [8, 9, 10] show that the existing framework, which uses only the zeroth order information (the function value of the smoothed classiﬁer g(x)), is incapable of producing large certiﬁed radii for (cid:96)p norms with high values of p.
Motivated by the two limitations above, we focus our attention on improving the certiﬁed safety region which is agnostic of threat models. To that end, we propose a general framework to provide a larger certiﬁed safety region by better utilizing the information derived from a hard-label classiﬁer. In particular, we summarize our contributions as follows: 1. We propose a general framework that calculates a certiﬁed safety region of a smoothed classiﬁer g, around an input point x, by exploiting the estimated local properties (e.g. gradient, Hessian, etc.) of the classiﬁer g at x. 2. We give a threat-model-agnostic asymptotic-optimality result for smoothed classiﬁers ob-tained by using standard Gaussian as the smoothing measure, i.e. gaussian-smoothed classiﬁers. Using Theorem 2 in Section 3.1, we show that theoretically it is possible to produce arbitrarily tight certiﬁcates for any classiﬁer1. As a consequence, we see that the impossibility results for the existing framework cannot be extended to certiﬁcates obtained using higher-order information. 3. We motivate and prove properties, like convexity (Proposition 1) and non-decreasing depen-dence on angle (Proposition 2), regarding the certiﬁed safety regions of gaussian-smoothed classiﬁers produced using the zeroth and ﬁrst-order local information. Using these properties, we give formulas for calculating certiﬁed radii for gaussian-smoothed classiﬁers under (cid:96)p threat models and their subspace variants with p = 1, 2, ∞. 4. We design new efﬁcient estimators (see Table 1) to provide high-conﬁdence interval esti-mates of relevant ﬁrst-order information about the classiﬁer since the naive Monte-Carlo estimators have prohibitively high sample complexity.
Finally, we use the 3rd and the 4th contributions above to empirically verify the effectiveness of the new certiﬁcation framework in providing state-of-the-art certiﬁed accuracy for multiple threat models simultaneously. In particular, our proposed framework substantially boosts the certiﬁed accuracy for (cid:96)1 norm and subspace (cid:96)2 norm while maintaining (at times marginally improving) the state-of-the-art near-optimal results for (cid:96)2 norm. On the CIFAR10 dataset, our results for the (cid:96)∞ norm also show improvement over the state-of-the-art bounds given by Gaussian smoothing. 2