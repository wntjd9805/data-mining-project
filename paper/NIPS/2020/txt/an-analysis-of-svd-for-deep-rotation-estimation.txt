Abstract
Symmetric orthogonalization via SVD, and closely related procedures, are well-known techniques for projecting matrices onto O(n) or SO(n). These tools have long been used for applications in computer vision, for example optimal 3D align-ment problems solved by orthogonal Procrustes, rotation averaging, or Essential matrix decomposition. Despite its utility in different settings, SVD orthogonal-ization as a procedure for producing rotation matrices is typically overlooked in deep learning models, where the preferences tend toward classic representations like unit quaternions, Euler angles, and axis-angle, or more recently-introduced methods. Despite the importance of 3D rotations in computer vision and robotics, a single universally effective representation is still missing. Here, we explore the viability of SVD orthogonalization for 3D rotations in neural networks. We present a theoretical analysis of SVD as used for projection onto the rotation group. Our extensive quantitative analysis shows simply replacing existing representations with the SVD orthogonalization procedure obtains state of the art performance in many deep learning applications covering both supervised and unsupervised training. 1

Introduction
There are many ways to represent a 3D rotation matrix. But what is the ideal representation to predict 3D rotations in a deep learning framework? The goal of this paper is to explore this seemingly low-level but practically impactful question, as currently the answer appears to be ambiguous.
In this paper we present a systematic study on estimating rotations in neural networks. We identify that the classic technique of SVD orthogonalization, widely used in other contexts but rarely in the estimation of 3D rotations in deep networks, is ideally suited for this task with strong empirical and theoretical support. 3D rotations are important quantities appearing in countless applications across different ﬁelds of study, and are now especially ubiquitous in learning problems in 3D computer vision and robotics.
The task of predicting 3D rotations is common to estimating object pose [53, 27, 32, 44, 49, 24, 45], relative camera pose [30, 36, 7], ego-motion and depth from video [55, 29], and human pose [56, 21].
A design choice common to all of these models is selecting a representation for 3D rotations. The most frequent choices are classic representations including unit quaternion, Euler angles, and axis-angle. Despite being such a well-studied problem, there is no universally effective representation or regression architecture due to performance variations across different applications.
A natural alternative to these classic representations is symmetric orthogonalization, a long-known technique which projects matrices onto the orthogonal group O(3) [26, 40]. Simple variations can restrict the projections onto the special orthogonal (rotation) group SO(3) [15, 20, 50]. This procedure, when executed by Singular Value Decomposition (SVD [11]), has found many applications contact: jake_levinson@sfu.ca, makadia@google.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
in computer vision, for example at the core of the Procrustes problem [2, 40] for point set alignment, as well as single rotation averaging [13]. A nearly identical procedure is used for factorizing Essential matrices [14].
Despite its adoption in these related contexts, orthogonalization via SVD has not taken hold as a procedure for generating 3D rotations in deep learning: it is rarely used when implementing a model (e.g. overlooked in [24, 7, 36, 29]), nor is it considered a benchmark when evaluating new representations [25, 57, 35].
In light of this, this paper explores the viability of SVD orthogonalization for estimating rotations in deep neural networks. Note, we do not claim to be the ﬁrst to introduce this tool to deep learning, rather our focus is on providing a comprehensive study of the technique speciﬁcally for estimating rotations. Our contributions include
• A theoretically motivated analysis of rotation estimation via SVD orthogonalization in the context of neural networks, and in comparison to the recently proposed Gram-Schmidt procedure [57]. One main result is that SVD improves over Gram-Schmidt by a factor of two for reconstruction, thus supporting SVD as the preferred orthogonalization procedure.
• An extensive quantitative evaluation of SVD orthogonalization spanning four diverse applica-tion environments: point cloud alignment, object pose from images, inverse kinematics, and depth prediction from images, across supervised and unsupervised settings, and benchmarked against classic and recently introduced rotation representations.
Our results show that rotation estimation via SVD orthogonalization achieves state of the art perfor-mance in almost all application settings, and is the best performing method among those that can be applied in both supervised and unsupervised settings. This is an important result given the prevalence of deep learning frameworks that utilize rotations, as well as for benchmarking future research into new representations. 2