Abstract
The sampling of probability distributions speciﬁed up to a normalization constant is an important problem in both machine learning and statistical mechanics. While classical stochastic sampling methods such as Markov Chain Monte Carlo (MCMC) or Langevin Dynamics (LD) can suffer from slow mixing times there is a growing interest in using normalizing ﬂows in order to learn the transformation of a simple prior distribution to the given target distribution. Here we propose a generalized and combined approach to sample target densities: Stochastic Normalizing Flows (SNF) – an arbitrary sequence of deterministic invertible functions and stochastic sampling blocks. We show that stochasticity overcomes expressivity limitations of normalizing ﬂows resulting from the invertibility constraint, whereas trainable transformations between sampling steps improve efﬁciency of pure MCMC/LD along the ﬂow. By invoking ideas from non-equilibrium statistical mechanics we derive an efﬁcient training procedure by which both the sampler’s and the
ﬂow’s parameters can be optimized end-to-end, and by which we can compute exact importance weights without having to marginalize out the randomness of the stochastic blocks. We illustrate the representational power, sampling efﬁciency and asymptotic correctness of SNFs on several benchmarks including applications to sampling molecular systems in equilibrium. 1

Introduction
A common problem in machine learning and statistics with important applications in physics is the generation of asymptotically unbiased samples from a target distribution deﬁned up to a normalization constant by means of an energy model u(x):
µX (x) ∝ exp(−u(x)). (1)
Sampling of such unnormalized distributions is often done with Markov Chain Monte Carlo (MCMC) or other stochastic sampling methods [13]. This approach is asymptotically unbiased, but suffers from the sampling problem: without knowing efﬁcient moves, MCMC approaches may get stuck in local energy minima for a long time and fail to converge in practice.
Normalizing ﬂows (NFs) [41, 40, 5, 35, 6, 33] combined with importance sampling methods are an alternative approach that enjoys growing interest in molecular and material sciences and nuclear physics [28, 25, 32, 22, 1, 30]. NFs are learnable invertible functions, usually represented by a neural network, pushing forward a probability density over a latent or “prior” space Z towards the target space X. Utilizing the change of variable rule these models provide exact densities of generated samples allowing them to be trained by either maximizing the likelihood on data (ML) or minimizing the Kullback-Leibler divergence (KL) towards a target distribution.
Let FZX be such a map and its inverse FXZ = F − invertible transformation layers F0, ..., FT with intermediate states yt given by:
ZX . We can consider it as composition of T 1 yt+1 = Ft(yt) yt = F − t 1 (yt+1) (2) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
By calling the samples in Z and X also z and x, respectively, the ﬂow structure is as follows: z = y0 (cid:29) y1 (cid:29) · · · (cid:29) yT
F0 1
F − 0
FT
− 1 1 (cid:29) yT = x 1 1
F −
T
−
− (3)
We suppose each transformation layer is differentiable with a Jacobian determinant |det Jt(y)|. This allows to apply the change of variable rule: pt+1(yt+1) = pt+1 (Ft(yt)) = pt(yt) |det Jt(yt)|− 1 .
As we often work with log-densities, we abbreviate the log Jacobian determinant as:
∆St = log |det Jt(y)| . (4) (5)
The log Jacobian determinant of the entire ﬂow is deﬁned by ∆SZX = (cid:80) ingly ∆SXZ for the inverse ﬂow. t ∆St(yt) and correspond-Unbiased sampling with Boltzmann Generators. Unbiased sampling is particularly important for applications in physics and chemistry where unbiased expectation values are required [25, 32, 1, 30]. A Boltzmann generator [32] utilizing NFs achieves this by (i) generating one-shot samples x ∼ pX (x) from the ﬂow and (ii) using a reweighing/resampling procedure respecting weights w(x) =
µX (x) pX (x)
∝ exp (−uX (x) + uZ(z) + ∆SZX (z)) , (6) turning these one-shot samples into asymptotically unbiased samples. Reweighing/resampling methods utilized in this context are e.g. Importance Sampling [28, 32] or Neural MCMC [25, 1, 30].
Training NFs. NFs are trained in either “forward” or “reverse” mode, e.g.: 1. Density estimation – given data samples x, train the ﬂow such that the back-transformed samples z = FXZ(x) follow a latent distribution µZ(z), e.g. µZ(z) = N (0, I). This is done by maximizing the likelihood – equivalent to minimizing the KL divergence KL [µX (cid:107)pX ]. 2. Sampling of a given target density µX (x) – sample from the simple distribution µZ(z) and minimize a divergence between the distribution generated by the forward-transformation x = FXZ(z) and µX (x). A common choice is the reverse KL divergence KL [pX (cid:107)µX ].
We will use densities interchangeably with energies, deﬁned by the negative logarithm of the density.
The exact prior and target distributions are: 1
µZ(z) = Z −
Z exp(−uZ(z)) 1
µX (x) = Z −
X exp(−uX (x)) (7) with generally unknown normalization constants ZZ and ZX . As can be shown (Suppl. Material Sec. 1) minimizing KL [pX (cid:107)µX ] or KL [µX (cid:107)pX ] corresponds to maximizing the forward or backward weights of samples drawn from pX or µX , respectively.
Topological problems of NFs. A major caveat of sampling with exactly invertible functions for physical problems are topological constraints. While these can be strong manifold results, e.g., if the sample space is restricted to a non-trivial Lie group [11, 12], another practical problem are induced Bi-Lipschitz constraints resulting from mapping uni-modal base distributions onto well-separated multi-modal target distributions[4]. For example, when trying to map a unimodal Gaussian distribution to a bimodal distribution with afﬁne coupling layers, a connection between the modes remains (Fig. 1a). This representational insufﬁciency poses serious problems during optimization – in the bimodal distribution example, the connection between the density modes seems largely determined by the initialization and does not move during optimization, leading to very different results in multiple runs (Suppl. Material, Fig. S1). More powerful coupling layers, e.g., [9], can mitigate this effect. Yet, as they are still diffeomorphic, strong Bi-Lipschitz requirements can make optimization difﬁcult. This problem can be resolved when relaxing bijectivity of the ﬂow by adding noise as we show in our results. Other proposed solutions are real-and-discrete mixtures of ﬂows [7] or augmentation of the bases space [8, 18] at the cost of losing asymptotically unbiased sampling. 2
Figure 1: Deterministic versus stochastic normalizing ﬂow for the double well. Red arrows indicate deterministic transformations, blue arrows indicate stochastic dynamics. a) 3 RealNVP blocks (2 layers each). b) Same with 20 BD steps before or after RealNVP blocks. c) Unbiased sample from true distribution.
Figure 2: Schematic for Stochastic Normalizing Flow (SNF). An SNF transforms a tractable prior
µZ(z) ∝ exp(−u0(z)) to a complicated target distribution µX (x) ∝ exp(−u1(x)) by a sequence of deterministic invertible transformations (ﬂows, grey boxes) and stochastic dynamics (sample, ochre) that sample with respect to a guiding potential uλ(x). SNFs can be trained and run in forward mode (black) and reverse mode (blue).
Contributions. We show that NFs can be interwoven with stochastic sampling blocks into arbitrary sequences, that together overcome topological constraints and improve expressivity over deterministic
ﬂow architectures (Fig. 1a, b). Furthermore, NSFs have improved sampling efﬁciency over pure stochastic sampling as the ﬂow’s and sampler’s parameters can be optimized jointly.
Our main result is that NSFs can be trained in a similar fashion as NFs and exact importance weights for each sample ending in x can be computed, facilitating asymptotically unbiased sampling from the target density. The approach avoids explicitly computing pX (x) which would require solving the intractable integral over all stochastic paths ending in x.
We apply the model to the recently introduced problem of asymptotically unbiased sampling of molecular structures with ﬂows [32] and show that it signiﬁcantly improves sampling the multi-modal torsion angle distributions which are the relevant degrees of freedom in the system. We further show the advantage of the method over pure ﬂow-based sampling / MCMC by quantitative comparison on benchmark data sets and on sampling from a VAE’s posterior distribution.
Code is available at github.com/noegroup/stochastic_normalizing_flows 2 Stochastic normalizing ﬂows
A SNF is a sequence of T stochastic and deterministic transformations. We sample z = y0 from the prior µZ, and generate a forward path (y1, . . . , yT ) resulting in a proposal yT (Fig. 2). Correspond-ingly, latent space samples can be generated by starting from a sample x = yT and invoking the backward path (yT 1, . . . , y0). The conditional forward / backward path probabilities are
−
Pf (z = y0 → yT = x) =
T 1 (cid:89)
− t=0 qt(yt → yt+1),
Pb(x = yT → y0 = z) =
T 1 (cid:89)
− t=0
˜qt(yt+1 → yt) (8) 3
where (9) denote the forward / backward sampling density at step t respectively. If step t is a deterministic transformation Ft this simpliﬁes as
, yt+1|yt ∼ qt(yt → yt+1) yt|yt+1 ∼ ˜qt(yt+1 → yt) yt+1 ∼ δ (yt+1 − Ft(yt)) , yt ∼ δ (cid:0)yt − F − t 1 (yt+1)(cid:1) .
In contrast to NFs, the probability that an SNF generates a sample x cannot be computed by Eq. (4) but instead involves an integral over all paths that end in x: (cid:90) pX (x) =
µZ(y0)Pf (y0 → yT ) dy0 · · · dyT 1.
− (10)
This integral is generally intractable, thus a feasible training method must avoid using Eq. (10).
Following [31], we can draw samples x ∼ µX (x) by running Metropolis-Hastings moves in the path-space of (z = y0, ..., yT = x) if we select the backward path probability µX (x)Pb(x → z) as the target distribution and the forward path probability µZ(z)Pf (z → x) as the proposal density.
Since we sample paths independently, it is simpler to assign an unnormalized importance weight proportional to the acceptance ratio to each sample path from z = y0 to x = yT : (cid:32) w(z → x) = exp
−uX (x) + uZ(z) + (cid:33)
∆St(yt)
∝ (cid:88) t
µX (x)Pb(x → z)
µZ(z)Pf (z → x)
, where
∆St = log
˜qt(yt+1 → yt) qt(yt → yt+1) (11) (12) denotes the forward-backward probability ratio of step t, and corresponds to the usual change of variable formula in NF for deterministic transformation steps (Suppl. Material Sec. 3). These weights allow asymptotically unbiased sampling and training of SNFs while avoiding Eq. (10). By changing denominator and numerator in (11) we can alternatively obtain the backward weights w(x → z).
µZ (z)Pf (z
SNF training. As in NFs, the parameters of a SNF can be optimized by minimizing the Kullback-Leibler divergence between the forward and backward path probabilities, or alternatively maximizing forward and backward path weights as long as we can compute ∆St (Suppl. Material Sec 1):
JKL = E x) [− log w(z → x)] = KL (µZ(z)Pf (z → x)||µX (x)Pb(x → z)) + const. (13)
In the ideal case of JKL = 0, all paths have the same weight w(z → x) = 1 and the independent and identically distributed sampling of µX can be achieved. Accordingly, we can maximize the likelihood of the generating process on data drawn from µX by minimizing:
JML = E z) [− log w(x → z)] = KL (µX (x)Pb(x → z)||µZ(z)Pf (z → x)) + const. (14)
µX (x)Pb(x
→
→
Variational bound. Minimization of the reverse path divergence JKL minimizes an upper bound on the reverse KL divergence between the marginal distributions:
KL (pX (x) (cid:107) µX (x)) ≤ KL (µZ(z)Pf (z → x) (cid:107) µX (x)Pb(x → z))
And the same relationship exists between the forward path divergence JM L and the forward KL divergence. While invoking this variational approximation precludes us from explicitly computing pX (x) and KL (pX (x) (cid:107) µX (x)), we can still generate asymptotically unbiased samples from the target density µX , unlike in variational inference. (15)
Asymptotically unbiased sampling. As stated in the theorem below (Proof in Suppl. Material.
Sec. 2), SNFs are Boltzmann Generators: We can generate asymptotically unbiased samples of x ∼ µX (x) by performing importance sampling or Neural MCMC using the path weight w(zk → xk) of each path sample k.
Theorem 1. Let O be a function over X. An asymptotically unbiased estimator is given by
Ex
∼
µX [O(x)] ≈ (cid:80) k w(zk → xk) O(xk) (cid:80) k w(zk → xk)
, (16) if paths are drawn from the forward path distribution µZ(z)Pf (z → x). 4
3
Implementing SNFs via Annealed Importance Sampling
In this paper we focus on the use of SNFs as samplers of µX (x) for problems where the target energy uX (x) is known, deﬁning the target density up to a constant, and provide an implementation of stochastic blocks via MCMC / LD. These blocks make local stochastic updates of the current state y with respect to some potential uλ(y) such that they will asymptotically sample from µλ(y) ∝ exp(−uλ(y)). While such potentials uλ(y) could be learned, a straightforward strategy is to interpolate between prior and target potentials uλ(y) = (1 − λ)uZ(y) + λuX (y), (17) similarly as it is done in annealed importance sampling [29]. Our implementation for SNFs is thus as follows: deterministic ﬂow layers in-between only have to approximate the partial density transformation between adjacent λ steps while the stochastic blocks anneal with respect to the given intermediate potential uλ. The parameter λ could again be learned – in this paper we simply choose a linear interpolation along the SNF layers: λ = t/T .
Langevin dynamics. Overdamped Langevin dynamics, also known as Brownian dynamics, using an Euler discretization with time step ∆t, are given by [10]: (cid:112) yt+1 = yt − (cid:15)t∇uλ(yt) + 2(cid:15)t/βηt, (18) where ηt ∼ N (0, I) is Gaussian noise. In physical systems, the constant (cid:15)t has the form (cid:15)t = ∆t/γm with time step ∆t, friction coefﬁcient γ and mass m, and β is the inverse temperature (here set to 1).
The backward step yt+1 → yt is realized under these dynamics with the backward noise realization (Suppl. Material Sec. 4 and [31]):
˜ηt = (cid:114)
β(cid:15)t 2
[∇uλ(yt) + ∇uλ(yt+1)] − ηt.
The log path probability ratio is (Suppl. Material Sec. 4): 1 2
We also give the results for non-overdamped Langevin dynamics in Suppl. Material. Sec. 5. (cid:107)˜ηt(cid:107)2 − (cid:107)ηt(cid:107)2(cid:17) (cid:16)
∆St = −
. (19) (20)
Markov Chain Monte Carlo. Consider MCMC methods with a proposal density qt that satisﬁes the detailed balance condition w.r.t. the interpolated density µλ(y) ∝ exp(−uλ(y)): exp(−uλ(yt))qt(yt → yt+1) = exp(−uλ(yt+1))qt(yt+1 → yt) (21)
We show that for all qt satisfying (21), including Metropolis-Hastings and Hamiltonian MC moves, the log path probability ratio is (Suppl. Material Sec. 6 and 7):
∆St = uλ(yt+1) − uλ(yt), (22) if the backward sampling density satisﬁes ˜qt = qt. 4 Results
Representational power versus sampling efﬁciency. We ﬁrst illustrate that SNFs can break topo-logical constraints and improve the representational power of deterministic normalizing ﬂows at a given network size and at the same time beat direct MCMC in terms of sampling efﬁciency. To this end we use images to deﬁne complex two-dimensional densities (Fig. 3a-c, “Exact”) as target densities µX (x) to be sampled. Note that a benchmark aiming at generating high-quality images would instead represent the image as a high-dimensional pixel array. We compare three types of ﬂows with 5 blocks each trained by samples from the exact density (details in Suppl. Material Sec. 9): 1. Normalizing ﬂow with 2 swapped coupling layers (RealNVP or neural spline ﬂow) per block 2. Non-trainable stochastic ﬂow with 10 Metropolis MC steps per block 3. SNF with both, 2 swapped coupling layers and 10 Metropolis MC steps per block. 5
Figure 3: Sampling of two-dimensional densities. a-c) Sampling of smiley, dog and text densities with different methods. Columns: (1) Normalizing Flow with RealNVP layers, (2) Metropolis
MC sampling, (3) Stochastic Normalizing Flow combining (1+2), (4) neural spline ﬂow (NSF), (5) Stochastic Normalizing Flow combining (1+4), (6) Unbiased sample from exact density. d-e)
Compare representative power and statistical efﬁciency of different ﬂow methods by showing KL divergence (mean and standard deviation over 3 training runs) between ﬂow samples and true density for the three images from Fig. 3. d) Comparison of deterministic ﬂows (black) and SNF (red) as a function of the number of RealNVP or Neural Spline Flow transformations. Total number of MC steps in SNF is ﬁxed to 50. e) Comparison of pure Metropolis MC (black) and SNF (red, solid line
RealNVP, dashed line Neural spline ﬂow) as a function of the number of MC steps. Total number of
RealNVP or NSF transformations in SNF is ﬁxed to 10.
The pure Metropolis MC ﬂow suffers from sampling problems – density is still concentrated in the image center from the prior. Many more MC steps would be needed to converge to the exact density (see below). The RealNVP normalizing ﬂow architecture [6] has limited representational power, resulting in a “smeared out” image that does not resolve detailed structures (Fig. 3a-c, RNVP). As expected, neural spline ﬂows perform signiﬁcantly better on the 2D-images than RealNVP ﬂows, but at the chosen network architecture their ability to resolve ﬁne details and round shapes is still limite (See dog and small text in Fig. 3c, NSF). Note that the representational power for all ﬂow architectures tend to increase with depth - here we compare the performance of different architectures at ﬁxed depth and similar computational cost.
In contrast, SNFs achieve high-quality approximations although they simply combine the same deterministic and stochastic ﬂow components that fail individually in the SNF learning framework (Fig. 3a-c, RNVP+Metropolis and NSF+Metropolis). This indicates that the SNF succeeds in performing the large-scale probability mass transport with the trainable ﬂow layers and sampling the details with Metropolis MC.
Fig. 3d-e quantiﬁes these impressions by computing the KL divergence between generated densities pX (x) and exact densities µX (x). Both normalizing ﬂows and SNFs improve with greater depth, but
SNFs achieve signiﬁcantly lower KL divergence at a ﬁxed network depth (Fig. 3d). Note that both
RealNVP and NSFs improve signiﬁcantly when stochasticty is added.
Moreover, SNFs have higher statistical efﬁciency than pure Metropolis MC ﬂows. Depending on the example and ﬂow architecture, 1-2 orders of magnitude more Metropolis MC steps are needed to achieve similar KL divergence as with an SNF. This demonstrates that the large-scale probability transport learned by the trainable deterministic ﬂow blocks in SNFs signiﬁcantly helps with the sampling.
Importantly, adding stochasticity is very inexpensive. Although every MCMC or Langevin inte-gration step adds a neural network layer, these layers are very lightweighted, and have only linear computational complexity in the number of dimensions. As an example, for our SNF implementation of the examples in Fig. 3 we can add 10-20 stochastic layers to each trainable normalizing ﬂow layer before the computational cost increases by a factor of 2 (Suppl. Material Fig. S2). 6
Figure 4: Reweighting results for the double well potential (see also Fig. 1). Free energy along x1 (negative log of marginal density) for deterministic normalizing ﬂows (RNVP, NSF) and
SNFs (RNVP+MC, NSF+MC). Black: exact energy, red: energy of proposal density pX (x), green: reweighted energy using importance sampling.
SNFs as asymptotically unbiased samplers. We demonstrate that SNFs can be used as Boltzmann
Generators, i.e., to sample target densities without asymptotic bias by revisiting the double-well example (Fig. 1). Fig. 4 (black) shows the free energies (negative marginal density) along the double-well coordinate x1. Flows with 3 coupling layer blocks (RealNVP or neural spline ﬂow) are trained summing forward and reverse KL divergence as a joint loss using either data from a biased distribution, or with the unbiased distribution (Details in Suppl. Material Sec. 9). Due to limitations in representational power the generation probability pX (x) will be biased – even when explicitly minimizing the KL divergence w.r.t. the true unbiased distribution in the joint loss. By relying on importance sampling we can turn the ﬂows into Boltzmann Generators [32] in order to obtain unbiased estimates. Indeed all generator densities pX (x) can be reweighted to an estimate of the unbiased density µX (x) whose free energies are within statistical error of the exact result (Fig. 4, red and green). var) of
We inspect the bias, i.e. the error of the mean estimator, and the statistical uncertainty ( the free energy in x1 ∈ {−2.5, 2.5} with and without reweighting using a ﬁxed number of samples (100,000). Using SNFs with Metropolis MC steps, both biases and uncertainties are reduced by half compared to purely deterministic ﬂows (Table 1). Note that neural spline ﬂows perform better than
RealNVP without reweighting, but signiﬁcantly worse with reweighting - presumably because the sharper features representable by splines can be detrimental for reweighting weights. With stochastic layers, both RealNVP and neural spline ﬂows perform approximately equally well.
The differences between multiple runs (see standard deviations of the uncertainty estimate) also reduce signiﬁcantly, i.e. SNF results are more reproducible than RealNVP ﬂows, conﬁrming that the training problems caused by the density connection between both modes (Fig. 1, Suppl. Material
Fig. S1) can be reduced. Moreover, the sampling performance of SNF can be further improved by optimizing MC step sizes based on loss functions JKL and JM L (Suppl. Material Table S1).
Reweighting reduces the bias at the expense of a higher variance. Especially in physics applications, a small or asymptotically zero bias is often very important, and the variance can be reduced by generating more samples from the trained ﬂow, which is relatively cheap and parallel.
Table 1: Unbiased sampling for double well potential: mean uncertainty of the reweighted energy along x1 averaged over 10 independent runs (± standard deviation).
√ not reweighted var
√ (cid:112) bias 1.4 ± 0.6
RNVP
RNVP + MC 1.5 ± 0.2 0.8 ± 0.4
NSF 0.4 ± 0.3
NSF + MC 0.4 ± 0.1 0.3 ± 0.1 1.0 ± 0.2 0.5 ± 0.1 bias2+var 1.5 ± 0.5 1.5 ± 0.2 1.3 ± 0.3 0.7 ± 0.2 bias 0.3 ± 0.2 0.2 ± 0.1 0.6 ± 0.2 0.1 ± 0.1 reweighted var
√ 1.1 ± 0.4 0.6 ± 0.1 2.1 ± 0.4 0.6 ± 0.2 (cid:112) bias2+var 1.2 ± 0.4 0.6 ± 0.1 2.2 ± 0.5 0.6 ± 0.2
Alanine dipeptide. We further evaluate SNFs on density estimation and sampling of molecular structures from a simulation of the alanine dipeptide molecule in vacuum (Fig. 5). The molecule has 66 dimensions in x, and we augment it with 66 auxiliary dimensions in a second channel v, similar 7
(cid:16) to “velocities” in a Hamiltonian ﬂow framework [42], resulting in 132 dimensions total. The target density is given by µX (x, v) = exp
, where u(x) is the potential energy of the 2 (cid:107)v(cid:107)2 is the kinetic energy term. µZ is an isotropic Gaussian normal distribution in molecule and 1 all dimensions. We utilize the invertible coordinate transformation layer introduced in [32] in order to transform x into normalized bond, angle and torsion coordinates. RealNVP transformations act between the x and v variable groups Details in Suppl. Material Sec. 9). 2 (cid:107)v(cid:107)2(cid:17)
−u(x) − 1
We compare deterministic normalizing ﬂows using 5 blocks of 2 RealNVP layers with SNFs that additionally use 20 Metropolis MC steps in each block totalling up to 100 MCMC steps in one forward pass. Fig. 5a shows random structures sampled by the trained SNF. Fig. 5b shows marginal densities in all ﬁve multimodal torsion angles (backbone angles φ, ψ and methyl rotation angles γ1,
γ2, γ3). While the RealNVP networks that are state of the art for this problem miss many of the modes, the SNF resolves the multimodal structure and approximates the target distribution better, as quantiﬁed in the KL divergence between the generated and target marginal distributions (Table 2). a b
Figure 5: Alanine dipeptide sampled with deterministic normalizing ﬂows and stochastic normaliz-ing ﬂows. a) One-shot SNF samples of alanine dipeptide structures. b) Energy (negative logarithm) of marginal densities in 5 unimodal torsion angles (top) and all 5 multimodal torsion angles (bottom).
Table 2: Alanine dipeptide: KL-divergences of RNVP ﬂow and SNF (RNVP+MCMC) between generated and target distributions for all multimodal torsion angles. Mean and standard deviation from 3 independent runs.
KL-div.
RNVP
SNF
φ 1.69±0.03 0.36 ± 0.05
γ1 3.82±0.01 0.21 ±0.01
ψ 0.98±0.03 0.27 ±0.03
γ2 0.79±0.03 0.12 ±0.02
γ3 0.79±0.09 0.15 ±0.04
Variational Inference. Finally, we use normalizing ﬂows to model the latent space distribution of a variational autoencoder (VAE) , as suggested in [35]. Table 3 shows results for the variational bound and the log likelihood on the test set for MNIST [23] and Fashion-MNIST [44]. For a 50-dimensional latent space we compare a six-layer RNVP to MCMC using overdamped Langevin dynamics as proposal (MCMC) and a SNF combining both (RNVP+MCMC). Both sampling and the deterministic
ﬂow improve over a naive VAE using a reparameterized diagonal Gaussian variational posterior distribution, while the SNF outperforms both, RNVP and MCMC. See Suppl. Material Sec. 8 for details.
Table 3: Variational inference using VAEs with stochastic normalizing ﬂows: JKL: variational bound of the KL-divergence computed during training. NLL: negative log likelihood of test set.
MNIST
Fashion-MNIST
JKL 108.4±24.3 91.8±0.4 102.1±8.0 89.7±0.1
NLL 98.1±4.2 87.0±0.2 96.2±1.9 86.8±0.1
JKL 241.3±7.4 233.7±0.1 234.7±0.4 232.4±0.2
NLL 238.0±2.9 231.4±0.2 235.2±2.4 230.9±0.2
Naive (Gaussian)
RNVP
MCMC
SNF 8
5