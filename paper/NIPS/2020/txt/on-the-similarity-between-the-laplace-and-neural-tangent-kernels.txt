Abstract
Recent theoretical work has shown that massively overparameterized neural net-works are equivalent to kernel regressors that use Neural Tangent Kernels (NTKs).
Experiments show that these kernel methods perform similarly to real neural net-works. Here we show that NTK for fully connected networks with ReLU activation is closely related to the standard Laplace kernel. We show theoretically that for normalized data on the hypersphere both kernels have the same eigenfunctions and their eigenvalues decay polynomially at the same rate, implying that their Repro-ducing Kernel Hilbert Spaces (RKHS) include the same sets of functions. This means that both kernels give rise to classes of functions with the same smoothness properties. The two kernels differ for data off the hypersphere, but experiments indicate that when data is properly normalized these differences are not signiﬁcant.
Finally, we provide experiments on real data comparing NTK and the Laplace kernel, along with a larger class of γ-exponential kernels. We show that these perform almost identically. Our results suggest that much insight about neural networks can be obtained from analysis of the well-known Laplace kernel, which has a simple closed form. 1

Introduction
Neural networks with signiﬁcantly more parameters than training examples have been successfully applied to a variety of tasks. Somewhat contrary to common wisdom, these models typically generalize well to unseen data. It has been shown that in the limit of inﬁnite model size, these neural networks are equivalent to kernel regression using a family of novel Neural Tangent Kernels (NTK)
[26, 2]. NTK methods can be analyzed to explain many properties of neural networks in this limit, including their convergence in training and ability to generalize [8, 9, 13, 32]. Recent experimental work has shown that in practice, kernel methods using NTK perform similarly, and in some cases better, than neural networks [4], and that NTK can be used to accurately predict the dynamics of neural networks [1, 2, 7]. This suggests that a better understanding of NTK can lead to new ways to analyze neural networks.
These results raise an important question: Is NTK signiﬁcantly different from standard kernels? For the case of fully connected (FC) networks, [4] provides experimental evidence that NTK is especially effective, showing that it outperforms the Gaussian kernel on a large suite of machine learning problems. Consequently, they argue that NTK should be added to the standard machine learning 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Left: An overlay of the NTK for a 6-layer FC network with ReLU activation with the Laplace and
Gaussian kernels, as a function of the angle between their arguments. The exponential kernels are modulated by an afﬁne transformation to achieve a least squares ﬁt to the NTK. Note the high degree of similarity between the
Laplace kernel and NTK. Middle left: eigenvalues as a function of frequency in S1. The slopes in these log-log plots indicate the rate of decay, which is similar for both the Laplace kernel and for NTK for the FC network with 6 layers. (Empirical slopes are -1.94 for both Laplace and NTK-FC.) The eigenvalues of the Gaussian kernel, in contrast, decay exponentially. Middle right: Same for S2. (Empirical slopes are -2.75 for the Laplace and NTK-FC.) Right: Same estimated for the UCI Abalone dataset (here we show eigenvalues as function of eigenvalue index). toolbox. [9] has shown empirically that the dynamics of neural networks on randomly labeled data more closely resembles the dynamics of learning through stochastic gradient descent with the Laplace kernel than with the Gaussian kernel. In this paper we show theoretically and experimentally that
NTK does closely resemble the Laplace kernel, already a standard tool of machine learning.
Kernels are mainly characterized by their corresponding Reproducing Kernel Hilbert Space (RKHS), which determines the set of functions they can produce [28]. They are further characterized by the
RKHS norm they induce, which is minimized (implicitly) in every regression problem. Our main result is that when restricted to the hypersphere Sd−1, NTK for a fully connected (FC) network with ReLU activation and bias has the same RKHS as the Laplace kernel, deﬁned as kLap(x, z) = e−c(cid:107)x−z(cid:107) for points x, z ∈ Sd−1 and constant c > 0. (In general, NTK for deeper networks is more sharply peaked, corresponding to larger values of c, see supplementary material.) This equivalence of RKHSs is shown by establishing that on the hypersphere the eigenfunctions of NTK and the
Laplace kernels coincide and their eigenvalues decay at the same rate (see Figure 1), implying in turn that gradient descent (GD) with both kernels should have the same dynamics, explaining [9]’s experiments. In previous work, the eigenfunctions and eigenvalues of NTK have been derived on the hypersphere for networks with only one hidden layer, while these properties of the Laplace kernel have been studied in Rd. We derive new results for the Laplace kernel on the hypersphere, and for
NTK for deep networks on the hypersphere and in Rd. In Rd, NTK gives rise to radial eigenfunctions, forgoing the shift invariance property of exponential kernels. Experiments indicate that this difference is not signiﬁcant in practice.
Finally, we show experiments indicating that the Laplace kernel achieves similar results to those obtained with NTK on real-world problems. We further show that by using the more general, γ-exponential kernel [41], which allows for one additional parameter, kγ(x, z) = e−c(cid:107)x−z(cid:107)γ
, we achieve slightly better performance than NTK on a number of standard datasets. 2