Abstract
Effectively leveraging large, previously collected datasets in reinforcement learn-ing (RL) is a key challenge for large-scale real-world applications. Ofﬂine RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, ofﬂine RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing ofﬂine RL methods, often learning policies that attain 2-5 times higher ﬁnal return, especially when learning from complex and multi-modal data distributions. 1

Introduction
Recent advances in reinforcement learning (RL), especially when combined with expressive deep net-work function approximators, have produced promising results in domains ranging from robotics [29] to strategy games [4] and recommendation systems [35]. However, applying RL to real-world problems consistently poses practical challenges: in contrast to the kinds of data-driven methods that have been successful in supervised learning [22, 10], RL is classically regarded as an active learning process, where each training run requires active interaction with the environment. Interaction with the real world can be costly and dangerous, and the quantities of data that can be gathered online are substantially lower than the ofﬂine datasets that are used in supervised learning [9], which only need to be collected once. Ofﬂine RL, also known as batch RL, offers an appealing alterna-tive [11, 15, 30, 3, 27, 54, 34]. Ofﬂine RL algorithms learn from large, previously collected datasets, without interaction. This in principle can make it possible to leverage large datasets, but in practice fully ofﬂine RL methods pose major technical difﬁculties, stemming from the distributional shift between the policy that collected the data and the learned policy. This has made current results fall short of the full promise of such methods.
Directly utilizing existing value-based off-policy RL algorithms in an ofﬂine setting generally results in poor performance, due to issues with bootstrapping from out-of-distribution actions [30, 15] and overﬁtting [13, 30, 3]. This typically manifests as erroneously optimistic value function estimates. If we can instead learn a conservative estimate of the value function, which provides a lower bound on the true values, this overestimation problem could be addressed. In fact, because policy evaluation 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
and improvement typically only use the value of the policy, we can learn a less conservative lower bound Q-function, such that only the expected value of Q-function under the policy is lower-bounded, as opposed to a point-wise lower bound. We propose a novel method for learning such conservative Q-functions via a simple modiﬁcation to standard value-based RL algorithms. The key idea behind our method is to minimize values under an appropriately chosen distribution over state-action tuples, and then further tighten this bound by also incorporating a maximization term over the data distribution.
Our primary contribution is an algorithmic framework, which we call conservative Q-learning (CQL), for learning conservative, lower-bound estimates of the value function, by regularizing the Q-values during training. Our theoretical analysis of CQL shows that only the expected value of this Q-function under the policy lower-bounds the true policy value, preventing extra under-estimation that can arise with point-wise lower-bounded Q-functions, that have typically been explored in the opposite context in exploration literature [46, 26]. We also empirically demonstrate the robustness of our approach to Q-function estimation error. Our practical algorithm uses these conservative estimates for policy evaluation and ofﬂine RL. CQL can be implemented with less than 20 lines of code on top of a number of standard, online RL algorithms [19, 8], simply by adding the CQL regularization terms to the Q-function update. In our experiments, we demonstrate the efﬁcacy of CQL for ofﬂine RL, in domains with complex dataset compositions, where prior methods are typically known to perform poorly [12] and domains with high-dimensional visual inputs [5, 3]. CQL outperforms prior methods by as much as 2-5x on many benchmark tasks, and is the only method that can outperform simple behavioral cloning on a number of realistic datasets collected from human interaction.
,
,
S
∈
A
A represent state and action spaces, T (s(cid:48) s) represents the behavior policy, s). The dataset 2 Preliminaries
The goal in reinforcement learning is to learn a policy that maximizes the expected cumulative
, T, r, γ). discounted reward in a Markov decision process (MDP), which is deﬁned by a tuple ( s, a) and r(s, a) represent the dynamics and reward
|
S (0, 1) represents the discount factor. πβ(a function, and γ
| is the dataset, and dπβ (s) is the discounted marginal state-distribution of πβ(a
| s,a∈D 1[s=s,a=a] is sampled from dπβ (s)πβ(a s). On all states s (cid:80) s∈D 1[s=s]
| empirical behavior policy, at that state. We assume that the rewards r satisfy: r(s, a)
|
Off-policy RL algorithms based on dynamic programming maintain a parametric Q-function Qθ(s, a) s). Q-learning methods train the Q-function by iteratively and, optionally, a parametric policy, πφ(a
|
∗Q(s, a) = r(s, a) + γEs(cid:48)∼P (s(cid:48)|s,a)[maxa(cid:48) Q(s(cid:48), a(cid:48))], applying the Bellman optimality operator and use exact or an approximate maximization scheme, such as CEM [29] to recover the greedy policy. In an actor-critic algorithm, a separate policy is trained to maximize the Q-value. Actor-critic methods alternate between computing Qπ via (partial) policy evaluation, by iterating the
πQ = r + γP πQ, where P π is the transition matrix coupled with the policy:
Bellman operator,
P πQ(s, a) = Es(cid:48)∼T (s(cid:48)|s,a),a(cid:48)∼π(a(cid:48)|s(cid:48)) [Q(s(cid:48), a(cid:48))] , and improving the policy π(a s) by updating it
| towards actions that maximize the expected Q-value. Since typically does not contain all possible transitions (s, a, s(cid:48)), the policy evaluation step actually uses an empirical Bellman operator that only backs up a single sample. We denote this operator ˆ of tuples
B from trajectories collected under a behavior policy πβ:
D
D denote the
Rmax. s) :=
, let ˆπβ(a
|
π. Given a dataset (s, a, rs(cid:48))
}
∈ D
| ≤
=
D
D
B
B (cid:80)
{
ˆQk+1 ← arg min
Q
Es,a,s(cid:48)∼D (cid:20)(cid:16) (r(s, a) + γE a(cid:48)∼ˆπk(a(cid:48)|s(cid:48))[ ˆQk(s(cid:48), a(cid:48))]) − Q(s, a) (cid:17)2(cid:21) (policy evaluation)
ˆπk+1 ← arg max
π
E s∼D,a∼πk(a|s) (cid:104) ˆQk+1(s, a) (cid:105) (policy improvement)
Ofﬂine RL algorithms based on this basic recipe suffer from action distribution shift [30, 59, 27, 34] during training, because the target values for Bellman backups in policy evaluation use actions sampled from the learned policy, πk, but the Q-function is trained only on actions sampled from the
, πβ. Since π is trained to maximize Q-values, it may be behavior policy that produced the dataset biased towards out-of-distribution (OOD) actions with erroneously high Q-values. In standard RL, such errors can be corrected by attempting an action in the environment and observing its actual value.
However, the inability to interact with the environment makes it challenging to deal with Q-values for OOD actions in ofﬂine RL. Typical ofﬂine RL methods [30, 27, 59, 54] mitigate this problem by constraining the learned policy [34] away from OOD actions. Note that Q-function training in ofﬂine
RL does not suffer from state distribution shift, as the Bellman backup never queries the Q-function on out-of-distribution states. However, the policy may suffer from state distribution shift at test time.
D 2
3 The Conservative Q-Learning (CQL) Framework
In this section, we develop a conservative Q-learning algorithm, such that the expected value of a policy under the learned Q-function lower-bounds its true value. Lower-bounded Q-values prevent the over-estimation that is common in ofﬂine RL settings due to OOD actions and function approximation error [34, 30]. We use the term CQL to refer broadly to both Q-learning and actor-critic methods. We start with the policy evaluation step in CQL, which can be used by itself as an off-policy evaluation procedure, or integrated into a complete ofﬂine RL algorithm, as we will discuss in Section 3.2. 3.1 Conservative Off-Policy Evaluation
We aim to estimate the value V π(s) of a target policy π given access to a dataset,
, generated by following a behavior policy πβ(a s). Because we are interested in preventing overestimation of the
| policy value, we learn a conservative, lower-bound Q-function by additionally minimizing Q-values alongside a standard Bellman error objective. Our choice of penalty is to minimize the expected Q-value under a particular distribution of state-action pairs, µ(s, a). Since standard Q-function training does not query the Q-function value at unobserved states, but queries the Q-function at unseen actions, we restrict µ to match the state-marginal in the dataset, such that µ(s, a) = dπβ (s)µ(a s). This gives
| rise to the iterative update for training the Q-function, as a function of a tradeoff factor α 0: (cid:17)2(cid:21) (cid:20)(cid:16)
D
α Es∼D,a∼µ(a|s) [Q(s, a)] +
Es,a∼D
Q(s, a) − ˆBπ ˆQk(s, a)
≥
, (1)
ˆQk+1 ← arg min
Q 1 2
, a
∈ D
∈ A
In Theorem 3.1, we show that the resulting Q-function, ˆQπ := limk→∞ ˆQk, lower-bounds Qπ at
. However, we can substantially tighten this bound if we are only interested in all s estimating V π(s). If we only require that the expected value of the ˆQπ under π(a s) lower-bound
|
V π, we can improve the bound by introducing an additional Q-value maximization term under the s), resulting in the iterative update (changes from Equation 1 in red): data distribution, πβ(a
| (cid:0)Es∼D,a∼µ(a|s) [Q(s, a)]
α arg min
ˆQk+1
←
Q
·
Es∼D,a∼ˆπβ (a|s) [Q(s, a)](cid:1) (cid:20)(cid:16)
Es,a,s(cid:48)∼D
Q(s, a)
− 1 2
+
π ˆQk(s, a)
ˆ
B
− (cid:17)2(cid:21)
. (2)
≤ s) = π(a
|
V π(s) when µ(a
In Theorem 3.2, we show that, while the resulting Q-value ˆQπ may not be a point-wise lower-bound, we have Eπ(a|s)[ ˆQπ(s, a)] s). Intuitively, since Equation 2
| maximizes Q-values under the behavior policy ˆπβ, Q-values for actions that are likely under ˆπβ might be overestimated, and hence ˆQπ may not lower-bound Qπ pointwise. While in principle the s), we prove in Appendix D.2 that maximization term can utilize other distributions besides ˆπβ(a
| s), the resulting value is not guaranteed to be a lower bound for other distribution besides ˆπβ(a
| though other distributions besides ˆπβ(a s) can still yield a lower bound if the Bellman error is also
| re-weighted to come from the distribution chosen to maximize the expected Q-value.
Theoretical analysis. We ﬁrst note that Equations 1 and 2 use the empirical Bellman operator, ˆ
π,
B
π. Following [47, 26, 45], we use concentration properties instead of the actual Bellman operator, of ˆ
B
Cr,T ,δ
√|D(s,a)| and T (s(cid:48)
ˆπβ(a (0, 1) (see Appendix D.3 for details). For simplicity, we assume that
∈ s
, containing square root
∀
≤
, where Cr,T,δ is a constant dependent on the concentration properties (variance) of r(s, a)
π to control this error. Formally, for all s, a s, a), and δ
| a s) > 0,
| denote a vector of size
, with probability (s, a)
| 1
√|D|
ˆ
π
B
|
|S||A| (s, a) = 0, in which case the corresponding inverse counts for each state-action pair, except when 2Rmax entry is a very large but ﬁnite value δ 1−γ . Now, we show that the conservative Q-function learned by iterating Equation 1 lower-bounds the true Q-function. Proofs can be found in Appendix C.
δ, ˆQπ (the Q-function supp ˆπβ, with probability
. Let
∈ A
∈ D
∈ D
− B
≥
≥
−
D
δ,
B
∀ 1 1
π
Theorem 3.1. For any µ(a s) with supp µ
| obtained by iterating Equation 1) satisiﬁes:
⊂
≥
−
, a, ˆQπ(s, a) s
∀
∈ D
Qπ(s, a)
≤ (cid:20)
α (I (cid:21)
γP π)−1 µ
ˆπβ
−
− (cid:34) (s, a) + (I
−
Thus, if α is sufﬁciently large, then ˆQπ(s, a) guarantees ˆQπ(s, a)
, a
Qπ(s, a),
≤ s
∀
∈ D
Qπ(s, a),
≤
.
∈ A 3 (cid:35) (s, a).
γP π)−1 Cr,T,δRmax (1
−
|D|
π, any α > 0
γ)(cid:112)
π = s
∀
∈ D
, a. When ˆ
B
B
Next, we show that Equation 2 lower-bounds the expected value under the policy π, when µ = π. We also show that Equation 2 does not lower-bound the Q-value estimates pointwise. For this result, we refers to a vector of inverse square root of only state counts, abuse notation and assume that 1
√|D| with a similar correction as before used to handle the entries of this vector at states with zero counts.
Theorem 3.2 (Equation 2 results in a tighter lower bound). The value of the policy under the Q-function from Equation 2, ˆV π(s) = Eπ(a|s)[ ˆQπ(s, a)], lower-bounds the true value of the policy obtained via exact policy evaluation, V π(s) = Eπ(a|s)[Qπ(s, a)], when µ = π, according to:
, ˆV π(s) s
∀
∈ D
V π(s)
−
≤ (cid:20)
α (I
−
γP π)−1 Eπ (cid:20) π
ˆπβ − (cid:21)(cid:21) 1 (cid:34) (s) + (I
−
Thus, if α > Cr,T Rmax 1−γ
· with probability 1
−
≥ maxs∈D
δ. When ˆ
B 1
|√|D(s)| ·
π =
B (cid:104)(cid:80) a π(a
| s)( π(a|s) (cid:105)−1 1)
ˆπβ (a|s)) −
π, then any α > 0 guarantees ˆV π(s) (cid:35) (s).
γP π)−1 Cr,T,δRmax (1
|D|
−
, ˆV π(s)
γ)(cid:112)
V π(s),
V π(s),
≤
≤ s
∀
.
∈ D
, s
∀
∈ D
The analysis presented above assumes that no function approximation is used in the Q-function, meaning that each iterate can be represented exactly. We can further generalize the result in Theo-rem 3.2 to the case of both linear function approximators and non-linear neural network function approximators, where the latter builds on the neural tangent kernel (NTK) framework [25]. Due to space constraints, we present these results in Theorem D.1 and Theorem D.2 in Appendix D.1.
In summary, we showed that the basic CQL evaluation in Equation 1 learns a Q-function that lower-bounds the true Q-function Qπ, and the evaluation in Equation 2 provides a tighter lower bound on the expected Q-value of the policy π. For suitable α, both bounds hold under sampling (s, a) error and function approximation. We also note that as more data becomes available and
| increases, the theoretical value of α that is needed to guarantee a lower bound decreases, which indicates that in the limit of inﬁnite data, a lower bound can be obtained by using extremely small values of α. Next, we will extend on this result into a complete RL algorithm.
|D 3.2 Conservative Q-Learning for Ofﬂine RL
We now present a general approach for ofﬂine policy learning, which we refer to as conservative
Q-learning (CQL). As discussed in Section 3.1, we can obtain Q-values that lower-bound the value of a policy π by solving Equation 2 with µ = π. How should we utilize this for policy optimization?
We could alternate between performing full off-policy evaluation for each policy iterate, ˆπk, and one step of policy improvement. However, this can be computationally expensive. Alternatively, since the policy ˆπk is typically derived from the Q-function, we could instead choose µ(a s) to approximate
| the policy that would maximize the current Q-function iterate, thus giving rise to an online algorithm.
We can formally capture such online algorithms by deﬁning a family of optimization problems over
µ(a s), presented below, with modiﬁcations from Equation 2 marked in red. An instance of this
| family is denoted by CQL( (µ):
R
α (cid:0)Es∼D,a∼µ(a|s) [Q(s, a)]
) and is characterized by a particular choice of regularizer
Es∼D,a∼ˆπβ (a|s) [Q(s, a)](cid:1)
− (cid:20)(cid:16) (cid:17)2(cid:21)
R min
Q max
µ
Es,a,s(cid:48)∼D
Q(s, a)
πk ˆQk(s, a)
ˆ
B
−
+ (µ) (CQL(
)) . (3)
R
R
+ 1 2
Variants of CQL. To demonstrate the generality of the CQL family of objectives, we discuss two speciﬁc instances within this family that are of special interest, and we evaluate them empirically s), i.e., in Section 6. If we choose
| exp(Q(s, a)) (for a derivation, see Appendix A). s)
ρ(a
DKL(µ, ρ), then we get µ(a
R
|
|
Frist, if ρ = Unif(a), then the ﬁrst term in Equation 3 corresponds to a soft-maximum of the Q-values at any state s and gives rise to the following variant of Equation 3, called CQL( (µ) to be the KL-divergence against a prior distribution, ρ(a (µ) =
R s)
∝
−
):
· (cid:34) min
Q
αEs∼D log (cid:88) a exp(Q(s, a))−Ea∼ˆπβ (a|s) [Q(s, a)]
+ (cid:35) 1 2
Es,a,s(cid:48)∼D (cid:20)(cid:16)
H
Q − ˆBπk ˆQk(cid:17)2(cid:21)
. (4)
Second, if ρ(a
| an exponential weighted average of Q-values of actions from the chosen ˆπk−1(a s) is chosen to be the previous policy ˆπk−1, the ﬁrst term in Equation 4 is replaced by s). Empirically, we
| 4
B
∗ instead of
ﬁnd that this variant can be more stable with high-dimensional action spaces (e.g., Table 2) where it is challenging to estimate log (cid:80) a exp via sampling due to high variance. In Appendix A, we discuss an additional variant of CQL, drawing connections to distributionally robust optimization [43]. We will discuss a practical instantiation of a CQL deep RL algorithm in Section 4. CQL can be instantiated as
π in Equations 3, 4) or as an actor-critic algorithm. either a Q-learning algorithm (with
B
Theoretical analysis of CQL. Next, we will theoretically analyze CQL to show that the policy updates derived in this way are indeed “conservative”, in the sense that each successive policy iterate is optimized against a lower bound on its value. For clarity, we state the results in the absence of ﬁnite-sample error, in this section, but sampling error can be incorporated in the same way as
) learns
Theorems 3.1 and 3.2, and we discuss this in Appendix C. Theorem 3.3 shows that CQL(
Q-value estimates that lower-bound the actual Q-function under the action-distribution deﬁned by the policy, πk, under mild regularity conditions (slow updates on the policy).
Theorem 3.3 (CQL learns lower-bounded Q-values). Let π ˆQk (a that DTV(ˆπk+1, π ˆQk )
≤ lower-bounds the actual policy value, ˆV k+1(s) (cid:35) 1 exp( ˆQk(s, a)) and assume
ε (i.e., ˆπk+1 changes slowly w.r.t to ˆQk). Then, the policy value under ˆQk, s
∀
V k+1(s) s)
|
H (cid:33)
∝
≤
ε.
Eπ ˆQk (a|s) max a s.t. ˆπβ (a|s)>0
≥ if
∈ D (cid:32) π ˆQk (a s)
| s)
ˆπβ(a
|
· (cid:34) π ˆQk (a s)
| s) −
ˆπβ(a
|
The LHS of this inequality is equal to the amount of conservatism induced in the value, ˆV k+1 in iteration k + 1 of the CQL update, if the learned policy were equal to soft-optimal policy for ˆQk, i.e., when ˆπk+1 = π ˆQk . However, as the actual policy, ˆπk+1, may be different, the RHS is the maximal amount of potential overestimation due to this difference. To get a lower bound, we require the amount of underestimation to be higher, which is obtained if ε is small, i.e. the policy changes slowly.
Our ﬁnal result shows that CQL Q-function update is “gap-expanding”, by which we mean that the difference in Q-values at in-distribution actions and over-optimistically erroneous out-of-distribution actions is higher than the corresponding difference under the actual Q-function. This implies that the policy πk(a s), thus the CQL update implicitly prevents the detrimental effects of OOD action and distribution shift, which has been a major concern in ofﬂine RL settings [30, 34, 15].
Theorem 3.4 (CQL is gap-expanding). At any iteration k, CQL expands the difference in expected s) and µk, such that for large enough values of αk, we have
Q-values under the behavior policy πβ(a
|
Eµk(a|s)[Qk(s, a)]. that exp( ˆQk(s, a)), is constrained to be closer to the dataset distribution, ˆπβ(a
|
Eµk(a|s)[ ˆQk(s, a)] > Eπβ (a|s)[Qk(s, a)]
, Eπβ (a|s)[ ˆQk(s, a)] s)
|
∝ s
∀
∈ D
−
−
When function approximation or sampling error makes OOD actions have higher learned Q-values,
CQL backups are expected to be more robust, in that the policy is updated using Q-values that prefer in-distribution actions. As we will empirically show in Appendix B, prior ofﬂine RL methods that do not explicitly constrain or regularize the Q-function may not enjoy such robustness properties.
To summarize, we showed that the CQL RL algorithm learns lower-bound Q-values with large enough α, meaning that the ﬁnal policy attains at least the estimated value. We also showed that the Q-function is gap-expanding, meaning that it should only ever over-estimate the gap between in-distribution and out-of-distribution actions, preventing OOD actions. 3.3 Safe Policy Improvement Guarantees
In Section 3.1 we proposed novel objectives for Q-function training such that the expected value of a policy under the resulting Q-function lower bounds the actual performance of the policy. In
Section 3.2, we used the learned conservative Q-function for policy improvement. In this section, we show that this procedure actually optimizes a well-deﬁned objective and provide a safe policy improvement result for CQL, along the lines of Theorems 1 and 2 in Laroche et al. [33].
To begin with, we deﬁne empirical return of any policy π, J(π, ˆM ), which is equal to the discounted return of a policy π in the empirical MDP, ˆM , that is induced by the transitions observed in the
. J(π, M ) refers to the expected discounted return attained by a dataset policy π in the actual underlying MDP, M . In Theorem 3.5, we ﬁrst show that CQL (Equation 2) optimizes a well-deﬁned penalized RL empirical objective. All proofs are found in Appendix D.4.
, i.e. ˆM = s, a, r, s(cid:48)
∈ D}
D
{ 5
Theorem 3.5. Let ˆQπ be the ﬁxed point of Equation 2, then π∗(a is equivalently represented as: π∗(a where DCQL(π, πβ)(s) := (cid:80) arg maxπ J(π, ˆM ) (cid:17) 1
← (cid:16) π(a|s)
−
. s)
| s) a π(a
|
·
πβ (a|s) − s) := arg maxπ Es∼ρ(s)[ ˆV π(s)]
|
α 1 (s) [DCQL(π, ˆπβ)(s)], 1−γ
Es∼dπ
ˆM
Intuitively, Theorem 3.5 says that CQL optimizes the return of a policy in the empirical MDP, ˆM , while also ensuring that the learned policy π is not too different from the behavior policy, ˆπβ via a penalty that depends on DCQL. Note that this penalty is implicitly introduced by virtue by the gap-expanding (Theorem 3.4) behavior of CQL. Next, building upon Theorem 3.5 and the analysis of
CPO [1], we show that CQL provides a ζ-safe policy improvement over ˆπβ.
Theorem 3.6. Let π∗(a s) be the policy obtained in Theorem 3.5. Then, the policy π∗(a
|
|
ζ-safe policy improvement over ˆπβ in the actual MDP M , i.e., J(π∗, M ) probability 1 s) is a
ζ with high
J(ˆπβ, M )
≥
−
δ, where ζ is given by, (cid:34) (cid:112) (cid:19)
−
γ (cid:18)
ζ =
O (1
γ)2
−
E s∼dπ∗
ˆM (s)
|A|(cid:112) (s)
|
|D (cid:113)
DCQL(π∗, ˆπβ)(s) + 1 (cid:35)
− (cid:16) (cid:17)
J(π∗, ˆM ) − J(ˆπβ, ˆM ) (cid:123)(cid:122)
[DCQL(π∗,ˆπβ )(s)] (cid:125)
E s∼dπ∗
ˆM (s) (cid:124)
≥ α 1−γ
The expression of ζ in Theorem 3.6 consists of two terms: the ﬁrst term captures the decrease in policy performance in M , that occurs due to the mismatch between ˆM and M , also referred to as sampling error. The second term captures the increase in policy performance due to CQL in empirical
MDP, ˆM . The policy π∗ obtained by optimizing π against the CQL Q-function improves upon the behavior policy, ˆπβ for suitably chosen values of α. When sampling error is small, i.e., (s) is
| large, then smaller values of α are enough to provide an improvement over the behavior policy.
|D
To summarize, CQL optimizes a well-deﬁned, penalized empirical RL objective, and performs high-conﬁdence safe policy improvement over the behavior policy. The extent of improvement is negatively inﬂuenced by higher sampling error, which decays as more samples are observed.
Algorithm 1 Conservative Q-Learning (both variants) 1: Initialize Q-function, Qθ, and optionally a policy, πφ. 2: for step t in {1, . . . , N} do 3: 4 Practical Algorithm and Implementation Details
We now describe two practical ofﬂine deep reinforcement learning methods based on CQL: an actor-critic vari-ant and a Q-learning variant. Pseu-docode is shown in Algorithm 1, with differences from conventional actor-critic algorithms (e.g., SAC [19]) and deep Q-learning algorithms (e.g.,
DQN [39]) in red. Our algorithm uses the CQL(
) in general) objective from the CQL framework for training the Q-function Qθ, which is parameterized by a neural network with parameters θ. For the actor-critic algorithm, a policy πφ is trained as well. Our algorithm modiﬁes the objective for the
)) or CQL(ρ) in a standard actor-critic or Q-learning
Q-function (swaps out Bellman error with CQL( setting, as shown in Line 3. As discussed in Section 3.2, due to the explicit penalty on the Q-function,
CQL methods do not use a policy constraint, unlike prior ofﬂine RL methods [30, 59, 54, 34]. Hence, we do not require ﬁtting an additional behavior policy estimator, simplifying our method.
Train the Q-function using GQ gradient steps on objective from Equation 4
θt := θt−1 − ηQ∇θCQL(R)(θ) (Use B∗ for Q-learning, Bπφt for actor-critic) (only with actor-critic) Improve policy πφ via Gπ gradient steps on φ with SAC-style entropy regularization:
φt := φt−1 + ηπEs∼D,a∼πφ(·|s)[Qθ(s, a)−log πφ(a|s)]
) (or CQL( 5: end for
R
H
H 4:
Implementation details. Our algorithm requires an addition of only 20 lines of code on top of standard implementations of soft actor-critic (SAC) [19] for continuous control experiments and on top of QR-DQN [8] for the discrete control. The tradeoff factor, α is is ﬁxed at constant values described in Appendix F for gym tasks and discrete control and is automatically tuned via Lagrangian dual gradient descent for other domains. We use default hyperparameters from SAC, except that the learning rate for the policy was chosen from {3e-5, 1e-4, 3e-4}, and is less than or equal to the
Q-function, as dictated by Theorem 3.3. Elaborate details are provided in Appendix F. 5