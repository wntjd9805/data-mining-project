Abstract
We extend randomized smoothing to cover parameterized transformations (e.g., rotations, translations) and certify robustness in the parameter space (e.g., rotation angle). This is particularly challenging as interpolation and rounding effects mean that image transformations do not compose, in turn preventing direct certiﬁcation of the perturbed image (unlike certiﬁcation with (cid:96)p norms). We address this challenge by introducing three different defenses, each with a different guarantee (heuris-tic, distributional and individual) stemming from the method used to bound the interpolation error. Importantly, in the individual case, we show how to efﬁciently compute the inverse of an image transformation, enabling us to provide individual guarantees in the online setting. We provide an implementation of all methods at https://github.com/eth-sri/transformation-smoothing. 1

Introduction
Deep neural networks are vulnerable to adversarial examples [1] – small changes that preserve semantics (e.g., (cid:96)p-noise or geometric transformations such as rotations) [2], but can affect the output of a network in undesirable ways. As a result, there has been substantial recent interest in methods which aim to ensure the network is certiﬁably robust to adversarial examples [3–13].
Certiﬁcation guarantees There are two principal robustness guarantees a certiﬁed defense can provide at inference time: (i) the (standard) distributional guarantee, where a robustness score is computed ofﬂine on the test set to be interpreted in expectation for images drawn from the data distribution, and (ii) an individual guarantee, where a certiﬁcate is computed online for the (possibly perturbed) input. The choice of guarantee depends on the application and regulatory constraints.
Guarantees with (cid:96)p norms When considering (cid:96)p norms, existing certiﬁcation methods can be directly used to obtain either of the above two guarantees: for an image x and adversarial noise
δ, (cid:107)δ(cid:107)p < r, proving that a classiﬁer f is r-robust around x(cid:48) := x + δ is enough to guarantee f (x) = f (x(cid:48)). That is, it sufﬁces to prove robustness of a perturbed input in order to certify that the perturbation did not change the classiﬁcation, as the r-ball around x(cid:48) includes x.
Key challenge: guarantees for geometric perturbations Perhaps not intuitively, however, for more complex perturbations such as geometric transformations, proving robustness around an image x(cid:48) via existing methods (e.g., [9–12]) does not imply that f (x) = f (x(cid:48)) for the original image x. To illustrate this issue, consider the rotation Rγ, by angle γ of an image x, followed by an interpolation
I. Certifying that the classiﬁcation of the rotated image x(cid:48) := I ◦ Rγ(x) for (cid:107)γ(cid:107) < r is robust under further rotations I ◦ Rβ for (cid:107)β(cid:107) < r is not sufﬁcient to imply that x and x(cid:48) classify the same, as rotating x(cid:48) back by β = −γ does not return the original image x due to interpolation. A central challenge then is to develop techniques that are able to handle more involved perturbations. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
This work: certiﬁcation beyond (cid:96)p norms
In this work we address this challenge and introduce the ﬁrst certiﬁcation methods for geometric transformations based on smoothing which provide the above two guarantees. Concretely, we extend randomized smoothing [7] to handle parameterized transformations (SPT) by adding (Gaussian) noise to the parameters of a transformation, enabling us to handle large models and datasets (e.g., ImageNet). We present three methods, BASESPT,
DISTSPT and INDIVSPT yielding different guarantees.
BASESPT The guarantees provided by SPT hold for composable parameterized perturbations ψγ, that is ψβ+γ = ψγ ◦ ψβ. SPT can be applied directly to obtain both a distributional guarantee and an individual guarantee. However, if used with non-composable transformations (e.g., rotations with interpolation), BASESPT will yield a heuristic guarantee.
DISTSPT By estimating a probabilistic upper bound for (cid:15) = (cid:107)ψβ ◦ ψγ(x) − ψβ+γ(x)(cid:107)2 ofﬂine on the training dataset, SPT can be combined with randomized smoothing (yielding DISTSPT) to provide the standard distributional guarantee for non-composable transformations.
INDIVSPT To obtain an individual guarantee for non-composable transformations, we calculate at inference time for each input x(cid:48) an individual upper bound of the expression (cid:15) without access to x, which is combined with SPT and smoothing to yield INDIVSPT. A key step here is computing the
γ (x(cid:48)) of a transformed image x(cid:48), for which we introduce an efﬁcient technique. inverse ψ−1
We remark that all three methods are suitable for online use as a defense and the choice of method depends on the particular trade-offs. To summarize, our core contributions are:
• A generalization of randomized smoothing to parameterized transformations.
• A certiﬁcation method which provides a distributional guarantee by calculating an upper bound on the interpolation error on the training dataset.
• A certiﬁcation method which provides an individual guarantee based on an algorithm that efﬁciently calculates the inverse of an image.
• A thorough evaluation of all methods on common image datasets, achieving provable distributional robust accuracy of 73% for rotations with up to ±30◦ on Restricted ImageNet. 2