Abstract
There is a rich and growing literature on producing local contrastive/counterfactual explanations for black-box models (e.g. neural networks). In these methods, for an input, an explanation is in the form of a contrast point differing in very few features from the original input and lying in a different class. Other works try to build globally interpretable models like decision trees and rule lists based on the data using actual labels or based on the black-box models predictions. Although these interpretable global models can be useful, they may not be consistent with local explanations from a speciﬁc black-box of choice. In this work, we explore the question: Can we produce a transparent global model that is simultaneously accurate and consistent with the local (contrastive) explanations of the black-box model? We introduce a natural local consistency metric that quantiﬁes if the local explanations and predictions of the black-box model are also consistent with the proxy global transparent model. Based on a key insight we propose a novel method where we create custom boolean features from sparse local contrastive explanations of the black-box model and then train a globally transparent model on just these, and showcase empirically that such models have higher local consistency compared with other known strategies, while still being close in performance to models that are trained with access to the original data. 1

Introduction
With the ever increasing adoption of black-box artiﬁcial intelligence technologies in various facets of society [12], a number of explainability algorithms have been developed to understand their decisions.
Two prominent types are: a) Local Explanations for a data point of interest [16, 20, 22, 25, 9] and b) Constructing interpretable global models directly like decision trees, rule lists and boolean rules
[23, 10, 7]. One of the arguments for b) is that it is possible to construct interpretable global models in such a way that for a single data point it can give a succinct local explanation in the form of a sparse conjunction [23] while also highlighting non-trivial global behavior of the output. Methods in a) naturally enjoy the parsimonious explanations on a single data point through either feature importance scores or contrastive points that differ in very few features. Therefore, one generally has the option of adopting an accurate black-box model (viz. Neural Network, XGBoost) with local explanations obtained with additional computational effort or adopt a simpler but interpretable model that can describe non local behavior as well as provide very fast local explanations. For tabular data there is a growing set of techniques to build models of the later kind directly [23] that can compete in 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Above we see the main steps behind our approach, where the key idea is to make the generated boolean clause match with predictions on the triplet (original point x, its PP p(x), its
PN n(x)) of the black-box (i.e. match input prediction and local explanation). To do this we ﬁrst obtain local explanations, then perform a custom discretization and binning obtaining upper and lower bounds for PPs (g(i) nu, g(i) pl and nu are just rounding of the PP and the PN values for feature i to the nearest grid point. While g(i) g(i) pu and g(i) nl ensure that the clause would be local to x. A new dataset with clauses created as shown above can serve as input to a simple model training algorithm. nl ) producing the ﬁnal boolean clauses. g(i) pl ) and PNs (g(i) pu , g(i) terms of accuracy with complex black-box models. However, black-box models have demonstrated performance in terms of higher accuracy and in many cases are still preferred [21, 1].
In domains such as ﬁnance and healthcare, one would like to offer explanations (local and global) that are loyal to the model deployed. Proxy models that provide global interpretability [11, 10, 5] may not capture the original model’s local behavior. This might be important in many practical applications
[19, 2], where we want the global interpretable proxy model to replicate the local behavior of the black-box model as much as possible. Given this need, we in this paper address the following two important questions: a) How do we measure if a simple model replicates the local behavior of a given black-box? b) Given access to a black-box model and a local explainbility method, how do we better train simple models belonging to a speciﬁc class that are accurate and good at mimicing the local behavior of the black-box model? Moreover, we want to answer b), where we primarily have access to local sparse explanations.
We consider local explanation methods [29, 8] that produce contrast point(s) for every input point.
Contrast points for a given data point are two different perturbations of the point such that the number of features altered are minimal. The ﬁrst type of perturbation retains minimal amount of information from the original point so that the new point is still classiﬁed by the model in the same class and is termed as a pertinent positive (PP). The second is a perturbation of the input point which would make it classify into a different class and is termed as a pertinent negative (PN). For example in a loan approval application, say a person’s loan was rejected whose debt was 30,000, age was 27 and salary was 50,000. If reducing their debt to 20,000 still resulted in a rejection, where other factors had to remain the same, then the proﬁle of debt 20,000, age 27 and salary 50,000 would be a PP. If on the other hand, their salary increasing to 70,000 with other things being the same, led to the loan being approved, then the resultant point would be a PN.
With this, to address the ﬁrst question above we propose a natural consistency metric where we check if a candidate transparent model agrees in the classiﬁcation of the original point along with the contrast points produced by a local explanation method for some black-box model. To address the second question, one of our key insights is that each sparse local explanation in the form of contrast points can be transformed into a logical conjunction of conditions on few features with a 2
custom discretization scheme. Ideally, an approach of building a global transparent model from local explanations of a black-box model would retain the structure (sparse interaction of various features) of the local explanations.
We thus propose a new algorithm that uses local explanations from a contrastive explanations method to generate boolean clauses which are conjunctions. These boolean conjunctions can be used as features, forming a new dataset, to train another simple model such as logistic regression or a small decision tree. An illustration of the whole process that we just described is given in Figure 1. The algorithm binarizes local contrastive explanations depending on the difference in feature values between the contrast point and the original. The binarization of features is directed by local explanations based on ranges that are deemed locally important by the black-box model. One of the most interesting aspects of this idea is that sparse interactions between original features required for explaining, are directly captured by these boolean clauses that can be used to train transparent models that generalize and thus effectively capture also global information. 2