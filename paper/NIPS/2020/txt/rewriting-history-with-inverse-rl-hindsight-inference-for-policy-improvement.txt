Abstract
Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward functions can improve sample efﬁciency. Relabeling methods typically pose the question: if, in hindsight, we assume that our experience was optimal for some task, for what task was it optimal? Inverse RL answers this question. In this paper we show that inverse RL is a principled mechanism for reusing experience across tasks. We use this idea to generalize goal-relabeling techniques from prior work to arbitrary types of reward functions. Our experiments conﬁrm that relabeling data using inverse RL outperforms prior relabeling methods on goal-reaching tasks, and accelerates learning on more general multi-task settings where prior methods are not applicable, such as domains with discrete sets of rewards and those with linear reward functions. 1

Introduction
Reinforcement learning (RL) aims to acquire control policies that take actions to maximize their reward, though existing RL algorithms remain data inefﬁcient [11, 26]. Multi-task RL, where many
RL problems are solved in parallel, has the potential to be more sample efﬁcient than single-task RL, as data can be shared across tasks. Nonetheless, the problem of effectively sharing data across tasks remains largely unsolved.
The idea of sharing data across tasks has been studied at least since the 1990s [5]. More recently, a number of works have observed that retroactive relabeling of experience with different tasks can improve data efﬁciency [3, 24]. A common theme in prior relabeling methods is to relabel past trials with whatever goal or task was performed successfully in that trial. For example, in a goal-reaching task, we might use the state actually reached at the end of the trajectory as the relabeled goal, since the trajectory corresponds to a successful trial for the goal that was actually reached [3, 38]. However, prior work on goal relabeling is inapplicable to more general reward functions, such as discrete sets of reward functions or tasks deﬁned by varying linear combinations of reward terms.
In this paper, we formalize prior relabeling techniques under the umbrella of inverse RL: by inferring the most likely task for a given trial via inverse RL, we provide a principled formula for relabeling in arbitrary multi-task problems. Inverse RL is not the same as evaluating a trajectory under all tasks and choosing whichever task yielded the highest reward. In fact, this strategy would often result in assigning most trajectories to the easiest task. Rather, inverse RL automatically takes into account the difﬁculty of each task by normalizing each reward function by the partition function.
RL and inverse RL can be seen as complementary tools for maximizing reward: RL takes tasks and produces high-reward trajectories, and inverse RL takes trajectories and produces task labels such that the trajectories receive high reward. Formally, we prove that maximum entropy (MaxEnt) RL and MaxEnt inverse RL optimize the same multi-task objective: MaxEnt RL optimizes with respect
∗Equal contribution. Correspondence to beysenba@cs.cmu.edu 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Hindsight Inference for Policy Improvement (HIPI): Given a dataset of prior experience, we use inverse RL to infer the intentions of the agent’s own past experience. We then use the relabeled experience with any policy learning algorithm, such as off-policy RL or supervised learning. to trajectories, while MaxEnt inverse RL optimizes with respect to tasks. Unlike prior goal-relabeling techniques, we can use inverse RL to relabel experience for arbitrary task distributions, including linear or discrete reward sets. This observation suggests that RL and inverse RL might be combined to efﬁciently solve many tasks simultaneously. The combination we develop, Hindsight Inference for Policy Improvement (HIPI), ﬁrst relabels experience with inverse RL and then uses the relabeled experience to learn a task-conditioned policy (see Fig. 1). One variant of this framework follows the same design as prior value-based goal-relabeling methods [3, 24, 38] but uses inverse RL to relabel experience, a difference that allows our method to handle arbitrary task families. The second variant has a similar design to self-imitation behavior cloning methods [15, 33, 45]: we relabel past experience using inverse RL and then learn a policy via task-conditioned behavioral cloning. Both algorithms are probabilistic reinterpretations and generalizations of prior work.
The main contribution of our paper is the observation that inverse RL is a principled mechanism for reusing experience across tasks. This observation not only provides insight into success of prior relabeling methods, but it also provides guidance on applying relabeling to arbitrary multi-task RL problems. Our second contribution is two simple algorithms that use inverse RL-based relabeling to accelerate multi-task RL. These algorithms do not require expert demonstrations, but rather perform inverse RL on the agent’s own (possibly-random) past experience. Our experiments on complex simulated locomotion and manipulation tasks demonstrate that our approach outperforms state-of-the-art multi-task RL methods. 2