Abstract
A residual network may be regarded as a discretization of an ordinary differential equation (ODE) which, in the limit of time discretization, deﬁnes a continuous-depth network. Although important steps have been taken to realize the advantages of such continuous formulations, most current techniques assume identical lay-ers. Indeed, existing works throw into relief the myriad difﬁculties of learning an inﬁnite-dimensional parameter in a continuous-depth neural network. To this end, we introduce a shooting formulation which shifts the perspective from pa-rameterizing a network layer-by-layer to parameterizing over optimal networks described only by a set of initial conditions. For scalability, we propose a novel particle-ensemble parameterization which fully speciﬁes the optimal weight tra-jectory of the continuous-depth neural network. Our experiments show that our particle-ensemble shooting formulation can achieve competitive performance. Fi-nally, though the current work is inspired by continuous-depth neural networks, the particle-ensemble shooting formulation also applies to discrete-time networks and may lead to a new fertile area of research in deep learning parameterization. 1

Introduction
Deep neural networks (DNNs) are closely related to optimal control (OC) where the sought-for control variable corresponds to the parameters of the DNN [24, 23, 19]. To be able to talk about an optimal control requires the deﬁnition of a control cost, i.e., a norm on the control variable.
We explore the ramiﬁcations of such a control cost in the context of DNN parameterization. For simplicity, we focus on continuous formulations in the spirit of neural ODEs [13]. However, both discrete and continuous OC formulations exist [12, 4, 37]; our approach could be developed for both.
Initial work on continuous DNN formulations was motivated by the realization that a ResNet
[20, 21] resembles Euler forward time-integration [19, 23]. Speciﬁcally, the forward pass of some input vector ˜x ∈ Rd through a network with L layers, speciﬁed as x(0) = ˜x and x(j + 1) = x(j) + f (x(j), θ(j)), j = 0, 1, . . . , L, closely relates to an explicit Euler [36] discretization of the
ODE (1.1)
In the continuous DNN formulation, we seek an optimal θ such that the terminal prediction given by x(T ), i.e., the solution to Eq. (1.1) at time T , minimizes (cid:96)(x(T )) for a task-speciﬁc loss function (cid:96).
˙x(t) = f (t, x(t), θ(t)), x(0) = ˜x, 0 ≤ t ≤ T .
Although Eq. (1.1) with time-varying parameter θ(t) can be considered as a neural network with an inﬁnite number of layers, current implementations of ODE-inspired networks largely assume the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Optimization in the neural ODE (NODE) framework [13] (left) amounts to a forward pass with the gradient computed via backpropagation (
). Optimization under the shooting principle (middle) turns the forward-backward system into a forward second-order system, where we essentially run the backpropagation equation forward. We use a Hamiltonian particle ensemble (right) consisting of K (position, momentum) pairs (qj, pj) to make shooting efﬁcient. Note that we write θ = θ({(qj, pj)}K j=1) since θ satisﬁes a compatibility equation which involves all K particles. In shooting θ is time-dependent, in standard NODE θ(t) = θ ∀t. parameters θ are ﬁxed in time, i.e., ∀t : θ(t) = θ [13, 14], or follow some prescribed dynamics [44].
Instead, we explore time-varying θ(t) by employing regularization (i.e., a control cost) to render the estimation well-posed and to assure regularity of the resulting ﬂow. Speciﬁcally, (for a single data point) we propose minimizing over θ the regularized loss
E(θ) = (cid:90) T 0
R(θ(t)) dt + γ (cid:96)(x(T )),
γ ∈ R+, subject to Eq. (1.1) , (1.2) where R is a real-valued complexity measure of θ corresponding to the control cost. We will mostly work with the Frobenius norm but R(θ(t)) can be more general (see Appendix B).
Instead of directly optimizing over the set of time-dependent θ(t) as in standard ResNets, we restrict the optimization set to those θ which are critical points of E(θ), thereby dramatically reducing the number of parameters. In doing so, one can describe the optimization task as an initial value problem.
Namely, we show that we can rewrite the loss in Eq. (1.2) solely in terms of the input x(0) and a corresponding ﬁnite-dimensional momentum variable, p(0). Such an approach, just like optimizing the initial speed of a mass particle to reach a given point, is called a shooting method in numerical analysis [30] and control [10], giving its name to our new formulation.
The ﬁrst two panels of Fig. 1 illustrate the difference between the optimization of a neural ODE (NODE) via [13] and our shooting formulation. Since in practice, we have multiple inputs ˜xi, i = 1, . . . , n, there is an initial momentum vector pi corresponding to each of them. If the shooting formulation is to scale up to a large sample size n, we must take care that the parameterization does not grow linearly with n. To this end, we propose what we call the Hamiltonian particle-ensemble parameterization. It is a ﬁnite set of particles, where each particle is a (position, momentum) pair.
The initial conditions of these particle pairs {(qj, pj)}K j=1 (where K (cid:28) n) completely determine
θ(t). This is illustrated in the rightmost panel of Fig. 1. Once the optimized set of particles has been computed, the computational efﬁciency of the forward model, similarly to NODE [13], is retained for vector ﬁelds f that are linear in their parameters θ(t).
Our contributions are as follows: 1) We introduce a shooting formulation for DNNs, amounting to an initial-value formulation for neural network parameterization. This allows for optimization over the original network parameter space via optimizing over the initial conditions of critical networks only; 2) We propose an efﬁcient implementation of the shooting approach based on a novel particle-ensemble parameterization in which a set of initial particles (the (position, momentum) pairs) describe the space of putative optimal network parameters; 3) We propose the UpDown model which gives rise to explicit shooting equations; 4) We prove universality for the ﬂows of the UpDown vector ﬁeld and demonstrate in experiments its good performance on several prediction tasks. 2