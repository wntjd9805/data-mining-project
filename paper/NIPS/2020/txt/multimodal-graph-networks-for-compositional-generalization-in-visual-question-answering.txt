Abstract
Compositional generalization is a key challenge in grounding natural language to visual perception. While deep learning models have achieved great success in multimodal tasks like visual question answering, recent studies have shown that they fail to generalize to new inputs that are simply an unseen combination of those seen in the training distribution [6]. In this paper, we propose to tackle this challenge by employing neural factor graphs to induce a tighter coupling between concepts in different modalities (e.g. images and text). Graph representations are inherently compositional in nature and allow us to capture entities, attributes and relations in a scalable manner. Our model ﬁrst creates a multimodal graph, processes it with a graph neural network to induce a factor correspondence matrix, and then outputs a symbolic program to predict answers to questions. Empirically, our model achieves close to perfect scores on a caption truth prediction problem and state-of-the-art results on the recently introduced CLOSURE dataset, improving on the mean overall accuracy across seven compositional templates by 4.77% over previous approaches.2 1

Introduction
In this paper, we explore the problem of systematic generalization to novel questions in the paradigm of visual question answering (VQA) [4]. Several neural architectures have shown great promise in learning multimodal representations to solve the task [42, 39, 54]. Recently, neuro-symbolic methods [55, 38] – a combination of connectionist and symbolic approaches – have pushed these limits, achieving close to perfect scores on benchmarks like CLEVR [28, 29]. However, these models lack the ability to generalize to new combinations of linguistic constructs, even if the distribution of visual inputs remains the same [6]. A key reason behind this is the lack of ﬁne-grained representations that allow for joint compositional reasoning over both visual and linguistic spaces.
We propose a graph-based approach for multi-modal representation learning – the Multimodal Graph
Network (MGN) – with an explicit focus on enabling better generalization. Our core idea is the following: representing both text and image as graphs naturally allows for tighter coupling of concepts between the two modalities and provides a compositional space for reasoning. Speciﬁcally, we ﬁrst parse both the image and text into individual graphs with object entities and attributes as nodes and relations as edges. Then, we induce a correspondence factor matrix between pairs of nodes from both modalities using message passing algorithms similar to the ones used in graph neural networks [16].
The output of our model is a matrix of values that captures the correspondence value of each pair of
∗Work done at Princeton as a Fulbright Scholar. 2Code is available at https://github.com/raeidsaqur/mgn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
individual nodes between modalities. A high-level overview of our approach is shown in Figure 1, where one can observe ﬁne-grained connections between nodes from graphs of both modalities.
Our work is different from previous approaches to multimodal learning for VQA since we explicitly parse and represent both modalities. This provides two key advantages – 1) it allows for learning ex-plicit correspondences between visual concepts and linguistic symbols at a ﬁne-grained level, and 2) our representations scale smoothly to longer pieces of text (e.g. questions) with novel compositions of lin-guistic constructs. Our entire model is end-to-end differentiable and can be trained using supervision provided in the end task (e.g. classiﬁcation or VQA).
We evaluate MGN on two tasks – a binary classi-ﬁcation task of predicting if a caption matches an image based on attribute compositions in the CLEVR dataset [28], and CLOSURE [6] – a recently released challenge for testing systematic generalization in lan-guage. We show the efﬁcacy of our approach with strong compositional reasoning skills, achieving a
≈98% average accuracy rate on statement truth pre-diction on previously unseen image feature combina-tions at test time. Further, on CLOSURE, our model outperforms state-of-the-art approaches by almost 4.77% overall accuracy, especially achieving impres-sive results on compositional questions entailing log-ical (and,or) objet relations. 2