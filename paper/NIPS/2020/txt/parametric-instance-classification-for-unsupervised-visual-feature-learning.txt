Abstract
This paper presents parametric instance classiﬁcation (PIC) for unsupervised vi-sual feature learning. Unlike the state-of-the-art approaches which do instance discrimination in a dual-branch non-parametric fashion, PIC directly performs a one-branch parametric instance classiﬁcation, revealing a simple framework similar to supervised classiﬁcation and without the need to address the information leakage issue. We show that the simple PIC framework can be as effective as the state-of-the-art approaches, i.e. SimCLR and MoCo v2, by adapting several common component settings used in the state-of-the-art approaches. We also propose two novel techniques to further improve effectiveness and practicality of PIC: 1) a sliding-window data scheduler, instead of the previous epoch-based data sched-uler, which addresses the extremely infrequent instance visiting issue in PIC and improves the effectiveness; 2) a negative sampling and weight update correction approach to reduce the training time and GPU memory consumption, which also enables application of PIC to almost unlimited training images. We hope that the
PIC framework can serve as a simple baseline to facilitate future study. The code and network conﬁgurations are available at https://github.com/bl0/PIC. 1

Introduction
Visual feature learning has long been dominated by supervised image classiﬁcation tasks, e.g.
ImageNet-1K classiﬁcation. Recently, unsupervised visual feature learning has started to demon-strate on par or superior transfer performance on several downstream tasks compared to supervised approaches [15, 5, 22]. This is encouraging, as unsupervised visual feature learning could utilize nearly unlimited data without annotations.
These unsupervised approaches [15, 22, 5, 6], which achieve revolutionary performance, are all built on the same pre-text task of instance discrimination [12, 32, 27], where each image instance is treated as a distinct class. To solve the instance discrimination task, often a dual-branch structure is employed where two augmentation views from the same image are encouraged to agree and views from different images to disperse. In general, for dual-branch approaches, special designs are usually required to address the information leakage issue, e.g. specialized networks [1], specialized BatchNorm layers [15, 5], momentum encoder [15], and limited negative pairs [5].
Unlike these dual-branch non-parametric approaches, this paper presents a framework which solves instance discrimination by direct parametric instance classiﬁcation (PIC) [12]. PIC is a one-branch scheme where only one view for each image is required per iteration, which avoids the need to
∗Equal Contribution. The work is done when Zhenda Xie, Bin Liu and Yutong Lin are interns at Microsoft
Research Asia. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
carefully address the information leakage issue. PIC can also be easily adapted from the simple supervised classiﬁcation frameworks. We additionally show that PIC can be as effective as the state-of-the-art approaches [6, 5] by adopting several recent advances, including a cosine soft-max loss, a stronger data augmentation and a 2-layer projection head.
There remain issues regarding effectiveness and practicality on large data. Speciﬁcally, PIC faces an extremely infrequent instance visiting issue, where each instance is visited as positive only once per epoch, which hinders representation learning in the PIC framework. PIC also has a practicality issue with respect to training time and GPU memory consumption, as there is a large classiﬁcation weight matrix to use and update.
Two novel techniques are proposed to address these two issues. The ﬁrst is a sliding window based data scheduler to replace the typical epoch-based one, to shorten the distance between two visits of the same instance class for the majority of instances. It proves to signiﬁcantly speed up convergence and improve the effectiveness. The second is a negative instance class sampling approach for loss computation along with weight update correction, to make the training time and GPU memory consumption near constant with increasing data size while maintaining effectiveness.
We hope the PIC framework could serve as a simple baseline to facilitate future study, because of its simplicity, effectiveness, and ability to incorporate improvements without considering the information leakage issue. The code and network conﬁgurations are available at https://github.com/bl0/
PIC. 2 Methodology 2.1 Parametric Instance Classiﬁcation (PIC) Framework
Figure 1: An illustration of the PIC framework.
PIC learns representations by parametric instance classiﬁcation, where each instance is treated as a distinct class. Like common supervised classiﬁcation frameworks [16], it consists of ﬁve major components: i. a data scheduler that feeds training images into networks during the course of training; ii. a data augmentation module that randomly augments each training example fed into the network; iii. a backbone network that extracts a feature map for each augmented image, which will also be transferred to downstream tasks; iv. a small projection head that projects the feature map to a feature vector for which the instance classiﬁcation loss is applied; v. an instance classiﬁcation loss that penalizes the classiﬁcation errors in training.
While directly applying the usual component settings of supervised category classiﬁcation for PIC will result in poor transfer performance as shown in Table 1, we show that there is no intrinsic limitation in the PIC framework, in contrast to the inherent belief in previous works [32]. We show that poor performance is mainly due to improper component settings. By replacing several usual component settings with the ones used in recent unsupervised frameworks [5, 6], including a cosine soft-max loss, a stronger data augmentation and a 2-layer MLP projection head, the transfer performance of the learnt features in the PIC framework are signiﬁcantly improved. The cosine soft-max loss is commonly used in metric learning approaches [29, 30] and also in recent state-of-the-art unsupervised learning frameworks [15, 5], as
L = − 1
|B| (cid:88) i∈B log exp (cos (wi, zi) /τ ) j=1 exp (cos (wj, zj) /τ ) (cid:80)N
, (1) where W = [w1, w2, ..., wN ] ∈ RD×N is the parametric weight matrix of the cosine classiﬁer;
B denotes the set of instance indices in a mini-batch; zi is the projected feature for instance i; 2
Figure 2: An illustration of the sliding window data scheduler. cos(wj, zi) = (wj · zi)/((cid:107)wj(cid:107)2 · (cid:107)zi(cid:107)2) is the cosine similarity between wj and zi; and τ is a scalar temperature hyper-parameter. While the choice of standard or cosine soft-max loss in supervised classiﬁcation is insigniﬁcant, the cosine soft-max loss in PIC performs signiﬁcantly better than the standard soft-max loss. The stronger data augmentation and 2-layer MLP projection head are introduced in [5], and we ﬁnd they also beneﬁt the PIC framework.
There are still obstacles preventing the PIC framework from better representation learning, e.g. the optimization issue caused by too infrequent visiting of each instance class (see Section 2.2). The PIC framework also has practicality problems regarding training time and GPU memory consumption, especially when the data size is large (see Section 2.3). To further improve the quality of learnt feature representations as well as to increase practicality, two novel techniques are proposed: (a) A novel sliding window data scheduler to replace the usual epoch-based data scheduler. The new scheduler well addresses the issue that each instance class is visited too infrequently in unsupervised instance classiﬁcation (e.g. once per epoch). (b) A negative instance class sampling approach for loss computation along with weight update correction, which makes the training time and GPU memory consumption near constant with increasing data size, and maintains the effectiveness of using all negative classes for loss computation.
With the above improvements, the PIC framework can be as effective as the state-of-the-art frame-works, such as SimCLR [5] and MoCo v2 [6]. It is also practical regarding training time and GPU memory consumption. 2.2 Sliding Window Data Scheduler
When a regular epoch-based data loader is employed, each instance class will be visited exactly once per epoch, which is extremely infrequent when the training data is large, e.g. more than 1 million images. The extremely infrequent class visits may affect optimization and likely result in sub-optimal feature learning. While it may beneﬁt optimization to decrease the visiting interval of each instance class, it is theoretically difﬁcult in the sense of expectation, according to the following proposition.
Proposition 1. Denote the number of images in an epoch as N . For an arbitrary data scheduler which visits each instance once per epoch, the expectation of distance between two consecutive visits of the same instance class is N .
To address this dilemma, our idea is to maintain a relatively low distance between visits (denoted as D) for the majority (the majority ratio is denoted as γ) of instance classes during training. To this goal, we propose a sliding window data scheduler, as illustrated in Figure 2. In contrast to the regular data scheduler which traverses over the whole set of training images epoch-by-epoch, the proposed sliding window data scheduler traverses images within windows, with the next window shifted from the previous one. There are overlaps between successive windows, and the overlapped instance classes are regarded as the majority, and thus visited twice in a relatively short time.
Hence, the window size W is equal to the distance between consecutive visits for majority classes,
W = D. And the sliding stride S is the number of instances by which the window shifts for each step, S = (1 − γ) × W = (1 − γ) × D, where γ represents the majority ratio within a window. As 3
the majority ratio γ and the average distance D between consecutive visits for majority classes are both constant for increasing number of total training images, this indicates that larger training data leads to almost no increase of the optimization issue by using the sliding window data scheduler.
Like the regular data scheduler, before each traversal, images in the current window are shufﬂed to introduce stochastic regularization.
In experiments, for the majority ratio γ = 87.5% with relatively low visiting distance D = 217 = 131, 072, we need to set the window size to W = 131, 072 and the sliding stride to S = 214 = 16, 384.
This performs noticeably better than the regular data scheduler on the ImageNet-1k dataset, which has 1.2M training images. For different data scales, the window size W and stride S may be adjusted accordingly to obtain better performance. 2.3 Negative Instance Sampling and Classiﬁcation Weight Correction
The vanilla PIC framework has a practical limitation on large-scale data (e.g billions of images) during the training phase. This limitation comes from two aspects: 1) in the forward/backward phase, logits of all negative classes are employed in the denominator of the soft-max computation in
Eq. (1); 2) in the weight update phase, since the commonly used SGD typically has weight decay and momentum terms, even if weights in the instance classiﬁer are not used in the current batch, they will still be updated and synchronized. As a result, the training time and GPU memory consumption are linearly increased w.r.t the data size, limiting practicability on large-scale data.
We propose two approaches to signiﬁcantly reduce the training time and GPU memory consumption, making them near constant with increasing data size. The ﬁrst is recent negative sampling to address the issues in the forward/backward phase, and the second is classiﬁcation weight update correction to address the issues in the weight update phase.
Recent negative sampling employs only the most recent K instances in recent iterations as the negative instance classes in the soft-max computation, which appear in the denominator of the cosine soft-max loss in Eq. (1). Hence, the computation costs in the forward phase to compute the loss and the backward phase to compute the gradients are reduced. As shown in Table 2, we ﬁnd that
K = 65536 achieves accuracy similar to the counterpart using all instances (about 1.28M) with 200-epoch pre-training on ImageNet.
Classiﬁcation weight update correction In the SGD optimizer, the weights are updated as: u(t+1) i
:= mu(t) i + (g(t) i + λw(t) i ), w(t+1) i
:= w(t) i − ηu(t+1) i
, (2) i and u(t) i where g(t) are the gradient and momentum terms at iteration t for classiﬁcation weight w(t) of instance class i, respectively; λ, m and η are the weight decay, momentum scalar and learning rate, respectively. Even when a negative instance class i is not included in the denominator of the soft-max loss computation, where g(t) i will still be updated due to the weight decay and momentum terms being non-zero, as shown in Eq. (2). Hence,
GPU memory consumption in the weight update and synchronization process cannot be reduced.
If we directly ignore the weight decay and momentum terms for the non-sampled negative classes, the different optimization statistics between sampled and non-sampled negative classes lead to a signiﬁcant decrease in accuracy. i = 0, the classiﬁcation weight vector w(t) i
Noticing that the update trajectories for the classiﬁcation weights of non-sampled negative classes are predictable (only affected by the weight decay and momentum terms), we propose a closed-form one-step correction approach to account for the effect of momentum and weight decay: (cid:19) (cid:18)w(t+t(cid:48)) i u(t+t(cid:48)) i
:= (cid:18)1 − η · λ −η · m
λ m (cid:19) (cid:19)t(cid:48) (cid:18)w(t) i u(t) i (3) where t(cid:48) is the distance from the last visit to the current visit for instance class i. The detailed deduction and implementation are shown in Appendix B.
Hence, the non-sampled negative classes are not required to be stored in GPU memory and updated at the current iteration, and the weight correction procedure is performed the next time these classes are sampled. The GPU memory consumption is thus determined by the number of sampled negatives
K, and are constant with respect to the data size. 4
3