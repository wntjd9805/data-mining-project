Abstract
DBSCAN is a popular density-based clustering algorithm. It computes the (cid:15)-neighborhood graph of a dataset and uses the connected components of the high-degree nodes to decide the clusters. However, the full neighborhood graph may be too costly to compute with a worst-case complexity of Opn2q. In this paper, we propose a simple variant called SNG-DBSCAN, which clusters based on a subsampled (cid:15)-neighborhood graph, only requires access to similarity queries for pairs of points and in particular avoids any complex data structures which need the embeddings of the data points themselves. The runtime of the procedure is Opsn2q, where s is the sampling rate. We show under some natural theoretical assumptions that s « log n{n is sufﬁcient for statistical cluster recovery guarantees leading to an Opn log nq complexity. We provide an extensive experimental analysis showing that on large datasets, one can subsample as little as 0.1% of the neighborhood graph, leading to as much as over 200x speedup and 250x reduction in RAM consumption compared to scikit-learn’s implementation of DBSCAN, while still maintaining competitive clustering performance. 1

Introduction
DBSCAN [13] is a popular density-based clustering algorithm which has had a wide impact on machine learning and data mining. Recent applications include superpixel segmentation [40], object tracking and detection in self-driving [51, 19], wireless networks [12, 53], GPS [10, 36], social network analysis [29, 55], urban planning [14, 38], and medical imaging [46, 3]. The clusters that
DBSCAN discovers are based on the connected components of the neighborhood graph of the data points of sufﬁciently high density (i.e. those with a sufﬁciently high number of data points in their neighborhood), where the neighborhood radius and the density threshold are the hyperparameters.
One of the main differences of density-based clustering algorithms such as DBSCAN compared to popular objective-based approaches such as k-means [2] and spectral clustering [50] is that density-based algorithms are non-parametric. As a result, DBSCAN makes very few assumptions on the data, automatically ﬁnds the number of clusters, and allows clusters to be of arbitrary shape and size [13]. However, one of the drawbacks is that it has a worst-case quadratic runtime [16]. With the continued growth of modern datasets in both size and richness, non-parametric unsupervised procedures are becoming ever more important in understanding such datasets. Thus, there is a critical need to establish more efﬁcient and scalable versions of these algorithms.
The computation of DBSCAN can be broken up into two steps. The ﬁrst is computing the (cid:15)-neighborhood graph of the data points, where the (cid:15)-neighborhood graph is deﬁned with data points as vertices and edges between pairs of points that are distance at most (cid:15) apart. The second is processing the neighborhood graph to extract the clusters. The ﬁrst step has worst-case quadratic complexity simply due to the fact that each data point may have of order linear number of points in its (cid:15)-neighborhood for sufﬁciently high (cid:15). However, even if the (cid:15)-neighborhood graph does not
˚Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
have such an order of edges, computing this graph remains costly: for each data point, we must query for neighbors in its (cid:15)-neighborhood, which is worst-case linear time for each point. There has been much work done in using space-partitioning data structures such as KD-Trees [4] to improve neighborhood queries, but these methods still run in linear time in the worst-case. Approximate methods (e.g. [23, 9]) answer queries in sub-linear time, but such methods come with few guarantees.
The second step is processing the neighborhood graph to extract the clusters, which consists in ﬁnding the connected components of the subgraph induced by nodes with degree above a certain threshold (i.e. the MinPts hyperparameter in the original DBSCAN [13]). This step is linear in the number of edges in the (cid:15)-neighborhood graph.
Our proposal is based on a simple but powerful insight: the full (cid:15)-neighborhood graph may not be necessary to extract the desired clustering. We show that we can subsample the edges of the neighborhood graph while still preserving the connected components of the core-points (the high density points) on which DBSCAN’s clusters are based.
To analyze this idea, we assume that the points are sampled from a distribution deﬁned by a density function satisfying certain standard [26, 44] conditions (e.g., cluster density is sufﬁciently high, and clusters do not become arbitrarily thin). Such an assumption is natural because DBSCAN recovers the high-density regions as clusters [13, 26]. Under this assumption we show that the minimum cut of the (cid:15)-neighborhood graph is as large as Ωpnq, where n is the number of datapoints. This, combined with a sampling lemma by Karger [28], implies that we can sample as little as Oplog n{nq of the edges uniformly while preserving the connected components of the (cid:15)-neighborhood graph exactly. - Our algorithm, SNG-DBSCAN, proceeds by constructing and processing this subsampled (cid:15)-neighborhood graph all in Opn log nq time. Moreover, our procedure only requires access to Opn log nq similarity queries for random pairs of points (adding an edge between pairs if they are at most (cid:15) apart). Thus, unlike most implementations of DBSCAN which take advantage of space-partitioning data structures, we don’t require the embeddings of the datapoints themselves. In particular, our method is compatible with arbitrary similarity functions instead of being restricted to a handful of distance metrics such as the Euclidean.
We provide an extensive empirical analysis showing that SNG-DBSCAN is effective on real datasets.
We show on large datasets (on the order of a million datapoints) that we can subsample as little as 0.1% of the neighborhood graph and attain competitive performance to sci-kit learn’s implementation of DBSCAN while consuming far fewer resources – as much as 200x speedup and 250x less RAM consumption on cloud machines with up to 750GB of RAM. In fact, for larger settings of (cid:15) on these datasets, DBSCAN fails to run at all due to insufﬁcient RAM. We also show that our method is effective even on smaller datasets. Sampling between 1% to 30% of the edges depending on the dataset, SNG-DBSCAN shows a nice improvement in runtime while still maintaining competitive clustering performance. 2