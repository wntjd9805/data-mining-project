Abstract
Normalizing ﬂows have shown great promise for modelling ﬂexible probability distributions in a computationally tractable way. However, whilst data is often naturally described on Riemannian manifolds such as spheres, tori, and hyperbolic spaces, most normalizing ﬂows implicitly assume a ﬂat geometry, making them either misspeciﬁed or ill-suited in these situations. To overcome this problem, we introduce Riemannian continuous normalizing ﬂows, a model which admits the parametrization of ﬂexible probability measures on smooth manifolds by deﬁning
ﬂows as the solution to ordinary diﬀerential equations. We show that this approach can lead to substantial improvements on both synthetic and real-world data when compared to standard ﬂows or previously introduced projected ﬂows. 1

Introduction
Learning well-speciﬁed probabilistic models is at the heart of many problems in machine learning and statistics. Much focus has therefore been placed on developing methods for modelling and inferring expressive probability distributions. Normalizing
ﬂows (Rezende and Mohamed, 2015) have shown great promise for this task as they provide a general and extensible framework for modelling highly complex and multimodal distributions (Pa-pamakarios et al., 2019).
An orthogonal but equally important aspect of well-speciﬁed models is to correctly characterize the geometry which describes the proximity of data points. Riemannian manifolds provide a general framework for this purpose and are a natural approach to model tasks in many scientiﬁc ﬁelds ranging from earth and cli-mate science to biology and computer vision. For instance, storm trajectories may be modelled as paths on the sphere (Karpatne et al., 2019), the shape of proteins can be parametrized using tori (Hamelryck et al., 2006), cell developmental processes can be described through paths in hyperbolic space (Klimovskaia et al., 2020), and human actions can be recognized in video using matrix manifolds (Lui, 2012). If appropriately chosen, manifold-informed methods can lead to improved sample complexity and generalization, improved ﬁt in the low parame-ter regime, and guide inference methods to interpretable models. They can also be understood as a geometric prior that encodes a practitioner’s assumption about the data and imposes an inductive bias.
Figure 1: Trajectories generated on the sphere to model volcano erup-tions. Note that these converge to the known Ring of Fire.
However, conventional normalizing ﬂows are not readily applicable to such manifold-valued data since their implicit Euclidean assumption makes them unaware of the underlying geometry or borders of the manifold. As a result they would yield distributions having some or all of their mass lying
∗Work done while at Facebook AI research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
outside the manifold, rendering them ill-suited or even misspeciﬁed so that central concepts like the reverse Kullback-Leibler (KL) divergence would not even be deﬁned.
In this work, we propose a principled way to combine both of these aspects and parametrize ﬂexible probability distributions on Riemannian manifolds. Speciﬁcally, we introduce Riemmanian contin-uous normalizing ﬂows in which ﬂows are deﬁned via vector ﬁelds on manifolds and computed as the solution to the associated ordinary diﬀerential equation (ODE) (see Figure 1 for an illustration).
Intuitively, our method operates by ﬁrst parametrizing a vector ﬁeld on the manifold with a neural network, then sampling particles from a base distribution, and ﬁnally approximating their ﬂow along the vector ﬁeld using a numerical solver. Both the neural network and the solver are aware of the underlying geometry which ensures that the ﬂow is always located on the manifold – yielding a
Riemannian method.
This approach allows us to combine multiple important advantages: One major challenge of normal-izing ﬂows lies in designing transformations that enable eﬃcient sampling and density computation.
By basing our approach on continuous normalizing ﬂows (CNFs) (Chen et al., 2018; Grathwohl et al., 2019; Salman et al., 2018) we avoid strong structural constraints to be imposed on the ﬂow, as is the case for most discrete normalizing ﬂows. Such unconstrained free-form ﬂows have empirically been shown to be highly expressive (Chen et al., 2019; Grathwohl et al., 2019). Moreover, projected methods require a diﬀerentiable mapping from a Euclidean space to the manifold, yet such a function cannot be bijective, which in turn leads to numerical challenges. By taking a Riemannian approach, our method is more versatile since it does not rely on an ad-hoc projection map and simultaneously reduces numerical artefacts that interfere with training. To the best of our knowledge, our method is the ﬁrst to combine these properties as existing methods for normalizing ﬂows on manifolds are either discrete (Bose et al., 2020; Rezende et al., 2020), projected (Gemici et al., 2016; Falorsi et al., 2019; Bose et al., 2020) or manifold-speciﬁc (Sei, 2011; Bose et al., 2020; Rezende et al., 2020).
We empirically demonstrate the advantages of our method on constant curvature manifolds – i.e., the
Poincaré disk and the sphere – and show the beneﬁts of the proposed approach compared to non-Riemannian and projected methods for maximum likelihood estimation and reverse KL minimization.
We also apply our method to density estimation on earth-sciences data (e.g., locations of earthquakes,
ﬂoods and wildﬁres) and show that it yields better generalization performance and faster convergence. 2 Continuous Normalizing Flows on Riemannian Manifolds
Normalizing ﬂows operate by pushing a simple base distribution through a series of parametrized invertible maps, referred as the ﬂow. This can yield a highly complex and multimodal distribution which is typically assumed to live in a Euclidean vector space. Here, we propose a principled approach to extend normalizing ﬂows to manifold-valued data, i.e. Riemmanian continuous normalizing ﬂows (RCNFs). Following CNFs (Chen et al., 2018; Grathwohl et al., 2019; Salman et al., 2018) we deﬁne manifold ﬂows as the solutions to ODEs. The high-level idea is to parametrize ﬂows through the time-evolution of manifold-valued particles z – in particular via their velocity ˙z(t) = fθ(z(t), t) where fθ denotes a vector ﬁeld. Particles are ﬁrst sampled from a simple base distribution, and then their evolution is integrated by a manifold-aware numerical solver, yielding a new complex multimodal distribution of the particles. This Riemannian and continuous approach has the advantages of allowing almost free-form neural networks and of not requiring any mapping from a Euclidean space which would potentially lead to numerical challenges.
For practical purposes, we focus our theoretical and experimental discussion on constant curvature manifolds (see Table 1). In addition to being widely used in the literature (Nickel and Kiela, 2017;
Davidson et al., 2018; Mardia and Jupp, 2000; Hasnat et al., 2017), these manifolds are convenient to work with since most related geometrical quantities are available in closed-form. However, our
Table 1: Summary of d-dimensional continuous constant (sectional) curvature manifolds.
Curvature Coordinates (cid:112) det g = d Vol /d LebRd Compact
Geometry
Model
Real vector space K = 0
Rd
Euclidean
Hyperbolic Bd
K < 0
Poincaré ball
K
Sd
Elliptic
K > 0
K Hypersphere
Cartesian z
Cartesian z n-spherical ϕ K− d−1 1 2 / 1 + K (cid:107)z(cid:107)2(cid:17)d (cid:16) 2 (cid:81)d−2 i=1 sin(ϕi)d−i−1
No
No
Yes 2
proposed approach is generic and could be used on a broad class of manifolds such as product and matrix manifolds like tori and Grassmanians. For a brief overview of relevant concepts in Riemannian geometry please see Appendix A.1 or Lee (2003) for a more thorough introduction.
In the following, we develop the key components which allow us to deﬁne continuous normalizing
ﬂows that are aware of the underlying Riemannian geometry: ﬂow, likelihood, and vector ﬁeld.
Vector ﬂows Flows in conventional normalizing ﬂows are deﬁned as smooth mappings φ : Rd → Rd which transform a base distribution z ∼ P0 into a complex distribution Pθ. For normalizing ﬂows to be well-behaved and convenient to work with, the ﬂow is required to be bijective and diﬀerentiable which introduces signiﬁcant structural constraints on φ. Continuous normalizing ﬂows overcome this issue by deﬁning the ﬂow φ : Rd × R → Rd generated by an ordinary diﬀerential equation, allowing for unrestricted neural network architectures. Here we show how vector ﬁelds can be used to deﬁne similar ﬂows φ : M × R → M on general Riemannian manifolds.
Consider the temporal evolution of a particle z(t) lying on a d-dimensional manifold M, whose velocity is given by a vector ﬁeld fθ(z(t), t). Intuitively, fθ(z(t), t) indicates the direction and speed along which the particle is moving on the manifold’s surface. Classic examples for such vector ﬁelds include weathercocks giving wind direction and compasses pointing toward the magnetic north pole of the earth. Formally, let TzM denote the tangent space at z and T M = ∩z∈M TzM the associated tangent bundle. Furthermore, let fθ : M × R (cid:55)→ T M denote a vector ﬁeld on M. The particle’s time-evolution according to fθ is then given by the following ODE dz(t) dt
= fθ(z(t), t). (1)
To transform a base distribution using this vector ﬁeld, we are then interested in a particle’s position after time t. When starting at an initial position z(0) = z0, the ﬂow operator φ : M × R (cid:55)→ M gives the particle’s position at any time t as z(t) = φ(z0, t). Leveraging the fundamental theorem of ﬂows (Lee, 2003), we can show that under mild conditions, this ﬂow is bijective and diﬀerentiable. We write C1 for the set of diﬀerentiable functions whose derivative are continuous.
Proposition 1 (Vector ﬂows). Let M be a smooth complete manifold. Furthermore, let fθ be a
C1-bounded time-dependent vector ﬁeld. Then there exists a global ﬂow φ : M × R (cid:55)→ M such that for each t ∈ R, the map φ(·, t) : M (cid:55)→ M is a C1-diﬀeomorphism (i.e. C1 bijection with C1 inverse).
Proof. See Appendix E.1 for a detailed derivation. (cid:3) (cid:44) α fθ results in a time-scaled ﬂow φα(z, t) = φ(z, αt).
Note that scaling the vector ﬁeld as f α
θ
The integration duration t is therefore arbitrary. Without loss of generality we set t = 1 and write
φ (cid:44) φ(·, 1). Concerning the evaluation of the ﬂow φ, it generally does no accept a closed-form solution and thus requires to be approximated numerically. To this extent we rely on an explicit and adaptive Runge-Kutta (RK) integrator of order 4 (Dormand and Prince, 1980). However, standard integrators used in CNFs generally do not preserve manifold constraints (Hairer, 2006) . To overcome this issue we rely on a projective solver (Hairer, 2011). This solver works by conveniently solving the
ODE in the ambient Cartesian coordinates and projecting each step onto the manifold. Projections onto Sd are computationally cheap since they amount to l2 norm divisions. No projection is required for the Poincaré ball.
Likelihood Having a ﬂow at hand, we are now interested in evaluating the likelihood of our pushforward model Pθ = φ(cid:93)P0. Here, the pushforward operator (cid:93) indicates that one obtains samples z ∼ φ(cid:93)P0 as z = φ(z0) with z0 ∼ P0. For this purpose, we derive in the following the change in density in terms of the geometry of the manifold and show how to eﬃciently estimate the likelihood.
Change in density in density from the base distribution to the pushforward. Applying the chain rule we get
In normalizing ﬂows, we can compute the likelihood of a sample via the change log pθ(z) − log p0(z0) = log (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) det
∂φ−1(z)
∂z (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= − log (cid:12)(cid:12)(cid:12)(cid:12)(cid:12) det
∂φ(z0)
∂z (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
. (2)
In general, computing the Jacobian’s determinant of the ﬂow is challenging since it requires d reverse-mode automatic diﬀerentiations to obtain the full Jacobian matrix, and O(d3) operations to 3
(cid:82) t 0
∂ log pθ(z(t))
∂t compute its determinant. CNFs side step direct computation of the determinant by leveraging the time-continuity of the ﬂow and re-expressing Equation 2 as the integral of the instantaneous change dt. However, standard CNFs make an implicit Euclidean assumption to in log density compute this quantity which is violated for general Riemannian manifolds. To overcome this issue we express the instantaneous change in log-density in terms of the Riemannian metric. In particular, let G(z) denote the matrix representation of the Riemannian metric for a given manifold M, then
G(z) endows tangent spaces TzM with an inner product. For instance in the Poincaré ball Bd, it holds that G(z) = (2 / 1 + K (cid:107)z(cid:107)2) Id, while in Euclidean space Rd we have G(z) = Id, where Id denotes the identity matrix. Using the Liouville equation, we can then show that the instantaneous change in variable is deﬁned as follows.
Proposition 2 (Instantaneous change of variables). Let z(t) be a continuous manifold-valued random variable given in local coordinates, which is described by the ODE from Equation 1 with probability density pθ(z(t)). The change in log-probability then also follows a diﬀerential equation given by
∂ log pθ(z(t))
∂t
= − div(fθ(z(t), t)) = −|G(z(t))|− 1 2 tr (cid:32)
∂
√
|G(z(t))|fθ(z(t), t) (cid:32)
= − tr (cid:33)
∂fθ(z(t), t)
∂z
− |G(z(t))|− 1 2 (cid:68) fθ(z(t), t),
∂z
∂ (cid:112)
∂z
|G(z(t))| (cid:33) (cid:69)
. (3) (4) (cid:3)
Proof. For a detailed derivation of Equation 3 see Appendix C.
Note that in the Euclidean setting recover the formula from Grathwohl et al. (2019); Chen et al. (2018).
|G(z)| = 1 thus the second term of Equation 4 vanishes and we
√
Estimating the divergence Even though the determinant of Equation 2 has been replaced in Equa-tion 3 by a trace operator with lower computational complexity, we still need to compute the full
Jacobian matrix of fθ. Similarly to Grathwohl et al. (2019); Salman et al. (2018), we make use of
Hutchinson’s trace estimator to compute the Jacobian eﬃciently. In particular, Hutchinson (1990) showed that tr(A) = Ep((cid:15))[(cid:15)(cid:124)A(cid:15)] with p((cid:15)) being a d-dimensional random vector such that E[(cid:15)] = 0 and Cov((cid:15)) = Id. Leveraging this trace estimator to approximate the divergence in Equation 3 yields div(fθ(z(t), t)) = |G(z(t))|− 1 2 Ep((cid:15)) (cid:34) (cid:15)(cid:124) ∂
√
|G(z(t))|fθ(z(t), t)
∂z (cid:35) (cid:15)
. (5)
We note that the variance of this estimator can potentially be high since it scales with the inverse of
|G(z(t))| (see Appendix D.2). By integrating Equation 3 over time with the the determinant term stochastic divergence estimator from Equation 5, we get the following total change in log-density between the manifold-valued random variables z and z0
√ log (cid:32) pθ(z) p0(z0) (cid:33) (cid:90) 1
= − 0 div(fθ(z(t), t)) dt = −Ep((cid:15)) (cid:34)(cid:90) 1 0
√
|G(z(t))|− 1 2 (cid:15)(cid:124) ∂
|G(z(t))|fθ(z(t), t)
∂z (cid:35) (cid:15) dt
. (6)
It can be seen that Equation 6 accounts again for the underlying geometry through the metric G(z(t)).
Table 1 lists closed-form solutions of its determinant for constant curvature manifolds. Furthermore, the vector-Jacobian product can be computed through backward auto-diﬀerentiation with linear complexity, avoiding the quadratic cost of computing the full Jacobian matrix. Additionally, the integral is approximated via the discretization of the ﬂow returned by the solver.
Choice of base distribution P0 The closer the initial base distribution is to the target distribution, the easier the learning task should be. However, it is challenging in practice to incorporate such prior knowledge. We consequently use a uniform distribution on Sd since it is the most "uncertain" distribution. For the Poincaré ball Bd, we rely on a standard wrapped Gaussian distribution N W (Nagano et al., 2019; Mathieu et al., 2019) because it is convenient to work with.
Vector ﬁeld Finally, we discuss the form of the vector ﬁeld fθ : M × R → T M which generates the ﬂow φ used to pushforward samples. We parametrize fθ via a feed-forward neural network which takes as input manifold-valued particles, and outputs their velocities. The architecture of the vector
ﬁeld has a direct impact on the expressiveness of the distribution and is thus crucially important. In order to take into account these geometrical properties we make use of speciﬁc input and output layers that we describe below. The rest of the architecture is based on a multilayer perceptron. 4
Input layer To inform the neural network about the geometry of the manifold M, we use as ﬁrst layer a geodesic distance layer (Ganea et al., 2018; Mathieu et al., 2019) which generalizes linear layers to manifolds, and can be seen as computing distances to decision boundaries on M. These boundaries are parametrized by geodesic hyperplanes Hw, and the associated neurons hw(z) ∝ dM(z, Hw), with dM being the geodesic distance. Horizontally stacking several of these neurons makes a geodesic distance layer. We refer to Appendix E.2 for more details.
Output layer To constrain the neural net to T M, we output vectors in Rd+1 when M = Sd, before projecting them to the tangent space i.e. fθ(z) = projTzM neural_net(z). This is not necessary in Bd since the ambient space is of equal dimension. Yet, velocities scale as (cid:107)fθ(z)(cid:107)z = |G(z)|1/2 (cid:107)fθ(z)(cid:107)2, hence we scale the neural_net by |G(z)|−1/2 s.t. (cid:107)fθ(z)(cid:107)z = (cid:107)neural_net(z)(cid:107)2.
Regularity For the ﬂow to be bijective, the vector ﬁeld fθ is required to be C1 and bounded (cf
Proposition 1). The boundness and smoothness conditions can be satisﬁed by relying on bounded smooth non-linearities in fθ such as tanh, along with bounded weight and bias at the last layer.
Training In density estimation and inference tasks, one aims to learn a model Pθ with parameters
θ by minimising a divergence L(θ) = D(PD || Pθ) w.r.t. a target distribution PD. In our case, the parameters θ refer to the parameters of the vector ﬁeld fθ. We minimize the loss L(θ) using ﬁrst-order stochastic optimization, which requires Monte Carlo estimates of loss gradients ∇θ L(θ). We back-propagate gradients through the explicit solver with O(1/h) memory cost, h being the step size.
When the loss L(θ) is expressed as an expectation over the model Pθ, as in the reverse KL divergence, we rely on the reparametrization trick (Kingma and Welling, 2014; Rezende et al., 2014). In our experiments we will consider both the negative log-likelihood and reverse KL objectives (cid:2)log pθ(z)(cid:3) and LKL(θ) = DKL (Pθ (cid:107) PD) = Ez∼Pθ (cid:2)log pθ(z) − log pD(z)(cid:3) . (7)
LLike(θ) = −Ez∼PD
Additionally, regularization terms can be added in the hope of improving training and generalization.
See Appendix D for a discussion and connections to the dynamical formulation of optimal transport. 3