Abstract
Metric learning aims to learn a distance measure that can beneﬁt distance-based methods such as the nearest neighbor (NN) classiﬁer. While considerable efforts have been made to improve its empirical performance and analyze its generaliza-tion ability by focusing on the data structure and model complexity, an unresolved question is how choices of algorithmic parameters, such as the number of training iterations, affect metric learning as it is typically formulated as an optimization problem and nowadays more often as a non-convex problem. In this paper, we theoretically address this question and prove the agnostic Probably Approximately
Correct (PAC) learnability for metric learning algorithms with non-convex objective functions optimized via gradient descent (GD); in particular, our theoretical guar-antee takes the iteration number into account. We ﬁrst show that the generalization
PAC bound is a sufﬁcient condition for agnostic PAC learnability and this bound can be obtained by ensuring the uniform convergence on a densely concentrated subset of the parameter space. We then show that, for classiﬁers optimized via GD, their generalizability can be guaranteed if the classiﬁer and loss function are both
Lipschitz smooth, and further improved by using fewer iterations. To illustrate and exploit the theoretical ﬁndings, we ﬁnally propose a novel metric learning method called Smooth Metric and representative Instance LEarning (SMILE), designed to satisfy the Lipschitz smoothness property and learned via GD with an early stopping mechanism for better discriminability and less computational cost of NN. 1

Introduction
A good measure of distance between instances is important to many machine learning algorithms, such as the nearest neighbor (NN) classiﬁer and k-means clustering. As it is difﬁcult to handcraft an optimal distance for each task, metric learning appears as an appealing technique to learn the distance metric automatically and directly from the data. The most widely studied metric is the
Mahalanobis distance and it is often learned via an optimization problem [51, 17, 49]. To enhance the discriminability of the learned metric, various loss functions have been designed, considering the local property of heterogeneous data [15, 47, 23, 5, 38, 54, 44, 12] and the nonlinear geometry of the sample space [24, 58, 8]. Meanwhile, to achieve good generalization and robustness, different regularizations have been imposed to control the model complexity [31, 26, 50, and references therein]. In addition to methodological advances, theoretical guarantees of metric learning algorithms, as well as guarantees
∗The ﬁrst two authors contributed equally to this work. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of metric-based classiﬁers [3, 19], have been provided. In particular, generalization bounds have been founded on the complexity measure of the model class [55, 4, 7, 46, 33, 53], algorithmic stability [27, 19, 16], and algorithmic robustness [2]. The intrinsic complexity of the dataset has also been considered in recent studies [46, 33].
While the data structure and model complexity play a vital role in metric learning, an equally important but as yet poorly understood factor is the choice of optimization algorithms and the associated parameters [42]. For example, when metric learning is formulated as a non-convex problem and optimized by using the gradient descent algorithm, its solution is inevitably inﬂuenced by factors such as the learning rate and the number of training iterations; the optimal models with respect to different optimization parameters will then exhibit different generalization behavior.
Therefore, the goal of this paper is to provide a new route to theoretical exploration and exploitation of the effect of the gradient descent (GD) algorithm on metric learning methods. To this end, we establish a generalization bound which suggests that early stopping, smooth classiﬁer and smooth loss function have crucial inﬂuence on the generalization error. We highlight that the proposed theoretical techniques do not take advantage of any property of convex optimization and are not speciﬁc to metric learning methods; they can be used to study the generalization ability of classiﬁcation algorithms with non-convex objectives. The contributions of this paper are fourfold. 1. We show that the generalization Probably Approximately Correct (PAC) bound is a sufﬁcient condition for a parametric hypothesis class to be agnostic PAC learnable (Theorem 1). Compared with the widely studied uniform convergence condition, the generalization PAC bound is a weaker notion but has the capability to analyze the inﬂuence of algorithmic parameters. 2. To facilitate the derivation of the generalization PAC bound of a hypothesis class, we propose a new decomposition theorem to decompose the bound into two terms that can be easily guaran-teed (Theorem 2). The ﬁrst term constrains the space of the estimated parameters of the hypothesis, reducing it from the entire parameter space to a high-conﬁdence subset of the parameter space. The second term considers the uniform convergence condition of the concentrated subset. 3. Based on the decomposition theorem, we obtain the generalization PAC bound for classiﬁers learned with the gradient descent algorithm (Theorem 3). The bound shows that the generalization gap increases over iterations, thus providing a theoretical support for the practical use of early stopping.
Moreover, it shows that a Lipschitz smooth (i.e. Lipschitz continuous of the gradient) classiﬁer and a
Lipschitz smooth loss function are sufﬁcient to guarantee good generalization. 4. We propose a novel metric learning method as a concrete example of using the generalization PAC bound. When classifying a test instance, the NN classiﬁer has to store the entire training set and calculate its distances to all training instances, thereby incurring high storage and computational costs.
To reduce these costs and improve the generalization performance, we propose to simultaneously learn the distance metric and few representative instances which serve as the reference points for testing; the new method is called Smooth Metric and representative Instance LEearning (SMILE).
More speciﬁc, to ensure good test performance, SMILE adopts a Lipschitz smooth classiﬁer and loss function and is optimized via GD with a designed early stopping mechanism. The method is evaluated on 12 datasets and shows competitive performance against existing methods. 1.1