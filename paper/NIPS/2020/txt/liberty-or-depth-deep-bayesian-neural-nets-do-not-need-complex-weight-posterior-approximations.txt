Abstract
We challenge the longstanding assumption that the mean-ﬁeld approximation for variational inference in Bayesian neural networks is severely restrictive, and show this is not the case in deep networks. We prove several results indicating that deep mean-ﬁeld variational weight posteriors can induce similar distributions in function-space to those induced by shallower networks with complex weight posteriors.
We validate our theoretical contributions empirically, both through examination of the weight posterior using Hamiltonian Monte Carlo in small models and by comparing diagonal- to structured-covariance in large settings. Since complex variational posteriors are often expensive and cumbersome to implement, our results suggest that using mean-ﬁeld variational inference in a deeper model is both a practical and theoretically justiﬁed alternative to structured approximations. 1

Introduction
While performing variational inference (VI) in Bayesian neural networks (BNNs) researchers often make the ‘mean-ﬁeld’ approximation which assumes that the posterior distribution factorizes over weights (i.e., diagonal weight covariance). Researchers have assumed that using this mean-ﬁeld approximation in BNNs is a severe limitation. This has motivated extensive exploration of VI methods that explicitly model correlations between weights (see