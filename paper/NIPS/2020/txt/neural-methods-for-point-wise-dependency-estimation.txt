Abstract
Since its inception, the neural estimation of mutual information (MI) has demon-strated the empirical success of modeling expected dependency between high-dimensional random variables. However, MI is an aggregate statistic and cannot be used to measure point-wise dependency between different events. In this work, instead of estimating the expected dependency, we focus on estimating point-wise dependency (PD), which quantitatively measures how likely two outcomes co-occur. We show that we can naturally obtain PD when we are optimizing MI neural variational bounds. However, optimizing these bounds is challenging due to its large variance in practice. To address this issue, we develop two methods (free of optimizing MI variational bounds): Probabilistic Classiﬁer and Density-Ratio
Fitting.We demonstrate the effectiveness of our approaches in 1) MI estimation, 2) self-supervised representation learning, and 3) cross-modal retrieval task. 1

Introduction
Mutual Information (MI) measures the average statistical dependency between two random variables, and it has found abundant applications in practice, such as feature selection [12, 39], interpretable factor discovery [14, 46], genetic association studies [50], to name a few. Recent work [9, 40] proposed to use neural networks with gradient descent to estimate MI, which empirically scales better in high-dimension settings as compared to classic approaches (e.g., Kraskov (KSG) [28] estimator), which are known to suffer from the curse of dimensionality. Inspired by this line of work, we take a step further to present neural methods for point-wise dependency (PD) estimation. At a colloquial level, PD serves to understand the instance-level dependency between a pair of events taken by two random variables, which gives us a ﬁne-grained understanding of the outcome. Formally, it can be realized as the ratio between likelihood of their co-occurrence to the likelihood of the product events: p(x, y)/p(x)p(y) with x and y being the corresponding outcomes.
At ﬁrst glance, it may seem straightforward to estimate PD by adopting prior density ratio estimation approaches [43, 44] to directly calculate the ratio between p(x, y) and p(x)p(y). Nonetheless, for the sake of tractability, previous methods are mainly kernel-based approaches that might be inadequate to scale to high-dimensional and complex-structured data. In this work, we introduce approaches for
PD estimation that leverage the recent advances in rich and ﬂexible neural networks. We show that we can naturally obtain PD when we are optimizing MI neural variational bounds [9, 40]. However, estimating these MI bounds often results in inevitably large variance [41]. To address this concern, we develop two data-driven approaches: Probabilistic Classiﬁer and Density-Ratio Fitting. Probabilistic
Classiﬁer turns PD estimation into a supervised binary classiﬁcation task, where we train a classiﬁer to distinguish the observed joint distribution from the product of marginal distribution. This approach adopts cross-entropy loss using neural networks, which is favorable for optimization and exhibits a stable training trajectory with less variance. Density-Ratio Fitting seeks to minimize the least-square difference between the true and the estimated PD. Its objective involves no logarithm and exponentiation; hence, it is practically preferable due to its numerical stability.
∗Work done at Carnegie Mellon University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We empirically analyze the advantages of PD neural estimation on three applications. First, we cast the challenging MI estimation problem to be a PD estimation problem. The re-formulation bypasses calculating MI lower bounds in prior work [9, 40], which suffers from large variance [41] in practice. Our empirical results demonstrate the low variance and bias of the proposed approach when comparing to prior MI neural estimators. Second, our PD estimation objectives also inspire new losses for contrastive self-supervised representation learning. Surprisingly, Density-Ratio Fitting inspired loss results in a consistent improvement over prior work in both shallow [48] and deep [5] neural architectures. Third, we study the use of PD estimation for data containing information across modalities. More speciﬁcally, we analyze the cross-modal retrieval task on human speech and text corpora. We make our experiments publicly available at https://github.com/yaohungt/
Pointwise_Dependency_Neural_Estimation. 2