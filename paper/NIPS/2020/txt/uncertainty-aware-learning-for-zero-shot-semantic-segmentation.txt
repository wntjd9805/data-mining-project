Abstract
Zero-shot semantic segmentation (ZSS) aims to classify pixels of novel classes without training examples available. Recently, most ZSS methods focus on learning the visual-semantic correspondence to transfer knowledge from seen classes to unseen classes at the pixel level. Yet, few works study the adverse effects caused by the noisy and outlying training samples of the seen classes. In this paper, we iden-tify this challenge and address it with a novel framework that learns to discriminate noisy samples based on Bayesian uncertainty estimation. Speciﬁcally, we model the network outputs with Gaussian and Laplacian distributions, with the variances accounting for the observation noise and uncertainty of input samples. Learning objectives are then derived with the estimated variances playing as adaptive atten-uation for individual samples in training. Consequently, our model learns more attentively from representative samples of seen classes while suffering less from noisy and outlying ones, thus providing better reliability and generalization toward unseen categories. We demonstrate the effectiveness of our framework through comprehensive experiments on multiple challenging benchmarks, and show that our method achieves signiﬁcant accuracy improvement over previous approaches for large-scale open-set segmentation. 1

Introduction
Semantic image segmentation aims to recognize and group pixels of the same object or stuff classes into segments [4, 16, 42, 64]. As a fundamental problem in computer vision, this task has attracted a lot of attention from the research community and achieved great success along with the development of deep learning in recent years [7, 8, 14, 15, 19, 24, 38, 45, 50, 51, 57, 59, 60, 63, 66]. Most of the existing methods focus on addressing the task over small and close sets of class labels, which relies on a large amount of training data to achieve effectiveness. Yet, due to the varying frequency of different object and stuff categories in natural scenes, annotations and samples for some categories may be difﬁcult to acquire [46], thus posing the challenge in extending those conventional models to address large and open sets of categories.
To achieve effective zero-shot semantic segmentation (ZSS), existing efforts have been made [2, 28, 31, 44, 55, 62] by treating each pixel as an independent classiﬁcation problem. And the classic zero-shot image recognition techniques [1, 3, 6, 18, 29, 35, 47, 52, 54, 61, 65] are directly applied to learn from seen classes the visual-semantic mappings, which are then transferred to unseen ones.
Though achieving promising results, these methods may still suffer from several limitations. At ﬁrst, these methods learn from pixels independently. Yet in images, category-consistent regions are more semantically meaningful than individual pixels. Thus learning with global information beneﬁts the effectiveness of learned visual-semantic mappings. Besides, a more critical challenge is that most of the existing methods ignore the noisy and outlying samples of seen classes, which may cause adverse learning effects. As shown in Fig. 1 (a), closed-set learning with visual examples available for all the classes typically results in balanced visual-semantic mappings. However in zero-shot learning, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: T-SNE visualization of visual feature encoded by ResNet [23]. Classes “A” and “B” are from
PascalContext [42]. (a) Closed-set learning with “A” and “B”. As both classes always have sufﬁcient visual samples during training, the learnt visual-semantic mappings are balanced. (b) Zero-shot learning with seen class
“A” and unseen class “B”. Due to the lack of training samples for “B”, the mapping learnt on “A” is sensitive to noisy and outlying samples, thus inferring biased mapping for the unseen class “B”. (c) In uncertainty-aware zero-shot learning, noisy samples are attenuated during training. Thus visual-semantic mappings are learnt from representative samples of “A”, and consequently infer a better mappings for unseen class “B”. without training data for unseen classes, the visual-semantic correspondence learned on seen classes is sensitive to the noisy and outlying samples. And consequently, as shown in Fig. 1 (b), sub-optimal mappings for unseen classes will be inferred due to the biased learning on seen classes.
To address these challenges of ZSS, in this paper we propose a novel framework that learns the visual-semantic mappings with global information, and leverages Bayesian uncertainty estimation [32, 37, 43] to automatically discriminate between representative samples and noisy ones during training.
The proposed framework has two output branches, with one for pixel-wise prediction and the other learns to measure the overall segmentation quality. We model the output of each branch with a probabilistic distribution, by letting the network simultaneously estimate the mean and variance. The variance is related to the input sample’s uncertainty [37, 43], thus allowing the model to explicitly account for the observation noise of the training data. Uncertainty-aware learning objectives are then derived with the estimated variances helping to adaptively strengthen representative training samples and attenuate noisy ones. Consequently, as illustrated in Fig. 1 (c) the model learns effective visual-semantic mappings from seen classes, which can be reliably transferred to unseen classes.
To the best of our knowledge, this is the ﬁrst work that leverages uncertainty estimation based on Bayesian modeling to address noisy training samples in zero-shot learning tasks. Our main contributions are summarized as follows:
• We identify the problem of learning robust visual-semantic correspondence from noisy training samples in zero-shot learning tasks, and provide an effective solution based on data-dependent uncertainty estimation.
• We propose a novel deep probabilistic network for zero-shot semantic segmentation together with uncertainty aware losses that learn at image level and pixel level.
• We conduct extensive experiments on multiple benchmarks with large open-set classes, and show signiﬁcant performance improvements over existing methods. 2