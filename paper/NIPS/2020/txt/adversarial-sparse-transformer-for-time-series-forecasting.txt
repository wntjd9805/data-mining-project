Abstract
Many approaches have been proposed for time series forecasting, in light of its signiﬁcance in wide applications including business demand prediction. However, the existing methods suffer from two key limitations. Firstly, most point prediction models only predict an exact value of each time step without ﬂexibility, which can hardly capture the stochasticity of data. Even probabilistic prediction using the likelihood estimation suffers these problems in the same way. Besides, most of them use the auto-regressive generative mode, where ground-truth is provided during training and replaced by the network’s own one-step ahead output during inference, causing the error accumulation in inference. Thus they may fail to forecast time series for long time horizon due to the error accumulation. To solve these issues, in this paper, we propose a new time series forecasting model –
Adversarial Sparse Transformer (AST), based on Generative Adversarial Networks (GANs). Speciﬁcally, AST adopts a Sparse Transformer as the generator to learn a sparse attention map for time series forecasting, and uses a discriminator to improve the prediction performance at a sequence level. Extensive experiments on several real-world datasets show the effectiveness and efﬁciency of our method. 1

Introduction
Time series forecasting has demonstrated its wide applications in business and industrial decision-making. For example, demand forecasting of energy consumption helps optimize the resource allocation and dispatch the power generation. There are many classical approaches to solve time series forecasting problems, such as Auto Regressive Integrated Moving Average(ARIMA) [3] models or exponential smoothing [7]. They incorporate prior knowledge about time series structures such as trend, seasonality and so on, and can achieve good performance for single linear time series prediction.
But they are ineffective in predicting complex time series data, partly because of their inability to utilize the time-related features.
∗This work is done when Sifan Wu and Qianggang Ding work as interns at Tencent AI Lab.
†Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Recent years, to solve the modern large-scale multiple time series forecasting problems, deep neural networks [11, 20, 24, 25, 30, 14] have been applied to model complicated sequential data. Naturally,
Recurrent Neural Network(RNN)-based [11, 24] and attention-based [14, 30] models are utilized to mine complex patterns for time series trends. However, all these models only optimize one speciﬁc objective such as the likelihood loss, MSE loss or other losses, while the real-world time series datasets have some tend of stochasticity which can hardly be modeled with a speciﬁc non-ﬂexible objective. Therefore, it is inappropriate to only optimize a single forecasting objective for time series forecasting models.
Another key issue of existing methods is the error accumulation. Most auto-regressive generator models adopt the teacher forcing strategy [12], where the previous target values are known during training. While during inference, real previous target values are replaced by previously generated values, which causes discrepancy between training and inference. The discrepancy leads to error accumulation, since the model can hardly handle the errors which never occur in the training process.
Recently, some non-autoregressive forecasting models have been proposed to resolve the error accumulation. However these models neglect the position information between steps, which is essential for time series forecasting, causing an inferior performance. Therefore, it has become one of the most important issues to alleviate the error accumulation and meanwhile improve the performance via training the time series forecasting model appropriately.
Generative adversarial networks (GANs) [6] use an adversarial training procedure to directly shape the output distribution of the network via back-propagation. Motivated by GANs, in this paper, we propose Adversarial Sparse Transformer (AST) for multiple time series forecasting, which is a framework that combines a modiﬁed Transformer and Generative Adversarial Networks (GANs).
The discriminator can regularize the modiﬁed Transformer at the sequence level and make it learn a better representation for time series, thereby eliminating the error accumulation and remedying the shortcomings of single forecasting objective. Speciﬁcally, the Vanilla Transformer is based on the multi-head attention mechanism where the representation of each time step is represented by multiple different weighted average of samples of its relevant time steps. The attention distribution of each head is typically computed by the softmax normalizing transformation, allocating non-zero attention weights to all samples. Considering only a few historical steps have strong correlations with the forecasting time step, we use sparse normalizing transforms like α-entmax [21], which can yield exactly zero probability for irrelevant time steps.
The main contributions of our paper are as follows:
• We propose an effective time series forecasting model – Adversarial Sparse Transformer based on sparse Transformer and Generative Adversarial Networks. Extensive experiments on different real-world time series datasets show the effectiveness of our model.
• We design a Generative Adversarial Encoder-Decoder framework to regularize the forecast-ing model which can improve the performance at the sequence level. The experiments show that adversarial training improves the robustness and generalization of the model.
The rest of this paper is organized as follows. Section 2 reviews related works on time series forecasting brieﬂy. Section 3 proposes the background of this model. Section 4 describes the model we propose. In Section 5, we demonstrate the effectiveness of AST empirically. Finally we conclude in Section 6. 2