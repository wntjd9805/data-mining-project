Abstract
The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating speciﬁc architectural designs of
GNNs, such as GCN, GIN, or GAT, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally,
GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly ﬁnd the best GNN design for a novel task or a novel dataset. Here we deﬁne and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efﬁcient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing
GNNs; (2) while best GNN designs for different tasks vary signiﬁcantly, the
GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance.
Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for speciﬁc tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management. 1

Introduction
The ﬁeld of Graph Neural Network (GNN) research has made substantial progress in recent years.
Notably, a growing number of GNN architectures, including GCN [16], GraphSAGE [6], and GAT
[31], have been developed. These architectures are then applied to a growing number of applications, such as social networks [39, 46], chemistry [13, 44], and biology [52]. However, with this growing trend several issues emerge, which limit further development of GNNs.
Issues in GNN architecture design.
In current GNN literature, GNN models are deﬁned and evaluated as speciﬁc architectural designs. For example, architectures, such as GCN, GraphSAGE,
GIN and GAT, are widely adopted in existing works [3, 4, 28, 41, 48, 37]. However, these models are speciﬁc instances in the GNN design space consisting of a cross product of design dimensions. For example, a GNN model that changes the aggregation function of GraphSAGE to a summation, or adds skip connections [8] across GraphSAGE layers, is not referred to as GraphSAGE model, but it could empirically outperform it in certain tasks [32]. Therefore, the practice of only focusing on a speciﬁc GNN design, rather than the design space, limits the discovery of successful GNN models.
Issues in GNN evaluation. GNN models are usually evaluated on a limited and non-diverse set of tasks, such as node classiﬁcation on citation networks [15, 16, 31]. Recent efforts use additional 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
tasks to evaluate GNN models [3, 10]. However, while these new tasks enrich the evaluation of GNN models, challenging and completely new tasks from various domains always emerge. For example, novel tasks such as circuit design [50], SAT generation [47], data imputation [45], or subgraph matching [40] have all been recently approached with GNNs. Such novel tasks do not naturally resemble any of the existing GNN benchmark tasks and thus, it is unclear how to design an effective
GNN architecture for a given new task. This issue is especially critical considering the large design space of GNNs and a surge of new GNN tasks, since re-exploring the entire design space for each new task is prohibitively expensive.
Issues in GNN implementation. A platform that supports extensive exploration over the GNN design space, with uniﬁes implementation for node, edge, and graph-level tasks, is currently lacking, which is a major factor that contributes to the above-mentioned issues.
Present work. Here we develop and systematically study a general design space of GNNs over a number of diverse of tasks1. To tackle the above-mentioned issues, we highlight three central components in our study, namely GNN design space, GNN task space, and design space evaluation: (1) The GNN design space covers important architectural design aspects that researchers often encounter during GNN model development. (2) The GNN task space with a task similarity metric allows us to identify novel tasks and effectively transfer GNN architectural designs between similar tasks. (3) An efﬁcient and effective design space evaluation allows insights to be distilled from a huge number of model-task combinations. Finally, we also develop GraphGym2, a platform for exploring different GNN designs and tasks, which features modularized GNN implementation, standardized
GNN evaluation, and reproducible and scalable experiment management.
GNN design space. We deﬁne a general design space of GNNs that considers intra-layer design, inter-layer design and learning conﬁguration. The design space consists of 12 design dimensions, resulting in 315,000 possible designs. Our purpose is thus not to propose the most extensive GNN design space, but to demonstrate how focusing on the design space can enhance GNN research. We emphasize that the design space can be expanded as new design dimensions emerge in state-of-the-art models. Our overall framework is easily extendable to new design dimensions. Furthermore, it can be used to quickly ﬁnd a good combination of design choices for a speciﬁc novel task.
GNN task space. We propose a task similarity metric to characterize relationships between different tasks. The metric allows us to quickly identify promising GNN designs for a brand new task/dataset.
Speciﬁcally, the similarity between two tasks is computed by applying a ﬁxed set of GNN architectures to the two tasks and then measuring the Kendall rank correlation [1] of the performance of these
GNNs. We consider 32 tasks consisting of 12 synthetic node classiﬁcation tasks, 8 synthetic graph classiﬁcation tasks, and 6 real-world node classiﬁcation and 6 graph classiﬁcation tasks.
Design space evaluation. Our goal is to gain insights from the deﬁned GNN design space, such as
“Is batch normalization generally useful for GNNs?” However, the deﬁned design and task space lead to over 10M possible combinations, prohibiting a full grid search. Thus, we develop a controlled random search evaluation procedure to efﬁciently understand the trade-offs of each design dimension.
Based on these innovations, our work provides the following key results: (1) A comprehensive set of guidelines for designing well-performing GNNs (Sec. 7.3). (2) While best GNN designs for different tasks/datasets vary signiﬁcantly, the deﬁned GNN task space allows for transferring the best designs across tasks (Sec. 7.4). This saves redundant algorithm development efforts for highly similar GNN tasks, while also being able to identify novel GNN tasks which can inspire new GNN designs. (3)
Models discovered from our design space achieve state-of-the-art performance on a new task from the Open Graph Benchmark [10] (Sec. 7.5).
Overall, our work suggests a transition from studying speciﬁc GNN designs for speciﬁc tasks to studying the GNN design space, which offers exciting opportunities for GNN architecture design.
Our work also facilitates reproducible GNN research, where GNN models and tasks are precisely described and then evaluated using a standardized protocol. Using the proposed GraphGym platform reproducing experiments and fairly comparing models requires minimal effort. 1Project website with data and code: http://snap.stanford.edu/gnn-design 2https://github.com/snap-stanford/graphgym 2
(a) GNN Design Space (b) GNN Task Space
Task Similarity Metric
Anchor Model 
Performance ranking
Task ! "! "" "# "$ "%
Task # "! "# "" "$ "%
Task $ "% "! "$ "# ""
Similarity to Task ! 1.0 0.8
-0.4
Building Task Space for GNN Tasks
!
"
# (c) Best GNN Designs Found in Different Tasks
Pre-process layers Message passing layers
Post-process layers
Layer connectivity
Aggregation
Task !: graph-IMDB
Task #: node-smallworld
Task $: node-CiteSeer 2 1 2 8 8 6 2 2 2 skip-sum skip-sum skip-cat sum sum mean
Figure 1: Overview of the proposed GNN design and task space. (a) A GNN design space consists of 12 design dimensions for intra-layer design, inter-layer design and learning conﬁguration. (b) We apply a ﬁxed set of “anchor models” to different tasks/datastes, then use the Kendall rank correlation of their performance to quantify the similarity between different tasks. This way, we build the GNN task space with a proper similarity metric. (c) The best GNN designs for tasks A, B, C. Notice that tasks with higher similarity share similar designs, indicating the efﬁcacy of our GNN task space. 2