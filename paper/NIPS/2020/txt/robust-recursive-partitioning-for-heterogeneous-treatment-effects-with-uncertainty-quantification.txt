Abstract
Subgroup analysis of treatment effects plays an important role in applications from medicine to public policy to recommender systems. It allows physicians (for example) to identify groups of patients for whom a given drug or treatment is likely to be effective and groups of patients for which it is not. Most of the current methods of subgroup analysis begin with a particular algorithm for estimating individualized treatment effects (ITE) and identify subgroups by maximizing the differences across subgroups of the average treatment effect in each subgroup.
These approaches have several weaknesses: they rely on a particular algorithm for estimating ITE, they ignore (in)homogeneity within identiﬁed subgroups, and they do not produce good conﬁdence estimates. This paper develops a new method for subgroup analysis, R2P, that addresses all these weaknesses. R2P uses an arbitrary, exogenously prescribed algorithm for estimating ITE and quantiﬁes the uncertainty of the ITE estimation, using a construction that is more robust than other methods. Experiments using synthetic and semi-synthetic datasets (based on real data) demonstrate that R2P constructs partitions that are simultaneously more homogeneous within groups and more heterogeneous across groups than the partitions produced by other methods. Moreover, because R2P can employ any ITE estimator, it also produces much narrower conﬁdence intervals with a prescribed coverage guarantee than other methods. 1

Introduction
The understanding of treatment effects plays an important role – especially in shaping interventions and treatments – in areas from clinical trials [1, 2] to recommender systems [3] to public policy [4].
In many settings, the relevant population is diverse, and different parts of the population display different reactions to treatment. In such settings, heterogeneous treatment effect (HTE) analysis – also called subgroup analysis– is used to ﬁnd subgroups consisting of subjects who have similar covariates and display similar treatment responses [5, 6]. The identiﬁcation of subgroups is informative of itself; it also improves the interpretation of treatment effects across the entire population and makes it possible to develop more effective interventions and treatments and to improve the design of further experiments. In a clinical trial, for example, HTE analysis can identify subgroups of the population
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for which the studied treatment is effective, even when it is found to be ineffective for the population in general [7].
To identify subjects who have similar covariates and display similar treatment responses, it is necessary to create reliable estimates of the treatment responses of individual subjects; i.e. of individualized treatment effects (ITE). The state-of-the-art work on HTE proceeds by simultaneously estimating ITE and recursively partitioning the subject population [8–11]. In these HTE methods, the criterion for partitioning is maximizing the heterogeneity of treatment effects across subgroups, using a sample mean estimator, under the assumption that treatment effects are homogeneous within subgroups.
In particular, the population (or any previously identiﬁed subgroup) would be partitioned into two subgroups provided that the sample means of these subgroups are sufﬁciently different, ignoring the possibility that treatment effects might be very heterogeneous within the groups identiﬁed. Put differently, these methods focus on inter-group heterogeneity but ignore intra-group heterogeneity.
An important problem with this approach is that, because it relies solely on inter-group heterogeneity based on sam-ple means, it may lead to false discovery. To illustrate, consider the toy example depicted in Fig. 1. In this ex-ample, the true treatment effect (shown on the vertical axis) was generated by iid random draws from the normal distribution having mean 0.0 and standard deviation 0.1.
In truth, the treatment under consideration is in fact totally ineffective and innocuous; on average, it has no effect at all and the treatment effects are entirely uncorrelated with the single covariate (shown on the horizontal axis). How-ever if the observed data – the realization of the random draws – happens to be the one shown in Fig. 1, standard methods will typically partition the population as shown in the ﬁgure, thereby “discovering” a segment of the population for whom the treatment is effective and a complementary segment where the treatment is dangerous. Obviously, decisions based on such false discovery are useless – or worse. Note that this false discovery occurs because, although the outcome variations between the two groups are indeed substantially different, the outcome variations within each group are just as different – but the latter variation is entirely ignored in the creation of subgroups.
This paper proposes a robust recursive partitioning (R2P)2 method that avoids such false discovery.
R2P has several distinctive characteristics.
Figure 1: A toy example with two sub-groups identiﬁed by HTE method in [10].
The solid red line shows the ITE estima-tion and their 95% conﬁdence interval is
ﬁlled in red.
• R2P discovers interpretable subgroups in a way that is not tied to any particular ITE estimator. This is in sharp contrast with previous methods [8–12], each of which relies on a speciﬁc ITE estimator.
R2P can leverage any ITE estimator for subgroup analysis, e.g. an ITE estimator based on Random
Forest [13], or on multi-task Gaussian processes [14] or on deep neural networks [15–17]. This
ﬂexibility is important because no one ITE estimator is consistently the best in all different settings
[18]. Furthermore, these ITE estimators are non-interpretable black-box models. R2P divides units into subgroups with respect to an interpretable tree-structure, and provides subgroup coverage guarantees for the ITE estimates in each subgroup. R2P enables these ITE estimators to produce trustworthy and interpretable ITE estimates in practice.
• R2P makes a conscious effort to guarantee homogeneity of treatment effects within each of the subgroups while maximizing heterogeneity across subgroups. This is also different from previous methods, e.g., [8, 10] where variation within the subgroup is largely ignored.
• R2P produces conﬁdence guarantees with narrower conﬁdence intervals than previous methods. It accomplishes this by using methods of conformal prediction [19] that produce valid conﬁdence intervals. Quantifying the uncertainty allows R2P to employ a novel criterion we call conﬁdent homogeneity in order to create partitions that take account of both heterogeneity across groups and homogeneity within groups.
These characteristics make R2P both more reliable and more informative than the existing methods for subgroup analysis. Extensive experiments using synthetic and semi-synthetic datasets (based on real-world data) demonstrate that R2P outperforms state-of-the-art methods by more robustly identifying subgroups while providing much narrower conﬁdence intervals. 2The code of R2P is available at: https://bitbucket.org/mvdschaar/mlforhealthlabpub. 2
2 Robust Recursive Partitioning with Uncertainty Quantiﬁcation
To highlight the core design principles, we begin by introducing robust recursive partitioning (R2P) in the regression setting; we extend to the more complicated HTE setting in the next section. 2.1 Preliminaries
We consider a standard regression problem with a d-dimensional covariate (input) space X ⊆ Rd and a outcome space Y ⊆ R. We are given a dataset D = {(xi, yi)}n i=1, where, for the i-th sample, xi ∈ X is the vector of input covariates and yi ∈ Y is the outcome. We assume that samples are independently drawn from an unknown distribution PX,Y on X × Y. We are interested in estimating
µ(x) = E[Y |X = x], which is the mean outcome conditional on x. We denote the estimator by
ˆµ : X → Y; ˆµ predicts an outcome ˆy = ˆµ(x) on the basis of the covariate information x. To quantify the uncertainty in the prediction, we apply the method of split conformal regression (SCR) [19] to construct a conﬁdence interval ˆC that satisﬁes the rigorous frequentist guarantee in the ﬁnite-sample regime. (To the best of our knowledge, SCR is the simplest method that achieves this guarantee.)
In SCR, we take as given a miscoverage rate α ∈ (0, 1). We split the samples in D into a training set
I1 and a validation set I2 that are disjoint and have the same size. We train the estimator ˆµI1 on I1 and compute the residual of ˆµI1 on each sample in I2. For a testing sample x the conﬁdence interval is given by
ˆC I1,I2(x) = (cid:2)ˆµlo(x), ˆµup(x)(cid:3) = (cid:104)
ˆµI1 (x) − ˆQI2 1−α, ˆµI1 (x) + ˆQI2 1−α
, (1) (cid:105) where ˆQI2 1−α is deﬁned to be the (1 − α)(1 + 1/|I2|)-th quantile of the set of residuals {|yi −
ˆµI1(xi)|}i∈I2. Assuming that the training and testing samples are drawn exchangeably from PX,Y , the conﬁdence intervals deﬁned in (1) satisfy the coverage guarantee P[y ∈ ˆC I1,I2] ≥ 1 − α [19].3
To illustrate, assume the miscoverage rate α is 0.05 and we are given 1000 testing samples. SCR prescribes a conﬁdence interval for each sample in such a way that for at least 950 samples the prediction is within the prescribed conﬁdence interval. (We often say the sample is covered.) However this coverage guarantee is marginal over the entire covariate space. If we perform a subgroup analysis that partitions the covariate space X into subgroups X1, X2 in such a way that X1 has 800 samples and X2 has 200 samples, it might be the case that 790 samples in X1 are covered but only 160 samples in X2 are covered. In this case, 80% of the samples in X2 would be covered. It seems obvious that such a situation is undesirable for subgroup analysis; we want to achieve the prescribed coverage rate for each subgroup, not just for the population as a whole. R2P overcomes this problem.
We begin by discussing how we use conﬁdence intervals to quantify outcome homogeneity within a subgroup. We then introduce our space partitioning algorithm and provide the required theoretical guarantee of subgroup coverage. 2.2 Partitioning for Robust Heterogeneity Analysis
Let Π be a partition of the covariate space X into (disjoint) subgroups. Write |Π| for the number of subgroups in the partition, lj for an element of Π and l(x; Π) for the subgroup that contains the sample x. Write Dl = {(xi, yi) ∈ D|xi ∈ l} for the samples whose covariates belong to the subgroup l. Note that when we restrict to covariates in the subgroup l, the samples are drawn from the truncated distribution P l
X,Y which is the distribution conditional on the requirement that the vector of covariates of samples lie in the subgroup l. 1 and I l
We evaluate homogeneity within the subgroup l by the concentration of outcome values for covariate vectors in the subgroup l. To do this, we apply SCR to the samples in Dl by splitting it into 2. Write ˆµl(x) denote the mean outcome model trained on I l two sets, I l 1. As in (1), we obtain the conﬁdence interval ˆCl(x) for subgroup l by setting the upper and lower endpoints to l (x) = ˆµl(x) − ˆQIl be ˆµup (To avoid notational complications, omit reference to the subsets I l 1 and I l 2 hereafter; this should not cause confusion.
Throughout, we follow the convention that the conﬁdence bound have been computed on the basis of the split.) To estimate the center of the subgroup l, we use the average outcome ˆµl,mean = E[ˆµl(x)]. l (x) = ˆµl(x) + ˆQIl 1−α, respectively. 1−α and ˆµlo 2 2 3Recall that assuming exchangeability is weaker than assuming iid. 3
Figure 2: Illustration of partitioning and impurity of the conﬁdent homogeneity. The regions shaded in red and gray (roughly) represent Wl and Sl, respectively. Partitioning the heterogeneous covariate space (left panel) reduces its impurity of the conﬁdent homogeneity. The partition with the smaller impurity (middle panel) makes the heterogeneity across subgroups and the homogeneity within subgroups both stronger compared to others with the larger impurity (e.g., right panel).
We deﬁne the expected absolute deviation in group l to be Sl = E[vl(x)], where l (x)(cid:3) . (cid:1) I (cid:2)ˆµl,mean < ˆµlo vl(x) = (cid:0)ˆµl,mean − ˆµup l (x)(cid:1) I (cid:2)ˆµl,mean > ˆµup l (x)(cid:3) + (cid:0)ˆµlo l (x) − ˆµl,mean l (x) is larger than ˆµlo (2) l (x) provided the residual quantile ˆQ1−α > 0. When the ﬁrst
By deﬁnition, ˆµup indicator function is one, i.e. the average outcome (the group center) ˆµl,mean is larger than the upper bound ˆµup l (x) at x, we are conﬁdent that the outcome value at x is smaller than the group center (and perhaps smaller than the outcome value for many covariate vectors in l). Similarly, when the second indicator function is one, we are certain that the outcome value at x is larger than the group center (and perhaps larger than the outcome value for many covariate vectors in l). It is worth noting that when ˆCl(x) = (cid:2)ˆµlo l (x)(cid:3) contains the center ˆµl,mean, both indicator functions are zero and vl(x) = 0. The quantity vl(x) evaluates the homogeneity in subgroup l on the basis of the conﬁdence interval for each x in l. (It is more conservative than the mean discrepancy |ˆµl(x) − ˆµl,mean| for partitioning, and hence provides greater protection against false discovery because of uncertainty.)
However, minimizing Sl is not enough to maximize subgroup homogeneity. If the intervals ˆCl(x) for all x ∈ l are very wide and contain the average outcome ˆµl,mean, homogeneity can be very low even though Sl = E[vl(x)] is zero. To resolve this issue, when partitioning the covariate space we jointly minimize Sl and the expected conﬁdence interval width Wl = E(cid:2)| ˆCl(x)|(cid:3). We formalize the robust partitioning problem as l (x), ˆµup minimize
Π (cid:88) l∈Π
λWl + (1 − λ)Sl, (3) where λ ∈ [0, 1] is a hyperparameter that balances the impact of Wl and Sl. We call the weighted sum, λWl + (1 − λ)Sl, the impurity of the conﬁdent homogeneity for subgroup l. Fig. 2 illustrates how minimizing the impurity of the conﬁdent homogeneity improves both homogeneity within subgroups and heterogeneity across subgroups. There may be more than one partition that achieves the minimum; because a larger number of subgroups is harder to interpret, we will choose a minimizer with the smallest number of subgroups. 2.3 Robust Recursive Partitioning Method with Conﬁdent Homogeneity
We can now describe our robust recursive partitioning (R2P) method for solving the optimization problem (3). We begin with the trivial partition Π = {X }. We denote the set of subgroups whose objectives in (3) can be potentially improved by Πc. In the initialization step, we set Πc = Π and apply the SCR on D to obtain the conﬁdence intervals in (1). Based on these intervals, we compute
ˆWX and ˆSX . More generally, using the intervals for subgroup l, we can estimate Wl and Sl by
ˆWl = 1
N l 2 (cid:88) i∈Il 2
| ˆCl(xi)| and ˆSl = 1
N l 2 (cid:88) i∈Il 2 vl(xi), respectively, where N l 2 is the number of samples in the validation set I l 2.
After initialization, we recursively partition the covariate space by splitting the subgroups in Πc with respect to the criterion in (3). To split each subgroup l ∈ Πc, we ﬁrst consider the two disjoint subsets from subgroup l given by l+ k (φ) = {x ∈ l|xk < φ}, where
φ ∈ (xl,min and k xl,max are the minimum and maximum values of the k-th covariate within the subgroup l, respectively. k
) is the threshold for splitting, xk is the k-th covariate element, and xl,min k (φ) = {x ∈ l|xk ≥ φ} and l−
, xl,max k k 4
k (φ) 1 k (φ) into training and validation sets: Dl+
∪ I l+ k (φ) = I l+ k (φ) 1 1 : xi ∈ l+
= {(xi, yi) ∈ I l
We then apply SCR to each of these subsets. Speciﬁcally, we split the samples corresponding to l+
∪ I l− and l− where the split subsets are I l+
= {(xi, yi) ∈ k (φ)} (same for l− 2 : xi ∈ l+
I l k (φ)). To compute residuals, we do not train new estimators for l+ k (φ) and l− k (φ); instead we use the previously trained estimator ˆµX ; this provides consistency of estimators across groups and within groups. (It also avoids the enormous computational burden of training new estimators for all the possible splits.) Using the residuals, we can construct the conﬁdence intervals ˆCl+ k (φ)(x) and the associated quantities in the objective function,
ˆWl+ k (φ), ˆSl+ l for splitting subgroup l as k (φ). Then we ﬁnd the optimal covariate k∗ k (φ) and Dl− 2 k (φ)} and I l+ k (φ)(x) and ˆCl− l and threshold φ∗ k (φ) and ˆSl− k (φ) = I l− k (φ) k (φ)
, 2 k (φ), ˆWl− k (φ) 1 k (φ) 2 (k∗ l , φ∗ l ) = argmin
λ (k,φ) (cid:16) ˆWl+ k (φ) + ˆWl− k (φ) (cid:17)
+ (1 − λ) (cid:16) ˆSl+ k (φ) + ˆSl− k (φ) (cid:17)
. l , φ∗ l ), we compute ˆW ∗
For (k∗ the objective in (3), we split the subgroup l into l+ impurity of the conﬁdent homogeneity is sufﬁciently large: k∗ (φ∗) and ˆS∗ k∗ (φ∗) and l− k∗ (φ∗) + ˆWl− l± = ˆWl+ l± = ˆSl+ k∗ (φ∗) + ˆSl− k∗ (φ∗). To improve k∗ (φ∗) only if the reduction in the (cid:104) (1 − γ)
λ ˆWl + (1 − λ) ˆSl (cid:105)
≥ λ ˆW ∗ l± + (1 − λ) ˆS∗ l± (4)
Here, γ ∈ [0, 1) is a hyperparameter for regularization. We refer to (4) as the conﬁdent criterion.
With an appropriate choice of γ, this criterion prevents overﬁtting, prevents the number of subgroups from becoming too large and prevents the size of each subgroup from becoming too small. Conﬁdent homogeneity does not necessarily improve as the group size shrinks because smaller groups lead to greater uncertainty. This alleviates the issue of generalization to unseen data in HTE analysis [8, 10]. (cid:104)
After the splitting decision, we remove l from Πc; if we have split l, we remove l from Π and add the two split sets to both Π and Πc. We continue recursively until Πc is empty, at which point no further splitting is productive. When the procedure stops, we will have obtained an estimator µX and a partition Π and for each l ∈ Π we will have corresponding conﬁdence intervals
, where ˆQl
ˆCl(x) = 2|)-th quantile of the set of the residuals on the validation set I l
. The following theorem guarantees that the R2P partition Algorithm 1 provides a valid conﬁdence interval ˆCl for each subgroup l ∈ Π; this is exactly what a user would want in subgroup analysis. The proof is provided in the supplementary material. 2 using ˆµX , {|yi − ˆµX (xi)|}i∈Il 1−α is the (1 − α)(1 + 1/|I l 1−α, ˆµX (x) + ˆQl
ˆµX (x) − ˆQl 1−α (cid:105) 2
Theorem 1 Given a prescribed miscoverage rate α, the created partition Π, estimator ˆµX , and conﬁdence interval function ˆCl(·) have the following property: for each l ∈ Π and for new samples (x, y) drawn from the truncated distribution P l
X,Y , we have P[y ∈ ˆCl(x)] ≥ 1 − α.
Algorithm 1 Robust Recursive Partitioning 1: Input: Samples D = {(xi, yi)}n 2: Initialize: Πc = Π, split D into I X the split subsets, and obtain ˆWX and ˆSX using I X 2 3: for l ∈ Πc do l± , ˆS∗
Obtain ˆW ∗ l± , i∗, and φ∗ 4: (cid:104)
λ ˆWl + (1 − λ) ˆSl if (1 − γ) 5: i=1, miscoverage rate α ∈ (0, 1), Π = {X } 1 and I X 2 , train ˆµX , compute its conﬁdence interval ˆCX using (cid:105)
≥ λ ˆW ∗ l± + (1 − λ) ˆS∗ l± then (cid:46) Conﬁdent criterion
Partition l into l+(i∗, φ∗) and l−(i∗, φ∗)
Π ← Π \ {l}
Π ← Π ∪ {l+(i∗, φ∗), l−(i∗, φ∗)} and Πc ← Πc ∪ {l+(i∗, φ∗), l−(i∗, φ∗)} 6: 7: 8: 9: 10: 11: end for 12: Output: Π, ˆµX , and ˆCl for all l ∈ Π end if
Πc ← Πc \ {l} 5
3 Robust Recursive Partitioning for Heterogeneous Treatment Effects
In this section, we extend the R2P method to the setting of HTE estimation, resulting in the R2P-HTE design as detailed below.
Heterogeneous Treatment Effect Model We consider a setup with n units (samples), For the unit i ∈ {1, 2, ..., n}, there exists a pair of potential outcomes, Yi(1) and Yi(0) that are independently drawn from an unknown distribution, where 0 and 1 represent whether the unit is treated or not, respectively. We deﬁne the treatment indicator as ti ∈ {0, 1}, where ti = 1 and 0 indicate the unit i is treated and untreated, respectively. The outcome for unit i is realized as the potential outcome corresponding to its treatment indicator yi = Yi(ti). A dataset is given as DHTE = {(xi, ti, yi)}n i=1.
The ITE for a given x is deﬁned as τ (x) = E[Y (1) − Y (0)|X = x]. Since the ITE is deﬁned as the expected difference between the two potential outcomes Y (1) and Y (0), its estimator ˆτ (x) is given as the contrast between two regression models: ˆµ0(x) for the conditional non-treated outcome
E[Y (0)|X = x], and ˆµ1(x) for the conditional treated outcome E[Y (1)|X = x].
R2P-HTE We adapt R2P to HTE estimation by constructing the quantities Wl and Sl in (3) based on the ITE estimator ˆτ (x). To construct an ITE estimator, many popular machine learning models have been considered in the literature [8, 13–17]. R2P-HTE can use one of these models to parameterize the outcome models ˆµ0(x) and ˆµ1(x). We set the target coverage rate of ˆµ0(x) and ˆµ1(x) as 1 − α.
As in the previous section, we can construct a conﬁdence interval for each estimator by using the 1 − α conﬁdence intervals for Y (1) and Y (0) by split conformal regression. Let us denote the (cid:105) (cid:105)
ˆC 1(x) =
, respectively. We set the conﬁdence interval for τ (x) to be 1−α, ˆµ1(x) + ˆQ1√ 1−α, ˆµ0(x) + ˆQ0√ and ˆC 0(x) =
ˆµ0(x) − ˆQ0√
ˆµ1(x) − ˆQ1√ 1−α 1−α
√
√ (cid:104) (cid:104)
ˆC τ (x) = (cid:104)
ˆµ1(x) − ˆµ0(x) − ˆQ1√ 1−α − ˆQ0√ 1−α, ˆµ1(x) − ˆµ0(x) + ˆQ1√ 1−α + ˆQ0√ 1−α (cid:105)
.
This conﬁdence interval ensures the coverage rate 1 − α for the estimated ITE ˆτ (x) = ˆµ1(x) − ˆµ0(x), because its upper endpoint is given as the difference between the upper endpoint of ˆC 1 and the lower endpoint of ˆC 0, and its lower endpoint is given as the difference between the lower endpoint of ˆC 1 and the upper endpoint of ˆC 0. If the coverage rates for ˆC 1 and ˆC 0 are 1 − α, the coverage rate for
ˆC τ will be 1 − α.
From the ITE estimator ˆτ (x) and its conﬁdence interval ˆC τ l (x) for each subgroup l, we can calculate
Wl and Sl and adapt the R2P method to HTE estimation. The robust partitioning problem for HTE in (3) is solved by applying the R2P method in Algorithm 1, with two minor changes: 1) each sample in the HTE dataset is a triple (xi, ti, yi) consisting of the covariate vector, the treatment indicator, and the observed outcome; 2) the outcome model ˆµ(x) in R2P is replaced by the ITE estimator ˆτ (x). As before, we show that this procedure achieves the speciﬁed coverage guarantee. The proof is provided in the supplementary material.
√
Theorem 2 Given a prescribed miscoverage rate α, the created partition Π, estimator ˆτX , and conﬁdence interval function ˆC τ l (·) have the following property: for each l ∈ Π and for new samples (x, τ ) drawn from the truncated distribution of the subgroup l, we have P[τ ∈ ˆC τ l (x)] ≥ 1 − α.
Well-identiﬁed subgroups and false discovery Theorem 2 guarantees that conﬁdence intervals achieve the required ﬁnite sample coverage for the ITE estimates in each subgroup, regardless of how (in)accurate the underlying ITE estimator ˆτX is. If the conﬁdence intervals exhibit large overlap across the constructed subgroups, we can conclude that the constructed subgroups are not well-identiﬁed.
Conversely, if the conﬁdence intervals have little or no overlap across subgroups, we conclude that the subgroups are well-identiﬁed. Given the theoretical guarantee of the conﬁdence intervals in R2P, the subgroups are robust against false discoveries if they are well-identiﬁed. 4