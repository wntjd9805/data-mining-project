Abstract
We present the Supermasks in Superposition (SupSup) model, capable of sequen-tially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, ﬁxed base network and for each task ﬁnds a subnet-work (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to ﬁnd a linear superposition of learned supermasks which minimizes the output entropy.
In practice we ﬁnd that a single gradient step is often sufﬁcient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions.
First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a ﬁxed-sized Hopﬁeld network. 1

Introduction
Learning many different tasks sequentially without forgetting remains a notable challenge for neural networks [47, 56, 23]. If the weights of a neural network are trained on a new task, performance on previous tasks often degrades substantially [33, 10, 12], a problem known as catastrophic forgetting.
In this paper, we begin with the observation that catastrophic forgetting cannot occur if the weights of the network remain ﬁxed and random. We leverage this to develop a ﬂexible model capable of learning thousands of tasks: Supermasks in Superposition (SupSup). SupSup, diagrammed in
Figure 1, is driven by two core ideas: a) the expressive power of untrained, randomly weighted subnetworks [57, 39], and b) inference of task-identity as a gradient-based optimization problem. a) The expressive power of subnetworks Neural networks may be overlaid with a binary mask that selectively keeps or removes each connection, producing a subnetwork. The number of possible subnetworks is combinatorial in the number of parameters. Researchers have observed that the number of combinations is large enough that even within randomly weighted neural networks, there exist supermasks that create corresponding subnetworks which achieve good performance on complex tasks. Zhou et al. [57] and Ramanujan et al. [39] present two algorithms for ﬁnding these supermasks while keeping the weights of the underlying network ﬁxed and random. SupSup scales to many tasks by ﬁnding for each task a supermask atop a shared, untrained network. b) Inference of task-identity as an optimization problem When task identity is unknown,
SupSup can infer task identity to select the correct supermask. Given data from task j, we aim
∗Equal contribution. †Also afﬁliated with the University of Washington. Code available at https://github. com/RAIVNLab/supsup and correspondence to {mitchnw,ramanv}@cs.washington.edu. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (left) During training SupSup learns a separate supermask (subnetwork) for each task. (right) At inference time, SupSup can infer task identity by superimposing all supermasks, each weighted by an αi, and using gradients to maximize conﬁdence. to recover and use the supermask originally trained for task j. This supermask should exhibit a conﬁ-dent (i.e. low entropy) output distribution when given data from task j [19], so we frame inference of task-identity as an optimization problem—ﬁnd the convex combination of learned supermasks which minimizes the entropy of the output distribution.
In the rest of the paper we develop and evaluate SupSup via the following contributions: 1. We propose a new taxonomy of continual learning scenarios. We use it to embed and contextualize related work (Section 2). 2. When task identity (ID) is provided during train and test (later dubbed GG), SupSup is a natural extension of Mallya et al. [30]. By using a randomly weighted backbone and controlling mask sparsity, SupSup surpasses recent baselines on SplitImageNet [51] while requiring less storage and time costs (Section 3.2). 3. When task ID is provided during train but not test (later dubbed GN), SupSup outperforms recent methods that require task ID [26, 23, 4], scaling to 2500 permutations of MNIST without forgetting. For these uniform tasks, ID can be inferred with a single gradient computation (Section 3.3). 4. When task identities are not provided at all (later dubbed NNs), SupSup can even infer task boundaries and allocate new supermasks as needed (Section 3.4). 5. We introduce an extension to the basic SupSup algorithm that stores supermasks implicitly as attractors in a ﬁxed-size Hopﬁeld network [20] (Section 3.5). 6. Finally, we empirically show that the simple trick of adding superﬂuous neurons results in more accurate task inference (Section 3.6). 2 Continual Learning Scenarios and