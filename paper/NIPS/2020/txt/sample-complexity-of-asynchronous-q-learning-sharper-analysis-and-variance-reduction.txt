Abstract
Asynchronous Q-learning aims to learn the optimal action-value function (or Q-function) of a Markov decision process (MDP), based on a single trajectory of
Markovian samples induced by a behavior policy. Focusing on a γ-discounted
MDP with state space S and action space A, we demonstrate that the (cid:96)∞-based sample complexity of classical asynchronous Q-learning — namely, the number of samples needed to yield an entrywise ε-accurate estimate of the Q-function — is at most on the order of 1
µmin(1 − γ)5ε2 + tmix
µmin(1 − γ) up to some logarithmic factor, provided that a proper constant learning rate is adopted. Here, tmix and µmin denote respectively the mixing time and the minimum state-action occupancy probability of the sample trajectory. The ﬁrst term of this bound matches the complexity in the case with independent samples drawn from the stationary distribution of the trajectory. The second term reﬂects the expense taken for the empirical distribution of the Markovian trajectory to reach a steady state, which is incurred at the very beginning and becomes amortized as the algorithm runs. Encouragingly, the above bound improves upon the state-of-the-art result by a factor of at least |S||A|. Further, the scaling on the discount complexity can be improved by means of variance reduction. 1

Introduction
Model-free algorithms such as Q-learning [46] play a central role in recent breakthroughs of rein-forcement learning (RL) [32]. In contrast to model-based algorithms that decouple model estimation and planning, model-free algorithms attempt to directly interact with the environment — in the form of a policy that selects actions based on perceived states of the environment — from the collected data samples, without modeling the environment explicitly. Therefore, model-free algorithms are able to process data in an online fashion and are often memory-efﬁcient. Understanding and improving the sample efﬁciency of model-free algorithms lie at the core of recent research activity [19], whose importance is particularly evident for the class of RL applications in which data collection is costly and time-consuming (such as clinical trials, online advertisements, and so on).
This paper concentrates on Q-learning — an off-policy model-free algorithm that seeks to learn the optimal action-value function by observing what happens under a behavior policy. The off-policy feature makes it appealing in various RL applications where it is infeasible to change the policy under evaluation on the ﬂy. There are two basic update models in Q-learning. The ﬁrst one is termed a synchronous setting, which hypothesizes on the existence of a simulator (or a generative model); at each time, the simulator generates an independent sample for every state-action pair, and the estimates are updated simultaneously across all state-action pairs. The second model concerns an asynchronous setting, where only a single sample trajectory following a behavior policy is accessible; at each time, the algorithm updates its estimate of a single state-action pair using one state transition from the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Paper
Sample complexity
Learning rate
Even-Dar and Mansour (2003) [20] 1 1−γ (tcover) (1−γ)4ε2 linear: 1 t
Even-Dar and Mansour (2003) [20] (cid:0) t1+3ω cover (1−γ)4ε2 (cid:1) 1
ω + (cid:0) tcover 1−γ (cid:1) 1 1−ω polynomial: 1 tω , ω ∈ ( 1 2 , 1)
Beck and Srikant (2012) [5]
Qu and Wierman (2020) [35]
This work (Theorem 1)
This work (Theorem 2) t3 cover|S||A| (1−γ)5ε2 tmix
µ2 min(1−γ)5ε2 constant: (1−γ)4ε2
|S||A|t2 cover rescaled linear: 1
µmin(1−γ)
µmin (1−γ) ,tmix} 1 t+max{ 1
µmin(1−γ)5ε2 + tmix
µmin(1−γ) constant: min (cid:8) (1−γ)4ε2
γ2 tcover (1−γ)5ε2 constant: min (cid:8) (1−γ)4ε2
γ2 (cid:9)
, 1 tmix
, 1(cid:9)
Table 1: Sample complexity of asynchronous Q-learning to compute an ε-optimal Q-function in the (cid:96)∞ norm, where we hide all logarithmic factors. With regards to the Markovian trajectory induced by the behavior policy, we denote by tcover, tmix, and µmin the cover time, mixing time, and minimum state-action occupancy probability of the associated stationary distribution, respectively. trajectory. Obviously, understanding the asynchronous setting is considerably more challenging than the synchronous model, due to the Markovian (and hence non-i.i.d.) nature of its sampling process.
Focusing on an inﬁnite-horizon Markov decision process (MDP) with state space S and action space
A, this work investigates asynchronous Q-learning on a single Markovian trajectory. We ask a fundamental question:
How many samples are needed for asynchronous Q-learning to learn the optimal Q-function?
Despite a considerable amount of prior work exploring this algorithm (ranging from the classical work [24, 43] to the very recent paper [35]), it remains unclear whether existing sample complexity analysis of asynchronous Q-learning is tight. As we shall elucidate momentarily, there exists a large gap — at least as large as |S||A| — between the state-of-the-art sample complexity bound for asynchronous Q-learning [35] and the one derived for the synchronous counterpart [44]. This raises a natural desire to examine whether there is any bottleneck intrinsic to the asynchronous setting that signiﬁcantly limits its performance.
Our contributions. This paper develops a reﬁned analysis framework that sharpens our understand-ing about the sample efﬁciency of classical asynchronous Q-learning on a single sample trajectory.
Setting the stage, consider an inﬁnite-horizon MDP with state space S, action space A, and a discount factor γ ∈ (0, 1). What we have access to is a sample trajectory of the MDP induced by a stationary behavior policy. In contrast to the synchronous setting with i.i.d. samples, we single out two parame-ters intrinsic to the Markovian sample trajectory: (i) the mixing time tmix, which characterizes how fast the trajectory disentangle itself from the initial state; (ii) the smallest state-action occupancy probability µmin of the stationary distribution of the trajectory, which captures how frequent each state-action pair has been at least visited.
With these parameters in place, our ﬁndings unveil that: the sample complexity required for asyn-chronous Q-learning to yield an ε-optimal Q-function estimate – in a strong (cid:96)∞ sense – is at most1 (cid:16) (cid:101)O 1
µmin(1 − γ)5ε2 + tmix
µmin(1 − γ) (cid:17)
. (1)
The ﬁrst component of (1) is consistent with the sample complexity derived for the setting with independent samples drawn from the stationary distribution of the trajectory [44]. In comparison, the second term of (1) — which is unaffected by the accuracy level ε — is intrinsic to the Markovian nature of the trajectory; in essence, this term reﬂects the cost taken for the empirical distribution of the sample trajectory to converge to a steady state, and becomes amortized as the algorithm runs. In other words, the behavior of asynchronous Q-learning would resemble what happens in the setting with independent samples, as long as the algorithm has been run for reasonably long. 1Let X := (cid:0)|S|, |A|, (cid:1). The notation f (X ) = O(g(X )) means there exists a universal constant
C1 > 0 such that f ≤ C1g. The notation (cid:101)O(·) is deﬁned analogously except that it hides any logarithmic factor. 1−γ , 1 1
ε 2
Furthermore, we leverage the idea of variance reduction to improve the scaling with the discount complexity 1 1−γ . We demonstrate that a variance-reduced variant of asynchronous Q-learning attains
ε-accuracy using at most (cid:16) (cid:101)O 1
µmin(1 − γ)3 min{1, ε2} samples, matching the complexity of its synchronous counterpart if ε ≤ min (cid:8)1, (cid:9) [45].
Moreover, by taking the action space to be a singleton set, the above results immediately lead to (cid:96)∞-based sample complexity for temporal difference (TD) learning [41] on Markovian samples; the interested reader is referred to [? ] for more details. tmix
µmin(1 − γ) 1 (1−γ) (2) tmix
+
√ (cid:17)
Due to the space limits, the proofs of all theorems are deferred to the full version [? ].
Comparisons with past work. A large fraction of the classical literature focused on asymptotic convergence analysis of asynchronous Q-learning (e.g. [24, 42, 43]); these results, however, did not lead to non-asymptotic sample complexity bounds. The state-of-the-art sample complexity analysis was due to the recent work [35], which derived a sample complexity bound (cid:101)O(cid:0) (cid:1). Given the obvious lower bound 1/µmin ≥ |S||A|, our result (1) improves upon that of [35] by a factor (cid:9). In addition, we note that several prior work at least on the order of |S||A| min (cid:8)tmix,
[5, 20] developed sample complexity bounds in terms of the cover time tcover of the sample trajectory, namely, the time taken for the trajectory to visit all state-action pairs at least once. Our analysis framework readily yields another sample complexity bound (cid:101)O(cid:0) (cid:1), which strengthens the tcover (1−γ)5ε2 cover|S||A| ≥ |S|3|A|3. See Table 1 for detailed comparisons. existing bounds by a factor of at least t2 tmix min(1−γ)5ε2 1 (1−γ)4ε2
µ2 2 Models and background
This paper studies an inﬁnite-horizon MDP with discounted rewards, as represented by a quintuple
M = (S, A, P, r, γ). Here, S and A denote respectively the (ﬁnite) state space and action space, whereas γ ∈ (0, 1) indicates the discount factor. We use P : S × A → ∆(S) to represent the probability transition kernel of the MDP, where for each state-action pair (s, a) ∈ S × A, P (s(cid:48) | s, a) denotes the probability of transiting to state s(cid:48) from state s when action a is executed. The reward function is represented by r : S × A → [0, 1], such that r(s, a) denotes the immediate reward from state s when action a is taken; for simplicity, we assume throughout that all rewards lie within [0, 1].
We focus on the tabular setting which, despite its basic form, is not yet well understood.
Q-function and the Bellman operator. An action selection rule is termed a policy and represented by a mapping π : S → ∆(A), which maps a state to a distribution over the set of actions. A policy is said to be stationary if it is time-invariant. We denote by {st, at, rt}∞ t=0 a sample trajectory, where st (resp. at) denotes the state (resp. the action taken), and rt = r(st, at) denotes the reward received at time t. It is assumed throughout that the rewards are deterministic and depend solely upon the current state-action pair. We denote by V π : S → R the value function of a policy π, namely,
∀s ∈ S :
V π(s) := E (cid:34) ∞ (cid:88) t=0
γtr(st, at) (cid:12) (cid:12) s0 = s
, (cid:35) which is the expected discounted cumulative reward received when (i) the initial state is s0 = s, (ii) the actions are taken based on the policy π (namely, at ∼ π(st) for all t ≥ 0) and the trajectory is generated based on the transition kernel (namely, st+1 ∼ P (·|st, at)). It can be easily veriﬁed that 1−γ for any π. The action-value function (also Q-function) Qπ : S × A → R of a 0 ≤ V π(s) ≤ 1 policy π is deﬁned by
∀(s, a) ∈ S × A :
Qπ(s, a) := E (cid:34) ∞ (cid:88) t=0
γtr(st, at) (cid:12) (cid:12) s0 = s, a0 = a
, (cid:35) where the actions are taken according to the policy π except the initial action (i.e. at ∼ π(st) for all t ≥ 1). As is well-known, there exists an optimal policy — denoted by π(cid:63) — that simultaneously maximizes V π(s) and Qπ(s, a) uniformly over all state-action pairs (s, a) ∈ (S × A). Here and throughout, we shall denote by V (cid:63) := V π(cid:63) the optimal value function and the and Q(cid:63) := Qπ(cid:63) 3
optimal Q-function, respectively. In addition, the Bellman operator T , which is a mapping from
R|S|×|A| to itself, is deﬁned such that the (s, a)-th entry of T (Q) is given by (cid:105)
Q(s(cid:48), a(cid:48))
T (Q)(s, a) := r(s, a) + γEs(cid:48)∼P (·|s,a) (cid:104)
. max a(cid:48)∈A
It is well known that the optimal Q-function Q(cid:63) is the unique ﬁxed point of the Bellman operator.
Sample trajectory and behavior policy.
Imagine we have access to a sample trajectory
{st, at, rt}∞ t=0 generated by the MDP M under a given stationary policy πb — called a behav-ior policy. The behavior policy is deployed to help one learn the “behavior” of the MDP under consideration, which often differs from the optimal policy being sought. Given the stationarity of πb, the sample trajectory can be viewed as a sample path of a time-homogeneous Markov chain over all state-action pairs. Throughout this paper, we impose the following assumption [34].
Assumption 1. The Markov chain induced by the stationary behavior policy πb is uniformly ergodic.
There are several properties concerning the behavior policy and its resulting Markov chain that play a crucial role in learning the optimal Q-function. Speciﬁcally, denote by µπb the stationary distribution (over all state-action pairs) of the aforementioned behavior Markov chain, and deﬁne
µmin := min (s,a)∈S×A
Intuitively, µmin reﬂects an information bottleneck — the smaller µmin is, the more samples are needed in order to ensure all state-action pairs are visited sufﬁciently many times. In addition, we deﬁne the associated mixing time of the chain as
µπb(s, a). (3) tmix := min (cid:110) t (cid:12) (cid:12) (cid:12) max (s0,a0)∈S×A dTV (cid:0)P t(·|s0, a0), µπb (cid:1) ≤ (cid:111)
, 1 4 (4) where P t(·|s0, a0) denotes the distribution of (st, at) conditional on the initial state-action pair (s0, a0), and dTV(µ, ν) stands for the total variation distance between two distributions µ and ν [34].
In words, the mixing time tmix captures how fast the sample trajectory decorrelates from its initial state. Moreover, we deﬁne the cover time associated with this Markov chain as follows (cid:110)
P(cid:0)Bt|s0, a0 (cid:1) ≥ (cid:111)
, 1 2 t | tcover := min min (s0,a0)∈S×A where Bt denotes the event such that all (s, a) ∈ S × A have been visited at least once between time (cid:1) denotes the probability of Bt conditional on the initial state (s0, a0). 0 and time t, and P(cid:0)Bt|s0, a0
Goal. Given a single sample trajectory {st, at, rt}∞ t=0 generated by the behavior policy πb, we aim to compute/approximate the optimal Q-function Q(cid:63) in an (cid:96)∞ sense. The current paper focuses on characterizing, in a non-asymptotic manner, the sample efﬁciency of classical Q-learning and its variance-reduced variant. (5) 3 Asynchronous Q-learning on a single trajectory
Algorithm. The Q-learning algorithm [46] is arguably one of the most famous off-policy algorithms aimed at learning the optimal Q-function. Given the Markovian trajectory {st, at, rt}∞ t=0 generated by the behavior policy πb, the asynchronous Q-learning algorithm maintains a Q-function estimate
Qt : S × A → R at each time t and adopts the following iterative update rule
Qt(st−1, at−1) = (1 − ηt)Qt−1(st−1, at−1) + ηtTt(Qt−1)(st−1, at−1)
Qt(s, a) = Qt−1(s, a),
∀(s, a) (cid:54)= (st−1, at−1) (6) for any t ≥ 0, whereas ηt denotes the learning rate or the step size. Here Tt denotes the empirical
Bellman operator w.r.t. the t-th sample, that is,
Tt(Q)(st−1, at−1) := r(st−1, at−1) + γ max a(cid:48)∈A
Q(st, a(cid:48)). (7)
It is worth emphasizing that at each time t, only a single entry — the one corresponding to the sampled state-action pair (st−1, at−1) — is updated, with all remaining entries unaltered. While the estimate Q0 can be initialized to arbitrary values, we shall set Q0(s, a) = 0 for all (s, a) unless otherwise noted. The corresponding value function estimate Vt : S → R at time t is thus given by (8)
∀s ∈ S :
Qt(s, a).
Vt(s) := max a∈A
The complete algorithm is described in Algorithm 1. 4
Algorithm 1: Asynchronous Q-learning 1 input parameters: learning rates {ηt}, number of iterations T . 2 initialization: Q0 = 0. 3 for t = 1, 2, · · · , T do 4
Draw action at−1 ∼ πb(st−1) and next state st ∼ P (·|st−1, at−1).
Update Qt according to (6). 5
Theoretical guarantees for asynchronous Q-learning. We are in a position to present our main theory regarding the non-asymptotic sample complexity of asynchronous Q-learning, for which the key parameters µmin and tmix deﬁned respectively in (3) and (4) play a vital role. The proof of this result is deferred to the full version [? ].
Theorem 1 (Asynchronous Q-learning). For the asynchronous Q-learning algorithm detailed in
Algorithm 1, there exist some universal constants c0, c1 > 0 such that for any 0 < δ < 1 and 0 < ε ≤ 1 1−γ , one has
∀(s, a) ∈ S × A :
|QT (s, a) − Q(cid:63)(s, a)| ≤ ε with probability at least 1 − δ, provided the iteration number T and the learning rates ηt ≡ η obey (cid:27) (cid:26)
T ≥
η = c0
µmin 1 (1 − γ)5ε2 + c1 log (cid:0) |S||A|T tmix 1 − γ (cid:26) (1 − γ)4ε2
γ2 (cid:1) min log (cid:16) |S||A|T
δ (cid:27)
, 1 tmix
.
δ (cid:17) log (cid:16) 1 (1 − γ)2ε (cid:17)
, (9a) (9b)
Theorem 1 delivers a ﬁnite-sample/ﬁnite-time analysis of asynchronous Q-learning, given that a ﬁxed learning rate is adopted and chosen appropriately. The (cid:96)∞-based sample complexity required for
Algorithm 1 to attain ε accuracy is at most (cid:16) (cid:101)O 1
µmin(1 − γ)5ε2 + tmix
µmin(1 − γ) (cid:17)
. (10)
A few implications are in order.
Dependency on the minimum state-action occupancy probability µmin. Our sample complexity bound (10) scales linearly in 1/µmin, which is in general unimprovable. Consider, for instance, the ideal scenario where state-action occupancy is nearly uniform across all state-action pairs, in which case 1/µmin is on the order of |S||A|. In such a “near-uniform” case, the sample complexity scales linearly with |S||A|, and this dependency matches the known minimax lower bound [3] derived for the setting with independent samples. In comparison, [35, Theorem 7] depends at least quadratically on 1/µmin, which is at least |S||A| times larger than our result (10).
Dependency on the discount complexity 1 1 (1−γ)5ε2 , which coincides with both [9, 44] (for the synchronous setting) and [5, 35] (for the asynchronous setting) with either a rescaled linear learning rate or a constant learning rate. This turns out to be the sharpest scaling known to date for the classical form of Q-learning. 1−γ . The sample size bound (10) scales as
Dependency on the mixing time tmix. The second additive term of our sample complexity (10) depends linearly on the mixing time tmix and is (almost) independent of the target accuracy ε. The inﬂuence of this mixing term is a consequence of the expense taken for the Markovian trajectory to reach a steady state, which is a one-time cost that can be amortized over later iterations if the algorithm is run for reasonably long. Put another way, if the behavior chain mixes not too slowly with respect to ε (in the sense that tmix ≤ (1−γ)4ε2 ), then the algorithm behaves as if the samples were independently drawn from the stationary distribution of the trajectory. In comparison, the inﬂuences of tmix and (1−γ)5ε2 in [35] (cf. Table 1) are multiplicative regardless of the value of ε, thus resulting in a much higher sample complexity. 1 1
Schedule of learning rates. An interesting aspect of our analysis lies in the adoption of a time-invariant learning rate, under which the (cid:96)∞ error decays linearly — down to some error ﬂoor whose value is dictated by the learning rate. Therefore, a desired statistical accuracy can be achieved by 5
properly setting the learning rate based on the target accuracy level ε and then determining the sample complexity accordingly. In comparison, classical analyses typically adopted a (rescaled) linear or a polynomial learning rule [20, 35]. While the work [5] studied Q-learning with a constant learning rate, their bounds were conservative and fell short of revealing the optimal scaling. Further, we note that adopting time-invariant learning rates is not the only option that enables the advertised sample complexity; as we shall elucidate shortly, one can also adopt carefully designed diminishing learning rates to achieve the same performance guarantees.
Sample complexity based on the cover time.
In addition, our analysis framework immediately leads to another sample complexity guarantee stated in terms of the cover time tcover (cf. (5)), which facilitates comparisons with several past work [5, 20].
Theorem 2. For the asynchronous Q-learning algorithm detailed in Algorithm 1, there exist some universal constants c0, c1 > 0 such that for any 0 < δ < 1 and 0 < ε ≤ 1 1−γ , one has
∀(s, a) ∈ S × A :
|QT (s, a) − Q(cid:63)(s, a)| ≤ ε with probability at least 1 − δ, provided the iteration number T and the learning rates ηt ≡ η obey
T ≥
η = (cid:17) log (cid:16) c0tcover (1 − γ)5ε2 log2 (cid:16) |S||A|T (cid:1) min c1 log (cid:0) |S||A|T (cid:26) (1 − γ)4ε2
γ2
δ
δ (cid:17)
, 1 (1 − γ)2ε (cid:27)
, 1
. (11a) (11b) tcover (1−γ)5ε2
In a nutshell, this theorem tells us that the (cid:96)∞-based sample complexity of classical asynchronous Q-(cid:1), which scales linearly with the cover time. This improves learning is bounded above by (cid:101)O(cid:0) upon the prior result [20] (resp. [5]) by an order of at least t3.29 cover ≥ |S|3.29|A|3.29 (resp. t2 cover|S||A| ≥
|S|3|A|3). See Table 1 for detailed comparisons. We also make note of some connections between tcover and tmix/µmin to help compare Theorems 1-2: (1) in general, tcover = (cid:101)O(tmix/µmin) for uniformly ergodic chains; (2) one can ﬁnd some cases where tmix/µmin = (cid:101)O(tcover). See [? ,
Appendix B] for more discussions.
Adaptive and data-driven learning rates. The careful reader might remark that the learning rates recommended in (9b) depend on the mixing time tmix — a parameter that might be either a priori unknown or difﬁcult to estimate. Fortunately, it is feasible to adopt a more adaptive learning rate schedule which does not rely on prior knowledge of tmix and which is still capable of achieving the performance advertised in Theorem 1.
In order to describe our new learning rate schedule, we need to keep track of the following quantities for all (s, a) ∈ S × A:
• Kt(s, a): the number of times that the sample trajectory visits (s, a) during the ﬁrst t iterations.
In addition, we maintain an estimate (cid:98)µmin,t of µmin, computed recursively as follows (cid:98)µmin,t =


 1
|S||A| , (cid:98)µmin,t−1, mins,a Kt(s, a)/t, mins,a Kt(s, a) = 0; 2 < mins,a Kt(s,a)/t 1 otherwise. (cid:98)µmin,t−1
< 2;
With the above quantities in place, we propose the following learning rate schedule:
ηt = min (cid:110) 1, cη exp (cid:16)(cid:106) log log t (cid:98)µmin,t(1 − γ)γ2t (cid:107)(cid:17)(cid:111)
, (12) (13) where cη > 0 is some proper constant, and (cid:98)x(cid:99) denotes the nearest integer less than or equal to x.
If (cid:98)µmin,t forms a reliable estimate of µmin, then one can view (13) as a sort of “piecewise constant approximation” of the rescaled linear stepsizes
µmin(1−γ)γ2t . Clearly, such learning rates are fully data-driven and do no rely on any prior knowledge about the Markov chain (like tmix and µmin) or the target accuracy ε. cη log t 6
Algorithm 2: Asynchronous variance-reduced Q-learning 1 input parameters: number of epochs M , epoch length tepoch, recentering length N , learning rate η. 0 ← 0. 2 initialization: set Qepoch 3 for each epoch m = 1, · · · , M do
/* Call Algorithm 3.
Qepoch m
= VR-Q-RUN-EPOCH( Qepoch 4 m−1 , N, tepoch) .
*/
Encouragingly, our theoretical framework can be extended without difﬁculty to accommodate this adaptive learning rate choice. Speciﬁcally, for the Q-function estimates (cid:98)Qt = (cid:26) Qt, (cid:98)Qt−1, if ηt+1 (cid:54)= ηt, otherwise, (14) we have the following theoretical guarantees.
Theorem 3. Consider asynchronous Q-learning with learning rates (13). There is some sufﬁciently large universal constant C > 0 such that: for any 0 < δ < 1 and 0 < ε ≤ 1 1−γ , one has
∀(s, a) ∈ S × A : (cid:12) (cid:98)QT (s, a) − Q(cid:63)(s, a)(cid:12) (cid:12) (cid:12) ≤ ε with probability at least 1 − δ, provided that
T ≥ C max (cid:110) 1
µmin(1 − γ)5ε2 , tmix
µmin(1 − γ) (cid:111) log (cid:17) (cid:16) |S||A|T
δ log (cid:16)
T (1 − γ)2ε (cid:17)
. (15) (16) 4 Extension: asynchronous variance-reduced Q-learning
As pointed out in prior literature, the classical form of Q-learning (6) often suffers from sub-optimal dependence on the discount complexity 1 1−γ . For instance, in the synchronous setting, the minimax (1−γ)3 (see, [3]), while the sharpest known upper bound for vanilla lower bound is proportional to
Q-learning scales as (1−γ)5 ; see detailed discussions in [44]. To remedy this issue, recent work proposed to leverage the idea of variance reduction to develop accelerated RL algorithms in the synchronous setting [37, 45], as inspired by the seminal SVRG algorithm [26]. These prior results, however, focused on the synchronous setting with independent samples. In this section, we adapt this idea to asynchronous Q-learning and characterize its sample efﬁciency. 1 1
Algorithm.
In order to accelerate the convergence, it is instrumental to reduce the variability of the empirical Bellman operator Tt employed in the update rule (6) of classical Q-learning. This can be achieved via the following means. Simply put, assuming we have access to (i) a reference Q-function estimate, denoted by Q, and (ii) an estimate of T (Q), denoted by (cid:101)T (Q), the variance-reduced
Q-learning update rule is given by
Qt(st−1, at−1) = (1 − ηt)Qt−1(st−1, at−1) + ηt (cid:16)
Tt(Qt−1) − Tt(Q) + (cid:101)T (Q) (cid:17) (st−1, at−1), (17)
Qt(s, a) = Qt−1(s, a),
∀(s, a) (cid:54)= (st−1, at−1), where Tt denotes the empirical Bellman operator at time t (cf. (7)). The empirical estimate (cid:101)T (Q) can be computed using a set of samples; more speciﬁcally, by drawing N consecutive sample transitions
{(si, ai, si+1)}0≤i<N from the observed trajectory, we compute (cid:101)T (Q)(s, a) = r(s, a) +
γ (cid:80)N −1 i=0 1{(si, ai) = (s, a)} maxa(cid:48) Q(si+1, a(cid:48)) (cid:80)N −1 1{(si, ai) = (s, a)} i=0
. (18)
Compared with the classical form (6), the original update term Tt(Qt−1) has been replaced by
Tt(Qt−1) − Tt(Q) + (cid:101)T (Q), in the hope of achieving reduced variance as long as Q (which serves as a proxy to Q(cid:63)) is chosen properly. 7
Algorithm 3: function Q = VR-Q-RUN-EPOCH( Q, N, tepoch) 1 Draw N new consecutive samples from the sample trajectory; compute (cid:101)T (Q) with (18). 2 Set s0 ← current state, and Q0 ← Q. 3 for t = 1, 2, · · · , tepoch do 4
Draw action at−1 ∼ πb(st−1) and next state st ∼ P (·|st−1, at−1).
Update Qt according to (17). 5 6 return: Q ← Qtepoch .
For convenience of presentation, we introduce the following notation
Q = VR-Q-RUN-EPOCH( Q, N, tepoch ) (19) to represent the above-mentioned update rule, which starts with a reference point Q and operates upon a total number of N + tepoch consecutive sample transitions. The ﬁrst N samples are employed to construct (cid:101)T (Q) via (18), with the remaining samples employed in tepoch iterative updates (17); see
Algorithm 3. To achieve the desired acceleration, the proxy Q needs to be periodically updated so as to better approximate the truth Q(cid:63) and hence reduce the bias. It is thus natural to run the algorithm in a multi-epoch manner. Speciﬁcally, we divide the samples into contiguous subsets called epochs, each containing tepoch iterations and using N + tepoch samples. We then proceed as follows
Qepoch m = VR-Q-RUN-EPOCH( Qepoch m−1 , N, tepoch ), m = 1, . . . , M, (20) where M is the total number of epochs, and Qepoch denotes the output of the m-th epoch. The whole procedure is summarized in Algorithm 2. Clearly, the total number of samples used in this algorithm is given by M (N + tepoch). We remark that the idea of performing variance reduction in RL is certainly not new, and has been explored in a number of recent work [16, 29, 37, 38, 45, 50]. m
Theoretical guarantees for variance-reduced Q-learning. We develop a non-asymptotic sample complexity bound for asynchronous variance-reduced Q-learning on a single trajectory. Before presenting our theoretical guarantees, there are several algorithmic parameters that we shall specify; for given target levels (ε, δ), choose min (cid:1) (cid:26) (1 − γ)2
γ2
, (cid:27)
, 1 tmix
ηt ≡ η =
N ≥ tepoch ≥
δ c0 log (cid:0) |S||A|tepoch (cid:16) c1
µmin c2
µmin (cid:16) 1 (1 − γ)3 min{1, ε2} (cid:17) 1 (1 − γ)3 + tmix 1 − γ
+ tmix (cid:17) log (cid:16) |S||A|tepoch
δ (cid:17)
, log (cid:16) (cid:17) 1 (1 − γ)2ε log (cid:16) |S||A|tepoch
δ (cid:17)
, (21a) (21b) (21c) where c0 > 0 is some sufﬁciently small constant, c1, c2 > 0 are some sufﬁciently large constants, and we recall the deﬁnitions of µmin and tmix in (3) and (4), respectively. Note that the learning rate (cid:1) (21a) chosen here could be larger than the choice (9b) for the classical form by a factor of O(cid:0) (which happens if tmix is not too large), allowing the algorithm to progress more aggressively.
Theorem 4 (Asynchronous variance-reduced Q-learning). Let Qepoch
M be the output of Algorithm 2 with parameters chosen according to (21). There exists some constant c3 > 0 such that for any 0 < δ < 1 and 0 < ε ≤ 1 1 (1−γ)2 1−γ , one has
∀(s, a) ∈ S × A :
|Qepoch
M (s, a) − Q(cid:63)(s, a)| ≤ ε with probability at least 1 − δ, provided that the total number of epochs exceeds
M ≥ c3 log 1
ε(1 − γ)2 . (22)
In view of Theorem 4, the (cid:96)∞-based sample complexity for variance-reduced Q-learning to yield ε accuracy — which is characterized by M (N + tepoch) — can be as low as (cid:16) (cid:101)O 1
µmin(1 − γ)3 min{1, ε2}
+ tmix
µmin(1 − γ) (cid:17)
. (23) 8
Except for the second term that depends on the mixing time, the ﬁrst term matches [45] derived for
}], the the synchronous settings with independent samples. In the range ε ∈ (0, min{1, sample complexity reduce to (cid:101)O(cid:0) derived in [3] for the synchronous setting. (1−γ)3 matches the minimax lower bound (cid:1); the scaling 1
µmin(1−γ)3ε2 1 (1−γ) tmix
√ 1 5