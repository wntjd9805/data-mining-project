Abstract
Normalization methods such as batch [Ioffe and Szegedy, 2015], weight [Salimans and Kingma, 2016], instance [Ulyanov et al., 2016], and layer normalization [Ba et al., 2016] have been widely used in modern machine learning. Here, we study the weight normalization (WN) method [Salimans and Kingma, 2016] and a variant called reparametrized projected gradient descent (rPGD) for overparametrized least squares regression. WN and rPGD reparametrize the weights with a scale g and a unit vector w and thus the objective function becomes non-convex. We show that this non-convex formulation has beneﬁcial regularization effects compared to gradient descent on the original objective. These methods adaptively regularize the weights and converge close to the minimum (cid:96)2 norm solution, even for initial-izations far from zero. For certain stepsizes of g and w, we show that they can converge close to the minimum norm solution. This is different from the behavior of gradient descent, which converges to the minimum norm solution only when started at a point in the range space of the feature matrix, and is thus more sensitive to initialization. 1

Introduction
Modern machine learning models often have more parameters than data points, allowing a ﬁne-grained adaptation to the data, but also suffering from the risk of over-ﬁtting. To alleviate this, various explicit and implicit regularization methods are used. For instance, weight decay can control the model complexity by shrinking the norm of the weights, and dropout can reduce the model capacity by sub-sampling features during training [Gal and Ghahramani, 2016, Mianjy et al., 2018, Arora et al., 2020]. Recent state-of-the-art techniques such as batch, weight, and layer normalization [Ioffe and Szegedy, 2015, Salimans and Kingma, 2016, Ba et al., 2016], empirically have a regularization effect, e.g., as described in Ioffe and Szegedy [2015], "batch normalisation acts as a regularizer, in some cases eliminating the need for dropout".
While normalization methods are practically popular and successful, their theoretical understanding has only started to emerge recently. For instance, normalization methods make learning more robust to hyperparameters such as the learning rate [Wu et al., 2018, Arora et al., 2019]. Moreover, it has
∗Equal Contribution, xwu@math.utexas.edu, dobriban@wharton.upenn.edu, tongzheng@utexas.edu, shan-shanw@google.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Comparison of the outputs ||(cid:98)x|| = (cid:107)(cid:98)g (cid:98)w(cid:107) pro-vided by GD, WN and rPGD on an overparametrized lin-ear regression problem (see Section 1.1). All algorithms (with stepsizes 0.005) start from the same initialization and stop when the loss reaches 10−5. Note that the or-ange (rPGD) and green (WN) curves overlap (see Lemma 2.2 for explanation and Section F for experimental de-tails). GD converges to the minimum (cid:96)2-norm solution only when (cid:107)x0(cid:107) = 0, while WN and rPGD converge close to the minimum norm solution for a wider range of initializations with smaller standard deviation. been argued that normalization methods can make the model robust to the shift and scaling of the inputs, preventing “internal covariate shift" [Ioffe and Szegedy, 2015] as well as smooth or modify
[Santurkar et al., 2018, Lian and Liu, 2019] the optimization landscape.
Yet, a precise characterization of the regularization effect of normalization methods in over-parametrized models is not available. For overparametrized models, there are typically inﬁnitely many global minima, as shown e.g., in matrix completion [Ge et al., 2016] and neural networks [Ge et al., 2017]. Thus, we can analyze how different algorithms converge to different global minima as a way of quantifying implicit bias. It is critical for the algorithm to converge to a solution with good generalization properties, e.g., Zhang et al. [2016], Neyshabur et al. [2019], etc. For the key model of over-parameterized linear least squares, it is well-known that gradient descent (GD) converges to the minimum Euclidean norm solution when started from zero, [see e.g. Hastie et al., 2019]. It has been argued that this may have favorable generalization properties in learning theory (norms can control the Radamechar complexity), as well as more recent analyses [Bartlett et al., 2019, Hastie et al., 2019, Belkin et al., 2019, Liang and Rakhlin, 2018].
However, for non-convex optimization, starting from the origin might be problematic – this is true in particular in neural networks with ReLU activation function which is often used [LeCun et al., 2015].
In neural networks, we often instead apply random initialization [Glorot and Bengio, 2010, He et al., 2015] which can for instance help escape saddle points [Lee et al., 2016]. Thus, it is important to study algorithms with initializations not close to zero.
With this in mind, we study how a particular normalization method, weight normalization (WN)
[Salimans and Kingma, 2016], affects the choice of global minimum in overparametrized least squares regression. WN writes the model parameters x as x = gw/(cid:107)w(cid:107)2, and optimizes over the "length" g ∈ R and the unnormalized direction w ∈ Rd separately. Inspired by weight normalization, we also study a related method where we parametrize the weight as x = gw, with g ∈ R and a normalized direction w with (cid:107)w(cid:107)2 = 1, [see e.g. Douglas et al., 2000]. Different from WN, this method performs projected GD on the unit norm vector v, while WN does GD on w such that w/(cid:107)w(cid:107) is the unit vector.
We call this variant the reparametrized projected gradient descent (rPGD) algorithm. We show that the two algorithms (rPGD and WN) have the same limit when the stepsize tends to zero. Arguing in both discrete and continuous time, we show that both ﬁnd global minima robust to initialization.
Our Contributions. We consider the overparametrized least squares (LS) optimization problem, which is convex but has inﬁnitely many global minima. As a simpliﬁed companion of WN in LS, we introduce the rPGD algorithm (Alg. 2), which is projected gradient descent on a nonconvex reparametrization of LS. We show that WN and rPGD have the same limiting ﬂow—the WN ﬂow—in continuous time (Lemma 2.2). We characterize the stationary points of the loss, showing that the nonconvex reparametrization introduces some additional stationary points that are in general not global minima. However, we also show that the loss still decreases at a geometric rate, if we can control the scale parameter g.
How to control the scale parameter? Perhaps surprisingly, we show the delicate property that the scale and the orthogonal complement of the weight vector are linked through an invariant (Lemma 2.5).
This allows us to show that the WN ﬂow converges at a geometric rate in spite of the non-convexity 2
of the reparameterized objective. We precisely characterize the solution, and when it is close to the min norm solution.
In discrete-time, when the stepsize is not inﬁnitely small, we ﬁrst consider a simple setting where the feature matrix is orthogonal and characterize the behavior of rPGD (Theorem 3.2). We show that by appropriately lowering the learning rate for the scale g, rPGD converges to the minimum (cid:96)2 norm solution. We give sharp iteration complexities and upper bounds for the stepsize required for g. We extend the result to general data matrices A (Theorem 3.3), where the results become more challenging to prove and a bit harder to parse. This sheds light the empirical observation that only optimizing the direction w training the last layer of neural nets improves generalization [Goyal et al., 2017, Xu et al., 2019]. 1.1 Setup
We use (cid:107) · (cid:107) for the (cid:96)2 norm, and consider the standard overparametrized linear regression problem: min x∈Rd 1 2 (cid:107)Ax − y(cid:107)2, (1) where A ∈ Rm×d (m < d) is the feature matrix and y ∈ Rm is the target vector. Without loss of generality, we assume that the feature matrix A has full rank m. This objective has inﬁnitely many global minimizers, and among them let the minimum (cid:96)2-norm solution be x∗. Observe that x∗ is characterized by the two properties: (1) Ax∗ = y; (2) x∗ is in the row space of the matrix A. We can describe condition (2) via Deﬁnition 1.1.
Deﬁnition 1.1. For any z ∈ Rd, we can write z = z(cid:107) + z⊥ where Az(cid:107) = Az and Az⊥ = 0.
Then we can equivalently write condition (2) as x∗(cid:107) = x∗. We focus on weight normalization and a related reparametrized projected gradient descent method. Notably, both transform the original convex LS problem to a non-convex problem, which increases the difﬁculty of theoretical analysis.
Weight normalization (WN) WN reparametrizes the variable x as g · w/(cid:107)w(cid:107), where g ∈ R and w ∈ Rd, which leads to the following minimization problem: min g∈R,w∈Rd h(w, g) = 1 2 (cid:107)gAw/(cid:107)w(cid:107) − y(cid:107)2 . (2)
We can write the min norm solution as x∗ = g∗w∗/(cid:107)w∗(cid:107), where w∗ is unique up to scale. However, we can always choose w∗ so that g∗ > 0, unless x∗ = 0, which implies that y = 0. We exclude this degenerate case throughout the paper. The discrete time WN algorithm is shown in Algorithm 1.
Algorithm 1 WN for (2)
Algorithm 2 rPGD for (3)
Input: Unit norm w0 and scalar g0,iterations
T , step-sizes {γt}T −1 for t = 0, 1, 2, · · · , T − 1 do t=0 and {ηt}T −1 t=0 wt+1 = wt − ηt∇wh(wt, gt) gt+1 = gt − γt∇gh(wt, gt) end for
Input: Unit norm w0 and g0, number of iter-ations T , step-sizes {γt}T −1 for t = 0, 1, 2, · · · , T − 1 do t=0 and {ηt}T −1 t=0 vt = wt − ηt∇wf (wt, gt) (gradient step) wt+1 = vt (projection) (cid:107)vt(cid:107) gt+1 = gt − γt∇gf (wt, gt) (gradient step) end for
Reparametrized Projected Gradient Descent (rPGD)
Inspired by WN algorithm, we investigate an algorithm that directly updates the direction of w. See Douglas et al. [2000] for an example of such algorithms. Since the direction is a unit vector, we can perform projected gradient descent on it.
To be more concrete, we reparametrize the variable x as gw, where g denotes the scale and w ∈ Rd with (cid:107)w(cid:107) = 1 denotes the direction, and transform (2) into the following problem: min g∈R,w∈Rd f (w, g) := 1 2 (cid:107)Agw − y(cid:107)2, s.t. (cid:107)w(cid:107) = 1. (3)
The minimum norm solution can be uniquely written as x∗ = g∗w∗, where g∗ > 0 and (cid:107)w∗(cid:107) = 1.
To solve (3), we update g with standard gradient descent, and update w via projected gradient descent (PGD) (see Algorithm 2). We call this algorithm reparameterized PGD (rPGD). 3
One may observe that both algorithms can heuristically be viewed as a variation of adaptive (cid:96)2 regularization, where the magnitude of the regularization depends on the current iteration. We refer the readers to Appendix A for a detailed discussion. 2 Continuous Time Analysis
In this section, we study the properties of a continuous limit of WN and rPGD, to give insight into the implicit regularization of normalization methods. We use constant stepsizes for both the update of the scale g and weight w, and take them to zero in a way that their ratio remains a constant.
Condition 2.1 (Stepsizes). For both Algorithms 1 and 2, use constant stepsizes ηt = η and γt = cη for g and w respectively, with c ≥ 0 a ﬁxed constant ratio. We take the continuous limit η → 0.
Setting c = 0 amounts to ﬁxing g and only updating w. We ﬁrst prove that the continuous limit of the dynamics of (gt, wt/(cid:107)wt(cid:107)) for WN evolves the same as the continuous limit of the dynamics of (gt, wt) for rPGD, assuming we start with (cid:107)w0(cid:107) = 1 for WN. The proof can be found in Appendix B.
Lemma 2.2 (Limiting ﬂow for WN and rPGD). Assume Condition 2.1 and that (cid:107)w0(cid:107) = 1 for WN.
Then WN (Algorithm 1) with (gt, wt/(cid:107)wt(cid:107)) and rPGD (Algorithm 2) with (gt, wt) have the same limiting dynamics, which we call WN ﬂow. This is given by the pair of ordinary differential equations dwt dt
Here f is from (3). With r = y − Agw to denote the residual, ∇wf = AT r, ∇gf = wT AT r, and
Pt = I − wtw(cid:62) t /(cid:107)wt(cid:107)2 the projection matrix onto the space orthogonal to wt.
= −gtPt (∇wf (wt, gt)) .
= −c∇gf (wt, gt) dgt dt (4)
While the ﬂow is valuable, the nonconvex reparametrization introduces some new stationary points.
We characterize them, and later use this to understand the convergence.
Lemma 2.3 (Stationary points). Suppose the smallest eigenvalue of AAT , is positive, λmin :=
λmin(AAT ) > 0. The stationary points of the reparameterized loss from (2) either (a) have loss equal to zero, or (b) belong to the set S := {(g, w) : g = 0, yT Aw = 0}. If the loss (2) at g, w is strictly less than the loss at (g = 0, w), i.e. (cid:107)y(cid:107)2 > (cid:107)Agw − y(cid:107)2, we are always in case (a).
It is a folklore result that under gradient ﬂow, the loss is non-increasing even in the nonconvex case
[see e.g. Rockafellar and Wets, 2009]. For the WN gradient ﬂow, we can make this folklore rigorous and, provided the scale parameter gt is lower bounded, show that the loss decreases at a geometric rate.
Lemma 2.4 (Rate of (cid:107)rt(cid:107)). Under the setting of Lemma 2.2, we have the bounds:
− max{g2 (5)
This shows that (cid:107)rt(cid:107) is non-increasing. If for some C > 0, gt > C for all t, then the loss decreases geometrically at rate min(C 2, c). t , c}(cid:107)AT rt(cid:107)2 ≤ d[1/2(cid:107)rt(cid:107)2]/dt ≤ − min{g2 t , c}(cid:107)AT rt(cid:107)2 ≤ 0.
How can we control the scale parameter? Perhaps surprisingly, we show that the scale parameter and the orthogonal complement of the weight vector are linked through an invariant.
Lemma 2.5 (Invariant). Assume c > 0 in Condition 2.1. Under the setting from Lemma 2.2, let wt = w⊥ t as deﬁned in Deﬁnition 1.1. We have at time t > 0, w⊥ 0 and so (cid:107)w⊥ t (cid:107)2 · exp(g2 t /2c) = (cid:107)w⊥ 0 (cid:107)2 · exp(g2 0/2c). (6) t + w(cid:107) w⊥ t = exp (cid:19) (cid:18) g2 0 − g2 t 2c t (cid:107)2 · exp(g2
Lemma 2.5 shows that the orthogonal complement w⊥ t can change during the WN ﬂow dynamics.
This is the key property of WN that can yield additional regularization. Lemma 2.5 also implies that (cid:107)w⊥ t /2c) is invariant along the path. If we initialize with small |g0| and |gt| is greater than |g0| (we will describe the dynamics of gt in the next part), then (cid:107)w⊥ t (cid:107)2 will decrease, and we get close to the minimum norm solution. This is in contrast to gradient descent and ﬂow, where (cid:107)w⊥(cid:107)2 is preserved (see e.g., [Hastie et al., 2019]).
The invariant (6) in the optimization path holds for certain more general settings. Speciﬁcally, it holds for linearly parametrized loss functions that only depend on a small dimensional linear subspace of the parameter space (e.g., overparametrized logistic regression). See Appendix D. Equipped with the above lemmas, we can discuss the solution and implicit regularization effect of the WN ﬂow. 4
Theorem 2.6 (WN ﬂow Solution). Assume Condition 2.1 and λmin > 0. Suppose we initialize the
WN ﬂow at g0, w0, such that (cid:107)w0(cid:107) = 1. We have that either (a) the loss converges to zero, or (b) the iterates (gt, wt) converge to a stationary point in S as deﬁned in Lemma 2.3. In case (a), we characterize the solutions based on gt:
Part I. If c > 0, and the loss converges to zero, the solution can be expressed as lim t→∞ gtwt = x∗ + g∗w⊥ 0 exp (cid:18) g2 0 − g∗2 2c (cid:19)
. (7)
A sufﬁcient condition for the loss converging to zero is that (cid:107)y(cid:107)2 > (cid:107)Ag0w0 − y(cid:107)2.
Part II. If c = 0 and A is orthogonal, i.e., AAT = I, then wt → w∗. If A is not orthogonal, then 0 = 0). When restarting the ﬂow still converges to a point ˜w0 in the row space of A (i.e, ˜w⊥ the WN ﬂow with c > 0 from g0, ˜w0, then (g0, ˜w0) → (g∗, w∗).
We defer the proofs of Lemmas 2.4, 2.5 and Theorem 2.6 to Appendix C. Part I of Theorem 2.6 shows that, if we initialize with g2 0 ≤ g∗2 and we are not stuck at S, the WN ﬂow will converge to a solution that is close to the minimum norm solution. Compared with GD where the ﬁnal solution is xt = x∗ + g∗w⊥ 0 , WN ﬂow has smaller component in the orthocomplement of the row space of A. 0 > g∗2, then WN ﬂow can converge to a solution that is farther from x∗ than GD.
In contrast, if g2
Part II in Theorem 2.6 shows a distinction between orthogonal and general A. For orthogonal A, even ﬁxing the scale g0 we can converge to the direction of the minimum norm solution. Although we do not directly recover g∗ in the ﬂow, this can be recovered as |g∗| = (cid:107)y(cid:107). For general A with
ﬁxed g, we do not necessarily converge to the right direction, only to the row span of A. However, if we run the ﬂow with c = 0 until convergence, and then turn on the ﬂow for g (i.e. set c > 0), we converge to the minimum norm solution. The results for discrete time presented later mirror this. See
Figure 2 for an illustration. We mention that the ﬂow for the ﬁxed g case is well known [See e.g.
Helmke and Moore, 2012, Section 1.6]), in the special case that the matrix A is square.
Theorem 2.6 provides no rate of convergence. By our results on the rate of (cid:107)rt(cid:107), and by controlling gt using the invariant, we can provide a convergence rate below. The following theorem has two convergence rates, depending on the magnitude of g0 and on whether we initialize g0 and w0 with the initial loss smaller than the loss at zero or not. If both rates are valid for a certain parameter conﬁguration, then the faster of the two applies.
Theorem 2.7 (Convergence Rate). Assume Condition 2.1, c > 0 with (cid:107)w0(cid:107) = 1 and the smallest eigenvalue λmin of AAT is strictly positive.
• If g2 0 > 2c log(1/(cid:107)w⊥ 0 (cid:107)), the loss along the WN ﬂow path (gt, wt) decreases geometrically, satisfying f (wT , gT ) ≤ ε after time
T = log(f (w0, g0)/ε)
λmin min (cid:8)2c log (cid:107)w⊥ 0 (cid:107) + g2 0, c(cid:9) .
• If the initial loss smaller than the loss at zero, δ = ((cid:107)y(cid:107)2 − (cid:107)Ag0w0 − y(cid:107)2)/λmax > 0, then f (wT , gT ) ≤ ε, after time
T = log(f (w0, g0)/ε)
λmin min {δ, c}
+ 1
λmax (cid:16) 2 − log (cid:17) g0
δ 1{g0<δ}.
The two convergence rates apply to somewhat complementary cases. In the ﬁrst case, it follows from the invariant that as long as g0 is above the required threshold 2c log(1/(cid:107)w⊥ 0 (cid:107)), the loss converges geometrically. Otherwise, if we have δ > 0 (that is, we initialize below the loss at zero), the dynamics of g2 t turn out to have a favorable "self-balancing" geometric property, i.e., they start to increase when they get sufﬁciently small (c.f. equation (14) in Lemma C.1), and we can also get the geometric convergence, instead of being stuck at S. The theorem only shows convergence, not implicit regularization. As described above, the regularization is favorable if |g0| < |g∗|.
A Concrete Example. To gain more insight, we provide here a simple example (see also Figure 2).
Suppose we have a two-dimensional parameter w, and we make a 1-dimensional observation using the matrix A = [1, 0], and y = 1. Then, the equation we are solving is gw[1] = 1 (where square 5
Figure 2: Consider the function f (w1, w2, g) with A =
[1, 0] ∈ R1×2. Suppose we start with g0 < g∗ (point a).
Then GD converges to d, while rPGD and WN could result in a point (e or c) closer to minimum norm c depending on the stepsize schedule of g. Part I in Theorem 2.6 suggests that rPGD and WN will follow the path a → e if γt and ηt converge to zero at the same rates, and Part II implies the red path a → b → c to the minimum norm solution (if g0 is ﬁxed for a certain time, and updated later). The optimal path a → c is taken when g is updated in a careful way.
On the other hand, starting with g0 > g∗, for instance at f , (7) shows the limit is g, further away from g∗w∗. 1The ﬁgure is for f (w1, w2, g) = (gw1 − 2)2, with mini-mum norm solution at c = (2, 0). brackets index coordinates of vectors), and the minimum norm solution is w = [1, 0]T , with g∗ = 1.
Our results guarantee that the WN ﬂow converges to either (1) a zero of the loss, or (2) to a stationary point such that g = 0 and wT AT y = 0. The second condition reduces to w[1] · y = 0. Now, if y (cid:54)= 0 (which is the typical case), then this reduces to w[1] = 0, and since (cid:107)w(cid:107) = 1, we have two solutions w[2] = ±1. So this leads to two spurious stationary points (g, w) = (0, [0, ±1]T ), which are not global minima. The loss value at these points is 1, and so if we start at any point such that the loss is less than one, then we converge to a global mininum. If y = 0, then this leads to inﬁnitely many stationary points, i.e. all of those with g = 0, but these turn out to be global minima.
Suppose moreover that we start with w0 = [0, 1]T and set c = 1. Suppose now that we start with some g0 (cid:54)= 0. Then WN ﬂow converges to a solution gw = [1, exp([g2 0 − 1]/2)](cid:62). If g0 is relatively small, this quantity is close to x∗ = [1, 0](cid:62), closer than the gradient ﬂow solution [1, 1]. 3 Discrete Time Analysis
In this section, we switch to discrete time. It turns out that analyzing rPGD is more tractable than
WN, so we will focus on rPGD. Since the two algorithms collapse to the same ﬂow in continuous time, their dynamics should be “close" in ﬁnite time, especially in the small stepsize regime. We show that rPGD with properly chosen learning rates converges close to the min norm solution even when the initialization is far away from the origin. We study rPGD based on the intuition that (cid:107)w⊥ t (cid:107) decreases after the normalization step.
Orthogonal Data Matrix. Consider ﬁrst the simple case where the feature matrix A has orthonor-mal rows, i.e., AA(cid:62) = I. Our strategy for rPGD to reach the minimum (cid:96)2-norm solution is to use the optimal stepsize for w and a small stepsize for g such that g2 t < g∗2 for all iterations. The key intuition is that with a small stepsize, the loss stays positive and ensures the direction wt has sufﬁcient time to ﬁnd w∗. On the contrary, if we use a large stepsize for g, then it is possible for gt to be greater than g∗ so that wt can potentially converge to the wrong direction.
Condition 3.1. (Two-stage learning rates) We update w with its optimal step-size ηt = 1/g2 t .2 For the stepsize of gt, we use two constant values: (a) γt = γ(1) when 0 ≤ t ≤ T1; (b) γt = γ(2) when t ≥ T1 + 1, for a T1 speciﬁed below.
Theorem 3.2 (Convergence for Orthogonal Matrix A). Suppose the initialization satisﬁes 0 < g0 < g∗, and that w0 is a vector with (cid:107)w0(cid:107) = 1. Let δ0 = (g∗)2 − (g0)2. Set an error parameter ε > 0 and the stepsize given in Condition 3.1 with a hyper-parameter ρ ∈ (0, 1] for γ(1). Running the (cid:107) ≤ ε and g2 rPGD algorithm, we can reach (cid:107)w⊥
T (cid:107) ≤ ε
T1
T1 and (cid:107)AgT wT − b(cid:107)2 ≤ 3εg∗2 after T = T1 + T2 iterations, if we set stepsizes as follows.
≤ g∗2 − ρδ0 after T1 iterations, and (cid:107)w⊥ 2The Hessian for w in problem 3 is ∇2 wf (w, g) = g2A(cid:62)A. For orthogonal A, λmax(∇2 wf (w, g)) = g2. 6
(a) Set γ(1) = O (cid:18)
ρ log(1/ε) (cid:17)2 (cid:16) g0 g∗ (cid:16) log (1 − ρ) g∗ g0
+ ρ (cid:17)(cid:19) and γ(2) ≤ 1 4 . Then we have
T1 = O (cid:18) (g∗)2
ρδ0 log (cid:18) 1
ε (cid:19)(cid:19)
; T2 = O (cid:18) 1
γ(2) log (cid:18) (ρδ0/g∗2)2
ε (cid:19)(cid:19)
. (b) Set γ(1) = 0 and γ(2) < 1 (cid:18) g2 0
δ0
T1 = O 4 . Then we have (cid:18) 1
ε (cid:19)(cid:19) log
; T2 = O (cid:32) 1
γ(2) log (cid:32) (cid:112)δ0/g∗2
ε (cid:33)(cid:33)
.
We restate the theorem with the explicit forms of T1 and T2, along with the proof, in Appendix E.1 and E.2. The theorem requires knowing g∗, which can be approximated by (cid:107)y(cid:107) (as g∗ = (cid:107)y(cid:107)). When all parameters other than ε are treated as constants, this shows that rPGD converges to the minimum norm solution with the same rate log(1/ε) as standard GD starting from the origin. However, the constants in front the log(1/ε) can be large: e.g., in case (a), (g∗)2/ρδ0 can be (cid:29) 1 if ρ is small or if |g0|/|g∗| ≈ 1. This ﬁrst T1 iterations allow wt to "ﬁnd" w∗, while the remaining T2 allow gt to converge to g∗. Both cases show an intrinsic tradeoff between T1 and T2: a larger δ0 (being far from g∗) leads to faster convergence in the ﬁrst phase for (cid:107)w⊥ t (cid:107) (i.e. smaller T1), but slower convergence in gt and loss (i.e. larger T2). Speciﬁcally, notice that δ0 is in the denominator of T1 but in the numerator of T2.
Our proof shows that gt is always increasing for any g0 > 0 (c.f. Lemma E.4). Moreover, (cid:107)w⊥ t (cid:107) t (cid:107)2, as long as |gt| is not too close to |g∗| decreases at a geometric rate, (cid:107)w⊥
≤ g∗2 − ρδ0 is needed, ensuring (c.f. Lemma E.6 or Equation (19)). This is why the condition g2
T1 that gt is far away from g∗ in all steps before T1. This is also why we require a stepsize for gt of order 1/ log(1/ε) (c.f. Equation 20), which is smaller than the constant stepsize in usual GD. Here
ρ leads to a tradeoff between T1 and T2: a smaller ρ results in larger T1 but smaller T2, vice versa.
When ρ ≈ ε, we have T1 = O(1/ε) up to log factors, (a slower rate) and T2 = O (1). 3 A constant order ρ leads to a faster log(1/ε) rate. However, we choose to state the result for the entire range of
ρ ∈ (0, 1] for completeness. When ρ = 1, the stepsize γ(1) becomes zero, hence gt does not change.
In this case, we can get a stronger result for T1 (stated in (b)) using a slightly different method of proof, improving the bounds of case (a) respectively with a factor of (g∗/g0)2 > 1 for T1.
We remark that for orthogonal A with the optimal stepsize (1/g2 t ) for wt and g0 (cid:54)= 0, we have
Awt+1 = Ag∗w∗/(gt(cid:107)vt(cid:107)) (cid:54)= 0 (c.f. Lemma E.2). Thus we can escape the saddle points and reach the global minimum, unlike in continuous time where we can be stuck at the stationary points S. t+1(cid:107)2 ≤ (g2 t /g∗2)(cid:107)w⊥
We reiterate that our motivation is not to outperform other methods (e.g. GD starts from zero) in search of the minimum norm solution, but to characterize the regularization effect of weight normalization and shed light on the empirical observation that ﬁxing the scalar g and only optimizing the directions w in training the last layer of neural networks can improve generalization [Goyal et al., 2017, Xu et al., 2019]. This is, to our knowledge, the ﬁrst kind of theory on how to control the learning rates of parameters in weight normalization such as to converge to minimum norm solutions for initialization not close to origin, which may have beneﬁcial generalization properties.
Inspired by the analysis for orthogonal A, we now study general data
General Data Matrix. matrices. As we have seen from the orthogonal A case, the stepsize for the scale parameter should be extremely small or even 0 to make (cid:107)w⊥ t (cid:107) small. Thus, for simplicity, we focus on ﬁxing g := g0 and update only w using rPGD so that the orthogonal component w⊥ decreases geometrically until (cid:107)w⊥ (cid:107) ≤ ε. In addition, we notice from the analysis in Theorem 3.2 that updating gt and wt
T1 separately after t > T1 (i.e., reaching small (cid:107)w⊥ t (cid:107)) shows no advantage over GD using x = gw.
Thus, the best strategy to ﬁnd g∗w∗ is to use rPGD only updating wt (so gt = g0 < g∗) and then apply standard GD after T1 once we have (cid:107)w⊥ (cid:107) ≤ ε. We focus on the complexity of T1 in the
T1 remainder, as the remaining steps are standard GD, which is well understood.
Even though we ﬁx g, the problem is still non-convex because the projection is on the sphere (rather than the ball), a non-convex surface. However, suppose we can ensure that after each update, the 3Note that the bound for T1 could be tightened, possibly to log(1/ε), by using reﬁned analysis at the step from (18) to (19). 7
gradient step vt = wt − ηt∇wf (wt, gt) has norm (cid:107)vt(cid:107) ≥ 1. Then the following two constrained non-convex problems are equivalent: min w∈Rd (cid:107)Ag0w − y(cid:107)2 s.t. w ∈ {w, (cid:107)w(cid:107) = 1} ⇔ min w∈Rd (cid:107)Ag0w − y(cid:107)2 s.t. w ∈ {w, (cid:107)w(cid:107) ≤ 1}
Thus our analysis will focus on showing that (cid:107)vt(cid:107) ≥ 1. Note that, without loss of generality we can always scale A so that its largest singular value is one.
Proposition 3.3 (General Matrix A). Fix δ > 0, and ﬁx a full rank matrix A with λmax(AA(cid:62)) = 1.
With a ﬁxed g = g0 satisfying g0 ≤ [g∗λmin(AA(cid:62))]/(2+δ), we can reach a solution with (cid:107)w⊥ (cid:107) ≤ ε
T1 in a number of iterations
T1 = log
/ log(1 + δ). (cid:19) (cid:18) (cid:107)w⊥ 0 (cid:107)
ε
The proof is in Appendix E.3. The proposition implies that if we set a small g0 = O(g∗λmin(AA(cid:62))) for general A and w∗, running rPGD with ﬁxed g0 helps regularize the iterates. After starting from wT1, we can converge close to the minimum norm solution using standard GD. If the eigenvalues of
A are "not too spread out", we can get a better condition for g0 using concentation inequalities for eigenvalues. See inequality (50) in Proposition E.9 for more details. 4 Discussion
Limitation of our work. It is important to recognize the limitations of our work. First, our theoretical work only addresses weight normalization (not batch, layer, instance or other normalization methods), and only concerns the setting of linear least squares regression. While this may seem limiting, it is still signiﬁcant: even in this setting, the problem is not understood, and leads to intriguing insights. In fact there is some recent work on Neural Tangent Kernels arguing overparametrized
NNs can be equivalent to linear problems, see e.g., [Jacot et al., 2018, Du et al., 2018, Lee et al., 2019], etc. Second, the continuous limit is only an approximation; however it leads to elegant and interpretable results, which are moreover also reﬂected in simulations. Third, some of our results concern a two-stage algorithm where the scale is ﬁxed for the ﬁrst stage; nevertheless, our results on the standard “one-stage" algorithm in continuous time suggest such discrete-time results extend to the situation where the scale is not ﬁxed, but slowly-varying for the ﬁrst stage.