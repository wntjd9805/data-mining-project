Abstract
Probabilistic neural networks are typically modeled with independent weight priors, which do not capture weight correlations in the prior and do not provide a parsimo-nious interface to express properties in function space. A desirable class of priors would represent weights compactly, capture correlations between weights, facilitate calibrated reasoning about uncertainty, and allow inclusion of prior knowledge about the function space such as periodicity or dependence on contexts such as inputs. To this end, this paper introduces two innovations: (i) a Gaussian process-based hierarchical model for network weights based on unit priors that can ﬂexibly encode correlated weight structures, and (ii) input-dependent versions of these weight priors that can provide convenient ways to regularize the function space through the use of kernels deﬁned on contextual inputs. We show these models provide desirable test-time uncertainty estimates on out-of-distribution data and demonstrate cases of modeling inductive biases for neural networks with kernels which help both interpolation and extrapolation from training data. 1

Introduction
Bayesian neural networks (BNNs) [see e.g. MacKay, 1992, Neal, 1993, Ghahramani, 2016] are one of the research frontiers on combining Bayesian inference and deep learning, potentially offering
ﬂexible modelling power with calibrated predictive performance. In essence, applying probabilistic inference to neural networks allows all plausible network parameters, not just the most likely, to be used for predictions. Despite the strong interest in the community for the exploration of BNNs, there remain unanswered questions: (i) how can we model neural network functions to encourage behaviors such as interpolation between signals and extrapolation from data in meaningful ways, for instance by encoding prior knowledge, or how to specify priors which facilitate uncertainty quantiﬁcation, and (ii) many scalable approximate inference methods are not rich enough to capture complicated posterior correlations in large networks, resulting in undesirable predictive performance at test time.
This paper attempts to tackle some of the aforementioned limitations by taking inspiration from the
Gaussian Process (GP) literature. First, we propose a hierarchical GP prior over weights to achieve a compact non-parametric representation of neural networks weights by utilizing unit-level latent variables. Second, we propose to utilize this non-parametric GP prior on weights to overcome a key limitation of current weight priors: their global nature. We explore the use of product kernels to implement input-dependence as a variation of the proposed prior, yielding models that have local (per-datapoint or per-layer) priors. We explore what effects these per-datapoint priors have in comparison to global priors on the network’s ability to generalize. A structured variational inference approach is employed that side-steps the need to do inference in the weight space and amortizes per-datapoint weight inference. A consequence of our model setup and usage of approximate inference is that our model is parametrized compactly by inducing points akin to GPs, but in our case those are
∗Both authors contributed equally. Work done while TK was at Uber AI 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
inducing weights. The proposed priors and approximate inference scheme are demonstrated to exhibit beneﬁcial properties for tasks such as generalization and uncertainty quantiﬁcation.
The paper is organized as follows: in Section 2 we review graph-based hierarchical modeling for
BNNs. In Section 3 we introduce the global and local weight models and their applications to neural networks. Efﬁcient inference algorithms are presented in Section 4, followed by a suite of experiment to validate their performance in Section 5. We review related work in Section 6. 2