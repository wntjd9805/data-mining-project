Abstract
We introduce a framework for inference in general state-space hidden Markov models (HMMs) under likelihood misspeciﬁcation. In particular, we leverage the loss-theoretic perspective of Generalized Bayesian Inference (GBI) to deﬁne generalised ﬁltering recursions in HMMs, that can tackle the problem of inference under model misspeciﬁcation. In doing so, we arrive at principled procedures for robust inference against observation contamination by utilising the β-divergence.
Operationalising the proposed framework is made possible via sequential Monte
Carlo methods (SMC), where most standard particle methods, and their associated convergence results, are readily adapted to the new setting. We apply our approach to object tracking and Gaussian process regression problems, and observe improved performance over both standard ﬁltering algorithms and other robust ﬁlters. 1

Introduction
Estimating the hidden states in dynamical systems is a long-standing problem in many ﬁelds of sci-ence and engineering. This can be formulated as an inference problem of a general state-space hidden
Markov model (HMM) deﬁned via two processes, the hidden process (xt)t≥0, and the observation pro-cess (yt)t≥1. More precisely, we consider the general state-space hidden Markov models of the form x0 ∼ π0(x0), (1) xt|xt−1 ∼ ft(xt|xt−1), (2) yt|xt ∼ gt(yt|xt), (3) where xt ∈ X for t ≥ 0, yt ∈ Y for t ≥ 1, ft is a Markov kernel on X and gt : Y × X → R+ is the likelihood function. We assume X ⊆ Rdx and Y ⊆ Rdy for convenience; however, the extension to general Polish spaces follows directly. The key inference problem in this model class is estimating is the ﬁltering distributions, i.e. the posterior distributions of the hidden states (xt)t≥0 given the observations y1:t denoted as (πt(xt|y1:t))t≥1 — commonly known as Bayesian ﬁltering [1, 2].
Under assumptions of linearity and Gaussianity, the inference problem for the hidden states of HMMs can be solved analytically via the Kalman ﬁlter [3]. However, inference for general HMMs of the form (1)–(3) with nonlinear, non-Gaussian transitions and likelihoods lacked a general, principled solution until the arrival of the particle ﬁltering schemes [4]. Particle ﬁlters (PFs) have become ubiquitous for
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Bayesian ﬁltering in the general setting. In short, the PFs retain a weighted collection of Monte Carlo samples representing the ﬁltering distribution πt(xt|y1:t) and recursively approximate the sequence of distributions (πt)t≥0 using a particle mutation-selection scheme [5].
While PFs (and other inference schemes for HMMs) implicitly assume that the assumed model is well-speciﬁed, it is important to consider whether the proposed model class includes the true data-generating mechanism (DGM). In particular, for general state-space HMMs, misspeciﬁcation can occur if the true dynamics of the hidden process signiﬁcantly differ from the assumed model ft, or if the true observation model is markedly different from the assumed likelihood model gt, e.g. corruption by heavy tailed noise. The latter case is of widespread interest within the ﬁeld of robust statistics [6] and has recently attracted signiﬁcant interest in the machine learning community [7]. It is the setting that this paper seeks to address.
When the true DGM cannot be modelled, one principled approach to address misspeciﬁcation is
Generalized Bayesian Inference (GBI) [8]. This approach views classical Bayesian inference as a loss minimisation procedure in the space of probability measures, a view ﬁrst developed by [9].
In particular, the standard Bayesian update can be derived from this view, where a loss function is constructed using the Kullback-Leibler (KL) divergence from the empirical distribution of the observations to the assumed likelihood [8]. The KL divergence is sensitive to outliers [10], hence the overall inference procedure is not robust to observations that are incompatible with the assumed model. A principled remedy is to replace the KL divergence with alternative discrepancy, such as the
β-divergence, which makes the overall procedure more robust [11] while retaining interpretability.
Previous work on robust particle ﬁlters have been done for handling outliers, sensor failures and misspeciﬁcation of the transition model [12, 13, 14, 15, 16, 17, 18, 19]. However, these approaches are either based on problem-speciﬁc heuristic outlier detection schemes, or make strong assumptions about the DGM in order to justify the use of heavy-tailed distributions [15]. This requires knowledge of the contamination mechanism that is implicitly embedded in the likelihood. Thus, this work considers the challenging M-open settings: we do not assume access to a family of models which includes the true generative model. This is qualitatively different from classical parameter estimation approaches [20]; consequently, model selection schemes cannot generally be used to correct for mis-speciﬁcation (note the additional complications associated with parameter estimation in misspeciﬁed scenarios, see, e.g. [21]: not only does estimating parameters not address misspeciﬁcation, but even the interpretation of estimated parameters is difﬁcult). Furthermore, this case is not addressed by sequential Monte Carlo (SMC) algorithms under model uncertainty (see, e.g., [22]) where the true model is assumed to be available among many candidate models. For instance in [22], information from many candidate models is fused according to their predictive performance, which is a pragmatic solution with good empirical performance when a good suite of candidates is available. In contrast, we assume that we do not have any access at all to the true underlying generating mechanism.
In this work we propose a principled approach to robust ﬁltering that does not impose additional modelling assumptions. We adapt the GBI approach of [8] to the Bayesian ﬁltering setting and develop sequential Monte Carlo methods for inference. We illustrate the performance of this approach, using the β-divergence, to mitigate the effect of outliers. We show that this approach signiﬁcantly improves the PF performance in settings with contaminated data, while retaining a general and principled approach to inference. We provide empirical results that demonstrate improvement over Kalman and particle ﬁlters for both linear and non-linear HMMs. We further provide comparisons with various robust schemes against heavy-tailed noise, including t-based likelihoods [15] or auxiliary particle
ﬁlters (APFs) [12]. Finally, exploiting the state-space representations of Gaussian processes (GPs)
[23], we demonstrate our framework on London air pollution data using robust GP regression which has linear time-complexity in the number of observations.
Notation. We denote the space of bounded, Borel measurable functions on X as B(X). We denote the
Dirac measure located at y as δy(dx) and note that f (y) = (cid:82) f (x)δy(dx) for f ∈ B(X). We denote the Borel subsets of X as B(X) and the set of probability measures on (X, B(X)) as P(X). For a probability measure µ ∈ P(X) and ϕ ∈ B(X), we write µ(ϕ) := (cid:82) ϕ(x)µ(dx). Given a probability measure µ, we abuse the notation denoting its density with respect to the Lebesgue measure as µ(x). 2
2