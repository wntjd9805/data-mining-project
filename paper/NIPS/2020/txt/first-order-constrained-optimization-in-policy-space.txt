Abstract
In reinforcement learning, an agent attempts to learn high-performing behaviors through interacting with the environment, such behaviors are often quantiﬁed in the form of a reward function. However some aspects of behavior—such as ones which are deemed unsafe and to be avoided—are best captured through constraints.
We propose a novel approach called First Order Constrained Optimization in Policy
Space (FOCOPS) which maximizes an agent’s overall reward while ensuring the agent satisﬁes a set of cost constraints. Using data generated from the current policy,
FOCOPS ﬁrst ﬁnds the optimal update policy by solving a constrained optimization problem in the nonparameterized policy space. FOCOPS then projects the update policy back into the parametric policy space. Our approach has an approximate upper bound for worst-case constraint violation throughout training and is ﬁrst-order in nature therefore simple to implement. We provide empirical evidence that our simple approach achieves better performance on a set of constrained robotics locomotive tasks. 1

Introduction
In recent years, Deep Reinforcement Learning (DRL) saw major breakthroughs in several challenging high-dimensional tasks such as Atari games (Mnih et al., 2013, 2016; Van Hasselt et al., 2016;
Schaul et al., 2015; Wang et al., 2017), playing go (Silver et al., 2016, 2018), and robotics (Peters and Schaal, 2008; Schulman et al., 2015, 2017b; Wu et al., 2017; Haarnoja et al., 2018). However most modern DRL algorithms allow the agent to freely explore the environment to obtain desirable behavior, provided that it leads to performance improvement. No regard is given to whether the agent’s behavior may lead to negative or harmful consequences. Consider for instance the task of controlling a robot, certain maneuvers may damage the robot, or worse harm people around it.
RL safety (Amodei et al., 2016) is a pressing topic in modern reinforcement learning research and imperative to applying reinforcement learning to real-world settings.
Constrained Markov Decision Processes (CMDP) (Kallenberg, 1983; Ross, 1985; Beutler and Ross, 1985; Ross and Varadarajan, 1989; Altman, 1999) provide a principled mathematical framework for dealing with such problems as it allows us to naturally incorporate safety criteria in the form of constraints. In low-dimensional ﬁnite settings, an optimal policy for CMDPs with known dynamics can be found by linear programming (Kallenberg, 1983) or Lagrange relaxation (Ross, 1985; Beutler and Ross, 1985).
While we can solve problems with small state and action spaces via linear programming and value iteration, function approximation is required in order to generalize over large state spaces. Based on recent advances in local policy search methods (Kakade and Langford, 2002; Peters and Schaal, 2008; Schulman et al., 2015), Achiam et al. (2017) proposed the Constrained Policy Optimization (CPO) algorithm. However policy updates for the CPO algorithm involve solving an optimization problem through Taylor approximations and inverting a high-dimensional Fisher information matrix. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
These approximations often result in infeasible updates which would require additional recovery steps, this could sometimes cause updates to be backtracked leading to a waste of samples.
In this paper, we propose the First Order Constrained Optimization in Policy Space (FOCOPS) algorithm. FOCOPS attempts to answer the following question: given some current policy, what is the best constraint-satisfying policy update? FOCOPS provides a solution to this question in the form of a two-step approach. First, we will show that the best policy update has a near-closed form solution when attempting to solve for the optimal policy in the nonparametric policy space rather than the parameter space. However in most cases, this optimal policy is impossible to evaluate.
Hence we project this policy back into the parametric policy space. This can be achieved by drawing samples from the current policy and evaluating a loss function between the parameterized policy and the optimal policy we found in the nonparametric policy space. Theoretically, FOCOPS has an approximate upper bound for worst-case constraint violation throughout training. Practically,
FOCOPS is extremely simple to implement since it only utilizes ﬁrst order approximations. We further test our algorithm on a series of challenging high-dimensional continuous control tasks and found that FOCOPS achieves better performance while maintaining approximate constraint satisfaction compared to current state of the art approaches, in particular second-order approaches such as CPO. 2 Preliminaries 2.1 Constrained Markov Decision Process (cid:20) (cid:20) (cid:80)∞ (cid:80)∞ (cid:21) s0 = s (cid:12) (cid:12) t=0 γtR(st, at) (cid:12) (cid:12)
Consider a Markov Decision Process (MDP) (Sutton and Barto, 2018) denoted by the tuple (S, A, R, P, µ) where S is the state space, A is the action space, P : S × A × S → [0, 1] is the transition kernel, R : S × A → R is the reward function, µ : S → [0, 1] is the ini-tial state distribution. Let π = {π(a|s) : s ∈ S, a ∈ A} denote a policy, and Π denote the set of all stationary policies. We aim to ﬁnd a stationary policy that maximizes the expected discount return J(π) := Eτ ∼π [(cid:80)∞ t=0 γtR(st, at)]. Here τ = (s0, a0, . . . , ) is a sample tra-jectory and γ ∈ (0, 1) is the discount factor. We use τ ∼ π to indicate that the trajectory distribution depends on π where s0 ∼ µ, at ∼ π(·|st), and st+1 ∼ P (·|st, at). The value function is expressed as V π(s) := Eτ ∼π (cid:12) (cid:12) t=0 γtR(st, at) (cid:12) (cid:12) as Qπ(s, a) := Eτ ∼π
Aπ(s, a) := Qπ(s, a) − V π(s). Finally, we deﬁne the discounted future state visitation distribution as dπ(s) := (1 − γ) (cid:80)∞
A Constrained Markov Decision Process (CMDP) (Kallenberg, 1983; Ross, 1985; Altman, 1999) is an MDP with an additional set of constraints C which restricts the set of allowable policies. The set C consists of a set of cost functions Ci : S × A → R, i = 1, . . . , m. Deﬁne the Ci-return as
JCi(π) := Eτ ∼π [(cid:80)∞ t=0 γtCi(s, a)]. The set of feasible policies is then ΠC := {π ∈ Π : JCi(π) ≤ bi, i = 1, . . . , m}. The reinforcement learning problem w.r.t. a CMDP is to ﬁnd a policy such that
π∗ = argmaxπ∈ΠC J(π).
Analogous to the standard V π, Qπ, and Aπ for return, we deﬁne the cost value function, cost action-value function, and cost advantage function as V π where we replace the reward R
Ci with Ci. Without loss of generality, we will restrict our discussion to the case of one constraint with a cost function C. However we will brieﬂy discuss in later sections how our methodology can be naturally extended to the multiple constraint case.
. The advantage function is deﬁned as (cid:21) s0 = s, a0 = a and action-value function t=0 γtP (st = s|π).
, and Aπ
Ci
, Qπ
Ci 2.2 Solving CMDPs via Local Policy Search
Typically, we update policies by drawing samples from the environment, hence we usually consider a set of parameterized policies (for example, neural networks with a ﬁxed architecture) Πθ = {πθ :
θ ∈ Θ} ⊂ Π from which we can easily evaluate and sample from. Conversely throughout this paper, we will also refer to Π as the nonparameterized policy space.
Suppose we have some policy update procedure and we wish to update the policy at the kth iteration
πθk to obtain πθk+1. Updating πθk within some local region (i.e. D(πθ, πθk ) < δ for some divergence 2
measure D) can often lead to more stable behavior and better sample efﬁciency (Peters and Schaal, 2008; Kakade and Langford, 2002; Pirotta et al., 2013). In particular, theoretical guarantees for policy improvement can be obtained when D is chosen to be DKL (πθ(cid:107)πθk ) (Schulman et al., 2015; Achiam et al., 2017).
However solving CMDPs directly within the context of local policy search can be challenging and sample inefﬁcient since after each policy update, additional samples need to be collected from the new policy in order to evaluate whether the C constraints are satisﬁed. Achiam et al. (2017) proposed replacing the cost constraint with a surrogate cost function which evaluates the constraint JC(πθ) using samples collected from the current policy πθk . This surrogate function is shown to be a good approximation to JC(πθ) when πθ and πθk are close w.r.t. the KL divergence. Based on this idea, the
CPO algorithm (Achiam et al., 2017) performs policy updates as follows: given some policy πθk , the new policy πθk+1 is obtained by solving the optimization problem maximize
πθ∈Πθ
E
πθk s∼d a∼πθ
[Aπθk (s, a)] subject to JC(πθk ) + 1 1 − γ
E
πθk s∼d a∼πθ (cid:104) (cid:105)
πθk
C (s, a)
A
≤ b
¯DKL(πθ (cid:107) πθk ) ≤ δ. (1) (2) (3) s∼d where ¯DKL(πθ (cid:107) πθk ) := E
πθk [DKL (πθ(cid:107)πθk ) [s]]. We will henceforth refer to constraint (2) as the cost constraint and (3) as the trust region constraint. For policy classes with a high-dimensional parameter space such as deep neural networks, it is often infeasible to solve Problem (1-3) directly in terms of θ. Achiam et al. (2017) solves Problem (1-3) by ﬁrst applying ﬁrst and second order Taylor approximation to the objective and constraints, the resulting optimization problem is convex and can be solved using standard convex optimization techniques.
However such an approach introduces several sources of error, namely (i) Sampling error resulting from taking sample trajectories from the current policy (ii) Approximation errors resulting from
Taylor approximations (iii) Solving the resulting optimization problem post-Taylor approximation involves taking the inverse of a Fisher information matrix, whose size is equal to the number of parameters in the policy network. Inverting such a large matrix is computationally expensive to attempt directly hence the CPO algorithm uses the conjugate gradient method (Strang, 2007) to indirectly calculate the inverse. This results in further approximation errors. In practice the presence of these errors require the CPO algorithm to take additional steps during each update in the training process in order to recover from constraint violations, this is often difﬁcult to achieve and may not always work well in practice. We will show in the next several sections that our approach is able to eliminate the last two sources of error and outperform CPO using a simple ﬁrst-order method. 2.3