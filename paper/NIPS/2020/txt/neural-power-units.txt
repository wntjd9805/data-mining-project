Abstract
Conventional Neural Networks can approximate simple arithmetic operations, but fail to generalize beyond the range of numbers that were seen during training.
Neural Arithmetic Units aim to overcome this difﬁculty, but current arithmetic units are either limited to operate on positive numbers or can only represent a subset of arithmetic operations. We introduce the Neural Power Unit (NPU).1 that operates on the full domain of real numbers R and is capable of learning arbitrary power functions in a single layer. The NPU thus ﬁxes the shortcomings of existing arithmetic units and extends their expressivity. We achieve this by using complex arithmetic without requiring a conversion of the network to complex numbers C. A simpliﬁcation of the unit to the RealNPU yields a highly transparent model. We show that the NPUs outperform their competitors in terms of accuracy and sparsity on artiﬁcial arithmetic datasets, and that the RealNPU can discover the governing equations of a dynamical system only from data. 1

Introduction
Numbers and simple algebra are essential not only to human intelligence but also to the survival of many other species [Dehaene, 2011, Gallistel, 2018]. A successful, intelligent agent should, therefore, be able to perform simple arithmetic. State of the art neural networks are capable of learning arithmetic, but they fail to extrapolate beyond the ranges seen during training [Suzgun et al., 2018, Lake and Baroni, 2018]. The inability to generalize to unseen inputs is a fundamental problem that hints at a lack of understanding of the given task. The model merely memorizes the seen inputs and fails to abstract the true learning task. The failure of numerical extrapolation on simple arithmetic tasks has been shown by Trask et al. [2018], who also introduced a new class of Neural Arithmetic
Units with good extrapolation performance on some arithmetic tasks.
Including Neural Arithmetic Units in standard neural networks promises to signiﬁcantly increase their extrapolation capabilities due to their inductive bias towards numerical computation. This is especially important for tasks in which the data generating process contains mathematical relationships. They also promise to reduce the number of parameters needed for a given task, which can improve the explainability of the model. We demonstrate this in a Neural Ordinary Differential Equation (NODE,
Chen et al. [2019]), where a handful of neural arithmetic units can outperform a much bigger network built from dense layers (Sec. 4.1). Moreover, our new unit can be used to directly read out the correct generating ODE from the ﬁtted model. This is in line with recent efforts to build transparent models instead of attempting to explain black-box models [Rudin, 2019], like conventional neural networks. We refer to the terminology by Lipton [2017] which deﬁnes the potential of understanding the parameters of a given model as transparency by decomposability. 1Implementation of Neural Arithmetic Units: github.com/nmheim/NeuralArithmetic.jl The code to reproduce our experiments is available at github.com/nmheim/NeuralPowerUnits. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The currently available arithmetic units all have different strengths and weaknesses, but none of them solve simple arithmetic completely. The Neural Arithmetic Logic Unit (NALU) by Trask et al.
[2018], chronologically, was the ﬁrst arithmetic unit. It can solve addition (+, including subtraction), multiplication (×), and division (÷), but is limited to positive inputs. The convergence of the NALU is quite fragile due to an internal gating mechanism between addition and multiplication paths as well as the use of a logarithm which is problematic for small inputs. Recently, Schlör et al. [2020] introduced the improved NALU (iNALU, to ﬁx the NALU’s shortcomings. It signiﬁcantly increases its complexity, and we observe only a slight improvement in performance. Madsen and Johansen [2020] solve (+, ×) with two new units: the Neural Addition Unit (NAU), and the Neural Multiplication
Unit (NMU). Instead of gating between addition and multiplication paths, they are separate units that can be stacked. They can work with the full range of real numbers, converge much more reliably, but cannot represent division.
Our Contributions
Neural Power Unit. We introduce a new arithmetic layer (NPU, Sec. 3) which is capable of learning products of power functions ((cid:81) xwi i ) of arbitrary real inputs xi and power wi, thus including 1x−1 multiplication (x1 × x2 = x1 2) as well as division (x1 ÷ x2 = x1 2 ). This is achieved by using formulas from complex arithmetic (Sec. 3.1). Stacks of NAUs and NPUs can thus learn the full spectrum of simple arithmetic operations. 1x1
Convergence improvement. We address the known convergence issues of neural arithmetic units by introducing a relevance gate that smooths out the loss surface of the NPU (Sec. 3.2). With the relevance gate, which helps to learn to ignore variables, the NPU reaches extrapolation errors and sparsities that are on par with the NMU on (×) and outperforms NALU on (÷,
Transparency. We show how a power unit can be used as a highly transparent2 model for equation discovery of dynamical systems. Speciﬁcally, we demonstrate its ability to identify a model that can be interpreted as a SIR model with fractional powers (Sec. 4.1) that was used to ﬁt the COVID-19 outbreak in various countries [Taghvaei et al., 2020].
·).
√ 2