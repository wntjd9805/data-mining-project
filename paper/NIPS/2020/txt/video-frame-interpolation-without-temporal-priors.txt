Abstract
Video frame interpolation, which aims to synthesize non-exist intermediate frames in a video sequence, is an important research topic in computer vision. Existing video frame interpolation methods have achieved remarkable results under speciﬁc assumptions, such as instant or known exposure time. However, in complicated real-world situations, the temporal priors of videos, i.e., frames per second (FPS) and frame exposure time, may vary from different camera sensors. When test videos are taken under different exposure settings from training ones, the interpolated frames will suffer signiﬁcant misalignment problems. In this work, we solve the video frame interpolation problem in a general situation, where input frames can be acquired under uncertain exposure (and interval) time. Unlike previous methods that can only be applied to a speciﬁc temporal prior, we derive a general curvilinear motion trajectory formula from four consecutive sharp frames or two consecutive blurry frames without temporal priors. Moreover, utilizing constraints within adjacent motion trajectories, we devise a novel optical ﬂow reﬁnement strategy for better interpolation results. Finally, experiments demonstrate that one well-trained model is enough for synthesizing high-quality slow-motion videos under complicated real-world situations. Codes are available on https://github. com/yjzhang96/UTI-VFI. 1

Introduction
Video frame interpolation aims to synthesize non-exist intermediate frames and thereby provides a visually ﬂuid video sequence. It has broad application prospects, such as slow motion production [13], up-converting frame rate [3] and novel-view rendering [6].
Many state-of-the-art video interpolation methods [1, 12, 17, 34] aim to estimate the object motion and occlusion with the assistance of optical ﬂow. Through reﬁning forward and backward motion
ﬂows among several frames, these methods can directly warp pixels to synthesize desired intermediate frames. To achieve this goal, some popular datasets, of which either triplet images or 240fps high-frame-rate videos, are collected as the ground-truth of real-world motions. Meanwhile, to evaluate the performance of proposed methods, the well-trained model is tested using frames collected in a similar way. Although signiﬁcant improvement has demonstrated by experiments of recent works, people may ask if the same (or similar) performance can be achieved in complicated real-world situations.
* indicates equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (a) Illustration of frame acquisition in video shooting. In real-world situation, the time interval t0 and t1 are unknown and may vary under different exposure setting. In the speciﬁc example in the ﬁgure, the time intervals are set to t0/t1 = 6 : 4, which indicates the intra-frame and inter-frame interpolation should be 7 and 3 frames respectively when we want to interpolate 10 frames. (b) Our proposed method aims to determine the uncertain time intervals and perform interpolation from the four consecutive states.
To comprehensively discuss this question, we ﬁrst revisit the principle of video frame acquisition.
As illustrated in Fig. 1 (a), the frame acquisition process usually includes two phases: exposure phase and readout phase. In the exposure phase, the shutter opens for a duration of t0 so that the photosensitive sensor is exposed. In the readout phase, the camera reads the charge on the pixel array and convert the signal to get the pixel value. Considering different technologies of cameras, the readout phase could be either overlapped or non-overlapped with the exposure phase. Here, Fig. 1 (a) is an example of non-overlapped exposure. For easy discussing, we deﬁne the time interval between two exposures as t1. Thus, a complete shutter period is deﬁned as the time period t0 + t1.
Correspondingly, frames per second (FPS) is deﬁned as the reciprocal of the shutter period. Note that, t1 cannot be eliminated because of the intrinsic demand of the sensor. Meanwhile, t0 cannot be too short compared to shutter period, otherwise it will produce a visually discontinuous video. 1 t0+t1
The exposure time t0 and the interval time t1 (or FPS
) are two important parameters of a camera sensor, and they could vary largely across different cameras [4]. Therefore, when we perform frame interpolation on real-world videos, following challenges should be further considered: 1) Due to the existence of exposure time, the movement of the camera and object may produce motion/dynamic blur within a video frame. Directly performing the interpolation between blurry frames would lead to inferior visual results. A more severe blur would usually occur in the lower frame-rate video since the exposure time is relatively long. 2) Simply combining deblurring and video interpolation techniques may not handle the blurry video frames well. For blurry video frames, we should not only focus on the inter-frame interpolation, but also perform the intra-frame interpolation. 3) Note that t0 and t1 may vary due to the limitation of equipment or different exposure settings, the number of interpolated frames and corresponding motion trajectories will vary accordingly. For example, in the instance of
Fig 1 (a), if we want to up-convert the FPS by 10 times, we should interpolate 7 frames underlying each blurry frame, and 3 frames between the two consecutive frames. Similarly, the estimation of the motion trajectory must consider the uneven time intervals. According to our observation, most existing works cannot overcome these three challenges simultaneously. Although the most recent works [13, 27] manage to solve the problem of motion blur in video interpolation, they are trained on the speciﬁc exposure setting and could be hard to generalize to different situations.
To address these issues, in this work, we consider the video frame interpolation problem in a more general situation and aim to deliver more accurate interpolation results. Speciﬁcally, giving a video sequence as the input, we ﬁrst train a second-order residual key-states restoration network to synthesize the start and the end states for each frame, e.g. L0 and L1 in Fig. 1 (b). If there exists zero movement (misalignment) between two states, the video frame is regarded as one instant frame (i.e. without blur). Otherwise, the exposure time cannot be ignored, and both inter- and intra-interpolation are performed. Moreover, following the same assumption as [34], i.e. the acceleration of motion remains consistent during consecutive frames, we apply the quadratic model [18, 34] to the general video acquisition situation. We derive the general curvilinear motion representation without temporal priors from consecutive four key-states, such as L0, L1, L2 and L3 in Fig. 1 (b). Meanwhile, the 2
relationship between t0 and t1 can be determined by the displacements between key-states, i.e. S01,
S12 and S23. In addition, to reduce the adverse effects caused by inferior optical-ﬂow estimation, we further reﬁne the optical ﬂows with the derived trajectory priors. Finally, the reﬁned optical ﬂows are utilized to perform high-quality intermediate frame synthesis.
Overall, in this paper, we make following contributions: 1) We propose a restoration network to synthesize start and end states of the input video frames. This network is able to handle different exposure settings, and remove blur in the original video clip; 2) We derive a curvilinear motion representation which is sensitive to different exposure settings, thereby providing a more accurate frame alignment for uncertain time interval interpolation; 3) We further reﬁne the optical ﬂow with the trajectory priors to improve the interpolation results. We construct different datasets to simulate the different exposure settings in real scenarios. Comprehensive experiments on these datasets and real-world vidoes demonstrate the effectiveness of our proposed framework. 2