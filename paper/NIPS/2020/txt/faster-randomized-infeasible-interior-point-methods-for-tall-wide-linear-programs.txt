Abstract
Linear programming (LP) is used in many machine learning applications, such as (cid:96)1-regularized SVMs, basis pursuit, nonnegative matrix factorization, etc. Interior
Point Methods (IPMs) are one of the most popular methods to solve LPs both in theory and in practice. Their underlying complexity is dominated by the cost of solving a system of linear equations at each iteration. In this paper, we consider infeasible IPMs for the special case where the number of variables is much larger than the number of constraints (i.e., wide), or vice-versa (i.e., tall) by taking the dual. Using tools from Randomized Linear Algebra, we present a preconditioning technique that, when combined with the Conjugate Gradient iterative solver, prov-ably guarantees that infeasible IPM algorithms (suitably modiﬁed to account for the error incurred by the approximate solver), converge to a feasible, approximately optimal solution, without increasing their iteration complexity. Our empirical evaluations verify our theoretical results on both real and synthetic data. 1

Introduction
Linear programming (LP) is one of the most useful tools available to theoreticians and practitioners throughout science and engineering. In Machine Learning, LP appears in numerous settings, including (cid:96)1-regularized SVMs [57], basis pursuit (BP) [54], sparse inverse covariance matrix estimation (SICE) [55], the nonnegative matrix factorization (NMF) [45], MAP inference [37], etc. Not surprisingly, designing and analyzing LP algorithms is a topic of paramount importance in computer science and applied mathematics.
One of the most successful paradigms for solving LPs is the family of Interior Point Methods (IPMs), pioneered by Karmarkar in the mid 1980s [25]. Path-following IPMs and, in particular, long-step path following IPMs, are among the most practical approaches for solving linear programs. Consider the standard form of the primal LP problem: min cTx , subject to Ax = b , x ≥ 0 , (1) where A ∈ Rm×n, b ∈ Rm, and c ∈ Rn are the inputs, and x ∈ Rn is the vector of the primal variables. The associated dual problem is max bTy , subject to ATy + s = c , s ≥ 0 , (2) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
where y ∈ Rm and s ∈ Rn are the vectors of the dual and slack variables respectively. Triplets (x, y, s) that uphold both (1) and (2) are called primal-dual solutions. Path-following IPMs typically converge towards a primal-dual solution by operating as follows: given the current iterate (xk, yk, sk), they compute the Newton search direction (∆x, ∆y, ∆s) and update the current iterate by following a step towards the search direction. To compute the search direction, one standard approach [41] involves solving the normal equations1:
AD2AT∆y = p. (3)
Here, D = X1/2S−1/2 is a diagonal matrix, X, S ∈ Rn×n are diagonal matrices whose i-th diagonal entries are equal to xi and si, respectively, and p ∈ Rm is a vector whose exact deﬁnition is given in eqn. (16)2. Given ∆y, computing ∆s and ∆x only involves matrix-vector products.
The core computational bottleneck in IPMs is the need to solve the linear system of eqn. (3) at each iteration. This leads to two key challenges: ﬁrst, for high-dimensional matrices A, solving the linear system is computationally prohibitive. Most implementations of IPMs use a direct solver; see Chapter 6 of [41]. However, if AD2AT is large and dense, direct solvers are computationally impractical. If
AD2AT is sparse, specialized direct solvers have been developed, but these do not apply to many
LP problems arising in machine learning applications due to irregular sparsity patterns. Second, an alternative to direct solvers is the use of iterative solvers, but the situation is further complicated since
AD2AT is typically ill-conditioned. Indeed, as IPM algorithms approach the optimal primal-dual solution, the diagonal matrix D is ill-conditioned, which also results in the matrix AD2AT being ill-conditioned. Additionally, using approximate solutions for the linear system of eqn. (3) causes certain invariants, which are crucial for guaranteeing the convergence of IPMs, to be violated; see
Section 1.1 for details.
In this paper, we address the aforementioned challenges, for the special case where m (cid:28) n, i.e., the number of constraints is much smaller than the number of variables; see Appendix A for a generalization. This is a common setting in ML applications of LP solvers, since (cid:96)1-SVMs and basis pursuit problems often exhibit such structure when the number of available features (n) is larger than the number of objects (m). This setting has been of interest in recent work on LPs [17, 4, 31]. For simplicity of exposition, we also assume that the constraint matrix A has full rank, equal to m. First, we propose and analyze a preconditioned Conjugate Gradient (CG) iterative solver for the normal equations of eqn. (3), using matrix sketching constructions from the Randomized Linear Algebra (RLA) literature. We develop a preconditioner for AD2AT using matrix sketching which allows us to prove strong convergence guarantees for the residual of CG solvers. Second, building upon the work of [39], we propose and analyze a provably accurate long-step infeasible IPM algorithm.
The proposed IPM solves the normal equations using iterative solvers. In this paper, for brevity and clarity, we primarily focus our description and analysis on the CG iterative solver. We note that a non-trivial concern is that the use of iterative solvers and matrix sketching tools implies that the normal equations at each iteration will be solved only approximately. In our proposed IPM, we develop a novel way to correct for the error induced by the approximate solution in order to guarantee convergence. Importantly, this correction step is relatively computationally light, unlike a similar step proposed in [39]. Third, we empirically show that our algorithm performs well in practice. We consider solving LPs that arise from (cid:96)1-regularized SVMs and test them on a variety of synthetic and real datasets. Several extensions of our work are discussed in Appendix A. 1.1 Our contributions
Our point of departure in this work is the introduction of preconditioned, iterative solvers for solving eqn. (3). Preconditioning is used to address the ill-conditioning of the matrix AD2AT. Iterative solvers allow the computation of approximate solutions using only matrix-vector products while avoiding matrix inversion, Cholesky or LU factorizations, etc. A preconditioned formulation of eqn. (3) is:
Q−1AD2AT∆y = Q−1p, (4) where Q ∈ Rm×m is the preconditioning matrix; Q should be easily invertible (see [3, 22] for background). An alternative yet equivalent formulation of eqn. (4), which is more amenable to 1Another widely used approach is to solve the augmented system [41] which is less relevant for this paper. 2The superscript k in eqn. (16) simply indicates iteration count and is omitted here for notational simplicity. 2
theoretical analysis, is
Q−1/2AD2ATQ−1/2z = Q−1/2p, (5) where z ∈ Rm is a vector such that ∆y = Q−1/2z. Note that the matrix in the left-hand side of the above equation is always symmetric, which is not necessarily the case for eqn. (4). We do emphasize that one can use eqn. (4) in the actual implementation of the preconditioned solver; eqn. (5) is much more useful in theoretical analyses.
Recall that we focus on the special case where A ∈ Rm×n has m (cid:28) n, i.e., it is a short-and-fat matrix. Our ﬁrst contribution starts with the design and analysis of a preconditioner for the Conjugate
Gradient solver that satisﬁes, with high probability, 2 2 + ζ 2 AD) ≤ σ2 max(Q− 1 min(Q− 1 2 2 − ζ 2 AD) ≤
≤ σ2 (6)
, for some error parameter ζ ∈ [0, 1]. In the above, σmin(·) and σmax(·) correspond to the smallest and largest singular value of the matrix in parentheses. The above condition says that the preconditioner effectively reduces the condition number of AD to a constant. We note that the particular form of the lower and upper bounds in eqn. (6) was chosen to simplify our derivations. RLA matrix-sketching techniques allow us to construct preconditioners for all short-and-fat matrices that satisfy the above inequality and can be inverted efﬁciently. Such constructions go back to the work of [2]; see Section 2 for details on the construction of Q and its inverse. Importantly, given such a preconditioner, we then prove that the resulting CG iterative solver satisﬁes (cid:107)Q−1/2AD2ATQ−1/2˜zt − Q−1/2p(cid:107)2 ≤ ζ t(cid:107)Q−1/2p(cid:107)2.
Here ˜zt is the approximate solution returned by the CG iterative solver after t iterations. In words, the above inequality states that the residual achieved after t iterations of the CG iterative solver drops exponentially fast. To the best of our knowledge, this result is not known in the CG literature: indeed, it is actually well-known that the residual of CG may oscillate [21], even in cases where the energy norm of the solution error decreases monotonically. However, we prove that if the preconditioner is sufﬁciently good, i.e., it satisﬁes the constraint of eqn. (6), then the residual decreases as well. (7)
Our second contribution is the analysis of a novel variant of a long-step infeasible IPM algorithm proposed by [39]. Recall that such algorithms can, in general, start with an initial point that is not necessarily feasible, but does need to satisfy some, more relaxed, constraints. Following the lines of [56, 39], let S be the set of feasible and optimal solutions of the form (x∗, y∗, s∗) for the primal and dual problems of eqns. (1) and (2) and assume that S is not empty. Then, long-step infeasible
IPMs can start with any initial point (x0, y0, s0) that satisﬁes (x0, s0) > 0 and (x0, s0) ≥ (x∗, s∗), for some feasible and optimal solution (x∗, s∗) ∈ S. In words, the starting primal and slack variables must be strictly positive and larger (element-wise) when compared to some feasible, optimal primal-dual solution. See Chapter 6 of [52] for a discussion regarding why such choices of starting points are relevant to computational practice and can be identiﬁed more efﬁciently than feasible points.
The ﬂexibility of infeasible IPMs comes at a cost: long-step feasible IPMs converge in O(n log 1/(cid:15)) iterations, while long-step infeasible IPMs need O(n2 log 1/(cid:15)) iterations to converge [56, 39] (Here (cid:15) is the accuracy of the approximate LP solution returned by the IPM; see Algorithm 2 for the exact deﬁnition.). Let (8) p ∈ Rn and r0
Ax0 − b = r0 p,
ATy0 + s0 − c = r0 d, (9) d ∈ Rm are the primal and dual residuals, respectively, and characterize how far where r0 the initial point is from being feasible. As long-step infeasible IPM algorithms iterate and update the d) ∈ Rn+m be the primal primal and dual solutions, the residuals are updated as well. Let rk = (rk and dual residual at the k-th iteration: it is well-known that the convergence analysis of infeasible long-step IPMs critically depends on rk lying on the line segment between 0 and r0. Unfortunately, using approximate solvers (such as the CG solver proposed above) for the normal equations violates this invariant. [39] proposed a simple solution to ﬁx this problem by adding a perturbation vector v to the current primal-dual solution that guarantees that the invariant is satisﬁed. Again, we use
RLA matrix sketching principles to propose an efﬁcient construction for v that provably satisﬁes the invariant. Next, we combine the above two primitives to prove that Algorithm 2 in Section 3 satisﬁes the following theorem. p, rk 3
Theorem 1 Let 0 ≤ (cid:15) ≤ 1 be an accuracy parameter. Consider the long-step infeasible IPM Algo-rithm 2 (Section 3) that solves eqn. (5) using the CG solver of Algorithm 1 (Section 2). Assume that the CG iterative solver runs with accuracy parameter ζ = 1/2 and iteration count t = O(log n). Then, with probability at least 0.9, the long-step infeasible IPM converges after O(n2 log 1/(cid:15)) iterations.
We note that the 0.9 success probability above is for simplicity of exposition and can be easily ampliﬁed using standard techniques. Also, at each iteration of our infeasible long-step IPM algorithm, the running time is O((nnz(A) + m3) log n), ignoring constant terms. See Section 3 for a detailed discussion of the overall running time.
Our empirical evaluation demonstrates that our algorithm requires an order of magnitude much fewer inner CG iterations than a standard IPM using CG, while producing a comparably accurate solution (see Section 4). 1.2 Prior Work
There is a large body of literature on solving LPs using IPMs. We only review literature that is immediately relevant to our work. Recall that we solve the normal equations inexactly at each iteration, and develop a way to correct for the error incurred. We also focus on IPMs that can use a sufﬁciently positive, infeasible initial point (see Section 1.1). We discuss below two papers that present related ideas.
[39] proposed the use of an approximate iterative solver for eqn. (3), followed by a correction step to “ﬁx” the approximate solution (see our discussion in Section 1.1). We propose efﬁcient, RLA-based approaches to precondition and solve eqn. (3), as well as a novel approach to correct for the approximation error in order to guarantee the convergence of the IPM algorithm. Speciﬁcally, [39] propose to solve eqn. (3) using the so-called maximum weight basis preconditioner [46]. However, computing such a preconditioner needs access to a maximal linearly independent set of columns of
AD in each iteration, which is costly, taking O(m2n) time in the worst-case. More importantly, while [38] was able to provide a bound on the condition number of the preconditioned matrix that depends only on properties of A, and is independent of D, this bound might, in general, be very large. In contrast, our bound is a constant and it does not depend on properties of A or its dimensions.
In addition, [39] assumed a bound on the two-norm of the residual of the preconditioned system, but it is unclear how their preconditioner guarantees such a bound. Similar concerns exist for the construction of the correction vector v proposed by [39], which our work alleviates.
The line of research in the Theoretical Computer Science literature that is closest to our work is [15], who presented an IPM that uses an approximate solver in each iteration. However, their accuracy guarantee is in terms of the ﬁnal objective value which is different from ours. More importantly, [15] focuses on short-step, feasible IPMs, whereas ours is long-step and does not require a feasible starting point. Finally, the approximate solver proposed by [15] works only for the special case of input matrices that correspond to graph Laplacians, following the lines of [47, 48].
We also note that in the Theoretical Computer Science literature, [26, 27, 28, 29, 30, 7, 12] proposed and analyzed theoretically ground-breaking algorithms for LPs based on novel tools such as the so-called inverse maintenance for accelerating the linear system solvers in IPMs. However, all these endeavors are primarily focused on the theoretically fast but practically inefﬁcient short-step feasible
IPMs and, to the best of our knowledge, no implementations of these approaches are available for comparisons to standard long-step IPMs. We highlight that our work is focused on infeasible long-step IPMs, known to work efﬁciently in practice.
Another relevant line of research is the work of [14], which proposed solving eqn. (3) using precondi-tioned Krylov subspace methods, including variants of generalized minimum residual (GMRES) or
CG methods. Indeed, [14] conducted extensive numerical experiments on LP problems taken from standard benchmark libraries, but did not provide any theoretical guarantees.
From a matrix-sketching perspective, our work was also partially motivated by [8], which presented an iterative, sketching-based algorithm to solve under-constrained ridge regression problems, but did not address how to make use of such approaches in an IPM-based framework, as we do here.
In another work, [1] proposed a similar sketching-based preconditioning technique. However, their efforts broadly revolved around speeding up and scaling kernel ridge regression. [43, 53] proposed the so-called Newton sketch to construct an approximate Hessian matrix for more general convex objective functions of which LP is a special case. Nevertheless, these randomized second-order 4
methods are signiﬁcantly faster than the conventional approach only when the data matrix is over-constrained, i.e. m (cid:29) n. It is unclear whether the approach of [43, 53] is faster than IPMs when the optimization problem to be solved is linear.
[49] proposed a probabilistic algorithm to solve
LP approximately in a random projection-based reduced feature-space. A possible drawback of this paper is that the approximate solution is infeasible with respect to the original region. Finally, we refer the interested reader to the surveys [51, 19, 33, 18, 24, 34] for more background on Randomized
Linear Algebra. 1.3 Notation and