Abstract
The task of vision-and-language navigation (VLN) requires an agent to follow text instructions to ﬁnd its way through simulated household environments. A promi-nent challenge is to train an agent capable of generalising to new environments at test time, rather than one that simply memorises trajectories and visual details observed during training. We propose a new learning strategy that learns both from observations and generated counterfactual environments. We describe an effective algorithm to generate counterfactual observations on the ﬂy for VLN, as linear combinations of existing environments. Simultaneously, we encourage the agent’s actions to remain stable between original and counterfactual environments through our novel training objective – effectively removing spurious features that would otherwise bias the agent. Our experiments show that this technique provides signiﬁcant improvements in generalisation on benchmarks for Room-to-Room navigation and Embodied Question Answering. 1

Introduction
Deep learning has generated signiﬁcant advances in computer vision and natural language processing.
The most striking successes are witnessed on perceptual tasks that essentially amount to pattern matching. A strength of deep learning is its ability to pick up statistical patterns in large labeled datasets. As a ﬂip side, this capacity leads to models that indiscriminately rely on dataset biases and spurious correlations as much as task-relevant features. This limits the generalisation capabilities of learned models and restrict their applicability on complex tasks (e.g. [1, 2] with images and [3, 4, 5, 6] in multimodal tasks). Most successful applications of deep learning rely on settings where the seen training data and the unseen test data are statistically similar. Yet we argue that better generalisation could be achieved with new training strategies. This is particularly relevant to multimodal, high-level tasks where training examples can only cover a tiny part of the input space.
In this paper, we propose to consider the unseen to learn representations that lead to better gen-eralisation. The method is applied to the task of vision-and-language navigation (VLN, [7, 8, 9]) which requires relating complex inputs with observations of unseen environments. In VLN, an agent receives instructions in natural language and it must decide on a sequence of actions (e.g. turn left, move forward, ...) to reach a target location while observing 2D images of its environment. The task is extremely ambitious: the agent must learn to ground language with visual observations, to understand sequences of instructions and high-level actions (e.g. wait by the door), to generate navigation plans, etc. The standard approach is to train an agent with a combination of reinforcement learning [10, 11] and imitation learning with human-generated examples of instructions and trajectories. These agents can memorise successful sequences of actions and grounding associations but they often fail to apply their capabilities to unseen environments at test time [11]. Our intuition is that a mechanism to reason about alternative observations and trajectories during training could help learning robust navigation strategies. We would like to consider, for example, what would happen if a desk were observed instead of a chair ? 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: We seek to improve a VLN agent’s capability to generalise to unseen environments at test time.
Agents are typically trained by reinforcement and imitation learning, using ground-truth pairs of instruc-tions/trajectories (“factual observations”, left). We propose to generate alternative, counterfactual training observations with combinations of existing environments. We determine the minimum intervention on the factual data that causes the current model to produce different outputs. We then formulate our novel training objective to best exploit these additional examples and improve its generalisation capabilities (right). The generation process is formalised with a causal model of the data, in which we introduce the interpolation coefﬁcients as an exogenous variable u, effectively modelling an intervention on the environment.
Various methods have been proposed to improve generalisation in VLN, such as feature and en-vironment dropout [11], ﬁne-tuning based on the exploration of unseen environments [10, 12] or using beam search [12, 13]. The method we propose is inspired by the framework of counterfactual reasoning [14]. Counterfactuals serve to reason about unobserved scenarios and to estimate the effect of an intervention not represented in the data. In the context of VLN, we essentially want to consider during training what if we observed a different environment. Throughout this paper, we call counterfactuals training environment examples that we could have observed. We consider the causal model underlying the training environments and introduce an exogenous variable that governs their visual features yet is unobserved. We utilise this variable in generating counterfactuals. Intuitively, this exogenous variable captures variations in visual features in the environments that are rather insigniﬁcant for the decision making of the agent and can be ignored. At each training iteration, we generate counterfactuals that represent the minimum edit of an existing training data that causes the model to change its action. Thereafter, we formulate a novel objective that encourages the agent to learn from both observed training data and their counterfactuals by explicitly removing the effects of intervention in the agent’s policy (see Fig. 1). By introducing additional variations in the observations during training, we encourages the model to rely less on idiosyncrasies of a given environment, and rather learn a policy that better generalises to unseen environments at test time.
The contributions of this paper are summarized as follows.
• We propose a novel training strategy for VLN that generates counterfactuals on the ﬂy to account for unseen scenarios. Using both training data and their counterfactuals, we improve agent’s capabilities to generalise to new environments at test time.
• We formalise the new procedure with a causal generative view of the data, in which we introduce an exogenous variable representing interpolation coefﬁcients between original training examples.
We derive an efﬁcient algorithm to generate counterfactual instances that represent minimum interventions over original examples that cause the model to change its output.
• We implement the technique on top of a VLN agent for both reinforcement and imitation learning.
Experiments on benchmarks for Room-to-Room (R2R) navigation [8] and Embodied Question
Answering [9] show signiﬁcant improvements. We reduce the success rate gap between seen and unseen environments in R2R from about 8% to less than 2.5%. 2