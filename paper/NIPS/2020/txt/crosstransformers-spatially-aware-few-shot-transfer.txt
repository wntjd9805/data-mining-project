Abstract
Given new tasks with very little data—such as new classes in a classiﬁcation problem or a domain shift in the input—performance of modern vision systems degrades remarkably quickly. In this work, we illustrate how the neural network representations which underpin modern vision systems are subject to supervision collapse, whereby they lose any information that is not necessary for performing the training task, including information that may be necessary for transfer to new tasks or domains. We then propose two methods to mitigate this problem. First, we employ self-supervised learning to encourage general-purpose features that transfer better. Second, we propose a novel Transformer based neural network architecture called CrossTransformers, which can take a small number of labeled images and an unlabeled query, ﬁnd coarse spatial correspondence between the query and the labeled images, and then infer class membership by computing distances between spatially-corresponding features. The result is a classiﬁer that is more robust to task and domain shift, which we demonstrate via state-of-the-art performance on
Meta-Dataset, a recent dataset for evaluating transfer from ImageNet to many other vision datasets. Code available at: https://github.com/google-research/ meta-dataset. 1

Introduction
General-purpose vision systems must be adaptable. Home robots must be able to operate in new, unseen homes; photo-organizing software must recognize unseen objects (e.g., to ﬁnd examples of
“my sixth-grade son’s abstract art project”); industrial quality-assurance systems must spot defects in new products. Deep neural network representations can bring some visual knowledge from datasets like ImageNet [68] to bear on different tasks beyond ImageNet [15, 32, 62], but empirically, this requires a non-trivial amount of labeled data in the new task. With too little labeled data, or for a large change in distribution, such systems empirically perform poorly.
Research on meta-learning directly benchmarks adaptability. At training time, the algorithm receives a large amount of data and accompanying supervision (e.g., labels). At test time, however, the algorithm receives a series of episodes, each of which consists of a small number of datapoints from a different distribution than the training set (e.g., a different domain or different classes). Only a subset of this data has the accompanying supervision (called the support set); the algorithm must make predictions about the rest (the query set). Meta-Dataset [86] is particularly relevant for vision, as the challenge is few-shot ﬁne-grained image classiﬁcation. The training data is a subset of ImageNet classes. At test time, each episode either contains images from the other ImageNet classes, or from one of nine other visually distinct ﬁne-grained recognition datasets. The algorithm must rapidly adapt its representations to the new classes and domains.
Simple centroid-based algorithms like Prototypical Nets [17, 76] are near state-of-the-art on Meta-Dataset, achieving around 50% accuracy on the held-out ImageNet classes in Meta-Dataset’s valida-tion set (chance is roughly 1 in 20). An equivalent classiﬁer trained on those validation classes can achieve roughly 84% accuracy on the same challenge. What accounts for the enormous discrepancy 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of supervision collapse with nearest neighbors. In each row, the leftmost image is a query taken from the Meta-Dataset ImageNet test classes, and the rest are the top 9 nearest neighbors from both training and test support set classes, using the embedding learned by a
Prototypical Net (training details in the extended version [1]). Images belonging to the test split are near the bottom left corner; rest are from the training split. For a simple classiﬁer indicated by a to work well on these test classes, semantically similar images should have similar representations, and so we hope the nearest neighbors would come from the same—or semantically similar—classes.
Instead, we observe that only 5% of matches for test-set queries are from the same class as the query.
Furthermore, many matches are all from the same incorrect training class (highlighted in red). We see a knot is matched with several gila monsters (and other reptiles); a bassoon with letter openers (and pens); a screw with hammers; another screw with buckeyes. The errors within that wrong class often have widely different appearance: for example, the bottom-most screw is matched with single buckeyes and also a pile of buckeyes. One interpretation is that the network picks up on image patterns during training that allow images of each class to be tightly grouped in the feature space, minimizing other ways that the image might be similar to other classes in preparation for a conﬁdent classiﬁcation. For out-of-domain samples, the network can then overemphasize a spurious image pattern that suggests membership in one training-set class. This is the consequence of supervision collapse, where image patterns that might help make the correct associations are lost. between performance on within-distribution samples and out-of-distribution samples? We hypothe-size that because the neural network backbone of Prototypical Nets is designed for classiﬁcation, they do just this: represent only an image’s (training-set) class, and discard information that might help with out-of-distribution classes. Doing so minimizes the losses for many meta-learning algorithms, including Prototypical Nets. We call this problem supervision collapse, and illustrate it in Figure 1.
Our ﬁrst contribution is to explore using self-supervision to overcome supervision collapse. We employ SimCLR [16], which learns embeddings that discriminate between every image in the dataset while maintaining invariance to transformations (e.g., cropping and color shifts), thus capturing more than just classes. However, rather than treat SimCLR as an auxiliary loss, we reformulate SimCLR as
“episodes” that can be classiﬁed in the same manner as a training episode.
Our second contribution is a novel architecture called CrossTransformers, which extends Trans-formers [88] to few-shot ﬁne-grained classiﬁcation. Our key insight is that objects and scenes are generally composed of smaller parts, with local appearance that may be similar to what has been seen at training time. The classical example of this is the centaur that appeared in several early papers on visual representation [12, 41, 89], where the parts from the human and horse composed the centaur.
CrossTransformers operationalize this insight of (i) local part-based comparisons, and (ii) accounting for spatial alignment, resulting in a procedure for comparing images which is more agnostic to the underlying classes. In more detail, ﬁrst a coarse alignment between geometric or functional parts in the query- and support-set images is established using attention as in Transformers. Then, given this alignment, distances between corresponding local features are computed to inform classiﬁcation. We demonstrate this improves generalization to unseen classes and domains. 2
In summary, our contributions in this paper are: (i) We improve the robustness of our local features with a self-supervised technique, modifying the state-of-the-art SimCLR [16] algorithm. (ii) We propose the CrossTransformer, a network architecture that is spatially aware and performs few-shot classiﬁcation using more local features, which improves transfer. Finally, (iii) we evaluate and ablate how the choices in these algorithms impact Meta-Dataset [86] performance, and demonstrate state-of-the-art results on nearly every dataset within it, often by large margins. 2