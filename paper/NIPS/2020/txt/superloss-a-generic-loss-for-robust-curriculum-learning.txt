Abstract
Curriculum learning is a technique to improve a model performance and general-ization based on the idea that easy samples should be presented before difﬁcult ones during training. While it is generally complex to estimate a priori the difﬁ-culty of a given sample, recent works have shown that curriculum learning can be formulated dynamically in a self-supervised manner. The key idea is to somehow estimate the importance (or weight) of each sample directly during training based on the observation that easy and hard samples behave differently and can therefore be separated. However, these approaches are usually limited to a speciﬁc task (e.g., classiﬁcation) and require extra data annotations, layers or parameters as well as a dedicated training procedure. We propose instead a simple and generic method that can be applied to a variety of losses and tasks without any change in the learning procedure. It consists in appending a novel loss function on top of any existing task loss, hence its name: the SuperLoss. Its main effect is to automatically downweight the contribution of samples with a large loss, i.e. hard samples, effectively mimicking the core principle of curriculum learning. As a side effect, we show that our loss prevents the memorization of noisy samples, making it possible to train from noisy data even with non-robust loss functions. Experimental results on image classiﬁcation, regression, object detection and image retrieval demonstrate consistent gain, particularly in the presence of noise. 1

Introduction
Curriculum learning (CL) [4], a paradigm inspired by the learning process of humans and animals, has recently received a sustained attention [13, 18, 48, 55]. CL is based on the intuitive observation that our learning process naturally starts from easy notions before gradually transitioning to more complex ones. When applied to machine learning, it essentially boils down to designing a sampling strategy, i.e. a curriculum, that would present easy samples to the model before harder ones [13, 22].
While this was shown to be effective at improving the model performance and its generalization power in earlier works [4, 22], they were limited to toy datasets and engineered sampling heuristics, leaving unaddressed the general problem of establishing a curriculum on real-world tasks and datasets.
For this reason, a growing line of research has focused on methods able to automatically determine the curriculum without requiring prior knowledge about the task at hand [18, 25, 34, 44, 48, 49].
This is indeed possible as easy and hard samples behave differently during training in terms of their respective loss, allowing them to be somehow discriminated [19, 34, 44]. In this context, CL is accomplished by predicting the easiness of each sample at each training iteration in the form 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Top-left: The classical supervised training. Bottom-left: Our approach consists in appending our SuperLoss on top of any existing loss, without changing anything else in the training procedure.
Back-propagation now starts from the SuperLoss. Right: At test time, no change is required. of a weight, such that easy samples receive larger weights during the early stages of training and conversely. Another beneﬁt of this type of approach, aside from improving the model generalization, is their resistance to noise. This is due to the fact that noisy samples (i.e. with wrong labels) tend to be harder for the model and thus receive smaller weights throughout training [18, 44, 48], effectively discarding them. This side effect makes these methods especially attractive when clean annotated data are expensive while noisy data are widely available and cheap [2, 20, 31, 36, 52]. Existing approaches for automatic CL nevertheless suffer from two important drawbacks that drastically limit their applicability. First, current methods like [6, 18, 19, 28, 48, 55] overwhelmingly focus and specialize on the classiﬁcation task, even though the principles mentioned above are general and can potentially apply to other tasks. Second, they all require important changes in the training procedure, often requiring dedicated training schemes [1, 6, 8, 16, 28, 55], involving multi-stage training with or without special warm-up periods [13, 18, 19, 23, 39, 60], extra learnable parameters and layers [6, 18, 29, 48, 60, 67] or a clean subset of data [18, 27, 44].
In this paper, we propose instead a simple yet generic approach to dynamic curriculum learning. It is inspired by recent conﬁdence-aware loss functions that yield the capability to jointly learn network parameters and sample weights, or conﬁdences, in a uniﬁed framework [38, 46, 48]. As a ﬁrst contribution, we introduce a novel type of conﬁdence-aware loss function that can transform any task loss into a conﬁdence-aware version. Thanks to an automatic and optimal conﬁdence-setting scheme, it can scale-up to any number of samples and requires no modiﬁcation of the learning procedure except the insertion of a new loss termed SuperLoss, making it broadly applicable. As shown in
Figure 1, the SuperLoss is simply plugged on top of the original task loss during training, hence its name. Its role is to monitor the loss of each sample during training and to determine the sample contribution dynamically by applying the core principle of curriculum learning. To the best of our knowledge, this is the ﬁrst time that a task-agnostic approach for curriculum learning without any change in the training procedure is proposed. As a second contribution, we present how the SuperLoss can be applied to various tasks: image classiﬁcation, deep regression, object detection and image retrieval. As a third contribution, we present empirical evidence that our approach leads to consistent gains when applied on clean and noisy datasets. In particular, we observe large gains in the case of training from noisy data, a typical case for large-scale datasets automatically collected from the web. 2 SuperLoss
We ﬁrst present a family of specialized loss functions that we denote as conﬁdence-aware and which are closely related to CL (Section 2.1). We then derive a generic task-agnostic formulation in
Section 2.2 and show how this formulation can be further simpliﬁed in the context of CL, yielding the SuperLoss (Section 2.3). Finally, we illustrate in Section 2.4 several applications of our approach. 2.1 Conﬁdence-aware loss
A novel type of loss functions, which we denote as conﬁdence-aware, has been recently and in-dependently proposed by several authors for a variety of tasks and backgrounds [21, 38, 46–48].
Consider a dataset {(xi, yi)}N i=1, where sample xi has label yi, and let f (·) be a trainable predictor to be optimized using empirical risk minimization. In contrast to traditional loss functions of the form (cid:96)(f (xi), yi), a conﬁdence-aware loss function (cid:96)(f (xi), yi, σi) takes an additional learnable parameter σi ≥ 0 as input. Such a parameter is associated with each sample xi and represents the conﬁdence or reliability of the corresponding prediction f (xi). 2
Figure 2: Overall shape of various conﬁdence-aware losses that learn the conﬁdence, with from left to right: conﬁdence-aware cross-entropy [48], reliability loss [46], introspection loss [38], and our proposed SuperLoss. Each plot shows the resulting loss according to the ‘correctness’ of a particular prediction (y-axis) and the corresponding conﬁdence (x-axis). Blue is smaller and yellow is larger.
The goal of a conﬁdence-aware loss is to handle difﬁcult samples without resorting to heuristics such as using robust versions of the loss, by instead modulating the loss amplitude w.r.t. the conﬁdence parameter [38]. We plot several existing conﬁdence-aware loss functions in Figure 2, where the sample conﬁdence σi is represented on the x-axis and a quantity measuring the correctness of the network prediction is represented on the y-axis. Interestingly, these losses have practically identical shapes despite the fact that they have been proposed independently for different tasks (e.g., patch matching [46], keypoint detection [38], dynamic CL [48]) and have seemingly unrelated formula (see Supplementary). Regardless of the manner the conﬁdence intervenes in the loss formula, a key property that they noticeably share is that the gradient of the loss w.r.t. the network parameters monotonously increases with the conﬁdence, all other parameters staying ﬁxed. Simply put, the left-hand side of the plots (low-conﬁdence area of the loss) is ﬂatter than the right-hand side (high-conﬁdence area). As a consequence, gradient updates towards the model parameters are smaller for samples with lower conﬁdence, which practically amounts to down-weight low-conﬁdence samples during training.
This property makes conﬁdence-aware losses particularly well suited to dynamic CL, as it allows to learn the conﬁdence, i.e. weight, of each sample automatically through back-propagation and without further modiﬁcation of the learning procedure. This principle was recently implemented by Saxena et al. [48] for the classiﬁcation task with a modiﬁed conﬁdence-aware cross-entropy loss. As a result, jointly minimizing the network parameters and the conﬁdence parameters, named data parameters in [48], via standard stochastic gradient descent leads to accurately estimate the reliability of each prediction, i.e. the difﬁculty of each sample, via the conﬁdence parameter. 2.2 A task-agnostic conﬁdence-aware loss function
While the closely-related principles behind dynamic CL and conﬁdence-aware losses appears to be completely generic, we surprisingly ﬁnd that none of the existing conﬁdence-aware formulations easily generalize to other tasks. For instance, the introspection loss [38] is designed for the retrieval task; the modiﬁed cross-entropy from [48] specializes in the classiﬁcation task; the multi-task loss [21] only handles regression and cross-entropy; etc. We refer to the Supplementary material for more details.
In this work, we propose instead a novel and task-agnostic type of conﬁdence-aware loss function. In contrast to existing conﬁdence-aware losses, it only takes two inputs, namely, the task loss (cid:96)(f (xi), yi) (simpliﬁed as (cid:96)i and referred to as input loss in the following) and a conﬁdence parameter σi. We denote this function as Lλ((cid:96)i, σi), where λ > 0 is a regularization hyper-parameter. Even though some conﬁdence-aware losses are built from probabilistic considerations [21, 38, 47], we ﬁnd it hard to derive a probabilistic framework that would suit all possible types of loss functions. Instead, we consider generic principles and identify three properties that Lλ((cid:96)i, σi) needs to satisfy: 1. Translation-invariance. Adding a constant to the input loss should have no effect on Lλ’s gradient, i.e. ∀K, ∃K (cid:48) | Lλ((cid:96)i + K, σi) = K (cid:48) + Lλ((cid:96)i, σi), where K and K (cid:48) are constant. 2. Homogeneity. Lλ should have a multiplicative scaling behavior: ∃λ, λ(cid:48) | ∀K > 0, Lλ(K(cid:96)i, σi) = K Lλ(cid:48)((cid:96)i, σi), where K is a constant. This way we can handle in-3
Figure 3: Left: We plot on top the typical losses incurred by an easy (green) and a hard (red) sample during training. At bottom, we show (a) their respective conﬁdence when learned via back-propagation using Lλ((cid:96)i, σi) (dotted lines) and (b) their optimal conﬁdence σ∗
λ((cid:96)i) (plain lines). In contrast to using the optimal conﬁdence, learning it induces a signiﬁcant delay between the moment a sample becomes easy (its loss passes under τ ) and the moment its conﬁdence becomes greater than 1. Right: we show SLλ((cid:96)i) as a function of (cid:96)i − τ for different λ. The SuperLoss emphasizes the input loss when (cid:96)i < τ , and reduces it in the opposite case, thus limiting the impact of hard samples. put losses of any amplitude, i.e. we just need to accordingly rescale the learning rate and
λ. 3. Generalization, in the sense that Lλ((cid:96)i, σi) should amount to the input loss for a particular conﬁdence σ, i.e. ∃σ | Lλ((cid:96)i, σ) = (cid:96)i + K, where K is a constant.
Optionally, a convenient aspect is that it should be easily interpretable.
We now propose one of the simplest possible formulation that meets all these criteria, including the interpretability. Similarly to the conﬁdence-aware cross-entropy proposed in [48], it is composed of a loss-amplifying term and a regularization term controlled by the hyper-parameter λ > 0: (cid:0)(cid:96)i, σi (cid:1) = ((cid:96)i − τ ) σi + λ (log σi)2 ,
Lλ (1) where τ is a threshold that ideally separates easy samples from hard samples based on their respective loss. In practice, τ is empirically estimated as a running average of the input loss during training, thereby trivially satisfying translation-invariance (property 1). Note that a similar thresholding involving extra learnable layers and parameters was proposed to separate easy and hard samples in
MentorNet [18]. In certain cases, τ can also be set as a constant based on prior knowledge on the task loss, but our results suggest that this makes almost no difference compared to using a running average (see Section 3). As for the other properties, homogeneity (property 2) is veriﬁed with λ = Kλ(cid:48) and generalization (property 3) is achieved for σi = 1 as Lλ((cid:96)i, 1) = (cid:96)i − τ . We plot the shape of
Lλ((cid:96)i, σi) in Figure 2 (right): its shape is similar to other specialized conﬁdence-aware losses. 2.3 Optimal conﬁdence and SuperLoss
While Saxena et al. [48] have shown the beneﬁts of learning the conﬁdence of each sample dy-namically via back-propagation, this has several shortcomings. First, it requires one extra learnable parameter σi per sample, which does not scale for tasks like detection or retrieval where the number of samples can be almost inﬁnite (Section 2.4). Second, learning the conﬁdence naturally induces a delay (i.e. the time of convergence), and thus potential inconsistencies between the true status of a sample and its respective conﬁdence, see Figure 3 (left). Third, it adds several hyper-parameters on top of the baseline approach such as the learning rate and weight decay of the secondary optimizer.
Instead of waiting for the conﬁdence parameters to converge, we therefore propose to directly use their converged value at the limit, which only depends on the input loss (cid:96)i:
σ∗
λ((cid:96)i) = arg min
σi
Lλ((cid:96)i, σi). (2)
As a consequence, the conﬁdence parameters do not need to be learned and are up-to-date with the sample status. The new loss function that we obtain takes a single parameter as input and can therefore simply be appended on top of any given task loss (see Figure 1), hence its name of SuperLoss (SL):
SLλ((cid:96)i) = Lλ ((cid:96)i, σ∗
λ((cid:96)i)) = min
σi
Lλ ((cid:96)i, σi) . (3) 4
As we demonstrate in Section 2.1 of the Supplementary, the optimal conﬁdence σ∗ has a closed-form solution. In practice, we cap the loss to avoid inﬁnite values as follows:
λ((cid:96)i) from Eq. (2)
λ((cid:96)i) = e−W ( 1
σ∗ 2 max(− 2 e ,β)) with β =
, (4) (cid:96)i − τ
λ where W stands for the Lambert W function. During back-propagation, τ and σ∗
λ((cid:96)i) are computed from the input loss (cid:96)i and then treated as constant. We plot our SuperLoss in Figure 3 (right) as a function of the input loss for various λ. As intended, it ampliﬁes the contribution of easy samples (i.e. when (cid:96)i < τ ) while strongly ﬂattening the input loss for hard ones. We show in Section 2.2 of the Supplementary that when the regularization parameter λ tends to inﬁnity, the optimal conﬁdence tends to 1, and thus the SuperLoss amounts to the input loss: limλ→∞ SLλ((cid:96)i) = (cid:96)i − τ . 2.4 Applications
We now present concrete application cases of the SuperLoss for various tasks.
Classiﬁcation. We straightforwardly plug the Cross-Entropy loss ((cid:96)CE) into the SuperLoss:
SLCE(xi, yi) := SL (cid:0)(cid:96)CE(f (xi), yi)(cid:1). When speciﬁed, we use a ﬁxed threshold for τ = log C (where C is the number of classes) as it represents the cross-entropy of a uniform distribution and hence appears to be a natural boundary between correct and incorrect predictions.
Regression. Likewise, we can plug a regression loss (cid:96)reg such as the smooth-L1 loss (smooth-(cid:96)1) or the Mean-Square-Error (MSE) loss ((cid:96)2) into the SuperLoss. Note that the range of values for a regression loss drastically differs from the one of the CE loss, but as we pointed out previously, this is not an issue for the SuperLoss thanks to its homogeneity property.
Object Detection. We apply the SuperLoss on the box classiﬁcation component of two object detection frameworks: Faster R-CNN [45] and RetinaNet [33]. Faster R-CNN classiﬁcation loss is a standard cross-entropy loss (cid:96)CE on which we plug our SuperLoss SLCE. RetinaNet classiﬁcation loss is a class-balanced focal loss (FL): (cid:96)F L(p, y) = −αy(1 − py)γlog(py) with p the probabilities predicted by the network for each box obtained with a softmax on the logits z = f (x), γ ≥ 0 a focusing hyper-parameter and αy a class-balancing hyper-parameter. We point out that, in contrast to classiﬁcation, object detection typically deals with an enormous number of negative detections, for which it is infeasible to store or learn individual conﬁdences. In contrast to approaches that learn a separate weight per sample like [48], our method estimates the conﬁdence of positive and negative detections on the ﬂy from their individual loss.
Retrieval/metric learning. We apply our SuperLoss to image retrieval using the contrastive loss [14], which was shown to be one of the most effective losses for metric learning [37].
In this case, the training set {(xi, xj, yij)}i,j is composed of pairs of samples labeled either positively (yij = 1) or negatively (yij = 0). The goal is then to learn a latent representa-tion where positive pairs lie close whereas negative pairs are far apart. The contrastive loss is composed of two losses: (cid:96)CL
+ (f (xi), f (xj)) = [(cid:107)f (xi) − f (xj)(cid:107)]+ for positive pairs and (cid:96)CL
− (f (xi), f (xj)) = [m − (cid:107)f (xi) − f (xj)(cid:107)]+ for negative pairs where m > 0 is a margin (we assume a null margin for positive pairs as is common [43] and [ . ]+ denotes the positive component).
We apply the SuperLoss on top of each of the two losses, i.e. with two independent thresholds τ+ and
τ−, but still sharing the same regularization parameter λ for simplicity:
+ (f (xi), f (xj))(cid:1)
− (f (xi), f (xj))(cid:1)
λ (f (xi), f (xj), yij) = if yij = 1, if yij = 0. (cid:26)SLλ
SLλ (cid:0)(cid:96)CL (cid:0)(cid:96)CL
SLCL (5)
The same strategy can be applied to other metric learning losses such as the triplet loss [56]. As for object detection, we note that other approaches that explicitly learn or estimate the importance of each sample cannot be applied to metric learning because (a) the number of potential pairs or triplets is enormous, making intractable to store their conﬁdence in memory; and (b) only a small fraction of them is seen at each epoch, which prevents the accumulation of enough evidence. 3 Experimental results
After describing our experimental protocol in Section 3.1, we evaluate the SuperLoss for regression (Section 3.2), image classiﬁcation (Section 3.3), object detection (Section 3.4) and image retrieval (Section 3.5) each time in clean and noisy conditions. 5
Figure 4: Performance (MAE) on digit regression and human age regression as a function of noise proportion, for a robust loss (smooth-(cid:96)1) and a non-robust loss ((cid:96)2). 3.1 Experimental protocol
We refer to the model trained with the original task loss as the baseline. Our protocol is to ﬁrst train the baseline and tune its hyper-parameters (e.g., learning rate, weight decay, etc.) using held-out validation for each noise level. For a fair comparison between the baseline and the SuperLoss, we train the model with the SuperLoss using the same hyper-parameters. Unlike most existing works (e.g., [6]), we do not need special warm-up periods or other tricks. Hyper-parameters speciﬁc to the
SuperLoss (regularization λ and loss threshold τ ) are either ﬁxed or tuned using held-out validation or cross-validation. More speciﬁcally, we experiment with three options for τ : (1) a global average of the loss so far, denoted as ‘Avg’; (2) an exponential running average with a ﬁxed smoothing parameter
α = 0.9, denoted as ‘ExpAvg’; or (3) a ﬁxed value given by prior knowledge on the task at hand.
Similar to SELF [6], we smooth the input loss (cid:96)i individually for each sample using exponential averaging with α(cid:48) = 0.9, as it makes the training more stable. This strategy is only applicable for limited size datasets; for metric learning or object detection, we do not use it. 3.2 Regression
We ﬁrst evaluate our SuperLoss on digit regression on MNIST and human age regression on UTKFace, with both a robust loss (smooth-(cid:96)1) and a non-robust one ((cid:96)2), and with different noise levels.
Digit regression. We perform a toy regression experiment on MNIST [26] by considering the original digit classiﬁcation problem as a regression problem. Speciﬁcally, we set the output dimension of
LeNet [26] to 1 instead of 10 and train it using a regression loss for 20 epochs using SGD (Stochastic
Gradient Descent). We cross-validate the hyper-parameters of the baseline for each loss and noise level. Typically, (cid:96)2 prefers a lower learning rate compared to smooth-(cid:96)1. For the SuperLoss, we experiment with a ﬁxed threshold τ = 0.5 as it is the acceptable bound for regressing the right integer.
Age regression. We experiment on the larger UTKFace dataset [70] which consists of 23,705 aligned and cropped face images, randomly split into 90% for training and 10% for testing. Races, genders and ages (between 1 to 116 years old) widely vary and are represented in imbalanced proportions, making the age regression task challenging. We use a ResNet-18 model (with a single output) initialized on ImageNet as predictor and train for 100 epochs using SGD. Likewise, we cross-validate the hyper-parameters for each loss and noise level. Because it is not clear which ﬁxed threshold would be optimal for this task, we do not use a ﬁxed threshold in the SuperLoss.
Results. To evaluate the impact of noise when training, we generate it artiﬁcially using a uniform distribution between 1 and 10 for digits and between 1 and 116 for ages. We report the mean absolute error (MAE) aggregated over 5 runs for both datasets and both losses with varying noise proportions in Figure 4. Models trained using the SuperLoss consistently outperform the baseline by a signiﬁcant margin, regardless of the noise level or the τ threshold. This is particularly true when the network is trained with a non-robust loss ((cid:96)2), suggesting that the SuperLoss makes a non-robust loss more robust.
Even when the baseline is trained using a robust loss (smooth-(cid:96)1), the SuperLoss still signiﬁcantly reduces the error (e.g., from 17.56 ± 0.33 to 13.09 ± 0.05 on UTKFace at 80% noise). Note that the two losses have drastically different ranges of amplitudes depending on the task (e.g., (cid:96)2 for age regression typically ranges in [0, 10000] while smooth-(cid:96)1 for digit regression ranges in [0, 10]). 6
Figure 5: Evolution of the normalized conﬁdence σ∗ from Equation (2) during training (median value and 25-75% percentiles). We arbitrarily deﬁne hard samples as correct samples failing to reach high conﬁdence within the ﬁrst 20 epochs of training.
Results on CIFAR-10
Results on CIFAR-100 y c a r u c c
A 80 60 40 20 0.0 0.2 0.4
Proportion of false labels 0.6 y c a r u c c
A 80 70 60 50 40 30 20 10 0.8 0.0 0.2 0.4
Proportion of false labels 0.6
Focal Loss [33] (2016)
Self-paced [25] (2010)
MentorNet DD [18] (2017)
Co-teaching [15] (2018)
Co-teaching+ [62] (2019)
Lq [69] (2018)
Trunc-Lq [69] (2018)
D2L [35] (2018)
Abstention [53] (2019)
CurriculumNet [49] (2019)
F-correction [39] (2017)
P-correction [60] (2019)
Meta-Learning [29] (2019)
Mixup [66] (2018)
O2U-Net [19] (2019)
Data Parameters [48] (2019)
SuperLoss (τ =2.3)
SuperLoss (τ =Avg)
SuperLoss (τ =ExpAvg) 0.8
Figure 6: Accuracy on CIFAR-10 and CIFAR-100 as a function of the proportion of noise for our
SuperLoss and the state of the art. 3.3
Image classiﬁcation
We next evaluate our SuperLoss for the image classiﬁcation task on CIFAR-10, CIFAR-100 and
WebVision.
CIFAR-10 and CIFAR-100 [24] consist of 50K training and 10K test images belonging to C = 10 and C = 100 classes respectively. We train a WideResNet-28-10 model [63] with the SuperLoss, strictly following the experimental settings and protocol from Saxena et al. [48] for comparison purpose. We set the regularization parameter to λ = 1 for CIFAR-10 and to λ = 0.25 for CIFAR-100.
We plot in Figure 5 the evolution of the conﬁdence σ∗
λ from Equation (2) for easy, hard and noisy samples. As training progresses, noisy and hard samples get more clearly separated.
We report our results (averaged over 5 runs) for different proportions of corrupted labels in Figure 6, as well as results from the state of the art. Once again, we observe very similar performance regardless of τ (either ﬁxed to log(C) or using automatic averaging). On clean datasets, the SuperLoss slightly improves over the baseline (e.g., from 95.8% ± 0.1 to 96.0% ± 0.1 on CIFAR-10) even though the performance is quite saturated. In the presence of symmetric noise, the SuperLoss performs better or on par compared to all recent approaches that we are aware of, at the exception of [6, 28]. In particular, our method slightly outperforms the conﬁdence-aware loss proposed by Saxena et al. [48] in fair settings, conﬁrming that conﬁdence parameters indeed do not need to be learned. For instance, using global averaging for τ , our method obtains 91.55% ± 0.33 and 71.05% ± 0.08 accuracy under 40% label noise on CIFAR-10 and CIFAR-100 respectively, compared to 91.10% ± 0.70 and 70.93%
± 0.15 for [48]. Finally, note that our method outperforms much more complex and specialized approaches, even though we do not particularly target classiﬁcation, nor require any change in the network, nor use a special training procedure.
WebVision [31] is a large-scale dataset of 2.4 million images with C = 1000 classes, automatically gathered from the web by querying search engines with the class names. It thus inherently contains a signiﬁcant level of noise. We follow [48] and train a ResNet-18 model using SGD for 120 epochs with a weight decay of 10−4, an initial learning rate of 0.1, divided by 10 at 30, 60 and 90 epochs.
The regularization parameter for the SuperLoss is set to λ = 0.25 and we use a ﬁxed threshold for τ = log(C). The ﬁnal accuracy is 66.7% ± 0.1 which represents a consistent gain of +1.2% (aggregated over 4 runs) compared to the baseline (65.5% ± 0.1). We point out that this gain is for free as the SuperLoss does not require any change in terms of training time or engineering efforts. 7
label noise 0% 20% 40% 60% t baseline e
N a n i t e
R
N
N
C
R
-r e t s a
F
SuperLoss (τ =ExpAvg)
SuperLoss (τ =Avg) baseline
SuperLoss (τ =log(C))
SuperLoss (τ =ExpAvg)
Co-teaching [15]†
SD-LocNet [68]†
Note RCNNN [9]†
CA-BBC [30]† 80.6 80.5 80.7 81.4 81.4 81.0 78.3 78.0 78.6 80.1 77.5 78.1 78.0 76.9 79.5 78.4 76.5 75.3 75.3 79.1 74.6 75.3 75.2 73.6 78.1 77.0 74.1 73.0 74.9 77.7 52.0 59.6 59.7 69.5 74.9 73.8 69.9 66.2 69.9 74.1
Figure 7: AP50 on Pascal VOC when using the SuperLoss for object detection with Faster R-CNN (left) and RetinaNet (middle), we report mean and standard deviation over 3 runs. The right table shows a comparison to the state of the art (AP50 metric). † denotes numbers reported from [30]. 3.4 Object detection
We perform experiments for the object detection task on Pascal VOC [7] and its noisy version from [30] where symmetric label noise is applied to 20%, 40% or 60% of the instances. We use two object detection frameworks from detectron21: Faster R-CNN [45] and RetinaNet [33]. Figure 7 shows the standard AP50 metric for varying levels of noise using the standard box classiﬁcation loss or the SuperLoss (more metrics are in the Supplementary). While the baseline and the SuperLoss are on par on clean data, the SuperLoss again signiﬁcantly outperforms the baseline in the presence of noise. For instance, the performance drop at 60% of label noise is reduced by 5 points for Faster
R-CNN, and by 9 points for Retina-Net. For τ , we observe a slight edge for τ = log(C) with Faster
R-CNN. The same ﬁxed threshold makes no sense for RetinaNet as it does not rely on cross-entropy loss, but we observe that global and exponential averaging perform similarly. In Figure 7 (right), we compare our method to some state-of-the-art noise-robust approaches [9, 15, 30, 68]. Once again our simple and generic SuperLoss outperforms other approaches leveraging complex strategies to identify and/or correct noisy samples. 3.5
Image retrieval
We evaluate the SuperLoss on the image retrieval task using the Revisited Oxford and Paris bench-mark [42]. To train our method, we use the large-scale Landmarks dataset [2] that is composed of about 200K images (divided into 160K/40K for training/validation) gathered semi-automatically using search engines. The fact that a cleaned version of the same dataset (released in [12]) comprises about 4 times less images gives a rough idea of the tremendous amount of noise it contains, and of the subsequent difﬁculty to leverage this data using standard loss functions. In order to establish a meaningful comparison we also experiment with the cleaned dataset [12] which comprises 42K training and 6K validation images. Following [12], we refer to these datasets as Landmarks-full and
Landmarks-clean. As retrieval model, we use ResNet-50 with Generalized-Mean (GeM) pooling [43] and a contrastive loss [14]2. When training on Landmarks-clean, the default hyper-parameters from [42] for the optimizer and the hard-negative mining procedure gives excellent results (i.e. 100 epochs, learning rate of 10−6 with exponential decay of exp(−1/100), 2000 queries per epoch and 20K negative pool size). In contrast, they lead to poor performance when training on Landmarks-full.
We thus retune the hyper-parameters for the baseline on the validation set of Landmarks-full and ﬁnd that reducing the hard negative mining hyper-parameters is important (200 queries and pool size of 500 negatives). In all cases, we train the SuperLoss with the same settings than the baseline using global averaging for τ . At test time, we follow [42] and use multiple scales and descriptor whitening.
Results. We report the mean Average Precision (mAP) in Table 1. On clean data, the SuperLoss has minor impact. However, it enables an impressive performance boost on noisy data (Landmarks-full), overall outperforming the baseline trained using clean data. This result shows that our SuperLoss makes it possible to train a model from a large automatically-collected dataset with a better perfor-mance than from a manually labelled subset. We also include state-of-the-art results trained and evaluated with identical code [42] at the end of Table 1. We perform slightly better than ResNet-1https://github.com/facebookresearch/detectron2 2https://github.com/filipradenovic/cnnimageretrieval-pytorch 8
Table 1: Image retrieval results (mAP) for different training sets and losses. Hard-neg indicates the couple of hyper-parameters (query size, pool size) used for hard-negative mining.
Loss
Hard-neg ROxf (M) ROxf (H) RPar (M) RPar (H) Avg
Network + pooling
Training set
ResNet-50+GeM
Landmarks-clean
Landmarks-full contrastive
SuperLoss contrastive contrastive
SuperLoss 2K,20K 2K,20K 2K,20K 200,500 200,500
ResNet-101+GeM [42]
SfM-120k (clean) contrastive 2K,20K 61.1 61.3 41.9 54.4 62.7 65.4 33.3 33.3 17.5 28.7 38.1 40.1 77.2 77.0 65.0 72.6 77.0 76.7 57.2 57.0 39.4 50.2 56.5 55.2 57.2 57.2 41.0 51.4 58.6 59.3 101+GeM on RParis despite the deeper backbone and the fact that it is trained on SfM-120k, a clean dataset of comparable size requiring a complex and expensive procedure to collect [43]. 4