Abstract
We introduce a sampling perspective to tackle the challenging task of training robust Reinforcement Learning (RL) agents. Leveraging the powerful Stochastic
Gradient Langevin Dynamics, we present a novel, scalable two-player RL algo-rithm, which is a sampling variant of the two-player policy gradient method. Our algorithm consistently outperforms existing baselines, in terms of generalization across different training and testing conditions, on several MuJoCo environments.
Our experiments also show that, even for objective functions that entirely ignore potential environmental shifts, our sampling approach remains highly robust in comparison to standard RL algorithms. 1

Introduction
Reinforcement learning (RL) promise automated solutions to many real-world tasks with beyond-human performance. Indeed, recent advances in policy gradient methods [1, 2, 3, 4] and deep reinforcement learning have demonstrated impressive performance in games [5, 6], continuous control [7], and robotics [8].
Despite the success of deep RL, the progress is still upset by the fragility in real-life deployments.
In particular, the majority of these methods fail to perform well when there is some difference between training and testing scenarios, thereby posting serious safety and security concerns. To this end, learning policies that are robust to environmental shifts, mismatched conﬁgurations, and even mismatched control actions are becoming increasingly more important.
A powerful framework to learning robust policies is to interpret the changing of the environment as an adversarial perturbation. This notion naturally lends itself to a two-player max-min problem involving a pair of agents, a protagonist and an adversary, where the protagonist learns to fulﬁll the original task goals while being robust to the disruptions generated by its adversary. Two prominent examples along this research vein, differing in how they model the adversary, are the Robust Adversarial
Reinforcement Learning (RARL) [9] and Noisy Robust Markov Decision Process (NR-MDP) [10].
Despite the impressive empirical progress, the training of the robust RL objectives remains an open and critical challenge. In particular, [10] prove that it is in fact strictly suboptimal to directly apply (deterministic) policy gradient steps to their NR-MDP max-min objectives. Owing to the lack of a better algorithm, the policy gradient is nonetheless still employed in their experiments; similar comments also apply to [9].
∗Work done while Parameswaran Kamalaruban and Cheng Shi were working at LIONS, EPFL. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The main difﬁculty originates from the highly non-convex-concave nature of the robust RL objectives, posing signiﬁcant burdens to all optimization methods. In game-theoretical terms, these methods search for pure Nash Equilibria (pure NE) which might not even exist [11]. Worse, even when pure NE are well-deﬁned, we show that optimization methods can still get stuck at non-equilibrium stationary points on certain extremely simple non-convex-concave objectives; cf. Section 4.
In this paper, we contend that, instead of viewing robust RL as a max-min optimization problem, the sampling perspective [12] from the so-called mixed Nash Equilibrium (mixed NE) presents a potential solution to the grand challenge of training robust RL agents. We substantiate our claim by demonstrating the advantages of sampling algorithms over-optimization methods on three fronts: 1. We show in Section 4 that, even in stylized examples that trap the common optimiza-tion methods, the sampling algorithms can still make progress towards the optimum in expectation, even tracking the NE points. 2. We conduct extensive experiments to show that sampling algorithms consistently outper-form state-of-the-arts in training robust RL agents. Moreover, our experiments on the
MuJoCo dataset reveal that sampling algorithms are able to handle previous failure cases of optimization methods, such as the inverted pendulum. 3. Finally, we provide strong empirical evidence that sampling algorithms are inherently more robust than optimization methods for RL. Speciﬁcally, we apply sampling algorithms to train an RL agent with non-robust objective (i.e., the standard expected cumulative reward maximizing objective in RL), and we compare against the policy learned by optimizing the robust objective (i.e., the max-min formulation). Despite the disadvantage, our results show that the sampling algorithms still achieve comparable or better performance than optimization methods (cf. Appendix C). 2