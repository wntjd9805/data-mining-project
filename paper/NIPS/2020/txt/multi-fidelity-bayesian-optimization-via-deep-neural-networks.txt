Abstract
Bayesian optimization (BO) is a popular framework for optimizing black-box functions. In many applications, the objective function can be evaluated at mul-tiple ﬁdelities to enable a trade-off between the cost and accuracy. To reduce the optimization cost, many multi-ﬁdelity BO methods have been proposed. Despite their success, these methods either ignore or over-simplify the strong, complex correlations across the ﬁdelities. While the acquisition function is therefore easy and convenient to calculate, these methods can be inefﬁcient in estimating the objective function. To address this issue, we propose Deep Neural Network Multi-Fidelity Bayesian Optimization (DNN-MFBO) that can ﬂexibly capture all kinds of complicated relationships between the ﬁdelities to improve the objective function es-timation and hence the optimization performance. We use sequential, ﬁdelity-wise
Gauss-Hermite quadrature and moment-matching to compute a mutual information based acquisition function in a tractable and highly efﬁcient way. We show the advantages of our method in both synthetic benchmark datasets and real-world applications in engineering design. 1

Introduction
Bayesian optimization (BO) (Mockus et al., 1978; Snoek et al., 2012) is a general and powerful approach for optimizing black-box functions. It uses a probabilistic surrogate model (typically
Gaussian process (GP) (Rasmussen and Williams, 2006)) to estimate the objective function. By repeatedly maximizing an acquisition function computed with the information of the surrogate model,
BO ﬁnds and queries at new input locations that are closer and closer to the optimum; meanwhile the new training examples are incorporated into the surrogate model to improve the objective estimation.
In practice, many applications allow us to query the objective function at different ﬁdelities, where low ﬁdelity queries are cheap yet inaccurate, and high ﬁdelity queries more accurate but costly. For example, in physical simulation (Peherstorfer et al., 2018), the computation of an objective (e.g., the elasticity of a part or energy of a system) often involves solving partial differential equations. Running a numerical solver with coarse meshes gives a quick yet rough result; using dense meshes substantially improves the accuracy but dramatically increases the computational cost. The multi-ﬁdelity queries enable us to choose a trade-off between the cost and accuracy. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Accordingly, to reduce the optimization cost, many multi-ﬁdelity BO methods (Huang et al., 2006;
Lam et al., 2015; Kandasamy et al., 2016; Zhang et al., 2017; Takeno et al., 2019) have been proposed to jointly select the input locations and ﬁdelities to best balance the optimization progress and query cost, i.e., the beneﬁt-cost ratio. Despite their success, these methods often ignore the strong, complex correlations between the function outputs at different ﬁdelities, and learn an independent GP for each ﬁdelity (Lam et al., 2015; Kandasamy et al., 2016). Recent works use multi-output GPs to capture the ﬁdelity correlations. However, to avoid intractable computation of the acquisition function, they have to impose simpliﬁed correlation structures. For example, Takeno et al. (2019) assume a linear correlation between the ﬁdelities; Zhang et al. (2017) use kernel convolution to construct the cross-covariance function, and have to choose simple, smooth kernels (e.g., Gaussian) to ensure a tractable convolution. Therefore, the existing methods can be inefﬁcient and inaccurate in estimating the objective function, which further lowers the optimization efﬁciency and increases the cost.
To address these issues, we propose DNN-MFBO, a deep neural network based multi-ﬁdelity Bayesian optimization that is ﬂexible enough to capture all kinds of complex (possibly highly nonlinear and nonstationary) relationships between the ﬁdelities, and exploit these relationships to jointly estimate the objective function in all the ﬁdelities to improve the optimization performance. Speciﬁcally, we stack a set of neural networks (NNs) where each NN models one ﬁdelity. In each ﬁdelity, we feed both the original input (to the objective) and output from the previous ﬁdelity into the NN to propagate information throughout and to estimate the complex relationships across the ﬁdelities. Then, the most challenging part is the calculation of the acquisition function. For efﬁcient inference and tractable computation, we consider the NN weights in the output layer as random variables and all the other weights as hyper-parameters. We develop a stochastic variational learning algorithm to jointly estimate the posterior of the random weights and hyper-parameters. Next, we sequentially perform
Gauss-Hermite quadrature and moment matching to approximate the posterior and conditional posterior of the output in each ﬁdelity, based on which we calculate and optimize an information based acquisition function, which is not only computationally tractable and efﬁcient, but also conducts maximum entropy search (Wang and Jegelka, 2017), the state-of-the-art criterion in BO.
For evaluation, we examined DNN-MFBO in three benchmark functions and two real-world applica-tions in engineering design that requires physical simulations. The results consistently demonstrate that DNN-MFBO can optimize the objective function (in the highest ﬁdelity) more effectively, mean-while with smaller query cost, as compared with state-of-the-art multi-ﬁdelity and single ﬁdelity BO algorithms. 2