Abstract
We present a novel system that gets as an input, video frames of a musician playing the piano, and generates the music for that video. The generation of music from visual cues is a challenging problem and it is not clear whether it is an attainable goal at all. Our main aim in this work is to explore the plausibility of such a transformation and to identify cues and components able to carry the association of sounds with visual events. To achieve the transformation we built a full pipeline named ‘Audeo’ containing three components. We ﬁrst translate the video frames of the keyboard and the musician hand movements into raw mechanical musical symbolic representation Piano-Roll (Roll) for each video frame which represents the keys pressed at each time step. We then adapt the Roll to be amenable for audio synthesis by including temporal correlations. This step turns out to be critical for meaningful audio generation. In the last step, we implement Midi synthesizers to generate realistic music. Audeo converts video to audio smoothly and clearly with only a few setup constraints. We evaluate Audeo on piano performance videos collected from YouTube and obtain that their generated music is of reasonable audio quality and can be successfully recognized with high precision by popular music identiﬁcation software. The source code with examples is available in a
Github repository 3. 1

Introduction
Melody is the essence of music. I compare a good melodist to a ﬁne racer.
Wolfagang Amadeus Mozart
The perfect combination of a musician’s skills with the musical instrument tones creates the delightful experience of ‘live music’. Such an event is inspiring from the perspective of the melody being played and also from the perspective of witnessing admirable synchrony between the musician and the instrument.
What makes the musical performance to sound as it sounds? The answer to this question is intertwined.
We know many of the components that make musical performance sound well, but what we do not know is how to rigorously quantify the contribution of the components. Notes, tempo, consistency, timed precision, mechanical accurateness, rhythmic movements, harmonics, frequencies; all these and more delicately compose the melody of a musical piece. Quantifying these aspects plays a key role in the attempt to better understand how to generate realistic melodies.
A particular test which informs regarding music generation is to constitute the music (transcribe the music) from visual information, i.e., ﬁnding possible ways to recreate the audio stream of a musical performance just from the visual stream. In the case of a piano recording, that would be
∗Department of Electrical & Computer Engineering, University of Washington, Seattle, USA
†Department of Applied Mathematics, University of Washington, Seattle, USA 3https://github.com/shlizee/Audeo 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Given an input of video frames of musician playing the piano, Audeo generates the music for that video. Please also see supplementary video and materials with sample results. taking into account the positions of the musician’s hands, body, the positions of the keys and the pedals and merge them into music. Timed precision between visual cues and sounds is known to have a profound effect on such a task and takes the form of a far more complicated process than a mere synchronization. The reasons for the complexity stem from visual stream perception being of signiﬁcantly slower rate than the perception of an audio stream, however, the perception of their combination requires the latency between the audio and the video signals to be faster than the rate of the visual stream. This creates an effect in which for the generation of an audio signal for a video, one should not only ﬁnd an association between the video frames and the audio but also to precisely complete the audio stream in between the video frames going back and forth between the past and the future frames. Such completion is nontrivial and requires exhaustive knowledge of the instrument and body mechanics, i.e., a model of a virtual instrument, or an ability to imagine the details from the visual features, similar to a composer’s ability to envision the melody from reading musical notes.
Video frames include an abundance of visual information, some of which could be irrelevant to music.
Therefore, it is plausible that instead of a direct transformation, intermediate features could be used for the translation from video to audio. These features should capture the mechanical and the perceptual features of the interaction between the musician and the instrument and be constructive tools for sound representation and synthesis. For example, the Musical Instrument Digital Interface (Midi) protocol is a candidate signal. It is used to interchange musical information between instruments and encodes various keyboard functions and musical attributes. Variants of Midi, such as Pseudo-Midi (binary, without expressive velocities), will provide an even more compact version to encode keyboard function and musical attributes altogether. Moreover, connecting visual actions with frequencies of the audio signal as it varies with time, i.e., the Spectrogram, can be a useful mediator.
In this work, we address the challenge of music generation from video by proposing a full pipeline, named Audeo, to generate the audio of a silent piano performance video. Audeo translates the performance from the video domain to the audio domain in three stages, through the recovery of mediator signals. In the ﬁrst stage, given a top-view video, we use multi-scale feature attention deep residual network to capture the visual information and to predict which keys are pressed at each frame (Video2Roll Net). We formulate this as a multi-label classiﬁcation task, and the collection of predictions can be seen as a ‘Piano-Roll’ [1]. However, ‘Roll’ is still coarse binary prediction and does not directly correspond to Pseudo-Midi critical for music synthesis. Therefore, in the second stage, we utilize a Generative Adversarial Network (GAN)[2] to reﬁne and enhance the Roll with musical attributes to output the Pseudo-Midi signal (Roll2Midi Net). This step turns out to be critical for providing symbolic musical representation. The third and last stage of the Audeo pipeline is the synthesis of Pseudo-Midi to the audio signal (Midi Synth). Since the predicted Pseudo-Midi is binary and missing expressive velocities, we thereby propose to use the same velocity to synthesize a mechanical audio via a classical Midi synthesizer, or a deep synthesizer to obtain more realistic audio. The deep synthesizer translates Pseudo-Midi to a spectrogram and then to audio. An overview of the Audeo system is shown in Fig. 1. Our main contributions are the following: (i) To the best of our knowledge, we are the ﬁrst work to transcribe the music audio from silent piano performance videos that are not recorded in speciﬁc lab setting. (ii) We introduce a full pipeline, named Audeo, containing three interpretable components to complete this transformation. (iii) Audeo is robust and generalizable and we show that the output audio of piano performances that Audeo generates will be consistently detected by popular music identiﬁcation software as the expected musical piece. 2
2