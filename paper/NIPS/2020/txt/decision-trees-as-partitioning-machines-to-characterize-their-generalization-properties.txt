Abstract
Decision trees are popular machine learning models that are simple to build and easy to interpret. Even though algorithms to learn decision trees date back to almost 50 years, key properties affecting their generalization error are still weakly bounded. Hence, we revisit binary decision trees on real-valued features from the perspective of partitions of the data. We introduce the notion of partitioning function, and we relate it to the growth function and to the VC dimension. Using this new concept, we are able to ﬁnd the exact VC dimension of decision stumps, (cid:1), where (cid:96) is the number which is given by the largest integer d such that 2(cid:96) ≥ (cid:0) d 2 (cid:99) (cid:98) d of real-valued features. We provide a recursive expression to bound the partitioning functions, resulting in a upper bound on the growth function of any decision tree structure. This allows us to show that the VC dimension of a binary tree structure with N internal nodes is of order N log(N (cid:96)). Finally, we elaborate a pruning algorithm based on these results that performs better than the CART algorithm on a number of datasets, with the advantage that no cross-validation is required. 1

Introduction
Decision trees are popular decision models that are versatile, intuitive, and thus useful in critical
ﬁelds where the interpretability of a model is important. They are particularly useful when data is limited and not organized as in a sequence or a picture. This makes them a good alternative to deep neural networks in several cases.
Due to their expressive power, decision trees are prone to overﬁtting. To handle this problem, algorithms usually make use of practical techniques such as cross-validation in the learning or the pruning step. Unfortunately, cross-validation increases the running time of the learning algorithm and impairs the generalization of the tree when the number of training examples is small. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
As an alternative, one can use learning algorithms based on generalization bounds. Indeed, this approach has proven its value in the work of Drouin et al. [2019], where decision trees were learned on a genomic dataset with success by optimizing a sample-compression-based bound. Such bounds guarantee that the true risk is bounded asymptotically with high probability (ignoring logarithmic terms) in (cid:101)O( k+d m−d ), where k is the number of errors made by the tree, d is the size of a compressed sample and m is the size of the initial dataset [Marchand and Sokolova, 2005].
Relative deviation bounds based on the VC dimension [Vapnik, 1998, Shawe-Taylor et al., 1998] are even tighter: in (cid:101)O( k+d m ), where d is the VC dimension of the tree. However, to be able to make use of such algorithms to learn or prune decision trees, we must have a reasonable estimate of the VC dimension of a decision tree class, given its structure. To the best of our knowledge, there currently exists no upper bound on the VC dimension nor the growth function of binary decision trees with real-valued features that share a common structure. The goal of this paper is to provide such bounds.
To do so, we introduce the idea of a realizable partition and deﬁne the notion of partitioning function, a concept closely related to the growth function and the VC dimension. We proceed to bound tightly the partitioning function of the class of decision stumps that can be constructed from a set of real-valued features, which leads us, through the use of graph theory, to ﬁnd an exact expression of its VC dimension. To the best of our knowledge, this was previously unknown. We then extend our bound of the partitioning function to general binary decision tree structures, from which we derive the asymptotic behavior of the VC dimension of a tree with N internal nodes. Finally, we show how these results can have practical implications by developing a pruning algorithm based on our bounds that outperforms CART [Breiman et al., 1984] on a number of datasets. 2