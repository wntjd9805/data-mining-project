Abstract
Underpinning the success of deep learning is effective regularizations that allow a variety of priors in data to be modeled. For example, robustness to adversarial perturbations, and correlations between multiple modalities. However, most regu-larizers are speciﬁed in terms of hidden layer outputs, which are not themselves optimization variables. In contrast to prevalent methods that optimize them indi-rectly through model weights, we propose inserting proximal mapping as a new layer to the deep network, which directly and explicitly produces well regularized hidden layer outputs. The resulting technique is shown well connected to kernel warping and dropout, and novel algorithms were developed for robust temporal learning and multiview modeling, both outperforming state-of-the-art methods. 1

Introduction
The success of deep learning relies on massive neural networks that often considerably out-scale the training dataset, defying the conventional learning theory [1, 2]. Regularization has been shown essential and a variety of forms are available. For example, invariances to transformations such as rotation [3] have been extended beyond group-based diffeomorphisms to indecipherable transfor-mations that are only exempliﬁed by pairs of views [4], e.g., sentences uttered by the same person.
Prior regularities are also commonly available a) within layers of neural networks, such as sparsity
[5], spatial invariance in convolutional nets, structured gradient that accounts for data covariance [6]; b) between layers of representation, such as stability under dropout and adversarial perturbations of preceding layers [7], contractivity between layers [8], and correlations in hidden layers among multiple views [9, 10]; and c) at batch level, e.g., disentangled representation and multiple modalities.
The most prevalent approach to incorporating priors is regularization, which leads to the standard regularized risk minimization (RRM) for a given dataset D, empirical distribution ˜p, and loss (cid:96): minf Ex∼ ˜p[(cid:96)(f (x))] + Γ(f ) + (cid:88) i
Ωi({hi(x, f )}x∈D). (1)
Here f is the predictor (e.g., neural network), and Γ is the data-independent regularizer (e.g., L2 norm), and Ωi is the data-dependent regularizer on the i-th layer output hi under f (e.g., invariance of hi with respect to the i-th step input xi in an RNN). Note Ωi can involve multiple layers (e.g., contractivity), or be decomposed over training examples. Optimization techniques such as end-to-end training have produced strong performance, along with progresses in the global analysis of the solution [e.g., 11]. However, all these analyses make assumptions on the landscape of the objective function, which, although often satisﬁed by the empirical risk Ex∼ ˜p[(cid:96)(f (x))], are typically violated or complicated by the addition of data-dependant regularizers Ωi. The nontrivial contention between accurate prediction and faithful regularization can often confound the optimization of model weights.
A natural question therefore arises: is it possible to further improve the effectiveness of regulariza-tion, potentially not only through the development of new solvers and analysis for RRM, but also through novel mechanisms of incorporating regularization? Although the former approach has been studied intensively, we hypothesize and will demonstrate empirically that the latter approach can 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Original two-moon dataset (b) Representation after prox-map (c) Contour to (cid:7) after prox-map
Figure 1: (a) The two-moon dataset with only two labeled examples ‘+’ and ‘−’ (left and right), but abundant unlabeled examples that reveal the inherent structure; (b) Representation inferred from top-2 kernel PCA based on the proximal mapping with gradient ﬂatness and Gaussian kernel (see
§3); (c) contour of distance to the leftmost point (cid:7), based on the result of proximal mapping. be surprisingly effective. Our key intuition is, now that Ωi is speciﬁed in terms of the hidden layer output hi (which is determined by f ), can we directly optimize hi as opposed to indirectly through f ? Treating hi as ground variables and optimizing them jointly with model weights has been used by
[12]. However, their motivation is on accelerating the optimization rather than improving the model.
It turns out that this idea can be conveniently implemented by leveraging the tool of proximal mapping (hence the name ProxNet), which has been extensively used in optimization to enforce structured solutions such as sparsity [13]. Given a closed convex set C ⊆ Rn and a convex function R : Rn → R which favor certain desirable prior (e.g., (cid:96)1 norm), the proximal mapping PR : Rn → Rn is deﬁned as
PR(x) := arg minz∈C{R(z) + λ 2 (cid:107)z − x(cid:107)2}, where the norm is L2. (2)
In essence, R and C encourage the mapping to respect the prior encoded by R, while remaining in the vicinity of x. For example, Figure 1a shows the two-moon dataset with only two labeled examples and many unlabeled ones. Figures 1b and 1c show the resulting representation and warped distance where R accounts for the underlying manifold, making the classiﬁcation trivial (§3).
In a deep network, the proximal mapping can be inserted after any layer to turn hi into PΩi(hi), and backpropagate through it. Why does this yield a more effective implementation of regularization?
First of all, it provides the modularity of decoupling regularization from supervised learning — the regularization is encapsulated within the proximal layer that is free of weights, and the resulting
PΩi(hi) is directly enforced to comply with the prior rather than indirectly through the optimization of weights in f . This frees weight optimization from simultaneously catering to unsupervised structures and supervised performance metrics, which plagues the conventional RRM. Such an advantage will be conﬁrmed in our experiments of end-to-end training that are highly efﬁcient (§5.1).
Secondly, proximal mapping can be interpreted as an intermediate step of denoising, where PΩi(hi) is a cleaned version of hi that conforms to the prior. This ensures that the downstream layers are presented with well regularized inputs, which will presumably facilitate their own learning. By gradually increasing λ, such a manual morphing can be annealed, allowing the upstream layers (e.g., feature extractors) to approach weight values that by themselves produce well-regularized hi.
ProxNet is also readily connected with meta-learning (§B) because of the bi-level optimization setup, where the proximal layer plays a similar role to base-learners.
Finally, PΩi (hi) can be carried out on a mini-batch B, where R is deﬁned on a set {hi(x)}x∈B. It also extends ﬂexibly to regularizers that account for multiple layers, e.g., invariance of hi to hi−1.
This paper will ﬁrst review the existing building blocks of deep networks through the lens of proximal mapping (§2), and then unravel its non-trivial connections with regularization when the latter is quadratic (e.g., manifold smoothness) or non-quadratic (e.g., dropout). Afterwards, two novel
ProxNets will be introduced that achieve robust recurrent modeling (§4) and multiview learning (§5).
Extensive experiments show that ProxNet outperforms state-of-the-art prediction models (§6).