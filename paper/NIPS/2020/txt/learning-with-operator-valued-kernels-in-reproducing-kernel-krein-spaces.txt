Abstract
Operator-valued kernels have shown promise in supervised learning problems with functional inputs and functional outputs. The crucial (and possibly restrictive) assumption of positive deﬁniteness of operator-valued kernels has been instrumental in developing efﬁcient algorithms. In this work, we consider operator-valued kernels which might not be necessarily positive deﬁnite. To tackle the indeﬁniteness of operator-valued kernels, we harness the machinery of Reproducing Kernel Krein
Spaces (RKKS) of function-valued functions. A representer theorem is illustrated which yields a suitable loss stabilization problem for supervised learning with function-valued inputs and outputs. Analysis of generalization properties of the proposed framework is given. An iterative Operator based Minimum Residual (OpMINRES) algorithm is proposed for solving the loss stabilization problem.
Experiments with indeﬁnite operator-valued kernels on synthetic and real data sets demonstrate the utility of the proposed approach. 1

Introduction
We consider the problem of learning a function-valued function F : X → Y between an input space X and an output space Y of functions. Sometimes this problem is called functional regression (Morris, 2015). Several applications (e.g. audio-visual apps, weather forecasting) motivate the need for considering data as functions. Though practical data is typically discrete, the need to consider inherent time-based correlations and its potential smoothness might be fruitful (Ramsay and
Silverman, 2007; Kokoszka and Reimherr, 2018). Among the machine learning methods to solve the functional regression problem, we are interested in the functional reproducing kernel Hilbert space (functional RKHS) idea introduced in (Lian, 2007) and substantially developed in (Kadri et al., 2016). Functional RKHS extends the RKHS framework popularly used for multivariate data (Schölkopf et al., 1999) to functional data. Similar to RKHS which is associated with a non-negative (or positive) scalar-valued kernel with the so-called reproducing property, a representer theorem for functional RKHS allows it to be associated with a corresponding non-negative (or positive deﬁnite) operator-valued kernel with reproducing property (see (Lian, 2007) and Appendix A). However construction of non-negative or positive deﬁnite operator-valued kernels is not straightforward and particular examples with separable structure are provided in (Lian, 2007; Kadri et al., 2016). The positive deﬁniteness of operator-valued kernels is crucial for establishing technical results associated with functional RKHS and also helps in designing efﬁcient algorithms (Lian, 2007; Kadri et al., 2016).
Note that demonstrating the positive deﬁniteness property of operator-valued kernels (even for particular cases) might be a difﬁcult exercise in itself. Demanding the non-negativeness or positive deﬁniteness of operator-valued kernels effectively restricts practitioners from trying other useful operator-valued kernels which might be indeﬁnite, yet potentially useful for some applications (e.g. similarity computation between function-valued data can involve indeﬁnite operator-valued kernels). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Similar concerns previously raised in the case of scalar-valued kernels (e.g. see (Ong et al., 2004)), have led to interesting theory establishing a counterpart of RKHS, namely the reproducing kernel
Krein space (RKKS) suitable for non-positive kernels of certain type (Ong et al., 2004; Oglic and
Gärtner, 2018). Here, we embark on a similar pursuit to develop the necessary theoretical tools which would help construct a function-valued RKKS for generalized operator-valued kernels which might not be non-negative. The structure of generalized operator-valued kernels may seem as an extension of generalized scalar-valued kernels considered in (Ong et al., 2004), however dealing with operator-valued nature of the kernels brings in challenges. Designing a suitable algorithmic scheme to make the framework of generalized operator-valued kernels useful for practical applications is also challenging. Therefore, a systematic development and study of generalized operator-valued kernels and related algorithms become imperative. We aim to address these objectives in this work and outline our major contributions below.
Contributions: We introduce the concepts of generalized operator-valued kernel (which might be indeﬁnite) and function-valued RKKS. We show the relevant properties required to associate function-valued RKKS with generalized operator-valued kernels. We remark that demonstrating the existence of an associated RKKS for a generalized operator-valued kernel (more speciﬁcally, deriving Lemma 2.3 and Corollary 2.3.1 leading to the proof of Theorem 2.4 below) is mathematically challenging.
We then cast the functional regression problem over function-valued RKKS in an appropriate learning setup using a regularized empirical loss stabilization formulation. We further prove a representer theorem for the function-valued RKKS which yields a tractable solution of the loss stabilization problem. To make the theoretical framework useful for practical scenarios, we devise an iterative
Krylov subspace method called Operator MINimum RESidual method (OpMINRES) to solve the loss stabilization problem. Further, using an appropriate Rademacher average, we provide technical results on generalization properties of the proposed learning setup. To the best of our knowledge, the technical results connecting the framework of generalized operator-valued kernel and its associated function-valued RKKS, and the proposed OpMINRES algorithmic scheme are new. An extensive empirical evaluation on real data and comparison with benchmark methods demonstrate that the proposed learning framework is competitive, while allowing for the ﬂexibility of using indeﬁnite operator-valued kernels in functional data settings.
Paper organization: Generalized operator-valued kernels and function-valued RKKS are introduced and their properties are discussed in Section 2. We formulate a regularized loss stabilization learning problem and furnish a representer theorem for function-valued RKKS in Section 3. The iterative
OpMINRES algorithm used to solve the loss stabilization problem is illustrated in Section 4. Bounds on the generalization error are established in Section 5.