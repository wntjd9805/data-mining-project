Abstract
Diverse image captioning models aim to learn one-to-many mappings that are innate to cross-domain datasets, such as of images and texts. Current methods for this task are based on generative latent variable models, e.g. VAEs with structured latent spaces. Yet, the amount of multimodality captured by prior work is limited to that of the paired training data – the true diversity of the underlying generative process is not fully captured. To address this limitation, we leverage the contextual descriptions in the dataset that explain similar contexts in different visual scenes.
To this end, we introduce a novel factorization of the latent space, termed context-object split, to model diversity in contextual descriptions across images and texts within the dataset. Our framework1 not only enables diverse captioning through context-based pseudo supervision, but extends this to images with novel objects and without paired captions in the training data. We evaluate our COS-CVAE approach on the standard COCO dataset and on the held-out COCO dataset consisting of images with novel objects, showing signiﬁcant gains in accuracy and diversity. 1

Introduction
Modeling cross-domain relations such as that of images and texts ﬁnds application in many real-world tasks such as image captioning [5, 9, 20, 23, 26, 31, 33, 48]. Many-to-many relationships are innate to such cross-domain tasks, where a data point in one domain can have multiple pos-sible correspondences in the other domain and vice-versa.
In particular for image captioning, given an image there are many likely sentences that can describe the un-derlying semantics of the image. It is thus desirable to model this multi-modal conditional distribution of cap-tions (texts) given an image to gener-ate plausible, varied captions.
Figure 1: Context-object split latent space of our COS-CVAE to ex-ploit similarities in the contextual annotations for diverse captioning.
Recent work has, therefore, explored generating multiple captions condi-tioned on an image to capture these diverse relationships in a (condi-tional) variational framework [6, 14, 32, 42]. These variational frameworks encode the conditional distribution of texts given the image in a low-dimensional latent space, allowing for efﬁcient sampling.
However, the diversity captured by current state-of-the-art latent variable models, e.g. [6, 42], is limited to the paired annotations provided for an image. While the recently proposed LNFMM approach [32] allows for additional unpaired data of images or texts to be incorporated, the amount of diversity for a given image is again limited to the paired annotated data. 1Code available at https://github.com/visinf/cos-cvae 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Since standard learning objectives reward only the generated captions that belong to ground-truth annotations, the underlying multimodality of the conditional image and text distributions in the latent space can be better captured with access to more annotated paired data. To this end, the initial work of
Kalyan et al. [21] improves multimodality of the input-output mappings with additional supervision from annotations of ‘nearby’ images. However, Kalyan et al. model one-to-many relationships in the posterior distribution, where the sampling time grows exponentially in the sequence length. Thus beam search is required for sampling diverse captions, which makes it computationally inefﬁcient [5].
In this work, we propose a novel factorized latent variable model, termed context-object split conditional variational autoencoder (COS-CVAE), to encode object and contextual information for image-text pairs in a factorized latent space (Fig. 1). We make the following contributions: (i) Our
COS-CVAE framework exploits contextual similarities between the images and captions within the dataset in a pseudo-supervised setup. Speciﬁcally, the COS factorization in addition to annotated paired data leverages diverse contextual descriptions from captions of images that share similar contextual information, thus encoding the differences in human captions that can be attributed to the many ways of describing the contexts. (ii) This additionally allows extending COS-CVAE to previously unseen (novel) objects in images. To the best of our knowledge, this is the ﬁrst (variational) framework that allows to describe images with novel objects in a setup for diverse caption generation. (iii) We show the beneﬁts of our approach on the COCO dataset [29], with a signiﬁcant boost in accuracy and diversity. Moreover, varied samples can be efﬁciently generated in parallel. 2