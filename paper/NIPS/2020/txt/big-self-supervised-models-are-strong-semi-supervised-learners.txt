Abstract
One paradigm for learning from few labeled examples while making best use of a large amount of unlabeled data is unsupervised pretraining followed by super-vised ﬁne-tuning. Although this paradigm uses unlabeled data in a task-agnostic way, in contrast to common approaches to semi-supervised learning for computer vision, we show that it is surprisingly effective for semi-supervised learning on
ImageNet. A key ingredient of our approach is the use of big (deep and wide) networks during pretraining and ﬁne-tuning. We ﬁnd that, the fewer the labels, the more this approach (task-agnostic use of unlabeled data) beneﬁts from a bigger network. After ﬁne-tuning, the big network can be further improved and distilled into a much smaller one with little loss in classiﬁcation accuracy by using the unlabeled examples for a second time, but in a task-speciﬁc way. The proposed semi-supervised learning algorithm can be summarized in three steps: unsupervised pretraining of a big ResNet model using SimCLRv2, supervised ﬁne-tuning on a few labeled examples, and distillation with unlabeled examples for reﬁning and transferring the task-speciﬁc knowledge. This procedure achieves 73.9% ImageNet top-1 accuracy with just 1% of the labels (≤13 labeled images per class) using
ResNet-50, a 10× improvement in label efﬁciency over the previous state-of-the-art. With 10% of labels, ResNet-50 trained with our method achieves 77.5% top-1 accuracy, outperforming standard supervised training with all of the labels. 1 1

Introduction
Figure 1: Bigger models yield larger gains when ﬁne-tuning with fewer labeled examples.
Figure 2: Top-1 accuracy of previous state-of-the-art (SOTA) meth-ods [1, 2] and our method (SimCLRv2) on ImageNet using only 1% or 10% of the labels. Dashed line denotes fully supervised ResNet-50 trained with 100% of labels. Full comparisons in Table 3.
Learning from just a few labeled examples while making best use of a large amount of unlabeled data is a long-standing problem in machine learning. One approach to semi-supervised learning involves unsupervised or self-supervised pretraining, followed by supervised ﬁne-tuning [3, 4]. This
Correspondence to: iamtingchen@google.com 1Code and pretrained checkpoints are available at https://github.com/google-research/simclr. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
approach leverages unlabeled data in a task-agnostic way during pretraining, as the supervised labels are only used during ﬁne-tuning. Although it has received little attention in computer vision, this approach has become predominant in natural language processing, where one ﬁrst trains a large language model on unlabeled text (e.g., Wikipedia), and then ﬁne-tunes the model on a few labeled examples [5–10]. An alternative approach, common in computer vision, directly leverages unlabeled data during supervised learning, as a form of regularization. This approach uses unlabeled data in a task-speciﬁc way to encourage class label prediction consistency on unlabeled data among different models [11, 12, 2] or under different data augmentations [13–15].
Motivated by recent advances in self-supervised learning of visual representations [16–20, 1], this paper ﬁrst presents a thorough investigation of the “unsupervised pretrain, supervised ﬁne-tune” paradigm for semi-supervised learning on ImageNet [21]. During self-supervised pretraining, images are used without class labels (in a task-agnostic way), hence the representations are not directly tailored to a speciﬁc classiﬁcation task. With this task-agnostic use of unlabeled data, we ﬁnd that network size is important: Using a big (deep and wide) neural network for self-supervised pretraining and ﬁne-tuning greatly improves accuracy. In addition to the network size, we characterize a few important design choices for contrastive representation learning that beneﬁt supervised ﬁne-tuning and semi-supervised learning.
Once a convolutional network is pretrained and ﬁne-tuned, we ﬁnd that its task-speciﬁc predictions can be further improved and distilled into a smaller network. To this end, we make use of unlabeled data for a second time to encourage the student network to mimic the teacher network’s label predictions. Thus, the distillation [22, 23] phase of our method using unlabeled data is reminiscent of the use of pseudo labels [11] in self-training [24, 12], but without much extra complexity.
In summary, the proposed semi-supervised learning framework comprises three steps as shown in Fig-ure 3: (1) unsupervised or self-supervised pretraining, (2) supervised ﬁne-tuning, and (3) distillation using unlabeled data. We develop an improved variant of a recently proposed contrastive learning framework, SimCLR [1], for unsupervised pretraining of a ResNet architecture [25]. We call this framework SimCLRv2. We assess the effectiveness of our method on ImageNet ILSVRC-2012 [21] with only 1% and 10% of the labeled images available. Our main ﬁndings and contributions can be summarized as follows:
• Our empirical results suggest that for semi-supervised learning (via the task-agnostic use of unlabeled data), the fewer the labels, the more it is possible to beneﬁt from a bigger model (Figure 1). Bigger self-supervised models are more label efﬁcient, performing signiﬁcantly better when ﬁne-tuned on only a few labeled examples, even though they have more capacity to potentially overﬁt.
• We show that although big models are important for learning general (visual) representations, the extra capacity may not be necessary when a speciﬁc target task is concerned. Therefore, with the task-speciﬁc use of unlabeled data, the predictive performance of the model can be further improved and transferred into a smaller network.
• We further demonstrate the importance of the nonlinear transformation (a.k.a. projection head) after convolutional layers used in SimCLR for semi-supervised learning. A deeper projection head not only improves the representation quality measured by linear evaluation, but also improves semi-supervised performance when ﬁne-tuning from a middle layer of the projection head.
We combine these ﬁndings to achieve a new state-of-the-art in semi-supervised learning on ImageNet as summarized in Figure 2. Under the linear evaluation protocol, SimCLRv2 achieves 79.8% top-1 accuracy, a 4.3% relative improvement over the previous state-of-the-art [1]. When ﬁne-tuned on only 1% / 10% of labeled examples and distilled to the same architecture using unlabeled examples, it achieves 76.6% / 80.9% top-1 accuracy, which is a 21.6% / 8.7% relative improvement over previous state-of-the-art. With distillation, these improvements can also be transferred to smaller ResNet-50 networks to achieve 73.9% / 77.5% top-1 accuracy using 1% / 10% of labels. By comparison, a standard supervised ResNet-50 trained on all of labeled images achieves a top-1 accuracy of 76.6%. 2 Method
Inspired by the recent successes of learning from unlabeled data [19, 20, 1, 11, 24, 12], the proposed semi-supervised learning framework leverages unlabeled data in both task-agnostic and task-speciﬁc 2
Figure 3: The proposed semi-supervised learning framework leverages unlabeled data in two ways: (1) task-agnostic use in unsupervised pretraining, and (2) task-speciﬁc use in self-training / distillation. ways. The ﬁrst time the unlabeled data is used, it is in a task-agnostic way, for learning general (visual) representations via unsupervised pretraining. The general representations are then adapted for a speciﬁc task via supervised ﬁne-tuning. The second time the unlabeled data is used, it is in a task-speciﬁc way, for further improving predictive performance and obtaining a compact model. To this end, we train student networks on the unlabeled data with imputed labels from the ﬁne-tuned teacher network. Our method can be summarized in three main steps: pretrain, ﬁne-tune, and then distill. The procedure is illustrated in Figure 3. We introduce each speciﬁc component in detail below.
Self-supervised pretraining with SimCLRv2. To learn general visual representations effectively with unlabeled images, we adopt and improve SimCLR [1], a recently proposed approach based on contrastive learning. SimCLR learns representations by maximizing agreement [26] between differently augmented views of the same data example via a contrastive loss in the latent space. More speciﬁcally, given a randomly sampled mini-batch of images, each image xi is augmented twice using random crop, color distortion and Gaussian blur, creating two views of the same example x2k−1 and x2k. The two images are encoded via an encoder network f (·) (a ResNet [25]) to generate representations h2k−1 and h2k. The representations are then transformed again with a non-linear transformation network g(·) (a MLP projection head), yielding z2k−1 and z2k that are used for the contrastive loss. With a mini-batch of augmented examples, the contrastive loss between a pair of positive example i, j (augmented from the same image) is given as follows: (cid:96)NT-Xent i,j
= − log exp(sim(zi, zj)/τ ) 1[k(cid:54)=i] exp(sim(zi, zk)/τ )
, (cid:80)2N k=1 (1)
Where sim(·, ·) is cosine similarity between two vectors, and τ is a temperature scalar.
In this work, we propose SimCLRv2, which improves upon SimCLR [1] in three major ways. Below we summarize the changes as well as their improvements of accuracy on Imagenet ILSVRC-2012 [21]. 1. To fully leverage the power of general pretraining, we explore larger ResNet models. Unlike
SimCLR [1] and other previous work [27, 20], whose largest model is ResNet-50 (4×), we train models that are deeper but less wide. The largest model we train is a 152-layer ResNet [25] with 3× wider channels and selective kernels (SK) [28], a channel-wise attention mechanism that improves the parameter efﬁciency of the network. By scaling up the model from ResNet-50 to
ResNet-152 (3×+SK), we obtain a 29% relative improvement in top-1 accuracy when ﬁne-tuned on 1% of labeled examples. 2. We also increase the capacity of the non-linear network g(·) (a.k.a. projection head), by making it deeper.2 Furthermore, instead of throwing away g(·) entirely after pretraining as in SimCLR [1], we ﬁne-tune from a middle layer (detailed later). This small change yields a signiﬁcant improve-ment for both linear evaluation and ﬁne-tuning with only a few labeled examples. Compared to 2In our experiments, we set the width of projection head’s middle layers to that of its input, so it is also adjusted by the width multiplier. However, a wider projection head improves performance even when the base network remains narrow. 3
SimCLR with 2-layer projection head, by using a 3-layer projection head and ﬁne-tuning from the 1st layer of projection head, it results in as much as 14% relative improvement in top-1 accuracy when ﬁne-tuned on 1% of labeled examples (see Figure E.1). 3. Motivated by [29], we also incorporate the memory mechanism from MoCo [20], which designates a memory network (with a moving average of weights for stabilization) whose output will be buffered as negative examples. Since our training is based on large mini-batch which already supplies many contrasting negative examples, this change yields an improvement of ∼1% for linear evaluation as well as when ﬁne-tuning on 1% of labeled examples (see Appendix D).
Fine-tuning. Fine-tuning is a common way to adapt the task-agnostically pretrained network for a speciﬁc task. In SimCLR [1], the MLP projection head g(·) is discarded entirely after pretraining, while only the ResNet encoder f (·) is used during the ﬁne-tuning. Instead of throwing it all away, we propose to incorporate part of the MLP projection head into the base encoder during the ﬁne-tuning.
In other words, we ﬁne-tune the model from a middle layer of the projection head, instead of the input layer of the projection head as in SimCLR. Note that ﬁne-tuning from the ﬁrst layer of the
MLP head is the same as adding an fully-connected layer to the base network and removing an fully-connected layer from the head, and the impact of this extra layer is contingent on the amount of labeled examples during ﬁne-tuning (as shown in our experiments).
Self-training / knowledge distillation via unlabeled examples. To further improve the network for the target task, here we leverage the unlabeled data directly for the target task. Inspired by [23, 11, 22, 24, 12], we use the ﬁne-tuned network as a teacher to impute labels for training a student network.
Speciﬁcally, we minimize the following distillation loss where no real labels are used: (cid:20) (cid:88) (cid:88) (cid:21)
Ldistill = −
P T (y|xi; τ ) log P S(y|xi; τ ) (2) xi∈D y where P (y|xi) = exp(f task(xi)[y]/τ )/ (cid:80) y(cid:48) exp(f task(xi)[y(cid:48)]/τ ), and τ is a scalar temperature parameter. The teacher network, which produces P T (y|xi), is ﬁxed during the distillation; only the student network, which produces P S(y|xi), is trained.
While we focus on distillation using only unlabeled examples in this work, when the number of labeled examples is signiﬁcant, one can also combine the distillation loss with ground-truth labeled examples using a weighted combination
L = −(1 − α) (cid:88) (cid:20) log P S(yi|xi) (cid:21)
− α (cid:88) (cid:20) (cid:88) (cid:21)
P T (y|xi; τ ) log P S(y|xi; τ )
. (3) (xi,yi)∈DL xi∈D y
This procedure can be performed using students either with the same model architecture (self-distillation), which further improves the task-speciﬁc performance, or with a smaller model architec-ture, which leads to a compact model. 3 Empirical Study 3.1 Settings and Implementation Details
Following the semi-supervised learning setting in [30, 19, 1], we evaluate the proposed method on ImageNet ILSVRC-2012 [21]. While all ∼1.28 million images are available, only a randomly sub-sampled 1% (12811) or 10% (128116) of images are associated with labels.3 As in previous work, we also report performance when training a linear classiﬁer on top of a ﬁxed representation with all labels [31, 16, 17, 1] to directly evaluate SimCLRv2 representations. We use the LARS optimizer [32] (with a momentum of 0.9) throughout for pretraining, ﬁne-tuning and distillation.
For pretraining, similar to [1], we train our model on 128 Cloud TPUs, with a batch size of 4096 and global batch normalization [33], for total of 800 epochs. The learning rate is linearly increased for the ﬁrst 5% of epochs, reaching maximum of 6.4 (= 0.1 × sqrt(BatchSize)), and then decayed with a cosine decay schedule. A weight decay of 1e−4 is used. We use a 3-layer MLP projection head on top of a ResNet encoder. The memory buffer is set to 64K, and exponential moving average (EMA) 3See https://www.tensorﬂow.org/datasets/catalog/imagenet2012_subset for the details of the 1%/10% subsets. 4
Table 1: Top-1 accuracy of ﬁne-tuning SimCLRv2 models (on varied label fractions) or training a linear classiﬁer on the representations. The supervised baselines are trained from scratch using all labels in 90 epochs. The parameter count only include ResNet up to ﬁnal average pooling layer. For
ﬁne-tuning results with 1% and 10% labeled examples, the models include additional non-linear projection layers, which incurs additional parameter count (4M for 1× models, and 17M for 2× models). See Table H.1 for Top-5 accuracy.
Depth Width Use SK [28] Param (M) 50 101 152 152 1× 2× 1× 2× 1× 2× 3×
False
True
False
True
False
True
False
True
False
True
False
True
True 24 35 94 140 43 65 170 257 58 89 233 354 795
Fine-tuned on 10% 100%
Linear eval Supervised 68.4 72.1 73.9 77.0 71.4 75.1 75.8 78.8 73.0 76.5 76.6 79.4 80.1 76.3 78.7 79.1 81.3 78.2 80.6 80.7 82.4 79.3 81.3 81.1 82.9 83.1 71.7 74.6 75.6 77.7 73.6 76.3 77.0 79.0 74.5 77.2 77.4 79.4 79.8 76.6 78.5 77.8 79.3 78.0 79.6 78.9 80.1 78.3 79.9 79.1 80.4 80.5 1% 57.9 64.5 66.3 70.6 62.1 68.3 69.1 73.2 64.0 70.0 70.2 74.2 74.9 decay is set to 0.999 according to [20]. We use the same set of simple augmentations as SimCLR [1], namely random crop, color distortion, and Gaussian blur.
For ﬁne-tuning, by default we ﬁne-tune from the ﬁrst layer of the projection head for 1%/10% of labeled examples, but from the input of the projection head when 100% labels are present. We use global batch normalization, but we remove weight decay, learning rate warmup, and use a much smaller learning rate, i.e. 0.16 (= 0.005 × sqrt(BatchSize)) for standard ResNets [25], and 0.064 (= 0.002 × sqrt(BatchSize)) for larger ResNets variants (with width multiplier larger than 1 and/or
SK [28]). A batch size of 1024 is used. Similar to [1], we ﬁne-tune for 60 epochs with 1% of labels, and 30 epochs with 10% of labels, as well as full ImageNet labels.
For distillation, we only use unlabeled examples, unless otherwise speciﬁed. We consider two types of distillation: self-distillation where the student has the same model architecture as the teacher (excluding projection head), and big-to-small distillation where the student is a much smaller network.
We set temperature to 0.1 for self-distillation, and 1.0 for large-to-small distillation (though the effect of temperatures between 0.1 and 1 is very small). We use the same learning rate schedule, weight decay, batch size as pretraining, and the models are trained for 400 epochs. Only random crop and horizontal ﬂips of training images are applied during ﬁne-tuning and distillation. 3.2 Bigger Models Are More Label-Efﬁcient
In order to study the effectiveness of big models, we train ResNet models by varying width and depth as well as whether or not to use selective kernels (SK) [28].4 Whenever SK is used, we also use the
ResNet-D [34] variant of ResNet. The smallest model is the standard ResNet-50, and biggest model is ResNet-152 (3×+SK).
Table 1 compares self-supervised learning and supervised learning under different model sizes and evaluation protocols, including both ﬁne-tuning and linear evaluation. We can see that increasing width and depth, as well as using SK, all improve the performance. These architectural manipulations have relatively limited effects for standard supervised learning (4% differences in smallest and largest models), but for self-supervised models, accuracy can differ by as much as 8% for linear evaluation, and 17% for ﬁne-tuning on 1% of labeled images. We also note that ResNet-152 (3×+SK) is only marginally better than ResNet-152 (2×+SK), though the parameter size is almost doubled, suggesting that the beneﬁts of width may have plateaued. 4Although we do not use grouped convolution in this work, we believe it can further improve parameter efﬁciency. 5
(a) Supervised (b) Semi-supervised (c) Semi-supervised (y-axis zoomed)
Figure 4: Top-1 accuracy for supervised vs semi-supervised (SimCLRv2 ﬁne-tuned) models of varied sizes on different label fractions. ResNets with depths of 50, 101, 152, width multiplier of 1×, 2× (w/o SK) are presented here. For supervised models on 1%/10% labels, AutoAugment [35] and label smoothing [36] are used. Increasing the size of SimCLRv2 models by 10×, from ResNet-50 to
ResNet-152 (2×), improves label efﬁciency by 10×.
Figure 4 shows the performance as model size and label fraction vary. These results show that bigger models are more label-efﬁcient for both supervised and semi-supervised learning, but gains appear to be larger for semi-supervised learning (more discussions in Appendix A). Furthermore, it is worth pointing out that although bigger models are better, some models (e.g. with SK) are more parameter efﬁcient than others (Appendix B), suggesting that searching for better architectures is helpful. 3.3 Bigger/Deeper Projection Heads Improve Representation Learning
To study the effects of projection head for ﬁne-tuning, we pretrain ResNet-50 using SimCLRv2 with different numbers of projection head layers (from 2 to 4 fully connected layers), and examine performance when ﬁne-tuning from different layers of the projection head. We ﬁnd that using a deeper projection head during pretraining is better when ﬁne-tuning from the optimal layer of projection head (Figure 5a), and this optimal layer is typically the ﬁrst layer of projection head rather than the input (0th layer), especially when ﬁne-tuning on fewer labeled examples (Figure 5b). (a) Effect of projection head’s depth when ﬁne-tuning from optimal middle layer. (b) Effect of ﬁne-tuning from middle of a 3-layer projection head (0 is SimCLR).
Figure 5: Top-1 accuracy via ﬁne-tuning under different projection head settings and label fractions (using ResNet-50).
It is also worth noting that when using bigger ResNets, the improvements from having a deeper projection head are smaller (see Appendix E). In our experiments, wider ResNets also have wider projection heads, since the width multiplier is applied to both. Thus, it is possible that increasing the depth of the projection head has limited effect when the projection head is already relatively wide. 6
Table 2: Top-1 accuracy of a ResNet-50 trained on different types of targets. For distillation, the temperature is set to 1.0, and the teacher is ResNet-50 (2×+SK), which gets 70.6% with 1% of the labels and 77.0% with 10%, as shown in in Table 1. The distillation loss (Eq. 2) does not use label information. Neither strong augmentation nor extra regularization are used.
Method
Label fraction 10% 1%
Label only
Label + distillation loss (on labeled set)
Label + distillation loss (on labeled+unlabeled sets)
Distillation loss (on labeled+unlabeled sets; our default) 12.3 23.6 69.0 68.9 52.0 66.2 75.1 74.3
When varying architecture, the accuracy of ﬁne-tuned models is correlated with the accuracy of linear evaluation (see Appendix C). Although we use the input of the projection head for linear classiﬁcation, we ﬁnd that the correlation is higher when ﬁne-tuning from the optimal middle layer of the projection head than when ﬁne-tuning from the projection head input. 3.4 Distillation Using Unlabeled Data Improves Semi-Supervised Learning
Distillation typically involves both a distillation loss that encourages the student to match a teacher and an ordinary supervised cross-entropy loss on the labels (Eq. 3). In Table 2, we demonstrate the importance of using unlabeled examples when training with the distillation loss. Furthermore, using the distillation loss alone (Eq. 2) works almost as well as balancing distillation and label losses (Eq. 3) when the labeled fraction is small. For simplicity, Eq. 2 is our default for all other experiments.
Distillation with unlabeled examples improves ﬁne-tuned models in two ways, as shown in Figure 6: (1) when the student model has a smaller architecture than the teacher model, it improves the model efﬁciency by transferring task-speciﬁc knowledge to a student model, (2) even when the student model has the same architecture as the teacher model (excluding the projection head after ResNet encoder), self-distillation can still meaningfully improve the semi-supervised learning performance.
To obtain the best performance for smaller ResNets, the big model is self-distilled before distilling it to smaller models. (a) Label fraction 1% (b) Label fraction 10%
Figure 6: Top-1 accuracy of distilled SimCLRv2 models compared to the ﬁne-tuned models as well as supervised learning with all labels. The self-distilled student has the same ResNet as the teacher (without MLP projection head). The distilled student is trained using the self-distilled ResNet-152 (2×+SK) model, which is the largest model included in this ﬁgure.
We compare our best models with previous state-of-the-art semi-supervised learning methods (and a concurrent work [43]) on ImageNet in Table 3. Our approach greatly improves upon previous results, for both small and big ResNet variants. 7
Table 3: ImageNet accuracy of models trained under semi-supervised settings. For our methods, we report results with distillation after ﬁne-tuning. For our smaller models, we use self-distilled
ResNet-152 (3×+SK) as the teacher.
Method
Architecture
Top-1
Label fraction 10% 1%
Top-5
Label fraction 10% 1%
Supervised baseline [30]
ResNet-50 25.4 56.4 48.4 80.4
Methods using unlabeled data in a task-speciﬁc way:
Pseudo-label [11, 30]
VAT+Entropy Min. [37, 38, 30]
Mean teacher [39]
UDA (w. RandAug) [14]
FixMatch (w. RandAug) [15]
S4L (Rot+VAT+Entropy Min.) [30] ResNet-50 (4×)
MPL (w. RandAug) [2]
CowMix [40]
ResNet-50
ResNet-50
ResNeXt-152
ResNet-50
ResNet-50
ResNet-50
ResNet-152
Methods using unlabeled data in a task-agnostic way:
InstDisc [17]
BigBiGAN [41]
PIRL [42]
CPC v2 [19]
SimCLR [1]
SimCLR [1]
SimCLR [1]
BYOL [43] (concurrent work)
BYOL [43] (concurrent work)
ResNet-50
RevNet-50 (4×)
ResNet-50
ResNet-161(∗)
ResNet-50
ResNet-50 (2×)
ResNet-50 (4×)
ResNet-50
ResNet-200 (2×)
Methods using unlabeled data in both ways:
SimCLRv2 distilled (ours)
SimCLRv2 distilled (ours)
SimCLRv2 self-distilled (ours)
ResNet-50
ResNet-50 (2×+SK)
ResNet-152 (3×+SK)
-----------52.7 48.3 58.5 63.0 53.2 71.2 73.9 75.9 76.6
---68.8 71.5 73.2 73.8 73.9
---73.1 65.6 71.7 74.4 68.8 77.7 77.5 80.2 80.9 51.6 47.0
------39.2 55.2 57.2 77.9 75.5 83.0 85.8 78.4 89.5 91.5 93.0 93.4 82.4 83.4 90.9 88.5 89.1 91.2
-91.2 77.4 78.8 83.8 91.2 87.8 91.2 92.6 89.0 93.7 93.4 95.0 95.5 4