Abstract
A deep visual tracker is typically based on either object detection or template matching while each of them is only suitable for a particular group of scenes.
It is straightforward to consider fusing them together to pursue more reliable tracking. However, this is not wise as they follow different tracking principles.
Unlike previous fusion-based methods, we propose a novel ensemble framework, named DTNet, with an online decision mechanism for visual tracking based on hierarchical reinforcement learning. The decision mechanism substantiates an intelligent switching strategy where the detection and the template trackers have to compete with each other to conduct tracking within different scenes that they are adept in. Besides, we present a novel detection tracker which avoids the common issue of incorrect proposal. Extensive results show that our DTNet achieves state-of-the-art tracking performance as well as a good balance between accuracy and efﬁciency. The project website is available at https://vsislab.github. io/DTNet/. 1

Introduction
As a fundamental task in computer vision, visual tracking aims to estimate the trajectory of a speciﬁed object in a sequence of images. Inspired by the success of deep learning in general computer vision tasks, recent visual tracking algorithms mostly used deep networks, particularly CNNs which extract deep representations for various scenes. Among these deep trackers are two dominant tracking schemes. The ﬁrst one treats tracking as a detection task, which typically builds a deep network to distinguish the foreground target from the background [5, 25, 39]. The second one regards tracking as a template matching task and addresses it via a matching network such as Siamese network, which learns a general similarity function to obtain the image patch best matching the target [11, 15, 29].
The detection tracker continuously updates the network online with the image patch detected as the target by itself. The diverse appearances of the patches lead to a good adaptability of the tracker while the continuous update is inefﬁcient for real-world tracking. Also, albeit occasionally, an incorrect detection in a frame which represents a noisy appearance of the target could mislead the tracker.
The template tracker utilizes the initial appearance of the target as a ﬁxed template to conduct the matching operation, which runs efﬁciently at the cost of adaptability.
Either the detection or the template tracker is merely suitable for a particular group of scenes. For instance, as shown in the top row of Fig. 1, due to the temporal occlusion within a frame, the detection tracker incorrectly captures the bicycle as the target in that frame and cannot recover from it in the succeeding frame. By contrast, the template tracker is robust to the temporal occlusion as it always looks back to the real target in the initial frame for delivering the matching. On the other hand, it
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Failure cases of the detection and the template tracker caused by temporal occlusion and temporal deformation. The leftmost column shows the initial frames of the two videos. is easy to understand that, as shown in the bottom row of Fig. 1, the template tracker is not reliable with the temporal deformation of the target while the detection tracker works well with it. Some recent works investigated various fusion schemes to pursue better performance [2, 17, 20, 34, 40].
However, directly fusing the two types of trackers together is not wise as they follow different tracking principles and thus cannot converge to each individual optimum simultaneously during training. Hence, it might be better to make them co-exist for handling different scenes alternatively.
Differing from previous fusion-based methods, this paper presents a framework of decision learning for the ensemble of the two types of trackers where we explore how to automatically and intelligently switch between them for tracking in different scenes. Speciﬁcally, our method makes the two trackers compete with each other through a hierarchical reinforcement learning (HRL) framework so that it can make a proper online decision to choose the tracker which captures the target better in the current scene. This idea is based on the common observation as shown in Fig. 1 that different types of trackers are merely good at tracking the targets in a particular group of frames.
We name the ensemble framework DTNet as it comprises a decision module and a tracker module as illustrated in Fig. 2. The decision module starts with a switch network that encodes the image patch inheriting from the previous frame and the target in the initial frame to decide whether the detection or the template tracker should be selected for the current frame. It is followed by a termination network which estimates the output of the tracker to generate a probability of terminating the current tracker.
The switch and the termination networks in fact form a “Actor-Critic” structure [21]. Such intelligent switching between the two trackers repeats till all frames of the video are processed. We provide a speciﬁcally designed scheme for jointly training the decision and the tracker modules end-to-end via
HRL.
Furthermore, to improve the detection tracker, a fully-convolutional classiﬁer is learned to differentiate the target from the distracting content, Since it does not rely on a number of candidate proposals to predict the bounding boxes of the target, it actually avoids the issue of the incorrect prediction of such proposals that could mislead the tracker. The contributions of this paper are summarized as follows.
• We propose an ensemble framework which learns an online decision for visual tracking based on HRL where the detection and the template trackers compete with each other to substantiate a switching strategy.
• We develop a novel proposal-free detection tracker, which does not require the proposal of candidate bounding boxes of the target and thus make the discriminating course ﬂexible.
• Our method demonstrates the state-of-the-art performance on several benchmarks. The abla-tion studies show that the decision mechanism composed of the switch and the termination networks can effectively select the proper trackers for different scenes. 2
Figure 2: Overview of the DTNet 2