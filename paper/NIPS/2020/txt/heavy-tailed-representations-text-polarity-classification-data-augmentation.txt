Abstract
The dominant approaches to text representation in natural language rely on learn-ing embeddings on massive corpora which have convenient properties such as compositionality and distance preservation. In this paper, we develop a novel method to learn a heavy-tailed embedding with desirable regularity properties regarding the distributional tails, which allows to analyze the points far away from the distribution bulk using the framework of multivariate extreme value theory. In particular, a classiﬁer dedicated to the tails of the proposed embedding is obtained which exhibits a scale invariance property exploited in a novel text generation method for label preserving dataset augmentation. Experiments on synthetic and real text data show the relevance of the proposed framework and conﬁrm that this method generates meaningful sentences with controllable attributes, e.g. positive or negative sentiments. 1

Introduction
Representing the meaning of natural language in a mathematically grounded way is a scientiﬁc challenge that has received increasing attention with the explosion of digital content and text data in the last decade. Relying on the richness of contents, several embeddings have been proposed
[44, 45, 19] with demonstrated efﬁciency for the considered tasks when learnt on massive datasets.
∗Both authors contributed equally 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of angular classiﬁer g dedicated to extremes {x, (cid:107)x(cid:107)∞ ≥ t} in R2 and green truncated cones are respectively labeled as +1 and −1 by g.
+. The red
However, none of these embeddings take into account the fact that word frequency distributions are heavy tailed [2, 11, 40], so that extremes are naturally present in texts (see also Fig. 6a and 6b in the supplementary material). Similarly, [3] shows that, contrary to image taxonomies, the underlying distributions for words and documents in large scale textual taxonomies are also heavy tailed. Exploiting this information, several studies, as [13, 38], were able to improve text mining applications by accurately modeling the tails of textual elements.
In this work, we rely on the framework of multivariate extreme value analysis, based on extreme value theory (EVT) which focuses on the distributional tails. EVT is valid under a regularity assumption which amounts to a homogeneity property above large thresholds: the tail behavior of the considered variables must be well approximated by a power law, see Section 2 for a rigorous statement. The tail region (where samples are considered as extreme) of the input variable x ∈ Rd is of the kind
{(cid:107)x(cid:107) ≥ t}, for a large threshold t. The latter is typically chosen such that a small but non negligible proportion of the data is considered as extreme, namely 25% in our experiments. A major advantage of this framework in the case of labeled data [30] is that classiﬁcation on the tail regions may be performed using the angle Θ(x) = (cid:107)x(cid:107)−1x only, see Figure 1. The main idea behind the present paper is to take advantage of the scale invariance for two tasks regarding sentiment analysis of text data: (i) Improved classiﬁcation of extreme inputs, (ii) Label preserving data augmentation, as the most probable label of an input x is unchanged by multiplying x by λ > 1.
EVT in a machine learning framework has received increasing attention in the past few years. Learning tasks considered so far include anomaly detection [48, 49, 12, 23, 53], anomaly clustering [9], unsupervised learning [22], online learning [6, 1], dimension reduction and support identiﬁcation [24, 8, 10, 29]. The present paper builds upon the methodological framework proposed by Jalalzai et al.
[30] for classiﬁcation in extreme regions. The goal of Jalalzai et al. [30] is to improve the performance of classiﬁers (cid:98)g(x) issued from Empirical Risk Minimization (ERM) on the tail regions {(cid:107)x(cid:107) > t}
Indeed, they argue that for very large t, there is no guarantee that (cid:98)g would perform well conditionally to {(cid:107)X(cid:107) > t}, precisely because of the scarcity of such examples in the training set. They thus propose to train a speciﬁc classiﬁer dedicated to extremes leveraging the probabilistic structure of the tails. Jalalzai et al. [30] demonstrate the usefulness of their framework with simulated and some real world datasets. However, there is no reason to assume that the previously mentioned text embeddings satisfy the required regularity assumptions. The aim of the present work is to extend
[30]’s methodology to datasets which do not satisfy their assumptions, in particular to text datasets embedded by state of the art techniques. This is achieved by the algorithm Learning a Heavy Tailed
Representation (in short LHTR) which learns a transformation mapping the input data X onto a random vector Z which does satisfy the aforementioned assumptions. The transformation is learnt by an adversarial strategy [26].
In Appendix C we propose an interpretation of the extreme nature of an input in both LHTR and
BERT representations. In a word, these sequences are longer and are more difﬁcult to handle (for next token prediction and classiﬁcation tasks) than non extreme ones.
Our second contribution is a novel data augmentation mechanism GENELIEX which takes advantage of the scale invariance properties of Z to generate synthetic sequences that keep invariant the attribute of the original sequence. Label preserving data augmentation is an effective solution to the data scarcity problem and is an efﬁcient pre-processing step for moderate dimensional datasets [55, 56].
Adapting these methods to NLP problems remains a challenging issue. The problem consists in constructing a transformation h such that for any sample x with label y(x), the generated sample h(x) would remain label consistent: y(cid:0)h(x)(cid:1) = y(x) [46]. The dominant approaches for text data augmentation rely on word level transformations such as synonym replacement, slot ﬁlling, swap deletion [56] using external resources such as wordnet [42]. Linguistic based approaches can also be 2
combined with vectorial representations provided by language models [32]. However, to the best of our knowledge, building a vectorial transformation without using any external linguistic resources remains an open problem. In this work, as the label y(cid:0)h(x)(cid:1) is unknown as soon as h(x) does not belong to the training set, we address this issue by learning both an embedding ϕ and a classiﬁer g satisfying a relaxed version of the problem above mentioned, namely ∀λ ≥ 1 (1) g(cid:0)hλ(ϕ(x))(cid:1) = g(cid:0)ϕ(x)(cid:1).
For mathematical reasons which will appear clearly in Section 2.2, hλ is chosen as the homothety with scale factor λ, hλ(x) = λx. In this paper, we work with output vectors issued by BERT [19].
BERT and its variants are currently the most widely used language model but we emphasize that the proposed methodology could equally be applied using any other representation as input. BERT embedding does not satisfy the regularity properties required by EVT (see the results from statistical tests performed in Appendix B.5) Besides, there is no reason why a classiﬁer g trained on such embedding would be scale invariant, i.e. would satisfy for a given sequence u, embedded as x, g(hλ(x)) = g(x) ∀λ ≥ 1. On the classiﬁcation task, we demonstrate on two datasets of sentiment analysis that the embedding learnt by LHTR on top of BERT is indeed following a heavy-tailed distribution. Besides, a classiﬁer trained on the embedding learnt by LHTR outperforms the same classiﬁer trained on BERT. On the dataset augmentation task, quantitative and qualitative experiments demonstrate the ability of GENELIEX to generate new sequences while preserving labels.
The rest of this paper is organized as follows. Section 2 introduces the necessary background in multivariate extremes. The methodology we propose is detailed at length in Section 3. Illustrative numerical experiments on both synthetic and real data are gathered in sections 4 and 5. Further comments and experimental results are provided in the supplementary material. 2