Abstract
Training Neural Ordinary Differential Equations (ODEs) is often computationally expensive. Indeed, computing the forward pass of such models involves solving an ODE which can become arbitrarily complex during training. Recent works have shown that regularizing the dynamics of the ODE can partially alleviate this.
In this paper we propose a new regularization technique: randomly sampling the end time of the ODE during training. The proposed regularization is simple to implement, has negligible overhead and is effective across a wide variety of tasks.
Further, the technique is orthogonal to several other methods proposed to regularize the dynamics of ODEs and as such can be used in conjunction with them. We show through experiments on normalizing ﬂows, time series models and image recognition that the proposed regularization can signiﬁcantly decrease training time and even improve performance over baseline models. 1

Introduction
Neural Ordinary Differential Equations (ODE) are an elegant approach for building continuous depth neural networks [4]. They offer various advantages over traditional neural networks, for example a constant memory cost as a function of depth and invertibility by construction. As they are inspired by dynamical systems, they can also take advantage of the vast amount of mathematical techniques developed in this ﬁeld.
Incorporating continuous depth also enables ef-ﬁcient density estimation [18] and modeling of irregularly sampled time series [46].
The aforementioned advantages come at the expense of a signiﬁcantly increased computa-tional cost compared with traditional models like Residual Networks [21]. The training dy-namics of Neural ODEs become more compli-cated as training progresses and the number of steps taken by the adaptive ODE solver (typically referred to as the number of function evaluations) can become prohibitively large [14]. In fact, several works have argued that this is one of the most
Figure 1: Flow ﬁelds of a standard Neural ODE (left) vs one equipped with STEER (right). These models transform a simple initial distribution p(zt0) to the target distribution p(zt). STEER learns simpler ﬂows, which enable faster training. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
pressing limitations of Neural ODEs [4, 14, 16]. It is therefore important to design methods that can help reduce the computational cost of this tool.
A natural approach for tackling this issue is to explore regularization techniques that simplify the underlying dynamics for these models. Indeed, simpler dynamics have shown beneﬁts in terms of faster convergence and a lower number of function evaluations both during training and testing [14].
[16] recently proposed an interesting technique for explicitly regularizing the dynamics based on ideas from optimal transport showing signiﬁcant improvements in the effective convergence time.
In this work, we propose a regularization technique in order to further accelerate training. Our regularization is orthogonal to previous approaches and can therefore be used in conjunction with them. Our technique is extremely simple: during training we randomly sample the upper limit of the
ODE integration.
Speciﬁcally, the solver in Neural ODEs takes as input the integration limits (t0, t1). We propose to introduce regularization by randomly perturbing the end time parameter t1 to be t1 ± δ where δ is a random variable. Empirically, we demonstrate that, in a wide variety of tasks, the proposed regulariza-tion achieves comparable performance to unregularized models at a signiﬁcantly lower computational cost. We test our regularization technique on continuous normalizing ﬂows [18], irregularly sampled time series [46] and image recognition tasks and show that it improves performance across all these tasks. It also performs better in certain instances of Stiff ODEs. Stiff ODEs are a widely studied topic in the ﬁeld of engineering[6]. During training the dynamics learnt by the implicit model parametrized by the neural network can turn into an instance of a Stiff ODE. Techniques which could tackle the instances of Stiff ODE would prove to be instrumental as these implicit deep learning models mature. 2