Abstract
Pre-training is a dominant paradigm in computer vision. For example, supervised
ImageNet pre-training is commonly used to initialize the backbones of object detec-tion and segmentation models. He et al. [1], for example, show a contrasting result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and ﬂexibility of self-training with three additional insights: 1) stronger data aug-mentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training beneﬁts when we use one ﬁfth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes.
In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help signiﬁcantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest
SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improve-ment of +1.5% mIOU over the previous state-of-the-art result by DeepLabv3+.1 1

Introduction
Pre-training is a dominant paradigm in computer vision. As many vision tasks are related, it is expected a model, pre-trained on one dataset, to help another. It is now common practice to pre-train the backbones of object detection and segmentation models on ImageNet classiﬁcation [2–5]. This practice has been recently challenged He et al. [1], among others [6, 7], who show a surprising result that such ImageNet pre-training does not improve accuracy on the COCO dataset.
A stark contrast to pre-training is self-training [8–10]. Let’s suppose we want to use ImageNet to help COCO object detection; under self-training, we will ﬁrst discard the labels on ImageNet. We then train an object detection model on COCO, and use it to generate pseudo labels on ImageNet.
The pseudo-labeled ImageNet and labeled COCO data are then combined to train a new model. The recent successes of self-training [11–14] raise the question to what degree does self-training work better than pre-training. Can self-training work well on the exact setup, using ImageNet to improve
COCO, where pre-training fails?
⇤Authors contributed equally. 1Code and checkpoints for our models are available at https://github.com/tensorflow/tpu/tree/ master/models/official/detection/projects/self_training 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our work studies self-training with a focus on answering the above question. We deﬁne a set of control experiments where we use ImageNet as additional data with the goal of improving COCO.
We vary the amount of labeled data in COCO and the strength of data augmentation as control factors. Our experiments show that as we increase the strength of data augmentation or the amount of labeled data, the value of pre-training diminishes. In fact, with our strongest data augmentation, pre-training signiﬁcantly hurts accuracy by -1.0AP, a surprising result that was not seen by He et al. [1].
Our experiments then show that self-training interacts well with data augmentations: stronger data augmentation not only doesn’t hurt self-training, but also helps it. Under the same data augmentation, self-training yields positive +1.3AP improvements using the same ImageNet dataset. This is another striking result because it shows self-training works well exactly on the setup that pre-training fails.
These two results provide a positive answer to the above question.
An increasingly popular pre-training method is self-supervised learning. Self-supervised learning methods pre-train on a dataset without using labels with the hope to build more universal representa-tions that work across a wider variety of tasks and datasets. We study ImageNet models pre-trained using a state-of-the-art self-supervised learning technique and compare to standard supervised Ima-geNet pre-training on COCO. We ﬁnd that self-supervised pre-trained models using SimCLR [15] have similar performance as supervised ImageNet pre-training. Both methods hurt COCO perfor-mance in the high data/strong augmentation setting, when self-training helps. Our results suggest that both supervised and self-supervised pre-training methods fail to scale as the labeled dataset size grows, while self-training is still useful.
Our work however does not dismiss the use of pre-training in computer vision. Fine-tuning a pre-trained model is faster than training from scratch and self-training in our experiments. The speedup ranges from 1.3x to 8x depending on the pre-trained model quality, strength of data augmentation, and dataset size. Pre-training can also beneﬁt applications where collecting sufﬁcient labeled data is difﬁcult. In such scenarios, pre-training works well; but self-training also beneﬁts models with and without pre-training. For example, our experiment with PASCAL segmentation dataset shows that ImageNet pre-training improves accuracy, but self-training provides an additional +1.3% mIOU boost on top of pre-training. The fact that the beneﬁt of pre-training does not cancel out the gain by self-training, even when utilizing the same dataset, suggests the generality of self-training.
Taking a step further, we explore the limits of self-training on COCO and PASCAL datasets, thereby demonstrating the method’s ﬂexibility. We perform self-training on COCO dataset with Open
Images dataset as the source of unlabeled data, and RetinaNet [16] with SpineNet [17] as the object detector. This combination achieves 54.3AP on the COCO test set, which is +1.5AP better than the strongest SpineNet model. On segmentation, we use PASCAL aug set [18] as the source of unlabeled data, and NAS-FPN [19] with EfﬁcientNet-L2 [12] as the segmentation model. This combination achieves 90.5AP on the PASCAL VOC 2012 test set, which surpasses the state-of-the-art accuracy of 89.0AP [20], who also use additional 300M labeled images. These results conﬁrm another beneﬁt of self-training: it’s very ﬂexible about unlabeled data sources, model architectures and computer vision tasks. 2