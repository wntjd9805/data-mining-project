Abstract
Recent work proposed the computation of so-called PI-explanations of Naive Bayes
Classiﬁers (NBCs) [35]. PI-explanations are subset-minimal sets of feature-value pairs that are sufﬁcient for the prediction, and have been computed with state-of-the-art exact algorithms that are worst-case exponential in time and space. In contrast, we show that the computation of one PI-explanation for an NBC can be achieved in log-linear time, and that the same result also applies to the more general class of linear classiﬁers. Furthermore, we show that the enumeration of PI-explanations can be obtained with polynomial delay. Experimental results demonstrate the performance gains of the new algorithms when compared with earlier work. The experimental results also investigate ways to measure the quality of heuristic explanations. 1

Introduction
Approaches proposed in recent years for computing explanations of Machine Learning (ML) models can be broadly characterized as heuristic or non-heuristic1. Heuristic approaches denote those providing no formal guarantees on their results. In contrast, non-heuristic approaches do provide some sort of formal guarantee(s) on their results, usually at the cost of increased computational complexity. Among the heuristic approaches for ﬁnding explanations, two have been studied in greater detail. One line of work focuses on devising model-agnostic linear approximations of the underlying model [29, 18]. Another line of work is exempliﬁed by Anchor [30], and targets the computation of a set of feature-value pairs associated with a given instance as a way of explaining the prediction.
To date, all non-heuristic methods have focused on computing sets of feature-value pairs that are sufﬁcient for the prediction [35, 10, 36, 6]2. Moreover, in terms of formal guarantees, [35] studies two distinct deﬁnitions of explanations. A PI-explanation represents a subset-minimal set of feature values that entails the outcome of the decision function for the predicted class whatever the values of the other features (i.e. it represents a prime implicant of the outcome of the decision function).
PI-explanations have also been studied under the name of abductive explanations [10]. In contrast, and assuming binary features, an MC-explanation is a cardinality-minimal set of equal-valued features that entails the outcome of the decision function. Non-heuristic approaches are model-based, and 1There is a growing body of work on explaining ML models. Example recent overviews include [9, 31, 32, 23, 22, 1, 24, 41, 25]. 2Earlier work imposed the additional restriction of considering boolean-valued features. Clearly, non-boolean features can be binarized, e.g. with the one hot encoding, at the cost of adding additional features. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
so earlier work speciﬁcally considered Naive-Bayes Classiﬁers (NBCs) and Latent-Tree Classiﬁers (LTCs) [35, 6], Bayesian Network Classiﬁers [36, 6], and Neural Networks [10, 34].
In the concrete case of computing (non-heuristic) PI-explanations for NBCs, earlier work [35] proposed algorithms that are worst-case exponential in both time and space. In contrast, in this paper we propose a novel non-heuristic solution for computing PI-explanations of NBCs and other linear classiﬁers 3, which exhibits two fundamental advantages over earlier work. First, the paper shows that computing PI-explanations for NBCs (but also for any linear classiﬁer) is in P, by proposing a log-linear algorithm for computing one smallest size PI-explanation. Second, the paper proposes a polynomial (log-linear) delay algorithm for enumerating the PI-explanations of NBCs (and also of any linear classiﬁer). Furthermore, the paper presents an experimental evaluation of different approaches for explaining NBCs with PI-explanations, including the heuristic solutions computed by
Anchor [30] and SHAP [18]4. Moreover, although (real-valued) linear classiﬁers can be viewed as interpretable [29], this does not equate with computing PI-explanations, particularly when features are categorical. To the best of our knowledge, proving the (polynomial) complexity of computing PI-explanations for linear classiﬁers (including NBCs) closes an open problem. Furthermore, the results in this paper rank among the ﬁrst to investigate classes of ML models for which PI-explanations can be computed in polynomial time [2, 11].
In a recent paper [34], Shi et al. investigate the use of knowledge compilation in