Abstract
Novel View Synthesis (NVS) is concerned with synthesizing views under camera viewpoint transformations from one or multiple input images. NVS requires explicit reasoning about 3D object structure and unseen parts of the scene to synthesize convincing results. As a result, current approaches typically rely on supervised training with either ground truth 3D models or multiple target images. We propose
Continuous Object Representation Networks (CORN), a conditional architecture that encodes an input image’s geometry and appearance that map to a 3D consistent scene representation. We can train CORN with only two source images per object by combining our model with a neural renderer. A key feature of CORN is that it requires no ground truth 3D models or target view supervision. Regardless,
CORN performs well on challenging tasks such as novel view synthesis and single-view 3D reconstruction and achieves performance comparable to state-of-the-art approaches that use direct supervision. For up-to-date information, data, and code, please see our project page 1. 1

Introduction
In 1971, Shephard and Metzler [44] introduced the concept of mental rotation, the ability to rotate 3D objects mentally and link the model to its projection. Novel View Synthesis (NVS) research seeks to replicate this capability in machines by generating images of a scene from previously unseen viewpoints, unlocking applications in image editing, animation, or robotic manipulation. View synthesis is a challenging problem, as it requires understanding the 3D scene structure, reason on image semantics, and the ability to render the internal representation into a target viewpoint. A common approach for NVS is to use a large collection of views to reconstruct a 3D mesh [10, 43].
Recent methods have made progress in learning 3D object representations, such as voxel grids [60, 46, 52, 33, 32], point clouds [1, 61, 56], or meshes [54, 12, 48, 55]. However, the discrete nature of these representations limit the achievable resolution and induce signiﬁcant memory overhead. Continuous representations [36, 25, 42, 47, 58, 6, 24, 27] address these challenges. However, proposed methods require either 3D ground truth or multi-view supervision, limiting these approaches’ applicability to domains where data is available.
We introduce Continuous Object Representation Networks (CORNs), a neural object representation that enforces multi-view consistency in geometry and appearance with natural generalization across scenes, learned from as few as two images per object. The key idea of CORNs is to extract local and global features from the input images and represent the scene implicitly as a continuous, differentiable function that maps local and global features to 3D world coordinates. We optimize
CORNs from only two source views using transformation chains and 3D feature consistency as self-supervision, requiring 50× fewer data during training than the current state-of-the-art models. 1Project page: nicolaihaeni.github.io/corn/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Prediction
Source 1
Source 2 (a)
Ground Truth (b)
Figure 1: Our model learns to synthesize novel views using only two source images per object during training (a). For this instance, even though both source images are from the back of the car, our model can reconstruct unseen areas with a reasonable detail level. During training, the target view prediction is not directly supervised with ground truth. Instead, it is transformed into the second source image while maintaining the consistency of the learned representation. During inference (b), our model predicts novel views from a single input image. It can accommodate drastically different source and target poses.
The conditional formulation of CORNs, combined with a differentiable neural renderer, enforces multi-view consistency and allows for the fast inference of novel views from a single image during test time, without additional optimization of latent variables. We evaluate CORNs on various challenging 3D computer vision problems, including novel view synthesis, 3D model reconstruction, and out of domain view synthesis.
To summarize, our approach makes the following contributions:
• A continuous, conditional novel view synthesis model, CORNs based on a novel repre-sentation that captures the scene’s appearance and geometry at arbitrary spatial resolution.
CORNs are end-to-end trainable and uses only two images per object during training time, without any 3D space or 2D target view supervision.
• Despite being self-supervised, CORN performs competitively or even outperforms current state-of-the-art approaches that use dozens of images per object and direct supervision on the target views.
• We demonstrate several applications of our method, including novel view synthesis, single-view 3D reconstruction, and novel view synthesis from out-of-domain samples. 2