Abstract
Convex optimization problems with staged structure appear in several contexts, including optimal control, veriﬁcation of deep neural networks, and isotonic re-gression. Off-the-shelf solvers can solve these problems but may scale poorly.
We develop a nonconvex reformulation designed to exploit this staged structure.
Our reformulation has only simple bound constraints, enabling solution via pro-jected gradient methods and their accelerated variants. The method automatically generates a sequence of primal and dual feasible solutions to the original convex problem, making optimality certiﬁcation easy. We establish theoretical properties of the nonconvex formulation, showing that it is (almost) free of spurious local minima and has the same global optimum as the convex problem. We modify PGD to avoid spurious local minimizers so it always converges to the global minimizer.
For neural network veriﬁcation, our approach obtains small duality gaps in only a few gradient steps. Consequently, it can quickly solve large-scale veriﬁcation problems faster than both off-the-shelf and specialized solvers. 1

Introduction
This paper studies efﬁcient algorithms for a particular class of stage-wise optimization problems: (1a) f (s, z) minimize (s,z)∈S×Rn s.t. µi(s, z1:i−1) zi
≤
ηi(s, z1:i−1) i (1b)
∀
≤
Rn and range R, the where n and m are positive integers, S
⊆
Ri−1 and range R. Given a vector z, we use the notation z1:i functions µi and ηi have domain S to denote the vector [z1, . . . , zi]. We let z1:0 be a vector of length zero. Throughout the paper we assume that η1, . . . , ηn are proper concave functions, f , µ1, . . . , µn are proper convex functions, and
S is a nonempty convex set.
Rm, the function f has domain S 1, . . . , n
∈ {
×
×
}
Problems that fall into this problem class are ubiquitous. They appear in optimal control [1], ﬁnite horizon Markov decision processes with cost function controlled by an adversary [2], generalized
Isotonic regression [3, 4], and veriﬁcation of neural networks [5–7]. Details explaining how these problems can be written in the form of (1) are given in Appendix A. Here we brieﬂy outline how neural network veriﬁcation falls into (1b). Letting s represent the input image and z the activation values, neural networks veriﬁcation can be written (unconventionally) as minimize (s,z)∈S×Rn f (s, z) s.t. zi = σ([s, z1:i−1] wi),
·
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for (sparse) weight vectors wi and activation function σ. A convex relaxation is created by picking functions satisfying µi(s, z1:i−1)
ηi(s, z1:i−1) for all s and z feasible to the
· original problem. Solving these convex relaxations with traditional methods can be time consuming.
For example, Salman et al. [8] reports spending 22 CPU years to solve problems of this type in order to evaluate the tightness of their proposed relaxation. Consequently, methods for solving these relaxations faster are valuable.
σi([s, z1:i−1] wi)
≤
≤ 1.1