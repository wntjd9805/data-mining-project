Abstract
It is not clear yet why ADAM-alike adaptive gradient algorithms suffer from worse generalization performance than SGD despite their faster training speed. This work aims to provide understandings on this generalization gap by analyzing their local convergence behaviors. Speciﬁcally, we observe the heavy tails of gradient noise in these algorithms. This motivates us to analyze these algorithms through their Lévy-driven stochastic differential equations (SDEs) because of the similar convergence behaviors of an algorithm and its SDE. Then we establish the escaping time of these SDEs from a local basin. The result shows that (1) the escaping time of both SGD and ADAM depends on the Radon measure of the basin positively and the heaviness of gradient noise negatively; (2) for the same basin, SGD enjoys smaller escaping time than ADAM, mainly because (a) the geometry adaptation in ADAM via adaptively scaling each gradient coordinate well diminishes the anisotropic structure in gradient noise and results in larger Radon measure of a basin; (b) the exponential gradient average in ADAM smooths its gradient and leads to lighter gradient noise tails than SGD. So SGD is more locally unstable than ADAM at sharp minima deﬁned as the minima whose local basins have small
Radon measure, and can better escape from them to ﬂatter ones with larger Radon measure. As ﬂat minima here which often refer to the minima at ﬂat or asymmetric basins/valleys often generalize better than sharp ones [1, 2], our result explains the better generalization performance of SGD over ADAM. Finally, experimental results conﬁrm our heavy-tailed gradient noise assumption and theoretical afﬁrmation. 1

Introduction
Stochastic gradient descent (SGD) [3, 4] has become one of the most popular algorithms for training deep neural networks [5–11]. In spite of its simplicity and effectiveness, SGD uses one learning rate for all gradient coordinates and could suffer from unsatisfactory convergence performance, especially for ill-conditioned problems [12]. To avoid this issue, a variety of adaptive gradient algorithms have been developed that adjust learning rate for each gradient coordinate according to the current geometry curvature of the objective function [13–16]. These algorithms, especially for ADAM, have achieved much faster convergence speed than vanilla SGD in practice.
Despite their faster convergence behaviors, these adaptive gradient algorithms usually suffer from worse generalization performance than SGD [12, 17, 18]. Speciﬁcally, adaptive gradient algorithms often show faster progress in the training phase but their performance quickly reaches a plateaus on test data. Differently, SGD usually improves model performance slowly but could achieve higher test performance. One empirical explanation [1, 19–21] for this generalization gap is that adaptive gradient algorithms tend to converge to sharp minima whose local basin has large curvature and usually generalize poorly, while SGD prefers to ﬁnd ﬂat minima and thus generalizes better. However, recent evidence [2, 22] shows that (1) for deep neural networks, the minima at the asymmetric 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) ADAM (b) SGD
Figure 1: Illustration of gradient noise in ADAM and SGD on AlexNet trained with CIFAR10. (b) is produced under the same setting in [23]. By comparison, one can observe (1) α-stable noise can better characterize real gradient noise and (2) SGD has heavier gradient noise tails than ADAM. basins/valleys where both steep and ﬂat directions exist also generalize well though they are sharp in terms of their local curvature, and (2) SGD often converges to these minima. So the argument of the conventional “ﬂat" and “sharp" minima deﬁned on curvature cannot explain these new results. Thus the reason for the generalization gap between adaptive gradient methods and SGD is still unclear.
In this work, we provide a new viewpoint for understanding the generalization performance gap. We
ﬁrst formulate ADAM and SGD as Lévy-driven stochastic differential equations (SDEs), since the
SDE of an algorithm shares similar convergence behaviors of the algorithm and can be analyzed more easily than directly analyzing the algorithm. Then we analyze the escaping behaviors of these SDEs at local minima to investigate the generalization gap between ADAM and SGD, as escaping behaviors determine which basin that an algorithm ﬁnally converges to and thus affect the generalization performance of the algorithm. By analysis, we ﬁnd that compared with ADAM, SGD is more locally unstable and is more likely to converge to the minima at the ﬂat or asymmetric basins/valleys which often have better generalization performance over other type minima. So our results can explain the better generalization performance of SGD over ADAM. Our contributions are highlighted below.
Firstly, this work is the ﬁrst one that adopts Lévy-driven SDE which better characterizes the algorithm gradient noise in practice, to analyze the adaptive gradient algorithms. Speciﬁcally, Fig. 1 shows that the gradient noise in ADAM and SGD, i.e. the difference between the full and stochastic gradients, has heavy tails and can be well characterized by symmetric α-stable (SαS) distribution [24]. Based on this observation, we view ADAM and SGD as discretization of the continuous-time processes and formulate the processes as Lévy-driven SDEs to analyze their behaviors. Compared with Gaussian gradient noise assumption in SGD [25–27], SαS distribution assumption can characterize the heavy-tailed gradient noise in practice more accurately as shown in Fig. 1, and also better explains the different generalization performance of SGD and ADAM as discussed in Sec. 3. This work extends
[23, 28] from SGD on the over-simpliﬁed one-dimensional problems to much more complicated adaptive algorithms on high-dimensional problems. It also differs from [29], as [29] considers escaping behaviors of SGD along several ﬁxed directions, while this work analyzes the dynamic underlying structures in gradient noise that plays an important role in the local escaping behaviors of both ADAM and SGD.
Next, we theoretically prove that for the Lévy-driven SDEs of ADAM and SGD, their escaping time
Γ from a local basin Ω, namely the least time for escaping from the inner of Ω to its outside, is at the order of O(ε−α/m(W)), where the constant ε ∈ (0, 1) relies on the learning rate of algorithms and α denotes the tail index of SαS distribution. Here m(W) is a non-zero Radon measure on the escaping set W of ADAM and SGD at the local basin Ω (see Sec. 4.1), and actually negatively relies on the Radon measure of Ω. So both ADAM and SGD have small escaping time at the “sharp" minima whose corresponding basins Ω have small Radon measure. It means that ADAM and SGD are actually unstable at “sharp" minima and would escape them to “ﬂatter" ones. Note, the Radon measure of Ω positively depends on the volume of Ω. So these results also well explain the observations in [1, 2, 20, 21] that the minima of deep networks found by SGD often locate at the ﬂat or asymmetric valleys, as their corresponding basins have large volumes and thus large Radon measure.
Finally, our results can answer why SGD often converges to ﬂatter minima than ADAM in terms of Radon measure, and thus explain the generalization gap between ADAM and SGD. Firstly, our analysis shows that even for the same basin Ω, ADAM often has smaller Radon measure m(W) on the escaping set W at Ω than SGD, as the geometry adaptation in ADAM via adaptively scaling each gradient coordinate well diminishes underlying anisotropic structure in gradient noise and leads to smaller m(W). Secondly, the empirical results in Sec. 5 and Fig. 1 show that SGD often has much 2
smaller tail index α of gradient noise than ADAM for some optimization iterations and thus enjoys smaller factor ε−α. These results together show that SGD is more locally unstable and would like to converge to ﬂatter minima with larger measure m(W) which often refer to the minima at the ﬂat and asymmetric basins/valleys, according with empirical evidences in [12, 17, 30, 31]. Considering the observations in [1, 19–21] that the minima at the ﬂat and asymmetric basins/valleys often generalize better, our results well explain the generalization gap between ADAM and SGD. Besides, our results also show that SGD beneﬁts from its anisotropic gradient noise on its escaping behaviors, while
ADAM does not. 2