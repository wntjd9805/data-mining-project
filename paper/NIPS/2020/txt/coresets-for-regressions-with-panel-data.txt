Abstract
This paper introduces the problem of coresets for regression problems to panel data settings. We ﬁrst deﬁne coresets for several variants of regression problems with panel data and then present efﬁcient algorithms to construct coresets of size that depend polynomially on 1/ε (where ε is the error parameter) and the number of regression parameters – independent of the number of individuals in the panel data or the time units each individual is observed for. Our approach is based on the Feldman-Langberg framework in which a key step is to upper bound the “total sensitivity” that is roughly the sum of maximum inﬂuences of all individual-time pairs taken over all possible choices of regression parameters. Empirically, we assess our approach with a synthetic and a real-world datasets; the coreset sizes constructed using our approach are much smaller than the full dataset and coresets indeed accelerate the running time of computing the regression objective. 1

Introduction
Panel data, represented as X ∈ RN ×T ×d and Y ∈ RN ×T where N is the number of enti-ties/individuals, T is the number of time periods and d is the number of features is widely used in statistics and applied machine learning. Such data track features of a cross-section of entities (e.g., customers) longitudinally over time. Such data are widely preferred in supervised machine learning for more accurate prediction and unbiased inference of relationships between variables relative to cross-sectional data (where each entity is observed only once) [28, 6].
The most common method for inferring relationships between variables using observational data involves solving regression problems on panel data. The main difference between regression on panel data when compared to cross-sectional data is that there may exist correlations within observations associated with entities over time periods. Consequently, the regression problem for panel data is the following optimization problem over regression variables β ∈ Rd and the covariance matrix Ω that is induced by the abovementioned correlations: minβ∈Rd,Ω∈RT ×T (cid:80) i∈[N ](yi − Xiβ)(cid:62)Ω−1(yi − Xiβ).
Here Xi ∈ RT ×d denotes the observation matrix of entity i whose t-th row is xit and Ω is constrained to have largest eigenvalue at most 1 where Ωtt(cid:48) represents the correlation between time periods t and t(cid:48). This regression model is motivated by the random effects model (Eq. (1) and Appendix A), common in the panel data literature [27, 24, 23]. A common way to deﬁne the correlation between observations is an autocorrelation structure AR(q) [25, 35] whose covariance matrix Ω is induced by a vector ρ ∈ Rq (integer q ≥ 1). This type of correlation results in the generalized least-squares estimator (GLSE), where the parameter space is P = Rd+q.
As the ability to track entities on various features in real-time has grown, panel datasets have grown massively in size. However, the size of these datasets limits the ability to apply standard learning algorithms due to space and time constraints. Further, organizations owning data may want to share only a subset of data with others seeking to gain insights to mitigate privacy or intellectual property related risks. Hence, a question arises: can we construct a smaller subset of the panel data on which we can solve the regression problems with performance guarantees that are close enough to those obtained when working with the complete dataset? 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
One approach to this problem is to appeal to the theory of “coresets.” Coresets, proposed in [1], are weighted subsets of the data that allow for fast approximate inference for a large dataset by solving the problem on the smaller coreset. Coresets have been developed for a variety of unsupervised and supervised learning problems; for a survey, see [43]. But, thus, far coresets have been developed only for (cid:96)2-regression cross-sectional data [18, 36, 8, 15, 33]; no coresets have been developed for regressions on panel data – an important limitation, given their widespread use and advantages.
Roughly, a coreset for cross-sectional data is a weighted subset of observations associated with entities that approximates the regression objective for every possible choice of regression parameters.
An idea, thus, is to construct a coreset for each time period (cross-section) and output their union as a coreset for panel data. However, this union contains at least T observations which is undesirable since T can be large. Further, due to the covariance matrix Ω, it is not obvious how to use this union to approximately compute regression objectives. With panel data, one needs to consider both how to sample entities, and within each entity how to sample observations across time. Moreover, we also need to deﬁne how to compute regression objectives on such a coreset consisting of entity-time pairs.
Our contributions. We initiate the study of coresets for versions of (cid:96)2-regression with panel data, including the ordinary least-squares estimator (OLSE; Deﬁnition 2.2), the generalized least-squares estimator (GLSE; Deﬁnition 2.3), and a clustering extension of GLSE (GLSEk; Deﬁnition 2.4) in which all entities are partitioned into k clusters and each cluster shares the same regression parameters.
Overall, we formulate the deﬁnitions of coresets and propose efﬁcient construction of ε-coresets of sizes independent of N and T . Our key contributions are: (a) We give a novel formulation of coresets for GLSE (Deﬁnition 3.1) and GLSEk (Deﬁnition 3.2). We represent the regression objective of GLSE as the sum of N T sub-functions w.r.t. entity-time pairs, which enables us to deﬁne coresets similar to the case of cross-sectional data. For GLSEk, the regression objective cannot be similarly decomposed due to the min operations in Deﬁnition 2.4. To deal with this issue, we deﬁne the regression objective on a coreset S by including min operations. (b) Our coreset for OLSE is of size O(min{ε−2d, d2}) (Theorems C.1 and C.2), based on a reduction to coreset for (cid:96)2-regression with cross-sectional data. (c) Our coreset for GLSE consists of at most
˜O(ε−2 max{q4d2, q3d3}) points (Theorem 4.1), independent of N and T as desired. (d) Our coreset for GLSEk is of size poly(M, k, q, d, 1/ε) (Theorem 5.2) where M upper bounds the gap between the maximum individual regression objective of OLSE and the minimum one (Deﬁnition 5.1). We provide a matching lower bound Ω(N ) (Theorem 5.4) for k, q, d ≤ 2, indicating that the coreset size should contain additional factors than k, q, d, 1/ε, justifying the M -bounded assumption.
Our coresets for GLSE/GLSEk leverage the Feldman-Langberg (FL) framework [21] (Algorithms 1
The ρ variables make the objective function of GLSE non-convex in contrast to the and 2). cross-sectional data setting where objective functions are convex. Thus, bounding the “sensitivity” (Lemma 4.4) of each entity-time pair for GLSE, which is a key step in coreset construction using the FL framework, becomes signiﬁcantly difﬁcult. We handle this by upper-bounding the maximum effect of ρ, based on the observation that the gap between the regression objectives of GLSE and
OLSE with respect to the same β ∈ Rd is always constant, which enables us to reduce the problem to the cross-sectional setting. For GLSEk, a key difﬁculty is that the clustering centers are subspaces induced by regression vectors, instead of points as in Gaussian mixture models or k-means. Hence, it is unclear how GLSEk can be reduced to projective clustering used in Gaussian mixture models; see [20]. To bypass this, we consider observation vectors of an individual as one entity and design a two-staged framework in which the ﬁrst stage selects a subset of individuals that captures the min operations in the objective function and the second stage applies our coreset construction for GLSE on each selected individuals. As in the case of GLSE, bounding the “sensitivity” (Lemma E.4) of each entity for GLSEk is a key step at the ﬁrst stage. Towards this, we relate the total sensitivity of entities to a certain “ﬂexibility” (Lemma E.3) of each individual regression objective which is, in turn, shown to be controlled by the M -bounded assumption (Deﬁnition 5.1).
We implement our GLSE coreset construction algorithm and test it on synthetic and real-world datasets while varying ε. Our coresets perform well relative to uniform samples on multiple datasets with different generative distributions. Importanty, the relative performance is robust and better on datasets with outliers. The maximum empirical error of our coresets is always below the guaranteed
ε unlike with uniform samples. Further, for comparable levels of empircal error, our coresets perform much better than uniform sampling in terms of sample size and coreset construction speed. 2
1.1