Abstract
Knowledge distillation is a strategy of training a student network with guide of the soft output from a teacher network. It has been a successful method of model compression and knowledge transfer. However, currently knowledge distillation lacks a convincing theoretical understanding. On the other hand, recent ﬁnding on neural tangent kernel enables us to approximate a wide neural network with a linear model of the network’s random features. In this paper, we theoretically analyze the knowledge distillation of a wide neural network. First we provide a transfer risk bound for the linearized model of the network. Then we propose a metric of the task’s training difﬁculty, called data inefﬁciency. Based on this metric, we show that for a perfect teacher, a high ratio of teacher’s soft labels can be beneﬁcial. Finally, for the case of imperfect teacher, we ﬁnd that hard labels can correct teacher’s wrong prediction, which explains the practice of mixing hard and soft labels. 1

Introduction
Deep neural network has been a successful tool in many ﬁelds of artiﬁcial intelligence. However, we typically require deep and complex networks and much effort of training to achieve good generalization. Knowledge distillation(KD) is a method introduced in [9], which can transfer teacher) to another smaller network (i.e. student). knowledge from a large trained model (i.e.
Through distillation, the student network can achieve better performance than direct training from scratch [9]. The vanilla form of KD in classiﬁcation problem has a combined loss of soft and hard labels,
L = ρL(ys, yt) + (1 − ρ)L(ys, yg) where yt and ys are teacher and student’s soft labels , yg are ground truth labels and ρ is called soft ratio.
Apart from this original form of KD, many variants that share the teacher-student paradigm are proposed. [10] uses intermediate layers of neural network to perform distillation. [23] and [25] adopt adversarial training to reduce the difference of label distribution between teacher and student. In
[12] and [14], the authors consider graph-based distillation. Self distillation, proposed in [8], distills the student from an earlier generation of student of same architecture. The latest generation can outperform the ﬁrst generation signiﬁcantly.
A common conjecture on why KD works is that it provides extra information on the output distribution, and student model can use this “dark knowledge” to achieve higher accuracy. However, KD still lacks a convincing theoretical explanation. [21] argues that KD not only transfers super-class correlations to students, but also gives higher credits to correct samples in gradient ﬂow. [2] ﬁnds that KD enables student to learn more task-related pixels and discard the background in image classiﬁcation tasks. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[16] shows that self-distillation has a regularization effect on student’s logits. However, very few works establish a comprehensive view on the knowledge distillation, including risk bound, the role of the soft ratio and how efﬁcient the distillation makes use of data.
In this work, we attempt to deal with these issues with the help of neural tangent kernel and wide network linearization, i.e. considering distillation process for linearized neural networks. We focus on the soft ratio ρ as it serves as a continuous switch between original hard label training and soft label distillation. The main contributions of our work are summarized as follows.
• We experimentally observe faster convergence rate of transfer risk with respect to sample size for softer tasks, i.e. with high ρ. We try to explain this with a new transfer risk bound for converged linearized student networks, based on distribution in random feature space.
We show that the direction of weights converges faster for softer tasks. (Sec. 3)
• We introduce a metric on task’s difﬁculty, called data inefﬁciency. Through this metric we show, for a perfect teacher, early stopping and higher soft ratio are beneﬁcial in terms of making efﬁcient use of data. (Sec. 4)
• We discuss the beneﬁts of hard labels in imperfect teacher distillation in the scenario of
KD practice. We show that a little portion of hard labels can correct student’s outputs pointwisely, and also reduce the angle between student and oracle weight. (Sec. 5)