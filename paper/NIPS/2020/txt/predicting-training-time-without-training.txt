Abstract
We tackle the problem of predicting the number of optimization steps that a pre-trained deep network needs to converge to a given value of the loss function. To do so, we leverage the fact that the training dynamics of a deep network during
ﬁne-tuning are well approximated by those of a linearized model. This allows us to approximate the training loss and accuracy at any point during training by solving a low-dimensional Stochastic Differential Equation (SDE) in function space. Using this result, we are able to predict the time it takes for Stochastic Gradient Descent (SGD) to ﬁne-tune a model to a given loss without having to perform any training.
In our experiments, we are able to predict training time of a ResNet within a 20% error margin on a variety of datasets and hyper-parameters, at a 30 to 45-fold reduction in cost compared to actual training. We also discuss how to further reduce the computational and memory cost of our method, and in particular we show that by exploiting the spectral properties of the gradients’ matrix it is possible to predict training time on a large dataset while processing only a subset of the samples. 1

Introduction
Say you are a researcher with many more ideas than available time and compute resources to test them. You are pondering to launch thousands of experiments but, as the deadline approaches, you wonder whether they will ﬁnish in time, and before your computational budget is exhausted. Could you predict the time it takes for a network to converge, before even starting to train it?
We look to efﬁciently estimate the number of training steps a Deep Neural Network (DNN) needs to converge to a given value of the loss function, without actually having to train the network. This problem has received little attention thus far, possibly due to the fact that the initial training dynamics of a randomly initialized DNN are highly non-trivial to characterize and analyze. However, in most practical applications, it is common to not start from scratch, but from a pre-trained model. This may simplify the analysis, since the ﬁnal solution obtained by ﬁne-tuning is typically not too far from the initial solution obtained after pre-training. In fact, it is known that the dynamics of overparametrized
DNNs [9, 32, 2] during ﬁne-tuning tends to be more predictable and close to convex [25]. Our main contribution is to introduce the problem of predicting training time in realistic use cases, in particular how training time depends on the hyper-parameters and, most importantly, on the interaction between target task and pre-training (which, to the best of our knowledge, is new).
We therefore characterize the training dynamics of a pre-trained network and provide a computation-ally efﬁcient procedure to estimate the expected proﬁle of the loss curve over time. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We use a linearized version of the DNN model around pre-trained weights to study its actual dynamics.
In [21] a similar technique is used to describe the learning trajectories of randomly initialized wide neural networks. Such an approach is inspired by the Neural Tangent Kernel (NTK) for inﬁnitely wide networks [14]. While we note that NTK theory may not correctly predict the dynamics of real (ﬁnite size) randomly initialized networks [12], we show that our linearized approach can be extended to ﬁne-tuning of real networks in a similar vein to [25]. In order to predict ﬁne-tuning Training Time (TT) without training we introduce a Stochastic Differential Equation (SDE) (similar to [13]) to approximate the behavior of SGD: we do so for a linearized DNN and in function space rather than in weight space. That is, rather than trying to predict the evolution of the weights of the network (a
D-dimensional vector), we aim to predict the evolution of the outputs of the network on the training set (a N
C-dimensional vector, where N is the size of the dataset and C the number of network’s
× outputs). This drastically reduces the dimensionality of the problem for over-parametrized networks (that is, when N C
D).
A possible limiting factor of our approach is that the memory requirement to predict the dynamics scales as O(DC 2N 2). This would rapidly become infeasible for datasets of moderate size and for real architectures (D is in the order of millions). To mitigate this, we show that we can use random projections to restrict to a much smaller D0-dimensional subspace with only minimal loss in prediction accuracy. We also show how to estimate Training Time using a small subset of N0 samples, which reduces the total complexity to O(D0 C 2N 2 0 ). We do this by exploiting the spectral properties of the Gram matrix of the gradients. Under mild assumptions the same tools can be used to estimate Training Time on a larger dataset without actually seeing the data. (cid:28)
To summarize, our main contributions are: (i) We present both a qualitative and quantitative analysis of the ﬁne-tuning Training Time as a function of the Gram-Matrix Θ of the gradients at initialization (empirical NTK matrix). (ii) We show how to reduce the cost of estimating the matrix Θ using random projections of the gradients, which makes the method efﬁcient for common architectures and large datasets. (iii) We introduce a method to estimate how much longer a network will need to train if we increase the size of the dataset without actually having to see the data (under the hypothesis that new data is sampled from the same distribution). (iv) We test the accuracy of our predictions on off-the-shelf state-of-the-art models trained on real datasets. We are able to predict the correct training time within a 20% error with 95% conﬁdence over several different datasets and hyperparameters at only a small fraction of the time it would require to actually run the training (30-45x faster in our experiments). 2