Abstract
Neural architecture search (NAS) has been extensively studied in the past few years.
A popular approach is to represent each neural architecture in the search space as a directed acyclic graph (DAG), and then search over all DAGs by encoding the adjacency matrix and list of operations as a set of hyperparameters. Recent work has demonstrated that even small changes to the way each architecture is encoded can have a signiﬁcant effect on the performance of NAS algorithms [22, 24].
In this work, we present the ﬁrst formal study on the effect of architecture encodings for NAS, including a theoretical grounding and an empirical study. First we formally deﬁne architecture encodings and give a theoretical characterization on the scalability of the encodings we study. Then we identify the main encoding-dependent subroutines which NAS algorithms employ, running experiments to show which encodings work best with each subroutine for many popular algorithms. The experiments act as an ablation study for prior work, disentangling the algorithmic and encoding-based contributions, as well as a guideline for future work. Our results demonstrate that NAS encodings are an important design decision which can have a signiﬁcant impact on overall performance.1 1

Introduction
In the past few years, the ﬁeld of neural architecture search (NAS) has seen a steep rise in interest [2], due to the promise of automatically designing specialized neural architectures for any given prob-lem. Techniques for NAS span evolutionary search, Bayesian optimization, reinforcement learning, gradient-based methods, and neural predictor methods. Many NAS instantiations can be described by the optimization problem mina∈A f (a), where A denotes a large set of neural architectures, and f (a) denotes the objective function of interest for a, which is usually a combination of validation accuracy, latency, or number of parameters. A popular approach is to describe each neural architecture a as a labeled directed acyclic graph (DAG), where each node or edge represents an operation.
Due to the complexity of DAG structures and the large size of the space, neural architecture search is typically a highly non-convex, challenging optimization problem. A natural consideration when designing a NAS algorithm is therefore, how should we encode the neural architectures to maximize performance? For example, NAS algorithms may involve manipulating or perturbing architectures, or training a model to predict the accuracy of a given architecture; as a consequence, the representation 1See the full-length paper here: https://arxiv.org/abs/2007.04965. Our code is available at https:
//github.com/naszilla/naszilla. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of the DAG-based architectures may signiﬁcantly change the outcome of these subroutines. The majority of prior work has not explicitly considered this question, opting to use a standard encoding consisting of the adjacency matrix of the DAG along with a list of the operations. Two recent papers have shown that even small changes to the architecture encoding can make a substantial difference in the ﬁnal performance of the NAS algorithm [22, 24]. It is not obvious how to formally deﬁne an encoding for NAS, as prior work deﬁnes encodings in different ways, inadvertently using encodings which are incompatible with other NAS algorithms.
In this work, we provide the ﬁrst formal study on NAS encoding schemes, including a theoretical grounding as well as a set of experimental results. We deﬁne an encoding as a multi-function from an architecture to a real-valued tensor. We deﬁne a number of common encodings from prior work, identifying adjacency matrix-based encodings [26, 24, 21] and path-based encodings [22, 20, 18] as two main paradigms. Adjacency matrix approaches represent the architecture as a list of edges and operations, while path-based approaches represent the architecture as a set of paths from the input to the output. We theoretically characterize the scalability of each encoding by quantifying the information loss from truncation. This characterization is particularly interesting for path-based encodings, which we ﬁnd to exhibit a phase change at rk/n, where r is the number of possible operations, n is the number of nodes, and k is the expected number of edges. In particular, we show that when the size of the path encoding is greater than r2k/n, barely any information is lost, but below rk/(2n), nearly all information is lost. We empirically verify these ﬁndings.
Next, we identify three major encoding-dependent subroutines used in NAS algorithms: sample random architecture, perturb architecture, and train predictor model. We show which of the encodings perform best for each subroutine by testing each encoding within each subroutine for many popular NAS algorithms. Our experiments retroactively provide an ablation study for prior work by disentangling the algorithmic contributions from the encoding-based contributions. We also test the ability of a neural predictor to generalize to new search spaces, using a given encoding. Finally, for encodings in which multiple architectures can map to the same encoding, we evaluate the average standard deviation of accuracies for the equivalence class of architectures deﬁned by each encoding.
Overall, our results show that NAS encodings are an important design decision which must be taken into account not only at the algorithmic level, but at the subroutine level, and which can have a signiﬁcant impact on the ﬁnal performance. Based on our results, we lay out recommendations for which encodings to use within each NAS subroutine. Our experimental results follow the guidelines in the recently released NAS research checklist [9]. In particular, we experiment on two popular NAS benchmark datasets, and we release our code.
Our contributions. We summarize our main contributions below.
• We demonstrate that the choice of encoding is an important, nontrivial question that should be considered not only at the algorithmic level, but at the subroutine level.
• We give a theoretical grounding for NAS encodings, including a characterization of the scalability of each encoding.
• We give an experimental study of architecture encodings for NAS algorithms, disentangling the algorithmic contributions from the encoding-based contributions of prior work, and laying out recommendations for best encodings to use in different settings as guidance for future work. 2