Abstract
In modern multilabel classiﬁcation problems, each data instance belongs to a small number of classes from a large set of classes. In other words, these problems involve learning very sparse binary label vectors. Moreover, in large-scale problems, the labels typically have certain (unknown) hierarchy. In this paper we exploit the sparsity of label vectors and the hierarchical structure to embed them in low-dimensional space using label groupings. Consequently, we solve the classiﬁcation problem in a much lower dimensional space and then obtain labels in the original space using an appropriately deﬁned lifting. Our method builds on the work of [35], where the idea of group testing was also explored for multilabel classiﬁcation. We
ﬁrst present a novel data-dependent grouping approach, where we use a group construction based on a low-rank Nonnegative Matrix Factorization (NMF) of the label matrix of training instances. The construction also allows us, using recent results, to develop a fast prediction algorithm that has a logarithmic runtime in the number of labels. We then present a hierarchical partitioning approach that exploits the label hierarchy in large-scale problems to divide up the large label space and create smaller sub-problems, which can then be solved independently via the grouping approach. Numerical results on many benchmark datasets illustrate that, compared to other popular methods, our proposed methods achieve competitive accuracy with signiﬁcantly lower computational costs. 1

Introduction
Multilabel classiﬁcation (MLC) problems involve learning how to predict a (small) subset of classes a given data instance belongs to from a large set of classes. Given a set of labeled training data i=1 instances with input feature vectors xi ∈ Rp and label vectors yi ∈ {0, 1}d, we wish
{xi, yi}n to learn the relationship between xis and yis in order to predict the label vector of a new data instance. MLC problems are encountered in many domains such as recommendation systems [16], bioinformatics [32], computer vision [9], natural language processing [26], and music [33]. In the large-scale MLC problems that we are interested in, the number of labels d can be as large as O(n) but the (cid:96)0-norm of the label vectors is quite small (constant). In some modern applications, the number of classes can be in the thousands, or even millions [40, 15]. However, the label vectors are typically sparse as individual instances belong to just a few classes. Examples of such large-scale MLC problems include image and video annotation for searches [39, 9], ads recommendation and web
∗Corresponding Author. Email : Shashanka.Ubaru@ibm.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
page categorization [1, 29], tagging text and documents for categorization [34, 16], and others [15].
There are two practical challenges associated with these large-scale MLC problems: (1) how many classiﬁers does one have to train, and later, (2) what is the latency to predict the label vector of a new data instance using these classiﬁers. In the rest of this paper, we address these two challenges.