Abstract
We present Cycle-Contrastive Learning (CCL), a novel self-supervised method for learning video representation. Following a nature that there is a belong and inclusion relation of video and its frames, CCL is designed to ﬁnd correspondences across frames and videos considering the contrastive representation in their domains respectively. It is different from recent approaches that merely learn correspon-dences across frames or clips. In our method, the frame and video representations are learned from a single network based on an R3D architecture, with a shared non-linear transformation for embedding both frame and video features before the cycle-contrastive loss. We demonstrate that the video representation learned by
CCL can be transferred well to downstream tasks of video understanding, outper-forming previous methods in nearest neighbour retrieval and action recognition tasks on UCF101, HMDB51 and MMAct. 1

Introduction
Self-supervised learning has achieved unignorable development in the domains of natural language processing and computer vision recently. Different from supervised learning which depends on manually annotated labels of training data, self-supervised learning utilizes prior knowledge from training data itself to forge supervisory signals for learning representations, such as the consistency of disparate views of the same images[21, 6]. Learning in self-supervised fashion unleashes the potential of massive unlabeled data, and is expected to accelerate many industrial applications of deep learning where acquiring labeled data is expensive or difﬁcult.
Comparing to natural language processing and image recognition where self-supervised learning leads to competitive results, video related tasks on the other hand, are still dominated by supervised learning.
We suggest that the main reason is insufﬁcient adoption of video speciﬁc prior knowledge for learning video representation. Most of existing works take temporal sequence ordering [20, 13, 36] or future frame prediction [8, 28, 18] as pre-text tasks for self-supervised learning of video representation, which assume that the nature of the correspondences across frames or clips could be generalized to represent a video. These methods give effective representations and decent results of downstream tasks, however we suggest that utilizing other nature characteristics of video can lead to different yet representative video representations. We notice that a video has two levels of representation, namely video and frame. Video representation is constructed by the continuous frame representations, thus the video representation is limited by the representation of frames, and reversely is the same.
Inspired by it, we argue that good video representations have certain properties across both video and frame domains. Concretely, the representations of video and its frames are supposed to be close to each other across video and frame domains and distant to all the other videos and frames in corresponding domain, respectively. To make use of this nature, we propose Cycle-Contrastive
Learning (CCL), a self-supervised method based on both cycle-consistency between video and its frames, and contrastive representations in each domain itself, in order to learning representations with 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the above desired properties. To verify that our proposed method can learn a good representation and can be transferred to downstream tasks by ﬁne-tuning, we show a competitive result of CCL on two tasks related to nearest neighbour retrieval and action recognition, demonstrating that CCL can learn a general representation and signiﬁcantly close the gap between the unsupervised and supervised video representation.
The contributions of this paper can be concluded as : (i) We argue that video representation is structured over two domains, video and frame, and a good video representation is supposed to be closed across both domains yet distant to all the other videos and frames in corresponding domain, respectively. (ii) We design cycle-contrastive loss to learn video representation with the above desired properties, and our experiments suggest that learned representations lead to decent results of downstream tasks. 2