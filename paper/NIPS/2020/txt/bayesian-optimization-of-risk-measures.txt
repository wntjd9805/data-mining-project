Abstract
We consider Bayesian optimization of objective functions of the form ρ[F (x, W )], where F is a black-box expensive-to-evaluate function and ρ denotes either the
VaR or CVaR risk measure, computed with respect to the randomness induced by the environmental random variable W . Such problems arise in decision making under uncertainty, such as in portfolio optimization and robust systems design. We propose a family of novel Bayesian optimization algorithms that exploit the struc-ture of the objective function to substantially improve sampling efﬁciency. Instead of modeling the objective function directly as is typical in Bayesian optimization, these algorithms model F as a Gaussian process, and use the implied posterior on the objective function to decide which points to evaluate. We demonstrate the effectiveness of our approach in a variety of numerical experiments. 1

Introduction
Traditional Bayesian optimization (BO) has focused on problems of the form minx F (x), or more generally minx E [F (x, W )], where F is a time-consuming black box function that does not provide derivatives, and W is a random variable. This has seen an enormous impact, and has expanded from hyper-parameter tuning [1, 2] to more sophisticated applications such as drug discovery and robot locomotion [3, 4, 5, 6]. However, in many truly high-stakes settings, optimizing average performance is inappropriate; we must be risk-averse. In such settings, risk measures have become a crucial tool for quantifying risk. For instance, by law, banks are regulated using the Value-at-Risk (VaR) [7]. Risk measures have also been used in cancer treatment planning [8, 9], healthcare operations [10], natural resource management [11], disaster management [12], data-driven stochastic optimization [13], and risk quantiﬁcation in stochastic simulation [14].
In this work, we consider risk averse optimization of the form minx ρ [F (x, W )], where ρ is a risk measure that maps the probability distribution of F (x, W ) (induced by the randomness on W ) onto a real number describing its level of risk. We focus on the setting, where, during the evaluation stage,
F can be evaluated for any (x, w) ∈ X × W, e.g., using a simulation oracle. After the evaluation stage, we choose a decision x∗ to be implemented in the real world, nature chooses the random W , and the objective F (x∗, W ) is then realized.
When F is inexpensive and has convenient analytic structure, optimizing against a risk measure is well understood [15, 16, 17, 18]. However, when F is expensive-to-evaluate, derivative-free, or is a black box, the existing literature is inadequate in answering the problem. A naive approach would be to use the standard BO algorithms with observations of ρ[F (x, W )]. However, a single evaluation of the objective function, ρ[F (·, W )], requires multiple evaluations of F , which can be prohibitively expensive when the evaluations of F are expensive. For example, if each evaluation 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of F takes one hour, and we need 102 samples to obtain a high-accuracy estimate of ρ[F (x, W )], a single evaluation of the objective function would require more than four days. Moreover, if we do not obtain a high-accuracy estimate, estimators are typically biased [19] in a way that is unaccounted for by traditional Gaussian process regression. For the risk measure Value-at-Risk, the recent work of [20] addresses this issue by modeling VaRα[F (x, W )] using individual observations of F (x, w).
However, their method only chooses the x to evaluate, and w is randomly set according to its environmental distribution. As evidenced by [21, 22], jointly selecting x and w to evaluate offers signiﬁcant improvements to query efﬁciency. This ability, unlocked when evaluations are made using a simulation oracle, is leveraged by the methods we introduce here.
As an example of the beneﬁts of choosing w intelligently, consider a function F that is monotone in w. To optimize VaR of F , we only need to evaluate a single value of w, the one that corresponds to the VaR, and evaluating any other w is simply inefﬁcient. In a black-box setting, we would not typically know that F was monotone but, by modeling F directly, we can discover the regions of w that matter and focus most of our effort into selectively evaluating w from these regions.
In this paper, we focus on two risk measures commonly used in practice, Value-at-Risk (VaR) and
Conditional Value-at-Risk (CVaR); and develop a novel approach that overcomes the aforementioned challenges. Our contributions are summarized as follows:
• To the best of our knowledge, our work is the ﬁrst to consider BO of risk measures while leveraging the ability to choose x and w at query time. The selection of w enables efﬁcient search of the solution space, and is a signiﬁcant contributor to the success of our algorithms.
• We provide a novel one-step Bayes optimal algorithm ρKG, and a fast approximation
ρKGapx that performs well in numerical experiments; signiﬁcantly improving the sampling efﬁciency over the state-of-the-art BO methods.
• We combine ideas from different strands of literature to derive gradient estimators for efﬁcient optimization of ρKG and ρKGapx, which are shown to be asymptotically unbiased and consistent.
• To further improve the computational efﬁciency, we propose a two time scale optimization approach that is broadly applicable for optimizing acquisition functions whose computation involves solving an inner optimization problem.
The remainder of this paper is organized as follows. Section 2 provides a brief background on BO and risk measures. Section 3 formally introduces the problem setting. Section 4 introduces the statistical model on F , a Gaussian process (GP) prior, and explains how to estimate VaR and CVaR of a GP.
Section 5 introduces ρKG, a knowledge gradient type of acquisition function for optimization of
VaR and CVaR, along with a cheaper approximation, ρKGapx, and efﬁcient optimization of both acquisition functions. Section 6 presents numerical experiments demonstrating the performance of the algorithms developed here. Finally, the paper is concluded in Section 7. 2