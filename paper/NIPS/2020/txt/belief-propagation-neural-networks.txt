Abstract
Learned neural solvers have successfully been used to solve combinatorial opti-mization and decision problems. More general counting variants of these problems, however, are still largely solved with hand-crafted solvers. To bridge this gap, we introduce belief propagation neural networks (BPNNs), a class of parameterized operators that operate on factor graphs and generalize Belief Propagation (BP).
In its strictest form, a BPNN layer (BPNN-D) is a learned iterative operator that provably maintains many of the desirable properties of BP for any choice of the parameters. Empirically, we show that by training BPNN-D learns to perform the task better than the original BP: it converges 1.7x faster on Ising models while pro-viding tighter bounds. On challenging model counting problems, BPNNs compute estimates 100’s of times faster than state-of-the-art handcrafted methods, while returning an estimate of comparable quality. 1

Introduction
Probabilistic inference problems arise in many domains, from statistical physics to machine learning.
There is little hope that efﬁcient, exact solutions to these problems exist as they are at least as hard as
NP-complete decision problems. Signiﬁcant research has been devoted across the ﬁelds of machine learning, statistics, and statistical physics to develop variational and sampling based methods to approximate these challenging problems [13, 34, 48, 6, 38]. Variational methods such as Belief
Propagation (BP) [31] are computationally efﬁcient and have been particularly successful at providing principled approximations due to extensive theoretical analysis.
While BP provably lower bounds the partition function for classes of factor graphs, these bounds are not reliably tight. Handcrafting alternative algorithms that are specialized to problem domains and that provide bounds is laborious. We introduce belief propagation neural networks (BPNNs), a
ﬂexible neural architecture designed to estimate the partition function of a factor graph that leverage the theoretical analysis behind BP. BPNNs generalize BP and can thus provide more accurate estimates than BP when trained on a small number of factor graphs with known partition functions.
During training BPNNs learn a modiﬁcation to the standard BP message passing operations so that the ﬁnal output is closer to the ground truth partition function. At the same time, BPNNs retain many of BP’s properties, which results in more accurate estimates compared to general neural architectures. BPNNs are composed of iterative layers (BPNN-D) and an optional Bethe free energy layer (BPNN-B), both of which maintain the symmetries of BP under factor graph isomorphisms.
BPNN-D is a parametrized iterative operator that strictly generalizes BP while preserving many of
BP’s guarantees. Like BP, BPNN-D is guaranteed to converge on tree structured factor graphs and return the exact partition function. For factor graphs with loops, BPNN-D computes a lower bound whenever the Bethe approximation obtained from ﬁxed points of BP is a provable lower bound (with mild restrictions on BPNN-D). BPNN-B performs regression from the trajectory of beliefs (over a
ﬁxed number of iterations) to the partition function of the input factor graph. While this sacriﬁces 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
some guarantees, the additional ﬂexibility introduced by BPNN-B generally improves estimation performance.
Experimentally, we show that on Ising models BPNN-D is able to converge faster than standard
BP and frequently ﬁnds better ﬁxed points that provide tighter lower bounds. BPNN-D generalizes well to Ising models sampled from a different distribution than seen during training and to models with nearly twice as many variables as seen during training, providing estimates of the log partition function that are signiﬁcantly better than BP or a standard graph neural network (GNN) in these settings. We also perform experiments on community detection problems, where BP is known to perform well both empirically and theoretically, and show improvements over BP and a standard
GNN. We then perform experiments on approximate model counting [46, 27, 28, 8], the problem of computing the number of solutions to a Boolean satisﬁability (SAT) problem. Unlike the ﬁrst two experiments it is very difﬁcult for BP to converge in this setting. Still, we ﬁnd that BPNN learns to estimate accurate model counts from a training set of 10’s of problems and generalize to problems that are signiﬁcantly harder for an exact model counter to solve. Compared to handcrafted approximate model counters, BP returns comparable estimates 100’s times faster using GPU computation. 2