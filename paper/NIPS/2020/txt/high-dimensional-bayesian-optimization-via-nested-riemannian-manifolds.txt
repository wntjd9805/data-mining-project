Abstract
Despite the recent success of Bayesian optimization (BO) in a variety of appli-cations where sample efﬁciency is imperative, its performance may be seriously compromised in settings characterized by high-dimensional parameter spaces. A solution to preserve the sample efﬁciency of BO in such problems is to intro-duce domain knowledge into its formulation. In this paper, we propose to exploit the geometry of non-Euclidean search spaces, which often arise in a variety of domains, to learn structure-preserving mappings and optimize the acquisition func-tion of BO in low-dimensional latent spaces. Our approach, built on Riemannian manifolds theory, features geometry-aware Gaussian processes that jointly learn a nested-manifold embedding and a representation of the objective function in the latent space. We test our approach in several benchmark artiﬁcial landscapes and report that it not only outperforms other high-dimensional BO approaches in several settings, but consistently optimizes the objective functions, as opposed to geometry-unaware BO methods. 1

Introduction
Bayesian optimization (BO) is considered as a powerful machine-learning based optimization method to globally maximize or minimize expensive black-box functions [50]. Thanks to its ability to model complex noisy cost functions in a data-efﬁcient manner, BO has been successfully applied in a variety of applications ranging from hyperparameters tuning for machine learning algorithms [51] to the optimization of parametric policies in challenging robotic scenarios [11, 16, 41, 49]. However, BO performance degrades as the search space dimensionality increases, which recently opened the door to different approaches dealing with the curse of dimensionality.
A common assumption in high-dimensional BO approaches is that the objective function depends on a limited set of features, i.e. that it evolves along an underlying low-dimensional latent space.
Following this hypothesis, various solutions based either on random embeddings [57, 43, 9] or on latent space learning [13, 23, 42, 59] have been proposed. Although these methods perform well on a variety of problems, they usually assume simple bound-constrained domains and may not be straightforwardly extended to complicatedly-constrained parameter spaces. Interestingly, several works proposed to further exploit the observed values of the objective function to determine or shape the latent space in a supervised manner [59, 42, 4]. However, the integration of a priori domain knowledge related to the parameter space is not considered in the learning process. Moreover, the aforementioned approaches may not comply easily to recover query points in a complex parameter space from those computed on the learned latent space.
Other relevant works in high-dimensional BO substitute or combine the low-dimensional assumption with an additive property, assuming that the objective function is decomposed as a sum of functions of low-dimensional sets of dimensions [33, 37, 21, 44, 24]. Therefore, each low-dimensional partition 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) S 2 → S 1 (b) S 3
++ → S 2
++
Figure 1: Illustration of the low-dimensional assumption on Riemannian manifolds. (a) The function on S 2 is not inﬂuenced by the value of x1 and may be represented more efﬁciently on the manifold S 1. (b) The stiffness matrix of a robot is optimized to push objects lying on a table. As the stiffness along the axis x3 does not inﬂuence the pushing skill, the cost function may be better represented in a latent space S 2
++. Note that the manifolds dimensionality is limited here due to the difﬁculty of visualizing high-dimensional parameter spaces.
However, these examples are extensible to higher dimensions. can be treated independently. In a similar line, inspired by the dropout algorithm in neural networks, other approaches proposed to deal with high-dimensional parameter spaces by optimizing only a random subset of the dimensions at each iteration [36]. Although the aforementioned strategies are well adapted for simple Euclidean parameter spaces, they may not generalize easily to complex domains. If the parameter space is not Euclidean or must satisfy complicated constraints, the problem of partitioning the space into subsets becomes difﬁcult. Moreover, these subsets may not be easily and independently optimized as they must satisfy global constraints acting on the parameters domain.
Introducing domain knowledge into surrogate models and acquisition functions has recently shown to improve the performance and scalability of BO [11, 3, 45, 30, 14]. Following this research line, we hypothesize that building and exploiting geometry-aware latent spaces may improve the performance of BO in high dimensions by considering the intrinsic geometry of the parameter space. Fig. 1 illustrates this idea for two Riemannian manifolds widely used (see § 2 for a short background).
The objective function on the sphere S 2 (Fig. 1a) does not depend on the value x1 and is therefore better represented on the low-dimensional latent space S 1. In Fig. 1b, the stiffness matrix X ∈ S 3 of a robot controller is optimized to push objects lying on a table, with S d
++ the manifold of d × d symmetric positive deﬁnite (SPD) matrices. In this case, the stiffness along the vertical axis x3 does not inﬂuence the robot’s ability to push the objects. We may thus optimize the stiffness along the axes x1 and x2, i.e., in the latent space S 2
++. Therefore, similarly to high-dimensional BO frameworks where a Euclidean latent space of the Euclidean parameter space is exploited, the objective functions may be efﬁciently represented in a latent space that inherits the geometry of the original Riemannian manifold. In general, this latent space is unknown and may not be aligned with the coordinate axes.
++
Following these observations, this paper proposes a novel high-dimensional geometry-aware BO framework (hereinafter called HD-GaBO) for optimizing parameters lying on low-dimensional
Riemannian manifolds embedded in high-dimensional spaces. Our approach is based on a geometry-aware surrogate model that learns both a mapping onto a latent space inheriting the geometry of the original space, and the representation of the objective in this latent space (see § 3). The next query point is then selected on the low-dimensional Riemannian manifold using geometry-aware optimization methods. We evaluate the performance of HD-GaBO on various benchmark functions and show that it efﬁciently and reliably optimizes high-dimensional objective functions that feature an intrinsic low dimensionality (see § 4). Potential applications of our approach are discussed in § 5. 2