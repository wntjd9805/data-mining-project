Abstract
We establish ﬁnite-sample guarantees for a polynomial-time algorithm for learning a nonlinear, nonparametric directed acyclic graphical (DAG) model from data.
The analysis is model-free and does not assume linearity, additivity, independent noise, or faithfulness. Instead, we impose a condition on the residual variances that is closely related to previous work on linear models with equal variances.
Compared to an optimal algorithm with oracle knowledge of the variable ordering, the additional cost of the algorithm is linear in the dimension d and the number of samples n. Finally, we compare the proposed algorithm to existing approaches in a simulation study. 1

Introduction
Modern machine learning (ML) methods are driven by complex, high-dimensional, and nonparametric models that can capture highly nonlinear phenomena. These models have proven useful in wide-ranging applications including vision, robotics, medicine, and natural language. At the same time, the complexity of these methods often obscure their decisions and in many cases can lead to wrong decisions by failing to properly account for—among other things—spurious correlations, adversarial vulnerability, and invariances [5, 7, 50]. This has led to a growing literature on correcting these problems in ML systems. A particular example of this that has received widespread attention in recent years is the problem of causal inference, which is closely related to these issues. While substantial methodological progress has been made towards embedding complex methods such as deep neural networks and RKHS embeddings into learning causal graphical models [21, 26, 31, 34, 61, 64, 65], theoretical progress has been slower and typically reserved for particular parametric models such as linear [1–3, 9, 15, 16, 29, 58, 59], generalized linear models [37, 40], and discrete models [6, 66].
In this paper, we study the problem of learning directed acyclic graphs (DAGs) from data in a nonparametric setting. Unlike existing work on this problem, we do not require linearity, additivity, independent noise, or faithfulness. Our approach is model-free and nonparametric, and uses non-parametric estimators (kernel smoothers, neural networks, splines, etc.) as “plug-in” estimators. As such, it is agnostic to the choice of nonparametric estimator chosen. Unlike existing consistency theory in the nonparametric setting [8, 20, 21, 35, 45, 49, 56], we provide explicit (nonasymptotic)
ﬁnite sample complexity bounds and show that the resulting method has polynomial time complexity.
The method we study is closely related to existing algorithms that ﬁrst construct a variable ordering
[9, 15, 16, 39]. Despite this being a well-studied problem, to the best of our knowledge our analysis is the ﬁrst to provide explicit, simultaneous statistical and computational guarantees for learning nonparametric DAGs.
Contributions Figure 1a illustrates a key motivation for our work: While there exist methods that obtain various statistical guarantees, they lack provably efﬁcient algorithms, or vice versa. As a result, these methods can fail in simple settings. Our focus is on simultaneous computational and statistical 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b)
Figure 1: (a) Existing methods may not ﬁnd a correct topological ordering in simple settings when d = 3. (b) Example of a layer decomposition L(G) of a DAG on d = 6 nodes. guarantees that are explicit and nonasymptotic in a model-free setting. More speciﬁcally, our main contributions are as follows:
• We show that the algorithms of Ghoshal and Honorio [15] and Chen et al. [9] rigourously extend to a model-free setting, and provide a method-agnostic analysis of the resulting extension (Theorem 4.1). That is, the time and sample complexity bounds depend on the choice of estimator used, and this dependence is made explicit in the bounds (Section 3.2,
Section 4).
• We prove that this algorithm runs in at most O(nd5) time and needs at most Ω((d2/ε)1+d/2) samples (Corollary 4.2). Moreover, the exponential dependence on d can be improved by imposing additional sparsity or smoothness assumptions, and can even be made polynomial (see Section 4 for discussion). This is an expected consequence of our estimator-agnostic approach.
• We show how existing identiﬁability results based on ordering variances can be uniﬁed and generalized to include model-free families (Theorem 3.1, Section 3.1).
• We show that greedy algorithms such as those used in the CAM algorithm [8] can provably fail to recover an identiﬁable DAG (Example 5), as shown in Figure 1a (Section 3.3).
• Finally, we run a simulation study to evaluate the resulting algorithm in a variety of settings against seven state-of-the-art algorithms (Section 5).
Our simulation results can be summarized as follows: When implemented using generalized additive models [19], our method outperforms most state-of-the-art methods, particularly on denser graphs with hub nodes. We emphasize here, however, that our main contributions lay in the theoretical analysis, speciﬁcally providing a polynomial-time algorithm with sample complexity guarantees.