Abstract
Stein Variational Gradient Descent (SVGD), a popular sampling algorithm, is often described as the kernelized gradient ﬂow for the Kullback-Leibler divergence in the geometry of optimal transport. We introduce a new perspective on SVGD that instead views SVGD as the (kernelized) gradient ﬂow of the chi-squared diver-gence which, we show, exhibits a strong form of uniform exponential ergodicity under conditions as weak as a Poincar´e inequality. This perspective leads us to propose an alternative to SVGD, called Laplacian Adjusted Wasserstein Gradient
Descent (LAWGD), that can be implemented from the spectral decomposition of the Laplacian operator associated with the target density. We show that LAWGD exhibits strong convergence guarantees and good practical performance. 1

Introduction
The seminal paper of Jordan, Kinderlehrer, and Otto [JKO98] has profoundly reshaped our under-standing of sampling algorithms. What is now commonly known as the JKO scheme interprets the evolution of marginal distributions of a Langevin diffusion as a gradient ﬂow of a Kullback-Leibler (KL) divergence over the Wasserstein space of probability measures. This optimization perspective on
Markov Chain Monte Carlo (MCMC) has not only renewed our understanding of algorithms based on
Langevin diffusions [Dal17a; Ber18; CB18; Wib18; DMM19; VW19], but has also fueled the discov-ery of new MCMC algorithms inspired by the diverse and powerful optimization toolbox [Mar+12;
Sim+16; Che+18; Ber18; Hsi+18; Wib18; Ma+19; Wib19; Che+20; DR20; Zha+20b].
In order to arrive at a practical sampling algorithm, one must discretize the Wasserstein gradient ﬂow.
The most common discretization is to discretize the Langevin diffusion, resulting in the Unadjusted
Langevin Algorithm (ULA) [Dal17b; DM17]. However, it is unclear whether this diffusion based discretization is the most effective one. In fact, ULA is asymptotically biased, which results in slow convergence and often requires ad-hoc adjustments [Dwi+19]. To overcome this limitation, various methods that track the Wasserstein gradient ﬂow more closely have been recently developed [Ber18;
Wib18; SKL20].
An alternative sampling approach that avoids diffusions is to construct a sequence of deterministic mappings that approximately pushes forward an initial distribution to the target distribution. Let F denote a functional over the Wasserstein space of distributions. The Wasserstein gradient ﬂow of F may be described as the deterministic and time-inhomogeneous Markov process (Xt)t≥0 started at a random variable X0 ∼ µ0 and evolving according to ˙Xt = −[∇W2 F (µt)](Xt), where µt denotes the distribution of Xt. Here [∇W2F (µ)](·) : Rd → Rd is the Wasserstein gradient of F at µ. If 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
F (µ) = DKL(µ (cid:107) π), where π ∝ e−V is a given target distribution on Rd, it is known [AGS08;
Vil09; San17] that ∇W2 F (µ) = ∇ ln(dµ/dπ). Therefore, a natural discretization of the Wasserstein gradient ﬂow with step size h > 0, albeit one that cannot be implemented since it depends on the distribution µt of Xt, is:
Xt+1 = Xt − h∇ ln (cid:0) dµt dπ (Xt)(cid:1), t = 0, 1, 2, . . . .
While µt can, in principle, be estimated by evolving a large number of particles X [1]
, . . . , X [N ]
, t estimation of µt is hindered by the curse of dimensionality and this approach still faces signiﬁcant computational challenges despite attempts to improve the original JKO scheme [SKL20; WL20]. t
A major advance in this direction was achieved by allowing for approximate Wasserstein gradients, which makes the push forward maps tractable. More speciﬁcally, Stein Variational Gradient De-scent (SVGD), recently proposed by [LW16] (see Section 2 for more details), consists in replacing
∇W2F (µ) by its image Kµ∇W2F (µ) under the integral operator Kµ : L2(µ) → L2(µ) associated to a chosen kernel K : Rd × Rd → R and deﬁned by Kµf (x) := (cid:82) K(x, y)f (y) dµ(y) for f ∈ L2(µ).
This leads to the following process:
Xt+1 = Xt − h[Kµt∇W2 F (µt)](Xt) , t = 0, 1, 2, . . . . (SVGDp) where we apply the integral operator Kµt individually to each coordinate of the Wasserstein gradient.
In turn, this kernelization trick overcomes most of the above computational bottleneck. Building on this perspective, [DNS19] introduced a new geometry, different from the Wasserstein geometry and which they call the Stein geometry, in which the continuous limit of (SVGDp) becomes the gradient
ﬂow of the KL divergence.
However, despite this recent advance, the theoretical properties of SVGD are still largely unexplored, resulting in little understanding of SVGD’s known problems, such as mode collapse or a lack of guidance on how to choose an appropriate kernel K. Consequently diffusion-based algorithms remain the dominant choice for applications. In this work, we we provide a new and stronger theoretical footing for the development of such deterministic mappings.
Our contributions. We introduce, in Section 2.3, a new perspec-tive on SVGD by viewing it as kernelized gradient ﬂow of the chi-squared divergence rather than the KL divergence. This per-spective is fruitful in two ways. First, it uses a single integral operator Kπ—as opposed to (SVGDp), which requires a family of integral operators Kµ, µ (cid:28) π—providing a conceptually clear guideline for choosing K, namely: K should be chosen to make
Kπ approximately equal to the identity operator. Second, under the idealized choice Kπ = id, we show that this gradient ﬂow converges exponentially fast in KL divergence as soon as the target distribution π satisﬁes a Poincar´e inequality. In fact, our results are stronger than exponential convergence and they highlight strong uniform ergodicity: the gradient ﬂow forgets the initial distribution after a ﬁnite time that is at most half of the Poincar´e constant. To establish this exponential convergence under a relatively weak con-dition (Poincar´e inequality), we employ the following technique. While the gradient ﬂow aims at minimizing the chi-squared divergence by following the curve in Wasserstein space with steepest descent, we do not track its progress with the objective function itself, the chi-squared divergence, but instead we track it with the KL divergence. This is in a sense dual to argument employed in [Che+20], where the chi-squared divergence is used to track the progress of a gradient ﬂow on the KL divergence. A more standard analysis relying on Łojasiewicz inequalities also yields rates of convergence on the chi-squared divergence under stronger assumptions such as a log-Sobolev inequality, and log-concavity. These results establish the ﬁrst ﬁnite-time theoretical guarantees for
SVGD in an idealized setting.
Figure 1: Sampling from a mix-ture of two 2D Gaussians with
LAWGD. See Appendix C.
Beyond providing a better understanding of SVGD, our novel perspective is instrumental in the development of a new sampling algorithm, which we call Laplacian Adjusted Wasserstein Gradient
Descent (LAWGD) and present in Section 4. We show that it possesses a striking theoretical property: 2
assuming that the target distribution π satisﬁes a Poincar´e inequality, LAWGD converges exponen-tially fast, with no dependence on the Poincar´e constant. This scale invariance has been recently demonstrated for the Newton-Langevin diffusion [Che+20], but under the additional assumption that
π is log-concave. A successful implementation of LAWGD hinges on the spectral decomposition of a certain differential operator which is within reach of modern PDE solvers. As a proof of concept, we show that LAWGD, implemented using a na¨ıve ﬁnite differences method, performs well on mixtures of Gaussians in one or two dimensions, whereas SVGD fails. This is an indication that our novel perspective could be the correct one to further advance the state-of-the-art for sampling via deterministic mappings. Implementing LAWGD in high dimensions is challenging, and we are not advocating for it as the deﬁnitive solution of the sampling problem. Instead, LAWGD serves as the start of a family of interacting particle systems with an interacting potential that depends strongly and non-trivially on the target distribution and furthermore comes with strong theoretical guarantees. We hope this work can encourage further research in the application of numerical PDEs for sampling.