Abstract
Meta-reinforcement learning (meta-RL) aims to learn from multiple training tasks the ability to adapt efﬁciently to unseen test tasks. Despite the success, existing meta-RL algorithms are known to be sensitive to the task distribution shift. When the test task distribution is different from the training task distribution, the per-formance may degrade signiﬁcantly. To address this issue, this paper proposes
Model-based Adversarial Meta-Reinforcement Learning (AdMRL), where we aim to minimize the worst-case sub-optimality gap – the difference between the optimal return and the return that the algorithm achieves after adaptation – across all tasks in a family of tasks, with a model-based approach. We propose a minimax objective and optimize it by alternating between learning the dynamics model on a ﬁxed task and ﬁnding the adversarial task for the current model – the task for which the policy induced by the model is maximally suboptimal. Assuming the family of tasks is parameterized, we derive a formula for the gradient of the suboptimality with respect to the task parameters via the implicit function theorem, and show how the gradient estimator can be efﬁciently implemented by the conjugate gradient method and a novel use of the REINFORCE estimator. We evaluate our approach on several continuous control benchmarks and demonstrate its efﬁcacy in the worst-case performance over all tasks, the generalization power to out-of-distribution tasks, and in training and test time sample efﬁciency, over existing state-of-the-art meta-RL algorithms. 1

Introduction
Deep reinforcement learning (Deep RL) methods can solve difﬁcult tasks such as Go [45], Atari games [30], robotic control [23] successfully, but often require sampling a large amount interactions with the environment. Meta-reinforcement learning and multi-task reinforcement learning aim to improve the sample efﬁciency by leveraging the shared structure within a family of tasks. For example, Model Agnostic Meta Learning (MAML) [13] learns in the training time a shared policy initialization across tasks, from which in the test time it can adapt to the new tasks quickly with a small amount of samples. The more recent work PEARL [38] learns latent representations of the tasks in the training time, and then infers the representations of test tasks and adapts to them.
The existing meta-RL formulation and methods are largely distributional. The training tasks and the testing tasks are assumed to be drawn from the same distribution of tasks. Consequently, the existing methods are prone to the distribution shift issue, as shown in [27] — when the tasks in the test time are not drawn from the same distribution as in the training, the performance degrades signiﬁcantly. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The performance of PEARL [38] on Ant2D-velocity tasks. Each task is represented by the target velocity (x, y) ∈ R2 with which the ant should run. The training tasks are uniformly drawn in [−3, 3]2. The color of each cell shows the sub-optimality gap of the corresponding task, namely, the optimal return of that task minus the return of PEARL. Lighter means smaller sub-optimality gap and is better. High-velocity tasks tend to perform worse, which implies that if the test task distribution shift towards high-velocity tasks, the performance will degrade.
Figure 1 also conﬁrms this issue for PEARL [38], a recent state-of-the-art meta-RL method, on the
Ant2D-velocity tasks. PEARL can adapt to tasks with smaller goal velocities much better than tasks with larger goal velocities, in terms of the relative difference, or the sub-optimality gap, from the optimal policy of the corresponding task.1 To address this issue, Mehta et al. [27] propose an algorithm that iteratively re-deﬁne the task distribution to focus more on the hard task.
In this paper, we instead take a non-distributional perspective by formulating the adversarial meta-RL problem. Given a parametrized family of tasks, we aim to minimize the worst sub-optimality gap — the difference between the optimal return and the return the algorithm achieves after adaptation — across all tasks in the family in the test time. This can be naturally formulated mathematically as a minimax problem (or a two-player game) where the maximum is over all the tasks and the minimum is over the parameters of the algorithm (e.g., the shared policy initialization or the shared dynamics).
Our approach is model-based. We learn a shared dynamics model across the tasks in the training time, and during the test time, given a new reward function, we train a policy on the learned dynamics. The model-based methods can outperform signiﬁcantly the model-free methods in sample-efﬁciency even in the standard single task setting [5, 8, 9, 12, 17, 20, 25, 33, 36, 37, 55, 56], and are particularly suitable for meta-RL settings where the optimal policies for tasks are very different, but the underlying dynamics is shared [22]. We apply the natural adversarial training [26] on the level of tasks — we alternate between the minimizing the sub-optimality gap over the parameterized dynamics and maximizing it over the parameterized tasks.
The main technical challenge is to optimize over the task parameters in a sample-efﬁcient way. The sub-optimality gap objective depends on the task parameters in a non-trivial way because the algorithm uses the task parameters iteratively in its adaptation phase during the test time. The naive attempt to back-propagate through the sequential updates of the adaptation algorithm is time costly, especially because the adaptation time in the model-based approach is computationally expensive (despite being sample-efﬁcient). Inspired by a recent work on learning equilibrium models in supervised learning [2], we derive an efﬁcient formula of the gradient w.r.t. the task parameters via the implicit function theorem. The gradient involves an inverse Hessian vector product, which can be efﬁciently computed by conjugate gradients and the REINFORCE estimator [58].
In summary, our contributions are: 1. We propose a minimax formulation of model-based adversarial meta-reinforcement learning (AdMRL, pronounced like “admiral”) with an adversarial training algorithm to address the distribution shift problem. 2. We derive an estimator of the gradient with respect to the task parameters, and show how it can be implemented efﬁciently in both samples and time. 3. Our approach signiﬁcantly outperforms the state-of-the-art meta-RL algorithms in the worst-case performance over all tasks, the generalization power to out-of-distribution tasks, and in training and test time sample efﬁciency on a set of continuous control benchmarks. 2