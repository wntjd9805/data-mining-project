Abstract
We present a new approach for efﬁcient exploration which leverages a low-dimensional encoding of the environment learned with a combination of model-based and model-free objectives. Our approach uses intrinsic rewards that are based on the distance of nearest neighbors in the low dimensional representa-tional space to gauge novelty. We then leverage these intrinsic rewards for sample-efﬁcient exploration with planning routines in representational space for hard ex-ploration tasks with sparse rewards. One key element of our approach is the use of information theoretic principles to shape our representations in a way so that our novelty reward goes beyond pixel similarity. We test our approach on a number of maze tasks, as well as a control problem and show that our exploration approach is more sample-efﬁcient compared to strong baselines. 1

Introduction
In order to solve a task efﬁciently in Reinforcement Learning (RL), one of the main challenges is to gather informative experiences via an efﬁcient exploration of the state space. A common approach to exploration is to leverage intrinsic rewards correlated with some metric or score for novelty (Schmidhuber, 2010; Stadie et al., 2015; Houthooft et al., 2016). With intrinsic rewards, an agent can be incentivized to efﬁciently explore its state space. A direct approach to calculating these novelty scores is to derive a reward based on the observations, such as a count-based reward (Bellemare et al., 2016; Ostrovski et al., 2017) or a prediction-error based reward (Burda et al., 2018b). However, an issue occurs when measuring novelty directly from the raw observations, as some information in pixel space (such as randomness or backgrounds) may be irrelevant. In this case, if an agent wants to efﬁciently explore its state space it should only focus on meaningful and novel information.
In this work, we propose a method of sample-efﬁcient exploration by leveraging intrinsic rewards in a meaningful latent state space. To build a meaningful state abstraction, we view Model-based RL (MBRL) from an information theoretic perspective - we optimize our dynamics learning through the Information Bottleneck (Tishby et al., 2000) principle. We also combine both model-based and model-free components through a joint representation. This method encodes high-dimensional observations into lower-dimensional representations such that states that are close in dynamics are brought close together in representation space (Franc¸ois-Lavet et al., 2018). We also add additional constraints to ensure that a measure of distance between abstract states is meaningful. We leverage these properties of our representation to formulate a novelty score based on Euclidean distance in low-dimensional representation space and we then use this score to generate intrinsic rewards that we can exploit for efﬁcient exploration.
One important element of our exploration algorithm is that we take a Model Predictive Control (MPC) approach (Garcia et al., 1989) and perform actions only after our model is sufﬁciently ac-curate (and hence ensure an accurate novelty heuristic). Through this training scheme, our agent is 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
also able to learn a meaningful representation of its state space in a sample-efﬁcient manner. The code with all experiments is available 1. 2 Problem setting
An agent interacts with its environment over discrete timesteps, modeled as a Markov Decision
Process (MDP), deﬁned by the 6-tuple (S, S0, A, τ, R, G) (Puterman, 1994). In this setting, S is the state space, S0 is the initial state distribution, A is the discrete action space, τ : S × A → S is the transition function that is assumed deterministic (with the possibility of extension to stochastic environments with generative methods), R : S × A → R is the reward function (R = [−1, 1]),
G : S × A → [0, 1) is the per timestep discount factor. At timestep t in state st ∈ S, the agent chooses an action at ∈ A based on policy π : S × A → [0, 1], such that at ∼ π(st, ·). After taking at, the agent is in state st+1 = τ (st, at) and receives reward rt ∼ R(st, at) and a discount factor γt ∼ G(st, at). Over n environment steps, we deﬁne the buffer of previously visited states as B = (s1, . . . , sn), where si ∈ S ∀i ∈ N. In RL, the usual objective is to maximize the sum of expected future rewards Vπ(s) = Eπ rt + (cid:80)∞ rt+i|s = st (cid:16)(cid:81)i−1 (cid:17) (cid:104) (cid:105)
. j=0 γt+j i=1
To learn a policy π that maximizes the expected return, an RL agent has to efﬁciently explore its environment (reach novel states in as few steps as possible). In this paper, we consider tasks with sparse rewards or even no rewards, and are interested in exploration strategies that require as few steps as possible to explore the state space. 3 Abstract state representations
We focus on learning a lower-dimensional representation of state when our state (or observations in the partially observable case (Kaelbling et al., 1998)) is high-dimensional (Dayan, 1993; Tamar et al., 2016; Silver et al., 2016; Oh et al., 2017; de Bruin et al., 2018; Ha and Schmidhuber, 2018;
Franc¸ois-Lavet et al., 2018; Hafner et al., 2018; Gelada et al., 2019). 3.1 Information Bottleneck
We ﬁrst motivate our methods for model learning. To do so, we consider the Information Bottleneck (IB) (Tishby et al., 2000) principle. Let Z denote the original source message space and ˜Z denote its compressed representation. As opposed to traditional lossless compression where we seek to
ﬁnd corresponding encodings ˜Z that compresses all aspects of Z, in IB we seek to preserve only relevant information in ˜Z with regards to another relevance variable, Y . For example when looking to compress speech waveforms (Z) if our task at hand is speech recognition, then our relevance variable Y would be a transcript of the speech. Our representation ˜Z would only need to maximize relevant information about the transcript Y instead of its full form including tone, pitch, background noise etc. We can formulate this objective by minimizing the following functional with respect to p(˜z | z):
L(p(˜z | z)) = I[Z; ˜Z] − βI[ ˜Z; Y ] where I[·; ·] is the Mutual Information (MI) between two random variables. β is the Lagrange multiplier for the amount of information our encoding ˜Z is allowed to quantify about Y . This corresponds to a trade-off between minimizing the encoding rate I[Z; ˜Z] and maximizing the mutual information between the encoding and our random variable Y .
We now apply this principle to representation learning of state in MBRL. If our source message space is our state S(cid:48) and our encoded message is X (cid:48), then to distill the most relevant information with regards to the dynamics of our environment one choice of relevance variable is {X, A}, i.e. our encoded state in the previous timestep together with the presence of an action. This gives us the functional
L(p(x(cid:48) | s(cid:48))) = I[S(cid:48); X (cid:48)] − βI[X (cid:48); {X, A}]. (1)
In our work, we look to ﬁnd methods to minimize this functional for an encoding that maximizes the predictive ability of our dynamics model.
We ﬁrst aim to minimize our encoding rate I[S(cid:48); X (cid:48)]. Since encoding rate is a measure of the amount of bits transmitted per message S(cid:48), representation dimension is analogous to number of bits per mes-sage. This principle of minimizing encoding rate guides our selection of representation dimension 1https://github.com/taodav/nsrs 2
- for every environment, we try to choose the smallest representation dimension possible such that the representation can still encapsulate model dynamics as we understand them. For example, in a simple Gridworld example, we look to only encode agent position in the grid-world.
Now let us consider the second term in Equation 1. Our goal is to learn an optimally predictive model of our environment. To do so we ﬁrst consider the MI between the random variable denoting our state representation X, in the presence of the random variable representing actions A and the random variable denoting the state representation in the next timestep X (cid:48) (Still, 2009). Note that MI is a metric and is symmetric:
I[{X, A} ; X (cid:48)] = Ep(x(cid:48),x,a) (cid:20) log (cid:18) p(x(cid:48) | x, a) p(x(cid:48)) (cid:19)(cid:21)
= H[X (cid:48)] − H[X (cid:48) | X, A] (2)
This quantity is a measure of our dynamics model’s predictive ability. If we consider the two entropy terms (denoted H[·]), we see that H[X (cid:48)] constitutes the entropy of our state representation and
H[X (cid:48) | X, A] as the entropy of the next state X (cid:48) given our current state X and an action A. Recall that we are trying to minimize I[X (cid:48); S(cid:48)] and maximize I[X (cid:48); {X, A}] with respect to some encoding function X = e(S). In the next section, we describe our approach for this encoding function as well as dynamics learning in MBRL. 3.2 Encoding and dynamics learning
For our purposes, we use a neural encoder ˆe : S → X parameterized by θˆe to map our high-dimensional state space into lower-dimensional abstract representations, where X ⊆ RnX . The dynamics are learned via the following functions: a transition function ˆτ : X × A → X parameter-ized by θˆτ , a reward function ˆr : X × A → [−1, 1] parameterized by θˆr, and a per timestep discount factor function ˆγ : X × A → [0, 1) parameterized by θˆγ. This discount factor is only learned to predict terminal states, where γ = 0.
In order to leverage all past experiences, we use an off-policy learning algorithm that samples tran-sition tuples (s, a, r, γ, s(cid:48)) from a replay buffer. We ﬁrst encode our current and next states with our encoder to get x ← ˆe(s; θˆe), x(cid:48) ← ˆe(s(cid:48); θˆe). The Q-function is learned using the DDQN algorithm (van Hasselt et al., 2015), which uses the target:
Y = r + γQ(ˆe(s(cid:48); θˆe−), argmax a(cid:48)∈A
Q(x(cid:48), a(cid:48); θQ); θQ− ), where θQ− and θˆe− are parameters of an earlier buffered Q-function (or our target Q-function) and encoder respectively. The agent then minimizes the following loss:
LQ(θQ) = (Q(x, a; θQ) − Y )2.
We learn the dynamics of our environment through the following losses:
LR(θˆe, θˆr) = |r − ˆr(x, a; θˆr)|2 , LG(θˆe, θˆγ) = |γ − ˆγ(x, a; θˆγ)|2 and our transition loss
Lτ (θˆe, θˆτ ) = ||[x + ˆτ (x, a; θˆτ )] − x(cid:48)||2 2. (3)
Note that our transition function learns the difference (given an action) between previous state x and current state x(cid:48). By jointly learning the weights of the encoder and the different components, the abstract representation is shaped in a meaningful way according to the dynamics of the environment.
In particular, by minimizing the loss given in Equation 3 with respect to the encoder parameters θˆe (or p(x | s)), we minimize our entropy H[X (cid:48)|X, A].
In order to maximize the entropy of our learnt abstracted state representations H[X (cid:48)], we minimize the expected pairwise Gaussian potential (Borodachov et al., 2019) between states:
Ld1(θˆe) = Es1,s2∼p(s) (cid:2)exp(−Cd1||ˆe(s1; θˆe) − ˆe(s2; θˆe)||2 (4) with Cd1 as a hyperparameter. Losses in Equation 3 and Equation 4 are reminiscent of the model-based losses in Franc¸ois-Lavet et al. (2018) and correspond respectively to the alignment and uni-formity contrastive loss formulation in Wang and Isola (2020), where alignment ensures that similar states are close together (in encoded representation space) and uniformity ensures that all states are spread uniformly throughout this low-dimensional representation space.
The losses Lτ (θˆe) and Ld1(θˆe) maximizes the I[{X, A}; X (cid:48)] term and selecting smaller dimen-sion for our representation minimizes I[X (cid:48), S(cid:48)]. Put together, our method is trying to minimize
L(p(x(cid:48)|s(cid:48))) as per Equation 1. 2)(cid:3) 3
3.3 Distance measures in representational space
For practical purposes, since we are looking to use a distance metric within X to leverage as a score for novelty, we ensure well-deﬁned distances between states by constraining the (cid:96)2 distance between two consecutive states:
Lcsc(θˆe) = max((cid:107)ˆe(s1; θe) − ˆe(s2; θe)(cid:107)2 − ω, 0) (5) where Lcsc is a soft constraint between consecutive states s1 and s2 that tends to enforce two con-secutive encoded representations to be at a distance ω apart. We add Lcsc to ensure a well-deﬁned (cid:96)2 distance between abstract states for use in our intrinsic reward calculation (a discussion of this loss is provided in Appendix B). We discuss how we use ω to evaluate model accuracy for our MPC updates in Appendix A. Finally, we minimize the sum of all the aforementioned losses through gradient descent:
L = LR(θˆe, θˆr) + LG(θˆe, θˆγ) + Lτ (θˆe, θˆτ ) + LQ(θQ) + Ld1(θˆe) + Lcsc(θˆe). (6)
Through these losses, the agent learns a low-dimensional representation of the environment that is meaningful in terms of the (cid:96)2 norm in representation space. We then employ a planning technique that combines the knowledge of the model and the value function which we use to maximize intrinsic rewards, as detailed in the next section and Section 4.3. 4 Novelty Search in abstract representational space
Our approach for exploration uses intrinsic motivation (Schmidhuber, 1990; Chentanez et al., 2005;
Achiam and Sastry, 2017) where an agent rewards itself based on the fact that it gathers interesting experiences. In a large state space setting, states are rarely visited and the count for any state after n steps is almost always 0. While Bellemare et al. (2016) solves this issue with density estimation using pseudo-counts directly from the high-dimensional observations, we aim to estimate some function of novelty in our learnt lower-dimensional representation space. 4.1 Sparsity in representation space as a measure for novelty
Through the minimization of Equation 1, states that are close together in dynamics are pushed close together in our abstract state space X . Ideally, we want an agent that efﬁciently explores the dynamics of its environment. To do so, we reward our agent for exploring areas in lower-dimensional representation space that are less visited and ideally as far apart from the dynamics that we currently know.
Given a point x in representation space, we deﬁne a reward function that considers the sparsity of states around x - we do so with the average distance between x and its k-nearest-neighbors in its visitation history buffer B:
ˆρX (x) = 1 k k (cid:88) d(x, xi), (7) i=1 where x ˙= ˆe(s; θˆe) is a given encoded state, k ∈ Z+, d(·, ·) is some distance metric in RnX and
˙= ˆe(si; θˆe), where si ∈ B for i = 1 . . . k are the k nearest neighbors (by encoding states in B to xi representational space) of x according to the distance metric d(·, ·). Implicit in this measure is the reliance on the agent’s visitation history buffer B.
An important factor in this score is which distance metric to use. With the losses used in Section 3, we use (cid:96)2 distance because of the structure imposed on the abstract state space with Equations 4 and 5.
As we show in Appendix D, this novelty reward is reminiscent of recoding probabilities (Bellemare et al., 2016; Cover and Thomas, 2012) and is in fact inversely proportional to these probabilities, suggesting that our novelty heuristic estimates visitation count. This is also the same score used to gauge “sparseness” in behavior space in Lehman and Stanley (2011).
With this reward function, we present the pseudo-code for our exploration algorithm in Algo-rithm 1. 4
Algorithm 1: The Novelty Search algorithm in abstract representational space. 1 Initialization: transition buffer B, agent policy π; 2 Sample ninit initial random transitions, let t = ninit; 3 while t ≤ nmax do
// We update our dynamics model and Q-function every nf req steps if t mod nf req == 0 then while j ≤ niters or Lτ ≤ (cid:0) ω (cid:1)2
Sample batch of transitions (s, a, rextr, rintr, γ, s(cid:48)) ∈ B;
Train dynamics model with (s, a, rextr, γ, s(cid:48));
Train Q-function with (s, a, rextr + rintr, γ, s(cid:48)); do
δ end
∀(s, a, rextr, rintr, γ, s(cid:48)) ∈ B, set rintr ← ˆρX (ˆe(s(cid:48); θˆe)); end at ∼ π(st);
Take action in environment: st+1 ← τ (st, at), rt,extr ← R(st, at), γt ← G(st, at);
Calculate intrinsic reward: rt,intr ← ˆρX (ˆe(st+1; θˆe))
B ← B ∪ {(st, at, rt,extr, rt,intr, γt, st+1)}; 4 5 6 7 8 9 10 11 12 13 14 15 16 end 4.2 Asymptotic behavior
This reward function also exhibits favorable asymptotic behavior, as it decreases to 0 as most of the state space is visited. We show this in Theorem 1.
Theorem 1. Assume we have a ﬁnite state space S ⊆ Rd, history of states B = (s1, . . . , sN ), encoded state space X ⊆ RnX , deterministic mapping f : Rd → RnX and a novelty reward deﬁned as ˆρX (x). With an optimal policy with respect to the rewards of the novelty heuristic, our agent will tend towards states with higher intrinsic rewards. If we assume a communicating MDP setting (Puterman, 1994), we have that lim
N→∞
ˆρX (f (s)) = 0, ∀s ∈ S.
Proof. We prove this theorem in Appendix E. 4.3 Combining model-free and model-based components for exploration policies
Similarly to previous works (e.g. Oh et al., 2017; Chebotar et al., 2017), we use a combination of model-based planning with model-free Q-learning to obtain a good policy. We calculate rollout estimates of next states based on our transition model ˆτ and sum up the corresponding rewards, which we denote as r : X × A → [0, Rmax] and can be a combination of both intrinsic and extrinsic rewards. We calculate expected returns based on the discounted rewards of our d-depth rollouts:
ˆQd(x, a) =


 r(x, a) + ˆγ(x, a; θˆγ)× max a(cid:48)∈A
Q(x, a; θQ),
ˆQd−1(τ (x, a; θˆτ ), a(cid:48)), if d > 0 if d = 0 (8)
Note that we simulate only b-best options at each expansion step based on Q(x, a; θQ), where b ≤
|A|. In this work, we only use full expansions. The estimated optimal action is given by a∗ = argmax
ˆQd(x, a). a∈A
The actual action chosen at each step follows an (cid:15)-greedy strategy ((cid:15) ∈ [0, 1]), where the agent follows the estimated optimal action with probability 1 − (cid:15) and a random action with probability (cid:15). 5
(a) (b) (c)
Figure 1: (a), (b): Plotting the full history of learned abstract representations of both open and 4-room labyrinth environments from Figures 7a and 7b after 500 environment steps. Colors denote which side of the maze the agent was in, grid coordinates and transitions are shown. (c): Two views of the same full history of learned abstract 3-dimensional representation of our multi-step maze after 300 steps. Orange and blue points denote states without and with keys respectively. Our agent is able to disentangle states where the agent has a key and when it doesn’t as seen in the distance between the two groups of states. Meaningful information about the agent position is also maintained in the relative positions of states in abstract state space. 5 Experiments
We conduct experiments on environments of varying difﬁculty. All experiments use a training scheme where we ﬁrst train parameters to converge on an accurate representation of the already experienced transitions before taking an environment step. We optimize the losses (over multiple training iterations) given in Section 3. We discuss all environment-speciﬁc hyperparameters in Ap-pendix J. 5.1 Labyrinth exploration
We consider two 21 × 21 versions of the grid-world environment (Figure 7 in Appendix). The ﬁrst is an open labyrinth grid-world, with no walls except for bordering walls. The second is a similar sized grid-world split into four connected rooms. In these environments the action space A is the set of four cardinal directions. These environments have no rewards or terminal states and the goal is to explore, agnostic of any task. We use two metrics to gauge exploration for this environment: the
ﬁrst is the ratio of states visited only once, the second is the proportion of total states visited. 5.1.1 Open labyrinth
In the open labyrinth experiments (Figure 2a), we compare a number of variations of our approach with a random baseline and a count-based baseline (Bellemare et al., 2016) (as we can count states in this tabular setting). Variations of the policy include an argmax over state values (d = 0) and planning depths of d ∈ {1, 5}. All variations of our method outperform the two baselines in this task, with a slight increase in performance as planning depth d increases. In the open labyrinth, our agent is able to reach 100% of possible states (a total of 19 × 19 = 361 unique states) in approximately 800 steps, and 80% of possible states (≈ 290 states) in approximately 500 steps.
These counts also include the ninit number of random steps taken preceding training.
Our agent is also able to learn highly interpretable abstract representations in very few environment steps (as shown in Figure 1a) as it explores its state space. In addition, after visiting most unseen states in its environment, our agent tends to uniformly explore its state space due to the nature of our novelty heuristic. A visualisation of this effect is available in Appendix H. 5.1.2 4-room labyrinth
We now consider the 4-room labyrinth environment, a more challenging version of the open labyrinth environment (Figure 1a). As before, our encoder ˆe is able to take a high-dimensional input and compress it to a low-dimensional representation. In the case of both labyrinth environments, the representation incorporates knowledge related to the position of the agent in 2-dimensions that we call primary features. In the 4-room labyrinth environment, it also has to learn other information 6
(a) Results for open labyrinth and different variations on policies compared to baselines. (b) Results for the 4-room labyrinth and different vari-ations on policies compared to baselines.
Figure 2: Labyrinth results for both open labyrinth and 4-room labyrinth over 10 trials, showing mean and standard deviations. such as agent surroundings (walls, open space) etc., but it does so only via the transition function learned through experience. We call this extraneous but necessary information secondary features.
As most of these secondary features are encoded only in the dynamics model ˆτ , our agent has to experience a transition in order to accurately represent both primary and secondary features.
In this environment speciﬁcally, our dynamics model might over-generalize for walls between rooms and can sometimes fail at ﬁrst to try out transitions in the passageways between rooms. However, because our agent tends to visit uniformly all the states that are reachable within the known rooms, the (cid:15)-greedy policy of our approach still ensures that the agent explores passageways efﬁciently even in the cases where it has over-generalized to the surrounding walls.
We run the same experiments on the 4-room labyrinth domain as we do on the open labyrinth and report results in Figure 2b. In both cases, our method outperforms the two baselines in this domain (random and count-based). 5.2 Control and sub-goal exploration
In order to test the efﬁcacy of our method beyond ﬁxed mazes, we conduct experiments on the control-based environment Acrobot (Brockman et al., 2016) and a multi-step maze environment.
Our method (with planning depth d = 5) is compared to strong exploration baselines with different archetypes: 1. Prediction error incentivized exploration (Stadie et al., 2015) 2. Hash count-based exploration (Tang et al., 2016) 3. Random Network Distillation (Osband et al., 2017) 4. Bootstrap DQN (BDQN, Osband et al. (2016))
In order to maintain consistency in our results, we use the same deep learning architectures through-out. Since we experiment in the deterministic setting, we exclude baselines that require some form of stochasticity or density estimation as baselines (for example, Shyam et al. (2018) and Osband et al. (2017)). A speciﬁcity of our approach is that we run multiple training iterations in between each environment step for all experiments, which allows the agent to use orders of magnitude less samples as compared to most model-free RL algorithms (all within the same episode). 5.2.1 Acrobot
We now test our approach on Acrobot (Brockman et al., 2016), which has a continuous state space unlike the labyrinth environment. We speciﬁcally choose this control task because the nature of this environment makes exploration inherently difﬁcult. The agent only has control of the actuator for the inner joint and has to transfer enough energy into the second joint in order to swing it to its goal state. We modify this environment so that each episode is at most 3000 environment steps. While this environment does admit an extrinsic reward, we ignore these rewards entirely. To measure the performance of our exploration approach, we measure the average number of steps per episode that the agent takes to move its second joint above a given line as per Figure 3a.
To demonstrate the ability of our method to learn a low dimensional abstract representation from pixel inputs, we use 4 consecutive pixel frames as input instead of the 6-dimensional full state 7
Acrobot
Multi-step Maze
Norm. & Combined
Reward
Avg
StdErr p-value
Avg
StdErr p-value Avg
StdErr
Random 1713.3 932.8 1007.0 953.8 592.5 576.0
Pred
Count
RND
BDQN
Novelty 316.25 141.54 174.81 85.98 43.65 66.13 0.0077 0.050 0.050 0.0042 0.85
-1863.3 1018.0 658.8 938.4 1669.1 524.6 308.35 79.31 71.73 135.88 291.26 73.24 0.0025 4.0e−4 0.23 0.024 0.0046
-3.26 1.78 1.50 1.72 2.11 1.00 0.41 0.15 0.18 0.15 0.37 0.090 p-value 3.1e−5 1.3e−4 0.019 3.5e−4 0.0099
-Table 1: Number of environment steps necessary to reach the goal state in the Acrobot and the multi-step maze environments (lower is better). Results are averaged over 5 trials for both experiments.
Best results are in bold. We provide p-values indicative of the null hypothesis H0 : ∆µ = µ1 −
µ2 = 0, calculated using Welch’s t-test, all as per (Colas et al., 2019).
In this case, we do a pair-wise comparison between the central tendencies of our algorithm (Novelty) and our baselines.
Normalized and combined results are also shown - results here are ﬁrst normalized with respect to the average number of steps taken for our algorithm and then combined on both environments. vector. We use a 4-dimensional abstract representation of our state and results from experiments are shown in Table 1. Our method reaches the goal state more efﬁciently than the baselines. (a) Left: Acrobot start state. right: Acrobot end state (b) Left: Start of our multi-step maze. right: After the agent has collected the key.
Figure 3: Illustrations of the Acrobot and multi-step goal maze environments. a) Left: The Acrobot environment in one conﬁguration of its start state. a) Right: One conﬁguration of the ending state of the Acrobot environment. The environment ﬁnishes when the second arm passes the solid black line. b) Left: The passageway to the west portion of the environment is blocked before the key (black) is collected. b) Right: The passageway is traversable after collecting the key, and the reward (red) is then available. The environment terminates after collecting the reward. 5.2.2 Multi-step goal maze
We also test our method on a more complex maze with the sub-task of picking up a key that opens the door to an area with a reward. We build our environment with the Pycolab game engine (Stepleton, 2017). The environment can be seen in Figure 3b, where the input to our agent is a top-down view of the environment. While this environment does admit an extrinsic reward (1 for picking up the key, 10 for reaching the ﬁnal state), we ignore these rewards and only focus on intrinsic rewards.
In our experiments, we show that our agent is able to learn an interpretable representation of the environment in a sample-efﬁcient manner. Figure 1c shows an example of learnt representations in this domain after reaching the goal - we observe that positions in the maze correspond to a nearly identical structure in the lower-dimensional representation. Our representation also nicely captures internal state information (whether the key has been picked up) by separating the two sets of states (states when the key has been picked up and states when the key has not been picked up). Similar positions in both sets of states are also mapped closely together in lower-dimensional space (ie. (1, 1, with key) is close in (cid:96)2 to (1, 1, without key)), suggesting good generalization between similar states. 8
6