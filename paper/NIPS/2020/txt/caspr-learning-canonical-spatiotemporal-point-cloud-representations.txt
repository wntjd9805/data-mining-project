Abstract
We propose CaSPR, a method to learn object-centric Canonical Spatiotemporal
Point Cloud Representations of dynamically moving or evolving objects. Our goal is to enable information aggregation over time and the interrogation of object state at any spatiotemporal neighborhood in the past, observed or not. Different from previous work, CaSPR learns representations that support spacetime conti-nuity, are robust to variable and irregularly spacetime-sampled point clouds, and generalize to unseen object instances. Our approach divides the problem into two subtasks. First, we explicitly encode time by mapping an input point cloud sequence to a spatiotemporally-canonicalized object space. We then leverage this canonicalization to learn a spatiotemporal latent representation using neural ordinary differential equations and a generative model of dynamically evolving shapes using continuous normalizing ﬂows. We demonstrate the effectiveness of our method on several applications including shape reconstruction, camera pose es-timation, continuous spatiotemporal sequence reconstruction, and correspondence estimation from irregularly or intermittently sampled observations. 1

Introduction
The visible geometric properties of objects around us are constantly evolving over time due to object motion, articulation, deformation, or observer movement. Ex-amples include the rigid motion of cars on the road, the deformation of clothes in the wind, and the articulation of moving humans. The ability to capture and recon-struct these spatiotemporally changing geometric object properties is critical in applications like autonomous driv-ing, robotics, and mixed reality. Recent work has made progress on learning object shape representations from static 3D observations [49, 52, 53, 62, 73] and dynamic point clouds [9, 11, 40, 41, 45, 50, 80]. Yet, important limitations remain in terms of the lack of temporal con-tinuity, robustness, and category-level generalization.
Figure 1: CaSPR builds a point cloud repre-sentation of (partially observed) objects con-tinuously in both space (x-axis) and time (y-axis), while canonicalizing for extrinsic object properties like pose.
In this paper, we address the problem of learning object-centric representations that can aggregate and encode spatiotemporal (ST) changes in object shape as seen from a 3D sensor. This is challenging since dynamic point clouds captured by depth sensors or LIDAR are often incomplete and sparsely sampled over space and time. Furthermore, even point clouds corresponding to adjacent frames in a sequence will experience large sampling variation.
Ideally, we would like spatiotemporal representations to satisfy several desirable properties. First, representations should allow us to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
capture object shape continuously over space and time. They should encode changes in shape due to varying camera pose or temporal dynamics, and support shape generation at arbitrary spatiotemporal resolutions. Second, representations should be robust to irregular sampling patterns in space and time, including support for full or partial point clouds. Finally, representations should support within-category generalization to unseen object instances and to unseen temporal dynamics. While many of these properties are individually considered in prior work [11, 30, 41, 45, 68], a uniﬁed and rigorous treatment of all these factors in space and time is largely missing.
We address the limitations of previous work by learning a novel object-centric ST representation which satisﬁes the above properties. To this end, we introduce CaSPR – a method to learn Canonical
Spatiotemporal Point Cloud Representations. In our approach, we split the task into two: (1) canoni-calizing an input object point cloud sequence (partial or complete) into a shared 4D container space, and (2) learning a continuous ST latent representation on top of this canonicalized space. For the former, we build upon the Normalized Object Coordinate Space (NOCS) [63, 72] which canonicalizes intra-class 3D shape variation by normalizing for extrinsic properties like position, orientation, and scale. We extend NOCS to a 4D Temporal-NOCS (T-NOCS), which additionally normalizes the duration of the input sequence to a unit interval. Given dynamic point cloud sequences, our ST canonicalization yields spacetime-normalized point clouds. In Sec. 5, we show that this allows learning representations that generalize to novel shapes and dynamics.
We learn ST representations of canonicalized point clouds using Neural Ordinary Differential
Equations (Neural ODEs) [9]. Different from previous work, we use a Latent ODE that operates in a lower-dimensional learned latent space which increases efﬁciency while still capturing object shape dynamics. Given an input sequence, the canonicalization network and Latent ODE together extract features that constitute an ST representation. To continuously generate novel spatiotemporal point clouds conditioned on an input sequence, we further leverage invertible Continuous Normalizing
Flows (CNFs) [6, 24] which transform Gaussian noise directly to the visible part of an object’s shape at a desired timestep. Besides continuity, CNFs provide direct likelihood evaluation which we use as a training loss. Together, as shown in Fig. 1, the Latent ODE and CNF constitute a generative model that is continuous in spacetime and robust to sparse and varied inputs. Unlike previous work [11, 41], our approach is continuous and explicitly avoids treating time as another spatial dimension by respecting its unique aspects (e.g., unidirectionality).
We demonstrate that CaSPR is useful in numerous applications including (1) continuous spacetime shape reconstruction from sparse, partial, or temporally non-uniform input point cloud sequences, (2) spatiotemporal 6D pose estimation, and (3) information propagation via space-time correspon-dences under rigid or non-rigid transformations. Our experiments show improvements to previous work while also providing insights on the emergence of intra-class shape correspondence and the learning of time unidirectionality [19]. In summary, our contributions are: 1. The CaSPR encoder network that consumes dynamic object point cloud sequences and canonical-izes them to normalized spacetime (T-NOCS). 2. The CaSPR representation of canonicalized point clouds using a Latent ODE to explicitly encode temporal dynamics, and an associated CNF for generating shapes continuously in spacetime. 3. A diverse set of applications of this technique, including partial or full shape reconstruction, spatiotemporal sequence recovery, camera pose estimation, and correspondence estimation. 2