Abstract
Deterministic neural networks (NNs) are increasingly being deployed in safety critical domains, where calibrated, robust, and efﬁcient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian
NNs to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the
NN to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution (OOD) examples for training, thus enabling efﬁcient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and OOD test samples. 1

Introduction
Regression-based neural networks (NNs) are being deployed in safety critical domains in computer vi-sion [15] as well as in robotics and control [1, 6], where the ability to infer model uncertainty is crucial for eventual wide-scale adoption. Furthermore, pre-cise and calibrated uncertainty estimates are useful for interpreting conﬁdence, capturing domain shift of out-of-distribution (OOD) test samples, and recog-nizing when the model is likely to fail.
There are two axes of NN uncertainty that can be modeled: (1) uncertainty in the data, called aleatoric uncertainty, and (2) uncertainty in the prediction, called epistemic uncertainty. While representations of aleatoric uncertainty can be learned directly from data, there exist several approaches for estimating epistemic uncertainty, such as Bayesian NNs, which place probabilistic priors over network weights and use sampling to approximate output variance [25].
However, Bayesian NNs face several limitations, including the intractability of directly inferring the posterior distribution of the weights given data, the requirement and computational expense of sampling during inference, and the question of how to choose a weight prior.
Figure 1: Evidential regression simultaneously learns a continuous target along with aleatoric (data) and epistemic (model) uncertainty. Given an input, the network is trained to predict the parame-ters of an evidential distribution, which models a higher-order probability distribution over the indi-vidual likelihood parameters, (µ, σ2).
In contrast, evidential deep learning formulates learning as an evidence acquisition process [42, 32].
Every training example adds support to a learned higher-order, evidential distribution. Sampling from this distribution yields instances of lower-order likelihood functions from which the data 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
was drawn. Instead of placing priors on network weights, as is done in Bayesian NNs, evidential approaches place priors directly over the likelihood function. By training a neural network to output the hyperparameters of the higher-order evidential distribution, a grounded representation of both epistemic and aleatoric uncertainty can then be learned without the need for sampling.
To date, evidential deep learning has been targeted towards discrete classiﬁcation problems [42, 32, 22] and has required either a well-deﬁned distance measure to a maximally uncertain prior [42] or relied on training with OOD data to inﬂate model uncertainty [32, 31]. In contrast, continuous regression problems present the complexity of lacking a well-deﬁned distance measure to regularize the inferred evidential distribution. Further, pre-deﬁning a reasonable OOD dataset is non-trivial in the majority of applications; thus, methods to obtain calibrated uncertainty on OOD data from only an in-distribution training set are required.
We present a novel approach that models the uncertainty of regression networks via learned evidential distributions (Fig. 1). Speciﬁcally, this work makes the following contributions: 1. A novel and scalable method for learning epistemic and aleatoric uncertainty on regression problems, without sampling during inference or training with out-of-distribution data; 2. Formulation of an evidential regularizer for continuous regression problems, necessary for penalizing incorrect evidence on errors and OOD examples; 3. Evaluation of epistemic uncertainty on benchmark and complex vision regression tasks along with comparisons to state-of-the-art NN uncertainty estimation techniques; and 4. Robustness and calibration evaluation on OOD and adversarially perturbed test input data. 2 Modelling uncertainties from data 2.1 Preliminaries
Consider the following supervised optimization problem: given a dataset, D, of N paired training examples, D = {xi, yi}N i=1, we aim to learn a functional mapping f , parameterized by a set of weights, w, which approximately solves the following optimization problem: min w
J(w);
J(w) = 1
N
N (cid:88) i=1
Li(w), (1) where Li(·) describes a loss function. In this work, we consider deterministic regression problems, 2 (cid:107)yi − f (xi; w)(cid:107)2. In doing so, the which commonly optimize the sum of squared errors, Li(w) = 1 model is encouraged to learn the average correct answer for a given input, but does not explicitly model any underlying noise or uncertainty in the data when making its estimation. 2.2 Maximum likelihood estimation
One can approach this problem from a maximum likelihood perspective, where we learn model parameters that maximize the likelihood of observing a particular set of training data. In the context of deterministic regression, we assume our targets, yi, were drawn i.i.d. from a distribution such as a Gaussian with mean and variance parameters θ = (µ, σ2). In maximum likelihood estimation (MLE), we aim to learn a model to infer θ that maximize the likelihood of observing our targets, y, given by p(yi|θ). This is achieved by minimizing the negative log likelihood loss function:
Li(w) = − log p(yi| µ, σ2 (cid:124) (cid:123)(cid:122) (cid:125)
θ
) = 1 2 log(2πσ2) + (yi − µ)2 2σ2
. (2)
In learning θ, this likelihood function successfully models the uncertainty in the data, also known as the aleatoric uncertainty. However, our model is oblivious to its predictive epistemic uncertainty [25].
In this paper, we present a novel approach for estimating the evidence supporting network predictions in regression by directly learning both the aleatoric uncertainty present in the data as well as the model’s underlying epistemic uncertainty. We achieve this by placing higher-order prior distributions over the learned parameters governing the distribution from which our observations are drawn. 2
Figure 2: Normal Inverse-Gamma distribution. Different realizations of our evidential distribution (A) correspond to different levels of conﬁdences in the parameters (e.g. µ, σ2). Sampling from a single realization of a higher-order evidential distribution (B), yields lower-order likelihoods (C) over the data (e.g. p(y|µ, σ2)).
Darker shading indicates higher probability mass. We aim to learn a model that predicts the target, y, from an input, x, with an evidential prior imposed on our likelihood to enable uncertainty estimation. 3 Evidential uncertainty for regression 3.1 Problem setup
We consider the problem where the observed targets, yi, are drawn i.i.d. from a Gaussian distribution, as in standard MLE (Sec. 2.2), but now with unknown mean and variance (µ, σ2), which we seek to also probabilistically estimate. We model this by placing a prior distribution on (µ, σ2). If we assume observations are drawn from a Gaussian, in line with assumptions Sec. 2.2, this leads to placing a
Gaussian prior on the unknown mean and an Inverse-Gamma prior on the unknown variance: (y1, . . . , yN ) ∼ N (µ, σ2)
µ ∼ N (γ, σ2υ−1)
σ2 ∼ Γ−1(α, β). (3) where Γ(·) is the gamma function, m = (γ, υ, α, β), and γ ∈ R, υ > 0, α > 1, β > 0.
Our aim is to estimate a posterior distribution q(µ, σ2) = p(µ, σ2|y1, . . . , yN ). To obtain an approxi-mation for the true posterior, we assume that the estimated distribution can be factorized [39] such that q(µ, σ2) = q(µ) q(σ2). Thus, our approximation takes the form of the Gaussian conjugate prior, the Normal Inverse-Gamma (NIG) distribution: p(µ, σ2 (cid:124) (cid:123)(cid:122) (cid:125)
θ
) =
| γ, υ, α, β (cid:125) (cid:123)(cid:122) (cid:124) m
βα√
√
υ 2πσ2
Γ(α) (cid:18) 1
σ2 (cid:19)α+1 (cid:26) exp
− 2β + υ(γ − µ)2 2σ2 (cid:27)
. (4)
A popular interpretation of the parameters of this conjugate prior distribution is in terms of “virtual-observations” in support of a given property [23]. For example, the mean of a NIG distribution can be intuitively interpreted as being estimated from υ virtual-observations with sample mean γ, while its variance is estimated from α virtual-observations with sample mean γ and sum of squared deviations 2υ. Following from this interpretation, we deﬁne the total evidence, Φ, of our evidential distributions as the sum of all inferred virtual-observations counts: Φ = 2υ + α.
Drawing a sample θj from the NIG distribution yields a single instance of our likelihood function, namely N (µj, σ2 j ). Thus, the NIG hyperparameters, (γ, υ, α, β), determine not only the location but also the dispersion concentrations, or uncertainty, associated with our inferred likelihood function.
Therefore, we can interpret the NIG distribution as the higher-order, evidential distribution on top of the unknown lower-order likelihood distribution from which observations are drawn.
For example, in Fig. 2A we visualize different evidential NIG distributions with varying model parameters. We illustrate that by increasing the evidential parameters (i.e. υ, α) of this distribution, the p.d.f. becomes tightly concentrated about its inferred likelihood function. Considering a single 3
parameter realization of this higher-order distribution (Fig. 2B), we can subsequently sample many lower-order realizations of our likelihood function, as shown in Fig. 2C.
In this work, we use neural networks to infer, given an input, the hyperparameters, m, of this higher-order, evidential distribution. This approach presents several distinct advantages compared to prior work. First, our method enables simultaneous learning of the desired regression task, along with aleatoric and epistemic uncertainty estimation, by enforcing evidential priors and without leveraging any out-of-distribution data during training. Second, since the evidential prior is a higher-order NIG distribution, the maximum likelihood Gaussian can be computed analytically from the expected values of the (µ, σ2) parameters, without the need for sampling. Third, we can effectively estimate the epistemic or model uncertainty associated with the network’s prediction by simply evaluating the variance of our inferred evidential distribution. 3.2 Prediction and uncertainty estimation
The aleatoric uncertainty, also referred to as statistical or data uncertainty, is representative of unknowns that differ each time we run the same experiment. The epistemic (or model) uncertainty, describes the estimated uncertainty in the prediction. Given a NIG distribution, we can compute the prediction, aleatoric, and epistemic uncertainty as
,
E[µ] = γ (cid:123)(cid:122) (cid:125) (cid:124) prediction
E[σ2] = β
,
α−1 (cid:125) (cid:123)(cid:122) (cid:124) aleatoric
Var[µ] = β (cid:124)
υ(α−1) (cid:125) (cid:123)(cid:122) epistemic
. (5)
Complete derivations for these moments are available in Sec. S1.1. Note that Var[µ] = E[σ2]/υ, which is expected as υ is one of our two evidential virtual-observation counts. 3.3 Learning the evidential distribution
Having formalized the use of an evidential distribution to capture both aleatoric and epistemic uncertainty, we next describe our approach for learning a model to output the hyperparameters of this distribution. For clarity, we structure the learning process as a multi-task learning problem, with two distinct parts: (1) acquiring or maximizing model evidence in support of our observations and (2) minimizing evidence or inﬂating uncertainty when the prediction is wrong. At a high level, we can think of (1) as a way of ﬁtting our data to the evidential model while (2) enforces a prior to remove incorrect evidence and inﬂate uncertainty. (1) Maximizing the model ﬁt. From Bayesian probability theory, the “model evidence”, or marginal likelihood, is deﬁned as the likelihood of an observation, yi, given the evidential distribution parame-ters m and is computed by marginalizing over the likelihood parameters θ: p(yi|m) = p(yi|θ, m)p(θ|m) p(θ|yi, m)
= (cid:90) ∞ (cid:90) ∞
σ2=0
µ=−∞ p(yi|µ, σ2)p(µ, σ2|m) dµ dσ2 (6)
The model evidence is, in general, not straightforward to evaluate since computing it involves integrating out the dependence on latent model parameters. However, in the case of placing a NIG evidential prior on our Gaussian likelihood function an analytical solution does exist: (cid:18) p(yi|m) = St yi; γ,
β(1 + υ)
υ α (cid:19)
, 2α
. (7)
St, υSt where St (cid:0)y; µSt, σ2
υSt degrees of freedom. We denote the loss, LNLL 2 log (cid:0) π (cid:1) − α log(Ω) + (cid:0)α + 1 (w) = 1
LNLL i
υ 2 i (cid:1) is the Student-t distribution evaluated at y with location µSt, scale σ2
St, and (w), as the negative logarithm of model evidence (cid:1) log((yi − γ)2υ + Ω) + log (cid:17) (cid:16) Γ(α)
Γ(α+ 1 2 ) (8) where Ω = 2β(1 + υ). Complete derivations for Eq. 7 and Eq. 8 are provided in Sec. S1.2. This loss provides an objective for training a NN to output parameters of a NIG distribution to ﬁt the observations by maximizing the model evidence. (2) Minimizing evidence on errors. Next, we describe how to regularize training by applying an incorrect evidence penalty (i.e., high uncertainty prior) to try to minimize evidence on incorrect pre-dictions. This has been demonstrated with success in the classiﬁcation setting where non-misleading 4
evidence is removed from the posterior, and the uncertain prior is set to a uniform Dirichlet [42]. The analogous minimization in the regression setting involves KL[ p(θ|m) || p(θ| ˜m) ], where ˜m are the parameters of the uncertain NIG prior with zero evidence (i.e., {α, υ} = 0). Unfortunately, the KL between any NIG and the zero evidence NIG prior is undeﬁned(1). Furthermore, this loss should not be enforced everywhere, but instead speciﬁcally where the posterior is “misleading”. Past works in classiﬁcation [42] accomplish this by using the ground truth likelihoood classiﬁcation (the one-hot encoded labels) to remove “non-misleading” evidence. However, in regression, it is not possible to penalize evidence everywhere except our single label point estimate, as this space is inﬁnite and unbounded. Thus, these previous approaches for regularizing evidential learning are not applicable.
To address these challenges in the regression setting, we formulate a novel evidence regularizer, LR i , scaled on the error of the i-th prediction, i (w) = |yi − E[µi]| · Φ = |yi − γ| · (2υ + α).
LR
This loss imposes a penalty whenever there is an error in the prediction and scales with the total evidence of our inferred posterior. Conversely, large amounts of predicted evidence will not be penalized as long as the prediction is close to the target. A naïve alternative to directly penalizing evidence would be to soften the zero-evidence prior to instead have (cid:15)-evidence such that the KL is
ﬁnite and deﬁned. However, doing so results in hypersensitivity to the selection of (cid:15), as it should be small yet KL → ∞ as (cid:15) → 0. We demonstrate the added value of our evidential regularizer through ablation analysis (Sec. 4.1), the limitations of the soft KL regularizer (Sec. S2.1.3), and the ability to learn disentangled aletoric and epistemic uncertainty (Sec. S2.1.4). (9)
Summary and implementation details. The total loss, Li(w), consists of the two loss terms for maximizing and regularizing evidence, scaled by a regularization coefﬁcient, λ,
Li(w) = LNLL (w) + λ LR (10) i (w). i
Here, λ trades off uncertainty inﬂation with model ﬁt. Setting λ = 0 yields an over-conﬁdent estimate while setting λ too high results in over-inﬂation(2). In practice, our NN is trained to output the parameters, m, of the evidential distribution: mi = f (xi; w). Since m is composed of 4 parameters, f has 4 output neurons for every target y. We enforce the constraints on (υ, α, β) with a softplus activation (and additional +1 added to α since α > 1). Linear activation is used for γ ∈ R. 4 Experiments 4.1 Predictive accuracy and uncertainty benchmarking
We ﬁrst qualitatively compare the performance of our approach against a set of baselines on a one-dimensional cubic regression dataset (Fig. 3). Fol-lowing [20, 28], we train models on y = x3 + (cid:15), where (cid:15) ∼ N (0, 3) within ±4 and test within ±6.
We compare aleatoric (A) and epistemic (B) uncer-tainty estimation for baseline methods (left), evidence without regularization (middle), and with regulariza-tion (right). Gaussian MLE [36] and Ensembling [28] are used as respective baseline methods. All aleatoric methods (A) accurately capture uncertainty within the training distribution, as expected. Epistemic un-certainty (B) captures uncertainty on OOD data; our proposed evidential method estimates uncertainty ap-propriately and grows on OOD data, without depen-dence on sampling. Training details and additional ex-periments for this example are available in Sec. S2.1.
Additionally, we compare our approach to baseline methods for NN predictive uncertainty estimation on
Figure 3: Toy uncertainty estimation. Aleatoric (A) and epistemic (B) uncertainty estimates on the dataset y = x3 + (cid:15), (cid:15) ∼ N (0, 3). Regularized evidential regression (right) enables precise predic-tion within the training regime and conservative epistemic uncertainty estimates in regions with no training data. Baseline results are also illustrated. (1)Please refer to Sec. S1.3 for derivation of the KL between two NIGs, along with a no-evidence NIG prior. (2)Experiments demonstrating the effect of λ on a learning problem are provided in Sec. S2.1.3 5
Dropout
Dataset 2.97 ± 0.19
Boston 5.23 ± 0.12
Concrete 1.66 ± 0.04
Energy
Kin8nm 0.10 ± 0.00 0.01 ± 0.00
Naval 4.02 ± 0.04
Power 4.36 ± 0.01
Protein 0.62 ± 0.01
Wine 1.11 ± 0.09
Yacht
RMSE
Ensembles 3.28 ± 1.00 6.03 ± 0.58 2.09 ± 0.29 0.09 ± 0.00 0.00 ± 0.00 4.11 ± 0.17 4.71 ± 0.06 0.64 ± 0.04 1.58 ± 0.48
Evidential 3.06 ± 0.16 5.85 ± 0.15 2.06 ± 0.10 0.09 ± 0.00 0.00 ± 0.00 4.23 ± 0.09 4.64 ± 0.03 0.61 ± 0.02 1.57 ± 0.56
Dropout 2.46 ± 0.06 3.04 ± 0.02 1.99 ± 0.02
-0.95 ± 0.01
-3.80 ± 0.01 2.80 ± 0.01 2.89 ± 0.00 0.93 ± 0.01 1.55 ± 0.03
NLL
Ensembles 2.41 ± 0.25 3.06 ± 0.18 1.38 ± 0.22
-1.20 ± 0.02
-5.63 ± 0.05 2.79 ± 0.04 2.83 ± 0.02 0.94 ± 0.12 1.18 ± 0.21
Evidential 2.35 ± 0.06 3.01 ± 0.02 1.39 ± 0.06
-1.24 ± 0.01
-5.73 ± 0.07 2.81 ± 0.07 2.63 ± 0.00 0.89 ± 0.05 1.03 ± 0.19
Inference Speed (ms)
Dropout Ensemble Evidential 3.35 3.43 3.80 3.79 3.37 3.36 3.68 3.32 3.36 3.24 2.99 3.08 3.24 3.31 2.93 3.45 3.00 2.99 0.85 0.94 0.87 0.97 0.84 0.85 1.18 0.86 0.87
Table 1: Benchmark regression tests. RMSE, negative log-likelihood (NLL), and inference speed for dropout sampling [9], model ensembling [28], and evidential regression. Top scores for each metric and dataset are bolded (within statistical signiﬁcance), n = 5 for sampling baselines. Evidential models outperform baseline methods for NLL and inference speed on all datasets. real world datasets used in [20, 28, 9]. We evaluate our proposed evidential regression method against results presented for model ensembles [28] and dropout [9] based on root mean squared error (RMSE), negative log-likelihood (NLL), and inference speed. Table 1 indicates that even though, unlike the competing approaches, the loss function for evidential regression does not explicitly optimize accuracy, it remains competitive with respect to RMSE while being the top performer on all datasets for NLL and speed. To give the two baseline methods maximum advantage, we parallelize their sampled inference (n = 5). Dropout requires additional multiplications with the sampled mask, resulting in slightly slower inference compared to ensembles, whereas evidence only requires a single forward pass and network. Training details for Table 1 are available in Sec. S2.2. 4.2 Monocular depth estimation
After establishing benchmark comparison results, in this subsection we demonstrate the scalability of our evidential learning approach by extending it to the complex, high-dimensional task of depth estimation. Monocular end-to-end depth estimation is a central problem in computer vision and involves learning a representation of depth directly from an RGB image of the scene. This is a challenging learning task as the target y is very high-dimensional, with predictions at every pixel.
Our training data consists of over 27k RGB-to-depth, H × W , image pairs of indoor scenes (e.g. kitchen, bedroom, etc.) from the NYU Depth v2 dataset [35]. We train a U-Net style NN [41] for inference and test on a disjoint test-set of scenes(3). The ﬁnal layer outputs a single H × W activation map in the case of vanilla regression, dropout, and ensembling. Spatial dropout uncertainty sampling [2, 45] is used for the dropout implementation. Evidential regression outputs four of these output maps, corresponding to (γ, υ, α, β), with constraints according to Sec. 3.3.
We evaluate the models in terms of their accuracy and their predictive epistemic uncertainty on unseen test data. Fig. 4A visualizes the predicted depth, absolute error from ground truth, and predictive entropy across two randomly picked test images. Ideally, a strong epistemic uncertainty measure would capture errors in the prediction (i.e., roughly correspond to where the model is making errors).
Compared to dropout and ensembling, evidential modeling captures the depth errors while providing clear and localized predictions of conﬁdence. In general, dropout drastically underestimates the amount of uncertainty present, while ensembling occasionally overestimates the uncertainty. Fig. 4B
Figure 4: Epistemic uncertainty in depth estimation. (A) Example pixel-wise depth predictions and uncer-tainty for each model. (B) Relationship between prediction conﬁdence level and observed error; a strong inverse trend is desired. (C) Model uncertainty calibration [27]; (ideal: y = x). Inset shows calibration errors. (3)Full dataset, model, training, and performance details for depth models are available in Sec. S3. 6
Figure 5: Uncertainty on out-of-distribution (OOD) data. Evidential models estimate low uncertainty (entropy) on in-distribution (ID) data and inﬂate uncertainty on OOD data. (A) Cumulative density function (CDF) of ID and OOD entropy for tested methods. OOD detection assessed via AUC-ROC. (B) Uncertainty (entropy) comparisons across methods. (C) Full density histograms of entropy estimated by evidential regression on ID and OOD data, along with sample images (D). All data has not been seen during training. shows how each model performs as pixels with uncertainty greater than certain thresholds are removed.
Evidential models exhibit strong performance, as error steadily decreases with increasing conﬁdence.
Fig. 4C additionally evaluates the calibration of our uncertainty estimates. Calibration curves are computed according to [27], and ideally follows y = x to represent, for example, that a target falls in a 90% conﬁdence interval approximately 90% of the time. Again, we see that dropout overestimates conﬁdence when considering low conﬁdence scenarios (calibration error: 0.126). Ensembling exhibits better calibration error (0.048) but is still outperformed by the proposed evidential method (0.033). Results show evaluations from multiple trials, with individual trials available in Sec. S3.3.
In addition to epistemic uncertainty experiments, we also evaluate aleatoric uncertainty estimates, with comparisons to Gaussian MLE learning. Since evidential models ﬁt the data to a higher-order
Gaussian distribution, it is expected that they can accurately learn aleatoric uncertainty (as is also shown in [42, 18]). Therefore, we present these aleatoric results in Sec. S3.4 and focus the remainder of the results on evaluating the harder task of epistemic uncertainty estimation in the context of out-of-distribution (OOD) and adversarily perturbed samples. 4.3 Out-of distribution testing
A key use of uncertainty estimation is to understand when a model is faced with test samples that fall out-of-distribution (OOD) or when the model’s output cannot be trusted. In this subsection, we investigate the ability of evidential models to capture increased epistemic uncertainty on OOD data, by testing on images from ApolloScape [21], an OOD dataset of diverse outdoor driving. It is crucial to note here that related methods such as Prior Networks in classiﬁcation [32, 33] explicitly require
OOD data during training to supervise instances of high uncertainty. Our evidential method, like
Bayesian NNs, does not have this limitation and sees only in distribution (ID) data during training.
For each method, we feed in the ID and OOD test sets and record the mean predicted entropy for every test image. Fig. 5A shows the cumulative density function (CDF) of entropy for each of the methods and test sets. A distinct positive shift in the entropy CDFs can be seen for evidential models on OOD data and is competitive across methods. Fig. 5B summarizes these entropy distributions as interquartile boxplots to again show clear separation in the uncertainty distribution on OOD data.
We focus on the distribution from our evidential models in Fig. 5C and provide sample predictions (ID and OOD) in Fig. 5D. These results show that evidential models, without training on OOD data, capture increased uncertainty on OOD data on par with epistemic uncertainty estimation baselines. 4.3.1 Robustness to adversarial samples
Next, we consider the extreme case of OOD detection where the inputs are adversarially perturbed to inﬂict error on the predictions. We compute adversarial perturbations to our test set using the
Fast Gradient Sign Method (FGSM) [16], with increasing scales, (cid:15), of noise. Note that the purpose of this experiment is not to propose a defense for state-of-the-art adversarial attacks, but rather to demonstrate that evidential models accurately capture increased predictive uncertainty on samples which have been adversarily perturbed. Fig. 6A conﬁrms that the absolute error of all methods increases as adversarial noise is added. We also observe a positive effect of noise on our predictive uncertainty estimates in Fig. 6B. Furthermore, we observe that the entropy CDF steadily shifts towards higher uncertainties as the noise in the input sample increases (Fig. 6C). 7
Figure 6: Evidential robustness under adversarial noise. Relationship between adversarial noise (cid:15) and predictive error (A) and estimated epistemic uncertainty (B). (C) CDF of entropy estimated by evidential regression under the presence of increasing (cid:15). (D) Visualization of the effects of increasing adversarial pertubation on the predictions, error, and uncertainty for evidential regression. Results of sample test-set image are shown.
The robustness of evidential uncertainty against adversarial perturbations is visualized in greater detail in Fig. 6D, which illustrates the predicted depth, error, and estimated pixel-wise uncertainty as we perturb the input image with greater amounts of noise (left to right). Not only does the predictive uncertainty steadily increase with increasing noise, but the spatial concentrations of uncertainty throughout the image also maintain tight correspondence with the error. 5