Abstract
We consider a natural model of online preference aggregation, where sets of preferred items R1, R2, . . . , Rt, . . ., along with a demand for kt items in each Rt, appear online. Without prior knowledge of (Rt, kt), the learner maintains a ranking
πt aiming that at least kt items from Rt appear high in πt. This is a fundamental problem in preference aggregation with applications to, e.g., ordering product or news items in web pages based on user scrolling and click patterns.
The widely studied Generalized Min-Sum-Set-Cover (GMSSC) problem serves as a formal model for the setting above. GMSSC is NP-hard and the standard application of no-regret online learning algorithms is computationally inefﬁcient, because they operate in the space of rankings. In this work, we show how to achieve low regret for GMSSC in polynomial-time. We employ dimensionality reduction from rankings to the space of doubly stochastic matrices, where we apply
Online Gradient Descent. A key step is to show how subgradients can be computed efﬁciently, by solving the dual of a conﬁguration LP. Using oblivious deterministic and randomized rounding schemes, we map doubly stochastic matrices back to rankings with a small loss in the GMSSC objective. 1

Introduction
In applications where items are presented to the users sequentially (e.g., web search, news, online shopping, paper bidding), the item ranking is of paramount importance (see e.g., [38, 12, 14, 43, 7]).
More often than not, only the items at the ﬁrst few slots are immediately visible and the users may need to scroll down, in an attempt to discover items that ﬁt their interests best. If this does not happen soon enough, the users get disappointed and either leave the service (in case of news or online shopping, see e.g., the empirical evidence presented in [9]) or settle on a suboptimal action (in case of paper bidding, see e.g., [8]).
To mitigate such situations and increase user retention, modern online services highly optimize item rankings based on user scrolling and click patterns. Each user t is typically represented by her set of preferred items (or item categories) Rt . The goal is to maintain an item ranking πt online such that each new user t ﬁnds enough of her favorite items at relatively high positions in πt (“enough” is typically user and application dependent). A typical (but somewhat simplifying) assumption is that the user dis-utility is proportional to how deep in πt the user should reach before that happens.
The widely studied Generalized Min-Sum Set Cover (GMSSC) problem (see e.g., [28] for a short survey) provides an elegant formal model for the practical setting above. In (the ofﬂine version of) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
GMSSC, we are given a set U = {1, . . . , n} of n items and a sequence of requests R1, . . . , RT ⊆ U .
Each request R ⊆ U is associated with a demand (or covering requirement) K(R) ∈ {1, . . . , |R|}.
The access cost of a request R wrt. an item ranking (or permutation) π is the index of the K(R)-th element from R in π. Formally, (cid:80)T t=1 AccessCost(π, Rt).
AccessCost(π, R) = {the ﬁrst index up to which K(R) elements of R appear in π}. (1)
The goal is to compute a permutation π∗ ∈ [n!] of the items in U with minimum total access cost, i.e., π∗ = arg minπ∈[n!]
Due to its mathematical elegance and its connections to many practical applications, GMSSC and its variants have received signiﬁcant research attention [20, 5, 4, 29]. The special case where the covering requirement is K(Rt) = 1 for all requests Rt is known as Min-Sum Set Cover (MSSC). MSSC is
NP-hard, admits a natural greedy 4-approximation algorithm and is inapproximable in polynomial time within any ratio less than 4, unless P = NP [13]. Approximation algorithms for GMSSC have been considered in a sequence of papers [6, 36, 30] with the state of the art approximation ratio being 12.5. Closing the approximability gap, between 4 and 12.5, for GMSSC remains an interesting open question.
Generalized Min-Sum Set Cover and Online Learning. Virtually all previous work on GMSSC (the recent work of [17] is the only exception) assumes that the algorithm knows the request sequence and the covering requirements well in advance. However, in the practical item ranking setting consid-ered above, one should maintain a high quality ranking online, based on little (if any) information about the favorite items and the demand of new users.
Motivated by that, we study GMSSC as an online learning problem [21]. I.e., we consider a learner that selects permutations over time (without knowledge of future requests), trying to minimize her total access cost, and an adversary that selects requests R1, . . . , RT and their covering requirements, trying to maximize the learner’s total access cost. Speciﬁcally, at each round t ≥ 1, 1. The learner selects a permutation πt over the n items, i.e., πt ∈ [n!]. 2. The adversary selects a request Rt with covering requirement K(Rt). 3. The learner incurs a cost equal to AccessCost(πt, Rt).
Based on the past requests R1, . . . , Rt−1 only, an online learning algorithm selects (possibly with the use of randomization) a permutation πt trying to achieve a total (expected) access cost as close as possible to the total access cost of the optimal permutation π∗. If the cost of the online learning algorithm is at most α times the cost of the optimal permutation, the algorithm is α-regret [21]. If
α = 1, the algorithm is no-regret. In this work, we investigate the following question:
Question 1. Is there an online learning algorithm for GMSSC that runs in polynomial time and achieves α-regret, for some small constant α ≥ 1?
Despite a huge volume of work on efﬁcient online learning algorithms and the rich literature on approximation algorithms for GMSSC, Question 1 remains challenging and wide open. Although the Multiplicative Weights Update (MWU) algorithm, developed for the general problem of Learning from Expert Advice, achieves no-regret for GMSSC, it does not run in polynomial-time. In fact,
MWU treats each permutation as a different expert and maintains a weight vector of size n!. Even worse, this is inherent to GMSSC, due to the inapproximability result of [13]. Hence, unless P = NP,
MWU’s exponential requirements could not be circumvented by a more clever GMSSC-speciﬁc implementation, because any polynomial-time α-regret online learning algorithm can be turned into a polynomial-time α-approximation algorithm for GMSSC. Moreover, the results of [32] on obtaining computationally efﬁcient α-regret online learning algorithms from known polynomial time α-approximation algorithms for NP-hard optimization problems do not apply to optimizing non-linear objectives (such as the access cost in GMSSC) over permutations.
Our Approach and Techniques. Departing from previous work, which was mostly focused on black-box reductions from polynomial-time algorithms to polynomial-time online learning algorithms, e.g.,
[33, 32], we carefully exploit the structure of permutations and GMSSC, and present polynomial-time low-regret online learning deterministic and randomized algorithms for GMSSC, based on dimensionality reduction and Online Projected Gradient Descent.
Our approach consists of two major steps. The ﬁrst step is to provide an efﬁcient no-regret polynomial-time learning algorithm for a relaxation of GMSSC deﬁned on doubly stochastic matrices. To 2
Figure 1: Our general approach, which is independent of the speciﬁc variant of GMSSC. optimize over doubly stochastic matrices, the learner needs to maintain only n2 values, instead of the n! values required to directly describe distributions over permutations. This dimensionality reduction step allows for a polynomial-time no-regret online algorithm for the relaxed version of GMSSC.
The second step is to provide computationally efﬁcient (deterministic and randomized) online round-ing schemes that map doubly stochastic matrices back to probability distributions over permutations.
The main challenge is to guarantee that the expected access cost of the (possibly random) permutation obtained by rounding is within a factor of α from the access cost of the doubly stochastic matrix rep-resenting the solution to the relaxed problem. Once such a bound is established, it directly translates to an α-regret online learning algorithm with respect to the optimal permutation for GMSSC. Our approach is summarized in Figure 1.
Designing and Solving the Relaxed Online Learning Problem. For the relaxed version of
GMSSC, we note that any permutation π corresponds to an integral doubly stochastic matrix
Aπ, with Aπ[i, j] = 1 iff π(j) = i. Moreover for any request R, each doubly stochastic matrix is associated with a fractional access cost. For integral doubly stochastic matrices, the fractional access cost is practically identical to the access cost of GMSSC in the respective permutation.
The fractional access cost is given by the optimal solution of an (exponentially large) conﬁguration linear program (LP) that relaxes GMSSC to doubly stochastic matrices (see also [30]), and is a convex function. Thus, we can use Online Projected Gradient Descent (OPGD) [44] to produce a no-regret sequence of doubly stochastic matrices for the GMSSC relaxation. However, the efﬁcient computation of the subgradient is far from trivial, due to the exponential size of the conﬁguration
LP. A key technical step is to show that the subgradient of the conﬁguration LP can be computed in polynomial time, by solving its dual (which is of exponential size, so we resort to the elipsoid method and use an appropriate separation oracle).
Our Results. In nutshell, we resolve Question 1 in the afﬁrmative. In addition to solving the relaxed version of GMSSC by a polynomial-time no-regret online learning algorithm, as described above, we present a polynomial-time randomized rounding scheme that maps any doubly stochastic matrix to a probability distribution on permutations. The expected access cost of such a probability distribution is at most 28 times the fractional access cost of the corresponding doubly stochastic matrix.
Consequently, a 28-regret polynomial-time randomized online learning algorithm for GMSSC can be derived by applying, in each round, this rounding scheme to the doubly stochastic matrix At, produced by OPGD. For the important special case of MSSC, we improve the regret bound to 11.713 via a similar randomized rounding scheme that exploits the fact that K(R) = 1 for all requests.
We also present a polynomial-time deterministic rounding scheme mapping any (possibly fractional) doubly stochastic matrix to permutations. As before, applying this scheme to the sequence of doubly stochastic matrices produced by OPGD for the relaxation of GMSSC leads to a polynomial-time 3
deterministic online learning algorithm with regret 2 maxt |Rt| for MSSC. Such a nontrivial upper bound on the regret of deterministic online learning algorithms is rather surprising. Typically, learners that select their actions deterministically fail to achieve any nontrivial regret bounds (e.g., recall that in Learning From Expert Advice, any deterministic online algorithm has Ω(#experts) regret, which in case of MSSC is n!). Although 2 maxt |Rt| is not constant, one should expect that the requests are rather small in most practical applications. The above result is approximately tight, since any deterministic online learning algorithm must have regret at least maxt |Rt|/2 [17,
Theorem 1.1]. We should also highlight that the positive results of [17] do not imply the existence of computationally efﬁcient online learning algorithms for MSSC, because their approach is based on the MWU algorithm and uses a state space of n!. The state of the art and our results (in bold) are summarized below.
Running Time
Upper Bound (Regret)
Lower Bound (Regret)
GMSSC
GMSSC
MSSC
MSSC
MSSC
Exponential (MWU)
Polynomial
Polynomial
Exponential (deterministic)
Polynomial (deterministic) 1 28 11.713 2 · maxt |Rt| 2 · maxt |Rt| 1 4 (any polynomial time) 4 (any polynomial time) maxt |Rt| 2 maxt |Rt| 2 (any deterministic) (any deterministic)