Abstract
Over-parametrization is an important technique in training neural networks. In both theory and practice, training a larger network allows the optimization algorithm to avoid bad local optimal solutions. In this paper we study a closely related tensor decomposition problem: given an l-th order tensor in (Rd)⊗l of rank r (where r (cid:28) d), can variants of gradient descent ﬁnd a rank m decomposition where m > r? We show that in a lazy training regime (similar to the NTK regime for neural networks) one needs at least m = Ω(dl−1), while a variant of gradient descent can ﬁnd an approximate tensor when m = O∗(r2.5l log d). Our results show that gradient descent on over-parametrized objective could go beyond the lazy training regime and utilize certain low-rank structure in the data. 1

Introduction
The success of training neural networks has sparked theoretical research in understanding non-convex optimization. Over-parametrization – using more neurons than the number of training data or than what is necessary for expressivity – is crucial to the success of optimizing neural networks (Livni et al., 2014; Jacot et al., 2018; Mei et al., 2018). The idea of over-parametrization also applies to other related or simpliﬁed problems, such as matrix factorization and tensor decomposition, which are of their own interests and also serve as testbeds for analysis techniques of non-convex optimization.
We focus on over-parameterized tensor decomposition in this paper (which is closely connected to over-parameterized neural networks (Ge et al., 2018)).
Concretely, given an order-l symmetric tensor T ∗ in (Rd)⊗l with rank r, we aim to decompose it into a sum of rank-1 tensors with as few components as possible. Finding the low-rank decomposition with the smallest possible rank r is known to be NP-hard (Hillar and Lim, 2013). The problem becomes easier if we relax the goal to ﬁnding a decomposition with m components where m is allowed to be larger than r. The natural approach is to optimize the following objective using gradient descent min ui∈Rd,ci∈R (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) m (cid:88) i=1 ciu⊗l i − r (cid:88) i=1 i [u∗ c∗ i ]⊗l (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13)
F
. (1)
When m = r, gradient descent on the objective above will empirically get stuck at a bad local minimum even for orthogonal tensors (Ge et al., 2015). On the other hand, when m = Ω(dl−1),
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
gradient descent provably converges to a global minimum near the initialization. This result follows straightforwardly from the Neural Tangent Kernel (NTK) technique (Jacot et al., 2018), which was originally developed to analyze neural network training, and is referred to as the “lazy training” regime because essentially the algorithm is optimizing a convex function near the initialization (Chizat and
Bach, 2018a).
The main goal of this paper is to understand whether we can go beyond the lazy training regime for the tensor decomposition problem via better algorithm design and analysis. In other words, we aim to use a much milder over-parametrization than m = Ω(dl−1) and still converge to the global minimum of objective (1). We view the problem as an important ﬁrst step towards analyzing neural network training beyond the lazy training regime.
We build upon the technical framework of mean-ﬁeld analysis (Mei et al., 2018), which was developed to analyze overparameterized neural networks. It allows the parameters to move far away from the initialization and therefore has the potential to capture the realistic training regime of neural networks.
However, to date, all the provable optimization results with mean-ﬁeld analysis essentially operate in the inﬁnite or exponential overparameterization regime (Chizat and Bach, 2018b; Wei et al., 2019), and applying these techniques to our problem naively would require m to be exponentially large in d, which is even worse than the NTK result. The exponential dependency is not surprising because the mean-ﬁeld analyses in (Chizat and Bach, 2018b; Wei et al., 2019) do not leverage or assume any particular structures of the data so they fail to produce polynomial-time guarantees on the worst-case data. Motivated by identifying problem structure that allows for polynomial-time guarantees, we study the mean-ﬁeld analysis applied to tensor decomposition.
The main contribution of this paper is to attain nearly dimension-independent over-parametrization for the mean-ﬁeld analysis in Wei et al. (2019) by leveraging the particular structure of the tensor decomposition problem, and to show that with m = O∗(r2.5l log d), a modiﬁed version of gradient descent on a variant of objective (1) converges to the global minimum and recovers the ground-truth tensor. This is a signiﬁcant improvement over the NTK requirement of m = Ω(dl−1) and an exponential improvement upon the existing mean-ﬁeld analysis that requires m = exp(d). Our analysis shows that unlike the lazy training regime, gradient descent with small initialization and appropriate regularizer can identify the subspace that the ground-truth components lie in, and automatically exploit such structure to reduce the number of necessary components. As shown in Ge et al. (2018), the population-level objective of two-layer networks is a mixture of tensor decomposition objectives with different orders, so our analysis may be extendable to improve the over-parametrization necessary in analysis of two-layer networks. 1.1