Abstract
Normalizing ﬂows are deep generative models that allow efﬁcient likelihood cal-culation and sampling. The core requirement for this advantage is that they are constructed using functions that can be efﬁciently inverted and for which the deter-minant of the function’s Jacobian can be efﬁciently computed. Researchers have introduced various such ﬂow operations, but few of these allow rich interactions among variables without incurring signiﬁcant computational costs. In this paper, we introduce Woodbury transformations, which achieve efﬁcient invertibility via the Woodbury matrix identity and efﬁcient determinant calculation via Sylvester’s determinant identity. In contrast with other operations used in state-of-the-art normalizing ﬂows, Woodbury transformations enable (1) high-dimensional in-teractions, (2) efﬁcient sampling, and (3) efﬁcient likelihood evaluation. Other similar operations, such as 1x1 convolutions, emerging convolutions, or periodic convolutions allow at most two of these three advantages. In our experiments on multiple image datasets, we ﬁnd that Woodbury transformations allow learning of higher-likelihood models than other ﬂow architectures while still enjoying their efﬁciency advantages. 1

Introduction
Deep generative models are powerful tools for modeling complex distributions and have been applied to many tasks such as synthetic data generation [26, 37], domain adaption [38], and structured prediction [32]. Examples of these models include autoregressive models [13, 27], variational autoencoders [20, 30], generative adversarial networks [11], and normalizing ﬂows [6, 7, 21, 29].
Normalizing ﬂows are special because of two advantages: They allow efﬁcient and exact computation of log-likelihood and sampling.
Flow-based models are composed of a series of invertible functions, which are speciﬁcally designed so that their inverse and determinant of the Jacobian are easy to compute. However, to preserve this computational efﬁciency, these functions usually cannot sufﬁciently encode dependencies among dimensions of a variable. For example, afﬁne coupling layers [6] split a variable to two parts and require the second part to only depend on the ﬁrst. But they ignore the dependencies among dimensions in the second part.
To address this problem, Dinh et al. [6, 7] introduced a ﬁxed permutation operation that reverses the ordering of the channels of pixel variables. Kingma and Dhariwal [21] introduced a 1×1 convolution, which are a generalized permutation layer, that uses a weight matrix to model the interactions among dimensions along the channel axis. Their experiments demonstrate the importance of capturing dependencies among dimensions. Relatedly, Hoogeboom et al. [15] proposed emerging convolution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
operations, and Hoogeboom et al. [15] and Finz et al. [9] proposed periodic convolution. These two convolution layers have d × d kernels that can model dependencies along the spatial axes in addition to the channel axis. However, the increase in representational power comes at a cost: These convolution operations do not scale well to high-dimensional variables. The emerging convolution is a combination of two autoregressive convolutions [10, 22], whose inverse is not parallelizable. To compute the inverse or determinant of the Jacobian, the periodic convolution requires transforming the input and the convolution kernel to Fourier space. This transformation is computationally costly.
In this paper, we develop Woodbury transformations for generative ﬂows. Our method is also a generalized permutation layer and uses spatial and channel transformations to model dependencies among dimensions along spatial and channel axes. We use the Woodbury matrix identity [36] and
Sylvester’s determinant identity [34] to compute the inverse and Jacobian determinant, respectively, so that both the training and sampling time complexities are linear to the input variable’s size. We also develop a memory-efﬁcient variant of the Woodbury transformation, which has the same advantage as the full transformation but uses signiﬁcantly reduced memory when the variable is high-dimensional.
In our experiments, we found that Woodbury transformations enable model quality comparable to many state-of-the-art ﬂow architectures while maintaining signiﬁcant efﬁciency advantages. 2 Deep Generative Flows
In this section, we brieﬂy introduce the deep generative ﬂows. More background knowledge can be found in the appendix.
A normalizing ﬂow [29] is composed of a series of invertible functions f = f1 ◦ f2 ◦ ... ◦ fK, which transform x to a latent code z drawn from a simple distribution. Therefore, with the change of variables formula, we can rewrite the log-likelihood log pθ(x) to be (cid:18) ∂fi
∂ri−1 log pθ(x) = log pZ(z) + (cid:19)(cid:12) (cid:12) (cid:12) (cid:12)
K (cid:88) det log (1) (cid:12) (cid:12) (cid:12) (cid:12)
, i=1 where ri = fi(ri−1), r0 = x, and rK = z.
Flow-based generative models [6, 7, 21] are developed on the theory of normalizing ﬂows. Each transformation function used in the models is a speciﬁcally designed neural network that has a tractable Jacobian determinant and inverse. We can sample from a trained ﬂow f by computing z ∼ pZ(z), x = f −1(z).
There have been many operations, i.e., layers, proposed in recent years for generative ﬂows. In this section, we discuss some commonly used ones, and more related works will be discussed in Section 4.
Actnorm layers [21] perform per-channel afﬁne transformations of the activations using scale and bias parameters to improve training stability and performance. The actnorm is formally expressed as y:,i,j = s (cid:12) x:,i,j + b, where both the input x and the output y are c × h × w tensors, c is the channel dimension, and h × w are spatial dimensions. The parameters s and b are c × 1 vectors.
Afﬁne coupling layers [6, 7] split the input x into two parts, xa, xb. And then ﬁx xa and force xb to only relate to xa, so that the Jacobian is a triangular matrix. Formally, we compute xa, xb = split(x), yb = s(xa) (cid:12) xb + b(xa), y = concat(ya, yb), ya = xa, where s and b are two neural networks with xa as input. The split and the concat split and concatenate the variables along the channel axis. Usually, s is restricted to be positive. An additive coupling layer is a special case when s = 1.
Actnorm layers only rescale the dimensions of x, and afﬁne coupling layers only relate xb to xa but omit dependencies among different dimensions of xb. Thus, we need other layers to capture local dependencies among dimensions.
Invertible convolutional layers [9, 15, 21] are generalized permutation layers that can capture correlations among dimensions. The 1×1 convolution [21] is y:,i,j = Mx:,i,j, where M is a c × c matrix. The Jacobian of a 1×1 convolution is a block diagonal matrix, so that its log-determinant is hw log | det(M)|. Note that the 1×1 convolution only operates along the channel axis and ignores the dependencies along the spatial axes. 2
Emerging convolutions [15] combine two autore-gressive convolutions [10, 22]. Each autoregressive convolution masks out some weights to force an autoregressive structure, so that the Jacobian is a triangular matrix and computing its determinant is efﬁcient. One problem of emerging convolution is the computation of inverse is non-parallelizable, so that is inefﬁcent for high-dimensional variables.
Periodic convolutions [9, 15] transform the input and kernel to the Fourier domain using discrete
Fourier transformations, so the convolution func-tion is an element-wise matrix product with a block-diagonal Jacobian. The computational cost of pe-riodic convolutions is O(chw log(hw) + c3hw).
Thus, when the input is high-dimensional, both training and sampling are expensive. (a) Flow step (b) Multi-scale arch.
Figure 1: Overview of architecture of generative
ﬂows. We can design the ﬂow step by selecting a suitable convolutional layer and a coupling layer based on the task. Glow [21] uses 1×1 convolutions and afﬁne coupling.
Multi-scale architectures [7] compose ﬂow layers to generate rich models, using split layers to factor out variables and squeeze layers to shufﬂe dimensions, resulting in an architecture with K ﬂow steps and L levels. See Fig. 1. 3 Woodbury Transformations
In this section, we introduce Woodbury transformations as an efﬁcient means to model high-dimensional correlations. 3.1 Channel and Spatial Transformations
Suppose we reshape the input x to be a c × n matrix, where n = hw. Then the 1×1 convolution can be reinterpreted as a matrix transformation y = W(c)x, (2) where y is also a c × n matrix, and W(c) is a c × c matrix. For consistency, we will call this a channel transformation. For each column x:,i, the correlations among channels are modeled by W(c).
However, the correlation between any two rows x:,i and x:,j is not captured. Inspired by Eq. 2, we use a spatial transformation to model interactions among dimensions along the spatial axis y = xW(s), (3) where W(s) is an n × n matrix that models the correlations of each row xi,:. Combining Equation 2 and Equation 3, we have xc = W(c)x, y = xcW(s). (4)
For each dimension of output yi,j, we have yi,j = (cid:80)c v=1 (cid:16)(cid:80)n u=1 W(c) i,u · xu,v (cid:17)
· W(s) v,j.
Therefore, the spatial and channel transformations together can model the correlation between any pair of dimensions. However, in this preliminary form, directly using Eq. 4 is inefﬁcient for large c or n. First, we would have to store two large matrices Wc and Ws, so the space cost is O(c2 + n2).
Second, the computational cost of Eq. 4 is O(c2n + n2c)—quadratic in the input size. Third, the computational cost of the Jacobian determinant is O(c3 + n3), which is far too expensive in practice. 3.2 Woodbury Transformations
We solve the three scalability problems by using a low-rank factorization. Speciﬁcally, we deﬁne
W(c) = I(c) + U(c)V(c), W(s) = I(s) + U(s)V(s), 3
where I(c) and I(s) are c- and n-dimensional identity matrices, respectively. The matrices Uc, Vc,
Us, and Vs are of size c × dc, dc × c, n × ds, and dc × n, respectively, where dc and ds are constant latent dimensions of these four matrices. Therefore, we can rewrite Equation 4 as xc = (I(c) + U(c)V(c))x, y = xc(I(s) + U(s)V(s)). (5)
We call Eq. 5 the Woodbury transformation because the Woodbury matrix identity [36] and Sylvester’s determinant identity [34] allow efﬁcient computation of its inverse and Jacobian determinant.
Woodbury matrix identity.1 Let I(n) and I(k) be n- and k-dimensional identity matrices, respec-tively. Let U and V be n × k and k × n matrices, respectively. If I(k) + VU is invertible, then (I(n) + UV)−1 = I(n) − U(Ik + VU)−1V.
Sylvester’s determinant identity. Let I(n) and I(k) be n- and k-dimensional identity matrices, respectively. Let U and V be n × k and k × n matrices, respectively. Then, det(I(n) + UV) = det(I(k) + VU).
Based on these two identities, we can efﬁciently compute the inverse and Jacobian determinant xc = y(I(s) − U(s)(I(ds) + V(s)U(s))−1V(s)), x = (I(c) − U(c)(I(dc) + V(c)U(c))−1V(c))xc, and log (cid:12) (cid:12) (cid:12) (cid:12) det (cid:18) ∂y
∂x (cid:19)(cid:12) (cid:12) (cid:12) (cid:12)
= n log (cid:12) (cid:12)det(I(dc) + V(c)U(c))(cid:12) (cid:12) + c log (cid:12) (cid:12)det(I(ds) + V(s)U(s))(cid:12) (cid:12) , (6) (7) where I(dc) and I(ds) are dc- and ds-dimensional identity matrices, respectively.
A Woodbury transformation is also a generalized permutation layer. We can directly replace an invert-ible convolution in Figure 1a with a Woodbury transformation. In contrast with 1×1 convolutions,
Woodbury transformations are able to model correlations along both channel and spatial axes. We illustrate this in Figure 2. To implement Woodbury transformations, we need to store four weight matrices, i.e., U(c), U(s), V(c), and V(s). To simplify our analysis, let dc ≤ d and ds ≤ d, where d is a constant. This setting is also consistent with our experiments. The size of U(c) and V(c) is
O(dc), and the size of U(c) and V(c) is O(dn). The space complexity is O(d(c + n)).
For training and likelihood computation, the main computational bottleneck is computing y and the Jacobian determinant. To compute y with Equation 4, we need to ﬁrst compute the channel transformation and then compute the spatial transformation. The computational complexity is O(dcn).
To compute the determinant with Equation 7, we need to ﬁrst compute the matrix product of V and
U, and then compute the determinant. The computational complexity is O(d2(c + n) + d3).
For sampling, we need to compute the inverse transformations, i.e., Equation 6. With the Woodbury identity, we actually only need to compute the inverses of I(ds) + V(s)U(s) and I(dc) + V(c)U(c), which are computed with time complexity O(d3). To implement the inverse transformations, we can compute the matrix chain multiplication, so we can avoid computing the product of two large matrices twice, yielding cost O(c2 + n2). For example, for the inverse spatial transformation, we can compute it as xc = y − ((yU(s))(I(ds) + V(s)U(s))−1)V(s), so that its complexity is O(d3 + cd2 + cnd).
The total computational complexity of Equation 6 is O(dcn + d2(n + c) + d3).
In practice, we found that for a high-dimensional input, a relatively small d is enough to obtain good performance, e.g., the input is 256 × 256 × 3 images, and d = 16. In this situation, nc ≥ d3.
Therefore, we can omit d and approximately see the spatial complexity as O(c + n), and the forward or inverse transformation as O(nc). They are all linear to the input size.
We do not restrict U and V to force W to be invertible. Based on analysis by Hoogeboom et al.
[15], the training maximizes the log-likelihood, which implicitly pushes det(I + VU) away from 0. Therefore, it is not necessary to explicitly force invertibility. In our experiments, the Woodbury transformations are as robust as other invertible convolution layers. 1A more general version replaces I(n) and I(k) with arbitrary invertible n × n and k × k matrices. But this simpliﬁed version is sufﬁcient for our tasks. 4
(a) 1×1 convolution (b) Woodbury (c) ME-Woodbury
Figure 2: Visualization of three transformations. The 1×1 convolution only operates along the channel axis. The Woodbury transformation operates along both the channel and spatial axes, modeling the dependencies of one channel directly via one transformation. The ME-Woodbury transformation operates along three axes. It uses two transformations to model spatial dependencies. 3.3 Memory-Efﬁcient Variant
In Eq. 4, one potential challenge arises from the sizes of U(s) and V((s)), which are linear in n. The challenge is that n may be large in some practical problems, e.g., high-resolution images. We develop a memory-efﬁcient variant of Woodbury transformations, i.e., ME-Woodbury, to solve this problem.
The ME version can effectively reduce space complexity from O(d(c + hw)) to O(d(c + h + w)).
The difference between ME-Woodbury transformations and Woodbury transformations is that the ME form cannot directly model spatial correlations. As shown in Figure 2c, it uses two transformations, for height and width, together to model the spatial correlations. Therefore, for a speciﬁc channel k, when two dimensions xk,i,j and xk,u,v are in two different heights, and widths, their interaction will be modeled indirectly. In our experiments, we found that this limitation only slightly impacts
ME-Woodbury’s performance. More details on ME-Woodbury transformations are in the appendix. 4