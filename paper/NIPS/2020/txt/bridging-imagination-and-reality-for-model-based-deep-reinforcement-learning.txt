Abstract
Sample efﬁciency has been one of the major challenges for deep reinforcement learning. Recently, model-based reinforcement learning has been proposed to address this challenge by performing planning on imaginary trajectories with a learned world model. However, world model learning may suffer from overﬁtting to training trajectories, and thus model-based value estimation and policy search will be prone to be sucked in an inferior local policy. In this paper, we propose a novel model-based reinforcement learning algorithm, called BrIdging Reality and Dream (BIRD). It maximizes the mutual information between imaginary and real trajectories so that the policy improvement learned from imaginary trajectories can be easily generalized to real trajectories. We demonstrate that our approach improves sample efﬁciency of model-based planning, and achieves state-of-the-art performance on challenging visual control benchmarks. 1

Introduction
Reinforcement learning (RL) is proposed as a general-purpose learning framework for artiﬁcial intelligence problems, and has led to tremendous progress in a variety of domains [1, 2, 3, 4]. Model-free RL adopts a trail-and-error paradigm, which directly learns a mapping function from observations to values or actions through interactions with environments. It has achieved remarkable performance in certain video games and continuous control tasks because of its simplicity and minimal assumptions about environments. However, model-free approaches are not yet sample efﬁcient and require several orders of magnitude more training samples than human learning, which limits its applications on real-world tasks [5].
A promising direction for improving sample efﬁciency is to explore model-based RL, which ﬁrst builds an action-conditioned world model and then performs planning or policy search based on the learned model. The world model needs to encode the representations and dynamics of an environment is then used as a “dreamer” to do multi-step lookaheads for planning or policy search. Recently, world models based on deep neural networks were developed to handle dynamics in complex high-dimensional environments, which offers opportunities for learning model-based polices with visual observations [6, 7, 8, 9, 10, 11, 12, 13].
⇤Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Model-based frameworks can be roughly grouped into four categories. First, Dyna-style algorithms alternate between building the world model from interactions with environments and performing policy optimization on simulated data generated by the learned model [14, 15, 16, 17, 11]. Second, model predictive control (MPC) and shooting algorithms alternate model learning, planning and action execution [18, 19, 20]. Third, model-augmented value expansion algorithms use model-based rollouts to improve targets for model-free temporal difference (TD) updates or policy gradients [21, 9, 6, 10].
Fourth, analytic-gradient algorithms leverage the gradients of the model-based imaginary returns with respect to the policy and directly propagate such gradients through a differentiable world model to the policy network [22, 23, 24, 25, 26, 27, 13]. Compared to conventional planning algorithms that generate numerous rollouts to select the highest performing action sequence, analytic-gradient algorithm is more computationally efﬁcient, especially in complex domains with deep neural networks. Dreamer [13] as a landmark of analytic-gradient model-based RL, achieves state-of-the-art performance on visual control tasks.
However, most existing breakthroughs on analytic gradients focus on optimizing the policy on imaginary trajectories and leave the discrepancy between imagination and reality largely unstudied, which often bottlenecks their performance on real trajectories. In practice, a learning-based world model is not perfect, especially in complex environments. Unrolling with an imperfect model for multiple steps generates a large accumulative error, leaving a gap between the generated trajectories and reality. If we directly optimize policy based on the analytic gradients through the imaginary trajectories, the policy will tend to deviate from reality and get sucked in an inferior local solution.
Evidence from humans’ cognition and learning in the physical world suggests that humans naturally have the capacity of self-reﬂection and introspection. In everyday life, we track and review our past thoughts and imaginations, introspect to further understand our internal states and interactions with the external world, and change our values and behavior patterns accordingly [28, 29]. Inspired by this insight, our basic idea is to leverage information from real trajectories to endow policy improvement on imaginations with awareness of discrepancy between imagination and reality.
We propose a novel reality-aware model-based framework, called BrIdging Reality and Dream (BIRD), which performs differentiable planning on imaginary trajectories, as well as enables adaptive generalization to reality for learned policy by optimizing mutual information between imaginary and real trajectories. Our model-based policy optimization framework naturally uniﬁes conﬁdence-aware analytic gradients, entropy regularization maximization, and model learning. We conduct experiments on challenging visual control benchmarks (DeepMind Control Suite with image inputs [30]) and the results demonstrate that BIRD achieves state-of-the-art performance in terms of sample efﬁciency. Our ablation study further veriﬁes the superiority of BIRD beneﬁts from mutual information maximization rather than from the increase of policy entropy. 2