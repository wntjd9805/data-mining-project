Abstract
Event cameras encode visual information with high temporal precision, low data-rate, and high-dynamic range. Thanks to these characteristics, event cameras are particularly suited for scenarios with high motion, challenging lighting conditions and requiring low latency. However, due to the novelty of the ﬁeld, the performance of event-based systems on many vision tasks is still lower compared to conventional frame-based solutions. The main reasons for this performance gap are: the lower spatial resolution of event sensors, compared to frame cameras; the lack of large-scale training datasets; the absence of well established deep learning architectures for event-based processing. In this paper, we address all these problems in the context of an event-based object detection task. First, we publicly release the ﬁrst high-resolution large-scale dataset for object detection. The dataset contains more than 14 hours recordings of a 1 megapixel event camera, in automotive scenarios, together with 25M bounding boxes of cars, pedestrians, and two-wheelers, labeled at high frequency. Second, we introduce a novel recurrent architecture for event-based detection and a temporal consistency loss for better-behaved training. The ability to compactly represent the sequence of events into the internal memory of the model is essential to achieve high accuracy. Our model outperforms by a large margin feed-forward event-based architectures. Moreover, our method does not require any reconstruction of intensity images from events, showing that training directly from raw events is possible, more efﬁcient, and more accurate than passing through an intermediate intensity image. Experiments on the dataset introduced in this work, for which events and gray level images are available, show performance on par with that of highly tuned and studied frame-based detectors. 1

Introduction
Event cameras [1, 2, 3, 4] promise a paradigm shift in computer vision by representing visual information in a fundamentally different way. Rather than encoding dynamic visual scenes with a sequence of still images, acquired at a ﬁxed frame rate, event cameras generate data in the form of a sparse and asynchronous events stream. Each event is represented by a tuple (x, y, p, t) corresponding to an illuminance change by a ﬁxed relative amount, at pixel location (x, y) and time t, with the polarity p ∈ {0, 1} indicating whether the illuminance was increasing or decreasing. Fig. 1 shows examples of data from an event camera in a driving scenario.
Since the camera does not rely on a global clock, but each pixel independently emits an event as soon as it detects an illuminance change, the events stream has a very high temporal resolution, typically 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Results of our event-camera detector on examples of the released 1Mpx Automotive
Detection Dataset. Our method is able to accurately detect objects for a large variety of appearances, scenarios, and speeds. This makes it the ﬁrst reliable event-based system on a large-scale vision task.
Detected cars, pedestrians and two-wheelers are shown in yellow, blue and cyan boxes respectively.
All ﬁgures in this work are best seen in electronic form. of the order of microseconds [1]. Moreover, due to a logarithmic pixel response characteristic, event cameras have a large dynamic range (often exceeding 120dB) [4]. Thanks to these properties, event cameras are well suited for applications in which standard frame cameras are affected by motion blur, pixel saturation, and high latency.
Despite the remarkable properties of event cameras, we are still at the dawn of event-based vision and their adoption in real systems is currently limited. This implies scarce availability of algorithms, datasets, and tools to manipulate and process events. Additionally, most of the available datasets have limited spatial resolution or they are not labeled, reducing the range of possible applications [5, 6].
To overcome these limitations, several works have focused on the reconstruction of gray-level information from an event stream [7, 8, 9, 10]. This approach is appealing since the reconstructed images can be fed to standard computer vision pipelines, leveraging more than 40 years of computer vision research. In particular, it was shown [10] that all information required to reconstruct high-quality images is present in the event data. However, passing through an intermediate intensity image comes at the price of adding considerable computational cost. In this work, we show how to build an accurate event-based vision pipeline without the need of gray-level supervision.
We target the problem of object detection in automotive scenarios, which is characterized by important objects dynamics and extreme lighting conditions. We make the following contributions to the ﬁeld:
First, we acquire and release the ﬁrst large scale dataset for event-based object detection, with a high resolution (1280×720) event camera [4]. We also deﬁne a fully automated labeling protocol, enabling fast and cheap dataset generation for event cameras. The dataset we release contains more than 14 hours of driving recording, acquired in a large variety of scenarios. We also provide more than 25 million bounding boxes of cars, pedestrians and two-wheelers, labeled at 60Hz.
Our second contribution is the introduction of a novel architecture for event-based object detection together with a new temporal consistency loss. Recurrent layers are the core building block of our architecture, they introduce a fundamental memory mechanism needed to reach high accuracy with event data. At the same time, the temporal consistency loss helps to obtain more precise localization over time. Fig. 1 shows some detections returned by our method on the released dataset. We show that directly predicting the object locations is more efﬁcient and more accurate than applying a detector on the gray-level images reconstructed with a state-of-the-art method [10]. In particular, since we do not impose any bias coming from intensity image supervision, we let the system learn the relevant features for the given task, which do not necessarily correspond to gray-level values.
Finally, we run extensive experiments on ours and another dataset for which gray-level images are also available, showing comparable accuracy to standard frame-based detectors and improved state-of-the-art results for event-based detection. To the best of our knowledge, this is the ﬁrst work showing an event-based system with on par performance to a frame-based one on a large vision task. 2