Abstract
The existence of adversarial examples capable of fooling trained neural network classiﬁers calls for a much better understanding of possible attacks to guide the development of safeguards against them. This includes attack methods in the chal-lenging non-interactive blackbox setting, where adversarial attacks are generated without any access, including queries, to the target model. Prior attacks in this setting have relied mainly on algorithmic innovations derived from empirical obser-vations (e.g., that momentum helps), lacking principled transferability guarantees.
In this work, we provide a theoretical foundation for crafting transferable adver-sarial examples to entire hypothesis classes. We introduce Adversarial Example
Games (AEG), a framework that models the crafting of adversarial examples as a min-max game between a generator of attacks and a classiﬁer. AEG provides a new way to design adversarial examples by adversarially training a generator and a classiﬁer from a given hypothesis class (e.g., architecture). We prove that this game has an equilibrium, and that the optimal generator is able to craft adversarial examples that can attack any classiﬁer from the corresponding hypothesis class.
We demonstrate the efﬁcacy of AEG on the MNIST and CIFAR-10 datasets, outper-forming prior state-of-the-art approaches with an average relative improvement of 29.9% and 47.2% against undefended and robust models (Table 2 & 3) respectively. 1

Introduction
Adversarial attacks on deep neural nets expose critical vulnerabilities in traditional machine learning systems [55, 3, 64, 8]. In order to develop models that are robust to such attacks, it is imperative that we improve our theoretical understanding of different attack strategies. While there has been con-siderable progress in understanding the theoretical underpinnings of adversarial attacks in relatively permissive settings (e.g. whitebox adversaries; [53]), there remains a substantial gap between theory and practice in more demanding and realistic threat models.
In this work, we provide a theoretical framework for understanding and analyzing adversarial attacks in the highly-challenging Non-interactive blackBox adversary (NoBox) setting, where the attacker has no direct access, including input-output queries, to the target classiﬁer it seeks to fool. Instead,
∗Equal Contribution, order chosen via randomization.
†Canada CIFAR AI Chair 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the attacker must generate attacks by optimizing against some representative classiﬁers, which are assumed to come from a similar hypothesis class as the target.
The NoBox setting is a much more challenging setting than more traditional threat models, yet it is representative of many real-world attack scenarios, where the attacker cannot interact with the target model [15]. Indeed, this setting—as well as the general notion of transferring attacks between classiﬁers—has generated an increasing amount of empirical interest [25, 51, 73, 71]. The ﬁeld, however, currently lacks the necessary theoretical foundations to understand the feasibility of such attacks.
Contributions. To address this theoretical gap, we cast NoBox attacks as a kind of adversarial example game (AEG). In this game, an attacker generates adversarial examples to fool a representative classiﬁer from a given hypothesis class, while the classiﬁer itself is trained to detect the correct labels from the adversarially generated examples. Our ﬁrst main result shows that the Nash equilibrium of an AEG leads to a distribution of adversarial examples effective against any classiﬁer from the given function class. More formally, this adversarial distribution is guaranteed to be the most effective distribution for attacking the hardest-to-fool classiﬁers within the hypothesis class, providing a worst-case guarantee for attack success against an arbitrary target. We further show that this optimal adversarial distribution admits a natural interpretation as being the distribution that maximizes a form of restricted conditional entropy over the target dataset, and we provide detailed analysis on simple parametric models to illustrate the characteristics of this optimal adversarial distribution. Note that while AEGs are latent games [30], they are distinct from the popular generative adversarial networks (GANs) [32]. In AEGs, there is no discrimination task between two datasets (generated one and real one); instead, there is a standard supervised (multi-class) classiﬁcation task on an adversarial dataset.
Guided by our theoretical results we instantiate AEGs using parametric functions —i.e. neural networks, for both the attack generator and representative classiﬁer and show the game dynamics progressively lead to a stronger attacker and robust classiﬁer pairs. We empirically validate AEG on standard CIFAR and MNIST benchmarks and achieve state-of-the-art performance —compared to existing heuristic approaches— in nearly all experimental settings (e.g., transferring attacks to unseen architectures and attacking robustiﬁed models), while also maintaining a ﬁrm theoretical grounding. 2