Abstract
Recent voxel-based 3D object detectors for autonomous vehicles learn point cloud representations either from bird eye view (BEV) or range view (RV, a.k.a. perspec-tive view). However, each view has its own strengths and weaknesses. In this paper, we present a novel framework to unify and leverage the beneﬁts from both BEV and RV. The widely-used cuboid-shaped voxels in Cartesian coordinate system only beneﬁt BEV feature map. Therefore, to enable learning both BEV and RV feature maps, we introduce Hybrid-Cylindrical-Spherical voxelization. Our ﬁndings show that simply adding detection on another view as auxiliary supervision will lead to poor performance. We proposed a pair of cross-view transformers to transform the feature maps into the other view and introduce cross-view consistency loss on them. Comprehensive experiments on the challenging NuScenes Dataset validate the effectiveness of our proposed method which leverages joint optimization and complementary information on both views. Remarkably, our approach achieved mAP of 55.8%, outperforming all published approaches by at least 3% in overall performance and up to 16.5% in safety-crucial categories like cyclist. 1

Introduction
With a great surge of autonomous vehicles and accessibility of cheaper laser sensors, e.g. LiDAR, learning directly from 3D LiDAR point clouds has become increasingly popular. Among LiDAR-based 3D object detectors, a line of works [1, 2, 3, 4] borrow the success of convolutional neural networks on 2D images, and group the unordered, irregular and sparse point clouds into cuboid-shaped volumetric grids, i.e. voxels. 3D feature maps are memory-consuming, and therefore most of the recent works [4, 5, 6, 2, 7, 8] project the feature maps into 2D at different stages in their pipelines.
When choosing 2D representations, it is important that objects in the input point cloud are still visible in the projected view. In autonomous driving scenarios, object do not overlap in the bird’s-eye-view (BEV) and the size of the objects are consistent regardless of its distance from the ego-vehicle. Hence each object projected into BEV remains visible. Alternatively, RV projection suffers from occlusion and object size variation with respect to distance but it generates dense features. Both BEV and RV representation are suitable for 3D detection. State-of-the-art voxel based detectors [4, 5, 2, 7] detect objects based on features from either BEV or RV.
* indicates equal contributions 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Features in each view has their strengths and weaknesses. From BEV, rigid objects are usually kept a distance from each other so it is easy to separate the objects. However, some important targets (e.g. trafﬁc cones) are tiny when viewed from BEV, and thus are hard to detect. From RV, similar to 2D images, objects may be partially occluded and appear as difference sizes at different ranges, i.e., distances to the sensor. Furthermore, existing RV-based detectors [5] lose depth information during projection, making it hard to localize accurately.
Main Contributions We present a novel Cross-view Consistent Network (CVCNet) which lever-ages the advantages of both range view (RV) and Bird’s-eye-view (BEV) in 3D detection. We highlight two main contributions in this work. Firstly, to the best of our knowledge, we are the ﬁrst work that introduces the concept of Cross-view Consistency to 3D detection task. We discover that the performance will degrade if we simply add detection on another view as an auxiliary supervisory signal. We posit that object appearances on two views are different and it’s hard for the network to learn the latent correlation and extract common features from two views. Based on the observation that the correspondences between two views have similar properties to Hough Transform, we propose a pair of Hough-Transform-like cross-view transformers that explicitly incorporate the correlation between two views and enforce consistency on the transformed features. We have conducted ablation studies and in-depth discussions to show that such consistency is a key factor to beneﬁt from joint learning in BEV and RV.
Secondly, we designed a new Voxel representation, Hybrid-Cylindrical-Spherical (HCS) Voxels, which enables us to extract features for both RV and BEV in a uniﬁed coordination system. In contrast, the commonly used cuboid-shaped voxels based on Cartesian coordinates provide beneﬁts to feature learning on BEV. Driven by outstanding performance of shared models that are applied to extract common low-level features across different tasks, our model uses the shared 3D network and two light-weight branches to adapt into different views. Our HCS Voxels play an essential role in this design as it contains all the dimensions needed for projection to RV and BEV.
Extensive experiments on NuScenes dataset [9] demonstrate that CVCNet outperforms all the published approaches in overall average precision (mAP). In particular, our mAP on pedestrians, motorcyclist and cyclist are 83.0%, 61.8%, 38.8%, which is at least 2.9%, 10.3%, and 16.5% better than existing published methods. These results signify substantial safety improvement when our algorithm is applied to autonomous vehicles. 2