Abstract
We consider the task of sampling with respect to a log concave probability distribu-tion. The potential of the target distribution is assumed to be composite, i.e., written as the sum of a smooth convex term, and a nonsmooth convex term possibly taking inﬁnite values. The target distribution can be seen as a minimizer of the Kullback-Leibler divergence deﬁned on the Wasserstein space (i.e., the space of probability measures). In the ﬁrst part of this paper, we establish a strong duality result for this minimization problem. In the second part of this paper, we use the duality gap arising from the ﬁrst part to study the complexity of the Proximal Stochastic
Gradient Langevin Algorithm (PSGLA), which can be seen as a generalization of the Projected Langevin Algorithm. Our approach relies on viewing PSGLA as a primal dual algorithm and covers many cases where the target distribution is not fully supported. In particular, we show that if the potential is strongly convex, the complexity of PSGLA is O(1/ε2) in terms of the 2-Wasserstein distance. In contrast, the complexity of the Projected Langevin Algorithm is O(1/ε12) in terms of total variation when the potential is convex. 1

Introduction
Sampling from a target distribution is a fundamental task in machine learning. Consider the Euclidean space X = Rd and a convex function V : X → (−∞, +∞]. Assuming that exp(−V ) has a positive
ﬁnite integral w.r.t. the Lebesgue measure Leb, we consider the task of sampling from the distribution
µ(cid:63) whose density is proportional to exp(−V (x)) (we shall write µ(cid:63) ∝ exp(−V )).
If V is smooth, Langevin algorithm produces a sequence of iterates (xk) asymptotically distributed according to a distribution close to µ(cid:63). Langevin algorithm performs iterations of the form xk+1 = xk − γ∇V (xk) + (cid:112)2γW k+1, (1) where γ > 0 and (W k)k is a sequence of i.i.d. standard Gaussian vectors in X. Each iteration of (1) can be seen as a gradient descent step for V , where the gradient of V is perturbed by a Gaussian vector.
Hence, the iterations of Langevin algorithm look like those of the stochastic gradient algorithm;
γ instead of γ. Nonasymptotic bounds however the noise in Langevin algorithm is scaled by for Langevin algorithm have been established in [17, 20]. Moreover, Langevin algorithm can be interpreted as an inexact gradient descent method to minimize the Kullback-Leibler (KL) divergence w.r.t. µ(cid:63) in the space of probability measures [1, 5, 14, 19, 23, 33].
√
In many applications, the function V is naturally written as the sum of a smooth and a nonsmooth term. In Bayesian statistics for example, µ(cid:63) typically represents some posterior distribution. In this case, V is the sum of the log-likelihood (which is itself a sum over the data points) and the possibly nonsmooth potential of the prior distribution [19, 21, 32], which plays the role of a regularizer. In some other applications in Bayesian learning, the support of µ(cid:63) is not the whole space X [9, 10] (i.e.,
V can take the value +∞). In order to cover these applications, we consider the case where V is 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
written as
V (x) := Eξ(f (x, ξ)) + G(x), (2) where ξ is a random variable, f (·, s) : X → R for every s ∈ Ξ, F (x) = Eξ(f (x, ξ)) is smooth and convex and G : X → (−∞, +∞] is nonsmooth and convex. We assume to have access to the stochastic gradient ∇xf (x, ξ) (where ξ is a random variable with values in Ξ) and to the proximity operator proxγG of G. The template (2) covers many log concave densities [10, 13, 19, 21]. In optimization, the minimization of V can be efﬁciently tackled by the proximal stochastic gradient algorithm [3]. Inspired by this optimization algorithm, the Proximal Stochastic Gradient Langevin
Algorithm (PSGLA) [19] is the method performing proximal stochastic gradient Langevin steps of the form xk+1 = proxγG (cid:16) xk − γ∇xf (xk, ξk+1) + (cid:112)2γW k+1(cid:17)
, (3) where γ > 0, (W k) is a sequence of i.i.d. standard Gaussian random vectors in X, and (ξk) is a sequence of i.i.d. copies of ξ. Remarkably, the iterates xk of PSGLA remain in the domain of G, i.e., the support of µ(cid:63), a property that is useful in many contexts. When G is Lipschitz continuous, the support of µ(cid:63) is X and PSGLA can be interpreted as an inexact proximal gradient descent method for minimizing KL, with convergence rates proven in terms of the KL divergence [19]. However, for general G, the KL divergence can take inﬁnite values along PSGLA. Therefore, a new approach is needed. 1.1