Abstract
Recently, there has been a surge of interest in representation learning in hyperbolic spaces, driven by their ability to represent hierarchical data with signiﬁcantly fewer dimensions than standard Euclidean spaces. However, the viability and beneﬁts of hyperbolic spaces for downstream machine learning tasks have received less attention. In this paper, we present, to our knowledge, the ﬁrst theoretical guarantees for learning a classiﬁer in hyperbolic rather than Euclidean space. Speciﬁcally, we consider the problem of learning a large-margin classiﬁer for data possessing a hierarchical structure. Our ﬁrst contribution is a hyperbolic perceptron algorithm, which provably converges to a separating hyperplane. We then provide an algorithm to efﬁciently learn a large-margin hyperplane, relying on the careful injection of adversarial examples. Finally, we prove that for hierarchical data that embeds well into hyperbolic space, the low embedding dimension ensures superior guarantees when learning the classiﬁer directly in hyperbolic space. 1

Introduction
Hyperbolic spaces have received sustained interest in recent years, owing to their ability to compactly represent data possessing hierarchical structure (e.g., trees and graphs). In terms of representation learning, hyperbolic spaces offer a provable advantage over Euclidean spaces for such data: objects requiring an exponential number of dimensions in Euclidean space can be represented in a polynomial number of dimensions in hyperbolic space [28]. This has motivated research into efﬁciently learning a suitable hyperbolic embedding for large-scale datasets [22, 4, 31].
Despite this impressive representation power, little is known about the beneﬁts of hyperbolic spaces for downstream tasks. For example, suppose we wish to perform classiﬁcation on data that is intrinsically hierarchical. One may naïvely ignore this structure, and use a standard Euclidean embedding and corresponding classiﬁer (e.g., SVM). However, can we design classiﬁcation algorithms that exploit the structure of hyperbolic space, and offer provable beneﬁts in terms of performance? This fundamental question has received surprisingly limited attention. While some prior work has proposed speciﬁc algorithms for learning classiﬁers in hyperbolic space [6, 21], these have been primarily empirical in nature, and do not come equipped with theoretical guarantees on convergence and generalization.
In this paper, we take a ﬁrst step towards addressing this question for the problem of learning a large-margin classiﬁer. We provide a series of algorithms to provably learn such classiﬁers in hyperbolic space, and establish their superiority over the classiﬁers naïvely learned in Euclidean space. This shows that by using a hyperbolic space that better reﬂects the intrinsic geometry of the data, one can see gains in both representation size and performance. Speciﬁcally, our contributions are:
∗Work performed while intern at Google Research, New York. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(i) we provide a hyperbolic version of the classic perceptron algorithm and establish its convergence for data that is separable with a margin (Theorem 3.1). (ii) we establish how suitable injection of adversarial examples to gradient-based loss minimization yields an algorithm which can efﬁciently learn a large-margin classiﬁer (Theorem 4.3). We further establish that simply performing gradient descent or using adversarial examples alone does not sufﬁce to efﬁciently yield such a classiﬁer. (iii) we compare the Euclidean and hyperbolic approaches for hierarchical data and analyze the trade-off between low embedding dimensions and low distortion (dimension-distortion trade-off ) when learning robust classiﬁers on embedded data. For hierarchical data that embeds well into hyperbolic space, it sufﬁces to use smaller embedding dimension while ensuring superior guarantees when we learn a classiﬁer in hyperbolic space.
Contribution (i) establishes that it is possible to design classiﬁcation algorithms that exploit the structure of hyperbolic space, while provably converging to some admissible (not necessarily large-margin) separator. Contribution (ii) establishes that it is further possible to design classiﬁcation algorithms that provably converge to a large-margin separator, by suitably injecting adversarial examples. Contribution (iii) shows that the adaptation of algorithms to the intrinsic geometry of the data can enable efﬁcient utilization of the embedding space without affecting the performance.