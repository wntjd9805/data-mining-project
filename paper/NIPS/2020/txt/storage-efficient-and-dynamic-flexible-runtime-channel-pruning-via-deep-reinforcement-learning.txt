Abstract
In this paper, we propose a deep reinforcement learning (DRL) based framework to efﬁciently perform runtime channel pruning on convolutional neural networks (CNNs). Our DRL-based framework aims to learn a pruning strategy to deter-mine how many and which channels to be pruned in each convolutional layer, depending on each individual input instance at runtime. Unlike existing runtime pruning methods which require to store all channels parameters for inference, our framework can reduce parameters storage consumption by introducing a static pruning component. Comparison experimental results with existing runtime and static pruning methods on state-of-the-art CNNs demonstrate that our proposed framework is able to provide a tradeoff between dynamic ﬂexibility and storage efﬁciency in runtime channel pruning. 1

Introduction
In recent years, convolutional neural networks (CNNs) have demonstrated remarkable performance in various computer vision tasks [17, 30, 8, 7, 36, 18, 7, 37, 4]. However, since most state-of-the-art
CNNs require expensive computation power for inference and huge storage space to store large amount of parameters, the limitation of energy, computation and storage on mobile or edge devices has become the major bottleneck on real-world deployments of CNNs. Existing studies have been focused on speeding up the execution of CNNs for inference on edge devices by model compression using matrix decomposition [3, 25], network quantization [2], network pruning [5], etc. Among these approaches, channel pruning, which discards an entire input or output channel and keeps the rest of the model with structures, has shown promising performance [11, 24, 38, 26].
Most channel pruning approaches can be categorized into two types: runtime and static. Static approaches aim to evaluate the importance of each channel over the whole training dataset and remove the least important channels to minimize the loss of performance after pruning. By permanently pruning a number of channels, the computation and storage cost of a CNN can be dramatically reduced when being deployed, and the inference execution can be accelerated consequently. On the other hand, runtime approaches have been recently proposed to achieve dynamic channel pruning on each individual instance [6]. To be speciﬁc, the goal of runtime approaches aims to evaluate the channel importance at runtime, which is assumed to be different on different input instances. By pruning channels dynamically, different pruned structures can be considered as different routing of data stream inside CNNs. This kind of approaches is able to signiﬁcantly improve the representation capability of a CNN, and thus achieve better performance in terms of prediction accuracy compared with static approaches. However, previous runtime approaches trade storage cost off dynamic ﬂexibility. To 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
achieve dynamic pruning on different individual instances, all parameters of kernels are required to be stored (or even more parameters are introduced). This makes runtime approaches not applicable on resource-limited edge devices. Moreover, most of previous runtime approaches only evaluate the importance among channels in each single layer independently, without considering the difference in efﬁciency among layers.
In this paper, to address the aforementioned issues of runtime channel pruning approaches, we propose a deep reinforcement learning (DRL) based pruning framework. Basically, we aim to apply
DRL to prune CNNs by maximizing received rewards, which are designed to satisfy the overall budget constraints along side with the network’s training accuracy. Note that automatic channel pruning by DRL is a challenging task because the action space is usually very huge. Speciﬁcally, the discrete action space for the DRL agent is as large as the number of channels at each layer, and the action space may vary among layers since there are different numbers of channels in different layers.
To facilitate pruning CNNs by DRL, for each layer, we ﬁrst design a novel prediction component to estimate the importance of channels, and then develop a DRL-based component to learn the sparsity ratio of the layer, i.e., how many channels should be pruned.
Different from previous runtime channel pruning approaches, which only learn runtime importance of each channel, we propose to learn both runtime importance and additionally static importance for each channel. While runtime importance maintains the saliency of speciﬁc channels for each individual input, the static importance captures the overall saliency of the corresponding channel among the whole dataset. According to each type of channel importance, we further design different
DRL agents (i.e., a runtime agent and a static agent) to learn a sparsity ratio in a layer-wise manner.
The sparsity ratio learned by the runtime agent together with the estimated runtime importance of channels is used to generate runtime pruning structures, while the sparsity ratio learned by the static agent together with the estimated static importance of channels is used to generate static (permanent) pruning structures. By considering both the pruning structures, our framework is able to provide a trade-off between storage efﬁciency and dynamic ﬂexibility for runtime channel pruning.
In summary, our contributions are 2-fold. First, we propose to prune channels by taking both runtime and static information of the environment into consideration. Runtime information endows pruning with ﬂexibility based on different input instances while static information reduces the number of parameters in deployment, leading to storage reduction, which cannot be achieved by conventional runtime pruning approaches. Second, we propose to use DRL to determine sparsity ratios, which is different from the previous pruning approaches that manually set sparsity ratios. The codes of our method can be found at https://github.com/jianda-chen/static_dynamic_rl_pruning . 2