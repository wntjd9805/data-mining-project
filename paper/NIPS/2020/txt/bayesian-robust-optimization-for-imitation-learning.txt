Abstract
One of the main challenges in imitation learning is determining what action an agent should take when outside the state distribution of the demonstrations. Inverse reinforcement learning (IRL) can enable generalization to new states by learning a parameterized reward function, but these approaches still face uncertainty over the true reward function and corresponding optimal policy. Existing safe imitation learning approaches based on IRL deal with this uncertainty using a maxmin framework that optimizes a policy under the assumption of an adversarial reward function, whereas risk-neutral IRL approaches either optimize a policy for the mean or MAP reward function. While completely ignoring risk can lead to overly aggressive and unsafe policies, optimizing in a fully adversarial sense is also problematic as it can lead to overly conservative policies that perform poorly in practice. To provide a bridge between these two extremes, we propose Bayesian
Robust Optimization for Imitation Learning (BROIL). BROIL leverages Bayesian reward function inference and a user speciﬁc risk tolerance to efﬁciently optimize a robust policy that balances expected return and conditional value at risk. Our empirical results show that BROIL provides a natural way to interpolate between return-maximizing and risk-minimizing behaviors and outperforms existing risk-sensitive and risk-neutral inverse reinforcement learning algorithms. 1

Introduction
Imitation learning [40] aims to train an agent without hand-specifying a reward function by providing demonstrations. One of the main challenges in imitation learning is determining what action an agent should take when outside the states contained in the demonstrations. Inverse reinforcement learning (IRL) [38] is an approach to imitation learning in which the learning agent seeks to recover the reward function of the demonstrator. Learning a parameterized reward function provides a compact representation of the demonstrator’s preferences and enables generalization to new states unseen in the demonstrations via policy optimization. However, IRL approaches still result in uncertainty over the true reward function and this uncertainty can have negative consequences if the learning agent infers a reward function that leads it to learn an incorrect policy. In this paper we propose that an imitation learning agent should learn a policy that is robust with respect to its uncertainty over the true objective of a task, but also be able to effectively trade-off epistemic risk with expected return.
For example, consider two scenarios: (1) an autonomous car detects a novel object lying in the road ahead of the car and (2) a domestic service robot tasked with vacuuming encounters a pattern on the ﬂoor it has never seen before. The ﬁrst example concerns autonomous driving where the car’s decisions have potentially catastrophic consequences. Thus, the car should treat the novel object as a hazard and either slow down or safely change lanes to avoid running into it. In the second example, vacuuming the ﬂoors of a house has certain risks, but the consequences of optimizing the wrong reward function are arguably much less signiﬁcant. Thus, when the vacuuming robot encounters a novel ﬂoor pattern it does not need to worry as much about negative side-effects.
∗Work done while at UT Austin. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Risk-averse optimization, especially in ﬁnancial domains, has a long history of seeking to address the trade-off between risk and return using measures of risk such as variance [37], value at risk [30] and conditional value at risk [48]. This work has been extended to risk-averse optimization in Markov decision processes [15, 43, 44] and in the context of reinforcement learning [23, 58, 59], where the transition dynamics and reward function are not known. However, there has only been limited work in applying techniques for trading off risk and return in the domain of imitation learning. Brown et al. [11] seek to bound the value at risk of a policy in the imitation learning setting; however, directly optimizing a policy for value at risk is NP-hard [16]. Lacotte et al. [33] and Majumadar et al. [35] assume that risk-sensitive trajectories are available from a safe demonstrator and seek to optimize a policy that matches the risk-proﬁle of this expert. In contrast, our approach directly optimizes a policy that balances expected return and conditional value at risk [48] which can be done via convex optimization. Furthermore, we do not try to match the demonstrator’s risk sensitivity, but instead ﬁnd a robust policy with respect to uncertainty over the demonstrator’s reward function, allowing us to optimize policies that are potentially safer than the demonstrations.
One of the concerns of imitation learning, and especially inverse reinforcement learning, is the possibility of learning an incorrect reward function that leads to negative side-effects, for example, a vacuuming robot that learns that it is good to vacuum up dirt, but then goes around making messes for itself to clean up [50]. To address negative side-effects, most prior work on safe inverse reinforcement learning takes a minmax approach and seeks to optimize a policy with respect to the worst-case reward function [25, 28, 57]; however, treating the world as if it is completely adversarial (e.g., completely avoiding a novel patch of red ﬂooring because it could potentially be lava [25]) can lead to overly conservative behaviors. On the other hand, other work on inverse reinforcement learning and imitation learning takes a risk neutral approach and simply seeks to perform well in expectation with respect to uncertainty over the demonstrator’s reward function [46, 65]. This can result in behaviors that are overly optimistic in the face of uncertainty and can lead to policies with high variance in performance which is undesirable in high-risk domains like medicine or autonomous driving. Instead of assuming either a purely adversarial environment or a risk-neutral one, we propose the ﬁrst inverse reinforcement learning algorithm capable of appropriately balancing caution with expected performance in a way that reﬂects the risk-sensitivity of the particular application.
The main contributions of this work are: (1) We propose Bayesian Robust Optimization for Imitation
Learning (BROIL), the ﬁrst imitation learning framework to directly optimize a policy that balances the expected return and the conditional value at risk under an uncertain reward function; (2) We derive an efﬁcient linear programming formulation to compute the BROIL optimal policy; (3)
We propose and compare two instantiations of BROIL: optimizing a purely robust policy with respect to uncertainty and optimizing a policy that minimizes baseline regret with respect to expert demonstrations; and (4) We demonstrate that BROIL achieves better expected return and robustness than existing risk-sensitive and risk-neutral IRL algorithms, as well as providing a richer class of solutions that correctly balance performance and risk based on different levels of risk aversion. 2