Abstract
Learning representations of sets of nodes in a graph is crucial for applications rang-ing from node-role discovery to link prediction and molecule classiﬁcation. Graph
Neural Networks (GNNs) have achieved great success in graph representation learn-ing. However, expressive power of GNNs is limited by the 1-Weisfeiler-Lehman (WL) test and thus GNNs generate identical representations for graph substructures that may in fact be very different. More powerful GNNs, proposed recently by mimicking higher-order-WL tests, only focus on representing entire graphs and they are computationally inefﬁcient as they cannot utilize sparsity of the underlying graph. Here we propose and mathematically analyze a general class of structure-related features, termed Distance Encoding (DE). DE assists GNNs in representing any set of nodes, while providing strictly more expressive power than the 1-WL test. DE captures the distance between the node set whose representation is to be learned and each node in the graph. To capture the distance DE can apply various graph-distance measures such as shortest path distance or generalized PageRank scores. We propose two ways for GNNs to use DEs (1) as extra node features, and (2) as controllers of message aggregation in GNNs. Both approaches can utilize the sparse structure of the underlying graph, which leads to computational efﬁciency and scalability. We also prove that DE can distinguish node sets embedded in almost all regular graphs where traditional GNNs always fail. We evaluate DE on three tasks over six real networks: structural role prediction, link prediction, and triangle prediction. Results show that our models outperform GNNs without DE by up-to 15% in accuracy and AUROC. Furthermore, our models also signiﬁcantly outperform other state-of-the-art methods especially designed for the above tasks. 1

Introduction
Graph representation learning aims to learn representation vectors of graph-structured data [1].
Representations of node sets in a graph can be leveraged for a wide range of applications, such as discovery of functions/roles of nodes based on individual node representations [2–6], link or link type prediction based on node-pair representations [7–10] and graph comparison or molecule classiﬁcation based on entire-graph representations [11–17].
Graph neural networks (GNNs), inheriting the power of neural networks [18], have become the de facto standard for representation learning in graphs [19]. Generaly, GNNs use message pass-ing procedure over the input graph, which can be summarized in three steps: (1) Initialize node representations with their initial attributes (if given) or structural features such as node degrees; 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a)
WLGNN-p to represent T = (S, A), |S| = p
Initialize: For all v ∈ V , h(0)
For layers l = 0, 1, ..., L − 1 and all v ∈ V , do: v = Avv (b) h(l+1) v
= f1(h(l)
Output Γ(T ) = AGG({h(L) v , AGG({f2(h(l) v }v∈S) u , Avu)}u∈Nv ))
Figure 1: (a) 3-regular graph with 8 nodes. Brieﬂy assume that all node attributes are the same and the nodes can only be distinguished based on their network structure. Then for all nodes, WLGNN will produce the same representation and thus fail to distinguish them. However, nodes with different colors should have different representations, as they are not structurally equivalent (or “isomorphic” as deﬁned in Section 2). Furthermore,
WLGNN cannot distinguish all the node-pairs (e.g., {v1, v2} vs {v4, v7}). However, if we use shortest-path-distances (SPDs) between nodes as features we can distinguish blue nodes from green and red nodes because there is another node with SPD= 3 to a blue node of interest (e.g., SPD between v3 and v8), while all SPDs between other nodes to red/green nodes are less than 3. Note that the structural equivalence between any two nodes of the same color can be obtained from the reﬂexivity of the graph while the equivalence between two vertically-aligned blue nodes can be further obtained from the node permutation shown in the right. (b) WLGNN algorithm to represent a node set S of size p — fi(·)’s are arbitrary neural networks; AGG(·)’s are set-pooling operators; L is the number of layers. (2) Iteratively update the representation of each node by aggregating over the representations of its neighboring nodes; (3) Readout the ﬁnal representation of a single node, a set of nodes, or the entire node set as required by the task. Under the above framework, researchers have proposed many
GNN architectures [14–16, 20–23]. Interested readers may refer to tutorials on GNNs for further details [1, 24].
Despite the success of GNNs, their representation power in representation learning is limited [16].
Recent works proved that the representation power of GNNs that follow the above framework is bounded by the 1-WL test [16, 25, 26] (We shall refer to these GNNs as WLGNNs). Concretely,
WLGNNs yield identical vector representations for any subgraph structure that the 1-WL test cannot distinguish. Consider an extreme case: If node attributes are all nodes are the same, then for any node in a r-regular graph GNN will output identical representation. Such an issue becomes even worse when WLGNNs are used to extract representations of node sets, e.g., node-pairs for link prediction (Fig. 1 (a)). A few works have been recently proposed to improve the power of WLGNNs [27].
However, they either focus on building theory only for entire-graph representations [26–30], or show empirical success using heuristic methods without strong theoretical characterization [9,10,17,31–33].
We review these methods in detail in Section 4.
Here we address the limitations of WLGNNs and propose and mathematically analyze a new class of node features, termed Distance Encoding (DE). DE comes with both theoretical guarantees and empirical efﬁciency. Given a node set S whose structural representation is to be learnt, for every node u in the graph DE is deﬁned as a mapping of a set of landing probabilities of random walks from each node of the set S to node u. DE may use measures such as shortest path distance (SPD) and generalized PageRank scores [34]. DE can be combined with any GNN architecture in simple but effective ways: First, we propose DE-GNN that utilizes DE as an extra node feature. We further enhance DE-GNN by allowing DE to control the message aggregation procedure of WLGNNs, which yields another model DEA-GNN. Since DE purely depends on the graph structure and is independent of node identiﬁers, DE also provides inductive and generalization capability.
We mathematically analyze the expressive power of DE-GNN and DEA-GNN for structural rep-resentation learning. We prove that the two models are able to distinguish two non-isomorphic equally-sized node sets (including nodes, node-pairs, . . . , entire-graphs) that are embedded in almost all sparse regular graphs, where WLGNN always fails to distinguish them unless discriminatory node/edge attributes are available. We also prove that the two models are not more powerful than
WLGNN when applied to distance regular graphs [35], which implies the limitation of DEs. However, we show that DE has an extra power to learn the structural representations of node-pairs over distance regular graphs [35].
We experimentally evaluate DE-GNN and DEA-GNN on three levels of tasks including node-structural-role classiﬁcation (node-level), link prediction (node-pair-level), triangle prediction (node-triad-level). Our methods outperform WLGNN on all three tasks by up-to 15% improvement in average accuracy. Our methods also outperform other baselines speciﬁcally designed for these tasks. 2
2 Preliminaries
In this section we formally deﬁne the notion of structural representation and review how WLGNN learns structural representation and its relation to the 1-WL test. 2.1 Graph Representation Learning
Deﬁnition 2.1. We consider an undirected graph which can be represented as G = (V, E, A), where
V = [n] is the node set, E ⊆ V × V is the edge set, and A contains all features in the space
A ⊂ Rn×n×k. Its diagonal component, Avv·, denotes the node attributes of node v(∈ V ), while its off-diagonal component in Avu· denotes the node-pair attributes of (v, u). We set Avu· as all zeros if (v, u) (cid:54)∈ E. In practice, graphs are usually sparse, i.e., |E| (cid:28) n2. We introduce A ∈ {0, 1}n×n to denote the adjacency matrix of G such that Auv = 1 iff (u, v) ∈ E. Note that A can be also viewed as one slice of the feature tensor A. If no node/edge attributes are available, we let A = A.
Deﬁnition 2.2. The node permutation denoted by π is a bijective mapping from V to V . All possible π’s are collected in the permutation group Πn. We denote π acting on a subset S of V as
π(S) = {π(i)|i ∈ S}. We further deﬁne π(A)uv· = Aπ−1(u)π−1(v)· for any (u, v) ∈ V × V .
Deﬁnition 2.3. Denote all p-sized subsets S of V as S ∈ Pp(V ) and deﬁne the space Ωp =
Pp(V ) × A. For two tuples T1 = (S(1), A(1)) and T2 = (S(2), A(2)) in Ωp, we call that that they are isomorphic (otherwise non-isomorphic), if ∃π ∈ Πn such that S(1) = π(S(2)) and A(1) = π(A(2)).
Deﬁnition 2.4. A function f deﬁned on Ωp is invariant if ∀π ∈ Πn, f (S, A) = f (π(S), π(A)).
Deﬁnition 2.5. The structural representation of a tuple (S, A) is an invariant function Γ(·) : Ωp →
Rd where d is the dimension of representation. Therefore, if two tuples are isomorphic, they should have the same structural representation.
The invariant property is critical for the inductive and generalization capability as it frees structural representations from node identiﬁers and effectively reduces the problem dimension by incorporating the symmetry of the parameter space [29] (e.g., the convolutional layers in GCN [20]). The invariant property also implies that structural representations do not allow encoding the absolute positions of S in the graph.
The deﬁnition of structural representation is very general. Suppose we set two node sets S(1), S(2) as two single nodes and set two graph structures A(1) and A(2) as the ego-networks around these two nodes. Then, the deﬁnition of structural representation provides a mathematical characterization the concept “structural roles” of nodes [3, 5, 6], where two far-away nodes could have the same structural roles (representations) as long as their ego-networks have the same structure.
Note that depending on the application one can vary/select the size p of the node set S. For example, when p = 1 then we are in the regime of node classiﬁcation, p = 2 is link prediction, and when
S = V , structural representations reduce to entire graph representations. However, in this work we will primarily focus on the case that the node set S has a ﬁxed and small size p, where p does not depend on the graph size n. Although Corollary 3.4 later shows the potential of our techniques on learning the entire graph representations, this is not the main focus of our work here. We expect the techniques proposed here can be further used for entire-graph representations while we leave the detailed investigation for future work.
Although structural representation deﬁnes a more general concept, it shares some properties with traditional entire-graph representation. For example, the universal approximation theorem regarding entire-graph representation [30] can be directly generalized to the case of structural representations:
Theorem 2.6. If structural representations Γ are different over any two non-isomorphic tuples T1 and T2 in Ωp, then for any invariant function f : Ωp → R, f can be universally approximated by feeding Γ into a 3-layer feed-forward neural network with ReLu as the activation function, as long as (1) the feature space A is compact and (2) f (S, ·) is continuous over A for any S ∈ Pp(V ).
Theorem 2.6 formally establishes the relation between learning structural representations and distin-guishing non-isomorphic structures, i.e., Γ(T1) (cid:54)= Γ(T2) iff T1 and T2 are non-isomorphic. However, no polynomial algorithm has been found to distinguish even just non-isomorphic entire graphs (S = V ) without node/edge attributes (A = A), which is known as the graph isomorphism prob-lem [36]. In this work, we will use the range of non-isomorphic structures that GNNs can distinguish to characterize their expressive power for graph representation learning. 3
2.2 Weisfeiler-Lehman Tests and WLGNN for Structural Representation Learning
Weisfeiler-Lehman test (WL-test) is a family of very successful algorithmic heuristics used in graph isomorphism problems [25]. 1-WL test, the simplest one among this family, starts with coloring nodes with their degrees, then it iteratively aggregates the colors of nodes and their neighborhoods, and hashes the aggregated colors into unique new colors. The coloring procedure ﬁnally converges to some static node-color conﬁguration. Here a node-color conﬁguration is a multiset that records the types of colors and their numbers. Different node-color conﬁgurations indicate two graphs are non-isomorphic while the reverse statement is not always true.
More than the graph isomorphism problem, the node colors obtained by the 1-WL test naturally pro-vide a test of structural isomorphism. Consider two tuples T1 = (S(1), A(1)) and T2 = (S(2), A(2)) according to Deﬁnition 2.3. We temporarily ignore node/edge attributes for simplicity, so A(1), A(2) reduce to adjacent matrices. It is easy to show that different node-color conﬁgurations of nodes in
S(1) and in S(2) obtained by the 1-WL test also indicate that T1 and T2 are not isomorphic.
WLGNNs refer to those GNNs that mimic the 1-WL test to learn structural representation, which is summarized in Fig. 1 (b). It covers many well-known GNNs of which difference may appear in the implementation of neural networks fi and set-poolings AGG(·) (Fig. 1 (b)), including GCN [20],
GraphSAGE [21], GAT [22], MPNN [14], GIN [16] and many others [29]. Note that we use
WLGNN-p to denote the WLGNN that is to learn structural representations of node sets S with size
|S| = p. One may directly choose S = V to obtain the entire-graph representation. Theoretically, the structural representation power of WLGNN-p is provably bounded by the 1-WL test [16]. The result can be also generalized to the case of structural representations as follows.
Theorem 2.7. Consider two tuples T1 = (S(1), A(1)) and T2 = (S(2), A(2)) in Ωp. If T1, T2 cannot be distinguished by the 1-WL test, then the corresponding outputs of WLGNN-p satisfy
Γ(T1) = Γ(T2). On the other side, if they can be distinguished by the 1-WL test and we suppose aggregation operations (AGG) and neural networks f1, f2 are all injective mappings, then with a large enough number of layers L, the outputs of WLGNN-p also satisfy Γ(T1) (cid:54)= Γ(T2).
Because of Theorem 2.7, WLGNN inherits the limitation of the 1-WL test. For example, WLGNN cannot distinguish two equal-sized node sets in all r-regular graphs (unless node/edge features are discriminatory). Here, a r-regular graph means that all its nodes have degree r. Therefore, researchers have recently focused on designing GNNs with expressive power greater than the 1-WL test. Here we will improve the power of GNNs by developing a general class of structural features. 3 Distance Encoding and Its Power 3.1 Distance Encoding
Suppose we aim to learn the structural representation of the target node set S. Intuitively, our proposed DE will then encode the distance from S to any other node u. We deﬁne DE as follows:
Deﬁnition 3.1. Given a target set of nodes S ∈ 2V \∅ of G with the adjacency matrix A, we denote distance encoding as a function ζ(·|S, A) : V → Rk. ζ should also be permutation invariant, i.e.,
ζ(u|S, A) = ζ(π(u)|π(S), π(A)) for all u ∈ V and π ∈ Πn. Then we denote DEs w.r.t. the size of
S and call them as DE-p if |S| = p.
Later we use ζ(u|S) for brevity where A could be inferred from the context. For simplicity, we choose DE as a set aggregation (e.g., the sum-pooling) of DEs between nodes u, v where v ∈ S:
ζ(u|S) = AGG({ζ(u|v)|v ∈ S}) (1)
More complicated DE may be used while this simple design can be efﬁciently implemented and achieves good empirical performance. Then, the problem reduces to choosing a proper ζ(u|v). Again for simplicity, we consider the following class of functions that is based on the mapping of a list of landing probabilities of random walks from v to u over the graph, i.e.,
ζ(u|v) = f3((cid:96)uv), (cid:96)uv = ((W )uv, (W 2)uv, ..., (W k)uv, ...) (2) where W = AD−1 is the random walk matrix, f3 may be simply designed by some heuristics or be parameterized and learnt as a feed-forward neural network. In practice, a ﬁnite length of (cid:96)vu, say 3,4, is enough. Note that Eq. (2) covers many important distance measures. First, setting f3((cid:96)uv) as 4
the ﬁrst non-zero position in (cid:96)uv gives the shortest-path-distance (SPD) from v to u. We denote this speciﬁc choice as ζspd(u|v). Second, one may also use generalized PageRank scores [34]:
ζgpr(u|v) = (cid:88) k≥1
γk(W k)uv = ( (cid:88)
γkW k)uv,
γk ∈ R, for all k ∈ N . (3) k≥1
Note that the permutation invariant property of DE is beneﬁcial for inductive learning, which fundamentally differs from positional node embeddings such as node2vec [37] or one-hot node identiﬁers. In the rest of this work, we will show that DE improves the expressive power of GNNs in both theory and practice. In Section 3.2, we use DE as extra node features. We term this model as DE-GNN, and theoretically demonstrate its expressive power. In the next subsection, we further use DE-1 to control the aggregation procedure of WLGNN. We term this model as DEA-GNN and extend our theory there. 3.2 DE-GNN— Distance Encodings as Node Features
DE can be used as extra node features. Speciﬁcally, we improve WLGNNs by setting h(0) v =
Avv ⊕ ζ(v|S) where ⊕ is the concatenation. We call the obtained model DE-GNN. We similarly use
DE-GNN-p to specify the case when |S| = p. For simplicity, we give the following deﬁnition.
Deﬁnition 3.2. DE-GNN is called proper if f1, f2, AGGs in the WLGNN (Fig. 1 (b)), and AGG in
Eq. (1), f3 in Eq. (2) are injective mappings as long as the input features are all countable.
We know that a proper DE-GNN exists because of the universal approximation theorem of feed-forward networks (to construct fi, i ∈ {1, 2, 3}) and Deep Sets [38] (to construct AGGs). 3.2.1 The Expressive Power of DE-GNN
Next, we demonstrate the power of DE-GNN to distinguish structural representations. Recall that the fundamental limit of WLGNN is the 1-WL test for structural representation (Theorem 2.7). One important class of graphs that cannot be distinguished by the 1-WL test are regular graphs (although, in practice, node/edge attributes may help diminish such difﬁculty by breaking the symmetry). In theory, we may consider the most difﬁcult case by assuming that no node/edge attributes are available.
In the following, our main theorem shows that even in the most difﬁcult case, DE-GNN is able to distinguish two equal-sized node sets that are embedded in almost all r-regular graphs. One example where DE-GNN using ζspd(·) (SPD) is shown in Fig. 1 (a): The blue nodes can be easily distinguished from the green or red nodes as SPD= 3 may appear between two nodes when a blue node is the node set of interest, while all SPDs from other nodes to red and green nodes are less than 3. Actually, DE-GNN-1 with SPD may also distinguish the red or green nodes by investigating its procedure in details (Fig. 3 in Appendix).
Theorem 3.3. Given two equal-sized sets S(1), S(2) ⊂ V , |S(1)| = |S(2)| = p. Consider two tuples T (1) = (S(1), A(1)) and T (2) = (S(2), A(2)) in the most difﬁcult setting where features A(1) and A(2) are only different in graph structures speciﬁed by A(1) and A(2) respectively. Suppose
A(1) and A(2) are uniformly independently sampled from all r-regular graphs over V where 3 ≤ r < (2 log n)1/2. Then, for any small constant (cid:15) > 0, within L ≤ (cid:100)( 1 log(r−1) (cid:101) layers, there exists a proper DE-GNN-p using DEs ζ(u|S(1)), ζ(u|S(2)) for all u ∈ V , such that with probability 1 − o(n−1), the outputs Γ(T (1)) (cid:54)= Γ(T (2)). Speciﬁcally, f3 can be simply chosen as SPD, i.e.,
ζ(u|v) = ζspd(u|v). The big-O notations here and later are w.r.t. n.
Remark 3.1. In some cases, we are to learn representations of structures that lie in a single large graph, i.e., A(1) = A(2). Actually, there is no additional difﬁculty to extend the proof of Theorem 3.3 to this setting as long as A(1)(= A(2)) is uniformly sampled from all r-regular graphs and S(1) ∩ S(2) = ∅.
The underlying intuition is that for large n, the local subgraphs (within L-hop neighbors) around two non-overlapping ﬁxed sets S(1), S(2) are almost independent. Simulation results to validate the single node case (p = 1) of Theorem 3.3 and Remark 3.1 are shown in Fig. 2 (a). 2 + (cid:15)) log n
Actually, the power of structural representations of small node sets can be used to further characterize the power of entire graph representations. Consider that we directly aggregate all the representations of nodes of a graph output by DE-GNN-1 via set-pooling as the graph representation, which is a common strategy adopted to learn graph representation via WLGNN-1 [14, 16, 23]. So how about the power of
DE-GNN-1 to represent graphs? To answer this question, suppose two n-sized r-regular graphs A(1) 5
(a) (b) u − h(L)
Figure 2: (a) Simulation to validate Theorem 3.3. We uniformly at random generate 104/n 3-regular graphs and compare node representations output by a randomly initialized but untrained DE-GNN-1 with L layers,
L ≤ 6. All the nodes in these graphs are considered and thus for each n, there are 104 nodes from the same or different graphs. For any two nodes u, v, if (cid:107)h(L) v (cid:107)2 is greater than machine accuracy, they are regarded to be distinguishable. The colors of the scatter plot indicate the portion of two nodes that are not distinguishable by DE-GNN-1. The red line is boundary predicted by our theory, which well matches the simulation. (b) The power of DE-2. The left is the Shrikhande graph while the right is the 4 × 4 Rook’s graph. DE-GNN-1 assigns all nodes with the same representation. DE-GNN-2 may distinguish the structures by learning representations of node-pairs (edges)—the node-pairs colored by black. Each node is colored with its DE-2 that is a set of
SPDs to either node in the target edges (Eq. (1)). Note the neighbors of nodes with DE-2= {1, 1} (covered by dashed boxes) that are highlighted by red ellipses. As these neighbors have different DE-2’s, after one layer of DE-GNN-2, the intermediate representations of nodes with DE-2= {1, 1} are different between these two graphs. Using another layer, DE-GNN-2 can distinguish the representations of two target edges. and A(2) satisfy the condition in Theorem 3.3. Then, by using a union bound, Theorem 3.3 indicates that for a node v ∈ V , its representation Γ((v, A(1))) (cid:54)∈ {Γ((u, A(2)))|u ∈ V } with probability 1 − no(n−1) = 1 − o(1). Therefore, these two graphs A(1) and A(2) can be distinguished via
DE-GNN-1 with high probability. We formally state this result in the following corollary.
Corollary 3.4. Suppose two graphs are uniformly independently sampled from all n-sized r-regular graphs over V where 3 ≤ r < (2 log n)1/2. Then, within L ≤ (cid:100)( 1 log(r−1) (cid:101) layers, DE-GNN-1 can distinguish these two graphs with probability 1 − o(1) by being concatenated with injective set-pooling over all the representations of nodes. 2 + (cid:15)) log n
One insightful observation is that the structural representation with a small set S may become easier to be learnt than the entire graph representation as the knowledge of the node set S can be viewed as a piece of side information. Models can be built to leverage such information, while when to compute the entire graph representation (S = V ), the model loses such side information. This could be a reason why the successful probability to learn structural representation of a small node set (Theorem 3.3) is higher than to that of an entire graph in (Corollary 3.4), though our derivation of the probabilistic bounds is not tight. Note that DE provides a convenient way to effectively capture such side information. One naive way to leverage the information of S is to simply annotate the nodes in S with binary encoding 1 and those out of S with 0. This is obviously a special case of DE but it is not as powerful as the general DE (even compared to the special case SPD). Think about setting the target node set in Eq.(1) as the entire graph (S = V ), where annotating the nodes in/out of S does not improve the representation power of WLGNN because all nodes are annotated as 1.
However, DE still holds extra representation power: For example, we want to distinguish a graph with two disconnected 3-circles and a graph with a 6-circle. These two graphs generate different
SPDs between nodes. 3.2.2 The Limitation of DE-GNN
Next, we show the limitation of DE-GNN. We prove that over a subclass of regular graphs, distance regular graphs (DRG), DE-1 is useless for structural representation. We provide the deﬁnition of
DRG as follows while we refer interested readers to check more properties of DRGs in [35].
Deﬁnition 3.5. A distance regular graph is a regular graph such that for any two nodes v, u ∈ V , the number of vertices w s.t. SPD(w, v) = i and SPD(w, u) = j, only depends on i, j and SPD(v, u).
The Shrikhande graph and the 4 × 4 Rook’s graph are two non-isomorphic DRGs shown in Fig. 2 (b) (We temporarily ignore the nodes colors which will be discussed later). For simplicity, we only consider connected DRGs that can be characterized by arrays of integers termed intersection arrays. 6
Deﬁnition 3.6. The intersection array of a connected DRG with diameter (cid:52) is an array of integers
{b0, b1, ..., b(cid:52)−1; c1, c2, ..., c(cid:52)} such that for any node pair (u, v) ∈ V ×V that satisﬁes SPD(v, u) = j, bj is the number of nodes w that are neighbors of v and satisfy SPD(w, u) = j + 1, and cj is the number of nodes w that are neighbors of v and satisfy SPD(w, u) = j − 1.
It is not hard to show that the two DRGs in Fig. 2 (b) share the same intersecion array {6, 3; 1, 2}.
The following theorem shows that over distance regular graphs, DE-GNN-1 requires discriminatory node/edge attributes to distinguish structures, which indicates the limitation of DE-1.
Theorem 3.7. Given any two nodes v, u ∈ V , consider two tuples T1 = (v, A(1)) and T2 = (u, A(2)) with graph structures A(1) and A(2) that correspond to two connected DRGs with a same intersection array. Then, DE-GNN-1 must use discriminatory node/edge attributes to distinguish T1 and T2.
Note Theorem 3.7 only works for node representations using DE-1. Therefore, DE-GNN-1 may not associate distinguishable node representations in the two DRGs in Fig. 2 (b).
However, if we are to learn higher-order structural representations (|S| ≥ 2) with DE-p (p ≥ 2),
DE-GNN-p may have even stronger representation power. We illustrate this point by considering structural representations of two node-pairs that form edges of the two DRGs respectively. Consider two node-pairs that correspond to two edges of these two graphs in Fig. 2 (b) respectively. Then, there exists a proper DE-GNN-2 via using SPD as DE-2, associating these two node-pairs with different representations. Moreover, by simply aggregating the obtained representations of all node-pairs into graph representations via a set-pooling, we may also distinguish these two graphs. Note that distinguishing the node-pairs of the two DRGs is really hard, because even the 2-WL test 1 will fail to distinguish any edges in the DRGs with a same intersection array and diameters exactly equal to 2 2.
This means that the recently proposed more powerful GNNs, such as RingGNN [30] and PPGN [27], will also fail in this case. However, it is possible to use DE-GNN-2 to distinguish those two DRGs.
It is interesting to generalize Theorem 3.3 to DRGs to demonstrate the power of DE-GNN-p (p ≥ 2).
However, missing analytic-friendly random models for DRGs makes such generalization challenging. 3.3 DEA-GNN— Distance Encoding-1’s as Controllers of the Message Aggregation
DE-GNN only uses DEs as initial node features. In this subsection, we further consider leveraging
DE-1 between any two nodes to control the aggregation procedure of DE-GNN. Speciﬁcally, we propose DE-Aggregation-GNN (DEA-GNN) to do the following change
AGG({f2(h(l) u , Avu)}u∈Nv ) → AGG({(f2(h(l) u , Avu), ζ(u|v))}u∈V ) (4)
Note that the representation power of DEA-GNN is at least no worse than DE-GNN because the later one is specialized by aggregating the nodes with ζspd(u|v) = 1, so Theorem 3.3, Corollary 3.4 are still true. Interestingly, its power is also limited by Theorem 3.7. We conclude as the follows.
Corollary 3.8. Theorem 3.3, Corollary 3.4 and Theorem 3.7 are still true for DEA-GNN.
The general form Eq. (4) that aggregates all nodes in each iteration holds more theoretical signiﬁcance than practical usage due to scalability concern. In practice, the aggregation procedure of DEA-GNN may be trimmed by balancing the tradeoff between complexity and performance. For example, we may choose ζ(u|v) = ζspd(u|v), and only aggregate the nodes u such that ζspd(u|v) ≤ K, i.e.,
K-hop neighbors. Multi-hop aggregation allows avoiding the training issues of deep architecture, e.g., gradient degeneration. Particularly, we may prove that K-hop aggregation decreases the number of layers L requested to (cid:100)( 1
K log(r−1) (cid:101) in Theorem 3.3 and Corollary 3.4 with proof in Appendix F.
We may also choose ζ(u|v) = ζgpr(u|v) with non-negative γk in Eq. (3) and aggregate the nodes whose ζ(u|v) are top-K ranked among all u ∈ V . This manner is able to control ﬁx-sized aggregation sets. As DEA-GNN does not show provably better representation power than DE-GNN, all the above approaches share the same theoretical power and limitations. However, in practice their speciﬁc performance may vary across datasets and applications. 2 + (cid:15)) log n 1We follow the terminology 2-WL test in [39], which reﬁnes representations of node-pairs iteratively and is proved to be more powerful than 1-WL test. This 2-WL test is termed 2-WL’ test in [40] or 2-FWL test in [27].
A brief introduction of higher-order WL tests can be found in Appendix H. 2Actually, the 2-WL test may not distinguish edges in a special class of DRGs, termed strongly regular graphs (SRG) [39]. A connected SRG is a DRG with diameter constrained as 2 [41]. 7
4