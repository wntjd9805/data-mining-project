Abstract
Prediction APIs offered for a fee are a fast-growing industry and an im-portant part of machine learning as a service. While many such services are available, the heterogeneity in their price and performance makes it challenging for users to decide which API or combination of APIs to use for their own data and budget. We take a ﬁrst step towards addressing this chal-lenge by proposing FrugalML, a principled framework that jointly learns the strength and weakness of each API on different data, and performs an efﬁcient optimization to automatically identify the best sequential strat-egy to adaptively use the available APIs within a budget constraint. Our theoretical analysis shows that natural sparsity in the formulation can be leveraged to make FrugalML efﬁcient. We conduct systematic experiments using ML APIs from Google, Microsoft, Amazon, IBM, Baidu and other providers for tasks including facial emotion recognition, sentiment analysis and speech recognition. Across various tasks, FrugalML can achieve up to 90% cost reduction while matching the accuracy of the best single API, or up to 5% better accuracy while matching the best API’s cost. 1

Introduction
Machine learning as a service (MLaaS) is a rapidly growing industry. For example, one could use Google prediction API [9] to classify an image for $0.0015 or to classify the sentiment of a text passage for $0.00025. MLaaS services are appealing because using such APIs reduces the need to develop one’s own ML models. The MLaaS market size was estimated at $1 billion in 2019, and it is expected to grow to $8.4 billion by 2025 [1].
Third-party ML APIs come with their own challenges, however. A major challenge is that different companies charge quite different amounts for similar tasks. For example, for image classiﬁcation, Face++ charges $0.0005 per image [6], which is 67% cheaper than Google [9], while Microsoft charges $0.0010 [11]. Moreover, the prediction APIs of different providers perform better or worse on different types of inputs. For example, accuracy disparities in gender classiﬁcation were observed for different skin colors [23, 37]. As we will show later in the paper, these APIs’ performance also varies by class—for example, we found that on the FER+ dataset, the Face++ API had the best accuracy on surprise images while the
Microsoft API had the best performance on neutral images. The more expensive APIs are not uniformly better; and APIs tend to have speciﬁc classes of inputs where they perform better than alternatives. This heterogeneity in price and in performance makes it challenging for users to decide which API or combination of APIs to use for their own data and budget.
In this paper, we propose FrugalML, a principled framework to address this challenge.
FrugalML jointly learns the strength and weakness of each API on different data, then 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Comparison of different approaches to use ML APIs. Naively calling a ﬁxed API in (a) provides a ﬁxed cost and accuracy. The simple cascade in (b) uses the quality score (QS) from a low-cost open source model to decide whether to call an additional service. Our proposed FrugalML approach, in (c), exploits both the quality score and predicted label to select APIs. Figure (d) shows the beneﬁts of FrugalML on FER+, a facial emotion dataset. performs an efﬁcient optimization to automatically identify the best adaptive strategy to use all the available APIs given the user’s budget constraint. FrugalML leverages the modular nature of APIs by designing adaptive strategies that can call APIs sequentially.
For example, we might ﬁrst send an input to API A. If A returns the label “dog” with high conﬁdence—and we know A tends to be accurate for dogs—then we stop and report “dog”.
But if A returns “hare” with lower conﬁdence, and we have learned that A is less accurate for “hare,” then we might adaptively select a second API B to make additional assessment.
FrugalML optimizes such adaptive strategies to substantially improve prediction perfor-mance over simpler approaches such as model cascades with a ﬁxed quality threshold (Figure 1). Through experiments with real commercial ML APIs on diverse tasks, we ob-serve that FrugalML typically reduces costs more than 50% and sometimes up to 90%.
Adaptive strategies are challenging to learn and optimize, because the choice of the 2nd predictor, if one is chosen, could depend on the prediction and conﬁdence of the ﬁrst API, and because FrugalML may need to allocate different fractions of its budget to predictions for different classes. We prove that under quite general conditions, there is natural sparsity in this problem that we can leverage to make FrugalML efﬁcient.
Contributions To sum up, our contributions are: 1. We formulate and study the problem of learning to optimally use commercial ML
APIs given a budget. This is a growing area of importance and is under-explored. 2. We propose FrugalML, a framework that jointly learns the strength and weakness of each API, and performs an optimization to identify the best strategy for using those
APIs within a budget constraint. By leveraging natural sparsity in this optimization problem, we design an efﬁcient algorithm to solve it with provable guarantees. 3. We evaluate FrugalML using real-world APIs from diverse providers (e.g., Google,
Microsoft, Amazon, and Baidu) for classiﬁcation tasks including facial emotion recognition, text sentiment analysis, and speech recognition. We ﬁnd that FrugalML can match the accuracy of the best individual API with up to 90% lower cost, or signiﬁcantly improve on this accuracy, up to 5%, with the the same cost. 4. We release our code and our dataset1 of 612,139 samples annotated by commercial
APIs as a resource to aid future research in this area.