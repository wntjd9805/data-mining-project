Abstract
In natural language processing (NLP), enormous pre-trained models like BERT have become the standard starting point for training on a range of downstream tasks, and similar trends are emerging in other areas of deep learning. In parallel, work on the lottery ticket hypothesis has shown that models for NLP and computer vision contain smaller matching subnetworks capable of training in isolation to full accuracy and transferring to other tasks. In this work, we combine these observations to assess whether such trainable, transferrable subnetworks exist in pre-trained BERT models. For a range of downstream tasks, we indeed ﬁnd matching subnetworks at 40% to 90% sparsity. We ﬁnd these subnetworks at (pre-trained) initialization, a deviation from prior NLP research where they emerge only after some amount of training. Subnetworks found on the masked language modeling task (the same task used to pre-train the model) transfer universally; those found on other tasks transfer in a limited fashion if at all. As large-scale pre-training becomes an increasingly central paradigm in deep learning, our results demonstrate that the main lottery ticket observations remain relevant in this context.
Codes available at https://github.com/VITA-Group/BERT-Tickets. 1

Introduction
In recent years, the machine learning research community has devoted substantial energy to scaling neural networks to enormous sizes. Parameter-counts are frequently measured in billions rather than millions [1–3], with the time and ﬁnancial outlay necessary to train these models growing in concert
[4]. These trends have been especially pronounced in natural language processing (NLP), where massive BERT models—built on the Transformer architecture [5] and pre-trained in a self-supervised fashion—have become the standard starting point for a variety of downstream tasks [6, 7]. Self-supervised pre-training is also growing in popularity in computer vision [8, 9], suggesting it may again become a standard practice across deep learning as it was in the past [10].
In parallel to this race for ever-larger models, an emerging subﬁeld has explored the prospect of training smaller subnetworks in place of the full models without sacriﬁcing performance [11–16]. For example, work on the lottery ticket hypothesis (LTH) [16] demonstrated that small-scale networks for computer vision contain sparse, matching subnetworks [17] capable of training in isolation from initialization to full accuracy. In other words, we could have trained smaller networks from the start if only we had known which subnetworks to choose. Within the growing body of work on the lottery ticket hypothesis, two key themes have emerged:
Initialization via pre-training. In larger-scale settings for computer vision and natural language processing [17–19], the lottery ticket methodology can only ﬁnd matching subnetworks at an early point in training rather than at random initialization. Prior to this point, these subnetworks perform 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
no better than those selected by pruning randomly. The phase of training prior to this point can be seen as dense pre-training that creates an initialization amenable to sparsiﬁcation. This pre-training can even occur using a self-supervised task rather than the supervised downstream task [20, 21].
Transfer learning. Finding matching subnetworks with the lottery ticket methodology is expensive.
It entails training the unpruned network to completion, pruning unnecessary weights, and rewinding the unpruned weights back to their values from an earlier point in training [16]. It is costlier than simply training the full network, and, for best results, it must be repeated many times iteratively.
However, the resulting subnetworks transfer between related tasks [22–24]. This property makes it possible to justify this investment by reusing the subnetwork for many different downstream tasks.
These two themes—initialization via pre-training and transfer learning—are also the signature attributes of BERT models: the extraordinary cost of pre-training is amortized by transferring to a range of downstream tasks. As such, BERT models are a particularly interesting setting for studying the existence and nature of trainable, transferable subnetworks. If we treat the pre-trained weights as our initialization, are there matching subnetworks for each downstream task? Do they transfer to other downstream tasks? Are there universal subnetworks that can transfer to many tasks with no degradation in performance? Practically speaking, this would allow us to replace a pre-trained BERT with a smaller subnetwork while retaining the capabilities that make it so popular for NLP work.
Although the lottery ticket hypothesis has been evaluated in the context of NLP [18, 19] and trans-formers [18, 25], it remains poorly understood in the context of pre-trained BERT models.1 To address this gap in the literature, we investigate how the transformer architecture and the initialization resulting from the lengthy BERT pre-training regime behave in comparison to existing lottery ticket results. We devote particular attention to the transfer behavior of these subnetworks as we search for universal subnetworks that can reduce the cost of ﬁne-tuning on downstream tasks going forward. In the course of this study, we make the following ﬁndings:
• Using unstructured magnitude pruning, we ﬁnd matching subnetworks at between 40% and 90% sparsity in BERT models on standard GLUE and SQuAD downstream tasks.
• Unlike previous work in NLP, we ﬁnd these subnetworks at (pre-trained) initialization rather after some amount of training. As in previous work, these subnetworks outperform those found by pruning randomly and randomly reinitializing.
• On most downstream tasks, these subnetworks do not transfer to other tasks, meaning that the matching subnetwork sparsity patterns are task-speciﬁc.
• Subnetworks at 70% sparsity found using the masked language modeling task (the task used for BERT pre-training) are universal and transfer to other tasks while maintaining accuracy.
We conclude that the lottery ticket observations from other computer vision and NLP settings extend to BERT models with a pre-trained initialization. In fact, the biggest caveat of prior work—that, in larger-scale settings, matching subnetworks can only be found early in training—disappears.
Moreover, there are indeed universal subnetworks that could replace the full BERT model without inhibiting transfer. As pre-training becomes increasingly central in NLP and other areas of deep learning [8, 9], our results demonstrate that the lottery ticket observations—and the tantalizing possibility that we can train smaller networks from the beginning—hold for the exemplar of this class of learning algorithms. 2