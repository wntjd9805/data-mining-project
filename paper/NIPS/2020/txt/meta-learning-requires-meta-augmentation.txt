Abstract
Meta-learning algorithms aim to learn two components: a model that predicts targets for a task, and a base learner that updates that model when given exam-ples from a new task. This additional level of learning can be powerful, but it also creates another potential source of overﬁtting, since we can now overﬁt in either the model or the base learner. We describe both of these forms of meta-learning overﬁtting, and demonstrate that they appear experimentally in common meta-learning benchmarks. We introduce an information-theoretic framework of meta-augmentation, whereby adding randomness discourages the base learner and model from learning trivial solutions that do not generalize to new tasks. We demonstrate that meta-augmentation produces large complementary beneﬁts to recently proposed meta-regularization techniques. 1

Introduction
In several areas of machine learning, data augmentation is critical to achieving state-of-the-art per-formance. In computer vision [31], speech recognition [17], and natural language processing [8], augmentation strategies effectively increase the support of the training distribution, improving gener-alization. Although data augmentation is often easy to implement, it has a large effect on performance.
For a ResNet-50 model [13] applied to the ILSVRC2012 object classiﬁcation benchmark [28], re-moving randomly cropped images and color distorted images from the training data results in a 7% reduction in accuracy (see Appendix B). Recent work in reinforcement learning [18, 21] also demonstrate large increases in reward from applying image augmentations.
Meta-learning has emerged in recent years as a popular framework for learning new tasks in a sample-efﬁcient way. The goal is to learn how to learn new tasks from a small number of examples, by leveraging knowledge from learning other tasks [5, 7, 14, 30, 33]. Given that meta-learning adds an additional level of model complexity to the learning problem, it is natural to suspect that data augmentation plays an equally important - if not greater - role in helping meta-learners generalize to new tasks. In classical machine learning, data augmentation turns one example into several examples.
In meta-learning, meta-augmentation turns one task into several tasks. Our key observation is that given labeled examples (x, y), classical data augmentation adds noise to inputs x without changing y, but for meta-learning we must do the opposite: add noise to labels y, without changing inputs x.
The main contributions of our paper are as follows: ﬁrst, we present a uniﬁed framework for meta-data augmentation and an information theoretic view on how it prevents overﬁtting. Under this framework, we interpret existing augmentation strategies and propose modiﬁcations to handle two forms of overﬁtting: memorization overﬁtting, in which the model is able to overﬁt to the training set without relying on the learner, and learner overﬁtting, in which the learner overﬁts to the training set and does not generalize to the test set. Finally, we show the importance of meta-augmentation on a variety of benchmarks and meta-learning algorithms.
∗Equal contribution.
†This work was done when the author was an intern at Google Brain. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Meta-learning problems provide support inputs (xs, ys) to a base learner, which applies an update to a model. Once applied, the model is given query input xq, and must learn to predict query target yq. (a) Memorization overﬁtting occurs when the base learner and (xs, ys) does not impact the model’s prediction of yq. (b) Learner overﬁtting occurs when the model and base learner leverage both (xs, ys) and xq to predict yq, but fails to generalize to the meta-test set. (c) Yin et al.
[37] propose an information bottleneck constraint on the model capacity to reduce memorization overﬁtting. (d) To tackle both forms of overﬁtting, we view meta-data augmentation as widening the task distribution, by encoding additional random bits (cid:15) in (xs, ys) that must be decoded by the base learner and model in order to predict a transformed y(cid:48) q. 2