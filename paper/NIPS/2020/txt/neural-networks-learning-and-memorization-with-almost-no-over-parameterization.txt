Abstract
Many results in recent years established polynomial time learnability of various models via neural networks algorithms (e.g. Andoni et al. [2014], Daniely et al.
[2016], Daniely [2017], Cao and Gu [2019], Ji and Telgarsky [2019], Zou and
Gu [2019], Ma et al. [2019], Du et al. [2018a], Arora et al. [2019], Song and
Yang [2019], Oymak and Soltanolkotabi [2019a], Ge et al. [2019], Brutzkus et al.
[2018]). However, unless the model is linearly separable Brutzkus et al. [2018], or the activation is quadratic Ge et al. [2019], these results require very large networks – much more than what is needed for the mere existence of a good predictor.
In this paper we make a step towards learnability results with near optimal network size. We give a tight analysis on the rate in which the Neural Tangent KernelJacot et al. [2018], a fundamental tool in the analysis of SGD on networks, converges to its expectations. This results enable us to prove that SGD on depth two neural networks, starting from a (non standard) variant of Xavier initialization Glorot and
Bengio [2010] can memorize samples, learn polynomials with bounded weights, and learn certain kernel spaces, with near optimal network size, sample complexity, and runtime. In particular, we show that SGD on depth two network with ˜O m d hidden neurons (and hence ˜O(m) parameters) can memorize m random labeled
� points in Sd 1.
�
− 1

Introduction
D
, f ∗) of input distribution and target function f ∗) on which
Understanding the models (i.e. pairs ( neural networks algorithms guaranteed to learn a good predictor is at the heart of deep learning theory today. In recent years, there has been an impressive progress in this direction. It is now known that neural networks algorithms can learn, in polynomial time, linear models, certain kernel spaces, polynomials, and memorization models (e.g. Andoni et al. [2014], Daniely et al. [2016], Daniely
[2017], Cao and Gu [2019], Ji and Telgarsky [2019], Zou and Gu [2019], Ma et al. [2019], Du et al.
[2018a], Arora et al. [2019], Song and Yang [2019], Oymak and Soltanolkotabi [2019a], Ge et al.
[2019], Brutzkus et al. [2018]).
D
Yet, while such models has been shown to be learnable in polynomial time and polynomial sized networks, the required size (i.e., number of parameteres) of the networks is still very large, unless the model is linear separable Brutzkus et al. [2018], or the activation is quadratic Ge et al. [2019]. This means that the proofs are valid for networks whose size is signiﬁcantly larger then the minimal size of the network that implements a good predictor1. 1More speciﬁcally, we mean that the proofs require number of parameters that is suboptimal by a multiplica-tive factor that grows polynomially with one of the problem parameters – either the model capacity (margin, VC dimension, etc.), the desired error (i.e. �), or the input dimension. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper we make a progress in this direction. We ﬁrst consider the neural tangent kernel Jacot et al. [2018], which is a linearization of the functions that can be computed by the network, with weights that are close to a given weight vector w. The NTK is one of the main technical tools in recent analysis of SGD on neural networks. Our ﬁrst result is a near optimal bound on the rate in which the
NTK converge to its expectation. We then utilize this results, and prove that it implies that SGD on depth two networks, starting form a (somewhat non-standard) variant of Xavier initialization Glorot and Bengio [2010] can learn memorization models, polynomials, and kernel spaces, with near optimal network size, sample complexity, and runtime (i.e. SGD iterations).
To the best of our knowledge, this is the ﬁrst result which shows near optimal learnability of these models, and we believe that the result about NTK will be an essential tool for further progress, and in particular for proving a similar results for additional settings, architectures, and initialization schemes (particularly, the standard Xavier initialization). We next give more details about our results.
Neural Network Algorithm We assume that the instance space is Sd networks with 2q hidden neurons. Such networks calculate a function of the form
− 1 and consider depth 2 2q hW,u(x) = i=1
� uiσ ( wi, x
) =
�
�
� u, σ (W x)
�
We assume that the network is trained via SGD, starting from random weights that are sampled from the following variant of Xavier initialization Glorot and Bengio [2010]: W will be initialized to be a duplication W = of a matrix W � of standard Gaussians and u will be a duplication of the all-B vector in dimension q, for some B > 0, with its negation. We will use rather large B, that will depend on the model that we want to learn.
W �
W �
�
� 2
D
≤ u, x
∼D � on Sd 1 is R-bounded if for every u is √d-bounded, and this bound is tight in the cases that
Bounded distributions Some of our results will depend on what we call the boundedness of 1, the data distribution. We say that a distribution
−
R2 d . To help the reader to calibrate our results, we ﬁrst note that by Cauchy-Ex
�
Schwartz, any distribution is supported
D on a single point. Despite that, many distributions of interest are O(1)-bounded or even (1 + o(1))-1, the uniform distribution on the discrete bounded. This includes the uniform distribution on Sd
, the uniform distribution on Ω (d) random points, and more (see section A.5). For cube simplicity, we will phrase our results in the introduction for O(1)-bounded distribution. We note that if the distribution is R-bounded (rather than O(1)-bounded), our results suffer a multiplicative factor of R2 in the number of parameters, and remains the same in the runtime (SGD steps). 1
√d
Sd
±
D
∈
�
�
−
− d
NTK Convergence For weights (W, u) and x gradient, w.r.t. the hidden weights W , of hW,u(x). (A slight variant of) The NTK at W is 1 we denote by ΨW,u(x)
Sd
∈
∈
−
R2q d the
× kW (x, y) = �
ΨW,u(x), ΨW,u(y) 2qB2
�
And the expected initial NTK is k(x, y) = EW kW (x, y) Our main technical contribution is near optimal analysis of the rate (it terms of the size of the network) in which kW converges to k.
Speciﬁcally, we show that for any O(1)-bounded distribution, and every function f : Rd
R in the
→
Hk corresponding to k, there is a function ˆf in the kernel space kernel space
HkW corresponding to kW such that
E
∼D x (f (x)
−
ˆf (x))2 = O
� 2 k f
� dq
�
�
� · �k denotes the kernel norm of f . The proof of the aforementioned result is based on a
Here, new analysis of vector random feature schemes. While standard analysis of random feature schemes
, our new analysis show that would lead to a bound of the form Ex
∼D for O(1)-bounded distributions, a factor of the input dimension d can be saved.
ˆf (x))2 = O (f (x) 2 f k
� q
−
�
�
�
As mentioned above, we utilize our result for NTK convergence to prove various learnability results for SGD on depth two networks. 2
{ (x1, y1), . . . , (xm, ym)
Memorization In the problem of memorization, we consider SGD training on top of a sample
S =
. The goal is to understand how large the networks should be, and (to somewhat leaser extent) how many SGD steps are needed in order to memorize 1
� fraction of the examples, where an example is considered memorized if yih(xi) > 0 for the output function h.
Many results assumes that the points are random or “look like random" in some sense.
−
}
In order to memorize even just slightly more that half of the m examples we need a network with at least m parameters (up to poly-log factors). However, unless m d (in which case the points are linearly separable), best know results require much more than m parameters, and the current state of the art results Song and Yang [2019], Oymak and Soltanolkotabi [2019a] require m2 parameters. We 1, and the labels are random, then show that if the points are sampled uniformly at random from Sd any fraction of the examples can be memorized by a network with ˜O(m) parameters, and ˜O
SGD iterations. Our result is valid for the hinge loss, and most popular activation functions, including
� the ReLU. m
�2
≤
�
−
Learning Polynomials For the sake of clarity, we will describe our result for learning even poly-nomials, with ReLU networks, and the loss being the logistic loss or the hinge loss. Fix a constant c and coefﬁcient vector norm at integer c > 0 and consider the class of even polynomials of degree
M c = p(x) = most M . Namely, where for is even and
P
|
Rd we denote xα = d and x
�
∈ {
M d requires a networks with at least Ω
≤ is even and
≤ d i=1 αi. Learning the class parameters (and this remains true even if we restrict
M c ,
P to O(1)-bounded distributions). We show that for O(1)-bounded distributions, SGD learns
� with error parameter � (that is, it returns a predictor with error
�), using a network with ˜O c aαxα : d i=1 xαi
| i and
� 0, 1, 2, . . . c a2
α ≤
�
∈
M 2
M 2
|
α
�
�
=
�
α
}
�
≤
α
α
|
|
|
≤
P
M 2
�2
�
� parameters and O
SGD iterations.
M 2
�2
�
�
Learning Kernel Spaces Our result for polynomials is a corollary of a more general result about learning certain kernel spaces, that we describe next. Our result about memorization is not a direct corollary, but is also a reﬁnement of that result. We consider the kernel k : Sd
R given by
Sd
→
×
−
− 1 1 k(x, y) = x, y
�
� · w
E
∼N (I,0)
σ� ( w, x
�
�
,
� w, y
)
� (1) which is a variant of the Neural Tangent Kernel Jacot et al. [2018]. We show that for O(1)-bounded
M in the corresponding kernel space, with error distributions, SGD learns functions with norm parameter �, using a network with ˜O
SGD iterations. We note that the network size is optimal up to the dependency on � and poly-log factors, and the number of iteration is optimal up to a constant factor. This result is valid for most Lipschitz losses including the hinge loss and the log-loss, and for most popular activation functions, including the ReLU. parameters and O
M 2
�2
M 2
�2
≤
�
�
�
� 1.1