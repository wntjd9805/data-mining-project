Abstract
Partial label learning assumes inaccurate supervision where each training example is associated with a set of candidate labels, among which only one is valid. In many real-world scenarios, however, it is costly and time-consuming to assign candidate label sets to all the training examples. To circumvent this difﬁculty, the problem of semi-supervised partial label learning is investigated in this paper, where unlabeled data is utilized to facilitate model induction along with partial label training examples. Speciﬁcally, label propagation is adopted to instantiate the labeling conﬁdence of partial label examples. After that, maximum margin formulation is introduced to jointly enable the induction of predictive model and the estimation of labeling conﬁdence over unlabeled data. The derived formulation enforces conﬁdence-rated margin maximization and conﬁdence manifold preserva-tion over partial label examples and unlabeled data. We show that the predictive model and labeling conﬁdence can be solved via alternating optimization which admits QP solutions in either alternating step. Extensive experiments on synthetic as well as real-world data sets clearly validate the effectiveness of the proposed semi-supervised partial label learning approach. 1

Introduction
In partial label (PL) learning, each training example is represented by a single instance while associated with multiple candidate labels. It is assumed that the ground-truth label of PL training example resides in its candidate label set, which is not directly accessible to the training algorithm
[8, 17, 32]. The need to learn from these inaccurate supervision information widely exists in various applications, such as image classiﬁcation [6, 9, 30], ecoinformatics [4, 17, 33], web mining [18], natural language processing [23, 24, 35], etc.
Most partial label learning approaches work under supervised setting where the candidate labeling information is available for all training examples. In many real-world scenarios, however, the process of acquiring training examples with candidate labels might be demanding while abundant unlabeled data are readily available to facilitate model training. For instance, in crowdsourced image tagging, acquiring candidate annotations from web users for a large number of images would be costly and time-consuming while abundant unlabeled images can be easily collected from the web. Therefore, it is a natural remedy to consider semi-supervised partial label learning which exploits unlabeled data in conjunction with PL training examples to help induce predictive model with strong generalization performance.
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Correspondingly, a novel approach named PARM, i.e. semi-supervised Partial label learning via conﬁdence-rated mARgin Maximization, is proposed in this paper. To make use of unlabeled data,
PARM chooses to jointly estimate the labeling conﬁdence over unlabeled data and induce the desired multi-class classiﬁcation model. Speciﬁcally, PARM considers conﬁdence-rated margin which is maximized by preserving labeling conﬁdence manifold structure between PL training examples and unlabeled data. PARM tackles the resulting formuation based on alternating optimization, where the predictive model and labeling conﬁdence are updated in either alternating step with QP solutions.
Comparative studies on both synthetic and real-world data sets show that PARM achieves favorable performance against state-of-the-art approaches in exploiting unlabeled data for partial label learning.
To the best of our knowledge, SSPL [27] corresponds to the only prior work which considers utilizing unlabeled data for partial label learning. Speciﬁcally, SSPL adopts graph-based techniques to disambiguate the labeing information between PL training examples and unlabeled data via label propagation. Due to the transductive nature of graph-based techniques, the resulting algorithm won’t generalize to make prediction on unseen instances. To account for this issue, kNN rule is further employed to enable prediction on unseen instances. Consequently, SSPL has to store all the disambiguated PL training examples as well as unlabeled data during testing phase, which makes
SSPL less efﬁcient in terms of storage overhead and prediction time. Due to the inductive nature of maximum margin approach, PARM is capable of making predictions on unseen examples without resorting to extra procedure.
The rest of this paper is organized as follows. Firstly, we brieﬂy review related work on partial label learning. Secondly, technical details of the proposed approach are presented. Thirdly, experimental results of comparative studies are reported. Finally, we conclude this paper. 2