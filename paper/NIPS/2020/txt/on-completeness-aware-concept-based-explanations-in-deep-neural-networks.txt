Abstract
Human explanations of high-level decisions are often expressed in terms of key concepts the decisions are based on. In this paper, we study such concept-based explainability for Deep Neural Networks (DNNs). First, we deﬁne the notion of completeness, which quantiﬁes how sufﬁcient a particular set of concepts is in explaining a model’s prediction behavior based on the assumption that complete concept scores are sufﬁcient statistics of the model prediction. Next, we propose a concept discovery method that aims to infer a complete set of concepts that are additionally encouraged to be interpretable, which addresses the limitations of existing methods on concept explanations. To deﬁne an importance score for each discovered concept, we adapt game-theoretic notions to aggregate over sets and propose ConceptSHAP. Via proposed metrics and user studies, on a synthetic dataset with apriori-known concept explanations, as well as on real-world image and language datasets, we validate the effectiveness of our method in ﬁnding concepts that are both complete in explaining the decisions and interpretable.1

Introduction 1
The lack of explainability of deep neural networks (DNNs) arguably hampers their full potential for real-world impact. Explanations can help domain experts better understand rationales behind the model decisions, identify systematic failure cases, and potentially provide feedback to model builders for improvements. Most commonly-used methods for DNNs explain each prediction by quantifying the importance of each input feature [Ribeiro et al., 2016, Lundberg and Lee, 2017]. One caveat with such explanations is that they typically focus on the local behavior for each data point, rather than globally explaining how the model reasons. Besides, the weighted input features are not necessarily the most intuitive explanations for human understanding, particularly when using low-level features such as raw pixel values. In contrast, human reasoning often comprise “concept-based thinking,” extracting similarities from numerous examples and grouping them systematically based on their resemblance [Armstrong et al., 1983, Tenenbaum, 1999]. It is thus of interest to develop such
“concept-based explanations” to characterize the global behavior of a DNN in a way understandable to humans, explaining how DNNs use concepts in arriving at particular decisions.
A few recent studies have focused on bringing such concept-based explainability to DNNs, largely based on the common implicit assumption that the concepts lie in low-dimensional subspaces of some intermediate DNN activations. Via supervised training based on labeled concepts, TCAV
[Kim et al., 2018] trains linear concept classiﬁers to derive concept vectors, and uses how sensitive predictions are to these vectors (via directional derivatives) to measure the importance of a concept with respect to a speciﬁc class. Zhou et al. [2018] considers the decomposition of model predictions in terms of projections onto concept vectors. Instead of human-labeled concept data, Ghorbani et al. 1The code is released at https://github.com/chihkuanyeh/concept_exp. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[2019] employs k-means clustering of super-pixel segmentations of images to discover concepts.
Bouchacourt and Denoyer [2019] proposes a Bayesian generative model involving concept vectors.
One drawback of these approaches is that they do not take into account how much each concept plays a role in the prediction. In particular, selecting a set of concepts salient to a particular class does not guarantee that these concepts are sufﬁcient in explaining the prediction. The notion of sufﬁciency is also referred to as “completeness” of explanations, as in [Gilpin et al., 2018, Yang et al., 2019]. This motivates the following key questions: Is there an unsupervised approach to extract concepts that are sufﬁciently predictive of a DNN’s decisions? If so, how can we measure this sufﬁciency?
In this paper, we propose such a completeness score for concept-based explanations. Our metric can be applied to a set of concept vectors that lie in a subspace of some intermediate DNN activations, which is a general assumption in previous work in this context [Kim et al., 2018, Zhou et al., 2018].
Intuitively speaking, a set of “complete” concepts can fully explain the prediction of the underlying model. By further assuming that for a complete set of concepts, the projections of activations onto the concepts are a sufﬁcient statistic for the prediction of the model, we may measure the “completion” of the concepts by the accuracy of the model just given these concept based sufﬁcient statistics.
For concept discovery, we propose a novel algorithm, which could also be viewed as optimizing a surrogate likelihood of the concept-based data generation process, motivated by topic modeling [Blei et al., 2003]. To ensure that the discovered complete concepts are also coherent (distinct from other concepts) and semantically meaningful, we further introduce an interpretability regularizer.
Beyond concept discovery, we also propose a score, ConceptSHAP, for quantiﬁcation of concept attributions as contextualized importance. ConceptSHAP uniquely satisﬁes a key set of axioms involving the contribution of each concept to the completeness score [Shapley, 1988, Lundberg and
Lee, 2017]. We also propose a class-speciﬁc version of ConceptSHAP that decomposes it with respect to each class in multi-class classiﬁcation. This can be used to ﬁnd class-speciﬁc concepts that contribute the most to a speciﬁc class. To verify the effectiveness of our automated completeness-aware concept discovery method, we create a synthetic dataset with apriori-known ground truth concepts. We show that our approach outperforms all compared methods in correct retrieval of the concepts as well as in terms of its coherency via a user study. Lastly, we demonstrate how our concept discovery algorithm provides additional insights into the behavior of DNN models on both image and language real-world datasets. 2