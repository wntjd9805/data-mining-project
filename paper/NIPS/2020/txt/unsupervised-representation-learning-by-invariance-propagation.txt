Abstract
Unsupervised learning methods based on contrastive learning have drawn increas-ing attention and achieved promising results. Most of them aim to learn repre-sentations invariant to instance-level variations, which are provided by different views of the same instance. In this paper, we propose Invariance Propagation to focus on learning representations invariant to category-level variations, which are provided by different instances from the same category. Our method recursively discovers semantically consistent samples residing in the same high-density regions in representation space. We demonstrate a hard sampling strategy to concentrate on maximizing the agreement between the anchor sample and its hard positive samples, which provide more intra-class variations to help capture more abstract invariance. As a result, with a ResNet-50 as the backbone, our method achieves 71.3% top-1 accuracy on ImageNet linear classiﬁcation and 78.2% top-5 accuracy
ﬁne-tuning on only 1% labels, surpassing previous results. We also achieve state-of-the-art performance on other downstream tasks, including linear classiﬁcation on Places205 and Pascal VOC, and transfer learning on small scale datasets. 1

Introduction
Deep convolutional neural networks have gained great progress since the emergence of large-scale annotated datasets [7, 44]. The learned networks perform well on classiﬁcation tasks [23, 37, 18], and can be transfered well to other tasks such as detection [13, 36] and segmentation [27, 17].
However, such progress requires large-scale human-annotated datasets, which are difﬁcult to acquire.
Unsupervised learning is introduced to give us the promise to learn useful representations without manual annotations. Speciﬁcally, many self-supervised methods are proposed to learn representations by solving handcrafted auxiliary tasks, such as jigsaw puzzle [31], rotation [12], colorization [42], etc. Although the self-supervised methods have gained remarkable performance, the design of pretext tasks depends on the domain-speciﬁc knowledge, limiting the generality of both the learned representations and the design of future methods. Recently, methods based on contrastive learning
[39, 33, 38, 16, 1] have drawn increasing attention and achieved promising results. Most of them aim to learn representations invariant to different views of the same instance, such as data augmentations
[39, 33, 16, 1], color information [38] and context information [33], while ignoring the relations between different instances which are the key to reﬂect the global semantic structure.
In this work, we present Invariance Propagation (InvP), a novel method which embodies the relations between different instances to learn representations with category-level invariance. Speciﬁcally, InvP discovers positive samples by recursively propagating the local invariance through the k-nearest neighbors graph. We keep k small to preserve a high semantic consistency. By applying transitivity
∗corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
on the kNN graph, we can obtain positive images exhibiting richer intra-class variations. In this way, the positive samples contain different semantically consistent instances discovered by our algorithm, which enable the network to learn representations invariant to intra-class inter-instance variations.
InvP keeps the positive samples residing in the same high-density regions, making the positive samples more consistent, which coincides with the smoothness assumption [4] widely adopted in semi-supervised learning.
After obtaining the positive samples, our goal is to learn a model which maps the pixel space to an embedding space where positive samples are attracted and negative samples are separated. To learn the model effectively, we demonstrate a hard sampling strategy. Speciﬁcally, we regard those positive samples with low similarities to the anchor sample as hard positive samples, and concentrate on maximizing the agreement between the anchor sample and its hard positive samples. We will show in the experiments section that the hard positive samples provide more intra-class variations, which helps improve the robustness of the learned representations.
We evaluate our method on extensive downstream tasks including linear classiﬁcation on ImageNet [7],
Places205 [44] and Pascal VOC [10], transfer learning on seven small scale datasets, semi-supervised learning and object detection. Overall, our contributions can be summarized as follows:
• We propose Invariance Propagation, a novel unsupervised learning method that exploits the relations between different instances to learn representations invariant to category-level variations.
Our method is both effective and reproducible on standard hardware.
• We demonstrate a hard sampling strategy to ﬁnd positive samples that provide more intra-class variations to help capture more abstract invariance. Experiments are conducted to validate the effectiveness of the proposed hard sampling strategy.
• We conduct extensive quantitative experiments to validate the effectiveness of our method, achiev-ing competitive results on object detection and state-of-the-art results on ImageNet, Places205, and VOC2007 linear classiﬁcation, semi-supervised learning, and transfer learning.
• We conduct qualitative experiments, including the visualization of similarity distribution, which shows our method successfully captures the category-level invariance, and the visualization of hard positive samples, which gives an intuitive understanding of the hard sampling strategy. 2