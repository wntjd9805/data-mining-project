Abstract
We study deep neural networks (DNNs) trained on natural image data with entirely random labels. Despite its popularity in the literature, where it is often used to study memorization, generalization, and other phenomena, little is known about what
DNNs learn in this setting. In this paper, we show analytically for convolutional and fully connected networks that an alignment between the principal components of network parameters and data takes place when training with random labels.
We study this alignment effect by investigating neural networks pre-trained on randomly labelled image data and subsequently ﬁne-tuned on disjoint datasets with random or real labels. We show how this alignment produces a positive transfer: networks pre-trained with random labels train faster downstream compared to train-ing from scratch even after accounting for simple effects, such as weight scaling.
We analyze how competing effects, such as specialization at later layers, may hide the positive transfer. These effects are studied in several network architectures, including VGG16 and ResNet18, on CIFAR10 and ImageNet. 1

Introduction
Over-parameterization helps deep neural networks (DNNs) to generalize better in real-life appli-cations [8, 24, 30, 54], despite providing them with the capacity to ﬁt almost any set of random labels [55]. This phenomenon has spawned a growing body of work that aims at identifying funda-mental differences between real and random labels, such as in training time [4, 19, 20, 56], sharpness of the minima [28, 40], dimensionality of layer embeddings [3, 11, 35], and sensitivity [4, 41], among other complexity measures [6, 7, 39, 40]. While it is obvious that over-parameterization helps DNNs to interpolate any set of random labels, it is not immediately clear what DNNs learn when trained in this setting. The objective of this study is to provide a partial answer to this question.
There are at least two reasons why answering this question is of value. First, in order to understand how DNNs work, it is imperative to observe how they behave under “extreme” conditions, such as
?Equal contribution.
†Work completed during the Google AI Residency Program. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1. Pre-training helps (real labels) 2. Pre-training helps (random labels) 3. Pre-training hurts (real labels) 4. Pre-training hurts (random labels)
Figure 1: Pre-training on random labels may exhibit both positive (1 & 2) and negative (3 & 4) effects on the downstream ﬁne-tuning depending on the setup. VGG16 models are pre-trained on
CIFAR10 examples with random labels and subsequently ﬁne-tuned on the fresh CIFAR10 examples with either real labels (1 & 3) or 10 random labels (2 & 4) using different hyperparameters. when trained with labels that are entirely random. Since the pioneering work of [55], several works have looked into the case of random labels. What distinguishes our work from others is that previous works aimed to demonstrate differences between real and random labels, highlighting the negative side of training on random labels. By contrast, this work provides insights into what properties of the data distribution DNNs learn when trained on random labels.
Second, observing DNNs trained on random labels can explain phenomena that have been previously noted, but were poorly understood. In particular, by studying what is learned on random labels, we offer new insights into: (1) why DNNs exhibit critical stages [1, 17], (2) how earlier layers in DNNs generalize while later layers specialize [3, 4, 10, 53], (3) why the ﬁlters learned by DNNs in the
ﬁrst layer seem to encode some useful structure when trained on random labels [4], and (4) why pre-training on random labels can accelerate training in downstream tasks [42]. We show that even when controlling for simple explanations like weight scaling (which was not always accounted for previously), such curious observations continue to hold.
The main contributions of this work are:
•
•
•
We investigate DNNs trained with random labels and ﬁne-tuned on disjoint image data with real or random labels, demonstrating unexpected positive and negative effects.
We provide explanations of the observed effects. We show analytically for convolutional and fully connected networks that an alignment between the principal components of the network parameters and the data takes place. We demonstrate experimentally how this effect explains why pre-training on random labels helps. We also show why, under certain conditions, pre-training on random labels can hurt the downstream task due to specialization at the later layers.
We conduct experiments verifying that these effects are present in several network architectures, including VGG16 [46] and ResNet18-v2 [22], on CIFAR10 [31] and ImageNet ILSVRC-2012 [14], across a range of hyper-parameters, such as the learning rate, initialization, number of training iterations, width and depth.
In this work, we do not use data augmentation as it provides a (weak) supervisory signal. Moreover, we use the terms “positive” and “negative” to describe the impact of what is learned with random labels on the downstream training, such as faster/slower training. The networks reported throughout the paper are taken from a big set of experiments that we conducted using popular network architectures, datasets, and wide hyperparameter ranges. Experimental details are provided in Appendix A and B.
We use boldface for random variables, small letters for their values, and capital letters for matrices. 1.1 Motivating example
Figure 1 shows learning curves of the VGG16 architecture [46] pre-trained on 20k CIFAR10 exam-ples [31] with random labels (upstream) and ﬁne-tuned on a disjoint subset of 25k CIFAR10 examples with either random or real labels (downstream). We observe that in this setup, pre-training a neural network on images with random labels accelerates training on a second set of images, both for real and random labels (positive effect). However, in the same setting but with a different initialization scale and number of random classes upstream, a negative effect can be observed downstream: training 2
becomes slower. We also observe a lower ﬁnal test accuracy for real labels in both cases, which we are not explicitly investigating in this paper (and which has been observed before, e.g. in [17]).
The fact that pre-training on random labels can accelerate training downstream has been observed previously, e.g. in [42]. However, there is a “simple” property that can explain improvements in the downstream task: Because the cross-entropy loss is scale-sensitive, training the network tends to increase the scale of the weights [40], which can increase the effective learning rate of the downstream task (see the gray curve in Figure 5). To eliminate this effect, in all experiments we re-scale the weights of the network after pre-training to match their `2 norms at initialization. We show that even after this correction, pre-training on random labels positively affects the downstream task. This holds for both VGG16 and ResNet18 trained on CIFAR10 and ImageNet (see Appendix B).
We show experimentally that some of the positive transfer is due to the second-order statistics of the network parameters. We prove that when trained on random labels, the principal components of weights at the ﬁrst layer are aligned with the principal components of data. Interestingly, this alignment effect implies that the model parameters learned at the ﬁrst layer can be summarized by a one-dimensional mapping between the eigenvalues of the data and the eigenvalues of the network parameters. We study these mappings empirically and raise some new open questions. We also analyze how, under certain conditions, a competing effect of specialization at the later layers may hide the positive transfer of pre-training on random labels, which we show to be responsible for the negative effect demonstrated in Figure 1.
To the best of our knowledge, the alignment effect has not been established in the literature before.
This paper proves the existence of this effect and studies its implications. Note that while these effects are established for training on random labels, we also observe them empirically for real labels. 1.2