Abstract
De novo molecular design attempts to search over the chemical space for molecules with the desired property. Recently, deep learning has gained considerable attention as a promising approach to solve the problem. In this paper, we propose genetic expert-guided learning (GEGL), a simple yet novel framework for training a deep neural network (DNN) to generate highly-rewarding molecules. Our main idea is to design a “genetic expert improvement” procedure, which generates high-quality targets for imitation learning of the DNN. Extensive experiments show that GEGL signiﬁcantly improves over state-of-the-art methods. For example, GEGL manages to solve the penalized octanol-water partition coefﬁcient optimization with a score of 31.40, while the best-known score in the literature is 27.22. Besides, for the
GuacaMol benchmark with 20 tasks, our method achieves the highest score for 19 tasks, in comparison with state-of-the-art methods, and newly obtains the perfect score for three tasks. Our training code is available at https://github.com/ sungsoo-ahn/genetic-expert-guided-learning. 1

Introduction
Discovering new molecules with the desired property is fundamental in chemistry, with critical applications such as drug discovery [1] and material design [2]. The task is challenging since the molecular space is vast; e.g., the number of synthesizable drug-like compounds is estimated to be around 1060 [3]. To tackle this problem, de novo molecular design [4, 5] aims to generate a new molecule from scratch with the desired property, rather than naïvely enumerate over the molecular space.
Over the past few years, molecule-generating deep neural networks (DNNs) have demonstrated successful results for solving the de novo molecular design problem [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]. For example, Gómez-Bombarelli et al. [8] perform Bayesian optimization for maximizing the desired property, on the embedding space of molecule-generating variational auto-encoders. On the other hand, Guimaraes et al. [6] train a molecule-generating policy using reinforcement learning with the desired property formulated as a reward.
Intriguingly, several works [23, 24, 25, 26] have recently evidenced that the traditional frameworks based on genetic algorithm (GA) can compete with or even outperform the recently proposed deep learning methods. They reveal that GA is effective, thanks to the powerful domain-speciﬁc genetic operators for exploring the chemical space. For example, Jensen [24] achieves outstanding performance by generating new molecules as a combination of subgraphs extracted from existing ones. Such observations also emphasize how domain knowledge can play a signiﬁcant role in de novo molecular design. On the contrary, the current DNN-based methods do not exploit such domain knowledge explicitly; instead, they implicitly generalize the knowledge of high-rewarding molecules by training a DNN on them. Notably, the expressive power of DNN allows itself to parameterize a distribution over the whole molecular space ﬂexibly. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of the proposed genetic expert-guided learning (GEGL) framework.
Contribution. In this work, we propose genetic expert-guided learning (GEGL), which is a novel framework for training a molecule-generating DNN guided with genetic exploration. Our main idea is to formulate an expert policy by applying the domain-speciﬁc genetic operators, i.e., mutation and crossover,1 to the DNN-generated molecules. Then the DNN becomes an apprentice policy that learns to imitate the highly-rewarding molecules discovered by the expert policy. Since the expert policy improves over the apprentice policy by design, the former policy consistently guides the latter policy to generate highly-rewarding molecules. We provide an overall illustration of our framework in Figure 1.
We note that our GEGL framework can be seen as a reinforcement learning algorithm with a novel mechanism for additional explorations. To be speciﬁc, the generation of a molecule can be regarded as an action and the desired property of the generated molecule as a reward. Similar to most reinforcement learning algorithms, reducing the sample complexity is crucial in our GEGL framework.
To this end, we design our framework with max-reward priority queues [13, 27, 28, 29, 30]. By storing the highly-rewarding molecules, the priority queues prevent the policies from “forgetting” the valuable knowledge.
We extensively evaluate our method on four experiments: (a) optimization of penalized octanol-water partition coefﬁcient, (b) optimization of penalized octanol-water partition coefﬁcient under similarity constraints, (c) the GuacaMol benchmark [31] consisting of 20 de novo molecular design tasks, and (d) the GuacaMol benchmark evaluated under post-hoc ﬁltering procedure [32]. Remarkably, our
GEGL framework outperforms all prior methods for de novo molecular design by a large margin. In particular, GEGL achieves the penalized octanol-water partition coefﬁcient score of 31.40, while the best baseline [22] and the second-best baseline [17] achieves the score of 27.22 and 26.1, respectively.
For the GuacaMol benchmark, our algorithm achieves the highest score for 19 out of 20 tasks and newly achieves the perfect score for three tasks. 2