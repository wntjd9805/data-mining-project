Abstract
This paper introduces a new method to build linear ﬂows, by taking the exponential of a linear transformation. This linear transformation does not need to be invertible itself, and the exponential has the following desirable properties: it is guaranteed to be invertible, its inverse is straightforward to compute and the log Jacobian determinant is equal to the trace of the linear transformation. An important insight is that the exponential can be computed implicitly, which allows the use of con-volutional layers. Using this insight, we develop new invertible transformations named convolution exponentials and graph convolution exponentials, which retain the equivariance of their underlying transformations. In addition, we generalize
Sylvester Flows and propose Convolutional Sylvester Flows which are based on the generalization and the convolution exponential as basis change. Empirically, we show that the convolution exponential outperforms other linear transformations in generative ﬂows on CIFAR10 and the graph convolution exponential improves the performance of graph normalizing ﬂows. In addition, we show that Convolutional
Sylvester Flows improve performance over residual ﬂows as a generative ﬂow model measured in log-likelihood. 1

Introduction
Deep generative models aim to learn a distribution pX (x) for a high-dimensional variable x. Flow-based generative models (Dinh et al., 2015, 2017) are particularly attractive because they admit exact likelihood optimization and straightforward sampling. Since normalizing ﬂows are based on the change of variable formula, they require the ﬂow transformation to be invertible. In addition, the
Jacobian determinant needs to be tractable to compute the likelihood.
In practice, a ﬂow is composed of multiple invertible layers. Since the Jacobian determinant is required to compute the likelihood, many ﬂow layers are triangular maps, as the determinant is then the product of the diagonal elements. However, without other transformations, the composition of triangular maps will remain triangular. For that reason, triangular ﬂows are typically interleaved with linear ﬂows that mix the information over dimensions. Existing methods include permutations (Dinh et al., 2015) and 1 1 convolutions (Kingma and Dhariwal, 2018) but these do not operate over feature maps spatially. Alternatives are emerging convolutions (Hoogeboom et al., 2019a) and
⇥ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Visualization of the equivalent matrix exponential exp(M) where M represents a 2d convolution on a 1 5 input (channel ﬁrst). In this example the computation is explicit, however in practice the exponential is computed implicit and the matrices M and exp(M) are never stored.
⇥
⇥ 5 periodic convolutions (Finzi et al., 2019; Karami et al., 2019). However, periodicity is generally not a good inductive bias for images, and emerging convolutions are autoregressive and their inverse is solved iteratively over dimensions.
In this paper, we introduce a new method to construct invertible transformations, by taking the exponential of any linear transformation. The exponential is always invertible, and computing the inverse and Jacobian determinant is straightforward. Extending prior work Goli´nski et al. (2019), we observe that the exponential can be computed implicitly. As a result, we can take the exponential of linear operations for which the corresponding matrix multiplication would be intractable. The canonical example of such a transformation is a convolutional layer, using which we develop a new transformation named the convolution exponential. In addition we propose a new residual transformation named Convolutional Sylvester Flow, a combination of a generalized formulation for
Sylvester Flows, and the convolution exponential as basis change. Code for our method can be found at: https://github.com/ehoogeboom/convolution_exponential_and_sylvester 2