Abstract
Feature attributions are a popular tool for explaining the behavior of Deep Neural
Networks (DNNs), but have recently been shown to be vulnerable to attacks that produce divergent explanations for nearby inputs. This lack of robustness is especially problematic in high-stakes applications where adversarially-manipulated explanations could impair safety and trustworthiness. Building on a geometric understanding of these attacks presented in recent work, we identify Lipschitz continuity conditions on models’ gradients that lead to robust gradient-based attributions, and observe that the smoothness of the model’s decision surface is related to the transferability of attacks across multiple attribution methods.
To mitigate these attacks in practice, we propose an inexpensive regularization method that promotes these conditions in DNNs, as well as a stochastic smoothing technique that does not require re-training. Our experiments on a range of image models demonstrate that both of these mitigations consistently improve attribution robustness, and conﬁrm the role that smooth geometry plays in these attacks on real, large-scale models. 1

Introduction
Attribution methods map each input feature of a model to a numeric score that quantiﬁes its relative importance towards the model’s output. At inference time, an analyst can view the attribution map alongside its corresponding input to interpret the data attributes that are most relevant to a given prediction. In recent years, this has become a popular way of explaining the behavior of Deep Neural
Networks (DNNs), particularly in domains such as medical imaging [5] and other safety-critical tasks [23] where the opacity of DNNs might otherwise prevent their adoption.
Recent work has shown that attribution methods are vulnerable to adversarial perturbations [12, 14, 16, 19, 47], showing that it is often possible to ﬁnd a small-norm set of feature changes that yield attribution maps with adversarially-chosen qualities while leaving the model’s output behavior intact.
For example, an attacker might introduce visually-imperceptible changes that cause the mapping generated for a medical image classiﬁer to focus attention on an irrelevant region. Such attacks have troubling implications for the continued adoption of attribution methods for explainability in high-stakes settings. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Contributions. In this paper, we characterize the vulnerability of attribution methods in terms of the geometry of the targeted model’s decision surface. Restricting our attention to attribution methods that primarily use information from the model’s gradients [38, 41, 45], we formalize attribution robustness as a local Lipschitz condition on the mapping, and show that certain smoothness criteria of the model ensure robust attributions (Sec. 3, Theorems 1 and 2).
Importantly, our analysis suggests that attacks are less likely to transfer across attribution methods when the model’s decision surface is smooth (Sec. 3.2), and our experimental results conﬁrm this on real data (Sec. 5.3). While this phenomenon is widely-known for adversarial examples [46], to our knowledge this is the ﬁrst systematic demonstration of it for attribution attacks.
As typical DNNs are unlikely to satisfy the criteria we present, we propose Smooth Surface Regular-ization (SSR) to impart models with robust gradient-based attributions (Sec. 4, Def. 7). Unlike prior regularization techniques that aim to mitigate attribution attacks [8], our approach does not require solving an expensive second-order inner objective during training, and our experiments show that it effectively promotes robust attribution without a signiﬁcant reduction in model accuracy (Sec. 5.2).
Finally, we propose a stochastic post-processing method as an alternative to SSR (Sec. 4), and validate its effectiveness experimentally on models of varying size and complexity, including pre-trained
ImageNet models (Sec. 5.1).
Taken together, our results demonstrate the central role that model geometry plays in attribution attacks, and that a variety of techniques that promote smooth geometry can effectively mitigate the problem on large-scale, state-of-the-art models. Proofs for all theorems and propositions in this paper are included in Appendix A and the implementation is available on: https://github.com/ zifanw/smoothed_geometry 2