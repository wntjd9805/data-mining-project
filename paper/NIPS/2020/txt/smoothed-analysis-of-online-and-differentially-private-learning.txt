Abstract
Practical and pervasive needs for robustness and privacy in algorithms have inspired the design of online adversarial and differentially private learning algorithms. The primary quantity that characterizes learnability in these settings is the Littlestone dimension of the class of hypotheses [Alon et al., 2019, Ben-David et al., 2009].
This characterization is often interpreted as an impossibility result because classes such as linear thresholds and neural networks have inﬁnite Littlestone dimension.
In this paper, we apply the framework of smoothed analysis [Spielman and Teng, 2004], in which adversarially chosen inputs are perturbed slightly by nature. We show that fundamentally stronger regret and error guarantees are possible with smoothed adversaries than with worst-case adversaries. In particular, we obtain regret and privacy error bounds that depend only on the VC dimension and the bracketing number of a hypothesis class, and on the magnitudes of the perturbations. 1

Introduction
Robustness to changes in the data and protecting the privacy of data are two of the main challenges faced by machine learning and have led to the design of online and differentially private learning algorithms. While ofﬂine PAC learnability is characterized by the ﬁniteness of VC dimension, online and differentially private learnability are both characterized by the ﬁniteness of the Littlestone dimension [Alon et al., 2019, Ben-David et al., 2009, Bun et al., 2020]. This latter characterization is often interpreted as an impossibility result for achieving robustness and privacy on worst-case instances, especially in classiﬁcation where even simple hypothesis classes such as 1-dimensional thresholds have constant VC dimension but inﬁnite Littlestone dimension.
Impossibility results for worst-case adversaries do not invalidate the original goals of robust and private learning with respect to practically relevant hypothesis classes; rather, they indicate that a new model is required to provide rigorous guidance on the design of online and differentially private learning algorithms. In this work, we go beyond worst-case analysis and design online learning algorithms and differentially private learning algorithms as good as their ofﬂine and non-private
PAC learning counterparts in a realistic semi-random model of data.
Inspired by smoothed analysis [Spielman and Teng, 2004], we introduce frameworks for online and differentially private learning in which adversarially chosen inputs are perturbed slightly by nature (reﬂecting, e.g., measurement errors or uncertainty). Equivalently, we consider an adversary restricted to choose an input distribution that is not overly concentrated, with the realized input then drawn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
from the adversary’s chosen distribution. Our goal is to design algorithms with good expected regret and error bounds, where the expectation is over nature’s perturbations (and any random coin ﬂips of the algorithm). Our positive results show, in a precise sense, that the known lower bounds for worst-case online and differentially private learnability are fundamentally brittle.
Our Model. Let us ﬁrst consider the standard online learning setup with an instance space X and a set H of binary hypotheses each mapping X to Y = {+1, −1}. Online learning is played over
T time steps, where at each step the learner picks a prediction function from a distribution and the adaptive adversary chooses a pair of (xt, yt) ∈ X × Y. The regret of an algorithm is the difference between the number of mistakes the algorithm makes and that of the best ﬁxed hypothesis in H. The basic goal in online learning is to obtain a regret of o(T ). In comparison, in differential privacy the data set B = {(x1, y1), . . . , (xn, yn)} is speciﬁed ahead of time. Our goal here is to design a randomized mechanism that with high probability ﬁnds a nearly optimal hypothesis in H on the set
B, while ensuring that the computation is differentially private. That is, changing a single element of
B does not signiﬁcantly alter the probability with which our mechanism selects an outcome. Similar to agnostic PAC learning, this can be done by ensuring that the error of each hypothesis h ∈ H on B (referred to as a query) is calculated accurately and privately.
We extend these two models to accommodate smoothed adversaries. We say that a distribution D over instance-label pairs is σ-smooth if its density function over the instance domain is pointwise bounded by at most 1/σ times that of the uniform distribution. In the online learning setting this means that at step t, the adversary chooses an arbitrary σ-smooth distribution Dt from which (xt, yt) ∼ Dt is drawn. In the differential privacy setting, we work with a database B for which the answers to the queries could have been produced by a σ-smooth distribution.
While we assume that 1/σ is slowly growing (subexponentiallly) in T , we can use values of σ that depend on the dimension of the space to account for the volume of high dimensional domains. Since our bounds are only logarithmic in 1/σ, they gracefully scale with the dimension of the space.
Note that this notion of bounded densities also incorporates smoothness models where worst-case in-stances are perturbed with small amount of noise, since convolution with noise produces distributions with bounded density. One advantage is that our smoothing model treats combinatorial domains (say
[n] or graphs on n vertices) and geometric domains (say D ⊆ Rd) in a uniﬁed manner and allows us to deliver results most meaningful to the analysis of learnability in presence of some smoothness without getting bogged down with domain-speciﬁc deﬁnitions of smoothness. Another advantage of our model — which is speciﬁcally important in machine learning — is that it naturally allows the adversary to have arbitrary correlations between the labels and the instances as long as the marginal on the instances is a “smooth" distribution. This can be handled by other models, albeit with more awkwardness in separating how an instance is generated by random shifts but its label is generated exactly by the adversary.
Why should smoothed analysis help in online learning? Consider the well-known lower bound for 1-dimensional thresholds over X = [0, 1], in which the learner may as well perform binary search and the adversary selects an instance within the uncertainty region of the learner that causes a mistake.
While the learner’s uncertainty region is halved each time step, the worst-case adversary can use ever-more precision to force the learner to make mistakes indeﬁnitely. On the other hand, a σ-smoothed adversary effectively has bounded precision. That is, once the width of the uncertainty region drops below σ, a smoothed adversary can no longer guarantee that the chosen instance lands in this region.
Similarly for differential privacy, there is a σ-smooth distribution that produces the same answers to the queries. Such a distribution has no more than α probability over an interval of width σα. So one can focus on computing the errors of the 1/(σα) hypotheses with discreized thresholds and learn a hypothesis of error at most α. Analogous observations have been made in prior works (Rakhlin et al. [2011], Cohen-Addad and Kanade [2017], Gupta and Roughgarden [2017]), although only for very speciﬁc settings (online learning of 1-dimensional thresholds, 1-dimensional piecewise constant functions, and parameterized greedy heuristics for the maximum weight independent set problem, respectively). Our work is the ﬁrst to demonstrate the breadth of the settings in which fundamentally stronger learnability guarantees are possible for smoothed adversaries than for worst-case adversaries. 2
Our Results and Contributions.
• Our main result concerns online learning with adaptive σ-smooth adversaries where Dt can depend on the history of the play, including the earlier realizations of xτ ∼ Dτ for τ < t. That is, xt and xt(cid:48) can be highly correlated. We show that regret against these powerful adversaries is bounded by
˜O((cid:112)T ln(N )), where N is the bracketing number of H with respect to the uniform distribution.1
Bracketing number is the size of an (cid:15)-cover of H with the additional property that hypotheses in the cover are pointwise approximations of those in H. We show that for many hypothesis classes, the bracketing number is nicely bounded as a function of the VC dimension. This leads to the regret bound of ˜O((cid:112)T VCDim(H) ln(1/σ)) for commonly used hypothesis classes in machine learning, such as halfspaces, polynomial threshold functions, and polytopes. In comparison, these hypothesis classes have inﬁnite Littlestone dimension and thus cannot be learned with regret o(T ) in the worst case [Ben-David et al., 2009].
From a technical perspective, we introduce a novel approach for bounding time-correlated non-independent stochastic processes over inﬁnite hypothesis classes using the notion of bracketing number. Furthermore, we introduce systematic approaches, such as high-dimensional linear embeddings and k-fold operations, for analyzing the bracketing number of complex hypothesis classes. We believe these techniques are of independent interest.
• For differentially private learning, we obtain an error bound of ˜O(cid:0) ln 8 (1/σ)(cid:112)VCDim(H)/n(cid:1); the key point is that this bound is independent of the size |X | of the domain and the size |H| of the hypothesis class. We obtain these bounds by modifying two commonly used mechanisms in differential privacy, the Multiplicative Weight Exponential Mechanism of Hardt et al. [2012] and the
SmallDB algorithm of Blum et al. [2008]. With worst-case adversaries, these algorithms achieve 4 (|X |)(cid:112)ln(|H|)/n) and ˜O( 3(cid:112)VCDim(H) ln(|X |)/n), respectively. only error bounds of ˜O(ln
Our results also improve over those in Hardt and Rothblum [2010] which concern a similar notion of smoothness and achieve an error bound of ˜O(ln 2 (1/σ)(cid:112)ln(|H|)/n). 1 1 3
Other