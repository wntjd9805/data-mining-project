Abstract
A latent bandit is a bandit problem where the learning agent knows reward distri-butions of arms conditioned on an unknown discrete latent state. The goal of the agent is to identify the latent state, after which it can act optimally. This setting is a natural midpoint between online and ofﬂine learning, where complex models can be learned ofﬂine and the agent identiﬁes the latent state online. This is of high practical relevance, for instance in recommender systems. In this work, we propose general algorithms for latent bandits, based on both upper conﬁdence bounds and Thompson sampling. The algorithms are contextual, and aware of model uncertainty and misspeciﬁcation. We provide a uniﬁed theoretical analysis of our algorithms, which have lower regret than classic bandit policies when the number of latent states is smaller than actions. A comprehensive empirical study showcases the advantages of our approach. 1

Introduction
Many online platforms, such as search engines and recommender systems, display results based on observed properties of the user and their query. However, the user’s behavior is often inﬂuenced by a latent state not explicitly revealed to the system. This might be user intent (e.g., reﬂecting a long-term task) in search, or short- and long-term preferences (e.g., reﬂecting topic interests) in a recommender. In each case, the unobserved latent state inﬂuences the user response to the displayed results, and thus the associated reward. A machine learning (ML) system, thus, should take steps to infer the latent state and tailor its results accordingly.
While many ML models use either heuristic features [1, 4] or recurrent models [33] to capture user history, explicit exploration for latent state identiﬁcation (i.e., reducing uncertainty regarding the true state) is less common in practice. In this paper, we study latent bandits, which model online interactions of the above type as follows. In each round, the learning agent observes context (e.g., query or user demographics), takes an action (e.g., a recommends), and observes its reward (e.g., user engagement with the recommendation). The reward depends stochastically on both the context and user latent state. As a result, it provides information about the unobserved latent state, which can be used to improve predictions in the future. We are interested in designing exploration policies that allow the agent to quickly maximize its per-round reward by resolving the relevant latent state uncertainty. Speciﬁcally, we want policies that have low n-round regret.
Our latent state structure allows an agent to quickly adapt its results to new users, or adapt to new user tasks or intents on a per-session basis. One example of that structure are clusters of users with similar item preferences [35]. In this example, a system should be able to identify which cluster a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
new user belongs to faster than learning all user preferences without any prior information. This would yield much better recommendations in the short-horizon, or cold-start, regime.
Fully online exploration (e.g., for personalization) involves learning a reward model, conditioned on the context and latent state, and generally requires massive amounts of interaction data. Fortunately, many platforms have such ofﬂine data (e.g., past user interactions) available, which can be used to construct both a reasonable latent state space and conditional reward models [23, 9]. We assume that such models are available and focus on a simpler online problem of latent state identiﬁcation. Prior works in this setting [25, 35] assumed that the true conditional reward models are given. Moreover, they focused on upper conﬁdence bound (UCB) designs, which are theoretically optimal but often perform poorly empirically. We provide a uniﬁed framework that combines ofﬂine-learned models with online exploration using both UCBs and Thompson sampling. Our algorithms are practical, analyzable, contextual, and robust to natural forms of model misspeciﬁcation.
We are the ﬁrst to propose algorithms for latent bandits that are aware of model uncertainty. Our paper is organized as follows. In Section 3, we propose novel practical algorithms based on UCBs and Thompson sampling. Based on a connection between UCBs and posterior sampling [28], we derive near-optimal bounds on the Bayes regret of our algorithms in Section 4. Finally, in Section 5, we demonstrate their effectiveness in synthetic simulations and on a large-scale real-world dataset. 2 Problem Formulation
We adopt the following notation. Random variables are capitalized. The set of actions is A, the set of contexts is X , and the set of latent states is S, with |S| (cid:28) |A|.
We study a latent bandit problem, where the learning agent interacts with an environment over n rounds as follows. In round t ∈ [n], the agent observes context Xt ∈ X , takes action At ∈ A, and observes reward Rt ∈ R. The random variable Rt depends on the context Xt, action At, and latent state s ∈ S, where s is ﬁxed but unknown.1 The observation history of the learning agent up to round t is a vector Ht = (X1, A1, R1, . . . , Xt−1, At−1, Rt−1). The policy of the agent maps Ht and Xt to the choice of action At.
Now we state our assumptions on how rewards and contexts are generated. The rewards are drawn i.i.d. from a conditional reward distribution P (· | A, X, s; θ), which is parameterized by a vector
θ ∈ Θ, where Θ is a set of feasible model parameters. We use µ(a, x, s; θ) = ER∼P (·|a,x,s;θ) [R] to denote the mean reward of action a in context x and latent state s under model θ. We denote the true (unknown) latent state by s∗ and true model parameters by θ∗. We assume that θ∗ can be estimated ofﬂine. We also assume that the rewards are σ2-sub-Gaussian, that is
ER∼P (·|a,x,s;θ∗) [exp(λ(R − µ(a, x, s; θ∗)))] ≤ exp(σ2λ2/2) for all a, x, s, and λ > 0. The contexts can be generated by an arbitrary process independent of the agent’s actions and the mean reward µ(a, x, s; θ) can be any complex function of θ.
The performance of the agent is measured by regret. Let At,∗ = arg maxa∈A µ(a, Xt, s∗; θ∗) be the optimal action for latent state s∗ ∈ S and model θ∗ ∈ Θ. Then the expected n-round regret is
R(n; s∗, θ∗) = E (cid:34) n (cid:88) t=1
µ(At,∗, Xt, s∗; θ∗) − µ(At, Xt, s∗; θ∗)
. (1) (cid:35)
While a ﬁxed-state regret is useful, we are often more concerned with the average performance over a range of states (e.g., multiple users and multiple sessions with the same user). Thus we also consider the Bayes regret, where we take an expectation over a random latent state and model. Suppose that
S∗ and θ∗ are drawn from some prior P1. Then the n-round Bayes regret is
BR(n) = E [R(n; S∗, θ∗)] = E
µ(At,∗, Xt, S∗; θ∗) − µ(At, Xt, S∗; θ∗)
. (2) (cid:35) (cid:34) n (cid:88) t=1
Note that S∗ and θ∗ in the deﬁnition of At,∗ = arg maxa∈A µ(a, Xt, S∗; θ∗) are random now. 1The latent state s can be viewed as a user’s current task or preferences, and is ﬁxed over all n rounds. 2
Algorithm 1 mUCB 1: Input: Model parameters (cid:98)θ 2: for t ← 1, 2, . . . do 3:
Deﬁne Nt(s) ← (cid:80)t−1 (cid:96)=1 1{B(cid:96) = s} and
Gt(s) ← t−1 (cid:88) (cid:96)=1 1{B(cid:96) = s} ((cid:98)µ(A(cid:96), X(cid:96), s) − R(cid:96)) (3) 4: 5:
Set of consistent latent states Ct ←
Select Bt, At ← arg maxs∈Ct,a∈A (cid:98)µ(a, Xt, s) (cid:110) s ∈ S : Gt(s) ≤ σ(cid:112)6Nt(s) log n (cid:111) 3 Algorithms
In this section, we develop both UCB and Thompson sampling (TS) algorithms that leverage an arbitrarily complex environment model, generally learned ofﬂine, to expedite online exploration. As discussed earlier, such ofﬂine models can be readily learned given large amounts of ofﬂine interaction data available in many interactive systems. In each subsection below, we specify a particular form of the ofﬂine-learned model, and develop a corresponding online algorithm. It is important to note that given an environment model, an optimal solution is to compute and maximize the Gittins index over actions [15]. However, this is often computationally intractable and does not generalize to complex latent variable models. We want our methods to be practical, and thus only consider myopic policies that can be efﬁciently implemented online. 3.1 UCB with Perfect Model (mUCB)
We ﬁrst design a UCB-like algorithm with learned model (cid:98)θ ∈ Θ. Let (cid:98)µ(a, x, s) = µ(a, x, s; (cid:98)θ) be the mean reward under model (cid:98)θ and µ(a, x, s) = µ(a, x, s; θ∗) be the true reward. We initially assume that the learned model is accurate, that is we are given (cid:98)θ = θ∗ as an input.
The key idea in UCB algorithms is to compute a high-probability upper conﬁdence bound Ut(a) on the mean reward of each action a in any round t, where the Ut is some function of history [8]. Then the algorithms take action At = arg maxa∈A Ut(a). Our model-based UCB algorithm, which we call mUCB, works very similarly. The pseudocode of mUCB is in Algorithm 1. The algorithm works as follows. In round t, it maintains a set of latent states Ct that are consistent with the observed rewards thus far. Then it chooses a speciﬁc believed latent state Bt from the consistent set Ct and takes action At with the maximum expected reward in that state, (Bt, At) = arg maxs∈Ct,a∈A (cid:98)µ(a, Xt, s).
Thus the UCB of action a is Ut(a) = arg maxs∈Ct (cid:98)µ(a, Xt, s). mUCB tracks two key quantities: the number of times that state s is selected up to round t, Nt(s); and the “gap” between the expected and realized rewards in state s up to round t, Gt(s). If Gt(s) is high, mUCB marks state s as inconsistent and does not consider it in round t. Note that the gap is deﬁned over latent states rather than actions, and with respect to realized rewards rather than expected rewards. 3.2 UCB with Misspeciﬁed Model (mmUCB)
Now we extend mUCB to misspeciﬁed models, where we are given (cid:98)θ (cid:54)= θ∗ as an input. We consider the following worst-case high-probability deﬁnition of model misspeciﬁcation: there exist ε, δ > 0 such that |(cid:98)µ(a, x, s) − µ(a, x, s)| ≤ ε holds with probability at least 1 − δ jointly for all a, x, and s.
Guarantees of this form, for example, exist for spectral methods for latent variable models, where ε and δ depend on the size of the ofﬂine dataset [5].
We modify mUCB to be robust to this type of model error, which gives rise to a new algorithm mmUCB.
The only change to mUCB is that Gt(s) is replaced with a high-probability lower bound
Gt(s) = t−1 (cid:88) (cid:96)=1 1{B(cid:96) = s} ((cid:98)µ(A(cid:96), X(cid:96), s) − ε − R(cid:96)) . (4) 3
Algorithm 2 mTS
Algorithm 3 mmTS 1: Input: 2: Model parameters (cid:98)θ 3:
Prior over latent states P1(s) 4: for t ← 1, 2, . . . do
Deﬁne 5: 1: Input: 2:
Prior over latent states and model parame-ters P1(s, θ) 3: for t ← 1, 2, . . . do
Deﬁne 4: (cid:96)=1 P (R(cid:96) | A(cid:96), X(cid:96), s; (cid:98)θ)
Pt(s) ∝ P1(s) (cid:81)t−1
Sample Bt ∼ Pt
Select At ← arg maxa∈A (cid:98)µ(a, Xt, Bt) 6: 7:
Pt(s, θ) ∝ P1(s, θ) (cid:81)t−1 (cid:96)=1 P (R(cid:96) | A(cid:96), X(cid:96), s; θ) 5: 6:
Sample Bt, (cid:98)θ ∼ Pt
Select At ← arg maxa∈A (cid:98)µ(a, Xt, Bt)
This allows mmUCB to be conservative when identifying inconsistent latent states, so that s∗ ∈ Ct remains to hold with a high probability. Just as importantly, it is also useful for deriving worst-case regret bounds, both for UCB algorithms and TS. 3.3 Thompson Sampling with Perfect Model (mTS)
Our UCB algorithms are designed for the worst case. Now we adopt a more relaxed design, where the latent state is random, and the model is ﬁxed and known. That is, we are given (cid:98)θ = θ∗ and a prior distribution over latent states P1 as inputs.
Our solution is a variant of Thompson sampling [32, 10, 28]. The key idea in TS is to sample actions according to their posterior probability of being optimal, conditioned on the history of the agent. In our case, the optimal action in round t is At,∗ = arg maxa∈A µ(a, Xt, S∗; θ∗), and is random both due to the observed context Xt and unknown latent state S∗. Therefore, TS should take action a with probability P (At = a | Xt, Ht) = P (At,∗ = a | Xt, Ht). An advantage of TS over UCB algorithms is that it obviates the need to design UCBs, which are often loose. As a result, UCB algorithms are often conservative in practice and TS typically offers better empirical performance [10].
Our model-based TS algorithm, which we call mTS, is presented in Algorithm 2. The algorithm works as follows. In round t, it maintains a posterior probability, Pt(s) = P (S∗ = s | Ht), that each latent state s is optimal. Then it samples the latent state from its posterior distribution, Bt ∼ Pt, and takes action At = maxa∈A (cid:98)µ(a, Xt, Bt). For any ﬁxed s, Pt(s) ∝ P1(s) (cid:81)t−1 (cid:96)=1 P (R(cid:96) | A(cid:96), X(cid:96), s; (cid:98)θ). As a result, Pt can be updated incrementally in the standard Bayesian ﬁltering fashion [30]. Note that unlike mUCB, mTS needs to know the conditional reward distribution P (· | a, x, s; (cid:98)θ) to update Pt(s). 3.4 Thompson Sampling with Misspeciﬁed Model (mmTS)
Now we extend mTS to misspeciﬁed models. However, instead of considering a worst-case estimate of θ∗, as in mmUCB, we assume that θ∗ ∼ P1 and that the learning agent knows P1. This is motivated by prior literature on modeling epistemic uncertainty [11]. In practice, learning a distribution over parameters is intractable for complex models; but approximate inference can be always performed, for instance by an ensemble of bootstrapped models [11].
Our TS algorithm for misspeciﬁed models, which we call mmTS, is presented in Algorithm 3. The algorithm seamlessly integrates model uncertainty into mTS as follows. In round t, the latent state
Bt and estimated model parameters (cid:98)θ are sampled from their joint posterior, and then an action is taken to maximize At = maxa∈A (cid:98)µ(a, Xt, Bt). In general, sequential Monte Carlo methods [12] can be used for approximate posterior sampling. However, when the model prior is conjugate to the likelihood, the posterior has a closed form and can be sampled from as follows. Since S is ﬁnite, we can tractably sample from the joint posterior by ﬁrst sampling latent state Bt from its marginal posterior and then (cid:98)θ conditioned on Bt. For exponential family distributions, the posterior parameters can also be updated online and efﬁciently. We provide more details in Appendix A, and a pseudocode for Gaussians in Appendix B. 4
4 Regret Analysis
Maillard and Mannor [25] derived gap-dependent regret bounds for UCB algorithms in latent bandits under the assumption that the true model is known and arms are independent. We provide a uniﬁed analysis that extends their results to include context, model misspeciﬁcation, and TS algorithms. 4.1 Regret Decomposition
UCB algorithms explore using upper conﬁdence bounds, while TS samples from the posterior. Russo and Van Roy [28] related these two designs with a uniﬁed regret decomposition. In our problems, this is reﬂected as follows. Let s∗ be the true latent state. Then the regret of our UCB algorithms in round t decomposes as
µ(At,∗, Xt, s∗) − µ(At, Xt, s∗) = µ(At,∗, Xt, s∗) − Ut(At) + Ut(At) − µ(At, Xt, s∗)
≤ [µ(At,∗, Xt, s∗) − Ut(At,∗)] + [Ut(At) − µ(At, Xt, s∗)] , where the inequality holds by the deﬁnition of At. A similar inequality without latent states appears in prior work [28]. This yields the following regret decomposition
R(n; s∗, θ∗) ≤ E
µ(At,∗, Xt, s∗) − Ut(At,∗)
+ E (cid:35) (cid:34) n (cid:88) t=1 (cid:34) n (cid:88) t=1
Ut(At) − µ(At, Xt, s∗)
. (5) (cid:35)
An analogous decomposition exists for the Bayes regret of our TS algorithms. Speciﬁcally, for any
TS algorithm and function Ut of history, we have
BR(n) = E (cid:34) n (cid:88) t=1
µ(At,∗, Xt, S∗; θ∗) − Ut(At,∗)
+ E (cid:35) (cid:34) n (cid:88) t=1
Ut(At) − µ(At, Xt, S∗; θ∗)
. (6) (cid:35)
The proof uses the fact that E [Ut(At,∗) | Xt, Ht] = E [Ut(At) | Xt, Ht] holds for any Ht and Xt from the design of TS. Thus Ut can be an upper conﬁdence bound in a UCB algorithm.
Though the UCBs Ut are not used by TS, they can be used to analyze it, due to the similarity of (5) and (6). Thus regret bounds for UCB algorithms can be translated to Bayes regret bounds for TS.
There are two caveats though. First, since actions in TS do not maximize Ut, the regret bound must be proved using a worst-case argument over suboptimal actions. Second, because the Bayes regret is in expectation over problem instances, the resulting regret bounds are problem-independent, also known as gap-free. 4.2 Key Steps in Our Proofs
Full proofs of our regret bounds are in Appendix C. All proofs follow the same outline, the key steps of which are outlined below. To ease the exposition, we assume that the suboptimality gap of any action is bounded by 1.
Step 1: Concentration of realized rewards at their means. We ﬁrst show that the total observed reward does not deviate too much from its expectation, under any believed latent state s. Formally, we show that P
= O(n−2) holds for any round t and state s. Since we consider the contextual case, which requires joint estimation over dependent arms, we use martingales and Azuma’s inequality. (cid:12) (cid:12) ≥ σ(cid:112)6Nt(s) log n 1{B(cid:96) = s} (µ(A(cid:96), X(cid:96), s∗) − R(cid:96)) (cid:12) (cid:80)t−1 (cid:96)=1 (cid:16)(cid:12) (cid:12) (cid:12) (cid:17)
Step 2: s∗ ∈ Ct in each round t with a high probability. This follows from the deﬁnition of Ct and the concentration argument in Step 1 for s = s∗. Then, in any round t where s∗ ∈ Ct, we can use that Ut(a) ≥ µ(a, Xt, s∗) for any action a in mUCB, and Ut(a) ≥ µ(a, Xt, s∗) − ε in mmUCB.
Step 3: Upper bound on the UCB regret. We prove this by bounding each term in (5) separately.
The ﬁrst term is at most 0 with a high probability by Step 2. The second term is a sum of conﬁdence widths, the differences between Ut and the mean reward in round t. We partition it by the chosen latent state in each round. For each latent state s, we introduce realized rewards Rt and get n (cid:88) t=1 1{Bt = s} (Ut(At) − µ(At, Xt, s∗)) ≤ Gn(s) + 1 + n (cid:88) t=1 1{Bt = s} (Rt − µ(At, Xt, s∗)) . 5
The key step in proving the above bound is that G(cid:96)(s) = Gn(s) holds in the last round (cid:96) where state s is chosen, where (cid:80)(cid:96)−1 1{Bt = s} (Ut(At) − Rt) ≤ G(cid:96)(s) holds. This relation also implies that t=1
Gn(s) ≤ σ(cid:112)6Nn(s) log n. The other term is bounded by Step 1, which gives a total upper bound of 2σ(cid:112)6Nn(s) log n. Finally, we apply the Cauchy-Schwarz inequality to combine the bounds for individual latent states.
Step 4: Upper bound on the TS regret. We use the fact that the regret decomposition for Bayes regret in (6) is similar to that for the UCB regret in (5). As mentioned in Section 4.1, as long as our
UCB analysis in Step 3 is worst-case over all possible sequences of actions and gap-free, the UCB regret bound transfers to a Bayes regret bound for TS. 4.3 Regret Bounds
Our ﬁrst result is an upper bound on the n-round regret of mUCB. This result differs from Maillard and Mannor [25] in two respects: our bound is gap-free and accounts for context.
Theorem 1. Assume that (cid:98)θ = θ∗. Then, for any s∗ ∈ S and θ∗ ∈ Θ, the n-round regret of mUCB is bounded as R(n; s∗, θ∗) ≤ 2σ(cid:112)6|S|n log n + 3|S|.
Kn) [7]. So our upper bound is
A gap-free lower bound on the regret in a K-armed bandit is Ω( optimal up to log factors, when substituting actions A with latent states S. The bound can be much lower when |S| (cid:28) K. It also holds for arbitrary reward models and is contextual. From Step 4 of the proof outline, we also have that the Bayes regret of mTS is bounded.
Corollary 1. Assume that (cid:98)θ = θ∗. Then, for S∗ ∼ P1 and any θ∗ ∈ Θ, the n-round Bayes regret of mTS is bounded as BR(n) ≤ 2σ(cid:112)6|S|n log n + 3|S|.
√
Our next results apply to misspeciﬁed models. We assume that (cid:98)θ is estimated by some black-box method. For mmUCB, our bound depends on a high-probability maximum error ε induced by (cid:98)θ.
Theorem 2. Let P
ε, δ > 0. Then, for any s∗ ∈ S and θ∗ ∈ Θ, the n-round regret of mmUCB is bounded as
∀a ∈ A, x ∈ X , s ∈ S : |µ(a, x, s; (cid:98)θ) − µ(a, x, s; θ∗)| ≤ ε
≥ 1 − δ for some (cid:16) (cid:17)
R(n; s∗, θ∗) ≤ 2σ(cid:112)6|S|n log n + 2εn + δn + 3|S| .
The proof of Theorem 2 follows our outline in Section 4.2. Steps 1–2 remain unchanged, but Step 3 changes to account for the error due to model misspeciﬁcation. The linear dependence on error ε and probability δ is unavoidable in the worst case, speciﬁcally when ε is larger than the suboptimality gap. Nevertheless, some ofﬂine learning methods, such as spectral methods for latent variable models
[5], allow ε and δ to be arbitrarily small as the size of the ofﬂine dataset grows. So our bound can be sublinear in n.
For mmTS, we assume that a prior distribution over model parameters is known. The key step in the proof is to introduce ¯µ(a, x, s) = (cid:82)
θ µ(a, x, s, θ)P1(θ)dθ, the conditional mean reward marginalized over the prior. Using this quantity, we can obtain the following Bayes regret bound.
Corollary 2. Let θ∗ ∼ P1 and P (∀a ∈ A, x ∈ X , s ∈ S : |¯µ(a, x, s) − µ(a, x, s; θ∗)| ≤ ε) ≥ 1 − δ for some ε, δ > 0. Then, for S∗, θ∗ ∼ P1, the n-round Bayes regret of mmTS is bounded as
BR(n) ≤ 2σ(cid:112)6|S|n log n + 2εn + δn + 3|S| .
The proof of Corollary 2 relies on a variant of mmUCB where (cid:98)µ(a, x, s) is replaced with ¯µ(a, x, s).
Note that unlike in mmUCB, the linear dependence of ε is conservative, as mmTS updates its model posteriors and eventually concentrates. We leave the derivation of a tighter bound for future work.
We can relate the error ε and its probability δ using the tails of the conditional reward distributions.
In particular, let µ(a, x, s; θ) − ¯µ(a, x, s) be v2-sub-Gaussian in θ ∼ P1 for any a, x, and s. Then for any δ ∈ [0, 1], ε = O(v(cid:112)2 log(K|X ||S|/δ)) satisﬁes the conditions on ε and δ in Corollary 2. 5 Experiments
In this section, we evaluate our algorithms on both synthetic and real-world datasets. We compare the following methods: (i) UCB: UCB1/LinUCB with no ofﬂine model [8, 1]; (ii) TS: TS/LinTS with 6
Figure 1: Left: Mean reward and standard error for low model noise (σ0 = 0.05). Middle/Right: Mean/worst-case reward and standard error for high model noise (σ0 = 0.2). no ofﬂine model [3, 4]; (iii) Exp4: Exp4 where models are experts [7] (iv) mUCB, mmUCB: our proposed UCB algorithms mUCB/mmUCB; (v) mTS, mmTS: our proposed TS algorithms mTS/mmTS.
In contrast to our methods, the UCB and TS baselines do not use an ofﬂine learned model. Exp4 uses models as experts, where each expert pulls the best arm given context and its latent state. Since we are interested in “fast personalization”, we experiment with short horizons of up to 1000 rounds. 5.1 Synthetic Experiments
We ﬁrst experiment with synthetic non-contextual bandits where A = [10] and S = [5]. The mean rewards are sampled uniformly at random as µ(a, s) ∼ Uniform(0, 1) for each a ∈ A, s ∈ S. Using rejection sampling, we constrain the suboptimality gap of all actions in each state s to be at least 0.1, to ensure statistically signiﬁcant comparisons at short horizons. The rewards are drawn i.i.d. from P (· | a, s) = N (µ(a, s), σ2) with σ = 0.5. We evaluate each algorithm on 500 independent runs, with a uniformly sampled latent state in each run, and report the average reward over time. We analyze the effect of model misspeciﬁcation by perturbing the mean rewards with various degrees of noise: for noise σ0 > 0, the estimated means are sampled as (cid:98)µ(a, s) ∼ N (µ(a, s), σ2 0) for each action a and latent state s.
The left plot in Figure 1 shows the average reward over time for low model noise, σ0 = 0.05. In this setting, our algorithms mUCB and mTS perform better than the baselines UCB1 and TS. In the middle plot, we increase the noise to σ0 = 0.2. Neither mUCB nor mTS accounts for modeling errors, and thus their performance degrades. On the other hand, the uncertainty-aware mmTS outperforms mTS.
However, mmUCB (not reported to reduce clutter) performs the same as mUCB. This is likely because of the conservative nature of UCBs. Despite having similar worst-case regret guarantees, Exp4 is tailored to adversarial bandits instead of the stochastic ones. Therefore, it is outperformed by mUCB and mTS with the same model.
The right plot in Figure 1 shows 10% of the worst runs from the middle plot, as measured by the average reward in the last round. This is a measure of a “worst-case” performance. Both UCB1 and
TS are unaffected by model misspeciﬁcation, and outperform mUCB and mTS. However, mmTS still performs best due to adapting the misspeciﬁed model to online data. This shows that employing uncertainty-awareness makes model-based algorithms much more robust to model misspeciﬁcation and learning error. 5.2 MovieLens Experiments
We also assess the performance of our algorithms on the MovieLens 1M dataset [17], a large-scale collaborative ﬁltering dataset where 6040 users rate 3883 movies. Each movie has a set of genres.
We ﬁlter the dataset to include only users who rated at least 200 movies, and movies rated by at least 200 users; resulting in 1353 users and 1124 movies.
We randomly select 50% of all ratings as our training set and use the remaining 50% as the test set; resulting in sparse rating matrices Mtrain and Mtest. We complete each matrix using least-squares matrix completion [29] with rank 20. This rank is high enough to yield a low prediction error, and yet small enough to avoid overﬁtting. The learned latent factors are Mtrain = (cid:98)U (cid:98)V T and Mtest = U V T .
User i and movie j correspond to rows Ui and Vj, respectively, in the matrix factors.
We deﬁne a latent contextual bandit instance with A = [20] and S = [5] as follows. Using k-means clustering on the rows of U , we cluster users into 5 clusters, where 5 is the largest value that does not yield empty clusters. First, a user i is sampled uniformly at random. In each round, 20 genres 7
Figure 2: Mean/worst-case rating and standard error on the MovieLens 1M dataset. and then a movie for each genre are uniformly sampled, creating a set of diverse movies. Context
Xt ∈ R20×20 is a matrix where each row is a training latent factor of one sampled movie, that is (cid:98)Vj for movie j. The agent selects one movies from Xt and observes its reward. The reward distribution for user i and movie j is N (U T i Vj, 0.5), and its mean is the product of the corresponding user and item factors on the test set. We evaluate on 500 users.
Our “ofﬂine” model is a Gaussian mixture that is learned in the same way as the true model, except on the training set. It has 5 clusters of users derived from k-means clustering on the rows of (cid:98)U . For each latent state, the prior is a Gaussian with the corresponding cluster mean and covariance. The context in LinUCB and LinTS is Xt, the same as in our algorithms, and they only learn the user latent factor. This is more information than in low-rank bandit algorithms [24], which learn both the user and movie representations, and thus perform poorly in short horizons that we consider.
The left plot in Figure 2 shows mean ratings and standard errors of 6 algorithms (as earlier, mmUCB performs similarly to mUCB and is not shown). mUCB and mTS clearly adapt and “personalize” to users faster than LinUCB and LinTS, and converge to better policies than Exp4. Both mUCB and mTS are affected by model misspeciﬁcation. In comparison, mmTS handles model uncertainty and converges to the best policies. The right plot in Figure 2 shows the average results for the worst 10% of users.
Again, mmTS dramatically outperforms mTS in the worst case. 6