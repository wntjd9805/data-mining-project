Abstract
Video object segmentation (VOS) describes the task of segmenting a set of objects in each frame of a video. In the semi-supervised setting, the ﬁrst mask of each object is provided at test time. Following the one-shot principle, ﬁne-tuning VOS methods train a segmentation model separately on each given object mask. However, recently the VOS community has deemed such a test time optimization and its impact on the test runtime as unfeasible. To mitigate the inefﬁciencies of previous
ﬁne-tuning approaches, we present efﬁcient One-Shot Video Object Segmentation (e-OSVOS). In contrast to most VOS approaches, e-OSVOS decouples the object detection task and predicts only local segmentation masks by applying a modiﬁed version of Mask R-CNN. The one-shot test runtime and performance are optimized without a laborious and handcrafted hyperparameter search. To this end, we meta learn the model initialization and learning rates for the test time optimization. To achieve an optimal learning behavior, we predict individual learning rates at a neuron level. Furthermore, we apply an online adaptation to address the common performance degradation throughout a sequence by continuously ﬁne-tuning the model on previous mask predictions supported by a frame-to-frame bounding box propagation. e-OSVOS provides state-of-the-art results on DAVIS 2016, DAVIS 2017 and YouTube-VOS for one-shot ﬁne-tuning methods while reducing the test runtime substantially.
Code is available at https://github.com/dvl-tum/e-osvos. 1

Introduction
Video object segmentation (VOS) describes a two class (foreground-background) pixel-level classiﬁ-cation task on each frame of a given video sequence. Multiple objects are discriminated by predicting individual foreground-background pixel masks. In this work, we address a variant of VOS which is semi-supervised at test time. To this end, the ground truth foreground-background segmentation mask of the ﬁrst frame is provided for each object. Machine learning methods that tackle semi-supervised
VOS are categorized by their utilization of the provided object ground truth masks.
We focus on ﬁne-tuning methods [6, 22, 39, 20, 34], which exploit the transfer learning capabilities of neural networks and follow a multi-step training procedure: (i) pre-training steps: learn general image and segmentation features from training the model on images and video sequences , and (ii)
ﬁne-tuning: one-shot test time optimization which enables the model to learn foreground-background characteristics speciﬁc to each object and video sequence. While elegant through their simplicity,
ﬁne-tuning methods face important shortcomings: (i) pre-training is ﬁxed and not optimized for the subsequent ﬁne-tuning, (ii) the hyperparameters of the test time optimization are often excessively handcrafted and fail to generalize between datasets. The common existing ﬁne-tuning setups [6, 20] are inefﬁcient and suffer from high test runtimes with as many as 1000 training iterations per segmented object. As a consequence, recent methods refrain from such an optimization at test time and 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Performance versus runtime comparison of modern video object segmentation (VOS) approaches on the DAVIS 2017 validation set. We only show methods with publicly available runtime information. Our e-OSVOS approach demonstrates the relevance of ﬁne-tuning for VOS and its inherent ﬂexibility as we apply the same meta learned optimization for a varying number of iterations and with online adaptation (OnA). instead opt for solutions such as template matching [7, 13] and mask propagation [8, 9, 24, 25, 35, 44] for semi-supervised VOS.
In this work, we revisit the concept of one-shot ﬁne-tuning for VOS, and show how to leverage the power of meta learning to overcome the aforementioned issues. To this end, we propose three key design choices which make one-shot ﬁne-tuning for VOS efﬁcient again:
Learning the Model Initialization The common pre-training [6, 22, 20, 34] yields a segmentation model not speciﬁcally optimized for the subsequent ﬁne-tuning task and requires an unlearning of potential false positive objects. Therefore, we propose to meta learn the pre-training step, i.e., we learn the best initialization of the segmentation model for a subsequent ﬁne-tuning to any object.
Learning Neuron-Level Learning Rates We replace the laborious and handcrafted hyperparameter search from [6, 22, 20, 34] and additionally optimize learning rates for each neuron of the model. In contrast to a single learning rate for the entire model [1] or millions for all of its parameters [39], this allows for an ideal balance between individual learning behavior and additional trainable parameters.
Optimization of Model with Object Detection To account for the foreground-background pixel imbalance and the challenging object discrimination by individual ﬁne-tuning, previous ﬁne-tuning methods [22, 20, 34] rely on additional mask proposal or bounding box prediction methods. In contrast, we directly ﬁne-tune Mask R-CNN [11] with its separate end-to-end trainable object detection head which limits mask predictions to local object bounding boxes.
This leads to our efﬁcient one-shot video object segmentation (e-OSVOS) approach, which achieves state-of-the-art segmentation performance on the DAVIS-2016, DAVIS-2017 and YouTube-VOS benchmarks compared to all previous ﬁne-tuning methods, at a much lower test runtime, see Figure 1.
Overall, our results combat the negative preconceptions with respect to ﬁne-tuning as a principle for semi-supervised VOS, and are intended to motivate future research in this direction. 1.1