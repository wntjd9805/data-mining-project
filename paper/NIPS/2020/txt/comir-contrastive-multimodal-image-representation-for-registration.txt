Abstract
We propose contrastive coding to learn shared, dense image representations, re-ferred to as CoMIRs (Contrastive Multimodal Image Representations). CoMIRs enable the registration of multimodal images where existing registration methods often fail due to a lack of sufﬁciently similar image structures. CoMIRs reduce the multimodal registration problem to a monomodal one, in which general intensity-based, as well as feature-based, registration algorithms can be applied. The method involves training one neural network per modality on aligned images, using a contrastive loss based on noise-contrastive estimation (InfoNCE). Unlike other contrastive coding methods, used for, e.g., classiﬁcation, our approach generates image-like representations that contain the information shared between modalities.
We introduce a novel, hyperparameter-free modiﬁcation to InfoNCE, to enforce rotational equivariance of the learnt representations, a property essential to the registration task. We assess the extent of achieved rotational equivariance and the stability of the representations with respect to weight initialization, training set, and hyperparameter settings, on a remote sensing dataset of RGB and near-infrared images. We evaluate the learnt representations through registration of a biomedical dataset of bright-ﬁeld and second-harmonic generation microscopy images; two modalities with very little apparent correlation. The proposed approach based on
CoMIRs signiﬁcantly outperforms registration of representations created by GAN-based image-to-image translation, as well as a state-of-the-art, application-speciﬁc method which takes additional knowledge about the data into account. Code is available at: https://github.com/MIDA-group/CoMIR.
∗Authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Registration of images of different modalities (here bright-ﬁeld (BF) and second-harmonic generation imaging (SHG)) may be very challenging. CoMIR successfully estimates the shared representation of these images, and enables their successful registration by monomodal approaches.
1

Introduction
Multimodal images refer to images captured with multiple types of sensors, where each sensor outputs information not fully provided by the other sensors. Multimodal image fusion is the process of combining information from multiple imaging modalities. It allows downstream tasks to exploit complementary information as well as relationships between modalities. In order to perform image fusion, the images need to be aligned, either by joint acquisition, or by manual or automated registration. To enable registration, common structures between the modalities need to be found, reﬂected by the shared or mutual information (MI); this is in general a very difﬁcult task. Our method,
Contrastive Multimodal Image Representation for registration (CoMIR) reduces the challenging problem of multimodal registration to a simpler, monomodal one, as shown in Fig. 1. While image-to-image translation aims to predict cross-modality, i.e. to predict one modality given the other modality as input, our method directly learns representations called CoMIRs, relying on the MI by maximizing the Mutual Information Noise-Contrastive Estimation (InfoNCE) [20, 25]. InfoNCE can, under certain assumptions, act as a lower bound to MI, and has been successfully applied in connection with contrastive coding in other tasks, such as classiﬁcation and segmentation. Although contrastive losses (CL) are often used for representation learning [1, 6, 22, 24, 25, 43, 46, 47, 49, 53, 62], to the best of our knowledge, the method presented here is the ﬁrst to produce dense representations for very different imaging modalities which can be utilized for monomodal registration by existing feature- or intensity-based registration methods. InfoNCE has been previously used to learn embeddings used in classﬁcation and segmentation tasks in which the resulting subspace is required to feature properties such as separability between classes. Furthermore, registration requires representations which are both translation and rotation equivariant, as opposed to classiﬁcation, for which invariance is required.
Our proposed method produces image-like, contrastive representations that possess the necessary equivariant properties to ﬁnd a transformation between the original inputs.
The contributions of this paper are the following: We show that (1) contrastive learning of aligned pairs of images can produce representations that reduce complex (or even non-feasible), rigid multimodal registration tasks to much simpler monomodal registration; (2) the proposed CoMIRs are rotation equivariant due to a modiﬁcation of a commonly used CL. This modiﬁcation is model-independent and does not require any architecture modiﬁcations, nor any additional hyperparameter tuning. Although the method is supervised, thanks to our sophisticated scheme for generating training patches, as little as one image pair can be sufﬁcient to generate CoMIRs (depending on the nature of the imaging data). 2