Abstract
It is believed that a model-based approach for reinforcement learning (RL) is the key to reduce sample complexity. However, the understanding of the sample optimality of model-based RL is still largely missing, even for the linear case. This work considers sample complexity of ﬁnding an (cid:15)-optimal policy in a Markov decision process (MDP) that admits a linear additive feature representation, given only access to a generative model. We solve this problem via a plug-in solver approach, which builds an empirical model and plans in this empirical model via an arbitrary plug-in solver. We prove that under the anchor-state assumption, which implies implicit non-negativity in the feature space, the minimax sample complexity of ﬁnding an (cid:15)-optimal policy in a γ-discounted MDP is O(K/(1 − γ)3(cid:15)2), which only depends on the dimensionality K of the feature space and has no dependence on the state or action space. We further extend our results to a relaxed setting where anchor-states may not exist and show that a plug-in approach can be sample efﬁcient as well, providing a ﬂexible approach to design model-based algorithms for RL. 1

Introduction
Reinforcement learning (RL) [Sutton and Barto, 2018] is about learning to make optimal decisions in an unknown environment. It has been believed to be one of the key approaches to reach artiﬁcial general intelligence. In recent years, RL achieves phenomenal empirical successes in many real-world applications, e.g., game-AI [Vinyals et al., 2017], robot control [Duan et al., 2016], health-care [Li et al., 2018]. Most of these successful applications are based on a model-free approach, where the agent directly learns the value function of the environment. Despite superior performance, these algorithms usually take tremendous amount of samples. E.g. a typical model-free Atari-game agent takes about several hours of training data in order to perform well [Mnih et al., 2013]. Reducing sample complexity becomes a critical research topic of in RL.
It is well believed that model-based RL, where the agent learns the model of the environment and then performs planning in the model, is signiﬁcantly more sample efﬁcient than model-free RL.
Recent empirical advances also justify such a belief (e.g. [Kaiser et al., 2019, Wang et al., 2019]).
However, the understanding of model-based RL is still far from complete. E.g., how to deal with issues like model-bias and/or error compounding due to long horizon, in model-based RL is still an open question [Jiang et al., 2015], especially with the presence of a function approximator (e.g. a neural network) on the model. In order to get a better understanding on these issues, we target on the sample complexity question of model-based RL from a very basic setting: feature-based
RL. In feature-based RL, we are given a hand-crafted or learned low-dimensional feature-vector for each state-action pair and the transition model can be represented by a linear combination of the feature vectors. Such a model recently has attracted much interest due to its provable guarantee 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
with model-free algorithms (e.g. [Yang and Wang, 2019, Jin et al., 2019]). In particular, we aim on answering the following fundamental question.
Does a model-based approach on feature-based RL achieve near-optimal sample complexity?
In particular, we focus on the generative model setting, where the agent is able to query samples freely from any chosen state-action pairs. Such a model is proposed by [Kearns and Singh, 1999,
Kakade et al., 2003] and gains a great deal of interests recently [Azar et al., 2013, Sidford et al., 2018a, Yang and Wang, Zanette et al., 2019]. Moreover, we focus on the plug-in solver approach, which is probably the most intuitive and simplest approach for model-based RL: we ﬁrst build an empirical model with an estimate of the transition probability matrix and then ﬁnd a near optimal policy by planning in this empirical model via arbitrary plug-in solver. In the tabular setting, where the state and action spaces, S and A, are ﬁnite, [Azar et al., 2013] shows that the value estimation of a plug-in approach is minimax optimal in samples. In particular, they show that to obtain an (cid:15)-optimal value, the number of samples required to estimate the model is (cid:101)O(|S||A|/(cid:15)2(1 − γ)3).1 Very recently,
[Agarwal et al., 2019] proves that the policy estimation is also minimax optimal and with the same sample complexity. Unfortunately, these results cannot be applied to the function approximation setting, especially when the number of states becomes inﬁnity.
In this paper, we show that the plug-in solver approach do achieve near-optimal sample complexity even in the feature-based setting, provided that the features are well conditioned. In particular, we show that under an anchor-state condition, where all features can be represented by the convex combination of some anchor-state features, an (cid:15)-optimal policy can be obtained from an approximate model with only (cid:101)O(K/(cid:15)2(1 − γ)3) samples from the generative model, where K is the feature dimension, independent of the size of state and action spaces. Under a more relaxed setting on the features, we also prove that ﬁnding an (cid:15)-optimal policy only needs (cid:101)O(K · poly(1/(1 − γ))/(cid:15)2) samples. To achieve our results, we observe that the value function actually lies in an one-dimensional manifold and thus we can construct a series of auxiliary MDPs to approximate the value function.
This auxiliary MDP technique breaks the statistical dependence that impedes the analysis. We have also extended our techniques to other settings e.g. ﬁnite horizon MDP (FHMDP) and two-players turn-based stochastic games (2-TBSG). To our best knowledge, this work ﬁrst proves that plug-in approach is sample-optimal for feature-based RL and we hope our technique can boost analysis in broader settings. 2