Abstract
Potential-based reward shaping provides an approach for designing good reward functions, with the purpose of speeding up learning. However, automatically ﬁnd-ing potential functions for complex environments is a difﬁcult problem (in fact, of the same difﬁculty as learning a value function from scratch). We propose a new framework for learning potential functions by leveraging ideas from graph representation learning. Our approach relies on Graph Convolutional Networks which we use as a key ingredient in combination with the probabilistic inference view of reinforcement learning. More precisely, we leverage Graph Convolutional
Networks to perform message passing from rewarding states. The propagated messages can then be used as potential functions for reward shaping to acceler-ate learning. We verify empirically that our approach can achieve considerable improvements in both small and high-dimensional control problems. 1

Introduction
Reinforcement learning (RL) algorithms provide a solution to the problem of learning a policy that optimizes an expected, cumulative function of rewards. Consequently, a good reward function is critical to the practical success of these algorithms. However, designing such a function can be challenging [Amodei et al., 2016]. Approaches to this problem include, amongst others, intrinsic motivation [Oudeyer and Kaplan, 2007, Schmidhuber, 2010], optimal rewards [Singh et al., 2010] and potential-based reward shaping [Ng et al., 1999]. The latter provides an appealing formulation as it does not change the optimal policy of an MDP while potentially speeding up the learning process.
However, the design of potential functions used for reward shaping is still an open question.
In this paper, we present a solution to this problem by leveraging the probabilistic inference view of
RL [Toussaint and Storkey, 2006, Ziebart et al., 2008]. In particular, we are interested in formulating the RL problem as a directed graph whose structure is analogous to hidden Markov models. In such graphs it is convenient to perform inference through message passing with algorithms such as the forward-backward algorithm [Rabiner and Juang, 1986]. This inference procedure essentially propagates information from the rewarding states in the MDP and results in a function over states.
This function could then naturally be leveraged as a potential function for potential-based reward shaping. However, the main drawback of traditional message passing algorithms is that they can be computationally expensive and are therefore hard to scale to large or continuous state space.
We present an implementation that is both scalable and ﬂexible by drawing connections to spectral graph theory [Chung, 1997]. We use Graph Convolutional Networks (GCN) [Kipf and Welling, 2016] to propagate information about the rewards in an environment through the message passing mechanism deﬁned by the GCN’s structural bias and loss function. Indeed, GCNs belong to the larger class of Message Passing Neural Networks [Gilmer et al., 2017] with the special characteristic that their message passing mechanism builds on the graph Laplacian. The framework of Proto-Value 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Functions [Mahadevan, 2005] from the reinforcement learning literature has previously studied the properties of the graph Laplacian and we build on these ﬁndings in our work.
We ﬁrst evaluate our approach in tabular domains where we achieve similar performance compared to potential based reward shaping built on the forward-backward algorithm. Unlike hand-engineered potential functions, our method scales naturally to more complex environments; we illustrate this on navigation-based vision tasks from the MiniWorld environment
[Chevalier-Boisvert, 2018], on a variety of games from the Atari 2600 benchmark [Bellemare et al., 2012] and on a set of continuous control environments based on MuJoCo [Todorov et al., 2012] , where our method fares signiﬁcantly better than actor-critic algorithms [Sutton et al., 1999a, Schulman et al., 2017] and additional baselines. 2