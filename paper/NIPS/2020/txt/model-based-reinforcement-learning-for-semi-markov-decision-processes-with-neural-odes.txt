Abstract
We present two elegant solutions for modeling continuous-time dynamics, in a novel model-based reinforcement learning (RL) framework for semi-Markov decision processes (SMDPs) using neural ordinary differential equations (ODEs).
Our models accurately characterize continuous-time dynamics and enable us to develop high-performing policies using a small amount of data. We also develop a model-based approach for optimizing time schedules to reduce interaction rates with the environment while maintaining the near-optimal performance, which is not possible for model-free methods. We experimentally demonstrate the efﬁcacy of our methods across various continuous-time domains. 1

Introduction
Algorithms for deep reinforcement learning (RL) have led to major advances in many applications ranging from robots [Gu et al., 2017] to Atari games [Mnih et al., 2015]. Most of these algorithms formulate the problem in discrete time and assume observations are available at every step. However, many real-world sequential decision-making problems operate in continuous time. For instance, control problems such as robotic manipulation are generally governed by systems of differential equations. In healthcare, patient observations often consist of irregularly sampled time series: more measurements are taken when patients are sicker or more concerns are suspected, and clinical variables are usually observed on different time-scales.
Unfortunately, the problem of learning and acting in continuous-time environments has largely been passed over by the recent advances of deep RL. Previous methods using semi-Markov decision processes (SMDPs) [Howard, 1964]—including Q-learning [Bradtke and Duff, 1995], advantage updating [Baird, 1994], policy gradient [Munos, 2006], actor-critic [Doya, 2000]—extend the standard
RL framework to continuous time, but all use relatively simple linear function approximators.
Furthermore, as model-free methods, they often require large amounts of training data. Thus, rather than attempt to handle continuous time directly, practitioners often resort to discretizing time into evenly spaced intervals and apply standard RL algorithms. However, this heuristic approach loses information about the dynamics if the discretization is too coarse, and results in overly-long time horizons if the discretization is too ﬁne.
In this paper, we take a model-based approach to continuous-time RL, modeling the dynamics via neural ordinary differential equations (ODEs) [Chen et al., 2018]. Not only is this more sample-efﬁcient than model-free approaches, but it allows us to efﬁciently adapt policies learned using one schedule of interactions with the environment for another. Our approach also allows for optimizing the measurement schedules to minimize interaction with the environment while maintaining the near-optimal performance that would be achieved by constant intervention.
Speciﬁcally, to build ﬂexible models for continuous-time, model-based RL, we ﬁrst introduce ways to incorporate action and time into the neural ODE work of [Chen et al., 2018, Rubanova et al., 2019]. We present two solutions, ODE-RNN (based on a recurrent architecture) and Latent-ODE 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(based on an encoder-decoder architecture), both of which are signiﬁcantly more robust than current approaches for continuous-time dynamics. Because these models include a hidden state, they can handle partially observable environments as well as fully-observed environments. Next, we develop a uniﬁed framework that can be used to learn both the state transition and the interval timing for the associated SMDP. Not only does our model-based approach outperform baselines in several standard tasks, we demonstrate the above capabilities which are not possible with current model-free methods. 2