Abstract
A recent line of research has highlighted the existence of a “double descent” phenomenon in deep learning, whereby increasing the number of training examples
N causes the generalization error of neural networks to peak when N is of the same order as the number of parameters P . In earlier works, a similar phenomenon was shown to exist in simpler models such as linear regression, where the peak instead occurs when N is equal to the input dimension D. Since both peaks coincide with the interpolation threshold, they are often conﬂated in the litterature. In this paper, we show that despite their apparent similarity, these two scenarios are inherently different. In fact, both peaks can co-exist when neural networks are applied to noisy regression tasks. The relative size of the peaks is then governed by the degree of nonlinearity of the activation function. Building on recent developments in the analysis of random feature models, we provide a theoretical ground for this sample-wise triple descent. As shown previously, the nonlinear peak at N = P is a true divergence caused by the extreme sensitivity of the output function to both the noise corrupting the labels and the initialization of the random features (or the weights in neural networks). This peak survives in the absence of noise, but can be suppressed by regularization. In contrast, the linear peak at N = D is solely due to overﬁtting the noise in the labels, and forms earlier during training. We show that this peak is implicitly regularized by the nonlinearity, which is why it only becomes salient at high noise and is weakly affected by explicit regularization. Throughout the paper, we compare analytical results obtained in the random feature model with the outcomes of numerical experiments involving deep neural networks.

Introduction
A few years ago, deep neural networks achieved breakthroughs in a variety of contexts [1, 2, 3, 4].
However, their remarkable generalization abilities have puzzled rigorous understanding [5, 6, 7]: classical learning theory predicts that generalization error should follow a U-shaped curve as the number of parameters P increases, and a monotonous decrease as the number of training examples
N increases. Instead, recent developments show that deep neural networks, as well as other machine learning models, exhibit a starkly different behaviour. In the absence of regularization, increasing P and N respectively yields parameter-wise and sample-wise double descent curves [8, 9, 10, 11, 12, 13], whereby the generalization error ﬁrst decreases, then peaks at the interpolation threshold (at which point training error vanishes), then decreases monotonically again. This peak1 was shown to be related 1Also called the jamming peak due to similarities with a well-studied phenomenon in the Statistical Physics literature [14, 15, 16, 17, 18]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Te(cid:86)(cid:87)  (cid:79)(cid:82)(cid:86)(cid:86)
N(cid:82)(cid:81)(cid:79)i(cid:81)ea(cid:85) (cid:83)eak
Te(cid:86)(cid:87)  (cid:79)(cid:82)(cid:86)(cid:86)
Li(cid:81)ea(cid:85)  (cid:83)eak
N(cid:82)(cid:81)(cid:79)i(cid:81)ea(cid:85) (cid:83)eak
P
Ac(cid:87)i(cid:89)a(cid:87)i(cid:82)(cid:81) f(cid:88)(cid:81)c(cid:87)i(cid:82)(cid:81)
S(cid:87)(cid:85)(cid:82)(cid:81)g(cid:79)(cid:92) (cid:81)(cid:82)(cid:81)(cid:79)i(cid:81)ea(cid:85)
Weak(cid:79)(cid:92) (cid:81)(cid:82)(cid:81)(cid:79)i(cid:81)ea(cid:85)
Li(cid:81)ea(cid:85)
Li(cid:81)ea(cid:85)  (cid:83)eak
N(cid:82)(cid:81)(cid:79)i(cid:81)ea(cid:85) (cid:83)eak
N
P
D
P
N
D
N
Figure 1: Left: The parameter-wise proﬁle of the test loss exhibits double descent, with a peak at
P = N . Middle: The sample-wise proﬁle can, at high noise, exhibit a single peak at N = P , a single peak at N = D, or a combination of the two (triple descent2) depending on the degree of nonlinearity of the activation function. Right: Color-coded location of the peaks in the (P, N ) phase space. to a sharp increase in the variance of the estimator [18, 9], and can be suppressed by regularization or ensembling procedures [9, 19, 20].
Although double descent has only recently gained interest in the context of deep learning, a seemingly similar phenomenon has been well-known for several decades for simpler models such as least squares regression [21, 22, 23, 14, 15], and has recently been studied in more detail in an attempt to shed light on the double descent curve observed in deep learning [24, 25, 26, 27]. However, in the context of linear models, the number of parameters P is not a free parameter: it is necessarily equal to the input dimension D. The interpolation threshold occurs at N = D, and coincides with a peak in the test loss which we refer to as the linear peak. For neural networks with nonlinear activations, the interpolation threshold surprisingly becomes independent of D and is instead observed when the number of training examples is of the same order as the total number of training parameters, i.e.
N
P : we refer to the corresponding peak as the nonlinear peak.
Somewhere in between these two scenarios lies the case of neural networks with linear activations.
They have P > D parameters, but only D of them are independent: the interpolation threshold occurs at N = D. However, their dynamical behaviour shares some similarities with that of deep nonlinear networks, and their analytical tractability has given them signiﬁcant attention [28, 29, 6]. A natural question is the following: what would happen for a “quasi-linear” network, e.g. one that uses a sigmoidal activation function with a high saturation plateau? Would the overﬁtting peak be observed both at N = D and N = P , or would it somehow lie in between?
⇠
In this work, we unveil the similarities and the differences between the linear and nonlinear peaks. In particular, we address the following questions:
•
•
•
Are the linear and nonlinear peaks two different phenomena?
If so, can both be observed simultaneously, and can we differentiate their sources?
How are they affected by the activation function? Can they both be suppressed by regulariz-ing or ensembling? Do they appear at the same time during training?
Contribution In modern neural networks, the double descent phenomenon is mostly studied by increasing the number of parameters P (Fig. 1, left), and more rarely, by increasing the number of training examples N (Fig. 1, middle) [13]. The analysis of linear models is instead performed by varying the ratio P/N . By studying the full (P, N ) phase space (Fig. 1, right), we disentangle the role of the linear and the nonlinear peaks in modern neural networks, and elucidate the role of the input dimension D.
In Sec.1, we demonstrate that the linear and nonlinear peaks are two different phenomena by showing that they can co-exist in the (P, N ) phase space in noisy regression tasks. This leads to a sample-wise triple descent, as sketched in Fig. 1. We consider both an analytically tractable model of random features [30] and a more realistic model of neural networks.
In Sec. 2, we provide a theoretical analysis of this phenomenon in the random feature model. We examine the eigenspectrum of random feature Gram matrices and show that whereas the nonlinear 2The name “triple descent” refers to the presence of two peaks instead of just one in the famous “double descent” curve, but in most cases the test error does not actually descend before the ﬁrst peak. 2
peak is caused by the presence of small eigenvalues [6], the small eigenvalues causing the linear peak gradually disappear when the activation function becomes nonlinear: the linear peak is implicitly regularized by the nonlinearity. Through a bias-variance decomposition of the test loss, we reveal that the linear peak is solely caused by overﬁtting the noise corrupting the labels, whereas the nonlinear peak is also caused by the variance due to the initialization of the random feature vectors (which plays the role of the initialization of the weights in neural networks).
Finally, in Sec. 3, we present the phenomenological differences which follow from the theoretical analysis. Increasing the degree of nonlinearity of the activation function weakens the linear peak and strengthens the nonlinear peak. We also ﬁnd that the nonlinear peak can be suppressed by regularizing or ensembling, whereas the linear peak cannot since it is already implicitly regularized. Finally, we note that the nonlinear peak appears much later under gradient descent dynamics than the linear peak, since it is caused by small eigenmodes which are slow to learn.