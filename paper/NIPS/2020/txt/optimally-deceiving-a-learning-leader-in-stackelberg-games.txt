Abstract
Recent results in the ML community have revealed that learning algorithms used to compute the optimal strategy for the leader to commit to in a Stackelberg game, are susceptible to manipulation by the follower. Such a learning algorithm operates by querying the best responses or the payoffs of the follower, who consequently can deceive the algorithm by responding as if their payoffs were much different than what they actually are. For this strategic behavior to be successful, the main challenge faced by the follower is to pinpoint the payoffs that would make the learning algorithm compute a commitment so that best responding to it maximizes the follower’s utility, according to the true payoffs. While this problem has been considered before, the related literature only focused on the simpliﬁed scenario in which the payoff space is ﬁnite, thus leaving the general version of the problem unanswered. In this paper, we ﬁll this gap by showing that it is always possible for the follower to efﬁciently compute (near-)optimal payoffs for various scenarios of learning interaction between the leader and the follower. 1

Introduction
Stackelberg games are a simple yet powerful model for sequential interaction among strategic agents.
In such games there are two players: a leader and a follower. The leader commits to an action, and the follower acts upon observing the leader’s commitment. The simple sequential structure of the game permits modeling a multitude of important scenarios. Indicative applications include the competition between a large and a small ﬁrm [43], the allocation of defensive resources [42], and the competition among mining pools in the Bitcoin network [32, 41].
In Stackelberg games, the leader is interested in ﬁnding the best commitment she can make, assuming that the follower behaves rationally. The combination of such a commitment by the leader and the follower’s rational best response to it leads to a strong Stackelberg equilibrium (SSE). In general, the utility that the leader obtains in an SSE is larger than what she would obtain in a Nash equilibrium of the corresponding one-shot game [40], implying that the leader prefers to commit than to engage in a simultaneous game with the follower. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In case the leader has access to both hers and the follower’s payoff parameters, computing an SSE is a computationally tractable problem [15]. In practice however, the leader may have limited or no information about the follower’s payoffs. Consequently, in order to determine the optimal commitment, the leader must endeavor to elicit information about the incentives of the follower through indirect means. This avenue of research has led to a plethora of active-learning-based approaches for the computation of SSEs [3, 6, 30, 36, 39]. At the same time, inspired by recent developments in the ML community regarding adversarial examples in classiﬁcation algorithms
[4, 31], there has been a stream of recent papers exploring the notion of adversarial deception by the follower, when facing algorithms used by the leader for learning SSEs in Stackelberg games.
Speciﬁcally, when an algorithm learns an SSE by querying the follower’s best responses, the follower can use fake best responses to distort the SSE learned by the algorithm. As recently explored by Gan et al. [20], one particular approach the follower can employ, is to imitate best responses implied by payoffs that are different from his actual ones. The key to the success of such a deceptive behavior is thus to pinpoint the fake payoffs that could make the leader learn an SSE in which the actual utility of the follower is maximized. In the scenario studied in [20], this task is trivial as the follower’s choices are limited to a ﬁnite set that has a size bounded by the size of the problem representation; thus, to efﬁciently ﬁnd out the optimal payoffs, the follower can simply enumerate all possible matrices.
To the best of our knowledge, the general version of this problem, where the follower is allowed to use any payoff matrix, without restrictions on the space of possible values, has been considered only in two very recent papers [19, 34], which however focused on the speciﬁc application of Stackelberg games to security resource allocation problems. Besides that, no progress has been made for general
Stackelberg games. In this paper, we aim to ﬁll in this gap, by completely resolving this computational problem, a result that reﬂects the insecurity of learning to commit in Stackelberg games.
Our Contribution. We explore how a follower can optimally deceive a learning leader in Stack-elberg games by misreporting his payoff matrix, and study the tractability of the corresponding optimization problem. As in previous work, our objective is to compute the fake payoff matrix according to which the follower can best respond to make the leader learn an SSE in which the true utility of the follower is maximized. However, unlike the related literature, we do not impose any restrictions on the space from which the payoffs are selected or on the type of the game.
By exploiting an intuitive characterization of all strategy proﬁles that can be induced as SSEs in
Stackelberg games, we show that it is always possible (irrespective of the learning algorithm employed by the leader) for the follower to compute, in polynomial time, a payoff matrix implying an SSE which maximizes his true utility. Furthermore, we strengthen this result to resolve possible equilibrium selection issues, by showing that the follower can construct a payoff matrix that induces a unique
SSE, in which his utility is maximized up to some arbitrarily small loss.
Other