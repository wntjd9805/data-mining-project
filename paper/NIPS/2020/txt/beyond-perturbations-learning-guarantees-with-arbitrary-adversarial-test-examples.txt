Abstract
We present a transductive learning algorithm that takes as input training examples from a distribution ğ‘ƒ and arbitrary (unlabeled) test examples, possibly chosen by an adversary. This is unlike prior work that assumes that test examples are small perturbations of ğ‘ƒ . Our algorithm outputs a selective classiï¬er, which abstains from predicting on some examples. By considering selective transductive learning, we give the ï¬rst nontrivial guarantees for learning classes of bounded VC dimension with arbitrary train and test distributionsâ€”no prior guarantees were known even for simple classes of functions such as intervals on the line. In particular, for any function in a class ğ¶ of bounded VC dimension, we guarantee a low test error rate and a low rejection rate with respect to ğ‘ƒ . Our algorithm is efï¬cient given an Empirical Risk Minimizer (ERM) for ğ¶. Our guarantees hold even for test examples chosen by an unbounded white-box adversary. We also give guarantees for generalization, agnostic, and unsupervised settings. 1

Introduction
Consider binary classiï¬cation where test examples are not from the training distribution. Speciï¬cally, consider learning a binary function ğ‘“ âˆ¶ ğ‘‹ â†’ {0, 1} where training examples are assumed to be iid from a distribution ğ‘ƒ over ğ‘‹, while the test examples are arbitrary. This includes both the possibility that test examples are chosen by an adversary or that they are drawn from a distribution ğ‘„ â‰  ğ‘ƒ (sometimes called â€œcovariate shiftâ€). For a disturbing example of covariate shift, consider learning to classify abnormal lung scans. A system trained on scans prior to 2019 may miss abnormalities due to
COVID-19 since there were none in the training data. As a troubling adversarial example, consider explicit content detectors which are trained to classify normal vs. explicit images. Adversarial spammers synthesize endless variations of explicit images that evade these detectors for purposes such as advertising and phishing [Yuan et al., 2019].
A recent line of work on adversarial learning has designed algorithms that are robust to imperceptible perturbations. However, perturbations do not cover all types of test examples. In the explicit image detection example, Yuan et al. [2019] ï¬nd adversaries using conspicuous image distortion techniques (e.g., overlaying a large colored rectangle on an image) rather than imperceptible perturbations. In the lung scan example, Fang et al. [2020] ï¬nd noticeable signs of COVID in many scans.
In general, there are several reasons why learning with arbitrary test examples is actually impossible.
First of all, one may not be able to predict the labels of test examples that are far from training examples, as illustrated by the examples in group (1) of Figure 1. Secondly, as illustrated by group (2), given any classiï¬er â„, an adversary or test distribution ğ‘„ may concentrate on or near an error. High error rates are thus unavoidable since an adversary can simply repeat any single erroneous example they can ï¬nd. This could also arise naturally, as in the COVID example, if ğ‘„ contains a concentration of new examples near one anotherâ€“individually they appear â€œnormalâ€ (but are suspicious as a group).
âˆ—Author order is alphabetical. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
This is true even under the standard realizable assumption that the target function ğ‘“ âˆˆ ğ¶ is in a known class ğ¶ of bounded VC dimension ğ‘‘ = VC(ğ¶).
As we now argue, learning with arbitrary test examples requires selective classiï¬ers and transductive learning, which have each been independently studied extensively. We refer to the combination as classiï¬cation with redaction, a term which refers to the removal/obscuring of certain information when documents are released. A selective classiï¬er (SC) is one which is allowed to abstain from predicting on some examples. In particular, it speciï¬es both a classiï¬er â„ and a subset ğ‘† âŠ† ğ‘‹ of examples to classify, and rejects the rest. Equivalently, one can think of a SC as â„|ğ‘† âˆ¶ ğ‘‹ â†’ {0, 1, â–®} where â–® indicates ğ‘¥ âˆ‰ ğ‘†, abstinence.
â„|ğ‘† (ğ‘¥) âˆ¶=
{â„(ğ‘¥)
â–® if ğ‘¥ âˆˆ ğ‘† if ğ‘¥ âˆ‰ ğ‘†.
We say the learner classiï¬es ğ‘¥ if ğ‘¥ âˆˆ ğ‘† and otherwise it rejects ğ‘¥. Following standard terminology, if
ğ‘¥ âˆ‰ ğ‘† (i.e., â„|ğ‘† (ğ‘¥) = â–®) we say the classiï¬er rejects ğ‘¥ (the term is not meant to indicate anything negative about the example ğ‘¥ but merely that its classiï¬cation may be unreliable). We sat that â„|ğ‘† misclassiï¬es or errs on ğ‘¥ if â„|ğ‘† (ğ‘¥) = 1 âˆ’ ğ‘“ (ğ‘¥). There is a long literature on SCs, starting with the work of Chow [1957] on character recognition. In standard classiï¬cation, transductive learning refers to the simple learning setting where the goal is to classify a given unlabeled test set that is presented together with the training examples [see e.g., Vapnik, 1998]. We will also consider the generalization error of the learned classiï¬er.
This raises the question: When are unlabeled test examples available in advance? In some appli-cations, test examples are classiï¬ed all at once (or in batches). Otherwise, redaction can also be beneï¬cial in retrospect. For instance, even if image classiï¬cations are necessary immediately, an offensive image detector may be run daily with rejections ï¬‚agged for inspection; and images may later be blocked if they are deemed offensive. Similarly, if a group of unusual lung scans showing COVID were detected after a period of time, the recognition of the new disease could be valuable even in hindsight. Furthermore, in some applications, one cannot simply label a sample of test examples. For instance, in learning to classify messages on an online platform, test data may contain both public and private data while training data may consist only of public messages. Due to privacy concerns, labeling data from the actual test distribution may be prohibited.
It is clear that a SC is necessary to guarantee few test misclassiï¬cations, e.g., if ğ‘ƒ is concentrated on a single point ğ‘¥, rejection is necessary to guarantee few errors on arbitrary test points. However, no prior guarantees (even statistical guarantees) were known even for learning elementary classes such as intervals or halfspaces with arbitrary ğ‘ƒ â‰  ğ‘„. This is because learning such classes is impossible without unlabeled examples.
To illustrate how redaction (transductive SC) is useful, consider learning an interval [ğ‘, ğ‘] on ğ‘‹ = â„ with arbitrary ğ‘ƒ â‰  ğ‘„. This is illustrated below with (blue) dots indicating test examples:
With positive training examples as in (a), one can guarantee 0 test errors by rejecting the two (grey) regions adjacent to the positive examples. When there are no positive training examples,2 as in (b), one can guarantee â‰¤ ğ‘˜ test errors by rejecting any region with > ğ‘˜ test examples and no training examples; and predicting negative elsewhere. Of course, one can guarantee 0 errors by rejecting everywhere, but that would mean rejecting even future examples distributed like ğ‘ƒ . While our error objective will be an ğœ– test error rate, our rejection objective will be more subtle since we cannot absolutely bound the test rejection rate. Indeed, as illustrated above, in some cases one should reject many test examples.
Note that our redaction model assumes that the target function ğ‘“ remains the same at train and test times. This assumption holds in several (but not all) applications of interest. For instance, in explicit 2Learning with an all-negative training set (trivial in standard learning) is a useful â€œanomaly detectionâ€ setting in adversarial learning, e.g., when one aims to classify illegal images without any illegal examples at train time or abnormal scans not present at train time. 2
image detection, U.S. laws regarding what constitutes an illegal image are based solely on the image
ğ‘¥ itself [U.S.C., 1996]. Of course, if laws change between train and test time, then ğ‘“ itself may change. Label shift problems where ğ‘“ changes from train to test is also important but not addressed here. Our focus is primarily the well-studied realizable setting, where ğ‘“ âˆˆ ğ¶, though we analyze an agnostic setting as well. 1.1 Redaction model and guarantees
Our goal is to learn a target function ğ‘“ âˆˆ ğ¶ of VC dimension ğ‘‘ with training distribution ğ‘ƒ over
ğ‘‹. In the redaction model, the learner ï¬rst chooses â„ âˆˆ ğ¶ based on ğ‘› iid training examples ğ± âˆ¼ ğ‘‹ğ‘›
) (ğ‘“ (ğ‘¥1), ğ‘“ (ğ‘¥2), â€¦ , ğ‘“ (ğ‘¥ğ‘›)
âˆˆ {0, 1}ğ‘›. (In other words, it trains a standard and their labels ğ‘“ (ğ±) = binary classiï¬er.) Next, a â€œwhite boxâ€ adversary selects ğ‘› arbitrary test examples Ìƒğ± âˆˆ ğ‘‹ğ‘› based on all information including ğ±, ğ‘“ , â„, ğ‘ƒ and the learning algorithm. Using the unlabeled test examples (and the labeled training examples), the learner ï¬nally outputs ğ‘† âŠ† ğ‘‹. Errors are those test examples in ğ‘† that were misclassiï¬ed, i.e., â„|ğ‘† (ğ‘¥) = 1 âˆ’ ğ‘“ (ğ‘¥).
Rather than jumping straight into the transductive setting, we ï¬rst describe the simpler generalization setting. We deï¬ne the ğ‘ƒ ğ‘„ model in which Ìƒğ± âˆ¼ ğ‘„ğ‘› are drawn iid by nature, for an arbitrary distribution
ğ‘„. While it will be easier to quantify generalization error and rejections in this simpler model, the
ğ‘ƒ ğ‘„ model does not permit a white-box adversary to choose test examples based on â„. To measure performance here, deï¬ne rejection and error rates for distribution ğ·, respectively:
â–®ğ·(ğ‘†) âˆ¶= Pr
ğ‘¥âˆ¼ğ· errğ·(â„|ğ‘† ) âˆ¶= Pr
ğ‘¥âˆ¼ğ·
[â„(ğ‘¥) â‰  ğ‘“ (ğ‘¥) âˆ§ ğ‘¥ âˆˆ ğ‘†]
[ğ‘¥ âˆ‰ ğ‘†] (1) (2)
We write â–®ğ· and errğ· when â„ and ğ‘† are clear from context. We extend the deï¬nition of PAC learning to ğ‘ƒ â‰  ğ‘„ as follows:
Deï¬nition 1.1 (PQ learning). Learner ğ¿ (ğœ–, ğ›¿, ğ‘›)-PQ-learns ğ¶ if for any distributions ğ‘ƒ , ğ‘„ over ğ‘‹ and any ğ‘“ âˆˆ ğ¶, its output â„|ğ‘† = ğ¿(ğ±, ğ‘“ (ğ±), Ìƒğ±) satisï¬es
â–®ğ‘ƒ + errğ‘„
â‰¤ ğœ–] â‰¥ 1 âˆ’ ğ›¿.
[
Pr
ğ±âˆ¼ğ‘ƒ ğ‘›,Ìƒğ±âˆ¼ğ‘„ğ‘›
ğ¿ PQ-learns ğ¶ if ğ¿ runs in polynomial time and if there is a polynomial ğ‘ such that ğ¿ (ğœ–, ğ›¿, ğ‘›)-PQ-learns ğ¶ for every ğœ–, ğ›¿ > 0, ğ‘› â‰¥ ğ‘(1âˆ•ğœ–, 1âˆ•ğ›¿).
Now, at ï¬rst it may seem strange that the deï¬nition bounds â–®ğ‘ƒ rather than â–®ğ‘„, but as mentioned â–®ğ‘„ cannot be bound absolutely. Instead, it can be bound relative to â–®ğ‘ƒ and the total variation distance (also called statistical distance) |ğ‘ƒ âˆ’ ğ‘„|ğ–³ğ–µ âˆˆ [0, 1], as follows:
â–®ğ‘„
â‰¤ â–®ğ‘ƒ +|ğ‘ƒ âˆ’ ğ‘„|ğ–³ğ–µ.
This new perspective, of bounding the rejection probability of ğ‘ƒ , as opposed to ğ‘„, facilitates the analysis. Of course when ğ‘ƒ = ğ‘„, |ğ‘ƒ âˆ’ ğ‘„|ğ–³ğ–µ = 0 and â–®ğ‘„ = â–®ğ‘ƒ , and when ğ‘ƒ and ğ‘„ have disjoint supports (no overlap), then |ğ‘ƒ âˆ’ ğ‘„|ğ–³ğ–µ = 1 and the above bound is vacuous. We also discuss tighter bounds relating â–®ğ‘„ to â–®ğ‘ƒ .
We provide two redactive learning algorithms: a supervised algorithm called ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡, and an unsupervised algorithm ğ–´ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡. ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡ takes as input ğ‘› labeled training data (ğ±, ğ²) âˆˆ ğ‘‹ğ‘› Ã—
{0, 1}ğ‘› and ğ‘› test data Ìƒğ± âˆˆ ğ‘‹ğ‘› (and an error parameter ğœ–). It can be implemented efï¬ciently using any
ğ–¤ğ–±ğ–¬ğ¶ oracle that outputs a function ğ‘ âˆˆ ğ¶ of minimal error on any given set of labeled examples.
It is formally presented in Figure 2. At a high level, it chooses â„ = ğ–¤ğ–±ğ–¬(ğ±, ğ²) and chooses ğ‘† in an iterative manner. It starts with ğ‘† = ğ‘‹ and then iteratively chooses ğ‘ âˆˆ ğ¶ that disagrees signiï¬cantly with â„|ğ‘† on Ìƒğ± but agrees with â„|ğ‘† on ğ±; it then rejects all ğ‘¥â€™s such that ğ‘(ğ‘¥) â‰  â„(ğ‘¥). As we show in
Lemma 4.1, choosing ğ‘ can be done efï¬ciently given oracle access to ğ–¤ğ–±ğ–¬ğ¶ .
Theorem 4.2 shows that ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡ PQ-learns any class ğ¶ of bounded VC dimension ğ‘‘, speciï¬cally with ğœ– = Ìƒğ‘‚(
ğ‘‘âˆ•ğ‘›). (The Ìƒğ‘‚ notation hides logarithmic factors including the dependence on the failure probability ğ›¿.) This is worse than the standard ğœ– = Ìƒğ‘‚(ğ‘‘âˆ•ğ‘›) bound of supervised learning when
ğ‘ƒ = ğ‘„, though Theorem 4.4 shows this is necessary with an Î©(
ğ‘‘âˆ•ğ‘›) lower-bound for ğ‘ƒ â‰  ğ‘„.
âˆš
âˆš
Our unsupervised learning algorithm ğ–´ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡, formally presented in Figure 3, computes ğ‘† only from unlabeled training and test examples, and has similar guarantees (Theorem 4.5). The algorithm 3
tries to distinguish training and test examples and then rejects whatever is almost surely a test example. More speciï¬cally, as above, it chooses ğ‘† in an iterative manner, starting with ğ‘† = ğ‘‹. It (iteratively) chooses two functions ğ‘, ğ‘â€² âˆˆ ğ¶ such that ğ‘|ğ‘† and ğ‘â€²
|ğ‘† have high disagreement on Ìƒğ± and low disagreement on ğ±, and rejects all ğ‘¥â€™s on which ğ‘|ğ‘† , ğ‘â€²
|ğ‘† disagree. As we show in Lemma B.1, choosing ğ‘ and ğ‘â€² can be done efï¬ciently given a (stronger) ğ–¤ğ–±ğ–¬ğ–£ğ–¨ğ–² oracle for the class ğ–£ğ–¨ğ–² of disagreements between ğ‘, ğ‘â€² âˆˆ ğ¶. We emphasize that ğ–´ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡ can also be used for multi-class learning as it does not use training labels, and can be paired with any classiï¬er trained separately.
This advantage of ğ–´ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡ over ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡ comes at the cost of requiring a stronger base classiï¬er to be used for ğ–¤ğ–±ğ–¬, and may lead to examples being unnecessarily rejected.
In Figure 1 we illustrate our algorithms for the class ğ¶ of halfspaces. A natural idea would be to train a halfspace to distinguish unlabeled training and test examplesâ€”intuitively, one can safely reject anything that is clearly distinguishable as test without increasing â–®ğ‘ƒ . However, this on its own is insufï¬cient. See for example group (2) of examples in Figure 1, which cannot be distinguished from training data by a halfspace. This is precisely why having test examples is absolutely necessary. Indeed, it allows us to use an ERM oracle to ğ¶ to PQ-learn ğ¶. We also present:
Transductive analysis A similar analysis of ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡ in a transductive setting gives error and rejection bounds directly on the test examples. The bounds here are with respect to a stronger white-box adversary who need not even choose a test set Ìƒğ± iid from a distribution. Such an adversary chooses the test set with knowledge of ğ‘ƒ , ğ‘“ , â„ and ğ±. In particular, ï¬rst â„ is chosen based on ğ± and ğ²; then the adversary chooses the test set Ìƒğ± based on all available information; and ï¬nally, ğ‘† is chosen.
We introduce a novel notion of false rejection, where we reject a test example that was in fact chosen from ğ‘ƒ and not modiï¬ed by an adversary. Theorem 4.3 gives bounds that are similar in spirit to
Theorem 4.2 but for the harsher transductive setting.
Agnostic bounds Thus far, we have considered the realizable setting where the target ğ‘“ âˆˆ ğ¶. In agnostic learning (Kearns et al. [1992]), there is an arbitrary distribution ğœ‡ over ğ‘‹ Ã— {0, 1} and the goal is to learn a classiï¬er that is nearly as accurate as the best classiï¬er in ğ¶. In our setting, we assume that there is a known ğœ‚ â‰¥ 0 such that the train and test distributions ğœ‡ and Ìƒğœ‡ over ğ‘‹ Ã— {0, 1} satisfy that there is some function ğ‘“ âˆˆ ğ¶ that has error at most ğœ‚ with respect to both ğœ‡ and Ìƒğœ‡.
Unfortunately, we show that in such a setting one cannot guarantee less than Î©(âˆšğœ‚) errors and rejections, but we show that ğ–±ğ–¾ğ—ƒğ–¾ğ–¼ğ—ğ—‹ğ—ˆğ—‡ nearly achieves such guarantees.
Experiments As a proof of concept, we perform simple controlled experiments on the task of handwritten letter classiï¬cation using lower-case English letters from the EMNIST dataset (Cohen et al. [2017]). In one setup, to mimic a spamming adversary, after a classiï¬er â„ is trained, test examples are identiï¬ed on which â„ errs and are repeated many times in the test set. Existing SC algorithms (no matter how robust) will fail on such an example since they all choose ğ‘† without using unlabeled test examplesâ€”as long as an adversary can ï¬nd even a single erroneous example, it can simply repeat it. In the second setup, we consider a natural test distribution which consists of a mix of lower- and upper-case letters, while the training set was only lower-case letters. 2