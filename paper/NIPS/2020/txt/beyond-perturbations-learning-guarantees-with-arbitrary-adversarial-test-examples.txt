Abstract
We present a transductive learning algorithm that takes as input training examples from a distribution 𝑃 and arbitrary (unlabeled) test examples, possibly chosen by an adversary. This is unlike prior work that assumes that test examples are small perturbations of 𝑃 . Our algorithm outputs a selective classiﬁer, which abstains from predicting on some examples. By considering selective transductive learning, we give the ﬁrst nontrivial guarantees for learning classes of bounded VC dimension with arbitrary train and test distributions—no prior guarantees were known even for simple classes of functions such as intervals on the line. In particular, for any function in a class 𝐶 of bounded VC dimension, we guarantee a low test error rate and a low rejection rate with respect to 𝑃 . Our algorithm is efﬁcient given an Empirical Risk Minimizer (ERM) for 𝐶. Our guarantees hold even for test examples chosen by an unbounded white-box adversary. We also give guarantees for generalization, agnostic, and unsupervised settings. 1

Introduction
Consider binary classiﬁcation where test examples are not from the training distribution. Speciﬁcally, consider learning a binary function 𝑓 ∶ 𝑋 → {0, 1} where training examples are assumed to be iid from a distribution 𝑃 over 𝑋, while the test examples are arbitrary. This includes both the possibility that test examples are chosen by an adversary or that they are drawn from a distribution 𝑄 ≠ 𝑃 (sometimes called “covariate shift”). For a disturbing example of covariate shift, consider learning to classify abnormal lung scans. A system trained on scans prior to 2019 may miss abnormalities due to
COVID-19 since there were none in the training data. As a troubling adversarial example, consider explicit content detectors which are trained to classify normal vs. explicit images. Adversarial spammers synthesize endless variations of explicit images that evade these detectors for purposes such as advertising and phishing [Yuan et al., 2019].
A recent line of work on adversarial learning has designed algorithms that are robust to imperceptible perturbations. However, perturbations do not cover all types of test examples. In the explicit image detection example, Yuan et al. [2019] ﬁnd adversaries using conspicuous image distortion techniques (e.g., overlaying a large colored rectangle on an image) rather than imperceptible perturbations. In the lung scan example, Fang et al. [2020] ﬁnd noticeable signs of COVID in many scans.
In general, there are several reasons why learning with arbitrary test examples is actually impossible.
First of all, one may not be able to predict the labels of test examples that are far from training examples, as illustrated by the examples in group (1) of Figure 1. Secondly, as illustrated by group (2), given any classiﬁer ℎ, an adversary or test distribution 𝑄 may concentrate on or near an error. High error rates are thus unavoidable since an adversary can simply repeat any single erroneous example they can ﬁnd. This could also arise naturally, as in the COVID example, if 𝑄 contains a concentration of new examples near one another–individually they appear “normal” (but are suspicious as a group).
∗Author order is alphabetical. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
This is true even under the standard realizable assumption that the target function 𝑓 ∈ 𝐶 is in a known class 𝐶 of bounded VC dimension 𝑑 = VC(𝐶).
As we now argue, learning with arbitrary test examples requires selective classiﬁers and transductive learning, which have each been independently studied extensively. We refer to the combination as classiﬁcation with redaction, a term which refers to the removal/obscuring of certain information when documents are released. A selective classiﬁer (SC) is one which is allowed to abstain from predicting on some examples. In particular, it speciﬁes both a classiﬁer ℎ and a subset 𝑆 ⊆ 𝑋 of examples to classify, and rejects the rest. Equivalently, one can think of a SC as ℎ|𝑆 ∶ 𝑋 → {0, 1, ▮} where ▮ indicates 𝑥 ∉ 𝑆, abstinence.
ℎ|𝑆 (𝑥) ∶=
{ℎ(𝑥)
▮ if 𝑥 ∈ 𝑆 if 𝑥 ∉ 𝑆.
We say the learner classiﬁes 𝑥 if 𝑥 ∈ 𝑆 and otherwise it rejects 𝑥. Following standard terminology, if
𝑥 ∉ 𝑆 (i.e., ℎ|𝑆 (𝑥) = ▮) we say the classiﬁer rejects 𝑥 (the term is not meant to indicate anything negative about the example 𝑥 but merely that its classiﬁcation may be unreliable). We sat that ℎ|𝑆 misclassiﬁes or errs on 𝑥 if ℎ|𝑆 (𝑥) = 1 − 𝑓 (𝑥). There is a long literature on SCs, starting with the work of Chow [1957] on character recognition. In standard classiﬁcation, transductive learning refers to the simple learning setting where the goal is to classify a given unlabeled test set that is presented together with the training examples [see e.g., Vapnik, 1998]. We will also consider the generalization error of the learned classiﬁer.
This raises the question: When are unlabeled test examples available in advance? In some appli-cations, test examples are classiﬁed all at once (or in batches). Otherwise, redaction can also be beneﬁcial in retrospect. For instance, even if image classiﬁcations are necessary immediately, an offensive image detector may be run daily with rejections ﬂagged for inspection; and images may later be blocked if they are deemed offensive. Similarly, if a group of unusual lung scans showing COVID were detected after a period of time, the recognition of the new disease could be valuable even in hindsight. Furthermore, in some applications, one cannot simply label a sample of test examples. For instance, in learning to classify messages on an online platform, test data may contain both public and private data while training data may consist only of public messages. Due to privacy concerns, labeling data from the actual test distribution may be prohibited.
It is clear that a SC is necessary to guarantee few test misclassiﬁcations, e.g., if 𝑃 is concentrated on a single point 𝑥, rejection is necessary to guarantee few errors on arbitrary test points. However, no prior guarantees (even statistical guarantees) were known even for learning elementary classes such as intervals or halfspaces with arbitrary 𝑃 ≠ 𝑄. This is because learning such classes is impossible without unlabeled examples.
To illustrate how redaction (transductive SC) is useful, consider learning an interval [𝑎, 𝑏] on 𝑋 = ℝ with arbitrary 𝑃 ≠ 𝑄. This is illustrated below with (blue) dots indicating test examples:
With positive training examples as in (a), one can guarantee 0 test errors by rejecting the two (grey) regions adjacent to the positive examples. When there are no positive training examples,2 as in (b), one can guarantee ≤ 𝑘 test errors by rejecting any region with > 𝑘 test examples and no training examples; and predicting negative elsewhere. Of course, one can guarantee 0 errors by rejecting everywhere, but that would mean rejecting even future examples distributed like 𝑃 . While our error objective will be an 𝜖 test error rate, our rejection objective will be more subtle since we cannot absolutely bound the test rejection rate. Indeed, as illustrated above, in some cases one should reject many test examples.
Note that our redaction model assumes that the target function 𝑓 remains the same at train and test times. This assumption holds in several (but not all) applications of interest. For instance, in explicit 2Learning with an all-negative training set (trivial in standard learning) is a useful “anomaly detection” setting in adversarial learning, e.g., when one aims to classify illegal images without any illegal examples at train time or abnormal scans not present at train time. 2
image detection, U.S. laws regarding what constitutes an illegal image are based solely on the image
𝑥 itself [U.S.C., 1996]. Of course, if laws change between train and test time, then 𝑓 itself may change. Label shift problems where 𝑓 changes from train to test is also important but not addressed here. Our focus is primarily the well-studied realizable setting, where 𝑓 ∈ 𝐶, though we analyze an agnostic setting as well. 1.1 Redaction model and guarantees
Our goal is to learn a target function 𝑓 ∈ 𝐶 of VC dimension 𝑑 with training distribution 𝑃 over
𝑋. In the redaction model, the learner ﬁrst chooses ℎ ∈ 𝐶 based on 𝑛 iid training examples 𝐱 ∼ 𝑋𝑛
) (𝑓 (𝑥1), 𝑓 (𝑥2), … , 𝑓 (𝑥𝑛)
∈ {0, 1}𝑛. (In other words, it trains a standard and their labels 𝑓 (𝐱) = binary classiﬁer.) Next, a “white box” adversary selects 𝑛 arbitrary test examples ̃𝐱 ∈ 𝑋𝑛 based on all information including 𝐱, 𝑓 , ℎ, 𝑃 and the learning algorithm. Using the unlabeled test examples (and the labeled training examples), the learner ﬁnally outputs 𝑆 ⊆ 𝑋. Errors are those test examples in 𝑆 that were misclassiﬁed, i.e., ℎ|𝑆 (𝑥) = 1 − 𝑓 (𝑥).
Rather than jumping straight into the transductive setting, we ﬁrst describe the simpler generalization setting. We deﬁne the 𝑃 𝑄 model in which ̃𝐱 ∼ 𝑄𝑛 are drawn iid by nature, for an arbitrary distribution
𝑄. While it will be easier to quantify generalization error and rejections in this simpler model, the
𝑃 𝑄 model does not permit a white-box adversary to choose test examples based on ℎ. To measure performance here, deﬁne rejection and error rates for distribution 𝐷, respectively:
▮𝐷(𝑆) ∶= Pr
𝑥∼𝐷 err𝐷(ℎ|𝑆 ) ∶= Pr
𝑥∼𝐷
[ℎ(𝑥) ≠ 𝑓 (𝑥) ∧ 𝑥 ∈ 𝑆]
[𝑥 ∉ 𝑆] (1) (2)
We write ▮𝐷 and err𝐷 when ℎ and 𝑆 are clear from context. We extend the deﬁnition of PAC learning to 𝑃 ≠ 𝑄 as follows:
Deﬁnition 1.1 (PQ learning). Learner 𝐿 (𝜖, 𝛿, 𝑛)-PQ-learns 𝐶 if for any distributions 𝑃 , 𝑄 over 𝑋 and any 𝑓 ∈ 𝐶, its output ℎ|𝑆 = 𝐿(𝐱, 𝑓 (𝐱), ̃𝐱) satisﬁes
▮𝑃 + err𝑄
≤ 𝜖] ≥ 1 − 𝛿.
[
Pr
𝐱∼𝑃 𝑛,̃𝐱∼𝑄𝑛
𝐿 PQ-learns 𝐶 if 𝐿 runs in polynomial time and if there is a polynomial 𝑝 such that 𝐿 (𝜖, 𝛿, 𝑛)-PQ-learns 𝐶 for every 𝜖, 𝛿 > 0, 𝑛 ≥ 𝑝(1∕𝜖, 1∕𝛿).
Now, at ﬁrst it may seem strange that the deﬁnition bounds ▮𝑃 rather than ▮𝑄, but as mentioned ▮𝑄 cannot be bound absolutely. Instead, it can be bound relative to ▮𝑃 and the total variation distance (also called statistical distance) |𝑃 − 𝑄|𝖳𝖵 ∈ [0, 1], as follows:
▮𝑄
≤ ▮𝑃 +|𝑃 − 𝑄|𝖳𝖵.
This new perspective, of bounding the rejection probability of 𝑃 , as opposed to 𝑄, facilitates the analysis. Of course when 𝑃 = 𝑄, |𝑃 − 𝑄|𝖳𝖵 = 0 and ▮𝑄 = ▮𝑃 , and when 𝑃 and 𝑄 have disjoint supports (no overlap), then |𝑃 − 𝑄|𝖳𝖵 = 1 and the above bound is vacuous. We also discuss tighter bounds relating ▮𝑄 to ▮𝑃 .
We provide two redactive learning algorithms: a supervised algorithm called 𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇, and an unsupervised algorithm 𝖴𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇. 𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇 takes as input 𝑛 labeled training data (𝐱, 𝐲) ∈ 𝑋𝑛 ×
{0, 1}𝑛 and 𝑛 test data ̃𝐱 ∈ 𝑋𝑛 (and an error parameter 𝜖). It can be implemented efﬁciently using any
𝖤𝖱𝖬𝐶 oracle that outputs a function 𝑐 ∈ 𝐶 of minimal error on any given set of labeled examples.
It is formally presented in Figure 2. At a high level, it chooses ℎ = 𝖤𝖱𝖬(𝐱, 𝐲) and chooses 𝑆 in an iterative manner. It starts with 𝑆 = 𝑋 and then iteratively chooses 𝑐 ∈ 𝐶 that disagrees signiﬁcantly with ℎ|𝑆 on ̃𝐱 but agrees with ℎ|𝑆 on 𝐱; it then rejects all 𝑥’s such that 𝑐(𝑥) ≠ ℎ(𝑥). As we show in
Lemma 4.1, choosing 𝑐 can be done efﬁciently given oracle access to 𝖤𝖱𝖬𝐶 .
Theorem 4.2 shows that 𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇 PQ-learns any class 𝐶 of bounded VC dimension 𝑑, speciﬁcally with 𝜖 = ̃𝑂(
𝑑∕𝑛). (The ̃𝑂 notation hides logarithmic factors including the dependence on the failure probability 𝛿.) This is worse than the standard 𝜖 = ̃𝑂(𝑑∕𝑛) bound of supervised learning when
𝑃 = 𝑄, though Theorem 4.4 shows this is necessary with an Ω(
𝑑∕𝑛) lower-bound for 𝑃 ≠ 𝑄.
√
√
Our unsupervised learning algorithm 𝖴𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇, formally presented in Figure 3, computes 𝑆 only from unlabeled training and test examples, and has similar guarantees (Theorem 4.5). The algorithm 3
tries to distinguish training and test examples and then rejects whatever is almost surely a test example. More speciﬁcally, as above, it chooses 𝑆 in an iterative manner, starting with 𝑆 = 𝑋. It (iteratively) chooses two functions 𝑐, 𝑐′ ∈ 𝐶 such that 𝑐|𝑆 and 𝑐′
|𝑆 have high disagreement on ̃𝐱 and low disagreement on 𝐱, and rejects all 𝑥’s on which 𝑐|𝑆 , 𝑐′
|𝑆 disagree. As we show in Lemma B.1, choosing 𝑐 and 𝑐′ can be done efﬁciently given a (stronger) 𝖤𝖱𝖬𝖣𝖨𝖲 oracle for the class 𝖣𝖨𝖲 of disagreements between 𝑐, 𝑐′ ∈ 𝐶. We emphasize that 𝖴𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇 can also be used for multi-class learning as it does not use training labels, and can be paired with any classiﬁer trained separately.
This advantage of 𝖴𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇 over 𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇 comes at the cost of requiring a stronger base classiﬁer to be used for 𝖤𝖱𝖬, and may lead to examples being unnecessarily rejected.
In Figure 1 we illustrate our algorithms for the class 𝐶 of halfspaces. A natural idea would be to train a halfspace to distinguish unlabeled training and test examples—intuitively, one can safely reject anything that is clearly distinguishable as test without increasing ▮𝑃 . However, this on its own is insufﬁcient. See for example group (2) of examples in Figure 1, which cannot be distinguished from training data by a halfspace. This is precisely why having test examples is absolutely necessary. Indeed, it allows us to use an ERM oracle to 𝐶 to PQ-learn 𝐶. We also present:
Transductive analysis A similar analysis of 𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇 in a transductive setting gives error and rejection bounds directly on the test examples. The bounds here are with respect to a stronger white-box adversary who need not even choose a test set ̃𝐱 iid from a distribution. Such an adversary chooses the test set with knowledge of 𝑃 , 𝑓 , ℎ and 𝐱. In particular, ﬁrst ℎ is chosen based on 𝐱 and 𝐲; then the adversary chooses the test set ̃𝐱 based on all available information; and ﬁnally, 𝑆 is chosen.
We introduce a novel notion of false rejection, where we reject a test example that was in fact chosen from 𝑃 and not modiﬁed by an adversary. Theorem 4.3 gives bounds that are similar in spirit to
Theorem 4.2 but for the harsher transductive setting.
Agnostic bounds Thus far, we have considered the realizable setting where the target 𝑓 ∈ 𝐶. In agnostic learning (Kearns et al. [1992]), there is an arbitrary distribution 𝜇 over 𝑋 × {0, 1} and the goal is to learn a classiﬁer that is nearly as accurate as the best classiﬁer in 𝐶. In our setting, we assume that there is a known 𝜂 ≥ 0 such that the train and test distributions 𝜇 and ̃𝜇 over 𝑋 × {0, 1} satisfy that there is some function 𝑓 ∈ 𝐶 that has error at most 𝜂 with respect to both 𝜇 and ̃𝜇.
Unfortunately, we show that in such a setting one cannot guarantee less than Ω(√𝜂) errors and rejections, but we show that 𝖱𝖾𝗃𝖾𝖼𝗍𝗋𝗈𝗇 nearly achieves such guarantees.
Experiments As a proof of concept, we perform simple controlled experiments on the task of handwritten letter classiﬁcation using lower-case English letters from the EMNIST dataset (Cohen et al. [2017]). In one setup, to mimic a spamming adversary, after a classiﬁer ℎ is trained, test examples are identiﬁed on which ℎ errs and are repeated many times in the test set. Existing SC algorithms (no matter how robust) will fail on such an example since they all choose 𝑆 without using unlabeled test examples—as long as an adversary can ﬁnd even a single erroneous example, it can simply repeat it. In the second setup, we consider a natural test distribution which consists of a mix of lower- and upper-case letters, while the training set was only lower-case letters. 2