Abstract
Photo-realistic free-viewpoint rendering of real-world scenes using classical com-puter graphics techniques is challenging, because it requires the difﬁcult step of capturing detailed appearance and geometry models. Recent studies have demon-strated promising results by learning scene representations that implicitly encode both geometry and appearance without 3D supervision. However, existing ap-proaches in practice often show blurry renderings caused by the limited network capacity or the difﬁculty in ﬁnding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. In this work, we introduce
Neural Sparse Voxel Fields (NSVF), a new neural scene representation for fast and high-quality free-viewpoint rendering. NSVF deﬁnes a set of voxel-bounded implicit ﬁelds organized in a sparse voxel octree to model local properties in each cell. We progressively learn the underlying voxel structures with a diffentiable ray-marching operation from only a set of posed RGB images. With the sparse voxel octree structure, rendering novel views can be accelerated by skipping the voxels containing no relevant scene content. Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF (Mildenhall et al., 2020)) at inference time while achieving higher quality results. Furthermore, by utilizing an explicit sparse voxel representation, our method can easily be applied to scene editing and scene composition. We also demonstrate several challenging tasks, including multi-scene learning, free-viewpoint rendering of a moving human, and large-scale scene rendering. 1

Introduction
Realistic rendering in computer graphics has a wide range of applications including mixed reality, visual effects, visualization, and even training data generation in computer vision and robot navigation.
Photo-realistically rendering a real world scene from arbitrary viewpoints is a tremendous challenge, because it is often infeasible to acquire high-quality scene geometry and material models, as done in high-budget visual effects productions. Researchers therefore have developed image-based rendering (IBR) approaches that combine vision-based scene geometry modeling with image-based view interpolation (Shum and Kang, 2000; Zhang and Chen, 2004; Szeliski, 2010). Despite their signiﬁcant progress, IBR approaches still have sub-optimal rendering quality and limited control over the results, and are often scene-type speciﬁc. To overcome these limitations, recent works have employed deep neural networks to implicitly learn scene representations encapsulating both geometry and appearance from 2D observations with or without a coarse geometry. Such neural representations are commonly combined with 3D geometric models, such as voxel grids (Yan et al., 2016; Sitzmann et al., 2019a;
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Lombardi et al., 2019), textured meshes (Thies et al., 2019; Kim et al., 2018; Liu et al., 2019a, 2020), multi-plane images (Zhou et al., 2018; Flynn et al., 2019; Mildenhall et al., 2019), point clouds (Meshry et al., 2019; Aliev et al., 2019), and implicit functions (Sitzmann et al., 2019b;
Mildenhall et al., 2020).
Unlike most explicit geometric representations, neural implicit functions are smooth, continuous, and can - in theory - achieve high spatial resolution. However, existing approaches in practice often show blurry renderings caused by the limited network capacity or the difﬁculty in ﬁnding accurate intersections of camera rays with the scene geometry. Synthesizing high-resolution imagery from these representations often requires time-consuming optical ray marching. Furthermore, editing or re-compositing 3D scene models with these neural representations is not straightforward.
In this paper, we propose Neural Sparse Voxel Fields (NSVF), a new implicit representation for fast and high-quality free-viewpoint rendering. Instead of modeling the entire space with a single implicit function, NSVF consists of a set of voxel-bounded implicit ﬁelds organized in a sparse voxel octree. Speciﬁcally, we assign a voxel embedding at each vertex of the voxel, and obtain the representation of a query point inside the voxel by aggregating the voxel embeddings at the eight vertices of the corresponding voxel. This is further passed through a multilayer perceptron network (MLP) to predict geometry and appearance of that query point. Our method can progressively learn
NSVF from coarse to ﬁne with a differentiable ray-marching operation from only a set of posed 2D images of a scene. During training, the sparse voxels containing no scene information will be pruned to allow the network to focus on the implicit functions learning for volume regions with scene contents. With the sparse voxels, rendering at inference time can be greatly accelerated by skipping empty voxels without scene content.
Our method is typically over 10 times faster than the state-of-the-art (namely, NeRF (Mildenhall et al., 2020)) at inference time while achieving higher quality results. We extensively evaluate our method on a variety of challenging tasks including multi-object learning, free-viewpoint rendering of dynamic and indoor scenes. Our method can be used to edit and composite scenes. To summarize, our technical contributions are:
• We present NSVF that consists of a set of voxel-bounded implicit ﬁelds, where for each voxel, voxel embeddings are learned to encode local properties for high-quality rendering;
• NSVF utilizes the sparse voxel structure to achieve efﬁcient rendering;
• We introduce a progressive training strategy that efﬁciently learns the underlying sparse voxel structure with a differentiable ray-marching operation from a set of posed 2D images in an end-to-end manner. 2