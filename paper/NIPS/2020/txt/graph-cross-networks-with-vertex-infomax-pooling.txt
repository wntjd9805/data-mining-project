Abstract
We propose a novel graph cross network (GXN) to achieve comprehensive fea-ture learning from multiple scales of a graph. Based on trainable hierarchical representations of a graph, GXN enables the interchange of intermediate features across scales to promote information ﬂow. Two key ingredients of GXN include a novel vertex infomax pooling (VIPool), which creates multiscale graphs in a trainable manner, and a novel feature-crossing layer, enabling feature interchange across scales. The proposed VIPool selects the most informative subset of vertices based on the neural estimation of mutual information between vertex features and neighborhood features. The intuition behind is that a vertex is informative when it can maximally reﬂect its neighboring information. The proposed feature-crossing layer fuses intermediate features between two scales for mutual enhancement by improving information ﬂow and enriching multiscale features at hidden layers. The cross shape of feature-crossing layer distinguishes GXN from many other multi-scale architectures. Experimental results show that the proposed GXN improves the classiﬁcation accuracy by 2.12% and 1.15% on average for graph classiﬁcation and vertex classiﬁcation, respectively. Based on the same network, the proposed
VIPool consistently outperforms other graph-pooling methods. 1

Introduction
Recently, there are explosive interests in studying graph neural networks (GNNs) [32, 25, 50, 31, 12, 19, 55, 53, 9, 35, 10], which expand deep learning techniques to ubiquitous non-Euclidean graph data, such as social networks [52], bioinformatic networks [17], human activities [35] and motion interaction [28]. Achieving good performances on graph-related tasks, such as vertex classiﬁcation [32, 25, 50] and graph classiﬁcation [19, 55, 53], GNNs learn patterns from both graph structures and vertex information with feature extraction in spectral domain [5, 13, 32] or vertex domain [25, 40, 50, 34, 36, 12, 3]. Nevertheless, most GNN-based methods learn features of graphs with ﬁxed scales, which might underestimate either local or global information. To address this issue, multiscale feature learning on graphs enables capturing more comprehensive graph features for downstream tasks [6, 23, 38].
Multiscale feature learning on graphs is a natural generalization from multiresolution analysis of images, whose related techniques, such as wavelets and pyramid representations, have been well studied in both theory and practice [26, 48, 44, 1, 56]. However, this generalization is technically nontrivial. While hierarchical representations and pixel-to-pixel associations across scales are
* This work was done while Siheng Chen was working at Mitsubishi Electric Research Laboratories (MERL). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Encoder-decoder [6].
Figure 1: Architectures of multiscale graph neural networks. Our architecture adopts intermediate fusion. (b) Graph U-net [23]. (c) Readout [33]. (d) GXN (ours). straightforward for images with regular lattices, the highly irregular structures of graphs cause challenges in producing graphs at various scales [8] and aggregating features across scales.
To generate multiscale graphs, graph pooling methods are essential to compress large graphs into smaller ones. Conventional graph pooling methods [8, 45] leverage graph sampling theory and designed rules. Recently, some data-driven pooling methods are proposed, which automatically merge a ﬁne-scale vertex subset to a coarsened vertex [15, 13, 46, 43, 37, 53]. The coarsened graph, however, might not have direct vertex-to-vertex association with the original scale. Some other graph pooling methods adaptively select vertices based on their importance over the entire graph [23, 33]; however, they fail to consider local information.
To aggregate features across multiple scales, existing attempts build encoder-decoder architec-ture [6, 37, 14] to learn graph features from the latent spaces, which might underestimate ﬁne-scale information. Some other works gather the multiscale features in parallel and merge them as the ﬁnal representation [38, 23, 22, 33], which might limit information ﬂow across scales.
In this work, we design a new graph neural network to achieve multiscale feature learning on graphs, and our technical contributions are two-folds: a novel graph pooling operation to preserve informative vertices and a novel model architecture to exploit rich multiscale information.
A novel graph pooling operation: Vertex infomax pooling (VIPool). We propose a novel graph pooling operation by selecting and preserving those vertices that can maximally express their corre-sponding neighborhoods. The criterion of vertex-selection is based on the neural estimation of mutual information [2, 27, 51] between vertex and neighborhood features, thus we call the proposed pooling mechanism vertex infomax pooling (VIPool). Based on VIPool, we can implement graph pooling and unpooling to coarsen and reﬁne multiple scales of a graph. Compared to the vertex-grouping-based methods [15, 13, 46, 43, 37, 53], the proposed VIPool provides the direct vertex-vertex association across scales and makes the coarsened graph structure and information fusion easier to achieve.
Compared to other vertex-selection-based methods [23, 33], VIPool considers both local and global information on graphs by learning both vertex representation and graph structures.
A novel model architecture: Graph cross network (GXN). We propose a new model with a novel architecture called graph cross network (GXN) to achieve feature learning on multiscale graphs.
Employing the trainable VIPool, our model creates multiscale graphs in data-driven manners. To learn features from all parallel scales, our model is built with a pyramid structure. To further promote information ﬂow, we propose novel intermediate feature-crossing layers to interchange features across scales in each network layer. The intuition of feature-crossing is that the it improves information ﬂow and exploits richer multiscale information in multiple network layers rather than only combine them in the last layer. Similar crossing structures have been explored for analyzing images [49, 48], but we cannot directly use those structures for irregular graphs. The proposed feature-crossing layer handles irregular graphs by providing the direct vertex-vertex associations across multiple graph scales and network layers; see typical multiscale architectures in Figure 1, where GXN is well distinguished because intermediate feature interchanging across scales forms a crossing shape.
Remark: In each individual scale, graph U-net [23] simply uses skip connections while GXN uses multiple graph propagation layers to extract features. The proposed feature-crossing layer is used to fuse intermediate features and cannot be directly applied to graph U-net.
To test our methods, we conduct extensive experiments on several standard datasets for both graph classiﬁcation and vertex classiﬁcation. Compared to state-of-the-art methods for these two tasks,
GXN improves the average classiﬁcation accuracies by 2.12% and 1.15%, respectively. Meanwhile, based on the same model architecture, our VIPool consistently outperforms previous graph pooling methods; and more intermediate connection leads to a better performance. 1 1 The code could be downloaded at https://github.com/limaosen0/GXN 2
2