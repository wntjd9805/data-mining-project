Abstract
Networks are a natural representation of complex systems across the sciences, and higher-order dependencies are central to the understanding and modeling of these systems. However, in many practical applications such as online social networks, networks are massive, dynamic, and naturally streaming, where pairwise interactions among vertices become available one at a time in some arbitrary order. The massive size and streaming nature of these networks allow only partial observation, since it is infeasible to analyze the entire network. Under such scenarios, it is challenging to study the higher-order structural and connectivity patterns of streaming networks. In this work, we consider the fundamental problem of estimating the higher-order dependencies using adaptive sampling. We propose a novel adaptive, single-pass sampling framework and unbiased estimators for higher-order network analysis of large streaming networks. Our algorithms exploit adaptive techniques to identify edges that are highly informative for efﬁciently estimating the higher-order structure of streaming networks from small sample data. We also introduce a novel James-Stein shrinkage estimator to reduce the estimation error. Our approach is fully analytic, computationally efﬁcient, and can be incrementally updated in a streaming setting. Numerical experiments on large networks show that our approach is superior to baseline methods. 1

Introduction
Network analysis has been central to the understanding and modeling of large complex systems in various domains, e.g., social, biological, neural, and technological systems [7, 37]. These complex systems are usually represented as a network (graph) where vertices represent the components of the system, and edges represent their direct (observed) interactions over time. The success of network analysis throughout the sciences rests on the ability to describe the complex structure and dynamics of arbitrary systems using only observed pairwise interaction data among the components of the system.
Many networked systems exhibit rich structural and connectivity patterns that can be captured at the level of pairwise links (edges) or individual vertices. However, higher-order dependencies that capture complex forms of interactions remain largely unknown, since they are beyond the reach of methods that focus primarily on pairwise links. Recently, there has been a surge of studies on higher-order network analysis [4, 9, 52, 43, 20]. These methods focus on generalizing the analysis and modeling of network data from pairwise relationships (e.g., edges) to more complex forms of relationships such as multi-node (many-body) relationships (e.g., motif patterns, hypergraphs) and higher-order network paths that depend on more history [46]. Higher-order connectivity patterns were shown to change node rankings [46, 57], reshape the community structure [52, 9, 56], reveal the hub structure [4], learn more accurate embeddings [42, 41], and generative network models [16].
Many networks are massive, dynamic, and naturally streaming over time [33, 44, 3], with pairwise interactions (i.e., edges that represent communication in the form of user-to-user, user-to-product interactions) are becoming available one at a time in some arbitrary order (e.g., online social networks,
Emails, Twitter data, recommendation engines). The massive size and streaming nature of these networks allow only partial observation, since it is infeasible to analyze the entire network. Under 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
such scenarios, the question of how to study and reveal the higher-order connectivity structure and patterns of streaming networks has remained a challenge. This work is motivated by large-scale streaming network data that are generated by measurement processes (i.e., from online social media, sensors, and communication devices), and we study how to estimate the higher-order connectivity structure of streaming networks under the constraints of partial observation and limited memory. We particularly focus on the estimation of higher-order network patterns captured by small subgraphs, also called network motifs (e.g., triangles or small cliques) [34, 6].
Randomization and sampling techniques are fundamental in the context of graph and matrix ap-proximations in both static and streaming settings; see [33, 29, 26, 5]. The general problem is setup as follows: given a graph G = (V, K) and a budget m, ﬁnd a sampled graph (cid:98)G such that the (expected) number of edges (non-zero entries) is at most m and (cid:98)G is a good proxy for G. In the data streaming model, the input graph G is a stream of edges K = {k1 = (u, v), k2 = (v, w) . . . } and is partially observed as the edges stream and become available to the algorithm one at a time in some arbitrary order. The streaming model is fundamental to applications of online social networks, social media, and recommendation systems where network data become available one at a time (e.g., friendship links, emails, Twitter feeds, user-item preferences, purchase transactions, etc). Moreover, the streaming model is also crucial where network data is streaming from disk storage and random accesses of edges are too expensive. However, the theory and algorithms of current graph sampling techniques are mostly well developed for sampling individual edges to estimate global network properties (e.g., total number of edges in a graph) [25, 50]. Here, we consider instead sampling techniques that can capture how edges connect locally to form small network substructures (i.e., network motifs). Designing new sampling algorithms to estimate the local higher-order connectivity patterns of streaming networks has the potential to improve accuracy and efﬁciency of sampling and knowledge discovery in streaming networks.
Contributions. We propose a novel topologically adaptive, single-pass priority sampling framework for unbiased estimation of higher-order network connectivity structure of large streaming networks, where edges become available one at a time in some arbitrary order. Speciﬁcally, we propose unbiased estimators for local counts of subgraphs or motifs containing each edge (Theorem 1) and show how to compute them efﬁciently for streaming networks (Theorem 2). These estimators are embodied in our proposed adaptive sampling framework (see Algorithm 1).
Our proposed adaptive sampling preferen-tially selects edges to include in the sample based on their importance weight relative to the variable of interest (i.e., higher-order graph properties), then adapts their weights to allow edges to gain importance during stream processing leading to reduction in es-timation variance as compared with static and/or uniform weights.
We also propose a novel shrinkage estimator which we formulate as a convex combina-tion estimator to reduce the mean squared error (MSE) (as shown in Figure 1), and we discuss its computation during stream pro-cessing (Section 3). Our approach is fully analytic, computationally efﬁcient, and can be incrementally updated as the edges become available one at a time during stream processing. The proposed methods are also generally applicable to a wide variety of networks, including directed, undirected, weighted, and heterogeneous networks.
Figure 1: Bias-Variance Trade-off in Graph Sampling 2 Adaptive Sampling Framework 2.1 Notation and Problem Deﬁnition
Consider an arriving stream K of unique graph edges labelled by the edge identiﬁers k ∈ [|K|].
Let G = (V, K) denote the undirected graph formed by the edges, where V is the vertex set and
K is the edge set. Assume M is a motif (subgraph) pattern of interest, let H denote the class of subgraphs in G that are isomorphic to M (e.g., all triangles or cliques of a given size that appear in G). We deﬁne the H-weighted graph of G as the weighted graph GH = (V, K, N ) with edge 2
weights N = {nk : k ∈ K}, such that for each edge k ∈ K, nk is the number of subgraphs in H that are isomorphic to motif M and incident to k, i.e., nk = |{h ∈ H : h (cid:51) k, h ∼= M }|. We refer to this graph as the motif-weighted graph, and we denote A as its motif adjacency matrix [9]. For brevity we will identify a subgraph h ∈ H with its edge set. Table 3 in the supplementary materials provides a summary of notation. Suppose the edges of G are labelled in some arbitrary order based on their arrival in the stream. Let Gt = (Vt, Kt) denote the subgraph of G formed by the ﬁrst t edges in this order, Ht = {h ∈ H : h ⊂ Kt} be the set of subgraphs in H all of whose edges have arrived by t, and (Vt, Kt, Nt) be the corresponding H-weighted graph of Gt (with weights
Nt = {nk,t : k ∈ Kt}). This paper studies two questions: (1) how to maintain a reservoir sample (cid:98)K of m edges from the unweighted edge stream K, and (2) how to obtain an unbiased estimate of the
H-weighted graph GH = (Vt, Kt, Nt) at any time t ∈ [|K|]. We propose a variable-weight adaptive sampling framework for streaming network/graph data, called adaptive priority sampling. Our proposed framework preferentially selects edges to include in the sample based on their importance weight, where the weights are relative to the role of these edges in the formation of motifs and general subgraphs of interest (e.g., triangles or small cliques) and can adapt to the changing topology during streaming. Next, we describe the proposed framework (Alg. 1), and discuss its theoretical foundation. 2.2 Algorithm Description and Key Intuition
We consider a generic reservoir sample (cid:98)K selected progressively from the edge stream labelled
K = [|K|] = {1, 2, . . . , |K|}. We assume edges are unique, and therefore they can be identiﬁed by their arrival positions (i.e., edge ids); nevertheless we will sometimes emphasize their graph or time aspects, denoting by kt the edge arriving at time slot t, and by tk the arrival time slot of edge k. In
Alg. 1, the ﬁrst m edges are admitted to the sample: (cid:98)Kt = [t] for t ≤ m. Then, each subsequent edge t is provisionally included in the current sample to form (cid:98)K (cid:48) t = (cid:98)Kt−1 ∪ {t} (see line 6), from which an edge is discarded to produce the sample (cid:98)Kt, and maintain the sample size m = | (cid:98)Kt| at any time t.
Algorithm 1 Adaptive Priority Sampling (APS) (cid:46) Initialize (cid:46) Initial Weight (cid:46) Initial probability (cid:46) Add k to the sample
Generate u(k) ∼ Uni(0, 1] w(k) ← φ p(k) ← 1 (cid:98)K ← (cid:98)K ∪ {k}
// Set of motifs contain k and isomorphic to M
∆ ← {h ⊂ (cid:98)K : h (cid:51) k, h ∼= M } for h ∈ ∆ and ∀j ∈ h do
Input: Edge stream, sample size m, Motif pattern M
Output: Reservoir Sample (cid:98)K 1: (cid:98)K ← ∅, z∗ ← 0 2: for a new edge k do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: k∗ ← arg minj∈ (cid:98)K r(j) z∗ ← max{z∗, r(k∗)}
Remove k∗ from (cid:98)K w(j) ← w(j) + 1 p(h) ← (cid:81) j∈h p(j) n(j) ← n(j) + 1/p(h) (cid:46) Update count for j r(j) ← w(j)/u(j), if j (cid:54)= k (cid:46) Update Rank for j (cid:46) Rank variable for new edge (cid:46) Update threshold (cid:46) Discard min rank edge r(k) ← w(k)/u(k) if | (cid:98)K| > m then p(j) ← min{p(j), w(j)/z∗}, if j (cid:54)= k (cid:46) Update weight for j if z∗ > 0 then t
In Algorithm 1, each edge i ∈ (cid:98)K (cid:48) t is as-signed a priority rank variable deﬁned as ri,t = wi,t/ui, where wi,t is the edge weight at time t, and ui is a uni-formly distributed random variable on (0, 1] assigned to the edge on its ﬁrst arrival. Then, the edge with minimum rank zt = minj∈ (cid:98)K(cid:48) rj,t is discarded from (cid:98)K (cid:48) t to obtain the sample (cid:98)Kt (see lines 17–20). For each edge i ∈ (cid:98)K (cid:48) t, we compute the weight wi,t > 0 as a func-tion of its previous weight wi,t−1 and the sample set (cid:98)K (cid:48) t.
Upon its arrival, a new edge k is as-signed an IID edge random variable uk uniformly distributed on (0, 1], and an initial (constant) weight φ (lines 3– 5), plus the number of target sub-graphs/motifs in (cid:98)K (cid:48) t that contains k (see lines 9–15). An edge i ∈ (cid:98)K (cid:48) t survives the sampling at time t, if and only if there is another edge in (cid:98)K (cid:48) t that has the minimum rank, i.e., ri,t > zt.
Conditional on zt, the effective sampling probability of an edge i ∈ (cid:98)Kt is: P{ri,t > zt} = P{ui < wi,t/zt} = min{1, wi,t/zt}. We note that in the experiments of Section 4, we choose the initial edge weight φ = 1 to be comparable with the edge weight increments due to subgraphs incident to each edge (see line 4). This procedure allows edges to have a chance to be included in the sample with a non-zero probability, regardless of the number of subgraphs incident to them, but not so large as to damp out their topological weight. Next, we discuss how the approach in Algorithm 1 leads to unbiased estimators of general subgraphs/motifs. 3
2.3 Unbiased Estimators of General Subgraphs
Let Si,t denote the arrival of an edge i, i.e., Si,t = I(i ≤ t). For any subgraph J ⊂ K, where
J is a subset of edges (or edge ids), let SJ,t = (cid:81) i∈J Si,t indicates whether all edges i ∈ J have arrived by time t, i.e., SJ,t = 1 if J ⊂ Kt and 0 otherwise. We observe the local edge count ni,t = (cid:80)
SJ,t, and Hi,t = {h ∈ Ht : h (cid:51) i} is the set of subgraphs (motifs) incident to edge i whose edges have arrived by time t.
J∈Hi,t
Theorem 1 establishes unbiased inverse probability estimators [23] for SJ,t in the form (cid:98)SJ,t = I(J ⊂ (cid:98)Kt)/PJ,t when t ≥ τJ := maxi∈J ti (i.e., all edges in J have arrived by time t), and PJ,t is the sampling probability for the subgraph J. For any subgraph J ⊂ K with |J| ≤ m ≤ t, let Jt = J ∩[t], and deﬁne the conditional minimum edge rank over the sample (cid:98)K (cid:48) rj,t. Hence, t as zJ,t = minj∈ (cid:98)K(cid:48) zt = z∅,t is the unrestricted minimum rank over (cid:98)K (cid:48) t. For i ∈ J, we deﬁne the edge probabilities pi,t,J to be 1 when t < i and min{1, mini≤s≤t wi,s/zJ,t} otherwise. This can be expressed in an iterative form as follows, t\Jt pi,t,J = (cid:26)1, min{pi,t−1,J , wi,t/zJ,t}, if t < i if t ≥ i (1)
We distinguish between (cid:101)PJ,t and PJ,t. We use (cid:101)PJ,t = (cid:81) pi,t,J to denote the sampling probability of subgraph J at time t, conditional on the ranks of edges not in J (i.e., using the conditional min rank zJ,t). We also use PJ,t = (cid:81) pi,t, where pi,t := pi,t,∅, to denote the sampling probability of subgraph J that employs the threshold zt = z∅,t, i.e., zt is the unrestricted minimum rank over (cid:98)K (cid:48) t. i∈Jt i∈Jt
Set tJ = mini∈J ti, then deﬁne (cid:101)SJt = I(Jt ∈ (cid:98)Kt)/ (cid:101)PJ,t and the set of variables ZJ,t = {zJ,s : tJ ≤ s ≤ t}. In Theorem 1, we establish ﬁrst that (cid:101)SJ,t is an unbiased estimator of SJ,t, but that estimates can be computed using (cid:98)SJ,t. This is preferable since PJ,t is computed using the unrestricted threshold zt, independent of the subgraph J to be estimated.
Theorem 1 (Unbiased Subgraph Estimation1). (I) The distributions of the edge random variables {ui : i ∈ J}, conditional on Jt ⊂ (cid:98)Kt and ZJ,t, are independent, with each ui being uniformly distributed on (0, pi,J,t]. (II) E[I(Jt ⊂ Kt)|ZJ,t, Jt−1 ⊂ (cid:98)Kt−1] = (cid:101)PJ,t/ (cid:101)PJ,t−1 (III) E[ (cid:101)SJ,t|ZJ,t−1, Jt−1 ⊂ (cid:98)Kt−1] = (cid:101)SJ,t−1, and hence E[ (cid:101)SJ,t] = 1, for t > tJ . (IV) (cid:101)PJ,t = PJ,t when Jt ∈ (cid:98)Kt and hence E[ (cid:98)SJ,t] = SJ,t, for all t.
Using Theorem 1, it is straightforward to show that for any edge i ∈ (cid:98)Kt, (cid:98)ni,t = (cid:80) unbiased estimator of ni,t, i.e. E[(cid:98)ni,t] = ni,t.
Unbiased Estimation from the Last Arriving Edge. Recall that τJ = maxi∈J ti denotes the time of the last arriving edge kτJ of the subgraph J ⊂ K. Set J (0) = J \ {kτJ }, and deﬁne
J,t = (cid:98)SJ (0),τJ −1, where S(cid:48) (cid:98)S(cid:48)
In Alg. 1, when a new edge arrives at time t = τJ , Algorithm 1 ﬁnds all subgraphs ∆ ⊂ Ht that are completed by the arriving edge and whose edges are in the sample (cid:98)K (cid:48) t (see line 8). For each subgraph
J ∈ ∆ and each edge i ∈ J, we increment the estimate (cid:98)ni,t by the inverse probability 1/PJ (0),t−1, where PJ (0),t−1 = (cid:81)
J,t indicates subgraph J right before the arrival of the last edge kτJ . i∈J (0) pi,t−1 is the sampling probability for S(cid:48)
J∈Hi,t (cid:98)SJ,t is an
Corollary 1 results from Theorem 1 and establishes that E[ (cid:98)S(cid:48)
J,t is an unbiased estimator for ni,t, for all i ∈ Kt. This allows us to update the estimates without risking loss of some edge in J during subsequent sampling (i.e., when the edge with minimum rank is discarded from the sample).
J∈Hi,t (cid:98)S(cid:48)
Corollary 1. E[ (cid:98)S(cid:48) subgraph count ni,t for all i ∈ Kt.
J,t] = 1 and hence (cid:98)ni,t = (cid:80)
J∈Hi,t (cid:98)S(cid:48)
J,t is an unbiased estimator of the local 1Proofs of all the theorems are discussed in the supplementary materials. 4
J,t (lines 13–14).
J,t] = 1, hence, (cid:98)ni,t = (cid:80)
2.4 Special Case of Non-decreasing Sampling Weights i,t, wi,t+1/z∗ t = max{z∗ i,t+1 = min{p∗ t \ (cid:98)Kt (line 20 in Alg. 1). We deﬁne the sample threshold z∗ t−1, zt} , for t > m (see line 19 in Algorithm 1). Deﬁne p∗
Computing the probabilities pi,t according to Equation 1 requires an update for each each edge i ∈ (cid:98)Kt at each time step t, i.e., O(m) for each arriving edge. We now show that this computational cost can be reduced when wi,t is non-decreasing in t. Let dt ≤ t denote the edge discarded at time t > m, t iteratively by z∗ i.e., {dt} = (cid:98)K (cid:48) m = 0 and z∗ i,i = min{1, wi,i/z∗ i } and p∗ t+1}, for t ≥ i, i.e., similar to Equation 1 but with zt replaced by z∗ t ( as shown in line 11 in Alg. 1).
Theorem 2. When wi,t is non-decreasing in t then (I) dt (cid:54)= t implies z∗ for all t ≥ i.
We take advantage of Theorem 2 to reduce the number of updates to the probability p∗ non-decreasing and z∗
During the intervals of constant wi,t, wi,t/z∗ p∗ i,t at times when wi,t increases, all other updates of p∗ (see line 11 of Alg. 1). t is non-increasing. Therefore, provided that we update i,t can be deferred until needed for estimation t can only increase when wi,t increases. t is also non-decreasing, wi,t/z∗ t = zt; and (II) p∗ i,t, Since wi,t is i,t = pi,t
Complexity Analysis. In Algorithm 1, the sampling reservoir is implemented as a min-heap. Any insertion, deletion, update operation has O(log m) complexity in the worst case. Retrieving the edge with minimum rank is done in constant time O(1). The complexity of the weight update depends on the target subgraph class, being proportional to the number of edges in new subgraphs created by the arriving edge. In the experiments reported in this paper, the target subgraphs are triangles. For an arriving edge k = (v1, v2), the third vertex of any new triangle incident to k lies in the set intersection of the sampled neighbors of v1 and v2 which can be computed in O(min{deg(v1), deg(v2)}), where deg(v1) and deg(v2) are the sampled vertex degrees of v1 and v2 respectively. This complexity can be achieved if a hash table (or Bloom ﬁlter) is used for storing and looping over the sampled neighborhood of the vertex with minimum degree and querying the hash table of the other vertex. 3 James-Stein Shrinkage Estimator
It is common in graph sampling to seek unbiased estimators with minimum variance that perform well, e.g., the estimator in Section 2. In this section, we also investigate another desirable estimator, called shrinkage estimator [24, 21], that directly reduces the mean squared error (MSE), which is a direct measure of estimation error. In Figure 1, we demonstrate the bias-variance trade-off in graph sampling, which leads to both biased and unbiased estimators. Unbiased estimators of local subgraph counts are subject to high relative variance when the motif counts are small, because in this case the individual count estimates, scaled by the inverse probabilities, are smoothed less by aggregation.
More generally, James and Stein originated the observation that unbiased estimators do not necessarily minimize the mean squared error [24]. In their study, unbiased estimates of high dimensional Gaussian random variables are adjusted through scaling-based regularization and linear combination with dimensional averages. Shrinkage estimation has been used in other settings such as covariance or afﬁnity matrix estimation [45, 55, 11, 28]. Here, we examine shrinkage for the estimated count (cid:98)nk by convex combination with the observed and un-normalized count provided by the edge sampling weight wk. By introducing bias through wk, we can obtain further reductions in mean squared error (MSE), additional to the adaptive sampling technique discussed in Section 2. 3.1 Optimizing Shrinkage Coefﬁcients
We deﬁne a family of shrinkage estimators η = λ(cid:98)n + λw, where the shrinkage coefﬁcient λ ∈ [0, 1] speciﬁes η as a convex combination of the unbiased estimator (cid:98)n = (cid:98)nk and the un-normalized edge weight w = wk, for any edge k. Let λ denote 1 − λ. The loss L(λ) associated with the shrinkage coefﬁcient λ is the mean squared error:
L(λ) = Var((cid:98)η) + (E[(cid:98)η] − n)2 = λ2 Var((cid:98)n) + λ since E[(cid:98)η − n] = E[(cid:98)η − (cid:98)n] = E[λ(cid:98)n + λw − (cid:98)n] = λE[w − (cid:98)n].
L is convex with derivative L(cid:48) speciﬁed by, 2
Var(w) + 2λλ Cov((cid:98)n, w) + λ 2E[(cid:98)n − w]2 (2)
L(cid:48)(λ)/2 = λ Var((cid:98)n) − λ Var(w) + (1 − 2λ) Cov((cid:98)n, w) − λ(E[(cid:98)n − w])2 (3) 5
We seek the minimum of L when L(cid:48)(λ) = 0, i.e., when
λ = 1 −
Cov((cid:98)n − w, (cid:98)n)
E[((cid:98)n − w)2]
= 1 −
Var((cid:98)n) − Cov((cid:98)n, w)
E[((cid:98)n − w)2] (4)
We truncate λ at 1 so that the constraint λ ≤ 1 always holds. Since the optimal λ is a function of the unknown true covariances, we follow the practice of [12] by employing a plug-in estimator (cid:98)λ for λ by substituting ((cid:98)n − w)2 in the denominator, and an unbiased estimate for Cov((cid:98)n − w, (cid:98)n) =
Var((cid:98)nk) − Cov((cid:98)nk, wk), whose computation we describe next. 3.2 Unbiased Estimation of the Variance Var((cid:98)n)
Let ∆j,t = Hj,t \ Hj,t−1 denote the set of subgraphs in Kt that contain an edge j and are completed by the new edge arrival at time t. Similarly, let (cid:98)∆j,t denote the (possibly empty) set of subgraphs in (cid:98)K (cid:48) t and are completed by the new edge arrival at time t. Thus, the estimated count (cid:98)nj,t can be decomposed as: (cid:98)nj,t = (cid:98)nj,t−1 + (cid:80)
J∈∆j,t (cid:98)S(cid:48)
J,t.
For any pair of subgraphs J, L ∈ Hj,t, the variance of (cid:98)nj,t is speciﬁed by: t that contain an edge j ∈ K (cid:48)
Var((cid:98)nj,t) = (cid:88)
J,L∈Hj,t
Cov( (cid:98)S(cid:48)
J,t, (cid:98)S(cid:48)
L,t) (5) where Cov( (cid:98)S(cid:48)
Var((cid:98)nj,t) can also be computed incrementally at each time t as follows,
L,t) is the covariance between two subgraph estimators. Furthermore, the variance
J,t, (cid:98)S(cid:48)
Var((cid:98)nj,t) = Var((cid:98)nj,t−1) + (cid:88)
J∈∆j,t (cid:2) Var( (cid:98)S(cid:48)
J,t) + 2 Cov((cid:98)nj,t−1, (cid:98)S(cid:48)
J,t) +
Cov( (cid:98)S(cid:48)
J,t, (cid:98)S(cid:48)
L,t)(cid:3) (6) (cid:88)
L∈∆j,t
L(cid:54)=J where the term Cov((cid:98)nj,t−1, (cid:98)S(cid:48)
Theorem 3 is used to establish an unbiased estimator for Cov( (cid:98)S(cid:48)
Cov( (cid:98)S(cid:48)
L∈∆j,s
J,t, (cid:98)S(cid:48) s<t
J,t) = (cid:80) (cid:80)
L,s), for s < t.
J,t, (cid:98)S(cid:48)
L,s) in the form,
CJ,t1;L,t2 = (cid:98)S(cid:48)
J,t1 (cid:98)S(cid:48)
L,t2
− (cid:98)S(cid:48)
J\L,t1 (cid:98)S(cid:48)
L\J,t2 (cid:98)S(cid:48)
J∩L,t1∨t2 (7) where t1 ≥ t2, and t1 ∨ t2 = max{t1, t2}.
Theorem 3. CJ,t1;L,t2 is an unbiased estimator of Cov( (cid:98)S(cid:48)
J,t1
, (cid:98)S(cid:48)
L,t2
), for some time t1 ≥ t2.
A special case of Theorem 3 happens when J = L and t1 = t2 = t, which leads to V ( (cid:98)S(cid:48)
J,t) is an unbiased estimator of Var( (cid:98)S(cid:48)
J,t( (cid:98)S(cid:48) (cid:98)S(cid:48)
J,t − 1), where V ( (cid:98)S(cid:48)
J,t).
J,t) = 3.3 Unbiased Estimation of the Covariance Cov((cid:98)n, w)
Following the notation in Section 3.2, for each edge j, the weight wj,t is a random quantity incre-mented by 1 for each subgraph J ∈ ∆j,t completed by the new edge arrival at time t. Thus, wj,t can be written as a sum of random counts, i.e., un-normalized indicator functions analogous to how (cid:98)nj,t is written as a sum of inverse probability estimators. Let IJ,t = I(J ⊂ (cid:98)Kt) be the indicator of subgraph
J, and recall that J (0) is the subgraph J without the last arriving edge kτJ . Deﬁne I (cid:48)
J,t = IJ0,τJ −1, i.e., the indicator that all edges but the ﬁnal edge are present in the sample (cid:98)Kt−1 immediately before the arrival of the ﬁnal edge (kτJ of J). When the new edge kτJ arrives at time t = τJ , each edge in
J (0) has its weight incremented; see line 12 of Algorithm 1. Thus, we can write wj,t = (cid:80)
I (cid:48)
J,t, analogous to Corollary 1, and decompose wj,t = wj,t−1 + (cid:80)
J∈Hj,t
J∈∆j,t
I (cid:48)
J,t.
Computing the optimal skrinkage λ estimator in Equation 4 requires estimates of the covariance
Cov((cid:98)nj,t, wj,t) for each edge j ∈ (cid:98)Kt, which is estimated in turn and follow by linearity from the estimates of the covariance Cov( (cid:98)S(cid:48)
J,t). Theorem 4 establishes an unbiased estimator for the
J,t, I (cid:48) 6
general case of Cov( (cid:98)S(cid:48)
J1,t1 and the computation of covariance estimates2.
, I (cid:48)
J2,t2
), when t1 ≥ t2. Lemma 1 is central to both the proof of Theorem 4
Lemma 1. For J1 ∩ J2 = ∅ and t1 ≥ t2,
Cov( (cid:98)S(cid:48)
) = 0.
, I (cid:48)
J1,t1
J2,t2 then E[ (cid:98)S(cid:48)
J1,t1
I (cid:48)
J2,t2
] = E[I (cid:48)
J2,t2
] and hence
Theorem 4 (Unbiased Subgraph Covariance Estimation). (I) When t1 ≥ t2, Cov( (cid:98)S(cid:48)
) has unbiased estimator DJ1,t1;J2,t2 = (cid:98)S(cid:48)
J1,t1
I (cid:48)
J2,t2
−
, I (cid:48)
J2,t2
J1,t1
PJ1∩J2,t2I (cid:48)
J1\J2,t1 (cid:98)S(cid:48) (cid:98)S(cid:48)
J1∩J2,t1∨t2 (II) DJ1,t1;J2,t2 > 0 iff (cid:98)S(cid:48)
J1,t1
.
J2\J1,t2
> 0 and I (cid:48)
J2,t2 samples that have been taken.
> 0. Hence DJ1,t1;J2,t2 can be computed from (III) For the special case J1 = J2 = J and t1 = t2 = t then DJ,t;J,t = (cid:98)S(cid:48)
J,tP J,t = I (cid:48)
J,t(P −1
J,t − 1). 4 Experiments & Discussion
Experimental Setup. We test on graphs from different domains and with different characteristics; see [40] for data down-loads. Table 1 provides a summary of dataset characteristics, where |V | is the number of vertcies, |K| is the number of edges, T is the number of triangles, and Tmax is the maximum triangle count per edge. For all graph datasets, we con-sider an undirected, unweighted, simpli-ﬁed graph without self loops.
Table 1: Summary of Graph Statistics graph
|V |
|K|
T
Tmax
SOC-FLICKR 514K 3.2M 58.8M 2236
SOC-LIVEJOURNAL 4.03M 27.9M 83.6M 586
SOC-YOUTUBE 1.13M 2.98M 3.05M 4034 2.4M 4.7M 9.2M 1631
WIKI-TALK
WEB-BERKSTAN-DIR 685K 6.7M 64.7M 45057 3.8M 16.5M 7.5M 591
CIT-PATENTS 3.07M 117.2M 627.6M 9145
SOC-ORKUT-DIR
Edge streams are obtained by randomly permuting the edges in each graph, and the same edge order is used for all the methods. We repeat the experiment ten different times with sample fractions f = {0.10, 0.20, 0.40, 0.50}. All experiments were performed using a server with two Intel Xeon
E5-2687W 3.1GHz CPUs, 256GB of memory. The experiments are executed independently for each sample fraction. Additional results and ablation studies are discussed in the supplementary materials.
Our experimental setup is summarized as follows:
• For each sample fraction, we use Algorithm 1 to collect a sample (cid:98)K, from edge stream K.
• The experiments in this section use triangles as an example of the motif pattern M . However, the approach itself is general and applicable to any motif patterns.
• During stream processing, we compute unbiased estimators and James-Stein shrinkage estimators of the local triangle counts for the sampled edges, as discussed in Sections 2–3.
• Given a sample (cid:98)K ⊂ K, we compute the mean squared error (MSE), and the relative spectral norm [1], (cid:107)A − (cid:98)A(cid:107)2/(cid:107)A(cid:107)2, where A is the exact triangle-weighted adjacency matrix of the input graph, (cid:98)A is the average estimated triangle-weighted adjacency matrix of the sampled graph, and (cid:107)A(cid:107)2 is the spectral norm of A.
• We compare the results of Algorithm 1 with uniform sampling (i.e., reservoir sampling [53]) using the Horvitz-Thompson estimator, and we also compare with Triest sampling [48]. All baseline methods use the same experimental setup as the proposed method. 4.1 Comparison to Baseline Methods
We collect a sample of edges (cid:98)K ⊂ K from the edge stream K in a single pass, which we use to construct the motif-weighted graph, where M is the triangle motif and A is adjacency matrix of the triangle-weighted graph. We use (cid:98)A to denote the estimator of A obtained by sampling. We compute the shrinkage estimator as discussed in Section 3. And, we report the MSE at sample fraction f = 0.20 in Table 2, which demonstrates the following insight: the shrinkage estimator applied to adaptive priority (APS) sampling signiﬁcantly improves the performance of the vanilla APS which 2The computational details and proofs for shrinkage estimation are discussed with examples in the supple-mentary materials 7
Table 2: MSE and Relative Spectral Norm at sampling fraction f = 0.2. APS: Adaptive Sampling,
APS JS: APS with shrinkage Estimation, UNIF: Uniform Sampling, TRIEST: Triest Sampling.
Mean Squared Error (MSE) (cid:107)A − (cid:98)A(cid:107)2/(cid:107)A(cid:107)2 graph
APS
APS JS
UNIF
TRIEST
APS
APS JS
UNIF
TRIEST
SOC-FLICKR
SOC-LIVEJOURNAL
SOC-YOUTUBE-SNAP
WIKI-TALK
WEB-BERKSTAN-DIR
CIT-PATENTS
SOC-ORKUT-DIR 22.30K 214.80 11.35 7.70 7.32K 6.02 2.08K 295.13 16.11 6.68 5.32 561.20 3.03 70.79 0.5793 7.46K 6.3K 0.0269 293.67 257.60 0.0455 145.87 119.79 0.0105 589.92 680.67 10.70K 14.03K 0.1169 0.0187 10.91 10.59 0.1086 613.89 467.90 0.0478 0.0089 0.079 0.0359 0.0557 0.0428 0.0726 0.4321 0.429 0.4159 0.4315 0.4381 0.4325 0.4385 0.5149 0.5092 0.4982 0.5109 0.6163 0.4914 0.4241
Figure 2: Relative spectral norm (cid:107)A − (cid:98)A(cid:107)2/(cid:107)A(cid:107)2 versus the sampling fraction using all sampling methods.
Notably, APS and APS with shrinkage converge faster than uniform and Triest sampling uses Horvitz-Thompson estimator for all graphs. This is particularly clear for soc-ﬂickr and soc-orkut for which the APS shrinkage (APS JS) signiﬁcantly outperforms all the other methods.
We also consider the spectral norm as another measure of approximation quality in addition to MSE.
The spectral norm (cid:107)A − (cid:98)A(cid:107)2 was previously used for matrix approximation [1]. (cid:107)A − (cid:98)A(cid:107)2 measures the strongest linear trend of A that is not captured by the estimator (cid:98)A. This is different from the mean squared error which focused on the magnitude of the estimates.
We report the relative spectral norm (i.e., (cid:107)A − (cid:98)A(cid:107)2/(cid:107)A(cid:107)2) at sample fraction f = 0.20 for various graphs in Table 2. The experiments demonstrate that for all of the example graphs, both APS and
APS with shrinkage signiﬁcantly outperform uniform reservoir sampling and Triest sampling. One observed exception is the soc-ﬂickr graph, where the estimates using APS is signiﬁcantly high due to the high variance of Horvitz-Thompson estimation for edges with small counts. Under such scenarios, the APS with shrinkage signiﬁcantly helps and improves the original APS estimates. We also notice the difference between how the MSE ranks the best methods versus the relative spectral norm. A good example of this is the soc-orkut graph, for which APS performs worse than the baselines. However,
APS is superior to uniform sampling and Triest sampling for the relative spectral norm. Thus, despite of the large mean squared error, APS (even without shrinkage) captures the linear trend and structure of the data better than uniform reservoir sampling and Triest sampling. Finally, Figure 2 shows the convergence performance of relative spectral norm as a function of the sampling fraction. Notably,
APS and APS with shrinkage converge faster than uniform and Triest sampling, and we observe that shrinkage estimation signiﬁcantly improves the vanilla APS. 4.2 Analysis of the Estimated Distribution
We take the top-k non-zero edge weights of the exact triangle-weighted adjacency matrix A, and we compare them against their corresponding estimates obtained by sampling. Figures 3 shows the top-1M weights for APS with shrinkage estimation. Similar ﬁgures for uniform sampling and Triest sampling are reported in Section D of the supplementary materials (Fig 8 and Fig 9 respectively).
The results demonstrate the more accurate performance of APS with shrinkage estimation compared to the baseline methods; more speciﬁcally, APS with shrinkage estimation preserves the distribution and ranks of the top-k edge weights compared to uniform and Triest sampling. We report the analysis for two sampling fractions f = {0.20, 0.40}. 8
Figure 3: Each Plot corresponds to one graph at sampling fractions f = {0.20, 0.40}, and shows the estimated weight of the top-1M edges using APS with Shrinkage Estimation vs the exact edge weight. The top-1M edges are ranked based on their true triangle counts.
In Figure 4, we compare APS against APS with shrinkage estimation for the soc-livejournal graph.
The results show how the shrinkage estimator reduces the variance of APS, in particular for small local counts with high variance (i.e., as observed in the tail of the edge weight distribution). In
Section C in the supplementary materials, we discuss an ablation study of Algorithm 1. 5