Abstract
Learning latent representations of registered meshes is useful for many 3D tasks.
Techniques have recently shifted to neural mesh autoencoders. Although they demonstrate higher precision than traditional methods, they remain unable to capture ﬁne-grained deformations. Furthermore, these methods can only be applied to a template-speciﬁc surface mesh, and is not applicable to more general meshes, like tetrahedrons and non-manifold meshes. While more general graph convolution methods can be employed, they lack performance in reconstruction precision and require higher memory usage. In this paper, we propose a non-template-speciﬁc fully convolutional mesh autoencoder for arbitrary registered mesh data. It is enabled by our novel convolution and (un)pooling operators learned with globally shared weights and locally varying coefﬁcients which can efﬁciently capture the spatially varying contents presented by irregular mesh connections. Our model outperforms state-of-the-art methods on reconstruction accuracy. In addition, the latent codes of our network are fully localized thanks to the fully convolutional structure, and thus have much higher interpolation capability than many traditional 3D mesh generation models. 1

Introduction
Learning latent representations for registered meshes 2, either from performance capture or physical simulation, is a core component for many 3D tasks, ranging from compressing and reconstruction to animation and simulation. While in the past, principal component analysis (PCA) models [1, 21, 30, 35] or manually deﬁned blendshapes [31, 6, 20] have been employed to construct a linear latent space for 3D mesh data, recent works tend towards deep learning models. These models are able to produce more descriptive latent spaces, useful for capturing details like cloth wrinkles or facial expressions.
Convolutional neural networks (CNN) are widely used to capture the spatial features in regular grids, but due to the irregular sampling and connections in the mesh data, spatially-shared convolution ker-nels cannot be directly applied on meshes as in regular 2D or 3D grid data. A common compromised approach is to ﬁrst map the 3D mesh data to a predeﬁned UV space, and then train a classic 2D CNN to learn features in the UV space. However, this will inevitably suffer from the parameterization
∗Work partially done during an internship at Facebook Reality Labs, Pittsburgh. 2Registered meshes are deformable meshes with a ﬁxed topology. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
distortion and the seam/cut in the UV to warp a watertight mesh to a 2D image (See Figure ?? in the appendix), not to mention that this work-around cannot be easily extended to more general mesh data, like tetrahedron meshes.
As a more elegant approach, convolutional neural networks (CNN) designed directly for meshes or graphs were utilized in 3D autoencoders (AE) to achieve state-of-the-art (SOTA) results. Ranjan et al. [26] proposed to use spectral convolution layers and quadric mesh up-and-down sampling methods for constructing a mesh autoencoder called CoMA, and achieved promising results in aligned 3D face data. However, the spectral CNN is not adept at learning data with greater global variations and suffers from oscillation problems. To solve that, Bouritsas et al. [7] replaced the spectral convolution layer by a novel spiral convolution operator and their named Neural3DMM model achieved the state-of-the-art accuracy for both 3D aligned face data and aligned human body motion data. However, both of these methods only work for 2-manifold meshes, and still don’t achieve the precision necessary to capture ﬁne-grained deformations or motion in the data.On the other hand, cutting-edge graph convolution operators like GAT [32], MoNet [25] and FeastConv [33], although capable of being applied on general mesh data, exhibit much worse performance for accurately encoding and decoding the vertices’ 3D positions.
One major challenge in developing these non-spectral methods is to deﬁne an operator that works with different numbers of neighbors, yet maintains the weight sharing property of CNNs. It is also necessary to enable transpose convolution and unpooling layers which allow for compression and reconstruction.
With these in mind, we propose the ﬁrst template-free fully-convolutional autoencoder for arbi-trary registered meshes, like tetrahedrons and non-manifold meshes. The autoencoder has a fully convolutional architecture empowered by our novel mesh convolution operators and (un)pooling operators.
One key feature of our method is the use a spatially-varying convolution kernel which accounts for irregular sampling and connectivity in the dataset. In simpler terms, every vertex will have its own convolution kernel. While a naive implementation of a different kernel at every vertex location can be memory intensive, we instead estimate these local kernels by sampling from a global kernel weight basis. By jointly learning the global kernel weight basis, and a low dimensional sampling function for each individual kernel, we greatly reduce the number of parameters compared to a naive implementation.
In our experiments, we demonstrate that both our proposed AE and the convolution and (un)pooling operators exceed SOTA performance on the D-FAUST [4] dynamic 3D human body dataset which contains large variations in both pose and local details. Our model is also the ﬁrst mesh autoencoder that demonstrates the ability to highly compress and reconstruct high resolution meshes as well as simulation data in the form of tetrahedrons or non-manifold meshes.
In addition, our fully convolutional autoencoder architecture has the advantage of semantically meaningful localized latent codes, which enables better semantic interpolation and artistic manipulation than that with global latent features. From our knowledge, we are the ﬁrst mesh AE that can achieve localized interpolation. 2