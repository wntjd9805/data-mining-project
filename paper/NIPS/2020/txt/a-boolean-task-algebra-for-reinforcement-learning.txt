Abstract
The ability to compose learned skills to solve new tasks is an important property of lifelong-learning agents. In this work, we formalise the logical composition of tasks as a Boolean algebra. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in speciﬁc ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains—including a high-dimensional video game environment requiring function approximation—where an agent ﬁrst learns a set of base skills, and then composes them to solve a super-exponential number of new tasks. 1

Introduction
Reinforcement learning (RL) has achieved recent success in a number of difﬁcult, high-dimensional environments (Mnih et al., 2015; Levine et al., 2016; Lillicrap et al., 2016; Silver et al., 2017).
However, these methods generally require millions of samples from the environment to learn optimal behaviours, limiting their real-world applicability. A major challenge is thus in designing sample-efﬁcient agents that can transfer their existing knowledge to solve new tasks quickly. This is particularly important for agents in a multitask or lifelong setting, since learning to solve complex tasks from scratch is typically impractical.
One approach to transfer is composition (Todorov, 2009), which allows an agent to leverage existing skills to build complex, novel behaviours. These newly-formed skills can then be used to solve or speed up learning in a new task. In this work, we focus on concurrent composition, where existing base skills are combined to produce new skills (Todorov, 2009; Saxe et al., 2017; Haarnoja et al., 2018; Van Niekerk et al., 2019; Hunt et al., 2019; Peng et al., 2019). This differs from other forms of composition, such as options (Sutton et al., 1999) and hierarchical RL (Barto & Mahadevan, 2003), where actions and skills are chained in a temporal sequence.
While previous work on logical composition considers only the union and intersection of tasks (Haarnoja et al., 2018; Van Niekerk et al., 2019; Hunt et al., 2019), they do not formally deﬁne them.
However, union and intersection are operations on sets, rather than tasks. We therefore formalise the notion of union and intersection of tasks using the Boolean algebra structure, since this is the algebraic structure that abstracts the notions of union, intersection, and complement of sets. We then deﬁne a Boolean algebra over the space of optimal value functions, and then prove that there exists a homomorphism between the task and value function algebras. Given a set of base tasks that have been previously solved by the agent, any new task written as a Boolean expression can immediately be solved without further learning, resulting in a zero-shot super-exponential explosion in the agent’s abilities. We summarise our main contributions as follows: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1. Boolean task algebra: We formalise the disjunction, conjunction, and negation of tasks in a Boolean algebra structure. This extends previous composition work to encompass all
Boolean operators, and enables us to apply logic to tasks, much as we would to propositions. 2. Extended value functions: We introduce a new type of goal-oriented value function that encodes how to achieve all goals in an environment. We then prove that this richer value function allows us to achieve zero-shot composition when an agent is given a new task. 3. Zero-shot composition: We improve on previous work (Van Niekerk et al., 2019) by showing zero-shot logical composition of tasks without any additional assumptions. This is an impor-tant result as it enables lifelong-learning agents to solve a super-exponentially increasing number of tasks as the number of base tasks they learn increase.
We illustrate our approach in the Four Rooms domain (Sutton et al., 1999), where an agent ﬁrst learns to reach a number of rooms, after which it can then optimally solve any task expressible in the
Boolean algebra. We then demonstrate composition in a high-dimensional video game environment, where an agent ﬁrst learns to collect different objects, and then composes these abilities to solve complex tasks immediately. Our results show that, even when function approximation is required, an agent can leverage its existing skills to solve new tasks without further learning. 2 Preliminaries
,
A
, ρ, r), where (i) is the state space, (ii)
We consider tasks modelled by Markov Decision Processes (MDPs). An MDP is deﬁned by the tuple is the action space, (iii) ρ is a Markov transition (
S
, and (iv) r is the real-valued reward function bounded by kernel (s, a)
[rMIN, rMAX]. In this work, we focus on stochastic shortest path problems (Bertsekas & Tsitsiklis, 1991), which model tasks in which an agent must reach some goal. We therefore consider the class of
. undiscounted MDPs with an absorbing set
S
ρ(s,a) from
S × A (cid:55)→
A to
S
A
The goal of the agent is to compute a Markov policy π from that optimally solves a given task. to
A given policy π induces a value function V π(s) = Eπ [(cid:80)∞
S t=0 r(st, at)], specifying the expected return obtained under π starting from state s.1 The optimal policy π∗ is the policy that obtains the greatest expected return at each state: V π∗
. A related quantity is the Q-value function, Qπ(s, a), which deﬁnes the expected return obtained by executing a from s, and thereafter following π. Similarly, the optimal Q-value function is given by Q∗(s, a) = maxπ Qπ(s, a) for all (s, a)
. Finally, we denote a proper policy to be a
∈ S × A policy that is guaranteed to eventually reach the absorbing set (James & Collins, 2006; Van Niekerk et al., 2019). We assume the value functions for improper policies—those that never reach absorbing states—are unbounded from below. (s) = V ∗(s) = maxπ V π(s) for all s
∈ S
G
G ⊆ S 3 Boolean Algebras for Tasks and Value Functions
) and disjunction (
In this section, we develop the notion of a Boolean task algebra. This formalises the notion of task
) introduced in previous work (Haarnoja et al., 2018; Van Niekerk conjunction ( et al., 2019; Hunt et al., 2019), while additionally introducing the concept of negation (
). We then show that, having solved a series of base tasks, an agent can use its knowledge to solve tasks expressible as a Boolean expression over those tasks, without any further learning.2
¬
∨
∧
We consider a family of related MDPs
Assumption 1 (Van Niekerk et al. (2019)). For all tasks in a set of tasks
, (i) the tasks share the
M same state space, action space and transition dynamics, (ii) the transition dynamics are deterministic,
. For all non-terminal and (iii) the reward functions between tasks differ only on the absorbing set
G states, we denote the reward rs,a to emphasise that it is constant across tasks. restricted by the following assumption:
M
Assumption 1 represents the family of tasks where the environment remains the same but the goals and their desirability may vary. This is typically true for robotic navigation and manipulation tasks 1Since we consider undiscounted MDPs, we can ensure the value function is bounded by augmenting the state space with a virtual state ω such that ρ(s,a)(ω) = 1 for all (s, a) ∈ G × A, and r = 0 after reaching ω. 2Owing to space constraints, all proofs are presented in the supplementary material. 2
where there are multiple achievable goals, the goals we want the robot to achieve may vary, and how desirable those goals are may also vary. Although we have placed restrictions on the reward functions, the above formulation still allows for a large number of tasks to be represented. Importantly, sparse rewards can be formulated under these restrictions. In practice, however, all of these assumptions can be violated with minimal impact. In particular, additional experiments in the supplementary material show that even for tasks with stochastic transition dynamics and dense rewards, and which differ in their terminal states, our composition approach still results in policies that are either identical or very close to optimal. 3.1 A Boolean Algebra for Tasks
An abstract Boolean algebra is a set that satisfy the Boolean axioms of (i) idempotence, (ii) commutativity, (iii) associativity, (iv) absorption, (v) distributivity, (vi) identity, and (vii) complements.3 equipped with operators
,
∨
,
¬
∧
B
We ﬁrst deﬁne the
Deﬁnition 1. Let operators over a set of tasks.
∨ be a set of tasks which adhere to Assumption 1, with
∧
U ,
M
∅
M
∈ M such that
R rM∅ :
S × A → (s, a) (cid:55)→ min
M ∈M rM (s, a) rMU :
, and
,
¬
M
S × A → (s, a) (cid:55)→ operators over max
M ∈M
R
, and rM (s, a)
Deﬁne the
,
¬
∨
∧ as
M
:
¬
M → M
,
M
S (cid:55)→ (
, ρ, r¬M ), where r¬M :
A
R (cid:0)rMU (s, a) + rM∅ (s, a)(cid:1)
S × A → (s, a) (cid:55)→ rM (s, a)
−
:
∨
M × M → M
, (M1, M2)
S (cid:55)→ (
, ρ, rM1∨M2 ), where rM1∨M2 :
A
:
∧
M × M → M
, (M1, M2) (
S (cid:55)→
, ρ, rM1∧M2), where rM1∧M2 :
A
S × A → (s, a) (cid:55)→
S × A → (s, a) (cid:55)→
R max rM1(s, a), rM2(s, a)
{
}
R min rM1 (s, a), rM2(s, a)
}
{
In order to formalise the logical composition of tasks under the Boolean algebra structure, it is necessary that the tasks have a Boolean nature. This is enforced by the following sparseness assumption:
Assumption 2. For all tasks in a set of tasks terminal rewards consists of only two values. That is, for all (g, a) in r∅, rU
{
Given the above deﬁnitions and the restrictions placed on the set of tasks we consider, we can now deﬁne a Boolean algebra over a set of tasks.
Theorem 1. Let is a Boolean algebra. be a set of tasks which adhere to Assumption 2. Then ( which adhere to Assumption 1, the set of possible
, we have that r(g, a)
[rMIN, rMAX] with r∅
G × A
,
M rU .4
,
∧
,
∨
,
¬
} ⊂
M
M
M
M
U ,
≤
∈
∅)
Theorem 1 allows us to compose existing tasks together to create new tasks in a principled way.
Figure 1 illustrates the semantics for each of the Boolean operators in a simple environment. 3.2 Extended Value Functions
The reward and value functions described in Section 2 are insufﬁcient to solve tasks speciﬁed by the
Boolean algebra above. To understand why, consider two tasks that have multiple different goals, but at least one common goal. Clearly, there is a meaningful conjunction between them—namely, achieving the common goal. Now consider an agent that learns standard value functions for both 3We provide a description of these axioms in the supplementary material. 4While Assumption 2 is necessary to establish the Boolean algebra, we show in Theorem 3 that it is not required for zero-shot negation, disjunction, and conjunction. 3
(a) rMLEFT (b) rMDOWN (c) rM¬LEFT (d) Disjunction (e) Conjunction (f) Average
Figure 1: Consider two tasks, MLEFT and MDOWN, in which an agent must navigate to the left and bottom regions of an xy-plane respectively. From left to right we plot the reward for entering a region of the state space for the individual tasks, the negation of MLEFT, and the union (disjunction) and intersection (conjunction) of tasks. For reference, we also plot the average reward function, which has been used in previous work to approximate the conjunction operator (Haarnoja et al., 2018; Hunt et al., 2019; Van Niekerk et al., 2019). Note that by averaging reward, terminal states that are not in the intersection are erroneously given rewards. (1) (2) tasks, and which is then required to solve their conjunction without further learning. Note that this is impossible in general, since the regular value function for each task only represents the value of each state with respect to the nearest goal. That is, for all states where the nearest goal for each task is not the common goal, the agent has no information about that common goal. We therefore deﬁne extended versions of the reward and value function such that the agent is able to learn the value of achieving all goals, and not simply the nearest one. These are given by the following two deﬁnitions:
Deﬁnition 2. The extended reward function ¯r :
R is given by the mapping (cid:26)¯rMIN
S × G × A → if g
= s r(s, a) otherwise,
∈ G (s, g, a) (cid:55)→ where ¯rMIN min rMIN, (rMIN
{
−
≤ rMAX)D
, and D is the diameter of the MDP (Jaksch et al., 2010).5
}
Because we require that tasks share the same transition dynamics, we also require that the absorbing set of states is shared. Thus the extended reward function adds the extra constraint that, if the agent enters a terminal state for a different task, it should receive the largest penalty possible. In practice, we can simply set ¯rMIN to be the lowest ﬁnite value representable by the data type used for the value function.
Deﬁnition 3. The extended Q-value function ¯Q :
R is given by the mapping (s, g, a) (cid:55)→
¯r(s, g, a) +
S
S × G × A → (cid:90)
¯V ¯π(s(cid:48), g)ρ(s,a)(ds(cid:48)), t=0 ¯r(st, g, at)]. where ¯V ¯π(s, g) = E¯π [(cid:80)∞
The extended Q-value function is similar to DG functions (Kaelbling, 1993) which also learn how to achieve all goals, except here we use task-dependent reward functions as opposed to measuring distance between states. Veeriah et al. (2018) refers to this idea of learning to achieve all goals in an environment as “mastery”. We can see that the deﬁnition of extended Q-value functions encapsulates this notion for arbitrary task rewards.
The standard reward functions and value functions can be recovered from their extended versions through the following lemma.
Lemma 1. Let rM , ¯rM , Q∗ function, and optimal extended Q-value function for a task M in have (i) rM (s, a) = max
M (s, a) = max g∈G g∈G
M be the reward function, extended reward function, optimal Q-value
, we
. Then for all (s, a) in
M (s, g, a).
¯rM (s, g, a), and (ii) Q∗
M , ¯Q∗
S × A
M
¯Q∗
In the same way, we can also recover the optimal policy from these extended value functions by ﬁrst applying Lemma 1, and acting greedily with respect to the resulting value function. 5The diameter is deﬁned as D = maxs(cid:54)=s(cid:48)∈S minπ E [T (s(cid:48)|π, s)], where T is the number of timesteps required to ﬁrst reach s(cid:48) from s under π. 4 (cid:54)
Lemma 2. Denote in
S \ G deﬁne MDPs M1,g and M2,g with reward functions
S as the non-terminal states of
− =
. Let M1, M2
M
, and let each g
∈ M
G rM1,g := ¯rM1(s, g, a) and rM2,g := ¯rM2 (s, g, a) for all (s, a) in
.
S × A
Then for all g in and s in
G
π∗ g (s)
∈
−,
S arg max a∈A
Q∗
M1,g (s, a) iff π∗ g (s) arg max a∈A
∈
Q∗
M2,g (s, a).
Combining Lemmas 1 and 2, we can extract the greedy action from the extended value func-tion by ﬁrst maximising over goals, and then selecting the maximising action: π∗(s)
∈ arg maxa∈A maxg∈G ¯Q∗(s, g, a). If we consider the extended value function to be a set of standard value functions (one for each goal), then this is equivalent to ﬁrst performing generalised policy improvement (Barreto et al., 2017), and then selecting the greedy action.
Finally, much like the regular deﬁnition of value functions, the extended Q-value function can be written as the sum of rewards received by the agent until ﬁrst encountering a terminal state.
Corollary 1. Denote G∗ but not including, g. Then let M s s:g,a as the sum of rewards starting from s and taking action a up until,
M be the extended Q-value function. Then for all
, there exists a G∗
R such that and ¯Q∗
∈ M
, a
, g
∈ S
∈ G
∈ A
¯Q∗
M (s, g, a) = G∗ s:g,a ∈ s:g,a + ¯rM (s(cid:48), g, a(cid:48)), where s(cid:48) and a(cid:48) = arg max b∈A
∈ G
¯rM (s(cid:48), g, b). 3.3 A Boolean Algebra for Value Functions
In the same manner we constructed a Boolean algebra over a set of tasks, we can also do so for a set of optimal extended Q-value functions for the corresponding tasks.
Deﬁnition 4. Let ¯
Q to Assumption 1, with ¯Q∗ the
∗ be the set of optimal extended ¯Q-value functions for tasks in
∅,
∗ the optimal ¯Q-functions for the tasks which adhere
.Deﬁne
¯
Q
∗ as,
∅, ¯Q∗
∈ M
, and
M
U
M
M
¯Q∗ :
¬
S × G × A → (s, g, a) (cid:55)→
R (cid:0) ¯Q∗
U (s, g, a) + ¯Q∗
∅(s, g, a)(cid:1)
¯Q∗(s, g, a)
−
,
¬
¬
∗
∨
: ¯
Q
¯Q∗
∧
→
U ∈ operators over ¯
Q
¯
∗
Q
¯Q∗, where (cid:55)→ ¬
∨
: ¯
∗
Q ( ¯Q∗
¯
∗
Q
× 1, ¯Q∗ 2)
¯
∗
Q
¯Q∗ 1 ∨
→ (cid:55)→
∧
: ¯
∗
Q ( ¯Q∗
¯
∗
Q
× 1, ¯Q∗ 2)
→ (cid:55)→
∗
¯
Q
¯Q∗ 1 ∧
¯Q∗ 2, where ¯Q∗ 1 ∨
¯Q∗ 2, where ¯Q∗ 1 ∧
¯Q∗ 2 :
¯Q∗ 2 :
S × G × A → (s, g, a) (cid:55)→ 1(s, g, a), ¯Q∗ 2(s, g, a)
¯Q∗
{
}
R max
R
S × G × A → (s, g, a) min
{ (cid:55)→
¯Q∗ 1(s, g, a), ¯Q∗ 2(s, g, a)
} which adhere
M
Theorem 2. Let ¯
Q to Assumption 2. Then ( ¯
Q
∗,
,
∨
,
∧
¬
, ¯Q∗
U , ¯Q∗
∗ be the set of optimal extended ¯Q-value functions for tasks in
∅) is a Boolean Algebra. 3.4 Between Task and Value Function Algebras
Having established a Boolean algebra over tasks and extended value functions, we ﬁnally show that there exists an equivalence between the two. As a result, if we can write down a task under the
Boolean algebra, we can immediately write down the optimal value function for the task.
Theorem 3. Let ¯
Q
Assumption 1. Then for all M1, M2
M1∧M2 = ¯Q∗ and (iii) ¯Q∗
.
∗ be the set of optimal extended ¯Q-value functions for tasks in
, we have (i) ¯Q∗ which adhere to
,
M1 ∨
M
M1∨M2 = ¯Q∗
, (ii) ¯Q∗
¬M1 =
∈ M
¯Q∗
¯Q∗
M1
M2
¬
M1 ∧
¯Q∗
M2
¯
∗ be any map from
Q
F
M → is a homomorphism between (
Corollary 2. Let
:
. Then
M
F
,
M
,
∨
M
,
,
¬
∧ to ¯
Q
U ,
M
∗ such that
F
∅) and ( ¯
Q
M (M ) = ¯Q∗
∗,
,
∨
,
∧
¬
M for all M in
U , ¯Q∗
, ¯Q∗
∅). 5
Theorem 3 shows that we can provably achieve zero-shot negation, disjunction, and conjunction provided Assumption 1 is satisﬁed. Corollary 2 extends this result by showing that the task and value function algebras are in fact homomorphic, which implies zero-shot composition of arbitrary combinations of negations, disjunctions, and conjunctions. 4 Zero-shot Transfer Through Composition
We can use the theory developed in the previous sections to perform zero-shot transfer by ﬁrst learning extended value functions for a set of base tasks, and then composing them to solve new tasks expressible under the Boolean algebra. To demonstrate this, we conduct a series of experiments in the Four Rooms domain (Sutton et al., 1999), where an agent must navigate a grid world to a particular location. The agent can move in any of the four cardinal directions at each timestep, but colliding with a wall leaves the agent in the same location. We add a 5th action for “stay” that the agent chooses to achieve goals. A goal position only becomes terminal if the agent chooses to stay in 0.1 for all non-terminal states, and 2 it. The transition dynamics are deterministic, and rewards are at the goal.
− 4.1 Learning Base Tasks
We use a modiﬁed version of Q-learning (Watkins, 1989) to learn the extended Q-value functions described previously. Our algorithm differs in a number of ways from standard Q-learning: we keep track of the set of terminating states seen so far, and at each timestep we update the extended Q-value function with respect to both the current state and action, as well as all goals encountered so far. We also use the deﬁnition of the extended reward function, and so if the agent encounters a terminal state of a different task, it receives reward ¯rMIN. The full pseudocode is listed in the supplementary material.
If we know the set of goals (and hence potential base tasks) upfront, then it is easy to select a minimal set of base tasks that can be composed to produce the largest number of composite tasks. We ﬁrst assign a Boolean label to each goal in a table, and then use the columns of the table as base tasks.
The goals for each base task are then those goals with value 1 according to the table. In this domain, the two base tasks we select are MT, which requires that the agent visit either of the top two rooms, and ML, which requires visiting the two left rooms. We illustrate this selection procedure in the supplementary material. 4.2 Boolean Composition
Having learned the optimal extended value functions for our base tasks, we can now leverage
Theorems 1–3 to solve new tasks with no further learning. Figure 2 illustrates this composition, where an agent is able to immediately solve complex tasks such as exclusive-or. We illustrate a few composite tasks here, but note that in general, if we have K base tasks, then a Boolean algebra allows for 22K new tasks to be constructed. Thus having trained on only two tasks, our agent has enough information to solve a total of 16 composite tasks.
By learning extended value functions, an agent can subsequently solve a massive number of tasks; however, the upfront cost of learning is likely to be higher. We investigate the trade-off between the two approaches by quantifying how the sample complexity scales with the number of tasks.
We compare to Van Niekerk et al. (2019), who use regular value functions to demonstrate optimal disjunctive composition. We note that while the upfront learning cost is therefore lower, the number of tasks expressible using only disjunction is 2K 1, which is signiﬁcantly less than the full Boolean
− algebra. We also conduct a test using an extended version of the Four Rooms domain, where additional goals are placed along the sides of all walls, resulting in a total of 40 goals. Empirical results are illustrated by Figure 3.
Our results show that while additional samples are needed to learn an extended value function, the agent is able to expand the tasks it can solve super-exponentially. Furthermore, the number of base tasks we need to solve is only logarithmic in the number of goal states. For an environment with K goals, we need to learn only
+ 1 base tasks, as opposed to the disjunctive approach which (cid:99) requires K base tasks. Thus by sacriﬁcing sample efﬁciency initially, we achieve an exponential increase in abilities compared to previous work (Van Niekerk et al., 2019). log2 K (cid:98) 6
(a) ML (b) MT (c) ML ∨ MT (d) ML ∧ MT (e) ML (cid:89) MT (f) ML
−∨ MT
Figure 2: An example of zero-shot Boolean algebraic composition using the learned extended value functions. The top row shows the extended value functions. For each, the plots show the value of each state with respect to the four goals (the centre of each room). The bottom row shows the recovered regular value functions obtained by maximising over goals. Arrows represent the optimal action in a given state. (a–b) The learned optimal extended value functions for the base tasks. (c) Zero-shot disjunctive composition. (d) Zero-shot conjunctive composition. (e) Combining operators to model exclusive-or composition. (f) Composition that produces logical nor. Note that the resulting optimal value function can attain a goal not explicitly represented by the base tasks. (a) Cumulative number of sam-ples required to learn optimal ex-tended and regular value func-tions. Error bars represent stan-dard deviations over 100 runs. (b) Number of tasks that can be solved as a function of the number of existing tasks solved. Results are plotted on a log-scale. (c) Cumulative number of sam-ples required to solve tasks in a 40-goal Four Rooms domain. Er-ror bars represent standard devia-tions over 100 runs.
Figure 3: Results in comparison to the disjunctive composition of Van Niekerk et al. (2019). (a) The number of samples required to learn the extended value function is greater than learning a standard value function. However, both scale linearly and differ only by a constant factor. (b) The extended value functions allow us to solve exponentially more tasks than the disjunctive approach without further learning. (c) In the modiﬁed task with 40 goals, we need to learn only 7 base tasks, as opposed to 40 for the disjunctive case. 5 Composition with Function Approximation
Finally, we demonstrate that our compositional approach can also be used to tackle high-dimensional domains where function approximation is required. We use the same video game environment as
Van Niekerk et al. (2019), where an agent must navigate a 2D world and collect objects of different shapes and colours from any initial position. The state space is an 84 84 RGB image, and the agent is able to move in any of the four cardinal directions. The agent also possesses a pick-up action, which allows it to collect an object when standing on top of it. There are two shapes (squares and circles) and three colours (blue, beige and purple) for a total of six unique objects.
×
To learn the extended action-value functions, we modify deep Q-learning (Mnih et al., 2015) sim-ilarly to the many-goals update method of Veeriah et al. (2018). Here, a universal value function approximator (UVFA) (Schaul et al., 2015) is used to represent the action values for each state and 7
goal (both speciﬁed as RGB images).6 Additionally, when a terminal state is encountered, it is added to the collection of goals seen so far, and when learning updates occur, these goals are sampled randomly from a replay buffer. We ﬁrst learn to solve two base tasks: collecting blue objects and collecting squares. As shown in Figure 4, by training the UVFA for each task using the extended rewards deﬁnition, the agent learns not only how to achieve all goals, but also how desirable each of those goals are for the current task. These UVFAs can now be composed to solve new tasks with no further learning.
Figure 4: Extended value function for collecting blue objects (left) and squares (right). To generate the value functions, we place the agent at every location and compute the maximum output of the network over all goals and actions. We then interpolate between the points to smooth the graph. Any error in the visualisation is due to the use of non-linear function approximation.
We demonstrate composition characterised by disjunction, conjunction and exclusive-or. This corresponds to tasks where the target items are: (i) blue or square, (ii) blue squares, and (iii) blue or squares, but not blue squares. Figure 5 illustrates the composed value functions and samples of the subsequent trajectories for the respective tasks. Figure 6 shows the average returns across random initial positions of the agent.7 6