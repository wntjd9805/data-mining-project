Abstract
We study minimax methods for off-policy evaluation (OPE) using value functions and marginalized importance weights. Despite that they hold promises of over-coming the exponential variance in traditional importance sampling, several key problems remain: (1) They require function approximation and are generally biased. For the sake of trustworthy OPE, is there anyway to quantify the biases? (2) They are split into two styles (“weight-learning” vs “value-learning”). Can we unify them?
In this paper we answer both questions positively. By slightly altering the deriva-tion of previous methods (one from each style [1]), we unify them into a single value interval that comes with a special type of double robustness: when either the value-function or the importance-weight class is well speciﬁed, the interval is valid and its length quantiﬁes the misspeciﬁcation of the other class. Our interval also provides a uniﬁed view of and new insights to some recent methods, and we further explore the implications of our results on exploration and exploitation in off-policy policy optimization with insufﬁcient data coverage. 1

Introduction
A major barrier to applying reinforcement learning (RL) to real-world applications is the difﬁculty of evaluation: how can we reliably evaluate a new policy before actually deploying it, possibly using historical data collected from a different policy? Known as off-policy evaluation (OPE), the problem is genuinely difﬁcult as the variance of any unbiased estimator—including the popular importance sampling methods and their variants [2, 3]—inevitably grows exponentially in horizon [4].
To overcome this “curse of horizon”, the RL community has recently gained interest in a new family of algorithms [e.g., 5], which require function approximation of value-functions and marginalized importance weights and provide accurate evaluation when both function classes are well-speciﬁed (or realizable). Despite that fast progress is made in this direction, several key problems remain:
• The methods are generally biased since they rely on function approximation. Is there anyway we can quantify the biases, which is important for trustworthy evaluation?
• The original method by Liu et al. [5] estimates the marginalized importance weights (“weight”) using a discriminator class of value-functions (“value”). Later, Uehara et al. [1] swap the roles of value and weight to learn a Q-function using weight discriminators. Not only we have two styles of methods now (“weight-learning” vs “value-learning”), each of them also ignores some important components of the data in their core optimization (see Sec. 3 for details). Can we have a uniﬁed method that makes effective use of all components of data? 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper we answer both questions positively. By modifying the derivation of one method from each style [1], we unify them into a single value interval, which automatically comes with a special type of double robustness: when either the weight- or the value-class is well speciﬁed, the interval is valid and its length quantiﬁes the misspeciﬁcation of the other class (Sec. 4). Each bound is computed from a single optimization program that uses all components of the data, which we show is generally tighter than the naïve intervals developed from previous methods. Our derivation also uniﬁes several recent OPE methods and reveals their simple and direct connections; see Table 1 in the appendix. Furthermore, we examine the potential of applying these value bounds to two long-standing problems in RL: reliable off-policy policy optimization under poor data coverage (i.e., exploitation), and efﬁcient exploration. Based on a simple but important observation, that poor data coverage can be treated as a special case of importance-weight misspeciﬁcation, we show that optimizing our lower and upper bounds over a policy class corresponds to the well-established pessimism and optimism principles for these problems, respectively (Sec. 5).
On Statistical Errors We assume exact expectations and ignore statistical errors in most of the derivations, as our main goal is to quantify the biases and unify existing methods. In Appendix L we show how to theoretically handle the statistical errors by adding generalization error bounds to the interval. That said, these generalization bounds are typically loose for practical purposes, and we handle statistical errors by bootstrapping in the experiments (Section 4.4) and show its effectiveness empirically. We also refer the readers to concurrent works that provide tighter and/or more efﬁcient computation of conﬁdence intervals for related estimators [6, 7]. 2 Preliminaries
Markov Decision Processes An inﬁnite-horizon discounted MDP is speciﬁed by (S, A, P, R, γ, s0), where S is the state space, A is the action space, P : S × A → ∆(S) is the transition function (∆(·) is probability simplex), R : S × A → ∆([0, Rmax]) is the reward function, and s0 is a known and deterministic starting state, which is w.l.o.g.1 For simplicity we assume S and A are ﬁnite and discrete but their cardinalities can be arbitrarily large. Any policy2 π : S → A induces a distribution of the trajectory, s0, a0, r0, s1, a1, r1, . . . , where s0 is the starting state, and ∀t ≥ 0, at = π(st), rt ∼ R(st, at), st+1 ∼ P (st, at). The expected discounted return determines the performance of policy π, which is deﬁned as J(π) := E[(cid:80)∞ t=0 γtrt | π]. It will be useful to deﬁne the discounted (state-action) occupancy of π as dπ(s, a) := (cid:80)∞ t (·, ·) is the marginal distribution of (st, at) under policy π. dπ behaves like an unnormalized distribution ((cid:107)dπ(cid:107)1 = 1/(1 − γ)), and for notational convenience we write Edπ [·] with the understanding that t (s, a), where dπ t=0 γtdπ
Edπ [f (s, a, r, s(cid:48))] := (cid:80) s∈S,a∈A dπ(s, a) Er∼R(s,a),s(cid:48)∼P (s,a)[f (s, a, r, s(cid:48))].
With this notation, the discounted return can be written as J(π) = Edπ [r].
The policy-speciﬁc Q-function Qπ satisﬁes the Bellman equations: ∀s ∈ S, a ∈ A, Qπ(s, a) =
Er∼R(s,a),s(cid:48)∼P (s,a)[r + γQπ(s(cid:48), π)] =: (T πQπ)(s, a), where Qπ(s(cid:48), π) is a shorthand for
Qπ(s(cid:48), π(s(cid:48))) and T π is the Bellman update operator. It will be also useful to keep in mind that
J(π) = Qπ(s0, π). (1)
Data and Marginalized Importance Weights In off-policy RL, we are passively given a dataset and cannot interact with the environment to collect more data. The goal of OPE is to estimate J(π) for a given policy π using the dataset. We assume that the dataset consists of i.i.d. (s, a, r, s(cid:48)) tuples, where (s, a) ∼ µ, r ∼ R(s, a), s(cid:48) ∼ P (s, a). µ ∈ ∆(S × A) is a distribution from which we draw (s, a), which determines the exploratoriness of the dataset. We write Eµ[·] as a shorthand for taking expectation w.r.t. this distribution. The strong i.i.d. assumption is only meant to simplify derivation and presentation, and does not play a crucial role in our results as we do not handle statistical errors.
A concept crucial to our discussions is the marginalized importance weights. Given any π, if µ(s, a) > 0 whenever dπ(s, a) > 0, deﬁne wπ/µ(s, a) := dπ(s,a)
µ(s,a) . When there exists (s, a) such that µ(s, a) = 0 but dπ(s, a) > 0, wπ/µ does not exist (and hence cannot be realized by any function class). When 1When s0 ∼ d0 is random, all our derivations hold by replacing q(s0, π) with Es0∼d0 [q(s0, π)]. 2Our derivation also applies to stochastic policies. 2
it does exist, ∀f , Edπ [f (s, a, r, s(cid:48))] = Ewπ/µ[f (s, a, r, s(cid:48))] := Eµ[wπ/µ(s, a)f (s, a, r, s(cid:48))], where
Ew[·] := Eµ[w(s, a) · (·)] is a shorthand we will use throughout the paper, and µ is omitted in Ew[·] since importance weights are always applied on the data distribution µ. Finally, OPE would be easy if we knew wπ/µ, as
J(π) = Edπ [r] = Ewπ/µ[r]. (2)
Function Approximation Throughout the paper, we assume access to two function classes Q ⊂ (S × A → R) and W ⊂ (S × A → R). To develop intuition, they are supposed to model Qπ and wπ/µ, respectively, though most of our main results are stated without assuming any kind of realizability. We use C(·) to denote the convex hull of a set. We also make the following compactness assumption so that inﬁma and suprema we introduce later are always attainable.
Assumption 1. We assume Q and W are compact subsets of RS×A. 3