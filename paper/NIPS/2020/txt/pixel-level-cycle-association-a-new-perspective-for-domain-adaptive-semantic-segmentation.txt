Abstract
Domain adaptive semantic segmentation aims to train a model performing sat-isfactory pixel-level predictions on the target with only out-of-domain (source) annotations. The conventional solution to this task is to minimize the discrepancy between source and target to enable effective knowledge transfer. Previous domain discrepancy minimization methods are mainly based on the adversarial training.
They tend to consider the domain discrepancy globally, which ignore the pixel-wise relationships and are less discriminative. In this paper, we propose to build the pixel-level cycle association between source and target pixel pairs and contrastively strengthen their connections to diminish the domain gap and make the features more discriminative. To the best of our knowledge, this is a new perspective for tackling such a challenging task. Experiment results on two representative domain adaptation benchmarks, i.e. GTAV → Cityscapes and SYNTHIA → Cityscapes, verify the effectiveness of our proposed method and demonstrate that our method performs favorably against previous state-of-the-arts. Our method can be trained end-to-end in one stage and introduces no additional parameters, which is expected to serve as a general framework and help ease future research in domain adaptive semantic segmentation. Code is available at https://github.com/kgl-prml/Pixel-Level-Cycle-Association. 1

Introduction
Semantic segmentation is a challenging task because it requires pixel-wise understandings of the image which is highly structured. Recent years have witnessed the huge advancement in this area, mainly due to the rising of deep neural networks. However, without massive pixel-level annotations, to train a satisfactory segmentation network remains challenging. In this paper, we deal with the semantic segmentation in the domain adaptation setting which aims to make accurate pixel-level predictions on the target domain with only out-of-domain (source) annotations.
The key to the domain adaptive semantic segmentation is how to make the knowledge learned from the distinct source domain better transferred to the target. Most previous works employ adversarial training to minimize the domain gap existing in the image [11, 12, 47] or the representations
[41, 31, 44] or both [19, 49]. Most adversarial training based methods either focus on the discrepancy globally or treat the pixels from both domains independently. All these methods ignore the abundant in-domain and cross-domain pixel-wise relationships and are less discriminative. Recently, many works resort to self-training [48, 27, 54] to boost the segmentation performance. Although self-training leads to promising improvement, it largely relies on a good initialization, is hard to tune and
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
usually leads to a result with large variance. Our work focuses on explicitly minimizing the domain discrepancy and is orthogonal to the self-training based method.
In this paper, we provide a new perspective to diminish the domain gap via exploiting the pixel-wise similarities across domains. We build the cross-domain pixel association cyclically and draw the associated pixel pairs closer compared to the unassociated ones. Speciﬁcally, we randomly sample a pair of images (i.e. a source image and a target image). Then starting from each source pixel, we choose the most similar target pixel and in turn ﬁnd its most similar source one, based on their features. The cycle-consistency is satisﬁed when the starting and the end source pixels come from the same category. The associations of the source-target pairs which satisfy the cycle-consistency are contrastively strengthened compared to other possible connections. Because of the domain gap and the association policy tending to choose the easy target pixels, the associated target pixels may only occupy a small portion of the whole image/feature map. In order to provide guidance for all of the target pixels, we perform spatial feature aggregation for each target pixel and adopt the internal pixel-wise similarities to determine the importance of features from other pixels. In this way, the gradients with respect to the associated target pixels can also propagate to the unassociated ones.
We verify our method on two representative benchmarks, i.e. GTAV → Cityscapes and SYNTHIA
→ CityScapes. With mean Intersection-over-Union (mIoU) as the evaluation metric, our method achieves more than 10% improvement compared to the network trained with source data only and performs favorably against previous state-of-the-arts.
In a nutshell, our contributions can be summarized as: 1) We provide a new perspective to diminish the domain shift via exploiting the abundant pixel-wise similarities. 2) We propose to build the cross-domain pixel-level association cyclically and contrastively strengthen their connections. Moreover, the gradient can be diffused to the whole target image based on the internal pixel-wise similarities. 3)
We demonstrate the effectiveness of our method on two representative benchmarks. 2