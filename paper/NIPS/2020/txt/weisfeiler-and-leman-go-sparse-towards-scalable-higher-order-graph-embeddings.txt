Abstract
Graph kernels based on the 1-dimensional Weisfeiler-Leman algorithm and corre-sponding neural architectures recently emerged as powerful tools for (supervised) learning with graphs. However, due to the purely local nature of the algorithms, they might miss essential patterns in the given data and can only handle binary relations.
The k-dimensional Weisfeiler-Leman algorithm addresses this by considering k-tuples, deﬁned over the set of vertices, and deﬁnes a suitable notion of adjacency between these vertex tuples. Hence, it accounts for the higher-order interactions between vertices. However, it does not scale and may suffer from overﬁtting when used in a machine learning setting. Hence, it remains an important open problem to design WL-based graph learning methods that are simultaneously expressive, scalable, and non-overﬁtting. Here, we propose local variants and corresponding neural architectures, which consider a subset of the original neighborhood, making them more scalable, and less prone to overﬁtting. The expressive power of (one of) our algorithms is strictly higher than the original algorithm, in terms of ability to distinguish non-isomorphic graphs. Our experimental study conﬁrms that the local algorithms, both kernel and neural architectures, lead to vastly reduced computation times, and prevent overﬁtting. The kernel version establishes a new state-of-the-art for graph classiﬁcation on a wide range of benchmark datasets, while the neural version shows promising performance on large-scale molecular regression tasks. 1

Introduction
Graph-structured data is ubiquitous across application domains ranging from chemo- and bioinfor-matics [10, 103] to image [101] and social network analysis [27]. To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in the graph structure, as well as the feature information contained within nodes and edges. In recent years, numerous approaches have been proposed for machine learning with graphs—most notably, approaches based on graph kernels [71] or using graph neural networks (GNNs) [19, 45, 47].
Here, graph kernels based on the 1-dimensional Weisfeiler-Leman algorithm (1-WL) [46, 111], and corresponding GNNs [83, 115] have recently advanced the state-of-the-art in supervised node and graph learning. Since the 1-WL operates via simple neighborhood aggregation, the purely local nature of these approaches can miss important patterns in the given data. Moreover, they are only applicable to binary structures, and therefore cannot deal with general t-ary structures, e.g., hypergraphs [124] or subgraphs, in a straight-forward way. A provably more powerful algorithm (for graph isomorphism testing) is the k-dimensional Weisfeiler-Leman algorithm (k-WL) [15, 46, 77]. The algorithm can capture more global, higher-order patterns by iteratively computing a coloring (or discrete labeling) for k-tuples, instead of single vertices, based on an appropriately deﬁned notion of adjacency between two k-tuples. However, it ﬁxes the cardinality of this neighborhood to k · n, where n denotes the number of
∗CERC in Data Science for Real-Time Decision-Making, Polytechnique Montréal
†Department of Computer Science, RWTH Aachen University
‡Department of Computer Science, University of Bonn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
vertices of a given graph. Hence, the running time of each iteration does not take the sparsity of a given graph into account. Further, new neural architectures [77, 78] that possess the same power as the k-WL in terms of separating non-isomorphic graphs suffer from the same drawbacks, i.e., they have to resort to dense matrix multiplications. Moreover, when used in a machine learning setting with real-world graphs, the k-WL may capture the isomorphism type, which is the complete structural information inherent in a graph, after only a couple of iterations, which may lead to overﬁtting, see [82], and the experimental section of the present work.
Present work To address this, we propose a local version of the k-WL, the local δ-k-dimensional
Weisfeiler-Leman algorithm (δ-k-LWL), which considers a subset of the original neighborhood in each iteration. The cardinality of the local neighborhood depends on the sparsity of the graph, i.e., the degrees of the vertices of a given k-tuple. We theoretically analyze the strength of a variant of our local algorithm and prove that it is strictly more powerful in distinguishing non-isomorphic graphs compared to the k-WL. Moreover, we devise a hierarchy of pairs of non-isomorphic graphs that a variant of the δ-k-LWL can separate while the k-WL cannot. On the neural side, we devise a higher-order graph neural network architecture, the δ-k-LGNN, and show that it has the same expressive power as the δ-k-LWL. Moreover, we connect it to recent advancements in learning theory for GNNs [41], which show that the δ-k-LWL architecture has better generalization abilities compared to dense architectures based on the k-WL. See Figure 1 for an overview of the proposed algorithms.
Experimentally, we apply the discrete algorithms (or kernels) and the (local) neural architectures to supervised graph learning, and verify that both are several orders of magnitude faster than the global, discrete algorithms or dense, neural architectures, and prevent overﬁtting. The discrete algorithms establish a new state-of-the-art for graph classiﬁcation on a wide range of small- and medium-scale classical datasets. The neural version shows promising performance on large-scale molecular regression tasks.