Abstract
Learning robotic grasps from visual observations is a promising yet challenging task. Recent research shows its great potential by preparing and learning from large-scale synthetic datasets. For the popular, 6 degree-of-freedom (6-DOF) grasp setting of parallel-jaw gripper, most of existing methods take the strategy of heuristically sampling grasp candidates and then evaluating them using learned scoring functions. This strategy is limited in terms of the conﬂict between sampling efﬁciency and coverage of optimal grasps. To this end, we propose in this work a novel, end-to-end Grasp Proposal Network (GPNet), to predict a diverse set of 6-DOF grasps for an unseen object observed from a single and unknown camera view. GPNet builds on a key design of grasp proposal module that deﬁnes anchors of grasp centers at discrete but regular 3D grid corners, which is ﬂexible to support either more precise or more diverse grasp predictions. To test GPNet, we contribute a synthetic dataset of 6-DOF object grasps; evaluation is conducted using rule-based criteria, simulation test, and real test. Comparative results show the advantage of our methods over existing ones. Notably, GPNet gains better simulation results via the speciﬁed coverage, which helps achieve a ready translation in real test. Our code and dataset are available on https://github.com/CZ-Wu/GPNet. 1

Introduction
Robotic object grasping is one of the basic functions that a robot system aims to emulate our human beings. The task is challenging due to imprecision in sensing, planning, and actuation, and also due to the possible absence of knowledge about physical properties of the object (e.g., mass distribution and surface material). It was recently demonstrated that deep learning on annotated datasets of robotic grasp can achieve good robustness and generalization [13, 21, 14, 10, 11]. Methods based on synthetic data (e.g., object CAD models and the correspondingly rendered images) [17, 5, 32] show particular promise, as they can ideally generate as many as inﬁnite numbers of grasp annotations.
Even though there exists a risk of domain discrepancy between simulated and real environments, deep learning models trained on such synthetic datasets show remarkable performance on real-world grasp testings, with better generalization to novel object instances and categories [17, 5]. In this work, we study deep learning optimal grasp conﬁgurations from synthetic images, with a particular focus on grasping with a parallel-jaw gripper, whose parametrization is typically of 6 degrees of freedom (6-DOFs), including 3D gripper center of the grasping location and 3D gripper orientation.
∗Authors contributed equally.
†Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Optimal grasp conﬁgurations depend on working conditions in real-world environments. In many cases, due to kinematical constraints of robotic arms and/or possible collisions, a diverse set of multiple grasp predictions are expected such that there exist grasps among the predictions that can be successfully actuated. There generally exist two strategies to predict multiple grasps for a given object. The ﬁrst strategy is used in [29, 30], which samples grasp candidates from observed object surface via heuristic manners, and then evaluates them using learned scoring functions; alternatively, full object surface model is assumed in [32, 15] to sample more reliable grasp candidates during the test phase. The second strategy learns to directly predict multiple grasps. We argue that the ﬁrst strategy is limited in the following aspects: (1) sampling can only be made ﬁnite, making it possible to miss optimal grasps, (2) increasing the density of grasp candidate sampling increases linearly the computation costs of both the sampling itself and the subsequent grasp estimation — note that sampling itself costs signiﬁcantly [29]. To address the limitation, a ﬁrst attempt is made in [20] that learns a latent grasp space via variational auto-encoder (VAE), and promising grasps can be obtained by sampling from the learned latent space. However, we empirically ﬁnd that grasps given by the
VAE model of [20] tend to focus on a single mode, e.g., centers of their predicted grasps are close to the object mass center; in order words, their generated grasps are less diverse.
In this work, we propose a novel end-to-end solution of Grasp Proposal Network (GPNet), in order to predict a diverse set of 6-DOF grasps for an unseen object observed from a single and unknown camera view. Figure 1 illustrates our pipeline. GPNet builds on a key design of grasp proposal module that deﬁnes anchors of grasp centers at a discrete set of regular 3D grid corners. It stacks three headers on top of the grasp proposal module, which for any grasp proposal, are trained to respectively specify antipodal validity [2], regress a grasp prediction and score the conﬁdence of ﬁnal grasp. The proposed GPNet is in fact ﬂexible enough to support either more precise or more diverse grasp predictions, by focusing or spreading the anchors of grasp centers in the 3D space. To test
GPNet, we contribute a synthetic dataset of 6-DOF object grasps, including 22.6M annotated grasps for 226 object models. We evaluate our proposed GPNet in terms of rule-based criteria, simulation test, and also real test. Experiments show the advantages of our method over existing ones. 2