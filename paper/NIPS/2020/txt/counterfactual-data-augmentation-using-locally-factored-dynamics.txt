Abstract
Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent causal mechanisms. Such local causal structures can be leveraged to improve the sample efﬁciency of sequence prediction and off-policy reinforcement learning.
We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for Counterfactual Data Augmenta-tion (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We ﬁnd that CoDA signiﬁcantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings.1 1

Introduction
High-dimensional dynamical systems are often composed of simple subprocesses that affect one another through sparse interaction. If the subprocesses never interacted, an agent could realize signif-icant gains in sample efﬁciency by globally factoring the dynamics and modeling each subprocess independently [28, 29]. In most cases, however, the subprocesses do eventually interact and so the prevailing approach is to model the entire process using a monolithic, unfactored model. In this paper, we take advantage of the observation that locally—during the time between their interactions—the subprocesses are causally independent. By locally factoring dynamic processes in this way, we are able to capture the beneﬁts of factorization even when their subprocesses interact on the global scale.
Consider a game of billiards, where each ball can be viewed as a separate physical subprocess.
Predicting the opening break is difﬁcult because all balls are mechanically coupled by their initial placement. Indeed, a dynamics model with dense coupling amongst balls may seem sensible when considering the expected outcomes over the course of the game, as each ball has a non-zero chance of colliding with the others. But at any given timestep, interactions between balls are usually sparse.
One way to take advantage of sparse interactions between otherwise disentangled entities is to use a structured state representation together with a graph neural network or other message passing transition model that captures the local interactions [26, 39]. When it is tractable to do so, such architectures can be used to model the world dynamics directly, producing transferable, task-agnostic models. In many cases, however, the underlying processes are difﬁcult to model precisely, and model-free [46, 87] or task-oriented model-based [18, 63] approaches are less biased and exhibit superior performance. In this paper we argue that knowledge of whether or not local interactions 1Code available at https://github.com/spitis/mrl 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Counterfactual Data Augmentation (CoDA). Given 3 factual samples, knowledge of the local causal structure lets us mix and match factored subprocesses to form counterfactual samples.
The ﬁrst proposal is rejected because one of its factual sources (the blue ball) is not locally factored.
The third proposal is rejected because it is not itself factored. The second proposal is accepted, and can be used as additional training data for a reinforcement learning agent. occur is useful in and of itself, and can be used to generate causally-valid counterfactual data even in absence of a forward dynamics model. In fact, if two trajectories have the same local factorization in their transition dynamics, then under mild conditions we can produce new counterfactually plausible data using our proposed Counterfactual Data Augmentation (CoDA) technique, wherein factorized subspaces of observed trajectory pairs are swapped (Figure 1). This lets us sample from a counterfactual data distribution by stitching together subsamples from observed transitions. Since
CoDA acts only on the agent’s training data, it is compatible with any agent architecture (including unfactored ones).
In the remainder of this paper, we formalize this data augmentation strategy and discuss how it can improve performance of model-free RL agents in locally factored tasks. Our main contributions are: 1. We deﬁne local causal models (LCMs), which are induced from a global model by conditioning on a subset of the state space, and show how local structure can simplify counterfactual reasoning. 2. We introduce CoDA as a generalized data augmentation strategy that is able to leverage local factorizations to manufacture unseen, yet causally valid, samples of the environment dynamics.
We show that goal relabeling [36, 1] and visual augmentation [2, 46] are instances of CoDA that use global independence relations and we propose a locally conditioned variant of CoDA that swaps independent subprocesses to form counterfactual experiences (Figure 1). 3. Using an attention-based method for discovering local causal structure in a disentangled state space, we show that our CoDA algorithm signiﬁcantly improves the sample efﬁciency in standard, batch-constrained, and goal-conditioned reinforcement learning settings. 2 Local Causality in MDPs 2.1 Preliminaries and Problem Setup
The basic model for decision making in a controlled dynamic process is a Markov Decision Process (MDP), described by tuple (cid:104)S, A, P, R, γ(cid:105) consisting of the state space, action space, transition function, reward function, and discount factor, respectively [72, 83]. Note that MDPs generalize uncontrolled Markov processes (set A = ∅), so that our work applies also to sequential prediction.
We denote individual states and actions using lowercase s ∈ S and a ∈ A, and variables using the uppercase S and A (e.g., s ∈ range(S) ⊆ S). A policy π : S × A → [0, 1] deﬁnes a probability distribution over the agent’s actions at each state, and an agent is typically tasked with learning a parameterized policy πθ that maximizes value EP,π
In most non-trivial cases, the state s ∈ S can be described as an object hierarchy together with global context. For instance, this decomposition will emerge naturally in any simulated process or game that is deﬁned using a high-level programming language (e.g., the commonly used Atari [7] or Minecraft
[35] simulators). In this paper we consider MDPs with a single, known top-level decomposition of the state space S = S 1 ⊕ S 2 ⊕ · · · ⊕ S n for ﬁxed n, leaving extensions to hierarchical decomposition and t γtR(st, at). (cid:80) 2
multiple representations [32, 17], dynamic factor count n [92], and (learned) latent representations
[13] to future work. The action space might be similarly decomposed: A = A1 ⊕ A2 ⊕ · · · ⊕ Am.
Given such state and action decompositions, we may model time slice (t, t+1) using a structural causal model (SCM) Mt = (cid:104)Vt, Ut, F(cid:105) ([65], Ch. 7) with directed acyclic graph (DAG) G, where: t[+1]}2n+m t[+1]}2n+m i=0
• Vt = {V i i=0 = {S1 t . . . Sn t , A1 t . . . Am t , S1 t+1 . . . Sn t+1} are the nodes (variables) of G.
• Ut = {U i is a set of noise variables, one for each V i, determined by the initial state, past actions, and environment stochasticity. We assume that noise variables at time t + 1 are independent from other noise variables: U i t[+1]∀i, j. The instance u = (u1, u2, . . . , u2n+m) of Ut denotes an individual realization of the noise variables. t+1 ⊥⊥ U j
• F = {f i}2n+m i=0 is a set of functions (“structural equations”) that map from U i t[+1], where Pa(V i to V i associated with the set of incoming edges to node V i t[+1]) ⊂ Vt \ V i t[+1] are the parents of V i t[+1] in G; see, e.g., Figure 2 (center). t[+1] × Pa(V i t[+1]) t[+1] in G; hence each f i is
Note that while Vt, Ut, and Mt are indexed by t (their distributions change over time), the structural equations f i ∈ F and causal graph G represent the global transition function P and apply at all times t. To reduce clutter, we drop the subscript t on V , U , and M when no confusion can arise.
Critically, we require the set of edges in G (and thus the number of inputs to each f i) to be structurally minimal ([67], Remark 6.6).
Assumption (Structural Minimality). V j ∈ Pa(V i) if and only if there exists some {ui, v−ij} with ui ∈ range(U i), v−ij ∈ range(V \ {V i, V j}) and pair (vj 2 ∈ range(V j) such that 1 = f i({ui, v−ij, vj vi 1}) (cid:54)= f i({ui, v−ij, vj 2) with vj 2}) = vi 2. 1, vj 1, vj
Intuitively, structural minimality says that V j is a parent of V i if and only if setting the value of V j can have a nonzero direct effect2 on the child V i through the structural equation f i. The structurally minimal representation is unique [67].
Given structural minimality, we can think of edges in G as representing global causal dependence.
The probability distribution of Si t+1) together with its noise t+1 ⊥⊥ V j | Pa(Si variable Ui; that is, we have P (Si t+1) for all nodes V j (cid:54)∈ Pa(Si t+1). We call an MDP with this structure a factored MDP [37]. When edges in G are sparse, factored MDPs admit more efﬁcient solutions than unfactored MDPs [28]. t+1 is fully speciﬁed by its parents Pa(Si t+1 | St, At) = P (Si t+1)) so that Si t+1 | Pa(Si 2.2 Local Causal Models (LCMs)
Limitations of Global Models Unfortunately, even if states and actions can be cleanly decomposed into several nodes, in most practical scenarios the DAG G is fully connected (or nearly so): since the f i apply globally, so too does structural minimality, and edge (Si k+1) at time k is present so long as there is a single instance—at any time t, no matter how unlikely—in which Si t+1.
In the words of Andrew Gelman, “there are (almost) no true zeros” [24]. As a result, the factorized causal model Mt, based on globally factorized dynamics, rarely offers an advantage over a simpler causal model that treats states and actions as monolithic entities (e.g., [12]). t inﬂuences Sj k, Sj t ∈ Pa(Sj t , Sj t+1 ⊥⊥ V i t+1) with V i t | Pa(Sj
LCMs Our key insight is that for each pair of nodes (V i t+1) in G, there often exists a large subspace L(j⊥⊥i) ⊂ S ×A for which Sj t , (st, at) ∈ L(j⊥⊥i).
For example, in case of a two-armed robot (Figure 2), there is a large subspace of states in which the two arms are too far apart to inﬂuence each other physically. Thus, if we restrict our attention to (st, at) ∈ L(j⊥⊥i), we can consider a local causal model ML(j⊥⊥i) is strictly sparser than the global DAG G, as the structural minimality assumption applied to GL(j⊥⊥i) t to Sj implies that there is no edge from V i t+1. More generally, for any subspace L ⊆ S × A, we can induce the Local Causal Model (LCM) ML t , F L(cid:105) with DAG GL from the global model t = (cid:104)V L
Mt as: whose local DAG GL(j⊥⊥i) t+1)\V i t , U L t 2Thus parentage does describe knock-on effects, e.g. V1 on V3 in the Markov chain V1 → V2 → V3. 3
Figure 2: A two-armed robot (left) might be modeled as an MDP whose state and action spaces decompose into left and right subspaces: S = S L ⊕ S R, A = AL ⊕ AR. Because the arms can touch, the global causal model (center left) between time steps is fully connected, even though left-to-right and right-to-left connections (dashed red edges) are rarely active. By restricting our attention to the subspace of states in which left and right dynamics are independent we get a local causal model (center right) with two components that can be considered separately for training and inference. i=0
• V L t = {V L,i t = {U L,i i=0
• F L = {f L,i}2n+m t[+1]}2n+m t[+1]}2n+m
• U L
, where P (V L,i t[+1] | (st, at) ∈ L). t[+1]) = P (V i t[+1]) = P (U i
, where P (U L,i t[+1] | (st, at) ∈ L).
, where f L,i = f i|L (f i with range of input variables restricted to L). Due to structural minimality, the signature of f L,i may shrink (as the range of the relevant variables is now restricted to L), and corresponding edges in G will not be present in GL.3 i=0
In case of the two-armed robot, conditioning on the arms being far apart simpliﬁes the global DAG to a local DAG with two connected components (Figure 2). This can make counterfactual reasoning considerably more efﬁcient: given a factual situation in which the robot’s arms are far apart, we can carry out separate counterfactual reasoning about each arm. t are removed from Gdo(Si t =x) = (cid:104)V, U, Fx(cid:105), where Fx = F \ f i ∪ {Si
Leveraging LCMs To see the efﬁciency therein, consider a general case with global causal model
M. To answer the counterfactual question, “what might the transition at time t have looked like if component Si t had value x instead of value y?”, we would ordinarily apply Pearl’s do-calculus to t = x} and incoming
M to obtain submodel Mdo(Si edges to Si t =x) [65]. The component distributions at time t + 1 can be computed by reevaluating each function f j that depends on Si t. When Si t has many children (as is often the case in the global G), this requires one to estimate outcomes for many structural equations
{f j|V j ∈ Children(V i t = y) and its new value (with
Si t = x) are in the set L, the intervention is “within the bounds” of local model ML and we can instead work directly with local submodel ML t =x) (deﬁned accordingly). The validity of this do(Si follows from the deﬁnitions: since f L,j = f j|L for all of Si t’s children, the nodes V k t for k (cid:54)= i at time t are held ﬁxed, and the noise variables at time t + 1 are unaffected, the distribution at time t + 1 is the same under both models. When Si t has fewer children in ML than in M, this reduces the number of structural equations that need to be considered. t )}. But if both the original value of St (with Si 3 Counterfactual Data Augmentation
We hypothesize that local causal models will have several applications, and potentially lead to improved agent designs, algorithms, and interpretability. In this paper we focus on improving off-policy learning in RL by exploiting causal independence in local models for Counterfactual Data
Augmentation (CoDA). CoDA augments real data by making counterfactual modiﬁcations to a subset of the causal factors at time t, leaving the rest of the factors untouched. Following the logic outlined in the Subsection 2.2, this can understood as manufacturing “fake” data samples using the counterfactual model M[L] and resample their children. While this is always possible using a model-based approach if we have good models of the structural equations, it is particularly nice when the causal mechanisms are independent, as we can do counterfactual reasoning directly by reusing subsamples from observed trajectories.
, where we modify the causal factors Si...j do(Si...j t =x) t 3As a trivial example, if f i is a function of binary variable V j, and L = {(s, a) | V j = 0}, then f L,i is not a function of V j (which is now a constant), and there is no longer an edge from V j to V i in GL. 4
Figure 3: Four instances of CoDA; orange nodes are relabeled, noise variables omitted for clarity.
First: Goal relabeling [36], including HER [1], augments transitions with counterfactual goals.
Second: Visual feature augmentation [2, 46] uses domain knowledge to change visual features SV t (such as textures, lighting, and camera positions) that the designer knows do not impact the physical state SP t+1. Third: Dyna [82], including MBPO [34], augments real states with new actions and resamples the next state using a learned dynamics model. Fourth (ours): Given two transitions that share local causal structures, we propose to swap connected components to form new transitions.
Deﬁnition. The causal mechanisms represented by subgraphs Gi, Gj ⊂ G are independent when Gi and Gj are disconnected in G.
When G is divisible into two (or more) connected components, we can think of each subgraph as an independent causal mechanism that can be reasoned about separately.
Existing data augmentation techniques can be interpreted as speciﬁc instances of CoDA (Figure 3).
For example, goal relabeling [36], as used in Hindsight Experience Replay (HER) [1] and Q-learning for Reward Machines [33], exploits the independence of the goal dynamics Gt (cid:55)→ Gt+1 (identity map) and the next state dynamics St × At (cid:55)→ St+1 in order to relabel the goal variable Gt with a counterfactual goal. While the goal relabeling is done model-free, we typically assume knowledge of the goal-based reward mechanism Gt × St × At × St+1 (cid:55)→ Rt+1 to relabel the reward, ultimately mixing model-free and model-based reasoning. Similarly, visual feature augmentation, as used in reinforcement learning from pixels [46, 41] and sim-to-real transfer [2], exploits the independence of the physical dynamics SP t+1 such as textures and camera position, assumed to be static (SV t ), to counterfactually augment visual features.
Both goal relabeling and visual data augmentation rely on global independence relationships. t+1 and visual feature dynamics SV t t × At (cid:55)→ SP t+1 = SV (cid:55)→ SV
We propose a novel form of Counterfactual Data Augmentation that uses knowledge of local indepen-dence relationships. In particular, we observe that whenever an environment transition is within the bounds of some local model ML whose graph GL has the locally independent causal mechanism
Gi as a disconnected subgraph (note: Gi itself need not be connected), that transition contains an unbiased sample from Gi. Thus, given two transitions in L, we may mix and match the samples of Gi to generate counterfactual data, so long as the resulting transitions are themselves in L.
Remark 3.1. How much data can we generate using our CoDA algorithm? If we have n independent samples from subspace L whose graph GL has m connected components, we have n choices for each of the m components, for a total of nm CoDA samples—an exponential increase in data! One might term this the “blessing of independent subspaces.”
Remark 3.2. Our discussion has been at the level of a single transition (time slice (t, t + 1)), which is consistent with the form of data that RL agents typically consume. But we could also use CoDA to mix and match locally independent components over several time steps (see, e.g., Figure 1).
Remark 3.3. As is typical, counterfactual reasoning changes the data distribution. While off-policy agents are typically robust to distributional shift, future work might explore different ways to control or prioritize the counterfactual data distribution [77, 43]. We note, however, that certain prioritization schemes may introduce selection bias [31], effectively entangling otherwise independent causal mechanisms (e.g., HER’s “future” strategy [1] may introduce “hindsight bias” [45, 78]).
Remark 3.4. The global independence relations relied upon by goal relabeling and image aug-mentation are incredibly general, as evidenced by their wide applicability. We posit that certain local independence relations are similarly general. For example, the physical independence of objects separated by space (the billiards balls of Figure 1, the two-armed robot of Figure 2, and the environments used in Section 4), and the independence between an agent’s actions and the truth of (but not belief about) certain facts the agent is ignorant of (e.g., an opponent’s true beliefs). 5
Algorithm 1 Mask-based Counterfactual Data Augmentation (CoDA) function CODA(transition t1, transition t2): function MASK(state s, action a): s1, a1, s1’ ← t1 s2, a2, s2’ ← t2 m1, m2 ← MASK(s1, a1), MASK(s2, a2)
D1 ← COMPONENTS(m1)
D2 ← COMPONENTS(m2) d ← random sample from (D1 ∩ D2)
˜s, ã, ˜s’ ← copy(s1, a1, s1’)
˜s[d], ã[d], ˜s’[d] ← s2[d], a2[d], s2’[d]
˜D ← COMPONENTS(MASK(˜s, ã)) return (˜s, ã, ˜s’) if d ∈ ˜D else ∅
Returns (n + m) × (n) matrix indicating if the n next state components (columns) locally depend on the n state and m action components (rows). function COMPONENTS(mask m):
Using the mask as the adjacency matrix for GL (with dummy columns for next action), ﬁnds the set of connected components C = {Cj}, and returns the set of independent components
D = {Gi = (cid:83) k | Ci ⊂ powerset(C)}. k Ci
Implementing CoDA We implement CoDA, as outlined above and visualized in Figure 3(d), as a function of two factual transitions and a mask function M (st, at) : S × A → {0, 1}(n+m)×n that represents the adjacency matrix of the sparsest local causal graph GL such that L is a neighborhood of (st, at).4 We apply M to each transition to obtain local masks m1 and m2, compute their connected components, and swap independent components Gi and Gj (mutually disjoint and collectively exhaus-tive groups of connected components) between the transitions to produce a counterfactual proposal.
We then apply M to the counterfactual (˜st, ˜at) to validate the proposal—if the counterfactual mask
˜m shares the same graph partitions as m1 and m2, we accept the proposal as a CoDA sample. See
Algorithm 1.
Note that masks m1, m2 and ˜m correspond to different neighborhoods L1, L2 and ˜L, so it is not clear that we are “within the bounds” of any model ML as was required in Subsection 2.2 for valid counterfactual reasoning. To correct this discrepancy we use the following proposition and additionally require the causal mechanisms (subgraphs) for independent components Gi and Gj to share structural equations in each local neighborhood: f L1,i = f L2,i = f ˜L,i and f L1,j = f L2,j = f ˜L,j.5 This makes our reasoning valid in the local subspace L∗ = L1 ∪ L2 ∪ ˜L. See Appendix A for proof.
Proposition 1. The causal mechanisms represented by Gi, Gj ⊂ G are independent in GL1∪L2 if and only if Gi and Gj are independent in both GL1 and GL2 , and f L1,i = f L2,i, f L1,j = f L2,j.
Since CoDA only modiﬁes data within local subspaces, this biases the resulting replay buffer to have more factorized transitions. In our experiments below, we specify the ratio of observed-to-counterfactual data heuristically to control this selection bias, but ﬁnd that off-policy agents are reasonably robust to large proportions of CoDA-sampled trajectories. We leave a full characterization of the selection bias in CoDA to future studies, noting that knowledge of graph topology was shown to be useful in mitigating selection bias for causal effect estimation [5, 6].
Inferring local factorization While the ground truth mask function M may be available in rare cases as part of a simulator, the general case either requires a domain expert to specify an approximate causal model (as in goal relabeling and visual data augmentation) or requires the agent to learn the local factorization from data. Given how common independence due to physical separation of objects is, the former option will often be available. In the latter case, we note that the same data could also be used to learn a forward model. Thus, there is an implicit assumption in the latter case that learning the local factorization is easier than modeling the dynamics. We think this assumption is rather mild, as an accurate forward dynamics model would subsume the factorization, and we provide some empirical evidence of its validity in Section 4.
Learning the local factorization is similar to conditional causal structure discovery [81, 75, 67], conditioned on neighborhood L of (st, at), except that the same structural equations must be applied globally (if the structural equations were conditioned on L, Proposition 1 would fail). As there are many algorithms for general structure discovery [81, 75], and the arrow of time simpliﬁes the inquiry 4If Jacobian ∂P/∂x exists at x = (st, at), the ground truth M (st, at) equals |(∂P/∂x)T | > 0. 5To see why this is not trivially true, imagine there are two rooms, one of which is icy. In either room the ground conditions are locally independent of movement dynamics, but not so if we consider their union. 6
[27, 67], there may be many ways to approach this problem. For now, we consider a generalization of the global network mask approach used by MADE [25] (eq. 10) for autoregressive distribution modeling and GraN-DAG [44] (eq. 6) for causal discovery, which additionally conditions the mask on the current state and action.
This approach computes a locally conditioned network mask M (st, at) by taking the matrix product of locally conditioned layer masks: M (st, at) = ΠL (cid:96)=1M(cid:96)(st, at). This mask can be understood as an upper bound on the network’s absolute Jacobian (see Appendix C). Again, there may be several models allow one to compute conditional layer masks. We tested two such models: a mixture of MLP experts and a single-head set transformer architecture [85, 47]. Each is described in more detail in
Appendix C. Both are trained to model forward dynamics using an L2 prediction loss and induce a sparse network mask either via a sparsity penality (in case of the mixture of experts model) or via a sparse attention mechanism (in case of the set transformer). In preliminary experiments (Appendix
C) we found that the set transformer performed better, and proceed to use it in our main experiments (Section 4). The set transformer uses the attention mask at each layer as the layer mask for that layer, so that the network mask is simply the product of the attention masks. Though trained to model forward dynamics, the CoDA models are used by the agent to infer local factorization rather than to directly sample future states as is typical in model-based RL. We found this produced reasonable results in the tested domains (below). See Appendix C for details. Future work should consider other approaches to inferring local structure such as graph neural networks [39, 13]. 4 Experiments
Our experiments evaluate CoDA in the online, batch, and goal-conditioned settings, in each case
ﬁnding that CoDA signiﬁcantly improves agent performance as compared to non-CoDA baselines.
Since CoDA only modiﬁes an agent’s training data, we expect these improvements to extend to other off-policy task settings in which the state space can be accurately disentangled. Below we outline our experimental design and results, deferring speciﬁc details and additional results to Appendix B.
Standard online RL We extend Spriteworld [89] to construct a “bouncing ball” environment (right), that consists of multiple objects (sprites) that move and collide within a conﬁned 2D canvas. We use tasks of varying difﬁculty, where the agent must navigate N ∈ {1, 2, 3, 4} of 4 sprites to their ﬁxed target positions.
The agent receives reward of 1/N for each of the N sprites placed; e.g., the hardest task (Place 4) gives 1/4 reward for each of 4 sprites placed. For each task, we use CoDA to expand the replay buffer of a TD3 agent [22] by about 8 times. We compare CoDA with a ground truth masking function (available via the Spriteworld environment) and learned masking function to the base TD3 agent, as well as a Dyna agent that generates additional training data by sampling from a model. For fair comparison, we use the same transformer used for CoDA masks for Dyna, which we pretrain using approximately 42,000 samples from a random policy. As in
HER, we assume access to the ground truth reward function to relabel the rewards. The results in
Figure 4 show that both variants of CoDA signiﬁcantly improve sample complexity over the baseline.
By contrast, the Dyna agent suffers from model bias, even though it uses the same model as CoDA.
Figure 4: Standard online RL (3 seeds): CoDA with the ground truth mask always performs the best, validating our basic idea. CoDA with a pretrained model also offers a signiﬁcant early boost in sample efﬁciency and maintains its lead over the base TD3 agent throughout training. Using the same model to generate data directly (a la Dyna [82]) performs poorly, suggesting signiﬁcant model bias. 7
|D| (1000s) 25 50 75 100 150 250
Real data 1R 13.2 ± 0.7 22.8 ± 3.0 43.2 ± 4.9 63.0 ± 3.1 77.4 ± 1.2 78.2 ± 2.7
MBPO 1R:1M 18.5 ± 1.5 36.6 ± 4.3 46.0 ± 4.7 66.4 ± 4.9 72.6 ± 5.6 77.9 ± 2.4
Ratio of Real:CoDA [:MBPO] data (ours) 1R:1C 1R:3C 1R:5C 1R:3C:1M 43.8 ± 2.8 66.6 ± 3.8 73.4 ± 2.8 77.8 ± 2.0 82.2 ± 1.8 85.0 ± 2.9 40.9 ± 2.5 64.4 ± 3.1 76.7 ± 2.6 82.7 ± 1.5 85.8 ± 1.4 87.8 ± 1.8 38.4 ± 4.9 62.5 ± 3.5 75.0 ± 3.4 76.6 ± 3.0 84.2 ± 1.0 87.0 ± 1.0 46.8 ± 3.1 70.4 ± 3.8 74.6 ± 3.2 73.7 ± 2.9 79.7 ± 3.6 78.3 ± 4.9
Table 1: Batch RL (10 seeds): Mean success (± standard error, estimated using 1000 bootstrap resamples) on Pong environment. CoDA with learned masking function more than doubles the effective data size, resulting in a 3x performance boost at smaller data sizes. Note that a 1R:5C
Real:CoDA ratio performs slightly worse than a 1R:3C ratio due to distributional shift (Remark 3.3).
Batch RL A natural setting for CoDA is batch-constrained RL, where an agent has access to an existing transition-level dataset, but cannot collect more data via exploration [21, 48]. This makes any additional, high quality data invalu-able. Another reason why CoDA is attractive in this setting is that there is no a priori reason to prefer the given batch data distribution to a counterfactual one. For this experiment we use a continuous control Pong environment based on RoboschoolPong [40]. The agent must hit the ball past the opponent, receiving reward of +1 when the ball is behind the opponent’s paddle, -1 when the ball is behind the agent’s paddle, and 0 otherwise. Since our transformer model performed poorly when used as a dynamics model, our Dyna baseline for batch RL adopts a state-of-the-art architecture [34] that employs a 7-model ensemble (MBPO). We collect datasets of up to 250,000 samples from an pretrained policy with added noise.
For each dataset, we train both mask and reward functions (and in case of MBPO, the dynamics model) on the provided data and use them to generate different amounts of counterfactual data. We also consider combining CoDA with MBPO, by ﬁrst expanding the dataset with MBPO and then applying CoDA to the result. We train the same TD3 agent on the expanded datasets in batch mode for 500,000 optimization steps. The results in Table 1 show that with only 3 state factors (two paddles and ball), applying CoDA is approximately equivalent to doubling the amount of real data.
Fig. 5: Goal-conditioned RL (5 seeds): In
FetchPush and the challenging Slide2 envi-ronment, a HER agent whose dataset has been enlarged with CoDA approximately doubles the sample efﬁciency of the base HER agent.
Goal-conditioned RL As HER [1] is an instance of prioritized CoDA that greatly improves sample efﬁciency in sparse-reward tasks, can our unprioritized
CoDA algorithm further improve HER agents? We use HER to relabel goals on real data only, relying on random CoDA-style goal relabeling for CoDA data.
After ﬁnding that CoDA obtains state-of-the-art results in FetchPush-v1 [71], we show that CoDA also accelerates learning in a novel and signiﬁcantly more challenging Slide2 environment, where the agent must slide two pucks onto their targets (Figure 5).
For this experiment, we speciﬁed a heuristic mask using domain knowledge (“objects are disentangled if more than 10cm apart”) that worked in both FetchPush and Slide2 despite different dynamics. 5