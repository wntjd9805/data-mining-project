Abstract
In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efﬁciently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the beneﬁt of variational semantic memory in boosting few-shot recognition. 1

Introduction
Memory plays an essential role in human intelligence, especially for aiding learning and reasoning in the present. In machine intelligence, neural memory [22, 73, 23] has been shown to enhance neural networks by augmentation with an external memory module. For instance, episodic memory storing past experiences helps reinforcement learning agents adapt more quickly and improve sample efﬁciency [7, 55, 24]. Memory is well-suited for few-shot learning by meta-learning in that it offers an effective mechanism to extract inductive bias [21] by accumulating prior knowledge from a set of previously observed tasks. One of the primary issues when designing a memory module is deciding what information should be memorized, which usually depends on the problems to solve. Though being highly promising, it is non-trivial to learn to store useful information in previous experience, which should be as non-redundant as possible. Existing few-shot learning works with external memory typically store the information from the support set of the current task [41, 70, 57, 40, 30], focusing on learning the access mechanism, which is assumed to be shared across tasks. The memory used in these works is short-term with limited capacity [18, 39] in that long-term information is not well retained, despite the importance for efﬁciently learning new tasks.
Semantic memory, also known as conceptual knowledge [43, 67, 59], refers to general facts and common world knowledge gathered throughout our lives [52]. It enables humans to quickly learn
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
new concepts by recalling the knowledge acquired in the past [59]. Compared to episodic memory, semantic memory has been less studied [67, 59], despite its pivotal role in remembering the past and imagining the future [29]. By its very nature, semantic memory can provide conceptual context to facilitate novel event construction [28] and support a variety of cognitive activities, e.g., object recognition [5]. We draw inspiration from the cognitive function of semantic memory and introduce it into meta-learning to learn to collect long-term semantic knowledge for few-shot learning.
In this paper, we propose an external memory module to accrue and store long-term semantic information gained from past experiences, which we call variational semantic memory. The function of semantic memory closely matches that of prototypes [61, 1], which identify the semantics of objects in few-shot classiﬁcation. The semantic knowledge accumulated in the memory helps build the new object concepts represented by prototypes typically obtained from only one or few samples [61]. We apply our variational semantic memory module to the probabilistic inference of class prototypes modelled as distributions. The probabilistic prototypes obtained are more informative and therefore better represent categories of objects compared to deterministic vectors [61, 1]. We formulate the memory recall as a variational inference of the latent memory, which is an intermediate stochastic variable. This offers a principled way to retrieve information from the external memory and incorporate it into the inference of class prototypes for each individual task. We cast the optimization as a hierarchical variational inference problem in the Bayesian framework; the parameters of the inference of prototypes are jointly optimized in conjunction with the memory recall and update.
The semantic memory is gradually consolidated throughout the course of learning by updating the knowledge from new observations in each experienced task via an attention mechanism. The long-term semantic knowledge on seen object categories is acquired, maintained and enhanced during the learning process. This contrasts with existing works [57, 70] in which the memory stores data from the support set and therefore only considers the short term. In our memory, each entry stores semantics representing a distinct object category by summarizing feature representations of class samples. This reduces redundant information and saves storage overhead. More importantly it avoids collapsing memory reading and writing into single memory slots [22, 74], which ensures that rich context information is provided for better construction of new concepts.
To summarize our three contributions: i) We propose variational semantic memory, a long-term memory module, which learns to acquire semantic information and enables new concepts of object categories to be quickly learned for few-shot learning. ii) We formulate the memory recall as a variational inference problem by introducing the latent memory variable, which offers a principled iii) We introduce variational way to retrieve relevant information that ﬁts with speciﬁc tasks. semantic memory into the probabilistic inference of prototypes modelled as distributions rather than deterministic vectors, which provides more informative representations of class prototypes. 2 Method
Few-shot classiﬁcation is commonly learned by constructing T few-shot tasks from a large dataset and optimizing the model parameters on these tasks. A task, also called an episode, is deﬁned as an N -way K-shot classiﬁcation problem [70, 50]. An episode is drawn from a dataset by randomly sampling a subset of classes. Data points in an episode are partitioned into a support S and query Q set. We adopt the episodic optimization [70], which trains the model in an iterative way by taking one episode-update at a time. The update of the model parameters is deﬁned by a variational learning objective, which is based on an evidence lower bound (ELBO) [6]. Different from traditional machine learning tasks, meta-learning for few-shot classiﬁcation trains the model on the meta-training set, and evaluates on the meta-test set, whose classes are not seen during meta-training.
In this work, we develop our method based on the prototypical network (ProtoNet) [61]. Speciﬁcally, the prototype zn of an object class n is obtained by: zn = 1 k Φ(xn,k) where Φ(xn,k) is the
K feature embedding of the sample xn,k, which is usually obtained by a convolutional neural network.
For each query sample x, the distribution over classes is calculated based on the softmax over distances to the prototypes of all classes in the embedding space: (cid:80) p(yn = 1|x) = exp(−d(Φ(x), zn)) n(cid:48) exp(−d(Φ(x), zn(cid:48)))
, (cid:80) (1) 2
where y denotes a random one-hot vector, with yn indicating its n-th element, and d(·, ·) is some distance function. Due to its non-parametric nature, the ProtoNet enjoys high ﬂexibility and efﬁciency, achieving great success in few-shot learning.
The ideal prototypical representation should be expressive and encompass enough intra-class variance, while being distinguishable between different classes. In the literature [61, 1], however, the prototypes are commonly modeled by a single or multiple deterministic vectors obtained by average pooling of only a few samples or clustering. Hence, they are not sufﬁciently representative of object categories.
Moreover, uncertainty is inevitable due to the scarcity of data, which should also be encoded into the prototypical representations. In this paper, we derive a probabilistic latent variable model by modeling prototypes as distributions, which are learned by variational inference. 2.1 Variational Prototype Inference
We introduce the probabilistic modeling of class prototypes, in which we treat the prototype z of each class as a distribution. In the few-shot learning scenario, to ﬁnd z is to infer the posterior p(z|x, y), where (x, y) denotes the sample from the query set Q. We derive a variational inference framework to solve z by leveraging the support set S.
Consider the conditional log-likelihood in a probabilistic latent variable model, where we incorporate the prototype z as the latent variable
|Q| (cid:89) log (cid:2) p(yi|xi)(cid:3) = log (cid:2)
|Q| (cid:89) (cid:90) p(yi|xi, z)p(z|xi)dz(cid:3), (2) i=1 i=1 where p(z|xi) is the conditional prior in which we make the prototype dependent on xi. In general, it is intractable to directly solve the posterior, and usually we resort to a variational distribution to approximate the true posterior by minimizing the KL divergence:
DKL[q(z|S)||p(z|x, y)], (3) where q(z|S) is the variational posterior that makes the prototype z dependent on the support set S to leverage the meta-learning setting for few-shot classiﬁcation. By applying the Baye’s rule, we obtain log (cid:2)
|Q| (cid:89) i=1 p(yi|xi)(cid:3) ≥
|Q| (cid:88) (cid:104) i=1
Eq(z|S) (cid:2) log p(yi|xi, z)(cid:3) − DKL(q(z|S)||p(z|xi)) (cid:105)
, (4) which is the ELBO of the conditional log-likelihood in (2). In practice, the variational posterior q(z|S) is implemented by a neural network that takes the average feature representations of samples in the support set S and returns the mean and variance of the prototype z. This can be directly adopted as the optimization objective for the variational inference of the prototype. While inheriting the
ﬂexibility of the prototype based few-shot learning [61, 1], our probabilistic inference enhances its class expressiveness by exploring higher-order information, i.e., variance, beyond a single or multiple deterministic mean vectors of samples in each class. More importantly, the probabilistic modeling provides a principled way of incorporating prior knowledge acquired from experienced tasks. In what follows, we introduce the external memory to augment the probabilistic latent model for enhanced variational inference of prototypes. 2.2 Variational Semantic Memory
We introduce the variational semantic memory to accumulate and store the semantic information from previous tasks for the inference of prototypes of new tasks. The knowledge on objects in the memory is consolidated episodically by seeing more object instances, which enables conceptual representations of new objects to be quickly built up for novel categories in tasks to come.
To be more speciﬁc, we deploy an external memory unit M which stores a key-value pair in each row of the memory array as [22]. The keys are the average feature representations of images from the same classes and the values are their corresponding class labels. The semantics of object categories in the memory provide context for quickly learning concepts of new object categories by seeing only a few examples in the current tasks. In contrast to most existing external memory modules [57, 47, 8], our variational semantic memory module stores semantic information by summarizing samples from individual categories, and therefore our memory module requires relatively light storage overhead, enabling more efﬁcient retrieval of content from the memory. 3
Memory recall and inference
It is pivotal to recall relevant information from the external memory and adapt it to learning new tasks when working with neural memory modules. When recalling a memory, it is not simply a read out; the content from the memory must be processed in order to ﬁt the data in a speciﬁc task [47, 22, 73]. We regard memory recall as a decoding process of chosen content in the memory, which we accomplish via variational inference, instead of simply reading out the raw content from the external memory and directly incorporating it into speciﬁc tasks.
To this end, we introduce an intermediate stochastic variable, referred to as the latent memory m. We cast the retrieval of memory into the inference of m from the addressed memory M ; the memory addressing is based on the similarity between the content in the memory and the support set from the current task. The latent memory m is inferred to connect the accrued semantic knowledge stored in the long-term memory to the current task, which is seamlessly coupled with the prototype inference under a hierarchical Bayesian framework.
From a Bayesian perspective, the prototype posterior can be inferred by marginalizing over the latent memory variable m: (cid:90) q(z|S) = q(z|m, S)p(m|S)dm, (5) where q(z|m, S) indicates that the prototype z is now dependent on the support set S and the latent memory m. To leverage the external memory M , we design a variational approximation q(m|M, S) to the posterior over the latent memory m by inferring from M conditioned on S: q(m|M, S) =
|M | (cid:88) a=1 p(m|Ma, S)p(a|M, S). (6)
Here, a is the addressed categorical variable, Ma denotes the corresponding memory content at address a, and |M | represents the memory size, i.e., the number of memory entries.
We establish a hierarchical Bayesian framework for the variational inference of prototypes:
˜q(z|M, S) = (cid:90) p(a|M, S)
|M | (cid:88) a=1 q(z|S, m)p(m|Ma, S)dm, (7) which is shown as a graphical model in Figure 1. We use the support set S and memory M to generate the categorical variable a to address the external memory, and then fetch the content Ma to infer the latent memory m, which is incorporated as a conditional variable to assist S in the inference of the prototype z. This offers a principled way to incorporate semantic knowledge and build up the prototypes of novel object categories. It mimics the cognitive mechanism of the human brain in learning new concepts by associating them with related concepts learned in the past [29]. Moreover, it naturally handles ambiguity and uncertainty when recalling memory better than the common strategy of using a deterministic transformation [22, 73].
When a is given, m only depends on Ma and no longer relies on S. Therefore, we can attain p(m|Ma, S) = p(m|Ma) by safely dropping S, which gives rise to: q(m|M, S) =
|M | (cid:88) a=1 p(m|Ma)p(a|M, S). (8)
Since the memory size is ﬁnite, bounded by the number of seen classes, we further approximate q(m|M, S) empirically by q(m|M, S) =
|M | (cid:88) a=1
λap(m|Ma), λa = exp (cid:0)g(Ma, S)(cid:1) i exp (cid:0)g(Mi, S)(cid:1) , (cid:80) (9) where Ma is the memory slot and stores the average feature representation of samples in each category that are seen in the learning stage, and g(·, ·) is a learnable similarity function, which we implement as a dot product for efﬁciency by taking the averages of samples in Mi and S, respectively.
Thus, the prototype inference can now be approximated by Monte Carlo sampling:
˜q(z|M, S) ≈ 1
J
J (cid:88) j=1 q(z|m(j), S), m(j) ∼
|M | (cid:88) a=1
λap(m|Ma), (10) where J is the number of Monte Carlo samples. 4
Figure 1: Graphical illustration of the proposed probabilistic prototype inference with variational semantic memory. M is the semantic memory module. St n denotes the samples from the n-th class in the support set in each t task. Qt is the query set. T is the number of tasks, and N is the number of classes in each task.
Memory update and consolidation The memory update is an important operation in the main-tenance of memory, which should be able to effectively absorb new useful information to enrich memory content. We draw inspiration from the concept formation process in the human cognitive function [29]: the concept of an object category is formed and grown by seeing a set of similar objects of the same category. The memory is built from scratch and gradually consolidated by being episodically updated with knowledge observed from a series of related tasks. We adopt an attention mechanism to refresh content in the memory by taking into account the structural information of data.
To be more speciﬁc, the memory is empty at the beginning of the learning. When a new task arrives, we directly append the mean feature representation of data from a given category to the memory entries if this category is not seen. Otherwise, for seen categories, we update the memory content with new observed data from the current task using self-attention [68] similar to the graph attention mechanism [69]. This enables the structural information of data to be better explored for memory update. We ﬁrst construct the graph with respect to the memory Mc to be updated. The nodes c ∈ Rd, of the graph are a set of feature representations: Hc = {h0
Nc = |Sc ∪ Qc|, h0 c), hφ(·) is the convolutional neural network for feature extraction, and xi c ∈ {Sc ∪ Qc} contains all samples including both the support and query set from the c-th category in the current task. c }, where hNc c = Mc, hi>0 c = hφ(xi c, . . . , hNc c, h1 c, h2
We use the nodes Hc on the graph to generate a new representation of memory Mc, which better explores structural information of data. To do so, we need to compute a similarity coefﬁcient between Mc and the nodes hi c on the graph. We implement this by a single-layer feed-forward neural network parameterized by a weight vector h ∈ R2d, that is, ei c] with [·, ·] being a
Mc concatenation operation. Here, ei indicates the importance of node i’s features to node Mc. In
Mc practice, we use the following normalized similarity coefﬁcients [69]:
= w(cid:62)[Mc, hi
βi
Mc
= softmaxi(ei
Mc
) = exp(LeakyReLU (cid:0)w(cid:62)[Mc, hi j=0 exp(LeakyReLU c](cid:1)) w(cid:62)[Mc, hj c] (cid:16) (cid:80)Nc
. (cid:17)
) (11)
We can now compute a linear combination of the feature representations of the nodes on the graph as the ﬁnal output representation of ¯Mc:
¯Mc = σ (cid:32) Nc(cid:88) (cid:33)
βi
Mc hi c
, (12) i=0 where σ(·) is a nonlinear activation function, e.g., softmax. The graph attention operation can effectively ﬁnd and assimilate the most useful information from the samples in the new task. We update the memory content with an attenuated weighted average,
Mc ← αMc + (1 − α) ¯Mc, (13) where α ∈ (0, 1) is a hyperparameter. This operation allows useful information to be retained in the memory, while erasing less relevant or trivial information. 5
2.3 Objective
To train the model, we adopt stochastic gradient variational Bayes [31] and implement it using deep neural networks for end-to-end learning. By combining (4), (9) and (7), we obtain the following objective for the hierarchical variational inference: arg min
{φ,θ,ψ,ϕ}
−
T (cid:88) (cid:34) N (cid:88) (cid:20)
|Qt n| (cid:88) (cid:104) L (cid:88) t n (xt i,yt i )∈Qt n (cid:96) (cid:2) log p(yt i|hφ(xt i), z((cid:96)) n )(cid:3)+
DKL( 1
J
J (cid:88) j=1
˜qϕ(zn|m(j), ¯hSt n
)||pθ(zn|hφ(xt i)) (cid:105)
+ DKL(
|M | (cid:88) a
λapψ(m|Ma)||pψ(m|¯hSt n (cid:21)(cid:35)
,
)) n x∈St n j=1 qϕ(z|m(j), St (cid:80)
= 1
|St n| (14) n), m(j) ∼ (cid:80)|M | (cid:80)J where z((cid:96)) ∼ 1 a=1 λapψ(m|Ma), L and J are numbers of Monte
J
Carlo samples, ¯hSt hφ(x), and n denotes the n-th class. To enable back propagation, we adopt the reparameterization trick [31] for sampling z and m. The third term in (14) essentially serves to constrain the inferred latent memory to ensure that it is relevant to the current task. Here, we make the parameters shared by the prior and the posterior for m, and we also amortize the inference of prototypes across classes [20], which involves using the samples St n from each class to infer their prototypes individually. In practice, the log-likelihood term is implemented as a cross entropy loss between predictions and ground-truth labels. The conditional probabilistic distributions are set to be diagonal Gaussian. We implement them using multi-layer perceptrons with the amortization technique and the reparameterization trick [31, 54], which take the conditionals as input and output the parameters of the Gaussian. In addition, we implement the model with the objective in (4), which we refer to as the variational prototypical network. 3