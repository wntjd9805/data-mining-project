Abstract
In recent years, post-hoc local instance-level and global dataset-level explainability of black-box models has received a lot of attention. Lesser attention has been given to obtaining insights at intermediate or group levels, which is a need outlined in recent works that study the challenges in realizing the guidelines in the General Data
Protection Regulation (GDPR). In this paper, we propose a meta-method that, given a typical local explainability method, can build a multilevel explanation tree. The leaves of this tree correspond to local explanations, the root corresponds to global explanation, and intermediate levels correspond to explanations for groups of data points that it automatically clusters. The method can also leverage side information, where users can specify points for which they may want the explanations to be similar. We argue that such a multilevel structure can also be an effective form of communication, where one could obtain few explanations that characterize the entire dataset by considering an appropriate level in our explanation tree.
Explanations for novel test points can be cost-efﬁciently obtained by associating them with the closest training points. When the local explainability technique is generalized additive (viz. LIME, GAMs), we develop fast approximate algorithm for building the multilevel tree and study its convergence behavior. We show that we produce high ﬁdelity sparse explanations on several public datasets and also validate the effectiveness of the proposed technique based on two human studies – one with experts and the other with non-expert users – on real world datasets. 1

Introduction
A very natural and effective way to communicate is to ﬁrst provide high level general concepts and then only dive into more of the speciﬁcs [1]. In addition, the transition from high level concepts to more and more speciﬁc explanations should ideally be as logical or smooth as possible [2, 3]. For example, when you call a service provider there is usually an automated message trying categorize the problem at a high level followed by more speciﬁc questions. Eventually if the issue is not resolved a human representative may intervene to delve into further details. In such cases, information or explanations you provide at multiple levels enables others to obtain insights that are otherwise opaque.
Recent work [4] has stressed the importance of having such multilevel explanations to successfully meet the requirements of Europe’s General Data Protection Regulation (GDPR) [5]. They argue that simply having local or global explanations may not sufﬁce for providing satisfactory explanations in many cases. In fact, even in the widely participated FICO explainability challenge [6] it was expected that one provides not just local explanations but also insights at the intermediate class level.
Motivated by this need, in this paper, we propose a novel model agnostic multilevel explanation (MAME) method that takes as input a post-hoc local explainability technique for black-box models (e.g. LIME [7]) and an unlabeled dataset. The method then generates multiple explanations for each of the examples corresponding to different degrees of cohesion (i.e. parameter tying) between explanations of the examples. This explicitly controllable degree of cohesion determines a level in our multilevel explanation tree. In addition, we constrain that the predictions of the explanation model to be close to that of the black-box at each tree node, ensuring ﬁdelity. At the extremes, the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of multilevel explanations generated by MAME for an industrial pump failure dataset consisting of 2500 wells. We show three levels: the bottom level (four) leaves which correspond to example local explanations, the top level corresponds to one global explanation and an intermediate level corresponds to explanations for two groups highlighted by MAME. Based on expert feedback, these intermediate explanations, although explaining the same type of pump, had semantic meaning as they corresponded to different manufacturer groups that behave noticeably differently. leaves would correspond to independent local explanations as with methods like LIME, while the root would correspond to a single global explanation given the high degree of cohesion.
An illustration of this is given in Figure 1, where multilevel explanations were generated by MAME for a real industrial pump failure dataset (see Section 4.4 for details). We show three levels: the four leaves correspond to example local explanations (amongst many), the root corresponds to one global explanation and an intermediate level corresponds to explanations for two groups highlighted by MAME. Note that levels are numbered from 1 (leaves) increasing up to the highest value (root).
The dotted lines indicate that the nodes are descendants of the node above, but not direct children.
Based on expert feedback these intermediate explanations correspond to pumps having different manufacturers resulting in noticeable difference in behaviors. Also note that each level provides distinct enough information not subsumed by just local or global explanations, thus motivating the need for such multilevel explanations. Such explanations can thus help identify key characteristics that bind together different examples at various levels of granularity. They can also provide exemplar based explanations based on the groupings at speciﬁc levels. These are provided in the supplement.
Our method can also take into account side information such as similarity in explanations based on class labels or user speciﬁed groupings based on domain knowledge for a subset of examples.
Moreover, one can also use non-linear additive models going beyond LIME to generate local explanations. Our method thus provides a lot of ﬂexibility in building multilevel explanations that can be customized apropos a speciﬁc application. We prove that our method actually forms a tree in that examples merged in a particular level of the tree remain together at higher levels. The proposed fast approximate algorithm for obtaining multilevel explanations is proved to converge to the exact solution. We show that we produce high ﬁdelity sparse explanations on several public datasets. We also validate the effectiveness of the proposed technique based on two human studies – one with experts and the other with non-expert users – on real world datasets. 2