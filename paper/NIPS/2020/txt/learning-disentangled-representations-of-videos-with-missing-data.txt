Abstract
Missing data poses signiﬁcant challenges while learning representations of video sequences. We present Disentangled Imputed Video autoEncoder (DIVE), a deep generative model that imputes and predicts future video frames in the presence of missing data. Speciﬁcally, DIVE introduces a missingness latent variable, disentan-gles the hidden video representations into static and dynamic appearance, pose, and missingness factors for each object. DIVE imputes each object’s trajectory where the data is missing. On a moving MNIST dataset with various missing scenarios,
DIVE outperforms the state of the art baselines by a substantial margin. We also present comparisons on a real-world MOTSChallenge pedestrian dataset, which demonstrates the practical value of our method in a more realistic setting. Our code and data can be found at https://github.com/Rose-STL-Lab/DIVE. 1

Introduction
Videos contain rich structured information about our physical world. Learning representations from video enables intelligent machines to reason about the surroundings and it is essential to a range of tasks in machine learning and computer vision, including activity recognition [1], video prediction
[2] and spatiotemporal reasoning [3]. One of the fundamental challenges in video representation learning is the high-dimensional, dynamic, multi-modal distribution of pixels. Recent research in deep generative models [4, 5, 6, 7] tackles the challenge by exploiting inductive biases of videos and projecting the high-dimensional data into substantially lower dimensional space. These methods search for disentangled representations by decomposing the latent representation of video frames into semantically meaningful factors [8].
Unfortunately, existing methods cannot reason about the objects when they are missing in videos.
In contrast, a ﬁve month-old child can understand that objects continue to exist even when they are unseen, a phenomena known as “object permanence” [9]. Towards making intelligent machines, we study learning disentangled representations of videos with missing data. We consider a variety of missing scenarios that might occur in natural videos: objects can be partially occluded; objects can disappear in a scene and reappear; objects can also become missing while changing their size, shape, color and brightness. The ability to disentangle these factors and learn appropriate representations is an important step toward spatiotemporal decision making in complex environments.
In this work, we build on the deep generative model of DDPAE [5] which integrates structured graphical models into deep neural networks. Our model, which we call Disentangled-Imputed-Video-autoEncoder (DIVE), (i) learns representations that factorize into appearance, pose and missingness
∗1College of Electrical and Computer Engineering, 2 Khoury College of Computer Sciences, Northeastern
University, MA, USA, 3Computer Science & Engineering, University of California San Diego, CA, USA. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
latent variables; (ii) imputes missing data by sampling from the learned latent variables; and (iii) performs unsupervised stochastic video prediction using the imputed hidden representation. Besides imputation, another salient feature of our model is (iv) its ability to robustly generate objects even when their appearances are changing by modeling the static and dynamic appearances separately.
Thismakes our technique more applicable to real-world problems.
We demonstrate the effectiveness of our method on a moving MNIST dataset with a variety of missing data scenarios including partial occlusions, out of scene, and missing frames with varying appearances.
We further evaluate on the Multi-Object Tracking and Segmentation (MOTSChallenge) object tracking and segmentation challenge dataset. We show that DIVE is able to accurately infer missing data, perform video imputation and reconstruct input frames and generate future predictions. Compared with baselines, our approach is robust to missing data and achieves signiﬁcant improvements in video prediction performances. 2