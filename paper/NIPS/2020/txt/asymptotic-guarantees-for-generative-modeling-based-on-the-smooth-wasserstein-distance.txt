Abstract
Minimum distance estimation (MDE) gained recent attention as a formulation of (implicit) generative modeling. It considers minimizing, over model parameters, a statistical distance between the empirical data distribution and the model. This for-mulation lends itself well to theoretical analysis, but typical results are hindered by the curse of dimensionality. To overcome this and devise a scalable ﬁnite-sample statistical MDE theory, we adopt the framework of smooth 1-Wasserstein distance (SWD) W(σ) 1 . The SWD was recently shown to preserve the metric and topological structure of classic Wasserstein distances, while enjoying dimension-free empirical convergence rates. In this work, we conduct a thorough statistical study of the minimum smooth Wasserstein estimators (MSWEs), ﬁrst proving the estimator’s measurability and asymptotic consistency. We then characterize the limit distribution of the optimal model parameters and their associated minimal 1/2) generalization bound for generative mod-SWD. These results imply an O(n− eling based on MSWE, which holds in arbitrary dimension. Our main technical tool is a novel high-dimensional limit distribution result for empirical W(σ) 1 . The characterization of a nondegenerate limit stands in sharp contrast with the classic empirical 1-Wasserstein distance, for which a similar result is known only in the one-dimensional case. The validity of our theory is supported by empirical results, posing the SWD as a potent tool for learning and inference in high dimensions. 1

Introduction
Θ, that approximates the empirical measure Pn := n−
Minimum distance estimation (MDE) considers the minimization of a statistical distance (SD) be-tween the empirical data distribution and a parametric model class. Given an identically and inde-pendently distributed (i.i.d.) dataset X1, . . . , Xn sampled from P , the goal is to learn a model Qθ, n i=1 δXi under a SD1 δ, i.e., we for θ
∈
Θ δ(Pn, Qθ). This classic mathematical statistics problem [1–3] was aim to ﬁnd adopted in recent years as a formulation of generative modeling. Indeed, both generative adversarial networks (GANs) [4–11] and variational (or Wasserstein) autoencoders [12, 13] stem from different strategies for (approximately) solving MDE2 for various choices of δ.
θn ∈ b argminθ
P
∈ 1
Beyond the practical effectiveness of MDE-based generative models, this formulation is well-suited for a theoretic analysis. This inspired a recent line of works studying GAN generalization in terms of
MDEs [9, 14, 15]. Such sample-complexity results boil down to the rate of empirical approximation under the chosen SD, i.e., the speed at which δ(Pn, P ) converges to zero. Unfortunately, popular
SDs, such as Wasserstein distances [16], f -divergences [17], and integral probability metrics [18] 1Recall that δ is an SD if δ(P, Q) = 0 ⇐⇒ P = Q. 2or a variant thereof, where Qθ is also estimated from samples. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(excluding maximum mean discrepancy [19]) suffer from the curse of dimensionality (CoD), con-1/d, with d being the data dimension [20–23].3 This limits the practical verging as δ(Pn, P ) usefulness of the devised results, which degrade exponentially fast with dimension. n−
≍ 1.1 MDE with Smooth Wasserstein Distance and Contributions
To circumvent the CoD impasse, we adopt the smooth 1-Wasserstein distance (SWD) [27,28] as our
SD. Namely, for any σ > 0, consider W(σ) (0, σ2Id) 1 (P, Q) := W1(P
∗Nσ), where
∗Nσ, Q
Nσ = is the d-dimensional isotropic Gaussian measure of parameter σ, P
∗ Nσ is the convolution of P
Nσ, and W1 is the regular 1-Wasserstein distance (see Section 2 for details). The motivation and for this choice is twofold. First, the 1-Wasserstein distance is widely used for generative model-ing [8, 13, 29, 30] due to its beneﬁcial attributes, such as metric structure, robustness to support mismatch, compatibility to gradient-based optimization, etc. As shown in [28], these properties are all preserved under Gaussian smoothing. Second, while W1 suffers from the CoD, [27] showed that E 1/2 in all dimensions, whenever P is sub-Gaussian.4 The considered minimum smooth Wasserstein estimator (MSWE) is thus 1 (Pn, P )
.σ,d n−
W(σ)
N (cid:2) (cid:3)
W(σ) 1 (Pn, Qθ). (1) argmin
Θ
θ
∈
θn ∈ b
We ﬁrst prove measurability and strong consistency of
θn, along with almost sure convergence of the associated minimal distance. Moving to a limit distribution analysis, we characterize the high-1/2 con-dimensional limits of √n( vergence rates for both quantities in arbitrary dimension. Leveraging these results along with the 1/2 on genera-framework from [14], we derive a high-dimensional generalization bound of order n− tive modeling with W(σ) 1 . Empirical results to support our theory are provided. Using synthetic data we validate both the limiting distributions of parameter estimates and the convergence of the SWD as the number of samples increases.
Θ W(σ) 1 (Pn, Qθ), thus establishing n− b
θ⋆) and √n inf θ
θn − b
∈
Our main technical tool is a novel high-dimensional limit distribution result for scaled empirical
SWD, i.e., √nW(σ) 1 (Pn, P ), which may be of independent interest. Our analysis relies on the
Kantorovich-Rubinstein (KR) duality for W1 [16], which allows representing √nW(σ) 1 (Pn, P ) as a supremum of an empirical process indexed by the class of 1-Lipschitz functions convolved with a Gaussian density. We then prove that this function class is Donsker (i.e., satisﬁes the uniform central limit theorem (CLT)) under a polynomial moment condition on P .5 By the continuous mapping theorem, we conclude that √nW(σ) 1 (Pn, P ) converges in distribution to the supremum of a tight Gaussian process. To enable evaluation of the distributional limit, we also prove that the nonparametric bootstrap is consistent. The characterization of a high-dimensional limit distribution for empirical SWD stands in sharp contrast to the classic W1 case, for which such a result is known only when d = 1 [35]. 1.2 Comparisons and