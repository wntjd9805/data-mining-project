Abstract
The goal of clustering is to group similar objects into meaningful partitions. This process is well understood when an explicit similarity measure between the objects is given. However, far less is known when this information is not readily available and, instead, one only observes ordinal comparisons such as “object i is more simi-lar to j than to k.” In this paper, we tackle this problem using a two-step procedure: we estimate a pairwise similarity matrix from the comparisons before using a clus-tering method based on semi-deﬁnite programming (SDP). We theoretically show that our approach can exactly recover a planted clustering using a near-optimal number of passive comparisons. We empirically validate our theoretical ﬁndings and demonstrate the good behaviour of our method on real data. 1

Introduction
In clustering, the objective is to group together objects that share the same semantic meaning, that are similar to each other, into k disjoint partitions. This problem has been extensively studied in the literature when a measure of similarity between the objects is readily available, for example when the examples have a Euclidean representation or a graph structure (Shi and Malik, 2000; Arthur and
Vassilvitskii, 2007; von Luxburg, 2007). However, it has attracted less attention when the objects are difﬁcult to represent in a standard way, for example cars or food. A recent trend to tackle this problem is to use comparison based learning (Ukkonen, 2017; Emamjomeh-Zadeh and Kempe, 2018) where, instead of similarities, one only observes comparisons between the examples:
Triplet comparison: Object xi is more similar to object xj than to object xk;
Quadruplet comparison: Objects xi and xj are more similar to each other than objects xk and xl.
There are two ways to obtain these comparisons. On the one hand, one can adaptively query them from an oracle, for example a crowd. This is the active setting. On the other hand, they can be directly given, with no way to make new queries. This is the passive setting. In this paper, we study comparison based learning for clustering using passively obtained triplets and quadruplets.
Comparison based learning mainly stems from the psychometric and crowdsourcing literature (Shep-ard, 1962; Young, 1987; Stewart et al., 2005) where the importance and robustness of collecting ordinal information from human subjects has been widely discussed. In recent years, this framework has attracted an increasing amount of attention in the machine learning community and three main learning paradigms have emerged. The ﬁrst one consists in obtaining an Euclidean embedding of the data that respects the comparisons as much as possible and then applying standard learning
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
techniques (Borg and Groenen, 2005; Agarwal et al., 2007; Jamieson and Nowak, 2011; Tamuz et al., 2011; van der Maaten and Weinberger, 2012; Terada and von Luxburg, 2014; Zhang et al., 2015;
Amid and Ukkonen, 2015; Arias-Castro, 2017). The second paradigm is to directly solve a speciﬁc task from the ordinal comparisons, such as data dimension or density estimation (Kleindessner and von Luxburg, 2015; Ukkonen et al., 2015), classiﬁcation and regression (Haghiri et al., 2018), or clustering (Vikram and Dasgupta, 2016; Ukkonen, 2017; Ghoshdastidar et al., 2019). Finally, the third paradigm is an intermediate solution where the idea is to learn a similarity or distance function, as in embedding approaches, but, instead of satisfying the comparisons, the objective is to solve one or several standard problems such as classiﬁcation or clustering (Kleindessner and von Luxburg, 2017). In this paper, we focus on this third paradigm and propose two new similarities based on triplet and quadruplet comparisons respectively. While these new similarities can be used to solve any machine learning problem, we show that they are provably good for clustering under a well known planted partitioning framework (Abbe, 2017; Yan et al., 2018; Xu et al., 2020).
O (cid:0)n3(cid:1) different triplets and
Motivation of this work. A key bottleneck in comparison based learning is the overall number of (cid:0)n4(cid:1) different available comparisons: given n examples, there exist quadruplets. In practice, it means that, in most applications, obtaining all the comparisons is not realistic. Instead, most approaches try to use as few comparisons as possible. This problem is relatively easy when the comparisons can be actively queried and it is known that Ω (n ln n) adaptively selected comparisons are sufﬁcient for various learning problems (Haghiri et al., 2017; Emamjomeh-Zadeh and Kempe, 2018; Ghoshdastidar et al., 2019). On the other hand, this problem becomes harder when the comparisons are passively obtained. The general conclusion in most theoretical results on (cid:0)n4(cid:1) learning from passive ordinal comparisons is that, in the worst case, almost all the comparisons should be observed (Jamieson and Nowak, 2011; Emamjomeh-Zadeh and Kempe, 2018).
The focus of this work is to show that, by carefully handling the passively obtained comparisons, it is possible to design comparison based approaches that use almost as few comparisons as active approaches for planted clustering problems. (cid:0)n3(cid:1) or
O
O
O
Near-optimal guarantees for clustering with passive comparisons. In hierarchical clustering,
Emamjomeh-Zadeh and Kempe (2018) showed that constructing a hierarchy that satisﬁes all compar-isons in a top-down fashion requires Ω (cid:0)n3(cid:1) passively obtained triplets in the worst case. Similarly,
Ghoshdastidar et al. (2019) considered a planted model and showed that Ω (cid:0)n3.5 ln n(cid:1) passive quadru-plets sufﬁce to recover the true hierarchy in the data using a bottom-up approach. Since the main difﬁculty lies in recovering the small clusters at the bottom of the tree, we believe that this latter result also holds for standard clustering. In this paper, we consider a planted model for standard clustering and we show that, when the number of clusters k is constant, Ω (cid:0)n(ln n)2(cid:1) passive triplets or quadruplets are sufﬁcient for exact recovery.2 This result is comparable to the sufﬁcient number of active comparisons in most problems, that is Ω (n ln n) (Haghiri et al., 2017; Emamjomeh-Zadeh and Kempe, 2018). Furthermore, it is near-optimal. Indeed, to cluster an example, it is necessary to observe it in a comparison at least once as, otherwise, it can only be assigned to a random cluster.
Thus, to cluster n objects, it is necessary to have access to at least Ω (n) comparisons. Finally, to obtain these results, we study a semi-deﬁnite programming (SDP) based clustering method and our analysis could be of signiﬁcant interest beyond the comparison based framework.
General noise model for comparison based learning. In comparison based learning, there are two main sources of noise. First, the observed comparisons can be noisy, that is the observed triplets and quadruplets are not in line with the underlying similarities. This noise stems, for example, from the randomness of the answers gathered from a crowd. It is typically modelled by assuming that each observed comparison is randomly (and independently) ﬂipped (Jain et al., 2016; Emamjomeh-Zadeh and Kempe, 2018). This is mitigated in the active setting by repeatedly querying each comparison, but may have a signiﬁcant impact in the passive setting where a single instance of each comparison is often observed. Apart from the aforementioned observation errors, the underlying similarities may also have intrinsic noise. For instance, the food data set by Wilber et al. (2014) contains triplet comparisons in terms of which items taste more similar, and it is possible that the taste of a dessert is closer to a main dish than to another dessert. This noise has been considered in Ghoshdastidar et al. (2019) by assuming that every pair of items possesses a latent random similarity, which affects the 2When we write that Ω (cid:0)n(ln n)2(cid:1) comparisons are sufﬁcient, we express that any number of comparisons greater than Cn(ln n)2 with C a constant is sufﬁcient to solve the problem. In other words, it means that having exactly Cn(ln n)2 comparisons is sufﬁcient but also that having more comparisons is not detrimental. This notation is used in statistics and information theory (Fletcher et al., 2009) and is equivalent to (cid:38). 2
responses to comparisons. In this paper, we propose, to the best of our knowledge, the ﬁrst analysis that considers and shows the impact of both types of noise on the number of passive comparisons.
Scalable comparison based similarity functions. Several similarity and kernel functions have been proposed in the literature (Kleindessner and von Luxburg, 2017; Ghoshdastidar et al., 2019). However, (n) passes over the set of computing these similarities is usually expensive as they require up to available comparisons. In this paper, we propose new similarity functions whose construction is much more efﬁcient than previous kernels. Indeed, they can be obtained with a single pass over the set of available comparisons. It means that our similarity functions can be computed in an online fashion where the comparisons are obtained one at a time from a stream. The main drawback compared to existing approaches is that we lose the positive semi-deﬁniteness of the similarity matrix, but our theoretical results show that this is not an issue in the context of clustering. We also demonstrate this empirically as our similarities obtain results that are comparable with state of the art methods.
O 2