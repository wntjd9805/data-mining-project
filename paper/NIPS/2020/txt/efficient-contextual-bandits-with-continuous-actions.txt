Abstract
We create a computationally tractable algorithm for contextual bandits with con-tinuous actions having unknown structure. Our reduction-style algorithm com-poses with most supervised learning representations. We prove that it works in a general sense and verify the new functionality with large-scale experiments. 1

Introduction
In contextual bandit learning [6, 1, 39, 3], an agent repeatedly observes its environment, chooses an action, and receives a reward feedback, with the goal of optimizing cumulative reward. When the action space is discrete, there are many solutions to contextual bandit learning with successful deployments in personalized health, content recommendation, and elsewhere [e.g., 42, 54, 2, 44, 25, 43]. However, in many practical settings the action chosen is actually continuous. How then can we efﬁciently choose the best action given the context? This question is also extremely relevant to reinforcement learning more generally since contextual bandit learning is one-step reinforcement learning.
There are many concrete examples of reinforcement learning problems with continuous actions.
In precision medicine [20, 31], doctors may prescribe to a patient a medication with a continuous value of dosage [32]. In data center optimization, the fan speeds and liquid coolant ﬂow may be controllable continuous values [41]. In operating systems, when a computer makes a connection over the network, we may be able to adjust its packet send rate in response to the current network status [30]. All of these may be optimizable based on feedback and context.
A natural baseline approach here is to posit smoothness assumptions on the world, as in much prior work, e.g., [5, 34, 18, 50, 19]. This approach comes with practical drawbacks. Many applications do not exhibit any smoothness structure. When/if they do, the smoothness parameters (such as a Lips-chitz constant) must be known in advance. Unfortunately, discovering the smoothness parameters is challenging, and requires knowing some other parameters and/or extensive exploration.
A recent approach to continuous actions [37] realizes similar performance guarantees without know-ing the Lipschitz constant (let alone a more reﬁned smoothness structure), while leveraging any preferred policy representation. Here, each action is “smoothed” to a distribution over an interval, and the benchmark one competes with is “smoothed” similarly. Unfortunately, their algorithm is computationally infeasible since it requires enumeration of all possible policy parameter settings.
∗mm7918@nyu.edu
†chichengz@cs.arizona.edu
‡rajan.chari@microsoft.com
§akshaykr@microsoft.com
¶jcl@microsoft.com (cid:107)slivkins@microsoft.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we realize beneﬁts similar to this approach with a computationally practical al-gorithm, for contextual bandits with continuous action space [0, 1]. Our algorithms are oracle-efﬁcient [39, 21, 3, 47, 53]: computationally efﬁcient whenever we can solve certain supervised learning problems. Our main algorithm chooses actions by navigating a tree with supervised learn-ers acting as routing functions in each node. Each leaf corresponds to an action, which is then
“smoothed” to a distribution from which the ﬁnal action is sampled. We use the reward feedback to update the supervised learners in the nodes to improve the “tree policy.”
Our contributions can be summarized as follows:
• We propose CATS, a new algorithm for contextual bandits with continuous actions (Algorithm 1).
It uses (cid:15)-greedy exploration with tree policy classes (Deﬁnition 2) and is implemented in a fully online and oracle-efﬁcient manner. We prove that CATS has prediction and update times scal-ing as log of the tree size, an exponential improvement over traditional approaches. Assuming realizability, CATS has a sublinear regret guarantee against the tree policy class (Theorem 6).
• We propose CATS Off, an off-policy optimization version of CATS (Algorithm 3) that can utilize logged data to train and select tree policies of different complexities. We also establish statistical guarantees for this algorithm (Theorem 7).
• We implement our algorithms in Vowpal Wabbit (vowpalwabbit.org), and compare with baselines on real datasets. Experiments demonstrate the efﬁcacy and efﬁciency of our approach (Section 5).
Discussion. The smoothing approach has several appealing properties. We look for a good interval of actions, which is possible even when the best single action is impossible to ﬁnd. We need to guess a good width, but the algorithm adjusts to the best location for the interval. This is less guessing compared to uniform discretization (where the width and location are tied to some extent). While the bandwidth controls statistical performance, an algorithm is free to discretize actions for the sake of computational feasibility. An algorithm can improve accuracy by reusing datapoints for overlapping bands. Finally, the approach is principled, leading to speciﬁc, easily interpretable guarantees.
The tree-based classiﬁer is a successful approach for supervised learning with a very large number of actions (which we need for computational feasibility). However, adapting it for smoothing runs into some challenges. First, a naive implementation leads to a prohibitively large per-round running time; we obtain an exponential improvement as detailed in Section 3.1. Second, existing statistical guarantees do not carry over to regret in bandits: they merely “transfer” errors from tree nodes to the root [10, 9], but the former errors could be huge. We posit a realizability assumption; even then, the analysis is non-trivial because the errors accumulate as we move down the tree.
Another key advantage of our approach is that it allows us to use off-policy model selection. For off-policy evaluation, we use smoothing to induce exploration distribution supported on the entire action space. Hence, we can discover when reﬁnements in tree depth or smoothing parameters result in superior performance. Such model selection is not possible when using discretization approaches.
When employed in an ofﬂine setup with data collected by a baseline logging policy, our experiments show that off-policy optimization can yield dramatic performance improvements.