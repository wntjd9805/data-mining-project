Abstract
In visual domain adaptation (DA), separating the domain-speciﬁc characteristics from the domain-invariant representations is an ill-posed problem. Existing meth-ods apply different kinds of priors or directly minimize the domain discrepancy to address this problem, which lack ﬂexibility in handling real-world situations.
Another research pipeline expresses the domain-speciﬁc information as a gradual transferring process, which tends to be suboptimal in accurately removing the domain-speciﬁc properties. In this paper, we address the modeling of domain-invariant and domain-speciﬁc information from the heuristic search perspective.
We identify the characteristics in the existing representations that lead to larger domain discrepancy as the heuristic representations. With the guidance of heuris-tic representations, we formulate a principled framework of Heuristic Domain
Adaptation (HDA) with well-founded theoretical guarantees. To perform HDA, the cosine similarity scores and independence measurements between domain-invariant and domain-speciﬁc representations are cast into the constraints at the initial and ﬁnal states during the learning procedure. Similar to the ﬁnal condi-tion of heuristic search, we further derive a constraint enforcing the ﬁnal range of heuristic network output to be small. Accordingly, we propose Heuristic Do-main Adaptation Network (HDAN), which explicitly learns the domain-invariant and domain-speciﬁc representations with the above mentioned constraints. Ex-tensive experiments show that HDAN has exceeded state-of-the-art on unsuper-vised DA, multi-source DA and semi-supervised DA. The code is available at https://github.com/cuishuhao/HDA. 1

Introduction
In visual domain adaptation scenarios, the domain-invariant (a.k.a. transferable) representations [36], are considered to be the information bottleneck to enhance the generalizability of a model transferred from labeled source domain to unlabeled or weakly-labeled target domain. Nevertheless, without sufﬁcient knowledge, separating domain-invariant information from the domain-speciﬁc information that is harmful for domain adaptation, is substantially an ill-posed problem with inﬁnite solutions.
To derive feasible solutions, a major way is to impose different prior knowledge on the structure of the mapping functions. For example, the domain-invariant feature is regarded as the information shared among domains, and obtained by averaging the domain-speciﬁc metrics [37] or by a joint neural network [8] for multiple learning tasks. Regularization with well-established statistical explanation, such as F -norm [15], structured L1-norm [3, 4] or orthogonality [2] constraints on linear or kernel mapping functions [14], is imposed on either the domain-speciﬁc or domain-invariant part to alleviate
∗Visiting student at Alibaba Group.
†Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the ill-posedness. However, the learning of domain-invariant representations may still suffer from inappropriate prior knowledge, which requires both a comprehensive understanding of domain data and the ﬂexibility in handling real-world domain adaptation problems.
In deep neural networks, the learning of domain-invariant features are directly guided by knowledge of prior distribution using diverse metrics on multiple layers [17, 18, 32, 55, 59], or by confusing the domain classiﬁer in an adversarial manner [18, 25, 28, 33]. Some other prior-based approaches are focused on additional properties along with the training procedure, such as the transferability [10, 54] and discriminability [10, 11, 60]. Nevertheless, by disregarding the explicit modeling of domain-speciﬁc characteristics, it can not be guaranteed that the domain-invariant representations contain as few domain-speciﬁc properties as possible. Other research pipeline expresses the domain-speciﬁc information from source to target as a gradual transferring process [12], undertaken on a geodesic domain ﬂow along the data manifolds [19, 20, 23]. In these methods, the geodesic transfer properties are enforced by reconstructing input images [6, 20], which tends to be less accurate in removing the domain-speciﬁc properties from domain-invariant representations.
In this paper, we propose a principled framework of Heuristic Domain Adaptation (HDA), with a lower error bound derived by theoretical analysis. The information in the current domain-invariant representations that leads to larger domain discrepancy is identiﬁed as the domain-speciﬁc counter-part. We assume that domain-speciﬁc representations could be more easily obtained compared with domain-invariant parts. Similar to the estimated distance in heuristic search, the explicit construc-tion of domain-speciﬁc representations could provide heuristics on learning the domain-invariant representations. Furthermore, the distance to the destination could be approximated by the range of domain-speciﬁc representations. Similar to the admissible constraint in heuristic search, the range is expected to be lower than the ideal range of domain-speciﬁc representations during the training. In what follows, we sometimes refer to the domain-speciﬁc representation as the heuristic representation.
To perform HDA, we derive three heuristic constraints, i.e., limited similarity, independence and termination. The analysis on cosine similarity scores between domain-invariant and domain-speciﬁc representations results in the constraint of limited similarity between their initial representations. The independence between the domain-invariant and domain-speciﬁc representations could be enforced by lessening the non-gaussianity discrepancy between the two representations. We also analyze the termination condition of heuristic search, and derive the constraint of reducing the range of heuristic representations to near zero at the end. By distinguishing the two counterparts more comprehensively, a better construction of domain-invariant representation can be achieved.
Accordingly, we propose Heuristic Domain Adaptation Network (HDAN), which is composed of the fundament network and heuristic network. The heuristic network is constructed by a series of subnetworks to enhance the construction of domain-speciﬁc properties. We devise HDAN for three typical domain adaptation tasks, i.e., unsupervised DA, semi-supervised DA and multi-source DA.
Experiments show that HDAN achieves state-of-the-art results on the three tasks. Our contributions could be summarized as follows:
• We propose HDA, a principled framework leveraging domain-speciﬁc representations as heuristics to obtain domain-invariant representations.
• We develop three heuristic constraints to distinguishing the two counterparts more compre-hensively, and a better construction of domain-invariant representation can be achieved.
• We design HDAN to perform HDA, and achieve state-of-the-art on three DA tasks. 2