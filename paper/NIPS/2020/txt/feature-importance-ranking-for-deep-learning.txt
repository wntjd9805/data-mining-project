Abstract
Feature importance ranking has become a powerful tool for explainable AI. How-ever, its nature of combinatorial optimization poses a great challenge for deep learning. In this paper, we propose a novel dual-net architecture consisting of operator and selector for discovery of an optimal feature subset of a ﬁxed size and ranking the importance of those features in the optimal subset simultaneously.
During learning, the operator is trained for a supervised learning task via optimal feature subset candidates generated by the selector that learns predicting the learn-ing performance of the operator working on different optimal subset candidates.
We develop an alternate learning algorithm that trains two nets jointly and incorpo-rates a stochastic local search procedure into learning to address the combinatorial optimization challenge. In deployment, the selector generates an optimal feature subset and ranks feature importance, while the operator makes predictions based on the optimal subset for test data. A thorough evaluation on synthetic, benchmark and real data sets suggests that our approach outperforms several state-of-the-art feature importance ranking and supervised feature selection methods. (Our source code is available: https://github.com/maksym33/FeatureImportanceDL) 1

Introduction
In machine learning, feature importance ranking (FIR) refers to a task that measures contributions of individual input features (variables) to the performance of a supervised learning model. FIR has become one of powerful tools in explainable/interpretable AI [1] to facilitate understanding of decision-making by a learning system and discovery of key factors in a speciﬁc domain, e.g., in medicine, what genes are likely main causes of a cancer [2].
Due to the existence of correlated/dependent and irrelevant features to targets in high-dimensional real data, feature selection [3] is often employed to address the well-known curse of dimensionality challenge and to improve the generalization of a learning system, where a subset of optimal features is selected in terms of the pre-deﬁned criteria to maximize the performance of a learning system.
Feature selection may be conducted at either population or instance level; the populationwise methods would ﬁnd out an optimal feature subset collectively for all the instances in a population, while the instancewise ones tend to uncover a subset of salient features speciﬁc to a single instance. In practice,
FIR is always closely associated with feature selection by ranking the importance of those features in an optimal subset and can also be used as a proxy for feature selection, e.g., [2, 4, 5].
Deep learning has turned out to be extremely powerful in intelligent system development but its purported “black box” nature makes it extremely difﬁcult to be applied to tasks demanding explain-ability/interpretability. Recently, FIR for deep learning has become an active research area where most works focus on instancewise FIR [6] and only few works exist for populationwise FIR/feature selection, e.g., [7]. In a populationwise scenario, feature selection needs to ﬁnd an optimum in detecting any functional dependence between input data and targets, which is NP-hard in general [8].
High degree of nonlinearity in deep learning execrates this combinatorial optimization problem. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we address a populationwise FIR issue in deep learning: for a feature set, ﬁnding an optimal feature subset of a ﬁxed size that maximizes the performance of a deep neural network and ranking the importance of all the features in this optimal subset simultaneously. To tackle this problem, we propose a novel dual-net neural architecture, where an operator net works for a supervised learning task via optimal subset candidates provided by a selector net that learns ﬁnding the optimal feature subset and ranking feature importance via the learning performance feedback of the operator. Two nets are jointly trained in an alternate manner. After learning, the selector net is used to ﬁnd an optimal feature subset and rank feature importance, while the operator net makes predictions based on the optimal feature subset for test data. A thorough evaluation on synthetic, benchmark and real datasets via a comparative study manifests that our approach leveraged by deep learning outperforms several state-of-the-art FIR and supervised feature selection methods. 2