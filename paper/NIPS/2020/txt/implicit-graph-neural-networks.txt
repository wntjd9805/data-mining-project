Abstract
Graph Neural Networks (GNNs) are widely used deep learning models that learn meaningful representations from graph-structured data. Due to the ﬁnite nature of the underlying recurrent structure, current GNN methods may struggle to capture long-range dependencies in underlying graphs. To overcome this difﬁculty, we propose a graph learning framework, called Implicit Graph Neural Networks (IGNN2), where predictions are based on the solution of a ﬁxed-point equilibrium equation involving implicitly deﬁned “state” vectors. We use the Perron-Frobenius theory to derive sufﬁcient conditions that ensure well-posedness of the framework.
Leveraging implicit differentiation, we derive a tractable projected gradient descent method to train the framework. Experiments on a comprehensive range of tasks show that IGNNs consistently capture long-range dependencies and outperform the state-of-the-art GNN models. 1

Introduction
Graph neural networks (GNNs) (Zhou et al., 2018; Zhang et al., 2020) have been widely used on graph-structured data to obtain a meaningful representation of nodes in the graph. By iteratively aggregating information from neighboring nodes, GNN models encode graph-relational information into the representation, which then beneﬁts a wide range of tasks, including biochemical structure discovery (Gilmer et al., 2017; Wan et al., 2019), computer vision (Kampffmeyer et al., 2018), and recommender systems (Ying et al., 2018). Recently, newer convolutional GNN structures (Wu et al., 2019b) have drastically improved the performance of GNNs by employing various techniques, including renormalization (Kipf and Welling, 2016), attention (Veliˇckovi´c et al., 2017), and simpler activation (Wu et al., 2019a).
The aforemetioned modern convolutional GNN models capture relation information up to T -hops away by performing T iterations of graph convolutional aggregation. Such information gathering procedure is similar to forward-feeding schemes in popular deep learning models, such as multi-layer perceptron and convolutional neural networks. However, despite their simplicity, these computation strategies cannot discover the dependency with a range longer than T -hops away from any given node.
One approach tackling this problem is to develop recurrent GNNs that iterate graph convolutional aggregation until convergence, without any a priori limitation on the number of hops. This idea
∗Equal contributions. Work done during Heng’s visit to University of California at Berkeley. 2Code available at https://github.com/SwiftieH/IGNN. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
arises in many traditional graph metrics, including eigenvector centrality (Newman, 2018) and
PageRank (Page et al., 1999), where the metrics are implicitly deﬁned by some ﬁxed-point equation.
Intuitively, the long-range dependency can be better captured by iterating the information passing procedure for an inﬁnite number of times until convergence. Pioneered by (Gori et al., 2005), new recurrent GNNs leverage partial training (Gallicchio and Micheli, 2010, 2019) and approximation (Dai et al., 2018) to improve performance. With shared weights, these methods avoid exploding memory issues and achieve accuracies competitive with convolutional counterparts in certain cases.
While these methods offer an alternative to the popular convolutional GNN models with added beneﬁts for certain problems, there are still signiﬁcant limitations in evaluation and training for recurrent GNN models. Conservative convergence conditions and sophisticated training procedures have limited the use of these methods in practice, and outweighed the performance beneﬁts of capturing the long-range dependency. In addition, most of these methods cannot leverage multi-graph information or adapt to heterogeneous network settings, as prevalent in social networks as well as bio-chemical graphs (Wan et al., 2019).
Paper contributions.
In this work, we present the Implicit Graph Neural Network (IGNN) frame-work to address the problem of evaluation and training for recurrent GNNs. We ﬁrst analyze graph neural networks through a rigorous mathematical framework based on the Perron-Frobenius theory (Berman and Plemmons, 1994), in order to establish general well-posedness conditions for conver-gence. We show that most existing analyses are special cases of our result. As for training, we propose a novel projected gradient method to efﬁciently train the IGNN, where we leverage implicit differentiation methods to obtain the exact gradient, and use projection on a tractable convex set to guarantee well-posedness. We show that previous gradient methods for recurrent graph neural networks can be interpreted as an approximation to IGNN. Further, we extend IGNN to heteroge-neous network settings. Finally, we conduct comprehensive comparisons with existing methods, and demonstrate that our method effectively captures long-range dependencies and outperforms the state-of-the-art GNN models on a wide range of tasks.
Paper outline.
In Section 2, we give an overview of related work on GNN and implicit models. In
Section 3, we introduce the background and notations for this paper. Section 4 discusses the IGNN framework together with its well-posedness and training under both ordinary and heterogeneous settings. Section 5 empirically compares IGNN with modern GNN methods. 2