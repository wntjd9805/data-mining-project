Abstract
Normalizing ﬂows (NFs) have become a prominent method for deep generative models that allow for an analytic probability density estimation and efﬁcient syn-thesis. However, a ﬂow-based network is considered to be inefﬁcient in parameter complexity because of reduced expressiveness of bijective mapping, which renders the models unfeasibly expensive in terms of parameters. We present an alternative parameterization scheme called NanoFlow, which uses a single neural density estimator to model multiple transformation stages. Hence, we propose an efﬁcient parameter decomposition method and the concept of ﬂow indication embedding, which are key missing components that enable density estimation from a single neural network. Experiments performed on audio and image models conﬁrm that our method provides a new parameter-efﬁcient solution for scalable NFs with signiﬁcant sublinear parameter complexity. 1

Introduction
Flow-based models have become a prominent approach for density estimation and generative models in recent times. These models are based on normalizing ﬂows (NFs) [27], wherein a deep invertible model is trained with an analytically estimated likelihood of data. The model learns a bijective mapping between the data and a known prior (typically isotropic Gaussian), and its reverse operation synthesizes realistic samples from the prior. Compared with the variational autoencoder [18] and generative adversarial network [10], NFs exhibit the distinct characteristic of an exact probability density estimation from a principled maximum likelihood training. When combined with non-autoregressive coupling methods [6, 19], NFs become a powerful generative model that offers a signiﬁcantly simpliﬁed training and efﬁcient inference.
Since the introduction of the framework into neural networks, the existing studies on ﬂow-based models have focused on improving the expressiveness of the bijective operation [2,7,12,19]. However, parameter complexity and memory efﬁciency are less emphasized by the research community.
The small efforts to maximize the expressiveness under a speciﬁed amount of capacity of the neural network has recently become problematic when expanding a ﬂow-based model for real-world applications. A notable example is the waveform synthesis model [15, 26]. Although the aforementioned studies have achieved audio generation faster than real-time (thereby removing the slow inference bottleneck of WaveNet [29]), they resulted in an increase in the number of parameters by an order of magnitude, which is unfeasibly expensive in terms of memory. Hence, building a complex, scalable, and memory-efﬁcient ﬂow-based model remains challenging.
This scenario raised a question: Is it true that NFs require a signiﬁcantly larger network capacity to perform expressive bijections, or is the representational power of deep neural networks inefﬁciently
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
utilized? We argue that studies regarding NFs should consider the parameter complexity, where the expressiveness of multiple ﬂows is not necessarily accompanied by a linearly growing number of parameters.
In this study, we challenge the typical assumption in building ﬂow-based models and aim to decouple the required number of parameters and the expressiveness of multiple bijective operations for ﬂow-based models. We present NanoFlow, an alternative parameterization scheme for NFs that operates on a single neural density estimator. Because the shared density estimator is applied to multiple stacks of ﬂows, the parameter requirement is no longer proportional to the number of ﬂows, and the memory footprint is signiﬁcantly reduced. Consequently, NanoFlow can consistently improve its expressiveness by stacking ﬂows without sacriﬁcing parameter efﬁciency.
Our results indicated that using a conventional notion of weight sharing did not yield a good performance on ﬂow-based models, which nulliﬁes the potential beneﬁts. To achieve the concept of a shared neural density estimator, we demonstrate several parameter-efﬁcient solutions for increasing the ﬂexibility of NanoFlow. We show that decomposing a deep hidden representation estimated by the shared neural network and the projected densities from the representation can signiﬁcantly enhance the expressiveness of NanoFlow with the addition of a few parameters. Furthermore, we also demonstrate that conditioning the shared estimator with our ﬂow indication embedding can remedy the modeling difﬁculties of multiple densities from a single estimator without dissatisfying any invertibility constraints.
Additionally, we provide a deeper analysis of the condition under which our method yields the a higher number of beneﬁts. Speciﬁcally, we assess the effectiveness of the single density estimator by varying the amount of autoregressive structural bias into the model. Our results demonstrate that our method performs best on bipartite ﬂows, which provides an expanded narrative on our belief regarding the performance gap between non-autoregressive and autoregressive models. In summary, our study is the ﬁrst to focus on a systematic assessment for enabling scalable NFs with an almost constant parameter complexity. 2