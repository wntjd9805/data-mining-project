Abstract
While Deep Reinforcement Learning (DRL) has emerged as a promising approach to many complex tasks, it remains challenging to train a single DRL agent that is capable of undertaking multiple different continuous control tasks. In this paper, we present a Knowledge Transfer based Multi-task Deep Reinforcement Learning framework (KTM-DRL) for continuous control, which enables a single DRL agent to achieve expert-level performance in multiple different tasks by learning from task-speciﬁc teachers. In KTM-DRL, the multi-task agent ﬁrst leverages an ofﬂine knowledge transfer algorithm designed particularly for the actor-critic architecture to quickly learn a control policy from the experience of task-speciﬁc teachers, and then it employs an online learning algorithm to further improve itself by learning from new online transition samples under the guidance of those teachers. We perform a comprehensive empirical study with two commonly-used benchmarks in the MuJoCo continuous control task suite. The experimental results well justify the effectiveness of KTM-DRL and its knowledge transfer and online learning algorithms, as well as its superiority over the state-of-the-art by a large margin. 1

Introduction
The recent breakthrough of Deep Learning (DL) enables Reinforcement Learning (RL) to deliver much better performance in real and complex control tasks. Tremendous successes have been made by Deep Reinforcement Learning (DRL) with Deep Q-Network (DQN) [12] on various discrete control tasks (such as Atari games). Recently, DRL has also been extended to the continuous control domain, which usually leverages an actor-critic based method (such as DDPG [10] and TD3 [4]) to deal with a continuous action space.
Despite the impressive performance of DRL on individual tasks, it remains challenging to train a single DRL agent to undertake multiple different tasks. Unlike single-task DRL, which learns a control policy for an individual task, multi-task DRL requires an agent to learn a single control policy that could perform well on multiple different tasks. Multi-task DRL is considered as an essential step towards Artiﬁcial General Intelligence (AGI) [5]. A straightforward approach is to directly train a
DRL agent for multiple tasks one by one using a traditional single-task learning algorithm, which has been shown to deliver poor performance and may even fail on some tasks [1, 24], due to differences and possible interference among multiple tasks. Recent research efforts have been made to address the challenges of multi-task DRL. An effective approach is to tackle this problem with knowledge transfer, e.g., Actor-Mimic [14] and policy distillation [15]. These methods usually aimed at training a single multi-task agent under the guidance of task-speciﬁc teachers. Through knowledge transfer, one or more control policies can be consolidated into a single one, which can achieve the same level 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
or sometimes even better performance on individual tasks. However, these methods were designed based on DQN for discrete control tasks. They can not be directly applied to multi-task DRL for continuous control, which usually has a quite different Deep Neural Network (DNN) architecture.
In this paper, we present a Knowledge Transfer based Multi-task Deep Reinforcement Learning framework (KTM-DRL) for continuous control, which enables a single DRL agent to achieve expert-level performance on multiple different tasks by learning from task-speciﬁc teachers. In KTM-DRL, the multi-task agent leverages an ofﬂine knowledge transfer algorithm designed particularly for the actor-critic architecture to quickly learn a control policy from the experience of task-speciﬁc teachers. Then, under the guidance of these knowledgeable teachers, the agent further improves itself by learning from new transition samples collected during online learning. In addition, KTM-DRL leverages hierarchical experience replay for mitigating catastrophic forgetting during both the ofﬂine transfer and online learning stages. For performance evaluation, we conduct extensive experiments on two commonly-used MuJoCo benchmarks [19]. The experimental results show that KTM-DRL exceeds the state of the art by a large margin, and its knowledge transfer, online learning, and hierarchical experience replay algorithms turn out to be effective. Particularly, we compare KTM-DRL with an “ideal” solution where the state of the art single-task DRL algorithm for continuous control, TD3 [4], is used to train multiple (instead of one) single-task agents (with much more DNN weights), each of which handles an individual task and is supposed to be the best in its own task. The experimental results show that KTM-DRL can even beat the ideal solution on some tasks and offer very close performance on the others. 2