Abstract
Synchronization is a key step in data-parallel distributed machine learning (ML).
Different synchronization systems and strategies perform differently, and to achieve optimal parallel training throughput requires synchronization strategies that adapt to model structures and cluster conﬁgurations. Existing synchronization systems often only consider a single or a few synchronization aspects, and the burden of deciding the right synchronization strategy is then placed on the ML practitioners, who may lack the required expertise. In this paper, we develop a model- and resource-dependent representation for synchronization, which uniﬁes multiple synchronization aspects ranging from architecture, message partitioning, placement scheme, to communication topology. Based on this representation, we build an end-to-end pipeline, AutoSync, to automatically optimize synchronization strategies given model structures and resource speciﬁcations, lowering the bar for data-parallel distributed ML. By learning from low-shot data collected in only 200 trial runs, AutoSync can discover synchronization strategies up to 1.6x better than manually optimized ones. We develop transfer-learning mechanisms to further reduce the auto-optimization cost – the simulators can transfer among similar model architectures, among similar cluster conﬁgurations, or both. We also present a dataset that contains nearly 10000 strategy and run-time pairs on a diverse set of models and cluster speciﬁcations. 1

Introduction
Recent advances in deep learning (DL) have beneﬁted greatly from training larger models on larger datasets. To cope with the associated computational complexity, data-parallel training [19, 5, 37] has been introduced and reported remarkable successes in scaling up model training to thousands of GPUs [10] with billions of parameters [7, 28]. Data parallelism partitions and dispatches large datasets to multiple worker devices, derives gradients for each worker on its independent data split, and synchronizes gradients of all workers at the end of each iteration.
A variety of strategies and system implementations have been developed to facilitate synchronization in data-parallel training, such as systems specialized in different communication architectures [27, 37, 29, 5], message encoding methods [21, 35], or parameter partitioning or merging schemes [18, 16].
Achieving desired data-parallel speedup, however, requires the synchronization strategy to ﬁt well with the statistical and algorithmic properties of the model and the cluster speciﬁcation. For examples, bipartite parameter servers work well for models whose sparsity structure creates “hot spots” [4, 19, 18], while collective all-reduce outperforms other communication architectures when the majority of the distributed communication happens between GPUs [29, 10]. Existing systems struggle to provide
† Equal contributions. The work is done while Yuan Li was an intern at Petuum Inc. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
excellent all-round performance on diverse models due to their oversimpliﬁed assumptions about the synchronization, and rigid application of ﬁx-formed synchronization strategies (e.g., parameter server (PS) [19, 33] for BytePS [27], Allreduce for Horovod [29]), ignoring the characteristics of models or clusters. More importantly, different strategies often exhibit sharp performance differences when applied to different ML building blocks (shallow, deep, sparse, dense, etc.) [37, 18], and the burden of selecting the right strategy for the model of interest is placed on ML practitioners, who may not have domain expertise on the trade-offs among these systems. Given the combinatorial number of choices for various synchronization factors, (e.g., architecture, variable partitioning, and placement conﬁguration), it is prohibitively costly to manually search for the optimal strategy, and the search has to be redone every time a new model is developed.
To address these challenges, this paper aims to answer: Can one automate the selection of the optimal synchronization strategy, given a model and cluster speciﬁcation? To this end, we identify multiple synchronization-affecting factors in data-parallel distributed DL. By factorizing the strategy with respect to each trainable building block of a DL model, we construct a valid and large strategy space spanned by multiple factors. To efﬁciently navigate the space and locate the optimal strategy, we build an end-to-end pipeline, AutoSync. AutoSync leverages domain knowledge about synchronization sys-tems to reduce the search space, and is equipped with a domain adaptive simulator, which combines principled communication modeling and data-driven ML models, to estimate the runtime of strategy proposals without launching real distributed execution. To further reduce practical development cost, we study the transferability of trained simulators across different models and resource speciﬁcations, which shows promising adaptability to unseen models or cluster conﬁgurations.
We evaluate AutoSync on a broad set of models and clusters, and show that there exist ample strategies in the proposed space that outperform hand-optimized systems by a signiﬁcant margin. AutoSync can effectively ﬁnd strategies that reduce the training time by 1.2x - 1.6x than hand-optimized ones on multiple, difﬁcult-to-parallelize model architectures (e.g. NCF [13], BERT [7] and VGG16 [30]), within an acceptable budget. Leveraging transfer learning, AutoSync simulators can be trained on cheaper trial data collected on smaller models or clusters, and used to derive strategies without additional training for larger models or costlier clusters. As an additional contribution, we collect a dataset containing nearly 10000 data points containing (model, resource, strategy) tuples and their corresponding runtime on real clusters. We share the dataset with the community to encourage extended studies.1 2 Problem Deﬁnition i=1