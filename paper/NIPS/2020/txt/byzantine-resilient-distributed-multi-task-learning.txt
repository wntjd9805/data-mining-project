Abstract
Distributed multi-task learning provides signiﬁcant advantages in multi-agent networks with heterogeneous data sources where agents aim to learn distinct but correlated models simultaneously. However, distributed algorithms for learning relatedness among tasks are not resilient in the presence of Byzantine agents. In this paper, we present an approach for Byzantine resilient distributed multi-task learning. We propose an efﬁcient online weight assignment rule by measuring the accumulated loss using an agent’s data and its neighbors’ models. A small accumulated loss indicates a large similarity between the two tasks. In order to ensure the Byzantine resilience of the aggregation at a normal agent, we introduce a step for ﬁltering out larger losses. We analyze the approach for convex models and show that normal agents converge resiliently towards the global minimum.
Further, aggregation with the proposed weight assignment rule always results in an improved expected regret than the non-cooperative case. Finally, we demonstrate the approach using three case studies, including regression and classiﬁcation problems, and show that our method exhibits good empirical performance for non-convex models, such as convolutional neural networks. 1

Introduction
Distributed machine learning models are gaining much attention recently as they improve the learning capabilities of agents distributed within a network with no central entity. In a distributed multi-agent system, agents interact with each other to improve their learning capabilities by leveraging the shared information via exchanging either data or models. In particular, agents that do not have enough data to build reﬁned models or agents that have limited computational capabilities, beneﬁt most from such cooperation. Distributed learning also addresses the single point of failure problem as well as scalability issues and is naturally suited to mobile phones, autonomous vehicles, drones, healthcare, smart cities, and many other applications [1, 2, 3, 4].
In networks with heterogeneous data sources, it is natural to consider the multi-task learning (MTL) framework, where agents aim to learn distinct but correlated models simultaneously [5]. Typically, prior knowledge of the relationships among models is assumed in MTL. The relationships among agents can be promoted via several methods, such as mean regularization, clustered regularization, low-rank and sparse structures regularization [6, 7, 8]. However, in real-world applications, such relationships are unknown beforehand and need to be estimated online from data. Learning similarities among tasks to promote effective cooperation is a primary consideration in MTL. There has been extensive work for learning the relationship matrix centrally by optimizing a global convex regularized function [9, 10, 11]. In contrast, this paper focuses on computationally efﬁcient distributed learning of the relationship among agents that does not require optimizing a relationship matrix centrally
[12, 13, 14, 15].
Although the distributed approach to learning and promoting similarities among neighbors from online data has many advantages, it is not resilient to Byzantine agents. Fault-tolerance for MTL 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is discussed in [5], focusing on dropped nodes that occasionally stop sending information to their neighbors. In [16], the relationship promoted by measuring the quadratic distance between two model parameters for distributed MTL is shown to be vulnerable to gradient-based attacks, and a
Byzantine resilient distributed MTL algorithm is proposed for regression problems to cope with such attacks. The proposed algorithm relies on a user-deﬁned parameter F to ﬁlter out information from F neighbors in the aggregation step and is resilient to F Byzantine neighbors, but requires exponential time with respect to the number of neighbors.
In this paper, we propose an online weight adjustment rule for MTL that is guaranteed to achieve resilient convergence for every normal agent using the rule. Compared to [16], the proposed method is suited for both regression and classiﬁcation problems, is resilient to an arbitrary number of Byzantine neighbors (without the need to select a pre-deﬁned parameter F bounding the number of Byzantine neighbors), and has linear time complexity. To the best of our knowledge, this is the ﬁrst solution that aims to address the Byzantine resilient cooperation in distributed MTL networks via a resilient similarity promoting method. We note that the proposed rule is not limited to the multi-task setting but can also be used for general distributed machine learning and federated learning systems to achieve resilient consensus. We list our contributions below.
• We propose an efﬁcient Byzantine resilient online weight adjustment rule for distributed MTL.
We measure similarities among agents based on the accumulated loss of an agent’s data and the models of its neighbors. In each iteration, a normal agent computes the weights assigned to its neighbors in time that is linear in the size of its neighborhood and the dimension of the data.
• We show that aggregation with the proposed weight assignment rule always results in an improved expected regret than the non-cooperative case, and normal agents converge resiliently towards the global minimum. Even when all the neighbors are Byzantine, a normal agent can still resiliently converge to the global minimum bounded by the same expected regret as without any cooperation with other agents, achieving resilience to an arbitrary number of Byzantine agents.
• We conduct three experiments for both regression and classiﬁcation problems and demonstrate that our approach yields good empirical performance for non-convex models, such as convolutional neural networks. 2