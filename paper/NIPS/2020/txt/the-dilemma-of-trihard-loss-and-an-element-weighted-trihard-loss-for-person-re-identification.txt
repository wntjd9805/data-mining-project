Abstract
Triplet loss with batch hard mining (TriHard loss) is an important variation of triplet loss inspired by the idea that hard triplets improve the performance of metric leaning networks. However, there is a dilemma in the training process. The hard negative samples contain various quite similar characteristics compared with anchors and positive samples in a batch. Features of these characteristics should be clustered between anchors and positive samples while are also utilized to repel between anchors and hard negative samples. It is harmful for learning mutual features within classes. Several methods to alleviate the dilemma are designed and tested.
In the meanwhile, an element-weighted TriHard loss is emphatically proposed to enlarge the distance between partial elements of feature vectors selectively which represent the different characteristics between anchors and hard negative samples.
Extensive evaluations are conducted on Market1501 and MSMT17 datasets and the results achieve state-of-the-art on public baselines. The implementation of this work is available at https://github.com/LvWilliam/EWTH-Loss. 1

Introduction
Person re-identiﬁcation (ReID) is an important branch of computer vision. Many successful designs of classiﬁcation networks [1][2][3] are applied as backbones of ReID networks to extract global features of samples [4][5][6][7]. It is efﬁcient to train such networks as a multi-class classiﬁcation task taking ids or other attributes as labels [8][9][10], which enhances the capability of networks to obtain more speciﬁc features. Some works focus on learning local features of samples, such as separated blocks of images [11][12], different semantical key points of bodies [13][14][15]. When more information is provided, such as ids of cameras [16][17] or viewpoints [18][19], networks can be trained to adapt the differences of inputs from multiple domains, which will improve the portability among different datasets. Other works concentrate on metric learning methods of ReID which are widely used in images retrieval area [20][21][22]. The key point is to learn distances between pairs of similar or dissimilar inputs [23]. Loss functions are essential for metric learning to realize the effect of metrics.
Many works combine contrastive loss with softmax cross-entropy loss [11][24][25] to improve the performance with the advantage of metric and feature representation learning. Triplet loss was ﬁrst proposed in FaceNet [26] and has applied in many works [27][28][29]. It’s a preferable function to make the distances of anchors and positive samples closer than anchors and negative samples by a margin. FaceNet [26] also proposes an idea to calculate triplet loss by hard samples to extract more meaningful and intrinsic features, which has been studied widely [30][31][32]. Triplet loss with batch 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
hard mining (TriHard loss) was proposed in [31] which improves the way of generating hard triplets during training. It is applied as the metric loss in many SOTA baselines of ReID[26][33][34].
However, there is a dilemma in TriHard loss during training, which will hamper the result of clustering within classes. That is proved both theoretically and experimentally in this paper. And three ways to alleviate the dilemma are proposed: 1) Half TriHard loss. To transform the Euclidean distances between anchors and hard negative samples into a constant within a batch in TriHard loss. 2) TriHard loss with feature normalization. To make the gradients more stable during training. 3) Half TriHard loss with average negative samples. To reduce the obstruction of hard negative samples.
In this paper, we emphatically propose an element-weighted TriHard loss adapted from TriHard loss to eliminate the dilemma as much as possible. The weights are element-wise to output feature vectors of network:
T i a,nh
= f (|W i a − W i nh
|)(i ∈ [1, q]) (1) where q is the dimension of output feature vectors, a is the anchors and nh is the hard negative samples of anchors. T i nh are the i th elements of weight vectors corresponding to the ids of a and nh in fully connected layers of classiﬁer in ReID networks. a,nh is the weight of i th element in feature vector. W i a, W i
The frameworks of networks are not inﬂuenced that makes EWTH loss convinient to be used in existing metric learning methods of ReID. All the proposed losses are evaluated on Market 1501
[35] and MSMT17 [36] datasets, which shows remarkable improvements towards TriHard loss. The baselines in this paper are published works named reid-strong-baseline [37] and AGW [33]. 2