Abstract
Deep reinforcement learning (DRL) algorithms and evolution strategies (ES) have been applied to various tasks, showing excellent performances. These have the opposite properties, with DRL having good sample efﬁciency and poor stability, while ES being vice versa. Recently, there have been attempts to combine these algorithms, but these methods fully rely on synchronous update scheme, making it not ideal to maximize the beneﬁts of the parallelism in ES. To solve this chal-lenge, asynchronous update scheme was introduced, which is capable of good time-efﬁciency and diverse policy exploration. In this paper, we introduce an Asyn-chronous Evolution Strategy-Reinforcement Learning (AES-RL) that maximizes the parallel efﬁciency of ES and integrates it with policy gradient methods. Specif-ically, we propose 1) a novel framework to merge ES and DRL asynchronously and 2) various asynchronous update methods that can take all advantages of asyn-chronism, ES, and DRL, which are exploration and time efﬁciency, stability, and sample efﬁciency, respectively. The proposed framework and update methods are evaluated in continuous control benchmark work, showing superior performance as well as time efﬁciency compared to the previous methods. 1

Introduction
Reinforcement Learning (RL) algorithms, one major branch in policy search algorithm, were com-bined with deep learning and showed excellent performance in various environments, such as playing simple video games with superhuman performance [1, 2], mastering the Go [3], and solving continu-ous control tasks [4–6]. Evolutionary methods, another famous policy search algorithm, were applied to the parameters of deep neural network and showed compatible results as Deep Reinforcement
Learning (DRL) [7].
These two branches of policy search algorithms have different properties in terms of sample efﬁciency and stability [8]. DRL is sample efﬁcient, since it learns from every step of an episode, but is sensitive to hyperparameters [9–12]. Evolution Strategies (ES) are often considered as the opposite because they are relatively stable, learning from the result of the whole episode [8], yet they require much more steps in the learning process [8, 13, 14].
ES and DRL are often considered as competitive approaches in policy search [7, 15], and relatively few studies have tried to combine them [16–18]. Recently, some works tried to utilize the useful gradient information of DRL into ES directly. Evolutionary Reinforcement Learning (ERL) has an independent RL agent that is periodically injected into the population [13]. In Cross-Entropy Method-Reinforcement Learning (CEM-RL), half of the population is trained with gradient information, and new mean and variance of the population are calculated with better performing individuals [19]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In both ES and RL, parallelization methods were introduced for faster learning and stability [2, 7, 15, 20, 21]. Most of these parallelization methods take synchronous update scheme, which aligns the update schedule of every agents to the one with the longest evaluation time. This causes crucial time inefﬁciency because agents with shorter evaluation should wait until the whole agents ﬁnish their job.
The asynchronous method is one of the direct solutions to this problem, as one agent can start the next evaluation immediately without waiting other agents [22–25]. Another advantage of the asynchronous method is that updates occur more often than those of synchronous methods, which can encourage diverse exploration [25, 26].
In this paper, we propose a novel asynchronous framework that efﬁciently combines both ES and
DRL, alongside with some effective asynchronous update schemes by thoroughly analyzing the property of each. The proposed framework and update schemes are evaluated on the continuous control benchmark, underlining its superior performance and time-efﬁciency compared to previous
ERL approaches.
Our contributions include the following:
• We propose a novel asynchronous framework that efﬁciently combines both Evolution
Strategies (ES) and Deep Reinforcement Learning algorithms.
• We introduce several asynchronous update methods for the population distribution. We thoroughly analyze all update methods and their properties. Finally, we propose the most effective asynchronous update rule.
• We demonstrate the time and sample efﬁciency of the proposed asynchronous method.
The proposed method reduces the entire training time about 75% wall clock on the same hardware conﬁguration. Also, the proposed method can achieve up to 20% score gain with given time steps through effective asynchronous policy searching algorithm. 2