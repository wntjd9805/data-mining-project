Abstract
A central problem in cognitive science and behavioural neuroscience as well as in machine learning and artiﬁcial intelligence research is to ascertain whether two or more decision makers—be they brains or algorithms—use the same strategy.
Accuracy alone cannot distinguish between strategies: two systems may achieve similar accuracy with very different strategies. The need to differentiate beyond accuracy is particularly pressing if two systems are at or near ceiling performance, like Convolutional Neural Networks (CNNs) and humans on visual object recogni-tion. Here we introduce trial-by-trial error consistency, a quantitative analysis for measuring whether two decision making systems systematically make errors on the same inputs. Making consistent errors on a trial-by-trial basis is a necessary condition if we want to ascertain similar processing strategies between decision makers. Our analysis is applicable to compare algorithms with algorithms, humans with humans, and algorithms with humans.
When applying error consistency to visual object recognition we obtain three main
ﬁndings: (1.) Irrespective of architecture, CNNs are remarkably consistent with one another. (2.) The consistency between CNNs and human observers, however, is little above what can be expected by chance alone—indicating that humans and
CNNs are likely implementing very different strategies. (3.) CORnet-S, a recurrent model termed the “current best model of the primate ventral visual stream”, fails to capture essential characteristics of human behavioural data and behaves essentially like a standard purely feedforward ResNet-50 in our analysis; highlighting that certain behavioural failure cases are not limited to feedforward models. Taken together, error consistency analysis suggests that the strategies used by human and machine vision are still very different—but we envision our general-purpose error consistency analysis to serve as a fruitful tool for quantifying future progress. 1

Introduction1
Complex systems are notoriously difﬁcult to understand—be they Convolutional Neural Networks (CNNs) or the human mind or brain. Paradoxically, for CNNs, we have access to every single model parameter, know exactly how the architecture is formed of stacked convolution layers, and 1Blog post summary: https://medium.com/@robertgeirhos/are-all-cnns-created-equal-d1 3a33b0caf7 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b) (c)
Figure 1: Do humans and CNNs make consistent errors? From left to right three steps for analysing this question are visualised. For a detailed description of these steps please see the intuition (1.1). (a) Observed vs. expected error overlap (errors on the same trials) for a classiﬁcation experiment where humans and CNNs classiﬁed the same images [11]. Values above the diagonal indicate more overlap than expected by chance. (b) Same data as on the left but measured by error consistency (κ).
Higher values indicate greater consistency; shaded areas correspond to a simulated 95% percentile for chance-level consistency. (c) Error consistency vs. ImageNet accuracy. we can inspect every single pixel of the training data—yet understanding the behaviour emerging from these primitives has proven surprisingly challenging [1], leaving us continually struggling to reconcile the success story of CNNs with their brittleness [2–4].2 In response to the need to better understand the internal mechanisms, a number of visualisation methods have been developed [6–8].
And while many of them have proven helpful in fuelling intuitions, some have later been found to be misleading [9, 10]; moreover, most visualisation analyses are qualitative at nature. On the other hand, quantitative comparisons of different algorithms like benchmarking model accuracies have led to a lot of progress across deep learning, but reveal little about the internal mechanism: two models may reach similar levels of accuracy with very different internal processing strategies, an aspect that is gaining importance as CNNs are rapidly approaching ceiling performance across tasks and datasets.
In order to understand whether two algorithms are implementing a similar or a different strategy, we need analyses that are quantitative and allow for drawing conclusions about the internal mechanism.
We here introduce error consistency3, a quantitative analysis for measuring whether two black-box perceptual systems systematically make errors on the same inputs. Irrespective of any potential differences at Marr’s implementational level [12] (which may be quite large, e.g. between two different neural network architectures or even larger between a CNN and a human observer), one can only conclude that two systems use a similar strategy if these systems make similar errors: not just a similar number of errors (as measured by accuracy), but also errors on the same inputs, i.e. if two systems ﬁnd the same individual stimuli difﬁcult or easy (as measured by error consistency). An agreement can be considered inverse to the Reichenbach-principle [13] of correlation: correlation between variables does not imply a direct causal relationship. However, correlation does imply at least an indirect causal link through other variables. For error consistency, zero error consistency implies that two decision makers are not using the same strategy. While error consistency can be applied across ﬁelds, tasks and domains (including vision, auditory processing, etc.), we believe it to be of particular relevance at the intersection of deep learning, neuroscience and cognitive science.
Both brains and CNNs have, at various points, been described as black-box mechanisms [14–16].
But do the spectacular advances in deep learning shed light on the perceptual and cognitive processes of biological vision? Does similar performance imply similar mechanism or algorithm? Do different
CNNs indeed make different errors?4 We believe that ﬁne-grained analysis techniques like error consistency may serve an important purpose in this debate. 2Note again the parallel in neuroscience, even for very simple brains: The nervous system of the nematode
C.elegans is basically known in its entirety— still it is not fully understood how the (comparatively) complex behaviour of C.elegans is brought about by the biological “hardware” [5]. 3For a discussion of this terminology we refer to Section S.1 in the appendix 4[17] found surprising similarities for self-supervised vs. supervised CNNs using error consistency. 2
Molecular psychophysics. Analysing errors for every single input is inspired by the idea of “molec-ular psychophysics” by David Green [18]. He argued that the goal of psychophysics should be to predict human responses to individual stimuli (trials) and not only aggregated responses (accuracy), let alone only averages across many individuals, as is common in much of the behavioural sciences.
Green also predicted that once models of perceptual processes became more advanced, accuracy would cease to be a good criterion to assess and compare them rigorously (see p. 394 in [18]).