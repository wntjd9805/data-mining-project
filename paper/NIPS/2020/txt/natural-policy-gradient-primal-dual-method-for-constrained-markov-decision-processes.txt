Abstract
We study sequential decision-making problems in which each agent aims to max-imize the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the dis-counted inﬁnite-horizon Constrained Markov Decision Processes (CMDPs) prob-lem. Speciﬁcally, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method for CMDPs which updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Even though the underlying maximization involves a nonconcave objective function and a nonconvex constraint set under the softmax policy parametrization, we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such a convergence is independent of the size of the state-action space, i.e., it is dimension-free. Furthermore, for the general smooth policy class, we establish sublinear rates of convergence regarding both the optimality gap and the constraint violation, up to a function approximation error caused by restricted policy parametrization. Finally, we show that two sample-based NPG-PD algorithms inherit such non-asymptotic convergence properties and provide ﬁnite-sample complexity guarantees. To the best of our knowledge, our work is the ﬁrst to establish non-asymptotic convergence guarantees of policy-based primal-dual methods for solving inﬁnite-horizon discounted CMDPs. We also provide computational results to demonstrate merits of our approach. 1

Introduction
Reinforcement learning (RL) studies sequential decision-making problems where the agent aims to maximize its expected total reward by interacting with an unknown environment over time [44]. The model of Markov Decision Processes (MDPs) is usually used to represent the environment dynam-ics. However, in many safety-critical applications, e.g., in autonomous driving [19], robotics [35], cyber-security [58], and ﬁnancial management [1], the agent is also subject to constraints on its utilities/costs. This naturally leads to a generalization of the environment dynamics to constrained
MDPs (CMDPs) [4]. Besides maximizing the expected total reward, the agent also has to take into account the constraint on the expected total utility/cost as an additional learning objective.
Policy gradient (PG) methods [45], including the natural policy gradient (NPG) [21], have enjoyed substantial empirical success in solving MDPs [39, 24, 31, 40, 44]. PG methods, or more generally direct policy search methods, have also been used to solve CMDPs [47, 12, 11, 15, 46, 23, 36, 2, 43]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
However, most existing theoretical guarantees are asymptotic in nature and/or only provide local convergence guarantees to stationary-point policies. Theoretical non-asymptotic global convergence guarantees are largely absent: for arbitrary initial points, algorithms with a ﬁnite number of iterations and samples converge to an ✏-optimal solution that enjoys ✏-optimality gap and ✏-constraint violation.
It is thus imperative to establish theoretical guarantees for PG methods in solving CMDPs. Our motivation also comes from recent advances on the global convergence properties of PG methods [18, 56, 9, 48, 3, 57].
In this work, we provide a theoretical foundation for the non-asymptotic global convergence of the
NPG method in solving CMDPs and answer the following questions: (i) can we employ NPG methods for solving CMDPs?; (ii) if and how fast do these methods converge to the globally optimal value within the underlying constraints?; (iii) what is the effect of the function approximation error caused by a restricted policy parametrization?; and (iv) what is the sample complexity of NPG methods?
Contribution. Our contribution is four-fold: (i) We propose a simple but effective primal-dual algorithm – Natural Policy Gradient Primal-Dual (NPG-PD) method – for solving discounted inﬁnite-horizon CMDPs. We employ natural policy gradient ascent to update the primal variable and projected sub-gradient descent to update the dual variable; (ii) Even though we show that the maximization problem has a nonconcave objective function and nonconvex constraint set under the softmax policy parametrization, we prove that our NPG-PD method achieves global convergence with rate O(1/pT ) regarding both the optimality gap and the constraint violation, where T is the total number of iterations. Our convergence guarantees are dimension-free, i.e., the rate is independent of the size of the state-action space; (iii) For the general smooth policy class, we establish convergence with rate O(1/pT ) for the optimality gap and O(1/T 1/4) for the constraint violation, up to a function approximation error caused by restricted policy parametrization; and (iv) We show that two sample-based NPG-PD algorithms that we propose inherit such non-asymptotic convergence properties and provide the ﬁnite-sample complexity guarantees. To the best of our knowledge, our work is the ﬁrst to provide non-asymptotic convergence guarantees for solving inﬁnite-horizon discounted CMDPs in the primal-dual framework.