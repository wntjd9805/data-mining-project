Abstract
We propose a novel regularization-based continual learning method, dubbed as
Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each neural network node based on its the importance, which is adaptively updated after learning each task. By utilizing the proximal gradient descent method, the exact sparsity and freezing of the model is guaranteed during the learning process, and thus, the learner explicitly controls the model capacity. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to facilitate efﬁcient learning and prevent the negative transfer. Throughout the extensive experimental results, we show that our
AGS-CL uses orders of magnitude less memory space for storing the regularization parameters, and it signiﬁcantly outperforms several state-of-the-art baselines on representative benchmarks for both supervised and reinforcement learning. 1

Introduction
Continual learning, also referred to as lifelong learning, is a long standing open problem in machine learning, in which the training data is given sequentially in a form divided into the groups of tasks. The goal of continual learning is to overcome the fundamental trade-off: the stability-plasticity dilemma
[7, 21], i.e., if the model focuses too much on the stability, it suffers from poor forward transfer to the new task, and if it focuses too much on the plasticity, it suffers from the catastrophic forgetting of past tasks. To address this dilemma, a comprehensive study for neural network-based continual learning was conducted broadly under the following categories: regularization-based [18, 13, 41, 22, 1, 3], dynamic architecture-based [27, 39, 10], and replay memory-based [26, 20, 34, 11] methods.
In this paper, we focus on the regularization-based methods, since they pursue to use the ﬁxed-capacity neural network model as efﬁciently as possible, which may potentially allow them to be combined with other approaches. These methods typically identify important learned weights for previous tasks and heavily penalize their deviations while learning new tasks. They have a natural connection with a separate line of research, the model compression of neural networks [17, 19, 42]. Namely, in order to obtain a compact model, typical model compression methods measure the importance of each node or weight in a given neural network and prune the unimportant ones, hence, share the similar principle with the regularization-based continual learning schemes. Several representative model compression methods [37, 4, 38, 29] used the group Lasso-like penalties, which deﬁne the incoming or outgoing weights to a node as groups and achieve structured sparsity within a neural network. Such focus on the node-level importance could lead to a more efﬁcient representation of the model and achieved better compression than focusing on the weight-wise importance.
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Inspired by such connection, we propose a new regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), that can adaptively control the plasticity and stability of a neural network learner by using two node-wise group sparsity-based penalties as regularization terms. Namely, our ﬁrst term, which is equivalent to the ordinary group
Lasso penalty, assigns and learns new important nodes when learning a new task while maintaining the structured sparsity (i.e., controls plasticity), whereas the second term, which is a group sparsity penalty imposed on the drifts of the important node parameters, prevents the forgetting of the previously learned important nodes via freezing the incoming weights to the nodes (i.e., controls stability). The two terms are selectively applied to each node based on the adaptive regularization parameter that represents the importance of each node, which is updated after learning each new task.
For learning, we utilize the proximal gradient descent (PGD) [23] such that the exact sparsity and freezing of the nodes can be elegantly attained, without any additional threshold to tune. Moreover, as a critical detail, we re-initialize the weights associated with the unimportant nodes after learning each task, such that the negative transfer can be prevented and plasticity can be maximized.
As a result, we convincingly show our AGS-CL efﬁciently mitigates the catastrophic forgetting while continuously learning new tasks, throughout extensive experiments on several benchmarks in both supervised and reinforcement learning. Our experimental contributions are multifold. First, we show that AGS-CL signiﬁcantly outperforms strong state-of-the-art baselines [13, 41, 2, 8] on all of benchmark datasets we tested. Second, we give a detailed analysis on the stability-plasticity trade-off of our model, by utilizing additional metrics beyond average accuracy. Third, we identify AGS-CL uses orders of magnitude less additional memory than the baselines to store the regularization parameters, thanks to only maintaining the node-wise regularization parameters. Such compact memory usage is a nice by-product and enables applying our method to much larger networks, which typically is necessary for applications with large-scale datasets. Finally, we stress that our RL results on Atari games are for the pure continual learning setting, in which past tasks cannot be learned again, in contrast to other works [13, 31] that allow the agents to learn multiple tasks in a recurring fashion.