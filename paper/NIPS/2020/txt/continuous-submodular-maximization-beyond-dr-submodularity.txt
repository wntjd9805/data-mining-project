Abstract
In this paper, we propose the ﬁrst continuous optimization algorithms that achieve a constant factor approximation guarantee for the problem of monotone continuous submodular maximization subject to a linear constraint. We ﬁrst prove that a simple variant of the vanilla coordinate ascent, called COORDINATE-ASCENT+, achieves a ( e−1 2e−1 − ε)-approximation guarantee while performing O(n/ε) iterations, where the computational complexity of each iteration is roughly O(n/
ε + n log n) (here, n denotes the dimension of the optimization problem). We then propose
COORDINATE-ASCENT++, that achieves the tight (1 − 1/e − ε)-approximation guarantee while performing the same number of iterations, but at a higher computa-tional complexity of roughly O(n3/ε2.5 + n3 log n/ε2) per iteration. However, the computation of each round of COORDINATE-ASCENT++ can be easily parallelized so that the computational cost per machine scales as O(n/
ε + n log n).
√
√ 1

Introduction
Submodularity is a fundamental concept in combinatorial optimization, usually associated with discrete set functions [Fujishige, 1991]. As submodular functions formalize the intuitive notion of diminishing returns, and thus provide a useful structure, they appear in a wide range of modern machine learning applications including various forms of data summarization [Lin and Bilmes, 2012, Mirzasoleiman et al., 2013], inﬂuence maximization [Kempe et al., 2003], sparse and deep representations [Balkanski et al., 2016, Oh Song et al., 2017], fairness Celis et al. [2016], Kazemi et al. [2018], experimental design [Harshaw et al., 2019], neural network interpretability Elenberg et al. [2017], human-brain mapping [Salehi et al., 2017], adversarial robustness [Lei et al., 2018], crowd teaching [Singla et al., 2014], to name a few. Moreover, submodularity ensures the tractability of the underlying combinatorial optimization problems as minimization of submodular functions can be done exactly and (constrained) maximization of submodular functions can be done approximately.
For more details regarding the theory and applications of submodular functions in machine learning and signal processing, we refer the interested reader to the recent surveys by Buchbinder and Feldman
[2018] and Tohidi et al. [2020].
To capture an even larger set of applications, while providing rigorous guarantees, the discrete notion of submodularity has been generalized in various directions, including adaptive and interactive submodualarity for sequential decision making problems [Golovin and Krause, 2011, Guillory and
Bilmes, 2010], weak submodularity for general set functions with a bounded submodularity distance
[Das and Kempe, 2011] and sequence submodularity for time series analysis [Tschiatschek et al., 2017, Mitrovic et al., 2019], among other variants.
Very recently, a surge of new applications in machine learning and statistics motivated researchers to study continuous submodular functions [Bach, 2015, Wolsey, 1982a], a large class of non-convex/non-34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
concave functions, which may be optimized efﬁciently. In particular, it has been shown that continuous submodular minimization can be done exactly Bach [2015]. In contrast, for continuous submodular maximization, it is usually assumed that the continuous function is not only submodular, but also has the extra condition of diminishing returns. Such functions are usually called continuous DR-submodular [Bian et al., 2017b]. We should highlight that even though in the discrete domain, submodularity and diminishing returns are equivalent; in the continuous domain, the diminishing returns condition implies continuous submodularity, but not vice versa.
In this paper, we propose the ﬁrst algorithms that achieve constant factor approximation guarantees for the maximization of a monotone continuous submodular function subject to a linear constraint.
More speciﬁcally, our contributions can be summarized as follows:
• We develop a variant of the coordinate ascent algorithm, called COORDINATE-ASCENT+, that achieves a ( e−1 2e−1 − ε)-approximation guarantee while performing O(n/(cid:15)) iterations, where the computational complexity of each iteration is O(n(cid:112)B/ε + n log n). Here, n and
B denote the dimension of the optimization problem and the (cid:96)1 radius of the constraint set, respectively.
• We then develop COORDINATE-ASCENT++, that achieves the tight (1 − 1/e − ε) approxi-mation guarantee while performing O(n/(cid:15)) iterations, where the computational complexity of each iteration is O(n3
B/ε2.5 + n3 log n/ε2). Moreover, COORDINATE-ASCENT++ can be easily parallelized so that the computational complexity per machine in each round scales as O(n(cid:112)B/(cid:15) + n log n).
√
Notably, to establish these results, we do not assume that the continuous submodular function satisﬁes the diminishing returns condition. 1.1