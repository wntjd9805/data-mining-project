Abstract
Much of the remarkable progress in computer vision has been focused around fully supervised learning mechanisms relying on highly curated datasets for a variety of tasks. In contrast, humans often learn about their world with little to no external supervision. Taking inspiration from infants learning from their environment through play and interaction, we present a computational framework to discover objects and learn their physical properties along this paradigm of learning from interaction. Our agent, when placed within the near photo-realistic and physics-enabled AI2THOR environment, interacts with its world and learns about objects, their geometric extents and relative masses, without any external guidance. Our experiments reveal that this agent learns efﬁciently and effectively; not just for objects it has interacted with before, but also for novel instances from seen categories as well as novel object categories. 1

Introduction
Over the past few years, the computer vision community has witnessed remarkable breakthroughs on a variety of tasks such as image classiﬁcation [31], object detection [49] and semantic segmentation [9].
Much of this progress can be attributed to the re-emergence of deep learning in an era of tremendous compute, and importantly, the availability of large annotated datasets that enable models to be trained in a fully supervised learning paradigm. While recent works [25, 10] have shown an impressive ability to train visual representation stacks in a self-supervised manner, downstream applications using these representations continue to be trained with fully supervised methods [23].
In stark contrast, humans often learn about their world with little or even no external supervision.
For instance, infants learn about objects in their physical environment and their behaviors just by observing [4] and interacting [21] with them. Inspired by these studies, we propose a computational approach to discover objects and learn their physical properties in a self-supervised setting along this paradigm of Learning from Interaction.
Learning about objects by interacting entails the following steps: First, given an environment, the learning agent must pick a location in space, perhaps an object, to interact with. Second, the agent must determine the nature of this interaction (for instance, pushing, lifting, throwing, etc). Third, the result of this interaction must be interpreted solely via visual feedback and with no external supervision. And ﬁnally, it must iterate over these steps effectively and efﬁciently with the goal of learning about objects and their attributes. For instance, an agent attempting to learn to estimate the mass of objects based on their appearance may ﬁnd it beneﬁcial to interact with a diverse set of objects, pick or push them and ﬁnally acquire feedback via a combination of the force applied and the resultant displacement on the objects. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Our goal is to learn geometric extents (segmentation) and masses of objects in a self-supervised fashion by interacting with the surrounding world. The agent should learn not only which parts of the current observation are interactable but also how to interact with them. For example, a small force may not move a sofa but moves an apple, which enables estimating the object properties.
In this work, we present an agent that learns to locate objects, predict their geometric extents as well as relative masses merely by interacting with its environment, with no external supervisory signal (Figure 1). Our proposed model learns to predict what it should interact with and what forces should be applied to those points. The interaction results in a sequence of raw observations, which are used as supervision for training a CNN model. The raw observations enable computing self-supervised losses for the interaction point, the amount of force, and for clustering points that move coherently upon interaction and so probably belong to a single object. To stabilize training and make it more efﬁcient, we use a memory bank with prioritized sampling which is inspired by prioritized replay and self-paced learning.
We train and evaluate our agent within the AI2THOR [30] environment, a near photo-realistic virtual indoor environment of 120 rooms such as kitchens and living rooms with more than 2,000 object instances across 125 categories. Importantly, AI2THOR is based on a physics engine which enables objects to have physical properties such as mass, friction and elasticity and for objects and scenes to interact realistically with each other. Our agent is placed in this world with no prior knowledge (apart from the inductive bias of a CNN and a self-supervision module), interacts with this world by applying forces, starts learning about the presence of objects via their displacement and eventually learns to visually estimate their attributes. Experimental evaluations show that our model obtains promising results for novel instances of seen object categories as well as unseen object categories. 2