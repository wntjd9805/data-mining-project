Abstract
We reformulate the Wasserstein Discriminant Analysis (WDA) as a ratio trace problem and present an eigensolver-based algorithm to compute the discrimina-tive subspace of WDA. This new formulation, along with the proposed algorithm, can be served as an efﬁcient and more stable alternative to the original trace ratio formulation and its gradient-based algorithm. We provide a rigorous convergence analysis for the proposed algorithm under the self-consistent ﬁeld framework, which is crucial but missing in the literature. As an application, we combine WDA with low-dimensional clustering techniques, such as K-means, to perform sub-space clustering. Numerical experiments on real datasets show promising results of the ratio trace formulation of WDA in both classiﬁcation and clustering tasks. 1

Introduction
Wasserstein Discriminant Analysis (WDA) [13] is a supervised linear dimensionality reduction tech-nique that generalizes the classical Fisher Discriminant Analysis (FDA) [16] using the optimal trans-port distances [41]. Many existing works [44, 29, 11, 4] have addressed the issue that FDA only considers global information. In particular, [49] proposed a new formula relaying on worst-case distance; [37] developed a localized version of FDA; [22] provided an adaptive method for learning local structure from data. The recently proposed WDA [13] has the advantage of adaptively cap-turing both local and global information, and shows competitive performance in classiﬁcation tasks compared to other supervised dimensionality reduction techniques.
WDA as developed in [13] used the trace ratio formulation to maximize the ratio of the inter-class’s regularized Wasserstein distances to the intra-class’s regularized Wasserstein distances. Formally, they aimed to solve maxP Trace(PT Cb(T)P)/Trace(PT Cw(T)P) where Cb and Cw are the inter-class and intra-class covariance matrices, respectively, and are functions of the optimal transport matrix T. The optimal transport matrix T quantiﬁes how important the distance between two sam-ples should be in order to obtain a good projection matrix P. The authors in [13] derived the gradient of the objective function with respect to P and also utilized automatic differentiation to compute the gradients. The difﬁculties of their approach are 1) the optimization objective is non-convex and non-smooth; and 2) Cb and Cw are functions of T and T is an implicit function on P. Thus WDA is a bi-level optimization problem [8] and requires solving an optimal transport problem in every step of gradient descent. Due to these complications, theoretical guarantees on the convergence are lack-ing. Vanilla gradient descent gets stuck easily in the non-smooth region, especially for real datasets, due to the natural structure of the data such as low rank or sparsity. In practice, the approach in-troduced in [13] can be sensitive to initialization and may take many iterations or even fail to reach convergence. All these issues raise concerns when WDA is applied to real data. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we circumvent the aforementioned challenges by reformulating WDA as a ratio trace problem, which has a closed-form solution and can be solved by the generalized eigenvalue decom-position if T is given. For algorithms of dimensionality reduction, it is common to use ratio trace formulation to approximate trace ratio problems [46]. For example, in Fisher Discriminant Anal-ysis (FDA), these two formulations are both deﬁned and are both served as criterion to maximize inter-class distance while minimizing intra-class distance [15]. Although there are many compar-isons between these two formulations when the inter-class and intra-class covariance matrices are
ﬁxed [43, 28, 17, 27], they do not concern with the case of the covariance matrices being functions of the discriminative subspace as in WDA. We give numerical comparisons between these two formu-lations in terms of classiﬁcation accuracy on simulated as well as real data, on which the proposed formulation either is comparable or outperforms the original formulation.
Speciﬁcally, we solve the ratio trace problem: argmaxP Trace(PT Cb(T)P(PT Cw(T)P)−1) instead of the original WDA formulation: argmaxP Trace(PT Cb(T)P)/Trace(PT Cw(T)P). We propose an algorithm: WDA-eig, to solve the ratio trace problem using the self-consistent ﬁeld iteration (SCF), and establish a convergence analysis for the general SCF framework with speciﬁc application to the
WDA context. The SCF iteration was originally used for solving Kohn-Sham equation arising in electronic structure calculations [7]. Most works on SCF concern with the standard eigenvalue prob-lem [47, 24, 6], while convergence analysis for the generalized eigenvalue problem has not appeared in current literature. Our numerical examples demonstrate that the algorithm based on SCF iteration usually converges within a few iterations in practice and is less sensitive to initialization compared to the original approach. We also give a convergent analysis under the SCF framework, which not only provides convergence guarantee to the ratio trace WDA problem but is also applicable to other eigenvector-dependent generalized eigenvalue problem.
As an application, we extend WDA-eig to unsupervised clustering. Since WDA requires class labels to calculate the inter- and intra-class Wasserstein distances, a natural solution is to combine WDA with low-dimensional clustering techniques, which requires iteratively applying WDA given updated label information. The new algorithm has a fast convergence compared to the original approach and aid in iteratively applying WDA to ﬁnd the most discriminative subspace. Several methods [10, 48, 42] that are closely related to our work leverage label information by combining FDA with Kmeans.
Our numerical experiments show that the WDA-Kmeans has promising performance compared to existing subspace clustering techniques on real-world datasets.
Our contribution in this paper is three-fold. First, we present a ratio trace formulation of the WDA problem. Second, we propose to solve the problem using the SCF iteration, and provide a convergent analysis for the SCF framework as well as speciﬁc application to the WDA context. Last but not least, we iteratively apply WDA and low-dimensional clustering technique to perform clustering.
We emphasize that we do not attempt solving the original trace ratio formulation of WDA with the proposed algorithm. A better solution to the original formulation is not the focus of this paper.
Notations. We use (cid:107) · (cid:107) to denote the 2-norm of a matrix or vector. In is used to denote the identity matrix of order n. For any matrix X, let xi denote its ith column vector and xi,j denote the (i, j)th entry. For any A, B ∈ Rm×n, (cid:104)A, B(cid:105) is the inner product of A and B, i.e., (cid:104)A, B(cid:105) = trace(AT B).
Let Sn = {A ∈ Rn×n|A = AT } be the set of symmetric matrices. For a symmetric matrix pair (A, B), A, B ∈ Sn with B being positive deﬁnite, we denote the generalized eigenvalues of (A, B) by
λmin(A, B) = λn(A, B) ≤ · · · ≤ λ1(A, B) = λmax(A, B). Let Od×p represent the set of orthogonal d × p matrices, i.e., Od×p = {A ∈ Rd×p | AT A = Id}. 2 Methodology
In this section, we ﬁrst review the existing supervised WDA problem and its gradient-based solver, and reformulate the problem as a nonlinear generalized eigenvalue problem. We then present an algorithm that solves the problem using the self-consistent ﬁeld iteration. 2.1