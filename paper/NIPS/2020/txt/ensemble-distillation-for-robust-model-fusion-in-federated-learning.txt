Abstract
Federated Learning (FL) is a machine learning setting where many devices collab-oratively train a machine learning model while keeping the training data decen-tralized. In most of the current training schemes the central model is reﬁned by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios.
In this work we investigate more powerful and more ﬂexible aggregation schemes for FL. Speciﬁcally, we propose ensemble distillation for model fusion, i.e. training the central classiﬁer through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows ﬂexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far. 1

Introduction
Federated Learning (FL) has emerged as an important machine learning paradigm in which a federation of clients participate in collaborative training of a centralized model [62, 51, 65, 8, 5, 42, 34]. The clients send their model parameters to the server but never their private training datasets, thereby ensuring a basic level of privacy. Among the key challenges in federated training are communication overheads and delays (one would like to train the central model with as few communication rounds as possible), and client heterogeneity: the training data (non-i.i.d.-ness), as well as hardware and computing resources, can change drastically among clients, for instance when training on commodity mobile devices.
Classic training algorithms in FL, such as federated averaging (FEDAVG) [51] and its recent adap-tations [53, 44, 25, 35, 26, 58], are all based on directly averaging of the participating client’s parameters and can hence only be applied if all client’s models have the same size and structure. In contrast, ensemble learning methods [77, 15, 2, 14, 56, 47, 75] allow to combine multiple hetero-geneous weak classiﬁers by averaging the predictions of the individual models instead. However, applying ensemble learning techniques directly in FL is infeasible in practice due to the large number of participating clients, as it requires keeping weights of all received models on the server and performing naive ensembling (logits averaging) for inference.
To enable federated learning in more realistic settings, we propose to use ensemble distillation [7, 22] for robust model fusion (FedDF). Our scheme leverages unlabeled data or artiﬁcially generated examples (e.g. by a GAN’s generator [17]) to aggregate knowledge from all received (heterogeneous)
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Limitations of FEDAVG. We consider a toy example of a 3-class classiﬁcation task with a 3-layer
MLP, and display the decision boundaries (probabilities over RGB channels) on the input space. The left two
ﬁgures show the individually trained local models. The right three ﬁgures evaluate aggregated models and the global data distribution; the averaged model results in much blurred decision boundaries. The used datasets are displayed in Figure 8 (Appendix C.1). client models. We demonstrate with thorough empirical results that our ensemble distillation approach not only addresses the existing quality loss issue [24] of Batch Normalization (BN) [31] for networks in a homogeneous FL system, but can also break the knowledge barriers among heterogeneous client models. Our main contributions are:
• We propose a distillation framework for robust federated model fusion, which allows for heteroge-neous client models and data, and is robust to the choices of neural architectures.
• We show in extensive numerical experiments on various CV/NLP datasets (CIFAR-10/100, Ima-geNet, AG News, SST2) and settings (heterogeneous models and/or data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique.
We further provide insights on when FedDF can outperform FEDAVG (see also Fig. 1 that highlights an intrinsic limitation of parameter averaging based approaches) and what factors inﬂuence FedDF. 2