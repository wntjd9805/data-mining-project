Abstract
While 2D generative adversarial networks have enabled high-resolution image syn-thesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint.
In this paper, we propose a generative model for radiance ﬁelds which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance ﬁelds are not conﬁned to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance ﬁelds are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high ﬁdelity. 1

Introduction
Convolutional generative adversarial networks have demonstrated impressive results in synthesizing high-resolution images [6, 26, 34, 53] from unstructured image collections. However, despite this success, state-of-the-art models struggle to properly disentangle the underlying generative factors including 3D shape and viewpoint. This is in stark contrast to humans who have the remarkable ability to reason about the 3D structure of the world and imagine objects from novel viewpoints.
As reasoning in 3D is fundamental for applications in robotics, virtual reality or data augmentation, several recent works consider the task of 3D-aware image synthesis [19, 39, 76], aiming at photo-realistic image generation with explicit control over the camera pose. In contrast to 2D generative adversarial networks, approaches for 3D-aware image synthesis learn a 3D scene representation which is explicitly mapped to an image using differentiable rendering techniques, hence providing control over both, scene content and viewpoint. Since 3D supervision or posed images are often hard to obtain in practice, recent works try to solve this task using 2D supervision only [19, 39].
Towards this goal, existing approaches generate discretized 3D representations, i.e., a voxel-grid representing either the full 3D object [19] or intermediate 3D features [39] as illustrated in Fig. 1.
While modeling the 3D object in color space allows for exploiting differentiable rendering, the cubic memory growth of voxel-based representations limits [19] to low resolution and results in visible artifacts. Intermediate 3D features [39] are more compact and scale better with image resolution.
∗ Joint ﬁrst authors with equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Motivation. Voxel-based approaches for 3D-aware image synthesis either generate a voxelized 3D model (e.g., PlatonicGAN [19], top) or learn an abstract 3D feature representation (e.g.,
HoloGAN [39], middle). This leads to discretization artifacts or degrades view-consistency of the generated images due to the learned neural projection function. In this paper, we propose a generative model for neural radiance ﬁelds (bottom) which represent the scene as a continuous function gθ that maps a location x and viewing direction d to a color value c and a volume density σ. Our model allows for generating 3D consistent images at high spatial resolution. We visualize 3D consistency by running a multi-view stereo algorithm (COLMAP [60]) on several outputs of each method (right).
Note that all models have been trained using 2D supervision only (i.e., from unposed RGB images).
However, this requires to learn a 3D-to-2D mapping for decoding the abstract features to RGB values, resulting in entangled representations which are not consistent across views at high resolutions.
In this paper, we demonstrate that the dilemma between coarse outputs and entangled latents can be resolved using conditional radiance ﬁelds, a conditional variant of a recently proposed continuous representation for novel view synthesis [36]. More speciﬁcally, we make the following contributions: i) We propose GRAF, a generative model for radiance ﬁelds for high-resolution 3D-aware image synthesis from unposed images. In addition to viewpoint manipulations, our approach allows to modify shape and appearance of the generated objects. ii) We introduce a patch-based discriminator that samples the image at multiple scales and which is key to learn high-resolution generative radiance ﬁelds efﬁciently. iii) We systematically evaluate our approach on synthetic and real datasets.
Our approach compares favorably to state-of-the-art methods in terms of visual ﬁdelity and 3D consistency while generalizing to high spatial resolutions. We release our code and datasets at https://github.com/autonomousvision/graf. 2