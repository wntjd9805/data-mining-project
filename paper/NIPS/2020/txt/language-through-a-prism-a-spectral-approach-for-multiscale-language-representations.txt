Abstract
Language exhibits structure at different scales, ranging from subwords to words, sentences, paragraphs, and documents. To what extent do deep models capture information at these scales, and can we force them to better capture structure across this hierarchy? We approach this question by focusing on individual neurons, ana-lyzing the behavior of their activations at different timescales. We show that signal processing provides a natural framework for separating structure across scales, enabling us to 1) disentangle scale-speciﬁc information in existing embeddings and 2) train models to learn more about particular scales. Concretely, we apply spectral ﬁlters to the activations of a neuron across an input, producing ﬁltered embeddings that perform well on part of speech tagging (word-level), dialog speech acts classiﬁcation (utterance-level), or topic classiﬁcation (document-level), while performing poorly on the other tasks. We also present a prism layer for training models, which uses spectral ﬁlters to constrain different neurons to model structure at different scales. Our proposed BERT + Prism model can better predict masked tokens using long-range context and produces multiscale representations that per-form better at utterance- and document-level tasks. Our methods are general and readily applicable to other domains besides language, such as images, audio, and video. 1

Introduction
Language exhibits structure at multiple levels, ranging from morphology at the subword level [1], word meaning at the lexical level [2], coherence and other discourse properties at the clause or sentence level [3, 4, 5], to topical and narrative structures for entire documents [6, 7]. Prior work in
NLP has shown how these kinds of structures can be explicitly modeled by representing individual levels of structure [8, 9, 10, 11, 12, 13], multiple levels of structure [14, 15, 16], building hierarchical models that capture structure at the sentence level [17, 18] or between sentences [19, 20], and probing to discover known linguistic levels of structure [21, 22, 23, 24].
We propose a new method for uncovering and learning this kind of structure in representations at every scale, from word meaning to document topics, without drawing on prior linguistic models of speciﬁc structural levels like "sentence" or "clause." To do so, we employ tools from spectral analysis, widely used in signal processing and other ﬁelds [25] to separate and control information at different timescales. Intuitively, any sequence of values, such as a neuron’s activations across input tokens, can be represented as a weighted sum of cosine waves with different frequencies. The weight for a particular frequency indicates the amount of structure in the sequence at that scale: weight on higher frequencies indicates faster changes in the neuron’s activation from token to token, while weight on lower frequencies indicates activations that shift more gradually across an input. By removing certain
†atamkin@stanford.edu 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The prism layer specializes different neurons for different scales. First, the representa-tions for an input are computed (left; in this case, the input is of length three). Next, a spectral ﬁlter (a low-, high-, or band-pass) is applied along the activations of each individual neuron (right). This produces neurons that are only able to represent structure at particular scales. Curved lines illustrate the scales at which neurons can change over an input. frequencies, called spectral ﬁltering, we can remove information about variation at particular scales.
See Figure 2 for a visualization.
In this work, we apply spectral ﬁlters to the activations of individual neurons in BERT [26], a popular deep NLP model. This enables us to separate information in model representations that changes at different rates across the input—for example, part of speech changes on a word-to-word basis, while topical changes are much more gradual. Concretely, we contribute: 1. A principled framework based on spectral analysis for describing structure at multiple scales in deep representations. While we consider applications to NLP models, this is a general framework that could extend to other models with representations arranged in spatial or temporal structure. (Section 2) 2. A technique, spectral ﬁltering, for extracting scale-speciﬁc information from language representations. We show how low-pass ﬁlters can alter representations to only perform well on topic classiﬁcation (document-level), while band-pass and high-pass ﬁlters do the same for dialog acts classiﬁcation (utterance-level) and part of speech tagging (word-level). (Section 3) 3. A new model component, the prism layer, which specializes neurons in a model for particular scales of structure. After training with a prism layer, our model is more sensitive to long-range interactions between tokens and produces individual representations that perform comparably or better than BERT’s across tasks at different scales. (Section 4) 2 Spectral ﬁltering of contextual word representations
This section provides some background on the spectral analysis tools we use and describes how we apply them to deep language representations. 2.1