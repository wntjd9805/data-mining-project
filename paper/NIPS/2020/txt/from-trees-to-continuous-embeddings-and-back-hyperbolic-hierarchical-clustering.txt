Abstract
Similarity-based Hierarchical Clustering (HC) is a classical unsupervised machine learning algorithm that has traditionally been solved with heuristic algorithms like
Average-Linkage. Recently, Dasgupta [25] reframed HC as a discrete optimization problem by introducing a global cost function measuring the quality of a given tree. In this work, we provide the ﬁrst continuous relaxation of Dasgupta’s discrete optimization problem with provable quality guarantees. The key idea of our method,
HYPHC, is showing a direct correspondence from discrete trees to continuous representations (via the hyperbolic embeddings of their leaf nodes) and back (via a decoding algorithm that maps leaf embeddings to a dendrogram), allowing us to search the space of discrete binary trees with continuous optimization. Building on analogies between trees and hyperbolic space, we derive a continuous analogue for the notion of lowest common ancestor, which leads to a continuous relaxation of Dasgupta’s discrete objective. We can show that after decoding, the global minimizer of our continuous relaxation yields a discrete tree with a (1 + ε)-factor approximation for Dasgupta’s optimal tree, where ε can be made arbitrarily small and controls optimization challenges. We experimentally evaluate HYPHC on a variety of HC benchmarks and ﬁnd that even approximate solutions found with gradient descent have superior clustering quality than agglomerative heuristics or other gradient based algorithms. Finally, we highlight the ﬂexibility of HYPHC using end-to-end training in a downstream classiﬁcation task. 1

Introduction
Hierarchical Clustering (HC) is a fundamental problem in data analysis, where given datapoints and their pairwise similarities, the goal is to construct a hierarchy over clusters, in the form of a tree whose leaves correspond to datapoints and internal nodes correspond to clusters. HC naturally arises in standard applications where data exhibits hierarchical structure, ranging from phylogenetics [26] and cancer gene sequencing [47, 48] to text/image analysis [49], community detection [36] and everything in between. A family of easy to implement, yet slow, algorithms includes agglomerative methods (e.g., Average-Linkage) that build the tree in a bottom-up manner by iteratively merging pairs of similar datapoints or clusters together. In contrast to “ﬂat” clustering techniques like k-means,
HC provides ﬁne-grained interpretability and rich cluster information at all levels of granularity and alleviates the requirement of specifying a ﬁxed number of clusters a priori.
Despite the abundance of HC algorithms, the HC theory was underdeveloped, since no “global” objective function was associated with the ﬁnal tree output. A well-formulated objective allows us to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
compare different algorithms, measure their quality, and explain their success or failure.1 To address this issue, Dasgupta [25] recently introduced a discrete cost function over the space of binary trees with n leaves. A key property of his cost function is that low-cost trees correspond to meaningful hierarchical partitions in the data. He initially proved this for symmetric stochastic block models, and later works provided experimental evidence [44], or showed it for hierarchical stochastic block models, suitable for inputs that contain a ground-truth hierarchical structure [20, 21]. These works led to important steps towards understanding old and building new HC algorithms [14, 15, 40, 4].
The goal of this paper is to improve the performance of HC algorithms using a differentiable relaxation of Dasgupta’s discrete optimization problem. There have been recent attempts at gradient-based HC via embedding methods, which do not directly relax Dasgupta’s optimization problem. UFit [19] addresses a different “ultrametric ﬁtting” problem using Euclidean embeddings, while gHHC [39] assumes that partial information about the optimal clustering—more speciﬁcally leaves’ hyperbolic embeddings—is known. Further, these two approaches lack theoretical guarantees in terms of clustering quality and are outperformed by discrete agglomerative algorithms.
A relaxation of Dasgupta’s discrete optimization combined with the powerful toolbox of gradient-based optimization has the potential to yield improvements in terms of (a) clustering quality (both theoretically and empirically), (b) scalability, and (c) ﬂexibility, since a gradient-based approach can be integrated into machine learning (ML) pipelines with end-to-end training. However, due to the inherent discreteness of the HC optimization problem, several challenges arise: (1) How can we continuously parameterize the search space of discrete binary trees? A promising direction is leveraging hyperbolic embeddings which are more aligned with the geometry of trees than standard Euclidean embeddings [46]. However, hyperbolic embedding methods typically assume a fully known [41, 45] or partially known graph [39] that will be embedded, whereas the challenge here is searching over an exponentially large space of trees with unknown structure. (2) How can we derive a differentiable relaxation of the HC cost? One of the key challenges is that this cost relies on discrete properties of trees such as the lowest common ancestor (LCA). (3) How can we decode a discrete binary tree from continuous representations, while ensuring that the ultimate discrete cost is close to the continuous relaxation?
Here, we introduce HYPHC, an end-to-end differentiable model for HC with provable guarantees in terms of clustering quality, which can be easily incorporated into ML pipelines. (1) Rather than minimizing the cost function by optimizing over discrete trees, we parameterize trees using leaves’ hyperbolic embeddings. In contrast with Euclidean embeddings, hyperbolic embeddings can represent trees with arbitrarily low distortion in just two dimensions [46]. We show that the leaves themselves provide enough information about the underlying tree, avoiding the need to explicitly represent the discrete structure of internal nodes. (2) We derive a continuous LCA analogue, which leverages the analogies between shortest paths in trees and hyperbolic geodesics (Fig. 1), and propose a differentiable variant of Dasgupta’s cost. (3) We propose a decoding algorithm for the internal nodes which maps the learned leaf embeddings to a dendrogram (cluster tree) of low distortion.
We show (a) that our approach produces good clustering quality, in terms of Dasgupta cost. The-oretically, assuming perfect optimization, the optimal clustering found using HYPHC yields a (1 + ε)-approximation to the minimizer of the discrete cost, where ε can be made arbitrarily small, and controls the tradeoffs between quality guarantees and optimization challenges. Notice that due to our perfect optimization assumption, this does not contradict previously known computational hardness results based on the Small Set Expansion for achieving constant approximations [14], but allows us to leverage the powerful toolbox of nonconvex optimization. Empirically, we ﬁnd that even approximate HYPHC solutions found with gradient descent outperform or match the performance of the best discrete and continuous methods on a variety of HC benchmarks. Additionally, (b) we propose two extensions of HYPHC that enable us to scale to large inputs and we also study the tradeoffs between clustering quality and speed. Finally, (c) we demonstrate the ﬂexibility of HYPHC by jointly optimizing embeddings for HC and a downstream classiﬁcation task, improving accuracy by up to 3.8% compared to the two-stage embed-then-classify approach. 1Contrast this lack of HC objectives with ﬂat clustering, where k-means objectives and others have been studied intensively from as early as 1960s (e.g., [29]), leading today to a comprehensive theory on clustering. 2
i ∨ j ∨ k i ∨ j i j k (a) (b) (c) (d)
Figure 1: The tree shown in (a) is embedded into hyperbolic space (B2) in (b), (c), and (d). In (c) and (d) we show the hyperbolic LCA (in red) and illustrate the relationship between the discrete LCA, which is central to Dasgupta’s cost, and geodesics (shortest paths) in hyperbolic space. 2