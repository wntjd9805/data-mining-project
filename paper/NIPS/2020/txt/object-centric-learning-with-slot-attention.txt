Abstract
Learning object-centric representations of complex scenes is a promising step towards enabling efﬁcient abstract reasoning from low-level perceptual features.
Yet, most deep learning approaches learn distributed representations that do not capture the compositional properties of natural scenes.
In this paper, we present the Slot Attention module, an architectural component that interfaces with perceptual representations such as the output of a convolutional neural network and produces a set of task-dependent abstract representations which we call slots.
These slots are exchangeable and can bind to any object in the input by specializing through a competitive procedure over multiple rounds of attention. We empirically demonstrate that Slot Attention can extract object-centric representations that enable generalization to unseen compositions when trained on unsupervised object discovery and supervised property prediction tasks. 1

Introduction
Object-centric representations have the potential to improve sample efﬁciency and generalization of machine learning algorithms across a range of application domains, such as visual reasoning [1], mod-eling of structured environments [2], multi-agent modeling [3–5], and simulation of interacting physi-cal systems [6–8]. Obtaining object-centric representations from raw perceptual input, such as an im-age or a video, is challenging and often requires either supervision [1, 3, 9, 10] or task-speciﬁc architec-tures [2, 11]. As a result, the step of learning an object-centric representation is often skipped entirely.
Instead, models are typically trained to operate on a structured representation of the environment that is obtained, for example, from the internal representation of a simulator [6, 8] or of a game engine [4, 5].
To overcome this challenge, we introduce the Slot Attention module, a differentiable interface between perceptual representations (e.g., the output of a CNN) and a set of variables called slots.
Using an iterative attention mechanism, Slot Attention produces a set of output vectors with permutation symmetry. Unlike capsules used in Capsule Networks [12, 13], slots produced by Slot
Attention do not specialize to one particular type or class of object, which could harm generalization.
Instead, they act akin to object ﬁles [14], i.e., slots use a common representational format: each slot can store (and bind to) any object in the input. This allows Slot Attention to generalize in a systematic way to unseen compositions, more objects, and more slots.
Slot Attention is a simple and easy to implement architectural component that can be placed, for example, on top of a CNN [15] encoder to extract object representations from an image and is trained end-to-end with a downstream task. In this paper, we consider image reconstruction and set prediction as downstream tasks to showcase the versatility of our module both in a challenging unsupervised object discovery setup and in a supervised task involving set-structured object property prediction.
†Work done while interning at Google, ∗equal contribution, ‡equal advising. Contact: tkipf@google.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(b) Object discovery architecture. (a) Slot Attention module. (c) Set prediction architecture.
Figure 1: (a) Slot Attention module and example applications to (b) unsupervised object discovery and (c) supervised set prediction with labeled targets yi. See main text for details.
Our main contributions are as follows: (i) We introduce the Slot Attention module, a simple architec-tural component at the interface between perceptual representations (such as the output of a CNN) and representations structured as a set. (ii) We apply a Slot Attention-based architecture to unsupervised object discovery, where it matches or outperforms relevant state-of-the-art approaches [16, 17], while being more memory efﬁcient and signiﬁcantly faster to train. (iii) We demonstrate that the Slot At-tention module can be used for supervised object property prediction, where the attention mechanism learns to highlight individual objects without receiving direct supervision on object segmentation. 2 Methods
In this section, we introduce the Slot Attention module (Figure 1a; Section 2.1) and demonstrate how it can be integrated into an architecture for unsupervised object discovery (Figure 1b; Section 2.2) and into a set prediction architecture (Figure 1c; Section 2.3). 2.1 Slot Attention Module
The Slot Attention module (Figure 1a) maps from a set of N input feature vectors to a set of K output vectors that we refer to as slots. Each vector in this output set can, for example, describe an object or an entity in the input. The overall module is described in Algorithm 1 in pseudo-code1.
Slot Attention uses an iterative attention mechanism to map from its inputs to the slots. Slots are initialized at random and thereafter reﬁned at each iteration t = 1 . . . T to bind to a particular part (or grouping) of the input features. Randomly sampling initial slot representations from a common distribution allows Slot Attention to generalize to a different number of slots at test time.
At each iteration, slots compete for explaining parts of the input via a softmax-based attention mechanism [18–20] and update their representation using a recurrent update function. The ﬁnal representation in each slot can be used in downstream tasks such as unsupervised object discovery (Figure 1b) or supervised set prediction (Figure 1c).
We now describe a single iteration of Slot Attention on a set of input features, inputs ∈ RN ×Dinputs , with K output slots of dimension Dslots (we omit the batch dimension for clarity). We use learnable linear transformations k, q, and v to map inputs and slots to a common dimension D.
Slot Attention uses dot-product attention [19] with attention coefﬁcients that are normalized over the slots, i.e., the queries of the attention mechanism. This choice of normalization introduces competition between the slots for explaining parts of the input. 1An implementation of Slot Attention is available at: https://github.com/google-research/ google-research/tree/master/slot_attention. 2
Algorithm 1 Slot Attention module. The input is a set of N vectors of dimension Dinputs which is mapped to a set of K slots of dimension Dslots. We initialize the slots by sampling their initial values as independent samples from a Gaussian distribution with shared, learnable parameters µ ∈ RDslots and σ ∈ RDslots. In our experiments we set the number of iterations to T = 3. 1: Input: inputs ∈ RN ×Dinputs , slots ∼ N (µ, diag(σ)) ∈ RK×Dslots 2: Layer params: k, q, v: linear projections for attention; GRU; MLP; LayerNorm (x3) 3: 4: 5: 6: 7: slots_prev = slots slots = LayerNorm (slots) attn = Softmax ( 1√
D updates = WeightedMean (weights=attn + (cid:15), values=v(inputs)) slots = GRU (state=slots_prev, inputs=updates) slots += MLP (LayerNorm (slots))
# norm. over slots
# aggregate
# GRU update (per slot)
# optional residual MLP (per slot) inputs = LayerNorm (inputs) for t = 0 . . . T k(inputs) · q(slots)T , axis=‘slots’) 8: 9: 10: 11: return slots
We further follow the common practice of setting the softmax temperature to a ﬁxed value of
√
D [20]: attni,j := eMi,j l eMi,l (cid:80) where
M := 1
√
D k(inputs) · q(slots)T ∈ RN ×K. (1)
In other words, the normalization ensures that attention coefﬁcients sum to one for each individual input feature vector, which prevents the attention mechanism from ignoring parts of the input. To aggregate the input values to their assigned slots, we use a weighted mean as follows: attni,j l=1 attnl,j updates := W T · v(inputs) ∈ RK×D
Wi,j := where (cid:80)N (2)
.
The weighted mean helps improve stability of the attention mechanism (compared to using a weighted sum) as in our case the attention coefﬁcients are normalized over the slots. In practice we further add a small offset (cid:15) to the attention coefﬁcients to avoid numerical instability.
The aggregated updates are ﬁnally used to update the slots via a learned recurrent function, for which we use a Gated Recurrent Unit (GRU) [21] with Dslots hidden units. We found that transforming the
GRU output with an (optional) multi-layer perceptron (MLP) with ReLU activation and a residual connection [22] can help improve performance. Both the GRU and the residual MLP are applied independently on each slot with shared parameters. We apply layer normalization (LayerNorm) [23] both to the inputs of the module and to the slot features at the beginning of each iteration and before applying the residual MLP. While this is not strictly necessary, we found that it helps speed up training convergence. The overall time-complexity of the module is O (T · D · N · K).
We identify two key properties of Slot Attention: (1) permutation invariance with respect to the input (i.e., the output is independent of permutations applied to the input and hence suitable for sets) and (2) permutation equivariance with respect to the order of the slots (i.e., permuting the order of the slots after their initialization is equivalent to permuting the output of the module). More formally:
Proposition 1. Let SlotAttention(inputs, slots) ∈ RK×Dslots be the output of the Slot Attention module (Algorithm 1), where inputs ∈ RN ×Dinputs and slots ∈ RK×Dslots . Let πi ∈ RN ×N and
πs ∈ RK×K be arbitrary permutation matrices. Then, the following holds:
SlotAttention(πi · inputs, πs · slots) = πs · SlotAttention(inputs, slots) .
The proof is in the supplementary material. The permutation equivariance property is important to ensure that slots learn a common representational format and that each slot can bind to any object in the input. 2.2 Object Discovery
Set-structured hidden representations are an attractive choice for learning about objects in an unsuper-vised fashion: each set element can capture the properties of an object in a scene, without assuming 3
a particular order in which objects are described. Since Slot Attention transforms input represen-tations into a set of vectors, it can be used as part of the encoder in an autoencoder architecture for unsupervised object discovery. The autoencoder is tasked to encode an image into a set of hidden rep-resentations (i.e., slots) that, taken together, can be decoded back into the image space to reconstruct the original input. The slots thereby act as a representational bottleneck and the architecture of the de-coder (or decoding process) is typically chosen such that each slot decodes only a region or part of the image [16, 17, 24–27]. These regions/parts are then combined to arrive at the full reconstructed image.
Encoder Our encoder consists of two components: (i) a CNN backbone augmented with positional embeddings, followed by (ii) a Slot Attention module. The output of Slot Attention is a set of slots, that represent a grouping of the scene (e.g. in terms of objects).
Decoder Each slot is decoded individually with the help of a spatial broadcast decoder [28], as used in IODINE [16]: slot representations are broadcasted onto a 2D grid (per slot) and augmented with position embeddings. Each such grid is decoded using a CNN (with parameters shared across the slots) to produce an output of size W × H × 4, where W and H are width and height of the image, respectively. The output channels encode RGB color channels and an (unnormalized) alpha mask.
We subsequently normalize the alpha masks across slots using a Softmax and use them as mixture weights to combine the individual reconstructions into a single RGB image. 2.3 Set Prediction
Set representations are commonly used in tasks across many data modalities ranging from point cloud prediction [29, 30], classifying multiple objects in an image [31], or generation of molecules with desired properties [32, 33]. In the example considered in this paper, we are given an input image and a set of prediction targets, each describing an object in the scene. The key challenge in predicting sets is that there are K! possible equivalent representations for a set of K elements, as the order of the targets is arbitrary. This inductive bias needs to be explicitly modeled in the architecture to avoid discontinuities in the learning process, e.g. when two semantically specialized slots swap their content throughout training [31, 34]. The output order of Slot Attention is random and independent of the input order, which addresses this issue. Therefore, Slot Attention can be used to turn a distributed representation of an input scene into a set representation where each object can be separately classiﬁed with a standard classiﬁer as shown in Figure 1c.
Encoder We use the same encoder architecture as in the object discovery setting (Section 2.2), namely a CNN backbone augmented with positional embeddings, followed by Slot Attention, to arrive at a set of slot representations.
Classiﬁer For each slot, we apply a MLP with parameters shared between slots. As the order of both predictions and labels is arbitrary, we match them using the Hungarian algorithm [35]. We leave the exploration of other matching algorithms [36, 37] for future work. 3