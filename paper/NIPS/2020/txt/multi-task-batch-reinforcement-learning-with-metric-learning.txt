Abstract
We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple datasets collected from different tasks, we train a multi-task policy to perform well in unseen tasks sampled from the same distribution. The task identities of the unseen tasks are not provided. To perform well, the policy must infer the task identity from collected transitions by modelling its dependency on states, actions and rewards. Because the different datasets may have state-action distributions with large divergence, the task inference module can learn to ignore the rewards and spuriously correlate only state-action pairs to the task identity, leading to poor test time performance. To robustify task inference, we propose a novel application of the triplet loss. To mine hard negative examples, we relabel the transitions from the training tasks by approximating their reward functions. When we allow further training on the unseen tasks, using the trained policy as an initialization leads to signiﬁcantly faster convergence compared to randomly initialized policies (up to 80% improvement and across 5 different Mujoco task distributions). We name our method MBML (Multi-task Batch RL with Metric Learning) 2. 1

Introduction
Combining neural networks (NN) with reinforcement learning (RL) has led to many recent advances
[1–5]. Since training NNs requires diverse datasets and collecting real world data is expensive, most
RL successes are limited to scenarios where the data can be cheaply generated in a simulation. On the other hand, ofﬂine data is essentially free for many applications and RL methods should use it whenever possible. This is especially true because practical deployments of RL are bottle-necked by its poor sample efﬁciency. This insight has motivated a ﬂurry of recent works in Batch RL [6–10].
These works introduce specialized algorithms to stabilize training from ofﬂine datasets. However, ofﬂine datasets are not necessarily diverse. In this work, we investigate how the properties of a diverse dataset inﬂuence the policy search procedure. By collecting diverse ofﬂine dataset, we hope the networks will generalize without further training to unseen tasks or provide good initialization that speeds up convergence when we perform further on-policy training.
To collect diverse datasets, it occurs to us that we should collect data from different tasks. However, datasets collected from different tasks may have state-action distributions with large divergence. Such dataset bias presents a unique challenge in robust task inference. We provide a brief description of the problem setting, the challenge and our contributions below. For ease of exposition, we refer to such datasets as having little overlap in their state-action visitation frequencies thereafter.
∗Equal Contribution 2Website: https://sites.google.com/eng.ucsd.edu/multi-task-batch-reinforcement/home 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: A toy example to illustrate the challenge. The agent must navigate from the origin to a goal location. Left: Goal 1 and Goal 2 denote the two training tasks. The red and blue squares indicate the transitions collected from task 1 and 2 respectively. We can train the task inference module to infer the task identity to be 1 when the context set contains the red transitions and 2 when the context set contains the blue transitions. Since there are no overlap between the red and blue squares, the task inference module learns to correlate the state-action pairs to the task identity. Right:
The failure of the task inference module. The policy must infer the task identity from the randomly collected transitions, denoted by the green squares. The agent needs to navigate to goal 1 during testing. However, if the green squares have more overlap with the blue squares, the task inference module will predict 2 to be the task identity. The agent therefore navigates to the wrong goal location.
We tackle the Multi-task Batch RL problem. We train a policy from multiple datasets, each generated by interaction with a different task. We measure the performance of the trained policy on unseen tasks sampled from the same task distributions as the training tasks. To perform well, the policy must
ﬁrst infer the identity of the unseen tasks from collected transitions and then take the appropriate actions to maximize returns. To train the policy to infer the task identity, we can train it to distinguish between the different training tasks when given transitions from the tasks as input. These transitions are referred to as the context set [11]. Ideally, the policy should model the dependency of the task identity on both the rewards and the state-action pairs in the context set. To achieve this, we can train a task identiﬁcation network that maps the collected experiences, including both state-action pairs and rewards, to the task identity or some task embedding. This approach, however, tends to fail in practice.
Since the training context sets do not overlap signiﬁcantly in state-action visitation frequencies, it is possible that the learning procedure would minimize the loss function for task identiﬁcation by only correlating the state-action pairs and ignoring rewards, which would cause mistakes in identifying testing tasks. This is an instance of the well-known phenomena of ML algorithms cheating when given the chance [12] and is further illustrated in Fig. 1. We limit our explanations to the cases where the tasks differ in reward functions. Extending our approach to task distribution with different transition functions is easily done. We provide experimental results for both cases.
Our contributions are as follows. To the best of our knowledge, we are the ﬁrst to highlight the issue of the task inference module learning the wrong correlation from biased dataset. We propose a novel application of the triplet loss to robustify task inference. To mine hard negative examples, we approximate the reward function of each task and relabel the rewards in the transitions from the other tasks. When we train the policy to differentiate between the original and relabelled transitions, we force it to consider the rewards since their state-action pairs are the same. Training with the triplet loss generalizes better to unseen tasks compared to alternatives. When we allow further training on the unseen tasks, using the policy trained from the ofﬂine datasets as initialization signiﬁcantly increase convergence speed (up to 80% improvement in sample efﬁciency).
To the best of our knowledge, the most relevant related work is [6], which is solving a different problem from ours. They assume access to the ground truth task identity and reward function of the testing task. Our policy does not know the testing task’s identity and must infer it through collected trajectories. We also do not have access to the reward function of the testing tasks. 2 Preliminaries and Problem Statement
To help the reader follow our explanation, we include a symbol deﬁnition table in Appendix A.
We model a task as a Markov Decision Process M = (S, A, T, T0, R, H), with state space S, action space A, transition function T , initial state distribution T0, reward function R, and horizon H. At each discrete timestep t, the agent is in a state st, picks an action at, arrives at s(cid:48) t ∼ T (·|st, at), and receives a reward R(st, at, s(cid:48) t). The performance measure of policy π is the expected sum of rewards JM (π) = EτM ∼π[(cid:80)H−1 t=0 R(st, at, s(cid:48) t)], where τM = (s0, a0, r0, s1, a1, r1, . . .) is a trajectory generated by using π to interact with M . 2
2.1 Batch Reinforcement Learning
A Batch RL algorithm solves the task using an existing batch of N transitions B = {(st, at, rt, s(cid:48) t)|t = 1, . . . , N }. A recent advance in this area is Batch Constrained Q-Learning (BCQ) [9]. Here, we explain how BCQ selects actions. Given a state s, a generator G outputs multiple candidate actions
{am}m. A perturbation model ξ takes as input the state-candidate action and generates small correction ξ(s, am). The corrected action with the highest estimated Q value is selected as π (s):
π (s) = arg max
Q (s, am + ξ (s, am)) , am+ξ(s,am)
{am = G (s, νm)}m ,
νm ∼ N (0, 1). (1)
To help the reader follow our discussion, we illustrate graphically how BCQ selects action in
Appendix B. In our paper, we use BCQ as a routine. The take-away is that BCQ takes as input a batch of transitions B = {(st, at, rt, s(cid:48) t)|t = 1, . . . , N } and outputs three learned functions Q, G, ξ. 2.2 Multi-task Batch Reinforcement Learning
Given K batches, each containing N transition tuples from one task, Bi = {(si,t, ai,t, ri,t, s(cid:48) 1, . . . , K, t = 1, . . . , N }, we deﬁne the Multi-task Batch RL problem as: i,t)|i = arg max
θ
J(θ) = EMi∼p(M ) [JMi(πθ)] , (2) t=0 R(si,t, ai,t, s(cid:48) where an algorithm only has access to the K batches and JMi(π) is the performance of the policy
π in task i, i.e. EτMi ∼π[(cid:80)H−1 i,t)]. p(M ) deﬁnes a task distribution. The subscript i indexes the different tasks. The tasks have the same state and action space and only differ in the transition and reward functions [13]. A distribution over the transition and/or the reward functions therefore deﬁnes the task distribution. We measure performance by computing average returns over unseen tasks sampled from the same task distribution. The policy is not given identity of the unseen tasks before evaluation and must infer it from collected transitions.
In multi-task RL, we can use a task inference module qφ to infer the task identity from a context set.
The context set for a task i consists of transitions from task i and is denoted ci. The task inference module qφ takes ci as input and outputs a posterior over the task identity. We sample a task identity zi from the posterior and inputs it to the policy in addition to the state, i.e. π(s, zi). We model qφ with the probabilistic and permutation-invariant architecture from [11]. qφ outputs the parameters of a diagonal Gaussian. For conciseness, we sometimes use the term policy to also refer to the task inference module. It should be clear from the context whether we are referring to qφ or π.
We evaluate a policy on unseen tasks in two different scenarios: (1) Allowing the policy to collect a small number of interactions to infer z, we evaluate returns without further training, (2) Training the policy in the unseen task and collecting as much data as needed, we evaluate the amount of transitions the policy needs to collect to converge to the optimal performance.
We assume that each batch Bi contains data generated by a policy while learning to solve task Mi.
Thus, if solving each task involve visiting different subspace of the state space, the different batches do not have signiﬁcant overlap in their state-action visitation frequencies. This is illustrated in Fig. 1. 3 Proposed algorithm 3.1 Learning multi-task policy from ofﬂine data with distillation
In Multi-task RL, [14–18] demonstrate the success of distilling multiple single-task policies into a multi-task policy. Inspired by these works, we propose a distillation procedure to obtain a multi-task policy in the Multi-task Batch RL setting. In Sec. 3.2, we argue such distillation procedure alone is insufﬁcient due to the constraints the batch setting imposes on the policy search procedure.
The distillation procedure has two phases. In the ﬁrst phase, we use BCQ to learn a different policy for each task, i.e. we learn K different and independent policies. While we can use any Batch
RL algorithm in the ﬁrst phase, we use BCQ due to its simplicity. As described in Sec. 2.1, for each training batch, BCQ learns three functions: a state-action value function Q, a candidate action generator G and a perturbation generator ξ. The output of the ﬁrst phase thus consists of three sets of networks {Qi}K i=1, where i indexes over the training tasks. i=1, and {ξi}K i=1, {Gi}K 3
In the second phase, we distill each set into a network by incorporating a task inference module.
The distilled function should recover different task-speciﬁc function depending on the inferred task identity. To distill the value functions {Qi}K i=1 into a function QD, for each task i, we sample a context ci and a pair (s, a) from the batch Bi. The task inference module qφ takes ci as input and infers a task identity zi. Given zi as input, QD should assign similar value to (s, a) as the value function for the ith task Qi(s, a). The loss function with a β-weighted KL term [11] is:
LQ = 1
K
K (cid:88) i=1
E (s,a),ci∼Bi (cid:2)(Qi(s, a) − QD(s, a, zi))2 + βKL(qφ(ci)||N (0, 1))(cid:3) , zi ∼ qφ(ci) (3)
We also use Eq. 3 to train qφ using the reparam trick [19]. Similarly, we distill the candidate action generators {Gi}K i=1 into GD. GD takes as input state s, random noise ν and task identity zi.
Depending on zi’s value, we train GD to regress towards the different candidate action generator:
LG = 1
K
K (cid:88) i=1
E s,ci∼Bi
ν∼N (0,1) (cid:2)||Gi(s, ν) − GD(s, ν, ¯zi)||2(cid:3) , zi ∼ qφ(ci). (4)
The bar on top of ¯zi in Eq. 4 indicates the stop gradient operation. We thus do not use the gradient of
Eq. 4 to train the task inference module [11]. Lastly, we distill the perturbation generators {ξi}K i=1 into a single network ξD (Eq. 5). ξD takes as input a state s, a candidate action a, and an inferred task identity zi. We train ξD to regress towards the output of ξi given the same state s and candidate action a as input. We obtain the candidate action a by passing s through the candidate action generator Gi.
Lξ = 1
K
K (cid:88) i=1
E s,ci∼Bi
ν∼N (0,1) (cid:2)||ξi(s, a) − ξD(s, a, ¯zi)||2(cid:3) , zi ∼ qφ(ci), a = Gi(s, ν) (5)
Note that the gradient of Lξ also updates Gi. The ﬁnal distillation loss is given in Eq. 6. We parameterize qφ, QD, GD, ξD with feedforward NN as detailed in Appendix C.1.
Ldistill = LQ + LG + Lξ. (6) 3.2 Robust task inference with triplet loss design
Given the high performance of distillation in Multi-task RL [14–18], it surprisingly performs poorly in Multi-task Batch RL, even on the training tasks. This is even more surprising because we can minimize the distillation losses (Fig. 2 top) and the single-task BCQ policies have high performance (Fig. 2 bottom). If the single-task policies perform well and we can distill them into a multi-task policy, why does the multi-task policy have poor performance? We argue the task inference module has learnt to model the posterior over task identity as conditionally dependent on only the state-action pairs in the context set , i.e. P (Z|S, A), where S, A are random variables denoting states and actions, rather than the correct dependency P (Z|S, A, R) where R denotes the rewards.
The behavior of the trained multi-task policy supports this argument.
In this experiment, each task corresponds to a running direction. To maximize returns, the policy should run with maximal velocity in the target direction. We found that the multi-task policy often runs in the wrong target direction, indicating incorrect task inference. At the beginning of evaluation, the task identity is not provided. The policy takes random actions, after which it uses the collected transitions to infer the task identity. Having learnt the wrong conditional dependency, the task inference module assigns high probability mass in the posterior to region in the task embedding space whose training batches overlap with the collected transitions (Fig. 1).
Figure 2: Top: Value func-tion distillation loss (Eq. 3) during training. Bottom:
The performance of the multi-task policy trained with Eq. 6 versus BCQ.
The fundamental reason behind the wrong dependency is the non-overlapping nature of the training batches. Minimizing the distillation loss does not require the policy to learn the correct but more 4
Algorithm 1 Distillation and triplet loss i=1; BCQ-trained {Qi}K
Input: Batches {Bi}K i=1,
{Gi}K i=1, and {ξi}K i=1; randomly initialized QD,
GD and ξD jointly parameterized by θ; task infer-ence module qφ with randomly initialized φ 1: repeat 2: 3:
Sample context set ci from Bi, ∀i
Obtain relabelled transitions cj→i accord-ing to Eq. 7 for all pair of task i, j
Calculate Ltriplet using Eq. 9
Calculate LQ, LG, Lξ using Eq. 3, 4, 5
Calculate L using Eq. 10
Update θ, φ to minimize L 4: 5: 6: 7: 8: until Done
Figure 3: Action selection. Given s, GD gen-erates candidate actions am. ξD generates small corrections for the actions am. The policy takes the corrected action ˜am with the highest value as estimated by QD. complex dependency. The multi-task policy should imitate different single-task policy depending on which batch the context set was sampled from. If the batches do not overlap in state-action visitation frequencies, the multi-task policy can simply correlate the state-action pairs in the context with which single-task policy it should imitate. In short, if minimizing the training objective on the given datasets does not require the policy to model the dependency of the task identity on the rewards in the context set, there is no guarantee the policy will model this dependency. This is not surprising given literature on the non-identiﬁability of causality from observations [20, 21]. They also emphasize the beneﬁt of using distribution change as training signal to learn the correct causal relationship [22].
Inspired by this literature, we introduce a distribution change into our dataset by approximating the reward function of each task i with a learned function ˆRi (training illustrated in Appendix D). Given a context set cj from task j, we relabel the reward of each transition in cj using ˆRi. Let t index the transitions and cj→i denote the set of the relabelled transitions, we illustrate this process below 3 : cj = (cid:8)(cid:0)sj,t, aj,t, rj,t, s(cid:48) j,t (cid:1)(cid:9) t
Relabelling
−−−−−−→ cj→i = (cid:110)(cid:16) sj,t, aj,t, ˆRi(sj,t, aj,t), s(cid:48) j,t (cid:17)(cid:111) t (7)
Given the relabelled transitions, we leverage the triplet loss from the metric learning community [23] to enforce robust task inference, which is the most important design choice in MBML. Let K be the number of training tasks, ci be a context set for task i, cj be a context set for task j (j (cid:54)= i) , and cj→i be the relabelled set as described above, the triplet loss for task i is:
Li triplet = 1
K − 1
K (cid:88) (cid:20) j=1,j(cid:54)=i (cid:0)cj→i (cid:1), qφ (ci)(cid:1) d(cid:0)qφ (cid:124) (cid:123)(cid:122)
Ensure cj→i and ci infer similar task identities (cid:125)
− d(cid:0)qφ (cj→i) , qφ (cj) (cid:1) (cid:124)
Ensure cj→i and cj infer different task identities (cid:123)(cid:122) (cid:21)
+ m
, (8) (cid:125)
+ where m is the triplet margin, [·]+ is the ReLU function and d is a divergence measure. qφ outputs the posterior over task identity, we thus choose d to be the KL divergence.
Minimizing Eq. 8 accomplishes two goals. It encourages the task inference module qφ to infer similar task identities when given either ci or cj→i as input. It also encourages qφ to infer different task identities for cj and cj→i. We emphasize that the task inference module can not learn to correlate only the state-action pairs with the task identity since cj and cj→i contain the same state-action pairs, but they correspond to different task identities. To minimize Eq. 8, the module must model the correct conditional dependency P (Z|S, A, R) when inferring the task identity.
Eq. 8 calculates the triplet loss when we use the learned reward function of task i to relabel transitions from the remaining tasks. Following similar procedures for the remaining tasks lead to the loss:
Ltriplet = 1
K
K (cid:88) i=1
Li triplet. (9) 3The idea of modeling the reward function and using that model to relabel the rewards from the batches originally came from Professor Keith Ross. 5
The ﬁnal loss to train the randomly initialized task inference module qφ, the distilled value functions
QD, the distilled candidate action generator GD, and the distilled perturbation generator ξD is:
L = Ltriplet + LQ + LG + Lξ. (10)
Alg. 1 illustrates the pseudo-code for the second phase of the distillation procedure. Detailed pseudo-code of the two-phases distillation procedures can be found in Appendix E. Fig. 3 brieﬂy describes action selection from the multi-task policy. Appendix F provides detailed explanations. In theory, we can also use the relabelled transitions in Eq. 7 to train the single-task BCQ policy in the ﬁrst phase, which we do not since we focus on task inference in this work. 4 Discussions
The issue of learning the wrong dependency does not surface when multi-task policies are tested in Atari tasks because their state space do not overlap [18, 24, 25]. Each Atari task has distinctive image-based state. The policy can perform well even when it only learns to correlate the state to the task identity. When Mujoco tasks are used to test online multi-task algorithms [13, 26], the wrong dependency becomes self-correcting. If the policy infers the wrong task identity, it will collect training data which increases the overlap between the datasets of the different training tasks, correcting the issue overtime. However, in the batch setting, the policy can not collect more transitions to self-correct inaccurate task inference. Our insight also leads to exciting possibility to incorporate mechanism to quickly infer the correct causal relationship and improve sample efﬁciency in Multi-task RL, similar to how causal inference method has motivated new innovations in imitation learning [27].
Our ﬁrst limitation is the reliance on the generalizability of simple feedforward NN. Future research can explore more sophisticated architecture, such as Graph NN with reasoning inductive bias [28–31] or structural causal model [32, 33], to ensure accurate task inference. We also assume the learnt reward function of one task can generalize to state-action pairs from the other tasks, even when their state-action visitation frequencies do not overlap signiﬁcantly. To increase the prediction accuracy, we use a reward ensemble to estimate epistemic uncertainty (Appendix D). We note that the learnt reward functions do not need to generalize to every state-action pairs, but only enough pairs so that the task inference module is forced to consider the rewards when trained to minimize Eq. 8. Crucially, we do not need to solve the task inference challenge while learning the reward functions and using them for relabelling, allowing us to side-step the challenge of task inference.
The second limitation is in scope. We only demonstrate our results on tasks using proprioceptive states. Even though they represent high-dimensional variables in a highly nonlinear ODE, the model does not need to tackle visual complexity. The tasks we consider also have relatively dense reward functions and not binary reward functions. These tasks, such as navigation and running, are also quite simple in the spectrum of possible tasks we want an embodied agents to perform. These limitations represent exciting directions for future work.
Another interesting future direction is to apply supervised learning self-distillation techniques [34, 35], proven to improve generalization, to further improve the distillation procedure. To address the multi-task learning problem for long-horizon tasks, it would also be beneﬁcial to consider skill discovery and composition from the batch data [36, 37]. However, in this setting, we still need effective methods to infer the correct task identity to perform well in unseen tasks. Our explanation in Sec. 3 only applies when the tasks differ in reward function. Extending our approach to task distributions with varying transition functions is trivial. Sec. 5 provide experimental results for both cases. 5 Experiment Results
We demonstrate the performance of our proposed algorithm (Sec. 5.1) and ablate the different design choices (Sec. 5.2). Sec. 5.3 shows that the multi-task policy can serve as a good initialization, signiﬁcantly speeding up training on unseen tasks. Appendix C provides all hyper-parameters. 5.1 Performance evaluation on unseen tasks
We evaluate in ﬁve challenging task distributions from MuJoCo [38] and a modiﬁed task distribution
UmazeGoal-M from D4RL [39]. In AntDir and HumanoidDir-M, a target direction deﬁnes a task. 6
Figure 4: Results on unseen test tasks. x-axis is training epochs. y-axis is average episode returns. The shaded areas denote one std.
The agent maximizes returns by running with maximal speed in the target direction. In AntGoal and UmazeGoal-M, a task is deﬁned by a goal location, to which the agent should navigate. In
HalfCheetahVel, a task is deﬁned as a constant velocity the agent should achieve. We also consider the WalkerParam environment where random physical parameters parameterize the agent, inducing different transition functions in each task. The state for each task distribution is the OpenAI gym state. We do not include the task-speciﬁc information, such as the goal location or the target velocity in the state. The target directions and goals are sampled from a 120◦ circular arc. Details of these task distributions can be found in Appendix H.1.
We argue that the version of HumanoidDir used in prior works does not represent a meaningful task distribution, where a single task policy can already achieve the optimal performance on unseen tasks.
We thus modify the task distribution so that a policy has to infer the task identity to perform well, and denote it as HumanoidDir-M. More details of this task distribution can be found in Appendix G.
There are two natural baselines. The ﬁrst is by modifying PEARL [11] to train from the batch, instead of allowing PEARL to collect more transitions. We thus do not execute line 1 − 10 in Algorithm 1 in the PEARL paper. On line 13, we sample the context and the RL batch uniformly from the batch.
The second baseline is Contextual BCQ. We modify the networks in BCQ to accept the inferred task identity as input. We train the task inference module using the gradient of the value function loss.
MBML and the baselines have the same network architecture. We are very much inspired by PEARL and BCQ. However, we do not expect PEARL to perform well in our setting because it does not explicitly handle the difﬁculties of learning from a batch without interactions. We also expect that our proposed algorithm will outperform Contextual BCQ thanks to more robust task inference.
We measure performance by the average returns over unseen tasks, sampled from the same task distribution. We do not count the ﬁrst two episodes’ returns [11]. We obtain the batch for each training task by training Soft Actor Critic (SAC) [40] with a ﬁxed number of environment interactions.
Appendix H provide more details on the environment setups and training procedures of the baselines.
From Fig. 4, MBML outperforms the baselines by a healthy margin in all task distributions. Even though PEARL does not explicitly handle the challenge of training from an ofﬂine batch, it is remarkably stable, only diverging in AntDir. Contextual BCQ is stable, but converges to a lower performance than MBML in all task distributions. An astude reader will notice the issue of overﬁtting, for example Contextual BCQ in HumanoidDir-M. Since our paper is not about determining early stopping conditions and to ensure fair comparisons among the different algorithms, we compute the performance comparisons using the best results achieved by each algorithm during training.
Figure 5: MetaGenRL quickly diverges and does not recover.
We also compare with MetaGenRL [41]. Since it relies on DDPG [42] to estimate value functions, which diverges in Batch RL [9], we do not expect it to perform well in our setting. Fig. 5 conﬁrms this, where its performance quickly plummets and does not recover with more training. Combining
MetaGenRL and MBML is interesting since MetaGenRL generalizes to out-of-distribution tasks. 7
Figure 6: Ablation study. x-axis is training epochs. y-axis is average episode re-turns. The shaded areas de-note one std. 5.2 Ablations
We emphasize that our contributions lie in the triplet loss design coupled with transitions relabelling.
Below, we provide ablation studies to demonstrate that both are crucial to obtain superior performance.
No relabelling. To obtain hard negative examples, we search over a mini-batch to ﬁnd the hardest positive-anchor and negative-anchor pairs, a successful and strong baseline from metric learning [23].
This requires sampling N context sets {cn n=1 for each task i, where n indexes the context sets sampled for each task. Let K be the number of training tasks, the triplet loss is: i }N 1
K
K (cid:88) (cid:20) i=1 max n,n(cid:48)=1,...,N (cid:16) d qφ(cn i (cid:1), qφ(cn(cid:48) i ) (cid:17)
− min n,n(cid:48)=1,...,N j=1,...,K,j(cid:54)=i (cid:16) d qφ(cn i (cid:1), qφ(cn(cid:48) j ) (cid:17) (cid:21)
+ m
. (11)
+
The max term ﬁnds the positive-anchor pair for task i by considering every pair of context sets from task i and selecting the pair with the largest divergence in the posterior over task identities. The min term ﬁnds the negative-anchor pair for task i by considering every possible pair between the context sets sampled for task i and the context sets sampled for the other tasks. It then selects the pair with the lowest divergence in the posterior over task identities as the negative-anchor pair.
No triplet loss. We train the task inference module using only gradient of the value function distillation loss (Eq. 3). To use the relabelled transitions, the module also takes as input the relabelled transitions during training. More concretely, given the context set ci from task i, we sample an equal number of relabelled transitions from the other tasks ˜ci ∼ ∪jcj→i. During training, the input to the task inference module is the union of the context set ci and the sampled relabelled transitions ˜ci. In the full model, we also perform similar modiﬁcation to the input of the module during training.
No transition relabelling and no triplet loss. This method is a simple combination of a task inference module and the distillation process. We refer to this algorithm as Neither in the graphs.
Fig. 6 compares our full model and the ablated versions. Our full model obtains higher returns than most of the ablated versions. For WalkerParam, our full model does not exhibit improvement over
Neither. However, from Fig. 4, our full model signiﬁcantly outperforms the baselines. We thus conclude that, in WalkerParam, the improvement over the baselines comes from distillation.
Comparing to the No triplet loss ablation, transition relabelling leads to more efﬁcient computation of the triplet loss. Without the relabelled transitions, computing Eq. 11 requires O(K 2N 2). Our loss in Eq. 9 only requires O(K 2). We also need to relabel the transitions only once before training the multi-task policy. It is also trivial to parallelize across tasks.
We also study reward estimation accuracy. Fig. 7 shows that our reward model achieves low error on state-action pairs from another task, both with and without an ensemble. We also compare MBML against an ablated version that uses the ground truth reward function for relabelling on
UmazeGoal-M. The model trained using the ground truth reward function only performs slightly better than the model trained using the learned reward function. We include in
Appendix I experiments on margin sensitivity analysis and the beneﬁt of the reward ensemble.
Figure 7: Error on un-seen task. 8
— : SAC initialized by our multi-task policy (Ours)
— : Randomly initialized SAC (Random)
Figure 8: Initialization results. x-axis is number of interactions in thousands. y-axis is the average episode returns over unseen tasks. The shaded areas denote one std. 5.3 Using the multi-task policy to enable faster convergence when training on unseen tasks
While the multi-task policy generalize to unseen tasks, its performance is not optimal. If we allow further training, initializing networks with our multi-task policy signiﬁcantly speeds up convergence to the optimal performance.
The initialization process is as followed. Given a new task, we use the multi-task policy to collect 10K transitions. We then train a new policy to imitate the actions taken by maximizing their log likelihood. As commonly done, the new policy outputs the mean and variance of a diagonal Gaussian distribution. The new policy does not take a task identity as input. The task inference module infers a task identity z from the 10K transitions. Fixing z as input, the distilled value function QD initializes the new value function. Given the new policy and the initialized value function, we train them with SAC by collecting more data. To stabilize training, we perform target policy smoothing
[43] and double-Q learning [44] by training two identically initialized value functions with different mini-batches (pseudo-codes and more motivations in Appendix J.1).
Fig. 8 compares the performance of the policies initialized with our multi-task policy to randomly initialized policies. Initializing the policies with the MBML policy signiﬁcantly increases convergence speed in all ﬁve task distributions, demonstrating our method’s robustness. Even in the complex
HumanoidDir-M task distribution, our method signiﬁcantly speeds up the convergence, requiring only 85K environment interactions, while the randomly initialized policies require 350K, representing a 76% improvement in sample efﬁciency. Similar conclusions hold when comparing against randomly initialized SAC where the two value functions are trained using different mini-batches (Appendix
J.2). We also note that our initialization method does not require extensive hyper-parameter tuning. 6