Abstract
Obtaining reliable and accurate quantiﬁcation of uncertainty estimates from deep neural networks is important in safety-critical applications. A well-calibrated model should be accurate when it is certain about its prediction and indicate high uncertainty when it is likely to be inaccurate. Uncertainty calibration is a chal-lenging problem as there is no ground truth available for uncertainty estimates.
We propose an optimization method that leverages the relationship between ac-curacy and uncertainty as an anchor for uncertainty calibration. We introduce a differentiable accuracy versus uncertainty calibration (AvUC) loss function that allows a model to learn to provide well-calibrated uncertainties, in addition to improved accuracy. We also demonstrate the same methodology can be extended to post-hoc uncertainty calibration on pretrained models. We illustrate our approach with mean-ﬁeld stochastic variational inference and compare with state-of-the-art methods. Extensive experiments demonstrate our approach yields better model calibration than existing methods on large-scale image classiﬁcation tasks under distributional shift. 1

Introduction
Probabilistic deep neural networks (DNNs) enable quantiﬁcation of principled uncertainty estimates, which are essential to understand the model predictions for reliable decision making in safety critical applications [1]. In addition to obtaining accurate predictions from the model, it is important for the model to indicate when it is likely to make incorrect predictions. Various probabilistic methods have been proposed to capture uncertainty estimates from DNNs including Bayesian [2–8] and non-Bayesian [9, 10] formulations. In spite of recent advances in probabilistic deep learning to improve model robustness, obtaining accurate quantiﬁcation of uncertainty estimates from DNNs is still an open research problem. A well-calibrated model should be conﬁdent about its predictions when it is accurate and indicate high uncertainty when making inaccurate predictions. Modern neural networks are poorly calibrated [11, 12] as they tend to be overconﬁdent on incorrect predictions.
Negative log-likelihood (NLL) loss is conventionally used for training the neural networks in multi-class classiﬁcation tasks. Miscalibration in DNNs has been linked to overﬁtting of NLL [11, 13].
Probabilistic DNNs fail to provide calibrated uncertainty in between separated regions of observations due to model misspeciﬁcation and the use of approximate inference [14–16]. Overcoming the problem of poor calibration in modern neural networks is an active area of research [11–21].
In real-world settings, the observed data distribution may shift from training distribution (dataset shift [22]) and there are possibilities of observing novel inputs that are far-off from training data manifold (out-of-distribution). DNN model predictions have been shown to be unreliable under such distributional shift [20, 23, 24]. Obtaining reliable uncertainties even under distributional shift is important to build robust AI systems for successful deployment in the real-world [25, 26].
Uncertainty calibration will also help in detecting distributional shift to caution AI practitioners, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
as well-calibrated uncertainty estimates can guide when to trust and when not to trust the model predictions. But uncertainty calibration is a challenging problem due to the unavailability of ground truth uncertainty estimates.
Contribution In this paper, we introduce the accuracy versus uncertainty calibration (AvUC) loss function for probabilistic deep neural networks to derive models that will be conﬁdent on accurate predictions and indicate higher uncertainty when likely to be inaccurate. We rely on theoretically sound loss-calibrated approximate inference framework [27, 28] with AvUC loss as utilty-dependent penalty term for the task of obtaining well-calibrated uncertainties along with improved accuracy.
We ﬁnd that accounting for predictive uncertainty while training the neural network improves model calibration. To evaluate model calibration under dataset shift, we use various image perturbations and corruptions at different shift intensities [20] and compare with high-performing baselines provided in uncertainty quantiﬁcation(UQ) benchmark [26]. In summary, we make the following contributions in this work:
• Propose an optimization method that leverages the relationship between accuracy and uncertainty as anchor for uncertainty calibration while training deep neural network classiﬁers (Bayesian and non-Bayesian). We introduce a differentiable proxy for Accuracy versus Uncertainty (AvU) measure and the corresponding accuracy versus uncertainty calibration (AvUC) loss function devised to obtain well-calibrated uncertainties, while maintaining or improving model accuracy.
• Investigate accounting for predictive uncertainty estimation in the training objective function and its effect on model calibration under distributional shift (dataset shift and out-of-distribution).
• Propose a post-hoc model calibration method extending the temperature scaling using AvUC loss.
• Empirically evaluate the proposed methods and compare with existing high-performing baselines on large-scale image classiﬁcation tasks using a wide range of metrics. We demonstrate our method yields state-of-the-art model calibration under distributional shift. We also compare the distributional shift detection performance using predictive uncertainty estimates obtained from different methods. 2