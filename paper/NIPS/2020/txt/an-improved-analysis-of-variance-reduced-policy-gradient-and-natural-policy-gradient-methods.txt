Abstract
In this paper, we revisit and improve the convergence of policy gradient (PG), natural PG (NPG) methods, and their variance-reduced variants, under general smooth policy parametrizations. More speciﬁcally, with the Fisher information matrix of the policy being positive deﬁnite: i) we show that a state-of-the-art variance-reduced PG method, which has only been shown to converge to stationary points, converges to the globally optimal value up to some inherent function approximation error due to policy parametrization; ii) we show that NPG enjoys a lower sample complexity; iii) we propose SRVR-NPG, which incorporates variance-reduction into the NPG update. Our improvements follow from an observation that the convergence of (variance-reduced) PG and NPG methods can improve each other: the stationary convergence analysis of PG can be applied to NPG as well, and the global convergence analysis of NPG can help to establish the global convergence of (variance-reduced) PG methods. Our analysis carefully integrates the advantages of these two lines of works. Thanks to this improvement, we have also made variance-reduction for NPG possible, with both global convergence and an efﬁcient ﬁnite-sample complexity. 1

Introduction
Policy gradient (PG) methods, or more generally direct policy search methods, have long been recognized as one of the foundations of reinforcement learning (RL) [1]. Speciﬁcally, PG methods directly search for the optimal policy parameter that maximizes the long-term return in Markov decision processes (MDPs), following the policy gradient ascent direction [2, 3]. This search direction can be more efﬁcient using a preconditioning matrix, e.g., using the natural PG direction
[4]. These methods have achieved tremendous empirical successes recently, especially boosted by the power of (deep) neural networks for policy parametrization [5, 6, 7, 8]. These successes are primarily attributed to the fact that PG methods naturally incorporate function approximation for policy parametrization, in order to handle massive and even continuous state-action spaces.
In practice, the policy gradients are usually estimated via samples using Monte-Carlo rollouts and bootstrapping [2, 9]. Such stochastic PG methods notoriously suffer from very high variances, which not only destabilize but also slow down the convergence. Several conventional approaches have been advocated to reduce the variance of PG methods, e.g., by adding a baseline [3, 10], or by using function approximation for estimating the value function, namely, developing actor-critic algorithms
[11, 12, 13]. More recently, motivated by the advances of variance-reduction techniques in stochastic optimization [14, 15, 16, 17], there have been surging interests in developing variance-reduced PG methods [18, 19, 20, 21, 22], which are shown to be faster.
In contrast to the empirical successes of PG methods, their theoretical convergence guarantees, especially non-asymptotic global convergence guarantees, have not been addressed satisfactorily 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
until very recently [23, 24, 25, 26, 27]. By non-asymptotic global convergence, here we mean the convergence behavior of PG methods from any initialization, and the quality of the point they converge to (usually enjoys global optimality up to some compatible function approximation error due to policy parametrization), after a ﬁnite number of iterations/samples. These recent prominent guarantees are normally beyond the folklore ﬁrst-order stationary-point convergence1, as expected from a stochastic nonconvex optimization perspective of solving RL with PG methods. Special landscapes of the RL objective, though nonconvex, have enabled the convergence to even globally optimal values. On the other hand, none of the aforementioned variance-reduced PG methods [18, 19, 20, 21, 22] have been shown to enjoy these desired global convergence properties. It remains unclear whether these methods can converge to beyond ﬁrst-order stationary policies.
Motivated by these advances and the questions that remain to be answered, we aim in this paper to improve the convergence of PG and natural PG (NPG) methods, and their variance-reduced variants, under general smooth policy parametrizations. Our contributions are summarized as follows.
Contributions. With a focus on the conventional Monte-Carlo-based PG methods, we propose a general framework for analyzing their global convergence. Our contribution is three-fold: ﬁrst, we establish the global convergence up to compatible function approximation errors due to policy parametrization, for a variance-reduced PG method SRVR-PG [21]; second, we improve the global convergence of NPG methods established in [27], from O (cid:0)ε−4(cid:1) to O (cid:0)ε−3(cid:1); third, we propose a new variance-reduced algorithm based on NPG, and establish its global convergence with an efﬁcient sample-complexity. These improvements are based on a framework that integrates the advantages of previous analyses on (variance reduced) PG and NPG, and rely on a (mild) assumption that the
Fisher information matrix induced by the policy parametrization is positive deﬁnite (see Assumption 2.1). A comparison of previous results and our improvements is laid out in Table 1.