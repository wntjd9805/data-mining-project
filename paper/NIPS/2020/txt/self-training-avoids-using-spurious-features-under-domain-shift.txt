Abstract
In unsupervised domain adaptation, existing theory focuses on situations where the source and target domains are close. In practice, conditional entropy minimization and pseudo-labeling work even when the domain shifts are much larger than those analyzed by existing theory. We identify and analyze one particular setting where the domain shift can be large, but these algorithms provably work: certain spurious features correlate with the label in the source domain but are independent of the label in the target. Our analysis considers linear classiﬁcation where the spurious features are Gaussian and the non-spurious features are a mixture of log-concave distributions. For this setting, we prove that entropy minimization on unlabeled target data will avoid using the spurious feature if initialized with a decently accurate source classiﬁer, even though the objective is non-convex and contains multiple bad local minima using the spurious features. We verify our theory for spurious domain shift tasks on semi-synthetic Celeb-A and MNIST datasets. Our results suggest that practitioners collect and self-train on large, diverse datasets to reduce biases in classiﬁers even if labeling is impractical. 1

Introduction
Reliable machine learning systems need to generalize to test distributions that are different from the training distribution. However, the test performance of machine learning models often signiﬁcantly degrades as the test domain drifts away from the training domain. Various approaches have been proposed to adapt the models to new domains [37, 10, 38] but theoretical understanding of these algorithms is limited. Prior theoretical works focus on settings where the target domain is sufﬁciently close to the source domain [5, 29, 17, 34]. To theoretically study realistic scenarios where domain shifts can be much larger, we need to leverage additional structure of the shifts.
Towards this goal, we propose to study a particular structured domain shift for which unsupervised domain adaptation is provably feasible: in the source domain, a subset of “spurious” features correlate with the label, whereas in the unlabeled target data, these features are independent of the label. In real-world training data, these spurious correlations can occur due to biased sampling or artifacts in crowd-sourcing [14]. For example, we may have a labeled dataset for recidivism prediction where race correlates with recurrence of crime due to sample selection bias, but this correlation does not hold on the population. Models which learn spurious correlations can generalize poorly on population data which does not have these biases [23]. In these settings, it could be impractical to acquire labels for an unbiased sub-sample of the population, but unlabeled data is often available.
We prove that in certain settings, perhaps surprisingly, self-training on unlabeled target data can avoid using these spurious features. Our theoretical results apply to two closely-related popular algorithms:
∗Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
self-training [20] and conditional entropy minimization [13]. In practice, self-training has achieved competitive or state-of-the-art results in unsupervised domain adaptation [22, 44, 30], but there are few theoretical analyses of self-training when there is domain shift.
Our theoretical setting and analysis are consistent with recent large-scale empirical results by Xie et al.
[42], which suggest that self-training on a more diverse unlabeled dataset can improve the robustness of a model, potentially by avoiding using spurious correlations. These results and our theory help emphasize the value of a large and diverse unlabeled dataset, even if labeled data is scarce.
Formally, we assume that each input consists of two subsets of features, denoted by x1 and x2. x1 is the “signal” feature that determines the label y in the target distribution. x2 is the spurious feature that correlates with the label y in the source domain, but x2 is independent of (x1, y) in the target domain. For a ﬁrst-cut result, we consider binary classiﬁcation and linear models on the features (x1, x2), where the spurious feature x2 is a multivariate Gaussian and x1 is a mixture of log-concave distributions. We aim to show that, initialized with some classiﬁer trained on the source data, self-training on the unlabeled target will remove usage of the spurious feature x2.
A challenge in the analysis is that self-training on an unlabeled loss can possibly harm, rather than help, target accuracy by amplifying the mistakes of source classiﬁer (see Section 3.1). The classical idea of co-training [8] deals with this by assuming the mistakes of the classiﬁer are independent of x, reducing the problem to learning from noisy labels. However, in our setting the source classiﬁer makes biased mistakes which depend on x, and self-training potentially reinforces these biases if there are no additional assumptions. For example, we require initialization with a decently accurate source classiﬁer, and we empirically verify the necessity of this assumption in Section 5.
Our main contribution (Theorem 3.1) is to prove that self-training and conditional min entropy using
ﬁnite unlabeled data converge to a solution that has 0 coefﬁcients on the spurious feature x2, assuming the following: 1. the signal x1 is a mixture of well-separated log-concave distributions and 2. the initial source classiﬁer is decently accurate on target data and avoids relying too heavily on the spurious feature. In a simpler setting where x1 is a univariate Gaussian, we show that self-training using a decently accurate source classiﬁer converges to the Bayes optimal solution (Theorem 3.2).
We run simulations on semi-synthetic colored MNIST [19] and celebA [21] datasets to verify the insights from our theory and show that they apply to multi-layer neural networks and datasets where the spurious features are not necessarily a subset of the input coordinates (Section 5). 1.1