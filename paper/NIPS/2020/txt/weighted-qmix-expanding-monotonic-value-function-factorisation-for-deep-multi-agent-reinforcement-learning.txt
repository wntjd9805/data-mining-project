Abstract
QMIX is a popular Q-learning algorithm for cooperative MARL in the centralised training and decentralised execution paradigm. In order to enable easy decentrali-sation, QMIX restricts the joint action Q-values it can represent to be a monotonic mixing of each agent’s utilities. However, this restriction prevents it from repre-senting value functions in which an agent’s ordering over its actions can depend on other agents’ actions. To analyse this representational limitation, we ﬁrst formalise the objective QMIX optimises, which allows us to view QMIX as an operator that
ﬁrst computes the Q-learning targets and then projects them into the space repre-sentable by QMIX. This projection returns a representable Q-value that minimises the unweighted squared error across all joint actions. We show in particular that this projection can fail to recover the optimal policy even with access to Q∗, which primarily stems from the equal weighting placed on each joint action. We rectify this by introducing a weighting into the projection, in order to place more impor-tance on the better joint actions. We propose two weighting schemes and prove that they recover the correct maximal action for any joint action Q-values, and therefore for Q∗ as well. Based on our analysis and results in the tabular setting, we introduce two scalable versions of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX and demonstrate improved performance on both predator-prey and challenging multi-agent StarCraft benchmark tasks [26]. 1

Introduction
Many critical tasks involve multiple agents acting in the same environment. To learn good behaviours in such problems from agents’ experiences, we may turn to multi-agent reinforcement learning (MARL). Fully decentralised policies are often used in MARL, due to practical communication constraints or as a way to deal with an intractably large joint action space. However, when training in simulation or under controlled conditions we may have access to additional information, and agents can freely share their observations and internal states. Exploiting these possibilities can greatly improve the efﬁciency of learning [7, 9].
In this paradigm of centralised training for decentralised execution, QMIX [25] is a popular Q-learning algorithm with state-of-the-art performance on the StarCraft Multi-Agent Challenge [26].
QMIX represents the optimal joint action value function using a monotonic mixing function of per-agent utilities. This restricted function class Qmix allows for efﬁcient maximisation during training, and easy decentralisation of the learned policy. However, QMIX is unable to represent joint action value functions that are characterised as nonmonotonic [16], i.e., an agent’s ordering over its
∗Now at Google DeepMind. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
own actions depends on other agents’ actions. Consequently, QMIX cannot solve tasks that require signiﬁcant coordination within a given timestep [27, 2]. In this work, we analyse an idealised, tabular version of QMIX to study this representational limitation, and then develop algorithms to resolve these limitations in theory and in practice.
We formalise the objective that QMIX optimises, which allows us to view QMIX as an operator that ﬁrst computes the Q-learning targets and then projects them into Qmix by minimising the unweighted squared error across all joint actions. We show that, since in general Q∗ /∈ Qmix, the projection of Q∗, which we refer to as Qtot, can have incorrect estimates for the optimal joint action, yielding suboptimal policies. These are fundamental limitations of the QMIX algorithm independent from exploration and compute constraints, and occur even with access to the true Q∗.
These limitations primarily arise because QMIX’s projection of Q∗ yields a Qtot that places equal importance on approximating the Q-values for all joint actions. Our key insight is that if we ultimately care only about the greedy optimal policy, it is more important to accurately represent the value of the optimal joint action than the suboptimal ones. Therefore, we can improve the policy recovered from Qtot by appropriately weighting each joint action when projecting Q∗ into Qmix.
Based on this intuition, we introduce a weighting function into our projection. In the idealised tabular setting we propose two weighting functions and prove that the projected Qtot recovers the correct maximal action for any Q, and therefore for Q∗ as well. Since this projection always recovers the correct maximal joint action, we beneﬁt from access to Q∗ (or a learned approximation of it). To this end, we introduce a learned approximation of Q∗, from an unrestricted function class, which we call ˆQ∗. By using Qtot, now a weighted projection of ˆQ∗, to perform maximisation, we show that
ˆQ∗ converges to Q∗ and that Qtot thus recovers the optimal policy.
Based on our analysis and results in the tabular setting, we present two scalable versions of our algo-rithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX. We demonstrate their improved ability to cope with environments with nonmonotonic value functions, by showing superior performance in a predator-prey task in which the trade-offs made by QMIX prevent it from solving the task. Additionally, we demonstrate improved robustness over QMIX to the amount of exploration performed, by showing better empirical performance on a range of SMAC maps. Our ablations and additional analysis experiments demonstrate the importance of both a weighting and an unrestricted ˆQ∗ in our algorithm. 2