Abstract
We present a new paradigm for Neural ODE algorithms, called ODEtoODE, where time-dependent parameters of the main ﬂow evolve according to a matrix
ﬂow on the orthogonal group O(d). This nested system of two ﬂows, where the parameter-ﬂow is constrained to lie on the compact manifold, provides stability and effectiveness of training and provably solves the gradient vanishing-explosion problem which is intrinsically related to training deep neural network architectures such as Neural ODEs. Consequently, it leads to better downstream models, as we show on the example of training reinforcement learning policies with evolution strategies, and in the supervised learning setting, by comparing with previous SOTA baselines. We provide strong convergence results for our proposed mechanism that are independent of the depth of the network, supporting our empirical studies. Our results show an intriguing connection between the theory of deep neural networks and the ﬁeld of matrix ﬂows on compact manifolds. 1

Introduction
Neural ODEs [13, 10, 27] are natural continuous extensions of deep neural network architectures, with the evolution of the intermediate activations governed by an ODE: dxt dt
= f (xt, t, θ), (1) parameterized by θ ∈ Rn and where f : Rd × R × Rn → Rd is some nonlinear mapping deﬁning dynamics. A solution to the above system with initial condition x0 is of the form: xt = x0 + (cid:90) t t0 f (xs, s, θ)ds, and can be approximated with various numerical integration techniques such as Runge-Kutta or Euler methods [48]. The latter give rise to discretizations:
∗equal contribution xt+dt = xt + f (xt, t, θ)dt, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
that can be interpreted as discrete ﬂows of ResNet-like computations [29] and establish a strong connection between the theory of deep neural networks and differential equations. This led to successful applications of Neural ODEs in machine learning. Those include in particular efﬁcient time-continuous normalizing ﬂows algorithms [11] avoiding the computation of the determinant of the Jacobian (a computational bottleneck for discrete variants), as well as modeling latent dynamics in time-series analysis, particularly useful for handling irregularly sampled data [44]. Parameterized neural ODEs can be efﬁciently trained via adjoint sensitivity method [13] and are characterized by constant memory cost, independent of the depth of the system, with different parameterizations encoding different weight sharing patterns across inﬁnitesimally close layers.
Such Neural ODE constructions enable deeper models than would not otherwise be possible with a
ﬁxed computation budget; however, it has been noted that training instabilities and the problem of vanishing/exploding gradients can arise during the learning of very deep systems [43, 4, 23].
To resolve these challenges for discrete recurrent neural network architectures, several improvements relying on transition transformations encoded by orthogonal/Hermitian matrices were proposed
[2, 33]. Orthogonal matrices, while coupled with certain classes of nonlinear mappings, provably preserve norms of the loss gradients during backpropagation through layers, yet they also incur potentially substantial Riemannian optimization costs [21, 14, 28, 1]. Fortunately, there exist several efﬁcient parameterizations of the subgroups of the orthogonal group O(d) that, even though in principle reduce representational capacity, in practice produce high-quality models and bypass
Riemannian optimization [36, 40, 34]. All these advances address discrete settings and thus it is natural to ask what can be done for continuous systems, which by deﬁnition are deep.
In this paper, we answer this question by presenting a new paradigm for Neural ODE algorithms, called
ODEtoODE, where time-dependent parameters of the main ﬂow evolve according to a matrix ﬂow on the orthogonal group O(d). Such ﬂows can be thought of as analogous to sequences of orthogonal matrices in discrete structured orthogonal models. By linking the theory of training Neural ODEs with the rich mathematical ﬁeld of matrix ﬂows on compact manifolds, we can reformulate the problem of
ﬁnding efﬁcient Neural ODE algorithms as a task of constructing expressive parameterized ﬂows on the orthogonal group. We show in this paper on the example of orthogonal ﬂows corresponding to the so-called isospectral ﬂows [8, 9, 16], that such systems studied by mathematicians for centuries indeed help in training Neural ODE architectures (see: ISO-ODEtoODEs in Sec. 3.1). There is a voluminous mathematical literature on using isospectral ﬂows as continuous systems that solve a variety of combinatorial problems including sorting, matching and more [8, 9], but to the best of our knowledge, we are the ﬁrst who propose to learn them as stabilizers for training deep Neural ODEs.
Our proposed nested systems of ﬂows, where the parameter-ﬂow is constrained to lie on the compact manifold, provide stability and effectiveness of training, and provably solve the gradient vanish-ing/exploding problem for continuous systems. Consequently, they lead to better downstream models, as we show on a broad set of experiments (training reinforcement learning policies with evolution strategies and supervised learning). We support our empirical studies with strong convergence results, independent of the depth of the network. We are not the ﬁrst to explore nested Neural ODE struc-ture (see: [55]). Our novelty is in showing that such hierarchical architectures can be signiﬁcantly improved by an entanglement with ﬂows on compact manifolds.
To summarize, in this work we make the following contributions:
• We introduce new, explicit constructions of non-autonomous nested Neural ODEs (ODEtoODEs) where parameters are deﬁned as rich functions of time evolving on compact matrix manifolds (Sec. 3). We present two main architectures: gated-ODEtoODEs and ISO-ODEtoODEs (Sec. 3.1).
• We establish convergence results for ODEtoODEs on a broad class of Lipschitz-continuous objective functions, in particular in the challenging reinforcement learning setting (Sec. 4.2).
• We then use the above framework to outperform previous Neural ODE variants and baseline architectures on RL tasks from OpenAI Gym and the DeepMind Control Suite, and simultaneously to yield strong results on image classiﬁcation tasks. To the best of our knowledge, we are the ﬁrst to show that well-designed Neural ODE models with a compact number of parameters make them good candidates for training reinforcement learning policies via evolutionary strategies (ES) [15].
All proofs are given in the Appendix. We conclude in Sec. 6 and discuss broader impact in Sec. 7. 2
2