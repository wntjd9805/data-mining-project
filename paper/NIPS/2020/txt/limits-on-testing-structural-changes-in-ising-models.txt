Abstract
We present novel information-theoretic limits on detecting sparse changes in Ising models, a problem that arises in many applications where network changes can occur due to some external stimuli. We show that the sample complexity for detecting sparse changes, in a minimax sense, is no better than learning the entire model even in settings with local sparsity. This is a surprising fact in light of prior work rooted in sparse recovery methods, which suggest that sample complexity in this context scales only with the number of network changes. To shed light on when change detection is easier than structured learning, we consider testing of edge deletion in forest-structured graphs, and high-temperature ferromagnets as case studies. We show for these that testing of small changes is similarly hard, but testing of large changes is well-separated from structure learning. These results imply that testing of graphical models may not be amenable to concepts such as restricted strong convexity leveraged for sparsity pattern recovery, and algorithm development instead should be directed towards detection of large changes. 1

Introduction
Recent technological advances have lead to the emergence of high-dimensional datasets in a wide range of scientiﬁc disciplines [YY17; Cos+10; PF95; Bre15; Lok+18; WSD19; Ban18], where the observations are modeled as arising from a probabilistic graphical model (GM), and the goal is to recover the network [Orl+15]. While full network recovery is sometimes useful, and there has been a
ﬂurry of activity [DM17; SW12] in this context, we are often interested in changes in network struc-ture in response to external stimuli, such as changes in protein-protein interactions across different disease states [IK12] or changes in neuronal connectivity as a subject learns a task [Moh+16].
A baseline approach is to estimate the network at each stage, and then compare the differences.
However, such observations exhibit signiﬁcant variability, and the amount of data available may be too small for this approach to yield meaningful results. On the other hand, reliably recovering net-work changes should be easier than full reconstruction. While prior works have proposed inference algorithms to explore this possibility [ZCL14; XCC15; FB16; BVB16; BZN18; Zha+19; Cai+19], we do not have a good mathematical understanding of when this is indeed easier.
To shed light on this question, we propose to derive information-theoretic limits for two structural inference problems over degree-bounded Ising models. The ﬁrst is goodness-of-ﬁt testing (GOF).
Let G(P ) be the network structure (see §2) of an Ising model P . GOF is posed as follows.
GOF : Given an Ising model P and i.i.d. samples from another Ising model Q, determine if P = Q or if G(P ) and G(Q) differ in at least s edges.
The second is a related estimation problem, termed error-of-ﬁt (EOF), that demands localising dif-ferences in G(P ) and G(Q) (if distinct).
EOF: Given an Ising model P and i.i.d. samples from another Ising model Q that is either equal to P , or has a network structure that differs from that of P in s edges or more, determine the edges where G(P ) and G(Q) differ. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Notice that the above problems are restricted to models that are either identical, or signiﬁcantly different. ‘Tolerant’ versions (separating small changes from large) are not pursued here. The main question of interest is: For what classes of Ising models is the sample complexity of the above inference problems signiﬁcantly smaller than that of recovering the underlying graph directly?
Contribution. We prove the following surprising fact: up to relatively large values of s, the sam-ple complexities of GOF and EOF are not appreciably separated from that of structure learning (SL). Our bound is surprising in light of the fact that prior works [Liu+14; Liu+17; FB16; KLK19;
Cai+19] propose algorithms for GOF and EOF, and claim recovery of sparse changes is possi-ble with sample complexity much smaller than SL. Concretely, for models with p nodes, degrees bounded by d, and non-zero edge weights satisfying α ≤ |θij| ≤ β (see §2), the sample complexity of SL is bounded as O(e2βdα−2 log p). We show that if s (cid:28) p, then the sample complexity of
GOF is at least e2βd−O(log(d))α−2 log p, and that if s (cid:28) p, then the sample complexity of EOF has the same lower bound. We further show that the same effect occurs in the restricted setting of detect-ing edge deletions in forest-structured Ising models, and, to some extent, in detecting edge deletions in high-temperature ferromagnets. In the case of forests, we tightly characterise this behaviour of
GOF, showing that for s (cid:28) p, GOF has sample complexity comparable to SL of forests, while p, it is vanishingly small relative to SL. For high-temperature ferromagnets, we show that for s (cid:29) detecting changes is easier than SL if s (cid:29) pd. These are the ﬁrst structural testing results for edge edits in natural classes of Ising models that show a clear separation from SL in sample complexity. pd, while this does not occur if s (cid:28)
√
√
√
√
√
Technical Novelty. The lower bounds are shown by constructing explicit and ﬂexible obstructions, utilising Le Cam’s method and χ2-based Fano bounds. The combinatorial challenges arising in directly showing obstructions on large graphs are avoided by constructing obstructions with well-controlled χ2-divergence on small graphs, and then lifting these to p nodes via tensorisation in a process that efﬁciently deals with combinatorial terms. The main challenge is obtaining precise control on the χ2-divergence between graphs based on cliques, which is attained by an elementary but careful analysis that exploits the symmetries inherent in Ising models on cliques. The most striking instance of this is the ‘Emmentaler clique’ (Fig. 2), which is constructed by removing Θ(d2) edges from a d-clique in a structured way. Despite this large edit, we show that it is exponentially hard (in low temperatures) to distinguish this clique with large holes from a full clique. 1.1