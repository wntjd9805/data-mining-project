Abstract
Pairwise alignment of DNA sequencing data is a ubiquitous task in bioinformatics and typically represents a heavy computational burden. State-of-the-art approaches to speed up this task use hashing to identify short segments (k-mers) that are shared by pairs of reads, which can then be used to estimate alignment scores. However, when the number of reads is large, accurately estimating alignment scores for all pairs is still very costly. Moreover, in practice, one is only interested in identifying pairs of reads with large alignment scores. In this work, we propose a new approach to pairwise alignment estimation based on two key new ingredients. The ﬁrst ingredient is to cast the problem of pairwise alignment estimation under a general framework of rank-one crowdsourcing models, where the workers’ responses correspond to k-mer hash collisions. These models can be accurately solved via a spectral decomposition of the response matrix. The second ingredient is to utilise a multi-armed bandit algorithm to adaptively reﬁne this spectral estimator only for read pairs that are likely to have large alignments. The resulting algorithm iteratively performs a spectral decomposition of the response matrix for adaptively chosen subsets of the read pairs. 1

Introduction
A key step in many bioinformatics analysis pipelines is the identiﬁcation of regions of similarity between pairs of DNA sequencing reads. This task, known as pairwise sequence alignment, is a heavy computational burden, particularly in the context of third-generation long-read sequencing technologies, which produce noisy reads [45]. This challenge is commonly addressed via a two-step approach: ﬁrst, an alignment estimation procedure is used to identify those pairs that are likely to have a large alignment. Then, computationally intensive alignment algorithms are applied only to the selected pairs. This two-step approach can greatly speed up the alignment task because, in practice, one only cares about the alignment between reads with a large sequence identity or overlap.
Several works have developed ways to efﬁciently estimate pairwise alignments [6, 29, 30, 36]. The proposed algorithms typically rely on hashing to efﬁciently ﬁnd pairs of reads that share many k-mers (length-k contiguous substrings). Particularly relevant to our discussion is the MHAP algorithm of
Berlin et al. [6]. Suppose we want to estimate the overlap size between two strings S0 and S1 and let
Γ(Si) be the set of all k-mers in Si, i = 0, 1. For a hash function h, we can compute a min-hash h(Si) (cid:44) min{h(x) : x ∈ Γ(Si)}, (1)
∗Equal Contributors 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for each read Si. The key observation behind MHAP is that, for a randomly selected hash function h,
P [h(S0) = h(S1)] =
|Γ(S0) ∩ Γ(S1)|
|Γ(S0) ∪ Γ(S1)|
. (2)
In other words, the indicator function 1{h(S0) = h(S1)} provides an unbiased estimator for the k-mer Jaccard similarity between the sets Γ(S0) and Γ(S1), which we denote by JSk(S0, S1). By computing 1{h(S0) = h(S1)} for several different random hash functions, one can thus obtain an arbitrarily accurate estimate of JSk(S0, S1). As discussed in [6], JSk(S0, S1) serves as an estimate for the overlap size and can be used to ﬁlter pairs of reads that are likely to have a signiﬁcant overlap.
Now suppose we ﬁx a reference read S0 and wish to estimate the size of the overlap between S0 and
Si, for i = 1, . . . , n. Assume that all reads are of length L and let pi ∈ [0, 1] be the overlap fraction between Si and S0 (i.e., the maximum p such that a pL-preﬁx of Si matches a pL-sufﬁx of S0 or vice-versa). By taking m random hash functions h1, . . . , hm, we can compute min-hashes hj(Si) for i = 0, 1, . . . , n and j = 1, . . . , m. The MHAP approach corresponds to estimating each pi as
ˆpi = 1 m (cid:88)m j=1 1{hj(S0) = hj(Si)}. (3)
In the context of crowdsourcing and vote aggregation [16, 19, 40], one can think of each hash function hj as a worker/expert/participant, who is providing binary responses Yi,j = 1{hj(S0) = hj(Si)} to the questions “do Si and S0 have a large alignment score?” for i = 1, . . . , n. Based on the binary matrix of observations Y = [Yi,j], we want to estimate the true overlap fractions p1, . . . , pn.
The idea of jointly estimating p1, . . . , pn from the whole matrix Y was recently proposed by Baharav et al. [5]. The authors noticed that in practical datasets the distribution of k-mers can be heavily skewed. This causes some hash functions hj to be “better than others” at estimating alignment scores.
Hence, much like in crowdsourcing models, each worker has a different level of expertise, which determines the quality of their answer to all questions. Motivated by this, Baharav et al. [5] proposed a model where each hash function hj has an associated unreliability parameter qj ∈ [0, 1] and, for i = 1, . . . , n and j = 1, . . . , m, the binary observations are modeled as
Yi,j ∼ Ber(pi) ∨ Ber(qj), where Ber(p) is a Bernoulli distribution with parameter p and ∨ is the OR operator. If a given hj assigns low values to common k-mers, spurious min-hash collisions are more likely to occur, leading to the observation Yi,j = 1 when Si and S0 do not have an overlap (thus being a “bad” hash function).
Similarly, some workers in crowdsourcing applications provide less valuable feedback, but we cannot know a priori how reliable each worker is. (4)
A key observation about the model in (4) is that, in expectation, the observation matrix Y is rank-one after accounting for an offset. More precisely, since EYi,j = pi + qj − piqj = (1 − pi)(qj − 1) + 1,
EY − 11T = (1 − p)(q − 1)T , (5) where p = [p1, . . . , pm]T and q = [q1, . . . , qn]T . Baharav et al. [5] proposed to estimate p by computing a singular value decomposition (SVD) of Y − 11T , and setting ˆp = 1 − u, where u is the leading left singular vector of Y − 11T . The resulting overlap estimates ˆp1, . . . , ˆpn are called the
Spectral Jaccard Similarity scores and were shown to provide a much better estimate of overlap sizes than the estimator given by (3), by accounting for the variable quality of hash functions for the task.
In this paper, motivated by the model of Baharav et al. [5], we consider the more general framework of rank-one models. In this setting, a vector of parameters u = [u1, . . . , un]T (the item qualities) is to be estimated from the binary responses provided by m workers, and the n × m observation matrix
X is assumed to satisfy EX = uvT . In the context of these rank-one models, a natural estimator for u is the leading left singular vector of X. Such a spectral estimator has been shown to have good performance both in the context of pairwise sequence alignment [5] and in voting aggregation applications [19, 26]. However, the spectral decomposition by default allocates worker resources uniformly across all items. In practice, one is often only interested in identifying the “most popular” items, which, in the context of pairwise sequence alignment, corresponds to the reads Si that have the largest overlaps with a reference read S0. Hence, we seek strategies that can harness the performance of spectral methods while using adaptivity to avoid wasting worker resources on unpopular items.
Main contributions: We propose an adaptive spectral estimation algorithm, based on multi-armed bandits, for identifying the k largest entries of the leading left singular vector u of EX. A key 2
technical challenge is that multi-armed bandit algorithms generally rely on our ability to build conﬁdence intervals for each arm, but it is difﬁcult to obtain tight element-wise conﬁdence intervals for the singular vectors of random matrices with low expected rank [1]. For that reason, we propose a variation of the spectral estimator for u, in which one computes the leading right singular vector ﬁrst and uses it to estimate the entries of u via a matched ﬁlter [44]. This allows us to compute entrywise conﬁdence intervals for each ui, which in turns allows us to adapt the Sequential Halving bandit algorithm of Karnin et al. [28] to identify the top-k entries of u. We provide theoretical performance bounds on the total workers’ response budget required to correctly identify the top-k items with a given probability. We empirically validate our algorithm on controlled experiments that simulate the vote aggregation scenario and on real data in the context of pairwise alignment of DNA sequences.
For a PacBio E. coli dataset [38], we show that adaptivity can reduce the budget requirements (which correspond to the number of min-hash comparisons) by around half.