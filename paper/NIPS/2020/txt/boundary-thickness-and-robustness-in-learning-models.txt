Abstract
Robustness of machine learning models to various adversarial and non-adversarial corruptions continues to be of interest. In this paper, we introduce the notion of the boundary thickness of a classiﬁer, and we describe its connection with and usefulness for model robustness. Thick decision boundaries lead to improved performance, while thin decision boundaries lead to overﬁtting (e.g., measured by the robust generalization gap between training and testing) and lower robustness.
We show that a thicker boundary helps improve robustness against adversarial examples (e.g., improving the robust test accuracy of adversarial training) as well as so-called out-of-distribution (OOD) transforms, and we show that many commonly-used regularization and data augmentation procedures can increase boundary thickness. On the theoretical side, we establish that maximizing boundary thickness during training is akin to the so-called mixup training. Using these observations, we show that noise-augmentation on mixup training further increases boundary thickness, thereby combating vulnerability to various forms of adversarial attacks and OOD transforms. We can also show that the performance improvement in several lines of recent work happens in conjunction with a thicker boundary. 1

Introduction
Recent work has re-highlighted the importance of various forms of robustness of machine learning models. For example, it is by now well known that by modifying natural images with barely-visible perturbations, one can get neural networks to misclassify images [1–4]. Researchers have come to call these slightly-but-adversarially perturbed images adversarial examples. As another example, it has become well-known that, even aside from such worst-case adversarial examples, neural networks are also vulnerable to so-called out-of-distribution (OOD) transforms [5], i.e., those which contain common corruptions and perturbations that are frequently encountered in natural images. These topics have received interest because they provide visually-compelling examples that expose an inherent lack of stability/robustness in these already hard-to-interpret models [6–12], but of course similar concerns arise in other less visually-compelling situations.
In this paper, we study neural network robustness through the lens of what we will call boundary thickness, a new and intuitive concept that we introduce. Boundary thickness can be considered a generalization of the standard margin, used in max-margin type learning [13–15]. Intuitively speaking, the boundary thickness of a classiﬁer measures the expected distance to travel along line segments between different classes across a decision boundary. We show that thick decision boundaries have a regularization effect that improves robustness, while thin decision boundaries lead to overﬁtting and reduced robustness. We also illustrate that the performance improvement in several lines of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
recent work happens in conjunction with a thicker boundary, suggesting the utility of this notion more generally.
More speciﬁcally, for adversarial robustness, we show that ﬁve commonly used ways to improve robustness can increase boundary thickness and reduce the robust generalization gap (which is the difference between robust training accuracy and robust test accuracy) during adversarial training.
We also show that trained networks with thick decision boundaries tend to be more robust against
OOD transforms. We focus on mixup training [16], a recently-described regularization technique that involves training on data that have been augmented with pseudo-data points that are convex combinations of the true data points. We show that mixup improves robustness to OOD transforms, while at the same time achieving a thicker decision boundary. In fact, the boundary thickness can be understood as a dual concept to the mixup training objective, in the sense that the former is maximized as a result of minimizing the mixup loss. In contrast to measures like margin, boundary thickness is easy to measure, and (as we observe through counter examples) boundary thickness can differentiate neural networks of different robust generalization gap, while margin cannot.
For those interested primarily in training, our observations also lead to novel training procedures.
Speciﬁcally, we design and study a novel noise-augmented extension of mixup, referred to as noisy mixup, which augments the data through a mixup with random noise, to improve robustness to image imperfections. We show that noisy mixup thickens the boundary, and thus it signiﬁcantly improves robustness, including black/white-box adversarial attacks, as well as OOD transforms.
In more detail, here is a summary of our main contributions.
•
•
•
We introduce the concept of boundary thickness (Section 2), and we illustrate its connection to various existing concepts, including showing that as a special case it reduces to margin.
We demonstrate empirically that a thin decision boundary leads to poor adversarial robust-ness as well as poor OOD robustness (Section 3), and we evaluate the effect of model adjustments that affect boundary thickness. In particular, we show that ﬁve commonly used regularization and data augmentation schemes (`1 regularization, `2 regularization, large learning rate [17], early stopping, and cutout [18]) all increase boundary thickness and reduce overﬁtting of adversarially trained models (measured by the robust accuracy gap between training and testing). We also show that boundary thickness outperforms margin as a metric in measuring the robust generalization gap.
We show that our new insights on boundary thickness make way for the design of new robust training schemes (Section 4). In particular, we designed a noise-augmentation training scheme that we call noisy mixup to increase boundary thickness and improve the robust test accuracy of mixup for both adversarial examples and OOD transforms. We also show that mixup achieves the minimax decision boundary thickness, providing a theoretical justiﬁcation for both mixup and noisy mixup.
Overall, our main conclusion is the following.
Boundary thickness is a reliable and easy-to-measure metric that is associated with model robustness, and training a neural network while ensuring a thick boundary can improve robustness in various ways that have received attention recently.
In order that our results can be reproduced and extended, we have open-sourced our code.1