Abstract
Spatial Description Resolution, as a language-guided localization task, is pro-posed for target location in a panoramic street view, given corresponding language descriptions. Explicitly characterizing an object-level relationship while distill-ing spatial relationships are currently absent but crucial to this task. Mimicking humans, who sequentially traverse spatial relationship words and objects with a
ﬁrst-person view to locate their target, we propose a novel spatial relationship induced (SIRI) network. Speciﬁcally, visual features are ﬁrstly correlated at an implicit object-level in a projected latent space; then they are distilled by each spatial relationship word, resulting in each differently activated feature representing each spatial relationship. Further, we introduce global position priors to ﬁx the absence of positional information, which may result in global positional reasoning ambiguities. Both the linguistic and visual features are concatenated to ﬁnalize the target localization. Experimental results on the Touchdown show that our method is around 24% better than the state-of-the-art method in terms of accuracy, measured by an 80-pixel radius. Our method also generalizes well on our proposed extended dataset collected using the same settings as Touchdown. The code for this project is publicly available at https://github.com/wong-puiyiu/siri-sdr.1 1

Introduction
Visual localization tasks aim to locate target positions according to language descriptions, where many downstream applications have been developed such as visual question answering (1; 22; 23), visual grounding (20; 18; 28; 6) and spatial description resolution (SDR) (3), etc. These language-guided location tasks can be categorized in terms of input formats, e.g. perspective images in visual grounding or panoramic images in the recently introduced SDR.
The Challenge of SDR: Both of visual grounding and spatial description resolution tasks need to explore the correlation between vision and language to locate the target locations. Unlike traditional visual grounding, the recently proposed spatial description resolution of panoramic images, however, presents its own difﬁculties due to the following aspects. (1) As shown in Figure 1, the complicated 1†: Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
entities, such as buildings in an image, present some challenges for the advanced object detection (9).
For example, existing methods may fail to instantiate multiple adjacent buildings. (2) The short language descriptions in visual grounding are more about well-described instances with multiple attributes, while the long descriptions in spatial description resolution describe multiple spatial relationship words, such as ‘your right/left’, ‘on the left/right’ and ‘in the front’, from a distant starting point to the target. It is worth noting that such crucial issues have not been well addressed in previous work. (3) Panoramic images in visual grounding with a ﬁrst-person view cover more complex visual details on a street compared to the perspective images with a third-person view in visual grounding. (a) Visual Grounding (b) Spatial Description Resolution
Figure 1: Examples of the datasets for the recently proposed spatial description resolution and conventional visual grounding. For the visual grounding illustrated on the left, there are simple entities in the image with a third-person view and the language descriptions are also simple and short.
Regarding SDR on the right, real-world environments with a ﬁrst-person view contain comprehensive entities. The corresponding languages have multiple entities and spatial relationships. The yellow star in the panorama on the right illustrates the target location according to the language descriptions.
Our Solution: To efﬁciently tackle SDR, humans start at their own position with a ﬁrst-person perspective and sequentially traverse the objects with spatial relationship words, ﬁnally locating their target. To mimic the human behavior on SDR, we propose a spatial relationship induced (SIRI) network to explicitly tackle the SDR task in a real-world environment. As shown in Figure 2, we
ﬁrstly leverage a graph-based global reasoning network (5) (GloRe) to model the correlations of all the object-object pairs in the extracted visual feature, where the visual feature is projected to a latent space to implicitly represent object instances in an unsupervised manner. Implicitly learning object concepts and their visual correlations free us from explicitly designing an object detector for a street view. Meanwhile it enables each object in the image to accumulate its contextual information, which is extremely important for scene understanding as well as for spatial description resolution.
Next, a local spatial relationship guided distillation module is appended to distill the visual features to different discriminative features, where each corresponds to a spatial relationship word. We argue that distilling visual features with local spatial relationships concentrates on speciﬁc features corresponding to these crucial language hints, consequently facilitating ﬁnal target localization. After averaging all the distilled features, we introduce two global coordinate maps, of which the origin is at the agent’s position, i,e., the bottom center of the image. Such a position prior alleviates the ambiguities of global positional reasoning in an efﬁcient way. All encoded linguistic features, distilled visual features and position priors are fed into LingUnet(3) to ﬁnalize target localization. It is worth noting that our solution tackles the task of SDR in a highly efﬁcient way and performs signiﬁcantly better than other existing methods.
Our contributions: (1) A novel framework is proposed to explicitly tackle the SDR task in terms of object-level visual correlation, local spatial relationship distillation and global spatial positional embedding. (2) Extensive experiments on the Touchdown dataset show that our method outperforms
LingUnet (3) by 24% in terms of accuracy, measured by an 80-pixel radius. (3) We propose an extended dataset collected using the same settings as Touchdown, and our proposed method also generalizes well. 2
2