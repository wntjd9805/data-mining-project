Abstract
Deep learning models are computationally intense, and implementations often have to be highly optimized by experts or hardware vendors to be usable in practice.
The DL compiler, together with Learning-to-Compile has proven to be a powerful technique for optimizing tensor programs. However, a limitation of this approach is that it still suffers from unbearably long overall optimization time.
In this paper, we present a new method, called AdaTune, that signiﬁcantly reduces the optimization time of tensor programs for high-performance deep learning inference.
In particular, we propose an adaptive evaluation method that statistically early terminates a costly hardware measurement without losing much accuracy. We further devise a surrogate model with uncertainty quantiﬁcation that allows the optimization to adapt to hardware and model heterogeneity better. Finally, we introduce a contextual optimizer that provides adaptive control of the exploration and exploitation to improve the transformation space searching effectiveness. We evaluate and compare the levels of optimization obtained by AutoTVM, a state-of-the-art Learning-to-Compile technique on top of TVM, and AdaTune. The experiment results show that AdaTune obtains up to 115% higher GFLOPS than the baseline under the same optimization time budget. Furthermore, AdaTune provides 1.3–3.9× speedup in optimization time over the baseline to reach the same optimization quality for a range of models across different hardware architectures. 1

Introduction
The enormous computational intensity of Deep Neural Network (DNN) models has attracted great interest in optimizing their performance. In particular, popular deep learning (DL) frameworks such as TensorFlow [6] and PyTorch [32] adopt custom optimized kernels such as Nvidia cuDNN [15] or
Intel MKL-DNN [2] as back-end. However, given the increasing complexity of tensor operations in
DNNs and the volatility of DL algorithms, it calls for developing fast and automated compilation frameworks to handle the unprecedented amount of innovations. To imitate or even surpass the success of hand-optimized libraries, recent research has developed neural network compilers, such as XLA [4], Halide [36], Glow [37], Tensor Comprehension [40], and TVM [13]. Among them,
TVM has shown superior performance improvements using a technique called Learning-to-Compile (AutoTVM) [14]. AutoTVM optimizes the code by generating many versions of a tensor program and chooses the best through simulated annealing search over a large space of code transformation choices. Furthermore, it employs a learned cost model trained by actual hardware performance measures to predict the performance of diverse inference computations on real hardware targets.
While the Learning-to-Compile approach produces highly optimized code of DNN models, they may suffer from excessively long optimization time. As an example, although AutoTVM is able to demonstrate close to 2× performance improvement over TensorFlow on ResNet-18, the compilation time can still take several hours or even tens of hours [14]. With the active research that has been pushing the model size to millions or even billion-scale parameters with a training time of only a
∗Both authors contributed equally. Order of appearance is random. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
few hours or less than one hour [46, 19, 47, 29, 38, 48], it makes reducing the DL compilation time for inference of the current solution even more prominent. Furthermore, since many of these DL compilers have been adopted by major players in the industry [43, 44, 26, 30], many users of these pipelines, including deployment engineers, would have to go through the optimization numerous times. Finally, as new neural architectures [18, 41, 16, 34] come out in rapid speed, and with deeper or wider networks [45, 38, 35, 5] on various hardware platforms [3, 22, 28], we are forced to optimize the networks more frequently. The excessive long optimization time hinders the turnaround time and even puts the practical utility of the current compiler-based solutions into question.
We aim at accelerating innovations by developing an automatic and efﬁcient optimization process for DNN models. For this purpose, we introduce AdaTune, a method that achieves similar or better optimization quality but with shorter optimization time. Furthermore, AdaTune improves the adaptivity of LTC and reduces the hyperparameter tuning required, accelerating the productivity and agility of DNN model deployment. Speciﬁcally, the contributions of our paper consist of (1) a preliminary analysis that reveals the inefﬁciency and challenges of the existing approaches, (2) an adaptive evaluator that statistically determines the number of runs for performance measurement, (3) a surrogate modeling with uncertainty quantiﬁcation that allows to better capture hardware and model heterogeneity, and (4) a contextual optimizer that provides control of exploration and exploitation dynamically to improve the transformation space searching effectiveness. We conduct extensive experiments to show that the proposed approach consistently outperforms the previous method on various models and hardware. It not only allows us to optimize DNN models 1.3-3.9× faster than the baseline to reach the same optimization quality but also obtains up to 115% higher GFLOPS under the same time budget. We conduct ablation analysis to study the effects of the proposed techniques, and we will make the source code publicly accessible to encourage further research. 2