Abstract
The increasing impact of black box models, and particularly of unsupervised ones, comes with an increasing interest in tools to understand and interpret them. In this paper, we consider in particular how to characterise visual groupings discovered automatically by deep neural networks, starting with state-of-the-art clustering methods. In some cases, clusters readily correspond to an existing labelled dataset.
However, often they do not, yet they still maintain an “intuitive interpretability”.
We introduce two concepts, visual learnability and describability, that can be used to quantify the interpretability of arbitrary image groupings, including unsupervised ones. The idea is to measure (1) how well humans can learn to reproduce a grouping by measuring their ability to generalise from a small set of visual examples (learnability) and (2) whether the set of visual examples can be replaced by a succinct, textual description (describability). By assessing human annotators as classiﬁers, we remove the subjective quality of existing evaluation metrics. For better scalability, we ﬁnally propose a class-level captioning system to generate descriptions for visual groupings automatically and compare it to human annotators using the describability metric. 1

Introduction
Recent advances in unsupervised and self-supervised learning have shown that it is possible to learn data representations that are competitive with, and sometimes even superior to, the ones obtained via supervised learning [27, 48]. However, this does not make unsupervised learning a solved problem; unsupervised representations often need to be combined with labelled datasets before they can perform useful data analysis tasks, such as image classiﬁcation. Such labels induce the semantic categories necessary to provide an interpretation of data that makes sense to a human. Thus, it remains unclear whether unsupervised representations develop a human-like understanding of complex data in their own right.
In this paper, we consider the problem of assessing to what extent abstract, human-interpretable concepts can be discovered by unsupervised learning techniques. While this problem has been looked at before, we wish to cast it in a more principled and general manner than previously done. We start from a simple deﬁnition of a class as a subset Xc ⊂ X of patterns (e.g. images). While our method is agnostic to the class generation mechanism, we are particularly interested in classes that are obtained from an unsupervised learning algorithm. We then wish to answer three questions: (1) whether a given class is interpretable and coherent, meaning that it can be indeed understood by humans, (2) if so, whether it is also describable, i.e. it is possible to distill the concept(s) that the class represents into a compact sentence in natural language, and (3) if such a summary description can be produced automatically by an algorithm. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Our framework. We evaluate classes obtained by self-supervised learning algorithms (SSL) using human judgements. We formulate our evaluation as two forced-choice tasks and measure (a) the learnability of a class by humans and (b) the describability, i.e. the ability to distill the gist of the class into a description in natural language (class-level captioning).
The ﬁrst problem has already been explored in the literature (e.g. [75]), usually using human judgment to assess class interpretability. In short, human annotators are shown example patterns from the class and they are asked to name or describe it. Unfortunately, such a metric is rather subjective, even after averaging responses by several annotators. In our work, we aim to minimize subjectivity in evaluating class interpretability. While still involving humans in the assessment, we cast the problem as the one of learning the class from a number of provided examples. Rather than asking annotators to identify the class, we test their ability to discriminate further examples of the class from non-class examples. The accuracy from this classiﬁcation task can then be used as an objective measure of human learnability of the class, which we call semantic coherence. As we show later, self-discovered classes are often found to be semantically coherent according to this criterion, even when their consistency with respect to an existing label set — such as ImageNet [17] — is low.
Note that our semantic coherence metric does not require naming or otherwise distilling the concept(s) captured by a class. Thus, we also look at the problem of describing the class using natural language.
We start by manually collecting names or short descriptions for a number of self-labelled classes.
Then, we modify the previous experiment to test whether annotators can correctly recognise examples of the class from negative ones based on the provided description. In this manner, we can quantify the quality of the description, which is directly related to how easily describable the underlying class is.
Finally, we ask whether the process of textual distillation can be automated; this problem is related to image captioning, but with some important differences. First, the description has a well-deﬁned goal: to teach humans about the class, which is measured by their ability to classify patterns based on the description. This approach also allows a direct and quantitative comparison between manual and automatic descriptions. Second, the text must summarise an entire class, i.e. a collection of several patterns, rather than a single image. To this end, we investigate ways of converting existing, single-image captioning systems to class-level captioning. Lastly, we propose an automated metric to validate descriptions and compare such systems before using the human-based metric. 2