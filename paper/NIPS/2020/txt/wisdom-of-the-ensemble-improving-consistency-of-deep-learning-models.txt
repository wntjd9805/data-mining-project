Abstract
Deep learning classiﬁers are assisting humans in making decisions and hence the user’s trust in these models is of paramount importance. Trust is often a function of constant behavior. From an AI model perspective it means given the same input the user would expect the same output, especially for correct outputs, or in other words consistently correct outputs. This paper studies a model behavior in the context of periodic retraining of deployed models where the outputs from successive generations of the models might not agree on the correct labels assigned to the same input. We formally deﬁne consistency and correct-consistency of a learning model. We prove that consistency and correct-consistency of an ensemble learner is not less than the average consistency and correct-consistency of individual learners and correct-consistency can be improved with a probability by combining learners with accuracy not less than the average accuracy of ensemble component learners. To validate the theory using three datasets and two state-of-the-art deep learning classiﬁers we also propose an efﬁcient dynamic snapshot ensemble method and demonstrate its value. Code for our algorithm is available at https://github.com/christa60/dynens. 1

Introduction
As AI is increasingly supporting humans in decision making [34, 13], there is more emphasis than ever on building trustworthy AI systems [5]. Despite the discrepancies in terminology, almost every recent research on trustworthy AI agree on the need for building AI models that produce consistently correct outputs for the same input [14]. Although this seems like a straightforward requirement, however as we periodically retrain AI models in the ﬁeld, there is no guarantee that different generations of the model will be consistently correct when presented with the same input. Consider an example of an AI-based car-safety system [11] that correctly detects distracted driving and then does not detect 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for the same input a day later after being retrained overnight. Attackers can easily sport such an inconsistent behavior and exploit them, further this can compromise safety of drivers. Also consider the damage that can be caused by an AI-agent for COVID-19 diagnosis that correctly recommends a true patient to self-isolate, and subsequently changes its recommendation after being retrained with more data. Regression models like in Remaining Useful Life estimation system also suffer from inconsistency problem. However, considering that classiﬁcation models are more predominant we focus on such models only.
In this paper, we deﬁne consistency of a model as the ability to make consistent predictions across successive model generations for the same input. This deﬁnition is different from the replicability of model performance at an aggregate level [12] - with a stable training pipeline of a classiﬁer, aggregate metrics can be relatively consistent across successive generations, but changes to the training data or even retraining on the same data often causes changes in the individual predictions.
Consistency is applicable to both correct and incorrect outputs, however, the more desirable case is producing consistently correct outputs for the same inputs. We deﬁne ability to make consistent correct predictions across successive model generations for the same input as correct-consistency.
To understand further the effect of consistency and correct-consistency on users’ trust, lets consider the following scenarios for the car safety example (driver distraction) with two model generations -M odeli and M odeli+1, and an input X. (1) If the outputs from both models are correct, then the issue of inconsistency does not arise and does not affect the user. This is a case of correct-consistency. (2) If the output from M odeli is incorrect while from M odeli+1 is correct, it won’t adversely affect users’ trust but in-fact can be considered as an improvement in the system. (3) If the output from M odeli is correct while from M odeli+1 is incorrect, it is a very severe case because this can adversely affect users trust in the system as well as its usability. In this case correct-consistency is desired. (4) If the outputs from both models are incorrect, although it is an undesirable scenario and can affect users but it is still less severe from consistency point of view.
Although mentioned in limited studies [4, 28, 20] no previous work has discussed or measured consistency formally. In this work, we investigate why and how ensembles can improve consistency and correct-consistency of deep learning classiﬁers theoretically and empirically. Ensembles have had success in improving accuracy and uncertainty quantiﬁcation [24, 32], but have not been studied in the context of consistency. To the best of our knowledge, we are the ﬁrst to deﬁne and measure the consistency for deep learning classiﬁers, and improve it using ensembles. Speciﬁcally we make the following contributions:
• Formally deﬁne consistency and correct-consistency of a model and multiple metrics to measure it;
• Provide a theoretical explanation of why and how ensemble learning can improve consistency and correct-consistency when the average performance of all predictors in the ensemble is considered;
• Prove that the consistency and correct-consistency of an ensemble learner is not less than the average consistency and average correct-consistency of individual learners;
• Prove that adding components with accuracy higher than the average accuracy of ensem-ble component learners to an ensemble learner can yield a better consistency for correct predictions;
• Propose a dynamic snapshot ensemble learning with pruning algorithm to boost predictive correct-consistency and accuracy in an efﬁcient way;
• Conduct experiments on CIFAR10, CIFAR100, and Yahoo!Answers using state-of-the-art deep learning classiﬁers and demonstrate effectiveness and efﬁciency of the proposed method and prove the theorems empirically. 2