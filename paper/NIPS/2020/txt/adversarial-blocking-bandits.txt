Abstract
We consider a general adversarial multi-armed blocking bandit setting where each played arm can be blocked (unavailable) for some time periods and the reward per arm is given at each time period adversarially without obeying any distribution.
The setting models scenarios of allocating scarce limited supplies (e.g., arms) where the supplies replenish and can be reused only after certain time periods. We
ﬁrst show that, in the optimization setting, when the blocking durations and rewards are known in advance, ﬁnding an optimal policy (e.g., determining which arm per round) that maximises the cumulative reward is strongly NP-hard, eliminating the possibility of a fully polynomial-time approximation scheme (FPTAS) for the problem unless P = NP. To complement our result, we show that a greedy algorithm that plays the best available arm at each round provides an approximation guarantee that depends on the blocking durations and the path variance of the rewards. In the bandit setting, when the blocking durations and rewards are not known, we design two algorithms, RGA and RGA-META, for the case of bounded duration an path variation. In particular, when the variation budget BT is known in advance,
T (2 ˜D + K)BT ) dynamic approximate regret. On the other
RGA can achieve O( hand, when BT is not known, we show that the dynamic approximate regret of
RGA-META is at most O((K + ˜D)1/4 ˜B1/2T 3/4) where ˜B is the maximal path variation budget within each batch of RGA-META (which is provably in order of o(
T ). We also prove that if either the variation budget or the maximal blocking duration is unbounded, the approximate regret will be at least Θ(T ). We also show that the regret upper bound of RGA is tight if the blocking durations are bounded above by an order of O(1). (cid:113)
√ 1

Introduction
This paper investigates the blocking bandit model where pulling an arm results in having that arm blocked for a deterministic number of rounds. For example, consider the classical problem of online task allocation, in which new task requests arrive at each time step, waiting to be assigned to one of many servers [Karthik et al., 2017]. Once a server is allocated to a task, it starts working on it, and becomes unavailable for future tasks until that task is done. If there are no servers available or none is allocated to the task at its arrival, the request will not be served and leave the system forever. A more recent example comes from the domain of expert crowdsourcing (e.g., Upwork,
Outsourcely, etc.). In this setting, a job requester can sequentially choose from a pool of workers and allocate a short-term job/project to the worker [Ho and Vaughan, 2012, Tran-Thanh et al., 2014]. The stochastic version of this problem, where the rewards are randomly drawn from a distribution in an 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
i.i.d. manner, with the constraint that the blocking durations are ﬁxed per arm over time, has been studied in [Basu et al., 2019] and [Basu et al., 2020]. However, in many applications, the stochastic setting is too restrictive and not realistic. For example, in the online task allocation problem, the tasks can be heterogeneous, and both the value and the serving time of the tasks can vary over time in an arbitrary manner. Furthermore, in the expert crowdsourcing setting, the time and quality workers need to deliver the job are unknown in advance, can vary over time, and do not necessarily follow an i.i.d. stochastic process. These examples demonstrate that for many real-world situations, the stochastic blocking bandit model is not an appropriate choice.
To overcome this issue, in this paper we propose the adversarial blocking bandit setting, where both the sequence of rewards and blocking durations per arm can be arbitrary. While the literature of adversarial bandits is enormous, to the best of our knowledge, this is the ﬁrst attempt to address the effect of blocking in adversarial models. In particular, we are interested in a setting where the rewards are neither sampled i.i.d., nor maliciously chosen in an arbitrary way. Instead, in many real-world systems, the change in the value of rewards is rather slow or smooth over time (e.g., in the online task allocation problem, similar tasks usually arrive in batch, or in the crowdsourcing system, workers may have periods when they perform consistently, and thus, their performance slowly varies over time). To capture this, we assume that there is a path variation budget which controls the change of the rewards over time 1. 1.1 Main Contributions
In this paper, apart from the adversarial blocking bandit setting, we also investigate two additional versions of the model: (i) The ofﬂine MAXREWARD problem, where all the rewards and blocking durations are known in advance; and (ii) the online version of MAXREWARD, in which we see the corresponding rewards and blocking durations of the arms at each time step before we choose an arm to pull. Our main ﬁndings can be summarised as follows: 1. We prove that the ofﬂine MAXREWARD problem is strongly NP-hard (Theorem 1). Note that this result is stronger than the computational hardness result in Basu et al. [2019], which depends on the correctness of the randomised exponential time hypothesis. 2. We devise a provable approximation ratio for a simple online greedy algorithm, Greedy-BAA, for the online MAXREWARD problem (Theorem 2). Our approximation ratio, when applied to the stochastic blocking bandit model with ﬁxed blocking durations, is slightly weaker than that of Basu et al. [2019]. However, it is more generic, as it can be applied to any arbitrary sequence of rewards and blocking durations. 3. For the bandit setting, we consider the case when both the maximal blocking duration and the path variance are bounded, and propose two bandit algorithms:
• We design RGA for the case of known path variation budget BT . In particular, we show that
RGA can provably achieve O regret, where T is the time horizon, K (cid:18)(cid:113)
T (2 ˜D + K)BT (cid:19) is the number of arms, ˜D is the maximum blocking duration, and the regret is computed against the performance of Greedy-BAA (Theorem 3).
• For the case of unknown path variation budget BT , we propose RGA-META that uses Exp3 as a meta-bandit algorithm to learn an appropriate path variation budget and runs RGA with it. We prove that RGA-META achieves O((K + ˜D)1/4 ˜B1/2T 3/4) regret bound where ˜B is the maximal path variance within a single batch of the algorithm, and is in order of O(
T ) in the worst case (Theorem 4).
√ 4. Finally, we also discuss a number of regret lower bound results. In particular, we show that if either BT or ˜D is in Θ(T ) (or unbounded), then the regret is at least Θ(T ) (Claims 1 and 2). We also discuss that if ˜D ∈ O(1), then there is a matching lower bound for the regret of RGA (Section 5). 1We will show in Section 5 that bounded variation budgets are necessary to achieve sub-linear regrets. 2
1.2