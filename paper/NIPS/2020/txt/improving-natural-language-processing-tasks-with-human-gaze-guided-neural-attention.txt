Abstract
A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing (NLP). We propose a novel hybrid text saliency model (TSM) that, for the ﬁrst time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a speciﬁc upstream
NLP task without the need for any task-speciﬁc human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google
Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks. 1

Introduction
Neural attention mechanisms have been widely applied in computer vision and have been shown to enable neural networks to only focus on those aspects of their input that are important for a given task [48, 81]. While neural networks are able to learn meaningful attention mechanisms using only supervision received for the target task, the addition of human gaze information has been shown to be beneﬁcial in many cases [32, 58, 80, 84]. An especially interesting way of leveraging gaze information was demonstrated by works incorporating human gaze into neural attention mechanisms, for example for image and video captioning [71, 83] or visual question answering [58].
While attention is at least as important for reading text as it is for viewing images [13, 78], integration of human gaze into neural attention mechanisms for natural language processing (NLP) tasks remains under-explored. A major obstacle to studying such integration is data scarcity: Existing corpora of human gaze during reading consist of too few samples to provide effective supervision for modern data-intensive architectures and human gaze data is only available for a small number of NLP tasks.
For paraphrase generation and sentence compression, which play an important role for tasks such as reading comprehension systems [23, 28, 54], no human gaze data is available.
We address this data scarcity in two novel ways: First, to overcome the low number of human gaze samples for reading, we propose a novel hybrid text saliency model (TSM) in which we combine a cognitive model of reading behavior with human gaze supervision in a single machine learning framework. More speciﬁcally, we use the E-Z Reader model of attention allocation during reading [60] to obtain a large number of synthetic training examples. We use these examples to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
pre-train a BiLSTM [22] network with a Transformer [75] whose weights we subsequently reﬁne by training on only a small amount of human gaze data. We demonstrate that our model yields predictions that are well-correlated with human gaze on out-of-domain data. Second, we propose a novel joint modeling approach of attention and comprehension that allows human gaze predictions to be ﬂexibly adapted to different NLP tasks by integrating TSM predictions into an attention layer. By jointly training the TSM with a task-speciﬁc network, the saliency predictions are adapted to this upstream task without the need for explicit supervision using real gaze data. Using this approach, we outperform the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10% in BLEU-4 and achieve state of the art performance on the Google Sentence
Compression corpus. As such, our work demonstrates the signiﬁcant potential of combining cognitive and data-driven models and establishes a general principle for ﬂexible gaze integration into NLP that has the potential to also beneﬁt tasks beyond paraphrase generation and sentence compression. 2