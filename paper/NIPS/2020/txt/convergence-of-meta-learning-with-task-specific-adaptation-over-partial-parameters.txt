Abstract
Although model-agnostic meta-learning (MAML) is a very successful algorithm in meta-learning practice, it can have high computational cost because it updates all model parameters over both the inner loop of task-speciﬁc adaptation and the outer-loop of meta initialization training. A more efﬁcient algorithm ANIL (which refers to almost no inner loop) was proposed recently by Raghu et al. 2019, which adapts only a small subset of parameters in the inner loop and thus has substantially less computational cost than MAML as demonstrated by extensive experiments.
However, the theoretical convergence of ANIL has not been studied yet. In this paper, we characterize the convergence rate and the computational complexity for
ANIL under two representative inner-loop loss geometries, i.e., strongly-convexity and nonconvexity. Our results show that such a geometric property can signiﬁcantly affect the overall convergence performance of ANIL. For example, ANIL achieves a faster convergence rate for a strongly-convex inner-loop loss as the number N of inner-loop gradient descent steps increases, but a slower convergence rate for a nonconvex inner-loop loss as N increases. Moreover, our complexity analysis provides a theoretical quantiﬁcation on the improved efﬁciency of ANIL over
MAML. The experiments on standard few-shot meta-learning benchmarks validate our theoretical ﬁndings. 1

Introduction
As a powerful learning paradigm, meta-learning [2, 29] has recently received signiﬁcant attention, especially with the incorporation of training deep neural networks [5, 30]. Differently from the conventional learning approaches, meta-learning aims to effectively leverage the datasets and prior knowledge of a task ensemble in order to rapidly learn new tasks often with a small amount of data such as in few-shot learning. A broad collection of meta-learning algorithms have been developed so far, which range from metric-based [16, 28], model-based [21, 30], to optimization-based algo-rithms [5, 22]. The focus of this paper is on the optimization-based approach, which is often easy to be integrated with optimization formulations of many machine learning problems.
One highly successful optimization-based meta-learning approach is the model-agnostic meta-learning (MAML) algorithm [5], which has been applied to many application domains including classiﬁcation [25], reinforcement learning [5], imitation learning [9], etc. At a high level, the MAML algorithm consists of two optimization stages: the inner loop of task-speciﬁc adaptation and the outer 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(meta) loop of initialization training. Since the outer loop often adopts a gradient-based algorithm, which takes the gradient over the inner-loop algorithm (i.e., the inner-loop optimization path), even the simple inner loop of gradient descent updating can result in the Hessian update in the outer loop, which causes signiﬁcant computational and memory cost. Particularly in deep learning, if all neural network parameters are updated in the inner loop, then the cost for the outer loop is extremely high.
Thus, designing simpliﬁed MAML, especially the inner loop, is highly motivated. ANIL (which stands for almost no inner loop) proposed in [24] has recently arisen as such an appealing approach.
In particular, [24] proposed to update only a small subset (often only the last layer) of parameters in the inner loop. Extensive experiments in [24] demonstrate that ANIL achieves a signiﬁcant speedup over MAML without sacriﬁcing the performance.
Despite extensive empirical results, there has been no theoretical study of ANIL yet, which motivates this work. In particular, we would like to answer several new questions arising in ANIL (but not in the original MAML). While the outer-loop loss function of ANIL is still nonconvex as MAML, the inner-loop loss can be either strongly convex or nonconvex in practice. The strong convexity occurs naturally if only the last layer of neural networks is updated in the inner loop, whereas the nonconvexity often occurs if more than one layer of neural networks are updated in the inner loop. Thus, our theory will explore how such different geometries affect the convergence rate, computational complexity, as well as the hyper-parameter selections. We will also theoretically quantify how much computational advantage ANIL achieves over MAML by training only partial parameters in the inner loop. 1.1 Summary of Contributions
In this paper, we characterize the convergence rate and the computational complexity for ANIL with N -step inner-loop gradient descent, under nonconvex outer-loop loss geometry, and under two representative inner-loop loss geometries, i.e., strongly-convexity and nonconvexity. Our analysis also provides theoretical guidelines for choosing the hyper-parameters such as the stepsize and the number N of inner-loop steps under each geometry. We summarize our speciﬁc results as follows.
• Convergence rate: ANIL converges sublinearly with the convergence error decaying sublinearly with the number of sampled tasks due to nonconvexity of the meta objective function. The convergence rate is further signiﬁcantly affected by the geometry of the inner loop. Speciﬁcally,
ANIL converges exponentially fast with N initially and then saturates under the strongly-convex inner loop, and constantly converges slower as N increases under the nonconvex inner loop.
• Computational complexity: ANIL attains an (cid:15)-accurate stationary point with the gradient and second-order evaluations at the order of O((cid:15)−2) due to nonconvexity of the meta objective function. The computational cost is also signiﬁcantly affected by the geometry of the inner loop. Speciﬁcally, under the strongly-convex inner loop, its complexity ﬁrst decreases and then increases with N , which suggests a moderate value of N and a constant stepsize in practice for a fast training. But under the nonconvex inner loop, ANIL has higher computational cost as N increases, which suggests a small N and a stepsize at the level of 1/N for desirable training.
• Our experiments validate that ANIL exhibits aforementioned very different convergence behav-iors under the two inner-loop geometries.
From the technical standpoint, we develop new techniques to capture the properties for ANIL, which does not follow from the existing theory for MAML [4, 14]. First, our analysis explores how different geometries of the inner-loop loss (i.e., strongly-convexity and nonconvexity) affect the convergence of ANIL. Such comparison does not exist in MAML. Second, ANIL contains parameters that are updated only in the outer loop, which exhibit special meta-gradient properties not captured in MAML. 1.2