Abstract
In naturalistic learning problems, a model’s input contains a wide range of features, some useful for the task at hand, and others not. Of the useful features, which ones does the model use? Of the task-irrelevant features, which ones does the model represent? Answers to these questions are important for understanding the basis of models’ decisions, as well as for building models that learn versatile, adaptable representations useful beyond the original training task. We study these questions using synthetic datasets in which the task-relevance of input features can be controlled directly. We ﬁnd that when two features redundantly predict the labels, the model preferentially represents one, and its preference reﬂects what was most linearly decodable from the untrained model. Over training, task-relevant features are enhanced, and task-irrelevant features are partially suppressed. Interestingly, in some cases, an easier, weakly predictive feature can suppress a more strongly predictive, but more difﬁcult one. Additionally, models trained to recognize both easy and hard features learn representations most similar to models that use only the easy feature. Further, easy features lead to more consistent representations across model runs than do hard features. Finally, models have greater representational similarity to an untrained model than to models trained on a different task. Our results highlight the complex processes that determine which features a model represents. 1

Introduction
How does a deep neural model see the world at initialization, and what changes over the course of training? If there are many latent features in the training data — such as the colors, textures, and shapes of objects in visual datasets — how does the model separate task-relevant features from irrelevant ones? Does a model gain sensitivity to diagnostic features by enhancing its representation of them, or by suppressing its representations of other features? What does a model represent among task-irrelevant features, features that are task-relevant but unreliable, and features that are redundantly predictive, each of which are present in naturalistic datasets? How similar are feature representations across models as a function of training task? How does the similarity depend on the feature?
From an engineering perspective, these questions are important as the ﬁeld tries to build models whose representations are versatile and general-purpose enough to support out-of-distribution generalization, and to transfer to new downstream tasks. For example, it is common practice to transfer weights or activations from ImageNet models to models used for other vision tasks [15, e.g.], and to use
BERT [7] as a starting point for language tasks. Yet untrained models can sometimes provide an initialization that is nearly as good as ImageNet pretraining [12, 23], raising the questions of what information is present in an untrained model, and how it is modiﬁed during training. Furthermore, models sometimes learn to use “shortcut features” [10] – an object’s texture, rather than shape
∗Contributed equally 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[11], for example – that solve the training task, but fail to generalize robustly. This phenomenon highlights the importance of understanding which features are learned in a given setting, especially when features are correlated. Additionally, understanding how models select features is important for ensuring that models make equitable and non-biased decisions [45, 47].
From a scientiﬁc perspective, understanding which features a model learns, and how consistent feature representations are across model instances, is relevant to understanding how models compute
[2], as well as how they should be compared to one another [29] or to neural data [19, 6, 5, 39, 43].
In this paper, we investigate the evolution of models’ feature preferences using synthetic datasets in which we can directly control the relationships between multiple features and task labels, which is not possible in naturalistic datasets like ImageNet. We create datasets from a data-generating process in which underlying latent variables give rise to input features like shape or texture (in our visual tasks), or linearly or non-linearly extractable features (in our binary features tasks). We train models on tasks that are either deterministically or probabilistically related to these input features. Leveraging the complementary approaches of decoding and representational similarity analysis [24], we probe feature representations within and between models across datasets, architectures, and training. Our results provide insight into how feature representations are affected by learning and how to analyze them. Our key contributions are:
• We ﬁnd that many input features are already partially represented in the higher layers of untrained models. Training on a task enhances task-relevant features (increases their decodability relative to an untrained model), and suppresses both task-irrelevant features and some task-relevant features.
• We investigate what a model represents when multiple features predict the label. We ﬁnd that, when a pair of features redundantly predicts the label, models prefer one of the features over the other, and the preference structure tracks untrained decodability. When only one feature is perfectly predictive, we ﬁnd that the representations of a correlated feature remain roughly constant as a function of correlation, until the correlation becomes quite high.
• We identify cases in which models are “lazy,” and suppress a more predictive feature in favor of an easier-to-extract, but less predictive, feature.
• We ﬁnd that easy features induce representations that are more consistent across model runs than do hard features, and that a multi-task model trained to report both easy and hard features produces representations that are very similar to those of a model trained only on the easy feature. 2