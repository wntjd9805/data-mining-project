Abstract
Rather than performing inefﬁcient local iterative routing between adjacent capsule layers, we propose an alternative global view based on representing the inherent un-certainty in part-object assignment. In our formulation, the local routing iterations are replaced with variational inference of part-object connections in a probabilistic capsule network, leading to a signiﬁcant speedup without sacriﬁcing performance.
In this way, global context is also considered when routing capsules by introducing global latent variables that have direct inﬂuence on the objective function, and are updated discriminatively in accordance with the minimum description length (MDL) principle. We focus on enhancing capsule network properties, and perform a thorough evaluation on pose-aware tasks, observing improvements in performance over previous approaches whilst being more computationally efﬁcient. 1

Introduction
Although capsule networks (CapsNets) have taken on a few different forms since their inception [1, 2, 3, 4], they are generally built upon the following core assumptions and premises: (i) Capturing equivariance w.r.t. viewpoints in neural activities, and invariance in the weights; (ii) High-dimensional coincidences are effective feature detectors; (iii) Viewpoint changes have nonlinear effects on pixels, but linear effects on object relationships; (iv) Object parts belong to a single object, and each location contains at most a single object.
In theory, a perfect instantiation of the above premises could yield more sample efﬁcient models, that leverage robust representations to better generalise to unseen cases. Unlike current methods, humans can extrapolate object appearance to novel viewpoints after a single observation. Evidence suggests that this is because we impose coordinate frames on objects [5, 6]. Capsules imitate this concept by representing neural activities as poses of objects w.r.t. a coordinate frame imposed by an observer, and attempt to disentangle salient features of objects into their composing parts. This is reminiscent of inverse graphics [7], but is not explicitly enforced in capsule formulations since the learned pose matrices are not constrained to interpretable geometric forms. Another argument for CapsNets, is one that views capsules as an extension to the very successful inductive biases already present in CNNs, by wiring in some additional complexity to deal with viewpoint changes. One of the desired effects is to align the learned representations with those perceptually consistent with humans, which would also make adversarial examples less effective [8]. The additional complexity comes from replacing scalar neurons with vector valued neural activities, along with a high-dimensional coincidence ﬁltering algorithm to detect capsule level features, known as capsule routing [2, 3]. This procedure is typically iterative, local and inefﬁcient which has prompted further research on the topic [9, 10, 11, 12, 13]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1.1 Motivation & Contribution
Weaknesses of Capsule Networks. The memory bottleneck incurred by vector valued activations in addition to the iterative nature of capsule routing algorithms results in inefﬁcient models. They are also prone to underﬁtting or overﬁtting if the number of routing iterations isn’t properly set [2, 3].
To address the above weaknesses one may decide to naively replace the iterative nature of capsule routing with some faster alternative. However, to stay true to the premises of CapsNets, we argue that the four following points are of paramount importance for the research community to consider, when proposing algorithmic variants of CapsNets or capsule routing going forward: (i) Whether viewpoint-invariance and afﬁne transformation robustness properties are retained; (ii) Changes in assumptions about part-object relationships are made explicit; (iii) Whether capsules are still activated based on high-dimensional coincidences; (iv) How do we handle the intrinsic uncertainty in assembling parts into objects.
Changes in the core assumptions of CapsNets aren’t always made clear in recent literature, but emerge incidentally via the proposed modiﬁcations. This leads to ambiguities regarding what qualiﬁes as a capsule network, which can make comparisons between methods more difﬁcult and hinder progress.
In this paper, we focus on the core premises of capsule networks, and on enhancing their advantages over CNNs: viewpoint-invariance, and afﬁne transformation robustness whilst being more efﬁcient.
Contribution. Rather than performing local iterative routing between adjacent capsule layers which is inefﬁcient, we propose an alternative global view based on representing the inherent uncertainty in part-object relationships, by approximating a posterior distribution over part-object connections.
Sources of uncertainty in assembling objects via a composition of parts can arise from numerous sources, such as: (i) feature occlusions due to observed viewpoints; (ii) sensory noise in captured data; (iii) object symmetries for which poses may be ambiguous such as spherical objects/parts.
In our formulation, the local routing iterations are replaced with variational inference of part-object connections in a probabilistic capsule network, leading to a signiﬁcant speedup (Figure 4). In this way, we encourage global context to be taken into account when routing information, by introducing global latent variables which have direct inﬂuence on the objective function, and are updated discriminatively in accordance with the minimum description length (MDL) principle [14, 15]. Our experiments demonstrate that local iterative routing can be replaced by variational posterior inference of part-object connections in a global context setting, allowing the model to leverage the inherent uncertainty in assembling objects as a composition of parts to improve performance on pose-aware tasks. 2