Abstract
Recent advances in deep reinforcement learning (RL) have led to considerable progress in many 2-player zero-sum games, such as Go, Poker and Starcraft. The purely adversarial nature of such games allows for conceptually simple and prin-cipled application of RL methods. However real-world settings are many-agent, and agent interactions are complex mixtures of common-interest and competitive aspects. We consider Diplomacy, a 7-player board game designed to accentuate dilemmas resulting from many-agent interactions. It also features a large com-binatorial action space and simultaneous moves, which are challenging for RL algorithms. We propose a simple yet effective approximate best response operator, designed to handle large combinatorial action spaces and simultaneous moves. We also introduce a family of policy iteration methods that approximate ﬁctitious play.
With these methods, we successfully apply RL to Diplomacy: we show that our agents convincingly outperform the previous state-of-the-art, and game theoretic equilibrium analysis shows that the new process yields consistent improvements. 1

Introduction
Artiﬁcial Intelligence methods have achieved exceptionally strong competitive play in board games such as Go, Chess, Shogi [108, 110, 20, 109], Hex [2], Poker [85, 17] and various video games [63, 84, 60, 94, 45, 120, 55, 14]. Despite the scale, complexity and variety of these domains, a common focus in multi-agent environments is the class of 2-player (or 2-team) zero-sum games: “1 vs 1” contests.
There are several reasons: they are polynomial-time solvable, and solutions both grant worst-case guarantees and are interchangeable, so agents can approximately solve them in advance [121, 122].
Further, in this case conceptually simple adaptations of reinforcement learning (RL) algorithms often have theoretical guarantees. However, most problems of interest are not purely adversarial: e.g. route planning around congestion, contract negotiations or interacting with clients all involve compromise and consideration of how preferences of group members coincide and/or conﬂict. Even when agents are self-interested, they may gain by coordinating and cooperating, so interacting among diverse groups of agents requires complex reasoning about others’ goals and motivations.
We study Diplomacy [19], a 7-player board game. The game was speciﬁcally designed to emphasize tensions between competition and cooperation, so it is particularly well-suited to the study of learning in mixed-motive settings. The game is played on a map of Europe partitioned into provinces. Each player controls multiple units, and each turn all players move all their units simultaneously. One unit may support another unit (owned by the same or another player), allowing it to overcome resistance by other units. Due to the inter-dependencies between units, players must coordinate the moves of their own units, and stand to gain by coordinating their moves with those of other players. Figure 1 depicts interactions among several players (moving and supporting units to/from provinces); we explain the basic rules in Section 2.1. The original game allows cheap-talk negotiation between 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
players before every turn. In this paper we focus on learning strategic interactions in a many-agent setting, so we consider the popular No Press variant, where no explicit communication is allowed.
Diplomacy is particularly challenging for RL agents. First, it is a many-player (n > 2) game, so methods cannot rely on the simplifying properties of 2-player zero-sum games. Second, it features simultaneous moves, with a player choosing an action without knowledge of the actions chosen by others, which highlights reasoning about opponent strategies. Finally,
Diplomacy has a large combinatorial action space, with an estimated game-tree size of 10900, and 1021 to 1064 legal joint actions per turn. 1
Consequently, although Diplomacy AI has been studied since the 1980s [65, 67], until recently progress has relied on handcrafted rule-based systems, rather than learning. Paquette et al. [90] achieved a major breakthrough: they collected a dataset of ∼ 150, 000 human Diplomacy games, and trained an agent, DipNet, using a graph neural network (GNN) to imitate the moves in this dataset. This agent defeated previous state-of-the-art agents conclusively and by a wide margin. This is promising, as imitation learning can often be a useful starting point for RL methods.
Figure 1: Simple exam-ple of interactions be-tween several players’ moves.
However, to date RL has not been successfully applied to Diplomacy. For example, Paquette et al. [90] used A2C initialised by their imitation learning agent, but this process did not improve performance as measured by the Trueskill rating system [50]. This is unfortunate, as without agents able to optimise their incentives, we cannot study the effects of mixed-motives on many-agent learning dynamics, or how RL agents might account for other agents’ incentives (e.g. with Opponent Shaping [36]).
Our Contribution: We train RL agents to play No-Press Diplomacy, using a policy iteration (PI) approach. We propose a simple yet scalable improvement operator, Sampled Best Responses (SBR), which effectively handles Diplomacy’s large combinatorial action space and simultaneous moves.
We introduce versions of PI that approximate iterated best response and ﬁctitious play (FP) [16, 97] methods. In Diplomacy, we show that our agents outperform the previous state-of-the-art both against reference populations and head-to-head. A game theoretic equilibrium analysis shows our process yields consistent improvements. We propose a few-shot exploitability metric, which our RL reduces, but agents remain fairly exploitable. We perform a case-study of our methods in a simpler game,
Blotto (Appendix A), and prove convergence results on FP in many-player games (Appendix H). 2