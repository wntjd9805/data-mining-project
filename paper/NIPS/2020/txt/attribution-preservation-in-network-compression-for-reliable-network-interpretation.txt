Abstract
Neural networks embedded in safety-sensitive applications such as self-driving cars and wearable health monitors rely on two important techniques: input attribution for hindsight analysis and network compression to reduce its size for edge-computing.
In this paper, we show that these seemingly unrelated techniques conﬂict with each other as network compression deforms the produced attributions, which could lead to dire consequences for mission-critical applications. This phenomenon arises due to the fact that conventional network compression methods only preserve the predictions of the network while ignoring the quality of the attributions. To combat the attribution inconsistency problem, we present a framework that can preserve the attributions while compressing a network. By employing the Weighted
Collapsed Attribution Matching regularizer, we match the attribution maps of the network being compressed to its pre-compression former self. We demonstrate the effectiveness of our algorithm both quantitatively and qualitatively on diverse compression methods. 1

Introduction
Riding on the recent success of deep learning in numerous ﬁelds, there is an emergent trend to utilize deep neural networks (DNNs) even for safety-critical applications such as self-driving cars and wearable health monitors. Due to the inherent nature of such devices, it is of paramount importance that the utilized DNNs be reliable and trustworthy to human users.
For a system to be reliable, perpetual service must be rendered and the integrity of the system must hold even under unexpected circumstances. For most commercially deployed DNNs, this condition is hardly met as they are often operated in the cloud due to their heavy computational requirements. However, this dependence on clouds acts as a critical weakness in safety-sensitive settings as intermittent communication failures to the cloud may cause difﬁculties in reacting to situations immediately, or even worse, the device’s connection to the cloud may be severed indeﬁnitely.
Thus, to guarantee reliable service, the DNNs must be embedded on the edge device. To this end, network compression techniques such as pruning [1, 2] and distillation [3, 4] are commonly employed
- as a compressed network would require less computational time and memory but maintain its prediction performance to a certain acceptable margin, effectively substituting the original network for edge computation.
At the same time, for a system to be trustworthy, the system must be transparent enough for humans to understand its workings and the reasons for its outputs. An example would be when a health monitor predicts an onset of a disease [5] - then the clinician would require an acceptable explanation to the device output. However, the black-box nature of deep neural networks complicates this goal -impeding its advance in safety-critical areas. For DNNs to gain trustworthiness, the ability to explain 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
∗Equal contribution. Listing order is alphabetical.
Figure 1: Attribution maps of a network before and after network compression. These ﬁgures are examples that the networks before and after compression predicted the same correct labels (bus, cat, sofa), but exhibit different attribution maps. Observe that for compressed networks, the max value of the heatmaps (blue circle) is evicted outside the segmentation boundaries (white line) while our method maintains the dot. why the network makes such decisions is essential. Such ﬁeld of interest - eXplainable AI (XAI) -has emerged as one of the important frontiers in the ﬁeld of deep learning. Among numerous XAI methods, the most commonly used methods are attribution methods [6], which weigh the parts of the input data according to how much they ‘contributed’ to produce the output prediction. Such attribution methods are beginning to be applied in safety-critical ﬁelds [7].
To ensure the safety of the system, the two aforementioned conditions should be simultaneously satisﬁed - the embedded DNNs must be equipped with both compression and attribution. However, we show for the ﬁrst time that these seemingly unrelated techniques conﬂict with each other: compressing a network causes deformations in the produced attributions, even if the predictions of the network stays the same before and after compression (See Figure 1). This is a potentially severe crack in the integrity of the compressed network, as the premise in which a compressed network is acceptable in safety-critical ﬁelds is that the compressed network is as reliable as its former self.
This implies that the compressed network must behave almost identically to the pre-compression network while being smaller in size. Moreover, the attributions of the compressed network are not only different from their past counterparts but also broken down compared to their respective segmentation ground truths, as shown in Figure 1 and Table 1. These attribution distortions directly cause incorrect interpretations, which could lead to dire consequences for safety-critical systems.
Such a problem arises from the pitfall of existing network compression approaches: they only aim to maintain the prediction quality of the network while reducing the size of the network.
Compressing a network forces the network to cram its necessary decision procedures and information inside a smaller space. This space restriction forces the network to abandon its standard decision procedures and resort to using shortcuts and hints that are seemingly indecipherable to humans.
Thus, its decision procedures would become harder to interpret, which is reﬂected in its production of deformed attribution maps.
Table 1: Evaluation of how many samples were broken compared to the ground truth (segmentation labels) by various compression methods. Here, AUC denotes the degree of overlap between the segmentation and attribution map (see Section 4).
Point accuracy [8] is a measure of whether the max value of the heatmap is inside the segmentation map. Only the samples that the predictions of the network were correct are counted.
Method
Full (Teacher)
Knowledge Distillation
Structured Pruning
Unstructured Pruning
KD (w/ Ours)
For samples with correct pred.
Point Acc
#Param AUC 80.21 15.22M 88.79 67.26 0.29M 78.74 75.29 3.27M 79.98 75.43 0.53M 84.13 79.12 0.29M 88.06
To resolve this newfound unintended issue, we propose a novel attribution-aware compression framework to ensure 2
both the reliability and trustworthiness of the compressed model. One way to tackle this problem is to inject the attribution information to the now-compressing network by employing a matching regularizer to match the attributions to a ground truth signal (e.g. ground truth segmentation data).
However, these kinds of signals are very rare as they require extensive human labor. To bypass this problem, we concentrate on the observation that the attributions of the pre-network (teacher) are closer to the ground truth signal compared to the post-network (student), as shown in Table 1. Thus, in the absence of ground truth signals, the attributions of the teacher can serve as a proxy. In this sense, we propose a regularizer that matches the attribution maps of the now-compressing network to its attribution maps before compression, transferring the attributional power of the pre-network to the post-network. Our work sheds new light on transfer learning techniques from the perspective of XAI, as they can be re-interpreted and subsumed under our framework.
Our contributions are as follows:
• We show for the ﬁrst time that compressing networks via pruning or distillation distorts the attributions of the network (i.e. compressed networks classify correctly but pay attention to wrong places), hence the well-calibrated explainability of the original model can be completely destroyed even with matched performance.
• We propose a matching technique to efﬁciently preserve diverse levels of attribution maps while compressing the networks, by regularizing the differences between the sampled attribution maps of the teacher and the student.
• Through extensive experiments, we validate the effectiveness of our framework and show that our attribution matching not only maintains the interpretation of the model but also yields signiﬁcant performance gains. 2