Abstract
We consider the setting where players run the Hedge algorithm or its optimistic variant to play an n-action game repeatedly for T rounds.
• For two-player games, we show that the regret of optimistic Hedge decays at rate O(1/T 5/6), improving the previous bound of O(1/T 3/4) by Syrgkanis,
Agarwal, Luo and Schapire [27].
• In contrast, we show that the convergence rate of vanilla Hedge is no better
T ), addressing an open question posed in Syrgkanis, Agarwal,
√ than O(1/
Luo and Schapire [27].
For general m-player games, we show that the swap regret of each player decays at
O(m1/2(n log n/T )3/4) when they combine optimistic Hedge with the classical external-to-internal reduction of Blum and Mansour [6]. Via standard connec-tions, our new (swap) regret bounds imply faster convergence to coarse correlated equilibria in two-player games and to correlated equilibria in multiplayer games. 1

Introduction
Online algorithms for regret minimization play an important role in many applications in machine learning where real-time sequential decision making is crucial [19, 7, 26]. A number of algorithms have been developed, including Hedge / Multiplicative Weights [2], Mirror Decent [19], Follow the
Regularized / Perturbed Leader [20], and their power and limits against an adversarial environment
T ) after T rounds, have been well understood: The average (external) regret decays at a rate of O(1/ which is known to be tight for any online algorithm.
√
√
What happens if players in a repeated game run one of these algorithms? Given that they are now running against similar algorithms over a ﬁxed game, could the regret of each player decay signiﬁ-T ? This was answered positively in a sequence of works [9, 24, 27]. Among cantly faster than 1/ these results, the one that is most relevant to ours is that of Syrgkanis, Agarwal, Luo and Schapire
[27]. They showed that if every player in a multiplayer game runs an algorithm that satisﬁes the
RVU (Regret bounded by Variation in Utilities) property, then the regret of each player decays at
O(1/T 3/4). Can this bound be further improved?
Besides regret minimization, understanding no-regret dynamics in games is motivated by connections with various equilibrium concepts [15, 13, 12, 18, 6, 17, 22]. For example, if every player runs an algorithm with vanishing regret, then the empirical distribution must converge to a coarse correlated equilibrium [7]. Nevertheless, to converge to a more preferred correlated equilibrium [3], a stronger notion of regrets called swap regrets (see Section 2) is required [13, 18, 6]. The minimization of swap regrets under the adversarial setting was studied by Blum and Mansour [6]. They gave a generic reduction from regret minimization algorithms which led to a tight O((cid:112)n log n/T )-bound for the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
average swap regret. A natural question is whether a speedup similar to that of [27] is possible for swap regrets in the repeated game setting.
√
Our contributions: Faster convergence of swap regrets. We give the ﬁrst algorithm that achieves an average swap regret that is signiﬁcantly lower than O(1/
T ) under the repeated game setting.
This algorithm, denoted by BM-Optimistic-Hedge, combines the external-to-internal reduction of
[6] with the optimistic Hedge algorithm [24, 27] as its regret minimization component. (Optimistic
Hedge can be viewed as an instantiation of the optimistic Follow the Regularized Leader algorithm; see Section 2.) We show that if every player in a repeated game of m players and n actions runs BM-Optimistic-Hedge, then the average swap regret is at most O(m1/2(n log n/T )3/4); see
Theorem 5.1 in Section 5. Via the relationship between correlated equilibria and swap regrets, our result implies faster convergence to a correlated equilibrium. When specialized to two-player games, the empirical distribution of players running BM-Optimistic-Hedge converges to an (cid:15)–correlated equilibrium after O(n log n/(cid:15)4/3) rounds, improving the O(n log n/(cid:15)2) bound of [6].
Our main technical lemma behind Theorem 5.1 shows that strategies produced by the algorithm of [6] with optimistic Hedge moves very slowly in (cid:96)1-norm under the adversarial setting (which in turn allows us to apply a stability argument similar to [27]). This came as a surprise because a key component of the algorithm of [6] each round is to compute the stationary distribution of a Markov chain, which is highly sensitive to small changes in the Markov chain. We overcome this difﬁculty by exploiting the fact that Hedge only incurs small multiplicative changes to the Markov chain, which allows us to bound the change in the stationary distribution using the classical Markov chain tree theorem. As a consequence, our algorithm enjoys the beneﬁt of faster convergence when playing with each other, while remain robust against adversaries.
Our contributions: Hedge in two-player games. In addition we consider regret minimization in a two-player game with n actions using either vanilla or optimistic Hedge. We show that optimistic
Hedge can achieve an average regret of O(1/T 5/6), improving the bound O(1/T 3/4) by [27] for two-player games; see Theorem 3.1 in Section 3. In contrast, we show that even under this game-T ) adversarial bound; theoretic setting, vanilla Hedge cannot asymptotically outperform the O(1/ see Theorem 4.1 in Section 4. This addresses an open question posed by [27] concerning the convergence rate of vanilla Hedge in a repeated game.
√
The key step in our analysis of optimistic Hedge is to show that, even under the adversarial setting, the trajectory length of strategy movements (in their squared (cid:96)1-norm) can be bounded using that of cost vectors (in (cid:96)∞-norm); see Lemma 3.2. (Intuitively, it is unlikely for the strategy of optimistic
Hedge to change signiﬁcantly over time while the loss vector stays stable.) This allows us to build a strong relationship between the trajectory length of each player’s strategy movements, and then use the RVU property of optimistic Hedge to bound their individual regrets.
Our lower bounds for vanilla Hedge use three very simple 2 × 2 games to handle different ranges of the learning rate η. For the most intriguing case when η is at least Ω(1/ n) and bounded from above by some constant, we study the zero-sum Matching Pennies game and use it to show that the overall
√ regret of at least one player is Ω(
T ). Our analysis is inspired by the result of [5] which shows that the KL divergence of strategies played by Hedge in a two-player zero-sum game is strictly increasing.
For Matching Pennies, we start with a quantitative bound on how fast the KL divergence grows in
T during which the cost of one of the
Lemma 4.3. This implies the existence of a window of length player grows by Ω(1) each round; the zero-sum structure of the game allows us to conclude that at least one of the players must have regret at least Ω(
T ) at some point in this window.
√
√
√ 1.1