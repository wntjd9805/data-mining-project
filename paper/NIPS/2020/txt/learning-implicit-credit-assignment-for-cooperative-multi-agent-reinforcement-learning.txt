Abstract
We present a multi-agent actor-critic method that aims to implicitly address the credit assignment problem under fully cooperative settings. Our key motivation is that credit assignment among agents may not require an explicit formulation as long as (1) the policy gradients derived from a centralized critic carry sufﬁcient in-formation for the decentralized agents to maximize their joint action value through optimal cooperation and (2) a sustained level of exploration is enforced through-out training. Under the centralized training with decentralized execution (CTDE) paradigm, we achieve the former by formulating the centralized critic as a hyper-network such that a latent state representation is integrated into the policy gradients through its multiplicative association with the stochastic policies; to achieve the latter, we derive a simple technique called adaptive entropy regularization where magnitudes of the entropy gradients are dynamically rescaled based on the current policy stochasticity to encourage consistent levels of exploration. Our algorithm, referred to as LICA, is evaluated on several benchmarks including the multi-agent particle environments and a set of challenging StarCraft II micromanagement tasks, and we show that LICA signiﬁcantly outperforms previous methods. 1

Introduction
Many complex real-world problems such as autonomous vehicle coordination [3], network rout-ing [55], and robot swarm control [17] can naturally be formulated as multi-agent cooperative games, where reinforcement learning (RL) presents a powerful and general framework for training robust agents. Though single-agent RL algorithms can be trivially applied to these environments by treating the multi-agent system as a single actor with a joint action space, the limited scalability and the inherent constraints on agent observability and communication in many multi-agent environments necessitate decentralized policies that act only on their local observations. A straightforward ap-proach to learn decentralized policies is to train the agents independently, but the simultaneous exploration of the agents often results in non-stationary environments where learning becomes highly unstable. To this end, previous work relies on a standard paradigm known as centralized training with decentralized execution (CTDE) [34, 25, 37, 10, 28, 4], where the independent agents can access additional state information that is unavailable during policy inference.
However, one major challenge of CTDE in cooperative settings is credit assignment, which refers to the task of attributing a global, shared reward from the environment to the agents’ individual actions. Solving the credit assignment problem explicitly may give useful insights into which agents or agent actions were responsible for the collective reward signal and may thus substantially facilitate policy optimization, but doing so is often nontrivial since the interactions between the agents and the environment can be highly complex. A notable approach is to assess individual agent actions by calculating difference rewards against a certain reward baseline [48, 36, 10], but these methods can
∗Equal contribution. Author ordering is random. Correspondence to Meng Zhou <mzho7212@gmail.com> and Ziyu Liu <kenziyuliu@outlook.com>. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
be inefﬁcient as they require separate estimations for the baselines and can become less effective with complex cooperation behaviors. Another line of approach is to represent the global state-action value as a (rule-based or learned) aggregation of the individual state-action values [46, 37, 45, 28].
A representative method is QMIX [37], which achieves implicit credit assignment by learning a non-linear mixing network that conditions on the global state and maps the individual agent Q-values into the joint action Q-value estimate. While these methods allow more complex credit assignment, the capacity of the value mixing network is still limited by the monotonic relationships between the joint and the individual Q-values. Their extensions to continuous action spaces may also require additional strategies that can compromise performance and/or complexity.
In this work, we propose a policy-based algorithm called LICA for learning implicit credit assignment that aims to address the above limitations. LICA is closely related to the family of value gradient methods [7, 52, 15, 43, 14, 26, 2] where policies are directly optimized in the direction of the approximate state/action value gradients. Apart from applying the framework to multi-agent settings, our key contribution is to extend the concept of value mixing [46, 37, 28, 45] to policy mixing, where the centralized critic is formulated as a hypernetwork [11] that maps the current state information into a set of weights which, in turn, mixes the individual action vectors into the joint action value estimate.
Compared to previous policy-based methods such as [10, 27, 18], this practical formulation introduces an extra latent state representation into the policy gradients to provide sufﬁcient information for learning optimal cooperative behaviors without explicit credit assignment strategies. It also trivially achieves higher expressiveness than value mixing methods as there are no inherent constraints on the mixing weights of the centralized critic. Following the above motivation, we also explore an alternative training regime where continuous approximations of action samples from the stochastic policies are replaced with explicit action distribution parameters to provide more information about the agents’ behaviors during policy optimization. While the resulting action value gradients can be less accurate, we observe that policies can learn stably and often converge faster to better solutions.
One notable challenge for policy-based algorithms is maintaining consistent levels of exploration to prevent premature convergence to sub-optimal policies [53, 30, 1]. Many existing methods [42, 13, 18, 27, 49] address this with entropy regularization, where the policy entropy is added to the training objective to favor more stochastic actions. While widely adopted, we argue that the vanilla form of entropy regularization could be ineffective for this purpose due to the undesirable curvature of the entropy function derivative. To this end, we further propose a simple technique called adaptive entropy regularization where the magnitudes of entropy gradients are inversely adjusted based on the policy entropy itself. We show that this allows easier tuning of the regularization strength and more consistent levels of policy stochasticity throughout training.
We benchmark our methods on two sets of cooperative environments, the Multi-Agent Particle
Environments [27] and the StarCraft Multi-Agent Challenge [39], and we observe considerable per-formance improvements over previous state-of-the-art algorithms. We also conduct further component studies to demonstrate that (1) compared to difference reward based credit assignment approaches (e.g. [10]), LICA has higher representational capacity and can readily handle environments where multiple global optima exist, and (2) our adaptive entropy regularization is crucial for encouraging sustained exploration and can lead to faster policy convergence in complex scenarios. 2