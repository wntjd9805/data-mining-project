Abstract
Learning monotonic models with respect to a subset of the inputs is a desirable feature to effectively address the fairness, interpretability, and generalization issues in practice. Existing methods for learning monotonic neural networks either require speciﬁcally designed model structures to ensure monotonicity, which can be too restrictive/complicated, or enforce monotonicity by adjusting the learning process, which cannot provably guarantee the learned model is monotonic on selected features. In this work, we propose to certify the monotonicity of the general piece-wise linear neural networks by solving a mixed integer linear programming problem. This provides a new general approach for learning monotonic neural networks with arbitrary model structures. Our method allows us to train neural networks with heuristic monotonicity regularizations, and we can gradually increase the regularization magnitude until the learned network is certiﬁed monotonic.
Compared to prior works, our method does not require human-designed constraints on the weight space and also yields more accurate approximation. Empirical studies on various datasets demonstrate the efﬁciency of our approach over the state-of-the-art methods, such as Deep Lattice Networks [34]. 1

Introduction
Monotonicity with respect to certain inputs is a desirable property of the machine learning (ML) predictions in many practical applications [e.g., 17, 28, 11, 9, 10, 6]. For real-world scenarios with fairness or security concerns, model predictions that violate monotonicity could be considered unacceptable. For example, when using ML to predict admission decisions, it may seem unfair to select student X over student Y, if Y has a higher score than X, while all other aspects of the two are identical. A similar problem can arise when applying ML in many other areas, such as loan application, criminal judgment, and recruitment. In addition to the fairness and security concerns, incorporating the monotonic property into the ML models can also help improve their interpretability, especially for the deep neural networks [22]. Last but not least, enforcing monotonicity could increase the generalization ability of the model and hence the accuracy of the predictions [10, 34], if the enforced monotonicity pattern is consistent with the underlying truth.
While incorporating monotonicity constraints has been widely studied for the traditional machine learning and statistical models for decades [e.g., 9, 8, 5, 27, 2, 21], the current challenge is how to incorporate monotonicity into complex neural networks effectively and ﬂexibly. Generally, existing approaches for learning monotonic neural networks can be categorized into two groups: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1) Hand-designed Monotonic Architectures. A popular approach is to design special neural architec-tures that guarantee monotonicity by construction [e.g., 2, 7, 10, 34]. Unfortunately, these designed monotonic architectures can be very restrictive or complex, and are typically difﬁcult to implement in practice. A further review of this line of work is provided at the end of Section 1. 2) Heuristic Monotonic Regularization. An alternative line of work focuses on enforcing monotonicity for an arbitrary, off-the-shelf neural network by training with a heuristically designed regularization (e.g., by penalizing negative gradients on the data) [13]. While this approach is more ﬂexible and easier to implement compared to the former method, it cannot provably ensure that the learned models would produce the desired monotonic response on selected features. As a result, the monotonicity constraint can be violated on some data, which may lead to costly results when deployed to solve real-world tasks.
Obviously, each line of the existing methods has its pros and cons. In this work, we propose a new paradigm for learning monotonic functions that can gain the best of both worlds: leveraging arbitrary neural architectures and provably ensuring monotonicity of the learned models. The key of our approach is an optimization-based technique for mathematically verifying, or rejecting, the monotonicity of an arbitrary piece-wise linear (e.g., ReLU) neural network. In this way, we transform the monotonicity veriﬁcation into a mixed integer linear programming (MILP) problem that can be solved by powerful off-the-shelf techniques. Equipped with our monotonicity veriﬁcation technique, we can learn monotonic networks by training the networks with heuristic monotonicity regularizations and gradually increasing the regularization magnitude until it passes the monotonicity veriﬁcation.
Empirically, we show that our method is able to learn more ﬂexible partially monotonic functions on various challenging datasets and achieve higher test accuracy than the existing approaches with best performance, including the recent Deep Lattice Network [34]. We also demonstrate the use of monotonic constraints for learning interpretable convolutional networks.