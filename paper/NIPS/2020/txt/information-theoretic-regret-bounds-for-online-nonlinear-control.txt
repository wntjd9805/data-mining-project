Abstract
This work studies the problem of sequential control in an unknown, nonlinear dy-namical system, where we model the underlying system dynamics as an unknown function in a known Reproducing Kernel Hilbert Space. This framework yields a general setting that permits discrete and continuous control inputs as well as non-smooth, non-differentiable dynamics. Our main result, the Lower Conﬁdence-based Continuous Control (LC3) algorithm, enjoys a near-optimal O(
T ) regret bound against the optimal controller in episodic settings, where T is the number of episodes. The bound has no explicit dependence on dimension of the system dy-namics, which could be inﬁnite, but instead only depends on information theoretic quantities. We empirically show its application to a number of nonlinear control tasks and demonstrate the beneﬁt of exploration for learning model dynamics.
√ 1

Introduction
The control of uncertain dynamical systems is one of the central challenges in Reinforcement Learn-ing (RL) and continuous control, and recent years has seen a number of successes in demanding sequential decision making tasks ranging from robotic hand manipulation [Todorov et al., 2012,
Al Borno et al., 2012, Kumar et al., 2016, Tobin et al., 2017, Lowrey et al., 2018, Akkaya et al., 2019] to game playing [Silver et al., 2016, Bellemare et al., 2016, Pathak et al., 2017, Burda et al., 2018]. The predominant approaches here are either based on reinforcement learning or continuous control (or a mix of techniques from both domains).
With regards to provably correct methods which handle both the learning and approximation in unknown, complex environments, and achieve optimality guarantees, the body of results in the rein-forcement learning literature [Russo and Van Roy, 2013, Jiang et al., 2017, Sun et al., 2019, Agarwal et al., 2019a] is more mature than in the continuous controls literature. In fact, only relatively re-cently has there been provably correct methods (and sharp bounds) for the learning and control of the Linear Quadratic Regulator (LQR) model [Mania et al., 2019, Simchowitz and Foster, 2020,
Abbasi-Yadkori and Szepesv´ari, 2011], arguably the most basic model due to having globally linear dynamics.
While Markov Decision Processes provide a very general framework after incorporating continuous states and actions into the model, there are a variety of reasons to directly consider learning in continuous control settings: even the simple LQR model provides a powerful framework when used for locally linear planning [Ahn et al., 2007, Todorov and Li, 2005, Tedrake, 2009, Perez et al., 2012]. More generally, continuous control problems often have continuity properties with respect
Project page: https://sites.google.com/view/lc3algorithm/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to the underlying “disturbance” (often modeled as statistical additive noise), which can be exploited for fast path planning algorithms [Jacobson and Mayne, 1970, Williams et al., 2017]; analogous continuity properties are often not leveraged in designing provably correct RL models (though there are a few exceptions, e.g. [Kakade et al., 2003]). While LQRs are a natural model for continuous control, they are prohibitive for a variety of reasons: LQRs rarely provide good global models of the system dynamics, and, furthermore, naive random search sufﬁces for sample efﬁcient learning of LQRs [Mania et al., 2019, Simchowitz and Foster, 2020] — a strategy which is unlikely to be effective for the learning and control of more complex nonlinear dynamical systems where one would expect strategic exploration to be required for sample efﬁcient learning (just as in RL, e.g. see Kearns and Singh [2002], Kakade [2003]).
This is the motivation for this line of work, where we focus directly on the sample efﬁcient learn-ing and control of an unknown, nonlinear dynamical system, under the assumption that the mean dynamics live within some known Reproducing Kernel Hilbert Space.
The Online Nonlinear Control Problem. This work studies the following nonlinear control prob-lem, where the nonlinear system dynamics are described, for h ∈ {0, 1, . . . H − 1}, by xh+1 = f (xh, uh) + (cid:15), where (cid:15) ∼ N (0, σ2I) where the state xh ∈ RdX ; the control uh ∈ U where U may be an arbitrary set (not necessarily even a vector space); f : X × U → X is assumed to live within some known Reproducing Kernel
Hilbert Space; the additive noise is assumed to be independent across timesteps.
Speciﬁcally, the model considered in this work was recently introduced in Mania et al. [2020], which we refer to as the kernelized nonlinear regulator (KNR) for the inﬁnite dimensional extension. The
KNR model assumes that f lives in the RKHS of a known kernel K. Equivalently, the primal version of this assumption is that: f (x, u) = W (cid:63)φ(x, u) for some known function φ : X × U → H where H is a Hilbert space (either ﬁnite or countably inﬁnite dimensional) and where W (cid:63) is a linear mapping. Given an immediate cost function c :
X × U → R+ (where R+ is the non-negative real numbers), the KNR problem can be described by the following optimization problem: min
π∈Π
J π(x0; c) where J π(x0; c) = E (cid:34)H−1 (cid:88) h=0 c(xh, uh) (cid:35) (cid:12) (cid:12) (cid:12)π, x0 where x0 is a given starting state; Π is some set of feasible controllers; and where a controller (or a policy) is a mapping π : X × {0, . . . H − 1} → U. We denote the best-in-class cumulative cost as
J (cid:63)(x0; c) = minπ∈Π J π(x0; c). Given any model parameterization W , we denote J π(x0; c, W ) as the expected total cost of π under the dynamics W φ(x, u) + (cid:15).
It is worthwhile to note that this KNR model is rather general in the following sense: the space of control inputs U may be either discrete or continuous; and the dynamics f need not be a smooth or differentiable function in any of its inputs. A more general version of this problem, which we leave for future work, would be where the systems dynamics are of the form xh+1 = fh(xh, uh, (cid:15)h), in contrast to our setting where the disturbance is due to additive Gaussian noise.
We consider an online version of this KNR problem: the objective is to sequentially optimize a sequence of cost functions where the nonlinear dynamics f are not known in advance. We assume that the learner knows the underlying Reproducing Kernel Hilbert Space. In each episode t, we observe an instantaneous cost function ct; we choose a policy πt; we execute πt and observe a sampled trajectory x0, u0, . . . , xH−1, uH−1; we incur the cumulative cost under ct. Our goal is to minimize the sum of our costs over T episodes. In particular, we desire to execute a policy that is nearly optimal for every episode.
A natural performance metric in this context is our cumulative regret, the increase in cost due to not knowing the nonlinear dynamics beforehand, deﬁned as:
REGRETT =
T −1 (cid:88)
H−1 (cid:88) t=0 h=0 ct(xt h, ut h) −
T −1 (cid:88) t=0
J π(x0; ct) min
π∈Π 2
h} is the observed states and {ut where {xt h} is the observed sequence of controls. A desirable asymptotic property of an algorithm is to be no-regret, i.e. the time averaged version of the regret goes to 0 as T tends to inﬁnity.
√
Our Contributions. The ﬁrst set of provable results in this setting, for the ﬁnite dimensional case and for the problem of system identiﬁcation, was provided by Mania et al. [2020]. Our work focuses on regret, and we provide the Lower Conﬁdence-based Continuous Control (LC3) algorithm, which enjoys a O(
T ) regret bound. We provide an informal version of our main result, specialized to the case where the dimension of the RKHS is ﬁnite and the costs are bounded.
Theorem 1.1 (Informal statement; ﬁnite dimensional case with bounded features). Consider the special case where: ct(x, u) ∈ [0, 1]; dφ is ﬁnite (with dX + dφ ≥ H); and φ is uniformly bounded, with (cid:107)φ(x, u)(cid:107)2 ≤ B; The LC3 algorithm enjoys the following expected regret bound: (cid:19)(cid:19) (cid:18) (cid:18)(cid:113)
E
LC3 [REGRETT ] ≤ (cid:101)O (cid:0)dX + dφ dφ (cid:1)H 3T · log 1 +
B2(cid:107)W (cid:63)(cid:107)2 2
σ2
, where (cid:101)O(·) notation drops logarithmic factors in T and H.
There are a number of notable further contributions with regards to our work:
• (Dimension and Horizon Dependencies) Our general regret bound has no explicit depen-dence on dimension of the system dynamics (the RKHS dimension), which could be inﬁ-nite, but instead only depends on information theoretic quantities; our horizon dependence is H 3, which we conjecture is near optimal. It is also worthwhile noting that our regret bound is only logarithmic in (cid:107)W (cid:63)(cid:107)2 and σ2.
• (Localized rates) In online learning, it is desirable to obtain improved rates if the loss of the
“best expert” is small, e.g. in our case, if J (cid:63)(x0; ct) is small. Under a bounded coefﬁcient of variation condition (which holds for LQRs and may hold more generally), we provide an improved regret bound whose leading term regret depends linearly on J (cid:63).
• (Moment bounds and LQRs) Our regret bound does not require bounded costs, but instead only depends on second moment bounds of the realized cumulative cost, thus making them applicable to LQRs, as a special case.
• (Empirical evaluation:) Coupled with the right features (e.g., kernels), our method pro-vides competitive results in common continuous control benchmarks, exploration tasks, and complex control problems such as dexterous manipulation.
While our techniques utilize methods developed for the analysis of linear bandits [Dani et al., 2008,
Abbasi-Yadkori et al., 2011] and Gaussian process bandits [Srinivas et al., 2009], there are a number of new technical challenges to be addressed with regards to the multi-step extension to Reinforce-ment Learning. In particular, some nuances for the more interested reader: we develop a stopping time martingale to handle the unbounded nature of the (realized) cumulative costs; we develop a novel way to handle Gaussian smoothing through the chi-squared distance function between two distributions; our main technical lemma is a “self-bounding” regret bound that relates the instanta-neous regret on any given episode to the second moment of the stochastic process.
Notation. We let (cid:107)x(cid:107)2, (cid:107)M (cid:107)2, and (cid:107)M (cid:107)F refer to the Euclidean norm, the spectral norm, and the
Frobenius norm, respectively, of a vector x and a matrix M . 2