Abstract
We study the problem of best-arm identiﬁcation with ﬁxed conﬁdence in stochastic linear bandits. The objective is to identify the best arm with a given level of cer-tainty while minimizing the sampling budget. We devise a simple algorithm whose sampling complexity matches known instance-speciﬁc lower bounds, asymptoti-cally almost surely and in expectation. The algorithm relies on an arm sampling rule that tracks an optimal proportion of arm draws, and that remarkably can be updated as rarely as we wish, without compromising its theoretical guarantees.
Moreover, unlike existing best-arm identiﬁcation strategies, our algorithm uses a stopping rule that does not depend on the number of arms. Experimental results suggest that our algorithm signiﬁcantly outperforms existing algorithms. The paper further provides a ﬁrst analysis of the best-arm identiﬁcation problem in linear bandits with a continuous set of arms. 1

Introduction
The stochastic linear bandit [1, 2] is a sequential decision-making problem that generalizes the classical stochastic Multi-Armed Bandit (MAB) problem [3, 4] by assuming that the average reward is a linear function of the arm. Linear bandits have been extensively applied in online services such us online advertisement and recommendation systems [5, 6, 7], and constitute arguably the most relevant structured bandit model in practice. Most existing analyses of stochastic linear bandits concern regret minimization [2, 8, 9, 10, 11], i.e., the problem of devising an online algorithm maximizing the expected reward accumulated over a given time horizon. When the set of arms is ﬁnite, this problem is solved in the sense that we know an instance-speciﬁc regret lower bound, and a simple algorithm whose regret matches this fundamental limit [10, 11].
The best-arm identiﬁcation problem (also referred to as pure exploration problem) in linear bandits with ﬁnite set of arms has received less attention [12, 13, 14, 15, 16], and does not admit a fully satisfactory solution. In the pure exploration problem with ﬁxed conﬁdence, one has to design a
δ-PAC algorithm (able to identify the best arm with probability at least 1 − δ) using as few samples as possible. Such an algorithm consists of a sampling rule (an active policy to sequentially select arms), a stopping rule, and a decision rule that outputs the estimated best arm. The number of rounds before the algorithm stops is referred to as its sample complexity. An instance-speciﬁc information-theoretical lower bound of the expected sample complexity has been derived in [17]. However, we are lacking simple and practical algorithms achieving this bound. Importantly, existing algorithms exhibit scalability issues as they always include subroutines that explicitly depend on the number of arms (refer to the related work for details). They may also be computationally involved.
In this paper, we present a new best-arm identiﬁcation algorithm for linear bandits with ﬁnite set of arms, whose sample complexity matches the information-theoretical lower bound. The algorithm follows the track-and-stop principle proposed in [18] for pure exploration in bandits without structure.
Its sampling rule tracks the optimal proportion of arm draws, predicted by the sample complexity lower bound and estimated using the least-squares estimator of the system parameter. Remarkably, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
this tracking procedure can be made as lazy as we wish (the estimated optimal proportion of draws can be updated rarely – not every round) without compromising the asymptotic optimality of the algorithm. The stopping rule of our algorithm is classically based on a generalized likelihood ratio test. However the exploration threshold deﬁning its stopping condition is novel, and critically, we manage to make it independent of the number of arms. Overall our algorithm is simple, scalable, and yet asymptotically optimal. In addition, its computational complexity can be tuned by changing the frequency at which the tracking rule is updated, without affecting its theoretical guarantees.
We also study the pure exploration problem in linear bandits with a continuous set of arms. We restrict our attention to the case where the set of arms consists of the (d − 1)-dimensional unit sphere. We establish a sample complexity lower bound satisﬁed by any ((cid:15), δ)-PAC algorithm (such algorithms identify an (cid:15)-optimal arm with probability at least 1 − δ). This bound scales as d
ε log(1/δ). We ﬁnally propose an algorithm whose sample complexity matches the lower bound order-wise.
µ log(K 2/δ))(log log(K 2/δ) + log(1/∆2