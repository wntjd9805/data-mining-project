Abstract
N
Imitation learning (IL) aims to mimic the behavior of an expert policy in a sequen-tial decision-making problem given only demonstrations. In this paper, we focus on understanding the minimax statistical limits of IL in episodic Markov Decision
Processes (MDPs). We ﬁrst consider the setting where the learner is provided a dataset of N expert trajectories ahead of time, and cannot interact with the MDP.
Here, we show that the policy which mimics the expert whenever possible is in expectation (cid:46) |S|H 2 log(N ) suboptimal compared to the value of the expert, even when the expert plays a stochastic policy. Here S is the state space and H is the length of the episode. Furthermore, we establish a suboptimality lower bound of (cid:38) |S|H 2/N which applies even if the expert is constrained to be deterministic, or if the learner is allowed to actively query the expert at visited states while interacting with the MDP for N episodes. To our knowledge, this is the ﬁrst algorithm with suboptimality having no dependence on the number of actions, under no additional assumptions. We then propose a novel algorithm based on minimum-distance functionals in the setting where the transition model is given and the expert is deterministic. The algorithm is suboptimal by (cid:46) |S|H 3/2/N , matching our lower
H factor, and breaks the O(H 2) error compounding barrier of IL. bound up to a
√ 1

Introduction
Imitation learning or apprenticeship learning is the study of learning from demonstrations in a sequential decision-making framework in the absence of reward feedback. The imitation learning problem differs from the typical setting of reinforcement learning in that the learner no longer has access to reward feedback to learn a good policy. In contrast, the learner is given access to expert demonstrations, with the objective of learning a policy that performs comparably to the expert’s with respect to the unobserved reward function. This is motivated by the fact that the desired behavior in typical reinforcement learning problems is easy to specify in words, but hard to capture accurately through manually-designed rewards [AN04]. Imitation learning has shown remarkable success in practice over the last decade - the work of [ACQN07] showed that using pilot demonstrations to learn the dynamics and infer rewards can signiﬁcantly improve performance in autonomous helicopter
ﬂight. More recently, the approach of learning from demonstrations has shown to improve the state-of-the-art in numerous areas: autonomous driving [HVP+18, PCS+20], robot control [ACVB09], game AI [ILP+18, Vin19] and motion capture [MTD+17] among others. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Following the approach pioneered by [BDH+05], several works [RB10, BSH20] show that carrying out supervised learning (among other approaches) to learn a policy provides black box guarantees on the suboptimality of the learner, in effect “reducing” the IL problem to supervised learning.
In particular when the expert follows a deterministic policy, [RB10] discuss the behavior cloning approach, which is a supervised learning method for imitation learning by minimizing the number of mistakes made by the learner compared to the expert under the empirical state distribution in the expert demonstrations. However, the authors conclude that supervised learning could lead to severe error compounding due to the “covariate shift problem”: the actual performance of learner depends on its own state distribution, whereas training takes place with respect to the expert’s state distribution.
Furthermore, it remains to see how the reduction approach fares when the expert follows a general stochastic policy. As we discuss later, in this setting, the reduction analysis is in fact loose and can be improved: we instead use a novel coupling based approach to provide near optimal guarantees.
Nevertheless, the aforementioned reduction approach is quite popular in studying IL and shows that it sufﬁces to approximately solve an intermediate problem to give one directional bounds on the suboptimality of a learner. But it is unclear whether a difﬁculty in solving the intermediate problem implies an inherent difﬁculty in solving the original imitation learning problem. In this work, we cast the imitation learning problem in the statistical decision theory framework and ask,
What are the statistical limits of imitation learning?
We investigate this question in a tabular, epsiodic MDP over state space S, action space A and episode length H, one of the most basic settings to start with. The value J(π) of a (possibly stochastic) policy
π is deﬁned as the expected cumulative reward accrued over the duration of an episode,
J(π) = Eπ (cid:20)(cid:88)H t=1 (cid:21) rt(st, at) (1) where rt is the unknown reward function of the MDP at time t, and the expectation is computed with respect to the distribution over trajectories {(s1, a1), · · · , (sH , aH )} induced by rolling out the policy
π. Denoting the expert’s policy by π∗ and the learner’s policy (cid:98)π, the subopimality of a learner, which is a random variable, is the difference in value of the expert’s and learner’s policies: J(π∗) − J((cid:98)π).
In the imitation learning framework, we emphasize that the learner does not observe rewards while interacting with the MDP, but is given access to expert demonstrations to learn a good policy. The learner is said to “interact” with the MDP by submitting a state-action pair to an oracle and receiving the next state, with the reward being hidden. We study the problem in the following 3 settings: (a) No-interaction: The learner is provided a dataset of N independent rollouts of the expert policy. The learner is not otherwise allowed to interact with the MDP. (b) Known-transition: The only difference compared to the no-interaction setting is that the
MDP state transition functions and initial state distribution are exactly known to the learner. (c) Active: The learner is not given a dataset of expert demonstrations in advance. However, the learner is allowed to actively query the expert using previous expert feedback while interacting with the MDP for N episodes.
The active setting gives the learner more power than the no-interaction setting: it can just follow the observed expert actions (same as no-interaction setting), or actively compute a new policy on the ﬂy based on expert feedback, and then use the most up-to-date policy to interact with the MDP. DAGGER
[RGB11] and AGGRAVATE [RB14] are popular approaches proposed for IL in the active setting. 1.1 Main results
The reduction approach in [RB10] shows that if the expert policy is deterministic, and the probability of error in guessing the expert’s action at each state is (cid:15), then J(π∗) − J((cid:98)π) (cid:46) min{H, (cid:15)H 2}. The appearance of this H 2 factor is called error compounding which can be intuitively understood as the inability to get back on track once the learner makes a mistake. Indeed the basis for this argument is that if at each time the learner makes an error at time t with probability (cid:15)t, and gets lost thereafter, incurring an error of H − t + 1, the error incurred is (cid:46) H(cid:15)1 + (H − 1)(cid:15)2 + · · · + (cid:15)H (cid:46) min{H, H 2(cid:15)} where (cid:15) = (cid:15)1+···+(cid:15)H is the probability of error averaged across time. This informal argument supports
H 2
the reduction of imitation learning to the problem of minimizing the empirical probability of error, known as “behavior cloning”.
Is this error compounding inevitable or is it just a consequence of the behavior cloning algorithm?
Our ﬁrst contribution shows that it is fundamental to the imitation learning problem without additional assumptions: even if the learner operates in the active setting and the expert is deterministic, no algorithm can beat the H 2 barrier. The term instance is used to refer to the underlying MDP and the expert’s policy in the imitation learning problem.
Theorem (informal) 1.1 (Formal version: Theorem 5.1 (a)). In the active setting, for any learner (cid:98)π, there exists an instance such that the suboptimality of the learner is at least |S|H 2/N up to universal constants, i.e., J(π∗) − E[J((cid:98)π)] (cid:38) |S|H 2/N . This lower bound applies even if the expert’s policy
π∗ is constrained to be deterministic.
The key intuition behind this result is to identify that at states which were never visited during the interactions with the MDP, the learner has no prior knowledge about the expert’s policy or what state transitions are induced under different actions. With no available information, the learner is essentially forced to play an arbitrary policy on these states. Indeed, for (cid:15) which is a function of |S| and N , we construct an instance such that for any learner, the probability of visiting at time t a state unseen during the interactions is (cid:15)(1 − (cid:15))t−1. Upon visiting such a state, the learner gets completely lost thereafter with constant probability, and incurs a loss of H − t. The suboptimality is therefore (cid:38) (H − 1)(cid:15) + (H−2)(cid:15)(1−(cid:15)) + · · · + (cid:15)(1−(cid:15))H−2 (cid:38) min{H, H 2(cid:15)}.
Our next result shows that if the expert is deterministic, one can in fact achieve the bound |S|H 2/N in the no-interaction setting by the behavior cloning algorithm:
Theorem (informal) 1.2 (Formal version: Theorem 3.2 (a)). When the expert’s policy is deterministic, in the no-interaction setting, the expected suboptimality of a learner (cid:98)π that carries out behavior cloning satisﬁes J(π∗) − E[J((cid:98)π)] (cid:46) |S|H 2/N on any instance.
We prove this result exactly as stated: by ﬁrst bounding the population 0-1 risk of the policy to be
|S|/N and subsequently invoking the black box reduction [RB10, Theorem 2.1] to get the ﬁnal bound on the expected suboptimality of the learner.
The optimality of behavior cloning and Theorem (informal) 1.1 point to an interesting observation: the ability to actively query the expert does not improve the minimax expected suboptimality beyond the no-interaction setting. An important implication of this result is that DAGGER [RGB11] and other algorithms that actively query an expert, cannot improve over behavior cloning in the worst case. However, we remark that our bounds are worst-case, and does not imply that behavior cloning outperforms other existing algorithms on more structured instances observed in practice.
The closest relative to our bounds on behavior cloning in the deterministic expert setting is [SVBB19] the authors of which propose the FAIL algorithm. When the expert’s policy is deterministic, FAIL is suboptimal by (cid:46) (cid:112)|S||A|H 5/N (ignoring logarithmic factors). In contrast, in Theorem 3.2 (a), we show that behavior cloning is suboptimal by (cid:46) |S|H 2/N which always improves on the guarantee of
FAIL: not only is it independent of |A|, but has optimal dependence on H and N . However, note that the two results apply in slightly different settings and are not directly comparable: FAIL applies in the ILFO setting where the learner does not observe the actions played by the expert in the set of demonstrations, and only observes the visited states. The ILFO setting assumes that the reward function of the MDP does not depend on the action chosen at a state.
Prompted by the success of behavior cloning in the deterministic expert setting, it is natural to ask whether supervised learning reduction continues to be a good approach when the expert is stochastic.
The reduction from [RB10] indeed still guarantees that any policy with total variation (TV) distance (cid:15) with expert’s action distribution has suboptimality (cid:46) H 2(cid:15) (see Lemma A.4). However there is a problem with invoking such a reduction to bound the suboptimality of the learner: the empirical action distribution at each state converges very slowly to the population expert action distribution under TV distance. Seeing as it corresponds to matching the expert’s and learner’s policies at different states which are distributions over A, the population risk suffers from a convergence rate dependent on the number of actions, with rate (cid:112)|A|/N instead of N −1. In order to prove tight guarantees on the expected suboptimality of a policy, we are therefore forced to circumvent the reduction framework.
We analyze the MIMIC-EMP policy in this setting, which carries out empirical risk minimization under log loss. This is a natural extension of empirical risk minimization under 0-1 loss to the 3
Expert
Setting
Upper bound
Det.
No-interaction
Active (cid:26)
Known-transition min
Non. Det. No-interaction
Active
Known-transition
|S|H 2
|S|H 2
N (Thm. 3.2 (a))
N (Thm. 3.2 (a)) (cid:27) (cid:113) |S|H 2
N (Thm. 4.1 (a))
|S|H 3/2
N ,
|S|H 2 log(N )
N
|S|H 2 log(N )
N
|S|H 2 log(N )
N (Thm. 3.3) (Thm. 3.3) (Thm. 3.3)
Lower bound
|S|H 2
N (Thm. 5.1 (a))
|S|H 2
N (Item 5.1 (a))
|S|H
N (Thm. 5.1 (b))
|S|H 2
|S|H 2
N (Thm. 5.1 (a))
N (Thm. 5.1 (a))
|S|H
N (Thm. 5.1 (b))
Table 1: Minimax expected suboptimality in different settings (bounds are up to universal constants) stochastic expert setting. The namesake for this policy follows from the fact that minimizing the empirical risk under log loss precisely translates to the learner playing the empirical expert policy distribution at states observed in the expert dataset. It is interesting to note that when the expert is determinstic, MIMIC-EMP indeed still minimizes the empirical 0-1 risk to 0 and continues to be optimal in this setting. We show that when the expert is stochastic, the expected suboptimality of
MIMIC-EMP does not depend on the number of actions. Moreover from the lower bound in Theorem (informal) 1.1, it is in fact minimax optimal up to logarithmic factors.
Theorem (informal) 1.3 (Formal version: Theorem 3.3). In the no-interaction setting, the expected suboptimality of a learner (cid:98)π carrying out MIMIC-EMP is upper bounded by J(π∗) − E[J((cid:98)π)] (cid:46)
|S|H 2 log(N )/N on any instance. This result applies even when the expert plays a stochastic policy.
The main ingredient in the proof of this result is a coupling argument which shows that the expected suboptimality of the learner results only from trajectories where the learner visits states unobserved in the expert dataset, and carefully bounding the probability of this event.
We next discuss the setting where the learner is not only provided expert demonstrations, but the state transitions functions of the MDP. The “known-transition” model appears frequently in robotics applications [ZWM+18], capturing the scenario where the learner has access to accurate models / simulators representing the dynamics of the system, but the rewards of the experts are difﬁcult to summarize. Our key contribution here is to propose the MIMIC-MD algorithm which breaks the lower bound in Theorem (informal) 1.1 and suppresses the issue of error compounding which the covariate shift problem entails. Recent works [BSH20] propose algorithms that claim to bypass the covariate shift problem. However to the best of our knowledge, this is the ﬁrst result that provably does so in the general tabular MDP setting without additional assumptions.
Theorem (informal) 1.4 (Formal version: Theorem 4.1 (a)). In the known-transition setting, if the expert is deterministic, the expected suboptimality of a learner (cid:98)π playing MIMIC-MD is bounded by
J(π∗) − E[J((cid:98)π)] (cid:46) min{H(cid:112)|S|/N , |S|H 3/2/N }.
The novel element of MIMIC-MD is a hybrid approach which mimics the expert on some states, and uses a minimum distance (MD) functional [Yat85, DL88] to learn a policy on the remaining states. The minimum distance functional approach was recently considered in [SVBB19], proposing to sequentially learn a policy by approximately minimizing a notion of discrepancy between the learner’s state distribution and the expert’s empirical state distribution. We remark that our approach is fundamentally different from matching the state distributions under the expert and learner policy: it crucially relies on exactly mimicking the expert actions on states visited in the dataset, and only applying the MD functional on the remaining states.
Next we establish a lower bound on the error of any algorithm in the known-transition setting.
Theorem (informal) 1.5 (Formal version: Theorem 5.1 (b)). In the known-transition setting, for any learner (cid:98)π, there exists an instance such that the expected suboptimality J(π∗) − E[J((cid:98)π)] (cid:38) |S|H/N .
This result applies even if the expert is constrained to be deterministic.
Note that our suboptimality lower bounds corresponding to the no-interaction, active and known-transition settings (Informal Theorems 1.1 and 1.5) are universal and apply for any learner’s policy (cid:98)π. In contrast, the lower bound example in [RB10] constructs an MDP showing that there exists a 4
particular learner policy which plays an action different than the expert with probability (cid:15) and has suboptimality (cid:38) H 2(cid:15). However, it turns out that the suboptimality incurred by behavior cloning is exactly 0 on the example provided in [RB10], given just a single expert trajectory. 1.2 Organization
In Section 1.3 we review related literature. In Section 2, we introduce necessary notations and deﬁnitions. In Section 3 we discuss the performance of behavior cloning and MIMIC-EMP in the no-interaction setting. In Section 4 we introduce and discuss our novel algorithm MIMIC-MD in the known transition setting. In Section 5 we complement the discussion in the previous sections with lower bounds. Technical proofs of our results are deferred to the supplementary material. 1.3