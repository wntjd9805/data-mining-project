Abstract
The principle of “optimism in the face of uncertainty” underpins many theoretically successful reinforcement learning algorithms. In this paper we provide a general framework for designing, analyzing and implementing such algorithms in the episodic reinforcement learning problem. This framework is built upon Lagrangian duality, and demonstrates that every model-optimistic algorithm that constructs an optimistic MDP has an equivalent representation as a value-optimistic dynamic programming algorithm. Typically, it was thought that these two classes of algo-rithms were distinct, with model-optimistic algorithms beneﬁting from a cleaner probabilistic analysis while value-optimistic algorithms are easier to implement and thus more practical. With the framework developed in this paper, we show that it is possible to get the best of both worlds by providing a class of algorithms which have a computationally efﬁcient dynamic-programming implementation and also a simple probabilistic analysis. Besides being able to capture many existing algo-rithms in the tabular setting, our framework can also address large-scale problems under realizable function approximation, where it enables a simple model-based analysis of some recently proposed methods. 1

Introduction
Reinforcement learning (RL) is a key framework for sequential decision-making under uncertainty
[45, 46]. In an RL problem, a learning agent interacts with a reactive environment by taking a series of actions. Each action provides the agent with some reward, but also takes them to a new state which determines their future rewards. The aim of the agent is to pick actions to maximize their total reward in the long run. The learning problem is typically modeled by a Markov Decision Process (MDP, [40]) where the agent does not know the rewards or transition probabilities. Dealing with this lack of knowledge is a crucial challenge in reinforcement learning: the agent must maximize their rewards while simultaneously learning about the environment. One class of algorithms that have been successful at balancing this exploration versus exploitation trade-off are optimistic reinforcement learning algorithms. In this paper, we provide a new framework for studying this class of algorithms.
Optimistic algorithms are built upon the principle of “optimism in the face of uncertainty” (OFU).
They operate by maintaining a set of statistically plausible models of the world, and selecting actions to maximize the returns in the best plausible world. Such algorithms were ﬁrst studied in the context of multi-armed bandit problems [29, 2, 14, 5, 30], and went on to inspire numerous algorithms for reinforcement learning. A closer look at the literature reveals two main approaches to incorporate optimism into RL. In the ﬁrst, optimism is introduced through estimates of the MDP: these approaches build a set of plausible MDPs by constructing conﬁdence bounds around the empirical transition and
∗This work was done while CPB was at Universitat Pompeu Fabra and Barcelona Graduate School of
Economics. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
reward functions, and select the policy that generates the highest total expected reward in the best feasible MDP. We refer to this family of methods as model-optimistic. Examples of model-optimistic methods include RMAX [13, 27, 47] and UCRL2 [4, 24, 44]. While conceptually appealing, model-optimistic methods tend to be difﬁcult to implement due to the complexity of jointly optimizing over models and policies. Another approach to incorporating optimism into RL is to construct optimistic upper bounds on the optimal value functions which are (informally) the total expected reward of the optimal policy in the true MDP. The optimistic policy greedily picks actions to maximize the optimistic values. We refer to this class of methods as value-optimistic. Examples of algorithms in this class are MBIE-EB [44], UCB-VI [6] and UBEV [16]. These algorithms compute the optimistic value functions via dynamic programming (cf. 9), making them computationally efﬁcient and compatible with empirically successful RL algorithms that are typically based on value functions. One downside of these approaches is that their probabilistic analysis is often excessively complex, with complicated recursive arguments necessary to guarantee optimism.
While these two approaches may look very different on the surface, we show in this paper that there is in fact a very strong connection between them. Our ﬁrst contribution is to show that the optimization problems associated with these two approaches exhibit strong duality. This implies that that for every model-optimistic approach, there exists an equivalent value-optimistic approach. This bridges the gap between the conceptually simple model-optimistic approaches and the computationally efﬁcient value-optimistic approaches. This result enables us to develop a general framework for designing, analyzing and implementing optimistic algorithms in the episodic reinforcement learning problem.
Our framework is broad enough to capture many existing algorithms for tabular MDPs, and for these we provide a simple analysis and computationally efﬁcient implementation. The framework can also be extended to incorporate realizable linear function approximation, where it leads to a new model-based analysis of two value-optimistic algorithms. Our analysis involves constructing a new model-optimistic formulation for factored linear MDPs which may be of independent interest. 2