Abstract
To guide behavior, the brain extracts relevant features from high-dimensional data streamed by sensory organs. Neuroscience experiments demonstrate that the pro-cessing of sensory inputs by cortical neurons is modulated by instructive signals which provide context and task-relevant information. Here, adopting a norma-tive approach, we model these instructive signals as supervisory inputs guiding the projection of the feedforward data. Mathematically, we start with a family of Reduced-Rank Regression (RRR) objective functions which include Reduced
Rank (minimum) Mean Square Error (RRMSE) and Canonical Correlation Anal-ysis (CCA), and derive novel ofﬂine and online optimization algorithms, which we call Bio-RRR. The online algorithms can be implemented by neural networks whose synaptic learning rules resemble calcium plateau potential dependent plas-ticity observed in the cortex. We detail how, in our model, the calcium plateau potential can be interpreted as a backpropagating error signal. We demonstrate that, despite relying exclusively on biologically plausible local learning rules, our algorithms perform competitively with existing implementations of RRMSE and CCA. 1

Introduction
In the brain, extraction of behaviorally-relevant features from high-dimensional data streamed by sensory organs occurs in multiple stages. Early stages of sensory processing, e.g., the retina, lack feedback and are naturally modeled by unsupervised learning algorithms [1]. In contrast, subsequent processing by cortical circuits is modulated by instructive signals from other cortical areas [2], which provide context and task-related information [3], thus calling for supervised learning models.
Unsupervised models of early sensory processing, despite employing many simplifying assumptions, have successfully bridged the salient features of biological neural networks, such as the architecture, synaptic learning rules and receptive ﬁeld structure, with computational tasks such as dimensionality reduction, decorrelation, and whitening [4, 5, 6, 7, 8]. The success of such models was driven by two major factors. First, following a normative framework, their synaptic learning rules, network architecture and activity dynamics were derived by optimizing a principled objective, leading to an analytic understanding of the circuit computation without the need for numerical simulation [9].
Second, these models went beyond purely theoretical explorations by appealing to and explaining various experimental observations of early sensory organs available at the time [5, 8, 9]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In contrast to early sensory processing, subsequent processing in the cortex (both neocortex [10, 2, 11, 12, 13] and hippocampus [14, 15, 16, 17]) is guided by supervisory signals. In particular, in cortical pyramidal neurons, proximal dendrites receive and integrate feedforward inputs leading to the generation of action potentials (i.e., the output of the neuron). The distal dendrites of the apical tuft, in contrast, receive and integrate instructive signals resulting in local depolarization. When the local depolarization is large relative to inhibitory currents, this generates a calcium plateau potential that propagates throughout the entire neuron. If the calcium plateau coincides with feedforward input, it strengthens corresponding proximal synapses, thereby providing an instructive signal in these circuits [12, 11, 15, 16].
In this work, we model cortical processing as a projection of feedforward sensory input that is modulated by instructive signals from other cortical areas. Inspired by the success of the normative approach in early sensory processing, we adopt it here. Mathematically, the projections of sensory input can be learned by minimizing the prediction error or maximizing the correlation of the pro-jected input with the instructive signal. These correspond to two instances of the Reduced-Rank
Regression (RRR) objectives: Reduced-Rank (minimum) Mean Square Error (RRMSE) [18] and
Canonical Correlation Analysis (CCA) [19].
To serve as a viable model of brain function, an algorithm must satisfy at least the following two criteria [9]. First, because sensory inputs are streamed to the brain and require real-time processing, it must be modeled by an online learning algorithm that does not store any signiﬁcant fraction of the data. To satisfy this requirement, unlike standard ofﬂine formulations, which output projection matrices, at each time step, the algorithm must compute the projection from the input of that time step. The projection matrices are updated at each time step and can be represented in synaptic weights. Second, a neural network implementation of such an algorithm must rely exclusively on local synaptic learning rules. Here, locality means that the plasticity rules depend exclusively on the variables available to the biological synapse, i.e., the physicochemical activities of the pre- and post-synaptic neurons in the synaptic neighborhood. The Hebbian update rule is an example of local learning, where the change of synaptic weight is proportional to the correlation between the output activities of the pre- and post-synaptic neurons [20].
Contributions
•
•
•
We derive novel algorithms for a family of RRR problems, which include RRMSE and
CCA, and implement them in biologically plausible neural networks that resemble cortical micro-circuits.
We demonstrate within the conﬁnes of our model how the calcium plateau potential in cortical microcircuits encodes a backpropagating error signal.
We show numerically on a real-world dataset that our algorithms perform competitively compared with current state-of-the-art algorithms. 2