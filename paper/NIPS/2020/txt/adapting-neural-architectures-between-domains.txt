Abstract
Neural architecture search (NAS) has demonstrated impressive performance in automatically designing high-performance neural networks. The power of deep neural networks is to be unleashed for analyzing a large volume of data (e.g.
ImageNet), but the architecture search is often executed on another smaller dataset (e.g. CIFAR-10) to ﬁnish it in a feasible time. However, it is hard to guarantee that the optimal architecture derived on the proxy task could maintain its advantages on another more challenging dataset. This paper aims to improve the generalization of neural architectures via domain adaptation. We analyze the generalization bounds of the derived architecture and suggest its close relations with the validation error and the data distribution distance on both domains. These theoretical analyses lead to AdaptNAS, a novel and principled approach to adapt neural architectures between domains in NAS. Our experimental evaluation shows that only a small part of ImageNet will be sufﬁcient for AdaptNAS to extend its architecture success to the entire ImageNet and outperform state-of-the-art comparison algorithms. 1

Introduction
Neural architecture search (NAS) is to automate the design of neural architectures for networks.
Recently, convolutional neural networks (CNNs) designed by NAS methods have already reached better performance than those manually designed ones on ImageNet. However, early NAS methods are computationally intensive, because of their demand for training and evaluation of a large number of architectures [20, 26]. This, therefore, makes it intractable to directly conduct the architecture search on large-scale benchmarks like ImageNet. As a trade-off, many NAS methods search on proxy tasks, such as CIFAR-10, and then retrain obtained architectures on ImageNet. However, even though on CIFAR-10, it is common for most methods to cost thousands of GPU days.
In the past years, great efforts were undertaken to signiﬁcantly reduce the architecture search cost.
Remarkably, the line of research on the differentiable manner for architecture search [17] have reduced the search cost dramatically to several GPU days or several GPU hours on the CIFAR-10 dataset. DARTS [17] relaxes discrete NAS search space as continuous architecture parameters and constructs a super network by weighted mixing of all candidate operations. In the super network, network weights and architecture parameters can be jointly optimized with gradient descent. The search cost of DARTS on CIFAR-10 is only 1 GPU day, but the parallel optimization of all candidate operations demands large GPU memory. GDAS [7] tackles this issue by using a differentiable sampler and sampling one operation per connection in each epoch. This dramatically reduces the usage of
GPU memory, and the search can be completed within about 4 to 5 GPU hours depending on the setting. As GDAS reduces the width of the super network, P-DARTS [6] starts with a shallow super network and progressively increases its depth. This method slightly increases the search cost to about 7 GPU hours but can reach a better test performance. Besides direct reducing the training cost, CARS
[24] proposes a novel efﬁcient continuous evolutionary approach based on the historical evaluation. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Similarly, PVLL-NAS [16] schedules their evaluation with a performance estimator, who samples neural architectures for both architecture searching and iterative training of the estimator itself.
However, due to the inconsistent performance of architectures on different domains, searching on proxy tasks and then reusing the obtained architectures on large-scale benchmarks has become a rut, which may result in a huge generalization gap of the neural architecture on the two different domains.
This generalization gap could be either positive or negative, but reﬂecting in practice, it would cause either omitting of good architectures or choosing of poor architectures and make the performance on the desired domain uncontrollable.
A few attempts are trying to break out of the rut by directly executing the architecture search on
ImageNet. MnasNet [21] was established within the framework of reinforcement learning and costed 288 TPU days for one search on ImageNet. By applying the differentiable NAS techniques,
ProxylessNAS [5] binarized architectures to boost search speed and reduced the search cost on
ImageNet to 8.33 GPU day, but it is still about 28 to 52 times slower than its counterparts [7, 6, 23, 25] on CIFAR-10. NAS on CIFAR-10 is fast but deploying the searched architecture on ImageNet will receive the accuracy ﬂuctuation; directly searching on ImageNet is slow but its architecture accuracy can be guaranteed. If it is infeasible to swallow the entire ImageNet, we ask whether a smaller part of
ImageNet on top of the efﬁcient NAS on CIFAR-10 would rescue us from this dilemma.
In this paper, we consider the inconsistency in the generalization of architectures from a new perspective of adapting neural architectures between domains. The proxy task such as CIFAR-10 for searching is considered as the source domain, and the large-scale benchmark such as ImageNet to deploy searched architectures for testing or application is our aiming target domain. Firstly, the relationship between the empirical source validation error and the expected target error of neural architectures is analyzed. Since NAS approaches typically optimize network weights during the training phase and then search for architectures during the validation phase, it is meaningful to ﬁnd a generalization bound by validation. Two versions of the generalization bound are proposed. One associates with the source validation error, while another introduces an additional target validation error calculated on a subset of target samples. Based on them, we propose a lightweight method to explicitly minimize the cross-domain generalization gap of neural architectures during NAS. We name it as Adaptable Neural Architecture Search (AdaptNAS). The generalisability and efﬁciency of AdaptNAS are demonstrated with extensive experiments. On the three digits dataset, we show that AdaptNAS generalizes better than baselines without generalization constraint. Then, large-scale experiments are performed on CIFAR-10 and ImageNet and compared with different state-of-the-art
NAS methods that search either with proxy tasks or directly on ImageNet. 2