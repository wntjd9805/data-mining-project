Abstract
A common problem in neuroscience is to elucidate the collective neural representa-tions of behaviorally important variables such as head direction, spatial location, upcoming movements, or mental spatial transformations. Often, these latent vari-ables are internal constructs not directly accessible to the experimenter. Here, we propose a new probabilistic latent variable model to simultaneously identify the latent state and the way each neuron contributes to its representation in an unsupervised way. In contrast to previous models which assume Euclidean latent spaces, we embrace the fact that latent states often belong to symmetric manifolds such as spheres, tori, or rotation groups of various dimensions. We therefore propose the manifold Gaussian process latent variable model (mGPLVM), where neural responses arise from (i) a shared latent variable living on a speciﬁc manifold, and (ii) a set of non-parametric tuning curves determining how each neuron con-tributes to the representation. Cross-validated comparisons of models with different topologies can be used to distinguish between candidate manifolds, and variational inference enables quantiﬁcation of uncertainty. We demonstrate the validity of the approach on several synthetic datasets, as well as on calcium recordings from the ellipsoid body of Drosophila melanogaster and extracellular recordings from the mouse anterodorsal thalamic nucleus. These circuits are both known to encode head direction, and mGPLVM correctly recovers the ring topology expected from neural populations representing a single angular variable. 1

Introduction
The brain uses large neural populations to represent low-dimensional quantities of behavioural relevance such as location in physical or mental spaces, orientation of the body, or motor plans. It is therefore common to project neural data into smaller latent spaces as a ﬁrst step towards linking neural activity to behaviour (Cunningham and Byron, 2014). This can be done using a variety of linear methods such as PCA or factor analysis (Cunningham and Ghahramani, 2015), or non-linear dimensionality reduction techniques such as tSNE (Maaten and Hinton, 2008). Many of these methods are explicitly probabilistic, with notable examples including GPFA (Yu et al., 2009) and
LFADS (Pandarinath et al., 2018). However, all these models project data into Euclidean latent spaces, thus failing to capture the inherent non-Euclidean nature of variables such as head direction or rotational motor plans (Chaudhuri et al., 2019; Finkelstein et al., 2015; Seelig and Jayaraman, 2015; Wilson et al., 2018). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Schematic illustration of the manifold Gaussian process latent variable model
Figure 1: (mGPLVM). In the generative model (left), neural activity arises from (i) M latent states {gj} on a manifold M, each corresponding to a different condition j (e.g. time or stimulus), and (ii) the tuning curves of N neurons, modelled as Gaussian processes and sharing the same latent states {gj} as inputs. Using variational inference, mGPLVM jointly infers the global latent states and the tuning curve of each neuron on the manifold (right).
Most models in neuroscience justiﬁably assume that neurons are smoothly tuned (Stringer et al., 2019). As an example, a population of neurons representing an angular variable θ would respond similarly to some θ and to θ + (cid:15) (for small (cid:15)). While it is straigthforward to model such smoothness by introducing smooth priors for response functions deﬁned over R, the activity of neurons modelled this way would exhibit a spurious discontinuity as the latent angle changes from 2π to 0 + (cid:15). We see that appropriately modelling smooth neuronal representations requires keeping the latent variables of interest on their natural manifold (here, the circle), instead of an ad-hoc Euclidean space. While periodic kernels have commonly been used to address such problems in GP regression (MacKay, 1998), topological structure has not been incorporated into GP-based latent variable models due to the difﬁculty of doing inference in such spaces.
Here, we build on recent advances in non-Euclidean variational inference (Falorsi et al., 2019) to develop the manifold Gaussian process latent variable model (mGPLVM), an extension of the
GPLVM framework (Lawrence, 2005; Titsias and Lawrence, 2010; Wu et al., 2018, 2017) to non-Euclidean latent spaces including tori, spheres and SO(3) (Figure 1). mGPLVM jointly learns the
ﬂuctuations of an underlying latent variable g and a probabilistic “tuning curve” p(fi|g) for each neuron i. The model therefore provides a fully unsupervised way of querying how the brain represents its surroundings and a readout of the relevant latent quantities. Importantly, the probabilistic nature of the model enables principled model selection between candidate manifolds. We provide a framework for scalable inference and validate the model on both synthetic and experimental datasets. 2 Manifold Gaussian process latent variable model
The main contribution of this paper is mGPLVM, a Gaussian process latent variable model (Titsias and Lawrence, 2010; Wu et al., 2018) deﬁned for non-Euclidean latent spaces. We ﬁrst present the generative model (Section 2.1), then explain how we perform approximate inference using reparameterizations on Lie groups (Falorsi et al., 2019; Section 2.2). Lie groups include Euclidean vector spaces Rn as well as other manifolds of interests to neuroscience such as tori T n (Chaudhuri et al., 2019; Rubin et al., 2019) and the special orthogonal group SO(3) (Finkelstein et al., 2015;
Wilson et al., 2018; extensions to non-Lie groups are discussed in Appendix D). We then provide speciﬁc forms for variational densities and kernels on tori, spheres, and SO(3) (Section 2.3). Finally we validate the method on both synthetic data (Section 3.1), calcium recordings from the fruit ﬂy head direction system (Section 3.2), and extracellular recordings from the mouse anterodorsal thalamic nucleus (Appendix A). 2.1 Generative model
We use xij to denote the individual elements of a matrix X. Let Y ∈ RN ×M be the activity of N neurons recorded in each of M conditions. Examples of “conditions” include time within a trial, 2
stimulus identity, or motor output. We assume that all neuronal responses collectively encode a shared, condition-speciﬁc latent variable gj ∈ M, where M is some manifold. We further assume that each neuron i is tuned to the latent state g with a “tuning curve” fi(g), describing its average response conditioned on g. Rather than assuming a speciﬁc parametric form for these tuning curves, we place a Gaussian process prior on fi(·) to capture the heterogeneity widely observed in biological systems (Churchland and Shenoy, 2007; Hardcastle et al., 2017). The model is depicted in Figure 1 and can be formally described as: gj ∼ pM(g) fi ∼ GP(0, kM i (·, ·)) yij|gj ∼ N (fi(gj), σ2 i ) (3)
In Equation 1, we use a uniform prior pM(g) inversely proportional to the volume of the manifold for bounded manifolds (Appendix B), and a Gaussian prior on Euclidean spaces to set a basic i (·, ·) : M × M → R is a covariance function deﬁned on manifold M lengthscale. In Equation 2, kM – manifold-speciﬁc details are discussed in Section 2.3. In the special case where M is a Euclidean space, this model is equivalent to the standard Bayesian GPLVM (Titsias and Lawrence, 2010). While
Equation 3 assumes independent noise across neurons, noise correlations can also be introduced as in (Wu et al., 2018) and Poisson noise as in (Wu et al., 2017). (prior over tuning curves) (prior over latents) (noise model) (1) (2)
This probabilistic model can be ﬁtted by maximizing the log marginal likelihood log p(Y ) = log p(Y |{fi}, {gj}) p({fi}) pM({gj}) d{fi}d{gj}. (4) (cid:90)
Following optimization, we can query both the posterior over latent states p({gj}|Y ) and the posterior predictive distribution p(Y (cid:63)|G(cid:63), Y ) at a set of query states G(cid:63). While it is possible to marginalise out fi when the states {gj} are known, further marginalising out {gj} is intractable and maximizing
Equation 4 requires approximate inference. 2.2 Learning and inference
To maximize log p(Y ) in Equation 4, we use variational inference as previously proposed for
GPLVMs (Titsias and Lawrence, 2010). The true posterior over the latent states, p({gj}|Y), is approximated by a variational distribution Qθ({gj}) with parameters θ that are optimized to minimize the KL divergence between Qθ({gj}) and p({gj}|Y ). This is equivalent to maximizing the evidence lower bound (ELBO) on the log marginal likelihood:
L(θ) = H(Qθ) + EQθ [log pM({gj})] + EQθ [log p(Y |{gj})]. (5)
Here, EQθ [·] indicates averaging over the variational distribution and H(Qθ) is its entropy. For simplicity, and because our model does not specify a priori statistical dependencies between the individual elements of {gj}, we choose a variational distribution Qθ that factorizes over conditions:
Qθ({gj}) =
M (cid:89) j=1 qθj (gj). (6)
In the Euclidean case, the entropy and expectation terms in Equation 5 can be calculated analytically for some kernels (Titsias and Lawrence, 2010), and otherwise using the reparameterization trick (Kingma and Welling, 2014; Rezende et al., 2014). Brieﬂy, the reparameterization trick involves ﬁrst sampling from a ﬁxed, easy-to-sample distribution (e.g. a normal distribution with zero mean and unit variance), and applying a series of differentiable transformations to obtain samples from Qθ. We can then use these samples to estimate the entropy term and expectations in Equation 5.
For non-Euclidean manifolds, inference in mGPLVMs poses two major problems. Firstly, we can no longer calculate the ELBO analytically nor evaluate it using the standard reparameterization trick.
Secondly, evaluating the Gaussian process log marginal likelihood log p(Y |{gj}) exactly becomes computationally too expensive for large datasets. We address these issues in the following. 2.2.1 Reparameterizing distributions on Lie groups
To estimate and optimize the ELBO in Equation 5 when Qθ is deﬁned on a non-Euclidean manifold, we use Falorsi et al.’s ReLie framework, an extension of the standard reparameterization trick to variational distributions deﬁned on Lie groups. 3
Sampling from Qθ Since we assume that Qθ factorizes (Equation 6), sampling from Qθ is per-formed by independently sampling from each qθj . We start from a differentiable base distribution rθj (x) in Rn. Note that Rn is isomorphic to the tangent space at the identity element of the group
G, known as the Lie algebra. We can thus deﬁne a ‘capitalized’ exponential map ExpG : Rn → G, which maps elements of Rn to elements in G (Sola et al., 2018; Appendix C). Importantly, ExpG maps a distribution centered at zero in Rn to a distribution ˜qθj in the group centered at the identity element. To obtain samples from a distribution qθj centered at an arbitrary gµ j in the group, we can simply apply the group multiplication with gµ j to samples from ˜qθj . Therefore, obtaining a sample gj from qθj involves the following steps: (i) sample from rθj (x), (ii) apply ExpG to obtain a sample ˜gj from ˜qθj , and (iii) apply the group multiplication gj = gµ j ˜gj.
Estimating the entropy H(Qθ) Since H(qθj ) = H(˜qθj ) (Falorsi et al., 2019), we use K inde-pendent Monte Carlo samples from ˜Qθ(·) = (cid:81)M j=1 ˜qθj (·) to calculate
H(Qθ) ≈ − 1
K
K (cid:88)
M (cid:88) k=1 j=1 log ˜qθj (˜gjk), (7) where ˜gjk = ExpGxjk and {xjk ∼ rθj (x)}K k=1.
Evaluating the density ˜qθ To evaluate log ˜qθj (ExpGxjk), we use the result from Falorsi et al. (2019) that
˜qθ(˜g) = (cid:88) rθ(x)|J (x)|−1 (8) x∈Rn : ExpG(x)=˜g where J (x) is the Jacobian of ExpG at x. Thus, ˜qθ(˜g) is the sum of the Jacobian-weighted densities rθ(x) in Rn at all those points that are mapped to ˜g through ExpG This is an inﬁnite but converging sum, and following Falorsi et al. (2019) we approximate it by its ﬁrst few dominant terms (Appendix I).
Note that ExpG(·) and the group multiplication by gµ are both differentiable operations. Therefore, as long as we choose a differentiable base distribution rθ(x), we can perform end-to-end optimization of the ELBO. In this work we choose the reference distribution to be a multivariate normal rθj (x) =
N (x; 0, Σj) for each qθj . We variationally optimize both {Σj} and the mean parameters {gµ j } for all j, and together these deﬁne the variational distribution. 2.2.2 Sparse GP approximation
To efﬁciently evaluate the EQθ [log p(Y |{gj})] term in the ELBO for large datasets, we use the variational sparse GP approximation (Titsias, 2009) which has previously been applied to Euclidean
GPLVMs (Titsias and Lawrence, 2010). Speciﬁcally, we introduce a set of m inducing points Zi for each neuron i, and use a lower bound on the GP log marginal likelihood: log p(yi|{gj}) ≥ − (cid:124) 1 2 i (Qi + σ2 yT i I)−1yi − 1 2 log |Qi + σ2 i I| − (cid:123)(cid:122) log ˜p(yi|{gj}) 1 2σ2 Tr(Ki − Qi) + const. (cid:125) (9) with Qi = K{gj }ZiK−1
ZiZi
KZi{gj } (10) where KAB denotes the Gram matrix associated with any two input sets A and B. Note that the latents {gj} are shared across all neurons. In this work we optimize the inducing points on G directly, but they could equivalently be optimized in Rn and projected onto G via ExpG.
Using the sparse GP framework, the cost of computing the GP likelihood reduces to O(M m2) for each neuron and Monte Carlo sample. This leads to an overall complexity of O(KN M m2) for approximating EQθ [log p(Y |{gj})] with K Monte Carlo samples, N neurons, M conditions and m inducing points (see Appendix I for further details on complexity and implementation). 2.2.3 Optimization
We are now equipped to optimize the ELBO deﬁned in Equation 5 using Monte Carlo samples drawn from a variational distribution Qθ deﬁned on a Lie group G. To train the model, we use 4
Adam (Kingma and Ba, 2014) to perform stochastic gradient descent on the following loss function:
L(θ) = 1
K
K (cid:88)


M (cid:88) k=1 j=1 (cid:0)log pM(gjk) − log ˜qθj (˜gjk)(cid:1) −
 log ˜p(yi|{gjk})
 (11)
N (cid:88) i where a set of K Monte-Carlo samples {˜gjk}K k=1 is drawn at each iteration from {˜qθj } as described in Section 2.2.1. In Equation 11, gjk = gµ j is a group element that is optimized together with all other model parameters. Finally, log ˜p(yi|{gj}) is the lower bound deﬁned in Equation 9 and pM(gjk) is the prior described in Section 2.1. The inner sums run over conditions j and neurons i. j ˜gjk, where gµ 2.2.4 Posterior over tuning curves
We approximate the posterior predictive distribution over tuning curves by sampling from the (approximate) posterior over latents. Speciﬁcally, for a given neuron i and a set of query states G(cid:63), the posterior predictive over f (cid:63) i is approximated by: p(f (cid:63) i |Y , G(cid:63)) = 1
K
K (cid:88) k=1 p(f (cid:63) i |G(cid:63), {Gk, Y }) (12) where each Gk is a set of M latent states (one for each condition in Y ) independently drawn from the variational posterior Qθ(·). In Equation 12, each term in the sum is a standard Gaussian process posterior (Rasmussen and Williams, 2006), which we approximate as described above (Section 2.2.2;
Appendix E; Titsias, 2009). 2.3 Applying mGPLVM to tori, spheres and SO(3)
At this stage, we have yet to deﬁne the manifold-speciﬁc GP kernels kM described in Section 2.1.
These kernels ought to capture the topology of the latent space and express our prior assumptions that the neuronal tuning curves, deﬁned on the manifold, have certain properties such as smoothness.
Here we take inspiration from the common squared exponential covariance function deﬁned over
Euclidean spaces and introduce analogous kernels on tori, spheres, and SO(3). This leads to the following general form: kM(g, g(cid:48)) = α2 exp (cid:18)
− dM(g, g(cid:48)) 2(cid:96)2 (cid:19) g, g(cid:48) ∈ M (13) where α2 is a variance parameter, (cid:96) is a characteristic lengthscale, and dM(g, g(cid:48)) is a manifold-speciﬁc distance function. While squared geodesic distances might be intuitive choices for d(·, ·) in
Equation 13, they result in positive semi-deﬁnite (PSD) kernels only for Euclidean latent spaces (Fer-agen et al., 2015; Jayasumana et al., 2015). Therefore, we build distance functions that automatically lead to valid covariance functions by observing that (i) dot product kernels are PSD, and (ii) the exponential of a PSD kernel is also PSD. Speciﬁcally, we use the following manifold-speciﬁc dot product-based distances: dRn (g, g(cid:48)) = ||g − g(cid:48)||2 2 dSn (g, g(cid:48)) = 2(1 − g · g(cid:48)) dT n (g, g(cid:48)) = 2 (cid:80) (cid:104) k (1 − gk · g(cid:48) k) 1 − (g · g(cid:48))2(cid:105) dSO(3)(g, g(cid:48)) = 4 g ∈ Rn g ∈ {x ∈ Rn+1; (cid:107)x(cid:107) = 1} g ∈ {(g1, · · · , gn); ∀k : gk ∈ R2, (cid:107)gk(cid:107) = 1} g ∈ {x ∈ R4; (cid:107)x(cid:107) = 1} (14) (15) (16) (17) where we have slightly abused notation by directly using “g” to denote a convenient parameterisation of the group elements which we deﬁne on the right of each equation. To build intuition, we note that the distance metric on the torus gives rise to a multivariate von Mises function; the distance metric on the sphere leads to an analogous von Mises Fisher function; and the distance metric on
SO(3) is 2(1 − cos ϕrot) where ϕrot is the angle of rotation required to transform g into g(cid:48). Notably, all these distance functions reduce to the Euclidean squared exponential kernel in the small angle limit. Laplacian (Feragen et al., 2015) and Matérn (Borovitskiy et al., 2020) kernels have previously been proposed for modelling data on Riemannian manifolds, and these can also be incorporated in mGPLVM. 5
Figure 2: Applying mGPLVM to syn-thetic data on the ring T 1. Top left: neu-ral activity of 100 neurons at 100 differ-ent conditions (here, time bins). Bottom: timecourse of the latent states (left) and tuning curves for 12 representative neurons (right). Green: ground truth; Black: poste-rior mean; Grey shaded regions: ±2 pos-terior s.t.d. Top right: data replotted from the top left panel, with neurons reordered according to their preferred angles as de-termined by the inferred tuning curves.
Finally, we provide expressions for the variational densities (Equation 8) deﬁned on tori, S3 and
SO(3):
˜qθ(ExpT n x) =
˜qθ(ExpSO(3)x) = (cid:88) rθ(x + 2πk), k∈Zn (cid:20) rθ(x + πk ˆx) (cid:88) 2(cid:107)x + πk ˆx(cid:107)2 1 − cos (2(cid:107)x + πk ˆx(cid:107)) (cid:21)
,
˜qθ(ExpS3 x) = k∈Z (cid:88) k∈Z (cid:20) rθ(x + 2πk ˆx) 2(cid:107)x + 2πk ˆx(cid:107)2 1 − cos (2(cid:107)x + 2πk ˆx(cid:107)) (cid:21)
, (18) (19) (20) where ˆx = x/(cid:107)x(cid:107). Further details and the corresponding exponential maps are given in Appendix C.
Since spheres that are not S1 or S3 are not Lie groups, ReLie does not provide a general framework for mGPLVM on these manifolds which we therefore treat separately in Appendix D. 3 Experiments and results
In this section, we start by demonstrating the ability of mGPLVM to correctly infer latent states and tuning curves in non-Euclidean spaces using synthetic data generated on T 1, T 2 and SO(3). We also verify that cross-validated model comparison correctly recovers the topology of the underlying latent space, suggesting that mGPLVM can be used for model selection given a set of candidate manifolds. Finally, we apply mGPLVM to a biological dataset to show that it is robust to the noise and heterogeneity characteristic of experimental recordings. 3.1 Synthetic data
To generate synthetic data Y, we specify a target manifold M, draw a set of M latent states {gj} on
M, and assign a tuning curve to each neuron i of the form (cid:32) fi(g) = a2 i exp
− (cid:33)
) geo(g, gpref d2 2b2 i i
+ ci, yij|gj ∼ N (fi(gj), σ2 i ) (21) (22) with random parameters ai, bi and ci. Thus, the activity of each neuron is a noisy bell-shaped function of the geodesic distance on M between the momentary latent state gj and the neuron’s preferred state gpref (sampled uniformly). While this choice of tuning curves is inspired by the common ‘Gaussian i bump’ model of neural tuning, we emphasize that the non-parametric prior over fi in mGPLVM can discover any smooth tuning curve on the manifold, not just Gaussian bumps. For computational simplicity, here we constrain the mGPLVM parameters αi, (cid:96)i and σi to be identical across neurons.
Note that we can only recover the latent space up to symmetries which preserve pairwise distances.
In all ﬁgures, we have therefore aligned model predictions and ground truth for ease of visualization (Appendix F). 6
Figure 3: Validating mGPLVM on synthetic data. (a-c) Torus dataset. (a) True latent states
{gj ∈ T 2} (dots) and posterior latent means {gµ j } (crosses). The color scheme is chosen to be smooth for the true latents. (b) Posterior tuning curves for two example neurons. Top: tuning curves on the tori. Bottom: projections onto the periodic [0; 2π] plane. Black circles indicate locations and widths of the true tuning curves. (c) Mean squared cross-validated prediction error (left) and negative log likelihood (right) when ﬁtting T 2 and R2 to data generated on T 2. Dashed lines connect datapoints for the same synthetic dataset. (d-f) SO(3) dataset. (d) Axis of the rotation represented by the true latent states {gj ∈ SO(3)} (dots) and the posterior latent means {gµ j } (crosses) projected onto the (ϕ, θ)-plane. (e) Magnitude of the rotations represented by {gj} and {gµ j }. (f) Same as (c), now comparing SO(3) to R3. (g) Test log likelihood ratio for 10 synthetic datasets on T 2, SO(3), & S3, with mGPLVM ﬁtted on each manifold (x-axis). Solid lines indicate mean across datasets.
We ﬁrst generated data on the ring (T 1, Figure 2, top left), letting the true latent state be a continuous random walk across conditions for ease of visualization. We then ﬁtted T 1-mGPLVM to the data and found that it correctly discovered the true latent states g as well as the ground truth tuning curves (Figure 2, bottom right). Reordering the neurons according to their preferred angles further exposed the population encoding of the angle (Figure 2, top right).
Next, we expanded the latent space to two dimensions with data now populating a 2-torus (T 2).
Despite the non-trivial topology of this space, T 2-mGPLVM provided accurate inference of both latent states (Figure 3a) and tuning curves (Figure 3b). To show that mGPLVM can be used to distinguish between candidate topologies, we compared T 2-mGPLVM to a standard Euclidean GPLVM in R2 on the basis of both cross-validated prediction errors and importance-weighted marginal likelihood estimates (Burda et al., 2015). We simulated 10 different toroidal datasets; for each, we used half the conditions to ﬁt the GP hyperparameters, and half the neurons to predict the latent states for the conditions not used to ﬁt the GP parameters. Finally, we used the inferred GP parameters and latent states to predict the activity of the held-out neurons at the held-out conditions. As expected, the predictions of the toroidal model outperformed those of the standard Euclidean GPLVM which cannot capture the periodic boundary conditions of the torus (Figure 3c).
Beyond toroidal spaces, SO(3) is of particular interest for the study of neural systems encoding ‘yaw, pitch and roll’ in a variety of 3D rotational contexts (Finkelstein et al., 2015; Shepard and Metzler, 1971; Wilson et al., 2018). We therefore ﬁtted an SO(3)-mGPLVM to synthetic data generated on
SO(3) and found that it rendered a faithful representation of the latent space and outperformed a
Euclidean GPLVM on predictions (Figure 3d-f). Finally we show that mGPLVM can also be used to select between multiple non-Euclidean topologies. We generated 10 datasets on each of T 2, SO(3) and S3 and compared cross-validated log likelihoods for T 2-, SO(3)- and S3-mGPLVM, noting that p(M|Y ) ∝ p(Y |M) under a uniform prior over manifolds M. Here we found that the correct latent manifold was consistently the most likely for all 30 datasets (Figure 3g). In summary, these 7
Figure 4: The Drosophila head direction circuit. (a)
Input data overlaid with the posterior varia-tional distribution over latent states of a T 1-mGPLVM. (b) Mean cross-validated prediction error (left) and negative log likelihood (right) for models ﬁtted on T 1 and R1. Each datapoint corresponds to a differ-ent partition of the timepoints into (c-a training set and a test set. d) Posterior tuning curves for eight example neurons in T 1 (c) and R1 (d). Color encodes the position of the maximum of each tuning curve. Shadings in (a,c,d) indicate
±2 s.t.d. results show robust performance of mGPLVM across various manifolds of interest in neuroscience and beyond, as well as a quantitative advantage over Euclidean GPLVMs which ignore the underlying topology of the latent space. 3.2 The Drosophila head direction circuit
Finally we applied mGPLVM to an experimental dataset to show that it is robust to biological and measurement noise. Here, we used calcium imaging data recorded from the ellipsoid body (EB) of
Drosophila melanogaster (Turner-Evans, 2020; Turner-Evans et al., 2020), where the so-called E-PG neurons have recently been shown to encode head direction (Seelig and Jayaraman, 2015). The EB is divided into 16 ‘wedges’, each containing 2-3 E-PG neurons that are not distinguishable on the basis of calcium imaging data, and we therefore treat each wedge as one ‘neuron’. Due to the physical shape of the EB, neurons come ‘pre-ordered’ since their joint activity resembles a bump rotating on a ring (Figure 4a, analogous to Figure 2, “ordered data”). While the EB’s apparent ring topology obviates the need for mGPLVM as an explorative tool for uncovering manifold representations, we emphasize that head direction circuits in higher organisms are not so obviously structured (Chaudhuri et al. (2019); Appendix A) – in fact, some brain areas such as the entorhinal cortex even embed concurrent representations of multiple spaces (Constantinescu et al., 2016; Hafting et al., 2005).
We ﬁtted the full mGPLVM with a separate GP for each neuron and found that T 1-mGPLVM performed better than R1-mGPLVM on both cross-validated prediction errors and log marginal likelihoods (Figure 4b). The model recovered latent angles that faithfully captured the visible rotation of the activity bump around the EB, with larger uncertainty during periods where the neurons were less active (Figure 4a, orange). When querying the posterior tuning curves from a ﬁt in R1, these were found to suffer from spurious boundary conditions with inﬂated uncertainty at the edges of the latent representation – regions where R1-mGPLVM effectively has less data than T 1-mGPLVM since R1 does not wrap around. In comparison, the tuning curves were more uniform across angles in
T 1 which correctly captures the continuity of the underlying manifold. In Appendix A, we describe similar results with mGPLVM applied to a dataset from the mouse head-direction circuit with more heterogeneous neuronal tuning and no obvious anatomical organization (Peyrache et al., 2015). 4 Discussion and future work
Conclusion We have presented an extension of the popular GPLVM model to incorporate non-Euclidean latent spaces. This is achieved by combining a Bayesian GPLVM with recently developed methods for approximate inference in non-Euclidean spaces and a new family of manifold-speciﬁc kernels. Inference is performed using variational sparse GPs for computational tractability with inducing points optimized directly on the manifold. We demonstrated that mGPLVM correctly infers the latent states and GP parameters for synthetic data of various dimensions and topologies, and that cross-validated model comparisons can recover the correct topology of the space. Finally, we showed 8
how mGPLVM can be used to infer latent topologies and representations in biological circuits from calcium imaging data. We expect mGPLVM to be particularly valuable to the neuroscience community because many quantities encoded in the brain naturally live in non-Euclidean spaces (Chaudhuri et al., 2019; Finkelstein et al., 2015; Wilson et al., 2018).