Abstract
For the gradient computation across the time domain in Spiking Neural Networks (SNNs) training, two different approaches have been independently studied. The
ﬁrst is to compute the gradients with respect to the change in spike activation (activation-based methods), and the second is to compute the gradients with respect to the change in spike timing (timing-based methods). In this work, we present a comparative study of the two methods and propose a new supervised learning method that combines them. The proposed method utilizes each individual spike more effectively by shifting spike timings as in the timing-based methods as well as generating and removing spikes as in the activation-based methods. Experimental results showed that the proposed method achieves higher performance in terms of both accuracy and efﬁciency than the previous approaches. 1

Introduction
Spiking neural networks (SNNs) have been studied not only for their biological plausibility but also for computational efﬁciency that stems from information processing with binary spikes [1]. One of the unique characteristics of SNNs is that the states of the neurons at different time steps are closely related to each other. This may resemble the temporal dependency in recurrent neural networks (RNNs), but in SNNs direct inﬂuences between neurons are only through the binary spikes. Since the true derivative of the binary activation function, or thresholding function, is zero almost everywhere,
SNNs have an additional challenge in precise gradient computation unless the binary activation function is replaced by an alternative as in [2].
Due to the difﬁculty of training SNNs, in some recent studies, parameters trained in non-spiking NNs were employed in SNNs. However, this approach is only feasible by using the similarity between rate-coded SNNs and non-spiking NNs [3, 4] or by abandoning several features of spiking neurons to maximize the similarity between SNNs and non-spiking NNs [5–7]. The unique characteristics of SNNs that enable efﬁcient information processing can only be utilized with dedicated learning methods for SNNs. In this context, several studies have reported promising results with the gradient-based supervised learning methods that takes account of those characteristics [8–13].
Previous works on gradient-based supervised learning for SNNs can be classiﬁed into two categories.
The methods in the ﬁrst category work around the non-differentiability of the spiking function with the surrogate derivative [14] and compute the gradients with respect to the spike activation [11–13].
The methods in the second category focus on the timings of existing spikes and computes the gradients with respect to the spike timing [8–10, 15]. Let us call those methods as the activation-based methods and the timing-based methods, respectively. Until now, the two approaches have been thought irrelevant to each other and studied independently.
The problem with previous works is that both approaches have limitations in computing accurate gradients, which become more problematic when the spike density is low. The computational cost of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Computational graphs representing (a) the RNN-like description and (b) the SRM-based description of our SNN model. Black solid arrows represent accumulation and decaying. Black dashed arrows represent synaptic integration, red solid arrows represent the spiking function, and red dashed arrows represent reset paths. (a) (b) the SNN is known to be proportional to the number of spikes, or the ﬁring rates [6, 16, 17]. To make the best use of the computational power of SNNs and use them more efﬁciently than non-spiking counterparts, it is important to reduce the required number of spikes for inference. If there are only a few spikes in the network, the network becomes more sensitive to the change in the state of each individual spike such as the generation of a new spike, the removal of an existing spike, or the shift of an existing spike. Training SNNs with fewer spikes requires the learning method to be aware of those changes through gradient computation.
In this work, we investigated the relationship between the activation-based methods and the timing-based methods for supervised learning in SNNs. We observed that the two approaches are complemen-tary when considering the change in the state of individual spikes. Then we devised a new learning method called activation- and timing-based learning rule (ANTLR) that enables more precise gradient computation by combining the two methods. In experiments with random spike-train matching task and widely used benchmarks (MNIST and N-MNIST), our method achieved the higher accuracy than that of existing methods when the networks are forced to use fewer spikes in training. 2