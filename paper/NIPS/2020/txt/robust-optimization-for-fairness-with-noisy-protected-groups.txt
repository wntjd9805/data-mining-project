Abstract
Many existing fairness criteria for machine learning involve equalizing some metric across protected groups such as race or gender. However, practitioners trying to audit or enforce such group-based criteria can easily face the problem of noisy or biased protected group information. First, we study the consequences of naïvely relying on noisy protected group labels: we provide an upper bound on the fairness violations on the true groups G when the fairness criteria are satisﬁed on noisy groups ˆG. Second, we introduce two new approaches using robust optimization that, unlike the naïve approach of only relying on ˆG, are guaranteed to satisfy fairness criteria on the true protected groups G while minimizing a training objective. We provide theoretical guarantees that one such approach converges to an optimal feasible solution. Using two case studies, we show empirically that the robust approaches achieve better true group fairness guarantees than the naïve approach. 1

Introduction
As machine learning becomes increasingly pervasive in real-world decision making, the question of ensuring fairness of ML models becomes increasingly important. The deﬁnition of what it means to be “fair” is highly context dependent. Much work has been done on developing mathematical fairness criteria according to various societal and ethical notions of fairness, as well as methods for building machine-learning models that satisfy those fairness criteria [see, e.g., 21, 32, 49, 41, 54, 14, 25, 51].
Many of these mathematical fairness criteria are group-based, where a target metric is equalized or enforced over subpopulations in the data, also known as protected groups. For example, the equality of opportunity criterion introduced by Hardt et al. [32] speciﬁes that the true positive rates for a binary classiﬁer are equalized across protected groups. The demographic parity [21] criterion requires that a classiﬁer’s positive prediction rates are equal for all protected groups.
One important practical question is whether or not these fairness notions can be reliably measured or enforced if the protected group information is noisy, missing, or unreliable. For example, survey
∗First two authors have equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
participants may be incentivized to obfuscate their responses for fear of disclosure or discrimination, or may be subject to other forms of response bias. Social desirability response bias may affect participants’ answers regarding religion, political afﬁliation, or sexual orientation [40]. The collected data may also be outdated: census data collected ten years ago may not an accurate representation for measuring fairness today.
Another source of noise arises from estimating the labels of the protected groups. For various image recognition tasks (e.g., face detection), one may want to measure fairness across protected groups such as gender or race. However, many large image corpora do not include protected group labels, and one might instead use a separately trained classiﬁer to estimate group labels, which is likely to be noisy [12]. Similarly, zip codes can act as a noisy indicator for socioeconomic groups.
In this paper, we focus on the problem of training binary classiﬁers with fairness constraints when only noisy labels, ˆG ∈ {1, ..., ˆm}, are available for m true protected groups, G ∈ {1, ..., m}, of interest. We study two aspects: First, if one satisﬁes fairness constraints for noisy protected groups
ˆG, what can one say with respect to those fairness constraints for the true groups G? Second, how can side information about the noise model between ˆG and G be leveraged to better enforce fairness with respect to the true groups G?
Contributions: Our contributions are three-fold: 1. We provide a bound on the fairness violations with respect to the true groups G when the fairness criteria are satisﬁed for the noisy groups ˆG. 2. We introduce two new robust-optimization methodologies that satisfy fairness criteria on the true protected groups G while minimizing a training objective. These methodologies differ in convergence properties, conservatism, and noise model speciﬁcation. 3. We show empirically that unlike the naïve approach, our two proposed approaches are able to satisfy fairness criteria with respect to the true groups G on average. j=1 can be given.
The ﬁrst approach we propose (Section 5) is based on distributionally robust optimization (DRO)
[19, 8]. Let p denotes the full distribution of the data X, Y ∼ p. Let pj be the distribution of the data conditioned on the true groups being j, so X, Y |G = j ∼ pj; and ˆpj be the distribution of
X, Y conditioned on the noisy groups. Given an upper bound on the total variation (TV) distance
γj ≥ T V (pj, ˆpj) for each j ∈ {1, ..., m}, we deﬁne ˜pj such that the conditional distributions (X, Y | ˜G = j ∼ ˜pj) fall within the bounds γi with respect to ˆG. Therefore, the set of all such ˜pj is guaranteed to include the unknown true group distribution pj, ∀j ∈ G. Because it is based on the well-studied DRO setting, this approach has the advantage of being easy to analyze. However, the results may be overly conservative unless tight bounds {γj}m
Our second robust optimization strategy (Section 6) uses a robust re-weighting of the data from soft protected group assignments, inspired by criteria proposed by Kallus et al. [37] for auditing the fairness of ML models given imperfect group information. Extending their work, we optimize a constrained problem to achieve their robust fairness criteria, and provide a theoretically ideal algorithm that is guaranteed to converge to an optimal feasible point, as well as an alternative practical version that is more computationally tractable. Compared to DRO, this second approach uses a more precise noise model, P ( ˆG = k|G = j), between ˆG and G for all pairs of group labels j, k, that can be estimated from a small auxiliary dataset containing ground-truth labels for both G and ˆG.
An advantage of this more detailed noise model is that a practitioner can incorporate knowledge of any bias in the relationship between G and ˆG (for instance, survey respondents favoring one socially preferable response over others), which causes it to be less likely than DRO to result in an overly-conservative model. Notably, this approach does not require that ˆG be a direct approximation of G—in fact, G and ˆG can represent distinct (but related) groupings, or even groupings of different sizes, with the noise model tying them together. For example, if G represents “language spoken at home,” then ˆG could be a noisy estimate of “country of residence.” 2