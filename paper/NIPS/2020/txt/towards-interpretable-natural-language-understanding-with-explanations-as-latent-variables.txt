Abstract
Recently generating natural language explanations has shown very promising re-sults in not only offering interpretable explanations but also providing additional information and supervision for prediction. However, existing approaches usually require a large set of human annotated explanations for training while collecting a large set of explanations is not only time consuming but also expensive. In this paper, we develop a general framework for interpretable natural language understanding that requires only a small set of human annotated explanations for training. Our framework treats natural language explanations as latent variables that model the underlying reasoning process of a neural model. We develop a variational EM framework for optimization where an explanation generation mod-ule and an explanation-augmented prediction module are alternatively optimized and mutually enhance each other. Moreover, we further propose an explanation-based self-training method under this framework for semi-supervised learning.
It alternates between assigning pseudo-labels to unlabeled data and generating new explanations to iteratively improve each other. Experiments on two natural language understanding tasks demonstrate that our framework can not only make effective predictions in both supervised and semi-supervised settings, but also generate good natural language explanations 2. 1

Introduction
Building interpretable systems for natural language understanding is critical in various domains such as healthcare and ﬁnance. One promising direction is generating natural language explanations for prediction [1–4], which has been shown very promising recently as they can not only offer interpretable explanations for back-box prediction systems but also provide additional information and supervision for prediction [5–7]. For example, given a sentence “The only thing more wonderful than the food is the service.”, a human annotator may write an explanation like “Positive, because the word ‘wonderful’ occurs within three words before the term food”, which is much more informative than the label “positive” as it explains how the decision was made. Moreover, the explanation can
∗ Equal contribution, with order determined by rolling a dice. Work was done during internship at Mila. 2 Code is available at https://github.com/JamesHujy/ELV.git 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
serve as an implicit logic rule that can be generalized to other instances like “The food is wonderful, I really enjoyed it.”
There are some recent works [3, 4] that study generating natural language explanations for predictions and/or leverage generated explanations as additional features for prediction. For example, Camburu et al. [3] trained a language model to general natural language explanations for the task of natural language inference by training on a corpus with annotated human explanations. Rajani et al. [4] proposed a two-stage framework for common sense reasoning which ﬁrst trained a natural language explanation model and then further trained a prediction model with the generated explanations as additional information. These approaches achieve promising performance in terms of both prediction performance and explainability. However, a large number of labeled examples with human explanations are required, which is expensive and sometimes impossible to obtain. Therefore, we are looking for an approach that makes effective prediction, offers good explainability, but requires a limited number of human explanations for training.
In this paper, we propose such an approach. We start from the intuition that the explanation-augmented prediction model is able to provide informative feedback for generating meaningful natural language explanations. Therefore, different from existing work which trains the explanation generation model and the explanation-augmented prediction model in separate stages, we propose to jointly train the two models. Speciﬁcally, taking the task of text classiﬁcation as an example, we propose a principled probabilistic framework for text classiﬁcation, where natural language Explanations are treated as Latent Variables (ELV). Variational EM [8] is used for the optimization, and only a set of human explanations are required for guiding the explanation generation process. In the E-step, the explanation generation model is trained to approximate the ground truth explanations (for instances with annotated explanations) or guided by the explanation-augmentation module through posterior inference (for instances without annotated explanations); in the M-step, the explanation-augmented prediction model is trained with high-quality explanations sampled from the explanation generation model. The two modules mutually enhance each other. As human explanations can serve as implicit logic rules, they can be used for labeling unlabeled data. Therefore, we further extend our ELV framework to an Explantion-based Self-Training (ELV-EST) model for leveraging a large number of unlabeled data in the semi-supervised setting.
To summarize, in this paper we make the following contributions:
• We propose a principled probabilistic framework called ELV for text classiﬁcation, in which natural language explanation is treated as a latent variable. It jointly trains an explanation generator and an explanation-augmented prediction model. Only a few annotated natural language explanations are required to guide the natural language generation process.
• We further extend ELV for semi-supervised learning (the ELV-EST model), which leverages natural language explanations as implicit logic rules to label unlabeled data.
• We conduct extensive experiments on two tasks: relation extraction and sentiment analysis.
Experimental results prove the effectiveness of our proposed approach in terms of both prediction and explainability in both supervised and semi-supervised settings. 2