Abstract
The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks.
While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or ofﬂine, and sensitive to many hyper-parameters. In this work, we propose Look-ahead
MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online-continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more ﬂexible and efﬁcient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classiﬁcation benchmarks. 1

Introduction
Embodied or interactive agents that accumulate knowledge and skills over time must possess the ability to continually learn. Catastrophic forgetting [11, 18], one of the biggest challenges in this setup, can occur when the i.i.d. sampling conditions required by stochastic gradient descent (SGD) are violated as the data belonging to different tasks to be learnt arrives sequentially. Algorithms for continual learning (CL) must also use their limited model capacity efﬁciently since the number of future tasks is unknown. Ensuring gradient-alignment across tasks is therefore essential, to make shared progress on their objectives. Gradient Episodic Memory (GEM) [17] investigated the connection between weight sharing and forgetting in CL and developed an algorithm that explicitly tried to minimise gradient interference. This is an objective that meta-learning algorithms implicitly optimise for (refer to [20] for derivations of the effective parameter update made in ﬁrst and second order meta learning algorithms). Meta Experience Replay (MER) [22] formalized the transfer-interference trade-off and showed that the gradient alignment objective of GEM coincide with the objective optimised by the ﬁrst order meta-learning algorithm Reptile [20].
Besides aligning gradients, meta-learning algorithms show promise for CL since they can directly use the meta-objective to inﬂuence model optimisation and improve on auxiliary objectives like generalisation or transfer. This avoids having to deﬁne heuristic incentives like sparsity [15] for better CL. The downside is that they are usually slow and hard to tune, effectively rendering them more suitable for ofﬂine continual learning [12, 22]. In this work, we overcome these difﬁculties and develop a gradient-based meta-learning algorithm for efﬁcient, online continual learning. We ﬁrst propose a base algorithm for continual meta-learning referred to as Continual-MAML (C-MAML) that utilizes a replay-buffer and optimizes a meta-objective that mitigates forgetting. Subsequently,
∗equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
we propose a modiﬁcation to C-MAML, named La-MAML, which incorporates modulation of per-parameter learning rates (LRs) to pace the learning of a model across tasks and time. Finally, we show that the algorithm is scalable, robust and achieves favourable performance on several benchmarks of varying complexity. 2