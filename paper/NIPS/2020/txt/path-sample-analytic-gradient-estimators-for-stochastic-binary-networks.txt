Abstract
In neural networks with binary activations and or binary weights the training by gradient descent is complicated as the model has piecewise constant response. We consider stochastic binary networks, obtained by adding noises in front of activa-tions. The expected model response becomes a smooth function of parameters, its gradient is well deﬁned but it is challenging to estimate it accurately. We propose a new method for this estimation problem combining sampling and analytic ap-proximation steps. The method has a signiﬁcantly reduced variance at the price of a small bias which gives a very practical tradeoff in comparison with existing unbiased and biased estimators. We further show that one extra linearization step leads to a deep straight-through estimator previously known only as an ad-hoc heuristic. We experimentally show higher accuracy in gradient estimation and demonstrate a more stable and better performing training in deep convolutional models with both proposed methods. 1

Introduction
Neural Networks with binary weights and binary activations are very computationally efﬁcient.
Rastegari et al. [24] report up to 58(cid:2) speed-up compared to ﬂoating point computations. There is a further increase of hardware support for binary operations: matrix multiplication instructions in recent NVIDIA cards, specialized projects on spike-like (neuromorphic) computation [3, 7], etc.
Binarized (or more generally quantized) networks have been shown to close up in performance to real-valued baselines [2, 22, 25, 28, 12, 6]. We believe that good training methods can improve their performance further. The main difﬁculty with binary networks is that unit outputs are computed using sign activations, which renders common gradient descent methods inapplicable. Nevertheless, experimentally oriented works ever so often deﬁne the lacking gradients in these models in a heuristic way. We consider the more sound approach of stochastic Binary Networks (SBNs) [19, 23]. This approach introduces injected noises in front of all sign activations. The network output becomes smooth in the expectation and its derivative is well-deﬁned. Furthermore, injecting noises in all layers makes the network a deep latent variable model with a very ﬂexible predictive distribution.
Estimating gradients in SBNs is the main problem that we address. We focus on handling deep dependencies through binary activations, which we believe to be the crux of the problem. That is why we consider all weights to be real-valued in the present work. An extension to binary weights would be relatively simple, e.g. by adding an extra stochastic binarization layer for them.
SBN Model Let x0 denote the input to the network (e.g. an image to recognize). We deﬁne a stochastic binary network (SBN) with L layers with neuron outputs X 1...L and injected noises Z 1...L 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Stochastic binary network with 2 hidden layers. Left: latent variable model view (injected noises). Right: directed graphical model view (Bayesian network). The PSA method performs explicit summation along paths (highlighted). as follows (Fig. 1 left):
X 0 (cid:16) x0; X k (cid:16) sgnpakpX k(cid:1)1; θkq (cid:1) Z kq;
F (cid:16) f pX L; θL(cid:0)1q. (1)
The output X k of layer k is a vector in Bn, where we denote binary states B (cid:16) t(cid:1)1, 1u. The network input x0 is assumed real-valued. The noise vectors Z k consist of n independent variables with a known distribution (e.g., logistic). The network pre-activation functions akpX k(cid:1)1; θkq are assumed differentiable in parameters θk and will be typically modeled as afﬁne maps (e.g., fully connected, convolution, concatenation, averaging, etc.). Partitioning the parameters θ by layers as above, incurs no loss of generality since they can in turn be deﬁned as any differentiable mapping θ (cid:16) θpηq and handled by standard backpropagation.
The network head function f pxL; θL(cid:0)1q denotes the remainder of the model not containing further binary dependencies. For classiﬁcation problems we consider the softmax predictive probability model ppy|xL; θL(cid:0)1q (cid:16) softmaxpaL(cid:0)1pxL; θL(cid:0)1qq, where the afﬁne transform aL(cid:0)1 computes class scores from the last binary layer. The function f pX L; θL(cid:0)1q is deﬁned as the cross-entropy of the predictive distribution ppy|xL; θL(cid:0)1q relative to the training label distribution p(cid:6)py|x0q.
Due to the injected noises, all states X become random variables and their joint distribution given the input x0 takes the form of a Bayesian network with the following structure (Fig. 1 right):
L ppx1...L | x0; θq (cid:16) – k(cid:16)1 ppxk|xk(cid:1)1; θkq, n ppxk|xk(cid:1)1; θkq (cid:16) – i(cid:16)1 ppxk i |xk(cid:1)1; θkq.
The equivalence to the injected noise model is established with ppxk j (cid:16) 1|xk(cid:1)1; θkq (cid:16) P(cid:0)ak j (cid:1)Z k j ¡ 0(cid:8) (cid:16) FZpak j q, (2a) (3) where FZ is the noise cdf. If we consider noises with logistic distribution, FZ becomes the common ijxk(cid:1)1 sigmoid logistic function and the network with linear pre-activations akpxk(cid:1)1qj (cid:16) (cid:176)j wk i becomes the well-known sigmoid belief network [19].
Problem The central problem for this work is to estimate the gradient of the expected loss:
B
Bθ
EZrF pθqs (cid:16) B
Bθ (cid:176)x1...L ppx1...L|x0; θqf pxL; θq. (4)
Observe that when the noise cdf FZ is smooth, the expected network output EZrF pθqs is differentiable in parameters despite having binary activations and a head function possibly non-differentiable in xL. This can be easily seen from the right hand side of (4), where we used the Bayesian network representation of SBN, in which all functions are differentiable in θ. The gradient estimation problem of this kind arises in several learning formulations, please see Appendix A for a motivation of the expected loss objective for training SBNs.
Bias-Variance Tradeoff A number of estimators for the gradient (4) exist, both biased and un-biased. Estimators using certain approximations and heuristics have been applied to deep SBNs with remarkable success. The use of approximations however introduces a systematic error, i.e. these estimators are biased. Many theoretical works therefore have focused on development of lower-variance unbiased stochastic estimators, but encounter serious limitations when applied to deep models. At the same time, allowing a small bias may lead to a considerable reduction in variance, and more reliable estimates overall. We advocate this approach and compare the methods using metrics that take into account both the bias and the variance, in particular the mean squared error of the estimator. When the learning has converged to 100% training accuracy (as we will see experimentally for the proposed method) the fact that we used a biased gradient estimator perhaps does not matter. 2
Contribution The proposed Path Sample-Analytic (PSA) method is a biased stochastic estimator. It takes one sample from the model and then applies a series of derandomization and approximation steps. It efﬁciently approximates the expectation of the stochastic gradient by explicitly computing summations along multiple paths in the network Fig. 1 (right). Such explicit summation over many conﬁgurations gives a huge variance reduction, in particular for deep dependencies. The approximation steps needed for keeping the computation simple and tractable, are clearly understood linearizations. They are designed with the goal to obtain a method with the same complexity and structure as the usual backpropagation, including convolutional architectures. This allows to apply the method to deep models and to compute the gradients in parameters of all layers in a single backwards pass.
A second simpliﬁcation of the method is obtained by further linearizations in PSA and leads to the
Straight-Through (ST) method. We thus provide the ﬁrst theoretical justiﬁcation of straight-through methods for deep models as derived in the SBN framework using a clearly understood linearization and partial summation along paths. This allows to eliminates guesswork and obscurity in the practical application of such estimators as well as opens possibilities for improving them. Both methods perform similar in learning of deep convolutional models, delivering a signiﬁcantly more stable and better controlled training than preceding techniques. 1.1