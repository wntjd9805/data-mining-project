Abstract
We consider the problem of robust and adaptive model predictive control (MPC) of a linear system, with unknown parameters that are learned along the way (adaptive), in a critical setting where failures must be prevented (robust). This problem has been studied from different perspectives by different communities. However, the existing theory deals only with the case of quadratic costs (the LQ problem), which limits applications to stabilisation and tracking tasks only. In order to handle more general (non-convex) costs that naturally arise in many practical problems, we carefully select and bring together several tools from different communities, namely non-asymptotic linear regression, recent results in interval prediction, and tree-based planning. Combining and adapting the theoretical guarantees at each layer is non trivial, and we provide the ﬁrst end-to-end suboptimality analysis for this setting. Interestingly, our analysis naturally adapts to handle many models and combines with a data-driven robust model selection strategy, which enables to relax the modelling assumptions. Last, we strive to preserve tractability at any stage of the method, that we illustrate on two challenging simulated environments.1 1

Introduction
Despite the recent successes of Reinforcement Learning [e.g. 32, 38], it has hardly been applied in real industrial issues. This could be attributed to two undesirable properties which limit its practical applications. First, it depends on a tremendous amount of interaction data that cannot always be simulated. This issue can be alleviated by model-based methods – which we consider in this work – that often beneﬁt from better sample efﬁciencies than their model-free counterparts. Second, it relies on trial-and-error and random exploration. In order to overcome these shortages, and motivated by the path planning problem for a self-driving car, in this paper we consider the problem of controlling an unknown linear system x(t) so as to maximise an arbitrary bounded reward function R, in a 1Code and videos available at https://eleurent.github.io/robust-beyond-quadratic/. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
critical setting where mistakes are costly and must be avoided at all times. This choice of rich reward space is crucial to have sufﬁcient ﬂexibility to model non-convex and non-smooth functions that naturally arise in many practical problems involving combinatorial optimisation, branching decisions, etc., while quadratic costs are mostly suited for tracking a ﬁxed reference trajectory [e.g. 23]. Since experiencing failures is out of question, the only way to prevent them from the outset is to rely on some sort of prior knowledge. In this work, we assume that the system dynamics are partially known, in the form of a linear differential equation with unknown parameters and inputs. More precisely, we
Rr, consider a linear system with state x and following dynamics in the form:
Rp, acted on by controls u
Rq and disturbances ω
∈
∈
∈
˙x(t) = A(θ)x(t) + Bu(t) + Dω(t), t 0, (1)
≥
∈
∈
Rp
Rp
∈
∈
Rp
×
Rp
Rs is a measurement noise and C
Rd. p belongs to a compact set Θ where the parameter vector θ in the state matrix A(θ) r are known. We also assume having q and disturbance matrix D
The control matrix B
×
× access to the observation of x(t) and to a noisy measurement of ˙x(t) in the form y(t) = ˙x(t) + Cν(t), s is known. Assumptions over the disturbance where ν(t)
ω and noise ν will be detailed further, and we denote η(t) = Cν(t) + Dω(t). We argue that this structure assumption is realistic given that most industrial applications to date have been relying on physical models to describe their processes and well-engineered controllers to operate them, rather than machine learning. Our framework relaxes this modelling effort by allowing some structured uncertainty around the nominal model. We adopt a data-driven scheme to estimate the parameters more accurately as we interact with the true system. Many model-based reinforcement learning algorithms rely on the estimated dynamics to derive the corresponding optimal controls [e.g. 24, 28], but suffer from model bias: they ignore the error between the learned and true dynamics, which can dramatically degrade control performances [37].
⊂
∈
×
To address this issue, we turn to the framework of robust decision-making: considering a point estimate of the dynamics, for any N instead of merely
N, we build an entire conﬁdence region
Θ, illustrated in Figure 1, that contains the true dynamics parameter with high probability:
∈
N,δ
C
⊂
P (θ
N,δ)
∈ C
δ, 1
−
≥ (2)
∈ (0, 1). In Section 2, having observed a history where δ
[N ] of transitions, our ﬁrst contribution extends the work of Abbasi-Yadkori et al. [2] who provide a conﬁdence ellipsoid for the least-square estimator to our setting of feature matrices, rather than feature vectors.
The robust control objective V r [8, 9, 18] aims to maximise the worst-case outcome with respect to this conﬁdence region (xn, yn, un)
{
}n
∈
N =
D
N,δ:
C
V r(u), where
V r(u) sup
N (Rq) u
∈ (cid:34)
∞(cid:88) (cid:35)
γnR(xn(u, ω))
, (3) n=N +1 def
= inf
θ
N,δ
∈C
R
[ω,ω]
ω
∈
∈ (0, 1) is a discount factor, and xn(u, ω) is the state reached at step n under controls u and
γ disturbances ω(t) within the given admissible bounds [ω(t), ω(t)]. Maximin problems such as (3) are notoriously hard if the reward R has a simple form. However, without a restriction on the shape of functions R, we cannot hope to derive an explicit solution. In our second contribution, we propose a robust MPC algorithm for solving (3) numerically. In Section 3, we leverage recent results from the uncertain system simulation literature to derive an interval predictor [x(t), x(t)] for the system
N, this predictor takes the information on the current state (1), illustrated in Figure 2. For any N xN , the conﬁdence region
N,δ, planned control sequence u and admissible disturbance bounds
[ω(t), ω(t)]; and must verify the inclusion property:
∈
C x(t) x(t) x(t), t
∀
≥ tN .
≤
≤ (4)
Since R is generic, potentially non-smooth and non-convex, solving the optimal – not to mention the robust – control objective is intractable. In Section 4, facing a sequential decision problem with continuous states, we turn to the literature of tree-based planning algorithms. Although there exist works addressing continuous actions [10, 41], we resort to a ﬁrst approximation and discretise the continuous decision (Rq)N space by adopting a hierarchical control architecture: at each time, the agent can select a high-level action a from a ﬁnite space
. Each action a corresponds to the
Kax(t) + ua. For selection of a low-level controller πa, that we take afﬁne: u(t) = πa(x(t))
∈ A def
=
A
− 2
Figure 1: The model estimation procedure, run-ning on the obstacle avoidance problem of Sec-tion 6. The conﬁdence region CN,δ shrinks with the number of samples N .
Figure 2: The state prediction procedure running on the obstacle avoidance problem of Section 6.
At each time step (red to green), we bound the set of reachable states under model uncertainty (2)
Algorithm 1 Robust Estimation, Prediction and Control
Input: conﬁdence level δ, structure (A, φ), reward R, for N = 0, 1, 2, . . . do
D[0] ← ∅
, a1 ← ∅
N,δ
MODEL ESTIMATION(
C for each planning step k
←
N ). (9)
D
N, . . . , N + K
}
INTERVAL PREDICTION(
C
∈ {
[xk+1, xk+1] ak+1 ←
←
PESSIMISTIC PLANNING(Rk+1([xk+1, xk+1])). (13)
= N + [K] do
N,δ, akb) for each action b end for
Execute the recommended control uN +1, and add the transition (xN +1, yN +1, uN +1) to
. (11)
∈ A end for
D[N +1].
−
∈A instance, a tracking a subgoal xg can be achieved with πg = K(xg x). This discretisation induces a suboptimality, but it can be mitigated by diversifying the controller basis. The robust objective (3)
N V r(a), where xn(a, ω) stems from (1) with un = πan (xn). However, tree-based becomes supa planning algorithms are designed for a single known generative model rather than a conﬁdence region for the system dynamics. Our third contribution adapts them to the robust objective (3) by approximating it with a tractable surrogate ˆV r that exploits the interval predictions (4) to deﬁne a pessimistic reward. In our main result, we show that the best surrogate performance achieved during planning is guaranteed to be attained on the true system, and provide an upper bound for the approximation gap and suboptimality of our framework in Theorem 3. This is the ﬁrst result of this kind for maximin control with generic costs to the best of our knowledge. Algorithm 1 shows the full integration of the three procedures of estimation, prediction and control.
In Section 5, our forth contribution extends the proposed framework to consider multiple modelling assumptions, while narrowing uncertainty through data-driven model rejection, and still ensuring safety via robust model-selection during planning.
Finally, in Section 6 we demonstrate the applicability of Algorithm 1 in two numerical experiments: a simple illustrative example and a more challenging simulation for safe autonomous driving.
Notation The system dynamics are described in continuous time, but sensing and control happen in discrete time with time-step dt > 0. For any variable z, we use subscript to refer to these discrete
N. We use bold symbols to denote temporal sequences times: zn = z(tn) with tn = ndt and n
= z+ + z− and [n] = z = (zn)n
N. We denote z+ = max(z, 0), z− = z+ z,
∈ z
∈
−
|
| 1, . . . , n
{
.
} 1.1