Abstract
In this paper, we study the problem of signal estimation from noisy non-linear measurements when the unknown n-dimensional signal is in the range of an L-Lipschitz continuous generative model with bounded k-dimensional inputs. We make the assumption of sub-Gaussian measurements, which is satisﬁed by a wide range of measurement models, such as linear, logistic, 1-bit, and other quantized models. In addition, we consider the impact of adversarial corruptions on these measurements. Our analysis is based on a generalized Lasso approach (Plan and
Vershynin, 2016). We ﬁrst provide a non-uniform recovery guarantee, which states that under i.i.d. Gaussian measurements, roughly O (cid:0) k (cid:15)2 log L(cid:1) samples sufﬁce for recovery with an (cid:96)2-error of (cid:15), and that this scheme is robust to adversarial noise.
Then, we apply this result to neural network generative models, and discuss various extensions to other models and non-i.i.d. measurements. Moreover, we show that our result can be extended to the uniform recovery guarantee under the assumption of a so-called local embedding property, which is satisﬁed by the 1-bit and censored
Tobit models. 1

Introduction
In standard compressive sensing (CS) [9, 37], one considers a linear observation model of the form yi = (cid:104)ai, x∗(cid:105) + (cid:15)i, (1) where x∗ ∈ Rn is an unknown k-sparse signal vector, ai ∈ Rn is the i-th measurement vector, and (cid:15)i ∈ R is the noise term. The goal is to accurately recover x∗ given A and y, where A ∈ Rm×n is i , and y ∈ Rm is the vector of observations. To obtain the measurement matrix whose i-th row is aT an estimate of x∗, a natural idea is to minimize the (cid:96)2 loss subject to a structural constraint: i = 1, 2, . . . , m, minimize (cid:107)Ax − y(cid:107)2 (2) where K captures the structure of x∗; this may be set to be the set of all k-sparse vectors in Rn, or for computational reasons, may instead be the scaled (cid:96)1-ball, giving rise to the constrained Lasso [33].
We refer to (2) as the K-Lasso (with observations y and measurement matrix A). subject to x ∈ K,
Despite the far-reaching utility of standard CS, in many real-world applications the assumption of a linear model is too restrictive. To address this problem, the semi-parametric single index model (SIM) is considered in various papers [10, 25, 26]: yi = f ((cid:104)ai, x∗(cid:105)), (3) where f : R → R is an unknown (possibly random) function that is independent of ai. In general, f plays the role of a nonlinearity, and we aim to estimate the signal x∗ despite this unknown nonlinearity.
Note that the norm of x∗ is sacriﬁced in SIM, since it may be absorbed into the unknown function f .
Hence, for simplicity of presentation, we assume that x∗ is a unit vector in Rn. In this paper, similar i = 1, 2, . . . , m, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to that in [26], we make the assumption that the random variables yi are sub-Gaussian; see Section 2.2 for several examples. In addition, to further strengthen the robustness guarantees, we also allow for adversarial noise. That is, we consider the case of corrupted observations ˜y that can be produced from y in an arbitrary manner (possibly depending on A) subject to an (cid:96)2-norm constraint.
Motivated by the tremendous success of deep generative models in abundance of real applications [8], a new perspective of CS has recently emerged, in which the assumption that the underlying signal can be well-modeled by a (deep) generative model replaces the common sparsity assumption [2]. In addition to the theoretical developments, existing works have presented impressive numerical results for CS with generative models, with large reductions (e.g., a factor of 5 to 10) in the required number of measurements compared to sparsity-based methods [2]. 1.1