Abstract
Deep reinforcement learning includes a broad family of algorithms that parame-terise an internal representation, such as a value function or policy, by a deep neural network. Each algorithm optimises its parameters with respect to an objective, such as Q-learning or policy gradient, that deﬁnes its semantics. In this work, we propose an algorithm based on meta-gradient descent that discovers its own objective, ﬂexibly parameterised by a deep neural network, solely from interactive experience with its environment. Over time, this allows the agent to learn how to learn increasingly effectively. Furthermore, because the objective is discovered online, it can adapt to changes over time. We demonstrate that the algorithm discovers how to address several important issues in RL, such as bootstrapping, non-stationarity, and off-policy learning. On the Atari Learning Environment, the meta-gradient algorithm adapts over time to learn with greater efﬁciency, eventually outperforming the median score of a strong actor-critic baseline. 1

Introduction
Recent advances in supervised and unsupervised learning have been driven by a transition from handcrafted expert features to deep representations [15]; these are typically learned by gradient descent on a suitable objective function to adjust a rich parametric function approximator. As a ﬁeld, reinforcement learning (RL) has also largely embraced the transition from handcrafting features to handcrafting objectives: deep function approximation has been successfully combined with ideas such as TD-learning [30, 34], Q-learning [42, 23], double Q-learning [36, 37], n-step updates [32, 14], general value functions [33, 18], distributional value functions [7, 3], policy gradients [43, 21] and a variety of off-policy actor-critics [8, 10, 29]. In RL, the agent doesn’t have access a differentiable performance metric, thus choosing the right proxy is of particular importance: indeed, each of the aforementioned algorithms differs fundamentally in their choice of objective, designed in each case by expert human knowledge. The deep RL version of these algorithms is otherwise very similar in essence: updating parameters via gradient descent on the corresponding objective function.
Our goal is an algorithm that instead learns its own objective, and hence its own deep reinforcement learning algorithm, solely from experience of interacting with its environment. Following the principles of deep learning, we parameterise the objective function by a rich function approximator, and update it by meta-gradient learning [28, 1, 11, 44, 47, 39, 2, 20] – i.e. by gradient descent on the sequence of gradient descent updates resulting from the choice of objective function – so as to maximise a naive outer loss function (such as REINFORCE) with minimal initial knowledge.
Importantly, and in contrast to the majority of recent work on meta-learning [11, 2, 20], our meta-gradient algorithm learns online, on a single task, during a single “lifetime" of training. This online approach to meta-learning confers several advantages. First, an online learning algorithm can be applied to any RL environment, and does not require a distribution of related environments, nor the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
ability to reset and rerun on different environments. Second, an online learning algorithm can adapt the objective function as learning progresses, rather than assume a global, static “one-size-ﬁts-all" objective. Our hypothesis is that an online meta-gradient learning agent will, over time, learn to learn with greater efﬁciency, and in the long-run this will outperform a ﬁxed (handcrafted) objective.
We show in toy problems that our approach can discover how to address important issues in RL, such as bootstrapping and non-stationarity. We also applied our algorithm for online discovery of an off-policy learning objective to independent training runs on each of 57 classic Atari games.
Augmented with a simple heuristic to encourage consistent predictions, our meta-gradient algorithm outperformed the median score of a strong actor-critic baseline on this benchmark. 2