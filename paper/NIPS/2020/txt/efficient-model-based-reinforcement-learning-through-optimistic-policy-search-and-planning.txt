Abstract
Model-based reinforcement learning algorithms with probabilistic dynamical models are amongst the most data-efﬁcient learning methods. This is often attributed to their ability to distinguish between epistemic and aleatoric uncertainty.
However, while most algorithms distinguish these two uncertainties for learning the model, they ignore it when optimizing the policy, which leads to greedy and insufﬁcient exploration. At the same time, there are no practical solvers
In this paper, we propose a practical for optimistic exploration algorithms. optimistic exploration algorithm (H-UCRL). H-UCRL reparameterizes the set of plausible models and hallucinates control directly on the epistemic uncertainty.
By augmenting the input space with the hallucinated inputs, H-UCRL can be solved using standard greedy planners. Furthermore, we analyze H-UCRL and construct a general regret bound for well-calibrated models, which is provably sublinear in the case of Gaussian Process models. Based on this theoretical foundation, we show how optimistic exploration can be easily combined with state-of-the-art reinforcement learning algorithms and different probabilistic models. Our experiments demonstrate that optimistic exploration signiﬁcantly speeds-up learning when there are penalties on actions, a setting that is notoriously difﬁcult for existing model-based reinforcement learning algorithms. 1

Introduction
Model-Based Reinforcement Learning (MBRL) with probabilistic dynamical models can solve many challenging high-dimensional tasks with impressive sample efﬁciency (Chua et al., 2018). These algorithms alternate between two phases: ﬁrst, they collect data with a policy and ﬁt a model to the data; then, they simulate transitions with the model and optimize the policy accordingly. A key feature of the recent success of MBRL algorithms is the use of models that explicitly distinguish between epistemic and aleatoric uncertainty when learning a model (Gal, 2016). Aleatoric uncertainty is in-herent to the system (noise), whereas epistemic uncertainty arises from data scarcity (Der Kiureghian and Ditlevsen, 2009). However, to optimize the policy, practical algorithms marginalize over both the aleatoric and epistemic uncertainty to optimize the expected performance under the current model, as in PILCO (Deisenroth and Rasmussen, 2011). This greedy exploitation can cause the optimization to
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Final returns in an inverted pendulum swing-up task with sparse rewards. As the action penalty increases, exploration through noise is penalized and algorithms get stuck in a local minimum, where the pendulum is kept at the bottom position. Instead, H-UCRL is able to solve the swing-up task reliably. This holds for for all considered dynamical models: Deterministic- (DE) and Probabilistic
Ensembles (PE) of neural networks as well as Gaussian Processes (GP) models. get stuck in local minima even in simple environments like the swing-up of an inverted pendulum: In
Fig. 1, all methods can solve this problem without action penalties (left plot). However, with action penalties, the expected reward (under the epistemic uncertainty) of swinging up the pendulum is low relative to the cost of the maneuver. Consequently, the greedy policy does not actuate the system at all and fails to complete the task. While optimistic exploration is a well-known remedy, there is currently a lack of efﬁcient, principled means of incorporating optimism in deep MBRL.
Contributions Our main contribution is a novel optimistic MBRL algorithm, Hallucinated-UCRL (H-UCRL), which can be applied together with state-of-the-art RL algorithms (Section 3). Our key idea is to reduce optimistic exploration to greedy exploitation by reparameterizing the model-space using a mean/epistemic variance decomposition. In particular, we augment the control space of the agent with hallucinated control actions that directly control the agent’s epistemic uncertainty about the 1-step ahead transition dynamics (Section 3.1). We provide a general theoretical analysis for
H-UCRL and prove sublinear regret bounds for the special case of Gaussian Process (GP) dynamics models (Section 3.2). Finally, we evaluate H-UCRL in high-dimensional continuous control tasks that shed light on when optimistic exploration outperforms greedy exploitation and Thompson sampling (Section 4). To the best of our knowledge, this is the ﬁrst approach that successfully implements optimistic exploration with deep-MBRL.