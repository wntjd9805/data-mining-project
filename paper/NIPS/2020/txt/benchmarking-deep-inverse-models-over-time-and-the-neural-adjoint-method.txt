Abstract
We consider the task of solving generic inverse problems, where one wishes to determine the hidden parameters of a natural system that will give rise to a particular set of measurements. Recently many new approaches based upon deep learning have arisen, generating promising results. We conceptualize these models as different schemes for efﬁciently, but randomly, exploring the space of possible inverse solutions. As a result, the accuracy of each approach should be evaluated as a function of time rather than a single estimated solution, as is often done now.
Using this metric, we compare several state-of-the-art inverse modeling approaches on four benchmark tasks: two existing tasks, a new 2-dimensional sinusoid task, and a challenging modern task of meta-material design. Finally, inspired by our conception of the inverse problem, we explore a simple solution that uses a deep neural network as a surrogate (i.e., approximation) for the forward model, and then uses backpropagation with respect to the model input to search for good inverse solutions. Variations of this approach - which we term the neural adjoint (NA) - have been explored recently on speciﬁc problems, and here we evaluate it comprehensively on our benchmark. We ﬁnd that the addition of a simple novel loss term - which we term the boundary loss - dramatically improves the NA’s performance, and it consequentially achieves the best (or nearly best) performance in all of our benchmark scenarios. 1

Introduction
In this work we consider the task of solving generic inverse problems. An inverse problem is characterized by a forward problem that models, for example, a real-world measurement process or an auxiliary prediction task. The forward problem can be written as y = f (x) (1) where y is the measurable data, f is a (non-)linear forward operator that models the measurement process, and x is an unobserved signal of interest. Given f , solving the inverse problem is then a matter of ﬁnding an inverse model x = f −1(y). However, if the problem is ill-posed (e.g., 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
non-existence, or non-uniqueness, of solutions), ﬁnding f −1 is a non-trivial task. Speciﬁc inverse problems can be solved using apriori knowledge about f , (e.g., sparsity in some basis, such as compressed sensing), however, we consider the task of solving generic inverse problems, where no such solutions are known.
Recently many new approaches based upon deep learning have arisen, generating impressive results.
These methods typically require a dataset of sample pairs {xn, yn}N n=1 from f , from which a deep neural network model can be trained to approximate the inverse model, ˆf −1. Some recent examples include models based on normalizing ﬂows (e.g., invertible neural networks [1, 2]), variational auto-encoders [3], tandem architectures [4, 5]. 1.1 Modern inverse models as stochastic search
Despite the apparent variety of recent approaches, most of these inverse models can be written in the form ˆx = ˆf −1(y, z), where z is randomly drawn from some probability distribution Z (e.g.,
Gaussian). Although the interpretation of z varies across these models, they all share the property that the ˆx returned by the model will vary depending upon the value of z. Furthermore, since it is usually trivial and fast to evaluate the accuracy of a candidate inverse solution using the forward model, f (e.g., a simulator), one can search for more accurate inverse solutions by sampling multiple values of z, each yielding a different inverse solution. Each solution can then be validated using f , and the best solution among all candidates can be retained. Therefore, each modern inverse model can be viewed as a means of efﬁciently, but nonetheless stochastically, searching through x-space for good solutions.
From this perspective, the performance of each inverse model depends upon the number of z samples that are considered, denoted T . For example, one model may perform best when T = 1, while another model performs best as T grows. Our experiments here show that this is indeed the case, and model performance (relative to others) is highly dependent upon T . Typically however the performance, r, of an inverse models is judged by estimating its expected “re-simulation” error [2] over the data and latent variable distributions, denoted D and Z respectively. Mathematically, we have r = E(x,y)∼D,z∼Z[L(ˆy(z), y)] (2) where ˆy(z) = f ( ˆf −1(y, z)) is the "re-simulated" value of y produced by passing ˆx (an estimate) through the forward model, and L is the user-chosen loss function (e.g., L2 loss). The metric r effectively measures error under the assumption we always utilize one sample of z (given a target y). Here we propose an alternative metric that quantiﬁes the expected minimum error if we draw a sequence of z values of length T , denoted ZT . Formally, this is given by rT = E(x,y)∼D,ZT ∼Ω (cid:104) min z∈ZT (cid:105)
[L(ˆy(z), y)] (3) where ZT is a sequence of length T drawn from a distribution Ω. This measure characterizes the expected loss of an inverse model as a function of the number of samples of z we can consider for each target y. In this work we conduct a benchmark study of four tasks with rT , and we ﬁnd that the performance of modern inverse models depends strongly on T , revealing the limitation of existing metrics, and revealing useful insights about the way in which each model stochastically searches x-space. In particular, we present analysis suggesting that modern inverse models suffer from one or both of the following limitations in their search process: (i) they don’t fully explore x-space, missing some solutions; or (ii) they do not precisely localize the optimal solutions, introducing error. 1.2 The neural-adjoint method
Inspired by our conception of the inverse problem, we explore a simple solution where the main idea is to train a neural network to approximate f and then, starting from different random locations in x-space, use ∂ ˆf /∂x to descend towards locally optimal x values. Variations of this approach have recently been employed on a few speciﬁc problems [6, 7], however, here we evaluate its competitiveness against other modern approaches on several tasks. We also add a novel simple term to its loss function - which we term the boundary loss - that dramatically improves its performance.
We call the resulting method the Neural Adjoint (NA), due to its resemblance to the classical Adjoint method for inverse design [8, 9]. Surprisingly, the relatively simple NA approach almost always yields the lowest error among all models, tasks and T -values considered in our benchmarks. Our 2
analysis suggests that, in contrast to other models, NA fully explores the x-space, and also accurately localizes inverse solutions. NA achieves this advantage at the cost of signiﬁcantly higher computation time, which as we discuss, may disqualify it from some time-sensitive applications.
In summary, the three primary contributions of this work are as follows: 1. A comprehensive benchmark comparison using rT . We compare ﬁve modern inverse models on four benchmark tasks. The results reveal the performance of modern models under many different conditions, and we ﬁnd that their accuracy depends strongly on T . 2. A new modern benchmark task, and a general method to replicate it. We introduce a contemporary and challenging inverse problem for meta-material design. Normally, it would be difﬁcult for others to replicate our studies because requires sophisticated electromagnetic simulations. However, we introduce a strategy for creating simple, fast, and sharable approximate simulators for complex problems, permitting easy replication. 3. The neural-adjoint (NA) method. The NA nearly always outperforms all other models we consider in our benchmark. Furthermore, our analysis provides insights about the limitations of existing models, and why NA is effective.
We release code for all inverse models, as well as (fast) simulation software for each benchmark problem, so that other researchers can easily repeat our experiments. 1 2