Abstract
Discrete integration is a fundamental problem in computer science that concerns the computation of discrete sums over exponentially large sets. Despite intense interest from researchers for over three decades, the design of scalable techniques for computing estimates with rigorous guarantees for discrete integration remains the holy grail. The key contribution of this work addresses this scalability challenge via an efﬁcient reduction of discrete integration to model counting. The proposed reduction is achieved via a signiﬁcant increase in the dimensionality that, contrary to conventional wisdom, leads to solving an instance of the relatively simpler problem of model counting. Building on the promising approach proposed by Chakraborty et al [9], our work overcomes the key weakness of their approach: a restriction to dyadic weights. We augment our proposed reduction, called DeWeight, with a state of the art efﬁcient approximate model counter and perform detailed empirical analysis over benchmarks arising from neural network veriﬁcation domains, an emerging application area of critical importance. DeWeight, to the best of our knowledge, is the ﬁrst technique to compute estimates with provable guarantees for this class of benchmarks. 1

Introduction
Given a large set of items S and a weight function W that assigns weight to each of the items, the problem of discrete integration, also known as weighted counting, is to compute the weight of S, deﬁned as the weighted sum of items of S. Often the set S is implicitly represented, for example as the set of assignments that satisfy a given set of constraints. Such a representation allows one to represent an exponentially large set with polynomially many constraints, and thus capture other representation forms such as probabilistic graphical models [12, 40]. In particular, it is commonplace to implicitly represent S as the set of assignments that satisfy a Boolean formula F by using standard transformations from discrete domains to Boolean variables [6]. This allows one to leverage powerful combinatorial solvers within the process of answering the discrete integration query [24, 25, 8].
Discrete integration is a fundamental problem in computer science with a wide variety of applications covering partition function computations [23, 1], probabilistic inference [33, 51], network reliability estimation [20], and the veriﬁcation of neural networks [5, 36, 37], which is the topic of this work.
In particular, recent work demonstrated that determining properties of neural networks, such as adversarial robustness, fairness, and susceptibility to trojan attacks, can be reduced to discrete integration. Since discrete integration is #P-complete even when the weight function is constant [47], this has motivated theoreticians and practitioners alike to focus on approximation techniques.
Of particular interest in many settings, including neural network veriﬁcation, is the case where the weight function can be expressed as log-linear distribution. Log-linear models are employed to capture a wide variety of distributions that arise from graphical models, conditional random ﬁelds, skip-gram models, and the like [28]. In the context of discrete integration, log-linear models are also 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
known as literal-weighted. Formally, the weight function W assigns a weight to each variable xi such that W (xi) + W (¬xi) = 1 (a variable or its negation is called a literal) [12]. Furthermore, the weight of an assignment σ ∈ is deﬁned as the product of all literals that are true in that assignment. For completeness, we formally state the equivalence between the two notions in Section 2. A special case of discrete integration is model counting, wherein the weight functions assigns a constant weight of 1 2 to each literal. In this special case, the computation of the weight of S reduces to computing the size of S, i.e. the number of solutions to the constraints. While from complexity theoretic viewpoint both discrete integration and model counting are #P-hard, tools for model counting have had signiﬁcantly better scalability than those for discrete integration in practice [44, 17].
In this paper, we make a concrete progress towards answering discrete integration queries. For that, we follow the work of Chakbraborty et al. [9] that demonstrated a reduction from discrete integration to model counting. The general idea is to construct another formula G from a discrete integration query (F, W ) such that that one can recover the weight of F from the model count of G. We can then use a hashing-based tool [11, 44, 43] to compute an approximate model count of G. Finally, we use this count to compute an approximation to the weight of F (with theoretical guarantees). Even though adding new variables in G increases the dimensionality of the problem, we turn this seeming curse into a boon by beneﬁting from scalability of model counting techniques.
Although promising, the reduction of [9] has a key weakness: it can only handle weight functions where each literal weight is dyadic, i.e. the weight of each xi is of the form ki 2mi for some integers ki and mi. Thus a pre-processing step is required to adjust each weight to a nearby dyadic weight.
Moreover, the reduction of [9] simulates the weight function W by adding variables and constraints in number proportional to mi. Scaling to large queries such as those obtained in neural network veriﬁcation thus entails using crude dyadic weights that poorly approximate the original query.
The primary technical contribution of this work is to lift the dyadic weights requirement. To this end, we introduce a new technique that can handle all rational weights, i.e. all weights of the form
W (xi) = pi
, while adding only an overhead of (cid:100)log2(max(pi, qi − pi))(cid:101) additional variables for qi each xi. Thus, the pre-processing step of [9] is no longer required and and the designer has the ability to use accurate weights in the reduction that entail a complete guarantee for his estimates. A key strength of our approach is in the simplicity and the negligible runtime requirements of the reduction subroutine. When the overhead for every xi is, however, budgeted to mi additional variables, we present a new algorithm to compute the closest fraction p/q to W (xi) such that mi ≤ log2(p, q − p).
Our algorithm employs an elegant connection to Farey Sequences [41].
Our technique results in a prototype implementation of our tool, called DeWeight. DeWeight employs the state-of-the-art approximate model counter ApproxMC [10, 11, 44] and allows theoretical ((cid:15), δ)-guarantees for its estimates with no required weight adjustments. We perform a detailed evaluation on benchmarks arising from the domain of neural network veriﬁcation.1 Based on our empirical evaluation, DeWeight is, to the best of our knowledge, the ﬁrst technique that can handle these benchmark instances while providing theoretical guarantees of its estimates. Our empirical results show that approximation of weights to the closest dyadic fractions leads to an astronomically large multiplicative error of over 108 in even very simple weight settings, while our reduction is exact.
As a strong evidence to the necessity of our tool, recent work on neural network veriﬁcation [5, 36, 37] was restricted to the uniformly weighted case (i.e., where for all xi, W (xi) = 1 2 ) due to unavailability of scalable techniques for discrete integration; the inability of existing (weighted) discrete integration tools to handle such benchmarks is well noted, and noted as promising future work in [5]. Our work thus contributes to broadening the scope of neural network veriﬁcation efforts.