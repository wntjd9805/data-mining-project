Abstract
Finding an effective medical treatment often requires a search by trial and error.
Making this search more efﬁcient by minimizing the number of unnecessary trials could lower both costs and patient suffering. We formalize this problem as learning a policy for ﬁnding a near-optimal treatment in a minimum number of trials using a causal inference framework. We give a model-based dynamic programming algorithm which learns from observational data while being robust to unmeasured confounding. To reduce time complexity, we suggest a greedy algorithm which bounds the near-optimality constraint. The methods are evaluated on synthetic and real-world healthcare data and compared to model-free reinforcement learning. We
ﬁnd that our methods compare favorably to the model-free baseline while offering a more transparent trade-off between search time and treatment efﬁcacy. 1

Introduction
Finding a good treatment for a patient often involves trying out different options before a satisfactory one is found (Murphy et al., 2007). If the ﬁrst-line drug is ineffective or has severe side-effects, guidelines may suggest it is replaced by or combined with another drug (Singh et al., 2016). These steps are repeated until an effective combination of drugs is found or all options are exhausted, a process which may span several years (NCCMH, 2010). A long search adds to patient suffering and postpones potential relief. It is therefore critical that this process is made as time-efﬁcient as possible.
We formalize the search for effective treatments as a policy optimization problem in an unknown decision process with ﬁnite horizon (Garcia and Ndiaye, 1998). This has applications also outside of medicine: For example, in recommendation systems, we may sequentially propose new products or services to users with the hope of ﬁnding one that the user is interested in. Our goal is to perform as few trials as possible until the probability that there are untried actions which are signiﬁcantly better is small—i.e., a near-optimal action has been found with high probability. Historical observations allow us to transfer knowledge and perform this search more efﬁciently for new subjects. As more actions are tried and their outcomes observed, our certainty about the lack of better alternatives increases. Importantly, even a failed trial may provide information that can guide the search policy.
In this work, we restrict our attention to actions whose outcomes are stationary in time. This implies both that repeated trials of the same action have the same outcome and that past actions do not causally impact the outcome of future actions. The stationarity assumption is justiﬁed, for example,
⇤This work was completed while the author was afﬁliated with Chalmers University of Technology.
†This work was completed while the author was afﬁliated with Harvard University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for medical conditions where treatments manage symptoms but do not alter the disease state itself, or where the impact of sequential treatments is known to be additive. In such settings, past actions and outcomes may help predict the outcomes of future actions without having a causal effect on them.
We formalize learning to search efﬁciently for causally effective treatments as off-policy optimization of a policy which ﬁnds a near-optimal action for new contexts after as few trials as possible. Our set-ting differs from those typical of reinforcement or bandit learning (Sutton et al., 1998): (i) Solving the problem relies on transfer of knowledge from observational data. (ii) The stopping (near-optimality) criterion depends on a model of unobserved quantities. (iii) The number of trials in a single sequence is bounded by the number of available actions. We address identiﬁcation of an optimal policy using a causal framework, accounting for potential confounding. We give a dynamic programming algorithm which learns policies that satisfy a transparent constraint on near-optimality for a given level of conﬁdence, and a greedy approximation which satisﬁes a bound on this constraint. We show that greedy policies are sub-optimal in general, but that there are settings where they return policies with informative guarantees. In experiments, including an application derived from antibiotic resistance tests, our algorithms successfully learn efﬁcient search policies and perform favorably to baselines. 2