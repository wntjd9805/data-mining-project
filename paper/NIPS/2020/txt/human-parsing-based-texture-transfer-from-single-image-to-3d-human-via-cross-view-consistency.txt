Abstract
This paper proposes a human parsing based texture transfer model via cross-view consistency learning to generate the texture of 3D human body from a single image.
We use the semantic parsing of human body as input for providing both the shape and pose information to reduce the appearance variation of human image and preserve the spatial distribution of semantic parts. Meanwhile, in order to improve the prediction for textures of invisible parts, we explicitly enforce the consistency across different views of the same subject by exchanging the textures predicted by two views to render images during training. The perceptual loss and total variation regularization are optimized to maximize the similarity between rendered and input images, which does not necessitate extra 3D texture supervision. Experimental results on pedestrian images and fashion photos demonstrate that our method can produce higher quality textures with convincing details than other texture generation methods. Code is available at https://github.com/zhaofang0627/HPBTT. 1

Introduction
Rebuilding 3D model of human body from 2D images is of great value for many applications, such as virtual reality, movie making, clothes try-on, generation of synthetic data for learning. Particularly, generating the 3D human model from a single image has been extensively studied in recent years due to its potential practical value. However, most research works mainly focus on estimating the pose and shape of the human body [17, 31, 34, 4, 14] and very few works aim at addressing the texture generation problem.
In existing methods, [18] introduces texture inference as prediction of an image in a canonical appearance space and optimizes the perceptual metric between the rendered image and the input image. [36] proposes to generate textures of human bodies under the supervision of person re-identiﬁcation (re-ID), which utilizes the distance metric learned by the re-ID task. [23] infers textures in a UV-space using an image-to-image translation method, which registers the SMPL model [26] to 3D scans of people to generate ground-truth 3D textures for training. [27] builds a paired dataset of 3D garments and 2D clothing images as training data and then learns a dense mapping from garment silhouettes to a UV map of a 3D garment model.
There still exist some issues that have not been handled well for generating textures of human body from a single image. Firstly, obtaining ground-truth 3D textures is time-consuming and labor-intensive. Secondly, textures of invisible human body parts are difﬁcult to predict due to only one image available and lack of information from other views at inference. Thirdly, the diversity of
∗Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Examples of the generated textures on (a) Market-1501 [39] and (b) DeepFashion [25]. human pose and appearance makes the model hard to ﬁt, especially when ground-truth 3D texture supervision is unavailable.
To address the aforementioned issues, we propose a human parsing based texture transfer model via cross-view consistency learning to generate the texture of 3D human body from a single image, without using 3D texture supervision. Examples of the generated textures are shown in Fig. 1. We
ﬁrst use the semantic parsing of human body as the model input. Compared to image pixels and silhouettes, human parsing reduces the appearance variation of human image and preserves its pose information. Then, an encoder employs two Convolutional Neural Networks (CNN) to extract shape and pose features from the human parsing, respectively. After that, a decoder combines features of shape and pose and deconvolves to produce a texture ﬂow, which stores coordinates of the input image to sample pixel values of a texture image from. In order to improve the texture prediction for invisible parts of human body, we explicitly enforce the cross-view consistency of texture prediction between two images with different views of the same subject during training. Speciﬁcally, the texture predicted by one view is used to render with the 3D mesh of another view and enforced to match the input image of another view. Finally, we optimize the perceptual loss and total variation regularization to maximize the similarity between rendered and input images.
Our main contributions include the following three aspects: 1) We propose a novel texture transfer model to effectively generate textures of 3D human body from a single image via cross-view consistency learning. 2) We leverage the semantic parsing of human body as input to reduce the human appearance variation and preserve the pose information. 3) Our model produces the state of 2
the art quality of textures on both pedestrian images from the surveillance scene and fashion photos from the web. 2