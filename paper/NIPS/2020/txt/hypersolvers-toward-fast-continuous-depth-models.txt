Abstract
The inﬁnite–depth paradigm pioneered by Neural ODEs has launched a renais-sance in the search for novel dynamical system–inspired deep learning primitives; however, their utilization in problems of non–trivial size has often proved impossi-ble due to poor computational scalability. This work paves the way for scalable
Neural ODEs with time–to–prediction comparable to traditional discrete networks.
We introduce hypersolvers, neural networks designed to solve ODEs with low overhead and theoretical guarantees on accuracy. The synergistic combination of hypersolvers and Neural ODEs allows for cheap inference and unlocks a new frontier for practical application of continuous–depth models. Experimental evaluations on standard benchmarks, such as sampling for continuous normalizing
ﬂows, reveal consistent pareto efﬁciency over classical numerical methods. 1

Introduction
The framework of neural ordinary differential equations (Neural ODEs) (Chen et al., 2018) has reinvigorated research in continuous deep learning (Zhang et al., 2014), offering new system–theoretic perspectives on neural network architecture design (Greydanus et al., 2019;
Bai et al., 2019; Poli et al., 2019; Cranmer et al., 2020) and generative modeling (Grathwohl et al., 2018; Yang et al., 2019). Despite the successes, Neural ODEs have been met with skep-ticism, as these models are often slow in both training and inference due to heavy numeri-cal solver overheads. These issues are further exacerbated by applications which require ex-tremely accurate numerical solutions to the differential equations, such as physics–inspired neu-ral networks (Raissi et al., 2019) and continuous normalizing ﬂows (CNFs) (Chen et al., 2018).
Common knowledge within the ﬁeld is that these models appear too slow in their current form for meaningful large-scale or embedded applica-tions. Several attempts have been made to either directly or indirectly address some of these lim-itations, such as redeﬁning the forward pass as a root ﬁnding problem (Bai et al., 2019), introduc-ing ad hoc regularization terms (Finlay et al., 2020; Massaroli et al., 2020a) and augment-ing the state to reduce stiffness of the solutions (Dupont et al., 2019; Massaroli et al., 2020b).
Unfortunately, these approaches either give up on the Neural ODE formulation altogether, do not re-duce computation overhead sufﬁciently or introduce additional memory requirements. Although there is no shortage of works utilizing Neural ODEs in forecasting or classiﬁcation tasks (Yıldız et al., 2019;
Figure 1: Hypersolvers for density estimation via continuous normalizing ﬂows: dopri5 infer-ence accuracy is achieved with 100x speedup.
∗Equal contribution. Author order was decided by ﬂipping a coin. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Jia & Benson, 2019; Kidger et al., 2020), current state–of–the–art is limited to ofﬂine applications with no constraints on inference time. In particular, high–potential application domains for Neural ODEs such as control and prediction often deal with tight requirements on inference speed and computation e.g robotics (Hester, 2013) that are not currently within reach. For example, a generic state–of–the–art convolutional Neural ODE takes at least an order of magnitude2 longer to infer the label of a single
MNIST image. This inefﬁciency results in inference passes far too slow for real–time applications.
NFEs
Local Error
Method p-th order solver O(pK) O((cid:15)p+1)
O(˜(cid:15)p+1) adaptive–step solver −
Euler hypersolver O(K) O(δ(cid:15)2) p-th order hypersolver O(pK) O(δ(cid:15)p+1)
Model–solver synergy The interplay between
Neural ODEs and numerical solvers has largely been overlooked as research on model variants has been predominant, often treating solver choice as a simple hyper–parameter to be tuned based on em-pirical observations. Here, we argue for the im-portance of computational scalability outside of speciﬁc Neural ODE architectural modiﬁcations, and highlight the synergistic combination of model– solver to be a likely candidate for unlocking the full potential of continuous–depth models. Namely, this work attempts to alleviate computational over-heads by introducing the paradigm of Neural ODE hypersolvers; these auxiliary neural networks are trained to solve the initial value problem (IVP) emerging from the forward pass of continuous–depth models. Hypersolvers improve on the computation–correctness trade–off provided by traditional numerical solvers, enabling fast and arbitrarily accurate solutions during inference.
Figure 2: Asymptotic complexity comparison.
Number of function evaluations (NFEs) needed to compute K solver’s steps. (cid:15) is the step size,
˜(cid:15) is the max step size of adaptive solvers, δ (cid:28) 1 is correlated to the hypersolver training results.
Pareto efﬁciency The trade–off between solution accuracy and computation is one of the oldest and best–studied topics in the numerics literature (Butcher, 2016) and was mentioned in the seminal work (Chen et al., 2018) as a feature of continuous models. Traditional approaches shift additional compute resources into improved accuracy via higher–order adaptive–step methods (Prince & Dor-mand, 1981). For the most part, the computation–accuracy pareto front determined by traditional methods has been treated as optimal, allowing practitioners its traversal with different solver choices.
We provide theoretical and practical results in support of the pareto efﬁciency of hypersolvers, measured with respect to both number of function evaluations (NFEs) as well as standard indicators of algorithmic complexity. Fig. 2 provides a comparison of hypersolvers and traditional methods.
Inference speed By leveraging Hypersolved Neural ODEs, we obtain signiﬁcant speedups on common benchmarks for continuous–depth models. In image classiﬁcation tasks, inference is sped up by at least one order of magnitude. Additionally, the proposed approach is capable of solving continuous normalizing ﬂow (CNF) (Chen et al., 2018; Grathwohl et al., 2018) sampling in few steps with little–to–no degradation of the sample quality as shown in Fig. 4.1. Moving beyond computational advantages at inference time, the proposed framework is compatible with continual learning (Parisi et al., 2019) or adversarial learning (Ganin et al., 2016) techniques where model and hypersolver are co–designed and jointly optimized. Sec. 6 provides an overview of this peculiar interplay. 2