Abstract
In neural combinatorial optimization (CO), reinforcement learning (RL) can turn a deep neural net into a fast, powerful heuristic solver of NP-hard problems. This approach has a great potential in practical applications because it allows near-optimal solutions to be found without expert guides armed with substantial domain knowledge. We introduce Policy Optimization with Multiple Optima (POMO), an end-to-end approach for building such a heuristic solver. POMO is applicable to a wide range of CO problems. It is designed to exploit the symmetries in the representation of a CO solution. POMO uses a modiﬁed REINFORCE algorithm that forces diverse rollouts towards all optimal solutions. Empirically, the low-variance baseline of POMO makes RL training fast and stable, and it is more resistant to local minima compared to previous approaches. We also introduce a new augmentation-based inference method, which accompanies POMO nicely.
We demonstrate the effectiveness of POMO by solving three popular NP-hard problems, namely, traveling salesman (TSP), capacitated vehicle routing (CVRP), and 0-1 knapsack (KP). For all three, our solver based on POMO shows a signiﬁcant improvement in performance over all recent learned heuristics. In particular, we achieve the optimality gap of 0.14% with TSP100 while reducing inference time by more than an order of magnitude. 1

Introduction
Combinatorial optimization (CO) is an important problem in logistics, manufacturing and distribution supply chain, and sequential resource allocation. The problem is studied extensively by Operations
Research (OR) community, but the real-world CO problems are ubiquitous, and each problem is different from one another with its unique constrains. Moreover, these constrains tend to vary rapidly with a changing work environment. Devising a powerful and efﬁcient algorithm that can be applied uniformly under various conditions is tricky, if not impossible. Therefore, many CO problems faced in industries have been commonly dealt with hand-crafted heuristics, despite their drawbacks, engineered by local experts.
In the ﬁeld of computer vision (CV) and natural language processing (NLP), classical methods based on manual feature engineering by experts have now been superseded by automated end-to-end deep learning algorithms [1, 2, 3, 4, 5]. Tremendous progresses in supervised learning, where a mapping from training inputs to their labels is learned, has made this remarkable transition possible.
Unfortunately, supervised learning is largely unﬁt for most CO problems because one cannot have an instant access to optimal labels. Rather, one should make use of the scores, that are easily calculable for most CO solutions, to train a model. Reinforcement learning paradigm suits combinatorial optimization problems very well.
Recent approaches in deep reinforcement learning (RL) have been promising [6], ﬁnding close-to-optimal solutions to the abstracted NP-hard CO problems such as traveling salesman (TSP) [7, 8, 9, 10, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
11, 12], capacitated vehicle routing (CVRP) [10, 11, 13, 14, 15], and 0-1 knapsack (KP) [7] in superior speed. We contribute to this line of group effort in the deep learning community by introducing
Policy Optimization with Multiple Optima (POMO). POMO offers a simple and straightforward framework that can automatically generate a decent solver. It can be applied to a wide range of general CO problems because it uses symmetry in the CO itself, found in sequential representation of
CO solutions.
We demonstrate the effectiveness of POMO by solving three NP-hard problems aforementioned, namely TSP, CVRP, and KP, using the same neural net and the same training method. Our approach is purely data-driven, and the human guidance in the design of the training procedure is kept to minimal. More speciﬁcally, it does not require problem-speciﬁc hand-crafted heuristics to be inserted into its algorithms. Despite its simplicity, our experiments conﬁrm that POMO achieves superior performances in reducing the optimality gap and inference time against all contemporary neural RL approaches.
The contribution of this paper is three-fold:
• We identify symmetries in RL methods for solving CO problems that lead to multiple optima.
Such symmetries can be leveraged during neural net training via parallel multiple rollouts, each trajectory having a different optimal solution as its goal for exploration.
• We devise a new low-variance baseline for policy gradient. Because this baseline is derived from a group of heterogeneous trajectories, learning becomes less vulnerable to local minima.
• We present the inference method based on multiple greedy rollouts that is more effective than the conventional sampling inference method. We also introduce an instance augmentation technique that can further exploit symmetries of CO problems at the inference stage. 2