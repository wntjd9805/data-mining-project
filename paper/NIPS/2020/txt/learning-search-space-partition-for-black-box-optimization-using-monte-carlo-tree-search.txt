Abstract xi, yi}
{
High dimensional black-box optimization has broad applications but remains a chal-lenging problem to solve. Given a set of samples
, building a global model (like Bayesian Optimization (BO)) suffers from the curse of dimensionality in the high-dimensional search space, while a greedy search may lead to sub-optimality.
By recursively splitting the search space into regions with high/low function values, recently LaNAS [1] shows good performance in Neural Architecture Search (NAS), reducing the sample complexity empirically. In this paper, we coin LA-MCTS that extends LaNAS to other domains. Unlike previous approaches, LA-MCTS learns the partition of the search space using a few samples and their function values in an online fashion. While LaNAS uses linear partition and performs uniform sampling in each region, our LA-MCTS adopts a nonlinear decision boundary and learns a local model to pick good candidates. If the nonlinear partition function and the local model ﬁt well with ground-truth black-box function, then good partitions and candidates can be reached with much fewer samples. LA-MCTS serves as a meta-algorithm by using existing black-box optimizers (e.g., BO, TuRBO [2]) as its local models, achieving strong performance in general black-box optimization and reinforcement learning benchmarks, in particular for high-dimensional problems. 1

Introduction
Black-box optimization has been extensively used in many scenarios, including Neural Architecture
Search (NAS) [3, 1, 4], planning in robotics [5, 6], hyper-parameter tuning in large scale databases [7] and distributed systems [8], integrated circuit design [9], etc.. In black-box optimization, we have a function f without explicit formulation and the goal is to ﬁnd x⇤ such that x⇤ = arg max
X x 2 f (x) (1) with the fewest samples (x). In this paper, we consider the case that f is deterministic.
Without knowing any structure of f (except for the local smoothness such as Lipschitz-continuity [10]), in the worst-case, solving Eqn. 1 takes exponential time, i.e. the optimizer needs to search every x to ﬁnd the optimum. One way to address this problem is through learning: from a few samples we learn a surrogate regressor ˆf is small and f can be well approximated within fewer samples. and optimize ˆf instead. If the model class
H
, then ˆf is a good approximator of f with much 2H
H
Many previous works go that route, such as Bayesian Optimization (BO) and its variants [11, 12, 13, 14]. However, in the case that f is highly nonlinear and high-dimensional, we need to use a very
, e.g. Gaussian Processes (GP) or Deep Neural Networks (DNN), that requires large model class many samples to ﬁt before generalizing well. For example, Oh et al [15] observed that the myopic acquisition in BO over-explores the boundary of a search space, especially in high dimensional problems. To address this issue, recent works start to explore space partitioning [5, 16, 17] and local
H 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
modeling [2, 18] that ﬁts local models in promising regions, and achieve strong empirical results in high dimensional problems. However, their space partitions follow a ﬁxed criterion (e.g., K-ary uniform partition) that is independent of the objective to be optimized.
Following the path of learning, one under-explored direction is to learn the space partition. Compared to learning a regressor ˆf that is expected to be accurate in the region of interest, it sufﬁces to learn a classiﬁer that puts the sample to the right subregion with high probability. Moreover, its quality requirement can be further reduced if done recursively.
In this paper, we propose LA-MCTS, a meta-level algorithm that recursively learns space partition in a hierarchical manner. Given a few samples within a region, it ﬁrst performs unsupervised K-mean algorithm based on their function values, learns a classiﬁer using K-mean labels, and partition the region into good and bad sub-regions (with high/low function value). To address the problem of mis-partitioning good data points into bad regions, LA-MCTS uses UCB to balance exploration and exploitation: it assigns more samples to good regions, where it is more likely to ﬁnd an optimal solution, and exploring other regions in case there are good candidates. Compared to previous space partition method, e.g. using Voronoi graph [5], we learn the partition that is adaptive to the objective function f (x). Compared to the local modeling method, e.g. TuRBO [2], our method dynamically exploits and explores the promising region w.r.t samples using Monte Carlos Tree Search (MCTS), and constantly reﬁne the learned boundaries with new samples.
LA-MCTS extends LaNAS [1] in three aspects. First, while LaNAS learns a hyper-plane, our approach learns a non-linear decision boundary that is more ﬂexible. Second, while LaNAS simply performs uniform sampling in each region as the next sample to evaluate, we make the key observation that local model works well and use existing solvers such as BO to ﬁnd a promising data point. This makes LA-MCTS a meta-algorithm usable to boost existing algorithms that optimize via building local models. Third, while LaNAS mainly focus on Neural Architecture Search (< 20 discrete parameters), our approach shows strong performance on generic black-box optimization.
We show that LA-MCTS, when paired with TurBO, outperforms various SoTA black-box solvers from Bayesian Optimizations, Evolutionary Algorithm, and Monte Carlo Tree Search, in several challenging benchmarks, including MuJoCo locomotion tasks, trajectory optimization, reinforcement learning, and high-dimensional synthetic functions. We also perform extensive ablation studies, showing LA-MCTS is relatively insensitive to hyper-parameter tuning. As a meta-algorithm, it also substantially improves the baselines.
The implementation of LA-MCTS can be found at https://github.com/facebookresearch/LaMCTS. 2