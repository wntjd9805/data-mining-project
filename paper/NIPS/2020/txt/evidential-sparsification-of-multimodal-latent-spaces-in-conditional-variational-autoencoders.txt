Abstract
Discrete latent spaces in variational autoencoders have been shown to effectively capture the data distribution for many real-world problems such as natural language understanding, human intent prediction, and visual scene representation. However, discrete latent spaces need to be sufﬁciently large to capture the complexities of real-world data, rendering downstream tasks computationally challenging. For instance, performing motion planning in a high-dimensional latent representation of the environment could be intractable. We consider the problem of sparsifying the discrete latent space of a trained conditional variational autoencoder, while preserv-ing its learned multimodality. As a post hoc latent space reduction technique, we use evidential theory to identify the latent classes that receive direct evidence from a particular input condition and ﬁlter out those that do not. Experiments on diverse tasks, such as image generation and human behavior prediction, demonstrate the effectiveness of our proposed technique at reducing the discrete latent sample space size of a model while maintaining its learned multimodality. 1

Introduction
Variational autoencoders (VAEs) with discrete latent spaces have recently shown great success in real-world applications, such as natural language processing [1], image generation [2, 3], and human intent prediction [4]. Discrete latent spaces naturally lend themselves to the representation of discrete concepts such as words, semantic objects in images, and human behaviors. The choice of a discrete latent space encoding over a continuous one has also been shown to encourage multimodal predictions [5, 6] as well as interpretability [7, 8] since it is easier to analyze input-output relationships on countable classes than continuous vector spaces [9]. However, prohibitively large discrete latent spaces are required to accurately learn complex data distributions [10, 11], thereby causing difﬁculties in interpretability and rendering downstream tasks computationally challenging. For instance, robotic motion planning algorithms often plan using a uniform distribution over the state space representation, requiring exorbitantly many samples to accurately cover a large latent space [12]. Similarly, in multi-agent robotics, a high dimensional latent space encoding may be too large to transmit over limited bandwidth between coordinating robots [13]. In an attempt to address these concerns, we propose a methodology that effectively reduces the latent sample space size while maintaining multimodality within the latent distribution.
Distributional multimodality arises in many real-world problems, such as video frame prediction [14] and human behavior prediction [8, 15], from multiple possibilities for the future (e.g., a pedestrian may turn right or left given the same trajectory history). The conditional variational autoencoder (CVAE) was developed to address prediction multimodality [16]. The CVAE encodes input data x and corresponding query label y into a latent space Z. At test time, the model samples from the latent space encoding of a query label to generate diverse data. The latent space distribution p(z | y) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
encodes the multimodality of the prediction task. During inference, the discrete latent distribution in a CVAE is often parameterized by the softmax function. A drawback to the softmax transformation is that uncertainty is distributed across all the available classes since, by deﬁnition, the softmax function cannot set a probability to zero (though it can become negligible). Removing latent classes that arise within the distribution solely due to this uncertainty has the potential to reduce the number of relevant latent classes to consider.
Contributions We introduce a novel method grounded in evidential theory for sparsifying the dis-crete latent space of a trained CVAE. Evidential theory, also known as Dempster-Shafer Theory (DST), differentiates lack of information (e.g., an uninformative prior) from conﬂicting information (e.g., ev-idence supporting multiple hypotheses) by considering the power set of hypotheses [17]. We propose using evidential conﬂict as a proxy for multimodality. We can then prune the latent classes from the distribution that do not directly receive evidence from the input features, thus performing post hoc latent space sparsiﬁcation without sacriﬁcing distributional multimodality. Experiments show that our algorithm achieves a signiﬁcant reduction in the discrete latent sample space of CVAE networks trained for the tasks of image generation and behavior prediction, without loss of network performance. Our approach sparsiﬁes the latent space while maintaining distributional multimodality, unlike baseline techniques which remove important modes with overly aggressive ﬁltering. Our proposed method provides a more accurate distribution over the latent encoding with fewer training iterations than baseline methods, and demonstrates consistent performance on downstream tasks. 2 Evidential Theory for Latent Space Reduction
Evidential theory distinguishes lack of information from conﬂicting information [17], making it appealing for handling epistemic uncertainty in machine learning tasks [18]. Denoeux [19] recently showed that, under a set of assumptions, the softmax transformation is equivalent to the Dempster-Shafer fusion of belief masses. This insight facilitates the use of evidential theory in multi-class machine learning classiﬁcation paradigms. We propose using the tools from evidential theory to sparsify discrete latent spaces in CVAEs. Our method automatically balances the objectives of sparsity and multimodality by keeping only the latent classes that receive direct evidence from the network’s features and weights. The following sections overview evidential theory, its application to neural network classiﬁers, and our proposed approach to evidential latent space sparsiﬁcation in
CVAEs. Further information on evidential theory can be found in Appendix A. 2.1 Evidential Theory
Mass Functions Evidential theory considers a discrete set of hypotheses or, equivalently, classes.
Let the set of allowable classes be Z = {z1, . . . , zK}, where zk can be represented as one-hot encodings, and denote its power set by 2Z. A belief mass is a function m : 2Z → [0, 1] such that (cid:80)
A⊆Z m(A) = 1 [17]. Evidential theory assumes that the allowable classes are exhaustive, that is m(∅) = 0 [20]. The mass function quantiﬁes the total evidential belief committed to some A ⊆ Z.
If m(Z) = 1, then the mass function is vacuous, in that it encodes a complete lack of evidence for any particular subset of classes. If the belief mass function is non-zero only for singleton sets, then it reduces to an approximation of the usual categorical distribution. Two mass functions, each representing an independent source of evidence, can be combined through Dempster’s rule [17] to generate a fused belief mass as deﬁned in Appendix A.3.
A belief mass function m is simple if there is at most a single strict subset A ⊂ Z for which m(A) is non-zero. Then, we can deﬁne, m(A) = s, m(Z) = 1 − s, w = − log(1 − s), (1) where s ∈ [0, 1] is the degree of support in A and w is the corresponding evidential weight for A [20].
Plausibility Transformation Plausibility pl(zk) represents the extent to which the evidence does not contradict the class zk [21]. Belief masses can be reduced to estimated probabilities through the plausibility transformation [22], where pl(zk) = (cid:80) pl(zk) l=1 pl(zl)
B:zk∈B,B⊆Z m(B) and k ∈ {1, . . . , K}. p(zk) = (cid:80)K
, (2) 2
2.2 Evidential Classiﬁers
It has been shown that all classiﬁers that transform a linear combination of features through the soft-max function can be formulated as evidential classiﬁers [19]. Each feature represents an elementary piece of evidence in support of a class or its complement. The softmax function then fuses these pieces of evidence to form class probabilities conditioned on the input. In this context, the softmax class probabilities are equivalent to normalized plausibilities (Eq. (2)). Thus, the neural network weights and features, which serve as arguments to the softmax function, can also be used to compute the corresponding belief mass function. When compared to a Bayesian probability distribution, a belief mass function provides an additional degree of freedom that allows it to distinguish between a lack of evidence and conﬂicting evidence.
A feature vector φ(yi) ∈ RJ is deﬁned as the output of the last hidden layer in a neural network, for a given query yi from a dataset. The evidential weights deﬁned in Eq. (1) are assumed to be afﬁne transformations of each feature φj(yi) by construction, wjk = βjkφj(yi) + αjk, (3) where αjk and βjk are parameters [19]. An assumption is made that the evidence supports at most either a singleton class {zk} when w+ jk = max(0, wjk) > 0 or its complement {zk} when jk = max(0, −wjk) > 0, such that w+ w− jk = wjk. Then, for each feature φj(yi) and each class zk, according to Eq. (1), there exist two simple mass functions, jk − w− m+ m− kj({zk}) = 1 − e−w+ kj({zk}) = 1 − e−w− jk , m+ jk , m− jk kj(Z) = e−w+ kj(Z) = e−w− jk . (4) (5)
These masses can then be fused through Dempster’s rule to arrive at the mass function at the output of the softmax layer as follows, m({zk}) = Ce−w− k

ew+ k − 1 + (cid:89) (cid:16) 1 − e−w− (cid:96) (cid:17)

 m(A) = C

 (cid:89) (cid:16) 1 − e−w− k zk /∈A (cid:96)(cid:54)=k
 (cid:17)
 (cid:32) (cid:89) e−w− k (cid:33)
, zk∈A j=1 w− k = (cid:80)J (6a) (6b) where A ⊆ Z, |A| > 1, C is a normalization constant, w− jk, and w+ k = (cid:80)J j=1 w+ jk. 2.3 Evidential Sparsiﬁcation
Under the assumptions outlined in Section 2.2 and using the plausibility transformation, the equiv-alence of the mass function in Eq. (6) and the softmax distribution holds under the constraint (cid:80)J j=1 αjk = ˆβ0k + c0, where ˆβ0k are the bias parameters learned by the neural network and c0 is a constant [19]. The evidential weight parameters αjk and βjk in Eq. (3) are not uniquely deﬁned due to the extra degree of freedom provided by the belief mass as compared to the softmax distribution.
Denoeux [19] selects the αjk and βjk parameters that maximize the Least Commitment Principle (LCP), which is analogous to maximum entropy in information theory [23]. k = 0 and w−
In contrast, we choose our parameters such that the singleton mass function in Eq. (6a) is sparse, rather than distributing uncertainty across the mass function using the LCP. We construct the singleton mass function to identify only the classes that receive direct evidence towards them. We observe that if w+ (cid:96) = 0 for at least one other class (cid:96) (cid:54)= k, then m({zk}) = 0. Intuitively, if no evidence directly supports class k and there is no evidence contradicting another class (cid:96), then the belief mass for the singleton set {zk} is zero. This situation occurs when at least one of w+ k is zero, which holds if w+ j=1 wjk. Hence, wk provides direct support either for or against a class k. This property does not hold in the original formulation by Denoeux [19]. Thus, we construct an evidential weight wjk that does not depend on j, enforcing this desideratum: k = max(0, −wk), where wk = (cid:80)J k = max(0, wk) and w− k and w−

βjkφj(yi)
 . (7) wjk = 1
J
J (cid:88) j=1

β0k + 3
Since wjk is constant across the index j, summing over j in w+ and w− k = max(0, −wk), as required. The corresponding parameters in Eq. (3) are then:
K (cid:88) jk yields w+ jk and w− k = max(0, wk)
βjk = ˆβjk − 1
K
ˆβj(cid:96) l=1
αjk(yi) =

β0k + 1
J
J (cid:88) j=1

βjkφj(yi)
 − βjkφj(yi), (8) (9) where ˆβjk are the output linear layer weights learned by the neural network a priori. These parameters match those of Denoeux [19], except φj(yi) replaces µj = 1 i=1 φj(yi) in αjk. The αjk bias
N term is now a function of the test input query yi. By choosing to treat each input yi individually at test time, we remove the dependency in wjk on j, facilitating our desired behavior in the singleton mass function. We show that the new αjk and βjk parameters satisfy the constraint to achieve equivalency with the softmax transformation in Appendix B. We posit that ﬁltering out the classes with zero singleton belief mass values according to the proposed deﬁnition removes only the classes without direct evidence in their support, while imposing a more concentrated distribution output. (cid:80)N 2.4 Post Hoc CVAE Latent Space Sparsiﬁcation
Since the discrete latent distribution in a CVAE is parameterized using a softmax function at test time, we can directly apply the evidential theory formulation developed in Sections 2.1–2.3 to sparsify the latent space of a trained CVAE. The evidence allocated to multiple singleton latent classes indicates internal conﬂict between them within the mass function. In the context of a CVAE, we posit that internal conﬂict is directly correlated with latent space multimodality. High evidential conﬂict between a subset of latent classes indicates distinct, multimodal latent features encoded by the network. We propose ﬁltering the latent distribution to maintain only these highly conﬂicting classes, thus, reducing latent sample space dimensionality, without compromising the captured multimodality.
Using Eq. (6a) and Eq. (7), we construct the singleton mass function that corresponds to the encoder’s output softmax distribution p(z | y) over the latent classes zk given an input query y. This distribution is ﬁltered by removing the probabilities for the latent classes with zero singleton mass values and then renormalizing, as follows, pﬁltered(zk | y) = 1{m({zk}) (cid:54)= 0}psoftmax(zk | y) (cid:80)K (cid:96)=1 1{m({z(cid:96)}) (cid:54)= 0}psoftmax(z(cid:96) | y)
. (10)
In this manner, we reduce the number of relevant latent classes, providing a more concentrated latent class distribution, while maintaining the learned distributional multimodality. 3 Experiments
To validate our method, we consider CVAE architectures designed for the tasks of class-conditioned image generation and pedestrian trajectory prediction. These real-world tasks require modeling high degrees of distributional multimodality. We compare our method to the softmax distribution and the popular class-reduction technique termed sparsemax which achieves a sparse distribution by project-ing an input vector onto the probability simplex [24]. By design, both our method and sparsemax compute an implicit threshold for each input query post hoc. Thus, they do not need to be tuned for each network or dataset, and automatically adapt to individual input features. Experiments indicate that our method is able to better balance the objectives of sparsity and multimodality than sparsemax by keeping only the latent classes that receive direct evidence from the network’s features and weights, as described in Section 2. We demonstrate that our method maintains distributional multimodality, unlike sparsemax, whilst yielding a signiﬁcantly reduced latent sample space size over softmax. The code to reproduce our results can be found at: https://github.com/sisl/EvidentialSparsiﬁcation. 3.1
Image Generation
To gain insight into our proposed approach, we run experiments on a small network trained on
MNIST [25]. We then demonstrate the performance of our sparsiﬁcation algorithm on the large 4
discrete latent space within the state-of-the-art VQ-VAE [2] architecture trained on miniImageNet [26].
All image generation experiments were run on a single NVIDIA GeForce GTX 1070 GPU. 3.1.1 MNIST Experiments
Task To investigate discrete, multimodal latent representations, we consider the multimodal task of generating digit images for even and odd queries (y ∈ {even, odd}) on MNIST. Appendix F and
Appendix G contain results on Fashion MNIST [27] and NotMNIST [28], respectively.
Model We present a proof of concept of our method for sparsifying multimodal discrete latent spaces on the
CVAE architecture shown in Fig. 1. We intentionally use a simplistic CVAE architecture trained on a reasonably simple task to 1) demonstrate the capability of our latent space reduction technique to improve performance post hoc and 2) easily characterize the results.
During training, the encoder consists of two multi-layer perceptrons (MLPs). One MLP takes as input the query y, and outputs a softmax distribution that parameterizes the categorical prior distribution p(z | y) over the latent vari-able z. The other MLP takes as input a feature vector x and the query y, and outputs the softmax distribution for the posterior q(z | x, y). The latent class is sampled from q during training and p at test time. It is then passed through the decoder MLP to generate the feature vector x(cid:48). The Gumbel-Softmax distribution is used to backprop-agate gradients through the discrete latent space [7, 29].
The model is trained to maximize the standard conditional evidence lower bound (ELBO) [16]. Further experimental details are provided in Appendix C. x y 784 256 10
ReLU
ReLU q(z | x, y) 30 10 2
ReLU
ReLU p(z | y)
Encoder 256 784 z 10
ReLU
ReLU x(cid:48)
Decoder
Figure 1: The CVAE architecture used for
MNIST image generation. The last layer in each MLP is a softmax layer. At test time, p(z | y) is used to sample the latent space.
Qualitative Performance We choose K = 10 latent classes; with a “perfectly trained” network, this would yield a 5-modal distribution when conditioned on one of y ∈ {even, odd}. For instance, conditioning on the even query should produce a uniform distribution over the encoded digits: 0, 2, 4, 6, and 8. Fig. 2 depicts the latent distributions of our proposed method, softmax, and sparsemax for the even and odd queries. Although the CVAE in Fig. 1 successfully learns a multimodal latent encoding, the learned softmax distribution has non-negligible probability mass associated with the incorrect latent classes zk for each query class y. Thus, sampling from the CVAE with the softmax distribution results in an imperfect set of generated digit images given a query as shown in Appendix D.
To remedy this problem, we consider both sparsemax and our proposed evidential ﬁltration technique.
In Fig. 2, our ﬁltered distribution selects an almost perfect set of correct latent classes given a query.
The proposed distribution provides a more robust signal to the decoder network, improving the accuracy of the sampled images as demonstrated in Appendix D. Thus, we successfully sparsify the y t i l i b a b o r
P 0.4 0.2 0
Even
Z
Odd
Z
Figure 2: Our proposed ﬁltered distribution (green) yields a more accurate distribution on the MNIST dataset than softmax (blue) and sparsemax (orange). Our method reduces the size of the relevant latent sample space without removing valid latent classes. The horizontal axis depicts the decoded image for each latent class. 5
Even, Wasserstein
Odd, Wasserstein
Even, Bhattacharyya
Odd, Bhattacharyya c i r t e
M 0.2 0.1 0 0.8 0.6 0.4 0.2 0 0 0.5 1 1.5 2
Iteration 0 0.5 1 1.5 2
Iteration 0 0.5 1 1.5 2
Iteration 0 0.5 1 1.5 2
Iteration
·104
·104
Figure 3: Our ﬁltered distribution (green) outperforms the softmax (blue) and sparsemax (orange) baselines across training iterations on the MNIST dataset. Lower is better.
·104
·104 latent space analytically for each query without knowledge of or comparison to the other query. The only error made by the ﬁltered distribution is the selection of the 9 image (ﬁfth latent class) for the even query. This error can be explained by the relatively high softmax probability assigned to the 9 image for both the even and odd queries. The key insight to our approach is that it performs only as well as the quality of the representation learned by the neural network, with the beneﬁt of extracting richer information than softmax. In contrast, sparsemax results in undesirably more aggressive
ﬁltering than our method. It removes the correct latent classes of 1 and 3 for the odd query. Both our method and sparsemax result in a ﬁltration decision through an implicit thresholding of the softmax distribution internal to each individual method. Our ﬁltration technique based in evidential theory results in an empirically lower implicit threshold than that of sparsemax. Thus, our more conservative
ﬁltration is a compelling latent space reduction technique, particularly for applications where false negatives might cause safety concerns. One such application is human behavior prediction in the context of autonomous driving, which we consider in Section 3.2.
As a further thought experiment, we consider a static threshold that could be chosen through hyper-parameter tuning on a validation set. By visual inspection, it is impossible to choose a single static threshold that would outperform our method in balancing sparsity and multimodality in Fig. 2 (either additional false negatives or false positives would result). While we could tune a static threshold for each individual input query for the MNIST task, this would be intractable for a continuous trajectory input query as in the behavior prediction task in Section 3.2.
Quantitative Evaluation Metrics Quantitatively evaluating the performance of the ﬁltered dis-tribution is nontrivial as the ground truth distribution can be ambiguous (e.g., a generated image may look like a 3 or an 8). To standardize the evaluation for this binary-input task, we introduce the following metric. The target probability for an input is set to zero if the learned softmax probability conditioned on the input p(zk | y) is smaller than its complement p(zk | ¯y). If it is larger, then the target probability takes the value of p(zk | y). The result is then normalized across latent classes.
Thus, we obtain a sparse, multimodal distribution over the more prominent classes as learned by the network. We write the target distribution as pT (zk | y) = 1{p(z(cid:96)|y)≥p(z(cid:96)|¯y)}p(z(cid:96)|y) . We use the Wasserstein and Bhattacharyya distances to the target distribution as evaluation metrics. The
Kullback-Leibler divergence is not used as it is undeﬁned for zero probabilities. 1{p(zk|y)≥p(zk|¯y)}p(zk|y) (cid:80)K (cid:96)=1
Training Iteration Evolution Fig. 3 demonstrates the robustness of the ﬁltered distribution to fewer training iterations in its ability to extract more accurate encoding information from the neural network earlier in the training process than softmax1. Our methodology provides signiﬁcant performance improvement over the softmax baseline when the latter assigns non-negligible probability mass to incorrect latent classes given a query, as is the case for the even query as shown in Fig. 2 and Fig. 3.
Although the performances of sparsemax and our proposed distributions are similar for the even query on the Bhattacharyya metric, sparsemax signiﬁcantly underperforms for the odd query, even as compared to softmax. The aggressive ﬁltration within sparsemax incorrectly ﬁlters out potentially valid latent classes for the odd query. Thus, our method consistently outperforms both baselines on
MNIST, on average resulting in a 22% improvement over softmax and a 10% improvement over sparsemax on the Bhattacharyya metric. Thus, the proposed latent distribution provides a more robust 1The results are over 25 different random seeds. 6
Table 1: Downstream classiﬁcation performance on 1600 sampled images (25 samples × 64 classes) shows that our sparse distribution maintains the original softmax performance, unlike sparsemax. For comparison, the classiﬁer was evaluated on a held-out subset of 1920 images from the original miniImageNet training set. Higher is better for all metrics and bold highlights the best performing latent distributions.
Accuracy (%)
Top 5 Class Accuracy (%)
Softmax 20.688 47.750
Sparsemax 6.125 17.500
Ours 19.937 47.875
Original Images 71.719 90.625 representation, retrieving richer information from the learned neural network weights with fewer training iterations. We present further experiments on a reduced data model in Appendix E. 3.1.2 VQ-VAE Experiments
We demonstrate the performance of our sparsiﬁcation methodology on a much larger latent sample space within the state-of-the-art VQ-VAE [2] image generation architecture. The VQ-VAE archi-tecture is trained in two stages. First, the encoder-decoder is trained assuming a uniform prior over the discrete latent space. Then an autoregressive prior is trained over the latent space to allow for sampling from the network. We use miniImageNet images randomly cropped to 128 × 128 pixels for training as opposed to ImageNet [30] due to limited computational resources. We consider a latent space of 32 × 32 discrete latent variables with K = 512 classes each. As in the original paper [2], we train a PixelCNN [31] network for the prior, but reduce its capacity to 20 layers with a hidden dimension size of 128. Further experimental details can be found in Appendix H2.
Our algorithm achieves an 89% reduction in the 512 latent classes required to represent each latent variable, while maintaining the multimodality of the distribution. On the other hand, sparsemax achieves a 99% reduction in the latent sample space, but sacriﬁces multimodality, resulting in degener-ate single color images sampled from the autoregressive prior. Although sparsemax reduces the latent sample space by a larger percentage than our technique, this negatively impacts its performance due to undesirable pruning of correct latent classes. To further evaluate the performance of our proposed sparse latent distribution, we consider the downstream task of image classiﬁcation. We train a Wide
Residual Network (WRN) [32] for classiﬁcation on the miniImageNet data. We generate a dataset of 25 examples from each of the 64 training classes in miniImageNet by sampling from the prior. The decoded images are then classiﬁed by the WRN. Table 1 shows that our much smaller latent space successfully maintains the performance of the original softmax distribution. 3.2 Behavior Prediction
We show that our sparse latent sample space allows for easier interpretability and maintains distribu-tional multimodality on the difﬁcult task of pedestrian trajectory prediction. Trajectron++ [4] is a state-of-the-art deep probabilistic generative model of pedestrian trajectories. It is a graph-structured recurrent model that produces a distribution over future trajectories given an agent’s past trajectory history and the past trajectories of its neighboring agents. The model uses a CVAE to capture the multimodality over future trajectory predictions, with a latent space comprised of two discrete variables with ﬁve classes each, resulting in 25 total latent classes. The loss function is comprised of the classic conditional ELBO loss in a β-VAE [33] scheme and a mutual information loss term on z and y as per [34]. We evaluate Trajectron++’s performance with different probability ﬁltering schemes on 203 randomly-sampled examples from the test set of the ETH pedestrian dataset [35], consisting of real world human trajectories with rich interaction scenarios. Behavior prediction model training and experiments were performed on two NVIDIA GTX 1080 Ti GPUs. Further experimental details are provided in Appendix I.
We demonstrate our method’s performance as compared to the softmax and sparsemax baselines in
Fig. 4. Our ﬁltered latent space kept 2 − 12 latent classes out of 25 total (51.7% of the test set resulted in 6 maintained latent classes), achieving more than a 50% reduction. Despite the signiﬁcant sample space reduction, our ﬁltered distribution successfully captures the ground truth when the learned latent space encompasses it, while maintaining the multimodality of the output as seen in Fig. 4. For instance, in Fig. 4a and Fig. 4c, our method identiﬁes two distinct modes where the pedestrian is 2We largely follow the training procedure here: https://github.com/ritheshkumar95/pytorch-vqvae/. 7
) m ( y 5 4.5 6.5 6 5.5 5 4 3 6 5.8 5.6 5.4 5.2 0 5 x (m) 10 (a) 0 5 x (m) (b) 10
−5 5 0 x (m) (c) 10 0 10 5 x (m) (d)
Input Past Trajectory
Ground Truth 25 Available Latent Classes
Sparsemax
Ours
Figure 4: Behavior prediction results on the ETH pedestrian dataset [35] show that our method selects distinct, interpretable modes in the latent space while capturing the ground truth. In contrast, sparsemax occasionally misses the ground truth due to its aggressive ﬁltering scheme. We averaged 100 samples from the Trajectron++’s output for each latent class. predicted to follow an approximately straight path or choose to turn, both visually valid options. The reduced number of trajectories and the appearance of distinct modes in the ﬁltered output aids with the interpretability of the latent space.
We consider two quantitative experiments in Table 2: 1) sampling from the network according to each of the softmax, sparsemax and our latent distributions and 2) considering the ﬁve most likely latent classes according to each distribution and taking the best metric across them. We use standard trajectory prediction metrics to evaluate performance: the Average Displacement Error (ADE), Final
Displacement Error (FDE), and Negative Log Likelihood (NLL) [4]. In the sampling experiment, both our proposed distribution and sparsemax quantitatively perform similarly on average to the original softmax distribution, but with the beneﬁt of a signiﬁcantly reduced latent sample space. However, sampling may not cover a sufﬁcient number of modes due to high likelihoods in a single class. In the second experiment, our method maintains the same performance as softmax, but outperforms sparsemax due to the latter generating false negatives, and collapsing the captured multimodality. As seen qualitatively, although, sparsemax results in a sparser latent distribution, it ﬁlters out potentially valid modes as in Fig. 4b and Fig. 4d, reducing its applicability in such safety critical applications as behavior prediction in the context of autonomous driving. 4