Abstract
Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators. 1

Introduction
Text-to-speech synthesis (TTS) has seen great advances in recent years, with neural network-based methods now signiﬁcantly outperforming traditional concatenative and statistical parametric ap-proaches [39, 35]. While autoregressive models such as WaveNet [35] or WaveRNN [15] constitute the current state of the art in speech synthesis, their sequential nature is often seen as a drawback.
They generate only a single sample at a time, and since audio is typically sampled at a frequency of 18kHz to 44kHz this means that tens of thousands of sequential steps are necessary for generating a single second of audio. The sequential nature of these models makes them ill-suited for use with modern deep learning hardware such as GPUs and TPUs that are built around parallelism.
At the same time, parallel speech generation remains challenging. Existing likelihood-based models either rely on elaborate distillation approaches [27, 36], or require large models and long training times [29, 22]. Recent GAN-based methods provide a promising alternative to likelihood-based methods for TTS [3, 22]. Although they do not yet match the speech quality of autoregressive models, they are efﬁcient to train and employ fully-convolutional architectures, allowing for efﬁcient parallel generation. However, due to their reliance on adversarial training, they can be difﬁcult to train.
⇤Equal contribution. † Work completed as a Google AI resident. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To address these limitations we propose a new training method based on the generalized energy distance [10, 32, 33], which enables the learning of implicit density models without the use of adversarial training or requiring a tractable likelihood. Our method minimizes a multi-resolution spectrogram loss similar to previous works [37, 38, 9, 6], but includes an additional repulsive term that encourages diverse samples and provides a statistical consistency guarantee. As a result, our models enjoy stable training and rapid convergence, achieving state-of-the-art speech quality among implicit density models.
In addition to demonstrating our proposed energy distance on the speech model of Bi´nkowski et al. [3], we further propose a new model for generating audio using an efﬁcient overlap-add upsampling module. The new model is faster to run, while still producing high quality speech.
Finally, we show that our proposed energy distance can be combined with GAN-based learning, further improving on either individual technique. An open source implementation of our generalized energy distance is available at https://github.com/google-research/google-research/ tree/master/ged_tts. 2