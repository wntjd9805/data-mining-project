Abstract
Domain adaptive object re-ID aims to transfer the learned knowledge from the labeled source domain to the unlabeled target domain to tackle the open-class re-identiﬁcation problems. Although state-of-the-art pseudo-label-based methods
[11, 54, 10, 55, 14] have achieved great success, they did not make full use of all valuable information because of the domain gap and unsatisfying clustering performance. To solve these problems, we propose a novel self-paced contrastive learning framework with hybrid memory. The hybrid memory dynamically gen-erates source-domain class-level, target-domain cluster-level and un-clustered instance-level supervisory signals for learning feature representations. Different from the conventional contrastive learning strategy, the proposed framework jointly distinguishes source-domain classes, and target-domain clusters and un-clustered instances. Most importantly, the proposed self-paced method gradually creates more reliable clusters to reﬁne the hybrid memory and learning targets, and is shown to be the key to our outstanding performance. Our method outperforms state-of-the-arts on multiple domain adaptation tasks of object re-ID and even boosts the performance on the source domain without any extra annotations. Our general-ized version on unsupervised object re-ID surpasses state-of-the-art algorithms by considerable 16.7% and 7.9% on Market-1501 and MSMT17 benchmarks†. 1

Introduction
Unsupervised domain adaptation (UDA) for object re-identiﬁcation (re-ID) aims at transferring the learned knowledge from the labeled source domain (dataset) to properly measure the inter-instance afﬁnities in the unlabeled target domain (dataset). Common object re-ID problems include person re-ID and vehicle re-ID, where the source-domain and target-domain data do not share the same identities (classes). Existing UDA methods on object re-ID [38, 11, 54, 10, 55, 45] generally tackled this problem following a two-stage training scheme: (1) supervised pre-training on the source domain, and (2) unsupervised ﬁne-tuning on the target domain. For stage-2 unsupervised ﬁne-tuning, a pseudo-label-based strategy was found effective in state-of-the-art methods [11, 54, 10, 55], which alternates between generating pseudo classes by clustering target-domain instances and training the network with generated pseudo classes. In this way, the source-domain pre-trained network can be adapted to capture the inter-sample relations in the target domain with noisy pseudo-class labels.
Although the pseudo-label-based methods have led to great performance advances, we argue that there exist two major limitations that hinder their further improvements (Figure 1 (a)). (1) During the target-domain ﬁne-tuning, the source-domain images were either not considered [11, 54, 10, 55] or were even found harmful to the ﬁnal performance [14] because of the limitations of their methodology
⇤Dapeng Chen is the corresponding author.
†Code is available at https://github.com/yxgeee/SpCL. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(cid:11)(cid:20)(cid:12)(cid:3)(cid:83)(cid:85)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:86)(cid:87)(cid:68)(cid:74)(cid:72)(cid:29) (cid:68)(cid:79)(cid:79)(cid:3)(cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:16)(cid:71)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68) (cid:72)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85) (cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:3)(cid:44)(cid:39)(cid:86) (cid:11)(cid:21)(cid:12)(cid:3)(cid:262)(cid:81)(cid:72)(cid:16)(cid:87)(cid:88)(cid:81)(cid:76)(cid:81)(cid:74)(cid:3)(cid:86)(cid:87)(cid:68)(cid:74)(cid:72)(cid:29) (cid:79)(cid:82)(cid:68)(cid:71)(cid:3)(cid:90)(cid:72)(cid:76)(cid:74)(cid:75)(cid:87)(cid:86) (cid:83)(cid:68)(cid:85)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:87)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:16)(cid:71)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68) (cid:72)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85) (cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:44)(cid:39)(cid:86) (cid:11)(cid:68)(cid:12)(cid:3)(cid:51)(cid:85)(cid:72)(cid:89)(cid:76)(cid:82)(cid:88)(cid:86)(cid:3)(cid:56)(cid:39)(cid:36)(cid:3)(cid:80)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71)(cid:86)(cid:3)(cid:82)(cid:81)(cid:3)(cid:82)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:3)(cid:85)(cid:72)(cid:16)(cid:44)(cid:39)(cid:29) (cid:11)(cid:69)(cid:12)(cid:3)(cid:50)(cid:88)(cid:85)(cid:3)(cid:80)(cid:72)(cid:87)(cid:75)(cid:82)(cid:71)(cid:29) (cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:16)(cid:71)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68) (cid:11)(cid:82)(cid:81)(cid:79)(cid:92)(cid:3)(cid:73)(cid:82)(cid:85)(cid:3)(cid:83)(cid:85)(cid:72)(cid:16)(cid:87)(cid:85)(cid:68)(cid:76)(cid:81)(cid:76)(cid:81)(cid:74)(cid:12) (cid:83)(cid:68)(cid:85)(cid:87)(cid:76)(cid:68)(cid:79)(cid:3)(cid:87)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:16)(cid:71)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68) (cid:72)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85) (cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:44)(cid:39)(cid:86) (cid:72)(cid:81)(cid:70)(cid:82)(cid:71)(cid:72)(cid:85) (cid:68)(cid:79)(cid:79)(cid:3)(cid:87)(cid:68)(cid:85)(cid:74)(cid:72)(cid:87)(cid:16)(cid:71)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68) (cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:3)(cid:44)(cid:39)(cid:86)(cid:15)(cid:3)(cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:3)(cid:44)(cid:39)(cid:86)(cid:3)(cid:9)(cid:3) (cid:88)(cid:81)(cid:16)(cid:70)(cid:79)(cid:88)(cid:86)(cid:87)(cid:72)(cid:85)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81)(cid:86)(cid:87)(cid:68)(cid:81)(cid:70)(cid:72)(cid:3)(cid:44)(cid:39)(cid:86) (cid:68)(cid:79)(cid:79)(cid:3)(cid:86)(cid:82)(cid:88)(cid:85)(cid:70)(cid:72)(cid:16)(cid:71)(cid:82)(cid:80)(cid:68)(cid:76)(cid:81)(cid:3)(cid:71)(cid:68)(cid:87)(cid:68) (cid:75)(cid:92)(cid:69)(cid:85)(cid:76)(cid:71)(cid:3)(cid:80)(cid:72)(cid:80)(cid:82)(cid:85)(cid:92)
Figure 1: State-of-the-arts [11, 10, 55, 54] on UDA object re-ID discarded both the source-domain data and target-domain un-clustered data for training, while our proposed self-paced contrastive learning framework fully exploits all available data with hybrid memory for joint feature learning. designs. The accurate source-domain ground-truth labels are valuable but were ignored during target-domain training. (2) Since the clustering process might result in individual outliers, to ensure the reliability of the generated pseudo labels, existing methods [11, 10, 55, 14] simply discarded the outliers from being used for training. However, such outliers might actually be difﬁcult but valuable samples in the target domain and there are generally many outliers especially in early epochs. Simply abandoning them might critically hurt the ﬁnal performance.
To overcome the problems, we propose a hybrid memory to encode all available information from both source and target domains for feature learning. For the source-domain data, their ground-truth class labels can naturally provide valuable supervisions. For the target-domain data, clustering can be conducted to obtain relatively conﬁdent clusters as well as un-clustered outliers. All the source-domain class centroids, target-domain cluster centroids, and target-domain un-clustered instance features from the hybrid memory can provide supervisory signals for jointly learning discriminative feature representations across the two domains (Figure 1 (b)). A uniﬁed framework is developed for dynamically updating and distinguishing different entries in the proposed hybrid memory.
Speciﬁcally, since all the target-domain clusters and un-clustered instances are equally treated as independent classes, the clustering reliability would signiﬁcantly impact the learned representations.
We thus propose a self-paced contrastive learning strategy, which initializes the learning process by using the hybrid memory with the most reliable target-domain clusters. Trained with such reliable clusters, the discriminativeness of feature representations can be gradually improved and additional reliable clusters can be formed by incorporating more un-clustered instances into the new clusters.
Such a strategy can effectively mitigate the effects of noisy pseudo labels and boost the feature learning process. To properly measure the cluster reliability, a novel multi-scale clustering reliability criterion is proposed, based on which only reliable clusters are preserved and other confusing clusters are disassembled back to un-clustered instances. In this way, our self-paced learning strategy gradually creates more reliable clusters to dynamically reﬁne the hybrid memory and learning targets.
Our contributions are summarized as three-fold. (1) We propose a uniﬁed contrastive learning framework to incorporate all available information from both source and target domains for joint feature learning. It dynamically updates the hybrid memory to provide class-level, cluster-level and instance-level supervisions. (2) We design a self-paced contrastive learning strategy with a novel clustering reliability criterion to prevent training error ampliﬁcation caused by noisy pseudo-class labels. It gradually generates more reliable target-domain clusters for learning better features in the hybrid memory, which in turn, improves clustering. (3) Our method signiﬁcantly outperforms state-of-the-arts [11, 54, 10, 55, 45] on multiple domain adaptation tasks of object re-ID with up to 5.0% mAP gains. The proposed uniﬁed framework could even boost the performance on the source domain with large margins (6.6%) by jointly training with un-annotated target-domain data, while most existing UDA methods “forget” the source domain after ﬁne-tuning on the target domain.
Our unsupervised version without labeled source-domain data on object re-ID task signiﬁcantly outperforms state-of-the-arts [26, 45, 53] by 16.7% and 7.9% in terms of mAP on Market-1501 and
MSMT17 benchmarks. 2