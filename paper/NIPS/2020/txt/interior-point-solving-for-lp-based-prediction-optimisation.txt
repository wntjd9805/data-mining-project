Abstract
Solving optimization problems is the key to decision making in many real-life analytics applications. However, the coefﬁcients of the optimization problems are often uncertain and dependent on external factors, such as future demand or energy or stock prices. Machine learning (ML) models, especially neural networks, are increasingly being used to estimate these coefﬁcients in a data-driven way. Hence, end-to-end predict-and-optimize approaches, which consider how effective the predicted values are to solve the optimization problem, have received increasing attention. In case of integer linear programming problems, a popular approach to overcome their non-differentiabilty is to add a quadratic penalty term to the continuous relaxation, such that results from differentiating over quadratic programs can be used. Instead we investigate the use of the more principled logarithmic barrier term, as widely used in interior point solvers for linear programming. Speciﬁcally, instead of differentiating the KKT conditions, we consider the homogeneous self-dual formulation of the LP and we show the relation between the interior point step direction and corresponding gradients needed for learning. Finally our empirical experiments demonstrate our approach performs as good as if not better than the state-of-the-art QPTL (Quadratic Programming task loss) formulation of Wilder et al. [29] and SPO approach of Elmachtoub and
Grigas [12]. 1

Introduction
There is recently a growing interest in data-driven decision making. In many analytics applications, a combinatorial optimization is used for decision making with the aim of maximizing a predeﬁned objective. However, in many real-world problems, there are uncertainties over the coefﬁcients of the objective function and they must be predicted from historical data by using a Machine Learning (ML) model, such as stock price prediction for portfolio optimization [5]. In this work, we propose a novel approach to integrate ML and optimization in a deep learning architecture for such applications.
We consider combinatorial optimization problems that can be formulated as a mixed integer linear program (MILP). MILP has been used, to tackle a number of combinatorial optimization problems, for instance, efﬁcient micro-grid scheduling [20], sales promotion planning [8] and more. Speciﬁcally, we want to train a neural network model to predict the coefﬁcients of the MILP in a way such that the parameters of the neural network are determined by minimizing a task loss, which takes the effect of the predicted coefﬁcients on the MILP output into consideration. The central challenge is how to compute the gradients from the MILP-based task loss, given that MILP is discrete and the Linear
Programming (LP) relaxation is not twice differentiable.
To do so, Wilder et al. [29] proposed to compute the gradients by differentiating the KKT conditions of the continuous relaxation of the MILP. However, to execute this, they have to add a quadratic 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
regularizer term to the objective. We overcome this by differentiating the homogeneous self-dual embedding of the relaxed LP. In summary, we present Linear Programs (LP) as the ﬁnal layer on top of a standard neural network architecture and this enables us to perform end-to-end training of an
MILP optimization problem.