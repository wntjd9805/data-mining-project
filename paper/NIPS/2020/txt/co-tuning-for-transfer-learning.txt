Abstract
Fine-tuning pre-trained deep neural networks (DNNs) to a target dataset, also known as transfer learning, is widely used in computer vision and NLP. Because task-speciﬁc layers mainly contain categorical information and categories vary with datasets, practitioners only partially transfer pre-trained models by discarding task-speciﬁc layers and ﬁne-tuning bottom layers. However, it is a reckless loss to simply discard task-speciﬁc parameters which take up as many as 20% of the total parameters in pre-trained models. To fully transfer pre-trained models, we propose a two-step framework named Co-Tuning: (i) learn the relationship between source categories and target categories from the pre-trained model with calibrated predic-tions; (ii) target labels (one-hot labels), as well as source labels (probabilistic labels) translated by the category relationship, collaboratively supervise the ﬁne-tuning process. A simple instantiation of the framework shows strong empirical results in four visual classiﬁcation tasks and one NLP classiﬁcation task, bringing up to 20% relative improvement. While state-of-the-art ﬁne-tuning techniques mainly focus on how to impose regularization when data are not abundant, Co-Tuning works not only in medium-scale datasets (100 samples per class) but also in large-scale datasets (1000 samples per class) where regularization-based methods bring no gains over the vanilla ﬁne-tuning. Co-Tuning relies on a typically valid assumption that the pre-trained dataset is diverse enough, implying its broad application areas. 1

Introduction
The practice of ﬁne-tuning pre-trained models, also known as transfer learning, is prevalent in deep learning. In computer vision, we have models pre-trained on the ImageNet (Deng et al., 2009) classiﬁcation task, such as ResNet (He et al., 2016) and DenseNet (Huang et al., 2017). In NLP, we witness the recent emergence of self-attention (Vaswani et al., 2017) based models pre-trained on a large-scale corpus for masked language modeling, including BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), etc. Compared with training from scratch, ﬁne-tuning requires less labeled data, enables faster training, and usually achieves better performance (He et al., 2019).
The rule of thumb for many deep learning tasks is to (i) start from a pre-trained model, (ii) remove task-speciﬁc top layers, and (iii) ﬁne-tune bottom layers on the target task as a feature extractor.
This way, pre-trained models are partially transferred because only parameters in bottom layers are transferred. As shown in Table 1, however, parameters in task-speciﬁc layers can take up over 20% of the total parameters. Simply discarding these layers is a reckless waste. To make the best of pre-trained models, task-speciﬁc layers should also be transferred to achieve full transfer of DNNs.
Although full transfer of DNNs is appealing, an obvious obstacle of transferring task-speciﬁc layers is that categories vary with datasets and it is difﬁcult to reuse these layers. For example, when transferring ImageNet pre-trained models to COCO-70 1, the relationship between 1000 categories 1A classiﬁcation dataset constructed from COCO (Lin et al., 2014). See Sec. 5.3 for details. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Parameter count in popular pre-trained models from torchvision and transformers.
Pre-trained model
ResNet-50 DenseNet-121
Inception-V3 BERT-base
Task-speciﬁc parameters / Million
Total parameters / Million
Percentage / % 2.0 25.6 7.8 1.0 8.0 12.5 2.0 27.2 7.4 22.9 108.9 21.0 in ImageNet and 70 categories in COCO-70 is required to transfer task-speciﬁc layers. However, manually ﬁguring out the relationship has many problems, including:
• Same concept, different names. COCO concept “elephant” almost means the same as
ImageNet concepts “indian elephant” and “african elephant”, but with different names.
• Same name, different concepts. The concept “horse” in COCO often co-occurs with human, while the concept “horse” in ImageNet often co-occurs with cart. Although they share the same name “horse”, they have different contexts.
• Probabilistic relationship. Concept relationship is probabilistic rather than binary, which is hard for human to reason about (see Fig. 1 for example).
Figure 1: The relationship between COCO categories and ImageNet categories learned by Alg. 1.
The algorithm successfully ﬁgures out the relationship among both concrete categories (“elephant”,
“tusker”, “donut”, “bagel”) and abstract categories (“bakery”).
To address the problem, we propose a Co-Tuning framework for transfer learning to achieve full transfer of pre-trained DNNs. It ﬁrst learns the relationship between source categories and target cate-gories (see Fig. 1 as an example), then translates one-hot target labels into probabilistic source labels which collaboratively supervise the ﬁne-tuning process. We also propose two possible approaches to learn the category relationship. It is also highly pluggable and easy to use.
The Co-Tuning framework is empirically evaluated in four visual classiﬁcation tasks and one named entity recognition task. It brings unanimous improvement compared to standard ﬁne-tuning, with up to 20% improvement. When abundant data are available and regularization-based methods fail to improve over the vanilla ﬁne-tuning, Co-Tuning still brings gains by a large margin. 2