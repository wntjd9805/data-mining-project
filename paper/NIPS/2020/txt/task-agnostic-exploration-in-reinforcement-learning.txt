Abstract
Efﬁcient exploration is one of the main challenges in reinforcement learning (RL).
Most existing sample-efﬁcient algorithms assume the existence of a single reward function during exploration. In many practical scenarios, however, there is not a single underlying reward function to guide the exploration, for instance, when an agent needs to learn many skills simultaneously, or multiple conﬂicting objectives need to be balanced. To address these challenges, we propose the task-agnostic
RL framework: In the exploration phase, the agent ﬁrst collects trajectories by exploring the MDP without the guidance of a reward function. After exploration, it aims at ﬁnding near-optimal policies for N tasks, given the collected trajectories augmented with sampled rewards for each task. We present an efﬁcient task-agnostic RL algorithm, UCBZERO, that ﬁnds ε-optimal policies for N arbitrary tasks after at most ˜O(log(N )H 5SA/ε2) exploration episodes, where H is the episode length, S is the state space size, and A is the action space size. We also provide an Ω(log(N )H 2SA/ε2) lower bound, showing that the log dependency on
N is unavoidable. Furthermore, we provide an N -independent sample complexity bound of UCBZERO in the recently proposed reward-free setting, a statistically easier setting where the ground truth reward functions are known. 1

Introduction
Efﬁcient exploration is widely regarded as one of the main challenges in reinforcement learn-ing (RL). Recent works in theoretical RL have provided near-optimal algorithms in both model-based [Jaksch et al., 2010, Azar et al., 2017, Zanette and Brunskill, 2019] and model-free
[Strehl et al., 2006, Jin et al., 2018, Zhang et al., 2020] paradigms, that are able to learn a near-optimal policy with a sample complexity that matches the information-theoretical lower bound.
However, these algorithms are designed to solve a single pre-deﬁned task and rely on the assump-tion that a well-deﬁned reward function is available during exploration. In such settings, policy optimization can be performed simultaneously with exploration and results in an effective exploration-exploitation trade-off.
In many real-world applications, however, a pre-speciﬁed reward function is not available during exploration. For instance, in recommendation systems, reward often consists of multiple conﬂict-ing objectives, and the balance between them is tuned via continuous trial and error to encour-age the desired behavior [Zheng et al., 2018]; In hierarchical and multi-task RL [Dietterich, 2000,
Tessler et al., 2017, Oh et al., 2017], the agent aims at simultaneously learning a set of skills; In the robotic navigation problem [Rimon and Koditschek, 1992, Kretzschmar et al., 2016], the agent needs to navigate to not only one goal location, but a set of locations in the environment. Motivated by these real-world challenges, we ask: Is it possible to efﬁciently explore the environment and simultaneously learn a set of potentially conﬂicting tasks?
To answer this question, we present the task-agnostic RL paradigm: During the exploration phase, the agent collects trajectories from an MDP without the guidance of pre-speciﬁed reward function.
Next, in the policy-optimization phase, it aims at ﬁnding near-optimal policies for N tasks, given the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Setting
No. of tasks
Task-speciﬁc RL
N = 1
˜O
Upper bound (cid:17) (cid:16) H 2SA
ε2
[ORLC]
Reward-free RL
Task-agnostic RL
N dep.
N indep.
N dep.
N indep.
˜O
˜O (cid:17) (cid:16) log(N )H 5SA
ε2 (cid:16) H 5S2A
ε2
˜O (cid:16) log(N )H 5SA
ε2 (cid:17)
[UCBZERO] (cid:17)
[RFE]
[UCBZERO]
Lower bound (cid:17) (cid:16) H 2SA
ε2
Ω (cid:17)
Ω
-(cid:16) H 2S2A
ε2
Ω( log(N )H 2SA
ε2
)
∞
∞
Table 1: Comparison of sample complexity results in three RL settings. Shaded cells represent our results. collected trajectories augmented with the instantiated rewards sampled from the unknown reward function for each task. We emphasize that our framework covers applications like recommendation systems, where stochastic objectives (click rate, dwell time, etc.) are observed with the transitions, but the ground-truth reward functions are not known. There is a common belief in task-speciﬁc RL that estimating the reward function is not a statistical bottleneck. We will show that, in the more challenging task-agnostic setting, the need to estimate rewards makes the problem strictly harder.
To address the task-agnostic RL problem, we present an efﬁcient algorithm, UCBZERO, that can explore the MDP without the guidance of a pre-speciﬁed reward function. The algorithm design adopts the Optimism Under Uncertainty principle to perform exploration and uses a conceptually simpler Azuma-Hoeffding-type reward bonus, instead of a Bernstein-Friedman-type reward bonus that typically achieves better bounds in the task-speciﬁc RL framework. The advantage of an Hoeffding-type bonus is that it only depends on the number of visitations to the current state-action pair, but not on the reward value. This is a key property that allows it to be used in the task-agnostic setting. The
UCBZERO algorithm is deﬁned in Alg. 1. Despite its simplicity, UCBZero provably ﬁnds ε-optimal policies for N arbitrary tasks simultaneously after at most ˜O(log(N )H 5SA/ε2) exploration episodes.
To complement our algorithmic results, we establish a near-matching Ω(log(N )H 2SA/ε2) lower bound, demonstrating the near-optimality of UCBZERO as well as the necessity of the dependency on N , which is unique to the task-agnostic setting.
Our Contributions: 1. We propose a novel task-agnostic RL framework and present an algorithm, UCBZERO, that ﬁnds near-optimal polices to all N tasks with small sample complexity. In addition, we provide a near-matching lower bound, demonstrating the near-optimality of our algorithm. 2. We investigate interesting properties of the UCBZERO algorithm that shine a light on its success.
In particular, we show that (i) UCBZERO is able to visit all state-action pairs sufﬁciently often, and (ii) the transitions collected by UCBZERO allows one to obtain accurate estimate of the dynamics model. 3. Lastly, we provide an N -independent sample complexity bound of UCBZERO in a statistically easier setting studied in a contemporary work [Jin et al., 2020], where all ground truth reward func-tions are known. We provide a uniﬁed view of the two settings and present detailed discussion con-trasting the statistical challenges between the two. Table 1 summarizes our key theoretical results. 2