Abstract
Pairwise learning refers to learning tasks with loss functions depending on a pair of training examples, which includes ranking and metric learning as speciﬁc examples.
Recently, there has been an increasing amount of attention on the generalization analysis of pairwise learning to understand its practical behavior. However, the ex-isting stability analysis provides suboptimal high-probability generalization bounds.
In this paper, we provide a reﬁned stability analysis by developing generalization n-times faster than the existing results, where n is the sam-bounds which can be ple size. This implies excess risk bounds of the order O(n−1/2) (up to a logarithmic factor) for both regularized risk minimization and stochastic gradient descent. We also introduce a new on-average stability measure to develop optimistic bounds in a low noise setting. We apply our results to ranking and metric learning, and clearly show the advantage of our generalization bounds over the existing analysis.
√ 1

Introduction
In modern machine learning, we frequently encounter problems where the performance of a model depends on pairs of training instances. As examples consider the following. In ranking problems, our aim is to learn a function that can predict the ordering of examples [13, 44]. In metric learning, which plays a key role in clustering problems [9, 28], we wish to learn an adequate distance metric between instances. In AUC maximization, which is deployed to class-imbalanced learning problems, we aim to ﬁnd a classiﬁer that maximizes the probability of scoring a positive example higher than a negative one [14]. Further examples include learning with minimum error entropy loss functions
[27], multiple kernel learning [31], preference learning [22], and gradient learning [41]. All these so-called pairwise learning problems involve a loss function based on pairs of training examples.
This is in a sharp contrast to classiﬁcation and regression, where the loss function depends only on a single instance. Those problems are referred to as pointwise learning problems.
In machine learning, we frequently build predictive models by optimizing their empirical behavior on training instances, that is, to achieve a small training error. However, a small training error does not imply that the learnt models will generalize well to test examples. Generalization analysis—which is a central topic in statistical learning theory (SLT) [40]—studies the generalization gap between the training and testing errors. There is a large amount of work on the generalization analysis of learning algorithms, largely based on either algorithmic stability [7, 17], complexity analysis of models [3, 52],
PAC-Bayesian analysis [38], or integral operators [49, 53]. Most of this work focuses on pointwise learning, while pairwise learning is far less studied. A difﬁculty occurring in the generalization analysis of pairwise learning is that the objective function is not a sum of identically and independently distributed (i.i.d.) random variables [1, 9, 13, 30]—a fundamental assumption in SLT.
∗The ﬁrst two authors contributed equally 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we employ the methodology of algorithmic stability for generalization analysis of pairwise learning. Appealingly, algorithmic stability considers just the one prediction function output by the learner [7], while methods based on uniform convergence, such as the Rademacher complexity
[3], bound the difference of training and testing errors for all prediction functions. The latter approach generally involves a square-root dependency on the input dimension [2, 18, 54]. For comparison, algorithmic stability enables dimension-independent generalization bounds [20].
√
While there is preliminary work on the algorithmic stability of metric learning and ranking, the resulting generalization bounds are not satisfactory. The best existing bounds decay at the order n) [1, 28, 55], where γ is the uniform-stability constant of the learning algorithm. In of O(γ regularized risk minimization (RRM), this results in an excess risk bound of order O(n− 1 4 ) at best, where n is the number of training examples.
√
As a main contribution of this paper, we show an improved bound for this setting of order O(γ log n), which translates into O(cid:0) log(n)/ n(cid:1) for excess risks of RRM. Remarkably, although the bound n/ log(n), improves the previously best known rate achieved through stability analysis by a factor of it applies more generally: we remove the standard assumption of a bounded loss function used in the prevalent stability analyses [1, 8, 19, 20, 55]. The loss of some of the most commonly used pairwise learning methods—including rankSVM [29] and MPRank [15]—is unbounded, for which we show, for the ﬁrst time, a stability analysis. Based on our connection between generalization and stability, we also derive, to the best of our knowledge, the ﬁrst probabilistic generalization bound for stochastic gradient descent (SGD) in pairwise learning. Our result quantiﬁes how to trade-off optimization and generalization to achieve a reﬁned excess risk bound in this setting.
√
The above bound holds generally for any conﬁdence level, which is informative to understand the variability of the algorithm and is necessary if the algorithm is used many times [20]. Furthermore, we show a sharper bound, but which holds in expectation and in a realizable case (where zero training error is achievable). Such bounds are called optimistic bounds in the literature [50]. For this setting, we show an excess risk bound of order O(n−1). For the proof, we introduce a new on-average stability measure for pairwise learning and quantify its implication to generalization.
Finally, we consider applications of our general theory to ranking and metric learning, where we obtain generalization bounds with signiﬁcantly improved dependence on n as compared to the existing stability analysis. Furthermore, our stability analysis also removes the dependency on the complexity of the hypothesis space and the input dimension in the uniform convergence analysis.
Structure. We review related work in Section 2, and give background information in Section 3. We list main results in Section 4 and give applications in Section 5. We conclude the paper in Section 6. 2