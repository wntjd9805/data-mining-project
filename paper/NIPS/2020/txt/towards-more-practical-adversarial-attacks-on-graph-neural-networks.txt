Abstract
We study the black-box attacks on graph neural networks (GNNs) under a novel and realistic constraint: attackers have access to only a subset of nodes in the network, and they can only attack a small number of them. A node selection step is essential under this setup. We demonstrate that the structural inductive biases of GNN models can be an effective source for this type of attacks. Speciﬁcally, by exploiting the connection between the backward propagation of GNNs and random walks, we show that the common gradient-based white-box attacks can be generalized to the black-box setting via the connection between the gradient and an importance score similar to PageRank. In practice, we ﬁnd attacks based on this importance score indeed increase the classiﬁcation loss by a large margin, but they fail to signiﬁcantly increase the mis-classiﬁcation rate. Our theoretical and empirical analyses suggest that there is a discrepancy between the loss and mis-classiﬁcation rate, as the latter presents a diminishing-return pattern when the number of attacked nodes increases. Therefore, we propose a greedy procedure to correct the importance score that takes into account of the diminishing-return pattern. Experimental results show that the proposed procedure can signiﬁcantly increase the mis-classiﬁcation rate of common GNNs on real-world data without access to model parameters nor predictions. 1

Introduction
Graph neural networks (GNNs) [22], the family of deep learning models on graphs, have shown promising empirical performance on various applications of machine learning to graph data, such as recommender systems [27], social network analysis [12], and drug discovery [16]. Like other deep learning models, GNNs have also been shown to be vulnerable under adversarial attacks [30], which has recently attracted increasing research interest [9]. Indeed, adversarial attacks have been an efﬁcient tool to analyze both the theoretical properties as well as the practical accountability of graph neural networks. As graph data have more complex structures than image or text data, researchers have come up with diverse adversarial attack setups. For example, there are different tasks (node classiﬁcation and graph classiﬁcation), assumptions of attacker’s knowledge (white-box, grey-box, and black-box), strategies (node feature modiﬁcation and graph structure modiﬁcation), and corresponding budget or other constraints (norm of feature changes or number of edge changes).
Despite these research efforts, there is still a considerable gap between the existing attack setups and the reality. It is unreasonable to assume that an attacker can alter the input of a large proportion of nodes, and even if there is a budget limit, it is unreasonable to assume that they can attack any node as they wish. For example, in a real-world social network, the attackers usually only have access to a few bot accounts, and they are unlikely to be among the top nodes in the network; it is difﬁcult for
∗School of Information, University of Michigan, Ann Arbor, Michigan, USA
†Equal contribution.
‡Department of EECS, University of Michigan, Ann Arbor, Michigan, USA 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the attackers to hack and alter the properties of celebrity accounts. Moreover, an attacker usually has limited knowledge about the underling machine learning model used by the platform (e.g., they may roughly know what types of models are used but have no access to the model parameters or training labels). Motivated by the real-world scenario of attacks, in this paper we study a new type of black-box adversarial attack for node classiﬁcation tasks, which is more restricted and more practical, assuming that the attacker has no access to the model parameters or predictions. Our setup differs from existing work with a novel constraint on node access, where attackers only have access to a subset of nodes in the graph, and they can only manipulate a small number of them.
The proposed black-box adversarial attack requires a two-step procedure: 1) selecting a small subset of nodes to attack under the limits of node access; 2) altering the node attributes or edges under a per-node budget. In this paper, we focus on the ﬁrst step and study the node selection strategy. The key insight of the proposed strategy lies in the observation that, with no access to the GNN parameters or predictions, the strong structural inductive biases of the GNN models can be exploited as an effective information source of attacks. The structural inductive biases encoded by various neural architectures (e.g., the convolution kernel in convolutional neural networks) play important roles in the success of deep learning models. GNNs have even more explicit structural inductive biases due to the graph structure and their heavy weight sharing design. Theoretical analyses have shown that the understanding of structural inductive biases could lead to better designs of GNN models [25, 11].
From a new perspective, our work demonstrates that such structural inductive biases can turn into security concerns in a black-box attack, as the graph structure is usually exposed to the attackers.
Following this insight, we derive a node selection strategy with a formal analysis of the proposed black-box attack setup. By exploiting the connection between the backward propagation of GNNs and random walks, we ﬁrst generalize the gradient-norm in a white-box attack into a model-independent importance score similar to the PageRank. In practice, attacking the nodes with high importance scores increases the classiﬁcation loss signiﬁcantly but does not generate the same effect on the mis-classiﬁcation rate. Our theoretical and empirical analyses suggest that such discrepancy is due to the diminishing-return effect of the mis-classiﬁcation rate. We further propose a greedy correction procedure for calculating the importance scores. Experiments on three real-world benchmark datasets and popular GNN models show that the proposed attack strategy signiﬁcantly outperforms baseline methods. We summarize our main contributions as follows: 1. We propose a novel setup of black-box attacks for GNNs with a constraint of limited node access, which is by far the most restricted and practical compared to existing work. 2. We demonstrate that the structural inductive biases of GNNs can be exploited as an effective information source of black-box adversarial attacks. 3. We analyze the discrepancy between classiﬁcation loss and mis-classiﬁcation rate and propose a practical greedy method of adversarial attacks for node classiﬁcation tasks. 4. We empirically verify the effectiveness of the proposed method on three benchmark datasets with popular GNN models. 2