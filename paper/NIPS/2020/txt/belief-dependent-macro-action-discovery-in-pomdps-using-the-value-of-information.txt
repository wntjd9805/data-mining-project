Abstract
This work introduces macro-action discovery using value-of-information (VoI) for robust and efﬁcient planning in partially observable Markov decision processes (POMDPs). POMDPs are a powerful framework for planning under uncertainty.
Previous approaches have used high-level macro-actions within POMDP policies to reduce planning complexity. However, macro-action design is often heuristic and rarely comes with performance guarantees. Here, we present a method for extracting belief-dependent, variable-length macro-actions directly from a low-level
POMDP model. We construct macro-actions by chaining sequences of open-loop actions together when the task-speciﬁc value of information (VoI) — the change in expected task performance caused by observations in the current planning iteration
— is low. Importantly, we provide performance guarantees on the resulting VoI macro-action policies in the form of bounded regret relative to the optimal policy.
In simulated tracking experiments, we achieve higher reward than both closed-loop and hand-coded macro-action baselines, selectively using VoI macro-actions to reduce planning complexity while maintaining near-optimal task performance. 1

Introduction
Partially observable Markov decision processes (POMDPs) are a powerful and general framework for model-based planning under uncertainty [9]. A core challenge in POMDP planning is that the optimally reachable belief space R∗(b0) — the set of beliefs that are reachable from an initial belief b0 under stochastic observation transitions when following an optimal policy — grows exponentially with the planning horizon in the size of the observation set. The complexity of computing an optimal
POMDP policy is related to the covering number of R∗(b0) [13], and this exponential growth poses a challenge for planning algorithms that attempt to approximate R∗(b0) using ofﬂine, point-based approximations [11, 15, 17] or online, sampling methods [18–20].
Previous approaches have introduced high-level macro-actions 1 or options [21], such as drive to the nearest exit, to reduce planning complexity in complex tasks. Policies that use open-loop macro-actions have the dual beneﬁts of a shorter effective planning horizon and smaller reachable belief space (RBS), as a policy’s reachable belief space grows linearly rather than exponentially when acting in open-loop (Figure 1). However, macro-actions are largely hand-coded [2, 6, 22] or learned without formal guarantees [1, 3, 8]. Here, we address the key challenge of generating macro-actions from a low-level POMDP model such that the resulting policies have bounded regret.
This paper introduces a method for generating belief-dependent, variable-length macro-actions using a point-based representation of the POMDP value function. Our key insight is to introduce a value of information (VoI) function — which estimates the change in expected task performance caused by sensing in the current planning iteration — and constrain policies to selectively act open-loop 1 We use the term macro-action synonymously with open loop action sequence. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
when VoI is low. Unlike hand-coded or learned macro-actions, we show that a horizon-H policy utilizing VoI macro-actions has bounded regret rH compared to the optimal policy. Letting V ∗
H be the expected reward of an optimal policy and V M A
H the expected reward of the VoI macro-action policy, our main result (Theorem 5.2) shows: rH = (cid:13) (cid:13)V ∗
H − V M A
H (cid:13) (cid:13)∞ ≤ (cid:16) 1 − γH 1 − γ (cid:0)3L +
δB
Rmax 1 − kγ
+ Lγk(cid:1) + τ (cid:17)
, (1) where γ is the POMDP discount factor, L is a Lipschitz constant describing the smoothness of the value function in belief space, and the POMDP reward function is bounded in [−Rmax, Rmax].
The three remaining terms — τ , δB, and k — elucidate the key trade-offs for macro-action-based
POMDP planning. Introducing potentially sub-optimal macro-actions into a policy increases regret.
The parameter τ is a VoI threshold, below which the planner acts in open-loop; high values of τ increase macro-action utilization but also increase regret. However, macro-action policies are often easier to approximate than an optimal policy. During planning, we approximate the value function at a set of beliefs that form a δB-covering of the macro-action policy’s RBS. Since the open-loop belief dynamics are a k-contractive mapping on belief space, this RBS grows slowly when acting in open-loop; macro-action utilization leads to lower values of δB and lower regret bounds. The form of
Eq. 1 makes the trade-off between policy complexity, as measured by the size of a policy’s reachable belief space, and policy performance explicit. Somewhat surprisingly, although consistent with Eq. 1, our empirical results demonstrate that macro-action policies can even outperform approximations of the optimal policy when planning with a ﬁnite point-based belief representation.
In the following sections, we introduce VoI macro-action generation and present empirical results in a set of simulated tracking experiments. Taken together, VoI macro-action generation and the associated regret bound address two fundamental questions for macro-action-based planning in partially observable domains: how do we construct high-value macro-actions and when can we use them without compromising policy performance? 2