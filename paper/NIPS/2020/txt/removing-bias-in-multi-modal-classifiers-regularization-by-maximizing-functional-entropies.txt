Abstract
Many recent datasets contain a variety of different data modalities, for instance, im-age, question, and answer data in visual question answering (VQA). When training deep net classiﬁers on those multi-modal datasets, the modalities get exploited at different scales, i.e., some modalities can more easily contribute to the classiﬁcation results than others. This is suboptimal because the classiﬁer is inherently biased towards a subset of the modalities. To alleviate this shortcoming, we propose a novel regularization term based on the functional entropy. Intuitively, this term encourages to balance the contribution of each modality to the classiﬁcation result.
However, regularization with the functional entropy is challenging. To address this, we develop a method based on the log-Sobolev inequality, which bounds the func-tional entropy with the functional-Fisher-information. Intuitively, this maximizes the amount of information that the modalities contribute. On the two challenging multi-modal datasets VQA-CPv2 and SocialIQ, we obtain state-of-the-art results while more uniformly exploiting the modalities. In addition, we demonstrate the efﬁcacy of our method on Colored MNIST. 1

Introduction
Multi-modal data is ubiquitous and commonly used in many real-world applications. For instance, discriminative visual question answering systems take into account the question, the image and a variety of answers. In general, we treat data as multi-modal if it can be partitioned into semantic features, e.g., color and shape can be treated as multi-modal data.
Training of discriminative classiﬁers on multi-modal datasets like discriminative visual question answering almost always follows the classical machine learning paradigm: use a common loss function like cross-entropy and employ a standard (cid:96)2-norm regularizer (a.k.a. weight decay). The regularizer favors ‘simple’ classiﬁers over more complex ones. These classical regularizers are suitable in traditional machine learning settings that predominantly use a single data modality.
Unfortunately, because they favor ‘simple’ models, their use is detrimental when learning from multi-modal data. Simplicity encourages use of information from a single modality, which often ends up biasing the learner. For instance, visual question answering models end up being driven by a language prior rather than visual understanding [1, 2, 3, 4]. E.g., answering ‘how many...?’ questions with ‘2’ regardless of the question. Another popular example consists of colored images whose label is correlated with their color modality and their shape modality. In these cases, standard learners often focus on the ‘simple’ color modality and largely ignore the shape modality [5, 6].
To address this issue, we develop a novel regularization term based on the functional entropy.
Intuitively, this term encourages to balance the contribution of each modality to classiﬁcation. To address the computational challenges of computing the functional entropy we develop a method based on the log-Sobolev inequality which bounds the functional entropy with the functional Fisher information. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: We illustrate our approach. In the visual question answering task, we are given a question about an image. Thus, we can partition our input into two modalities: a textual modality, and a visual modality. We measure the modalities’ functional Fisher information by evaluating the sensitivity of the prediction by perturbing each modality. We maximize the functional Fisher information by incorporating it into our loss as a regularization term. Our results show that our regularization permits higher utilization of the visual modality.
We illustrate the efﬁcacy of the proposed approach on the three challenging multi-modal datasets
Colored MNIST, VQA-CPv2, and SocialIQ. We ﬁnd that our regularization maximizes the utilization of essential information. We verify this empirically on the synthetic dataset Colored MNIST. We also evaluate on popular benchmarks, ﬁnding that our method permits a state-of-the-art performance on two datasets: SocialIQ (68.53% vs. 64.82%) and VQA-CPv2 (54.55% vs. 52.05%). 2