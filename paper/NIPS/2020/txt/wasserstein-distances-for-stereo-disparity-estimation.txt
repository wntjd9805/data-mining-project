Abstract
Existing approaches to depth or disparity estimation output a distribution over a set of pre-deﬁned discrete values. This leads to inaccurate results when the true depth or disparity does not match any of these values. The fact that this distribution is usually learned indirectly through a regression loss causes further problems in ambiguous regions around object boundaries. We address these issues using a new neural network architecture that is capable of outputting arbitrary depth values, and a new loss function that is derived from the Wasserstein distance between the true and the predicted distributions. We validate our approach on a variety of tasks, including stereo disparity and depth estimation, and the downstream 3D object detection. Our approach drastically reduces the error in ambiguous regions, especially around object boundaries that greatly affect the localization of objects in 3D, achieving the state-of-the-art in 3D object detection for autonomous driving.
Our code will be available at https://github.com/Div99/W-Stereo-Disp. 1

Introduction
Depth estimation from stereo images is a long-standing task in computer vision [28, 34]. It is a key component of many downstream problems, ranging from 3D object detection in autonomous vehicles [8, 19, 31, 40, 51] to graphics applica-tions such as novel view generation [21, 52].
The importance of this task in practical appli-cations has led to a ﬂurry of recent research.
Convolutional networks have now superseded more classical techniques and led to signiﬁcant improvements in accuracy [4, 25, 41, 54].
These techniques estimate depth by ﬁnding ac-curate pixel correspondences and estimating the disparity between their X-coordinates, which is inversely proportional to depth. Because pixels have integral coordinates, so does the estimated disparity — causing even the resulting depth estimates to be discrete. This introduces inaccu-racy, as the ground truth disparity and depth are naturally real-valued. This discrepancy is typically addressed by predicting a categorical distribution over a ﬁxed set of discrete values, and then computing the expected depth from this distribution, which can in theory be any arbitrary real value (within the range of the set) [4, 12, 41, 51, 54].
Figure 1: The effect of our continuous disparity network (CDN). We show a person (green box) in front of a wall. The blue 3D points are obtained using PSMNet [4]. The red points from our CDN model are much better aligned with the shape of the objects: they do not suffer the streaking artifacts near edges. Yellow points are from the ground truth LiDAR. (One ﬂoor square is 1m×1m.)
In this paper, we argue that such a design choice may lead to inaccurate depth estimates, especially around object boundaries. For example, in Figure 1 we show the pixels (back-projected into 3D 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 2: Continuous disparity network (CDN). We propose to predict a real-value offset (yellow arrows) for each pre-deﬁned discrete disparity value (e.g., {1, 2, 3, 4}), turning a categorical distribution (magenta bars) to a continuous distribution (red bars), from which we can output the mode disparity for accurate estimation. using the depth estimates) along the boundary between a person in the foreground at 30m depth and a wall in the background at 70m depth. The predicted depth distribution of these border pixels is likely to be multi-modal, having two peaks around 30 and 70 meters. Simply taking the mean outputs a low probability value in between the two modes (e.g., 50m). Such “smoothed” depth estimates can have a strong negative impact on subsequent 3D object detection, as they “smear” the pedestrian around the edges towards the background (note the many blue points between the wall and the pedestrian). A bounding box including all these trailing points, far from the actual person, would strongly misrepresent the scene’s geometry. What may further aggravate the problem is how the distribution is usually learned. Existing approaches mostly learn the distribution via a regression loss: minimizing the distance between the mean value and the ground truth [12, 51]. In other words, there is no direct supervision to teach the model to assign higher probabilities around the truth depth.
To address these issues, we propose a novel neural network architecture for stereo disparity estimation that is capable of outputting a distribution over arbitrary disparity values, from which we can directly take the mode and bypass the mean. As with existing work, our model predicts a probability for each disparity value in a pre-deﬁned, discrete set. Additionally, it predicts a real-valued offset for each discrete value. This is a simple architectural modiﬁcation, but it has a profound impact. With these offsets, the output is converted from a discrete categorical distribution to a continuous distribution over disparity values: a mixture of Dirac delta functions, centered at the pre-deﬁned discrete values shifted by predicted offsets1. This simple addition of predicted offsets allows us to use the mode as the prediction during inference, instead of the mean, guaranteeing that the predicted depth has a high estimated probability. Figure 2 illustrates our model, continuous disparity network (CDN).
Next, we propose a novel loss function that provides a more informative objective during training.
Concretely, we allow uni- or multi-modal ground truth depth distributions (obtained from nearby pixels) and represent them as (mixtures of) Dirac delta functions. The learning objective is then to minimize the divergence between the predicted and the ground truth distributions. Noting that the two distributions might not have a common support, we apply the Wasserstein distance [39] to measure the divergence. While computing the exact Wasserstein distance of arbitrary distributions can be time-consuming, computing it for one-dimensional distributions (e.g., distributions of one-dimensional disparity) enjoys efﬁcient solutions, creating negligible training overhead.
Our proposed approach is both mathematically well-founded and practically extremely simple. It is compatible with most existing stereo depth or disparity estimation approaches — we only need to add an additional offset branch and replace the commonly used regression loss by the Wasserstein distance. We validate our approach using multiple existing stereo networks [4, 51, 54] on three tasks: stereo disparity estimation [25], stereo depth estimation [9], and 3D object detection [9]. The last is a downstream task using stereo depth as the input to detect objects in 3D. We conduct comprehensive experiments and show that our algorithm leads to signiﬁcant improvement in all three tasks. 2