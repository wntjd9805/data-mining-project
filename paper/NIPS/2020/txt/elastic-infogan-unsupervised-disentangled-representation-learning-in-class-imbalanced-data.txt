Abstract
We propose a novel unsupervised generative model that learns to disentangle object identity from other low-level aspects in class-imbalanced data. We ﬁrst investigate the issues surrounding the assumptions about uniformity made by InfoGAN [10], and demonstrate its ineffectiveness to properly disentangle object identity in im-balanced data. Our key idea is to make the discovery of the discrete latent factor of variation invariant to identity-preserving transformations in real images, and use that as a signal to learn the appropriate latent distribution representing object identity. Experiments on both artiﬁcial (MNIST, 3D cars, 3D chairs, ShapeNet) and real-world (YouTube-Faces) imbalanced datasets demonstrate the effectiveness of our method in disentangling object identity as a latent factor of variation. 1

Introduction
Generative models aim to model the true data distribution, so that fake samples that seemingly belong to the modeled distribution can be generated [1, 42, 6]. Recent deep neural network based models such as Generative Adversarial Networks [19, 44, 43] and Variational Autoencoders [33, 24] have led to promising results in generating realistic samples for high-dimensional and complex data such as images. More advanced models show how to discover disentangled (factorized) representations
[57, 10, 49, 26, 47], in which different latent dimensions can be made to represent independent factors of variation (e.g., pose, identity) in the data (e.g., human faces).
InfoGAN [10] in particular, learns an unsupervised disentangled representation by maximizing the mutual information between the discrete or continuous latent variables and the corresponding generated samples. For discrete latent factors (e.g., digit identities), it assumes that they are uni-formly distributed in the data, and approximates them accordingly using a ﬁxed uniform categorical distribution. Although this assumption holds true for many benchmark datasets (e.g., MNIST [34]), real-word data often follows a long-tailed distribution and rarely exhibits perfect balance between the categories. Indeed, applying InfoGAN on imbalanced data can result in incoherent groupings, since it is forced to discover potentially non-existent factors that are uniformly distributed in the data; see
Fig. 1.
In this work, we augment InfoGAN to discover disentangled categorical representations from imbal-anced data. Our model, Elastic-InfoGAN, makes two improvements to InfoGAN which are simple and intuitive. First, we remodel the way the latent distribution is used to fetch the latent variables; we lift the assumption of any knowledge about the underlying class distribution, where instead of deciding and ﬁxing them beforehand, we treat the class probabilities as learnable parameters of the optimization process. To enable the ﬂow of gradients back to the class probabilities, we employ the Gumbel-Softmax distribution [30, 36], which acts as a proxy for the categorical distribution, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (Left & Center) Samples generated with an InfoGAN model learned with a ﬁxed uniform categorical distribution Cat(K = 10, p = 0.1) on balanced and imbalanced data, respectively. Each row corresponds to a different learned latent category. (Right) Samples generated with Elastic-InfoGAN using its automatically learned latent categorical distribution. Although InfoGAN discovers digit identities in the balanced data, it produces redundant/incoherent groupings in the imbalanced data. In contrast, our model is able to discover digit identities in the imbalanced data. generating differentiable samples having properties similar to that of categorical samples. Our second improvement stems from an observation of a failure case of InfoGAN (Fig. 1 center); we see that the model has trouble generating consistent images from the same category for a latent dimension (e.g., rows 1, 2, 4). This indicates that there are other low-level factors (e.g., rotation, thickness) which the model focuses on while categorizing the images. Although there are multiple meaningful ways to partition unlabeled data—e.g., with digits, one partitioning could be based on identity, whereas another could be based on stroke width—we aim to discover the partitioning that groups objects according to a high-level factor like identity while being invariant to low-level “nuisance” factors like lighting, pose, and scale changes. To this end, we take inspiration from self-supervised contrastive representation learning literature [4, 23, 9] to learn representations focusing on object identity. Speciﬁcally, we enforce (i) similar representations for positive pairs (e.g., an image and its mirror-ﬂipped version), and (ii) dissimilar representations for negative pairs (e.g., two different images). As a result, the discovered latent factors align more closely with object identity, and less with other factors. Such partitionings focusing on object identity are more likely to be useful for downstream visual recognition applications; e.g. (i) semi-supervised object recognition [43, 41] or image retrieval using object-identity based image features; (ii) performing data augmentation to remove class-imbalance using synthetic images.
Importantly, Elastic-InfoGAN retains InfoGAN’s ability to jointly model both continuous and discrete factors in either balanced or imbalanced data scenarios. To our knowledge, our work is the ﬁrst to tackle the problem of disentangled representation learning in the scenario of imbalanced data, without the knowledge of ground-truth class distribution (Fig. 1 right). We show qualitatively and quantitatively our superiority in terms of the ability to disentangle object identity as a factor of variation, in comparison to relevant baselines. And in order to discover object identity as a factor, our results also provide interesting observations regarding the ideal distribution for the latent variables. 2