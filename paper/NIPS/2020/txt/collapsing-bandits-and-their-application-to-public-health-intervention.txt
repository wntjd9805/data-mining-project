Abstract
We propose and study Collapsing Bandits, a new restless multi-armed bandit (RMAB) setting in which each arm follows a binary-state Markovian process with a special structure: when an arm is played, the state is fully observed, thus
“collapsing” any uncertainty, but when an arm is passive, no observation is made, thus allowing uncertainty to evolve. The goal is to keep as many arms in the “good” state as possible by planning a limited budget of actions per round. Such Collapsing
Bandits are natural models for many healthcare domains in which health workers must simultaneously monitor patients and deliver interventions in a way that maximizes the health of their patient cohort. Our main contributions are as follows: (i) Building on the Whittle index technique for RMABs, we derive conditions under which the Collapsing Bandits problem is indexable. Our derivation hinges on novel conditions that characterize when the optimal policies may take the form of either
“forward” or “reverse” threshold policies. (ii) We exploit the optimality of threshold policies to build fast algorithms for computing the Whittle index, including a closed form. (iii) We evaluate our algorithm on several data distributions including data from a real-world healthcare task in which a worker must monitor and deliver interventions to maximize their patients’ adherence to tuberculosis medication. Our algorithm achieves a 3-order-of-magnitude speedup compared to state-of-the-art
RMAB techniques, while achieving similar performance. The code is available at: https://github.com/AdityaMate/collapsing_bandits 1

Introduction
Motivation. This paper considers scheduling problems in which a planner must act on k out of N binary-state processes each round. The planner fully observes the state of the processes on which she acts, then all processes undergo an action-dependent Markovian state transition; the state of the process is unobserved until it is acted upon again, resulting in uncertainty. The planner’s goal is to maximize the number of processes that are in some “good” state over the course of T rounds. This class of problems is natural in the context of monitoring tasks which arise in many domains such as sensor/machine maintenance [12, 10, 1, 31], anti-poaching patrols [26], and especially healthcare. For example, nurses or community health workers are employed to monitor and improve the adherence of patient cohorts to medications for diseases like diabetes [23], hypertension [4], tuberculosis [27, 5]
∗equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
and HIV [16, 15]. Their goal is to keep patients adherent (i.e., in the “good” state) but a health worker can only intervene on (visit) a limited number of patients each day. Health workers can play a similar role in monitoring and delivering interventions for patient mental health, e.g., in the context of depression [20, 22] or Alzheimer’s Disease [18].
We adopt the solution framework of Restless Multi-Arm Bandits (RMABs), a generalization of Multi-Arm Bandits (MABs) in which a planner may act on k out of N arms each round that each follow a
Markov Decision Process (MDP). Solving an RMAB is PSPACE-hard in general [24]. Therefore, a common approach is to consider the Lagrangian relaxation of the problem in which the k
N budget constraint is dualized. Solving the relaxed problem gives Lagrange multipliers which act as a greedy index heuristic, known as the Whittle index, for the original problem. Speciﬁcally, the Whittle index policy computes the Whittle index for each arm, then plays the top k arms with the largest indices.
The Whittle index policy has been shown to be asymptotically optimal (i.e., N → ∞ with ﬁxed k
N ) under a technical condition [32] and generally performs well empirically [3] making it a common solution technique for RMABs.
Critically, using the Whittle index policy requires two key components: (i) a fast method for computing the index and (ii) proving the problem satisﬁes a technical condition known as indexability.
Without (i) the approach can be prohibitively slow, and without (ii) asymptotic performance guarantees are sacriﬁced [32]. Neither (i) nor (ii) are known for general RMABs. Therefore, to capture the scheduling problems addressed in this work, we introduce a new subclass of RMABs, Collapsing
Bandits, distinguished by the following feature: when an arm is played, the agent fully observes its state, “collapsing” any uncertainty, but when an arm is passive, no observation is made and uncertainty evolves. We show that this RMAB subclass is more general than previous models and leads to new theoretical results, including conditions under which the problem is indexable and under which optimal policies follow one of two simple threshold types. We use these results to develop algorithms for quickly computing the Whittle index. In experiments, we analyze the algorithms’ performance on (i) data from a real-world healthcare scheduling task in which our approach ties state-of-the-art performance at a fraction the runtime and (ii) various synthetic distributions, some of which the algorithm achieves performance comparable to the state of the art even outside its optimality conditions.
To summarize, our contributions are as follows: (i) We introduce a new subclass of RMABs, Collaps-ing Bandits, (ii) Derive theoretical conditions for Whittle indexability and for the optimal policy to be threshold-type, and (iii) Develop an efﬁcient solution that achieves a 3-order-of-magnitude speedup compared to more general state-of-the-art RMAB techniques, without sacriﬁcing performance. 2 Restless Multi-Armed Bandits
An RMAB consists of a set of N arms, each associated with a two-action MDP [25]. An MDP
{S, A, r, P } consists of a set of states S, a set of actions A, a state-dependent reward function r : S → R, and a transition function P , where P a s,s(cid:48) denotes the probability of transitioning from state s to s(cid:48) when action a is taken. An MDP policy π : S → A represents a choice of action to take at each state. We will consider both discounted and average reward criteria. The long-term discounted reward
β(s) = E [(cid:80)∞ t=0 βtr(st+1 ∼ T (st, π(st), st+1)|π, s0 = s] starting from state s0 = s is deﬁned as Rπ where β ∈ [0, 1) is the discount factor and actions are selected using π. To deﬁne average reward, let f π(s) : S → [0, 1] denote the occupancy frequency induced by policy π, i.e., the fraction of time spent in each state of the MDP. The average reward R of policy π be deﬁned as the expected reward computed over the occupancy frequency: R
= (cid:80)
π
π s∈S f π(s)r(s).
Each arm in an RMAB is an MDP with the action set A = {0, 1}. Action 1 (0) is called the active (passive) action and denotes the arm being pulled (not pulled). The agent can pull at most k arms at each time step. The agent’s goal is to maximize either her discounted or average reward across the arms over time. Some RMAB problems need to account for partial observability of states. It is sufﬁcient to let the MDP state be the belief state: the probability of being in each latent state
[14]. While intractable in general due to inﬁnite number of reachable belief states, most partially observable RMABs studied (including our Collapsing Bandits) have polynomially many belief states due to a ﬁnite time horizon or other structures. 2