Abstract
Previous work on adversarially robust neural networks for image classiﬁcation requires large training sets and computationally expensive training procedures.
On the other hand, few-shot learning methods are highly vulnerable to adversar-ial examples. The goal of our work is to produce networks which both perform well at few-shot classiﬁcation tasks and are simultaneously robust to adversarial examples. We develop an algorithm, called Adversarial Querying (AQ), for produc-ing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classiﬁcation tasks, such as Mini-ImageNet and
CIFAR-FS, than robust transfer learning. 1

Introduction
For safety-critical applications like facial recognition, algorithmic trading, and copyright control, adversarial attacks pose an actionable threat [35, 12, 25]. Conventional adversarial training and pre-processing defenses aim to produce networks that resist attack [19, 34, 26], but such defenses rely heavily on the availability of large training datasets. In applications that require few-shot learning, such as face recognition from few images, recognition of a video source from a single clip, or recognition of a new object from few example photos, the conventional robust training pipeline breaks down.
When data is scarce or new classes arise frequently, neural networks must adapt quickly [7, 14, 24, 30].
In these situations, meta-learning methods conduct few-shot learning by creating networks that learn quickly from little data and with computationally cheap ﬁne-tuning. While state-of-the-art meta-learning methods perform well on benchmark few-shot classiﬁcation tasks, these naturally trained neural networks are highly vulnerable to adversarial examples. In fact, even adversarially trained feature extractors fail to resist attacks in the few-shot setting (see Section 4.1).
We propose a new approach, called adversarial querying, in which the network is exposed to adversarial attacks during the query step of meta-learning. This algorithm-agnostic method produces a feature extractor that is robust, even without adversarial training during ﬁne-tuning. In the few-shot setting, we show that adversarial querying outperforms other robustness techniques by a wide margin in terms of both clean accuracy and adversarial robustness (see Table 1). We solve the following minimax problem: min
θ
ES,(x,y) max
�p<� L
δ
�
�
� (FA(θ,S), x + δ, y)
, (1) where S and (x, y) are data sampled from the training distribution, A is a ﬁne-tuning algorithm for the model parameters, θ, and � is a p-norm bound for the attacker. In Section 4, we further
∗Authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
motivate adversarial querying and exhibit a wide range of experiments. To motivate the necessity for adversarial querying, we test methods, such as adversarial ﬁne-tuning and pre-processing defenses, which if successful, would eliminate the need for expensive adversarial training routines. We ﬁnd that these methods are far less effective than adversarial querying.
Model
Naturally Trained R2-D2
AT transfer (R2-D2 backbone)
ADML
AQ R2-D2 (ours)
Anat 73.01 % 39.13 % 47.75% 57.87% 0.13 0.13 0.13 0.13
±
±
±
±
Aadv 0.00 % 25.33% 18.49 % 31.52% 0.13 0.13 0.13 0.13
±
±
±
±
Table 1: R2-D2 [3], adversarially trained transfer learning, ADML [33], and our adversarially queried (AQ) R2-D2 model on 5-shot Mini-ImageNet. Natural accuracy is denoted
Anat, and robust accuracy,
Aadv, is computed with a 20-step PGD attack as in [19] with � = 8/255. A description of our training regime can be found in Appendix A.5. All results are tested on 150000 total samples, so conﬁdence intervals of one standard error are of width at most 100 (0.5)(0.5)/150000% < 0.13%.
� 2