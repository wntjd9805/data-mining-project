Abstract
Deterministic-policy actor-critic algorithms for continuous control improve the actor by plugging its actions into the critic and ascending the action-value gradient, which is obtained by chaining the actor’s Jacobian matrix with the gradient of the critic with respect to input actions. However, instead of gradients, the critic is, typically, only trained to accurately predict expected returns, which, on their own, are useless for policy optimization. In this paper, we propose MAGE, a model-based actor-critic algorithm, grounded in the theory of policy gradients, which explicitly learns the action-value gradient. MAGE backpropagates through the learned dynamics to compute gradient targets in temporal difference learning, leading to a critic tailored for policy improvement. On a set of MuJoCo continuous-control tasks, we demonstrate the efﬁciency of the algorithm in comparison to model-free and model-based state-of-the-art baselines. 1

Introduction
Reinforcement learning (RL) [36, 52] studies sequential decision making problems, in which an agent aims at maximizing the cumulative reward it collects in an environment. One of the most popular classes of algorithms for RL are policy gradient methods [10, 53], which involve differentiable control policies improved by gradient ascent. They feature suitability to environments with continuous state and action spaces, and compatibility with state-of-the-art deep learning [42] methods. Policy gradient algorithms often employ an actor-critic [26] scheme: an actor, which determines the control policy, is evaluated using a critic. Thus, the degree of actor’s improvement is limited by the information provided by the critic, naturally raising the question of how the critic should be trained.
Typically, algorithms that use powerful function approximators [18, 28] learn the critic by temporal difference [50], optimizing for an accurate prediction of the expected return of the actor. For deterministic-policy continuous-control [28, 47], however, the value provided by the critic is neither used for improving the policy nor for acting in the environment [53]. Instead, only the action-gradient of the value function, i.e., the gradient of the critic w.r.t. the action performed by the actor, is employed during policy optimization. Speciﬁcally, the policy gradient is obtained through the computation of the action-value gradient, by chaining the actor’s Jacobian with the action-gradient of the critic.
Learning the critic by value rather than by action-gradient of the value relies on hazy smoothness assumptions on the real value function [47]. This means that, in conventional temporal difference learning, the critic learns action-value gradients implicitly, which could harm the performance of a deterministic policy gradient algorithm.
In this paper, we propose Model-based Action-Gradient-Estimator Policy Optimization (MAGE), a continuos-control deterministic-policy actor-critic algorithm that explicitly trains the critic to provide accurate action-gradients for the use in the policy improvement step. Motivated by both the theory on
∗Work done while at NNAISENSE. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Deterministic Policy Gradients [47] and practical considerations, MAGE uses temporal difference methods to minimize the error on the action-value gradient. For this, the algorithm leverages a trained dynamics model as a proxy for a differentiable environment and techniques reminiscent of double backpropagation [12]. On a challenging continuous control benchmark [6, 55], we show that MAGE is signiﬁcantly more sample-efﬁcient than state-of-the-art model-free and model-based baselines.
The rest of the paper is organized as follows. In Section 2, we provide the notation and background on deterministic policy gradients. Our algorithm, together with its theoretical motivation, is introduced in Section 3, followed by empirical results in Section 4. In Section 5, we present some of the related work and its relationship with our approach. 2