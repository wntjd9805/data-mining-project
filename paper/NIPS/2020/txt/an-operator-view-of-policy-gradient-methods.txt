Abstract
We cast policy gradient methods as the repeated application of two operators: a policy improvement operator I, which maps any policy π to a better one Iπ, and a projection operator P, which ﬁnds the best approximation of Iπ in the set of realizable policies. We use this framework to introduce operator-based versions of well-known policy gradient methods such as REINFORCE and PPO, which leads to a better understanding of their original counterparts. We also use the understanding we develop of the role of I and P to propose a new global lower bound of the expected return. This new perspective allows us to further bridge the gap between policy-based and value-based methods, showing how REINFORCE and the Bellman optimality operator, for example, can be seen as two sides of the same coin. 1

Introduction
Model-free reinforcement learning algorithms aim at learning a policy that maximizes the (discounted) sum of rewards directly from samples generated by the agent’s interactions with the environment.
These techniques mainly fall in one of two categories: value-based methods [e.g., 26, 15], where the agent predicts the value of taking an action and then chooses the action with the largest predicted value; and policy-based methods, where the agent directly learns a good distribution over actions at each state. Although several past works created connections between the two views [e.g., 11, 19], such connections are often limited to the optimal policy and they do not capture training dynamics.
In particular, value-based methods like ﬁtted Q-iteration [15] and Q-learning [26] are often cast as the iterative application of an improvement operator, the Bellman optimality operator, which transforms the value function into a “better” one (unless the value function is already the optimal one). When dealing with a restricted set of policies, we often use function approximation for the value function.
In this case, the learning procedure interleaves the improvement operator with a projection operator, which ﬁnds the best approximation of this improved value function in the space of realizable value functions.
While this view is the basis for many intuitions around the convergence of value-based methods, no such view exists for policy-gradient (PG) methods, which are usually cast as doing gradient ascent on a parametric function representing the expected return of the policy [e.g., 27, 18]. Although this property can be used to show the convergence when using a sufﬁciently small step-size, it does little to our understanding of the relationship of policy-gradient methods and value-based ones.
In this work, we show that PG methods can also be seen as repeatedly applying two operators akin to those encountered in value-based methods: (a) a policy improvement operator, which maps any policy to a policy achieving strictly larger return; and (b) a projection operator, which ﬁnds the best approximation of this new policy in the space of realizable policies. We then recast common PG methods under this framework, using their operator interpretations to shed light on their properties.
We also make the following additional contributions: (a) We present a lower bound on the perfor-mance of a policy using the state-action formulation, leading to an alternative to conservative policy improvement; (b) We provide a formal justiﬁcation of α-divergences in the imitation learning setting. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2