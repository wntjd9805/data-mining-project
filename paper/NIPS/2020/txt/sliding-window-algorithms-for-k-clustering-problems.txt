Abstract
The sliding window model of computation captures scenarios in which data is arriving continuously, but only the latest w elements should be used for analysis.
The goal is to design algorithms that update the solution efﬁciently with each arrival rather than recomputing it from scratch. In this work, we focus on k-clustering problems such as k-means and k-median.
In this setting, we provide simple and practical algorithms that offer stronger performance guarantees than previous results. Empirically, we show that our methods store only a small fraction of the data, are orders of magnitude faster, and ﬁnd solutions with costs only slightly higher than those returned by algorithms with access to the full dataset. 1

Introduction
Data clustering is a central tenet of unsupervised machine learning. One version of the problem can be phrased as grouping data into k clusters so that elements within the same cluster are similar to each other. Classic formulations of this question include the k-median and k-means problems for which good approximation algorithms are known [1, 44]. Unfortunately, these algorithms often do not scale to large modern datasets requiring researchers to turn to parallel [8], distributed [9], and streaming methods. In the latter model, points arrive one at a time and the goal is to ﬁnd algorithms that quickly update a small sketch (or summary) of the input data that can then be used to compute an approximately optimal solution.
One signiﬁcant limitation of the classic data stream model is that it ignores the time when a data point arrived; in fact, all of the points in the input are treated with equal signiﬁcance. However, in practice, it is often important (and sometimes necessary) to restrict the computation to very recent data. This restriction may be due to data freshness—e.g., when training a model on recent events, data from many days ago may be less relevant compared to data from the previous hour. Another motivation arises from legal reasons, e.g., data privacy laws such as the General Data Protection
Regulation (GDPR), encourage and mandate that companies not retain certain user data beyond a speciﬁed period. This has resulted in many products including a data retention policy [54]. Such recency requirements can be modeled by the sliding window model. Here the goal is to maintain a small sketch of the input data, just as with the streaming model, and then use only this sketch to approximate the solution on the last w elements of the stream.
Clustering in the sliding window model is the main question that we study in this work. A trivial solution simply maintains the w elements in the window and recomputes the clusters from scratch at each step. We intend to ﬁnd solutions that use less space, and are more efﬁcient at processing each 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
new element. In particular, we present an algorithm which uses space linear in k, and polylogarithmic in w, but still attains a constant factor approximation.