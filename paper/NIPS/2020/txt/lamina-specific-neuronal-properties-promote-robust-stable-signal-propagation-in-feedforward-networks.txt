Abstract
Feedforward networks (FFN) are ubiquitous structures in neural systems and have been studied to understand mechanisms of reliable signal and information transmis-sion. In many FFNs, neurons in one layer have intrinsic properties that are distinct from those in their pre-/postsynaptic layers, but how this affects network-level information processing remains unexplored. Here we show that layer-to-layer heterogeneity arising from lamina-speciﬁc cellular properties facilitates signal and information transmission in FFNs. Speciﬁcally, we found that signal transforma-tions, made by each layer of neurons on an input-driven spike signal, demodulate signal distortions introduced by preceding layers. This mechanism boosts infor-mation transfer carried by a propagating spike signal and thereby supports reliable spike signal and information transmission in a deep FFN. Our study suggests that distinct cell types in neural circuits, performing different computational functions, facilitate information processing on the whole. 1

Introduction
How different cell types in a neural system contribute to signal processing by the entire circuit is a prime question in neuroscience. Experimental investigations of this question are increasingly common, primarily due to advances in observing and manipulating neurons based on their genetic signature. Feedforward circuits are notable targets of those studies, since, in many systems, they have been observed to comprise cell groups or “layers” with properties distinct from those of other layers, in size, morphology, expression of membrane/intracellular mechanisms, etc. For example, in the
Drosophila antennal lobe (AL), projection neurons (PN) tend to show noisy ﬁring, slow responses to the onset of olfactory receptor neuron (ORN) ﬁring, and static voltage thresholds for spike generation whereas postsynaptic neurons of PNs in lateral horns (LHN) are less noisy, ﬁre early, and have dynamical ﬁring thresholds (Jeanne and Wilson, 2015). In the cerebellum, the granule cells are tiny neurons with simple morphology, but their postsynaptic targets, Purkinje cells, are large, with complex dendritic trees. Pre- and postsynaptic neurons having distinct intrinsic properties can be ubiquitously found in a wide variety of neural systems. These observations raise questions about the
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Lamina-speciﬁc intrinsic properties stabilize information transmission in a neural network. (A) Left: FFN with a single cell type (Top), and spikes at each layer, in two different modes of signal propagation (Bottom). One mode is ampliﬁcation by progressively evoking more and more spikes (red dots), and the other is dissipation by gradually losing spikes (blue dots). Right:
Trajectories of the two propagating signals in a signal space. The x- and y-axes represent independent signal characteristics, such as the number of spikes, temporal precision, etc. A star is a ﬁxed point of neuronal signal transformation, and a dotted line is a separatrix separating the two modes. (B)
Left: An FFN where neurons have lamina-speciﬁc intrinsic properties (Top). Each layer performs a layer-speciﬁc transformation, and can selectively transfer a subset of input spikes (circled red dots), ignoring those that cause signal distortion (Bottom). Right: Trajectory of a propagating signal in a signal space. The dotted circle surrounds a region (basin) where layer-speciﬁc transformations conﬁne the propagating signal. (C) Behaviors of the two types of neurons used in this study with different dynamics of their spiking thresholds, shown by membrane potential response (color) to constant or ﬂuctuating current injection (black). role of intrinsic properties and their laminar speciﬁcity. However, most theoretical and computational studies rarely take neuronal heterogeneity into account.
We addressed this question by studying the classical problem of how a spike signal, deﬁned by the evoked ﬁring of multiple neurons in one layer, can stably propagate through multiple downstream layers in an FFN (Diesmann et al., 1999; van Rossum et al., 2002; Reyes, 2003; Kumar et al., 2008a, 2010; Kremkow et al., 2010; Moldakarimov et al., 2015; Joglekar et al., 2018). FFN is an important model for reliable information transfer between distant brain regions, since, if a need for new functional connections arises due to learning, long-distance axons for direct connections cannot grow in adult brains but can be added only through evolutionary processes. Instead, the connectome data suggest that brain regions tend to organize into clusters to allow for strong indirect connectivity (Oh et al., 2014), which can form FFNs. Stable propagation of spike signals in FFNs also plays a key role in models of conscious perception (Joglekar et al., 2018; Van Vugt et al., 2018), learning in deep artiﬁcial networks (Samuel S Schoenholz and Sohl-Dickstein, 2017), etc.
Most of those studies assumed that FFNs have identical types of neurons, and thus each layer makes similar input/output transformations. In this case, an input-driven spike signal either gets stronger or weaker as it passes through layers, depending on the efﬁcacy of output spike generation, given input spikes and also given the characteristics of the network input (Fig. 1(A), Left). Then, the signal eventually reaches a ﬁxed point of layer-to-layer transformation or dissipates (Diesmann et al., 1999; Reyes, 2003; Moldakarimov et al., 2015) (Fig. 1(A), Right). In this scenario, stable signal transmission is achieved by speciﬁc conditions for a non-trivial ﬁxed point, which are often not robust for a wide range of initial signals. Also, irreversible signal distortion during propagation can cause the inevitable dissipation of information.
Introducing lamina-speciﬁc intrinsic properties in neurons can change this fundamentally (Fig. 1(B)).
If each layer transforms a propagating signal in a different direction than the previous one, a ﬁxed point will not exist in general. Instead, this prevents the repeated transformation of the signal in one direction, and the overall signal distortion over multiple layers can become smaller, compared to 2
networks with identical layers. In particular, if the transformation by one layer is in the opposite or nearly opposite direction to those by its presynaptic layer, it can limit signal distortion across multiple layers and facilitate stable propagation (Fig. 1(B) Right). At the same time, information transfer also improves. Signal distortion at each layer will accumulate if neurons in each layer repeatedly encode similar preferred features of a network input, and this will cause irreversible loss of information.
Contrarily, if the pre- and postsynaptic neurons encode distinct input features, more selective ﬁltering on presynaptic output by postsynaptic neurons will demodulate distortions that presynaptic signal transformation introduced (Fig. 1(B) Left). In this manner, FFNs with heterogeneous, lamina-speciﬁc neuronal properties can show enhanced information transmission compared to homogeneous FFNs.
Here we demonstrate how robust, stable signal and information transmission arise from laminar speciﬁcity of cell-intrinsic properties by computational FFN models. We ﬁrst introduce two types of neurons with different spiking dynamics, referred to as integrator and differentiator (Kumar et al., 2010; Ratté et al., 2013, 2015) (Fig. 1(C)). Using neural layers with such laminar speciﬁcity, we develop a model of the Drosophila AL network with three layers of ORNs, PNs, and LHNs. We show that this model replicates key ﬁndings of a recent experimental study (Jeanne and Wilson, 2015) that differences in spiking dynamics between PNs and LHNs can balance accuracy and speed in processing olfactory information, and furthermore that PN-to-LHN information transfer is nearly optimal. Then, we extend the model to a deep FFN and demonstrate robust and stable spike signal propagation, contrary to models with no laminar speciﬁcity in neuronal properties. Finally, we demonstrate that the speed of a propagating signal depends on the input signal property and, therefore, that deeper layers can use the latency coding for the input. 2