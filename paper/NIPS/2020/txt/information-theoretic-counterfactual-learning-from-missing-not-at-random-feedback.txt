Abstract
Counterfactual learning for dealing with missing-not-at-random data (MNAR) is an intriguing topic in the recommendation literature since MNAR data are ubiqui-tous in modern recommender systems. Missing-at-random (MAR) data, namely randomized controlled trials (RCTs), are usually required by most previous counter-factual learning methods for debiasing learning. However, the execution of RCTs is extraordinarily expensive in practice. To circumvent the use of RCTs, we build an information-theoretic counterfactual variational information bottleneck (CVIB), as an alternative for debiasing learning without RCTs. By separating the task-aware mutual information term in the original information bottleneck Lagrangian into factual and counterfactual parts, we derive a contrastive information loss and an additional output conﬁdence penalty, which facilitates balanced learning between the factual and counterfactual domains. Empirical evaluation on real-world datasets shows that our CVIB signiﬁcantly enhances both shallow and deep models, which sheds light on counterfactual learning in recommendation that goes beyond RCTs. 1

Introduction
A surge of research shows that the real-world logging policy often collects missing-not-at-random (MNAR) data (or selective labels) [25]. For example, users tend to reveal ratings for items they like, thus the observed users’ feedback, usually described by click-through-rate (CTR), can be substantially higher than those not observed yet. Consider a mini system with two users and three items in Fig. 1, when ignoring the unobserved events, the estimated average CTR from the observed outcomes is 2/3 ≈ 0.67. However, if the rest unobserved outcomes were 0, the true CTR would be 1/3 ≈ 0.33.
This gap between the factual and counterfactual ratings in MNAR situation further exaggerates due to path-dependence that the learned policy tends to overestimate on the observed events outcomes
[32].
Table 1: Missing ratings in a recommender system, where (cid:51), (cid:55) and ? mean positive, negative and unknown outcomes, respectively.
Item 1 (cid:51) (cid:55)
Item 2
Item 3
?
?
? (cid:51)
User A
User B
Vast majority of existing works in the recommendation literature neglect the MNAR effect, as they mainly focus on designing novel architectures and training techniques for improving model performance on the observed events [8, 12, 24], where the objective function is designed in principle 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: O decides the appearance of the event, and depends on the previous recommendation policy. of empirical risk minimization (ERM) as
LERM = 1
Nl (cid:88) (cid:96)u,i( ˆY , Y ). (u,i):Ou,i=1 (1) x = (u, i) is an event composed of user u ∈ U and item i ∈ I; Ou,i ∈ {0, 1} indicates whether the outcome of an event x = (u, i) is observed (i.e., whether item i is presented to user u); (cid:96)u,i( ˆY , Y ) is the loss function taking true outcome Y and predicted outcome ˆY as its inputs; and Nl and Nul are the numbers of observed and unobserved events, respectively. However, LERM is not an unbiased estimate of the true risk L [32]:
EO[LERM ] (cid:54)= L := 1
Nl + Nul (cid:88) u,i (cid:96)u,i( ˆY , Y ), (2) which indicates that the naive ERM-based method cannot guarantee model’s generalization ability on the counterfactuals.
As the distribution of O depends on the deployed recommendation policy at past, as shown in Fig. 1, we can regard it a representative of the policy bias, which inﬂuences the distribution of factual events and then the learned policy. In order to alleviate the policy bias, there are a series of works emphasizing on employing randomized controlled trials (RCTs) [6] to collect the so-called unbiased dataset. By adopting a uniform policy that randomly displays items to users, the logged feedback can be regarded as missing at random (MAR), which is consistent with the underlying joint distribution p(x, y). Therefore, one can either evaluate the model’s true generalization ability on MAR data [23], or utilize MAR data to debias learning via importance sampling [30]. Besides, Rosenfeld et al. [29] and Bonner and Vasile [5] proposed to employ domain adaptation from MNAR data to MAR data, in order to balance the predictive capability of the learned model over the factuals and counterfactuals.
However, RCTs are extraordinarily expensive to be executed in a real-world recommender system. It is tricky because no solid theoretical deﬁnition of how large RCTs would be representative enough. It is questionable if small RCTs, compared with the enormous quantity of possible events, can lead to a proper estimate of the users’ true preference distribution.
Considering the above challenges, we try to mitigate MNAR sampling bias via an RCT-free paradigm.
Different from the domain adaption based methods, we propose an information theoretic approach for learning a balanced model based on Information Bottleneck (IB) [35, 36], which is promising for learning a representation that is informative to labels and robust against input noise [2]. In particular, we extend the original IB to counterfactual learning with a variational approximation, termed Counterfactual Variational Information Bottleneck (CVIB). The principle of CVIB is to encourage the learned representation to be equally informative to both the factual and counterfactual outcomes, by specifying a contrastive information in order to improve the learned embeddings generalization ability on the counterfactual domain. Besides, the minimality term in CVIB contributes to pushing the embeddings independent of the policy bias. In summary, our contributions are as follows: 2
• We establish a novel CVIB framework adapted from IB, which suggests new avenues in counter-factual learning from MNAR data in an RCT-free paradigm.
• A novel solution is proposed to handle the optimization of CVIB on MNAR data, which speciﬁes a contrastive information term associated with a minimality term.
• We empirically investigate our method’s advantages on real-world datasets for correcting MNAR bias without the need of acquiring the RCTs.1 2 Counterfactual Variational Information Bottleneck
In this section, we start from the conception about the information bottleneck. Then, we propose our novel CVIB objective function by separating events into factual and counterfactual domains. After that, we provide insights in the interpretation of the minimality term in our CVIB. 2.1 Problem Setup
Embedding is a conceptually classical approach for modeling the user and item in rating prediction.
For example, in collaborative ﬁltering [22], it represents an event x = (u, i) by concatenation as z = (eu, ei), then generates outcome prediction by ˆy = e(cid:62) u ei. In deep learning models [13], there are multiple hidden layers h1, . . . , hL sequentially processing the embedding z, such as h1 = W (1)z and hl = W (l)hl−1 for l = 2, . . . , L. In this work, we view the feedforward neural network layers as a Markov chain of successive representations, indicated with y → x → z → h1 → · · · → hL → ˆy, (3) where y is the true feedback returned by users (may be unknown to our learning algorithm). According to Tishby et al. [36], a standard information bottleneck has the following form
LIB := βI(z; x) − I(z; y), (4) where I(·; ·) denotes mutual information of two random variables, and β is a Lagrangian multiplier.
Minimizing I(z; x) encourages the representation z to be compressed, i.e., being minimal to x.
And, maximizing the second term I(z; y) encourages z to be sufﬁcient to task y, such that z can be predictive of y. Essentially, LIB in Eq. (4) can be regarded as a supervised loss term plus an additional information regularization term on the representation [1]. Moreover, there is another random variable O that affects the appearance of events x, but is not informative to the true outcomes y, i.e., O⊥⊥y. In this scenario, we would like z to be independent of O, thus being free of policy bias.
The main challenge in adopting IB for optimization is that the mutual information in LIB is cumber-some for calculation. Although previous works try to derive computable proxy for speciﬁc tasks [3], they are not suitable for MNAR data. Since we only have partial feedback about y, i.e., the majority of events are counterfactuals, we have no access to their true outcomes. Next, we will turn to derive a new objective function addressing this challenge. 2.2 Building Contrastive Information Regularizer
For conciseness, we focus on a simple model with only the embedding layer z, namely x → z → ˆy, and extend it to multi-layer scenario in Section 2.3. Speciﬁcally, the second term in Eq. (4) is mutual information between embedding z and target task y. We separate the embeddings into two parts: z+ and z−, which represent factual and counterfactual embeddings, respectively, i.e., z+ ∼ p(z|x+) and z− ∼ p(z|x−).
As shown in Fig. 2, we factorize the original mutual information term by I(z; y) = I(z+, z−; y). We postulate that z+ and z− are independent, therefore according to the chain rule of mutual information:
I(z+, z−; y) = I(z+; y|z−) + I(z−; y) = I(z+; y) + I(z−; y). (5)
However, as the outcomes y of the counterfactuals are unknown, we have to identify another reﬁned solution. Speciﬁcally, we cast Eq. (5) to
I(z+; y) + I(z−; y) = I(z−; y) − I(z+; y) (cid:125) (cid:124) (cid:123)(cid:122) contrastive
+2I(z+; y) (6) 1Code is available at https://github.com/RyanWangZf/CVIB-Rec. 3
Figure 2: The events and embeddings are separated by O, and we introduce additional constrastive scheme between z+ and z−. from which we derive a contrastive term between z+ and z−. This characterization is helpful for us to introduce a hyperparameter α to control the degree of this contrastive penalty. We then rewrite the original IB loss to
LCV IB := βI(z; x) + α[I(z+; y) − I(z−; y)] − I(z+; y) as our new objective function, where we propose an information theoretic regularization on z+ and z−. Intuitively, minimizing this term corresponds to encouraging z+ and z− to be equally informative to the targeted task variable y, thus resulting in a more balanced model. More importantly, it does not need access to the counterfactual outcomes, which will be speciﬁed in Section 3.2. (7) 2.3 Minimal Embedding Insensitive to Policy Bias
Aside from the task-aware mutual information I(z; y) in Eq. (7), I(z; x) corresponds to the min-imality of the learned embedding. Recall that we assume that the event x follows the generative process p(x, O) = p(O)p(x|O), where O inﬂuences the appearance of x. Because O is independent to task y, we hope the predicted outcome ˆy is not inﬂuenced by O, or the learned embedding z should contain low information about O. In this viewpoint, following the practice by Achille and Soatto [1], we identify that the minimality term is actually beneﬁcial for embedding’s insensitivity against the nuisance O.
Proposition 1 (Minimal Representation Insensitive to Policy Bias). With the Markov chain assump-tion deﬁned by Eq. (3), for any hidden embedding hk, we can derive the upper bound of the I(hk; O)
I(hk; O) ≤ I(hk; x) − I(x; y) ≤ I(z; x) − I(x; y) ≤ I(z; x), (8) where the last term I(x; y) is a constant with respect to the training process.
Please refer to Appendix A for a proof. Above proposition implies that an embedding z is insensitive to the policy bias O, by simply reducing I(z; x). Meanwhile, by maximizing I(z; y) in IB Lagrangian, embedding z will be forced to retain minimum information from x that is pertinent to the task y. In deep models, according to the Data Processing Inequality (DPI) [9], minimizing I(z; x) also works for controlling the policy bias of the successive layers. 3 Tractable Optimization Framework
The proposed LCV IB is still intractable for optimization. In this section, we attempt to ﬁnd a tractable solution for three terms in LCV IB, respectively. And, we present our algorithm of learning debiased embeddings by LCV IB at last. 3.1 Minimality Term
The minimality term I(z; x) in Eq. (7) can be measured with a Kullback-Leibler (KL) divergence as
I(z; x) = Ex[KL(p(z|x) (cid:107) p(z))] = Ex (cid:20)(cid:90) p(z|x) log p(z|x)dz − (cid:90) p(z) log p(z)dz
. (9) (cid:21) 4
To avoid operating on the marginal p(z) = (cid:82) p(z|x)p(x)dx, we use a variational approximation of q(z) as the marginal p(z), which renders (cid:90)
− p(z) log p(z)dz ≤ − (cid:90) p(z) log q(z)dz ⇒ KL(p(z|x) (cid:107) p(z)) ≤ KL(p(z|x) (cid:107) q(z)). (10)
Suppose the posterior p(z|x) = N (e(x); diag(σ)) is a Gaussian distribution, where e(x) is the encoded embedding of input event x and diag(σ) indicates a diagonal matrix with elements σ =
{σd}D d=1. In other words, we assume the embedding is generated by z = e(x) + ε (cid:12) σ where ε ∼ N (0, I). (11)
If we ﬁx σd = 0 , ∀d, then z would default to a deterministic embedding. Moreover, by considering a standard Gaussian variational marginal q(z) = N (0, I), the KL term reduces to
KL(p(z|x) (cid:107) q(z)) = (cid:107)e(x)(cid:107)2 2 + (cid:18) (cid:88) d
σd − 1 2 (cid:19) log σd
− D, (12) which means for a deterministic embedding, minimizing term I(z; x) is equivalent to directly applying (cid:96)2-norm regularization on the embedding vector. 3.2 Contrastive Mutual Information Term
The mutual information I(z; y) = Hp(y) − Hp(y|z), where Hp(·) demotes the entropy of p(·). The
ﬁrst entropy term is constant, which means maximizing I(z; y) is equivalent to minimizing the second term Hp(y|z). We then further derive the lower bound of −Hp(y|z) as2 (cid:90) (cid:90) (cid:90) (cid:90)
I(z; y) =
≥ p(y, z) log p(y|z)dzdy + const p(y, z) log q(y|z)dzdy + const = −Hp,q(y|z) + const. (13)
The term q(y|z) is an estimate of p(y|z) with a classiﬁer parameterized by θ, e.g., weight matrices in deep networks or embedding parameters. We can use the cross entropy as a proxy for the mutual information in the IB objective [1, 15], because max I(z; y) ⇔ min Hp,q(y|z) as shown above.
Therefore, replacing the contrastive term I(z+; y)−I(z−; y) in Eq. (7) with Hp,q(y|z−)−Hp,q(y|z+) obtains
Hp,q(y|z−) − Hp,q(y|z+) = (cid:90) (cid:90) p(y, z+, z−)[log q(y|z+) − log q(y|z−)]dydz+dz−. (14)
Since we assume z+ and z− are independent, the generative process can be written as p(y, z+, z−) = p(y|z+, z−)p(z+)p(z−). In order to make the term tractable, we here approximate p(y|z+, z−) by q(y|z+),3 then cast Eq.(14) to (cid:90) p(y|z+, z−)[log q(y|z+) − log q(y|z−)]dy ⇒ (cid:90) q(y|z+)[log q(y|z+) − log q(y|z−)]dy. (15)
This term further goes to our ﬁnal results as (cid:90) q(y|z+) log q(y|z+)dy − (cid:90) q(y|z+) log q(y|z−)dy ⇒ Hq(y|z+, y|z−) − Hq(y|z+), (16) where the ﬁrst term of the right hand side is the cross entropy between q(y|z+) and q(y|z−). We identify that the second term is in line with the maximum entropy principle [4, 17] and the conﬁdence penalty proposed by Pereyra et al. [27] that is imposed on the output distribution of deep neural networks. While out of our derivation, the conﬁdence penalty is only imposed on the factual output q(y|z+). We hence propose balanced learning by restricting the distance between factual and counterfactual posterior, which provably strengthens model’s generalization over the underlying users’ true preference distribution. 2Here we slightly abuse notation by denoting Hp,q(y|z) := Ex∼p(x)Ez∼p(z|x) 3We may also pick p(y|z+) as the approximation, which renders Hp,q(y|z+, y|z−) − Hp,q(y|z+). However, (cid:82) −p(y|x) log q(y|z)dy. it has less operational meaning and its last term cancels out with another task-aware term I(z+; y). 5
Algorithm 1 Counterfactual Learning with CVIB in MNAR Data for Recommendation.
Require: Training factual set Ω+, counterfactual set Ω−; Hyperparameters α, β, γ;
Initialize model’s parameters θ; repeat
Sample a batch of paired factuals X +, Y + from Ω+, and counterfactuals X − from Ω−;
Compute the sufﬁciency term 1(cid:13) in Eq. (18);
Compute the balancing term 2(cid:13) in Eq. (18);
Compute the conﬁdence penalty term 3(cid:13) in Eq. (18);
Compute the minimality term 4(cid:13) in Eq. (18);
Compute the batch objective loss as ˆLCV IB = 1(cid:13) + α 2(cid:13) − γ 3(cid:13) + β 4(cid:13);
Update the model parameters θ via stochastic gradient descent based on ˆLCV IB; until training loss stops to decrease. 3.3 Task-aware Mutual Information Term
We next omit the superscript of z+ in I(z+; y) for simplicity in the following. Similar to the operation of task-aware mutual information in Eq. (13), we use an approximation q(y|z) to substitute p(y|z)
I(z; y) ≥ Ez,x (cid:20)(cid:90) p(y|x) log q(y|z)dy
⇒ −I(z; y) ≤ Hp,q(y|z), (17) (cid:21) which indicates that this term is a proxy of the cross entropy loss between the true outcome p(y|x) and the prediction q(y|z). 3.4 Algorithm
Taking all the above derivations together, we conclude to the ﬁnal objective function ˆLCV IB, which encompasses four terms:
ˆLCV IB = Hp,q(y|z+) (cid:124) (cid:125) (cid:123)(cid:122) 1(cid:13) Sufﬁciency
+ αHq,q(y|z+, y|z−) (cid:125) (cid:123)(cid:122) 2(cid:13) Balancing (cid:124)
− γHq(y|z+) (cid:123)(cid:122) (cid:125) 3(cid:13) Penalty (cid:124) (cid:124)
+ β((cid:107)e(x+)(cid:107)2 2 + (cid:107)e(x−)(cid:107)2 2) (cid:125) (cid:123)(cid:122) 4(cid:13) Minimality (18) where term 1(cid:13) denotes the cross entropy between p(y|z+) and q(y|z+); term 2(cid:13) is cross entropy between q(y|z+) and q(y|z−); and term 3(cid:13) is entropy of q(y|z+). For computing this objective, we need to draw x+, y+ from the factual set Ω+ and x− from the counterfactual set Ω− in one iteration. Speciﬁcally, term 1(cid:13) is supervised loss on the factual outcomes; terms 2(cid:13), 3(cid:13) are contrastive regularization for balancing between factual and counterfactual domains; and term 4(cid:13) is minimality loss to improve the model’s robustness against policy bias. The whole optimization process over the
ˆLCV IB is hence summarized in Algorithm 1, where we should specify the values of α, β and γ to balance these terms in optimization. 4 Empirical Evaluation
Aiming to validate CVIB’s effectiveness, we perform experiments on real-world datasets in this section. We elaborate on the experimental setups, and report the comparison results between CVIB and other baselines, which substantiate its all-round superiority in counterfactual learning. 4.1 Datasets
In order to evaluate the learned model’s generalization ability on the underlying groundtruth distribu-tion p(x, y), rather than the only on the logged feedback, i.e., the factual domain, we usually need an additional MAR test set, where the users are served with uniformly displayed items, namely RCTs.
As far as we know, there are only two open datasets that satisfy this requirement:
Yahoo! R3 Dataset [25]. This is a user-song rating dataset, where there are over 300K ratings self-selected by 15,400 users in its training set, hence it is MNAR data. Besides, they collect an additional MAR test set by asking 5,400 users to rate 10 randomly displayed songs. 6
Table 2: MSE and AUC on the MAR test set of COAT [30] and YAHOO [25], where the best ones are in bold.
COAT
YAHOO
MF
+IPS [30]
+SNIPS [33]
+DR [18]
+DRJL [37]
+CVIB (ours)
NCF
+IPS [30]
+SNIPS [33]
+DR [18]
+DRJL [37]
+CVIB (ours)
MSE 0.2451 0.2299 0.2374 0.2357 0.2423 0.2189 0.2030 0.2008 0.1922 0.2161 0.2097 0.2017
AUC 0.7020 0.7156 0.6960 0.7058 0.6915 0.7218 0.7688 0.7708 0.7695 0.7514 0.7579 0.7713
MSE 0.2493 0.2260 0.1945 0.2108 0.2745 0.1671 0.3313 0.1777 0.1699 0.1698 0.2789 0.2820
AUC 0.6767 0.6793 0.6810 0.6883 0.6892 0.7198 0.6772 0.6708 0.6880 0.6886 0.6820 0.6989
Coat Shopping Dataset [30]. This dataset consists of 290 users and 300 items. Each user rates 24 items by themselves, and is asked to rate 16 uniformly displayed items as the MAR test set.
We simplify the underlying rating prediction problem to binary classiﬁcation in our experiments, by making rating which is 3 or higher be positive feedback and those lower than 3 as negative. 4.2 Baselines
Our CVIB is model-agnostic, thus applicable for most models in recommendation that take embed-dings to encode the events, including the shallow and deep models. In our experiments, we pick matrix factorization (MF) [21] as shallow backbone and neural collaborative ﬁltering (NCF) [13] as the deep backbone. Both of these methods project users and items into a shared space and represent them with unique vectors, namely embeddings.
The most popular technique to debias MNAR data by involving RCTs is the inverse propensity score (IPS) method [30]. Its variants, the self-normalized IPS (SNIPS) [33], doubly robust (DR) [18] and joint learning doubly robust (DRJL) [37] are also widely used. In our experiments, we take 5% of test data to learn the propensity scores via a naive Bayes estimator [30] p(Ou,i = 1|Yu,i = y) = p(y|O = 1)p(O = 1) p(y)
, (19) for IPS, SNIPS, DR, and DRJL. Applying these methods to both shallow and deep backbones, we involve muliple baselines in comparison with our MF-CVIB and NCF-CVIB. Note that only CVIB is
RCT-free among all methods. 4.3 Experimental Protocol
We implement all the methods on PyTorch [26]. For both the MF and NCF, we ﬁx the embedding size of both users and items to be 4 because in our experiments, we ﬁnd when embedding size gets larger, the performance of all methods on the MAR test set decays, which may be caused by overﬁtting.
We randomly draw 30% data from the training set for validation, on which we apply a grid search for hyperparameters to pick the best conﬁguration. Adam [20] is utilized as the optimizer for fast convergence during training, with its learning rate in {0.1, 0.05, 0.01, 0.005, 0.001}, weight decay in
{10−3, 10−4, 10−5}, and batch size in {128, 256, 512, 1024, 2048}. For NCF, we set an additional hidden layer with width 8. Speciﬁcally for CVIB, we set the hyperparameters α ∈ {2, 1, 0.5, 0.1}, and γ ∈ {1, 0.1, 10−2, 10−3}. Since we already set weight decay for Adam, we do not apply the (cid:96)2-norm term on the embeddings. After ﬁnding out the best conﬁguration on the validation set, we evaluate the trained models on the MAR test set. 7
Table 3: Average nDCG with 10 runs on the MAR test set of COAT and YAHOO where the best ones are in bold.
COAT nDCG@5 nDCG@10
YAHOO nDCG@5 nDCG@10
MF 0.589 0.667
MF 0.633 0.762
IPS 0.633 0.689
IPS 0.636 0.760
SNIPS 0.603 0.676
SNIPS 0.635 0.762
DR 0.622 0.693
DR 0.659 0.774
DRJL 0.608 0.679
DRJL 0.652 0.770
CVIB 0.663 0.721
CVIB 0.734 0.820 (a) COAT (γ = 10−3) (b) YAHOO (γ = 10−3) (c) COAT (α = 1) (d) YAHOO (α = 1)
Figure 3: Test results of MF-CVIB with varying α and γ. Shaded regions show the 90% conﬁdence intervals of the test AUC. 4.4 Results
The evaluation results are reported in Table 2. There are three main observations: (1) Even if they utilize additional RCTs, the IPS, SNIPS, DR and DRJL methods sometimes work worse than the naive model. By contrast, we identify that our RCT-free CVIB method is capable of enhancing both the shallow and deep models signiﬁcantly in all experiments. It indicates that the contrastive regularization on the task-aware information, contained in embeddings of factual and counterfactual events, results in improvement of model generalization ability. To further evaluate our method in terms of ranking quality, we report the results of nDCG in Table 3. CVIB shows more signiﬁcant gain over the baselines than on AUC. (2) We perform repeated experiments to quantify our CVIB’s sensitivity to the balancing term weight
α and conﬁdent penalty term weight γ in Eq. (18). From Fig. 3 we identify that α inﬂuences results signiﬁcantly on COAT, while dose not make much difference on YAHOO. In general, increasing
α enhances the test AUC, which is aligned with our argument that contrastive information term beneﬁts in balancing model between factual and counterfactual domains and then leads to better generalization ability. The conﬁdence penalty γ plays less role in accuracy, but we should not set it too high to avoid underﬁtting. (3) One would notice that the performance of NCF-CVIB in terms of MSE is relatively weak, while we argue that good recommendation does not necessarily rely on low MSE prediction. For instance, for the true outcomes y = {1, 0, 0, 0}, the predictions ˆy1 = {0, 0.1, 0.1, 0.1} have much lower MSE than ˆy2 = {0.6, 0.4, 0.4, 0.4}, although the former is obviously a worse prediction than the latter in 8
terms of ranking. It turns out that NCF-CVIB overestimates the outcomes on the test set of YAHOO, nevertheless it still reaches the best ranking quality measured by AUC. In practice of recommender systems, instead of low MSE, we would rather appreciate high ranking quality, namely AUC, which demonstrates the model’s capability of ﬁnding out the positive examples. 5