Abstract
Randomized smoothing has been shown to provide good certiﬁed-robustness guar-antees for high-dimensional classiﬁcation problems. It uses the probabilities of predicting the top two most-likely classes around an input point under a smoothing distribution to generate a certiﬁed radius for a classiﬁer’s prediction. However, most smoothing methods do not give us any information about the conﬁdence with which the underlying classiﬁer (e.g., deep neural network) makes a prediction. In this work, we propose a method to generate certiﬁed radii for the prediction conﬁdence of the smoothed classiﬁer. We consider two notions for quantifying conﬁdence: average prediction score of a class and the margin by which the average prediction score of one class exceeds that of another. We modify the Neyman-Pearson lemma (a key theorem in randomized smoothing) to design a procedure for computing the certiﬁed radius where the conﬁdence is guaranteed to stay above a certain threshold.
Our experimental results on CIFAR-10 and ImageNet datasets show that using information about the distribution of the conﬁdence scores allows us to achieve a signiﬁcantly better certiﬁed radius than ignoring it. Thus, we demonstrate that extra information about the base classiﬁer at the input point can help improve certiﬁed guarantees for the smoothed classiﬁer. Code for the experiments is available at https://github.com/aounon/cdf-smoothing. 1

Introduction
Deep neural networks have been shown to be vulnerable to adversarial attacks, in which a nearly imperceptible perturbation is added to an input image to completely alter the network’s prediction
[37, 31, 12, 21]. Several empirical defenses have been proposed over the years to produce classiﬁers that are robust to such perturbations [20, 4, 16, 8, 30, 14, 11]. However, without robustness guarantees, it is often the case that these defenses are broken by stronger attacks [5, 1, 40, 39]. Certiﬁed defenses, such as those based on convex-relaxation [41, 33, 35, 6, 36] and interval-bound propagation
[13, 17, 9, 32], address this issue by producing robustness guarantees within a neighborhood of an input point. However, due to the complexity of present-day neural networks, these methods have seen limited use in high-dimensional datasets such as ImageNet.
Randomized smoothing has recently emerged as the state-of-the-art technique for certifying adver-sarial robustness with the scalability to handle datasets as large as ImageNet [22, 28, 7, 34]. This defense uses a base classiﬁer, e.g. a deep neural network, to make predictions. Given an input image, a smoothing method queries the top class label at a large number of points in a Gaussian distribution surrounding the image, and returns the label with the majority vote. If the input image is perturbed slightly, the new voting population overlaps greatly with the smoothing distribution around the original image, and so the vote outcome can change only a small amount. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Conventional smoothing throws away a lot of information about class labels, and has limited ca-pabilities that make its outputs difﬁcult to use for decision making. Conventional classiﬁcation networks with a softmax layer output a conﬁdence score that can be interpreted as the degree of certainty the network has about the class label [15]. This is a crucial piece of information in real world decision-making applications such as self-driving cars [3] and disease-diagnosis networks [18], where safety is paramount.
In contrast, standard Gaussian smoothing methods take binary votes at each randomly sampled point – i.e., each point votes either for or against the most likely class, without conveying any information about how conﬁdent the network is in the class label. This may lead to scenarios where a point has a large certiﬁed radius but the underlying classiﬁer has a low conﬁdence score. For example, imagine a 2-way classiﬁer for which a large portion, say 95%, of the sampled points predict the same class. In this case, the certiﬁed radius will be very large (indicating that this image is not an (cid:96)2-bounded adversarial example). However, it could be that each point predicts the top class with very low conﬁdence. In this case, one should have very low conﬁdence in the class label, despite the strength of the adversarial certiﬁcate. A Gaussian smoothing classiﬁer counts a 51% conﬁdence vote exactly the same way as a 99% conﬁdence vote, and this important information is erased.
In this work, we restore conﬁdence information in certiﬁed classiﬁers by proposing a method that produces class labels with a certiﬁed conﬁdence score. Instead of taking a vote at each Gaussian sample around the input point, we average the conﬁdence scores from the underlying base classiﬁer for each class. The prediction of our smoothed classiﬁer is given by the argmax of the expected scores of all the classes. Using the probability distribution of the conﬁdence scores under the Gaussian, we produce a lower bound on how much the expected conﬁdence score of the predicted class can be manipulated by a bounded perturbation to the input image. To do this, we adapt the Neyman-Pearson lemma, the fundamental theorem that characterizes the worst-case behaviour of the classiﬁer under regular (binary) voting, to leverage the distributional information about the conﬁdence scores. The lower bound we obtain is monotonically decreasing with the (cid:96)2-norm of the perturbation and can be expressed as a linear combination of the Gaussian CDF at different points. This allows us to design an efﬁcient binary search based algorithm to compute the radius within which the expected score is guaranteed to be above a given threshold. Our method endows smoothed classiﬁers with the new and important capability of producing conﬁdence scores.
We study two notions of measuring conﬁdence: the average prediction score of a class, and the margin by which the average prediction score of one class exceeds that of another. The average prediction score is the expected value of the activations in the ﬁnal softmax-layer of a neural network under the smoothing distribution. A class is guaranteed to be the predicted class if its average prediction score is greater than one half (since softmax values add up to one) or it maintains a positive margin over all the other classes. For both these measures, along with the bounds described in the previous paragraph, we also derive naive lower bounds on the expected score at a perturbed input point that do not use the distribution of the scores. We perform experiments on CIFAR-10 and ImageNet datasets which show that using information about the distribution of the scores allows us to achieve better certiﬁed guarantees than the naive method.