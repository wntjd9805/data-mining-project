Abstract
Although Q-learning is one of the most successful algorithms for ﬁnding the best action-value function (and thus the optimal policy) in reinforcement learning, its im-plementation often suffers from large overestimation of Q-function values incurred by random sampling. The double Q-learning algorithm proposed in Hasselt (2010) overcomes such an overestimation issue by randomly switching the update between two Q-estimators, and has thus gained signiﬁcant popularity in practice. However, the theoretical understanding of double Q-learning is rather limited. So far only the asymptotic convergence has been established, which does not characterize how fast the algorithm converges. In this paper, we provide the ﬁrst non-asymptotic (i.e., ﬁnite-time) analysis for double Q-learning. We show that both synchronous and asynchronous double Q-learning are guaranteed to converge to an (cid:15)-accurate 1−ω (cid:19) neighborhood of the global optimum by taking ˜Ω (cid:18)(cid:16) (cid:17) 1 (cid:17) 1
+
ω 1 (1−γ)6(cid:15)2 (cid:16) 1 1−γ iterations, where ω ∈ (0, 1) is the decay parameter of the learning rate, and γ is the discount factor. Our analysis develops novel techniques to derive ﬁnite-time bounds on the difference between two inter-connected stochastic processes, which is new to the literature of stochastic approximation. 1

Introduction
Q-learning is one of the most successful classes of reinforcement learning (RL) algorithms, which aims at ﬁnding the optimal action-value function or Q-function (and thus the associated optimal policy) via off-policy data samples. The Q-learning algorithm was ﬁrst proposed by Watkins and
Dayan (1992), and since then, it has been widely used in various applications including robotics (Tai and Liu, 2016), autonomous driving (Okuyama et al., 2018), video games (Mnih et al., 2015), to name a few. Theoretical performance of Q-learning has also been intensively explored. The asymptotic convergence has been established in Tsitsiklis (1994); Jaakkola et al. (1994); Borkar and Meyn (2000); Melo (2001); Lee and He (2019). The non-asymptotic (i.e., ﬁnite-time) convergence rate of Q-learning was ﬁrstly obtained in Szepesvári (1998), and has been further studied in (Even-Dar and Mansour, 2003; Shah and Xie, 2018; Wainwright, 2019; Beck and Srikant, 2012; Chen et al., 2020) for synchronous Q-learning and in (Even-Dar and Mansour, 2003; Qu and Wierman, 2020) for asynchoronous Q-learning.
One major weakness of Q-learning arises in practice due to the large overestimation of the action-value function (Hasselt, 2010; Hasselt et al., 2016). Practical implementation of Q-learning involves using the maximum sampled Q-function to estimate the maximum expected Q-function (where the
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
expectation is taken over the randomness of reward). Such an estimation often yields a large positive bias error (Hasselt, 2010), and causes Q-learning to perform rather poorly. To address this issue, double Q-learning was proposed in Hasselt (2010), which keeps two Q-estimators (i.e., estimators for Q-functions), one for estimating the maximum Q-function value and the other one for update, and continuously changes the roles of the two Q-estimators in a random manner. It was shown in
Hasselt (2010) that such an algorithm effectively overcomes the overestimation issue of the vanilla
Q-learning. In Hasselt et al. (2016), double Q-learning was further demonstrated to substantially improve the performance of Q-learning with deep neural networks (DQNs) for playing Atari 2600 games. It inspired many variants (Zhang et al., 2017; Abed-alguni and Ottom, 2018), received a lot of applications (Zhang et al., 2018a,b), and have become one of the most common techniques for applying Q-learning type of algorithms (Hessel et al., 2018).
Despite its tremendous empirical success and popularity in practice, theoretical understanding of double Q-learning is rather limited. Only the asymptotic convergence was provided in Hasselt (2010); Weng et al. (2020c). There has been no non-asymptotic result on how fast double Q-learning converges. From the technical standpoint, such ﬁnite-time analysis for double Q-learning does not follow readily from those for the vanilla Q-learning, because it involves two randomly updated
Q-estimators, and the coupling between these two random paths signiﬁcantly complicates the analysis.
This goes much more beyond the existing techniques for analyzing the vanilla Q-learning that handles the random update of a single Q-estimator. Thus, the goal of this paper is to develop new ﬁnite-time analysis techniques that handle the inter-connected two random path updates in double Q-learning and provide the convergence rate. 1.1 Our contributions
The main contribution of this paper lies in providing the ﬁrst ﬁnite-time analysis for double Q-learning with both the synchronous and asynchronous implementations.
• We show that synchronous double Q-learning with a learning rate αt = 1/tω (where ω ∈ (0, 1)) attains an (cid:15)-accurate global optimum with at least the probability of 1 − δ by taking (cid:18)(cid:16)
Ω 1 (1−γ)6(cid:15)2 ln |S||A| (1−γ)7(cid:15)2δ (cid:17) 1
ω (cid:16) 1 1−γ ln
+ 1 (1−γ)2(cid:15) 1−ω (cid:19) (cid:17) 1 iterations, where γ ∈ (0, 1) is the discount factor, |S| and |A| are the sizes of the state space and action space, respectively.
• We further show that under the same accuracy and high probability requirements, asynchronous double Q-learning takes Ω 1−γ ln where L is the covering number speciﬁed by the exploration strategy. (1−γ)7(cid:15)2δ (1−γ)6(cid:15)2 ln |S||A|L4
+
L4
ω 1 (1−γ)2(cid:15) (cid:17) 1 (cid:16) L2 (cid:18)(cid:16) 1−ω (cid:19) (cid:17) 1 iterations,
Our results corroborate the design goal of double Q-learning, which opts for better accuracy by making less aggressive progress during the execution in order to avoid overestimation. Speciﬁcally, our results imply that in the high accuracy regime, double Q-learning achieves the same convergence rate as vanilla Q-learning in terms of the order-level dependence on (cid:15), which further indicates that the high accuracy design of double Q-learning dominates the less aggressive progress in such a regime. In the low-accuracy regime, which is not what double Q-learning is designed for, the cautious progress of double Q-learning yields a slightly weaker convergence rate than Q-learning in terms of the dependence on 1 − γ.
From the technical standpoint, our proof develops new techniques beyond the existing ﬁnite-time analysis of the vanilla Q-learning with a single random iteration path. More speciﬁcally, we model the double Q-learning algorithm as two alternating stochastic approximation (SA) problems, where one SA captures the error propagation between the two Q-estimators, and the other captures the error dynamics between the Q-estimator and the global optimum. For the ﬁrst SA, we develop new techniques to provide the ﬁnite-time bounds on the two inter-related stochastic iterations of
Q-functions. Then we develop new tools to bound the convergence of Bernoulli-controlled stochastic iterations of the second SA conditioned on the ﬁrst SA. 1.2