Abstract
Visual and audio modalities are highly correlated, yet they contain different infor-mation. Their strong correlation makes it possible to predict the semantics of one from the other with good accuracy. Their intrinsic differences make cross-modal prediction a potentially more rewarding pretext task for self-supervised learning of video and audio representations compared to within-modality learning. Based on this intuition, we propose Cross-Modal Deep Clustering (XDC), a novel self-supervised method that leverages unsupervised clustering in one modality (e.g., audio) as a supervisory signal for the other modality (e.g., video). This cross-modal supervision helps XDC utilize the semantic correlation and the differences between the two modalities. Our experiments show that XDC outperforms single-modality clustering and other multi-modal variants. XDC achieves state-of-the-art accuracy among self-supervised methods on multiple video and audio benchmarks. Most importantly, our video model pretrained on large-scale unlabeled data signiﬁcantly outperforms the same model pretrained with full-supervision on ImageNet and
Kinetics for action recognition on HMDB51 and UCF101. To the best of our knowl-edge, XDC is the ﬁrst self-supervised learning method that outperforms large-scale fully-supervised pretraining for action recognition on the same architecture. 1

Introduction
Do we need to explicitly name the actions of “laughing” or “sneezing” in order to recognize them? Or can we learn to visually classify them without labels by associating characteristic sounds with these actions? Indeed, a wide literature in perceptual studies provides evidence that we rely heavily on hearing sounds to make sense of actions and dynamic events in the visual world. For example, objects moving together are perceived as bouncing off each other when the visual stimulus is accompanied by a brief sound [55], and the location and timing of sounds are leveraged as important cues to direct our spatiotemporal visual attention [19, 42]. The inﬂuence of hearing sounds in visual perception is also suggested by perceptual studies showing that individuals affected by profound deafness exhibit poorer visual perceptual performance compared to age-matched hearing controls [11, 39].
In this work, we investigate the hypothesis that spatiotemporal models for action recognition can be reliably pretrained from unlabeled videos by capturing cross-modal information from audio and video. The motivation for our study stems from two fundamental challenges facing a fully-supervised
∗Work done during an internship at Facebook AI 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
line of attack to learning video models. The ﬁrst challenge is the exorbitant cost of scaling up the size of manually-labeled video datasets. The recent creation of large-scale action recognition datasets [5, 15, 25, 26] has undoubtedly enabled a major leap forward in video models accuracies.
However, it may be argued that additional signiﬁcant gains by dataset growth would require scaling up existing labeled datasets by several orders of magnitude. The second challenge is posed by the unclear deﬁnition of suitable label spaces for action recognition. Recent video datasets differ substantially in their label spaces, which range from sports actions [25] to verb-noun pairs for kitchen activities [7].
This suggests that the deﬁnition of the “right” label space for action recognition, and more generally for video understanding, is still very much up for debate. It also implies that ﬁnetuning models pretrained on large-scale labeled datasets is a suboptimal proxy for learning models for small- or medium-size datasets due to the label-space gap often encountered between source and target datasets.
In this paper, we present three approaches for training video models from self-supervised audio-visual information. At a high-level, the idea behind all three frameworks is to leverage one modality (say, audio) as a supervisory signal for the other (say, video). We posit that this is a promising avenue because of the simultaneous synergy and complementarity of audio and video: correlations between these two modalities make it possible to perform prediction from one to the other, while their intrinsic differences make cross-modal prediction an enriching self-supervised task compared to within-modality learning. Speciﬁcally, we adapt the single-modality DeepCluster work of Caron et al. [6] to our multi-modal setting. DeepCluster was introduced as a self-supervised procedure for learning image representation. It alternates between unsupervised clustering of image features and using these cluster assignments as pseudo-labels to revise the image representation. In our work, the clusters learned from one modality are used as pseudo-labels to reﬁne the representation for the other modality. In two of our approaches—Multi-Head Deep Clustering (MDC) and Concatenation
Deep Clustering (CDC)—the pseudo-labels from the second modality are supplementary, i.e., they complement the pseudo-labels generated in the ﬁrst modality. The third approach—Cross-Modal
Deep Clustering (XDC)—instead uses the pseudo-labels from the other modality as an exclusive supervisory signal. This means that in XDC, the audio clusters drive the learning of the video representation and vice versa. Our experiments support several interesting conclusions:
• All three of our cross-modal methods yield representations that generalize better to the downstream tasks of action recognition and audio classiﬁcation, compared to their within-modality counterparts.
• XDC (i.e., the cross-modal deep clustering relying on the other modality as an exclusive supervi-sory signal) outperforms all the other approaches. This underscores the complementarity of audio and video and the beneﬁts of learning label-spaces across modalities.
• Self-supervised cross-modal learning with XDC on a large-scale video dataset yields an action recognition model that achieves higher accuracy when ﬁnetuned on HMDB51 or UCF101, com-pared to that produced by fully-supervised pretraining on Kinetics. To the best of our knowledge, this is the ﬁrst method to demonstrate that self-supervised video representation learning outper-forms large-scale fully-supervised pretraining for action recognition. Moreover, unlike previous self-supervised methods that are only pretrained on curated data (e.g., Kinetics [26] without action labels), we also report results of XDC pretrained on a large-scale uncurated video dataset. 2