Abstract
Neural architecture search (NAS) enables researchers to automatically explore broad design spaces in order to improve efﬁciency of neural networks. This efﬁciency is especially important in the case of on-device deployment, where im-provements in accuracy should be balanced out with computational demands of a model. In practice, performance metrics of model are computationally expensive to obtain. Previous work uses a proxy (e.g., number of operations) or a layer-wise measurement of neural network layers to estimate end-to-end hardware perfor-mance but the imprecise prediction diminishes the quality of NAS. To address this problem, we propose BRP-NAS, an efﬁcient hardware-aware NAS enabled by an accurate performance predictor-based on graph convolutional network (GCN).
What is more, we investigate prediction quality on different metrics and show that sample efﬁciency of the predictor-based NAS can be improved by considering binary relations of models and an iterative data selection strategy. We show that our proposed method outperforms all prior methods on NAS-Bench-101 and NAS-Bench-201, and that our predictor can consistently learn to extract useful features from the DARTS search space, improving upon the second-order baseline. Finally, to raise awareness of the fact that accurate latency estimation is not a trivial task, we release LatBench – a latency dataset of NAS-Bench-201 models running on a broad range of devices.

Introduction 1
Neural architecture search (NAS) has demonstrated great success in automatically designing com-petitive neural networks compared with hand-crafted alternatives [1, 2, 3, 4]. However, NAS is computationally expensive requiring to train models [5, 6] or introduce non-trivial complexity into the search process [7, 8]. Additionally, real-world deployment demands models meeting efﬁciency or hardware constraints (e.g., latency, memory and energy consumption) on top of being accurate, but acquiring various performance metrics of a model can also be time consuming, independently from the cost of training it.
In this paper, we (a) show the limitations of the layer-wise predictor both in terms of prediction accuracy and NAS performance and (b) propose a Graph convolutional networks (GCN)-based predictor for the end-to-end latency which signiﬁcantly outperforms the layer-wise approach on devices of various speciﬁcations.
One of the key challenges in obtaining a reliable accuracy predictor is that acquiring training samples (pairs of (model, accuracy)) is computationally expensive. Sample efﬁciency denotes how many samples are required to ﬁnd the best model during a search under a target hardware constraint, and 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
signiﬁcantly advancing this metric is a key contribution of our work. We propose several methods to improve sample efﬁciency – (a) We observe that in the context of NAS, instead of getting precise estimates of accuracy, we want to produce a linear ordering of accuracy to search for the best model.
Therefore, we propose a binary relation predictor to decide the accuracy ranking of neural networks without requiring to estimate absolute accuracy values. (b) To help the predictor focus on predicting the rankings of top candidates, which is the most important to yield the best results in NAS, we propose an iterative data selection scheme which vastly improves the sample efﬁciency of NAS.
The contributions of this paper are summarized as follows:
• Latency prediction. We empirically show that an accurate latency predictor plays an important role in NAS where latency on the target hardware is of interest, and existing latency predictors are overly error-prone. We propose an end-to-end NAS latency predictor-based on a GCN and show that it outperforms previous approaches (proxy, layer-wise) on various devices. To the best of our knowledge, this is the ﬁrst end-to-end latency predictor.
We illustrate its behaviour on various devices and show that this predictor works well across all of them (Section 3).
• Accuracy prediction. We introduce a novel training methodology for a NAS-speciﬁc accuracy predictor by turning an accuracy prediction problem into a binary prediction prob-lem, where we predict which one of two neural architectures performs better, resulting in improved overall ranking correlation between predicted and ground-truth rankings. Further-more, we propose a new prediction-based NAS framework called BRP-NAS. It combines a binary relation accuracy predictor architecture and an iterative data selection strategy to improve the top-K ranking correlation. BRP-NAS outperforms previous NAS methods by being more sample efﬁcient (Section 4 and 5).
• Towards reproducible research: latency benchmark. We introduce LatBench, the ﬁrst large-scale latency measurement dataset for multi-objective NAS. Unlike existing datasets which either approximate the latency or focus on a single device, LatBench provides measurement dataset on a broad range of systems covering desktop CPU/GPU, embedded
GPU/TPU and mobile GPU/DSP. We also release Eagle which is a tool to measure and predict performance of models on various systems. We make LatBench and the source code of Eagle available publicly1 (Section 6). 2