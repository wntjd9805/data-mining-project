Abstract
We propose self-adaptive training—a new training algorithm that dynamically calibrates training process by model predictions without incurring extra computa-tional cost—to improve generalization of deep learning for potentially corrupted training data. This problem is important to robustly learning from data that are corrupted by, e.g., random noise and adversarial examples. The standard empir-ical risk minimization (ERM) for such data, however, may easily overﬁt noise and thus suffers from sub-optimal performance. In this paper, we observe that model predictions can substantially beneﬁt the training process: self-adaptive training signiﬁcantly mitigates the overﬁtting issue and improves generalization over ERM under both random and adversarial noise. Besides, in sharp contrast to the recently-discovered double-descent phenomenon in ERM, self-adaptive training exhibits a single-descent error-capacity curve, indicating that such a phe-nomenon might be a result of overﬁtting of noise. Experiments on the CIFAR and
ImageNet datasets verify the effectiveness of our approach in two applications: classiﬁcation with label noise and selective classiﬁcation. The code is available at https://github.com/LayneH/self-adaptive-training. 1

Introduction
Empirical Risk Minimization (ERM) has received signiﬁcant attention due to its impressive general-ization in various ﬁelds [36, 16]. However, recent works [46, 23] cast doubt on the traditional views on ERM: techniques such as uniform convergence might be unable to explain the generalization of deep neural networks, because ERM easily overﬁts training data even though the training data are partially or completely corrupted by random noise.
To take a closer look at this phenomenon, we evaluate the generalization of ERM on the CIFAR10 dataset [18] with 40% of data being corrupted at random (see Section 2 for details). Figure 1a displays the accuracy curves of ERM that are trained on the noisy training sets under four kinds of random corruptions: ERM easily overﬁts noisy training data and achieves nearly perfect training accuracy. However, the four subﬁgures exhibit very different generalization behaviors which are indistinguishable by the accuracy curve on the noisy training set on its own.
Despite a large literature devoted to analyzing the phenomenon either in the theoretical or empirical manners, many fundamental questions remain unresolved. To name a few, the work of [46] showed that early stopping can improve generalization. On the theoretical front, the work of [19] considered the label corruption setting, and proved that the ﬁrst few training iterations ﬁts the correct labels and overﬁtting only occurs in the last few iterations: in Figure 1a, the accuracy increases in the early stage and the generalization errors grow quickly after certain epochs. Admittedly, stopping at early epoch improves generalization in the presence of label noise (see the ﬁrst column in Figure 1a); however, it
∗Corresponding authors 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Accuracy curves of model trained by ERM. (b) Accuracy curves of model trained by our method.
Figure 1: Accuracy curves of model trained on noisy CIFAR10 training set (corresponding to the red dashed curve) with 40% corrupted data. The horizontal dotted line displays the percentage of clean data in the training sets. remains unclear how to properly identify such an epoch. Moreover, the early-stop mechanism may signiﬁcantly hurt the performance on the clean validation sets, as we can see in the second to the fourth columns of Figure 1a.
Our work is motivated by this fact and goes beyond ERM. We begin by making the following observations in the leftmost subﬁgure of Figure 1a: the peak of accuracy curve on the clean training set (80%) is much higher than the percentage of clean data in the noisy training set (60%). This
ﬁnding was also previously reported by [33, 15, 19] under label corruption and suggested that model predictions might be able to magnify useful underlying information in data. We conﬁrm this ﬁnding and show that the pattern occurs under various kinds of corruptions more broadly (see
Figure 1a). We thus propose self-adaptive training, a carefully designed approach which dynamically uses model predictions as a guiding principle in the design of training algorithm. Figure 1b shows that our approach signiﬁcantly alleviates the overﬁtting issue on the noisy training set, reduces the generalization error on the corrupted distributions, and improves the performance on the clean data. 1.1 Summary of our contributions
Our work sheds light on understanding generalization of deep neural networks under noise.
• We analyze the standard ERM training process of deep networks on four kinds of corruptions (see Figure 1a). We describe the failure scenarios of ERM and observe that useful information for classiﬁcation has been distilled to model predictions in the ﬁrst few epochs. This observation motivates us to propose self-adaptive training for robustly learning under noise.
• We show that self-adaptive training improves generalization under both label-wise and instance-wise random noise (see Figures 1 and 2). Besides, self-adaptive training exhibits a single-descent error-capacity curve (see Figure 3). This is in sharp contrast to the recently-discovered double-descent phenomenon in ERM which might be a result of overﬁtting of noise.
• While adversarial training may easily overﬁt adversarial noise, our approach mitigates the overﬁt-ting issue and improves adversarial accuracy by ∼3% over the state-of-the-art (see Figure 4).
Our approach has two applications and advances the state-of-the-art by a signiﬁcant gap.
• Classiﬁcation with label noise, where the goal is to improve the performance of deep networks on clean test data in the presence of training label noise. On the CIFAR datasets, our approach achieves up to 9.3% absolute improvement on the classiﬁcation accuracy over the state-of-the-art.
On the ImageNet dataset, our approach improves over ERM by 2% under 40% noise rate.
• Selective classiﬁcation, which aims to trade prediction coverage off against classiﬁcation accuracy.
Our approach achieves up to 50% relative improvement over the state-of-the-art on two datasets. 2
Differences between our methodology and existing works on robust learning Self-adaptive training consists of two components: a) the moving-average scheme that progressively corrects problematic labels using model predictions; b) the re-weighting scheme that dynamically puts less weights on the erroneous data. With the two components, our algorithm is robust to both instance-wise and label-wise noise, and is ready to combine with various training schemes such as natural and adversarial training, without incurring multiple rounds of training. In contrast, a vast majority of works on learning from corrupted data follow a preprocessing-training fashion with an emphasis on the label-wise noise only: this line of research either discards samples based on disagreement between noisy labels and model predictions [7, 6, 50, 25], or corrects noisy labels [4, 40]; [41] investigated a more generic approach that corrects both label-wise and instance-wise noise. However, their approach inherently suffers from extra computational overhead. Besides, unlike the general scheme in robust statistics [34] and other re-weighting methods [17, 32] that use an additional optimization step to update the sample weights, our approach directly obtains the weights based on accumulated model predictions and thus is much more efﬁcient. 2
Improved Generalization of Deep Networks 2.1 Preliminary
In this section, we conduct the experiments on the CIFAR10 dataset [18], of which we split the original training data into a training set (consists of ﬁrst 45,000 data pairs) and a validation set (consists of last 5,000 data pairs). We consider four random noise schemes according to [46], where the data are partially corrupted with probability p: 1) Corrupted labels. Labels are assigned uniformly at random; 2) Gaussian. Images are replaced by random Gaussian samples with the same mean and standard deviation as the original image distribution; 3) Random pixels. Pixels of each image are shufﬂed using independent random permutations; 4) Shufﬂed pixels. Pixels of each image are shufﬂed using a ﬁxed permutation pattern. We consider the performance on both the noisy and the clean sets (i.e., the original uncorrupted data), while the models can only have access to the noisy training sets.
Notations We consider c-class classiﬁcation problem and denote the images by xi ∈ Rd, labels by (cid:124) yi ∈ {0, 1}c, y i 1 = 1. The images xi or labels yi might be corrupted by one of the four schemes we have described. We denote the logits of the classiﬁer (e.g., parameterized by a deep network) by f (·). 2.2 Our approach: Self-Adaptive Training
To alleviate the overﬁtting issue of ERM in Figure 1a, we present our approach to improve the generalization of deep networks on the corrupted data.
The blessing of model predictions As a warm-up algorithm, a straight-forward way to incorporate model predictions into the training process is to use a convex combination of labels and predictions as the training targets. Concretely, given data pair (xi, yi) and prediction pi = softmax(f (xi)), (cid:124) we consider the training target ti = α × yi + (1 − α) × pi, where ti ∈ [0, 1]c, t i 1 = 1. We then minimize the cross entropy loss between pi and ti to update the classiﬁer f in each training iteration.
However, this naive algorithm suffers from multiple drawbacks: 1) model predictions are inaccurate in the early stage of training, and may be unstable in the presence of regularization such as data augmentation. This leads to instability of ti; 2) this scheme can assign at most 1 − α weight on the true class when yi is wrong. However, we aim to correct the erroneous labeling. In other words, we expect to assign nearly 100% weight on the true class.
To overcome the drawbacks, we use the accumulated predictions to augment the training dynamics.
Formally, we initialize ti ← yi, ﬁx ti in the ﬁrst Es training epochs, and update ti ← α × ti + (1 − α) × pi in each following training epoch. The exponential-moving-average scheme alleviates the instability issue of model predictions, smooths out ti during the training process and enables our algorithm to completely change the training labels if necessary. Momentum term α controls the weight on the model predictions. The number of initial epochs Es allows the model to capture informative signals in the data set and excludes ambiguous information that is provided by model predictions in the early stage of training.
Sample re-weighting Based on the scheme presented above, we introduce a simple yet effective sample re-weighting scheme on each sample. Concretely, given training target ti, we set wi = maxj ti,j. The sample weight wi ∈ [ 1 c , 1] reveals the labeling conﬁdence of this sample. Intuitively, 3
Figure 2: Generalization error and clean validation error under four kinds of random noise (represented by different colors) for ERM (the dashed curves) and our approach (the solid curves) on CIFAR10.
We zoom-in the dashed rectangle region and display it in the third column for clear demonstration. all samples are treated equally in the ﬁrst Es epochs. As target ti being updated, our algorithm pays less attention to potentially erroneous data and learns more from potentially clean data. This scheme also allows the corrupted samples to re-attain attention if they are conﬁdently corrected.
Putting everything together We use stochastic gradient descent to minimize:
L(f ) = − 1 (cid:80) i wi (cid:88) wi (cid:88) i j ti,j log pi,j (1) during the training process. Here, the denominator normalizes per sample weights and stabilizes the loss scale. We name our approach Self-Adaptive Training and display the pseudocode in Algorithm 1.
We ﬁx the hyper-parameters Es = 60, α = 0.9 by default if not speciﬁed. Our approach requires no modiﬁcation to existing network architecture and incurs almost no extra computational cost.
Algorithm 1 Self-Adaptive Training
Fetch mini-batch data {(xi, ti)}m at current epoch e for i = 1 to m (in parallel) do pi = softmax(f (xi)) if e > Es then ti = α × ti + (1 − α) × pi wi = maxj ti,j
Require: Data {(xi, yi)}n, initial targets {ti}n = {yi}n, batch size m, classiﬁer f , Es = 60, α = 0.9 1: repeat 2: 3: 4: 5: 6: 7: 8: 9: until end of training end for
Update f by SGD on L(f ) = − 1 (cid:80) j ti,j log pi,j i wi i wi (cid:80) (cid:80) 2.3
Improved generalization of self-adaptive training under random noise
We consider noise scheme (including noise type and noise level) and model capacity as two factors that affect the generalization of deep networks under random noise. We analyze self-adaptive training by varying one of the two factors while ﬁxing the other.
Varying noise schemes We use ResNet-34 [16] and rerun the same experiments in Figure 1a by replacing ERM with our approach. In Figure 1b, we plot the accuracy curves of models trained with our approach on four corrupted training sets and compare with Figure 1a. We highlight the following observations.
• Our approach mitigates the overﬁtting issue in deep networks. The accuracy curves on noisy training sets (i.e., the red dashed curves in Figure 1b) nearly converge to the percentage of clean data in the training sets, and do not reach perfect accuracy.
• The generalization errors of self-adaptive training (the gap between the red and blue dashed curves in Figure 1b) are much smaller than Figure 1a. We further conﬁrm this observation by displaying the generalization errors of the models trained on the four noisy training sets under various noise rates in the leftmost subﬁgure of Figure 2. Generalization errors of ERM consistently grow as we increase the injected noise level. In contrast, our approach signiﬁcantly reduces the generalization errors across all noise levels from 0% (no noise) to 90% (overwhelming noise).
• The accuracy on the clean sets (cyan and yellow solid curves in Figure 1b) is monotonously increasing and converges to higher values than their correspondence in Figure 1a. We also show 4
Figure 3: Double-descent ERM vs. single-descent self-adaptive training on the error-capacity curve. The model of width 64 corresponds to standard ResNet-18. The vertical dashed line represents the interpolation threshold [5, 24].
Figure 4: Robust Accuracy (%) on CI-FAR10 test set under white box (cid:96)∞ PGD-20 attack ((cid:15)=0.031). The vertical dashed lines indicate learning rate decay. the clean validation errors in the right two subﬁgures in Figure 2. The ﬁgures show that the error of self-adaptive training is consistently much smaller than that of ERM.
Varying model capacity We notice that such analysis is related to a recent-discovered intriguing phenomenon [5, 24] in modern machine learning models: as the capacity of model increases, the test error initially decreases, then increases, and ﬁnally shows a second descent. This phenomenon is termed double descent [5] and has been widely observed in deep networks [24]. To evaluate the double-descent phenomenon on self-adaptive training, we follow exactly the same experimental settings as
[24]: we vary the width parameter of ResNet-18 [16] and train the networks on the CIFAR10 dataset with 15% training label being corrupted at random (details are given in Appendix A.1). Figure 3 shows the curves of test error. It shows that self-adaptive training overall achieves much lower test error than that of ERM in most cases. Besides, we observe that the curve of ERM clearly exhibits the double-descent phenomenon, while the curve of our approach is monotonously decreasing as the model capacity increases. Since the double-descent phenomenon may vanish when label noise is absent [24], our experiment indicates that this phenomenon may be a result of overﬁtting of noise and we can bypass it by a proper design of training process such as the self-adaptive training.
Potential failure scenarios We notice that self-adaptive training could perform worse than ERM when using extremely small models that underﬁt the training data. Under such cases, the models do not have enough capacity to capture sufﬁcient information, incorporating their ambiguous prediction may even hinder the training dynamics. However, as shown in Figure 3, the ERM can only outperform our self-adaptive training in some extreme cases that the models are 10× smaller than the standard
ResNet-18, indicating that our method can work well in most realistic settings. 2.4
Improved generalization of self-adaptive training under adversarial noise
Adversarial noise [39] is different from the random noise in that the noise is model-dependent and imperceptible to humans. We use the state-of-the-art adversarial training algorithm TRADES
[48] as our baseline to evaluate the performance of self-adaptive training under adversarial noise.
Algorithmally, TRADES minimizes (cid:40) (cid:41)
Ex,y
CE(p(x), y) + max (cid:107)(cid:101)x−x(cid:107)∞≤(cid:15)
KL(p(x), p((cid:101)x))/λ
, (2) where p(·) is the model prediction, (cid:15) is the maximal allowed perturbation, CE stands for the cross entropy, KL stands for the Kullback–Leibler divergence, and the hyper-parameter λ controls the trade-off between robustness and accuracy. We replace the CE term in TRADES loss with our method. 1{argmax p((cid:101)xi) = argmax yi}, where
The models are evaluated using robust accuracy 1 n adversarial example (cid:101)x are generated by white box (cid:96)∞ projected gradient descent (PGD) attack [22] with (cid:15) = 0.031, perturbation steps of 20. We set the initial learning rate as 0.1 and decay it by a factor of 0.1 in epochs 75 and 90, respectively. We choose 1/λ = 6.0 as suggested by [48] and use Es = 70,
α = 0.9 for our approach. Experimental details are given in Appendix A.2. (cid:80) i
We display the robust accuracy on CIFAR10 test set after Es = 70 epochs in Figure 4. It shows that the robust accuracy of TRADES reaches its highest value around the epoch of ﬁrst learning rate decay (epoch 75) and decreases later, which suggests that overﬁtting might happen if we train the model without early stopping. On the other hand, self-adaptive training considerably mitigates the overﬁtting issue in the adversarial training and consistently improves the robust accuracy of TRADES 5
Table 1: Test Accuracy (%) on CIFAR datasets with various levels of uniform label noise injected to training set. We compare with previous works under exactly the same experiment settings. It shows that in most settings, self-adaptive training improves over the state-of-the-art as signiﬁcant as 9%.
Backbone
ResNet-34
WRN28-10
Label Noise Rate
ERM + Early Stopping
Label Smoothing [38]
Forward ˆT [29]
Mixup [47]
Trunc Lq [49]
Joint Opt [40]
SCE [43]
DAC [42]
SELF [25]
Ours
ERM + Early Stopping
MentorNet [17]
DAC [42]
SELF [25]
Ours 0.2 85.57 85.64 87.99 93.58 89.70 92.25 90.15 92.91
-94.14 87.86 92.0 93.25
-94.84
CIFAR10 0.6 0.4 0.8 0.2
CIFAR100 0.6 0.4 81.82 71.59 83.25 89.46 87.62 90.79 86.74 90.71 91.13 92.64 83.40 89.0 90.93 93.34 93.23 76.43 50.51 74.96 78.32 82.70 86.87 80.80 86.30
-89.23 76.92
-87.58
-89.42 60.99 28.19 54.64 66.32 67.92 69.16 46.28 74.84 63.59 78.58 63.54 49.0 70.80 67.41 80.13 63.70 67.44 39.19 69.31 67.61 58.15 71.26 73.55
-75.77 68.46 73.0 75.75
-77.71 48.60 53.84 31.05 58.12 62.64 54.81 66.41 66.92 66.71 71.38 55.43 68.0 68.20 72.48 72.60 37.86 33.01 19.12 41.10 54.04 47.94 57.43 57.17
-62.69 40.78
-59.44
-64.87 0.8 17.28 9.74 8.99 18.77 29.60 17.18 26.41 32.16 35.56 38.72 20.25 35.0 34.06 42.06 44.17 by 1%∼3%, which indicates that our method can improve the generalization in the presence of adversarial noise. 3 Application I: Classiﬁcation with Label Noise
Given improved generalization of self-adaptive training over ERM under noise, we provide applica-tions of our approach which outperforms the state-of-the-art with a signiﬁcant gap. 3.1 Problem formulation
Given a set of noisy training data {(xi, (cid:101)yi)}n ∈ (cid:101)D, where (cid:101)D is the distribution of noisy data and (cid:101)yi is the noisy label for each uncorrupted sample xi, the goal is to be robust to the label noise in the training data and improve the classiﬁcation performance on clean test data that are sampled from clean distribution D. 3.2 Experiments on CIFAR datasets
Setup We consider the case that the labels are assigned uniformly at random with different noise rates. Following prior works [49, 42], we conduct the experiments on the CIFAR10 and CIFAR100 datasets [18] using ResNet-34 [16] and Wide ResNet 28 [45] as our base classiﬁers. The networks are implemented on PyTorch [28] and optimized using SGD with initial learning rate of 0.1, momentum of 0.9, weight decay of 0.0005, batch size of 256, total training epochs of 200. The learning rate is decayed to zero using cosine annealing schedule [21]. We use data augmentation of random horizontal ﬂipping and cropping. We report the average performance over 3 trials.
Main results We summarize the experiments in Table 1. Most of the results are cited from original papers when they are under the same experiment settings; the results of Label Smoothing [38],
Mixup [47], Joint Opt [40] and SCE [43] are reproduced by rerunning the ofﬁcial open-sourced implementations. From the table, we can see that our approach outperforms the state-of-the-art methods in most entries by 1% ∼ 9% on both CIFAR10 and CIFAR100 datasets. Notably, unlike
Joint Opt, DAC and SELF methods that require multiple iterations of training, our method enjoys the same computational budget as ERM.
Ablation study and parameter sensitivity First, we report the performance of ERM equipped with simple early stopping scheme in the ﬁrst row of Table 1. We observe that our approach achieves substantial improvements over this baseline. This demonstrates that simply early stopping the 6
Table 2: Ablation study on CIFAR datasets in terms of classiﬁcation Accuracy (%). (a) Inﬂuence of the two components of our approach. (b) Parameters sensitivity when label noise of 40% is injected to CIFAR10 training set.
Noise Rate
CIFAR10 0.8 0.4
CIFAR100 0.8 0.4 92.64 78.58 71.38 38.72
Ours
- Re-weighting 92.49 78.10 69.52 36.78
- Moving Average 72.00 28.17 50.93 11.57 0.6 0.8
α 0.99
Fix Es=60 90.17 91.91 92.64 92.54 84.38
Es 100 60
Fix α=0.9 89.58 91.89 92.64 92.26 88.83 0.95 0.9 40 80 20 training process is a sub-optimal solution. Then, we further report the inﬂuences of two individual components of our approach: Exponential Moving Average (EMA) and sample re-weighting scheme.
As displayed in Table 2a, removing any component considerably hurts the performance under all noise rates and removing EMA scheme leads to a signiﬁcant performance drop. This suggests that properly incorporating model predictions is important in our approach. Finally, we analyze the sensitivity of our approach to the parameters α and Es in Table 2b (and also Table 5 of Appendix).
The performance is stable for various choices of α and Es, indicating that our approach is insensitive to the hyper-parameter tuning. 3.3 Experiments on ImageNet dataset
The work of [35] suggested that ImageNet dataset [8] contains anno-tation errors even after several rounds of cleaning. Therefore, in this subsection, we use ResNet-50 [16] to evaluate self-adaptive training on the ImageNet under both standard setup (i.e., using original labels) and the case that 40% training labels are corrupted. We provide the exper-imental details in Appendix A.3 and report model performance on the
ImageNet validation set in terms of top1 accuracy in Table 3. We see that self-adaptive training consistently improves the ERM baseline by a considerable margin (e.g., 2% when 40% labels are corrupted), which validates the effectiveness of our approach on large-scale dataset. 3.4 Further inspection on self-adaptive training
Table 3: Top1 Accuracy (%) on ImageNet validation set.
Noise Rate 0.0
ERM
Ours 76.8 77.2 0.4 69.5 71.5
Label recovery We demonstrate that our approach is able to recover the true labels from noisy training labels: we obtain the recovered labels by the moving average targets ti and compute the 1{argmax yi = argmax ti}, where yi is the clean label of each recovered accuracy as 1 n training sample. When 40% label are corrupted in the CIFAR10 and ImageNet training set, our approach successfully corrects a huge amount of labels and obtains recovered accuracy of 94.6% and 81.1%, respectively. We also display the confusion matrix of recovered labels w.r.t the clean labels on CIFAR10 in Figure 5, from which we see that our approach performs impressively well for all classes. (cid:80) i
Sample weights Following the same procedure, we display the average sample weights in Figure 6.
In the ﬁgure, the (i, j)-th block contains the average weight of samples with clean label i and recovered label j, the white areas represent the case that no sample lies in the cell. We see that the weights on the diagonal blocks are clearly higher than those on non-diagonal blocks. The ﬁgure indicates that, aside from impressive ability to recover the correct labels, self-adaptive training could properly down-weight the noisy examples. 4 Application II: Selective Classiﬁcation 4.1 Problem formulation
Selective classiﬁcation, a.k.a. classiﬁcation with rejection, trades classiﬁer coverage off against accuracy [10], where the coverage is deﬁned as the fraction of classiﬁed samples in the dataset; the classiﬁer is allowed to output “don’t know” for certain samples. The task focuses on noise-free setting and allows classiﬁer to abstain on potential out-of-distribution samples or samples lies in the tail of 7
Figure 5: Confusion matrix of recovered labels w.r.t clean labels on CIFAR10 training set with 40% of label noise.
Figure 6: Average sample weights wi under var-ious labels. The white areas indicate that no sample lies in the cell. data distribution, that is, making prediction only on samples with conﬁdence. Formally, a selective classiﬁer is a composition of two functions (f, g), where f is the conventional c-class classiﬁer and g is the selection function that reveals the underlying uncertainty of inputs. Given an input x, selective classiﬁer outputs (f, g)(x) = (cid:26)Abstain, f (x), g(x) > τ ; otherwise, (3) for a given threshold τ that controls the trade-off. 4.2 Approach
Inspired by [42, 20], we adapt our presented approach in Algorithm 1 to the selective classiﬁcation task. We introduce an extra (c + 1)-th class (represents abstention) during training and replace selection function g(·) in Equation (3) by f (·)c. In this way, we can train a selective classiﬁer in an end-to-end fashion. Besides, unlike previous works that provide no explicit signal for learning abstention class, we use model predictions as a guideline in the design of learning process. Given a mini-batch of data pairs {(xi, yi)}m, model predictions pi and its exponential moving average ti for each sample, we optimize the classiﬁer f by minimizing:
L(f ) = − 1 m (cid:88)
[ti,yi log pi,yi + (1 − ti,yi) log pi,c], (4) i where yi is the index of non-zero element in the one hot label vector yi. The ﬁrst term measures the cross-entropy loss between prediction and original label yi, in order to learn a good multi-class classiﬁer. The second term acts as the selection function, identiﬁes uncertain samples in datasets. ti,yi dynamically trades-off these two terms: if ti,yi is very small, the sample is deemed as uncertain and the second term enforces the selective classiﬁer to learn to abstain this sample; if ti,yi is close to 1, the loss recovers the standard cross entropy minimization and enforces the selective classiﬁer to make perfect prediction. 4.3 Experiments
We conduct the experiments on two datasets: CIFAR10 [18] and Dogs vs. Cats [1]. We compare our method with previous state-of-the-art methods on selective classiﬁcation, including Deep Gam-blers [20], SelectiveNet [13], Softmax Response (SR) and MC-dropout [12]. We use the same experimental settings as these works for fair comparison (details are given in Appendix A.4). The results of prior methods are cited from original papers and are summarized in Table 4. We see that our method achieves up to 50% relative improvements compared with all other methods under various coverage rates, on all datasets. Notably, Deep Gamblers also introduces an extra abstention class in their method but without applying model predictions. The improved performance of our method comes from the use of model predictions in the training process. 8
Table 4: Selective classiﬁcation error rate (%) on CIFAR10 and Dogs vs. Cats datasets for various coverage rates (%). Mean and standard deviation are calculated over 3 trials. The best entries and those overlap with them are marked bold.
Dataset
Coverage
Ours
Deep Gamblers
SelectiveNet
SR
MC-dropout
CIFAR10
Dogs vs. Cats 100 95 90 85 80 75 70 100 95 90 85 80 6.05±0.20 3.37±0.05 1.93±0.09 1.15±0.18 0.67±0.10 0.44±0.03 0.34±0.06 3.01±0.17 1.25±0.05 0.59±0.04 0.25±0.11 0.15±0.06 6.12±0.09 3.49±0.15 2.19±0.12 1.09±0.15 0.66±0.11 0.52±0.03 0.43±0.07 2.93±0.17 1.23±0.12 0.59±0.13 0.47±0.10 0.46±0.08 6.79±0.03 4.16±0.09 2.43±0.08 1.43±0.08 0.86±0.06 0.48±0.02 0.32±0.01 3.58±0.04 1.62±0.05 0.93±0.01 0.56±0.02 0.35±0.09 6.79±0.03 4.55±0.07 2.89±0.03 1.78±0.09 1.05±0.07 0.63±0.04 0.42±0.06 3.58±0.04 1.91±0.08 1.10±0.08 0.82±0.06 0.68±0.05 6.79±0.03 4.58±0.05 2.92±0.01 1.82±0.09 1.08±0.05 0.66±0.05 0.43±0.05 3.58±0.04 1.92±0.06 1.10±0.05 0.78±0.06 0.55±0.02 5