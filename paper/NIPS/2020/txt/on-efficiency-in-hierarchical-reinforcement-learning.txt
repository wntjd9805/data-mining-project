Abstract
Hierarchical Reinforcement Learning (HRL) approaches promise to provide more efﬁcient solutions to sequential decision making problems, both in terms of sta-tistical as well as computational efﬁciency. While this has been demonstrated empirically over time in a variety of tasks, theoretical results quantifying the ben-eﬁts of such methods are still few and far between. In this paper, we discuss the kind of structure in a Markov decision process which gives rise to efﬁcient
HRL methods. Speciﬁcally, we formalize the intuition that HRL can exploit well repeating "subMDPs", with similar reward and transition structure. We show that, under reasonable assumptions, a model-based Thompson sampling-style HRL algorithm that exploits this structure is statistically efﬁcient, as established through a ﬁnite-time regret bound. We also establish conditions under which planning with structure-induced options is near-optimal and computationally efﬁcient. 1

Introduction
Hierarchical reinforcement learning (HRL) refers to the ability of an agent to act and plan at multiple levels of temporal abstraction [Sutton et al., 1999, Barto and Mahadevan, 2003]. In principle, this ability can present several beneﬁts: (1) more efﬁcient exploration, by employing policies that help an agent circulate more efﬁciently over the state space; (2) more efﬁcient credit assignment, because temporally extended models propagate credit over many time steps, and the same stream of data can be re-used to learn about many possible contingencies (for example, expressed as sub-goals); and (3) the ability to solve smaller problems and compose the resulting policies and models quickly in new situations. From a theoretical point of view, (1) has been investigated in recent work [Fruit et al., 2017]. Aspects (2) and (3) have received empirical validation in a large number of papers, but not much theoretical analysis, except for some special cases, such as the work of Mann et al.
[2015]. In this paper, we present two general results which highlight the types of problems in which
HRL is expected to provide beneﬁts, in terms of planning speed, as well as in terms of statistical efﬁciency. First, as has been highlighted empirically in the past, having repeated structure in the
Markov decision process (MDP) can lead to large speedups in both aspects. Second, in terms of planning, HRL provides beneﬁts when we are able to "insulate" well sub-problems that can be solved in isolation, and whose solutions can then be "stitched" together. We formalize the latter intuition.
Contributions: First, we formalize a notion of MDP decomposition into sub-problems, using state partitions. Second, we show that the existence of hierarchical structure in the environment can lead to statistically efﬁcient learning, by extending the results in Osband et al. [2013] to establish a regret bound that separates errors due to sub-optimal planning and errors due to learning. If the original 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
MDP can be decomposed into repeating problems that are relatively small, the expected regret of a posterior sampling exploration algorithm can be dramatically reduced. Finally, we study planning under decompositions of the original problem. We establish a relationship between the complexity of planning and the number of contingencies under which sub-problems are solved, formalized through a notion of exit proﬁles. We show formally that near-optimal planning can be obtained much faster if the original problem can be partitioned into repeated problems that are small and well separated. 2 Problem formulation
Consider a ﬁnite-time horizon MDP M = (cid:104)S, A, P, r, se, s0(cid:105), where S is a ﬁnite state space, A is a
ﬁnite action space, P and r respectively encode the transition model and the reward model, se ∈ S is a ﬁxed terminal state, and s0 ∈ S is a ﬁxed initial state1. If the agent takes action a ∈ A at a non-terminal state s ∈ S \ {se}, it receives a random reward drawn from distribution r(·|s, a), and transits to the next state s(cid:48) ∈ S with probability P (s(cid:48)|s, a). Without loss of generality, we assume that the support of reward distribution r(·|s, a) ⊆ [0, 1], ∀s ∈ S \ {se}, ∀a ∈ A. We use ¯r(s, a) to denote the mean of r(·|s, a). In this paper, we consider two different but related problems in MDPs: planning and reinforcement learning (RL). (cid:104)(cid:80)τ −1
In the planning setting, the agent knows M, and its goal is to compute a near-optimal policy, (cid:105)
π : S → A, i.e., a policy that maximizes the expected total reward: maxπ Eπ
, where rh ∼ r(·|sh, ah) is the reward received by taking action ah = π(sh) in time period h, and τ is a random variable that denotes the time at which the agent enters se. To simplify the exposition, we assume that under any policy π, τ ≤ τmax with probability 1 and E[τ ] ≤ H.
In the RL setting, the agent knows S, A, se, s0, but does not know P or r. The agent will repeatedly interact with M for T episodes. An episode always starts at s0 and ends immediately upon entering se. The agent’s goal is to maximize its expected cumulative reward over the T episodes, max (cid:80)T h=1 rth], where we use t to index episodes and h to index periods in an episode, rth ∼ r(·|sth, ath) is the reward received in period h of episode t and τt is the duration of episode t.
Note that an RL algorithm might call a planning algorithm to compute a policy (for example, if the
RL algorithm is model-based).
E [(cid:80)τt h=1 rh t=1 3 Deﬁning sub-problems and hierarchical structure
We would like to capture the intuitive notion of modularity in the context of MDPs. Modularity allows a large problem to be broken down into sub-problems, which could be tackled and solved inde-pendently from the rest. Sub-problem solutions could then be "stitched" together to (approximately) solve the entire problem. Intuitively, if these sub-problems are relatively small and repeated, this approach can lead to large computational gains. We will now formalize these intuitions for MDPs.
Deﬁnition 1 Consider a partition of the non-terminal states S \ {se} into L disjoint subsets H =
{Si}L i=1. We deﬁne an induced subMDP Mi = (cid:104)Si ∪ Ei, A, Pi, ri, Ei(cid:105) as follows:
• Si is the internal state set, and the action space is still A.
• The exit state set Ei is deﬁned as Ei = {e ∈ S \ Si : ∃(s, a) ∈ Si × A s.t. P (e| s, a) > 0}.
• The state space of Mi is Si ∪ Ei.
• Pi and ri are respectively the restriction of P and r to domain Si × A.
• The subMDP Mi terminates once it reaches a state in Ei (i.e., an exit state).
Given a partition H of M, consider the set of induced subMDPs, {Mi}L maximum size of any subMDP, and E, the set of all exit states, as follows: i=1. We deﬁne M , the
M = maxi |Si ∪ Ei| and E = ∪L i=1Ei, (1) 1Note that the ﬁxed initial state assumption can be easily relaxed to allow for an initial state drawn from any
ﬁxed distribution. 2
where | · | denotes the cardinality of a set. Intuitively, each subMDP can be viewed as a sub-problem of the original MDP. We can exploit the fact that some sub-problems may be similar to each other, by solving only one instance and re-using this solution. We now deﬁne the notion of equivalent subMDPs, in order to capture this idea.
Deﬁnition 2 (Equivalent subMDPs) Two subMDPs Mi and Mj are equivalent if there is a bijec-tion f : Si ∪ Ei → Sj ∪ Ej s.t. f (Si) = Sj, f (Ei) = Ej, and, through f , the subMDPs have the same transition probabilities and rewards at internal states.
Note that the constraints f (Si) = Sj and f (Ei) = Ej ensure that an internal (or exit) state in Mi is mapped to an internal (or exit) state in Mj. Let K ≤ L be the number of equivalence classes of subMDPs induced by a particular partition H of M. When there is no repeatable structure, K = L.
When the partition produces repeatable structure, K < L.
In summary, any state space partition H yields three parameters: the maximum size of an induced subMDP M , the number of subMDP equivalence classes K, and the total number of exit states |E|.
Our analyses will depend on these three quantities, rather than |S|, the number of states in M. While the results that we will present hold for any MDP M and any partition H, they will offer dramatic improvements over the standard algorithms only if M exhibits hierarchical structure with respect to a partition H, by which we mean: 1. M K (cid:28) |S|; 2. the number of exit states |E| is small relative to the total number of states |S|.
Intuitively, condition 1 can be satisﬁed by having small M , small K or both. If the number of equivalence classes K is small, solving a subMDP once may produce solutions that can be re-used in many other parts of the original problem. If M is small, all subMDPs have small size, so they would be relatively easy to solve. Finally, if |E| is small, intuitively we have a small number of states that connect the sub-problems. We can think of these as “bottleneck" states in M, which have been shown before to enable computationally efﬁcient planning (see e.g. Sutton et al. [1999], McGovern and Barto [2001], Stolle and Precup [2002], Simsek and Barto [2009], Solway et al. [2014]).
To make this notion of hierarchical structure more concrete, consider a garbage collecting robot navigating in a building. The robot’s goal is to maximize the amount of trash it collects before exiting.
The building has L ﬂoors, and each ﬂoor belongs to one of K ﬂoor “types” deﬁned according to some criterion relevant to the robot. A ﬂoor of type i has |Ei| ≤ |E| exits to other ﬂoors (elevators, stairs, etc.). When exiting a ﬂoor, the agent will either get to another ﬂoor or leave the building. Each
ﬂoor has L(cid:48) rooms, which can also be grouped into K (cid:48) groups. Room type j can be divided into j ≤ M (cid:48) regions that deﬁne the robot’s current state; some of these regions contain garbage that
M (cid:48) should be collected. Rooms are connected through up to |E (cid:48)| doors. If we think of the robot as an
RL agent and the building as an MDP, the problem can be partitioned at two different levels: each subMDP can be either a ﬂoor or a room. Each of these partitions will result in different values for the constants appearing in items 1 and 2 above, which will in turn deﬁne how efﬁciently the agent can solve the problem.
In the next two sections, we will analyze RL (Sec. 4) and planning (Sec. 5), assuming that an agent starts with a known partition H, in which the equivalence classes of the induced subMDPs are also known. We will establish results showing that leveraging hierarchical structure allows agents to achieve better statistical efﬁciency, in the case of learning, and better computational complexity, in the case of planning (at the cost of some controlled sub-optimality). 4 Statistically Efﬁcient Learning with Hierarchical Structure
It is natural to think of hierarchical reinforcement learning as what an agent does when it possesses prior knowledge that the environment obeys hierarchical structure. As we will establish in this section, hierarchical structure can enable more efﬁcient learning, and the difference can be dramatic if the
MDP exhibits highly repetitive structure. This occurs when the number of subMDPs far exceeds the number of equivalence classes. We will formally characterize improvements in statistical efﬁciency through studying a speciﬁc reinforcement learning algorithm that can leverage prior knowledge in a coherent manner. We expect our qualitative insights to extend to other algorithms that carefully account for prior knowledge. 3
4.1 Posterior Sampling for Reinforcement Learning
Posterior sampling for reinforcement learning (PSRL), as introduced by Strens [2000] and analyzed in Osband et al. [2013] and Gopalan and Mannor [2015], offers an often effective approach to episodic reinforcement learning. Before each episode of interaction, the agent samples a model of the environment, possibly in the form of an MDP, from the posterior distribution over environments conditioned on data gathered over previous episodes. Then, the agent computes an optimal policy for the sampled model and applies that to select actions over the next episode. To guide exploration,
PSRL relies on representation of epistemic uncertainty in terms of a probability distribution over environment. The prior distribution reﬂects the agent’s initial partial knowledge about the environment.
To quantify performance, regret bounds that apply under any prior distribution are established in
Osband et al. [2013]. These bounds do not capture the beneﬁts of greater degrees of prior knowledge, but subsequent results [Osband and Van Roy, 2014a,b] demonstrate that stronger regret bounds, reﬂecting dramatic improvements in agent performance, are possible when the prior distribution reﬂects knowledge of special environment structure.
Algorithm 1 offers pseudocode for a generalized form of PSRL. Over each episode, a sampler produces an MDP Mt that represents a statistically plausible model of the environment given the state of knowledge P t. Then, a planner computes a policy πt that approximately optimizes Mt.
This policy is executed over the episode, leading to a data set Dt made up of the trajectory of states, actions, and rewards. Finally, the state of knowledge is updated by an inference algorithm.
Ideally, as in the pure form of PSRL, P t encodes a posterior distribution over MDPs, the sampling algorithm draws Mt from P t, the planner computes an optimal policy for Mt, and the inference algorithm applies Bayes’ rule. However, the more ﬂexible generalization of Algorithm 1 can be applied more broadly, even when it is infeasible to exactly represent, compute, or sample from the posterior distribution or to identify an optimal policy.
Algorithm 1: PSRL with a Planner, Sampler, and Inferer
Initialization: prior knowledge P 0, planning algorithm plan, sampling algorithm sample, inference algorithm infer; for episode t = 1, 2, . . . T do sample Mt ∼ sample(P t); plan πt = plan(Mt); execute πt over episode t, observe Dt; infer P t+1 = infer(P t, Dt) ; end 4.2 Hierarchical Reinforcement Learning
We consider posterior sampling for hierarchical reinforcement learning (PSHRL) to be PSRL applied with a particular kind of prior distribution, and possibly with a planner that is customized for such an environment. In particular, we will consider priors that include only MDPs that obey hierarchical structure, as described earlier, for ﬁxed values of M and K. As for the planner, we will alternately consider an optimal planner and one designed to produce approximately optimal policies more efﬁciently by leveraging hierarchical structure, as we will discuss in Section 5.
The per-episode computational complexity of PSHRL depends on the special structure obeyed by P t and Mt, as well as the choice of plan, sample, and infer. As we will show in Section 5, suitable hierarchical structure can be leveraged to improve computational efﬁciency. The choice of P 0, sample, and infer determine tractability of sampling and inference. Again, suitable hierarchical structure can allow for more efﬁcient execution of these steps.
Recall that K is the number of subMDP equivalence classes and M is the maximal number of states per subMDP. Hierarchical structure is especially informative when M K is small relative to the number of MDP states |S|. This can yield dramatic improvements in statistical efﬁciency, as we will establish via a regret bound. 4
4.3 Regret Bound
For any learning algorithm alg, we deﬁne the Bayesian regret over the ﬁrst T episodes as
BayesRegret(alg, T ) =
E (cid:104)
V ∗(s0) − V πt (cid:105) (s0)
,
T (cid:88) t=1 (2) where V ∗ is the optimal value function of M, and V πt is the value function under policy πt.
The regret is “Bayesian" in the sense that the expectation integrates over M with respect to the prior distribution P 0. Note that minimizing BayesRegret(alg, T ) is equivalent to maximizing (cid:80)T h=1 rth]. We will study BayesRegret(PSRL, T ), which is the
Bayesian regret of PSRL, when applied with a prior that exhibits hierarchical structure.
Recall that, under any policy π, E[τ ] ≤ H and τ ≤ τmax with probability 1. The following theorem is our main result on statistical efﬁciency.
E [V πt(s0)] = (cid:80)T
E [(cid:80)τt t=1 t=1
Theorem 1 (Regret Bound) If P 0 exhibits hierarchical structure with a maximum of M states per subMDP and K subMDP equivalence classes, sample draws from the posterior distribution, and infer applies Bayes’ rule, then
BayesRegret(PSRL, T ) ≤ E (cid:2)V ∗(s0) − V ˜π(s0)(cid:3) T (cid:125) (cid:17)
K(cid:112)|A|T log(|A|KHτmaxT ) 3 2 M
√
H (cid:16) (cid:124) (cid:123)(cid:122) due to sub-optimal planning
+ O (cid:124)
, (cid:125) (cid:123)(cid:122) due to learning where ˜π = plan(M).
Note that the expectation in E (cid:2)V ∗(s0) − V ˜π(s0)(cid:3) represents an integral with respect to the prior distribution P 0 of M. Also note that if plan exactly optimizes M then ˜π = π∗, and there-fore, E (cid:2)V ∗(s0) − V ˜π(s0)(cid:3) = 0. Approximately optimal planning can contribute to regret a term
E (cid:2)V ∗(s0) − V ˜π(s0)(cid:3) T that grows linearly with the number T of episodes. One factor of O(H) in the second term is due to the magnitude of optimal value E [V ∗(s0)], which can grow with horizon, and is therefore inevitable.
√
We show that the hierarchical structure can enable statistically more efﬁcient learning by comparing to the PSRL regret bound of Osband et al. [2013], which applies for any prior. Assuming that the MDP has ﬁxed horizon τ = H and plan always returns an optimal policy, Osband et al. [2013] established 2 |S|(cid:112)|A|T )2, where the ˜O(·) notation hides logarithmic factors.
BayesRegret(PSRL, T ) ≤ ˜O(H 3
K(cid:112)|A|T
Our regret bound, under the same assumptions, is ˜O
, that is, we have replaced
˜O(|S|) with ˜O(M
K), which is highlighted in red font in Theorem 1. Notice that if M is treated as one subMDP, then we have K = L = 1 and M = |S|, hence our regret bound reduces to
√ that in Osband et al. [2013]. On the other hand, when M
K (cid:28) |S|, our regret bound conveys a dramatic improvement. This improvement can be interpreted in terms of two components. First, the replacement of ˜O((cid:112)|S|) with ˜O(
M K) arises because the agent needs to learn about a smaller number of distinct states in the hierarchical MDP. Second, ˜O((cid:112)|S|) is replaced by ˜O(
M ) because at each state-action pair in the hierarchical MDP, the agent can transition to at most M successor states.
H 3 2 M
√
√
√ (cid:17) (cid:16)
The proof of Theorem 1 is partially motivated by analysis in Osband et al. [2013]. However, we consider a different setting and our results are technically more complex. Speciﬁcally, compared with Osband et al. [2013], Theorem 1 considers hierarchical structure, and allows for both sub-optimal planning and a random time horizon τ . Please refer to Appendix A for the detailed proof of
Theorem 1. 5 Computationally Efﬁcient Planning with Hierarchical Structure
We now turn our attention to the problem of planning in M given a partition H. This problem has been tackled in the framework of options, by using option models [Sutton et al., 1999]. Options can 2Some notations in Osband et al. [2013] have different meanings. Speciﬁcally, “τ " in Osband et al. [2013] means H and “T " in Osband et al. [2013] means HT . 5
Algorithm 2: Planning with Exit Proﬁles (PEP)
Input: MDP M, k sets of exit proﬁles ˜Jk, one for each equivalent subMDP class k;
Step 1: Option generation for k = 1, 2, . . . K do
For each exit proﬁle J ∈ Jk, compute one option πk,J for subMDPs in equivalence class k, and its associated model; end
Step 2: Plan with options
Compute a policy for the induced high-level MG, which induces a policy ˜π for M
Return: ˜π be viewed as policies which act in a subset of states, and with which one can associate temporally extended reward and transition models. The option policies can be thought of as solutions to sub-problems, generated by an MDP’s structure, and some subgoals, which are additional rewards associated with particular states [Sutton et al., 1999]. One can view this approach as constructing options corresponding to subMDPs. To make this problem well deﬁned, one needs to consider possible combinations of values associated with the exit states of a subMDP. For example, an agent which is in a room with two doors might consider making either door a subgoal, by giving it a high reward. An option can then be trained inside the room, which amasses treasure if it makes sense, then exits through the designated door. We now formalize this intuition through the notion of exit proﬁles.
Deﬁnition 3 (Exit Proﬁle) An exit proﬁle J for subMDP Mi is a vector of values J(e), ∀e ∈ Ei.
Note that from the perspective of a subMDP, the structure outside is summarized in an exit proﬁle J.
An exit proﬁle induces an optimal policy for the subMDP Mi, πi,J , which we will think of as an option. By deﬁnition, an exit proﬁle J will induce the same option for equivalent subMDPs.
Once a set of options and associated models have been computed, one can deﬁne an induced high-level MDP MG = (cid:10)SG, AG, P G, rG(cid:11), whose state space SG = E ∪ {s0} is the union of all exit states and the initial state s0. For each s ∈ SG, if s is a state in a subMDP Mi, then its action space
AG(s) is the set of options computed for Mi. rG(s, πi,J ) is the expected reward obtained from s ∈ Si under option πi,J until this option reaches an exit state e ∈ Ei, and P G(e|s, πi,J ) gives the probability of transitioning to e ∈ Ei. These quantities form the option model for πi,J , deﬁned as in Sutton et al. [1999], which can be computed at the same time as the option3.
The process of creating a set of options and models corresponding to a set of exit proﬁles, then using them to solve MG, is summarized in Algorithm 2, which we call Planning with Exit Proﬁles (PEP). 5.1 Computational Complexity of Planning with Options
PEP is a blueprint for planning with options, which can be instantiated by using different dynamic programming approaches [Bertsekas, 2015] to implement steps 1 and 2. We will discuss the com-plexity of this algorithm when using value iteration (VI) for both steps, but similar analyses could be carried out easily for other algorithms (e.g., policy iteration).
In order to simplify the analysis and ensure that VI terminates in a ﬁnite number of steps, we make the following assumption:4
Assumption 1 For M, all its induced subMDPs with exit proﬁles, and the induced MG, the transi-tion probability graph corresponding to an optimal policy is acyclic.
Under Assumption 1, VI will compute the value function in n iterations under a proper initialization, where n is the cardinality of the state space (see Section 3.4.1 of Bertsekas [2015]). Under this 3Note that we have no discount factor, due to the ﬁnite-time horizon, which simpliﬁes the model and gives rise to an MDP at the high level as well, instead of an SMDP. 4We make Assumption 1 to simplify the exposition of the computational complexity results. This assumption is not strictly necessary and can be relaxed. 6
assumption, the computational complexity of VI in M is O(|S|2|A|M ), because by our deﬁnition,
M is an upper bound on the number of states into which any state-action pair can transition. Let
X = maxk | ˜Jk| denote the maximum cardinality of an exit proﬁle set over all equivalent subMDP classes (see Algorithm 2). The computational complexity of PEP can then be expressed as:
O(KXM 2|A|M ) (cid:125) (cid:123)(cid:122) (cid:124) for step 1
+ O(|E|2XM ) (cid:123)(cid:122) (cid:125) for step 2 (cid:124)
≤ O (cid:0)X[KM 2|A| + |E|2]M (cid:1) . (3)
Roughly speaking, planning with options will be efﬁcient if XM 2K < O(|S|2) and |E|2X <
O(|S|2|A|), which means that all subMDPs are small, a small number of exit proﬁles are used to
ﬁnd options for each equivalent subMDP class, and the total number of exit states |E| is small. 5.2 Performance of Planning with Options
We now provide a performance bound for PEP, based on the “quality" of the exit proﬁles. Let
Ji ⊂ [0, H]|Ei| be the space of possible exit proﬁles for subMDP Mi. Let V π i,J denote the value of a policy π for an exit proﬁle J in Mi. We denote V ∗ i,J the value of the optimal policy of Mi w.r.t. exit proﬁle J, πi,J . We now deﬁne the suboptimality of a set of exit proﬁles:
Deﬁnition 4 (Exit Proﬁle Suboptimality) The suboptimality of a set of exit proﬁles ˜J for Mi is deﬁned as:
∆i( ˜J ) = max s∈S 0 i , J∈Ji (cid:20)
V ∗ i,J (s) − max
˜J∈ ˜J (cid:21)
πi, ˜J i,J (s)
V
, (4) where S0 i is the set of possible start states in Mi, and Ji is the space of possible exit proﬁles.
πi, ˜J i,J (s) is the value with exit proﬁle J, under policy πi, ˜J that is optimal for another exit
Notice that V proﬁle ˜J, at the start state s. In other words, the deﬁnition of ∆i( ˜J ) ensures that for any exit proﬁle
J, there exists an exit proﬁle in ˜J that induces an ∆i( ˜J )-optimal policy under J. Recall that in
Algorithm 2, for subMDP Mi in equivalence class k, exit proﬁles ˜Jk are used for option generation.
Thus, we deﬁne ∆ = maxi ∆i( ˜Jki), where ki is the equivalence class Mi is in. We can also prove that PEP with VI is near-optimal under Assumption 1 and a mild technical assumption.
Proposition 1 If in step 2 of PEP, the agent uses VI with initial V = 0 to compute a policy ˜π, then under Assumption 1 and a mild technical assumption, we have: V ∗(s0) − V ˜π(s0) ≤ ∆|E|.
Please refer to Appendix B.1 for the proof of this proposition. Roughly speaking, Proposition 1 states that if the total number of exit states, |E|, is small, and the exit proﬁles used in PEP have high quality (∆ is small), then PEP returns a near-optimal policy. 5.3 Sufﬁcient Conditions for High-Quality Exit Proﬁles
The previous results indicate that in order to ensure that planning with options is both computationally efﬁcient and returns a near-optimal policy, we need the set of exit proﬁles considered by PEP to have small cardinality (for computational efﬁciency) and high quality (for near-optimality). We now discuss how to choose such exit proﬁle sets.
In general, an exit proﬁle set can be chosen based on an (cid:15)-cover, deﬁned as follows: a ﬁnite set ˜J is an (cid:15)-cover for J if for any J ∈ J , there exists ˜J ∈ ˜J s.t. (cid:107)J − ˜J(cid:107)∞ ≤ (cid:15). We then have the following result:
Proposition 2 For subMDP Mi in equivalence class k, if ˜Jk is an (cid:15)-cover for Ji, then ∆i( ˜Jk) ≤ 2(cid:15).
Please refer to Appendix B.2 for the proof of Proposition 2. Since Ji ⊆ [0, H]|Ei|, there always exists a ﬁnite (cid:15)-cover ˜Jk for Ji with | ˜Jk| ≤ (cid:6) H
. In general, the cardinality of an (cid:15)-cover is too large to guarantee PEP’s computational efﬁciency. However, if maxi |Ei| is very small (e.g. maxi |Ei| ≤ 3), then PEP with (cid:15)-cover will be computationally efﬁcient. (cid:7)|Ei| (cid:15)
Another favorable special case is when subMDP Mi has deterministic exiting. That is, with any start state s and under any deterministic policy π, the agent exits Mi at a single state es,π ∈ Ei with 7
probability 1. Note that in general es,π depends on both the start state s and the deterministic policy
π. Deterministic exiting will occur if Mi has only one exit state, or deterministic transitions. For any e ∈ Ei, we deﬁne Je : Ei → (cid:60) as Je(s) = (H + 1)1[s = e], where 1[·] is the indicator function. We then have the following result:
Proposition 3 For subMDP Mi in equivalence class k, if Mi has deterministic exiting and ˜Jk =
{Je : e ∈ Ei}, then ∆i( ˜Jk) = 0.
Please refer to Appendix B.3 for the proof of Proposition 3. Note that in this case | ˜Jk| = |Ei|. Based on Proposition 1, if all the subMDPs have deterministic exiting, then we can efﬁciently compute an optimal policy π∗ for M by using PEP. 6 Summary of Results learning alg. planner regret bound computation per episode
PSRL
PSHRL
PSHRL
VI
VI
PEP 2 |S|(cid:112)|A|T )
˜O(H 3 2 M
√
˜O(H 3
∆|E|T + ˜O(H 3
K(cid:112)|A|T )
√ 2 M
O(|S|2|A|M )
O(|S|2|A|M )
K(cid:112)|A|T ) O(X(M 2K|A| + |E|2)M )
Table 1: Algorithm Comparison. Differences in regret bounds and computational complexities are highlighted in red font. Recall that S and A are the state and action space of M; H is a bound on the expected time horizon of M; T is the number of interaction episodes; M is the maximum subMDP size; K is the number of subMDP equivalence classes; E is the set of all exit states; ∆ and
X respectively measure the quality and the number of exit proﬁles used in PEP.
In Table 1, we summarize the regret bounds and per-episode computational complexities of three algorithms: (1) PSRL with value iteration (VI) planning, (2) PSHRL with VI planning, and (3) PSHRL with PEP planning5. As discussed before, if a partition with many repeated, small subMDPs is available, the regret bound for (2) is much smaller than (1), since PSHRL exploits hierarchical structure during learning. Moreover, if all the subMDPs also have few exit states and admit a small set of high-quality exit proﬁles, then (3) will be computationally much more efﬁcient than (1) and (2), and only incurs an additional O(∆|E|T ) regret compared with (2), due to sub-optimal planning. 7