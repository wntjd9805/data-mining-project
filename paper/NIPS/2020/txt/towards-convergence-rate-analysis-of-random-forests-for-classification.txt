Abstract
Random forests have been one of the successful ensemble algorithms in machine learning. The basic idea is to construct a large number of random trees individually and make prediction based on an average of their predictions. The great successes have attracted much attention on the consistency of random forests, mostly focusing on regression. This work takes one step towards convergence rates of random forests for classiﬁcation. We present the ﬁrst ﬁnite-sample rate O(n−1/(8d+2)) on the convergence of pure random forests for classiﬁcation, which can be improved to be of O(n−1/(3.87d+2)) by considering the midpoint splitting mechanism. We introduce another variant of random forests, which follow Breiman’s original random forests but with different mechanisms on splitting dimensions and positions.
We get a convergence rate O(n−1/(d+2)(ln n)1/(d+2)) for the variant of random forests, which reaches the minimax rate, except for a factor (ln n)1/(d+2), of the optimal plug-in classiﬁer under the L-Lipschitz assumption. We achieve tighter convergence rate O((cid:112)ln n/n) under proper assumptions over structural data. 1

Introduction
From the pioneer work [12], random forests have been recognized as one of the successful algorithms for classiﬁcation and regression, which construct a large number of random trees individually and make prediction based on an average of their predictions. This idea is partly motivated from geometric feature selection [2], random subspace [29], random split selection [23] and earlier ensemble decision trees [32]. Random forests make good performance in empirical studies [10, 12, 24, 48], and have been involved in diverse real applications such as ecology [18], computational biology [41], objection recognition [47], remote sensing [7], computer vision [16], etc. Numerous variants have been developed to improve performance and reduce computational costs [4, 6, 19, 27, 33, 34, 38, 43, 52, 56].
For an overview of random forests, we refer readers to the works of [10, 17, 26].
Empirical successes have attracted much attention on theoretical explorations of random forests.
Breiman [12] presented the generalization bounds for random forests based on the correlation and strength of individual random trees, followed by consistency analysis of a simple model of random forests [13]. Lin and Jeon [35] established a connection between random forests and adaptive nearest neighbors, and Meinshausen [37] studied consistency of random forests for regression in the context of conditional quantile predictions. The consistency results place random forests in a favored category of ensemble algorithms [8, 9, 40, 44, 45, 51]. Denil et al. [20] narrowed the gap between theory and practice of random forests for regression, and Goetz et al. [28] proposed active learning algorithm for non-parametric regression using random forests. Li et al. [34] derived non-asymptotic bounds on the expected bias of MDI importance for random forests, along with variable importance [30, 36].
Tang et al. [50] discussed when random forests fail and examined the inﬂuences of parameters over performance. Most previous theoretical studies focus on random forests for regression. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
For classiﬁcation, Biau et al. [9] took a crucial milestone on the consistency of randomized ensemble classiﬁers, and Denil et al. [19] showed the ﬁrst consistency of online random forests. For a fuller understanding, however, it is necessary to take one further step on the convergence rates of random forests for classiﬁcation, which would be beneﬁcial to design better random forests, and comprehend the effects of different splitting mechanisms during the constructions of random forests.
In this work, we take one step towards convergence rates of random forests for classiﬁcation, and the main contributions can be summarized as follows:
• We present the ﬁrst ﬁnite-sample rate on the convergence of pure random forests proposed originally by Breiman [11], that is, a convergence rate O(n−1/(8d+2)) is derived by selecting leaves parameter k = O(n4d/(4d+1)), where n and d denote the size of training data and dimension, respectively. This rate can be further improved to be of O(n−1/(3.87d+2)) if we instead split a leaf along the dimension at the midpoint of the chosen side.
• We introduce another simpliﬁed variant of random forests, which follow Breiman’s original random forests [12] but with different mechanisms on splitting dimensions and positions.
We derive a convergence rate O(n−1/(d+2)(ln n)1/(d+2)) for the simpliﬁed random forests, which reaches the minimax rate, except for a factor (ln n)1/(d+2), of the optimal plug-in classiﬁers under the L-Lipschitz assumption. We ﬁnally achieve tighter convergence rate
O((cid:112)ln n/n) based on proper assumptions over structural data, which may shed insights to random forests by correlating randomization process with data-dependent tree structure.
• In addition, we establish a relationship for the convergence rates between random forests and individual random trees, and make a better estimate on the height of random trees than was previously known.
The rest of this work is organized as follows: Section 2 shows the convergence rate between random forests and individual random trees. Section 3 presents the convergence rates of pure random forests.
Section 4 provides the convergence rates of the simpliﬁed variant of Breiman’s original random forests. Section 5 introduces related work. Section 6 concludes with future work. Some proofs for theorems and lemmas are given in the supplementary material due to the page limitation. 2 Convergence Rates between Random Forests and Random Trees
Let X = [0, 1]d and Y = {0, 1} denote the instance and label space, respectively. Suppose that D is an (unknown) underlying distribution over space X × Y. Let DX be the marginal distribution over the instance space X , and denote by
η(x) = Pr[y = +1|x] the conditional probability of positive instance with respect to distribution D. In this work, we assume that conditional probability η(x) is L-Lipschitz for some constant L > 0, i.e., for every x, x(cid:48) ∈ X ,
|η(x) − η(x(cid:48))| ≤ L(cid:107)x − x(cid:48)(cid:107) .
This assumption has been taken in random forests for regression [8, 40] and binary classiﬁcation
[15, 46]. Intuitively, it implies that two instances are likely to have similar labels for smaller distance.
Given a hypothesis h : X → Y, we deﬁne the classiﬁcation error over distribution D as
RD(h) = Pr(x,y)∼D[h(x) (cid:54)= y] = E(x,y)∼D[I[h(x) (cid:54)= y]] .
Here, I[·] denotes the indicator function, which returns 1 if the argument is true and 0 otherwise. It is well-known [22, 46] that the optimal Bayes’ error (i.e., the minimum of classiﬁcation error) and the
Bayes’ classiﬁer can be given by
R∗
D = Ex[min{η(x), 1 − η(x)}] and
D(x) = I[η(x) ≥ 1/2], h∗ respectively.
Notice that distribution D is unknown in practice, and what we observe is a training data
Sn = {(x1, y1), (x2, y2), . . . , (xn, yn)} , where each example is drawn independently and identically (i.i.d.) from distribution D. Our goal is to learn a classiﬁer hn with smaller classiﬁcation error from the training data Sn. As the training 2
n=1 is said to be consistent if ESn [RD(hn)] → R∗ data size n increases, we get a sequence of classiﬁers h1, h2, · · · , hn, · · ·. A sequence of classiﬁers
{hn}∞
Random forests classiﬁer fm(x) takes a majority vote over m individual randomized trees fSn,Θ1(x), fSn,Θ2 (x), . . . , fSn,Θm(x), that is,
D as n → ∞. fm(x) = I (cid:34) m (cid:88) i=1 fSn,Θi(x) ≥ (cid:35)
. m 2 (1)
Here, the random vectors Θ1, Θ2, . . . , Θm are distributed identically and independently, and charac-terize the mechanisms of random selections of splitting leaves, dimensions, and positions during the construction of randomized trees. The random vectors Θ1, Θ2, . . . , Θm will be speciﬁed according to different random forests in the subsequent section.
We ﬁrst present the following relationship of convergence rate between random forests classiﬁer and individual random tree classiﬁer, and the detailed proof is given in Appendix A.
Lemma 1 Let fm(x) be the random forests classiﬁer given by Eqn. (1), and fSn,Θ(x) denotes a classiﬁer of individual tree with respect to random vector Θ. We have
EΘ1,...,Θm [RD(fm(x))] − R∗
D ≤ 2(EΘ[RD(fSn,Θ(x))] − R∗) .
From this lemma, the convergence rate of random forests classiﬁer fm(x) is no more than twice that of individual random tree classiﬁer fSn,Θ(x); therefore, the consistency of random forests can be derived from the consistency of individual random tree. A relevant result that the consistency of a random classiﬁer is preserved by averaging [9, Proposition 1], while Lemma 1 is easier to obtain the convergence rates. In addition, the convergence rate of random forests is obtained from the expectation of convergence rates of individual trees, which can be viewed as the average of convergence rate of all of individual random trees.
It is necessary to introduce some notations used in this work. Write [d] = {1, 2, . . . , d} for some integer d > 0. We denote by B(p) a Bernoulli distribution with parameter p ∈ [0, 1], and let U(a, b) denote a uniform distribution over the interval [a, b]. We further represent ξ ∼ B(p) and ξ ∼ U(a, b) that a random variable ξ is chosen according to Bernoulli distribution B(p) and uniform distribution
U(a, b), respectively. Denote by e = 2.718... the Euler’s constant. For positive f (n) and g(n), we write f (n) = O(g(n)) if g(n)/f (n) → c for some constant c ∈ (0, +∞) as n → ∞. 3 Convergence Rates of the Pure Random Forests for Classiﬁcation
We begin with the pure random forests, which were originally proposed by Breiman [11]. Genuer [25] studied the variance reductions of pure random forests for regression, and Arlot and Genuer [3] further presented its bias-variance analysis. For classiﬁcation, Biau et al. [9] made an important milestone on the consistency of pure random forests. In this work, we take one further step on the convergence rate of pure random forests for classiﬁcation.
Formally, a pure random tree can be constructed as follows. Each node is associated with a rectangular cell, and all leaves (external nodes) constitute a partition of [0, 1]d at each iteration of tree construction.
The root of random partition is [0, 1]d itself. The following procedure is repeated k − 1 iterations for some pre-deﬁned k ≥ 2 in advance, and hence the output random tree has k leaves.
• A split leaf is selected at random, uniformly over all leaves at the current iteration.
• Once the leaf is selected, a split dimension is selected at random, uniformly over [d].
• The leaf is split along the split dimension at random, uniformly over the chosen side.
A pure random tree classiﬁer fSn,Θ(x) takes a majority vote over labels yi whose corresponding instances xi belong to the same cell of random partition as instance x. The main difference, between pure random tree and Breiman’s original random tree [12], is that recursive cell splits are irrelevant to label information, and the growth of individual random tree is independent of training sample.
Given m individual pure random trees fSn,Θ1(x), fSn,Θ2 (x), . . . , fSn,Θm (x), the random forests classiﬁer takes a majority vote over those random trees, that is, fm(x) = I[(cid:80)m i=1 fSn,Θi(x) ≥ m/2].
We now present the convergence rates of pure random forests for classiﬁcation. 3
Theorem 1 Let fm(x) be the random forests classiﬁer by applying pure random tree to training data Sn of k leaves (k ≥ 2). For L-Lipschitz conditional probability η(x), we have
R∗
D ≤ ESn,Θ1,...,Θm [RD(fm)] ≤ R∗
D +
√ 4 2eLd3/2 k1/8d (cid:114)
+ 2 k n
+ 6k n
.
Based on this theorem, we obtain a convergence rate O(n−1/(8d+2)) of pure random forests for classiﬁcation, by selecting leaves parameter k = O(n4d/(4d+1)). To the best of our knowledge, this presents the ﬁrst ﬁnite-sample converge rate of pure random forests for classiﬁcation. Also, it is easy to observe that
ESn,Θ1,...,Θm[RD(fm)] → R∗
D as k → +∞ and k/n → 0, which recovers the consistency result of random forests for classiﬁcation [9, Theorem 2].
Before the proof of Theorem 1, we go into the details of randomness Θ on the construction of pure random forests. Given a pure random tree, we associate k leaves with k disjoint rectangular cells
C1, C2, . . . , Ck, constituting a partition of instance space X = [0, 1]d. Given an instance x ∈ X , let
C(x) denote the rectangular cell of the random tree, that contains the instance x.
Given an instance x ∈ X , we introduce k − 1 Bernoulli random variables X1, X2, · · · , Xk−1 to characterize the random events that the node containing instance x was selected for splitting in the construction of random tree. Specially, the event Xi = 1 implies that the node containing x is selected for splitting in the i-th iteration of random tree construction; otherwise, Xi = 0. It follows that Xi ∼ B(1/i), because there are i leaves for selection with identical probability in the i-th iteration of random tree construction.
Let h(C(x)) denote the height of the rectangular cell C(x), i.e., the splitting times of C(x) during the construction of random tree. It is easy to obtain h(C(x)) = k−1 (cid:88) i=1
Xi .
We further present upper and lower bounds on h(C(x)) in expectation and in probability as follows:
Lemma 2 Let X1, X2, . . . , Xk−1 be k − 1 random variables such that Xi ∼ B(1/i) for i ∈ [k − 1].
For an instance x ∈ X , we have ln(k) ≤ EX1,X2,...,Xk−1 [h(C(x))] ≤ 1 + ln(k − 1) , and we also have, for any (cid:15) ∈ (0, 1),
PrX1,X2,...,Xk−1 [h(C(x)) ≤ (1 − (cid:15)) ln k] ≤ k−(cid:15)2/2 ,
PrX1,X2,...,Xk−1 [h(C(x)) ≥ (1 + (cid:15))(1 + ln(k − 1))] ≤ k−(cid:15)2/2 .
We have h(C(x)) = O(log k) with large probability, especially for large k. Lemma 2 improves the previous work [9] on the bounds of h(C(x)), where the saturation level is considered in random binary search tree [21, 42], and their bounds can be rewritten (with our notation) as follows:
Pr[h(C(x)) < (c∗ − (cid:15)) ln k] ≤ O(log(k)k(c∗−(cid:15)) ln(2e/(c∗−(cid:15)))−1) .
Here, c∗ = 0.3733 . . . is the unique solution of c ln(2e/c) = 1 (c < 1) and (cid:15) < c∗. As can be seen, Lemma 2 makes better estimations of h(C(x)) with larger probability. The detailed proof of
Lemma 2 is presented in Appendix B.
Given a cell C(x), we deﬁne its diameter as ν(C(x)) = maxx,x(cid:48)∈C(x){(cid:107)x − x(cid:48)(cid:107)}. Then, we can bound ν(C(x)) in probability as follows:
Lemma 3 For integer k ≥ 2, real (cid:15) > −1 and instance x ∈ X , we have (cid:34)
Pr
ν[C(x)] ≥ (1 + (cid:15)) (cid:35)
√ d k1/8d
≤ ed (1 + (cid:15))k1/8d
, where the probability takes over the random selections of splitting leaves, dimensions and positions. 4
√
This lemma shows that, for every instance x ∈ X , the diameter of rectangle cell of C(x) can be d/k1/8d with probability at least 1 − ed/(1 + (cid:15))k1/8d. We also have upper bounded by (1 + (cid:15))
ν(C(x)) → 0 in probability as k → +∞. For simplicity, we do not formalize the random selections of splitting leaves, dimensions and positions in Lemma 3, while the detailed formalization and proof are presented in Appendix C.
Recall that there are k disjoint rectangular cells C1, C2, . . . , Ck during the construction of pure random tree with k − 1 iterations. We present the following lemma to bound the classiﬁcation error over each rectangular cell, and the detailed proof is given in Appendix D.
Lemma 4 Let C1, C2, . . . , Ck be the k disjoint rectangular cells associating with the leaves of randomized tree, and fΘ,Sn(x) denotes the classiﬁer generated by random tree. For L-Lipschitz conditional probability η(x) and for every i ∈ [k], we have
Pr
Sn,(x,y)
[fΘ,Sn(x) (cid:54)= y|x ∈ Ci] Pr[x ∈ Ci] ≤ 2Lν(Ci) Pr[x ∈ Ci]
+ Ex[min{η(x), 1 − η(x)}|x ∈ Ci] Pr[x ∈ Ci] + (cid:112)Pr[x ∈ Ci]/n + 3/n .
Based on the previous lemmas, we now present the detailed proof of Theorem 1 as follow:
Proof of Theorem 1. We ﬁrst derive the convergence rate of individual random tree classiﬁer fSn,Θ(x), and then complete the proof by combining with Lemma 1. We have
RD(fSn,Θ) = Pr (x,y)∼D
[fΘ,Sn(x) (cid:54)= y] = Ex∼DX
Pr y∼B(η(x))
[fΘ,Sn (x) (cid:54)= y]
. (cid:20) (cid:21)
For random tree classiﬁer fΘ,Sn (x), we associate a set as follows: (cid:110)
Λ = x ∈ X : ν(C(x)) ≥ (1 + (cid:15))
√ d/k1/8d(cid:111)
, (2) where ν(C(x)) denotes the diameter of rectangle cell C(x). It follows that
RD(fSn,Θ) = Ex∼DX (cid:20)
Pr y∼B(η(x))
[fΘ,Sn (x) (cid:54)= y] (I[x ∈ Λ] + I[x /∈ Λ]) (cid:21)
≤ Ex∼DX [I[x ∈ Λ]] + Ex∼DX (cid:20)
Pr y∼B(η(x))
[fΘ,Sn(x) (cid:54)= y]I[x /∈ Λ] (cid:21)
. (3)
Notice that C1, C2, . . . , Ck is a partition of the instance space X from the construction of random tree. Based on the law of total probability, we have (cid:20)
Ex∼DX
Pr y∼B(η(x))
[fΘ,Sn(x) (cid:54)= y]I[x /∈ Λ] (cid:21)
= k (cid:88) i=1
Pr[fΘ,Sn (x) (cid:54)= y|x ∈ Ci] Pr[x ∈ Ci]I[Ci (cid:54)⊆ Λ] , where we use the fact C(x) = Ci for every x ∈ Ci. By combining with Eqns. (2) and (3), we have
ESn,Θ [RD(fSn,Θ)] ≤ Ex∼DX (cid:104) (cid:20)
Pr
Sn,Θ
ν[C(x)] ≥ (1 + (cid:15))
√ d/k1/8d(cid:105)(cid:21) (cid:34) k (cid:88) (cid:35)
ESn [Pr[fΘ,Sn (x) (cid:54)= y|x ∈ Ci]] Pr[x ∈ Ci]I[Ci (cid:54)⊆ Λ]
.
+EΘ
From Lemma 3, Eqn. (4) can be further upper bounded by i=1
Ex∼DX (cid:104) (cid:20)
Pr
Sn,Θ
ν[C(x)] ≥ (1 + (cid:15))
√ d/k1/8d(cid:105)(cid:21)
≤ ed (1 + (cid:15))k1/8d
.
Based on Lemma 4 and Eqn. (2), we can bound Eqn. (5) as follows k (cid:88) i=1
ESn [Pr[fΘ,Sn(x) (cid:54)= y|x ∈ Ci]] Pr[x ∈ Ci]I[Ci (cid:54)⊆ Λ]
≤ R∗
D +
√ 2(1 + (cid:15))L k1/8d (cid:114) k (cid:88) d
+ i=1
Pr[Ci] n
+ 3k n
, 5 (4) (5) (6) (7)
where we use the law of total expectation and R∗ inequality, we have (EX)2 ≤ E[X 2], and this gives
D = Ex∼DX [min{η(x), 1 − η(x)}]. By Jensen’s (cid:32) 1 k k (cid:88) i=1 (cid:33)2 (cid:112)Pr[Ci]
≤ 1 k k (cid:88) i=1
Pr[Ci] = 1 k
.
It follows that, by combining with Eqns. (4)-(7),
ESn,Θ [RD(fSn,Θ)] ≤ R∗
D + ed (1 + (cid:15))k1/8d
+
√ 2(1 + (cid:15))L k1/8d (cid:114) d
+ k n
+ 3k n
.
We have, by setting (cid:15) =
√ (cid:113) e d/2L and algebra calculations,
ESn,Θ [RD(fSn,Θ)] ≤ R∗
D + 2
√ 2eLd3/2 k1/8d (cid:114)
+ k n
+ 3k n
, which completes the proof by combining with Lemma 1.
We further study the effects of different splitting mechanisms during the construction of random forests. For example, how about the convergence rates for different selections of splitting leaves, dimensions and positions? Here, we consider pure random forests with midpoint splits, where midpoint splits have been well-studied for random forests in regression [3, 8, 31]. Formally, a pure random tree with midpoint splits can be constructed as follows. The root of random partition is [0, 1]d itself. The following procedure is repeated k − 1 iterations for some pre-deﬁned k ≥ 2 in advance.
• A split leaf is selected at random, uniformly over all leaves at the current iteration.
• Once the leaf is selected, a split dimension is selected at random, uniformly over [d].
• The leaf is split along the split dimension at the midpoint of the chosen side.
Given individual random tree classiﬁers fSn,Θ1(x), fSn,Θ2(x), . . . , fSn,Θm (x), the random forests classiﬁer takes a majority vote over m random trees. We present a convergence rate of pure random forests with midpoint splits for classiﬁcation as follows:
Theorem 2 Let fm(x) be the random forests classiﬁer by applying pure random tree with midpoint splits to training data Sn of k leaves (k ≥ 2). For L-Lipschitz conditional probability η(x), we have
R∗
D ≤ ESn,Θ1,...,Θm[RD(fm)] ≤ R∗
D + 8L3/5d7/10 k1/3.87d (cid:114)
+ 2 k n
+ 6k n
.
Based on this theorem, we get a convergence rate O(n−1/(3.87d+2)) of pure random forests with midpoint splits for classiﬁcation, by selecting leaves parameter k = O(n3.87d/(3.87d+2)). As can be seen, we achieve better convergence rate by instead considering the midpoint splitting mechanism during the construction of pure random forests, and an intuitive explanation is that midpoint splits yield smaller rectangle cells. The detailed proof of Theorem 2 is presented in Appendix E. 4 Convergence Rates of the Simpliﬁed Random Forests for Classiﬁcation
In this section, we present the convergence analysis towards Breiman’s original random forests [12] for classiﬁcation. We follow the procedures of Breiman’s random forests, but with different mecha-nisms on the selections of splitting dimensions and positions due to technical analysis challenges.
Algorithm 1 gives a detailed description of the simpliﬁed variant of random forests.
We introduce a structural list P to store leaves (or rectangle cells) for further splitting in Algorithm 1, which aims to keep the leaves split in successive layer. Such mechanism is essentially the same as that of random forests for regression [45]. At each iteration, the ﬁrst leaf (or rectangle cell) is selected and removed from P, and it will not be split if all training examples have the same label in the leaf (including less than one example in the leaf). For a split leaf, we select a dimension at 6
Algorithm 1 A simpliﬁed variant of Breiman’s original random tree [12]
Input: Training sample Sn and leaves parameter k.
Output: A random tree
Initialize: Set P = {[0, 1]d} and nleaf = 1. 1: while nleaf < k and P is not empty do 2: 3: 4: 5: 6:
Do nothing and the cell C will not be split any more. else
Let C be the ﬁrst rectangle cell in P, and remove it from P. if All training examples in C have the same label (including less than one example) then
Select a dimension Y at random, uniformly over dimensions along which the side length is maximal in the cell C.
Split cell C along Y at the midpoint of the chosen side, called CL, CR two resulting cells.
Update P by appending CL and CR, and nleaf ← nleaf + 1. 7: 8: 9: 10: end while end if random, uniformly over dimensions along which the side length is maximal in the leaf, and then split the leaf along the dimension at the midpoint of the chosen side. We ﬁnally update list P by appending two resulting leaves.
A leaf (rectangle cell) will not be split in Algorithm 1 if all training examples have the same label in this leaf. Such stopping-splitting criterion is different from pure random forests [11] and Mondrian forests [33, 40], where the growth of individual random tree is independent of training sample. In addition, it is also different from random forests regression [45], where a leaf will not be split only when the leaf has exactly one training example.
Given m individual random tree classiﬁers fSn,Θ1(x), fSn,Θ2(x), . . . , fSn,Θm(x) according to
Algorithm 1, the random forests classiﬁer takes a majority vote over m random trees, that is, fm(x) = I [(cid:80)m i=1 fSn,Θi(x) ≥ m/2]. We present a convergence rate of the simpliﬁed variant of random forests for classiﬁcation as follows:
Theorem 3 For k ≥ 2 and n ≥ 4, let fm(x) be the random forests classiﬁer by applying Algorithm 1 to training data Sn of k leaves. For L-Lipschitz conditional probability η(x), we have
R∗
D ≤ ESn,Θ1,...,Θm[RD(fm)] ≤ R∗
D + 4 (cid:114) k ln n n (cid:114)
+ 2 4 4k3 ln n n3 + 12k n (cid:114)
+ 4 k n
+
√ 4L d k1/d
.
We obtain a convergence rate O(n−1/(d+2)(ln n)1/(d+2)) for random forests based on Algorithm 1, by selecting leaves parameter k = O((n/ ln n)2d/(d+2)). This presents signiﬁcantly better convergence rate than that of pure random forests due to different splitting mechanisms and stopping-splitting criteria. The detailed proof of Theorem 3 is given in Appendix F.
Under the L-Lipschitz assumption, it is well-known [5, 54] that the minimax rate is of O(n−1/(d+2)) for the optimal plug-in classiﬁers f (x) = I[ˆη(x) ≥ 1/2], where ˆη(x) is a conditional probability estimated by learning algorithms. As can be seen, our simpliﬁed variant of random forests reaches the minimax convergence rate, except for a factor (ln n)1/(1+d), as that of the optimal plug-in classiﬁers, despite random forests are not plug-in classiﬁers, since random forests take a majority vote over the predictions of individual random trees, rather than the estimation of conditional probability.
Breiman’s original random forests [12] took some splitting criteria, such as information gain and entropy, to select the best-split dimension and position, which correlates the randomization process with data-dependent tree structure. Intuitively, such correlation could yield tighter convergence rates of random forests for classiﬁcation, whereas this makes it quite challenging to present convergence analysis from a technical view. To date, it is still an open problem on the consistency of Breiman’s original random forests [12] for classiﬁcation, let alone the analysis of convergence rate.
We now make some assumptions over structural data, which could yield tighter convergence rate for the simpliﬁed variant of random forests. Suppose that there is a constant k0 ≥ 2, such that the output random trees from Algorithm 1 have at most k0 leaves with all training examples in each leaf having the same label. Based on such assumption, we present a convergence rate of the simpliﬁed variant of random forests for classiﬁcation. 7
Theorem 4 Suppose that there is a constant k0 ≥ 2, such that the output random trees from
Algorithm 1 have at most k0 leaves with all training examples having the same label in each leaf. Let fm(x) be the random forests classiﬁer by applying Algorithm 1 to training data Sn. We have
ESn,Θ1,...,Θm[RD(fm)] ≤ 4 (cid:114) k0 ln n n (cid:114)
+ 2 4 (cid:114) 4k3 0 ln n n3 + 2 k0 n ln n
+ 6k0 n
.
Based on this theorem, we achieve tighter convergence rate O((cid:112)ln n/n) of the simpliﬁed variant of random forests for classiﬁcation, which is independent of dimension d. This theorem may show some lights on Breiman’s original random forests [12] with tighter convergence rates, by correlating randomization process and data-dependent tree structure.
The assumption in Theorem 4 is relevant to algorithm, while it still holds for some irrelevant cases, for example, Algorithm 1 satisﬁes such assumption when the data is separable and the separable hyperplane is parallel to axis. The detailed proof of Theorem 4 is presented in Appendix G. 5