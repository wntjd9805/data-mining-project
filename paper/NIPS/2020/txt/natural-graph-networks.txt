Abstract
A key requirement for graph neural networks is that they must process a graph in a way that does not depend on how the graph is described. Traditionally this has been taken to mean that a graph network must be equivariant to node permutations.
Here we show that instead of equivariance, the more general concept of naturality is sufﬁcient for a graph network to be well-deﬁned, opening up a larger class of graph networks. We deﬁne global and local natural graph networks, the latter of which are as scalable as conventional message passing graph neural networks while being more ﬂexible. We give one practical instantiation of a natural network on graphs which uses an equivariant message network parameterization, yielding good performance on several benchmarks. 1

Introduction
Graph-structured data is among the most ubiquitous forms of structured data used in machine learning and efﬁcient practical neural network algorithms for processing such data have recently received much attention [Wu et al., 2020]. Because of their scalability to large graphs, graph convolutional neural networks or message passing networks are widely used. However, it has been shown [Xu et al., 2018] that such networks, which pass messages along the edges of the graph and aggregate them in a permutation invariant manner, are fundamentally limited in their expressivity. (a) A global isomorphism. (b) Induced local isomorphisms.
Figure 1: A global graph isomorphism corresponds for each edge to a local isomorphism on its neighbourhood, shown for three example edges - denoted with arrows. Hence, when a message passing kernel satisﬁes the naturality condition for local isomorphisms of the edge neighbourhood (Eq. 4), it also satisﬁes the global naturality condition (Eq. 2).
˚Qualcomm AI Research in an initiative of Qualcomm Technologies, Inc. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
More expressive equivariant graph networks exist [Maron et al., 2018], but these treat the entire graph as a monolithic linear structure (e.g. adjacency matrix) and as a result their computational cost scales superlinearly with the size of the graph. In this paper we ask the question: how can we design maximally expressive graph networks that are equivariant to global node permutations while using only local computations?
If we restrict a global node relabeling / permutation to a local neighbourhood, we obtain a graph isomorphism between local neighbourhoods (see Figure 1).
If a locally connected network is to be equivariant to global node relabelings, the message passing scheme should thus process isomorphic neighbourhoods in an identical manner. Concretely, this means that weights must be shared between isomorphic neighbourhoods. Moreover, when a neighbourhood is symmetrical (Figure 1), it is isomorphic to itself in a non-trivial manner, and so the convolution kernel has to satisfy an equivariance constraint with respect to the symmetry group of the neighbourhood.
Local equivariance has previously been used in gauge equivariant neural networks [Cohen et al., 2019]. However, as the local symmetries of a graph are different on different edges, we do not have a single gauge group here. Instead, we have more general structures that can be captured by elementary category theory. We thus present a categorical framework we call natural graph networks that can be used describe maximally ﬂexible global and local graph networks. In this framework, an equivariant kernel is “just” a natural transformation between two functors. We will not assume knowledge of category theory in this paper, and explicit category theory is limited to Section 5.
When natural graph networks (NGNs) are applied to graphs that are regular lattices, such as a 2D square grid, or to a highly symmetrical grid on the icosahedron, one recovers conventional equivariant convolutional neural networks [Cohen and Welling, 2016, Cohen et al., 2019]. However, when applied to irregular grids, like knowledge graphs, which generally have few symmetries, the derived kernel constraints themselves lead to impractically little weight sharing. We address this by parameterizing the kernel with a message network, an equivariant graph network which takes as input the local graph structure. We show that our kernel constraints coincide with the constraints on the message network being equivariant to node relabelings, making this construction universal whenever the network that parameterizes the kernel is universal. 2 Global Natural Graph Networks
As mentioned before, there are many equivalent ways to encode (directed or undirected) graphs.
The most common encoding used in the graph neural networks literature is to encode a graph as a (node-node) adjacency matrix A, whose rows and columns correspond to the nodes and whose pi, jq-th entry signals the presence (Aij “ 1) or absence (Aij “ 0) of an edge between node i and j.
There are many other options, but here we will adopt the following deﬁnition:
Deﬁnition 2.1. A Concrete Graph G is a ﬁnite set of nodes2 VpGq Ă N and a set of edges EpGq Ă
VpGq ˆ VpGq.
The natural number labels of the nodes of a concrete graph are essential for representing a graph in a computer, but contain no actual information about the underlying graph. Hence, different concrete graphs that are related by a relabelling, encode the graphs that are essentially the same. Such relabellings are called graph isomorphisms.
Deﬁnition 2.2 (Graph isomorphism and automorphism). Let G and G1 be two graphs. An isomor-phism φ : G Ñ G1 is a mapping (denoted by the same symbol) φ : VpGq Ñ VpG1q that is bijective and preserves edges, i.e. satisﬁes for all pi, jq P VpGq ˆ VpGq: pi, jq P EpGq ðñ pφpiq, φpjqq P EpG1q. (1)
If there exists an isomorphism between G and G1, we say they are isomorphic. An isomorphism from a graph to itself is also known as an automorphism or simply symmetry.
In order to deﬁne graph networks, we must ﬁrst deﬁne the vector space of features on a graph.
Additionally, we need to deﬁne how the feature spaces of isomorphic graphs are related, so we can express a feature on one concrete graph on other isomorphic concrete graphs. 2Note that the set of node ids may be non-contiguous. This is useful because a graph may arise as a subgraph of another one, in which case we wish to preserve the node ids. 2
Figure 2: A graph feature ρ assigns to each graph G a vector space ρpGq (here ρpGq “ ρpG1q “ R4, ρ “ ρ1) and to each graph isomorphism φ : G Ñ G1 a linear map ρpφq : ρpGq Ñ ρpG1q (here swapping the ﬁrst and fourth row). Global Natural Graph Network layer K between features ρ and ρ1 has for each graph G a map KG : ρpGq Ñ ρ1pGq, such that for each graph isomorphism φ : G Ñ G1 the above naturality diagram commutes.
Deﬁnition 2.3 (Graph feature space). A graph feature space, or graph representation, ρ associates to each graph G a vector space VG “ ρpGq, and to each graph isomorphism φ : G Ñ G1 an invertible linear map ρpφq : VG Ñ VG1 , such that the linear maps respect composition of graph isomorphisms:
ρpφ ˝ φ1q “ ρpφq ˝ ρpφ1q. 3
As the nodes in a concrete graph have a unique natural number as a label, the nodes can be ordered.
A graph isomorphism φ : G Ñ G1 induces a permutation of that ordering. This gives a convenient way of constructing graph feature spaces. For example, for the vector representation, we associate with graph G the vector space ρpGq “ R|VpGq| and associate to graph isomorphisms the permutation matrix of the corresponding permutation. Similarly, for the matrix representation, we associate to graph G feature matrix vector space ρpGq “ R|VpGq|ˆ|VpGq| and to graph isomorphism φ : G Ñ G1, linear map ρpφqpvq “ P vP T , where P is the permutation matrix corresponding to φ.
A neural network operating on such graph features can, in general, operate differently on different graphs. Its (linear) layers, mapping from graph feature space ρ to feature space ρ1, thus has for each possible graph G, a (linear) map KG : ρpGq Ñ ρ1pGq. However, as isomorphic graphs G and G1 are essentially the same, we will want KG and KG1 to process the feature space in an equivalent manner.
Deﬁnition 2.4 (Global Natural Graph Network Layer). A layer (or linear layer) in a global natural graph network (GNGN) is for each concrete G a map (resp. linear map) KG : ρpGq Ñ ρ1pGq between the input and output feature spaces such that for every graph isomorphism φ : G Ñ G1, the following condition (“naturality”) holds:
ρ1pφq ˝ KG “ KG1 ˝ ρpφq. (2)
Equivalently, the following diagram should commute:
ρpGq
KG
ρ1pGq
ρpφq
ρ1pφq
ρpG1q
KG1
ρ1pG1q
The constraint on the layer (Eq. 2) says that if we ﬁrst transition from the input feature space ρpGq to the equivalent input feature space ρpG1q via ρpφq and then apply KG1 we get the same thing as ﬁrst applying KG and then transitioning from the output feature space ρ1pGq to ρ1pG1q via ρ1pφq. Since
ρpφq is invertible, if we choose KG for some G then we have determined KG1 for any isomorphic G1 by KG1 “ ρ1pφq ˝ KG ˝ ρpφq´1. Moreover, for any automorphism φ : G Ñ G, we get a equivariance constraint ρ1pφq ˝ KG “ KG ˝ ρpφq. Thus, to choose a layer we must choose for each isomorphism class of graphs one map KG that is equivariant to automorphisms. For linear layers, these can in principle be learned by ﬁrst ﬁnding a complete solution basis to the automorphism equivariance constraint, then linearly combining the solutions with learnable parameters. 3As is common in the category theory literature for functors (see Sec. 5), we overload the ρ symbol. ρpGq denotes a vector space, while ρpφq denotes a linear map. 3
The construction of the graph isomorphisms, the graph feature space and the natural graph network layer resemble mathematical formalization that are used widely in machine learning: groups, group representations and equivariant maps between group representations. However, the fact that the natural graph network layer can be different for each graph, suggests a different formalism is needed, namely the much more general concepts of a category, a functor and a natural transformation. How natural transformations generalize over equivariant maps is described in section 5. 2.1 Relation to Equivariant Graph Networks
The GNGN is a generalization of equivariant graph networks (EGN) [Maron et al., 2018, 2019], as an EGN can be viewed as a GNGN with a particular choice of graph feature spaces and layers. The feature space of an EGN for a graph of n nodes is deﬁned by picking a group representation of the permutation group Sn over n symbols. Such a representation consists of a vector space Vn and an invertible linear map ρpσq : Vn Ñ Vn for each permutation σ P Sn, such that ρpσσ1q “ ρpσq ˝ ρpσ1q.
A typical example is Vn “ Rnˆn, with ρpσq acting by permuting the rows and columns. The (linear) layers of an EGN between features ρ and ρ1 are (linear) maps Kn : Vn Ñ V 1 n, for each n, such that the map is equivariant: ρ1pσq ˝ Kn “ Kn ˝ ρpσq for each permutation σ P Sn.
Comparing the deﬁnitions of EGN features and layers to GNGN features and layers, we note the former are instances of the latter, but with the restriction that an EGN picks a single representation vector space Vn and single equivariant map Kn for all graphs of n nodes, while in a general GNGN, the representation vector space and equivariant map can arbitrarily differ between non-isomorphic graphs. In an EGN, the graph structure must be encoded as a graph feature. For example, the adjacency matrix can be encoded as a matrix representation of the permutation group. Such constructions are shown to be universal [Keriven and Peyré, 2019], but impose considerable constraints on the parameterization. For example, one may want to use a GNGN with completely separate sets of parameters for non-isomorphic graphs, which is impossible to express as an EGN. 3 Local Graph Networks
Global NGNs provide a general framework of specifying graph networks that process isomorphic graphs equivalently. However, in general, its layers perform global computations on entire graph features, which has high computational complexity for large graphs. 3.1 Local Invariant Graph Networks
An entirely different strategy to building neural networks on graphs is using graph convolutional neural networks or message passing networks [Kipf and
Welling, 2016, Gilmer et al., 2017]. We will refer to this class of methods as local invariant graph networks (LIGNs). Such convolutional architectures are generally more computationally efﬁcient compared to the global methods, as the computation cost of computing one linear transformation scales linearly with the number of edges.
Figure 3: Two regular graphs.
LIGNs are instances of GNGNs, where the feature space for a graph consists of a copy of the same vector space VN at each node, and graph isomorphisms permute these node vector spaces. In their simplest form, the linear layers of an LIGN pass messages along edges of the graph:
ÿ
KGpvqp “
W vq, pp,qqPE (3) where vp P VN is a feature vector at node p and W : VN Ñ V 1
N is a single matrix used on each edge of any graph. This model can be generalized into using different aggregation functions than the sum and having the messages also depend on vp instead of just vq [Gilmer et al., 2017]. It is easy to see that these constructions satisfy the GNGN constraint (Eq. 2), but also result in the output KGpvqp being invariant under a permutation of its neighbours, which is the reason for the limited expressivity noted by [Xu et al., 2018]. For example, no invariant message passing network can discriminate between the two regular graphs in ﬁgure 3. Furthermore, if applied to the rectangular pixel grid graph of an image, it corresponds to applying a convolution with isotropic ﬁlters. 4
Figure 4: A node feature ρ assigns to each node neighbourhood Gp (here the dark colored nodes around node p) a vector space ρpGpq (here ρpGpq “ R5) and to each local node isomorphism ψ : Gp Ñ G1 p1 a linear map
ρpψq : ρpGq Ñ ρpG1q (here swapping the third and ﬁfth row). 3.2 Local Natural Graph Networks
The idea of a Local Natural Graph Network (LNGN) is to implement a scalable GNGN layer that consists of passing messages along edges with a message passing kernel and then aggregating the incoming messages. It generalises over local invariant graph networks by making the node features transform under isomorphisms of the neighbourhood of the node and by allowing different message passing kernels on non-isomorphic edges.
Deﬁnition 3.1 (Neighbourhoods and local isomorphisms). A node neighbourhood4 Gp is a subgraph
Gp of a concrete graph G in which one node p P VpGpq is marked. Subgraph Gp inherits the node labels from G, making Gp a concrete graph itself. A local node isomorphism is a map between node neighbourhoods ψ : Gp Ñ G1 p1 such that
ψppq “ p1. Similarly, an edge neighbourhood is a concrete graph Gpq with a marked edge pp, qq and a local edge isomorphism that maps between edge neighbourhoods such that the marked edge is mapped to the marked edge. p1, consisting of a graph isomorphism ψ : Gp Ñ G1
Given a graph G, we can assign to node p P VpGq a node neighbourhood Gp in several ways. In our experiments, we choose Gp to contain all nodes in G that are at most k edges removed from p, for some natural number k, and all edges between these nodes. Similarly, we pick for edge pp, qq P EpGq neighbourhood Gpq containing all nodes at most k edges removed from p or q and all edges between these nodes. In all experiments, we chose k “ 1, unless otherwise noted. General criteria for the selection of neighbourhoods are given in App. C. Neighbourhood selections satisfying these criteria have that any global graph isomorphism φ : G Ñ G1, when restricted to a node neighbourhood
Gp equals a node isomorphism φp : Gp Ñ G1 p1 and when restricted to an edge neighbourhood Gpq equals a local edge isomorphism φpq : Gpq Ñ G1 p1q1. Furthermore, it has as a property that any local edge isomorphism ψ : Gpq Ñ G1 p1 and
ψq : Gq Ñ G1
Next, we choose a feature space for the local NGN by picking a node feature space ρ, which is a graph feature space (Def. 2.4) for node neighbourhoods in complete analogy with the previous section on global NGNs. Node feature space ρ consists of selecting for any node neighbourhood
Gp a vector space ρpGpq and for any local node isomorphism φ : Gp Ñ G1 p1, a linear bijection
ρpφq : ρpGpq Ñ ρpG1 p1q, respecting composition: ρpφq ˝ ρpφ1q “ ρpφ ˝ φ1q.
A node neighbourhood feature space ρ deﬁnes a graph feature space ˆρ on global graphs by concatenat-ing (taking the direct sum of) the node vector spaces: ˆρpGq “ pPVpGq ρpGpq. For a global feature vector v P ˆρpGq, we denote for node p P VpGq the feature vector as vp P ρpGpq. The global graph feature space assigns to global graph isomorphism φ : G Ñ G1 a linear map ˆρpφq : ˆρpGq Ñ ˆρpG1q, which permutes the nodes and applies ρ to the individual node features: p1q1 can be restricted to node isomorphisms ψp : Gp Ñ G1 q1 of the start and tail node of the edge.
À
ˆρpφqpvqφppq “ ρpφpqpvpq
Given two such node feature spaces ρ and ρ1, we can deﬁne a (linear) local NGN message passing ker-nel k by choosing for each possible edge neighbourhood Gpq a (linear) map kpq : ρpGpq Ñ ρ1pGqq, 4In the graph literature, such graphs are also called node/edge rooted graphs. 5
Figure 5: Local Natural Graph Network kernel k between node features ρ and ρ1 consists of a map kpq :
ρpGpq Ñ ρ1pGqq for each edge pp, qq, satisfying the above commuting diagrams for each edge isomorphism
ψ : Gpq Ñ G1 p1q1 and automorphism χ : Gpq Ñ Gpq. In this example, the node neighbourhoods of p, p1, q and q1 are colored dark. Edge isomorphism ψ, which swaps nodes 1 and 5, restricts to node isomorphisms ψp and ψq on input and output node neighbourhoods. The associated linear maps ρpψpq and ρ1pψqq swap second and third row and ﬁrst and second row respectively - corresponding to the reordering of the nodes in the neighbourhood by the node isomorphism. Similarly, the automorphism χ swaps nodes 3 and 5. The isomorphism leads to weight sharing between kpq and kp1q1 and the automorphism to a kernel constraint on kpq. which takes the role of W in Eq. 3. These maps should satisfy that for any edge neighbourhood isomorphism ψ : Gpq Ñ G1 p1q1 , we have that
ρ1pψqq ˝ kpq “ kp1q1 ˝ ρpψpq. (4)
In words, this “local naturality” criterion states that passing the message along an edge from p to q, then transporting with a local isomorphism to q1 yields the same result as ﬁrst transporting from p to p1, then passing the message along the edge to q1. In analogy to the global NGN layer, we have that isomorphisms between different edge neighbourhoods bring about weight sharing - with a change of basis given by Eq. 4, while automorphisms create constraints on the kernel k.
Using the local NGN kernel k between node feature spaces ρ and ρ1, we can deﬁne a global NGN layer between graph feature spaces ˆρ and ˆρ1 as:
ÿ
KGpvqq “ kpqpvpq pp,qqPEpGq (5)
The following main result, proven in Appendix D, shows that this gives a global NGN layer.
Theorem 1. Let k be a local NGN kernel between node feature spaces ρ and ρ1. Then the layer in equation 5 deﬁnes a global NGN layer between the global graph feature spaces ˆρ and ˆρ1, satisfying the global NGN naturality condition (Eq. 2).
In appendix F, we show when a local NGN is applied to a regular lattice, which is a graph with a global transitive symmetry, the NGN is equivalent to a group equivariant convolutional neural network
[Cohen and Welling, 2016], when the feature spaces and neighbourhoods are chosen appropriately.
In particular, when the graph is a square grid with edges on the diagonals, we recover an equivariant planar CNN with 3x3 kernels. Bigger kernels are achieved by adding more edges. When the graph is a grid on a locally ﬂat manifold, such as a icosahedron or another platonic solid, and the grid is a regular lattice, except at some corner points, the NGN is equivalent to a gauge equivariant CNN
[Cohen et al., 2019], except around the corners. 6
Figure 6: Local NGN message passing with an equivariant graph network kernel. The node feature vp at p can be embedded into a graph feature vpÑq of the edge neighbourhood, to which any equivariant graph neural pÑq can be projected to obtain the message from p to q, v1p network can be applied. The output graph feature v1 q .
The messages to q are invariantly aggregated to form output feature v1 q. 4 Graph Neural Network Message Parameterization
Local naturality requires weight sharing only between edges with isomorphic neighbourhoods, so, in theory, one can use separate parameters for each isomorphism class of edge neighbourhoods to parameterize the space of natural kernels. In practice, graphs such as social graphs are quite heterogeneous, so that that few edges are isomorphic and few weights need to be shared, making learning and generalization difﬁcult. This can be addressed by re-interpreting the message from p to q, kpqvp, as a function kpGpq, vpq of the edge neighbourhood Gpq and feature value vp at p, potentially generalized to being non-linear in vp, and then letting k be a neural network-based
“message network”.
Local naturality (Eq. 4) can be guaranteed, even without explicitly solving kernel constraints for each edge in the following way. By construction of the neighbourhoods, the node feature vp can always be embedded into an edge feature, a graph feature vpÑq of the edge neighbourhood Gpq. The resulting graph feature can then be processed by an appropriate equivariant graph neural network operating on
Gpq, in which nodes p and q have been distinctly marked, e.g. by a additional feature. The output graph feature v1 q at q, which is the message output. The pp,qqPE v1p messages are then aggregated using e.g. summing to create the convolution output v1 q.
This is illustrated in ﬁgure 6. It is proven in appendix E that the graph equivariance constraint on the message network ensures that the resulting message satisﬁes the local naturality constraint (Eq. 4). pÑq can be restricted to create a node feature v1p q “
ř
The selection of the type of graph feature and message network forms a large design space of natural graph networks. If, as in the example above, the node feature vp is a vector representation of the permutation of the node neighbourhood, the feature can be embedded into an invariant scalar feature of the edge neighbourhood graph by assigning an arbitrary node ordering to the edge neighbourhood and transporting from the node neighbourhood to the edge neighbourhood, setting a 0 for nodes outside the node neighbourhood. Any graph neural network with invariant features can subsequently be used to process the edge neighbourhood graph feature, whose output we restrict to obtain the message output at q. As a simplest example, we propose GCN2, which uses an invariant message passing algorithm, or Graph Convolutional Neural Network [Kipf and Welling, 2016], on graph Gpq as message network. 5 Naturality as Generalization of Equivariance
As explained in Section 2.1, the difference between a global natural graph network and an equivariant graph network is that the GNGN does not require that non-isomorphic graphs are processed similarly, while the EGN requires all graphs to be processed the same. EGNs can be understood in terms of groups, representations and equivariant maps, but the more general GNGN requires the more general framework category theory, originally developed in algebraic topology, but recently also used as a modelling tool for more applied problems [Fong and Spivak, 2018]. Its constructions give rise to an elegant framework for building equivariant message passing networks, which we call “Natural
Networks”, potentially applicable beyond graph networks. In this section, we will outline the key ingredients of natural networks. We refer a reader interested in learning more about category theory to Leinster [2016] and Fong and Spivak [2018].
A (small) category C consists of a set of objects ObpCq and for each two objects, X, Y P ObpCq, a set of abstract (homo)morphisms, or arrows, f P HomCpX, Y q, f : X Ñ Y between them. The arrows can be composed associatively into new arrows and each object has an identity arrow idX : X Ñ X with the obvious composition behaviour. When arrow f : X Ñ Y, g : Y Ñ X compose to identities on X and Y , they are isomorphisms (with f ´1 “ g). 7
A map between two categories C and D is a functor F : C Ñ D, when it maps each object
X P ObpCq to an object F pXq P ObpDq and to each morphism f : X Ñ Y in C, a morphism
F pf q : F pXq Ñ F pY q in D, such that F pg ˝ f q “ F pgq ˝ F pf q. Given two functors F, G : C Ñ D, a natural transformation η : F ñ G consists of, for each object X P ObpCq, a morphism ηX :
F pXq Ñ F pY q, such that for each morphism f : X Ñ Y in C, the following diagram commutes, meaning that the two compositions ηY ˝ F pf q, Gpf q ˝ ηX : F pXq Ñ GpY q are the same:
F pXq
F pf q
F pY q
ηX
ηY
GpXq
Gpf q
GpY q (6)
A group is an example of a category with one object and in which all arrows, corresponding to group elements, are isomorphisms. Group representations are functors from this category to the category of vector spaces, mapping the single object to a vector space and morphisms to linear bijections of this space. The functor axioms specialise exactly to the axioms of a group representation. A natural transformation between such functors is exactly an equivariant map. As the group category has only one object, the natural transformation consists of a single morphism (linear map). Equivariant Graph
Networks on graphs with N nodes are examples of these, in which the group is the permutation group
SN , the representation space are N ˆ N matrices, whose columns and rows are permuted by the group action, and the layer is a single equivariant map.
To study global NGNs, we deﬁne a category of graphs, whose objects are concrete graphs and morphisms are graph isomorphisms. The graph feature spaces (Def. 2.4) are functors from this graph category to the category Vec of vector spaces. The GNGN layer is a natural transformation between such functors, consisting of a different map for each graph, but with a naturality constraint (Eq. 6) for each graph isomorphism (including automorphisms).
Similarly, for local NGNs, we deﬁne a category C of node neighbourhoods and local node iso-morphisms and a category D of edge neighbourhoods and local edge isomorphisms. A functor
F0 : D Ñ C maps an edge neighbourhood to the node neighbourhood of the start node and an edge isomorphisms to the node isomorphism of the start node – which is well deﬁned by the construction of the neighbourhoods. Similarly, functor F1 : D Ñ C maps to the neighbourhood of the tail node of the edge. Node feature spaces are functors ρ, ρ1 : C Ñ Vec. Composition of functors leads to two functors ρ ˝ F0, ρ1 ˝ F1 : D Ñ Vec, mapping an edge neighbourhood to the input feature at the start node or the output feature at the end node. A local NGN kernel k is a natural transformation between these functors. 6