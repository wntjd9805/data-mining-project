Abstract
In this study, we demonstrate that the linear combination of atomic orbitals (LCAO), an approximation introduced by Pauling and Lennard-Jones in the 1920s, corresponds to graph convolutional networks (GCNs) for molecules. However,
GCNs involve unnecessary nonlinearity and deep architecture. We also verify that molecular GCNs are based on a poor basis function set compared with the standard one used in theoretical calculations or quantum chemical simulations.
From these observations, we describe the quantum deep ﬁeld (QDF), a machine learning (ML) model based on an underlying quantum physics, in particular the density functional theory (DFT). We believe that the QDF model can be easily understood because it can be regarded as a single linear layer GCN. Moreover, it uses two vanilla feedforward neural networks to learn an energy functional and a
Hohenberg–Kohn map that have nonlinearities inherent in quantum physics and the DFT. For molecular energy prediction tasks, we demonstrated the viability of an “extrapolation,” in which we trained a QDF model with small molecules, tested it with large molecules, and achieved high extrapolation performance. We believe that we should move away from the competition of interpolation accuracy within benchmark datasets and evaluate ML models based on physics using an extrap-olation setting; this will lead to reliable and practical applications, such as fast, large-scale molecular screening for discovering effective materials. 1

Introduction
Recently, graph convolutional networks (GCNs) [1, 2] have been applied to molecular graphs. Al-though numerous variants of the molecular GCN have been developed [3, 4, 5] (Section 2.1), they have a basic computational procedure: the GCN model (1) considers that each node (i.e., atom) of a molecular graph has a multidimensional variable (i.e., the atom feature vector), (2) uses the convolutional operation to update the feature vectors according to the graph structure deﬁned by the adjacency or distance matrix between the atoms in the molecule, and ﬁnally (3) outputs a value (e.g., the energy of the molecule) via a readout function (e.g., the sum/mean of the updated vectors).
Deep neural networks (DNNs) are used in the convolutional operation. Therefore, GCNs involve strong nonlinearity when modeling the molecular graph structure and have achieved good prediction performance on large-scale benchmark datasets, such as QM9 [6].
In this study, from the perspective of quantum physics, we demonstrate that extant molecular GCNs involve unnecessary nonlinearity and deep architecture. We ﬁrst describe an approximation of quan-tum physics introduced by Pauling and Lennard-Jones in the 1920s [7, 8, 9], which states that the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Overview of the computational graph of our proposed quantum deep ﬁeld (QDF) frame-work from the input molecule M to the output energy E. The energy functional predicts E and the
Hohenberg–Kohn (HK) map imposes the potential constraint on the electron density (cid:26). superposition of atomic wave functions (called orbitals) is based on their linear combination (Sec-tion 2.2). We demonstrate that this linear superposition/combination corresponds to the convolu-tional operation in GCNs; that is, the nonlinear DNN used in the above (2) is not required for modeling the molecular structure (Section 3). Additionally, in the linear superposition/combination, although the number of wave functions/orbitals (or basis functions) and the types of each basis func-tion are important, the molecular GCNs do not consider these points. In particular, the reason for performance degradation in deeper GCNs has been discussed recently in [2, 10]; however, it is trivial with regard to molecules. The molecular GCNs are built on a poor and incorrect basis function set compared with the standard one used in theoretical calculations or quantum chemical simulations.
From these observations, we describe the quantum deep ﬁeld (QDF), a machine learning (ML) model based on an underlying quantum physics, in particular the density functional theory (DFT) [11]. The model is separated into linear and nonlinear components. The former is the lin-ear combination of atomic orbitals (LCAO) [7, 8, 9], which is implemented through matrix–vector multiplication; the latter is the energy functional that has nonlinearity inherent in quantum physics.
This study implements this nonlinear functional using a vanilla feedforward DNN (Section 4.1).
Additionally, over the entire model, we impose a physical constraint based on the Hohenberg–Kohn theorem [12], which has nonlinearity inherent in DFT and can therefore be implemented using a vanilla feedforward DNN (Section 4.2). The components and constraint can be represented as a computational graph that learns the energy in a supervised fashion (Figure 1), and all model parame-ters are trained by back-propagation and stochastic gradient descent (SGD) algorithms (Section 4.3).
For atomization energy prediction with the QM9 dataset [6], our QDF model was competitive with a state-of-the-art model called SchNet [13] but with a million fewer parameters (Section 5.1).
Furthermore, this study demonstrated an “extrapolation” [14, 15] with regard to predicting the ener-gies of totally unknown molecules, in which we trained a QDF model with small molecules, tested it with large molecules, and achieved high extrapolation performance (Section 5.2). In a standard ML evaluation, the training and test sets have the same data distribution; in other words, the molecular sizes and structures in both sets are the same or very similar. Under this “interpolation” evalua-tion, if a highly nonlinear DNN model is trained, it can easily ﬁt to a physically meaningless but high-accuracy function that maps the input molecules into the output energies; this is because DNN can easily learn many non-linear properties inside the training data distribution [16, 17] unrelated with physics. However, the ML evaluation is mainly interested in the ﬁnal output (i.e., interpolation accuracy within a benchmark dataset), and even if the energy prediction performs well, the model parameters may not always reﬂect physics. The extrapolation can evaluate ML models focusing on physical validity; this will lead to the development of more reliable and practical ML applications. 2