Abstract
Neural Ordinary Differential Equations (NODEs) are a new class of models that transform data continuously through inﬁnite-depth architectures. The continuous nature of NODEs has made them particularly suitable for learning the dynamics of complex physical systems. While previous work has mostly been focused on
ﬁrst order ODEs, the dynamics of many systems, especially in classical physics, are governed by second order laws. In this work, we consider Second Order
Neural ODEs (SONODEs). We show how the adjoint sensitivity method can be extended to SONODEs and prove that the optimisation of a ﬁrst order coupled
ODE is equivalent and computationally more efﬁcient. Furthermore, we extend the theoretical understanding of the broader class of Augmented NODEs (ANODEs) by showing they can also learn higher order dynamics with a minimal number of augmented dimensions, but at the cost of interpretability. This indicates that the advantages of ANODEs go beyond the extra space offered by the augmented dimensions, as originally thought. Finally, we compare SONODEs and ANODEs on synthetic and real dynamical systems and demonstrate that the inductive biases of the former generally result in faster training and better performance. 1

Introduction
Residual Networks (ResNets) [8] have been an essential tool for scaling the capabilities of neural networks to extreme depths. It has been observed that the skip layers that these networks employ can be seen as an Euler discretisation of a continuous transformation [7, 12, 19]. Neural Ordinary
Differential Equations (NODEs) [3] are a new class of models that consider the limit of this discreti-sation step, naturally giving rise to an ODE that can be optimised via black-box ODE solvers. Their continuous depth makes them particularly suitable for learning and modelling the unknown dynamics of complex systems, which often cannot be described analytically.
Since the introduction of NODEs, many variants have been proposed [4, 10, 14, 17, 20, 22, 24]. While a few of these models use second order dynamics [14, 17, 24], no in-depth study on second order behaviour in Neural ODEs exists even though most dynamical systems that arise in science, such as
Newton’s equations of motion and oscillators, are governed by second order laws. To ﬁll this void, we consider Second Order Neural ODEs (SONODEs) and second order dynamics for the broader class of
∗corresponding authors 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Three learnt trajectories from the one-dimensional compact parity experiment (originally named g1d in Dupont et al. [4]). NODEs, as expected, are not able to learn the mapping (left),
ANODE(1) is able to learn it (middle), and SONODEs learn the simplest trajectory given by the solution in Equation (7) (right). models formed by Augmented Neural ODEs (ANODEs). Unlike previous approaches, which mainly focus on classiﬁcation tasks, we use low-dimensional physical systems, often with known analytic solutions, as our main arena of investigation. As we will show, the simplicity of these systems is useful in analysing the properties of these models.
To summarise our contributions, we begin by studying more closely the optimisation of SONODEs by generalising the adjoint sensitivity method to second order models. We continue by analysing how some of the properties of ANODEs extend to SONODEs and show that the latter can often
ﬁnd simpler solutions for the problems we consider. Our analysis also extends to ANODEs and demonstrates that they are capable of learning higher-order dynamics, sometimes with just a few additional dimensions. However, the way they do so has deeper implications for their functional loss landscape and their interpretability as a scientiﬁc tool. Finally, we compare SONODEs and ANODEs on real and synthetic second order dynamical systems. Our results reveal that the inductive biases in SONODEs are beneﬁcial in this setting. Our code is available online at https://github.com/ a-norcliffe/sonode. 2