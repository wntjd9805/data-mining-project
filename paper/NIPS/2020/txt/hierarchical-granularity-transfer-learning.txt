Abstract
In the real world, object categories usually have a hierarchical granularity tree.
Nowadays, most researchers focus on recognizing categories in a speciﬁc granular-ity, e.g., basic-level or sub(ordinate)-level. Compared with basic-level categories, the sub-level categories provide more valuable information, but its training annota-tions are harder to acquire. Therefore, an attractive problem is how to transfer the knowledge learned from basic-level annotations to sub-level recognition. In this paper, we introduce a new task, named Hierarchical Granularity Transfer Learning (HGTL), to recognize sub-level categories with basic-level annotations and seman-tic descriptions for hierarchical categories. Different from other recognition tasks,
HGTL has a serious granularity gap, i.e., the two granularities share an image space but have different category domains, which impede the knowledge transfer. To this end, we propose a novel Bi-granularity Semantic Preserving Network (BigSPN) to bridge the granularity gap for robust knowledge transfer. Explicitly, BigSPN con-structs speciﬁc visual encoders for different granularities, which are aligned with a shared semantic interpreter via a novel subordinate entropy loss. Experiments on three benchmarks with hierarchical granularities show that BigSPN is an effective framework for Hierarchical Granularity Transfer Learning. 1

Introduction
In the real world, object categories usually form a hierarchical tree of different granularities [5, 33, 21, 48], e.g., a hierarchical tree of bird is shown in Fig. 1. For example, a bird has a basic-category
“Albatross" and several sub(ordinate)-categories, such as “Footed Albatross" and “Sooty Albatross" species. Compared with basic-level categories, the sub-level categories contain more information, but the annotations are also harder to obtain [41], which require expert taxonomy knowledge to distinguish subtle differences. Thus, how to recognize sub-categories without sub-level image annotations is an interesting and important problem.
To address this issue, we introduce a new task of Hierarchical Granularity Transfer Learning (HGTL), which targets to recognize the subordinate-level categories with only basic-level image annotations and semantic descriptions for hierarchical categories, e.g., attributes [39], as shown in Fig. 1. The insight of HGTL is inspired by the semantic cognition of human. For example, when informed that the “Footed Albatross" has brown wing and the “Sooty Albatross" has black wing, human can distinguish these two sub-species visually.
Among the existing visual recognition tasks, the ﬁne-grained visual categorization (FGVC) [16, 17, 49, 24], domain adaptation (DA) [27–29], and zero-shot learning (ZSL) [12, 13, 39, 23] are most related to HGTL. Different from FGVC that depends on sub-level image annotations, HGTL requires only basic-level annotations and extra semantic information. Compared with DA whose categories of two domains are overlapped, HGTL has disjoint category domains, i.e., basic-categories and
∗Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: An example of Hierarchical Granularity Transfer Learning (HGTL). Given the basic-level image annotations and category descriptions for hierarchical categories, HGTL aims to recognize the subordinate categories. sub-categories. ZSL can recognize new categories by transferring the learned visual and semantic embedding functions from seen to unseen domains. However, in HGTL, each image has two disjoint categories, thus a shared visual embedding function cannot well model the visual distributions of two category granularities. In summary, HGTL presents a new challenge of disjoint category domains of two granularities, which has not been explored in the existing recognition methods.
In this paper, we propose a novel Bi-granularity Semantic Preserving Network (BigSPN) to solve the
HGTL by constructing two speciﬁc visual encoders for respective basic- and sub-domain categories.
The core motivation of BigSPN is to leverage the semantic relationship between two category domains for visual knowledge transfer. To this end, BigSPN ﬁrst learns a visual encoder and a semantic interpreter in the basic domain via the semantic-visual alignment. Since the semantic information can associate two domains, the semantic interpreter is directly transferred to the sub-domain. Then, a new part-based visual encoder is developed to capture the subtle visual difference for sub-category domain. Due to unavailable sub-level image annotations, a subordinate entropy loss is developed to train the new visual encoder to be aligned with the corresponding sub-level semantics, by solving a multi-instance optimization problem. Finally, the sub-domain recognition becomes a nearest neighbor searching problem between part-based visual representations and semantic embeddings for sub-categories. Compared with previous recognition models, BigSPN can preserve the visual distributions for both basic- and sub-domains via two separate visual encoders.
The overall contributions of this paper are summarized by: a) to our best knowledge, we introduce a new task of Hierarchical Granularity Transfer Learning (HGTL) that targets to transfer knowledge between hierarchical categories without subordinate category annotations; b) we propose a novel
Bi-granularity Semantic Preserving Network (BigSPN) to bridge the granularity gap for HGTL, by constructing speciﬁc visual encoders for hierarchical categories. Due to unavailable sub-level image annotations, the two visual encoders are learned via a shared semantic interpreter and a subordinate entropy loss; c) the evaluations on three benchmarks with hierarchical categories, i.e.,
CUB-HGTL, AWA2-HGTL, and Flowers-HGTL, demonstrate that the BigSPN is a robust framework for Hierarchical Granularity Transfer Learning. 2