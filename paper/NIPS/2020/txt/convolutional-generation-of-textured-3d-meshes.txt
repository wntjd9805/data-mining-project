Abstract
While recent generative models for 2D images achieve impressive visual results, they clearly lack the ability to perform 3D reasoning. This heavily restricts the degree of control over generated objects as well as the possible applications of such models. In this work, we bridge this gap by leveraging recent advances in differentiable rendering. We design a framework that can generate triangle meshes and associated high-resolution texture maps, using only 2D supervision from single-view natural images. A key contribution of our work is the encoding of the mesh and texture as 2D representations, which are semantically aligned and can be easily modeled by a 2D convolutional GAN. We demonstrate the efﬁcacy of our method on Pascal3D+ Cars and CUB, both in an unconditional setting and in settings where the model is conditioned on class labels, attributes, and text. Finally, we propose an evaluation methodology that assesses the mesh and texture quality separately. 1

Introduction
State-of-the-art image synthesis models based on the GAN framework [13] nowadays achieve photo-realistic results thanks to a series of key contributions in this area [19, 34, 35, 65, 25]. A recent trend in this ﬁeld has been to make generative models more controllable and of better use for downstream applications. This includes works that condition generative models on class labels [35, 65, 4], text
[66, 67, 59, 30], input images [70, 22], as well as structured scene layouts such as semantic maps
[53, 41, 36], bounding boxes [69, 20, 47], and scene graphs [23]. While these approaches achieve impressive visual results, they are all based on architectures that fundamentally ignore the concept of image formation. Real-world images depict 2D projections of 3D objects, and explicitly considering this aspect would lead to better generative models that can provide disentangled control over shape, color, pose, lighting, and can better handle spatial phenomena such as occlusions. A recent trend to account for such effects has been to disentangle factors of variation during the generation process in the hope of making it more interpretable [63, 44, 25, 26]. These approaches potentially learn a hierarchical decomposition of objects, and in some settings (e.g. faces) they can provide some degree of control over pose. However, the pose disentanglement assumptions made by these approaches have been shown to be unrealistic without some form of supervision [32], and they have not reached the degree of controllability that a native 3D representation would be capable of. More recent efforts have focused on incorporating 3D information into the model architecture, using either rigid transfor-mations in feature space [38] or analysis-by-synthesis [37]. These approaches represent an interesting middle ground between 2D and 3D generators, although their objective remains 2D image synthesis.
In this work, we propose a GAN framework for generating triangle meshes and associated textures, using only 2D supervision from single-view natural images. In terms of applications, our approach could greatly facilitate content creation for art, movies, video games, virtual reality, as well as 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
augment the possible downstream applications of generative models. We leverage recent advances in differentiable rendering [33, 27, 31, 5] to incorporate 3D reasoning into our approach. In particular, we initially adopt a reconstruction framework to estimate meshes through a representation we name convolutional mesh which consists of a displacement map that deforms a mesh template in its tangent space. This representation is particularly well-suited for 2D convolutional architectures as both the mesh and its texture share the same topology, and the mesh beneﬁts from the spatial smoothness property of convolutions. We then project natural images onto the UV map (mapping between texture coordinates and mesh vertices) and reduce the problem to a 2D modeling task where the representation is independent of the pose of the object. Finally, we train a 2D convolutional GAN in
UV space where inputs to the discriminator are masked in order to deal with occlusions.
Our model generates realistic meshes and can easily scale to high-resolution textures (512×512 and possibly more) owing to the precise semantic alignment between maps in UV space, without requiring progressive growing [25]. Most importantly, since our model is based exclusively on 2D convolutions, we can easily adapt ideas from state-of-the-art GAN methods for 2D images, and showcase our approach under a wide range of settings: conditional generation from class labels, attributes, text (with and without attention), as well as unconditional generation. We evaluate our approach on Pascal3D+
Cars [57] and CUB Birds [52], and propose metrics for evaluating FID scores [19] on meshes and textures separately as well as collectively. In summary, we make the following contributions:
• A novel convolutional mesh representation that is smooth by deﬁnition, and alongside the texture, is easy to model using standard 2D convolutional GAN architectures.
• A GAN framework for producing textured 3D meshes from a pose-independent 2D representation.
In particular, in a GAN setting, we are the ﬁrst to demonstrate full generation of textured triangle meshes using 2D supervision from natural images, whereas prior attempts have focused on limited settings supervised on synthetic data without a principled texture learning strategy.
• We demonstrate conditional generation of 3D meshes from text (with and without an attention mechanism) and show that our model provides disentangled control over shape and appearance.
• We release our code and pretrained models at https://github.com/dariopavllo/convmesh. 2