Abstract
Out-of-distribution (OOD) testing is increasingly popular for evaluating a machine learning system’s ability to generalize beyond the biases of a training set. OOD benchmarks are designed to present a different joint distribution of data and labels between training and test time. VQA-CP has become the standard OOD benchmark for visual question answering, but we discovered three troubling practices in its current use. First, most published methods rely on explicit knowledge of the construction of the OOD splits. They often rely on “inverting” the distribution of labels, e.g. answering mostly “yes” when the common training answer is “no”.
Second, the OOD test set is used for model selection. Third, a model’s in-domain performance is assessed after retraining it on in-domain splits (VQA v2) that exhibit a more balanced distribution of labels. These three practices defeat the objective of evaluating generalization, and put into question the value of methods speciﬁcally designed for this dataset. We show that embarrassingly-simple methods, including one that generates answers at random, surpass the state of the art on some question types. We provide short- and long-term solutions to avoid these pitfalls and realize the beneﬁts of OOD evaluation. 1

Introduction
Goodhart’s law: When a measure becomes a target, it ceases to be a good measure.
The practical value of a machine learning (ML) system is strongly related to its capacity to generalize, i.e. to produce relevant outputs for data beyond its training set. The common paradigm in learning theory [42] assumes that the training and test data are drawn as independent and identically distributed (IID) samples. Therefore, most datasets are built following this IID assumption, such that data points are assigned randomly to training or test splits. For many tasks however, this fails to assess whether an ML system adequately generalizes, or whether it has simply captured the idiosyncrasies of a dataset, including spurious correlations that manifest in both the training and test sets [28].
Out-of-distribution (OOD) testing is an increasingly popular method for evaluating generalization [3, 4, 7, 23, 25, 29]. In this paper, we use “OOD” to refer to splits designed such that the joint distribution of inputs and labels differs between the training and testing sets. The differences in this distribution concern features of the data that are irrelevant to the task of interest, e.g. the background in an image recognition task. Such irrelevant factors can be spuriously correlated with the correct labels. They form “dataset biases” and other statistical patterns that a robust model should not rely on. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: In the VQA-CP dataset, the distribution of answers given a question preﬁx differs between training and testing. We show that many existing methods exploit the fact that the training and test distributions are approximately inverse of each other. This is made possible through the bad practice of using the OOD test set for model selection. Moreover, the issue is completely hidden from the in-domain evaluation typically performed after retraining on the VQA v2 dataset, because its distribution of answers is more uniform.
This paper takes a close look at VQA-CP [3], an OOD benchmark for visual question answering (VQA). In VQA, a model is provided with an image and a related question and must produce a relevant answer. VQA is usually approached as a classiﬁcation problem over a large set (1000+) of candidate answers, and usually trained with a large dataset of questions and correct answers [39, 44].
These datasets are produced by human annotators and contain strong biases. For example, most questions of the form Is there a ... in the image ? are correctly answered with yes [6]. The VQA-CP dataset was designed to evaluate models in an OOD setting. It was built by re-splitting the VQA v2 dataset [20] such that the joint distribution of answers and question preﬁxes differs between training and testing (see Fig. 1). Considerable effort has been put into methods speciﬁcally designed to improve performance on VQA-CP [3, 10, 12, 22, 32, 36, 41]. Unfortunately, we discovered multiple
ﬂaws in the experimental setup and design of many of these methods.
This paper exposes three critical issues. First, almost all published methods evaluated on VQA-CP are designed for high performance speciﬁcally on its OOD test set. They rely on the known construction procedure of the OOD splits (see Fig. 1). Second, since the dataset has no ofﬁcial validation set, almost all methods use the OOD test set for model selection. Third, the common practice is to verify that a model performs well on in-domain data, but this is performed after retraining on standard splits (VQA v2), which exhibit a different label distribution.
This paper discusses how these issues defeat the purpose of an OOD evaluation. The situation is a striking example of Goodhart’s law: the metrics of the benchmark are gamed in a way that defeats its original, well-intended purpose. Instead of making advances towards generalization in vision-and-language, the performance on VQA-CP has been treated as a standalone objective. This puts the value of many published methods into question. To demonstrate this point, we present several embarrassingly-simple baselines (including one that draws answers at random) that surpass the state of the art on some or all question types on VQA-CP.
In summary, the contributions of this paper are as follows. 1. We highlight three major ﬂaws in the experimental setup and in the design of methods for VQA-CP, and we discuss how they amount to subtly cheating the OOD evaluation. 2. To demonstrate the point concretely, we describe and evaluate embarrassingly-simple methods that surpass published results on VQA-CP on some or all question types. 3. We provide guidelines for the continued use of VQA-CP to best capture the beneﬁts of OOD evaluation. We also point at promising directions for the design of future benchmarks. 2