Abstract
Unsupervised and self-supervised learning approaches have become a crucial tool to learn representations for downstream prediction tasks. While these approaches are widely used in practice and achieve impressive empirical gains, their theoret-ical understanding largely lags behind. Towards bridging this gap, we present a unifying perspective where several such approaches can be viewed as imposing a regularization on the representation via a learnable function using unlabeled data. We propose a discriminative theoretical framework for analyzing the sam-ple complexity of these approaches, which generalizes the framework of [3] to allow learnable regularization functions. Our sample complexity bounds show that, with carefully chosen hypothesis classes to exploit the structure in the data, these learnable regularization functions can prune the hypothesis space, and help reduce the amount of labeled data needed. We then provide two concrete examples of functional regularization, one using auto-encoders and the other using masked self-supervision, and apply our framework to quantify the reduction in the sample complexity bound of labeled data. We also provide complementary empirical results to support our analysis. 1

Introduction
Advancements in machine learning have resulted in large prediction models, which need large amounts of labeled data for effective learning. Expensive label annotation costs have increased the popularity of unsupervised (or self-supervised) representation learning techniques using additional unlabeled data. These techniques learn a representation function on the input, and a prediction function over the representation for the target prediction task. Unlabeled data is utilised by posing an auxiliary unsupervised learning task on the representation, e.g., using the representation to reconstruct the input. Some popular examples of the auxiliary task are auto-encoders [45, 4], sparse dictionaries [46], masked self-supervision [13], manifold learning [11], among others [9]. These approaches have been extensively used in applications in domains such as computer vision (e.g., [56, 61, 16]) and natural language processing (e.g., [58, 13, 34]), and have achieved impressive empirical performance.
An important takeaway from these empirical studies is that learning representations using unlabeled data can drastically reduce the size of labeled data needed for the prediction task. In contrast to the popularity and impressive practical gains of these representation learning approaches, there have been far fewer theoretical studies focused at understanding them, most of which have been speciﬁc to individual approaches. While intuition dictates that the unlabeled and labeled data distributions along with the choice of models are crucial factors which govern the empirical gains, theoretically there is still ambiguity over questions like "When can the auxiliary task over the unlabeled data help the target prediction task? How much can it reduce the sample size of the labeled data by?"
∗ Work completed at the University of Wisconsin-Madison 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we take a step towards improving the theoretical understanding of the beneﬁts of learning representations for the target prediction task via an auxiliary task. We focus on analyzing the sample complexity of labeled and unlabeled data for this learning paradigm. Such an analysis can help to identify conditions when a signiﬁcant reduction in sample complexity of the labeled data can be achieved. Arguably, this is one of the most fundamental questions for this learning paradigm, and existing literature on this has been limited and scattered, speciﬁc to individual approaches.
Our contribution is to propose a uniﬁed perspective where several representation learning approaches can be viewed as if they impose a regularization on the representation via a learnable regularization function. Under this paradigm, representations are learned jointly on unlabeled and labeled data.
The former is used in the auxiliary task to learn the representation and the regularization function.
The latter is used in the target prediction task to learn the representation and the prediction function.
Henceforth, we refer to this paradigm as representation learning via functional regularization.
In particular, we present a PAC-style discriminative framework [53] to bound the sample complexities of labeled and unlabeled data under different assumptions on the models and data distributions. This is inspired from the work of [3] which bounds the sample complexities of labeled and unlabeled data for semi-supervised learning. Our generalized framework allows learnable regularization functions and thus uniﬁes multiple unsupervised (or self-supervised) representation learning approaches. Our analysis shows that functional regularization with unlabeled data can prune the model hypothesis class for learning representations, reducing the labeled data required for the prediction task.
To demonstrate the application of our framework, we construct two concrete examples of functional regularization, one using auto-encoder and the other using masked self-supervision. These speciﬁc functional regularization settings allow us to quantify the reduction in the sample bounds of labeled data more explicitly. While our main focus is the theoretical framework, we also provide comple-mentary empirical support through experiments on synthetic and real data. Now we ﬁrst discuss related work followed by a formal problem description. Then we present our theoretical framework involving sample complexity bounds followed by the concrete examples with empirical support. 2