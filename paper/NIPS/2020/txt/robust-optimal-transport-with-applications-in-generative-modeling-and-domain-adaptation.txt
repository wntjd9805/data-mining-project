Abstract
Optimal Transport (OT) distances such as Wasserstein have been used in several areas such as GANs and domain adaptation. OT, however, is very sensitive to outliers (samples with large noise) in the data since in its objective function, every sample, including outliers, is weighed similarly due to the marginal constraints. To remedy this issue, robust formulations of OT with unbalanced marginal constraints have previously been proposed. However, employing these methods in deep learning problems such as GANs and domain adaptation is challenging due to the instability of their dual optimization solvers. In this paper, we resolve these issues by deriving a computationally-efﬁcient dual form of the robust OT optimization that is amenable to modern deep learning applications. We demonstrate the effectiveness of our formulation in two applications of GANs and domain adaptation. Our approach can train state-of-the-art GAN models on noisy datasets corrupted with outlier distributions. In particular, the proposed optimization method computes weights for training samples reﬂecting how difﬁcult it is for those samples to be generated in the model. In domain adaptation, our robust OT formulation leads to improved accuracy compared to the standard adversarial adaptation methods. Our code is available at https://github.com/yogeshbalaji/robustOT. 1

Introduction
Estimating distances between probability distributions lies at the heart of several problems in machine learning and statistics. A class of distance measures that has gained immense popularity in several machine learning applications is Optimal Transport (OT) [27]. In OT, the distance between two probability distributions is computed as the minimum cost of transporting a source distribution to the target distribution under some transportation cost function. Optimal transport enjoys several nice properties including structure preservation, existence in smooth and non-smooth settings, being well deﬁned for discrete and continuous distributions [27], etc.
Two recent applications of OT in machine learning include generative modeling and domain adap-tation. In Wasserstein GAN [1], a generative model is trained by minimizing the (approximate)
Wasserstein distance between real and generative distributions. In the dual form, this objective 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Clean dataset (b) Dataset with outliers (c) Dataset with outliers
Figure 1: Visualizing couplings of Wasserstein computation between two distributions shown in red and blue. In (a), we show the couplings when no outliers are present. In (b), we show the couplings when 5% outliers are added to the data. The Wasserstein distance increases signiﬁcantly indicating high sensitivity to outliers. In (c), we show the couplings produced by the Robust Wasserstein measure. Our formulation effectively ignores the outliers yielding a Wasserstein estimate that closely approximates the true Wasserstein distance. reduces to a two-player min-max game between a generator and a discriminator. Similar ideas involving distance minimization between source and target feature distributions are common in domain adaptation [10]. In [24, 2], Wasserstein distance is used as the choice of distance measure for minimizing the domain gap.
One of the fundamental shortcomings of optimal transport is its sensitivity to outlier samples. By outliers, we mean samples with large noise. In the OT optimization, to satisfy the marginal constraints between the two input distributions, every sample is weighed equally in the feasible transportation plans. Hence, even a few outlier samples can contribute signiﬁcantly to the OT objective. This leads to poor estimation of distributional distances when outliers are present. An example is shown in
Fig. 1, where the distances between distributions shown in red and blue are computed. In the absence of outliers (Fig. 1(a)), proper couplings (shown in green) are obtained. However, even in the presence of a very small fraction of outliers (as small as 5%), poor couplings arise leading to a large change in the distance estimate (Fig. 1(b)).
The OT sensitivity to outliers is undesirable, especially when we deal with large-scale datasets where the noise is inevitable. This sensitivity is a consequence of exactly satisfying the marginal constraints in OT’s objective. Hence, to boost OT’s robustness against outliers, we propose to utilize recent formulations of unbalanced optimal transport [6, 13] which relax OT’s marginal constraints. The authors in [6, 13] provide an exact dual form for the unbalanced OT problem. However, we have found that using this dual optimization in large-scale deep learning applications such as GANs results in poor convergence and an unstable behaviour (see Section 3.1 and the appendix for details).
To remedy this issue, in this work, we derive a computationally efﬁcient dual form for the unbalanced
OT optimization that is suited for practical deep learning applications. Our dual simpliﬁes to a weighted OT objective, with low weights assigned to outlier samples. These instance weights can also be useful in interpreting the difﬁculty of input samples for learning a given task. We develop two solvers for this dual problem based on either a discrete formulation or a continuous stochastic relaxation. These solvers demonstrate high stability in large-scale deep learning applications.
We show that, under mild assumptions, our robust OT measure (which is similar in form to the unbalanced OT) is upper bounded by a constant factor of the true OT distance (OT ignoring outliers) for any outlier distribution. Hence, our robust OT can be used for effectively handling outliers. This is visualized in Figure 1(c), where couplings obtained by robust OT effectively ignores outlier samples, yielding a good estimate of the true OT distance. We demonstrate the effectiveness of the proposed robust OT formulation in two large-scale deep learning applications of generative modeling and domain adaptation. In generative modeling, we show how robust Wasserstein GANs can be trained using state-of-the-art GAN architectures to effectively ignore outliers in the generative distrubution.
In domain adaptation, we utilize the robust OT framework for the challenging task of synthetic to real adaptation, where our approach improves adversarial adaptation techniques by ∼ 5%. 2
2