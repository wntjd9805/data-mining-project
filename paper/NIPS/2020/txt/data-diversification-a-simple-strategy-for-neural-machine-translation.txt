Abstract
We introduce Data Diversiﬁcation: a simple but effective strategy to boost neural machine translation (NMT) performance. It diversiﬁes the training data by using the predictions of multiple forward and backward models and then merging them with the original dataset on which the ﬁnal NMT model is trained. Our method is applicable to all NMT models.
It does not require extra monolingual data like back-translation, nor does it add more computations and parameters like ensembles of models. Our method achieves state-of-the-art BLEU scores of 30.7 and 43.7 in the WMT’14 English-German and English-French translation tasks, respectively. It also substantially improves on 8 other translation tasks: 4 IWSLT tasks (English-German and English-French) and 4 low-resource translation tasks (English-Nepali and English-Sinhala). We demonstrate that our method is more effective than knowledge distillation and dual learning, it exhibits strong correlation with ensembles of models, and it trades perplexity off for better BLEU score. 1

Introduction
The invention of novel architectures for neural machine translation (NMT) has been fundamental to the progress of the ﬁeld. From the traditional recurrent approaches [22, 14], NMT has advanced to self-attention method [24], which is more efﬁcient and powerful and has set the standard for many other NLP tasks [3]. Another parallel line of research is to devise effective methods to improve
NMT without intensive modiﬁcation to model architecture, which we shall refer to as non-intrusive extensions. Examples of these include the use of sub-word units to solve the out-of-vocabulary (OOV) problem [18] or exploiting extra monolingual data to perform semi-supervised learning using back-translation [17, 4]. One major advantage of these methods is the applicability to most existing
NMT models as well as potentially future architectural advancements with little change. Thus, non-intrusive extensions are used in practice to avoid the overhead cost of developing new architectures and enhance the capability of existing state-of-the-art models.
In this paper, we propose Data Diversiﬁcation1, a simple but effective way to improve machine translation consistently and signiﬁcantly. In this method, we ﬁrst train multiple models on both backward (target→source) and forward (source→target) translation tasks. Then, we use these models to generate a diverse set of synthetic training data from both lingual sides to augment the original data.
Our approach is inspired from and a combination of multiple well-known strategies: back-translation, ensemble of models, data augmentation and knowledge distillation for NMT. 1Code: https://github.com/nxphi47/data_diversiﬁcation 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our method establishes the state of the art (SOTA) in the WMT’14 English-German and English-French translation tasks with 30.7 and 43.7 BLEU scores, respectively.2 Furthermore, it gives 1.0-2.0
BLEU gains in 4 IWSLT tasks (English↔German and English↔French) and 4 low-resource tasks (English↔Sinhala and English↔Nepali). We demonstrate that data diversiﬁcation outperforms other related methods – knowledge distillation [13] and dual learning [27], and is complementary to back-translation [17] in semi-supervised setup. Our analysis further reveals that the method is correlated with ensembles of models and it sacriﬁces perplexity for better BLEU. 2