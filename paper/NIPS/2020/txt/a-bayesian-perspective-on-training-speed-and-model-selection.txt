Abstract
We take a Bayesian perspective to illustrate a connection between training speed and the marginal likelihood in linear models. This provides two major insights:
ﬁrst, that a measure of a model’s training speed can be used to estimate its marginal likelihood. Second, that this measure, under certain conditions, predicts the relative weighting of models in linear model combinations trained to minimize a regression loss. We verify our results in model selection tasks for linear models and for the inﬁnite-width limit of deep neural networks. We further provide encouraging empirical evidence that the intuition developed in these settings also holds for deep neural networks trained with stochastic gradient descent. Our results suggest a promising new direction towards explaining why neural networks trained with stochastic gradient descent are biased towards functions that generalize well. 1

Introduction
Choosing the right inductive bias for a machine learning model, such as convolutional structure for an image dataset, is critical for good generalization. The problem of model selection concerns itself with identifying good inductive biases for a given dataset. In Bayesian inference, the marginal likelihood (ML) provides a principled tool for model selection. In contrast to cross-validation, for which computing gradients is cumbersome, the ML can be conveniently maximised using gradients when its computation is tractable. Unfortunately, computing the marginal likelihood for complex models such as neural networks is typically intractable. Workarounds such as variational inference suffer from expensive optimization of many parameters in the variational distribution and differ signiﬁcantly from standard training methods for Deep Neural Networks (DNNs), which optimize a single parameter sample from initialization. A method for estimating the ML that closely follows standard optimization schemes would pave the way for new practical model selection procedures, yet remains an open problem.
A separate line of work aims to perform model selection by predicting a model’s test set performance.
This has led to theoretical and empirical results connecting training speed and generalization error
[17, 21]. This connection has yet to be fully explained, as most generalization bounds in the literature depend only on the ﬁnal weights obtained by optimization, rather than on the trajectory taken during training, and therefore are unable to capture this relationship. Understanding the link between training speed, optimization and generalization thus presents a promising step towards developing a theory of generalization which can explain the empirical performance of neural networks.
In this work, we show that the above two lines of inquiry are in fact deeply connected. We investigate the connection between the log ML and the sum of predictive log likelihoods of datapoints, condi-tioned on preceding data in the dataset. This perspective reveals a family of estimators of the log
†OATML Group, University of Oxford. Correspondence to clare.lyle@cs.ox.ac.uk
‡Imperial College London 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
ML which depend only on predictions sampled from the posterior of an iterative Bayesian updating procedure. We study the proposed estimator family in the context of linear models, where we can conclusively analyze its theoretical properties. Leveraging the fact that gradient descent can produce exact posterior samples for linear models [31] and the inﬁnite-width limit of deep neural networks
[7, 26], we show that this estimator can be viewed as the sum of a subset of the model’s training losses in an iterative optimization procedure. This immediately yields an interpretation of marginal likelihood estimation as measuring a notion of training speed in linear models. We further show that this notion of training speed is predictive of the weight assigned to a model in a linear model combination trained with gradient descent, hinting at a potential explanation for the bias of gradient descent towards models that generalize well in more complex settings.
We demonstrate the utility of the estimator through empirical evaluations on a range of model selection problems, conﬁrming that it can effectively approximate the marginal likelihood of a model. Finally, we empirically evaluate whether our theoretical results for linear models may have explanatory power for more complex models. We ﬁnd that an analogue of our estimator for DNNs trained with stochastic gradient descent is predictive of both ﬁnal test accuracy and the ﬁnal weight assigned to the model after training a linear model combination. Our ﬁndings in the deep learning setting hint at a promising avenue of future work in explaining the empirical generalization performance of DNNs. 2