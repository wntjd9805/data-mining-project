Abstract
We present an efﬁcient and practical (polynomial time) algorithm for online pre-diction in unknown and partially observed linear dynamical systems (LDS) under stochastic noise. When the system parameters are known, the optimal linear predic-tor is the Kalman ﬁlter. However, in unknown systems, the performance of existing predictive models is poor in important classes of LDS that are only marginally stable and exhibit long-term forecast memory. We tackle this problem by bounding the generalized Kolmogorov width of the Kalman ﬁlter coefﬁcient set. This moti-vates the design of an algorithm, which we call spectral LDS improper predictor (SLIP), based on conducting a tight convex relaxation of the Kalman predictive model via spectral methods. We provide a ﬁnite-sample analysis, showing that our algorithm competes with the Kalman ﬁlter in hindsight with only logarithmic regret.
Our regret analysis relies on Mendelson’s small-ball method, providing sharp error bounds without concentration, boundedness, or exponential forgetting assumptions.
Empirical evaluations demonstrate that SLIP outperforms state-of-the-art methods in LDS prediction. Our theoretical and experimental results shed light on the conditions required for efﬁcient probably approximately correct (PAC) learning of the Kalman ﬁlter from partially observed data. 1

Introduction
Predictive models based on linear dynamical systems (LDS) have been successfully used in a wide range of applications with a history of more than half a century. Example applications in AI-related areas range from control systems and robotics [11] to natural language processing [4], healthcare
[41], and computer vision [6, 7]. Other applications are found throughout the physical, biological, and social sciences in areas such as econometrics, ecology, and climate science.
The evolution of a discrete-time LDS is described by the following state-space model with t 1:
≥ ht+1 = Aht + Bxt + ηt, yt = Cht + Dxt + ζt, where ht are the latent states, xt are the inputs, yt are the observations, and ηt and ζt are process and measurement noise, respectively.
When the system parameters are known, the optimal linear predictor is the Kalman ﬁlter. When they are unknown, a common approach for prediction is to ﬁrst estimate the parameters of a Kalman
ﬁlter and then use them to predict system evolution. Direct parameter estimation usually involves solving a non-convex optimization problem, such as in the expectation maximization (EM) algorithm, whose theoretical guarantees may be difﬁcult [55]. Several recent works have studied ﬁnite-sample properties of LDS identiﬁcation. For fully observed LDS, system identiﬁcation is shown to be possible without a strict stability (ρ(A) < 1) assumption, where ρ(A) is the spectral radius of A [46, 43, 13]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
For partially observed LDS, methods such as gradient descent [19] and subspace identiﬁcation [49] are developed, whose performances degrade polynomially when ρ(A) is close to one.
We focus on constructing LDS predictors without identifying the parameters. For a stochastic LDS, the recent work of Tsiamis and Pappas [48] is most related to our question. Their method performs linear regression over a ﬁxed-length lookback window to predict the next observation yt given its causal history. Without using a mixing-time argument, [48] showed logarithmic regret with respect to the Kalman ﬁlter in hindsight even when the system is marginally stable (ρ(A) 1). However, the prediction performance deteriorates if the true Kalman ﬁlter exhibits long forecast memory.
≤
To illustrate the notion of forecast memory, we recall the recursive form of the (stationary) Kalman
ﬁlter for 1 t
≤
≤
T , where T is the ﬁnal horizon [25, Chapter 9]:
ˆht+1|t = Aˆht|t−1 + Bxt + K(yt
Cˆht|t−1 −
−
= (A
KC)ˆht|t−1 + Kyt + (B (2) where ˆht|t−1 denotes the optimal linear predictor of ht given all the observations y1, y2, . . . , yt−1 and inputs x1, x2, . . . , xt−1. The matrix K is called the (predictive) Kalman gain.1 The Kalman predictor of yt given y1, y2, . . . , yt−1 and x1, x2, . . . , xt, denoted by ˆyt|t−1, is Cˆht|t−1 + Dxt. Assume that
ˆh1|0 = 0. By expanding Equation (2), we obtain
KD)xt,
−
−
Dxt) (1) mt (cid:44) ˆyt|t−1 = t−1 (cid:88) i=1
CGt−i−1Kyi + t−1 (cid:88) i=1
CGt−i−1(B
−
KD)xi + Dxt, (3)
− where G = A
KC. In an LDS, the transition matrix A controls how fast the process mixes—i.e., how fast the marginal distribution of yt becomes independent of y1. However, it is G that controls how long the forecast memory is. Indeed, it was shown in [25, chap. 14] that if the spectral radius
ρ(G) is close to one, then the performance of a linear predictor that uses only yt−k to yt−1 for ﬁxed k in predicting yt would be substantially worse than that of a predictor that uses all information y1 up to yt−1 as t
. Conceivably, the sample size required by the algorithm of Tsiamis and Pappas
[48] explodes to inﬁnity as ρ(G) 1, since the predictor uses a ﬁxed-length lookback window to conduct linear regression.
→ ∞
→
The primary reason to focus on long-term forecast memory is the ubiquity of long-term dependence in real applications, where it is often the case that not all state variables change according to a similar timescale2 [5]. For example, in a temporal model of the cardiovascular system, arterial elasticity changes on a timescale of years, while the contraction state of the heart muscles changes on a timescale of milliseconds; see Appendix I for a discussion on systems with long forecast memory.
Designing provably computationally and statistically efﬁcient algorithms in the presence of long-term forecast memory is challenging, and in some cases, impossible. A related problem studied in the literature is the prediction of auto-regressive model with order inﬁnity: AR(
). Without imposing structural assumptions on the coefﬁcients of an AR(
) model, there is no hope to guarantee vanishing prediction error. One common approach to obtain a smaller representation is to make an exponential forgetting assumption to justify ﬁnite-memory truncation. This approach has been used in approximating AR(
) with decaying coefﬁcients [18], LDS identiﬁcation [19], and designing predictive models for LDS [48, 26]. Inevitably, the performance of these methods degrade by either losing long-term dependence information or requiring very large sample complexity as ρ(G) (and sometimes, ρ(A)) gets closer to one.
∞
∞
∞
However, the Kalman predictor in (3) does seem to have a structure and in particular, the coefﬁcients are geometric in G, which gives us hope to exploit it. Our main contributions are the following: 1. Generalized Kolmogorov width and spectral methods: We analyze the generalized Kolmogorov width (deﬁned in Section 5.1) of the Kalman ﬁlter coefﬁcient set. In Theorem 2, we show that when the matrix G is diagonalizable with real eigenvalues, the Kalman ﬁlter coefﬁcients can be approximated 1One can interpret the Kalman ﬁlter Equation (1) as linear combinations of optimal predictor given existing data Aˆht|t−1, known drift Bxt, and ampliﬁed innovation K(yt − Cˆht|t−1 − Dxt), where the term yt −
Cˆht|t−1 − Dxt, called the innovation of process yt, measures how much additional information yt brings compared to the known information of observations up to yt−1. 2Indeed, a common practice is to set the timescale to be small enough to handle the fastest-changing variables. 2
by a linear combination of polylog (T ) ﬁxed known ﬁlters with 1/ poly (T ) error. It then motivates the algorithm design of linear regression based on the transformed features, where we ﬁrst transform the observations y1:t and inputs x1:t for 1
T via these ﬁxed ﬁlters. In some sense, we use
≤ the transformed features to achieve a good bias-variance trade-off: the small number of features guarantees small variance and the generalized Kolmogorov width bound guarantees small bias. We show that the ﬁxed known ﬁlters can be computed efﬁciently via spectral methods. Hence, we choose spectral LDS improper predictor (SLIP) as the name for our algorithm.
≤ t 2. Difﬁculty of going beyond real eigenvalues: We show in Theorem 2 that if the dimension of matrix G in (3) is at least 2, then without assuming real eigenvalues one has to use at least Ω(T )
ﬁlters to approximate an arbitrary Kalman ﬁlter. In other words, the Kalman ﬁlter coefﬁcient set is very difﬁcult to approximate via linear subspaces in general. This suggests some inherent difﬁculty of constructing provable algorithms for prediction in an arbitrary LDS. 3. Logarithmic regret uniformly for ρ(G) ≤ 1, ρ(A) ≤ 1: When ρ(A) or ρ(G) is equal to one the process does not mix and common assumptions regarding boundedness, concentration, or stationarity do not hold. Recently, Mendelson [38] showed that such assumptions are not required and learning is possible under a milder assumption referred to as the small-ball condition. In Theorem 1, we leverage this idea as well as results on self-normalizing martingales and show a logarithmic regret bound for our algorithm uniformly for ρ(G) 1 and ρ(A) 1.
≤
≤ 4. Experimental results: We demonstrate in simulations that our algorithm performs better than the state-of-the-art in LDS prediction algorithms. In Section 7, we compare the performance of our algorithm to wave ﬁltering [22] and truncated ﬁltering [48]. 2