Abstract
We are interested in understanding how well Transformer language models (TLMs) can perform reasoning tasks when trained on knowledge encoded in the form of natural language. We investigate their systematic generalization abilities on a logi-cal reasoning task in natural language, which involves reasoning over relationships between entities grounded in ﬁrst-order logical proofs. Speciﬁcally, we perform soft theorem-proving by leveraging TLMs to generate natural language proofs.
We test the generated proofs for logical consistency, along with the accuracy of the ﬁnal inference. We observe length-generalization issues when evaluated on longer-than-trained sequences. However, we observe TLMs improve their general-ization performance after being exposed to longer, exhaustive proofs. In addition, we discover that TLMs are able to generalize better using backward-chaining proofs compared to their forward-chaining counterparts, while they ﬁnd it easier to generate forward chaining proofs. We observe that models that are not trained to generate proofs are better at generalizing to problems based on longer proofs.
This suggests that Transformers have efﬁcient internal reasoning strategies that are harder to interpret. These results highlight the systematic generalization behavior of TLMs in the context of logical reasoning, and we believe this work motivates deeper inspection of their underlying reasoning strategies. 1

Introduction
Systematic Generalization has been characterized as the capacity to understand and produce a poten-tially inﬁnite number of novel combinations from known components (Chomsky, 1957; Montague, 1970). For example, in Figure 1, a model could be exposed to a set of facts (e.g., “Nat is the grand-daughter of Betty”, “Greg is the brother of Nat”, “Flo is the sister of Greg”), but not to all the possible facts that can be inferred by combination of the known components (e.g., “Flo is the granddaughter of Betty”). More recent work has examined systematic generalization in terms of the ability of “a model to manipulate concepts in new combinations after being trained on all concepts, but only on a limited set of their combinations” (Bahdanau et al., 2019a). This view of systematic generalization shifts emphasis from reasoning to learning. Here we examine systematic generalization through measuring the ability of a model to reason about new inference step combinations despite being trained on a limited subset of them. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Recent developments in natural language processing (NLP) have shown that Transformer (Vaswani et al., 2017) Language Models (TLMs) are able to capture linguistic knowledge (Peters et al., 2018; Goldberg, 2019; Tenney et al., 2019), and yield state-of-the-art performance for many NLP tasks (Radford et al., 2018; Devlin et al., 2019), including but not limited to answering reading comprehension questions (Radford et al., 2019; Brown et al., 2020) and generating factual knowledge (Petroni et al., 2019) with little to no task supervision. These models are optimized on large corpora to predict the next words or a set of masked words in a sentence. While yielding impressive results, it is not clear if TLMs rely on many superﬁcial patterns in the data or if they actually learn re-usable skills, enabling them to generalize to new tasks by leveraging the compositionality of those skills (Lake and Baroni, 2018; Liška et al., 2018). Training on massive data can give certain advantages with respect to understanding the meanings of words, but we conjecture that such data gives models less experience with reasoning over inference chains.
In our work, we study the less understood issues related to how well TLMs are able to perform long chains of reasoning. In particular, we use TLMs for the task of theorem proving, where facts and proofs are speciﬁed in natural language. Using theorem proving, we test if TLMs can generate interpretable proofs with logically consistent language modeling as their main objective. In particular, we study their behavior as logical reasoners on text by analyzing the generated proofs and the ﬁnal answer. This setup allows us to evaluate the reasoning and generalization capabilities of TLMs. Recent work such as Petroni et al. (2019); Raffel et al. (2020);
Brown et al. (2020) suggest that language models can be treated as knowledge bases. This directly motivates us to investigate if language models can also learn certain reasoning strategies. Studying these abilities can give us insights to facilitate the use of such models as dynamic knowledge bases that could infer new knowledge even if it is not seen during pre-training.
Figure 1: Example of a CLUTRR graph with known facts (solid lines) and unknown facts to in-fer (dotted lines).
For natural language theorem proving, we use the question answering CLUTRR benchmark suite (Sinha et al., 2019) to perform controlled studies. This dataset is of interest because: (i) the compositional nature of tasks involved make it well suited for evaluating systematic generalization, and (ii) each question–answer pair is accompanied by a proof that can be used to explain how to arrive at the answer. We use this dataset as a medium to understand the reasoning capacity of TLMs.
Our experiments reveal the following: 1. TLMs suffer from length generalization: they cannot extrapolate to proofs requiring more proof steps than seen during training time. 2. They generalize better when trained to generate long proofs compared to short proofs. 3. They generalize better when trained to generate backward-chaining proofs rather than forward-chaining. 4. Surprisingly, they generalize better when they are trained to directly generate the answer instead of learning to generate the proof and then the answer.
To the best of our knowledge, we are the ﬁrst to use a language modeling objective to do interpretable theorem proving with a Transformer. We hope that this work can shed some light on the reasoning capacity of TLMs and inspire future research to design models with greater reasoning capacity. 2