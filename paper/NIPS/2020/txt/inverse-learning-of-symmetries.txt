Abstract
Symmetry transformations induce invariances which are frequently described with deep latent variable models. In many complex domains, such as the chemical space, invariances can be observed, yet the corresponding symmetry transformation cannot be formulated analytically. We propose to learn the symmetry transformation with a model consisting of two latent subspaces, where the ﬁrst subspace captures the target and the second subspace the remaining invariant information. Our approach is based on the deep information bottleneck in combination with a continuous mutual information regulariser. Unlike previous methods, we focus on the challenging task of minimising mutual information in continuous domains. To this end, we base the calculation of mutual information on correlation matrices in combination with a bijective variable transformation. Extensive experiments demonstrate that our model outperforms state-of-the-art methods on artiﬁcial and molecular datasets. 1

Introduction
In physics, symmetries are used to model quantities which are retained after applying a certain class of transformations. From the mathematical perspective, symmetry can be seen as an invariance property of mappings, where such mappings leave a variable unchanged. Consider the example of rotational invariance from Figure 1a. We ﬁrst observe the 3D representation of a speciﬁc molecule m. The molecule is then rotated. For any rotation g, we calculate the distance matrix D between the atoms of the rotated molecule g(m) with a predeﬁned function f . Note that a rotation is a simple transformation which admits a straightforward analytical form. As g induces an invariance class, we obtain the same distance matrix for every rotation g, i.e. f (m) = f (g(m)) for any rotation g.
Now, consider highly complex domains e.g. the chemical space, where analytical forms of symmetry transformations g are difﬁcult or impossible to ﬁnd (Figure 1b). The task of discovering novel molecules for the design of organic solar cells in material science is an example of such a domain.
Here, all molecules must possess speciﬁc properties, e.g. a bandgap energy of exactly 1.4 eV [37], in order to adequately generate electricity from the solar spectrum. In such scenarios, no predeﬁned symmetry transformation (such as rotation) is known or can be assumed. For example, there exist various discrete molecular graphs with different atom and bond composition that result in the n equivalent band gap energy. The only available data deﬁning our invariance class are the m, e
} numeric point-wise samples from the function f where n is the number of samples, m the molecule and e = f (m) the bandgap energy. Therefore, no analytical form of a symmetry transformation g which alters the molecule m and leaves the bandgap energy e unchanged can be assumed.
{
The goal of our model is thus to learn the class of symmetry transformations g which result in a symmetry property f of the modelled system. To this end, we learn a continuous data representation and the corresponding continuous symmetry transformation in an inverse fashion from data samples n only. To do so, we introduce the Symmetry-Transformation Information Bottleneck (STIB) m, e
}
{ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Rotational symmetry transformation. (b) Unknown symmetry transformation.
Figure 1: Left: a molecule is rotated by g admitting an analytical form. The distance matrix D between atoms is calculated by a known function f and remains unchanged for all rotations. Right: n where m is the molecule and e the bandgap energy. These samples approximate n samples the function f whereas the class of functions g leading to the same bandgap energy is unknown. m, e
}
{ where we encode the input X (e.g. a molecule) into a latent space Z and subsequently decode it to X and a preselected target property Y (e.g. the bandgap energy). Speciﬁcally, we divide the latent space into two subspaces Z0 and Z1 to explore the variations of the data with respect to a speciﬁc target.
Here, Z1 is the subspace that contains information about input and target, while Z0 is the subspace that is invariant to the target. In doing so, we capture symmetry transformations not affecting the target Y in the isolated latent space Z0.
The central element of STIB is minimising the information about continuous Y (e.g. bandgap energy) present in Z0 by employing adversarial learning. In contrast, cognate models have to the best of our knowledge solely focused on discrete Y . The potential reason is that naively using the negative log-likelihood (NLL) as done for maximising mutual information in other deep information bottleneck models leads to critical problems in continuous domains. This stems from the fact that fundamental properties of mutual information, such as invariance to one-to-one transformations, are not captured by this mutual information estimator. Simple alternatives such as employing a coarse-grained discretisation approach as proposed in [35] are not feasible in our complex domain.
The main reason is that we want to consider multiple properties at once, every one of which might require a high-resolution. Simultaneous high-resolutional discretisation of multiple targets would result in an intractable classiﬁcation problem.
To overcome the aforementioned issues, we propose a new loss function based on Gaussian mutual information with a bijective variable transformation as an addition to our modelling approach. In contrast to using the NLL, this enables the calculation of the full mutual information on the basis of correlations. Thus, we ensure mutual information estimation that is invariant against linear one-to-one transformations. In summary, we make the following contributions: 1. We introduce a deep information bottleneck model that learns a continuous low-dimensional representation of the input data. We augment it with an adversarial training mechanism and a partitioned latent space to learn symmetry transformations based on this representation. 2. We further propose a continuous mutual information regulation approach based on correla-tion matrices. This makes it possible to address the issue of one-to-one transformations in the continuous domain. 3. Experiments on an artiﬁcial as well as two molecular datasets demonstrate that the proposed model learns both pre-deﬁned and arbitrary symmetry transformations and outperforms state-of-the-art methods. 2