Abstract
Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive
TTS models as their external aligners. In this work, we propose Glow-TTS, a
ﬂow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of ﬂows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforc-ing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative ﬂows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting. 1

Introduction
Text-to-speech (TTS) is a task in which speech is generated from text, and deep-learning-based TTS models have succeeded in producing natural speech. Among neural TTS models, autoregressive models, such as Tacotron 2 [23] and Transformer TTS [13], have shown state-of-the-art performance.
Despite the high synthesis quality of autoregressive TTS models, there are a few difﬁculties in deploying them directly in real-time services. As the inference time of the models grows linearly with the output length, undesirable delay caused by generating long utterances can be propagated to the multiple pipelines of TTS systems without designing sophisticated frameworks [14]. In addition, most of the autoregressive models show a lack of robustness in some cases [20, 16]. For example, when an input text includes repeated words, autoregressive TTS models sometimes produce serious attention errors.
To overcome such limitations of the autoregressive TTS models, parallel TTS models, such as
FastSpeech [20], have been proposed. These models can synthesize mel-spectrograms signiﬁcantly faster than the autoregressive models. In addition to the fast sampling, FastSpeech reduces the failure cases of synthesis, such as mispronouncing, skipping, or repeating words, by constraining its alignment to be monotonic. However, to train the parallel TTS models, well-aligned attention
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
maps between text and speech are necessary. Recently proposed parallel models extract attention maps from their external aligners, pre-trained autoregressive TTS models [16, 20]. Therefore, the performance of the models critically depends on that of the external aligners.
In this work, we eliminate the necessity of any external aligner and simplify the training procedure of parallel TTS models. Here, we propose Glow-TTS, a ﬂow-based generative model for parallel TTS that can internally learn its own alignment.
By combining the properties of ﬂows and dynamic programming, Glow-TTS efﬁciently searches for the most probable monotonic alignment between text and the latent representation of speech. The proposed model is directly trained to maximize the log-likelihood of speech with the alignment. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing ﬂows enables fast, diverse, and controllable speech synthesis.
Glow-TTS can generate mel-spectrograms 15.7 times faster than the autoregressive TTS model,
Tacotron 2, while obtaining comparable performance. As for robustness, the proposed model outper-forms Tacotron 2 signiﬁcantly when input utterances are long. By altering the latent representation of speech, we can synthesize speech with various intonation patterns and regulate the pitch of speech. We further show that our model can be extended to a multi-speaker setting with only a few modiﬁcations.
Our source code2 and synthesized audio samples3 are publicly available. 2