Abstract
In safety-critical but computationally resource-constrained applications, deep learn-ing faces two key challenges: lack of robustness against adversarial attacks and large neural network size (often millions of parameters). While the research com-munity has extensively explored the use of robust training and network pruning independently to address one of these challenges, only a few recent works have studied them jointly. However, these works inherit a heuristic pruning strategy that was developed for benign training, which performs poorly when integrated with robust training techniques, including adversarial training and veriﬁable robust training. To overcome this challenge, we propose to make pruning techniques aware of the robust training objective and let the training objective guide the search for which connections to prune. We realize this insight by formulating the pruning objective as an empirical risk minimization problem which is solved efﬁciently us-ing SGD. We demonstrate that our approach, titled HYDRA1, achieves compressed networks with state-of-the-art benign and robust accuracy, simultaneously. We demonstrate the success of our approach across CIFAR-10, SVHN, and ImageNet dataset with four robust training techniques: iterative adversarial training, random-ized smoothing, MixTrain, and CROWN-IBP. We also demonstrate the existence of highly robust sub-networks within non-robust networks. Our code and compressed networks are publicly available2. 1

Introduction
How can we train deep neural networks (DNNs) that are robust against adversarial examples while minimizing the size of the neural network?
In safety-critical and resource-constrained envi-ronments, both robustness and compactness are simultaneously necessary. However, existing work is limited in its ability to answer this ques-tion since it has largely addressed these chal-lenges in isolation. For example, neural network pruning is an efﬁcient approach to minimize the size of the neural networks. In parallel, robust training techniques can signiﬁcantly improve the adversarial robustness of neural networks.
However, improving adversarial robustness has been shown to require even larger neural net-works [30, 49]. Thus it is even more critical
Figure 1: Comparison of our proposed approach ((cid:63)) and least-weight magnitude based pruning (+) for ad-versarial training (l
≤ (cid:15)) with VGG16 network and CIFAR-10 dataset. For both a weaker adversary ((cid:15)=2/255) or a stronger adversary ((cid:15)=8/255), our pro-posed technique leads to higher empirical robust accu-racy where the gap increases with compression ratio.
∞ 1a small organism with high resiliency and biological immortality due to regenerative abilities. 2https://github.com/inspire-group/compactness-robustness 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to ask whether network pruning techniques can reduce the size of the network, i.e., number of connections, while preserving robustness?
A gold standard for network pruning has been the approach of Han et al. [19], which prunes connections that have the lowest weight magnitude (LWM) under the assumption that they are the least useful. Sehwag et al. [34] demonstrated early success of LWM pruning with adversarially robust networks while Ye et al. [45] and Gui et al. [15] further improved its performance by integrating with alternating direction method of multipliers (ADMM) based optimization. These works inherit the heuristic assumption that connections with the least magnitude are also unimportant in the presence of robust training. While both LWM and ADMM based pruning techniques are highly successful with benign training [19, 50], they incur a huge performance degradation with adversarial training. Our design goal is to develop a pruning technique which achieves high performance and also generalizes to multiple types of robust training objectives including veriﬁable robustness [30, 49, 38, 48, 8].
Instead of inheriting a pruning heuristic and applying it to all robust training objectives, we argue that a better approach is to make the pruning technique aware of the robust training objective itself.
We achieve this by formulating the pruning step, i.e., deciding which connections to prune, as an empirical risk minimization problem with a robust training objective, which can be solved efﬁciently using stochastic gradient descent (SGD). Our formulation is generalizable and can be integrated with multiple types of robust training objectives including veriﬁable robustness. Given a pre-trained network, we optimize the importance score [32] for each connection in the pruning step while keeping the ﬁne-tuning step intact. Connections with the lowest importance scores are pruned away. We propose a scaled initialization of importance scores, which is a key driver behind the high benign and robust accuracy of our compressed networks.
Our proposed technique achieves much higher robust accuracy compared to LWM. Fig. 1 shows these results for adversarial training with both a weaker ((cid:15)=2) and a stronger ((cid:15)=8) adversary. With increasing pruning ratios, the gap between the robust accuracy achieved with both techniques further increases. Due to the accuracy-robustness trade-off in DNNs [49, 30], a rigorous comparison of pruning techniques should consider both benign and robust accuracy. We demonstrate that our compressed networks simultaneously achieve both state-of-the-art benign and robust accuracy.
Recently, Ramanujan et al. [32] demonstrated that there exist hidden sub-networks with high benign accuracy within randomly initialized networks. Using our pruning technique, we extend this observa-tion to robust training, where we uncover highly robust (both empirical and veriﬁable) sub-networks within non-robust networks. In particular, within empirically robust networks that have no veriﬁable robustness, we found sub-networks with veriﬁed robust accuracy close to the state-of-the-art [33].
Key contributions: We make the following key contributions.
• We develop a novel pruning technique, which is aware of the robust training objective, by formulat-ing it as an empirical risk minimization problem, which we solve efﬁciently with SGD. We show the generalizability of our formulation by considering multiple types of robust training objectives, including veriﬁable robustness. We employ an importance score based optimization technique with our proposed scaled initialization of importance scores, which is the key driver behind the success of our approach.
• We evaluate the proposed approach across four robust training objectives, namely iterative adver-sarial training [7, 30, 49], randomized smoothing [8, 7], MixTrain [38], and CROWN-IBP [48] on
CIFAR-10, SVHN, and ImageNet dataset with multiple network architectures. Notably, at 99% connection pruning ratio, we achieve gains up to 3.2, 11.2, and 17.8 percentage points in robust accuracy, while simultaneously achieving state-of-the-art benign accuracy, compared to previous works [34, 45, 15] for ImageNet, CIFAR-10, and SVHN dataset, respectively.
• We also demonstrate the existence of highly robust sub-networks within non-robust or weakly robust networks. In particular, within empirically robust networks that have no veriﬁable robustness, we were able to ﬁnd sub-networks with veriﬁed robust accuracy close to state-of-the-art. 2