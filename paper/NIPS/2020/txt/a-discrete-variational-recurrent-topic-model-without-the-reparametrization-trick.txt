Abstract
We show how to learn a neural topic model with discrete random variables—one that explicitly models each word’s assigned topic—using neural variational in-ference that does not rely on stochastic backpropagation to handle the discrete variables. The model we utilize combines the expressive power of neural methods for representing sequences of text with the topic model’s ability to capture global, thematic coherence. Using neural variational inference, we show improved perplex-ity and document understanding across multiple corpora. We examine the effect of prior parameters both on the model and variational parameters, and demonstrate how our approach can compete and surpass a popular topic model implementation on an automatic measure of topic quality. 1

Introduction
With the successes of deep learning models, neural variational inference (NVI) [27]—also called variational autoencoders (VAE)—has emerged as an important tool for neural-based, probabilistic modeling [19, 40, 37]. NVI is relatively straight-forward when dealing with continuous random variables, but necessitates more complicated approaches for discrete random variables.
The above can pose problems when we use discrete variables to model data, such as capturing both syntactic and semantic/thematic word dynamics in natural language processing (NLP). Short-term memory architectures have enabled Recurrent Neural Networks (RNNs) to capture local, syntactically-driven lexical dependencies, but they can still struggle to capture longer-range, thematic dependencies
[44, 29, 5, 20]. Topic modeling, with its ability to effectively cluster words into thematically-similar groups, has a rich history in NLP and semantics-oriented applications [4, 3, 50, 6, 46, 35, i.a.].
However, they can struggle to capture shorter-range dependencies among the words in a document
[8]. This suggests these problems are naturally complementary [42, 53, 45, 41, 27, 15, 8, 48, 51, 22].
NVI has allowed the above recurrent topic modeling approaches to be studied, but with two primary modiﬁcations: the discrete variables can be reparametrized and then sampled, or each word’s topic assignment can be analytically marginalized out, prior to performing any learning. However, previous work has shown that topic models that preserve explicit topics yield higher quality topics than similar models that do not [25], and recent work has shown that topic models that have relatively consistent word-level topic assignments are preferred by end-users [24]. Together, these suggest that there are beneﬁts to preserving these assignments that are absent from standard RNN-based language models.
Speciﬁcally, preservation of word-level topics within a recurrent neural language model may both improve language prediction as well as yield higher quality topics.
To illustrate this idea of thematic vs. syntactic importance, consider the sentence “She received bachelor’s and master’s degrees in electrical engineering from Anytown University.” While a topic 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
model can easily learn to group these “education” words together, an RNN is designed explicitly to capture the sequential dependencies, and thereby predict coordination (“and”), metaphorical (“in”) and transactional (“from”) concepts given the previous thematically-driven tokens.
In this paper we reconsider core modeling decisions made by a previous recurrent topic model [8], and demonstrate how the discrete topic assignments can be maintained in both learning and inference without resorting to reparametrizing them. In this reconsideration, we present a simple yet efﬁcient mechanism to learn the dynamics between thematic and non-thematic (e.g., syntactic) words. We also argue the design of the model’s priors still provides a key tool for language understanding, even when using neural methods. The main contributions of this work are: 1. We provide a recurrent topic model and NVI algorithm that (a) explicitly considers the surface dynamics of modeling with thematic and non-thematic words and (b) maintains word-level, discrete topic assignments without relying on reparametrizing or otherwise approximating them. 2. We analyze this model, both theoretically and empirically, to understand the beneﬁts the above modeling decisions yield. This yields a deeper understanding of certain limiting behavior of this model and inference algorithm, especially as it relates to past efforts. 3. We show that careful consideration of priors and the probabilistic model still matter when using neural methods, as they can provide greater control over the learned values/distributions. Speciﬁ-cally, we ﬁnd that a Dirichlet prior for a document’s topic proportions provides ﬁne-grained control over the statistical structure of the learned variational parameters vs. using a Gaussian distribution.
Our code, scripts, and models are available at https://github.com/mmrezaee/VRTM. 2