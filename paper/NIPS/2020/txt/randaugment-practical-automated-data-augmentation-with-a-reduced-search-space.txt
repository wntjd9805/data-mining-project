Abstract
Recent work on automated data augmentation strategies has led to state-of-the-art results in image classiﬁcation and object detection. An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task. In this work, we rethink the process of designing automated data augmentation strategies.
We ﬁnd that while previous work required searching for many augmentation parameters (e.g. magnitude and probability) independently for each augmentation operation, it is sufﬁcient to only search for a single parameter that jointly controls all operations. Hence, we propose a search space that is vastly smaller (e.g. from 1032 to 102 potential candidates). The smaller search space signiﬁcantly reduces the computational expense of automated data augmentation and permits the removal of a separate proxy task. Despite the simpliﬁcations, our method achieves state-of-the-art performance on CIFAR-10, SVHN, and ImageNet. On EfﬁcientNet-B7, we achieve 84.7% accuracy, a 1.0% increase over baseline augmentation and a 0.4% improvement over AutoAugment on the ImageNet dataset. On object detection, the same method used for classiﬁcation leads to 1.0-1.3% improvement over the baseline augmentation method on COCO. Code is available online. 2 3 1

Introduction
Although data augmentation is a widely used method to add additional knowledge when training vision models [30, 15, 5, 42], the fact that it is manually designed makes it difﬁcult to scale to new applications. Learning data augmentation strategies from data has recently emerged as a new paradigm to automate the design of augmentation and has the potential to address some weaknesses of traditional data augmentation methods [3, 45, 13, 17]. Training a machine learning model with a learned data augmentation policy may signiﬁcantly improve image classiﬁcation [3, 17, 13], object detection [45], model robustness [23, 39, 28], and semi-supervised learning for image classiﬁcation [37]. Unlike architecture search [47], all of these improvements in predictive performance incur no additional computational cost at inference time since data augmentation is only used during training.
An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. Although the proxy task helps speeding up the search process, it also adds extra complexity to the methods and causes further issues. For example, it was not clear if the optimal hyperparameters found on the proxy task are also optimal for the actual task. In fact, we will provide experimental
⇤Authors contributed equally. 2github.com/tensorflow/tpu/tree/master/models/official/efficientnet 3github.com/tensorflow/tpu/tree/master/models/official/resnet 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
search space 0 1032 1032 1061 1032 102
Baseline
AA
Fast AA
PBA
Adv. AA
RA (ours)
CIFAR-10
PyramidNet Wide-ResNet
SVHN 97.3 98.5 98.3 98.5 98.6 98.5 98.5 98.9 98.8 98.9
-99.0
ImageNet
EfﬁcientNet-B7 83.7 84.3
---84.7
Table 1: Simple grid search on a vastly reduced search space matches or exceeds predictive performance of other augmentation methods. We report the search space size, and the test accu-racy achieved for AutoAugment (AA) [3], Fast AutoAugment [17], Population Based Augmentation (PBA) [13], Adversarial AutoAugment [43] and the proposed RandAugment (RA) on CIFAR-10
[14], SVHN [24], and ImageNet [4] classiﬁcation tasks. Search space size is reported as the order of magnitude of the number of possible augmentation policies. Dash indicates that results are not available. evidence in this paper to challenge this core assumption. In particular, we demonstrate that using proxy tasks is sub-optimal as the strength of the augmentation depends strongly on model and dataset size. These results suggest that improved data augmentation methods may be possible if one could remove the separate search phase on a proxy task.
In this work, we aim to make AutoAugment and related methods [3, 13, 17] more practical. While previous work focused on the search methodology [17, 13], our analysis shows that the search space plays a more signiﬁcant role. In previous work, it was required to search for both the probability and the magnitude (e.g. how many degrees to rotate an image) of each operation in the search space.
Our experiments surprisingly show that it is sufﬁcient to optimize all of the operations jointly with a single distortion magnitude. To further simplify the search space we simply set the probability of each operation to be equally likely. With the reduced search space, we also simplify the whole search process: we no longer need a separate expensive search phase and proxy tasks.
The reduction in parameter space is in fact so dramatic that a simple grid search is sufﬁcient to ﬁnd a data augmentation policy that outperforms or closely matches AutoAugment. We name our method
RandAugment because for each image it uniformly samples from all available data augmentation operations in the search space. Table 1 shows a summary of our main results, which reveals
RandAugment can be easily optimized thanks to having a much smaller search space. RandAugment also achieves higher accuracy on a wide range of benchmarks, thanks to its ability to adjust its distortion magnitude accordingly to the the model and dataset size. With EfﬁcientNet-B7, we achieve an accuracy of 84.7%, a 0.4% increment over AutoAugment and 1.0% over baseline augmentation of
ﬂips and crops [32]. 2