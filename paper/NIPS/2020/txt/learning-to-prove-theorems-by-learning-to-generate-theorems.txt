Abstract
We consider the task of automated theorem proving, a key AI task. Deep learning has shown promise for training theorem provers, but there are limited human-written theorems and proofs available for supervised learning. To address this limitation, we propose to learn a neural generator that automatically synthesizes theorems and proofs for the purpose of training a theorem prover. Experiments on real-world tasks demonstrate that synthetic data from our approach improves the theorem prover and advances the state of the art of automated theorem proving in
Metamath. Code is available at https://github.com/princeton-vl/MetaGen. 1

Introduction
Automated theorem proving aims to automatically generate a proof given a conjecture (the target theorem) and a knowledge base of known facts, all expressed in a formal language. Automated theorem proving is useful in a wide range of applications, including the veriﬁcation and synthesis of software and hardware systems (Gu et al., 2016; Darvas et al., 2005; Kern & Greenstreet, 1999).
Automated theorem proving boils down to a search problem: ﬁnding the sequence of symbol manipulations that generate a valid proof. The fundamental challenge lies in the explosion of search space, in particular with long proofs and large knowledge bases. The success of theorem proving thus relies on effective heuristics that guide the prover by deciding the next step the prover should take.
Deep learning has emerged as a promising approach to learning search heuristics in an automated theorem prover (Irving et al., 2016; Whalen, 2016; Loos et al., 2017; Bansal et al., 2019a; Lee et al., 2019). The search process fundamentally reduces to a sequence of actions on manipulating a set of symbols. Thus a deep network can be trained to select the best action at each step.
A key challenge is how to train such networks. Prior work has used human-written theorems and proofs to perform imitation learning and has shown promising results (Loos et al., 2017; Yang &
Deng, 2019; Whalen, 2016; Paliwal et al., 2019). The training data consists of theorems and proofs manually written by human experts in a formal language, and the prover is trained to imitate the proof steps demonstrated by humans.
However, relying on human-written data has a major drawback: such data has limited availability and scalability. Writing theorems and proofs in a formal language requires highly specialized knowledge and skills, including mathematics, computer programming, and proﬁciency in the particular formal language. For a CS graduate student, it can take months to master a new formal language such as
Mizar, Metamath or HOLight (Wiedijk, 2003), after which it can take days to formalize a single page of a math textbook. This makes it impractical to crowdsource human-written proofs at large scale.
In this paper, we propose to train a theorem prover using synthetic data. The basic idea is to construct a generator that automatically synthesizes new theorems and their proofs, which serve to augment human-written data for training the prover. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Left: A proof task. Middle: The proof tree of the theorem 3eqtri. Each leaf node is a hypothesis and each internal node corresponds to a proof step. Right: The overview of our approach.
To generate a new theorem and its proof, the generator performs a sequence of symbol manipulations, similar to a prover. It repeatedly applies inference rules on a set of existing theorems and combines their proofs to form the proof of the new theorem. It is important to note that despite the similarity of operations, the generator has a much easier task than the prover. The generator just needs to generate some new theorem of its own choice, whereas the prover needs to ﬁnd the proof for a particular target theorem speciﬁed by someone else.
One challenge of generating synthetic theorems is that there are inﬁnitely many possibilities but the prover can only use a ﬁnite amount of them during training. Not all theorems are equally useful as training data. Thus a key question is how to generate synthetic theorems that are more useful. To this end we make the generator learnable by parameterizing it with deep networks.
We hypothesize that the generated data will be more useful if they are similar to human-written data. Therefore we use human-written data to train a generator. We consider two scenarios. If the human-written data consist of both theorem statements and their proofs, we train the generator to follow the proof steps in the forward direction, so that a well-trained generator would derive theorems humans tend to derive. If the human-written data consist of only theorem statements but not their proofs, i.e. no human actions to imitate, we use reinforcement learning to let the generator discover good actions that lead to synthetic theorems that are similar to the human-written theorems. To measure similarity between synthetic theorems and human theorems, we use a discriminator trained to distinguish the human theorems from synthetic ones, similar to GANs (Goodfellow et al., 2014).
We instantiate our approach in Metamath (Megill & Wheeler, 2019), a popular language for formal mathematics, and with Holophrasm (Whalen, 2016), a Metamath neural prover. We propose a neural theorem generator called “MetaGen”, which synthesizes new theorems and their proofs expressed in the formalism of Metamath. To the best of our knowledge, MetaGen is the ﬁrst neural generator of synthetic training data for theorem proving. Experiments on real-world Metamath tasks show that synthetic data from MetaGen can help train better provers, advancing the state of art in theorem proving on Metamath. 2