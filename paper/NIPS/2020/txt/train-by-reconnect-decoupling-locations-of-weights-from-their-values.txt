Abstract
What makes untrained deep neural networks (DNNs) different from the trained performant ones? By zooming into the weights in well-trained DNNs, we found that it is the location of weights that holds most of the information encoded by the training. Motivated by this observation, we hypothesized that weights in DNNs trained using stochastic gradient-based methods can be separated into two dimensions: the location of weights, and their exact values. To assess our hypothesis, we propose a novel method called lookahead permutation (LaPerm) to train DNNs by reconnecting the weights. We empirically demonstrate LaPerm’s versatility while producing extensive evidence to support our hypothesis: when the initial weights are random and dense, our method demonstrates speed and performance similar to or better than that of regular optimizers, e.g., Adam. When the initial weights are random and sparse (many zeros), our method changes the way neurons connect, achieving accuracy comparable to that of a well-trained dense network. When the initial weights share a single value, our method ﬁnds a weight agnostic neural network with far-better-than-chance accuracy. 1

Introduction
Conventional gradient-based algorithms for training deep neural networks (DNNs), such as stochastic gradient descent (SGD), ﬁnd the appropriate numerical values for a set of predetermined weight vectors θ. These algorithms apply the changes ∆θ to θ at each iteration. Denoting the weight vectors at the t-th iteration as θt, the following update rule is used: θt ← θt−1 + ∆θt−1. Therefore, we have the relationship between a trained and an untrained DNN: θT = θ0 + (cid:80)T t=1 ∆θt, given the initial weights θ0 and the weights θT obtained by training the network for T iterations. However, since
∆θt is dependent on θt−1 and ∆θt−1 for every t, it is difﬁcult to directly interpret from the term (cid:80)T t=1 ∆θt what is the most substantial change that the training has applied to the initial weights.
In this work, we examine the relationship between θ0 and θT from a novel perspective by hypoth-esizing that weights can be decoupled into two dimensions: the locations of weights and their exact values. DNNs can be trained following the same stochastic gradient-based regime but using a fundamentally different update rule: θt ← σt(θt−1), for a permutation operation σt. Consequently, we have θT = σT (...(σ1(θ0))), and thus have θT = σk(θ0) for σk from the same permutation group. Supporting our hypothesis, in the ﬁrst half of this paper, we demonstrate that SGD encodes information to DNNs in the way their weights are connected. In the latter half of this paper, we show that given an appropriately chosen neural architecture initialized with random weights, while
ﬁne-tuning of the exact values of weights is essential for reaching state-of-the-art results, properly determining the location of weights alone plays a crucial role in making neural networks performant.
In Section 2, we describe an interesting phenomenon in the distribution of weights in trained DNNs, which casts light on understanding how training encodes information. In Section 3, we show that 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
it is possible to translate stochastic gradient updates into permutations. In Section 4, we propose a novel algorithm, named lookahead permutation (LaPerm), to effectively train DNNs by reconnection.
In Section 5, we showcase LaPerm’s versatility in both training and pruning. We then improve our hypothesis based on empirical evidence. 2 Similarity of Weight Proﬁles (SoWP)
DNNs perform chains of mathematical transformations from their input to output layers. At the core of these transformations is feature extraction. Artiﬁcial neurons, the elementary vector-to-scalar functions in neural networks, are where feature extraction takes place. We represent the incoming weighted connections of a neuron using a one-dimensional vector (ﬂattened if it has a higher dimension, e.g., convolutional kernel), which we refer to as a weight vector. We then represent all neuron connections between two layers by a weight matrix in which columns are weight vectors.
Figure 1: Proﬁling a weight matrix in a pre-trained VGG16 on ImageNet. Parts (b) and (d) are the same plots as (a) and (c), respec-tively, from different viewing angles. The color of each single scatter plot is chosen in order, cyclically, from midnight-blue, gold, dark-green, and steel-blue.
Figure 2: Proﬁling all weight matrices in a pre-trained VGG16. The weight proﬁle se-lected for Figure 1 is marked in red. The z-axes are hidden, as in Figure 1.
To gain insight into how the information is encoded in a trained DNN, we zoom into the weight vectors. Here, we visualize a weight matrix by drawing a scatter plot for each of its weight vectors, where the x- and y-axis indicate the indices and weight values associated with each entry, respectively.
We stack these scatter plots on the same ﬁgure along the z-axis, such that all plots share the same x and y-axis. Figure 1 (a) and (b) show such a visualization of a weight matrix from different viewing angles; it represents all weighted connections between the last convolutional layer and the ﬁrst fully-connected layer in VGG16 [43] pre-trained on ImageNet [7]. At a glance, these plots appear to be roughly zero-mean, but as a jumble of random-looking points. Nothing is particularly noticeable until we sort all the weight vectors and redo the plots to obtain Figure 1 (c) and (d). The patterns shown on these ﬁgures imply that all 4096 weight vectors have almost identical distributions and centers as their scatter plots closely overlap with each other. In the forthcoming discussion, we refer to sorted weight vectors as a weight proﬁle.
As shown in Figure 2, although the shapes may vary, similar patterns are observed in most layers.
We also found these patterns in every other tested pre-trained convolutional neural network (CNN), such as ResNet50 [15], MobileNet [17], and NASNet [54]. Please refer to Appendix A.2 for their visualizations. We call this phenomenon of weight vectors associated with a group of neurons possessing strikingly similar statistical properties, the similarity of weight proﬁles (SoWP). It reveals the beauty and simplicity of how well-trained neural networks extract and store information.
Similarity is where two or more objects lack differentiating features. SoWP implies that many features encoded by training are lost after sorting. In other words, the features are mostly stored in the order of weights before sorting. Consequently, SoWP allows the weights of a well-trained DNN 2
to be near-perfectly separated into two components: the locations (relative orders) of weights and a statistical distribution describing their exact values. 3
Is Permutation the Essence of Learning?
If information can truly be encoded in the relative order of weights, we would expect changes in this order to reﬂect the training progress. We train a fully-connected DNN with two hidden layers (100
ReLU units each) using the MNIST database [26]. In isolation, we train using the same architecture and initialization under three different settings: (1) SGD (1e-1) with no regularization. (2) Adam (1e-3) with no regularization. (3) Adam (1e-3) with L2 regularization [24]. The learning rates in all experiments are divided by 2 and 5 at the 10th and 20th epochs. Full experimental settings are given in the Appendix. We extract the orders of weights during the training by focusing on their rankings within each weight vector.
The ranking of a weight vector wj is deﬁned as a vector Rj, where #Rj = #wj, of distinct integers in [0, #wj), such that wj[p] > wj[q] implies Rj[p] > Rj[q], for all integers p, q ∈ [0, #wj), p (cid:54)= q.
Here, #wj denotes the number of elements in wj; and wj[p] denotes the p-th element of the vector wj. If the ranking Rj,t of wj at the t-th iteration is different from Rj,t−1 at the t − 1-th iteration, there must exist a permutation σt such that Rj,t = σt(Rj,t−1). For simplicity, we compute
Dj,t = |Rj,t − Rj,t−1|, which we refer to as the ranking distance. The i-th entry of the ranking distance Dj,t[i] indicates the distance of change in the ranking of wj[i] in the past iteration.
For a weight matrix W , we compute mean: Dt = (cid:80) entries in W , and the standard deviation: SD[Dt] = i Dj,t[i]/#W , where #W is the total number of (cid:80) i(Dj,t[i] − Dt)2/#W of the ranking distance. (cid:80) j (cid:113)(cid:80) j
Figure 3: Monitoring ranking distance and validation loss in the ﬁrst weight matrix of the network.
Each column title indicates the experimental setting. Shown under each title (top to bottom) is the evolution of the ratio of the mean and standard deviation of the ranking distance to the size of the weight vector, i.e., Dt/784 and SD[Dt]/784, and the trend of validation loss on 10,000 test images.
Results and Analysis We brieﬂy point out that in the results shown in Figure 3, changes in the ranking reﬂect the progress of learning. The behaviors of permutations in (a)~(c) show unique traits under each setting. In (a), the trend seems random, especially when the learning rate is 0.1, reﬂecting how SGD updates largely depend on the randomly sampled batches. In contrast, in (b) and (c), since the Adam updates consider previous gradients, the permutations appear to follow a particular trend.
In (c), when L2 regularization is applied, the change in ranking is more signiﬁcant in both number and size. This implies that the weights become smaller and closer to each other because of the weight penalties. The closer they are, the easier their rankings can be swapped, and the greater the ranking distance the swap would cause by an update. Moreover, in (b) and (e) at around the 24th and 29th epoch, the sharp rise in the mean of the ranking distance predicts a deterioration in validation loss.
The full experiment and analysis are presented in the Appendix. 4 Lookahead Permutation (LaPerm)
Motivated by the observation in Section 3 that the changes in the ranking (order) of weights reﬂect the progress of training, we try to achieve the inverse: we propose LaPerm, a method for training DNNs by reconnecting the weights. This method adopts an inner loop structure similar to the Lookahead (LA) optimizer [52] and the Reptile optimizer [38]. Pseudocode for LaPerm is shown in Algorithm 1.
We consider training a feedforward network Fθ0 (x) with initial weights θ0 ∼ Dθ. Before the training starts, LaPerm creates a copy of θ0 and sorts every weight vector of this copy in ascending order. 3
We store the newly created copy as θsorted. At any step t during training, LaPerm holds onto both
θsorted and θt, where θsorted is served as a preparation for synchronization and is maintained as sorted throughout training; Weights θt, which are updated regularly at each mini-batch using an inner optimizer, Opt, of choice, e.g., Adam, are used as a reference to permute θsorted.
Algorithm 1 LaPerm
Require: Loss function L
Require: initial weights θ0
Require: Synchronization period k
Require: Inner optimizer Opt
θsorted ← Sort weight vectors in θ0 for t = 1, 2, . . . do
Sample mini-batch dt ∼ Dtrain
θt ← θt−1 + Opt(L, θt−1, dt) if k divides t then
θt ← σθt(θsorted) // Synchronization end if end for
Figure 4: (Left) Pseudocode for LaPerm. (Right) Given a randomly initialized fully-connected DNN with one hidden layer trained using MNIST [26], one typical weight vector associated with the hidden layer before (upper left) and after training (lower right) using LaPerm. Reconnected weights with same values are connected using a green or orange arrow chosen at random. j in θsorted according to its counterpart wj in θt such that wj and w(cid:48)
Synchronization Once every k steps, synchronization: θt ← σθt(θsorted) is performed, where σθt is a permutation operation generated based on θt. We refer to k as the synchronization period (sync period). More formally, synchronization involves the following two steps: 1) permuting the weight vector w(cid:48) j have the same ranking (deﬁned in Section 3) for every j; and 2) assigning the permuted θsorted to θt. It is important to keep weight vectors in θsorted as always sorted so that the permutation can be directly generated by indexing each w(cid:48) j using the ranking Rj of wj with no extra computational overhead. Optionally, we could make a copy θ(cid:48) sorted before synchronization and only permute θ(cid:48) sorted so that θsorted is unchanged.
In essence, how exactly the magnitude of weights in θt have been updated by Opt is not of interest;
θt is only considered to be a correction to the ranking of θ0. In other words, we extract permutations from θt. If the total number of training batches N and synchronization period k are chosen such that k divides N , at the end of the training, the network’s weights θT is guaranteed to be σ(θ0), for a weight vector–wise permutation σ. A visualization of such permutation is shown in Figure 4 (Right).
Computational Complexity Sorting is required to get the rankings of weight vectors at synchroniza-tion. Suppose we use a linearithmic sorting method for weight vectors of size #wj, an inner optimizer with time complexity T , and sync period k. In this case, the amortized computational complexity for one LaPerm update is O(T + 1 j #wj log #wj). When k and the learning rate of the inner k optimizer are chosen such that the weight distribution and range of θt are similar to those of the initial weights θ0, the performance of sorting can be improved by adopting, e.g., bucket sort, especially when the weights are near-uniformly distributed. In modern DNN architectures, the average size of weight vectors is usually under 104, e.g., in ResNet50 and MobileNet, it is approximately 1017 and 1809, respectively. (cid:80) 5 Experiments: A Train-by-Reconnect Approach
In this section, we reconnect randomly weighted CNNs listed in Table 1 trained with the MNIST [26] and CIFAR-10 [23] datasets using LaPerm under various settings. LaPerm has two hyperparameters to itself: the initial weights θ0 and sync period k. In Section 5.1, we examine how the distribution of θ0 affects LaPerm’s performance. In Section 5.2, we vary the size of k within a wide range and analyze its effect on optimization. In Section 5.3, based on the experimental results, we improve our hypothesis initially stated in Section 1. In Section 5.4, we test our hypothesis as well as comprehensively assess the capability of LaPerm to train sparsely-initialized neural networks from scratch. In Section 5.5, we create weight agnostic neural networks with LaPerm. Here, we only show hyperparameter settings 4
Conv Layers
FC Layers
Network
Conv7
Conv2
Conv4
Conv13 2x32, 32(5x5;Stride 2) 2x64, 64(5x5;Stride 2) 128 (4x4) 2x64, pool 2x64, pool 2x128, pool 2x64, pool, 2x128, pool 3x256, pool 3x512, pool, 3x512, pool
ResNet50 16, 16x16 16x32 16x64 10 256, 256, 10 256, 256, 10 512, 10 avg-pool, 10
All / Conv Weights 325k / 326k 4.3M / 38K 2.4M / 260K 14.9M / 14.7M 760K / 760K
Epochs / Batch 45 / 50 125 / 50 90 or 125 / 50 125 / 50 200 / 50
Table 1: Architectures used in the experiments. The table is modiﬁed based on [9, 53]. Convolutional networks, if not speciﬁed, use 3x3 ﬁlters in convolutional (Conv) layers with 2x2 maxpooling (pool) followed by fully-connected (FC) layers. Conv2 and Conv4 are identical to those introduced in [9].
Newly introduced Conv7 is modiﬁed based on LeNet5 [25], and Conv13 is a modiﬁed VGG [43] network for CIFAR-10 adapted from [35]. necessary for understanding the experiments. Detailed settings are in Table 1 and Appendix A.4. The usage of batch normalization (BN) [19] is explained in Appendix A.4.6. 5.1 Varying the Initial Weights We train Conv7 on MNIST using different random initializations:
He’s uniform UH and normal NH [14], Glorot’s uniform UG and normal NG [11]. We also train the same network initialized with NH using Adam [22] and LA [52] in isolation. The trained weights obtained with Adam at the best accuracy are shufﬂed weight vector–wise and used as another initialization for LaPerm, which we refer to as NS. We use ﬁve random seeds and train the network for 45 epochs with a batch size of 50 and a learning rate decay of 0.95. We choose k = 20 for
LaPerm. For all experiments in this paper, LaPerm and LA use Adam as the inner optimizer.
The results are presented in Figure 5. While only a small discrepancy is observed between LaPerm’s validation accuracy using UH and NH, they are consis-tently ~0.1% above that of UG and NG, which shows the importance of the statistical properties of weight for LaPerm to reach the last bit of accuracy. NS per-forms similarly to NG until the 25th epoch, where it stops improving. A possible cause is that Adam over-adapted the values of weights in its training.
When weights were shufﬂed to obtain NS, it became difﬁcult to rediscover the right permutation.
Overall, although LaPerm pulls all the weights back to uniform random values every 20 batches (k=20), we see no disadvantage in its performance compared with Adam and LA. This observation implies that the inner optimizer of LaPerm, between each synchronization, encodes information to the weights in a manner that can be almost perfectly captured by extracting their change in ranking. In the end, we obtained state-of-the-art accuracy using MNIST, i.e., ~0.24% test error, which slightly outperformed Adam and LA.
Figure 5: MNIST experiments.
Text
“LaPerm” is omitted except for UH. The band, if shown, indicates the minimum and maxi-mum values obtained from ﬁve runs, other-wise they are omitted for visual clarity. 5.2 Understanding the Sync Period k In Section 5.1, we observe that LaPerm succeeds at inter-preting θt as a permutation of θ0. What happens if we vary the value of k? We train Conv4 on
CIFAR-10 and sweep over 1 to 2000 for k. The results are shown in Figure 6(a). We observe an unambiguous positive correlation between the size of the sync period k and the ﬁnal validation and training accuracy. Interestingly, when k = 2000, i.e., sync only once every two epochs, its accuracy started as the slowest but converged to the highest point, whereas for k ≤100, the trend starts fast but ends up with much lower accuracy. To see this clearly, in Figure 6 (b) and (c), we smoothed [42] the accuracy curves and found that before the 60th epoch (shown in (b)), the accuracies are negatively correlated with k, but are reversed afterward (shown in (c)).
It is worth noting that when k=1, LaPerm shows signiﬁcant degradation in its performance, which implies that one batch update is not enough to alter the structure of weights such that the newly added information can be effectively interpreted as permutations. In contrast, in (d) for k=2000, sharp
ﬂuctuations are observed after and before synchronization, which implies that the 2000 updates of the inner optimizer might have encoded information in a way that is beyond just permutations. However, we argue that this extra information in the latter case is not fundamentally important, as omitting it did not have a signiﬁcant negative impact on the ﬁnal accuracy. 5
Figure 6: Conv4 on CIFAR-10. Conv4 trained for 90 epochs using a batch size of 50, with k chosen from {1, 5, 10, 20, 50, 100, 200, 500, 1000, 2000}. In (a)~(d), Conv4 is fully initialized. (b) and (c) are smoothed curves for speciﬁc epochs in the experiment. (d) shows validation accuracy of the inner optimizer when k=2000. (e) shows the behavior of LaPerm when the initial weights are randomly pruned. The training accuracies, if shown, are obtained by going through 30,000 randomly selected training examples.
Finally, we train Conv2, Conv4, and Conv13 initialized with UH on CIFAR-10 [23], using a batch size of 50, and compare LaPerm with Adam and LA. We safely choose k to be 1000 for all architectures, i.e., the synchronization is done once per epoch.
Figure 7: Validation accuracy of Conv2, Conv4, and Conv13 on CIFAR-10.
The results are shown in Figure 7. Similar to Figure 6, LaPerm using large k started slow but demon-strated a steep growth trend in all cases. In addition, we observe growth in LaPerm’s performance compared with regular optimizers as the network grows large. This observation could be partially attributed to a large number of possible permutations in heavily parameterized DNNs. For a DNN with N weight vectors each of size #wj, not considering biases, LaPerm has access to (cid:81)N j #wj! different permutations. This number for Conv2, Conv4, Conv7, and Conv13 is approximately 101.6e7, 108e6, 108e5, and 105e7, respectively. 5.3 Two Dimensions of Weights Hypothesis We formalize our claim by hypothesizing that there are two dimensions of differences between initial and learned weights as follows. D1: locations of weights; and D2: the exact values of weights. D2 can be further decoupled into a well-chosen common distribution (D2θ ) and the deviations of exact learned weights (D2δ ) from that distribution.
Each weight vector wj can thus be represented as wj = σj(θ) + δj, where θ is a vector drawn from the common distribution (D2θ ), σj is a permutation operation (D1), and δj is the remainder (D2δ ).
However, this decomposition is not unique unless we put more restrictions on the choice of θ and σj.
In section 2, by sorting the weight vectors (eliminating D1), we observed SoWP in which a common distribution is enough to approximate all the information remaining in (D2θ ). SoWP implies that given a moderately chosen D2θ , modifying D1 alone can result in a performant DNN.
As demonstrated in Section 3, 5.1, and 5.2, SGD-based methods update D1 and D2 simultaneously at each learning step. However, we hypothesize that after enough iterations, the changes applied to
D1 and D2 become increasingly isolatable as the SoWP begins to appear. Especially in Section 5.2, we demonstrate that LaPerm is more capable of extracting effective permutations when k is larger.
In Figure 6(a), we observe a negative correlation between k and the convergence speed in the early epochs. We consider it to be possible that although D1 serves as a foundation for a trained DNN to perform well, its progress may or may not be immediately reﬂected in the performance. In Section 5.1, we saw how over-adapted (begin with a well-tuned D2 before learning D1) weight values Ns performed poorly in the end. In Section 5.2, we saw that LaPerm with smaller k converged faster, but to a worse ﬁnal accuracy. We consider is to be possible that modifying D2 can help the neural network 6
appear to learn quickly; however, without a properly established D1, prematurely calibrating D2 may introduce difﬁculties for further improving D1. Nevertheless, LaPerm, which leaves D2 as completely uncalibrated (the resulted weights are still random values), was usually slightly outperformed by
Adam. It implies that D2 might be crucial for the ﬁnal squeeze of performance. 5.4 Reconnecting Sparsely Connected Neural Networks We further assess our hypothesis by creating a scenario in which D1 is crucial: we let the neural network be sparsely connected with random weights, i.e., many weights are randomly set to zero before training. We expect a well-isolated
D1 to effectively reconnect the random weights and result in good performance.
We use p to denote the percentage of initial weights that are randomly pruned, e.g., p=10% means that 90% of the weights in θ0 remain non-zero. We redo the experiments on Conv4 as in Figure 6 (a) with p=50%. As expected, in Figure 6 (e), we see that the removal of weights has no noticeable impact on the performance, especially when k is large. In fact, the performance for k=1000 has improved.
Next, we create a scenario in which D2 is crucial: we perform the same random pruning as in the previous scenario; while freezing all zero connections, we train the network using Adam. In the results shown in Figure 6 (e) labeled as “Adam;50% Weights remain”, we observe its performance to be similar to that of LaPerm with k=50; it is clearly outperformed by LaPerm for k ≥200. We consider the possibility that when k is relatively small, it is difﬁcult to trigger a zero and non-zero weight swap, as there are not enough accumulated updates to alter the rankings substantially. The resulting reconnection (permutation of weights within non-zero connections), in this situation, thus behaves similarly to SGD weight updates within frozen non-zero connections.
Since pruning 50% of the weights from an over-parameterized DNN may not sig-niﬁcantly impact its accuracy, we test our hypothesis on ResNet50 [15], which has 760K trainable parameters (1/3 compared with Conv4), using a wider range of p. We initialize the network from UH. Adam’s learning rates for all experiments begin at 0.001 and are divided by 10 at the 80th, 120th, 160th epoch, and by 2 at the 180th epoch. For ResNet50 with a different ini-tial weight sparsity, we sweep over k from
{250,400,800} and pick the one with the best performance.
Figure 8: Accuracies of ResNet50 on CIFAR-10.
In the results shown in Figure 8, we observe the accuracies of LaPerm when p ∈
{0%, 30%, 50%, 70%} to be comparable or better than that of Adam when p=0%, which again demonstrates the importance of a well-learned D1. Moreover, before the ﬁrst learning rate drop at the 80th epoch, LaPerm behaves in an extremely unstable manner. For p=30%, LaPerm acts as if it has diverged for almost 20 epochs, but when its learning rate drops by 10×, its training accuracy increases from 10% to 95% within three epochs. When p=50%, we observe a similar but less typical trend. This demonstrates that the progress on D1 is not reﬂected in the accuracies.
Figure 9: The networks are randomly pruned before training, and reconnected using LaPerm. The percentage of weights remaining in the network before training is shown as “% of Weights.”
Finally, similar pruning experiments are performed on all architectures shown in Table 1. The results are summarized in Figure 9 using the previous hyperparameter settings. For all experiments, the weights are initialized from UH. We sweep over a sparse grid for k from 200 to 1000 and safely choose the largest one that does not diverge within the ﬁrst 50 epochs. We observe that LaPerm, 7
when p ≤ 70%, achieves comparable accuracy to that of the original unpruned network. Since setting weights to zeros in a weight vector decreases the number of permutations by a factor of the factorial of the number of zeros, we expect difﬁculties in optimization as the number of zeros increases. On the other hand, sparsely wired DNNs allow LaPerm, especially when k is large, to trigger architectural (zero and non-zero weight swaps) change, which adds a new dimension to training that may neutralize the damage of losing the possible permutations. In addition, since LaPerm never alters D2, they can be tuned to further encode information.
In all of our pruning experiments, we simply remove p% of randomly chosen weights from every weight matrix in the network except for the input layer which is pruned up to 20%. This naive pruning approach is not recommended if a high p is the goal, as it may cause layer-collapse, i.e., improperly pruning layers hinders signal propagation and renders the resulting sparse network difﬁcult to train
[27]. In Figure 9, we observe a severe drop in performance when p ≥ 90%. The intent of using the most naive pruning approach is to showcase and isolate the effectiveness of a well-learned D1. Since our work is on reconnecting the weights and is orthogonal to those on pruning the weights, previous works [2, 28, 33, 36, 46, 48, 49] can be combined to improve performance at higher p. 5.5 Weight Agnostic Neural Networks Inspired by what
Gaier and Ha [10] achieved for weight agnostic networks, we reduce the information stored in D2 to an extreme by setting all weights to a single shared value. We reconnect two simple networks: one with no hidden layers (F1, a linear model) and one with two hidden ReLU layers of size 128 and 64 (F2). Both networks use 10 softmax output units without bias. Since pruning is necessary for trig-gering architectural change, before training, we randomly prune 40% of the weights in F1 and 90% of the weights in F2. The remaining weights in F1 and F2 are all set to 0.08 and 0.03, respectively. We train F1 and F2 for 10 and 25 epochs, with a batch size of 128 using LaPerm with k = 250 on MNIST [26] for 30 random seeds each. As shown in Figure 10, we achieved ~85.5% and ~53% on F1 and F2. By using a slightly less naive pruning method, F2 achieved 78.14% test accuracy. Detailed settings can be found in the Appendix A.5.0. 6