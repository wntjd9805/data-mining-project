Abstract
We propose a local-to-global representation learning algorithm for 3D point cloud data, which is appropriate for handling various geometric transformations, espe-cially rotation, without explicit data augmentation with respect to the transfor-mations. Our model takes advantage of multi-level abstraction based on graph convolutional neural networks, which constructs a descriptor hierarchy to encode rotation-invariant shape information of an input object in a bottom-up manner. The descriptors in each level are obtained from a neural network based on a graph via stochastic sampling of 3D points, which is effective in making the learned representations robust to the variations of input data. The proposed algorithm presents the state-of-the-art performance on the rotation-augmented 3D object recognition and segmentation benchmarks. We further analyze its characteristics through comprehensive ablative experiments. 1

Introduction 3D object recognition based on point cloud data has witnessed remarkable achievement in recent years thanks to deep learning technologies [1–10]. While sparse and irregular structures and missing and noisy information of 3D point cloud data have been addressed actively using deep neural networks, their geometric transformations remain challenging problems; transformation-invariant representation learning requires a more principled formulation. Among many geometric transformations, rotation is particularly challenging to handle in practice, and existing algorithms often rely on the assumption of upright object poses. One of the straightforward solutions to address this issue without the prior is data augmentation, but it is not trivial to cover all possible rotations and generalize on realistic examples due to high computational cost and unexpected corner cases.
There exist a handful of techniques to tackle geometric transformations of 3D point cloud data in the context of 3D object recognition. For example, [1, 2] canonicalize input point coordinates using a spatial transform network, but they require data augmentation to work consistently on the variable transformation of input examples. More recent works [11–14] attempt to employ handcrafted transformation-invariant features such as distances and angles for robust recognition. Although these approaches present practical performance gains, such low-level geometric features have limited capability to express detailed shape information and their computation suffers from an asymptotic increase of computational complexity due to the joint consideration of multiple points.
To address such critical challenges, we introduce a novel rotation-invariant 3D object recognition framework based on graph convolutional neural networks (GCNs). The proposed approach designs the descriptors based on the local reference frame (LRF), i.e., a local coordinate system. The receptive
ﬁeld of our descriptor is enlarged hierarchically and stochastically, which leads to the construction of better regularized and representative features even with substantial variations of the object shape.
We utilize GCNs upon stochastically generated graphs and encode local-to-global shape information effectively, which provides the capability to represent global rotation-invariant information of a target 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
object and facilitates the robustness to perturbations and outliers in observations. The source codes are available on our project page1.
The contributions of this paper are summarized below:
• We introduce a local-to-global representation learning method for 3D point cloud data based on GCNs, referred to as RI-GCN, to model rotation-invariant features in a progressive manner without computing any geometric features, such as angle or distance.
• RI-GCN enlarges receptive ﬁeld sizes and regularizes learned features by computing the descriptors stochastically. This strategy is effective in enhancing robustness to perturbations and outliers.
• RI-GCN presents great performance gain on 3D object classiﬁcation and segmentation benchmarks based on point cloud data, even without data augmentation with respect to rotation.
The rest of the paper is organized as follows. We ﬁrst discuss existing works about object recognition based on 3D point cloud data in Section 2. Section 3 describes the proposed approach to generate rotation-invariant local descriptors and construct graph convolutional neural networks using the features hierarchically. We present the experimental results on the standard benchmarks in Section 4, and make the conclusion in Section 5. 2