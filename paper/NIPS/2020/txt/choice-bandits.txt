Abstract
There has been much interest in recent years in the problem of dueling bandits, where on each round the learner plays a pair of arms and receives as feedback the outcome of a relative pairwise comparison between them. Here we study a natural generalization, that we term choice bandits, where the learner plays a set of up to k ≥ 2 arms, and receives limited relative feedback in the form of a single multiway choice among the pulled arms, drawn from an underlying multiway choice model. We study choice bandits under a very general class of choice models that is characterized by the existence of a unique ‘best’ arm (which we term generalized Condorcet winner), and includes as special cases the well-studied multinomial logit (MNL) and multinomial probit (MNP) choice models, and more generally, the class of random utility models with i.i.d. noise (IID-RUMs). We propose an algorithm for choice bandits, termed Winner Beats All (WBA), with a distribution dependent O(log T ) regret bound under all these choice models. The challenge in our setting is that the decision space is Θ(nk), which is large for even moderate k. Our algorithm addresses this challenge by extracting just O(n2) statistics from multiway choices and exploiting the existence of a unique ‘best’ arm to ﬁnd arms that are competitive to this arm in order to construct sets with low regret.
Since these statistics are extracted from the same choice observations, one needs a careful martingale analysis in order to show that these statistics are concentrated.
We complement our upper bound result with a lower bound result, which shows that our upper bound is order-wise optimal. Our experiments demonstrate that for the special case of k = 2, our algorithm is competitive with previous dueling bandit algorithms, and for the more general case k > 2, outperforms the recently proposed MaxMinUCB algorithm designed for the MNL model. 1

Introduction
The dueling bandit problem has received a lot of interest in recent years [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. Here there are n arms {1, . . . , n}; on each trial t, the learner pulls a pair of arms (it, jt), and receives relative feedback indicating which of the two arms has a better quality/reward.
In the regret minimization setting, the goal is to identify the ‘best’ arm(s) while also minimizing the regret due to playing sub-optimal arms in the learning (exploration) phase.
∗Work done while at the University of Pennsylvania. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In many applications, however, it can be natural for the learner to pull more than two arms at a time, and seek relative feedback among them. For example, in recommender systems, it is natural to display several items or products to a user, and seek feedback on the most preferred item among those shown. In online advertising, it is natural to display several ads at a time, and observe which of them is clicked (preferred). In online ranker evaluation for information retrieval, one can easily imagine a generalization of the setting studied by Yue & Joachims [15], where one may want to
"multi-leave" several rankers at a time to help identify the best ranking system while also presenting good/acceptable results to users using the system during the exploration phase. In general, there is also support in the marketing literature for showing customers more than two items at a time [16].
Motivated by such applications, we consider a framework that generalizes the dueling bandit problem to allow the learner to pull more than two arms at a time. Here, on each trial t, the learner pulls a set St of up to k arms (for ﬁxed k ∈ {2, . . . , n}), and receives relative feedback in the form of a multiway choice yt ∈ St indicating which arm in the set has the highest quality/reward. The goal of the learner is again to identify a ‘best’ arm (to be formalized below) while minimizing a suitable notion of regret that penalizes the learner for playing sub-optimal arms during the exploration phase.
We term the resulting framework choice bandits.
In the (stochastic) dueling bandits framework, the underlying probabilistic model from which feedback is observed is a pairwise comparison model, which for each pair of arms (i, j), deﬁnes a probability
Pij that arm i has higher reward/quality than arm j. In our choice bandits framework, the underlying probabilistic model is a multiway choice model, which for each set of arms S ⊆ [n] with |S| ≤ k and each arm i ∈ S, deﬁnes a probability Pi|S that arm i has the highest reward/quality in the set S.
We study choice bandits under a new class of choice models, that are characterized by the existence of a unique generalized Condorcet winner (GCW), which we deﬁne to be an arm that has larger probability of being chosen than any other arm in any choice set. This class includes as special cases the well-studied multinomial logit (MNL) [17, 18, 19] and multinomial probit (MNP) [20] choice models, and more generally, the class of random utility models with i.i.d. noise (IID-RUMs) [21, 22].
Our main contribution is a computationally efﬁcient algorithm, termed Winner Beats All (WBA), that achieves a distribution dependent O(n2 log n + n log T ) regret bound under any choice model that exhibits a unique GCW, where T is the time-horizon. We complement our upper bound result with an order-wise lower bound of Ω(n log T ) for any no-regret algorithm, showing that our algorithm has asymptotically order optimal regret under our general class of choice models. If the underlying model is MNL, then WBA achieves an instance-wise asymptotically optimal regret bound, which is better than the regret bound for the recent MaxMinUCB algorithm under MNL [23].
The main challenge in designing an algorithm under our framework is that the space of exploration (number of possible sets the learner can play) is Θ(nk) which is large even for moderate k. Therefore, it can be challenging to simultaneously explore/learn the choice sets with low regret out of the possible
Θ(nk) sets and exploit these low regret sets. We overcome these challenges by extracting just O(n2) pairwise statistics from the observed multiway choices under different sets, and using these statistics to ﬁnd choice sets with low regret. Since these pairwise statistics are extracted from multiway choices under different sets, a technical challenge is to show that these statistics are concentrated. We resolve this challenge by using a novel coupling argument that couples the stochastic process generating choices with another stochastic process, and showing that pairwise estimates according to this other process are concentrated. We believe that our results for efﬁcient learning under this large class of choice models that is considerably more general than the MNL class are of independent interest.
We also run experiments on several synthetic and real-world datasets. Our experiments on these datasets show that our algorithm for the special case of k = 2 is competitive as compared to previous dueling bandit algorithms, even though it is designed for a more general setting. For the case of k > 2, we compare our algorithm with the MaxMinUCB algorithm of [23] which was designed for the MNL model. We observe that our algorithm performs better in terms of regret than MaxMinUCB under all datasets (even under synthetic MNL datasets). We further observe that under several datasets the regret achieved by our algorithm for k > 2 is better than the regret for k = 2.