Abstract
We analyse the properties of an unbiased gradient estimator of the evidence lower bound (ELBO) for variational inference, based on the score function method with leave-one-out control variates. We show that this gradient estimator can be obtained using a new loss, deﬁned as the variance of the log-ratio between the exact posterior and the variational approximation, which we call the log-variance loss. Under certain conditions, the gradient of the log-variance loss equals the gradient of the (negative) ELBO. We show theoretically that this gradient estimator, which we call
VarGrad due to its connection to the log-variance loss, exhibits lower variance than the score function method in certain settings, and that the leave-one-out control variate coefﬁcients are close to the optimal ones. We empirically demonstrate that
VarGrad offers a favourable variance versus computation trade-off compared to other state-of-the-art estimators on a discrete variational autoencoder (VAE). 1

Introduction
Estimating the gradient of the expectation of a function is a problem with applications in many areas of machine learning, ranging from variational inference to reinforcement learning [Mohamed et al., 2019]. Different gradient estimators lead to different algorithms; two examples of estimators are the score function gradient [Williams, 1992] and the reparameterisation gradient [Kingma and Welling, 2014, Rezende et al., 2014, Titsias and Lázaro-Gredilla, 2014]. Many recent works develop new estimators with different properties (such as their variance); see Section 5 for a review.
We focus on variational inference (VI), where the goal is to approximate the posterior distribution p(z | x) of a model p(x, z), where x denotes the observations and z refers to the latent variables of the model [Jordan et al., 1999, Blei et al., 2017]. VI approximates the posterior using a parameterised family of distributions qφ(z), and ﬁnds the parameters φ by minimising the Kullback-Leibler (KL)
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
divergence from qφ(z) to p(z | x), i.e., KL(qφ(z) || p(z | x)). Since the KL is intractable, VI solves instead an equivalent problem that maximises the evidence lower bound (ELBO), given by
ELBO(φ) = Eqφ (cid:20) log (cid:21)
. p(x, z) qφ(z) (1)
Thus, VI casts the inference problem as an optimisation problem, which can be solved with stochastic optimisation tools when the ELBO is not available in closed form. In particular, VI forms a Monte
Carlo estimator of the gradient of the ELBO, ∇φELBO(φ).
In this paper, we analyse a multi-sample estimator of the gradient of the ELBO. In particular, we focus on an estimator ﬁrst introduced by Salimans and Knowles [2014] and Kool et al. [2019], which is based on the score function method [Williams, 1992] with leave-one-out control variates.
We ﬁrst show the connection between this estimator and an alternative divergence measure between the variational distribution qφ(z) and the exact posterior p(z | x). This divergence, which is different from the standard KL used in variational inference, is deﬁned as the variance, under some arbitrary distribution r(z), of the log-ratio log qφ(z) p(z | x) . We refer to this divergence as the log-variance loss.
Inspired by Nüsken and Richter [2020], we show that we recover the gradient estimator of Salimans and Knowles [2014] and Kool et al. [2019] by taking the gradient with respect to the variational parameters φ of the log-variance loss and evaluating the result at r(z) = qφ(z). Due to this property, we refer to the gradient estimator as VarGrad. This property also suggests a simple algorithm for computing the gradient estimator, based on differentiating through the log-variance loss.
We then study the relationship between VarGrad and the score function estimator [Williams, 1992,
Carbonetto et al., 2009, Paisley et al., 2012, Ranganath et al., 2014] with optimal control variate coefﬁcients. We show that the control variate coefﬁcients of VarGrad are close to the (intractable) optimal coefﬁcients. Indeed, we show both theoretically and empirically that the difference between both is small in many cases; for example when the KL from qφ(z) to the posterior is either small or large, which is generally the case in the late and early stages of the optimisation, respectively. This explains the success of the VarGrad estimator in a variety of settings [Kool et al., 2019, 2020].
Since it is based on the score function, VarGrad is a black-box, general purpose estimator because it makes no assumptions on the model p(x, z), such as differentiability with respect to the latent variables z. It introduces no additional parameters to be tuned and it is not computationally expensive.
In Section 6, we show empirically that VarGrad exhibits a favourable variance versus computation trade-off compared to other unbiased gradient estimators, including the score function gradient with control variates [Williams, 1992, Ranganath et al., 2014], REBAR [Tucker et al., 2017], RELAX
[Grathwohl et al., 2018], and ARM [Yin and Zhou, 2019]. 2