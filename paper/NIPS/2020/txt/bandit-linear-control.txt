Abstract
We consider the problem of controlling a known linear dynamical system under stochastic noise, adversarially chosen costs, and bandit feedback. Unlike the full feedback setting where the entire cost function is revealed after each decision, here only the cost incurred by the learner is observed. We present a new and eﬃcient algorithm that, for strongly convex and smooth costs, obtains regret that grows with the square root of the time horizon 𝑇. We also give extensions of this result to general convex, possibly non-smooth costs, and to non-stochastic system noise. A key component of our algorithm is a new technique for addressing bandit optimization of loss functions with memory.

Introduction 1
Reinforcement learning studies sequential decision making problems where a learning agent repeatedly interacts with an environment and aims to improve her strategy over time based on the received feedback. One of the most fundamental tradeoﬀs in reinforcement learning theory is the exploration vs. exploitation tradeoﬀ, that arises whenever the learner observes only partial feedback after each of her decisions, thus having to balance between exploring new strategies and exploiting those that are already known to perform well. The most basic and well-studied form of partial feedback is the so-called “bandit” feedback, where the learner only observes the cost of her chosen action on each decision round, while obtaining no information about the performance of other actions.
Traditionally, the environment dynamics in reinforcement learning are modeled as a Markov Decision
Process (MDP) with a ﬁnite number of possible states and actions. The MDP model has been studied and analyzed in numerous diﬀerent settings and under various assumptions on the transition parameters, the nature of the reward functions, and the feedback model. Recently, a particular focus has been given to continuous state-action MDPs, and in particular, to a speciﬁc family of models in classic control where the state transition function is linear. Concretely, in linear control the state evolution follows the linear dynamics:
𝑥𝑡+1 = 𝐴★𝑥𝑡 + 𝐵★𝑢𝑡 + 𝑤𝑡 , where 𝑥𝑡 ∈ ℝ𝑑𝑥 , 𝑢𝑡 ∈ ℝ𝑑𝑢 , 𝑤𝑡 ∈ ℝ𝑑𝑥 are respectively the system state, action (control), and noise at round 𝑡, and 𝐴★ ∈ ℝ𝑑𝑥 ×𝑑𝑥 , 𝐵★ ∈ ℝ𝑑𝑥 ×𝑑𝑢 are the system parameters. The goal is to minimize the total control costs with respect to cost function 𝑐𝑡 (𝑥, 𝑢) : ℝ𝑑𝑥 × ℝ𝑑𝑢 → ℝ put forward on round 𝑡.
However, in contrast to the reinforcement learning literature, existing work on learning in linear control largely assumes the full feedback model, where after each decision round the learning agent observes the entire cost function 𝑐𝑡 used to assign costs on the same round. In fact, to the best of our knowledge, thus far linear control has not been studied in the bandit setting, even in the special case where the costs are generated stochastically over time. (1)
Contributions. In this paper, we introduce and study the bandit linear control problem, where a learning agent has to control a known linear dynamical system (as in Eq. (1)) under stochastic noise, adversarially chosen convex cost functions, and bandit feedback. Namely, after each decision round 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the learner only observes the incurred cost 𝑐𝑡 (𝑥𝑡 , 𝑢𝑡 ) as feedback. (We still assume, however, that the state evolution is fully observable.) For strongly convex and smooth cost functions, we present an eﬃcient bandit algorithm that achieves ˜𝑂 (
𝑇) regret over 𝑇 decision rounds, with a polynomial dependence on the natural problem parameters. This result is optimal up to polylogarithmic factors as it matches the optimal regret rate in the easier stationary (i.e., stateless) strongly convex and smooth bandit optimization setting [26, 17].
√
The starting point of our algorithmic approach is an approximate reparameterization of the online control problem due to [1, 2], called the Disturbance-Action Policy. In this new parameterization, the control problem is cast in terms of bounded memory convex loss functions, under which the cost of the learner on each round depends explicitly only on her last few decisions rather than on the entire history (this is thanks to strong stability conditions of the learned policies [9]).
As a key technical tool, we develop a new reduction technique for addressing bandit convex optimization with bounded memory. While an analogous reduction has been well established in the full feedback model [3, 1, 2], its adaptation to bandit feedback is far from straightforward. Indeed, loss functions with memory in the bandit setting were previously studied by [4], that showed a black-box reduction via a mini-batching approach that, for an algorithm achieving 𝑂 (𝑇 1/2) regret in the no-memory bandit setting, achieves 𝑂 (𝑇 2/3) regret with memory. While this technique imposes very few restrictions on the adversary, it degrades performance signiﬁcantly even for adversaries with bounded memory. In contrast, [3] show that the full-feedback setting enjoys nearly no degradation when the adversary’s memory is ﬁxed and bounded. Our new technique establishes a similar lossless reduction for bandit feedback with adversaries restricted to choosing smooth cost functions. Combining these ideas with standard techniques in (no-memory) bandit convex optimization [15, 25, 17] gives our main result.
Our techniques readily extend to weakly convex costs with regret scaling as ˜𝑂 (𝑇 2/3) in the smooth case and ˜𝑂 (𝑇 3/4) without smoothness. Moreover, these hold even without the stochastic assumptions on the system noise 𝑤𝑡 , which were only required in our analysis for preserving the strong convexity of the costs through the reduction to loss functions with memory. We defer further details on these extensions to later sections and choose to focus ﬁrst on the more challenging case—demonstrating how both the strong convexity and smoothness of the costs are exploited—and where ˜𝑂 (
𝑇) rates are possible.
√