Abstract
We present a deep learning algorithm for the numerical solution of parametric fam-ilies of high-dimensional linear Kolmogorov partial differential equations (PDEs).
Our method is based on reformulating the numerical approximation of a whole family of Kolmogorov PDEs as a single statistical learning problem using the
Feynman-Kac formula. Successful numerical experiments are presented, which empirically conﬁrm the functionality and efﬁciency of our proposed algorithm in the case of heat equations and Black-Scholes option pricing models parametrized by afﬁne-linear coefﬁcient functions. We show that a single deep neural network trained on simulated data is capable of learning the solution functions of an entire family of PDEs on a full space-time region. Most notably, our numerical observa-tions and theoretical results also demonstrate that the proposed method does not suffer from the curse of dimensionality, distinguishing it from almost all standard numerical methods for PDEs. 1

Introduction
Linear parabolic partial differential equations (PDEs) of the form 2 Trace (cid:0)σγ[σγ]∗∇2 (cid:1) + (cid:104)µγ, ∇xuγ(cid:105),
∂uγ
∂t = 1 xuγ uγ(x, 0) = ϕγ(x), (1) are referred to as Kolmogorov PDEs, see [23] for a thorough study of their mathematical properties.
Throughout this paper, the functions
ϕγ : Rd → R (initial condition) and σγ : Rd → Rd×d, µγ : Rd → Rd (coefﬁcient maps)
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
are continuous, and are implicitly determined by a real parameter vector γ ∈ D, whereby D is a compact set in Euclidean space.
Kolmogorov PDEs frequently appear in applications in a broad variety of scientiﬁc disciplines, including physics and ﬁnancial engineering [9, 42, 58]. In particular, note that the heat equation from physical modelling as well as the widely-known Black-Scholes equation from computational ﬁnance are important special cases of Equation (1). Typically, one is interested in ﬁnding the (viscosity) solution2 uγ : [v, w]d × [0, T ] → R of Equation (1) on a predeﬁned space-time region of the form [v, w]d × [0, T ]. In almost all cases, however, Kolmogorov PDEs cannot be solved explicitly. Furthermore, standard numerical solution algorithms for PDEs, in particular those based on a discretization of the considered domain, are known to suffer from the so-called curse of dimensionality3, meaning that their computational cost grows exponentially in the dimension of the domain [2, 51]. The development of new, computationally efﬁcient methods for the numerical solution of Kolmogorov PDEs is therefore of high interest for applied scientists.
In this work, we present a novel deep learning algorithm capable of numerically approximating the solutions (uγ)γ∈D of a whole family of γ-parametrized Kolmogorov PDEs on a full space-time region. Speciﬁcally, our proposed method allows to train a single deep neural network
Φ : D × [v, w]d × [0, T ] → R (2) to approximate the parametric solution map
¯u : D × [v, w]d × [0, T ] → R, (γ, x, t) (cid:55)→ ¯u(γ, x, t) := uγ(x, t), (3) of a family of γ-parametrized Kolmogorov PDEs on the generalized domain D × [v, w]d × [0, T ].
Most notably, we also theoretically investigate the associated approximation and generalization errors and rigorously show that our algorithm does not suffer from the curse of dimensionality with respect to the neural network size as well as the sample size. We emphasize that our empirical results strongly suggest that also the empirical risk minimization (ERM) algorithm, usually a variant of stochastic gradient descent, does not suffer from the curse of dimensionality but proving this is out of scope of this paper. 1.1 PDEs and Deep Learning: Current Research and