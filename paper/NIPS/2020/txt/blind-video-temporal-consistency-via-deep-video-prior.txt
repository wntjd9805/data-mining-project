Abstract
Applying image processing algorithms independently to each video frame often leads to temporal inconsistency in the resulting video. To address this issue, we present a novel and general approach for blind video temporal consistency. Our method is only trained on a pair of original and processed videos directly instead of a large dataset. Unlike most previous methods that enforce temporal consistency with optical ﬂow, we show that temporal consistency can be achieved by training a convolutional network on a video with the Deep Video Prior. Moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. We demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. Extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. Our source codes are publicly available at github.com/ChenyangLEI/deep-video-prior. 1

Introduction
Numerous image processing algorithms have demonstrated great performance in single image process-ing tasks [9, 15, 23, 38, 41], but applying them directly to videos often results in undesirable temporal inconsistency (e.g., ﬂickering). To encourage video temporal consistency, most researchers design speciﬁc methods for different video processing tasks [20, 25, 28] such as video colorization [21], video denoising [24] and video super resolution [32]. Although task-speciﬁc video processing al-gorithms can improve temporal coherence, it is unclear or challenging to apply similar strategies to other tasks. Therefore, a generic framework that can turn an image processing algorithm into its video processing counterpart with strong temporal consistency is highly valuable. In this work, we study a novel approach to obtain a temporally consistent video from a processed video, which is a video after independently applying an image processing algorithm to each frame of an input video.
Prior work has studied general frameworks instead of task-speciﬁc solutions to improve temporal consistency [3, 7, 19, 39]. Bonneel et al. [3] present a general approach that is blind to image processing operators by minimizing the distance between the output and the processed video in the gradient domain and a warping error between two consecutive output frames. Based on this approach, Yao et al. [39] further leverage more information from a key frame stack for occluded areas. However, these two methods assume that the output and processed videos are similar in the gradient domain, which may not hold in practice. To address this issue, Lai et al. [19] maintain the perceptual similarity with processed videos by adopting a perceptual loss [16]. In addition to blind video temporal consistency methods, Eilertsen et al. [7] propose a framework to ﬁnetune a convolutional network (CNN) by enforcing regularization on transform invariance if the pretrained
CNN is available. Moreover, most approaches [3, 19, 39] enforce the regularization based on dense correspondence (e.g., optical ﬂow or PatchMatch [1]), and the long-term temporal consistency often degrades.
∗Joint ﬁrst authors 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of unimodal and multimodal inconsistency. In unimodal inconsistency, similar input video frames are mapped to moderately different processed frames within the same mode by f .
In multimodal inconsistency, similar input video frames may be mapped to processed frames within two or more modes by f . A function g is to improve the temporal consistency for these two cases.
We propose a general and simple framework, utilizing the Deep Video Prior by training a convolutional network on videos: the outputs of CNN for corresponding patches in video frames should be consistent. This prior allows recovering most video information ﬁrst before the ﬂickering artifacts are eventually overﬁtted. Our framework does not enforce any handcrafted temporal regularization to improve temporal consistency, while previous methods are built upon enforcing feature similarity for correspondences among video frames [3, 19, 39]. Our idea is related to DIP (Deep Image Prior [37]), which observes that the structure of a generator network is sufﬁcient to capture the low-level statistics of a natural image. DIP takes noise as input and trains the network to reconstruct an image. The network performs effectively to inverse problems such as image denoising, image inpainting, and super-resolution. For instance, the noise-free image will be reconstructed before the noise since it follows the prior represented by the network. We conjecture that the ﬂickering artifacts in a video are similar to the noise in the temporal domain, which can be corrected by deep video prior.
Our method only requires training on the single test video, and no training dataset is needed. Training without large-scale data has been adopted commonly in internal learning [34, 37]. In addition to
DIP [37], various tasks [8, 33, 35, 40] show that great performance can be achieved by using only test data.
As another contribution, we further propose a carefully designed iteratively reweighted training (IRT) strategy to address the challenging multimodal inconsistency problem. Multimodal inconsistency may appear in a processed video. Our method selects one mode from multiple possible modes to ensure temporal consistency and preserve perceptual quality. We apply our method to diverse computer vision tasks. Results show that although our method and implementation are simple, we do not only show better temporal consistency but also suffer less performance degradation compared with current state-of-the-art methods. 2