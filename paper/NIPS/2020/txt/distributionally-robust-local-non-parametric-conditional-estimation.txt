Abstract
Conditional estimation given speciﬁc covariate values (i.e., local conditional estima-tion or functional estimation) is ubiquitously useful with applications in engineer-ing, social and natural sciences. Existing data-driven non-parametric estimators mostly focus on structured homogeneous data (e.g., weakly independent and sta-tionary data), thus they are sensitive to adversarial noise and may perform poorly under a low sample size. To alleviate these issues, we propose a new distribution-ally robust estimator that generates non-parametric local estimates by minimizing the worst-case conditional expected loss over all adversarial distributions in a
Wasserstein ambiguity set. We show that despite being generally intractable, the local estimator can be efﬁciently found via convex optimization under broadly applicable settings, and it is robust to the corruption and heterogeneity of the data.
Experiments with synthetic and MNIST datasets show the competitive performance of this new class of estimators. 1

Introduction
We consider the estimation of conditional statistics of a response variable, Y ∈ Rm, given the value of a predictor or covariate X ∈ Rn. The single most important instance of these types of problems involves estimating the conditional mean, or also known as the regression function. Under ﬁnite variance assumptions, the conditional mean EP[Y |X = x0] is technically deﬁned as ψ(cid:63)(x0) for some measurable function ψ(cid:63) that solves the minimum mean square error problem min
ψ
EP[(cid:107)Y − ψ(X)(cid:107)2 2], where the minimization is taken over the space of all measurable functions from Rn to Rm. While the optimal solution ψ(cid:63) is unique up to sets of P-measure zero, unfortunately, solving for ψ(cid:63) is challenging because it is an inﬁnite-dimensional optimization problem. The regression function ψ(cid:63) can be efﬁciently found only under speciﬁc settings, for example, if one assumes that (X, Y ) follows a jointly Gaussian distribution. However, these speciﬁc situations are overly restrictive in practice.
In order to bypass the inﬁnite-dimensional challenge involved in directly computing ψ(cid:63), we may instead consider a family of optimization problems that are parametrized by x0. More speciﬁcally, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
in the presence of a regular conditional distribution, the conditional mean EP[Y |X = x0] can be estimated pointwise by (cid:98)β deﬁned as (cid:98)β ∈ arg min
β
EP[(cid:107)Y − β(cid:107)2 2|X = x0] for any covariate value x0 of interest. This presents the challenge of effectively accessing the conditional distribution, which is particularly difﬁcult if the event X = x0 has P-probability zero.
Using an analogous argument, if we are interested in the conditional (τ × 100%)-quantile of Y given
X, then this conditional statistics can be estimated pointwise at any location x0 of interest by (cid:98)β ∈ arg min
β
EP[max{−τ (Y − β), (1 − τ )(Y − β)}|X = x0].
The previous examples illustrate that the estimation of a wide range of conditional statistics can be recast into solving a family of ﬁnite-dimensional optimization problems parametrically in x0
EP[(cid:96)(Y, β)|X = x0] min
β (1) with an appropriately chosen statistical loss function (cid:96).
Problem (1) poses several challenges, some of which were alluded to earlier. First, it requires the integration with respect to a difﬁcult to compute conditional probability distribution. Second, the probability measure P is generally unknown, hence we lack a fundamental input to solve (1). Finally, in a data-driven setting, there may be few, or even no, observations with value covariate X = x0.
To alleviate these difﬁculties, our formulation, as we shall explain, involves two features. First, we consider a relaxation of problem (1) in which the event X = x0 is replaced by a neighborhood
Nγ(x0) of a suitable radius γ ≥ 0 around x0. Second, we introduce a data-driven distributionally robust optimization (DRO) formulation (e.g. [8, 5, 21]) in order to mitigate the problem that P is unknown. In turn, the DRO formulation involves a novel class of conditional ambiguity set which copes with the underlying conditional distribution being unknown.
In particular, we propose the following distributionally robust local conditional estimation problem min
β sup
Q∈B∞
ρ ,Q(X∈Nγ (x0))>0
EQ (cid:2)(cid:96)(Y, β)|X ∈ Nγ(x0)(cid:3), (2) where the maximization is taken over all probability measures Q that are within ρ distance in the
∞-Wasserstein sense of a benchmark nominal model, which often corresponds to the empirical distribution of available data. The probability measures Q are constrained so that Q(X ∈ Nγ(x0)) > 0 to eliminate the complication of conditioning on a set of measure zero.
Contributions. Resting on formulation (2), our main contributions are summarized as follows. 1. We introduce a novel paradigm of non-parametric local conditional estimation based on distribu-tionally robust optimization. In contrast to classical non-parametric conditional estimators, our new class of estimators are endowed by design with robustness features. They are structurally built to mitigate the impact of model contamination and therefore they may be reasonably applied to heterogeneous data (e.g., non i.i.d. input). 2. We demonstrate that when the ambiguity set is a type-∞ Wasserstein ball around the empirical measure, the proposed min-max estimation problem can be efﬁciently solved in many applicable settings, including notably the local conditional mean and quantile estimation. 3. We show that this class of type-∞ Wasserstein local conditional estimators can be considered as a systematic robustiﬁcation of the k-nearest neighbor estimator. We also provide further insights on the statistical properties of our approach and empirical evidence, with both a synthetic and real data sets, that our approach can provide more accurate estimations in practically relevant settings.