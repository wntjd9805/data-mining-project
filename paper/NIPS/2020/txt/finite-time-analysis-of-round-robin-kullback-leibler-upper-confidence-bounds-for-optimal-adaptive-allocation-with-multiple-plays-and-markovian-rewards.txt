Abstract
We study an extension of the classic stochastic multi-armed bandit problem which involves multiple plays and Markovian rewards in the rested bandits setting. In order to tackle this problem we consider an adaptive allocation rule which at each stage combines the information from the sample means of all the arms, with the
Kullback-Leibler upper conﬁdence bound of a single arm which is selected in round-robin way. For rewards generated from a one-parameter exponential family of Markov chains, we provide a ﬁnite-time upper bound for the regret incurred from this adaptive allocation rule, which reveals the logarithmic dependence of the regret on the time horizon, and which is asymptotically optimal. For our analysis we devise several concentration results for Markov chains, including a maximal inequality for Markov chains, that may be of interest in their own right. As a byproduct of our analysis we also establish asymptotically optimal, ﬁnite-time guarantees for the case of multiple plays, and i.i.d. rewards drawn from a one-parameter exponential family of probability densities. Additionally, we provide simulation results that illustrate that calculating Kullback-Leibler upper conﬁdence bounds in a round-robin way, is signiﬁcantly more efﬁcient than calculating them for every arm at each round, and that the expected regrets of those two approaches behave similarly. 1

Introduction
In this paper we study a generalization of the stochastic multi-armed bandit problem, where there are
K independent arms, and each arm a ∈ [K] = {1, . . . , K} is associated with a parameter θa ∈ R, and modeled as a discrete time stochastic process governed by the probability law Pθa . A time horizon
T is prescribed, and at each round t ∈ [T ] = {1, . . . , T } we select M arms, where 1 ≤ M ≤ K, without any prior knowledge of the statistics of the underlying stochastic processes. The M stochastic processes that correspond to the selected arms evolve by one time step, and we observe this evolution through a reward function, while the stochastic processes for the rest of the arms stay frozen, i.e. we consider the rested bandits setting. Our goal is to select arms in such a way so as to make the cumulative reward over the whole time horizon T as large as possible. For this task we are faced with an exploitation versus exploration dilemma. At each round we need to decide whether we are going to exploit the best M arms according to the information that we have gathered so far, or we are going to explore some other arms which do not seem to be so rewarding, just in case that the rewards we have observed so far deviate signiﬁcantly from the expected rewards. The answer to this dilemma is usually coming by calculating indices for the arms and ranking them according to those indices, which should incorporate both information on how good an arm seems to be as well as on how many 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
times it has been played so far. Here we take an alternative approach where instead of calculating the indices for all the arms at each round, we just calculate the index for a single arm in a round-robin way. 1.1 Contributions 1. We ﬁrst consider the case that the K stochastic processes are irreducible Markov chains, coming from a one-parameter exponential family of Markov chains. The objective is to play as much as possible the M arms with the largest stationary means, although we have no prior information about the statistics of the K Markov chains. The difference of the best possible expected rewards coming from those M best arms and the expected reward coming from the arms that we played is the regret that we incur. To minimize the regret we consider an index based adaptive allocation rule, Algorithm 1, which is based on sample means and
Kullback-Leibler upper conﬁdence bounds for the stationary expected rewards using the
Kullback-Leibler divergence rate. We provide a ﬁnite-time analysis, Theorem 1, for this
KL-UCB adaptive allocation rule which shows that the regret depends logarithmically on the time horizon T , and matches exactly the asymptotic lower bound, Corollary 1. 2. In order to make the ﬁnite-time guarantee possible we devise several deviation lemmata for Markov chains. An exponential martingale for Markov chains is proven, Lemma 3, which leads to a maximal inequality for Markov chains, Lemma 1. In the literature there are several approaches that use martingale techniques either to derive Hoeffding inequalities for
Markov chains [15, 30], or more generally to study concentration of measure for Markov chains [24, 25, 26, 34, 27, 8, 19, 33]. Nonetheless, they’re all based either on Dynkin’s martingale or on Doob’s martingale, combined with coupling ideas, and there is no evidence that they can lead to maximal inequalities. Moreover, a Chernoff bound for Markov chains is devised, Lemma 2, and its relation with the work of [31] is discussed in Remark 4. 3. We then consider the case that the K stochastic processes are i.i.d. processes, each corre-sponding to a density coming from a one-parameter exponential family of densities. We establish, Theorem 2, that Algorithm 1 still enjoys the same ﬁnite-time regret guarantees, which are asymptotically optimal. The case where Theorem 2 follows directly from The-orem 1 is discussed in Remark 2. The setting of single plays is studied in [7], but with a much more computationally intense adaptive allocation rule. 4. In Section 6 we provide simulation results illustrating the fact that round-robin KL-UCB adaptive allocation rules are much more computationally efﬁcient than KL-UCB adaptive allocation rules, and similarly round-robin UCB adaptive allocation rules are more compu-tationally efﬁcient than UCB adaptive allocation rules, while the expected regrets, in each family of algorithms, behave in a similar way. This brings to light round-robin schemes as an appealing practical alternative to the mainstream schemes that calculate indices for all the arms at each round. 1.2 Motivation
Multi-armed bandits provide a simple abstract statistical model that can be applied to study real world problems such as clinical trials, ad placement, gambling, adaptive routing, resource allocation in computer systems etc. We refer the interested reader to the survey of [6] for more context, and to the recent books of [21, 35]. The need for multiple plays can be understood in the setting of resource allocation. Scheduling jobs to a single CPU is an instance of the multi-armed bandit problem with a single play at each round, where the arms correspond to the jobs. If there are multiple CPUs we get an instance of the multi-armed bandit problem with multiple plays. The need of a richer model which allows the presence of Markovian dependence is illustrated in the context of gambling, where the arms correspond to slot-machines. It is reasonable to try to model the assertion that if a slot-machine produced a high reward the n-th time played, then it is very likely that it will produce a much lower reward the (n + 1)-th time played, simply because the casino may decide to change the reward distribution to a much stingier one if a big reward was just produced. This assertion requires, the reward distributions to depend on the previous outcome, which is precisely captured by the Markovian reward model. Moreover, we anticipate this to be an important problem attempting to bridge classical stochastic bandits, controlled Markov chains (MDPs), and non-stationary bandits. 2
Table 1: Summary of relevant results about regret minimization for stochastic multi-armed bandits work
[20]
[3]
[1]
[4]
[37]
[12] this work rewards i.i.d.
Markovian i.i.d. i.i.d.
Markovian i.i.d.
Markovian analysis indices round-robin KL-UCB asymptotic round-robin KL-UCB asymptotic asymptotic
UCB
ﬁnite-time
UCB
ﬁnite-time
UCB
KL-UCB
ﬁnite-time round-robin KL-UCB ﬁnite-time 1.3