Abstract
Imitation learning is a popular approach for teaching motor skills to robots. How-ever, most approaches focus on extracting policy parameters from execution traces alone (i.e., motion trajectories and perceptual data). No adequate communication channel exists between the human expert and the robot to describe critical aspects of the task, such as the properties of the target object or the intended shape of the motion. Motivated by insights into the human teaching process, we introduce a method for incorporating unstructured natural language into imitation learning. At training time, the expert can provide demonstrations along with verbal descriptions in order to describe the underlying intent (e.g., “go to the large green bowl”). The training process then interrelates these two modalities to encode the correlations between language, perception, and motion. The resulting language-conditioned visuomotor policies can be conditioned at runtime on new human commands and instructions, which allows for more ﬁne-grained control over the trained policies while also reducing situational ambiguity. We demonstrate in a set of simulation experiments how our approach can learn language-conditioned manipulation poli-cies for a seven-degree-of-freedom robot arm and compare the results to a variety of alternative methods. 1

Introduction
Learning robot control policies by imitation [31] is an appealing approach to skill acquisition and has been successfully applied to several tasks, including locomotion, grasping, and even table tennis [8, 2, 25]. In this paradigm, expert demonstrations of robot motion are ﬁrst recorded via kinesthetic teaching, teleoperation, or other input modalities. These demonstrations are then used to derive a control policy that generalizes the observed behavior to a larger set of scenarios that allow for responses to perceptual stimuli (e.g., joint angles and an RGBD camera image of the work environment) with appropriate actions (e.g., moving a table-tennis paddle to hit an incoming ball).
In goal-conditioned tasks, perceptual inputs alone may be insufﬁcient to dictate optimal actions
[10] (e.g., without a target object, what should a picking robot retrieve from a bin when activated?).
Consequently, expert demonstration and control policies must also be conditioned on a representation of the goal. While we use the term goals, it may refer to end goals (e.g., target objects) or constraints on motion (e.g., minimizing end-point effector acceleration) [14]. Prior work has typically employed manually designed goal speciﬁcations (e.g., vectors indicating a target position, a one-hot vector indicating target objects, or a single value indicating the execution speed). However, this is an inﬂexible approach that must be pre-deﬁned before training and cannot be modiﬁed after deployment.
In the present work, we consider language as a ﬂexible goal speciﬁcation for imitation learning in manipulation tasks. As shown in Fig. 1(center), we consider a seven-degree-of-freedom robot 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Overview of the general system architecture. (Left) Details of the controller model, which synthesizes robot control signals . (Right) details of the semantic model, which extracts critical information about the task from both perceptual input and language commands. Dark-blue boxes indicate pre-trained components of our model. manipulator anchored to a ﬂat workspace populated with a set of objects that vary in shape, size, and color. The agent is instructed by a user to manipulate these objects in language for picking (e.g., “grab the blue cup”) and pouring tasks (e.g., “pour some of its contents into the small red bowl”). In order to succeed, the agent must relate these instructions to the objects in the environment, as well as constraints on how they are manipulated (e.g., pouring some or all of something require different motions). We examine the role of imitation learning from demonstrations in this setting that consist of developing a training set of instructions and associated robot motion trajectories.
We developed an end-to-end model for the language-conditioned control of an articulated robotic arm – mapping directly from observation pixels and language-speciﬁed goals to motor control. We conceptually divided our architecture into two modules: a high-level semantic network that encodes goals and the world state and a lower-level controller network that uses the higher encoding to generate suitable control policies. Our high-level semantic network must relate language-speciﬁed goals, visual observation of the work environment, and the robot’s current joint positions into a single encoding.
To do this, we leveraged advances in attention mechanisms from vision-and-language research [3] to associate instructions and target objects. Our low-level controller synthesizes parameters of a motor primitive that speciﬁes the entire future motion trajectory, providing insight into the predicted future behavior of the robot from the current observation. The model was trained end-to-end to reproduce demonstrated behavior while minimizing a set of auxiliary losses to guide the intermediate outputs.
We evaluated our model in a dynamic-enabled simulator with random assortments of objects and procedurally generated instructions, with success in 84% of sequential tasks that required picking up a cup and pouring its contents into another vessel. This result signiﬁcantly outperformed state-of-the-art baselines. We provided detailed ablations of modeling decisions and auxiliary losses, as well as detailed analysis of our model’s generalization to combinations of modiﬁers (color, shape, size, and pour-quantity speciﬁers). We also assessed robustness to visual and physical perturbations in the environments. While our model was trained on synthetic language, we also ran human-user experiments with free-form natural-language instructions for picking/pouring tasks, with a success rate of 64% for these instructions.
All data used in this paper, along with a trained model and the full source code can be found at: https://github.com/ir-lab/LanguagePolicies. The release features a number of videos and examples on how to train and validate language-conditioned control policies in a physics-based simulation environment. Additionally, detailed information about the experimental setup and the human data collection process can be found under the link above.
Contributions. To summarize our contributions, we – introduced a language-conditioned manipulation task setting in a dynamically accurate simulator, – provided a natural-language interface which allows laymen users to provide robot task speciﬁca-tions in an intuitive fashion, – developed an end-to-end, language-conditioned control policy for manipulation tasks composed of a high-level semantic module and low-level controller, integrating language, vision, and control within a single framework, – demonstrated that our model, trained with imitation learning, achieved a high success rate on both synthetic instructions and unstructured human instructions. 2
2