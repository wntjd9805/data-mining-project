Abstract
Exploration is one of the core challenges in reinforcement learning. A common formulation of curiosity-driven exploration uses the difference between the real future and the future predicted by a learned model [1]. However, predicting the future is an inherently difﬁcult task which can be ill-posed in the face of stochastic-ity. In this paper, we introduce an alternative form of curiosity that rewards novel associations between different senses. Our approach exploits multiple modalities to provide a stronger signal for more efﬁcient exploration. Our method is inspired by the fact that, for humans, both sight and sound play a critical role in exploration.
We present results on several Atari environments and Habitat (a photorealistic navi-gation simulator), showing the beneﬁts of using an audio-visual association model for intrinsically guiding learning agents in the absence of external rewards. For videos and code, see https://vdean.github.io/audio-curiosity.html. 1

Introduction
Many successes in reinforcement learning (RL) have come from agents maximizing a provided extrinsic reward such as a game score. However, in real-world settings, reward functions are hard to formulate and require signiﬁcant human engi-neering. On the other hand, humans explore the world driven by intrinsic motivation, such as cu-riosity, often in the absence of rewards. But what is curiosity and how would one formulate it?
Recent work in RL [1–3] has focused on curios-ity using future prediction. In this formulation, an exploration policy receives rewards for ac-tions that lead to differences between the real future and the future predicted by a forward dy-In turn, the dynamics model namics model. improves as it learns from novel states. While the core idea behind this curiosity formulation is simple, putting it into practice is quite challeng-ing. Learning and modeling forward dynamics is still an open research problem; it is unclear how to handle multiple possible futures, whether to explicitly incorporate physics, or even what the right prediction space is (pixel space or some latent space).
Figure 1: See, Hear, Explore: We propose a for-mulation of curiosity that encourages the agent to explore novel associations between modalities, such as audio and vision. In Habitat, shown above, our method allows for more efﬁcient exploration than baselines. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The use of multiple modalities in human learning has a long history. Research in psychology has suggested that humans look for incongruity [4]. A baby might hit an object to hear what it sounds like. Have you ever found yourself curious to touch a material different from anything you have seen before? Humans are drawn towards discovering and exploring novel associations between different modalities. Dember and Earl [5] argued that intrinsic motivation arises with discrepancy between expected sensory perception and the actual stimulus. More recent work has shown the presence of multimodal stimulation and exploration in infants [6, 7]. In cognitive development, both sight and sound guide exploration: babies are drawn towards colorful toys that squeak and rattle [8].
Inspired by human exploration, we introduce See Hear Explore (SHE): a curiosity for novel associa-tions between sensory modalities (Figure 1). SHE rewards actions that generate novel associations (shared information) between different sensory modalities (in our case, pixels and sounds). We ﬁrst demonstrate that our formulation is useful in several Atari games: SHE allows for more exploration, is more sample-efﬁcient, and is more robust to noise compared to existing curiosity baselines on these environments. Finally, we show experiments on area exploration in the realistic Habitat simulator [9].
Our results demonstrate that in this setting our approach signiﬁcantly outperforms baselines.
To summarize, our contributions in this paper include: 1) SHE, a curiosity formulation that searches for novel associations in the world. To the best of our knowledge, multimodal associations have not been investigated in self-supervised exploration; 2) we show our approach outperforms the commonly-used curiosity approaches on standard Atari benchmark tasks; 3) most importantly, multimodality is one of the most basic facets of our rich physical world (audio and vision are generated by the same physical processes [10]). We show experiments on realistic area exploration in which SHE signiﬁcantly outperforms baselines. This work builds on efﬁcient exploration, which will be crucial as we push agents to explore more complex unknown environments. 2