Abstract
Existing Graph Neural Network (GNN) methods that learn inductive unsupervised graph representations focus on learning node and edge representations by predicting observed edges in the graph. Although such approaches have shown advances in downstream node classiﬁcation tasks, they are ineffective in jointly representing larger k-node sets, k>2. We propose MHM-GNN, an inductive unsupervised graph representation approach that combines joint k-node representations with energy-based models (hypergraph Markov networks) and GNNs. To address the intractability of the loss that arises from this combination, we endow our optimization with a loss upper bound using a ﬁnite-sample unbiased Markov Chain
Monte Carlo estimator. Our experiments show that the unsupervised joint k-node representations of MHM-GNN produce better unsupervised representations than existing approaches from the literature. 1

Introduction
Inductive unsupervised learning using Graph Neural Networks (GNNs) in (dyadic) graphs is currently restricted to node and edge representations due to their reliance on edge-based losses [7, 15, 21, 46]. If we want to tackle downstream tasks that require jointly reasoning about k > 2 nodes, but whose input data are dyadic relations (i.e., standard graphs) rather than hyperedges, we must develop techniques that can go beyond edge-based losses.
Joint k-node representation tasks with dyadic relational inputs include drone swarms that com-municate amongst themselves to jointly act on a task [41, 43], but also include more traditional product-recommendation tasks. For instance, an e-commerce website might want to predict which k products could be jointly purchased in the same shopping cart, while the database only records (product, product) dyads to safeguard user information.
Srinivasan and Ribeiro [42] have recently shown that GNN node representations are insufﬁcient to capture joint characteristics of k nodes that are unique to this group of nodes. Indeed, our experiments show that using existing unsupervised GNN —with their node representations and edge losses— one cannot accurately detect these k-product carts on an e-commerce website. Unfortunately, existing
GNN extensions that give joint k-node representations require supervised graph-wide losses [27, 25], leaving a signiﬁcant gap between edge and supervised whole-graph losses (i.e., we need multiple to obtain true labeled graphs for these to work). The main reason for this gap is scalability: unsupervised joint k-node representations, one must optimize a model deﬁned over all k-node induced subgraphs of a graph. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The proposed unsupervised graph representation using motif compositions. Here, we present the MHM-GNN model from Equation (1), the energy estimator (cid:98)Φ from Equation (4), the motif energy and representation from Equation (2).
Our approach MHM-GNN (Motif Hypergraph Markov Graph Neural Networks) leverages the compositionality of hypergraph Markov network models (HMNs) [38, 54, 22] that allows us to deﬁne an unsupervised objective (energy-based model) over GNN representations of motifs (see upper half of Figure 1).
Scalability is the main challenge we have to overcome, a type of scalability issue not addressed in the hypergraph Markov network literature [38, 54, 22]. First, there is the traditional likelihood intractability associated with computing the partition function Z(W) of energy models —Z(W) is shown in the likelihood P(A, X|W) in Figure 1 and also in Equation (1). There are standard solutions for this challenge (e.g., Noise-Contrastive Estimation (NCE) [14]). The more vexing challenge comes from the intractability created by our inductive graph representation that applies motif representations to all k-node subgraphs, which requires (cid:0)n (cid:1) operations per gradient step, typically with n (cid:29) k. To make this step tractable, we leverage recent advances in ﬁnite-sample unbiased Markov Chain Monte
Carlo estimation for sums of subgraph functions over large graphs [45]. This unbiased estimate, combined with Jensen’s inequality, allows us to optimize a lower bound on the intractable likelihood (assuming Z(W) is known). Fold that into the asymptotics of NCE and we get a principled, tractable optimization. k
Contributions. Our contributions are three-fold. First, we introduce MHM-GNN, which produces joint (k > 2)-node representations, where k is a hyperparameter of the model. Second, we introduce a principled and scalable stochastic optimization method that learns MHM-GNN with a ﬁnite-sample unbiased estimator of the graph energy (see Fig. 1) and a NCE objective. Finally, we show how the joint k-node representations from MHM-GNN produce better unsupervised joint k-node representations than existing approaches that aggregate node representations. Our code is available at https://github.com/PurdueMINDS/minds-mhm-gnn. 2