Abstract
We present ShapeFlow, a ﬂow-based model for learning a deformation space for entire classes of 3D shapes with large intra-class variations. ShapeFlow allows learning a multi-template deformation space that is agnostic to shape topology, yet preserves ﬁne geometric details. Different from a generative space where a latent vector is directly decoded into a shape, a deformation space decodes a vector into a continuous ﬂow that can advect a source shape towards a target. Such a space naturally allows the disentanglement of geometric style (coming from the source) and structural pose (conforming to the target). We parametrize the deformation between geometries as a learned continuous ﬂow ﬁeld via a neural network and show that such deformations can be guaranteed to have desirable properties, such as bijectivity, freedom from self-intersections, or volume preservation. We illus-trate the effectiveness of this learned deformation space for various downstream applications, including shape generation via deformation, geometric style transfer, unsupervised learning of a consistent parameterization for entire classes of shapes, and shape interpolation. 1

Introduction
Learning a shared representation space for geometries is a central task in 3D Computer Vision and in
Geometric Modeling as it enables a series of important downstream applications, such as retrieval, reconstruction, and editing. For instance, morphable models [1] is a commonly used representation for entire classes of shapes with small intra-class variations (i.e., faces), allowing high quality geometry generation. However, morphable models generally assume a shared topology and even the same mesh connectivity for all represented shapes, and are thus less extensible to general shape categories with large intra-class variations. Therefore, such approaches have limited applications beyond collections with a shared structure such as humans [1, 2] or animals [3].
In contrast, when trained on large shape collections (e.g., ShapeNet [4]), 3D generative models are not only able to learn a shared latent space for entire classes of shapes (e.g., chairs, tables, airplanes), but also capture large geometric variations between classes. A main area of focus in this ﬁeld has been developing novel geometry decoders for these latent representations. These generative spaces allow the mapping from a latent code z∈Rc to some geometric representation of a shape, examples being voxels [5, 6], meshes [7, 8], convexes [9, 10], or implicit functions [11, 12]. Such latent spaces are
∗Equal Contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Schematic for learning a deformation space using ShapeFlow. (a)Our input is either a sparse point cloud, or a depth map converted into a point cloud. (b) The visualization of the learned latent embedding (2D PCA) of sample shapes in the training set. ShapeFlow learns a geometrically meaningful embedding of geometries based on deformation distances in an unsupervised manner. (c) The unsupervised deformation space facilitates various downstream applications, including shape correspondences, reconstruction, and style transfer. generally smooth and allow interpolation or deformation between arbitrary objects represented in this encoding. However, the shape generation quality is highly dependent on the decoder performance and generally imperfect. While some decoder architectures are able to produce higher quality geometries, auto-encoded shapes never exactly match their inputs, leading to a loss of ﬁne geometric details.
In this paper we introduce a different approach to shape generation based on continuous ﬂows between shapes that we term ShapeFlow. The approach views the shape generation process from a new perspective – rather than learning a generative space where a learned decoder Fθ directly maps a latent code zi∈Rc to the shapeXi as Xi=Fθ(zi), ShapeFlow learns a deformation space facilitated by a learned deformer Dθ, where a novel shape Xi←j is acquired by deforming one of many possible template shapes Xj∈X via this learned deformer: Xi←j=Dθ(Xj; zj, zi), where zi, zj ∈ Rc are the latent codes corresponding of Xi and Xj.
This deformation-centric view of shape generation has various unique properties. First, a deformation space, compared to a generative space, naturally disentangles geometry style from structure. Style comes from the choice of source shape Xj, which also includes the shape topology and mesh connectivity. Structure includes the general placement of different parts, such as limb positioning in a human ﬁgure (i.e., pose), height and width of chair parts etc. Second, unlike template-based mesh generation frameworks such as [13, 14, 7], whose generated shapes are inherently limited by the the template topology, a deformation space allows a multi-template scenario where each of the source shapes Xj∈X can be viewed as a template. Also, unlike volumetric decoders that require a potentially computationally intensive step for (e.g., querying a large number of sample points), ShapeFlow directly outputs a mesh (or a point cloud) through deforming the source shape. Finally, by routing the deformations through a common waypoint in this space, we can learn a shared template for all geometries of the same class, despite differences in meshing or topology, allowing unsupervised learning of dense correspondences between all shapes within the same class.
The learned deformation function Φij
θ deforms the template shape Xj into Xi←j so that it is geomet-rically close to the target shape Xi. Our deformation function is based on neurally parametrized 3D vector ﬁelds or ﬂows that locally advect a template shape towards its destination. This novel way of modeling deformations has various innate advantages compared to existing methods. We show that deformation induced by a ﬂow naturally prevents self-intersections. Furthermore, we demonstrate that we can parametrize a divergence-free ﬂow ﬁeld effectively using a neural network, which ensures volume conservation during the deformation process. Finally, ShapeFlow ensures path invertibility (Φij
θ (Xi)). Compared to traditional deformation parameterizations in computer graphics such as control handles [15, 16]
θ )−1), and therefore also identity preservation (Xi=Φii
θ =(Φji 2
and control cages [17, 18, 19], ShapeFlow is a ﬂow-model realized by a neural network, allowing a more ﬁne grained deformation without requiring user intervention.
In summary, our main contributions are: 1. We propose a ﬂow-based deformation model via a neural network that allows exact preservation of identity, good preservation of local geometric features, and disentangles geometry style and structure. 2. We show that our deformations by design prevent self-intersections and can preserve volume. 3. We demonstrate that we can learn a common template for a class of shapes through which we can derive dense correspondences. 4. We apply our method to interpolate shapes in different poses, producing smooth interpolation between key frames that can be used for animation and content creation. 2