Abstract
Neural sequence-to-sequence models are well established for applications which can be cast as mapping a single input sequence into a single output sequence.
In this work, we focus on one-to-many sequence transduction problems, such as extracting multiple sequential sources from a mixture sequence. We extend the standard sequence-to-sequence model to a conditional multi-sequence model, which explicitly models the relevance between multiple output sequences with the probabilistic chain rule. Based on this extension, our model can conditionally infer output sequences one-by-one by making use of both input and previously-estimated contextual output sequences. This model additionally has a simple and efﬁcient stop criterion for the end of the transduction, making it able to infer the variable number of output sequences. We take speech data as a primary test ﬁeld to evaluate our methods since the observed speech data is often composed of multiple sources due to the nature of the superposition principle of sound waves. Experiments on several different tasks including speech separation and multi-speaker speech recognition show that our conditional multi-sequence models lead to consistent improvements over the conventional non-conditional models. 1

Introduction
Many machine learning tasks can be formulated as a sequence transduction problem, where a system provides an output sequence given the corresponding input sequence. Examples of such tasks include machine translation, which maps text from one language to another, automatic speech recognition (ASR), which receives a speech waveform and produces a transcription, and video captioning, which generates the descriptions of given video scenes. In recent years, the development of neural sequence-to-sequence (seq2seq) models [9, 40] with attention mechanisms has led to signiﬁcant progress in such tasks [2, 51, 44, 48, 10, 5].
In reality, the observed data contains various entangled components, making the one-to-many se-quence transduction for mixture signals a common problem in machine learning [37, 18, 32]. This problem often happens in audio and speech processing due to the sequential properties and superposi-tion principle of sound waves. For example, given the overlapped speech signal, speech separation is a problem of extracting individual speech sources, and multi-speaker speech recognition is a problem
∗equal contribution
†corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of decoding transcriptions of individual speakers. This type of problem is called the cocktail party problem [8, 3]. The existing methods to tackle this common sequence-to-multi-sequence problem can be roughly divided into two categories according to the correlation strength of multiple output sequences: serial mapping and parallel mapping. Serial mapping aims to learn the mappings through
Figure 1: Sequence-to-multi-sequence mapping approaches. (a) and (b) shows the serial mapping and parallel mapping used by existing methods respectively, and (c) refers to our Conditional Chain mapping strategy. a forward pipeline, as shown in Figure 1(a). With the serial form, the output sequence of the ﬁrst seq2seq model is fed into the following seq2seq model to output another sequence. This method is quite common when the logic and relationship between different output sequences are straightforward.
For example, a cross-lingual speech translation system contains two components: speech recognition and machine translation. Serial mapping ﬁrst recognizes the speech into the source language text and then use another model to translate it into the target language text. However, serial mapping methods usually suffer from some drawbacks. First, many of them need to be trained separately for different components, without taking advantage of the raw input information in the latter components. And the error accumulation through the pipeline will make the system suboptimal. The other category is parallel mapping, as shown in Figure 1(b), which simultaneously outputs multiple sequences.
This method is often used when the outputs are from the same domain. Speech separation and multi-speaker ASR are typical examples following this paradigm. Similar to serial mapping, parallel mapping could not effectively model the inherent relationship that exists between different outputs, and usually assumes the number of the output sequence is ﬁxed (e.g., the ﬁxed number of speakers in speech separation tasks), which limits its application scenarios.
In this paper, we propose a new uniﬁed framework aiming at the sequence-to-multi-sequence (seq2Mseq) transduction task, which can address the disadvantages of both the serial mapping and parallel mapping methods. For clarity, we refer to our methods as Conditional Chain (Cond-Chain) model, combining both the serial mapping and parallel mapping with the probabilistic chain rule. Simultaneous modeling for these two methods not only makes the framework more ﬂexible but also encourages the model to automatically learn the efﬁcient relationship between multiple outputs.
To instantiate the idea, as shown in Figure 1(c), we assume that the input sequence O can be mapped into N different sequences si, i ∈ {1, .., N }. We take sequence O as the primary input for every output sequence. Meanwhile, the outputs will be generated one-by-one with the previous output sequence as a conditional input. We consider that the multiple outputs from the same input have some relevance at the information level. By combining both the serial and parallel connection, our model learns the mapping from the input to each output sequence as well as the relationship between the output sequences.
In this paper, we introduce the general framework in Section 2, and present a speciﬁc implementation for the tasks of speech separation and recognition in Section 3. We discuss some related work in
Section 4 and describe our experiments in Section 5, and ﬁnally conclude in Section 6. Our source code and Supplementary Material could be available on our webpage: https://demotoshow. github.io/. 2 General framework
We assume that the input sequence O ∈ O with length of T can be mapped into N different sequences si, i ∈ {1, .., N }, where the output index i represents a particular domain Di. All the output sequences form a set S = {si | i ∈ {1, ..., N }}. The basic formulation of our strategy is to estimate the joint probability of multiple output sequences, i.e., p(S|O). The joint probability is factorized into the 2
product of conditional probabilities by using the probabilistic chain rule with/without the conditional independence assumption (denoted by ·), as follows: p(s1|O) (cid:81)N (cid:81)N (cid:81)N serial mapping parallel mapping conditional chain mapping i=1 p(si|O , si−1, ..., s1) i=1 p(si|O , si−1, ..., s1) i=2 p(si|si−1 , O, si−2, ..., s1) p(S|O) =

 (1)
 where, we also present the formulation of serial mapping and parallel mapping for comparison. As shown in Eq. 1, the serial mapping methods adopt a ﬁxed order and the conditional distributions are only constructed with sequence from the previous step, i.e., p(S|O) = p(s1|O) (cid:81)N i=2 p(si|si−1).
As a contrast, parallel mapping simpliﬁes the joint distribution by the conditional independence assumption, which means all the output sequences are only conditioned on the raw input, i.e., p(S|O) = (cid:81) p(si|O). For our conditional chain mapping, we manage to explicitly model the inherent relevance from the data, even if it seems very independent intuitively. To achieve this, we depart from the conditional independence assumption in parallel mapping or the Markov assumption in serial mapping. Instead, with the probabilistic chain rule, our method models the joint distribution of output sequences over an input sequence O as a product of conditional distributions. We can also apply the same methodology to the non-probabilistic regression output (e.g., speech separation).
In our model, each distribution p(si|O, si−1, ..., s1) in Eq. 1 is represented with a conditional encoder-decoder structure. Different from the conventional one-to-one sequence transduction for learning the mapping O (cid:55)→ Di, additional module in our model preserves the information from previous target sequences and takes it as a condition for the following targets. This process is formulated as follows: i ×T E i ,
Ei = Encoderi(O) ∈ RDE
Hi = CondChain(Ei, ˆsi−1) ∈ RDH
ˆsi = Decoderi(Hi) ∈ DTi i
, i ×T H
, (2) (3) (4) i , T H i where, all the Di symbols are the number of dimensions for the features, and T E
, Ti represent the size of temporal dimension. In the above equations, Encoderi and Decoderi refer to the speciﬁc networks designed for learning the mapping for the reference sequence si. Note that the Encoderi and Decoderi here may also consist of linear layers, attention mechanism or other neural networks besides the standard RNN layer, so the lengths of the hidden embeddings Ei and the estimation sequence ˆsi may vary from the input, i.e., T E i , T H
, Ti may not equal the T . For the i-th output, i the Ei in Encoder gets a length of T E i while the ˆsi should get the same length with the reference si ∈ DTi
, where Ti is the length of the sequence si from domain Di. Different from the conventional i seq2seq model, we utilize a conditional chain (CondChain in Eq. 3) to store the information from the previous sequences and regard them as conditions. This conditional chain is analogous to the design of memory cell in the LSTM model and the key component to realize Figure 1(c). Similarly, the conditional chain in Eq. 3 does not serve a speciﬁc target domain alone, it models some uniﬁed information for multi-sequence outputs. In other words, the encoder-decoder is specialized for each target sequence, but the conditional chain is shared by all the transduction steps i.
For most situations, when the logic and relationship between different output sequences is straightfor-ward, we could set a ﬁxed ordering of the outputted sequence, like the cross-lingual speech translation showed in Figure 1(a). Differently, for the outputs from the same domain, i.e., Di = Dj, i (cid:54)= j, the Encoder and Decoder for each step could be shared with the same architecture and parameters, which yields less model parameters and better efﬁciency for training. 3
Implementation for speech processing
This section describes our implementation of the proposed conditional chain model by using speciﬁc multi-speaker speech separation / recognition tasks as examples. Both of them are typical examples of seq2Mseq tasks with input from mixture signals. 3.1 Basic model
Multi-speaker speech separation / recognition aims at isolating individual speaker’s voices from a recording with overlapped speech. Figure 2 shows the network structure under our conditional chain 3
Figure 2: Sequence-to-multi-sequence mapping with conditional model for multi speaker speech separation or recognition. In each sub-ﬁgure, the block with same name are all shared. mapping for these two tasks. For both of them, the input sequence is from the speech domain. Let us
ﬁrst assume this to be an audio waveform O ∈ RT . Another common feature for these two tasks lies in that the output sequences are from the same domain (Di = Dj, i (cid:54)= j), which means we could use a shared model at each step, i.e., Encoderi in Eq. 2 and Decoderi in Eq. 4 are respectively the same networks with different i.
For speech separation, the target sequences si ∈ RT are all from the same domain as the input, i.e., Di = O, and with the same length of the input mixture. Thus, we could use an identical basic
Encoder (Enc in Figure 2(a)) to extract acoustic features from the input waveform O and predicted waveform ˆsi. As a contrast, multi-speaker speech recognition outputs a predicted token sequence si ∈ V T with token vocabulary V. We introduce an additional embedding layer, Embed, to map these predicted tokens as continuous representations, which is used for conditional representation.
In speech separation, as illustrated in Figure 2(a), both the mixed audio and the predicted source will go through an Encoder (Enc) to extract some basic auditory features. For the mixture waveform O, another separator (Separator) will also be used as the function to learn some hidden representation which is suitable for separation. And both the Enc and the Separator form the process in Eq. 2. For the Fusion block, due to the same lengths from input and output, a simple concatenation operation is used to stack the feature dimension for each frame. For the CondChain in Eq. 3, we use a unidirectional LSTM. At each source step i, the Decoder (Dec) is used to map the hidden state Hi into the ﬁnal separated speech source. Multi-speaker ASR is also performed in a similar form, as illustrated in Figure 2(b). Note that we use connectionist temporal classiﬁcation (CTC) [14] as a multi-speaker ASR network, since CTC is simple but yet powerful end-to-end ASR, and also the
CTC output tokens without removing blank and repetition symbols can have the same length with the auditory feature sequence. Thus, we can realize the Fusion processing with a simple concatenation operation, similarly to speech separation. 3.2 Stop criterion
One beneﬁt of the conventional seq2seq model is the ability to output a variable-length sequence by predicting the end of the sequence ((cid:104)EOS(cid:105) symbol) as a stop criterion. This advantage is inherited in our model to tackle the variable numbers of multiple sequences. For example, current speech separation or recognition models are heavily depending on a ﬁxed number of speakers [21] or require extra clustering steps [15]. Thanks to the introduction of the above stop criterion, we can utilize the mixture data with various numbers of speakers during training, and can be applied to the case of unknown numbers of speakers during inference.
In our implementation, when we have the total number of output sequences as N , we attach an extra sequence to reach the stop condition during training. The target of this last sequence prediction for both speech separation and recognition tasks must be the silence, and we use the silent waveform and silent symbol (an entire (cid:104)blank(cid:105) label sequence in CTC), respectively. 4
For different tasks, the stop criterion should correspond to the form of target sequences. In speech separation task, we set the stop criterion as the prediction of silent waveform, implying that there is no more speech left. Similarly, in multi-speaker recognition, we encourage the last predicted utterance as all (cid:104)blank(cid:105) labels, which is a higher dimensional (cid:104)EOS(cid:105) used by seq2seq model. More explicitly, we use the average energy to determine pure silence in separation task and average posterior of <blank> label in ASR task to determine the end of prediction. 3.3 Training strategy with teacher-forcing and ordering
Like the conventional seq2seq approach [2], we use a popular teacher-forcing [47] technique by exploiting the ground-truth reference as a conditional source si−1. Teacher-forcing provides proper guidance and makes training more efﬁcient, especially at the beginning of the training, when the model is not good enough to produce reasonable estimation. Considering the unordered nature of multiple sources in multi-speaker speech separation or recognition, we adopt a greedy search method to choose the appropriate permutation of the reference sequences. This method achieves good performance in practice while maintaining high efﬁciency. More details about teacher-forcing and reference permutation search could be found in Section B in the Supplementary Material. 4