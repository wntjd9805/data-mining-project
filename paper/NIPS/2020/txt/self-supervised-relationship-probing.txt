Abstract
Structured representations of images that model visual relationships are beneﬁ-cial for many vision and vision-language applications. However, current human-annotated visual relationship datasets suffer from the long-tailed predicate distribu-tion problem which limits the potential of visual relationship models. In this work, we introduce a self-supervised method that implicitly learns the visual relationships without relying on any ground-truth visual relationship annotations. Our method relies on 1) intra- and inter-modality encodings to respectively model relation-ships within each modality separately and jointly, and 2) relationship probing, which seeks to discover the graph structure within each modality. By leveraging masked language modeling, contrastive learning, and dependency tree distances for self-supervision, our method learns better object features as well as implicit visual relationships. We verify the effectiveness of our proposed method on various vision-language tasks that beneﬁt from improved visual relationship understanding. 1

Introduction
Visual relationships that describe object relationships in images have become more and more important for high-level computer vision (CV) tasks that need complex reasoning [1, 2, 3, 4]. They are often organized in a structured graph representation called scene graph, where nodes represent objects and edges represent relationships between objects. In recent years, we have witnessed great progress with visual relationship datasets such as Visual Genome [5] and the application of scene graphs to various
CV reasoning tasks such as image captioning [6, 7], image retrieval [8], and visual reasoning [9].
Despite this, current visual relationship models still rely on human-annotated relationship labels.
Due to the combinatorics involved — two objects and one relationship between them, where objects and relationships each have different types — relationships are numerous and have a long-tailed distribution and, thus, it is difﬁcult to collect enough annotations to sufﬁciently represent important but less frequently observed relationships. Consequently, current visual relationship models tend to focus on modeling only a few relationships that have a large number of human annotations [10], and they ignore relationship categories with few annotations. We have seen some research attempts that use external knowledge databases to help enrich visual relationships, however, the total number of relationships modeled is still relatively small [11].
On the other hand, in the past few years, we have seen signiﬁcant progress in natural language processing (NLP) towards building contextualized language models with self-supervised pretraining objectives [12, 13]. The removal of human annotators from the training loop has enabled training on massive unlabeled datasets, leading to signiﬁcant advances in NLP performance [14, 15]. These trends have also brought signiﬁcant advances in vision-language (VL) pretraining tasks [16, 17, 18, 19, 20].
Most existing VL pretraining methods concatenate visual objects and the corresponding sentences as one input and adopt the Transformer [21] as the core module to learn contextualized multi-modal representations in a self-supervised manner via self- and cross-attentions. These models rely heavily 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
on the multi-head attention layers to explore implicit relations, or they directly rely on attention distributions to explain the relations between objects [17, 22]. However, different layers vary in their behaviors [23, 24], and it has been shown that attention alone can be deceiving when used for interpretability and explanation [25]. Thus, existing VL pretraining algorithms suffer from two problems: discovered relationships are not modeled explicitly, but are instead expected to be implicitly represented as transformer weights; and, the concatenation of multimodal inputs at training time restricts the model to require multimodal inputs at prediction time, as well.
Motivated by textual relation mining work in NLP [26], we propose a novel framework that discovers dependencies between objects from the model’s representation space which addresses the problems highlighted above. Our approach is based on two simple observations: (1) when we slightly change the images, the relative visual relationships in those images remain unchanged; (2) relationships mentioned in image descriptions are visually observable in the corresponding image. Our approach relies on three modules, each consisting of a set of layers. In the ﬁrst module, implicit intra-modal relationships are modeled using transformer encoders. In the second module, cross-modal learning allows for implicit relationship information to be leveraged across modalities. In the third module, relationships between visual and linguistic entities are represented explicitly as latent variables via a technique we call relationship probe. All modules are trained using self-supervision, with a ﬁrst stage relying on masked language modeling to train the ﬁrst two modules, and a second stage relying on contrastive learning and linguistic dependency trees as supervisory signals to train the relationship probe network.
Our main contribution is a novel self-supervised relationship probing (SSRP) framework for ﬁnding dependencies in visual objects or textual entities that address issues with existing visual relationship models: it relies on self-supervision rather than explicit supervision, it explicitly models relationships as latent variables, and it leverages cross-modal learning but allows a single modality as input at prediction time. We conduct extensive experiments to demonstrate that our method can beneﬁt both vision and VL understanding tasks. 2