Abstract
This paper tackles the modelling of large, complex and multivariate time series panels in a probabilistic setting. To this extent, we present a novel approach rec-onciling classical state space models with deep learning methods. By augmenting state space models with normalizing ﬂows, we mitigate imprecisions stemming from idealized assumptions in state space models. The resulting model is highly
ﬂexible while still retaining many of the attractive properties of state space models, e.g., uncertainty and observation errors are properly accounted for, inference is tractable, sampling is efﬁcient and good generalization performance is observed, even in low data regimes. We demonstrate competitiveness against state-of-the-art deep learning methods on the tasks of forecasting real world data and handling varying levels of missing data. 1

Introduction
In most real world applications of time series analysis, e.g., risk management in ﬁnance, cannibal-ization of products in retail or anomaly detection in cloud computing environments, time series are not mutually independent and an accurate modelling approach must take these dependencies into account [1]. The classical approach [2] is to extend standard univariate models resulting in vector autoregression [3], multivariate GARCH [4] and multivariate state space models [5, 6]. Although these approaches yield useful theoretical properties, they make idealized assumptions like Gaussianity, linear inter-dependencies, and are not scalable to even moderate number of time series [7] due to the number of parameters required to be estimated, which is restrictive for many modern applications involving large panels of time series. Recently, more expressive, scalable deep learning methods [8, 9] were developed for forecasting applications that learn a joint global model for multiple time series; however, they still assume that these time series are mutually independent.
In this paper we propose the Normalizing Kalman Filter (NKF), a novel approach for modelling and forecasting complex multivariate time series by augmenting classical linear Gaussian state space models (LGM) with normalizing ﬂows [10]. The combined model allows us to leverage the ﬂexibility of normalizing ﬂows (NF), alleviating strong assumptions of traditional multivariate models, while still beneﬁting from the rich set of mathematical properties of LGM. In fact, we prove that despite modelling non-Gaussian data with nonlinear inter-dependencies, we can achieve exact inference since our model has closed-form expressions for ﬁltering, smoothing and likelihood computation. We thus retain the main attractive properties of LGM, in contrast to related methods [11, 12]. Moreover, since
∗Equal contribution.
†Work done while at Amazon. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
our model is based on LGM, handling of missing data and integrating prior knowledge, e.g., seasonality and trend, becomes trivial. Therefore, the proposed model can be used in forecasting time series with missing or noisy data irrespective of whether the data regime is sparse (in terms of observed time points) or dense. More importantly, LGM directly gives us the ability to provide tractable multi-step ahead forecast distributions while accounting for all uncertainties; this is in contrast to recent deep learning-based autoregressive models [8, 1] that do not incorporate accumulated prediction errors into forecast distributions since predictions of the model are used as lagged inputs in a multi-step forecast scenario. For the forecasting application, we show that our method scales linearly with the number of dimensions and number of time points, unlike most of the existing work that exhibits quadratic scaling with the number of dimensions. The necessary structural assumptions do not result in a loss of generality or expressiveness of the overall model.
In summary, our main contributions are as follows:
◦ A tractable method for modelling non-Gaussian multivariate time series data with nonlinear inter-dependencies that has Kalman-like recursive updates for ﬁltering and smoothing.
◦ A scalable, robust multivariate forecasting method that handles missing data naturally and provides tractable multi-step ahead forecast distributions while accounting for uncertainties unlike autoregressive models [8, 1].
◦ A thorough evaluation of applicability of normalizing ﬂows in the context of high-dimensional time series forecasting for handling non-Gaussian multivariate data with nonlinear dependencies. 2 Normalizing Kalman Filters
Let yt ∈ RN denote the value of a multivariate time series at time t, with yt,i ∈ R the value of the corresponding i-th univariate time series. Further, let xt,i ∈ Rk be time varying covariate vectors associated to each univariate time series at time t, and xt := [xt,1, . . . , xt,N ] ∈ Rk
N . Non-random and random variables are denoted by normal and bold letters, i.e., x and x, respectively. We use the shorthand y1:T to denote the sequence {y1, y2, . . . , yT }.
× 2.1 Generative Model
The core assumption behind our Normalizing Kalman Filter (NKF) model is the existence of a latent state that evolves according to simple (linear) dynamics, with potentially complex and nonlinear dependencies between latent state and observations–and thus, among observations. More precisely, the dynamics of the latent state lt ∈ Rd are governed by a time-dependent transition matrix Ft ∈ Rd d, up to additive Gaussian noise (cid:15)t as in (1b). The state is then mapped into the space of observations with emission matrix At ∈ Rd
N , and additive Gaussian noise εt before being transformed by a potentially nonlinear function ft : RN → RN parametrized by Λ, generating observation yt ∈ RN :
×
× (NKF model) l1 ∼ N (µ1, Σ1) lt = Ftlt 1 + (cid:15)t,
− yt = ft(AT t lt + εt), (cid:15)t ∼ N (0, Σt),
εt ∼ N (0, Γt). (initial state) (transition dynamics) (observation model) (1a) (1b) (1c) 2), the model is fully speciﬁed.3 Note
With parameters Λ and Θ = (µ1, Σ1, {Γt, At}t that the special case ft = id recovers the standard LGM where both the transition dynamics and the observation model are linear. In the following, this similarity with LGM will yield numerous computational beneﬁts and it will further allow us to easily inject prior knowledge on the structural form of the dynamics (e.g., levels, trends, seasonalities [5]) for good generalization properties. 1, {Σt, Ft}t
≥
≥
We consider a ﬂexible nonlinear transformation for the observation model, assuming invertibility of ft. This guarantees the conservation of probability mass and allows the evaluation of the associated density function at any given point of interest. In particular, the probability density of an observation 3Placing the noise before the non-linearity ft in the observation model is important to obtain tractability for ﬁltering and smoothing. However, this does not imply that data generated from a process where additive noise is added after the non-linear function cannot be modelled; in fact we use this particular model (nonlinear transformation with additive non-Gaussian noise) to generate data and test our method in the qualitative experiments in Section 4.1 (refer to appendix C.1 in the supplementary material for details). 2
yt given the state lt can be computed using the change of variables formula:
)(cid:3)(cid:12) (cid:12) , (cid:12)det (cid:2)Jacyt(f − p(yt|lt; Θ, Λ) = pz(f − t (yt)|lt; Θ) (cid:12) 1 1 t (2) where the ﬁrst term in pz(zt|lt; Θ) is the density of the Gaussian variable zt := Atlt + εt conditioned 1 given Λ, evaluated on lt, and the second is the absolute value of the determinant of the Jacobian of f − t at yt. This equation and Figure 1 illustrate the intuition behind our approach: we would like f − to t transform the observations such that the dynamics become simple and the noise is Gaussian. 1
Computing the density (2) raises several issues: (i) ﬁnding a ﬂexible ft while ensuring in-vertibility, (ii) being able to compute the inverse efﬁciently and (iii) tractability of the com-putation of the Jacobian term when the number of time series N is large. To this extent, we will take inspiration from normalizing ﬂows [13, 14, 15], which are invertible neural net-works that typically transform isotropic Gaussians to ﬁt a more complex data distribution.
These invertible networks are tailored to compute the Ja-cobian term efﬁciently. Moreover, they have proven to work very well for nonlinear high-dimensional data, e.g., images [14], both in terms of ﬂexibility and generalization.
In our approach, we apply these invertible neural networks to temporal data, using them to map the distribution pz given by the LGM to the complex data distribution. This yields a powerful function ft where the Jacobian term is computable in linear time in N .
Inference and Learning. With the presented generative model it is possible to do inference (e.g., ﬁltering and smoothing) and training in a simple and tractable way.
Similar to the LGM, computing the ﬁltered distribution p(lt|y1:t; Θ, Λ) is essential as it determines our current belief on the state having observed all the data up to time t, and it takes part in the computation of the likelihood of the model parameters as well as the forecast distribution.
For general nonlinear state space models its computation is tedious as it involves integrating out previous states.
Methods such as Particle Filters [16] resort to Monte Carlo approximations of these integrals but have difﬁculty scaling to high dimensions. Other methods circumvent this by locally linearizing the nonlinear transformation [17] or by using a ﬁnite sample approximation [18] in order to apply–in both cases– the techniques of the standard LGM, but introduce a bias. In contrast, our model allows for computing this quantity in a tractable and efﬁcient manner, without resorting to any form of simpliﬁcation or approximation. In fact, despite the nonlinear nature of ft, the ﬁltered distribution remains Gaussian and its parameters can be computed in closed form similarly to the Kalman Filter, as shown in the following proposition.
Figure 1: Generative model of the NKF.
States lt and pseudo-observations zt are produced from an LGM, which are then transformed through a normalizing ﬂow ft, producing observations yt.
Proposition 1 (Filtering). The ﬁltered distributions of the NKF model are Gaussian and are given by (yt), t ≥ 1. the ﬁltered distributions of the corresponding LGM with pseudo-observations zt := f − t
That is, p(lt|y1:t; Θ, Λ) = pLGM (lt|z1:t; Θ) where pLGM refers to the distribution given by the LGM. 1
This can be proved by induction using recursive Bayesian estimation and is available in Appendix A.1 along with the exact updates. Proposition 1 shows that ﬁltered distributions for our model are available in closed-form and have the same computational complexity as that of the LGM, plus the complexity of the inverse of the nonlinear function f and the Jacobian term in (2).4
Our nonlinear model (1) is also amenable to smoothing, i.e., computing the smoothed posterior distribution p(lt|y1:T ; Θ, Λ), given past, present and future observations. Smoothing is a prevalent problem in many ﬁelds, with applications such as estimating the distribution of missing data and providing explanations in the context of ofﬂine anomaly detection. Smoothing can be obtained with a backward iterative approach using the quantities computed during a preliminary ﬁltering pass, starting from the ﬁltered distribution corresponding to step T , p(lT |y1:T ; Θ). Similar to ﬁltering, smoothing updates also directly translate to the corresponding updates of the LGM. 4In Sec. 3, the complexity of the inverse is the same as the forward map, and the Jacobian term is linear in N . 3
Proposition 2 (Smoothing). The smoothed distributions of the NKF model are Gaussian and are given 1 by the smoothed distributions of the corresponding LGM with pseudo-observations zt := f − (yt), t = t 1, 2 . . . , T . That is, p(lt|y1:T ; Θ, Λ) = pLGM (lt|z1:T ; Θ).
The tractability of the computation of the ﬁltered distribution implies that the likelihood of the model parameters can be computed efﬁciently by integrating out the latent state. In case of the standard LGM, the likelihood of its parameters Θ given the observations z1:T can be written as (cid:96)(Θ) =
T (cid:89) t=1 pLGM(zt|z1:t 1; Θ),
− (3) where pLGM(zt|z1:t 1; Θ) is the predictive distribution of LGM. This distribution is available in closed-form thanks to the ﬁltering updates [19]. We now show that the likelihood of our nonlinear model 1 given the observations {y1:T } is essentially a reweighted version of this expression with zt = f − (yt). t
−
Proposition 3 (Likelihood). The likelihood of the parameters (Θ, Λ) of the NKF model given the observations {y1:T } can be computed as (cid:96)(Θ, Λ) = p(y1:T ; Θ, Λ) =
T (cid:89) t=1 pLGM(zt|z1:t
− 1; Θ) |det [Jaczt(ft)]|− 1 , (4) 1 where zt = f − t (yt) and pLGM(zt|z1:t
− 1; Θ) denotes the predictive distribution of LGM.
Hence, we can compute the likelihood of the parameters of the NKF exactly (Appendix A.3 contains the closed-form expressions), and ﬁt our model to the given data by directly maximizing the likelihood.
This is done without resorting to any approximations typically required in other nonlinear extensions of LGM such as the Particle Filter and the Extended/Unscented Kalman Filter. 2.2 Parameter Estimation using RNNs
In Sec. 2.1, we assumed the data was generated from a given family of NKF state space models char-acterized by its parameters Θ1:T = (µ1, Σ1, {Γt, At}T t=2) along with Λ, the parameters of the normalizing ﬂow transformation ft (which we assume to be constant over time). However the exact values of the parameters are unknown. Similar to [9, 8], we propose to predict the parameters
Θ from the covariates, using a recurrent neural network (RNN) where the recurrent function Ψ is parametrized by Φ, taking into account the possibly nonlinear relationship between covariates xt: t=1, {Σt, Ft}T
Θt = σ(ht; Φ), ht = Ψ(xt, ht
− 1; Φ), t = 1, . . . T, (5) where σ denotes the transformation mapping of the RNN output to domains of the parameters (Appendix B.1). The RNN allows our model to be more expressive by allowing time-varying parameters along with temporal dependencies in the covariates, a requirement for forecasting. The parameters of the RNN Φ and of the normalizing ﬂow Λ are estimated by maximizing the conditional likelihood, p(y1:T |x1:T ; Φ, Λ) := p(y1:T ; Θ1:T , Λ) (6) where p(y1:T ; Θ1:T , Λ) is given by (4). Although the transition model in our case is linear, complex dynamics can still be taken into account as the parameters of the LGM, which are now the outputs of a
RNN, may change in a nonlinear way. 2.3 Applications: Forecasting and Missing Values
Given a model for sequential data, we can apply it to obtain future time steps T +1 : T +τ , or in other words, forecasts of the time series, starting from past observations y1:T and covariates x1:T +τ .
In our case such a forecast distribution p(yT +1:T +τ |y1:T , x1:T +τ ; Φ, Λ) can also be computed efﬁ-ciently and is available in closed-form. From this, not only can we readily evaluate possible future scenarios (see Appendix A.4), but also draw samples from it to generate forecasts. 4
This is achieved by ﬁrst computing the ﬁltered distribution for the input range 1 : T , and then recursively apply the transition equation and the observation model to generate prediction samples.
More precisely, starting with the RNN state hT and lT sampled from p(lT |y1:T , x1:T ; Θ1:T ), given by (5) and (6), respectively, we iteratively apply:
Ft, At, Σt, Γt = σ(ht; Φ), lt = Ftlt 1 + (cid:15)t,
− yt = ft(AT t lt + εt), 1; Φ), ht = Ψ(xt, ht (cid:15)t sampled from N (0, Σt),
εt sampled from N (0, Γt),
− t = T + 1, . . . , T + τ. (7a) (7b) (7c)
In contrast to alternative deep learning approaches [1, 8, 11], this generative procedure is not autoregressive in the sense that observations yt are never fed to the model. Instead, it is the ﬁltering that correctly updates our beliefs based on the observed data. This has several notable consequences: we do not have to resort to large and cumbersome beam searches to obtain proper uncertainty estimates, noisy observations with varying levels of uncertainty can be properly handled, and long-term forecast computations are accelerated as intermediate observations need not be computed.
In many real world applications, observations for each time step may not be available, e.g., out-of-stock situations in the context of demand time series (no sales does not mean no demand if out-of-stock) or network failures in the case of sensor data streams. Handling missing data is then of central importance and there are two aspects to it: (i) learning in the presence of missing values without imputing them and (ii) imputing the missing values. Similar to LGM [20], our approach offers a straightforward, tractable and unbiased way for handling missing data in both of these scenarios.
In case of learning with missing values the likelihood terms corresponding to the missing entries should be ignored. This amounts to effectively dealing with them in the ﬁltering step. Without loss of generality, let us assume that targets until time t − 1 are observed but are missing for t. In this case, 1; Θ) with the method outlined in Sec. 2. As yt we can compute the ﬁltered distribution p(lt is not observed, the ﬁltered distribution at time t then simply corresponds to p(lt|y1:t 1; Θ), and can be obtained by applying the prediction step, starting from the ﬁltered distribution at time t − 1.
For the second case, we wish to impute missing values at time t /∈ tobs based on observed values at times tobs. This can be easily achieved by computing the smoothed distribution p(lt|ytobs ; Θ) in the presence of missing data, and from this compute p(yt|ytobs; Θ) (full details in Appendix A.5).
This distribution is available in closed form and can be readily sampled from. This allows us to use the same model for forecasting and imputation without the need for bidirectional RNNs [21] as the smoothing procedure inherently handles future data. 1|y1:t
−
−
− 3 Local-Global Instantiation
The number of parameters of the NKF scales quadratically in the dimensionality d of the state (stemming from the covariance matrices of the Gaussian distributions) in the worst case. Leveraging the strength of NF, we present an instantiation of NKF with an induced local-global structure, that displays several advantages over the general unstructured form, e.g., exhibiting linear scaling in d.
We assume that each time series is associated with latent factors that evolve independently w.r.t. the factors of the other time series. These factors will in turn be mixed together with the normalizing
ﬂow, producing dependent time series observations. Formally, for each univariate time series i we associate a local LGM with parameters Θ(i) = (µ(i) 2), whose dynamics are characterized by: 1 , {Γ(i) 1 , Σ(i) 1, {Σ(i) t
, F (i)
, A(i) t }t t }t
≥
≥ t t t 1 + (cid:15)(i) l(i) t = F (i) l(i)
, t
− t + ε(i) l(i) t = A(i)T z(i)
,
, . . . , z(N ) yt = ft(z(1) t t t t
). t ∼ N (0, Σ(i) (cid:15)(i) t ), t ∼ N (0, Γ(i)
ε(i) t ), (8a) (8b) (8c)
Note that here Γ(i) is a scalar and denotes variance of the Gaussian noise. This local-global structure t has several advantages: (i) the dynamics of the local states need not be the same for each time series and thus, prior knowledge on the evolution can be readily injected per time series, (ii) computations can be done in parallel for each time series and ﬁnally (iii) we may beneﬁt from the effects of amortization by predicting local parameters for each time series and sharing the same weights Φ: 5
t = Ψ(x(i)
Θ(i) 1; Φ), allowing the RNN to make analogies between time series. In this case, dependencies across time series are captured with the normalizing ﬂow ft, mixing the components together in a nonlinear fashion.
, h(i) t
− t
We now explain a possible instantation of the LGM (Eq. (8a), (8b)), which is an innovation-based model, similar to [9]. Note that this is a choice and not a requirement as any other LGM instance may be used if that better reﬂects the data at hand. Nonetheless, with this form a wide range of phenomena can be captured, e.g., long term direction (trends), patterns that repeat (seasonality), cycles with different periodicity, multiplicative errors, etc.; we refer the reader to [5]. In our instantiation, we combine both level-trend and seasonality components, as described in Appendix B.2.
For ft, we use the RealNVP architecture [13]: it is a network that is composed of a series of parametrized invertible transformations with a lower triangular Jacobian structure and vector com-ponent permutations in order to capture complex dependencies. This structure has the advantage of being ﬂexible, while maintaining a tractable Jacobian term, computable in linear time. Moreover, as opposed to more recent architectures e.g., [14], the number of parameters scales linearly. For the
RNN, we use an LSTM with 2 layers.
In terms of parameter complexity, this particular instantiation scales as O(N ) for each timestep, due to the diagonal structure of the covariance matrices (although they do not correspond to the effective number of parameters as these are predicted from the RNN). Moreover, time complexity for likelihood computation and forecasting scales as O(N (d2 + k)) for each timestep, where k is the number of covariates and d is the dimension of latent state; in experiments d = 32 (24 hourly components + 7 daily components + one level component) for hourly data and d = 8 for daily data.
Partial observations: The local-global instantiation model presented above could deal with partial observations (i.e., only some entries of yt ∈ RN are missing but not all); however it would require marginalisation over the missing dimensions, which cannot be done in closed-form. Alternatively, if one is interested in dealing with partial observations, it is possible to consider an instantiation with a global (i.e., multivariate) LGM with non-diagonal covariance matrix modelling linear dependencies among the time-series yt ∈ RN at any given time step t, and a normalising ﬂow applied locally to each time-series: yt = ft(zt) = (ut(z1,t), . . . , ut(zN,t)), with ut : R → R (see ablation study in our experiments). In this case, the marginalisation of any missing set of time series actually yields an analytic form for the ﬁltering, smoothing and forecast distribution [20], and hence the handling of partial observations can be efﬁciently dealt with. 4 Experiments 4.1 Qualitative Results
We qualitatively assess our approach in
Section 3 with two synthetic datasets
S1 and S2 with increasing difﬁcultly.
The datasets are composed of 2 daily univariate time series with a weekly seasonality pattern. We compare our approach against a variant without any normalizing ﬂow (this amounts to setting ft = id in (8)). For dataset S1 the daily data has three different modes and highly non-Gaussian time-independent observational noise. S2 is similar, but the time series are mixed together in a nonlinear fashion, and the observational noise is not only non-Gaussian, but time-dependent (Appendix C.1 contains a detailed description).
Target data (blue) and forecasts (green) are plotted in Fig. 2, where the ﬁrst and second axis correspond to the ﬁrst and second time series respectively, and data is the temporal axis. aggregated over (a) S1 : Results with (left) and without (right) NF. (b) S2 : Results with (left) and without (right) NF.
Figure 2: Evaluation w/ and w/o NF. The axes correspond to the two components of the 2D time series; the temporal axis is marginalized out. Green points correspond to model samples when predicting 24 days ahead and blue points to future samples from the data-generating process. 6
Approach
VES VAR GARCH DeepAR GP-Copula KVAE NKF(Ours) (cid:88) (cid:88)
Multivariate
×
×
Non-Linear, Non-Gaussian (cid:88) NA
Filtering & Smoothing
Tractable Multi-step Forecast (cid:88) × (cid:88) ×
Tractable Data Imputation (cid:88)
×
NA
×
×
Table 1: Comparative summary of competing approaches on various parameters. (cid:88) (cid:88)
NA
×
×
× (cid:88)
NA
×
× (cid:88) (cid:88) (cid:88)
×
× (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Observations and forecasts can also be seen in Appendix C.1 from a viewpoint that better highlights the seasonal nature of the observations.
For both datasets, the variant with ft = id captures the daily modes correctly, but assumptions such as Gaussianity and independence between time series introduce errors. In contrast, visual inspection reveals that applying the normalizing ﬂow allows us to ﬁt the data better, capturing the complex dependencies between time series and the non-Gaussian noise, for both S1 and S2. 4.2 Quantitative Results
We follow the experimental set up proposed in [1] since it focusses on the same problem in the forecasting application. In particular, we evaluate on the public datasets used in [1]; see C.2.1 for details and Table 3 for the summary of datasets. Similar to [1], the forecasts of different methods are evaluated by splitting each dataset in the following fashion: all data prior to a ﬁxed forecast start date compose the training set and the remainder is used as the test set. We measure the accuracy of the forecasts of various methods on all the time points of the test set. The hyperparameters have been selected using a validation set of equal size to the test set, created from the last time steps of the training data.
Our evaluation is extensive, covering relevant classical multivariate approaches as well as recent deep learning based models. In particular, we compare against VES, a direct generalization of univariate innovation state space model (a special case of LGM used in forecasting) to multivariate time series (see Chapter 17 of [5]), VAR, a multivariate linear autoregressive model and GARCH a multivariate conditional heteroskedastic model [22]; we include result of Lasso-regularized VAR as well. We also compare against the recent deep-learning based approaches GP-Copula [1] and KVAE [12] speciﬁcally developed for handling non-Gaussian multivariate data with non-linear dependencies. GP-Copula builds on ideas of VAR and relies on RNN and low-rank Gaussian Copula process for going beyond
Gaussianity and linearity. In contrast, KVAE uses a variational autoencoder on top of linear state space models to achieve the same. Unlike our NKF model, inference and likelihood computation are not tractable in KVAE and it relies on particle ﬁlters for their approximation. Additionally, we compare with DeepAR [8] an autoregressive recurrent neural network based method for univariate time series forecasting. DeepState [9] is a special case of NKF model and is part of the ablation study. See
Table 1 for summary of the compared methods based on various parameters.
In order to evaluate forecasting models, continuous ranked probability score (CRPS) is generally accepted as one of the most well-founded metrics[23, 24]. However, this metric is only deﬁned for univariate timeseries and cannot assess if dependencies across time series are accurately captured.
Different generalizations to the multivariate case have been used, e.g., the energy score, or CRPS-Sum
[1]. We have opted for the CRPS-Sum, as the energy score suffers from the curse of dimensionality, from both a statistical and computational viewpoint [25, 26]. The introduction of the CRPS-Sum [1] was experimentally justiﬁed. In this work, we prove it is theoretically sound as justiﬁed formally in Appendix C.4 following the proper scoring rule framework [24]. Note that, opposed to [1], as different time series observations may have drastically different scales, we ﬁrst normalize each time series by the sum of its absolute values before computing this metric (hence the ‘-N’ sufﬁx). We also did not choose log-likelihood since not all methods yield analytical forecast distributions and is not meaningful for some methods [1].
We report CRPS-Sum-N metric values achieved by all methods in Table 2. Classical methods, because of the Gaussianity and linear dependency assumptions, typically yield inferior results; entries marked with ‘-‘ are runs failed with numerical issues. Deep learning based models have superior 7
method exchange solar elec wiki traffic
VES
VAR 0.005 ± 0.000 0.005 ± 0.000 0.9 ± 0.003 0.88 ± 0.0035 0.83 ± 0.006 0.039 ± 0.0005
VAR-Lasso 0.012 ± 0.0002 0.51 ± 0.006 0.025 ± 0.0002 0.19 ± 0.001 0.023±0.001 0.024±0.000 0.051 ± 0.019 0.016±0.001 0.023 ± 0.000 0.006±0.001 0.007±0.000 0.014 ± 0.002 0.005 ± 0.000 0.88 ± 0.002 0.336±0.014 0.363±0.002 0.34 ± 0.025 0.320±0.020
--3.1 ± 0.004
-0.127±0.042 0.092±0.012 0.095 ± 0.012 0.071±0.002 0.35 ± 0.0023 0.29 ± 0.005 0.15 ± 0.002 0.37 ± 0.0016 0.055±0.003 0.051±0.000 0.1 ± 0.005 0.10±0.002
GARCH
DeepAR
GP-Copula
KVAE
NKF(Ours) (cid:26) ft = id ft Local ablation study 0.005±0.000 0.005±0.000 0.415±0.002 0.405±0.005 0.026±0.000 0.018±0.001 0.082±0.000 0.068±0.004 0.123±0.000 0.102±0.013
Table 2: CRPS-Sum-N (lower is better), averaged over 3 runs. The case ft = id is DeepState [9] and VES can be seen as part of ablation where normalizing ﬂow and RNN are removed from NKF. performance overall. In particular, NKF achieves the best result in 4 out of 5 datasets. On traffic
NKF is better than all methods except for DeepAR and GP-Copula which are purely data-driven autoregressive approaches with minimal modelling assumptions. Given the domain of traffic dataset is (0, 1), it would be interesting to verify in future work if the relatively weak performance is due to a short-coming of the normalizing ﬂow part or due to the modelling choice of adopting a state space model instead of an autoregressive process.
Ablation Study First note that VES, which models only Gaussian data with linear dependencies, can be seen as an ablation where RNN and normalizing ﬂow are not used. Next, to evaluate the usefulness of the normalizing ﬂow in accurately modelling real world data, we analyze the performance of two particular instances of NKF : (i) ‘ft = id’: the normalizing ﬂow is set to be the identity function, as in Section 4.1, therefore also reducing to the DeepState model proposed in [9] (ii) ‘ft Local’: the normalizing ﬂow is applied locally for each time series, modelling non-Gaussianity and non-linearity, but not any dependencies between time series, as explained in Appendix C.2.2. This ablation is important in order to analyze the advantages of capturing the potential dependencies across time series in the data. Overall, we observe (bottom lines in Table 2) a signiﬁcant increase in performance from the identity function to a local NF, along with another increase when applying the global NF (see NKF results), apart from the wiki dataset. While we do not have a satisfying explanation, we speculate that this is due to modelling errors, the optimization algorithm, or simply because the time series may not exhibit much dependencies between each other.
Missing Data Experiment We evaluate our model in the context of varying degrees of miss-ing data during training and evaluation. From elec, we remove p = 10 + 20k, k = 0 . . . , 4, percent of the training data at random. This dataset, while realistic, is highly regular with clear seasonality patterns. The models are then evaluated on rolling windows, where the in-put range observations are missing with the same probability. In Fig. 3 and Appendix C.3, we report results for our approach, along with
DeepAR, GP-Copula, KVAE. We observe that not only does NKF outperform other approaches by a large margin, but its error also increases slower than in other methods when the percent-age of missing data is increased. We believe that this is due to the proper handling of the uncertainties in our approach since our model does not directly take observations as input. Moreover, the strong results obtained for up to 90% of missing data demonstrate that our method encodes useful prior knowledge due to the structure induced in the LGM, rendering this method useful even in low data regimes (the same observation is made in [9] for this dataset).
Figure 3: Forecasting with missing data. 8
5