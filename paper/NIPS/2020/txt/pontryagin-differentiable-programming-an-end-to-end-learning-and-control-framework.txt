Abstract
This paper develops a Pontryagin Differentiable Programming (PDP) methodology, which establishes a uniﬁed framework to solve a broad class of learning and control tasks. The PDP distinguishes from existing methods by two novel techniques: ﬁrst, we differentiate through Pontryagin’s Maximum Principle, and this allows to obtain the analytical derivative of a trajectory with respect to tunable parameters within an optimal control system, enabling end-to-end learning of dynamics, policies, or/and control objective functions; and second, we propose an auxiliary control system in the backward pass of the PDP framework, and the output of this auxiliary control system is the analytical derivative of the original system’s trajectory with respect to the parameters, which can be iteratively solved using standard control tools. We investigate three learning modes of the PDP: inverse reinforcement learning, system identiﬁcation, and control/planning. We demonstrate the capability of the PDP in each learning mode on different high-dimensional systems, including multi-link robot arm, 6-DoF maneuvering quadrotor, and 6-DoF rocket powered landing. 1

Introduction
Many learning tasks can ﬁnd their counterpart problems in control ﬁelds. These tasks both seek to obtain unknown aspects of a decision-making system with different terminologies compared below.
Table 1: Topic connections between control and learning (details presented in Section 2)
UNKNOWNS IN A SYSTEM
LEARNING METHODS
CONTROL METHODS
Dynamics xt+1=f θ(xt, ut)
Policy ut = πθ(t, xt)
Control objective J= (cid:80) t cθ(xt, ut)
System identiﬁcation
Markov decision processes
Reinforcement learning (RL) Optimal control (OC)
Inverse RL
Inverse OC
With the above connections, learning and control ﬁelds have begun to explore the complementary beneﬁts of each other: control theory may provide abundant models and structures that allow for efﬁcient or certiﬁcated algo-rithms for high-dimensional tasks, while learning enables to obtain these models from data, which are otherwise not readily attainable via classic control tools. Examples that enjoy both beneﬁts include model-based RL [1, 2], where dynamics models are used for sample efﬁciency;
Figure 1: left: PDP learns rocket landing and Koopman-operator control [3, 4], where via learning, control, right: PDP learns quadrotor dy-nonlinear systems are lifted to a linear observable space namics and control objective for imitation. to facilitate control design. Inspired by those, this paper aims to exploit the advantage of integrating learning and control and develop a uniﬁed framework that enables to solve a wide range of learning and control tasks, e.g., the challenging problems in Fig. 1. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2