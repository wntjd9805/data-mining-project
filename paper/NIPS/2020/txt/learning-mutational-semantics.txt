Abstract
In many natural domains, changing a small part of an entity can transform its semantics; for example, a single word change can alter the meaning of a sentence, or a single amino acid change can mutate a viral protein to escape antiviral treatment or immunity. Although identifying such mutations can be desirable (for example, therapeutic design that anticipates avenues of viral escape), the rules governing semantic change are often hard to quantify. Here, we introduce the problem of identifying mutations with a large effect on semantics, but where valid mutations are under complex constraints (for example, English grammar or biological viability), which we refer to as constrained semantic change search (CSCS). We propose an unsupervised solution based on language models that simultaneously learn continuous latent representations. We report good empirical performance on CSCS of single-word mutations to news headlines, map a continuous semantic space of viral variation, and, notably, show unprecedented zero-shot prediction of single-residue escape mutations to key inﬂuenza and HIV proteins, suggesting a productive link between modeling natural language and pathogenic evolution.1 1

Introduction
Much of the effort devoted to learning machine-intelligible representations of natural language semantics has been built on the “distributional hypothesis,” in which the context and co-occurrence of words is assumed to provide insight into the meaning of words [25, 22, 35, 38, 41, 43]. While distributional semantics was developed to model human intuitive notions of “meaning,” similar reasoning may be useful for domains beyond human intuition.
For example, like linguistic semantics, biological function is encoded by a sequence of tokens (the bases of nucleic acids or the amino acid residues of proteins) that is determined by a complex distributional structure. Promisingly, recent analyses of biological sequence inspired by tools for modeling natural language have been shown to improve prediction of biological function [9, 45, 5].
A pressing and still poorly understood biological problem is understanding how rapidly mutating viral proteins can evade recognition by “escaping” the immune system’s antibodies. Viral escape, which can be caused by even a single-residue change, has prevented the development of a universal antibody-based vaccine for inﬂuenza [30, 33] or human immunodeﬁciency virus (HIV) [6]. However, the rules governing viral ﬁtness are complex and a biological experiment that empirically tests the escape potential of all mutations to all viral strains would be prohibitively expensive. A key concept underlying this study is that, in order to escape the immune system, a mutation must not only preserve 1Code at https://github.com/brianhie/mutational-semantics-neurips2020. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
viral infectivity (i.e., it must be “grammatical”) but it must also be functionally altered so that it is no longer recognized by the immune system’s antibodies (i.e., it must have substantial “semantic change”).
Here, we introduce the problem of searching for sequence mutations based on both high semantic change and grammatical validity, which we call constrained semantic change search (CSCS). This is in contrast to settings concerned with semantic similarity search, rather than change. To gain intuition, we apply CSCS to natural language and, to demonstrate broader impact, we apply CSCS to predict viral escape. Our key contributions are (1) we introduce the CSCS problem formulation and show how learned language models offer a compelling solution with strong empirical results on both natural language and biological applications, suggesting that the distributional hypothesis from linguistics is also useful for modeling pathogenic evolution; (2) we develop an unsupervised neural language model for viral proteins and show that it learns semantically meaningful embeddings; and (3) we use
CSCS for zero-shot prediction of escape mutations for inﬂuenza and for HIV with quantitative results much higher than baseline methods. To our knowledge, we present the ﬁrst computational model that effectively predicts viral escape, potentially enabling vaccine or therapeutic design that anticipates escape before it occurs. 2 Methods 2.1 Problem Formulation
Intuitively, our goal is to identify mutations that induce high semantic change (e.g., a large impact on biological function) while being grammatically acceptable (e.g, biologically viable). More precisely, we are given a sequence of tokens deﬁned as x (cid:44) (x1, ..., xN ) such that xi ∈ X , i ∈ [N ], where X is a ﬁnite alphabet (e.g., characters or words for natural language, or amino acids for protein sequence).
Let ˜xi denote a mutation at position i and the mutated sequence as x[˜xi] (cid:44) (..., xi−1, ˜xi, xi+1, ...).
We ﬁrst require a semantic embedding z (cid:44) fs(x), where fs : X N → RK embeds discrete-alphabet sequences into a continuous space, where, ideally, closeness in embedding space would correspond to semantic similarity. We denote semantic change as the distance in embedding space, i.e.,
∆z[˜xi] (cid:44) (cid:107)z − z[˜xi](cid:107) = (cid:107)fs(x) − fs(x[˜xi])(cid:107) where (cid:107)·(cid:107) denotes a vector norm. The grammaticality of a mutation is described by p(˜xi|x), (1) (2) which takes values close to zero if x[˜xi] is not grammatical and close to one if it is grammatical.
Our objective combines semantic change and grammaticality as a linear combination a(˜xi; x) (cid:44) ∆z[˜xi] + βp(˜xi|x) for each possible mutation ˜xi and a user-speciﬁed parameter β ∈ [0, ∞). Mutations ˜xi are prioritized based on a(˜xi; x). We refer to ranking mutations based on semantic change and grammaticality as
CSCS. 2.2 Algorithms 2.2.1 Language Modeling
Algorithms for CSCS could potentially take many forms; for example, separate algorithms could be used to compute ∆z[˜xi] and p(˜xi|x) independently, or a two-step approach might be possible that computes one of the terms based on the value of the other.
Instead, we reasoned that a single approach could compute both terms simultaneously, based on learned language models that learn the probability distribution of a word given its context [38, 15, 43, 16, 44]. The language model we use throughout our experiments considers the full sequence context of a word and learns a latent variable probability distribution ˆp and function ˆfs, where, for all i ∈ [N ],
ˆp(xi|x[N ]\{i}, ˆzi) = ˆp(xi|ˆzi) and
ˆzi = ˆfs(x[N ]\{i}), 2
Figure 1: Constrained semantic change search (CSCS) for viral escape prediction. Left: Given an input sequence x and its semantics encoded by z, CSCS aims to ﬁnd a mutation to xCSCS that causes the largest semantic change (high ∆z), while remaining grammatical (high ˆp(x)). Right: Language model architecture with two stacked BiLSTM layers instantiating the semantic embedding function
ˆf , with the ﬁnal language model output used as grammaticality. i.e., latent variable ˆzi encodes the context x[N ]\{i} (cid:44) (..., xi−1, xi+1, ...) such that xi is conditionally independent of its context given the value of ˆzi.
We use different aspects of the language model to describe semantic change and grammaticality by setting terms (1) and (2) as
∆z[˜xi] (cid:44) (cid:107)ˆz − ˆz[˜xi](cid:107)1 and p(˜xi|x) (cid:44) ˆp(˜xi|ˆzi), 1 (cid:3)T
· · · where ˆz (cid:44) (cid:2)ˆzT is the concatenation of embeddings for each token, ˆz[˜xi] is deﬁned similarly but for the mutated sequence, and (cid:107)·(cid:107)1 is the (cid:96)1 norm, chosen because of more favorable properties compared to other standard distance metrics, though other metrics could be empirically quantiﬁed in future work [2].
ˆzT
N
Effectively, distances in embedding space are used to approximate semantic change and the emitted probability approximates grammaticality. We note that these modeling assumptions are not guaranteed to be perfectly speciﬁed, since, in the natural language setting for example, antonyms may also be close in embedding space and the language model output can also encode linguistic pragmatics in addition to grammaticality. However, we still ﬁnd these modeling assumptions to have good empirical support.
Training or parameterizing the language model is separate from CSCS, and the novelty of CSCS is in leveraging these models in a new way. An advantage of this approach is that it does not require any bespoke modiﬁcations to the general language modeling framework, other than requiring a continuous latent variable. CSCS can therefore leverage the noted multitask generality of language models [44].
Importantly, this approach to CSCS is completely unsupervised. Rather than assume access to labels explicitly encoding semantics or grammaticality, the model instead extracts this information from a large unlabeled corpus. This is critical in domains, like viral genomics, in which large sequence corpuses are available but functional proﬁling is limited. These corpuses implicitly contain information related to grammaticality or infectivity (e.g., all sequences are grammatically acceptable or come from infectious virus), but the algorithm must learn these rules from data. 2.2.2 Architecture
Based on the success of recurrent architectures for protein-sequence representation learning [9, 45, 5], we use similar encoder models for viral protein sequences (Figure 1). Our model passes the full context sequence into bidirectional long-short-term-memory (BiLSTM) hidden layers. We used the concatenated output of the ﬁnal LSTM layers as the semantic embedding, i.e.,
ˆzi (cid:44) (cid:2)LSTMf (gf (x1, ..., xi−1))T LSTMr(gr(xi+1, ..., xN ))T(cid:3)T where gf is the output of the preceding forward-directed layer, LSTMf is the ﬁnal forward-directed
LSTM layer, and gr and LSTMr are the corresponding reverse-directed components. The ﬁnal output 3
probability is a softmax-transformed linear transformation of ˆzi, i.e.,
ˆp(xi|x[N ]\{i}) (cid:44) softmax(Wˆzi + b) for some learned model parameters W and b. In our experiments, we used a 20-dimensional dense embedding for each element in the alphabet X , two BiLSTM layers with 512 units, and categorical cross entropy loss optimized by Adam with a learning rate of 0.001, β1 = 0.9, and β2 = 0.999.
Additional details on hyperparameter selection are given in Appendix 6.3.1. 2.2.3 Rank-Based Acquisition
Rather than acquiring mutations based on raw semantic change and grammaticality values, which may be on very different scales, we ﬁnd that selecting β is much easier in practice when ﬁrst rank-transforming the semantic change and grammaticality terms, i.e., acquiring based on a(cid:48)(˜xi; x) (cid:44) rank(∆z[˜xi]) + β rank(p(˜xi|x)).
All possible mutations ˜xi are then given priority based on the corresponding values of a(cid:48)(˜xi; x), from highest to lowest. Our empirical results have consistently good performance by simply setting β = 1 (equally weighting both terms), which we used in all experiments below unless otherwise noted. In this study, we deal with the unsupervised setting where β is a parameter but note that adding some supervision could learn β (or other, non-rank, transformations) from data. 2.2.4 Connection to Viral Escape
A language model is a probability distribution over sequences learned from a corpus of data. For any sequence x, the model will output a predicted probability p(x) of observing that sequence in the training data distribution. We call p(x) “grammaticality” because in natural language tasks, p(x) tends to be high for grammatically correct sentences. In the case of viral sequences, the training distribution consists of viral proteins that have evolved for high ﬁtness/virality, so we hypothesize that high grammaticality corresponds to high viral ﬁtness.
However, high ﬁtness alone does not indicate an escape mutation. For example, a viral protein with a neutral mutation will have equally high ﬁtness but may not look different enough to escape detection by the immune system, i.e., it will have no “antigenic” change. To identify mutations that do lead to large antigenic changes, we exploit the internal sequence embeddings learned by the language model. If two sequences have similar embeddings, then they have similar distributions over sequence continuations given the input tokens. As a natural-language example, “the men advance”, “the soldiers advance”, and “the three advance” have a similar set of possible word continuations and would have similar embeddings, while “the cash advance” has a nearly disjoint set of continuations and thus a different embedding. We hypothesize that neutral mutations should not affect the distribution over amino acids at other positions, while mutations that affect antigenicity do affect the distribution over other positions. Thus, the combination of high sequence probability (high ﬁtness) and a large change in embedding (antigenic change) indicates an escape mutation. 3