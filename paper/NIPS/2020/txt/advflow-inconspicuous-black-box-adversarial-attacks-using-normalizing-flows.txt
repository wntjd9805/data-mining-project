Abstract
Deep learning classiﬁers are susceptible to well-crafted, imperceptible variations of their inputs, known as adversarial attacks. In this regard, the study of power-ful attack models sheds light on the sources of vulnerability in these classiﬁers, hopefully leading to more robust ones. In this paper, we introduce AdvFlow: a novel black-box adversarial attack method on image classiﬁers that exploits the power of normalizing ﬂows to model the density of adversarial examples around a given target image. We see that the proposed method generates adversaries that closely follow the clean data distribution, a property which makes their detection less likely. Also, our experimental results show competitive performance of the proposed approach with some of the existing attack methods on defended classiﬁers.
The code is available at https://github.com/hmdolatabadi/AdvFlow. 1

Introduction
Deep neural networks (DNN) have been successfully applied to a wide variety of machine learning tasks. For instance, trained neural networks can reach human-level accuracy in image classiﬁca-tion [46]. However, Szegedy et al. [53] showed that such classiﬁers can be fooled by adding an imperceptible perturbation to the input image. Since then, there has been extensive research in this area known as adversarial machine learning, trying to design more powerful attacks and devising more robust neural networks. Today, this area encompasses a broader type of data than images, with video [25], graphs [65], text [34], and other types of data classiﬁers being attacked.
In this regard, the design of stronger adversarial attacks plays a crucial role in understanding the nature of possible real-world threats. The ultimate goal of such studies is to help neural networks become more robust against such adversaries. This line of research is extremely important as even the slightest ﬂaw in some real-world applications of DNNs such as self-driving cars can have severe, irreparable consequences [12].
In general, adversarial attack approaches can be classiﬁed into two broad categories: white-box and black-box. In white-box adversarial attacks, the assumption is that the threat model has full access to the target DNN. This way, adversaries can leverage their knowledge about the target model to generate adversarial examples (for instance, by taking the gradient of the neural network). In contrast, black-box attacks assume that they do not know the internal structure of the target model a priori. Instead, they can only query the model about some inputs, and work with the labels or conﬁdence levels associated with them [62]. Thus, black-box attacks seem to be making more realistic assumptions. In the beginning, black-box attacks were mostly thought of as the transferability of white-box adversarial examples to unseen models [42]. Recently, however, there has been more research to attack black-box models directly. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b) (c) (d) (e)
Figure 1: Adversarial perturbations generated by AdvFlow take the structure of the original image into account, resulting in less detectable adversaries compared to N ATTACK [33] (see Section 4.1).
The classiﬁer is a VGG19 [50] trained to detect smiles in CelebA [36] faces. (a) AdvFlow magniﬁed difference (b) AdvFlow adversarial example (c) clean image (d) N ATTACK adversarial example (e)
N ATTACK magniﬁed difference.
In this paper, we introduce AdvFlow: a black-box adversarial attack that makes use of pre-trained normalizing ﬂows to generate adversarial examples. In particular, we utilize ﬂow-based methods pre-trained on clean data to model the probability distribution of possible adversarial examples around a given image. Then, by exploiting the notion of search gradients from natural evolution strategies (NES) [59, 58], we solve the black-box optimization problem associated with adversarial example generation to adjust this distribution. At the end of this process, we wind up having a data distribution whose realizations are likely to be adversarial. Since this density is constructed on the top of the original data distribution estimated by normalizing ﬂows, we see that the generated perturbations take on the structure of data rather than an additive noise (see Figure 1). This property impedes distinguishing AdvFlow examples from clean data for adversarial example detectors, as they often assume that the adversaries come from a different distribution than the clean data. Moreover, we prove a lemma to conclude that adversarial perturbations generated by the proposed approach can be approximated by a normal distribution with dependent components. We then put our model under test and show its effectiveness in generating adversarial examples with 1) less detectability, 2) higher success rate, 3) lower number of queries, and 4) higher rate of transferability on defended models compared to the similar method of N ATTACK [33].
In summary, we make the following contributions:
• We introduce AdvFlow, a black-box adversarial attack that leverages the power of normaliz-ing ﬂows in modeling data distributions. To the best of our knowledge, this is the ﬁrst work that explores the use of ﬂow-based models in the design of adversarial attacks.
• We prove a lemma about the adversarial perturbations generated by AdvFlow. As a result of this lemma, we deduce that AdvFlows can generate perturbations with dependent elements, while this is not the case for N ATTACK [33].
• We show the power of the proposed approach in generating adversarial examples that have a similar distribution to the data. As a result, our method is able to mislead adversarial example detectors for they often assume adversaries come from a different distribution than the clean data. We then see the performance of the proposed approach in attacking some of the most recent adversarial training defense techniques. 2