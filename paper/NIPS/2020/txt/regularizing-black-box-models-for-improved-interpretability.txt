Abstract
Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for in-terpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, EXPO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to deﬁne. We demonstrate that post-hoc explanations for EXPO-regularized models have better explanation quality, as measured by the common ﬁdelity and stability metrics. We verify that improving these metrics leads to signiﬁcantly more useful explanations with a user study on a realistic task. 1

Introduction
Complex learning-based systems are increasingly shaping our daily lives. To monitor and understand these systems, we require clear explanations of model behavior. Although model interpretability has many deﬁnitions and is often application speciﬁc [Lipton, 2016], local explanations are a popular and powerful tool [Ribeiro et al., 2016] and will be the focus of this work.
Recent techniques in interpretable machine learning range from models that are interpretable by-design [e.g., Wang and Rudin, 2015, Caruana et al., 2015] to model-agnostic post-hoc systems for explaining black-box models such as ensembles and deep neural networks [e.g., Ribeiro et al., 2016,
Lei et al., 2016, Lundberg and Lee, 2017, Selvaraju et al., 2017, Kim et al., 2018]. Despite the variety of technical approaches, the underlying goal of these methods is to develop an interpretable predictive system that produces two outputs: a prediction and its explanation.
Both by-design and post-hoc approaches have limitations. On the one hand, by-design approaches are restricted to working with model families that are inherently interpretable, potentially at the cost of accuracy. On the other hand, post-hoc approaches applied to an arbitrary model usually offer no recourse if their explanations are not of suitable quality. Moreover, recent methods that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design approaches that impose constraints on the model families they consider [e.g., Al-Shedivat et al., 2017, Plumb et al., 2018, Alvarez-Melis and Jaakkola, 2018a].
In this work, we propose a strategy called Explanation-based Optimization (EXPO) that allows us to interpolate between these two paradigms by adding an interpretability regularizer to the loss function used to train the model. EXPO uses regularizers based on the ﬁdelity [Ribeiro et al., 2016, Plumb et al., 2018] or stability [Alvarez-Melis and Jaakkola, 2018a] metrics. See Section 2 for deﬁnitions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Neighborhood Fidelity of LIME-generated explanations (lower is better) vs. predictive Mean
Squared Error of several models trained on the UCI ‘housing’ regression dataset. The values in blue denote the regularization weight of EXPO. One of the key contributions of EXPO is allowing us to pick where we are along the accuracy-interpretability curve for a black-box model.
Unlike by-design approaches, EXPO places no explicit constraints on the model family because its regularizers are differentiable and model agnostic. Unlike post-hoc approaches, EXPO allows us to control the relative importance of predictive accuracy and explanation quality. In Figure 1, we see an example of how EXPO allows us to interpolate between these paradigms and overcome their respective weaknesses.
Although ﬁdelity and stability are standard proxy metrics, they are only indirect measurements of the usefulness of an explanation. To more rigorously test the usefulness of EXPO, we additionally devise a more realistic evaluation task where humans are asked to use explanations to change a model’s prediction. Notably, our user study falls under the category of Human-Grounded Metric evaluations as deﬁned by Doshi-Velez and Kim [2017].
The main contributions of our work are as follows: 1. Interpretability regularizer. We introduce, EXPO-FIDELITY, a differentiable and model agnos-tic regularizer that requires no domain knowledge to deﬁne. It approximates the ﬁdelity metric on the training points in order to improve the quality of post-hoc explanations of the model. 2. Empirical results. We compare models trained with and without EXPO on a variety of regression and classiﬁcation tasks.1 Empirically, EXPO slightly improves test accuracy and signiﬁcantly improves explanation quality on test points, producing at least a 25% improvement in terms of explanation ﬁdelity. This separates it from many other methods which trade-off between predictive accuracy and explanation quality. These results also demonstrate that EXPO’s effects generalize from the training data to unseen points. 3. User study. To more directly test the usefulness of EXPO, we run a user study where participants complete a simpliﬁed version of a realistic task. Quantitatively, EXPO makes it easier for users to complete the task and, qualitatively, they prefer using the EXPO-regularized model. This is additional validation that the ﬁdelity and stability metrics are useful proxies for interpretability. 2