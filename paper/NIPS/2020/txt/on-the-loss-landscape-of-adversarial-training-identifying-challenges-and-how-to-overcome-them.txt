Abstract
We analyze the inﬂuence of adversarial training on the loss landscape of machine learning models. To this end, we ﬁrst provide analytical studies of the properties of adversarial loss functions under different adversarial budgets. We then demonstrate that the adversarial loss landscape is less favorable to optimization, due to increased curvature and more scattered gradients. Our conclusions are validated by numerical analyses, which show that training under large adversarial budgets impede the escape from suboptimal random initialization, cause non-vanishing gradients and make the model ﬁnd sharper minima. Based on these observations, we show that a periodic adversarial scheduling (PAS) strategy can effectively overcome these challenges, yielding better results than vanilla adversarial training while being much less sensitive to the choice of learning rate. 1

Introduction
State-of-the-art deep learning models have been found to be vulnerable to adversarial attacks [18, 34, 45]. Imperceptible perturbations of the input can make the model produce wrong predictions with high conﬁdence. This raises concerns about deep learning’s deployment in safety-critical applications.
Although many training algorithms have been proposed to counter such adversarial attacks, most of them were observed to fail when facing stronger attacks [4, 10]. Adversarial training [33] is one of the few exceptions, so far remaining effective and thus popular. It uses adversarial examples generated with the attacker’s scheme to update the model parameters. However, adversarial training and its variants [2, 6, 24, 42, 53] have been found to have a much larger generalization gap [37] and to require larger model capacity for convergence [49]. Although recent works [6, 40] show that the adversarial training error reduces to almost 0% with a large enough model and that the generalization gap can be narrowed by using more training data, convergence in adversarial training remains much slower than in vanilla training on clean data. This indicates discrepancies in the underlying optimization landscapes. While much work has studied the loss landscape of deep networks in vanilla training [12, 13, 14, 15, 31], such an analysis in adversarial training remains unaddressed.
Here we study optimization in adversarial training. Vanilla training can be considered as a special case where no perturbation is allowed, i.e., zero adversarial budget. Therefore, we focus on the impact of the adversarial budget size on the loss landscape. In this context, we investigate from a theoretical and empirical perspective how different adversarial budget sizes affect the loss landscape to make optimization more challenging. Our analyses start with linear models and then generalize to nonlinear deep learning ones. We study the whole training process and identify different behaviors in the early and ﬁnal stages of training. Based on our observations, we then introduce a scheduling strategy for the adversarial budget during training. We empirically show this scheme to yield better performance and to be less sensitive to the learning rate than vanilla adversarial training. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Contributions. Our contributions can be summarized as follows. 1) From a theoretical perspective, we show that, for linear models, adversarial training under a large enough budget produces a constant classiﬁer. For general nonlinear models, we identify the existence of an abrupt change in the adversarial examples, which makes the loss landscape less smooth. This causes severe gradient scattering and slows down the convergence of training. 2) Our numerical analysis shows that training under large adversarial budgets hinders the model to escape from suboptimal initial regions, while also causing large non-vanishing gradients in the ﬁnal stage of training. Furthermore, by Hessian analysis, we evidence that the minima reached in the adversarial loss landscape are sharper when the adversarial budget is bigger. 3) We show that a periodic adversarial scheduling (PAS) strategy, corresponding to a cyclic adversarial budget scheduling scheme with warmup, addresses these challenges. Speciﬁcally, it makes training less sensitive to the choice of learning rate and yields better robust accuracy than vanilla adversarial training without any computational overhead.
Notation and Terminology. We use plain letters, bold lowercase letters and bold uppercase letters represents the Euclidean norm of vector to represent scalars, vectors and matrices, respectively.
N v and [K] is an abbreviation of the set (xi, yi) i=1,
−
}
{
Rk, which is usually
Rm where (xi, yi)
×
→ a neural network, and a risk function (cid:96) : Rk
R, which is the softmax cross-entropy loss. The (p) (x) of a data point x, whose size is (cid:15), is deﬁned based on an lp norm-based adversarial budget (cid:15)
S x(cid:48) x constraint p (cid:107)
−
[K], the classiﬁer consists of a logit function f : Rm (cid:15)(x) to denote the l∞ constraint for simplicity. (cid:107)
. In a classiﬁcation problem 0, 1, 2, ..., K
{
, and we use v (cid:107) 1
} x(cid:48)
{
[K]
→
×
∈
} (cid:15)
|(cid:107)
Given the model parameters θ individual data point, ignoring the label y for simplicity. If we use loss function under adversarial budget
S
Θ, we use g(x, θ) : Rm
→
×
Θ
≤
∈
R to denote the loss function for an (cid:15)(θ) to denote the adversarial
L (x), adversarial training solves the min-max problem (p) (cid:15)
S min
θ L (cid:15)(θ) := 1
N
N (cid:88) i=1 g(cid:15)(xi, θ) where g(cid:15)(xi, θ) := max i∈S (p) x(cid:48) (cid:15) (xi) g(x(cid:48) i, θ) . (1) (θ) :=
L input in
= 0, the adversarial example x(cid:48) (cid:15)(θ) the vanilla and adversarial loss landscape, respectively. Similarly, we use 0(θ) is the vanilla loss function. If (cid:15)
L (p) (cid:15)
S i, i.e., the worst-case (xi), depends on the model parameters. We call the landscape of functions (θ) and
L (cid:15)(θ) to (θ) and
E
L (x). In this paper, we call a represent the clean error and robust error under adversarial budget function smooth if it is C 1-continuous. We use θ0 to denote the initial parameters. “Initial plateau” or “suboptimal region in the early stage of training” indicate the parameters that are close to the initial ones and have similar performance. “Vanilla training” means training based on clean input data, while “vanilla adversarial training” represents the popular adversarial training method in [33]. (p) (cid:15)
S
E 2