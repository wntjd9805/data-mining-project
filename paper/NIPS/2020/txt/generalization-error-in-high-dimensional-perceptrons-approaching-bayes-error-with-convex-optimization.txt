Abstract
We consider a commonly studied supervised classiﬁcation of a synthetic dataset whose labels are generated by feeding a one-layer neural network with random i.i.d inputs. We study the generalization performances of standard classiﬁers in the high-dimensional regime where α = n/d is kept ﬁnite in the limit of a high dimension d and number of samples n. Our contribution is three-fold: First, we prove a formula for the generalization error achieved by (cid:96)2 regularized classiﬁers that minimize a convex loss. This formula was ﬁrst obtained by the heuristic replica method of statistical physics. Secondly, focussing on commonly used loss functions and optimizing the (cid:96)2 regularization strength, we observe that while ridge regression performance is poor, logistic and hinge regression are surprisingly able to approach the Bayes-optimal generalization error extremely closely. As α
→ ∞ they lead to Bayes-optimal rates, a fact that does not follow from predictions of margin-based generalization error bounds. Third, we design an optimal loss and regularizer that provably leads to Bayes-optimal generalization error. 1

Introduction
High-dimensional statistics, where the ratio α = n/d is kept ﬁnite while the dimensionality d and the number of samples n grow, often display interesting non-intuitive features. Asymptotic generalization performances for such problems in the so-called teacher-student setting, with synthetic data, have been the subject of intense investigations spanning many decades [1–6]. To understand the effectiveness of modern machine learning techniques, and also the limitations of the classical statistical learning approaches [7, 8], it is of interest to revisit this line of research. Indeed, this direction is currently the subject to a renewal of interests, as testiﬁed by some very recent, yet already rather inﬂuential papers [9–13]. The present paper subscribes to this line of work and studies high-dimensional classiﬁcation within one of the simplest models considered in statistics and machine learning: convex linear estimation with data generated by a teacher perceptron [14]. We will focus on the generalization abilities in this problem, and compare the performances of Bayes-optimal estimation to the more standard Empirical Risk Minimization (ERM). We then compare the results with the prediction of standard generalization bounds that illustrate in particular their limitation even in this simple, yet non-trivial, setting. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Synthetic data model — We consider a supervised machine learning task, whose dataset is gener-ated by a single layer neural network, often named a teacher [1–3], that belongs to the Generalized
Linear Model (GLM) class. Therefore, we assume the n samples are generated according to y = ϕout(cid:63) (cid:19)
Xw(cid:63) (cid:18) 1
√d
⇔ y
∼
Pout(cid:63) (cid:18)
.(cid:12) (cid:12) 1
√d (cid:19)
,
Xw(cid:63) (1) 1 d
∈ w(cid:63) 2 2 (cid:107)
≡
Rn
E (cid:2) (cid:107) (cid:3) and ϕout(cid:63) : R
Rd denotes the ground truth vector drawn from a probability distribution Pw(cid:63) with second where w(cid:63)
R represents a component-wise deterministic or moment ρd,w(cid:63) stochastic activation function equivalently associated to a distribution Pout(cid:63) . The input data matrix
X = (xµ)n (0, Id). Even though the framework we use and the theorems and results we derive are valid for a rather generic channel in eq. (1) — including regression problems — we will mainly focus the presentation on the commonly considered perceptron case: a binary classiﬁcation task with data given by a sign activation function
ϕout(cid:63) (z) = sign (z), with a Gaussian weight distribution Pw(cid:63) (w(cid:63)) = 1 labels are thus generated as d contains i.i.d Gaussian vectors, i.e
[1 : n], xµ ∼ N w(cid:63) (0, ρd,w(cid:63) Id). The
µ=1 ∈ (cid:55)→
N
±
∈
µ
∀
× y = sign (cid:19)
Xw(cid:63) (cid:18) 1
√d
, with w(cid:63) w(cid:63) (0, ρd,w(cid:63) Id) .
∼ N (2)
This particular setting was extensively studied in the past [1, 15] and is interesting in the sense it does not show a computational-to-statistical gap. Yet, our analysis and the set of equations presented in
SM. III.15 are valid more generically to any other ground truth distributions Pout(cid:63) and Pw(cid:63) . Finally, the isotropic Gaussian hypothesis of the input vectors X can be relaxed to non-isotropic Gaussian.
Empirical Risk Minimization — The workhorse of machine learning is Empirical Risk Minimiza-tion (ERM), where one minimizes a loss function in the corresponding high-dimensional parameter space Rd. To avoid overﬁtting of the training set one often adds a regularization term r. ERM then (w; y, X)] where the regularized training loss corresponds to estimating ˆwerm = argminw [ is
L x(cid:124) 1
µw, deﬁned by, using the notation zµ (w, xµ)
√d
L (w; y, X) =
L
≡ n (cid:88)
µ=1 l (yµ, zµ (w, xµ)) + r (w) . (3)
The goal of the present paper is to discuss the generalization performance of these estimators for the classiﬁcation task (2) in the high-dimensional limit. We focus our analysis on commonly used loss functions l, namely the square lsquare(y, z) = 1 z)2, logistic llogistic(y, z) = log(1 + exp( 2 (y yz)) and hinge losses lhinge(y, z) = max (0, 1 yz). We will mainly illustrate our results for the (cid:96)2 2 regularization r (w) = λ 2/2, where we introduced a regularization strength hyper-parameter λ, (cid:107) even though a similar rigorous analysis can be conducted for any separable and convex regularizer. w (cid:107)
−
−
−