Abstract
How to obtain informative representations of molecules is a crucial prerequisite in AI-driven drug design and discovery. Recent researches abstract molecules as graphs and employ Graph Neural Networks (GNNs) for molecular representation learning. Nevertheless, two issues impede the usage of GNNs in real scenarios: (1) insufﬁcient labeled molecules for supervised training; (2) poor generalization capability to new-synthesized molecules. To address them both, we propose a novel framework, GROVER, which stands for Graph Representation frOm self-superVised mEssage passing tRansformer. With carefully designed self-supervised tasks in node-, edge- and graph-level, GROVER can learn rich structural and seman-tic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks into the Transformer-style architecture to deliver a class of more expressive en-coders of molecules. The ﬂexibility of GROVER allows it to be trained efﬁciently on large-scale molecular dataset without requiring any supervision, thus being immunized to the two issues mentioned above. We pre-train GROVER with 100 million parameters on 10 million unlabelled molecules—the biggest GNN and the largest training dataset in molecular representation learning. We then leverage the pre-trained GROVER for molecular property prediction followed by task-speciﬁc
ﬁne-tuning, where we observe a huge improvement (more than 6% on average) from current state-of-the-art methods on 11 challenging benchmarks. The insights we gained are that well-designed self-supervision losses and largely-expressive pre-trained models enjoy the signiﬁcant potential on performance boosting. 1

Introduction
Inspired by the remarkable achievements of deep learning in many scientiﬁc domains, such as com-puter vision [57, 19], natural language processing [53, 51], and social networks [31, 3], researchers are exploiting deep learning approaches to accelerate the process of drug discovery and reduce costs by facilitating the rapid identiﬁcation of molecules [5]. Molecules can be naturally represented by molecular graphs which preserve rich structural information. Therefore, supervised deep learning of graphs, especially with Graph Neural Networks(GNNs) [46, 25] have shown promising results in many tasks, such as molecular property prediction [13, 23] and virtual screening [56, 64].
∗Equal contribution.
†Wenbing Huang is the corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Despite the fruitful progress, two issues still impede the usage of deep learning in real scenarios: (1) insufﬁcient labeled data for molecular tasks; (2) poor generalization capability of models in the enormous chemical space. Different from other domains (such as image classiﬁcation) that have rich-source labeled data, getting labels of molecular property requires wet-lab experiments which is time-consuming and resource-costly. As a consequence, most public molecular benchmarks contain far-from-adequate labels. Conducting deep learning on these benchmarks is prone to over-ﬁtting and the learned model can hardly cope with the out-of-distribution molecules.
Indeed, it has been a long-standing goal in deep learning to improve the generalization power of neural networks. Towards this goal, certain progress has been made. For example, in Natural
Language Processing (NLP), researchers can pre-train the model from large-scale unlabeled sentences via a newly-proposed technique—the self-supervised learning. Several successful self-supervised pretraining strategies, such as BERT [9] and GPT [38] have been developed to tackle a variety of language tasks. By contending that molecule can be transformed into sequential representation—
SMILES [59], the work by [58] tries to adopt the BERT-style method to pretrain the model, and Liu et.al. [29] also exploit the idea from N-gram approach in NLP and conducts vertices embedding by predicting the vertices attributes. Unfortunately, these approaches fail to explicitly encode the structural information of molecules as using the SMILES representation is not topology-aware.
Without using SMILES, several works aim to establish a pre-trained model directly on the graph representations of molecules. Hu et.al. [18] investigate the strategies to construct the three pre-training tasks, i.e., context prediction and node masking for node-level self-supervised learning and graph property prediction for graph-level pre-training. We argue that the formulation of pre-training in this way is suboptimal. First, in the masking task, they treat the atom type as the label. Different from NLP tasks, the number of atom types in molecules is much smaller than the size of a language vocabulary. Therefore, it would suffer from serious representation ambiguity and the model is hard to encode meaningful information especially for the highly frequent atoms. Second, the graph-level pre-training task in [18] is supervised. This limits the usage in practice since most of molecules are completely unlabelled, and it also introduces the risk of negative transfer for the downstream tasks if they are inconsistent to the graph-level supervised loss.
In this paper, we improve the pre-training model for molecular graph by introducing a novel molecular representation framework, GROVER, namely, Graph Representation frOm self-superVised mEssage passing tRansformer. GROVER constructs two types of self-supervised tasks. For the node/edge-level tasks, instead of predicting the node/edge type alone, GROVER randomly masks a local subgraph of the target node/edge and predicts this contextual property from node embeddings. In this way,
GROVER can alleviate the ambiguity problem by considering both the target node/edge and its context being masked. For the graph-level tasks, by incorporating the domain knowledge, GROVER extracts the semantic motifs existing in molecular graphs and predicts the occurrence of these motifs for a molecule from graph embeddings. Since the semantic motifs can be obtained by a low-cost pattern matching method, GROVER can make use of any molecular to optimize the graph-level embedding.
With self-supervised tasks in node-, edge- and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks with the Transformer-style architecture to deliver a class of highly expressive encoders of molecules. The ﬂexibility of GROVER allows it to be trained efﬁciently on large-scale molecular data without requiring any supervision.
We pre-train GROVER with 100 million parameters on 10 million of unlabelled molecules—the biggest GNN and the largest training dataset that have been applied. We then leverage the pre-trained
GROVER models to downstream molecular property prediction tasks followed by task-speciﬁc
ﬁne-tuning. On the downstream tasks, GROVER achieve 22.4% relative improvement compared with [29] and 7.4% relative improvement compared with [18] on classiﬁcation tasks. Furthermore, even compared with current state-of-the-art results for each data set, we observe a huge relative improvement of GROVER (more than 6% on average) over 11 popular benchmarks. 2