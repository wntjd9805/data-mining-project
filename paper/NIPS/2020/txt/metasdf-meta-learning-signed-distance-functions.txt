Abstract
Neural implicit shape representations are an emerging paradigm that offers many potential beneﬁts over conventional discrete representations, including memory efﬁciency at a high spatial resolution. Generalizing across shapes with such neural implicit representations amounts to learning priors over the respective function space and enables geometry reconstruction from partial or noisy observations.
Existing generalization methods rely on conditioning a neural network on a low-dimensional latent code that is either regressed by an encoder or jointly optimized in the auto-decoder framework. Here, we formalize learning of a shape space as a meta-learning problem and leverage gradient-based meta-learning algorithms to solve this task. We demonstrate that this approach performs on par with auto-decoder based approaches while being an order of magnitude faster at test-time inference.
We further demonstrate that the proposed gradient-based method outperforms encoder-decoder based methods that leverage pooling-based set encoders. 1

Introduction
Humans possess an impressive intuition for 3D shapes; given partial observations of an object we can easily imagine the shape of the complete object. Computer vision and machine learning researchers have long sought to reproduce this ability with algorithms. An emerging class of neural implicit shape representations, for example using signed distance functions parameterized by neural networks, promises to achieve these abilities by learning priors over neural implicit shape spaces [1, 2]. In such methods, each shape is represented by a function Φ, e.g., a signed distance function. Generalizing across a set of shapes thus amounts to learning a prior over the space of these functions Φ. Two questions arise: (1) How do we parameterize functions Φ, and (2) how do we infer the parameters of such a Φ given a set of (partial) observations?
Existing methods assume that the space of functions Φ is low-dimensional and represent each shape as a latent code, which parameterizes the full function Φ via concatenation-based conditioning or hypernetworks. These latent codes are either directly inferred by a 2D or 3D convolutional encoder, taking either a single image or a classic volumetric representation as input, or via the auto-decoder framework, where separate latent codes per training sample are treated as free variables at training time. Convolutional encoders, while fast, require observations on a regular grid, and are not equivariant to 3D transformations [3]. They further do not offer a straightforward way to accumulate information from a variable number of observations, usually resorting to permutation-invariant pooling of per-observation latent codes [4]. Recently proposed pooling-based set encoders may encode sets of
∗These authors contributed equally to this work. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: We propose to leverage gradient-based meta-learning to learn the initialization of an implicit neural representation such that it can be adapted to any speciﬁc instance in few gradient descent steps. variable cardinality [5, 6], but have been found to underﬁt the context. This is corroborated by theoretical evidence, showing that to guarantee universality of the embedded function, the embedding requires a dimensionality of at least the number of context points [7]. In practice, most approaches thus leverage the auto-decoder framework for generalization, which is agnostic to the number of observations and does not require observations on a regular grid. This has yielded impressive results on few-shot reconstruction of geometry, appearance and semantic properties [8, 9].
To infer the parameters of a single Φ from a set of observations at test time, encoder-based methods only require a forward pass. In contrast, the auto-decoder framework does not learn to infer an embedding from observations, and instead requires solving an optimization problem to ﬁnd a low-dimensional latent embedding at test time, which may take several seconds even for simple scenes, such as single 3D objects from the ShapeNet dataset.
In this work, we identify a key connection between learning of neural implicit function spaces and meta-learning. We then propose to leverage recently proposed gradient-based meta-learning algorithms for the learning of shape spaces, with beneﬁts over both encoder and auto-decoder based approaches. Speciﬁcally, the proposed approach performs on par with auto-decoder based models while being an order of magnitude faster at inference time, does not require observations on a regular grid, naturally interfaces with a variable ‘number of observations, outperforms pooling-based set-encoder approaches, and does not require the assumption of a low-dimensional latent space. We note that we do not propose any improvements to gradient-based meta-learning algorithms in general. 2