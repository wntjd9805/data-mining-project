Abstract
Value function approximation has demonstrated phenomenal empirical success in reinforcement learning (RL). Nevertheless, despite a handful of recent progress on developing theory for RL with linear function approximation, the understanding of general function approximation schemes largely remains missing. In this paper, we establish the ﬁrst provably efﬁcient RL algorithm with general value function approximation. We show that if the value functions admit an approximation with
O(poly(dH)pT ) a function class
, our algorithm achieves a regret bound of
F where d is a complexity measure of that depends on the eluder dimension [Russo and Van Roy, 2013] and log-covering numbers, H is the planning horizon, and
T is the number interactions with the environment. Our theory generalizes the linear MDP assumption to general function classes. Moreover, our algorithm is model-free and provides a framework to justify the effectiveness of algorithms used in practice.
F e 1

Introduction
In reinforcement learning (RL), we study how an agent maximizes the cumulative reward by interact-ing with an unknown environment. RL ﬁnds enormous applications in a wide variety of domains, e.g., robotics [32], education [33], gaming-AI [50], etc. The unknown environment in RL is often modeled as a Markov decision process (MDP), in which there is a set of states that describes all possible status of the environment. At a state s
, an agent interacts with the environment by taking an action a from an action space 2S which is drawn from some unknown transition distribution, and the agent also receives an immediate reward. The agent interacts with the environment episodically, where each episode consists of H steps. The goal of the agent is to interact with the environment strategically such that after a certain number of interactions, sufﬁcient information is collected so that the agent can act nearly optimally afterward. The performance of an agent is measured by the regret, which is deﬁned as the difference between the total rewards collected by the agent and those a best possible agent would collect.
. The environment then transits to another state s0 2S
A
S
Without additional assumptions on the structure of the MDP, the best possible algorithm achieves a
T )1 [7], where T is the total number of steps the agent interacts with regret bound of the environment. In other words, the algorithm learns to interact with the environment nearly as well
|S||A|
⇥(
H e 1Throughout the paper, we use p
) to suppress logarithmic factors.
O(
· 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. e
|S||A| as an optimal agent after roughly H steps. This regret bound, however, can be unacceptably large in practice. E.g., the game of Go has a state space with size 3361, and the state space of certain robotics applications can even be continuous. Practitioners apply function approximation schemes to tackle this issue, i.e., the value of a state-action pair is approximated by a function which is able to predict the value of unseen state-action pairs given a few training samples. The most commonly used function approximators are deep neural networks (DNN) which have achieved remarkable success in playing video games [40], the game of Go [52], and controlling robots [3]. Nevertheless, despite the outstanding achievements in solving real-world problems, no convincing theoretical guarantees were known about RL with general value function approximators like DNNs.
Recently, there is a line of research trying to understand RL with simple function approximators, e.g. linear functions. For instance, given a feature extractor which maps state-action pairs to d-dimensional feature vectors, [63, 64, 29, 9, 41, 26, 67, 20, 61, 66, 19] developed algorithms with regret bound proportional to poly(dH)pT which is independent of the size of
. Although being much more efﬁcient than algorithms for the tabular setting, these algorithms require a well-designed feature extractor and also make restricted assumptions on the transition model. This severely limits the scope that these approaches can be applied to, since obtaining a good feature extractor is by no means easy and successful algorithms used in practice usually specify a function class (e.g. DNNs with a speciﬁc architecture) rather than a feature extractor. To our knowledge, the following fundamental question about RL with general function approximation remains unanswered at large:
S⇥A
Does RL with general function approximation learn to interact with an unknown environment provably efﬁciently?
In this paper, we address the above question by developing a provably efﬁcient (both computationally and statistically) Q-learning algorithm that works with general value function approximators. To run the algorithm, we are only required to specify a value function class, without the need for feature extractors. Since this is the same requirement as algorithms used in practice like deep Q-learning [40], our theoretical guarantees on the algorithm provide a justiﬁcation of why practical algorithms work
O(poly(dH)pT ) where so well. Furthermore, we show that our algorithm enjoys a regret bound of d is a complexity measure of the function class that depends on the eluder dimension [48] and log-covering numbers. Our theory generalizes the linear MDP assumption in [63, 29] to general function classes, and our algorithm provides comparable regret bounds when applied to the linear case. e 1.1