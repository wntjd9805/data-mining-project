Abstract
Deep learning models are increasingly popular in many machine learning appli-cations where the training data may contain sensitive information. To provide formal and rigorous privacy guarantee, many learning systems now incorporate differential privacy by training their models with (differentially) private SGD. A key step in each private SGD update is gradient clipping that shrinks the gradient of an individual example whenever its (cid:96)2 norm exceeds some threshold. We ﬁrst demonstrate how gradient clipping can prevent SGD from converging to a station-ary point. We then provide a theoretical analysis that fully quantiﬁes the clipping bias on convergence with a disparity measure between the gradient distribution and a geometrically symmetric distribution. Our empirical evaluation further suggests that the gradient distributions along the trajectory of private SGD indeed exhibit symmetric structure that favors convergence. Together, our results provide an explanation why private SGD with gradient clipping remains effective in practice despite its potential clipping bias. Finally, we develop a new perturbation-based technique that can provably correct the clipping bias even for instances with highly asymmetric gradient distributions. 1

Introduction
Many modern applications of machine learning rely on datasets that may contain sensitive personal information, including medical records, browsing history, and geographic locations. To protect the private information of individual citizens, many machine learning systems now train their models subject to the constraint of differential privacy [Dwork et al., 2006], which informally requires that no individual training example has a signiﬁcant inﬂuence on the trained model. To achieve this formal privacy guarantee, one of the most popular training methods, especially for deep learning, is differentially private stochastic gradient descent (DP-SGD) [Bassily et al., 2014, Abadi et al., 2016b].
At a high level, DP-SGD is a simple modiﬁcation of SGD that makes each step differentially private with the Gaussian mechanism: at each iteration t, it ﬁrst computes a gradient estimate gt based on a random subsample, and then updates the model using a noisy gradient ˜gt = gt + η, where η is a noise vector drawn from a multivariate Gaussian distribution.
Despite the simple form of DP-SGD, there is a major disparity between its theoretical analysis and practical implementation. The formal privacy guarantee of Gaussian mechanism requires that the per-coordinate standard deviation of the noise vector η scales linearly with the (cid:96)2 sensitivity of the gradient estimate gt—that is, the maximal change on gt in (cid:96)2 distance if by changining a single example. To bound the (cid:96)2-sensitivity, existing theoretical analyses typically assume that the loss function is L-Lipschitz in the model parameters, and the constant L is known to the algorithm designer for setting the noise rate [Bassily et al., 2014, Wang and Xu, 2019]. Since this assumption implies that the gradient of each example has (cid:96)2 norm bounded by L, any gradient estimate from averaging over the gradients of m examples has (cid:96)2-sensitivity bounded by L/m. However, in many 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
practical settings, especially those with deep learning models, such Lipschitz constant or gradient bounds are not a-priori known or even computable (since it involves taking the worst case over both examples and pairs of parameters). In practice, the bounded (cid:96)2-sensitivity is ensured by gradient clipping [Abadi et al., 2016b] that shrinks an individual gradient whenever its (cid:96)2 norm exceeds certain threshold c. More formally, given any gradient g on a simple example and a clipping threshold c, the gradient clipping does the following clip(g, c) = g · min 1, (cid:18) (cid:19)
. c (cid:107)g(cid:107) (1)
However, the clipping operation can create a substantial bias in the update direction. To illustrate this clipping bias, consider the following two optimization problems even without the privacy constraint. 2 (x − ai)2 over x ∈ R, where a1 = a2 = −3 and
Example 1. Consider optimizing f (x) = 1 1 3 a3 = 9. Since the gradient ∇f (x) = x − 1, the optimum is x∗ = 1. Now suppose we run SGD with gradient clipping with a threshold of c = 1. At the optimum, the gradients for all three examples are clipped and the expected clipped gradient is 1/3, which leads the parameter to move away from x∗. i=1 (cid:80)3 (cid:80)2
Example 2. Let f (x) = 1 2 (x − ai)2, where a1 = −3 and a2 = 3. The minimum of f is 2 achieved at x∗ = 0, where the expected clipped gradient is also 0. However, SGD with clipped gradients and c = 1 may never converge to x∗ since the expected clipped gradients are all 0 for any x ∈ [−2, 2], which means all these points are "stationary" for the algorithm. i=1 1
Both examples above show that clipping bias can prevent convergence in the worst case. Existing analyses on gradient clipping quantify this clipping bias either with 1) the difference between clipped and unclipped gradients [Pichapati et al., 2019], or 2) the fraction of examples with gradient norms exceeding the clip threshold c [Zhang et al., 2019]. These approaches suggest that a small clip threshold will lead to large clipping bias and worsen the training performance of DP-SGD. However, in practice, DP-SGD often remains effective even with a small clip threshold, which indicates a gap in the current theoretical understanding of gradient clipping. 1.1 Our results
We study the effects of gradient clipping on SGD and DP-SGD and provide:
Symmetricity-based analysis. We characterize the clipping bias on the convergence to stationary points through the geometric structure of the gradient distribution. To isolate the clipping effects, we
ﬁrst analyze the non-private SGD with gradient clipping (but without Gaussian perturbation), with the following key analysis steps. We ﬁrst show that the inner product E[(cid:104)∇f (xt), gt(cid:105)] goes to zero in
SGD, where ∇f (x) denotes the true gradient and gt denotes a clipped stochastic gradient. Secondly, we show that when the gradient distribution is symmetric, the inner product E[(cid:104)∇f (xt), gt(cid:105)] upper bounds a constant re-scaling of (cid:107)∇f (xt)(cid:107), and so SGD minimizes the gradient norm. We then quantify the clipping bias via a coupling between the gradient distribution and a nearby symmetric distribution and express it as a disparity measure (that resembles the Wasserstein distance) between the two distributions. As a result, when the gradient distributions are near-symmetric or when the clipping bias favors convergence, the clipped gradient remains aligned with the true gradient, even if clipping aggressively shrinks almost all the sample gradients.
Theoretical and empirical evaluation of DP-SGD. Building on the SGD analysis, we obtain a similar convergence guarantee on DP-SGD with gradient clipping. Importantly, we are able to prove such convergence guarantee even without Lipschitzness of the loss function, which is often required for DP-SGD analyses. We also provide extensive empirical studies to investigate the gradient distributions of DP-SGD across different epoches on two real datasets. To visualize the symmetricity of the gradient distributions, we perform multiple random projections on the gradients and examine the two-dimensional projected distributions. Our results suggest that the gradient distributions in
DP-SGD quickly exhibit symmetricity, despite the asymmetricity at initialization.
Gradient correction mechanism. Finally, we provide a simple modiﬁcation to DP-SGD that can mitigate the clipping bias. We show that perturbing the gradients before clipping can provably reduce the clipping bias for any gradient distribution. The pre-clipping perturbation does not by itself provide privacy guarantees, but can trade-off the clipping bias with higher variance. 2
1.2