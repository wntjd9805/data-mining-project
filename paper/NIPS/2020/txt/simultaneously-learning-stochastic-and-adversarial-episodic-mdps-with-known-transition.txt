Abstract
√
This work studies the problem of learning episodic Markov Decision Processes with known transition and bandit feedback. We develop the ﬁrst algorithm with a
“best-of-both-worlds” guarantee: it achieves O(log T ) regret when the losses are stochastic, and simultaneously enjoys worst-case robustness with (cid:101)O(
T ) regret even when the losses are adversarial, where T is the number of episodes. More
C) regret in an intermediate setting where the losses generally, it achieves (cid:101)O( are corrupted by a total amount of C. Our algorithm is based on the Follow-the-Regularized-Leader method from Zimin and Neu [26], with a novel hybrid regularizer inspired by recent works of Zimmert et al. [27, 29] for the special case of multi-armed bandits. Crucially, our regularizer admits a non-diagonal Hessian with a highly complicated inverse. Analyzing such a regularizer and deriving a particular self-bounding regret guarantee is our key technical contribution and might be of independent interest.
√ 1

Introduction
We study the problem of learning episodic Markov Decision Processes (MDPs). In this problem, a learner interacts with the environment through T episodes. In each episode, the learner starts from a ﬁxed state, then sequentially selects one of the available actions and transits to the next state according to a ﬁxed transition function for a ﬁxed number of steps. The learner observes only the visited states and the loss for each visited state-action pair, and her goal is to minimize her regret, the difference between her total loss over T episodes and that of the optimal ﬁxed policy in hindsight.
√
√
When the losses are adversarial and can change arbitrarily between episodes, the state-of-the-art is achieved by the UOB-REPS algorithm of [12] with near-optimal regret (cid:101)O(
T ) (ignoring dependence on other parameters). On the other hand, the majority of the literature focuses on the stochastic/i.i.d. loss setting where the loss for each state-action pair follows a ﬁxed distribution. For example, Azar
T ) in this case. Moreover, the recent work of Simchowitz et al. [5] achieve the minimax regret (cid:101)O( and Jamieson [23] shows the ﬁrst non-asymptotic gap-dependent regret bound of order O(log T ) for this problem, which is considerably more favorable than the worst-case (cid:101)O(
A natural question then arises: is it possible to achieve the best of both worlds with one single algorithm? In other words, can we achieve O(log T ) regret when the losses are stochastic, and simultaneously enjoy worst-case robustness with (cid:101)O(
T ) regret when the losses are adversarial?
Considering that the existing algorithms from [12] and [23] for these two settings are drastically different, it is highly unclear whether this is possible.
T ) regret.
√
√
In this work, we answer the question afﬁrmatively and develop the ﬁrst algorithm with such a best-of-both-worlds guarantee, under the condition that the transition function is known. We emphasize that even in the case with known transition, the problem is still highly challenging. For example, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the adversarial case was studied in [26] and still requires using the Follow-the-Regularized-Leader (FTRL) or Online Mirror Descent framework from the online learning literature (see e.g., [10]) over the occupancy measure space, which the UOB-REPS algorithm [12] adopts as well. This is still signiﬁcantly different from the algorithms designed for the stochastic setting.
Moreover, our algorithm achieves the logarithmic regret O(log T ) for a much broader range of situations besides the usual stochastic setting. In fact, neither independence nor identical distributions are required, as long as a certain gap condition similar to that of [27] (for multi-armed bandits) holds (see Eq. (2)). Even more generally, our algorithm achieves (cid:101)O(log T +
C) regret in an intermediate setting where the losses are corrupted by a total amount of C. This bound smoothly interpolates between the logarithmic regret for the stochastic setting and the worst-case (cid:101)O(
T ) regret for the adversarial setting as C increases from 0 to T .
√
√
Techniques. Our algorithm is mainly inspired by recent advances in achieving best-of-both-worlds guarantees for the special case of multi-armed bandits or semi-bandits [24, 27, 29]. These works show that, perhaps surprisingly, such guarantees can be obtained with the standard FTRL framework originally designed only for the adversarial case. All we need is a carefully designed regularizer and a particular analysis that relies on proving a certain kind of self-bounding regret bounds, which then
C) regret automatically implies O(log T ) regret for the stochastic setting, and more generally (cid:101)O( for the case with C corruption.
√
We greatly extend this idea to the case of learning episodic MDPs. As mentioned, Zimin and Neu [26] already solved the adversarial case using FTRL over the occupancy measure space, in particular with
Shannon entropy as the regularizer in the form (cid:80) s,a q(s, a) ln q(s, a), where q(s, a) is the occupancy for state s and action a. Our key algorithmic contribution is to design a new regularizer based on the 1/2-Tsallis-entropy used in [27]. However, we argue that using only the Tsallis entropy, in the (cid:112)q(s, a), is not enough. Instead, inspired by the work of [29] for semi-bandits, form of − (cid:80) s,a((cid:112)q(s, a) + (cid:112)q(s) − q(s, a)) where we propose to use a hybrid regularizer in the form − (cid:80) q(s) = (cid:80) a q(s, a). In fact, to stabilize the algorithm, we also need to add yet another regularizer in the form − (cid:80) s,a log q(s, a) (known as log-barrier), borrowing the idea from [7, 8, 16]. See
Section 2.2 and Section 3 for more detailed discussions on the design of our regularizer. s,a
More importantly, we emphasize that analyzing our new regularizer requires signiﬁcantly new ideas, mainly because it admits a non-diagonal Hessian with a highly complicated inverse. Indeed, the key of the FTRL analysis lies in analyzing the quadratic norm of the loss estimator with respect to the inverse Hessian of the regularizer. As far as we know, almost all regularizers used in existing FTRL methods are decomposable over coordinates and thus admit a diagonal Hessian, making the analysis relatively straightforward (with some exceptions mentioned in related work below). Our approach is the ﬁrst to apply and analyze an explicit non-decomposable regularizer with non-diagonal Hessian.
Our analysis heavily relies on rewriting q(s) in a different way and constructing the Hessian inverse recursively (see Section 4). The way we analyze our algorithm and derive a self-bounding regret bound for MDPs is the key technical contribution of this work and might be of independent interest.
While we only resolve the problem with known transition, we believe that our approach, providing the
ﬁrst best-of-both-worlds result for MDPs, sheds light on how to solve the general case with unknown transition.