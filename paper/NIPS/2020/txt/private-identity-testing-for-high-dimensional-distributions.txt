Abstract
We present novel differentially private identity (goodness-of-ﬁt) testers for natural and widely studied classes of multivariate product distributions: product distribu-tions over {±1}d and Gaussians in Rd with known covariance. Our testers have improved sample complexity compared to those derived from previous techniques, and are the ﬁrst testers whose sample matches the order-optimal minimax sample complexity of O(d1/2/α2) in many parameter regimes. We construct two types of testers, exhibiting tradeoffs between sample complexity and computational complexity. Finally, we provide a two-way reduction between testing a subclass of multivariate product distributions and testing univariate distributions, thereby obtaining upper and lower bounds for testing this subclass of product distributions. 1

Introduction
A foundation of statistical inference is hypothesis testing: given two disjoint sets of probability distributions H0 and H1, we want to design an algorithm T that takes a random sample X from some distribution P ∈ H0 ∪ H1 and, with high probability, determines whether P is in H0 or
H1. Hypothesis tests formalize yes-or-no questions about an underlying population given a random sample from that population, and are ubiquitous in the physical, life, and social sciences, where hypothesis tests with high conﬁdence are the gold standard for publication in top journals.
In many of these applications—clinical trials, social network analysis, or demographic studies, to name a few—this sample contains sensitive data belonging to individuals, in which case it is crucial for the hypothesis test to respect these individuals’ privacy. It is particularly desirable to guarantee differential privacy [33], which has become the de facto standard for the analysis of private data. Differential privacy is used as a measure of privacy for data analysis systems at Google [35],
Apple [31], and the U.S. Census Bureau [26]. Differential privacy and related notions of algorithmic stability are also crucial for statistical validity even when conﬁdentiality is not a direct concern, as they provide generalization guarantees in an adaptive setting [32, 8, 58]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
While differentially private hypothesis testing has been extensively studied (see Section 1.3), almost all this work has focused on low-dimensional distributions. Our main contribution is to give novel algorithms for hypothesis testing of high-dimensional distributions with improved sample complexity.
In particular, we give differentially private algorithms for the following fundamental problems: 1. Given samples from a product distribution P over {±1}d, decide if P is the uniform distribution or is α-far from the uniform distribution in total variation distance. Or, equivalently, decide if
E[P ] = 0 or if (cid:107)E[P ](cid:107)2 ≥ α. 2. Given samples from a product distribution P over {0, 1}d, decide if P is equal to some given d ) or is α-far from Q in total variation extremely biased distribution Q with mean E[Q] (cid:22) O( 1 distance. In this case our tester achieves the provably optimal sample complexity. 3. Given samples from a multivariate Gaussian P in Rd whose covariance is known to be the identity
Id×d, decide if P is N (0, Id×d) or is α-far from N (0, Id×d) in total variation distance. Or, equivalently, decide if E[P ] = 0 or if (cid:107)E[P ](cid:107)2 ≥ α.
Although we will focus on the ﬁrst contribution since it highlights the main technical contributions, we note that the third contribution, (private) hypothesis testing on the mean of a Gaussian, is one of the most fundamental statistical primitives (see, e.g., the Z-test and Student’s t-test). The main challenge in solving these high-dimensional testing problems privately is that the only known non-private test statistics for these problems have high worst-case sensitivity. That is, these test statistics can potentially be highly brittle to changing even a single one of the samples. We overcome this challenge by identifying two methods for reducing the sensitivity of the test statistic without substantially changing its average-case behavior on typical datasets sampled from the distributions we consider.
The ﬁrst is based on a novel private ﬁltering method, which gives a computationally efﬁcient tester.
The second combines the method of Lipschitz extensions [13, 52] with recursive preconditioning, which yields an exponential-time tester, but with improved sample complexity. 1.1