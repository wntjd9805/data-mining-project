Abstract
Pruning the parameters of deep neural networks has generated intense interest due to potential savings in time, memory and energy both during training and at test time. Recent works have identiﬁed, through an expensive sequence of training and pruning cycles, the existence of winning lottery tickets or sparse trainable subnetworks at initialization. This raises a foundational question: can we iden-tify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? We provide an afﬁrmative answer to this question through theory driven algorithm design. We ﬁrst mathematically formulate and experimentally verify a conservation law that explains why existing gradient-based pruning algorithms at initialization suffer from layer-collapse, the premature pruning of an entire layer rendering a network untrainable. This theory also elucidates how layer-collapse can be entirely avoided, motivating a novel pruning algorithm Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total ﬂow of synaptic strengths through the network at initialization subject to a sparsity constraint. Notably, this algorithm makes no reference to the training data and consistently competes with or outperforms existing state-of-the-art pruning algorithms at initialization over a range of models (VGG and ResNet), datasets (CIFAR-10/100 and Tiny ImageNet), and sparsity constraints (up to 99.99 percent). Thus our data-agnostic pruning algorithm chal-lenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important. 1

Introduction
Network pruning, or the compression of neural networks by removing parameters, has been an important subject both for reasons of practical deployment [1, 2, 3, 4, 5, 6, 7] and for theoretical understanding of artiﬁcial [8] and biological [9] neural networks. Conventionally, pruning algorithms have focused on compressing pre-trained models [1, 2, 3, 5, 6]. However, recent works [10, 11] have identiﬁed through iterative training and pruning cycles (iterative magnitude pruning) that there exist sparse subnetworks (winning tickets) in randomly-initialized neural networks that, when trained in
⇤Equal contribution. kunin@stanford.edu.
Correspondence to hidenori.tanaka@ntt-research.com and 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
isolation, can match the test accuracy of the original network. Moreover, its been shown that some of these winning ticket subnetworks can generalize across datasets and optimizers [12]. While these results suggest training can be made more efﬁcient by identifying winning ticket subnetworks at initialization, they do not provide efﬁcient algorithms to ﬁnd them. Typically, it requires signiﬁcantly more computational costs to identify winning tickets through iterative training and pruning cycles than simply training the original network from scratch [10, 11]. Thus, the fundamental unanswered question is: can we identify highly sparse trainable subnetworks at initialization, without ever training, or indeed without ever looking at the data? Towards this goal, we start by investigating the limitations of existing pruning algorithms at initialization [13, 14], determine simple strategies for avoiding these limitations, and provide a novel data-agnostic algorithm that achieves state-of-the-art results. Our main contributions are: 1. We study layer-collapse, the premature pruning of an entire layer making a network un-trainable, and formulate the axiom Maximal Critical Compression that posits a pruning algorithm should avoid layer-collapse whenever possible (Sec. 3). 2. We demonstrate theoretically and empirically that synaptic saliency, a general class of gradient-based scores for pruning, is conserved at every hidden unit and layer of a neural network (Sec. 4). 3. We show that these conservation laws imply parameters in large layers receive lower scores than parameters in small layers, which elucidates why single-shot pruning disproportionately prunes the largest layer leading to layer-collapse (Sec. 4). 4. We hypothesize that iterative magnitude pruning [10] avoids layer-collapse because gradient descent effectively encourages the magnitude scores to observe a conservation law, which combined with iteration results in the relative scores for the largest layers increasing during pruning (Sec. 5). 5. We prove that a pruning algorithm avoids layer-collapse entirely and satisﬁes Maximal
Critical Compression if it uses iterative, positive synaptic saliency scores (Sec. 6). 6. We introduce a new data-agnostic algorithm Iterative Synaptic Flow Pruning (SynFlow) that satisﬁes Maximal Critical Compression (Sec. 6) and demonstrate empirically2 that this algorithm achieves state-of-the-art pruning performance on 12 distinct combinations of models and datasets (Sec. 7). 2