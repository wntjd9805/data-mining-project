Abstract
While various complexity measures for deep neural networks exist, specifying an appropriate measure capable of predicting and explaining generalization in deep networks has proven challenging. We propose Neural Complexity (NC), a meta-learning framework for predicting generalization. Our model learns a scalar complexity measure through interactions with many heterogeneous tasks in a data-driven way. The trained NC model can be added to the standard training loss to regularize any task learner in a standard supervised learning scenario. We contrast
NC’s approach against existing manually-designed complexity measures and other meta-learning models, and we validate NC’s performance on multiple regression and classiﬁcation tasks. 1

Introduction
Deep neural networks have achieved excellent performance on numerous tasks, including image classiﬁcation [15] and board games [27]. Although they achieve superior empirical performance, why and how these models generalize remains a mystery. Thus, understanding which properties of deep networks allow them to generalize an important problem with far-reaching potential beneﬁts such as principled model design and safety-aware models. To explain why deep networks generalize in practice, recent works have proposed novel complexity measures for deep networks [11, 12, 20, 22].
Such measures quantify the complexity of the function that a neural network represents. Ideally, such complexity measures should be good predictors of the degree of generalization of a network.
However, in practice, such manually-designed complexity measures have failed to capture essential properties of generalization in deep networks, such as improving with network size and worsening with label noise.
To overcome such limitations, we propose an alternative data-driven approach for constructing a complexity measure. Our model, Neural Complexity (NC), meta-learns a neural network that takes a predictor as input and outputs a scalar. Similarly to previous complexity-based generalization bounds, we provide a probabilistic bound of the true loss using NC. Our bound has very different characteristics from previous generalization bounds: it depends on both data distribution and architecture, and more importantly, becomes tighter as the NC model improves.
Experimentally, we show that a learned NC model consistently accelerates training in addition to preventing overﬁtting. We also show the degree to which the measure learned by NC transfers to different hypothesis classes, such as using a different network architecture, learning rate, or nonlinearity for the task learner. Compared to other recent meta-learning methods [34], the meta-learned knowledge in NC is much more stable across long learning trajectories. Finally, while most meta-learning works focus on improving performance on small tasks such as few-shot classiﬁcation, we show that NC is also capable of regularizing learning in single large tasks. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (left) The true and empirical losses are correlated but different. Neural Complexity (NC) is regularized (solid lines) by the estimates their difference (colored). (right) The training loss (cid:98)
L
N C mimics the test loss. output of the trained NC model (dotted lines). NC is meta-learned so that
L 2 Problem Setup
We adopt a meta-learning problem formulation in which a model (the "meta-learner") facilitates learning in new tasks using previous experience learning in other related tasks. Speciﬁcally, we
R. Each assume all tasks share sample space
, hypothesis space
H × Z →
Z task T consists of i.i.d. sampled ﬁnite training set S = from the underlying hidden
T and empirical loss distribution DT over the sample space
T,S for each task T are respectively deﬁned as (cid:98)
L
, and loss function
H z1, . . . , zm
{ associated with task T . The true loss
Z
L
L
}
:
T (h) def= Ez∼DT [
L
L (h, z)] and
T,S(h) def= (cid:98)
L 1 m (cid:88) z∈S
L (h, z). (1)
Tasks themselves are sampled i.i.d. from a distribution of tasks: T
τ . The objective of our meta-learner is to predict the difference between the true and empirical losses, otherwise known as the generalization gap GT,S:
∼
L
In other words, our model meta-learns a mapping
GT,S(h) =
T (h)
−
T,S(h). (cid:98)
L
R which mimics h (2)
GT,S(h) by observing (cid:55)→
H →
T (h) and (cid:98)
L
T,S(h) in many different tasks that follow T
τ .
∼
L
Even in the usual single-task supervised learning setup, we can still use this problem formulation to meta-learn by constructing a set of tasks in the following way. Given one large dataset S =
, we randomly split S into disjoint training and validation sets. For each task with this z1, . . . , zM
{ random split, the task learner uses the train set to train h, and the meta-learner evaluates
T computed with the validation set as its target. After training a meta-learner on this simulated set of tasks, we use the same model to estimate the gap GT,S of the full dataset S. This task-splitting scheme is similar to traditional cross-validation. However, instead of choosing among a few hyperparameters, we meta-learn a neural network to mimic the complex mapping h
GT,S(h).
L
} (cid:55)→ 3 Neural Complexity
We now describe Neural Complexity (NC), a meta-learning framework for predicting generalization.
At the core of NC is a neural network that directly meta-learns a complexity measure through interactions with many tasks. We show how this network integrates with any standard task learner in Figure 1. We show NC’s training loop in Figure 2 and also provide a detailed description in
Algorithms 1 and 2. 3.1 Motivation: From Gap Estimate to Generalization Bound
We motivate our meta-learning objective through a simple method of extending the identity (cid:98)
L
GT,S =
T using any estimator of the gap GT,S.
T to a probabilistic bound of
T,S +
L
L 2
Algorithm 1 Task Learning
Require: NC, Train and test datasets
Randomly initialize parameters θ of learner h loop
Sample minibatch Xtr, Xte, Ytr
T,S(h) + λ (cid:98)
L reg
− ∇
L reg
θ
·
L
←
θ
θ
← end loop
GT,S(h) return Snapshot H = (Xtr, Xte, Ytr, h(Xtr), h(Xte), GT,S(h))
T,S(h) (cid:98)
L
T (h)
← L
−
NC(Xtr, Xte, Ytr, h(Xtr), h(Xte))
NC-regularized task loss (4)
Gradient step
Compute gap (2)
Save to memory bank
Algorithm 2 Meta-Learning
Require: Memory bank
Randomly initialize parameters φ of NC while not converged do
Sample Xtr, Xte, Ytr, h(Xtr), h(Xte), GT,S(h) from memory bank
∆
φ
NC(Xtr, Xte, Ytr, h(Xtr), h(Xte))
−
NC(∆)
φ
GT,S(h)
←
φ
← end while
− ∇
L
NC’s loss function (5)
Proposition 1. Let DH be a distribution of hypotheses, and let f : the training set and hypothesis. Let D∆ denote the distribution of GT,S(h) and let ∆1, . . . , ∆n be i.i.d. copies of D∆. The following holds for all (cid:15) > 0:
× H →
−
Z m
R be any function of
DH, f (S, h) where h
∼
P (cid:104)(cid:12) (cid:12) (cid:12)L
T (h) (cid:12) (cid:12)
T,S(h) (cid:98) (cid:12) ≤
L
− f (S, h) + (cid:15) (cid:105) 1
−
≥ i
|{
∆i > (cid:15)
| n
}| (cid:115) 2
− log 2
δ 2n
. (3)
We defer the proof to the supplementary material. First, note that the role of f in this bound mirrors that of complexity measures in previous generalization bounds. Since we can compute f given S and
T,S(h) + f (S, h) differs from h, we can restate Proposition 1 as stating that the regularized loss (cid:98)
L
T by at most (cid:15) (with the given probability). Furthermore, making f more accurately predict GT,S term.
L tightens this bound by decreasing the |{i|∆i>(cid:15)}|
Taking motivation from this result, our NC meta-learns such a function f by regressing towards the gap GT,S. Rather than designing a measure of complexity that yields a tight generalization bound,
NC directly learns such a measure in a data-driven way by posing the tightening of the bound as an optimization problem. n 3.2 Training
We ﬁrst illustrate NC’s training loop. Recall from Section 3 that we consider a meta-learning setup consisting of a set of related but different tasks. Given a task T with dataset S, the task learner minimizes the following regularized loss using stochastic gradient descent: reg(h) = (cid:98)
L
L
·
T,S(h) + λ
NC(h). (4)
We set λ = 0 at initialization, and use a linear schedule where λ = 1 after a certain number of reg(h) using minibatches, just as in regular supervised episodes. We calculate the two terms in learning with neural networks. Further note that one can use NC regularization alongside any of the usual tricks for training, such as data augmentation or batch normalization.
L
T,S for any of the hypotheses
T and (cid:98)
The objective of NC is to estimate the difference between
L hT 0 , . . . , hT
τ . NC is a permutation-invariant neural network that takes features of the function h and minibatches of the data as input and outputs a scalar. We train NC using the Huber loss [9] with target GT,S:
N , for any task T
∼
L
NC(∆) =
L (cid:26) 1 2 ∆2
∆
| −
| 1 2 for ∆ 1 otherwise
≤
, (5) 3
Figure 2: NC regularizes task learning. We store snapshots of task learning in a memory bank, from which we uniformly sample batches to train NC. where ∆ = GT,S(h)
MSE loss, likely because the scale of GT,S can vary widely depending on h.
NC(h). We found that the Huber loss was more stable than the standard
− 3.3 Architecture
We now describe the architecture of NC used in our experiments. We have mentioned in Proposition 1 and Equation 4 that NC takes a representation of the function h as input. We accomplish this by passing both data x and predictions h(x) to NC. In terms of the function h, the tuple (x, h(x)) can be written as δx[(id, h)] where δx is the evaluation functional at x and id is the identity function.
This representation of h captures the behavior of h at the datapoints considered during training and evaluation. In our experiments, this structure sufﬁced for extracting relevant features of h.
Rm×D, Xte
RD).
Regression We ﬁrst describe NC when each task T is a regression task with vector data (x
∈
Rm×1 denote train data, test data, and train labels,
Let Xtr
Rm(cid:48)×1 for respectively. The learner’s hypothesis h produces outputs h(Xtr) train and test data. NC ﬁrst embeds all data with a shared encoding network fenc, which is an MLP that operates row-wise on these matrices:
Rm×1 and h(Xte)
Rm(cid:48)×D, Ytr
∈
∈
∈
∈
∈ fenc(Xtr) = etr
∈
Rm×d, fenc(Xte) = ete
Rm(cid:48)×d.
∈ (6)
These embeddings are fed into a multi-head attention layer [33] where queries, keys, and values are
Rm×(d+1)), respectively. The output of this attention layer is a
Q = ete, K = etr, V = [etr, ytr]( set of m(cid:48) items, each corresponding to a test datapoint:
∈
Finally, these embeddings are passed through a decoding MLP network and averaged: fatt(Q, K, V ) = eatt
Rm(cid:48)×d.
∈
NC(Xtr, Xte, Ytr, h(Xtr), h(Xte)) = 1 m(cid:48) m(cid:48) (cid:88) i=1 fdec(eatt)i
R.
∈ (7) (8)
Note that NC is permutation invariant because it is consists of permutation invariant components.
This property is essential since NC’s objective is also invariant with respect to permutation of the input dataset.
Classiﬁcation The architecture of NC for classiﬁcation tasks is identical to that of regression, except for the following additional interaction layer used to compute V . Representing labels as
Rm(cid:48)×c. Instead of concatenating etr one-hot vectors in a classiﬁcation task with c classes gives Ytr and Ytr as in (7), we use a bilinear layer to produce V :
∈
V = W(etr, [Ytr, 1, (Xtr)]))
Rm(cid:48)×d (W
Rd×d×(c+2)).
∈
Note that we concatenate a vector of ones and the train loss to Ytr before passing into the bilinear layer: this vector acts like a residual connection for the embedding etr, allowing its information
L
∈ (9) 4
to freely ﬂow to the next layer. See Section 5 for an ablation study on each of our architectural choices. The bilinear layer (Equation 9) generalizes the interaction layer proposed in [36]: while they explicitly choose a subnetwork to use according to class, (9) implicitly multiplies 0 to all but one of the c weights in each slice of the last dimension. Additionally, to scale NC up to high-dimensional image data such as the CIFAR dataset, we use a convolutional neural network for the encoder fenc.
Because training runs (4) are time-consuming for large networks h, we use a memory bank to store and re-use the information necessary for the meta-learning loss (5). Speciﬁcally, we store tuples (Xtr, Xte, Ytr, h(Xtr), h(Xte)) along with the observed gap (cid:98)
T (h). This memory bank
L has manageable memory cost because we can store only the indices for Xtr, Xte, and the other tensors have low dimensions. We randomly sample minibatches of such tuples to train NC with the meta-learning loss (5). Figure 2 shows how the memory bank interacts with NC.
T,S(h)
− L 3.4
Interpretations
We provide several different interpretations the NC framework.
Causality in Generalization As noted in [11], the correla-tion between a complexity measure and generalization does not directly imply a causal relationship. Since regularizing the complexity measure implicitly assumes that reducing the measure will cause the model to generalize, this lack of a causal connection can be problematic. NC’s framework provides an alternative way around this issue: because we keep using NC as a regularizer, it continually gets feedback on whether its predictions have caused the task learner to generalize.
Meta-learned Complexity Measure As mentioned in Sec-tion 1, many recent works attempt to understand generalization in deep networks by proposing novel complexity measures.
Such measures are designed to correlate well with generaliza-tion while being directly computable for any given set of param-eters. NC can be seen as a meta-learned complexity measure, and its target (2) is the generalization gap. Instead of hand-designing an appropriate complexity measure, NC meta-learns it by regressing towards observed degrees of generalization.
τ
T
L
M
S (cid:98)
L h
τ
T
G
L
M
S (cid:98)
L h
Figure 3: Graphical models corre-sponding to (left) Neural Processes and (right) NC. Observed nodes are shaded and red arrows denote amor-tized inference.
Optimal Regularizer A standard approach to generalization is to augment the empirical loss (cid:98)
L by adding a regularization term λ: regularized loss function that makes therefore, NC can be seen as a learned approximation to this optimal regularizer.
T,S
T,S(h) + λ(h). Since the purpose of λ is to make the reg = (cid:98)
L
T , we argue that the optimal regularizer for task T is the
T (h) for all h. This unique "optimal regularizer" is exactly GT,S; reg close to the true loss reg(h) =
L
L
L
L
L
Neural Processes and Sufﬁcient Statistics of True Loss We contrast the graphical models of NC with the Neural Process (NP) [7] in Figure 3. Both approaches involve a single meta-learner which observes multiple tasks to achieve low test loss. The two approaches infer different sufﬁcient statistics
T . NP infers the data distribution of T and NC infers the gap G. While both G and for the true loss
R, the data distribution are sufﬁcient for reconstructing whereas T is a complex distribution over
T , G has much lower dimension: G(h)
L
= RdX +dY .
L
∈
X × Y
Actor-Critic Generalizing to unseen test data can be seen as a reinforcement learning environment with known dynamics: the observations are train data and train loss, and the selection of hypothesis h is the action. The objective is to maximize the return, which is
T . Within this interpretation, our approach is an actor-critic method where NC takes the value network’s role.
−L 5
Method
Full
- Huber loss
- Bias
- Loss conditioning
- Bilinear layer
Accuracy 92.93 89.74 84.30 72.09 23.69
Figure 4: NC predictions and true gap values. 4