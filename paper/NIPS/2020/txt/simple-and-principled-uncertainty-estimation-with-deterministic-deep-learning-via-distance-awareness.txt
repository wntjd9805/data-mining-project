Abstract
Bayesian neural networks and deep ensembles are principled approaches to esti-mate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantiﬁcation as a minimax learning problem, we ﬁrst identify distance awareness, i.e., the model’s ability to properly quantify the dis-tance of a testing example from the training data manifold, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation.
We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a
Gaussian Process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.3 1

Introduction
Efﬁcient methods that reliably quantify a deep neural network (DNN)’s predictive uncertainty are important for industrial-scale, real-world applications, which include examples such as object recognition in autonomous driving [22], ad click prediction in online advertising [76], and intent understanding in a conversational system [84]. For example, for a natural language understanding (NLU) model built for a domain-speciﬁc chatbot service (e.g, weather inquiry), the user’s input utterance to the model can be of any topic, and the model needs to understand reliably and in real-time whether to abstain or to trigger one of its known APIs.
When deep classiﬁers make predictions on input examples that are far from the support of the training set, their performance can be arbitrarily bad [4, 14]. This motivates the need for methods that are aware of the distance between an input test example and previously seen training examples, so they can return a uniform (i.e., maximum entropy) distribution over output labels if the input is too far from the training set (i.e., the input is out-of-domain) [30]. Gaussian processes (GPs) with suitable kernels enjoy such a property. However, to apply Gaussian processes to a high-dimensional machine
∗Work done at Google Research.
†Work done as an Google AI Resident. 3Code available at https://github.com/google/uncertainty-baselines/tree/master/baselines. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
learning problem, it is usually necessary to perform some form of feature extraction or dimensionality reduction using a DNN. Ideally, the hidden representation of a DNN should reﬂect a meaningful distance in the data manifold (e.g., the semantic textual similarity between two sentences), such that this “distance aware” property is preserved. However, as we will show in the experiments, this is often not guaranteed for common deep learning models (cf. Figure 1). (a) Gaussian Process (b) Deep Ensemble (c) MC Dropout (d) DNN-GP (e) SNGP (Ours) (i) DNN-GP (h) MC Dropout (j) SNGP (Ours) (f) Gaussian Process (g) Deep Ensemble
Figure 1: The uncertainty surface of a GP and different DNN approaches on the two ovals (Top Row) and two moons (Bottom Row) 2D classiﬁcation benchmarks. SNGP is the only DNN-based approach achieving a distance-aware uncertainty similar to the gold-standard GP. Training data for positive (Orange) and negative classes (Blue). OOD data (Red) not observed during training.