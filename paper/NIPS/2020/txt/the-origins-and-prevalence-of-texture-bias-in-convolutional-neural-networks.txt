Abstract
Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We ﬁnd that, when trained on datasets of images with conﬂicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on
ImageNet? Different unsupervised training objectives and different architectures have small but signiﬁcant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texture-based classiﬁcation decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see. 1

Introduction
Convolutional neural networks (CNNs) deﬁne state-of the-art-performance in many computer vision tasks, such as image classiﬁcation (57), object detection (83; 40), and segmentation (40). Although their performance in several of these tasks approaches that of humans, recent ﬁndings show that
CNNs differ in intriguing ways from human vision, indicating fundamental deﬁciencies in our understanding of these models (87; 70; 32; 23; 76; 37; 3; 44; 46; 1; 48). This paper focuses on one such result, namely that CNNs appear to make classiﬁcations based on superﬁcial textural features (36; 4) rather than on the shape information preferentially used by humans (61; 60). Building on a long tradition of work in psychology and neuroscience documenting humans’ shape-based object classiﬁcation, Geirhos et al. (36) compared humans to ImageNet-trained CNNs on a dataset of images with conﬂicting shape and texture information (e.g. an elephant-textured knife) and found that models tended to classify according to texture (e.g. “elephant”), and humans according to shape (e.g.
“knife”). Following their terminology, we call a learner that prefers texture over shape texture-biased and one the prefers shape over texture shape-biased.
From the point of view of computer vision, texture bias is an important phenomenon for several reasons. First, it may be related to the vulnerability of CNNs to adversarial examples (87), which may exploit features that carry information about the class label but are undetectable to the human visual system (48). Second, a CNN preference for texture could indicate an inductive bias different than that of humans, making it difﬁcult for models to learn human-relevant vision tasks in small-data regimes, and to generalize to different distributions than the distribution on which the model is trained (35). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In addition to these engineering considerations, texture bias raises important scientiﬁc questions.
ImageNet-trained CNNs have emerged as the model of choice in neuroscience for modeling elec-trophysiological and neuroimaging data from primate visual cortex (101; 52; 15; 71; 7). Evidence that CNNs are preferentially driven by texture indicates a signiﬁcant divergence from primate visual processing, in which shape bias is well documented (61; 60; 38). This mismatch raises an important puzzle for human-machine comparison studies. As we show in Section 7, even models speciﬁcally designed to match neural data exhibit a strong texture bias.
This paper explores the origins of texture bias in ImageNet-trained CNNs, looking at the effects of data augmentation, training procedure, model architecture, and task. In Section 4, we show that
CNNs learn to classify the shape of ambiguous images at least as easily as they learn to classify the texture. So what makes ImageNet-trained CNNs classify images by texture when humans do not?
While we ﬁnd effects of all of the factors investigated, the most important factor is the data itself.
Our contributions are as follows:
• We ﬁnd that naturalistic data augmentation involving color distortion, noise, and blur substantially decreases texture bias, whereas random-crop augmentation increases texture bias. Combining these observations, we train models that classify ambiguous images by shape a majority of the time, without using the non-naturalistic style transfer augmentation of Geirhos et al. (36). These models also outperform baselines on out-of-distribution test sets that exemplify different notions of shape (ImageNet-Sketch (96) and Stylized ImageNet (36)).
• We investigate the texture bias of networks trained with different self-supervised learning objectives.
While some objectives decrease texture bias relative to a supervised baseline, others increase it.
• We show that architectures that perform better on ImageNet generally exhibit lower texture bias, but neither architectures designed to match the human visual system nor models that replace convolution with self-attention have texture biases substantially different from ordinary CNNs.
• We separate the extent to which shape information is represented in an ImageNet-trained model from how much it contributes to the model’s classiﬁcation decisions. We ﬁnd that it is possible to extract more shape information from a CNN’s later layers than is reﬂected in the model’s classiﬁcations, and study how this information loss occurs as data ﬂows through a network. 2