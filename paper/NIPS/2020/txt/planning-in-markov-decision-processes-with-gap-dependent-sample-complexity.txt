Abstract
We propose MDP-GapE, a new trajectory-based Monte-Carlo Tree Search algo-rithm for planning in a Markov Decision Process in which transitions have a ﬁnite support. We prove an upper bound on the number of calls to the generative model needed for MDP-GapE to identify a near-optimal action with high probability.
This problem-dependent sample complexity result is expressed in terms of the sub-optimality gaps of the state-action pairs that are visited during exploration.
Our experiments reveal that MDP-GapE is also effective in practice, in contrast with other algorithms with sample complexity guarantees in the ﬁxed-conﬁdence setting, that are mostly theoretical. 1

Introduction
,
=
A
In reinforcement learning (RL), an agent repeatedly takes actions and observes rewards in an unknown environment described by a state. Formally, the environment is a Markov Decision Process (MDP) the action space, p =
, p, r 1 a set of transition
, where (cid:105)
M (cid:104)S 1 a set of reward functions. By taking action a in state s at step h, the agent kernels and r = rh
}h
{
≥ s, a) and receives a random reward with mean rh(s, a). A reaches a state s(cid:48) with probability ph(s(cid:48)
| common goal is to learn a policy π = (πh)h 1 that maximizes cumulative reward by taking action
πh(s) in state s at step h. If the agent has access to a generative model, it may plan before acting by generating additional samples in order to improve its estimate of the best action to take next. is the state space, ph
{
}h
A
S
≥
≥
In this work, we consider Monte-Carlo planning as the task of recommending a good action to be taken by the agent in a given state s1, by using samples gathered from a generative model.
Let Q(cid:63)(s1, a) be the maximum cumulative reward, in expectation, that can be obtained from state s1 by ﬁrst taking action a, and let ˆan be the recommended action after n calls to the generative model. The quality of the action recommendation is measured by its simple regret, deﬁned as
¯rn(ˆan) := V (cid:63)(s1)
We propose an algorithm in the ﬁxed conﬁdence setting (ε, δ): after n calls to the generative model, the algorithm should return an action ˆan such that ¯rn(ˆan)
δ. We prove that its sample complexity n is bounded in high probability by a quantity that depends on
Q(cid:63)(s, ˆan), where V (cid:63)(s1) := maxa Q(cid:63)(s1, a).
ε with probability at least 1
−
≤
− 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Different settings of planning algorithms in the literature
Setting (1) Fixed conﬁdence (action-based) (2) Fixed conﬁdence (value-based) (3) Fixed budget (4) Anytime
Input
ε, δ
ε, δ n (budget)
-Output Optimality criterion (cid:98)an (cid:98)V (s1) (cid:98)an (cid:98)an
P (¯rn((cid:98)an) ≤ ε) ≥ 1 − δ (cid:16)
P
| (cid:98)V (s1) − V (cid:63)(s1)| ≤ ε
E [¯rn((cid:98)an)] decreasing in n
E [¯rn((cid:98)an)] decreasing in n (cid:17)
≥ 1 − δ
Table 2: Algorithms with sample complexity guarantees
Algorithm
Setting Sample complexity (cid:16) 2+ log(BK)
Sparse Sampling [19] (1)-(2) H 5(BK)H /ε2 or ε− log(1/γ)
OLOP [2] log κ log(1/γ) (3) (cid:17)
OP [3]
BRUE [8]
StOP [28]
TrailBlazer [13]
SmoothCruiser [14]
MDP-GapE (ours) (4) (4) (1) (2) (2) (1) (cid:16) 2, log(1/γ) (cid:16)
ε− max
ε− log κ
H 4(BK)H /∆2 (cid:17) 2+ log κ log(1/γ) +o(1)
ε− (cid:16) 2, log(Bκ)
ε− max
ε−4 (cid:80) (cid:17) log(1/γ) +o(1)
H2(BK)H−1B (∆1(s1,a1)∨∆∨ε)2 a1∈A
Remarks (cid:17) proved in Lemma 1 open loop, κ ∈ [1, K] known MDP, κ ∈ [1, BK] minimal gap ∆
κ ∈ [1, BK]
κ ∈ [1, K] only regularized MDPs see Corollary 1 the sub-optimality gaps of the actions that are applicable in state s1. We also provide experiments showing its effectiveness. The only assumption that we make on the MDP is that the support of the transition probabilities ph( s, a) should have cardinality bounded by B <
, for all s, a and h.
·|
∞
Monte-Carlo Tree Search (MCTS) is a form of Monte-Carlo planning that uses a forward model to sample transitions from the current state, as opposed to a full generative model that can sample anywhere. Most MCTS algorithms sample trajectories from the current state [1], and are widely used in deterministic games such as Go. The AlphaZero algorithm [26] guides planning using value and policy estimates to generate trajectories that improve these estimates. The MuZero algorithm [25] combines MCTS with a model-based method which has proven useful for stochastic environments.
Hence efﬁcient Monte-Carlo planning may be instrumental for learning better policies. Despite their empirical success, little is known about the sample complexity of state-of-the-art MCTS algorithms.