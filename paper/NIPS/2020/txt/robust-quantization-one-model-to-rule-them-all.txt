Abstract
Neural network quantization methods often involve simulating the quantization process during training, making the trained model highly dependent on the target bit-width and precise way quantization is performed. Robust quantization offers an alternative approach with improved tolerance to different classes of data-types and quantization policies. It opens up new exciting applications where the quan-tization process is not static and can vary to meet different circumstances and implementations. To address this issue, we propose a method that provides intrinsic robustness to the model against a broad range of quantization processes. Our method is motivated by theoretical arguments and enables us to store a single generic model capable of operating at various bit-widths and quantization policies.
We validate our method’s effectiveness on different ImageNet models. A reference implementation accompanies the paper. 1

Introduction
Low-precision arithmetic is one of the key techniques for reducing deep neural networks compu-tational costs and ﬁtting larger networks into smaller devices. This technique reduces memory, bandwidth, power consumption and also allows us to perform more operations per second, which leads to accelerated training and inference.
Naively quantizing a ﬂoating point (FP32) model to 4 bits (INT4), or lower, usually incurs a signiﬁcant accuracy degradation. Studies have tried to mitigate this by offering different quantization methods.
These methods differ in whether they require training or not. Methods that require training (known as quantization aware training or QAT) simulate the quantization arithmetic on the ﬂy [Esser et al., 2019,
Zhang et al., 2018, Zhou et al., 2016], while methods that avoid training (known as post-training quantization or PTQ) quantize the model after the training while minimizing the quantization noise
[Banner et al., 2019, Choukroun et al., 2019, Finkelstein et al., 2019, Zhao et al., 2019].
But these methods are not without disadvantages. Both create models sensitive to the precise way quantization is done (e.g., target bit-width). Krishnamoorthi [2018] has observed that in order to avoid accuracy degradation at inference time, it is essential to ensure that all quantization-related artifacts are faithfully modeled at training time. Our experiments in this paper further assess this observation. For example, when quantizing ResNet-18 [He et al., 2015] with DoReFa [Zhou et al., 2016] to 4 bits, an error of less than 2% in the quantizer step size results in an accuracy drop of 58%.
There are many compelling practical applications where quantization-robust models are essential.
For example, we can consider the task of running a neural network on a mobile device with limited resources. In this case, we have a delicate trade-off between accuracy and current battery life, which can be controlled through quantization (lower bit-width => lower memory requirements => less 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
energy). Depending on the battery and state of charge, a single model capable of operating at various quantization levels would be highly desirable. Unfortunately, current methods quantize the models to a single speciﬁc bit-width, experiencing dramatic degradations at all other operating points.
Recent estimates suggest that over 100 companies are now producing optimized inference chips
[Reddi et al., 2019], each with its own rigid quantizer implementation. Different quantizer imple-mentations can differ in many ways, including the rounding policy (e.g., round-to-nearest, stochastic rounding, etc), truncation policy, the quantization step size adjusted to accommodate the tensor range, etc. To allow rapid and easy deployment of DNNs on embedded low-precision accelerators, a single pre-trained generic model that can be deployed on a wide range of deep learning accelerators would be very appealing. Such a robust and generic model would allow DNN practitioners to provide a single off-the-shelf robust model suitable for every accelerator, regardless of the supported mix of data types, precise quantization process, and without the need to re-train the model on customer side.
In this paper, we suggest a generic method to produce robust quantization models. To that end, we introduce KURE — a KUrtosis REgularization term, which is added to the model loss function. By imposing speciﬁc kurtosis values, KURE is capable of manipulating the model tensor distributions to adopt superior quantization noise tolerance qualities. The resulting model shows strong robustness to variations in quantization parameters and, therefore, can be used in diverse settings and various operating modes (e.g., different bit-width).
This paper makes the following contributions: (i) we ﬁrst prove that compared to the typical case of normally-distributed weights, uniformly distributed weight tensors have improved tolerance to quantization with a higher signal-to-noise ratio (SNR) and lower sensitivity to speciﬁc quantizer implementation; (ii) we introduce KURE — a method designed to uniformize the distribution of weights and improve their quantization robustness. We show that weight uniformization has no effect on convergence and does not hurt state-of-the-art accuracy before quantization is applied; (iii) We apply KURE to several ImageNet models and demonstrate that the generated models can be quantized robustly in both PTQ and QAT regimes. 2