Abstract
Modiﬁcations to a neural network’s input and output layers are often required to accommodate the speciﬁcities of most practical learning tasks. However, the impact of such changes on architecture’s approximation capabilities is largely not understood. We present general conditions describing feature and readout maps that preserve an architecture’s ability to approximate any continuous functions uniformly on compacts. As an application, we show that if an architecture is capable of universal approximation, then modifying its ﬁnal layer to produce binary values creates a new architecture capable of deterministically approximating any classiﬁer.
In particular, we obtain guarantees for deep CNNs and deep feed-forward networks.
Our results also have consequences within the scope of geometric deep learning.
Speciﬁcally, when the input and output spaces are Cartan-Hadamard manifolds, we obtain geometrically meaningful feature and readout maps satisfying our criteria.
Consequently, commonly used non-Euclidean regression models between spaces of symmetric positive deﬁnite matrices are extended to universal DNNs. The same result allows us to show that the hyperbolic feed-forward networks, used for hierarchical learning, are universal. Our result is also used to show that the common practice of randomizing all but the last two layers of a DNN produces a universal family of functions with probability one. We also provide conditions on a DNN’s ﬁrst (resp. last) few layer’s connections and activation function which guarantee that these layer’s can have a width equal to the input (resp. output) space’s dimension while not negatively effecting the architecture’s approximation capabilities. 1

Introduction
Modiﬁcations made to a neural network’s input and output maps to extract features from a data-set or to better suit a learning task is prevalent throughout learning theory. Typically, such changes are made by pre-(resp. post-)composing an architecture with a ﬁxed and untrainable feature (resp. readout) map.
Examples prevail classiﬁcation by neural networks, random feature maps obtained by randomizing all but the last few layers of a feed-forward network, and numerous illustrations throughout geometric deep-learning theory, which we detail below. This motivates the central question of this paper: "Which modiﬁcations to the input and output layers of a neural network architecture preserve its universal approximation capabilities?"
Speciﬁcally, in this paper we obtain a simple sufﬁcient condition on a pair of a feature map φ : X →
Rm and a readout map ρ : Rn → Y , where X and Y are topological spaces, guaranteeing that if F is dense in C(Rm, Rn) for the uniform convergence on compacts (ucc) topology then
{ f ∈ C(X , Y ) : ρ ◦ f ◦ φ , f ∈ F }, (1)
∗Department of Mathematics, Eidgenössische Technische Hochschule Zürich, HG G 32.3, Rämistrasse 101, 8092 ürich, Switzerland. email: anastasis.kratsios@math.ethz.ch
†Department of Mathematics and Statistical Sciences, University of Alberta, 11324 89 Ave NW, Edmonton,
AB T6G 2J5, Canada. email: bilokopy@ualberta.ca 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is dense in C(X , Y ) in the uniform convergence on compacts topology when Y is metric and, more generally, in the compact-open topology when Y is non-metrizable (such as in the hard classiﬁcation problem). Simpliﬁed conditions are obtained when Y is a metrizable manifold, and characterization of ρ and φ is obtained when both X and Y are smooth manifolds.
The set F represents any expressive neural network architecture. For example, by [35] F can be taken to be the set of feed-forward networks with one hidden layer and continuous, locally-bounded, and non-polynomial activation function. Or, by [56], F can be taken to be the set of deep convolution networks with speciﬁc sparsity structures and ReLu activation function. Throughout F is often referred to as an architecture. The results are not limited to neural networks and remain valid when, for example, F is taken to be the set of posterior means generated by a Gaussian processes universal kernel, as in [41]. The central results are motivated by the following consequences.
Implication: Method for Constructing Non-Euclidean Universal Approximators
A natural hub for our results is in geometric deep learning, an emerging ﬁeld of machine learning, which acknowledges and makes use of the latent non-Euclidean structures present in many types of data. Applications of geometric deep learning are prevalent throughout neuroimaging [16], computer-vision [48], covariance learning [40], and learning from hierarchical structures such as complex social networks [34], undirected graphs [43], and trees [50].
For instance, in [44], it is shown that low-dimensional representations for complex hierarchical structures into hyperbolic space outperform the current state-of-the-art high-dimensional Euclidean embedding methods due to the tree-like geometry of the former. Using the theory of gyro-vector space, introduced in [55], [17] proposed a hyperbolic-space variant of the feed-forward architecture and demonstrated its superior performance in learning hierarchical structure from these hyperbolic representations. A direct application of our main result conﬁrms that this non-Euclidean architecture can indeed approximate any continuous function between hyperbolic spaces.
More generally, we obtain an explicit construction of feed-forward networks between any Cartan-Hadamard manifold and a guarantee that our construction is universal. Cartan-Hadamard manifolds appear throughout applied mathematics from the symmetric positive-deﬁnite matrix-valued regression problems of [16, 40], which we extend to universal approximators, to applications in mathematical
ﬁnance in [23], Gaussian processes in [39], information geometric in [4], and to the geometry of the
Wasserstein space [36] commonly used in Generative Adversarial Networks as in [3].
Implication: Universal Approximation Implies Universal Classiﬁcation
Perhaps the most commonly used readout maps are those used when modifying neural-networks to perform classiﬁcation tasks. The currently available theoretical results, found in [15], guarantee that for a random vector in Rm with random labels, the set of feed-forward networks with one hidden layer, step activation function σ (x) = I[0,∞) − I(−∞,0], and readout map ρ(x)i = I[ 1 2 ,∞) can approximate the Bayes’ classiﬁer in probability.
As an application of this paper’s main results, we obtain deterministic guarantees of generic hard (n-ary) and soft (fuzzy) classiﬁcation on Rm for any given universal approximator in C(Rm, Rn) once it’s outputs are modiﬁed by a continuous surjection ρ to take values in {0, 1}n or (0, 1)n, respectively.
For example, our result applies to feed-forward networks with at-least one hidden layer holds when ρ the component-wise logistic ρ(x)i = I[ 1
Implication: DNNs with Randomly Generated First Layers are Universal 1+exi readout map. 2 ,1] ◦ exi
We show that the commonly employed practice of only training the ﬁnal layers of a deep feed-forward network and randomizing the rest preserves its universal approximation capabilities with probability 1. Though widely used, this practice has only recently begun to be studied in [19, 37]. The link with our results arises from an observation made in [37], stating that the ﬁrst portion of such a random architecture can be seen as a randomly generated feature map.
Implication: DNNs Can be Narrowed
In [29, 45] the authors provide lower bounds on a DNN layer’s width, under which it is no longer a universal approximator. However, there is a wealth of literature which shows that arranging a network’s neurons to create depth rather than width yields empirically superior performance. As a
ﬁnal application of our theory, we provide explicit conditions on a DNN’s connections and activation functions so additional initial and ﬁnal few layers may be added to a DNN which do not respect the minimum width requirements of [29, 45] but do not negatively impact the DNN’s approximation 2
capabilities. Numerical implementations show that additional depth build using our main results improve predictive performance additional deep layers failing our assumptions reduced the network’s predictive performance.
This paper is organized as follows. Section 2 discusses the necessary topological and geometric background needed to formulate the paper’s central results. Section 3 contains the paper’s main results discussed above. The conclusion follows in section 4. The proofs of the main results are contained within this paper’s supplementary material. 2