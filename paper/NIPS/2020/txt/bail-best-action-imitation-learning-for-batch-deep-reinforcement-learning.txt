Abstract
There has recently been a surge in research in batch Deep Reinforcement Learning (DRL), which aims for learning a high-performing policy from a given dataset without additional interactions with the environment. We propose a new algorithm,
Best-Action Imitation Learning (BAIL), which strives for both simplicity and performance. BAIL learns a V function, uses the V function to select actions it believes to be high-performing, and then uses those actions to train a policy network using imitation learning. For the MuJoCo benchmark, we provide a comprehensive experimental study of BAIL, comparing its performance to four other batch Q-learning and imitation-learning schemes for a large variety of batch datasets. Our experiments show that BAIL’s performance is much higher than the other schemes, and is also computationally much faster than the batch Q-learning schemes. 1

Introduction
The ﬁeld of Deep Reinforcement Learning (DRL) has recently seen a surge in research in batch reinforcement learning, which is the problem of sample-efﬁcient learning from a given dataset without additional interactions with the environment. Batch RL enables reusing the data collected by a policy to possibly improve the policy without further interactions with the environment, it has the potential to leverage existing large datasets to obtain much better sample efﬁciency. A batch RL algorithm can also be deployed as part of a growing-batch algorithm, where the batch algorithm seeks a high-performing exploitation policy using the data in an experience replay buffer [18], combines this policy with exploration to add fresh data to the buffer, and then repeats the whole process [15, 3].
Batch RL may also be necessary for learning a policy in safety-critical systems where a partially trained policy cannot be deployed online to collect data.
Fujimoto et al. [8] made the critical observation that when conventional Q-function based algorithms, such as Deep Deterministic Policy Gradient (DDPG), are directly applied to batch reinforcement learning, they learn very poorly, or even entirely diverge due to extrapolation error. Therefore, in order to obtain high-performing policies from batch data, new algorithms are required. Recent batch
DRL algorithms roughly fall into two categories: Q-function-based algorithms such as BCQ [8] and
BEAR [14]; and Imitation Learning (IL)-based algorithms such as MARWIL [27] and AWR [20].
We propose a new algorithm, Best-Action Imitation Learning (BAIL), which strives for both simplicity and performance. BAIL is an advanced IL method and its value estimates are updated only with data in the batch, giving stable estimates. BAIL not only provides state-of-the-art performance, it is also
∗Correspondence to: Keith Ross <keithwross@nyu.edu>. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
computationally fast. Moreover, it is conceptually and algorithmically simple, thereby satisfying the principle of Occam’s razor.
BAIL has three steps. In the ﬁrst step, BAIL learns a V function by training a neural network to obtain the “upper envelope of the data”. In the second step, it selects from the dataset the state action-pairs whose Monte Carlo returns are close to the upper envelope. In the last step, it simply trains a policy network with vanilla imitation learning using the selected actions. The method thus combines a novel approach for V-learning with IL.
Because the BCQ and BEAR codes are publicly available, we are able to make a careful and com-prehensive comparison of the performance of BAIL, BCQ, BEAR, MARWIL and vanilla Behavior
Cloning (BC) using the Mujoco benchmark. For our experiments, we create training batches in a manner identical to what was done in the BCQ paper (using DDPG [17] to create the batches), and add additional training batches for the environments Ant and Humanoid using SAC [10], giving a total of 22 training batches with non-expert data. Our experimental results show that BAIL wins for 20 of the 22 batches, with overall performance 42% or more higher than the other algorithms. Moreover,
BAIL is computationally 30-50 times faster than the Q-learning algorithms. BAIL therefore achieves state-of-the-art performance while being signiﬁcantly simpler and faster than BCQ and BEAR.
In summary, the contributions of this paper are as follows: (i) BAIL, a new high-performing batch
DRL algorithm, along with the novel concept of “the upper envelope of data”; (ii) extensive, carefully-designed experiments comparing ﬁve batch DRL algorithms over diverse datasets. The computational results give signiﬁcant insight into how different types of batch DRL algorithms perform for different types of data sets. We provide public open source code for reproducibility 2. We will also make our datasets publicly available for future benchmarking. 2