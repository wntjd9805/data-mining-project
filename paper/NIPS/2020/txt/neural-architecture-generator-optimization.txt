Abstract
Neural Architecture Search (NAS) was ﬁrst proposed to achieve state-of-the-art performance through the discovery of new architecture patterns, without human intervention. An over-reliance on expert knowledge in the search space design has however led to increased performance (local optima) without signiﬁcant architec-tural breakthroughs, thus preventing truly novel solutions from being reached. In this work we 1) are the ﬁrst to investigate casting NAS as a problem of ﬁnding the optimal network generator and 2) we propose a new, hierarchical and graph-based search space capable of representing an extremely large variety of network types, yet only requiring few continuous hyper-parameters. This greatly reduces the dimensionality of the problem, enabling the effective use of Bayesian Optimisation as a search strategy. At the same time, we expand the range of valid architectures, motivating a multi-objective learning approach. We demonstrate the effectiveness of this strategy on six benchmark datasets and show that our search space generates extremely lightweight yet highly competitive models. The code is available at https://github.com/rubinxin/vega_NAGO. 1

Introduction
Neural Architecture Search (NAS) has the potential to discover paradigm-changing architectures with state-of-the-art performance, and at the same time removes the need for a human expert in the network design process. While signiﬁcant improvements have been recently achieved [1, 2, 3, 4, 5, 6], this has taught us little about why a speciﬁc architecture is more suited for a given dataset. Similarly, no conceptually new architecture structure has emerged from NAS works. We attribute this to two main issues: (i) reliance on over-engineered search spaces and (ii) the inherent difﬁculty in analyzing complex architectures.
The ﬁrst point is investigated in [7]. In order to reduce search time, current NAS methods often restrict the macro-structure and search only the micro-structure at the cell level, focusing on which operations to choose but ﬁxing the global wiring pattern [1, 8, 9, 10]. This leads to high accuracy but restricts the search to local minima: indeed deep learning success stories, such as ResNet [11],
DenseNet [12] and Inception [13] all rely on speciﬁc global wiring rather than speciﬁc operations.
The second issue appears hard to solve, as analyzing the structure of complex networks is itself a demanding task for which few tools are available. We suggest that by moving the focus towards network generators we can obtain a much more informative solution, as the whole network can then be represented by a small set of parameters. This idea, ﬁrst introduced by [14], offers many 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Architecture sampled from HNAG, given hyperparameters Θ. Each node, both in the top-level and mid-level graphs, is an independently sampled graph. Finally, at the bottom level each node corresponds to an independently sampled atomic operation. Note how features at the top level can ﬂow between different stages (e.g. from node 1 and 4 to 7), which is beneﬁcial for certain tasks. advantages for NAS: the smaller number of parameters is easier to optimize and easier to interpret when compared to the popular categorical, high-dimensional search spaces. Furthermore it allows the algorithm to focus on macro differences (e.g. global connectivity) rather than the micro differences arising from minor variations with little impact on the ﬁnal accuracy.
To summarize, our main contributions are as follows. 1) A Network Architecture Generator Optimization framework (NAGO), which redirects the focus of NAS from optimizing a single architecture to optimizing an architecture generator. To the best of our knowledge, we are the ﬁrst to investigate this direction and we demonstrate the usefulness of this by using Bayesian Optimization (BO) in both multi-ﬁdelity and multi-objective settings. 2) A new hierarchical, graph-based search space, together with a stochastic network genera-tor which can output an extremely wide range of previously unseen networks in terms of wiring complexity, memory usage and training time. 3) Extensive empirical evaluation showing that NAGO achieves state-of-the-art NAS results on a variety of vision tasks, and ﬁnds lightweight yet competitive architectures. 2 Neural Architecture Generator
Previous research has shown that small perturbations in the network’s structure do not signiﬁcantly change its performance, i.e. the speciﬁc connection between any single pair of nodes is less important than the overall connectivity [14, 7]. As such, we hypothesize, and experimentally conﬁrm in
Section 4.1, that architectures sampled from the same generative distribution perform similarly. This assumption allows us to greatly simplify the search and explore more conﬁgurations in the search space by only evaluating those sampled from different generator hyperparameters. Therefore, instead of optimizing a speciﬁc architecture, we focus on ﬁnding the optimal hyperparameters for a stochastic network generator [14]. 2.1 Hierarchical Graph-based Search Space (HNAG)
Our network search space is modelled as a hierarchical graph with three levels (Figure 1). At the top-level, we have a graph of cells. Each cell is itself represented by a mid-level graph. Similarly, each node in a cell is a graph of basic operations (conv3×3, conv5×5, etc.). This results in 3 sets of graph hyperparameters: θtop, θmid, θbottom, each of which independently deﬁnes the graph generation model in each level. Following [14] we use the Watts-Strogatz (WS) model as the random graph generator for the top and bottom levels, with hyperparameters θtop = [Nt, Kt, Pt] and
θbottom = [Nb, Kb, Pb]; and use the Erd˝os–Rényi (ER) graph generator for the middle level, with hyperparameters θmid = [Nm, Pm] to allow for the single-node case 1. This gives us the ﬂexibility to reduce our search space to two levels (when the mid-layer becomes single node) and represent a 1The WS model cannot generate a single-node graph but the ER model can. 2
DARTS-like architecture. Indeed HNAG is designed to be able to emulate existing search spaces while also exploring potentially better/broader ones.
By varying the graph generator hyperparameters and thus the connectivity properties at each level, we can produce a extremely diverse range of architectures (see end of this section). For instance, if the top-level graph has 20 nodes arranged in a feed-forward conﬁguration and the mid-level graph has a single node, then we obtain networks similar to those sampled from the DARTS search space
[1]. While if we ﬁx the top-level graph to 3 nodes, the middle level to 1 and the bottom-level graph to 32, we can reproduce the search space from [14].
Stages. CNNs are traditionally divided into stages, each having a different image resolution and number of channels [15, 1, 14]. In previous works, both the length of each stage and the number of channels were ﬁxed. Our search space is the ﬁrst that permits the learning of the optimal channel ratio as well as the channel multiplier for each stage. To do so, we deﬁne two hyperparameter vectors: stage ratio θS and channel ratio θC. θS is normalized and represents the relative length of each stage.
For example, if there are 20 nodes at the top level and θS = [0.2, 0.2, 0.6] then the three stages will have 4, 4 and 12 nodes, respectively. θC controls the number of channels in each stage; e.g. if it is set to [4, 1, 4] than stages 1 and 3 hold the same number of channels while stage 2 only holds one fourth of that. The absolute number of channels depends on the overall desired number of parameters while θC only controls the relative ratio.
Merging options and Operations. When multiple edges enter the same node, they are merged.
Firstly, all activations are downscaled via pooling to match the resolution of the smallest tensor. Note we only tried pooling for our work but strided convolution is an alternative option to achieve the same effect. Likewise, we use 1 × 1 convolutions to ensure that all inputs share the same number of channels. Then, independently for each node, we sample, according to the probability weights
θM , one merging strategy from: weighted sum, attention weighted sum, concatenation. Each atomic operation is sampled from a categorical distribution parameterized with θop, which can be task speciﬁc. is our fully space search speciﬁed hyperparameters Θ =
Therefore,
[θtop, θmid, θbottom, θS, θC, θM , θop]. The top-level enables a mixture of short- and long-range connections among features of different stages (resolutions/channels). The mid-level regulates the search complexity of the bottom-level graph by connecting features computed locally (within each mid-level node).2 This serves a function similar to the use of cells in other NAS method but relaxes the requirement of equal cells. Our hierarchical search expresses a wide variety of networks (see Section 4.1). The total number of networks in our search space is larger than 4.58 × 1056. For reference, in the DARTS search space that number is 814 ≈ 4.40 × 1012 (details in Appendix A). the by 2.2 BO-based Search Strategy
Our proposed hierarchical graph-based search space allows us to represent a wide variety of neural architectures with a small number of continuous hyperparameters, making NAS amenable to a wide range of powerful BO methods such as multi-ﬁdelity and multi-objective BO. The general algorithm for applying BO to our search space is presented in Appendix B.
Multi-ﬁdelity BO (BOHB). We use the multi-ﬁdelity BOHB approach [16], which uses partial evalu-ations with smaller-than-full budget in order to exclude bad conﬁgurations early in the search process, thus saving resources to evaluate more promising conﬁgurations and speeding up optimisation. Given the same time constraint, BOHB evaluates many more conﬁgurations than conventional BO which evaluates all conﬁgurations with full budget.
Multi-objective BO (MOBO). We use MOBO to optimize for multiple objectives which are conﬂict-ing in nature. For example, we may want to ﬁnd architectures which give high accuracy but require low memory. Given the competing nature of the multiple objectives, we adapt a state-of-the-art
MOBO method to learn the Pareto front [17] 3. The method constructs multiple acquisition functions, one for each objective function, and then recommends the next query point by sampling the point with 2For example, a 32-nodes graph has 496 possible connections. If we divide this into 4 subgraphs of 8 nodes, that number is 118 = 28 × 4 (within subgraphs) + 6 (between subgraphs). 3Note modifying BOHB to also accommodate the multi-objective setting is an interesting future direction.
One potential way is to do so is by selecting the Pareto set points at each budget to be evaluated for longer epochs during Successive Halving. 3
the highest uncertainty on the Pareto front of all the acquisition functions. We modify the approach in the following two aspects for our application: 1) Heteroscedastic surrogate model. We use a stochastic gradient Hamiltonian Monte Carlo (SGHMC)
BNN [18] as the surrogate, which does a more Bayesian treatment of the weights and thus gives better-calibrated uncertainty estimates than other alternatives in prior BO-NAS works [19, 20, 21]. SGHMC
BNN in [18] assumes homoscedastic aleatoric noise with zero mean and constant variance w2 n. By sampling the network weights wf and the noise parameter wn from their posterior wi ∼ p(w|D) where w = [wf , wn] and D is the surrogate training data, the predictive posterior mean µ(f |Θ, D) and variance σ2(f |Θ, D) are approximated as:
µ(f |Θ, D) = 1
N
N (cid:88) i=1
ˆf (Θ; wi f ),
σ2(f |Θ, D) = 1
N
N (cid:88) i=1
ˆf (Θ; wi f )2 − µ(f |D)2 + w2 n (1) i=1 (cid:80)N f )2 + (cid:0)wi (cid:16) ˆf (Θ; wi n(Θ)(cid:1)2(cid:17)
However, our optimization problem has heteroscedastic aleatoric noise: the variance in the net-work performance, in terms of test accuracy or other objectives, changes with the generator hy-perparameters (Figure 2). Therefore, we propose to append a second output to the surrogate network and model the noise variance as a deterministic function of the inputs, w2 n(Θ). Our heteroscedastic BNN has the same predictive posterior mean as Equation (2.2) but a slightly different predictive posterior variance: σ2(f |Θ, D) = 1
− µ(f |Θ, D)2.
N
Our resultant surrogate network comprises 3 fully-connected layers, each with 10 neurons, and two outputs. The hyperparameter details for our BNN surrogate is described in Appendix C.
Table 1: Regression performance of heteroscedas-tic (Het) and homoscedastic (Hom) BNN surro-gates, trained on 50 generator samples and tested on 100 samples, in terms of negative log-likelood (NLL) and root mean square error (RMSE)
We verify the modelling performance of our het-eroscedastic surrogate network by comparing it to its homoscedastic counterpart. We randomly sample 150 points from BOHB query data for each of the ﬁve image datasets and randomly split them into a train-test ratio of 1:2. The me-dian results on negative log likelihood (NLL) and root mean square error (RMSE) over 10 random splits are shown in Table 1. The het-eroscedastic model not only improves over the homoscedastic model on RMSE, which depends on the predictive posterior mean only, but more importantly, shows much lower NLL, which de-pends on both the predictive posterior mean and variance. This shows that the heteroscedastic surrogate can model the variance of the objective function better, which is important for the BO exploration.
Hom Het 3.43 5.92 0.89 7.15 19.0 23.8
-0.92 7.23 7.49 15.6
Hom Het 0.01 0.02 0.02 0.02 0.14 0.15 0.11 0.12 0.18 0.19
CIFAR10
CIFAR100
SPORT8
MIT67
FLOWERS102
RMSE
NLL 2) Parallel evaluations per BO iteration. The original multi-objective BO algorithm is sequential (i.e. recommends one new conﬁguration per iteration). We modify the method to a batch algorithm which recommends multiple new conﬁgurations per iteration and enjoys faster convergence in terms of BO iterations [22, 23]. This allows the use of parallel computation to evaluate batches of conﬁgurations simultaneously. We collect the batch by applying the local penalisation on the uncertainty metric of the original multi-objective BO [23]. See Appendix D for details. 3