Abstract
The goal of compressed sensing is to estimate a high dimensional vector from an underdetermined system of noisy linear equations. In analogy to classical compressed sensing, here we assume a generative model as a prior, that is, we assume the vector is represented by a deep generative model G : Rk → Rn.
Classical recovery approaches such as empirical risk minimization (ERM) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel
MOM-based algorithm enjoys the same sample complexity guarantees as ERM under sub-Gaussian assumptions. Our experiments validate both aspects of our claims: other algorithms are indeed fragile and fail under heavy-tailed and/or corrupted data, while our approach exhibits the predicted robustness. 1

Introduction
Compressive or compressed sensing is the problem of reconstructing an unknown vector x∗ ∈ Rn after observing m < n linear measurements of its entries, possibly with added noise: y = Ax∗ + η, where A ∈ Rm×n is called the measurement matrix and η ∈ Rm is noise. Even without noise, this is an underdetermined system of linear equations, so recovery is impossible without a structural assumption on the unknown vector x∗. The vast literature [84, 37, 72, 9, 18, 27, 2, 86, 11] on this subject typically assumes that the unknown vector is “natural,” or “simple,” in some application-dependent way.
Compressed sensing has been studied on a wide variety of structures such as sparse vectors [19], trees [20], graphs [90], manifolds [21, 89] or deep generative models [15].
In this paper, we concentrate on deep generative models, which were explored by [15] as priors for sample-efﬁcient reconstruction. Theoretical results in [15] showed that if x∗ lies close to the range of a generative model G : Rk → Rn with d−layers, a variant of ERM can recover x∗ with m = O(kd log n) measurements. Empirically, [15] shows that generative models require 5 − 10× fewer measurements to obtain the same reconstruction accuracy as Lasso. This impressive empirical performance has motivated signiﬁcant recent research to better understand the behaviour and theoretical limits of compressed sensing using generative priors [36, 50, 62]
A key technical condition for recovery is the Set Restricted Eigenvalue Condition (S-REC) [15], which is a generalization of the Restricted Eigenvalue Condition [14, 17] in sparse recovery. This
∗Link to our code: https://github.com/ajiljalal/csgm-robust-neurips 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
condition is satisﬁed if A is a sub-Gaussian matrix and the measurements satisfy y = Ax∗ + η. This leads to the question: can the conditions on A be weakened, and can we allow for outliers in y and
A? This has signiﬁcance in applications such as MRI and astronomical imaging, where data is often very noisy and requires signiﬁcant pruning/cleansing.
As we show in this paper, the analysis and algorithm proposed by [15] are quite fragile in the presence of heavy-tailed noise or corruptions in the measurements. In the statistics literature, it is well known that algorithms such as empirical risk minimization (ERM) and its variants are not robust to even a single outlier. Since the algorithm in [15] is a variant of ERM, it is susceptible to the same failures in the presence of heavy-tails and outliers. Indeed, as we show empirically in Section 6, precisely this occurs.
Importantly, recovery failure in the setting of [15] (which is also the focus of this paper) can be pernicious, precisely because generative models (by design) output images in their range space, and for well-designed models, these have high perceptual quality. In contrast, when a classical algorithm like LASSO [84] fails, the typical failure mode is the output of a non-sparse vector. Thus in the context of generative models, resilience to outliers and heavy-tails is especially critical. This motivates the need for algorithms that do not require strong assumptions on the measurements.
In this paper, we propose an algorithm for compressed sensing using generative models, which is robust to heavy-tailed distributions and arbitrary outliers. We study its theoretical recovery guarantees as well as empirical performance, and show that it succeeds in scenarios where other existing recovery procedures fail, without additional cost in sample complexity or computation. 1.1 Contributions
We propose a new reconstruction algorithm in place of ERM. Our algorithm uses a Median-of-Means (MOM) loss to provide robustness to heavy-tails and arbitrary corruptions. As S-REC may no longer hold, we necessarily use a different analytical approach. We prove recovery results and sample complexity guarantees for this setting even though previous assumptions such as the S-REC [15] condition do not hold. Speciﬁcally, our main contributions are as follows.
• (Algorithm) We consider robust compressed sensing for generative models where (i) a constant frac-tion of the measurements and measurement matrix are arbitrarily (perhaps maliciously) corrupted and (ii) the random ensemble only satisﬁes a weak moment assumption.
We propose a novel algorithm to replace ERM. Our algorithm uses a median-of-means (MOM) tournament [65, 54] i.e., a min-max optimization framework for robust reconstruction. Each iteration of our MOM-based algorithm comes at essentially no additional computational cost compared to an iteration of standard ERM. Moreover, as our code shows, it is straightforward to implement.
• (Analysis and Guarantees) We analyze the recovery guarantee and outlier-robustness of our algo-rithm when the generative model is a d-layer neural network using ReLU activations. Speciﬁcally, in the presence of a constant fraction of outliers in y and A, we achieve (cid:107)G((cid:98)z) − G(z∗)(cid:107)2 ≤
O(σ2 + τ ) with sample size m = O(kd log n), where σ2 is the variance of the heavy-tailed noise, and τ is the optimization accuracy. Using different analytical tools (necessarily, since we do not assume sub-Gaussianity), we show our algorithm, even under heavy-tails and corruptions, has the same sample complexity as the previous literature has achieved under much stronger sub-Gaussian assumptions. En route to our result, we also prove an interesting result for ERM: by avoiding the S-REC-based analysis, we show that the standard ERM algorithm does in fact succeed in the presence of a heavy-tailed measurement matrix, thereby strengthening the best-known recovery guarantees from [15]. This does not extend (as our empirical results demonstrate) to the setting of outliers, or of heavy-tailed measurement noise. For these settings, our new algorithm is required.
• (Empirical Support) We empirically validate the effectiveness of our robust recovery algorithm on MNIST and CelebA-HQ. Our results demonstrate that (as our theory predicts) our algorithm succeeds in the presence of heavy-tailed noise, heavy-tailed measurements, and also in the presence of arbitrary outliers. At the same time our experiments conﬁrm that ERM can fail, and in fact fails dramatically: through an experiment on the CelebA-HQ data set, we demonstrate that the ERM recovery approach [15], as well as other natural approaches including (cid:96)1 loss minimization and trimmed loss minimization [81], can recover images that have little resemblance to the original. 2
1.2