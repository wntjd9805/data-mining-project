Abstract
We propose an adaptively weighted stochastic gradient Langevin dynamics algo-rithm (SGLD), so-called contour stochastic gradient Langevin dynamics (CSGLD), for Bayesian learning in big data statistics. The proposed algorithm is essentially a scalable dynamic importance sampler, which automatically ﬂattens the target distribution such that the simulation for a multi-modal distribution can be greatly fa-cilitated. Theoretically, we prove a stability condition and establish the asymptotic convergence of the self-adapting parameter to a unique ﬁxed-point, regardless of the non-convexity of the original energy function; we also present an error analysis for the weighted averaging estimators. Empirically, the CSGLD algorithm is tested on multiple benchmark datasets including CIFAR10 and CIFAR100. The numeri-cal results indicate its superiority over the existing state-of-the-art algorithms in training deep neural networks. 1

Introduction
AI safety has long been an important issue in the deep learning community. A promising solution to the problem is Markov chain Monte Carlo (MCMC), which leads to asymptotically correct uncertainty quantiﬁcation for deep neural network (DNN) models. However, traditional MCMC algorithms
[Metropolis et al., 1953, Hastings, 1970] are not scalable to big datasets that deep learning models rely on, although they have achieved signiﬁcant successes in many scientiﬁc areas such as statistical physics and bioinformatics. It was not until the study of stochastic gradient Langevin dynamics (SGLD) [Welling and Teh, 2011] that resolves the scalability issue encountered in Monte Carlo computing for big data problems. Ever since, a variety of scalable stochastic gradient Markov chain
Monte Carlo (SGMCMC) algorithms have been developed based on strategies such as Hamiltonian dynamics [Chen et al., 2014, Ma et al., 2015, Ding et al., 2014], Hessian approximation [Ahn et al., 2012, Li et al., 2016, ¸Sim¸sekli et al., 2016], and higher-order numerical schemes [Chen et al., 2015,
Li et al., 2019]. Despite their theoretical guarantees in statistical inference [Chen et al., 2015, Teh et al., 2016, Vollmer et al., 2016] and non-convex optimization [Zhang et al., 2017, Raginsky et al., 2017, Xu et al., 2018], these algorithms often converge slowly, which makes them hard to be used for efﬁcient uncertainty quantiﬁcation for many AI safety problems.
To develop more efﬁcient SGMCMC algorithms, we seek inspirations from traditional MCMC algorithms, such as simulated annealing [Kirkpatrick et al., 1983], parallel tempering [Swendsen and
Wang, 1986, Geyer, 1991], and ﬂat histogram algorithms [Berg and Neuhaus, 1991, Wang and Landau,
∗To whom correspondence should be addressed: Faming Liang. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2001]. In particular, simulated annealing proposes to decay temperatures to increase the hitting probability to the global optima [Mangoubi and Vishnoi, 2018], which, however, often gets stuck into a local optimum with a fast cooling schedule. Parallel tempering proposes to swap positions of neighboring Markov chains according to an acceptance-rejection rule. However, under the mini-batch setting, it often requires a large correction which is known to deteriorate its performance [Deng et al., 2020a]. The ﬂat histogram algorithms, such as the multicanonical [Berg and Neuhaus, 1991] and Wang-Landau [Wang and Landau, 2001] algorithms, were ﬁrst proposed to sample discrete states of Ising models by yielding a ﬂat histogram in the energy space, and then extended as a general dynamic importance sampling algorithm, the so-called stochastic approximation Monte Carlo (SAMC) algorithm [Liang, 2005, Liang et al., 2007, Liang, 2009]. Theoretical studies [Lelièvre et al., 2008, Liang, 2010, Fort et al., 2015] support the efﬁciency of the ﬂat histogram algorithms in Monte
Carlo computing for small data problems. However, it is still unclear how to adapt the ﬂat histogram idea to accelerate the convergence of SGMCMC, ensuring efﬁcient uncertainty quantiﬁcation for AI safety problems.
This paper proposes the so-called contour stochastic gradient Langevin dynamics (CSGLD) algorithm, which successfully extends the ﬂat histogram idea to SGMCMC. Like the SAMC algorithm [Liang, 2005, Liang et al., 2007, Liang, 2009], CSGLD works as a dynamic importance sampling algorithm, which adaptively adjusts the target measure at each iteration and accounts for the bias introduced thereby by importance weights. However, theoretical analysis for the two types of dynamic importance sampling algorithms can be quite different due to the fundamental difference in their transition kernels.
We proceed by justifying the stability condition for CSGLD based on the perturbation theory, and establishing ergodicity of CSGLD based on newly developed theory for the convergence of adaptive
SGLD. Empirically, we test the performance of CSGLD through a few experiments. It achieves remarkable performance on some synthetic data, UCI datasets, and computer vision datasets such as
CIFAR10 and CIFAR100. 2 Contour stochastic gradient Langevin dynamics
Suppose we are interested in sampling from a probability measure π(x) with the density given by
π(x) ∝ exp(−U (x)/τ ), x ∈ X , (1) where X denotes the sample space, U (x) is the energy function, and τ is the temperature. It is known that when U (x) is highly non-convex, SGLD can mix very slowly [Raginsky et al., 2017]. To accelerate the convergence, we exploit the ﬂat histogram idea in SGLD.
Suppose that we have partitioned the sample space X into m subregions based on the energy function
U (x): X1 = {x : U (x) ≤ u1}, X2 = {x : u1 < U (x) ≤ u2}, . . ., Xm−1 = {x : um−2 < U (x) ≤ um−1}, and Xm = {x : U (x) > um−1}, where −∞ < u1 < u2 < · · · < um−1 < ∞ are speciﬁed by the user. For convenience, we set u0 = −∞ and um = ∞. Without loss of generality, we assume ui+1 − ui = ∆u for i = 1, . . . , m − 2. We propose to simulate from a ﬂattened density (cid:36)Ψθ (x) ∝
π(x)
θ(U (x))
Ψζ
, (2) where ζ > 0 is a hyperparameter controlling the geometric property of the ﬂatted density (see Figure 1(a) for illustration), and θ = (θ(1), θ(2), . . . , θ(m)) is an unknown vector taking values in the space: (cid:40)
Θ = (θ(1), θ(2), · · · , θ(m)) (cid:12) (cid:12)0 < θ(1), θ(2), · · · , θ(m) < 1 and (cid:41)
θ(i) = 1
. (3) m (cid:88) i=1 2.1 A naïve contour SGLD
It is known if we set † (i) ζ = 1 and Ψθ(U (x)) = m (cid:88) i=1 (ii) θ(i) = θ(cid:63)(i), where θ(cid:63)(i) =
θ(i)1ui−1<U (x)≤ui, (cid:90)
χi
π(x)dx for i ∈ {1, 2, · · · , m}, (4)
†1A is an indicator function that takes value 1 if event A occurs and 0 otherwise. 2
the algorithm will act like the SAMC algorithm [Liang et al., 2007], yielding a ﬂat histogram in the space of energy (see the pink curve in Fig.1(b)). Theoretically, such a density ﬂattening strategy enables a sharper logarithmic Sobolev inequality and accelerates the convergence of simulations
[Lelièvre et al., 2008, Fort et al., 2015]. However, such a density ﬂattening setting only works under the framework of the Metropolis algorithm [Metropolis et al., 1953]. A naïve application of the step function in formula (4(i)) to SGLD results in ∂ log Ψθ(u)
∂u = 0 almost everywhere, which leads to the vanishing-gradient problem for SGLD. Calculating the gradient for the naïve contour SGLD, we have
= 1
∂Ψθ(u)
Ψθ(u)
∂u
∇x log (cid:36)Ψθ (x) = − 1 + ζτ (cid:20)
∂ log Ψθ(u)
∂u (cid:21) ∇xU (x)
τ
= −
∇xU (x)
τ
.
As such, the naïve algorithm behaves like SGLD and fails to simulate from the ﬂattened density (2). 2.2 How to resolve the vanishing gradient
To tackle this issue, we propose to set Ψθ(u) as a piecewise continuous function:
Ψθ(u) = m (cid:88) (cid:16) i=1
θ(i − 1)e(log θ(i)−log θ(i−1)) u−ui−1
∆u (cid:17) 1ui−1<u≤ui, where θ(0) is ﬁxed to θ(1) for simplicity. A direct calculation shows that
∇x log (cid:36)Ψθ (x) = − 1 + ζτ (cid:20)
∂ log Ψθ(u)
∂u (cid:21) ∇xU (x)
τ (cid:20)
= − 1 + ζτ log θ(J(x)) − log θ((J(x) − 1) ∨ 1)
∆u (cid:21) ∇xU (x)
τ
, (5) (6) where J(x) ∈ {1, 2, · · · , m} denotes the index that x belongs to, i.e., uJ(x)−1 < U (x) ≤ uJ(x). § 2.3 Estimation via stochastic approximation
Since θ(cid:63) is unknown, we propose to estimate it on the ﬂy under the framework of stochastic approximation [Robbins and Monro, 1951]. Provided that a scalable transition kernel Πθk(xk, ·) is available and the energy function U (x) on the full data can be efﬁciently evaluated, the weighted density (cid:36)Ψθ (x) can be simulated by iterating between the following steps: (i) Simulate xk+1 from Πθk(xk, ·), which admits (cid:36)θk (x) as the invariant distribution, k(J(xk+1)) (cid:0)1i=J(xk+1) − θk(i)(cid:1) for i ∈ {1, 2, · · · , m}. (ii) θk+1(i) = θk(i) + ωk+1θζ (7) where θk denotes a working estimate of θ at the k-th iteration. We expect that in a long run, such an algorithm can achieve an optimization-sampling equilibrium such that θk converges to the ﬁxed point θ(cid:63) and the random vector xk converges weakly to the distribution (cid:36)Ψθ(cid:63)
To make the algorithm scalable to big data, we propose to adopt the Langevin transition kernel for drawing samples at each iteration, for which a mini-batch of data can be used to accelerate computation. In addition, we observe that evaluating U (x) on the full data can be quite expensive for big data problems, while it is free to obtain the stochastic energy (cid:101)U (x) in evaluating the stochastic gradient ∇x (cid:101)U (x) due to the nature of auto-differentiation [Paszke et al., 2017]. For this reason, we propose a biased index ˜J(x), where u ˜J(x)−1 < N n (cid:101)U (x) ≤ u ˜J(x), N is the sample size of the full dataset and n is the mini-batch size. Let {(cid:15)k}∞ k=1 denote the learning rates and step sizes for SGLD and stochastic approximation, respectively. Given the above notations, the proposed algorithm can be presented in Algorithm 1, which can be viewed as a scalable Wang-Landau algorithm for deep learning and big data problems. k=1 and {ωk}∞ (x). 2.4