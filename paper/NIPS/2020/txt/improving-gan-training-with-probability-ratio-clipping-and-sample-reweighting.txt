Abstract
Despite success on a wide range of problems related to vision, generative adver-sarial networks (GANs) often suffer from inferior performance due to unstable training, especially for text generation. To solve this issue, we propose a new variational GAN training framework which enjoys superior training stability. Our approach is inspired by a connection of GANs and reinforcement learning under a variational perspective. The connection leads to (1) probability ratio clipping that regularizes generator training to prevent excessively large updates, and (2) a sample re-weighting mechanism that improves discriminator training by downplaying bad-quality fake samples. Moreover, our variational GAN framework can provably overcome the training issue in many GANs that an optimal discriminator cannot provide any informative gradient to training generator. By plugging the training approach in diverse state-of-the-art GAN architectures, we obtain signiﬁcantly improved performance over a range of tasks, including text generation, text style transfer, and image generation.1 1

Introduction
Generative adversarial networks (GANs) [13] have achieved remarkable success in image and video synthesis [4, 32, 39]. However, it is usually hard to train a GAN well, because the training process is commonly unstable, subject to disturbances and even collapses. To alleviate this issue, substantial efforts have been paid to improve the training stability from different perspectives, e.g., divergence minimization [36, 37], Wasserstein distance with Lipschitz continuity of the discriminator [2, 15, 52], energy-based models [3, 58], to name a few.
In spite of the above progresses, the instability in training has not been well resolved [8], since it is difﬁcult to well balance the strength of the generator and the discriminator. What is worse, such an instability issue is exacerbated in text generation due to the sequential and discrete nature of text [6, 12, 22]. Speciﬁcally, the high sensitivity of text generation to noise and the underlying errors caused by sparse discriminator signals in the generated text can often result in destructive updates to both generator and discriminator, enlarging the instability in GANs.
In this work, we develop a novel variational GAN training framework to improve the training stability, which is broadly applicable to GANs of a variety of architectures for image and text generation. This training framework is derived from a variational perspective of GANs [24] and the resulting connections to reinforcement learning (in particular, RL-as-inference) [1, 44] and other rich literature [5, 14, 23]. Our approach consists of two stabilization techniques, namely, probability ratio clipping and sample re-weighting, for stabilizing the generator and discriminator respectively. (1) Under the variational perspective, the generator update is subject to a KL penalty on the change 1Code available at: github.com/Holmeswww/PPOGAN 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of the proposed approach for stabilizing GAN training. Results are from the
CIFAR-10 experiment in Sec.4.1. Left: The conventional and surrogate objectives for generator training, as we interpolate between the initial generator parameters θold and the updated generator parameters θnew which we compute after one iteration of training. The θnew obtains maximal surrogate objective. The surrogate objective curve starts decreasing after x = 1, showing the objective imposes a penalty for having too large of a generator update. In contrast, the conventional objective (for WGAN-GP) keeps increasing with larger generator updates. Middle and right: Discriminator and generator losses w/ and w/o sample re-weighting. WGAN-GP with our re-weighting plugged in shows lower variance in both discriminator and generator losses throughout training. of the generator distribution. This KL penalty closely resembles that in the popular Trust-Region
Policy Optimization (TRPO) [43] and its variant, i.e., Proximal Policy Optimization (PPO) [44]. This connection motivates a simple surrogate objective with a clipped probability ratio between the new generator and the old one. The probability ratio clipping discourages excessively large generator updates, and has shown to be effective in the context of stabilizing policy optimization [44]. Figure 1 (left) shows the intuition about the surrogate objective, where we can observe the objective value decreases with an overly large generator change and thus imposes regularization on the updates. (2) When updating the discriminator, the new perspective induces an importance sampling mechanism, which effectively re-weights fake samples by their discriminator scores. Since low-quality samples tend to receive smaller weights, the discriminator trained on the re-weighted samples is more likely to maintain stable performance, and in turn provide informative gradients for subsequent generator updates. Figure 1 (middle/right) demonstrates the effect of the re-weighting in reducing the variance of both discriminator and generator losses.
Besides, our variational GAN training framework can provably overcome the training issue [59] that an optimal discriminator cannot provide any informative gradient to training generator. This issue usually occurs in GAN training [59], since the discriminator often converges much faster than the generator. Empirically, we conduct extensive experiments on a wide range of tasks, including text generation, text style transfer, and image generation. Our approach shows signiﬁcant improvement over state-of-the-art methods, demonstrating its broad applicability and efﬁcacy. 2