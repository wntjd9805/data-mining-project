Abstract
Fair representation learning provides an effective way of enforcing fairness con-straints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the ﬁrst method that enables data consumers to obtain certiﬁcates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness.
That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at (cid:96)∞-distance at most (cid:15), thus allowing data consumers to certify individual fairness by proving (cid:15)-robustness of their classiﬁer. Our experimental evaluation on ﬁve real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach. 1

Introduction
The increased use of machine learning in sensitive domains (e.g., crime risk assessment [1], ad targeting [2], and credit scoring [3]) has raised concerns that methods learning from data can reinforce human bias, discriminate, and lack fairness [4–6]. Moreover, data owners often face the challenge that their data will be used in (unknown) downstream applications, potentially indifferent to fairness concerns [7]. To address this challenge, the paradigm of learning fair representations has emerged as a promising approach to obtain data representations that preserve fairness while maintaining utility for a variety of downstream tasks [8, 9]. The recent work of McNamara et al. [10] has formalized this setting by partitioning the landscape into: a data regulator who deﬁnes fairness for the particular task at hand, a data producer who processes sensitive user data and transforms it into another representation, and a data consumer who performs predictions based on the new representation.
In this setting, a machine learning model M : Rn → Ro is composed of two parts: an encoder fθ : Rn → Rk, provided by the data producer, and a classiﬁer hψ : Rk → Ro, provided by the data consumer, with Rk denoting the latent space. The data regulator selects a deﬁnition of fairness that the model M should satisfy. Most work so far has explored two main families of fairness deﬁnitions [11]: statistical and individual. Statistical notions deﬁne speciﬁc groups in the population and require that particular statistics, computed based on model decisions, should be equal for all groups. Popular notions of this kind include demographic parity [12] and equalized odds [13]. While these notions do not require any assumptions on the data and are easy to certify, they offer no guarantees for individuals or other subgroups in the population [14]. In contrast, individual notions of fairness [12] are desirable as they explicitly require that similar individuals in the population are treated similarly.
Key challenge A central challenge then is to enforce individual fairness in the setting described above. That is, to both learn an individually fair representation and to certify that individual fairness 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is actually satisﬁed across the end-to-end model M without compromising the independence of the data producer and the data consumer.
This work In this work, we propose the ﬁrst method for addressing the above challenge. At a high level, our approach is based on the observation that recent advances in training machine learning models with logical constraints [15] together with new methods for proving that constraints are satisﬁed [16] open the possibility for learning certiﬁed individually fair models.
Concretely, we identify a practical class of individual fairness deﬁnitions captured via declarative fairness constraints. Such a fairness constraint is a binary similarity function φ : Rn × Rn →
{0, 1}, where φ(x, x(cid:48)) evaluates to 1 if and only if two individuals x and x(cid:48) are similar (e.g., if all their attributes except for race are the same). By working with declarative constraints, data regulators can now express interpretable, domain-speciﬁc notions of similarity, a problem known to be challenging [8, 17–21].
Given the fairness constraint φ, we can now train an individually fair representation and use it to obtain a certiﬁcate of individual fairness for the end-to-end model. For training, the data producer can employ our framework to learn an encoder fθ with the goal that two individuals satisfying φ should be mapped close together in (cid:96)∞-distance in latent space. As a consequence, individual fairness can then be certiﬁed for a data point in two steps: ﬁrst, the data producer computes a convex relaxation of the latent set of similar individuals and passes it to the data consumer. Second, the data consumer certiﬁes individual fairness by proving local robustness within the convex relaxation. Importantly, the data consumer can now perform modular certiﬁcation: it does not need to know the fairness constraint φ and the concrete data point x.
Our experimental evaluation on several datasets and fairness constraints shows a substantial in-crease (up to 72.6%) of certiﬁed individuals (unseen during training) when compared to standard representation learning.
Main contributions Our key contributions are:
• A practical family of similarity notions for individual fairness deﬁned via interpretable logical constraints.
• A method to learn individually fair representations (deﬁned in an expressive logical frag-ment), which comes with provable certiﬁcates.
• An end-to-end implementation of our method in an open-source tool called LCIFR, together with an extensive evaluation on several datasets, constraints, and architectures. We make
LCIFR publicly available at https://github.com/eth-sri/lcifr. 2 Overview
This section provides a high-level overview of our approach, with the overall ﬂow shown in Figure 1.
As introduced earlier, our setting consists of three parties. The ﬁrst party is a data regulator who deﬁnes similarity measures for the input and the output denoted as φ and µ, respectively. The properties φ and µ are problem-speciﬁc and can be expressed in a rich logical fragment which we describe later in Section 4. For example, for classiﬁcation tasks µ could denote equal classiﬁcation (i.e., µ(M (x), M (x(cid:48))) = 1 ⇐⇒ M (x) = M (x(cid:48))) or classifying M (x) and M (x(cid:48)) to the same label group; for regressions tasks µ could evaluate to 1 if (cid:107)M (x) − M (x(cid:48))(cid:107) ≤ 0.1 and 0 otherwise.
We focus on equal classiﬁcation in the classiﬁcation setting for the remainder of this work.
The goal of treating similar individuals as similarly as possible can then be formulated as ﬁnding a classiﬁer M which maximizes
Ex∼D [∀x(cid:48) ∈ Rn : φ(x, x(cid:48)) =⇒ µ(M (x), M (x(cid:48)))] , (1) where D is the underlying data distribution (we assume a logical expression evaluates to 1 if it is true and to 0 otherwise). As usual in machine learning, we approximate this quantity with the empirical risk, by computing the percentage of individuals x from the test set for which we can certify that
∀x(cid:48) ∈ Sφ(x) : µ(M (x), M (x(cid:48))), (2) 2
Data Producer
Data Consumer x2 x
Sφ (x)
U
L e
R x1
U
L e
R fθ z2
C
F z z2 z, (cid:15) z
U
L e
R d i o m g i
S hψ (z) z1 z1 fθ (Sφ (x))
⊆ B∞ (z, (cid:15))
B∞ (z, (cid:15)) hψ hψ (B∞ (z, (cid:15)))
Figure 1: Conceptual overview of our framework. The left side shows the component corresponding to the data producer who learns an encoder fθ which maps the entire set of individuals Sφ(x) that are similar to individual x, according to the similarity notion φ, to points near fθ(x) in the latent space.
The data producer then computes an (cid:96)∞-bounding box B∞ around the latent set of similar individuals fθ(Sφ(x)) with center z = fθ(x) and radius (cid:15) and passes it to the data consumer. The data consumer receives the latent representation z and radius (cid:15), trains a classiﬁer hψ, and certiﬁes that the entire (cid:96)∞-ball centered around z with radius (cid:15) is classiﬁed the same (green color shows fair output region). where Sφ(x) = {x(cid:48) ∈ Rn | φ(x, x(cid:48))} denotes the set of all points similar to x. Note that Sφ(x) generally contains an inﬁnite number of individuals. In Figure 1, Sφ(x) is represented as a brown shape, and x is shown as a single point inside of Sφ(x).
The key idea of our approach is to train the encoder fθ to map point x and all points x(cid:48) ∈ Sφ(x) close to one another in the latent space with respect to (cid:96)∞-distance, speciﬁed as
φ (x, x(cid:48)) =⇒ ||fθ(x(cid:48)) − fθ(x)||∞ ≤ δ, (3) where δ is a tunable parameter of the method, determined in agreement between producer and consumer (we could also use another (cid:96)p-norm). If the encoder indeed satisﬁes Equation (3), the data consumer, potentially indifferent to the fairness constraint, can then train a classiﬁer hψ independently of the similarity notion φ. Namely, the data consumer only has to train hψ to be robust to perturbations up to δ in (cid:96)∞-norm, which can be solved via standard min-max optimization, discussed in Section 4.
We now explain our end-to-end inference with provable certiﬁcates for encoder fθ and classiﬁer hψ.
Processing the producer model Given a data point x, we ﬁrst propagate both x and its set of similar points Sφ(x) through the encoder, as shown in Figure 1, to obtain the latent representations z = fθ(x) and fθ(Sφ(x)). As Equation (3) may not hold for the particular x and δ due to the stochastic nature of training, we compute the smallest (cid:96)∞-bounding box of radius (cid:15) such that fθ(Sφ(x)) ⊆ B∞(z, (cid:15)) := {z(cid:48) | (cid:107)z − z(cid:48)(cid:107)∞ ≤ (cid:15)}. This (cid:96)∞-bounding box with center z and radius (cid:15) is shown as orange in Figure 1.
Processing the consumer model Next, we provide the latent representation z and the radius (cid:15) to the data consumer. The data consumer then knows that all points similar to x are in the (cid:96)∞-ball of radius (cid:15), but does not need to know the similarity constraint φ nor the particular shape fθ(Sφ(x)).
The key observation is the following: if the data consumer can prove its classiﬁer hψ is robust to (cid:96)∞-perturbations up to (cid:15) around z, then the end-to-end classiﬁer M = hψ ◦ fθ satisﬁes individual fairness at x with respect to the similarity rule φ imposed by the data regulator.
There are two central technical challenges we need to address. The ﬁrst challenge is how to train an encoder to satisfy Equation (3), while not making any domain-speciﬁc assumptions about the point x or the similarity constraint φ. The second challenge is how to provide a certiﬁcate of individual fairness for x, which requires both computing the smallest radius (cid:15) such that fθ(Sφ(x)) ⊆ B∞(z, (cid:15)), as well as certifying (cid:96)∞-robustness of the classiﬁer hψ.
To train an encoder, we build on Fischer et al. [15], who provide a translation from logical constraints
φ to a differentiable loss function. The training of the encoder network can then be formulated as a min-max optimization problem, which alternates between (i) searching for counterexamples x(cid:48) ∈ Sφ(x) that violate Equation (3), and (ii) training fθ on the counterexamples. We employ gradient descent to minimize a joint objective composed of a classiﬁcation loss and the constraint loss obtained from translating Equation (3). Once no more counterexamples are found, we can conclude the encoder empirically satisﬁes Equation (3). We discuss the detailed procedure in Section 4. 3
We compute a certiﬁcate for individual fairness in two steps. First, to provide guarantees on the latent representation generated by encoder fθ, we solve the optimization problem (cid:15) = max x(cid:48)∈Sφ(x)
||z − fθ(x(cid:48))||∞.
Recall that the set Sφ(x) generally contains an inﬁnite number of individuals x(cid:48), and thus this optimization problem cannot be solved by simple enumeration. In Section 5 we show how this optimization problem can be encoded as a mixed-integer linear program (MILP) and solved using off-the-shelf MILP solvers. After obtaining (cid:15), we certify local robustness of the classiﬁer hψ around z = fθ(x) by proving (using MILP) that for each z(cid:48) where ||z(cid:48) − z|| ≤ (cid:15), the classiﬁcation results of hψ(z(cid:48)) and hψ(z) coincide. Altogether, this implies the overall model M = hψ ◦ fθ satisﬁes individual fairness for x. Finally, note that since the bounding box B (z, (cid:15)) is a convex relaxation of the latent set of similar individuals fθ(Sφ(x)), the number of individuals for which we can obtain a certiﬁcate is generally lower than the number of individuals that actually satisfy Equation (2). 3