Abstract
Multiplication (e.g., convolution) is arguably a cornerstone of modern deep neural networks (DNNs). However, intensive multiplications cause expensive resource costs that challenge DNNs’ deployment on resource-constrained edge devices, driv-ing several attempts for multiplication-less deep networks. This paper presented
ShiftAddNet, whose main inspiration is drawn from a common practice in energy-efﬁcient hardware implementation, that is, multiplication can be instead performed with additions and logical bit-shifts. We leverage this idea to explicitly parameterize deep networks in this way, yielding a new type of deep network that involves only bit-shift and additive weight layers. This hardware-inspired ShiftAddNet immedi-ately leads to both energy-efﬁcient inference and training, without compromising the expressive capacity compared to standard DNNs. The two complementary operation types (bit-shift and add) additionally enable ﬁner-grained control of the model’s learning capacity, leading to more ﬂexible trade-off between accuracy and (training) efﬁciency, as well as improved robustness to quantization and prun-ing. We conduct extensive experiments and ablation studies, all backed up by our
FPGA-based ShiftAddNet implementation and energy measurements. Compared to existing DNNs or other multiplication-less models, ShiftAddNet aggressively re-duces over 80% hardware-quantiﬁed energy cost of DNNs training and inference, while offering comparable or better accuracies. Codes and pre-trained models are available at https://github.com/RICE-EIC/ShiftAddNet. 1

Introduction
Powerful deep neural networks (DNNs) come at the price of prohibitive resource costs during both
DNN inference and training, limiting the application feasibility and scope of DNNs in resource-constrained device for more pervasive intelligence. DNNs are largely composed of multiplication operations for both forward and backward propagation, which are much more computationally costly than addition [1]. The above roadblock has driven several attempts to design new types of hardware-friendly deep networks, which rely less on heavy multiplications in order for higher energy efﬁciency. ShiftNet [2, 3] adopted spatial shift operations paired with pointwise convolutions, to replace a large portion of convolutions. DeepShift [4] employed an alternative of bit-wise shifts, which are equivalent to multiplying the input with powers of 2. Lately, AdderNet [5] pioneered to demonstrate the feasibility and promise of replacing all convolutions with merely addition operations.
This paper takes one step further along this direction of multiplication-less deep networks, by drawing a very fundamental idea in the hardware-design practice, computer processors, and even digital signal processing. It has been known for long that multiplications can be performed with additions and logical bit-shifts [6, 7], whose hardware implementation are very simple and much faster [8], without compromising the result quality or precision. Also on currently available processors, a bit-shift instruction is faster than a multiply instruction and can be leveraged to multiply (shift left) and divide 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(shift right) by powers of two. Multiplication (or division) by a constant is then implemented using a sequence of shifts and adds (or subtracts). The above clever “shortcut” saves arithmetic operations, and can readily be applied to accelerating the hardware implementation of any machine learning algorithm involving multiplication (either scalar, vector, or matrix). But our curiosity is well beyond this: can we learn from this hardware-level “shortcut" to design efﬁcient learning algorithms?
The above uniquely motivates our work: in order to be more “hardware-friendly”, we strive to re-design our model to be “hardware-inspired”, leveraging the successful experience directly form the efﬁcient hardware design community. Speciﬁcally, we explicit re-parameterize our deep networks by replacing all convolutional and fully-connected layers (both built on multiplications) with two multiplication-free layers: bit-shift and add. Our new type of deep model, named ShiftAddNet, immediately lead to both energy-efﬁcient inference and training algorithms.
We note that ShiftAddNet seamlessly integrates bit-shift and addition together, with strong motivations that address several pitfalls in prior arts [2, 3, 4, 5]. Compared to utilizing spatial- or bit-shifts alone
[2, 4], ShiftAddNet can be fully expressive as standard DNNs, while [2, 4] only approximate the original expressive capacity since shift operations cannot span the entire continuous space of multiplicative mappings (e.g., bit-shifts can only represent the subset of power-of-2 multiplications).
Compared to the fully additive model [5], we note that while repeated additions can in principle replace any multiplicative mapping, they do so in a very inefﬁcient way. In contrast, by also exploiting bit-shifts, ShiftAddNet is expected to be more parameter-efﬁcient than [5] which relies on adding templates. As a bonus, we notice the bit-shift and add operations naturally correspond to coarse- and
ﬁne-grained input manipulations. We can exploit this property to more ﬂexibly trade-offs between training efﬁciency and achievable accuracy, e.g., by freezing all bit-shifts and only training add layers.
ShiftAddNet with ﬁxed shift layers can achieve up to 90% and 82.8% energy savings than fully additive models [5] and shift models [4] under ﬂoating-point or ﬁxed-point precisions trainings, while leading to a comparable or better accuracies (-3.7% ∼ +31.2% and 3.5% ∼ 23.6%), respectively. Our contributions can be summarized as follow:
• Uniquely motivated by the hardware design expertise, we combine two multiplication-less and complementary operations (bit-shift and add) to develop a hardware-inspired network called
ShiftAddNet that is fully expressive and ultra-efﬁcient.
• We develop training and inference algorithms for ShiftAddNet. Leveraging the two operations’ distinct granularity levels, we also investigate ShiftAddNet trade-offs between training efﬁciency and achievable accuracy, e.g., by freezing all the bit-shift layers.
• We conduct extensive experiments to compare ShiftAddNet with existing DNNs or multiplication-less models. Results on multiple benchmarks demonstrate its superior compactness, accuracy, training efﬁciency, and robustness. Speciﬁcally, we implement ShiftAddNet on a ZYNQ-7 ZC706
FPGA board [9] and collect all real energy measurements for benchmarking. 2