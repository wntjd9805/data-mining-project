Abstract
Second-order information, in the form of Hessian- or Inverse-Hessian-vector prod-ucts, is a fundamental tool for solving optimization problems. Recently, there has been signiﬁcant interest in utilizing this information in the context of deep neural networks; however, relatively little is known about the quality of existing approximations in this context. Our work examines this question, identiﬁes issues with existing approaches, and proposes a method called WoodFisher to compute a faithful and efﬁcient estimate of the inverse Hessian.
Our main application is to neural network compression, where we build on the classic Optimal Brain Damage/Surgeon framework. We demonstrate that
WoodFisher signiﬁcantly outperforms popular state-of-the-art methods for one-shot pruning. Further, even when iterative, gradual pruning is allowed, our method results in a gain in test accuracy over the state-of-the-art approaches, for standard image classiﬁcation datasets such as ImageNet ILSVRC. We examine how our method can be extended to take into account ﬁrst-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime. The code is available at the following link, https://github.com/IST-DASLab/WoodFisher. 1

Introduction
The recent success of deep learning, e.g. [1, 2] has brought about signiﬁcant accuracy improvement in areas such as computer vision [3, 4] or natural language processing [5, 6]. Central to this performance progression has been the size of the underlying models, with millions or even billions of trainable parameters [4, 5], a trend which seems likely to continue for the foreseeable future [7].
Deploying such large models is taxing from the performance perspective. This has fuelled a line of work where researchers compress such parameter-heavy deep neural networks into “lighter,” easier to deploy variants. This challenge is not new, and in fact, results in this direction can be found in the early work on neural networks, e.g. [8–10]. Thus, most of the recent work to tackle this challenge can
ﬁnd its roots in these classic references [11], and in particular in the Optimal Brain Damage/Surgeon (OBD/OBS) framework [8, 10]. Roughly, the main idea behind this framework is to build a local quadratic model approximation based on the second-order Taylor series expansion to determine the optimal set of parameters to be removed. (We describe it precisely in Section 4.)
A key requirement to apply this approach is to have an accurate estimate of the inverse Hessian matrix, or at least to accurate inverse-Hessian-vector-products (IHVPs). In fact, IHVPs are a central ingredient in many parts of machine learning, most prominently for optimization [12–15], but also in other applications such as inﬂuence functions [16] or continual learning [17]. Applying second-order methods at the scale of model sizes described above might appear daunting, and so is often done via
∗Work done while at IST Austria. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
coarse-grained approximations (such as diagonal, block-wise, or Kronecker-factorization). However, relatively little is understood about the quality and scalability of such approximations.
Motivation. Our work centers around two main questions. The ﬁrst is analytical, and asks if second-order approximations can be both accurate and scalable in the context of neural network models. The second is practical, and concerns applications of second-order approximations to neural network compression. In particular, we investigate whether these methods can be competitive with both industrial-scale methods such as magnitude-based pruning [18], as well as with the series of non-trivial compression methods proposed by researchers over the past couple of years [19–24].
Contribution. We ﬁrst examine second-order approximation schemes in the context of convolu-tional neural networks (CNNs). In particular, we identify a method of approximating Hessian-Inverse information leveraging the structure of the empirical Fisher information matrix to approximate the Hessian, in conjunction with the Woodbury matrix identity to provide iteratively improving approximations of Inverse-Hessian-vector products. We show that this method, which we simply call
WoodFisher, can be computationally-efﬁcient, and that it faithfully represents the structure of the
Hessian even for relatively low sample sizes. We note that early variants of this method have been considered previously [10, 25], but we believe we are the ﬁrst to consider its accuracy, efﬁciency, and implementability in the context of large-scale deep models, as well as to investigate its extensions.
To address the second, practical, question, we demonstrate in Section 4 how WoodFisher can be used in conjunction with variants of the OBD/OBS pruning framework, resulting in state-of-the-art compression of popular convolutional models such as ResNet50 and MobileNet on the ILSVRC (ImageNet) dataset [26]. We investigate two practical application scenarios.
The ﬁrst is one-shot pruning, in which the model has to be compressed in a single step, without any re-training. Here, WoodFisher easily outperforms all previous methods based on approximate second-order information or global magnitude pruning. The second scenario is gradual pruning, allowing for re-training between pruning steps. Surprisingly, even here WoodFisher either matches or outperforms state-of-the-art pruning approaches, including recent dynamic pruners [24, 27]. Our study focuses on unstructured pruning, but we can exhibit non-trivial speedups for real-time inference by running on a CPU framework which efﬁciently supports unstructured sparsity [28].
WoodFisher has several useful features and extensions. Since it approximates the full Hessian inverse, it can provide a global measure of parameter importance, and therefore removes the need for manually choosing sparsity targets per layer. Second, it allows us to apply compression in the limited-data regime, where either e.g. 99% of the training is unavailable, or no data labels are available. Third, we show that we can also take into account the ﬁrst-order (gradient) term in the local quadratic model, which leads to further accuracy gain, and the ability to prune models which are not fully converged. 2