Abstract
Binary Neural Network (BNN) shows its predominance in reducing the complexity of deep neural networks. However, it suffers severe performance degradation. One of the major impediments is the large quantization error between the full-precision weight vector and its binary vector. Previous works focus on compensating for the norm gap while leaving the angular bias hardly touched. In this paper, for the ﬁrst time, we explore the inﬂuence of angular bias on the quantization error and then introduce a Rotated Binary Neural Network (RBNN), which considers the angle alignment between the full-precision weight vector and its binarized version. At the beginning of each training epoch, we propose to rotate the full-precision weight vector to its binary vector to reduce the angular bias. To avoid the high complexity of learning a large rotation matrix, we further introduce a bi-rotation formulation that learns two smaller rotation matrices. In the training stage, we devise an adjustable rotated weight vector for binarization to escape the potential local optimum. Our rotation leads to around 50% weight ﬂips which maximize the information gain. Finally, we propose a training-aware approximation of the sign function for the gradient backward. Experiments on CIFAR-10 and
ImageNet demonstrate the superiorities of RBNN over many state-of-the-arts. Our source code, experimental settings, training logs and binary models are available at https://github.com/lmbxmu/RBNN. 1

Introduction
The community has witnessed the remarkable performance improvements of deep neural networks (DNNs) in computer vision tasks, such as image classiﬁcation [26, 19], object detection [39, 20] and semantic segmentation [35, 33]. However, the cost of massive parameters and computational complexity makes DNNs hard to be deployed on resource-constrained and low-power devices.
To solve this problem, many compression techniques have been proposed including network pruning
[30, 14, 29], low-rank decomposition [12, 43, 18], efﬁcient architecture design [24, 42, 7] and network quantization [28, 2, 21], etc. In particular, network quantization resorts to converting the weights and activations of a full-precision network to low-bit representations. In the extreme case, a binary neural network (BNN) restricts its weights and activations to only two possible values (−1 and +1) such that: 1) the network size is 32× less than its full-precision counterpart; 2) the multiply-accumulation convolution can be replaced with the efﬁcient xnor and bitcount logics.
Though BNN has attracted great interest, it remains a challenge to close the accuracy gap between a full-precision network and its binarized version [38, 6]. One of the major obstacles comes at the large
∗Corresponding Author: rrji@xmu.edu.cn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (a) Early works [8, 9] suffer from large quantization error caused by both the norm gap and angular bias between the full-precision weights and its binarized version. (b) Recent works [38, 37] introduce a scaling factor to re-duce the norm gap but cannot reduce the angular bias, i.e., θ. Therefore the quantization error (cid:107)w sin θ(cid:107)2 is still large when θ is large.
Figure 2: Cosine similarity and quantization error in various layers of ResNet-20. (a) Our
RBNN achieves a signiﬁcantly higher cosine sim-ilarity between the full-precision weight and its binarization than XNOR-Net [38] does, imply-ing fewer angular bias. (b) XNOR-Net suffers great quantization error while RBNN leads to a much smaller one. quantization error between the full-precision weight vector w and its binary vector b [8, 9] as shown in Fig. 1(a). To solve this, state-of-the-art approaches [38, 37] try to lessen the quantization error by introducing a per-channel learnable/optimizable scaling factor λ to minimize the quantization error: (cid:107)λb − w(cid:107)2. min
λ,b (1)
However, the introduction of λ only partly mitigates the quantization error by compensating for the norm gap between the full-precision weight and its binarized version, but cannot reduce the quantization error due to an angular bias as shown in Fig. 1(b). Apparently, with a ﬁxed angular bias
θ, when λb − w is orthogonal to λb, Eq. (1) reaches the minimum and we have (cid:107)w sin θ(cid:107)2 ≤ (cid:107)λb − w(cid:107)2, (2)
Thus, (cid:107)w sin θ(cid:107)2 serves as the lower bound of the quantization error and cannot be diminished as long as the angular bias exists. This lower bound could be huge with a large angular bias θ. Though the training process updates the weights and may close the angular bias, we experimentally observe the possibility of this case is small, as shown in Fig. 2. Thus, it is desirable to reduce this angular error for the sake of further reducing the quantization error. Moreover, the information of BNN learning is upper-bounded by 2n where n is the total number of weight elements and the base 2 denotes the two possible values in BNN [32, 37]. Weight ﬂips refer to that positive value turns to −1 and vice versa.
It is easy to see that when the probability of ﬂip achieves 50%, the information reaches the maximum of 2n. However, the scaling factor results in a small ratio of ﬂipping weights thus leading to little information gain in the training process [21, 37]2.
In this paper, we propose a Rotated Binary Neural Network (RBNN) to further mitigate the quantiza-tion error from the intrinsic angular bias as illustrated in Fig. 3. To the best of our knowledge, this is the ﬁrst work that explores and reduces the inﬂuence of angular bias on quantization error in the ﬁeld of BNN. To this end, we devise an angle alignment scheme by learning a rotation matrix that rotates the full-precision weight vector to its geometrical vertex of the binary hypercube at the beginning of each training epoch. Instead of directly learning a large rotation matrix, we introduce a bi-rotation formulation that learns two smaller matrices with a signiﬁcantly reduced complexity. A series of optimization steps are then developed to learn the rotation matrices and binarization alternatingly to align the angle difference as shown in Fig. 2(a), which signiﬁcantly reduces the quantization error as illustrated in Fig. 2(b). To get rid of the possible local optimum in the optimization, we dynamically adjust the rotated weights for binarization in the training stage. We show that the proposed rotation not only reduces the angular bias which leads to less quantization error, but also achieves around 50% weight ﬂips thereby achieving maximum information gain. Finally, we provide a training-aware approximation of the sign function for gradient backpropagation. We show the superiority of RBNN through extensive experiments. 2The binarization in Eq. (1) is obtained by b = sign(w), which does not change the coordinate quadrant as shown in Fig. 1. Thus, only a small number of weight ﬂips occur in the training stage. See Sec. 4.3 for our experimental validation. 2
Figure 3: Framework of our RBNN. The weight vector is rotated at the beginning of each training epoch such that the angular bias ϕ between the rotated weight vector and the geometrical binary vertex is smaller than that of the original θ. After rotation, the weights are either unﬂipped (b) or ﬂipped (c) which increases the information gain. During training, the rotated weights are dynamically adjusted such that ˜w with much less angular bias ϕ(cid:48) is obtained, which then follows up the binarization. 2