Abstract
This paper explores useful modiﬁcations of the recent development in contrastive learning via novel probabilistic modeling. We derive a particular form of con-trastive loss named Joint Contrastive Learning (JCL). JCL implicitly involves the simultaneous learning of an inﬁnite number of query-key pairs, which poses tighter constraints when searching for invariant features. We derive an upper bound on this formulation that allows analytical solutions in an end-to-end training man-ner. While JCL is practically effective in numerous computer vision applications, we also theoretically unveil the certain mechanisms that govern the behavior of
JCL. We demonstrate that the proposed formulation harbors an innate agency that strongly favors similarity within each instance-speciﬁc class, and therefore remains advantageous when searching for discriminative features among distinct instances. We evaluate these proposals on multiple benchmarks, demonstrating considerable improvements over existing algorithms. Code is publicly available at: https://github.com/caiqi/Joint-Contrastive-Learning. 1

Introduction
In recent years, supervised learning has seen tremendous progress and made great success in numerous real-world applications. By heavily relying on human annotations, supervised learning allows for convenient end-to-end training of deep neural networks, and has made human-crafted features the least popular in the machine learning community. However, the underlying feature behind the data potentially has a much richer structure than what the sparse labels or rewards describe, while label acquisition is also time-consuming and economically expensive. In contrast, unsupervised learning uses no manually labeled annotations, and aims to characterize the underlying feature distribution completely depending on the data itself. This overcomes several disadvantages that the supervised learning encounters, including overﬁtting of the speciﬁc tasks-led features that cannot be readily transferred to other objectives. Unsupervised learning therefore is an important stepping stone towards more robust and generic representation learning.
Contrastive learning is at the core of several advances in unsupervised learning. The use of contrastive loss dates back to [17]. In brief, the loss function in [17] runs over pairs of samples, returning low values for similar pairs and high values for dissimilar pairs, which encourages invariant features on the low dimensional manifold. The seminal work Noise Contrastive Estimation (NCE) [16] then builds up the foundation of the contemporary contrastive learning, as NCE provides rigorous theoretical justiﬁcation by posing the contrastive learning problem into the “two-class” problem. InfoNCE [36] roots in the principles of NCE and links the contrastive formulation with mutual information.
However, most existing contrastive learning methods only consider independently penalizing the incompatibility of each single positive query-key pair at a time. This does not fully leverage the assumption that all augmentations corresponding to a speciﬁc image are statistically dependent on
∗Qi Cai and Yu Wang contributed equally to this work. This work was performed at JD AI Research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
each other, and are simultaneously similar to the query. In order to take this advantage of shared similarity across augmentations, we derive a particular form of loss for contrastive learning named
Joint Contrastive Learning (JCL). Our launching point is to introduce dependencies among different query-key pairs, so that similarity consistency is encouraged within each instance-speciﬁc class.
Speciﬁcally, our contributions include:
• We consider simultaneously penalizing multiple positive query-key pairs in regard of their “in-pair” dissimilarity. However, carrying multiple query-key pairs in a mini-batch is beyond the practical computational budget. To mitigate this issue, we push the limit and take the number of pairs to inﬁnity. This novel formulation inherently absorbs the impact of large number of positive pairs via a principled probabilistic modeling. We could therefore approach an analytic form of loss that allows for end-to-end training of the deep network.
• We also theoretically unveil plenty of interesting interpretations behind the loss. Empirical evidences are presented that strongly echo these hypotheses.
• Empirical results show that JCL is advantageous when searching for discriminative features and
JCL demonstrates considerable boosts over existing algorithms on various benchmarks. 2