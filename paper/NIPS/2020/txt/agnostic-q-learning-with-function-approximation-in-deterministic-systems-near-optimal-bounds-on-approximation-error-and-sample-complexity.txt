Abstract
The current paper studies the problem of agnostic Q-learning with function approx-imation in deterministic systems where the optimal Q-function is approximable by a function in the class F with approximation error δ ≥ 0. We propose a novel recursion-based algorithm and show that if δ = O (cid:0)ρ/ (cid:1), then one can ﬁnd the optimal policy using O(dimE) trajectories, where ρ is the gap between the optimal Q-value of the best actions and that of the second-best actions and dimE is the Eluder dimension of F. Our result has two implications: dimE
√ 1. In conjunction with the lower bound in [Du et al., 2020], our upper bound (cid:1) is necessary and sufﬁcient for suggests that the condition δ = (cid:101)Θ (cid:0)ρ/ algorithms with polynomial sample complexity. dimE
√ 2. In conjunction with the obvious lower bound in the tabular case, our upper bound suggests that the sample complexity (cid:101)Θ (dimE) is tight in the agnostic setting.
Therefore, we help address the open problem on agnostic Q-learning proposed in [Wen and Van Roy, 2013]. We further extend our algorithm to the stochastic reward setting and obtain similar results. 1

Introduction
Q-learning is a fundamental approach in reinforcement learning [Watkins and Dayan, 1992]. Empiri-cally, combining Q-learning with function approximation schemes has lead to tremendous success on various sequential decision-making problems. However, theoretically, we only have a good understanding of Q-learning in the tabular setting. Strehl et al. [2006], Jin et al. [2018] show that with certain exploration techniques, Q-learning provably ﬁnds a near-optimal policy with sample complexity polynomial in the number of states, number of actions and the planning horizon. However, modern reinforcement learning applications often require dealing with large state space where the polynomial dependency on the number of states is not acceptable.
Recently, there has been great interest in designing and analyzing Q-learning algorithms with linear function approximation [Wen and Van Roy, 2013, Du et al., 2019]. Under various additional assumptions, these works show that one can obtain a near-optimal policy using Q-learning with sample complexity polynomial in the feature dimension d and the planning horizon, if the optimal
Q-function is an exact linear function of the d-dimensional features of the state-action pairs.
∗University of Washington. Email: ssdu@cs.washington.edu
†Princeton University. Email: jasonlee@princeton.edu
‡University of California, San Diego. Email: gmahajan@eng.ucsd.edu
§Carnegie Mellon University. Email:ruosongw@andrew.cmu.edu 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
A major drawback of these works is that the algorithms can only be applied in the well-speciﬁed case, i.e., the optimal Q-function is an exact linear function. In practice, the optimal Q-function is usually linear up to small approximation errors instead of being exactly linear. In this paper, we focus on the agnostic setting, i.e., the optimal Q-function can only be approximated by a function class with approximation error δ, which is closer to practical scenarios. Indeed, designing a provably efﬁcient
Q-learning algorithm in the agnostic setting is an open problem posed by Wen and Van Roy [2013].
Technically, the agnostic setting is arguably more challenging than the exact setting. As recently shown by Du et al. [2020], for the class of linear functions, when the approximation error δ =
Ω((cid:112)poly(H)/d) where H is the planning horizon, any algorithm needs to sample exponential number of trajectories to ﬁnd a near-optimal policy even in deterministic systems. Therefore, for algorithms with polynomial sample complexity, additional assumptions are needed to bypass the hardness result. For the exact setting δ = 0, Wen and Van Roy [2013] show that one can ﬁnd an optimal policy using polynomial number of trajectories for linear functions in deterministic systems, which implies that the agnostic setting could be exponentially harder than the exact setting.
Due to the technical challenges, for the agnostic setting, previous papers mostly focus on the bandit setting or reinforcement learning with a generative model [Lattimore and Szepesvari, 2019, Van Roy and Dong, 2019, Neu and Olkhovskaya, 2020], and much less is known for the standard reinforcement learning setting. In this paper, we design Q-learning algorithms with provable guarantees in the agnostic case for the standard reinforcement learning setting. 1.1 Our Contributions
Our main contribution is a provably efﬁcient Q-learning algorithm for the agnostic setting with general function approximation in deterministic systems. Our results help address the open problem posed by Wen and Van Roy [2013].
Theorem 1.1 (Informal). For a given episodic deterministic system and a function class F, suppose there exists f ∈ F such that the optimal Q-function Q∗ satisﬁes |f (s, a) − Q∗(s, a)| ≤ δ for
√ any state-action pair (s, a). Suppose ρ = Ω( dimEδ), where the optimality gap ρ is the gap between the optimal Q-value of the best action and that of the second-best action (formally deﬁned in
Deﬁnition 3.1) and dimE is the Eluder dimension of F (see Deﬁnition 3.5), our algorithm ﬁnds the optimal policy using O(dimE) trajectories.
Our main assumption is that the optimality gap ρ satisﬁes ρ = Ω( necessity of this assumption and its connection with the recent hardness result in Du et al. [2020]. dimEδ). Below we discuss the
√
In Du et al. [2020], it has been proved that in deterministic systems, if the optimality gap ρ = 1 and the optimal Q-function can be approximated by linear functions with approximation error
δ = Ω((cid:112)poly(H)/d), any algorithm needs to sample exponential number of trajectories to ﬁnd a near-optimal policy even in deterministic systems. Here d is the input dimension of the linear functions. Using the same technique as in [Du et al., 2020], we show the following hardness result for Q-learning with linear function approximation in the agnostic setting.
Proposition 1.2 (Generalization of Theorem 4.1 in [Du et al., 2020]). For any ρ ≤ 1, there exists a family of deterministic systems where the optimality gap is ρ and the optimal Q-function can be approximated by linear functions with approximation error δ = O(C/ d · ρ), such that any algorithm that returns a ρ/2-optimal policy needs to sample Ω(2C) trajectories.
√
√
√
By setting C = O(log(Hd)) such that 2C = poly(Hd), Proposition 1.2 implies that for any algorithm with polynomial sample complexity, the approximation error δ that can be handled is at most (cid:101)O(ρ/ d). Recall that the Eluder dimension of linear functions is (cid:101)O(d). Theorem 1.1 suggests that as long as ρ = ˜Ω( dδ), our algorithm ﬁnds the optimal policy using polynomial number of samples. Note that this applies to every pair of (ρ, δ) that satisﬁes the condition. Proposition 1.2 suggests that there exist environments with ρ = ˜Ω( dδ) which require exponential number of samples to ﬁnd a near-optimal policy. Therefore, combining Theorem 1.1 and Proposition 1.2, we give a tight characterization (up to logarithmic factors) on the quantitative relation between ρ and δ under which one can use polynomial number of samples to ﬁnd the optimal policy.
√
Our result is in the same spirit as the results in [Lattimore and Szepesvari, 2019, Van Roy and
Dong, 2019], which also demonstrate the tightness of the hardness result in [Du et al., 2020]. 2
However, as will be made clear, technically our result signiﬁcantly deviates from those in [Lattimore and Szepesvari, 2019, Van Roy and Dong, 2019]. See Section 2 for more detailed comparison with [Lattimore and Szepesvari, 2019, Van Roy and Dong, 2019].
Note that the sample complexity of our algorithm is linear in the Eluder dimension of the function class. For the tabular setting, the Eluder dimension is as large as the cardinality of the state-action space [Russo and Van Roy, 2013]. This cardinality is also a sample complexity lower bound, i.e., for the tabular setting, the sample complexity lower bound is Ω (dimE). Therefore, our sample complexity is also tight.
Finally, we show how to generalize our results to handle stochastic rewards. Under the same assumption that ρ = Ω( log(1/p) trajectories with failure probability p. We would like to remark that the log(1/p)/ρ2 dependency is necessary for ﬁnding optimal policies even in the bandit setting [Mannor and Tsitsiklis, 2004]. dimEδ), our algorithm ﬁnds an optimal policy using poly(dimE ,H)
√
ρ2
Organization In Section 2, we review related work. In Section 3, we introduce necessary notations, deﬁnitions and assumptions. In Section 4, we discuss the special case where F is the class of linear functions to demonstrate the high-level approach of our algorithm and the intuition behind the analysis. We then present the result for general function classes in Section 5. We conclude in Section 6 and defer proofs to the supplementary material. 2