Abstract
Learning heuristics for combinatorial optimization problems through graph neural networks have recently shown promising results on some classic NP-hard problems.
These are single-level optimization problems with only one player. Multilevel combinatorial optimization problems are their generalization, encompassing sit-uations with multiple players taking decisions sequentially. By framing them in a multi-agent reinforcement learning setting, we devise a value-based method to learn to solve multilevel budgeted combinatorial problems involving two players in a zero-sum game over a graph. Our framework is based on a simple curriculum: if an agent knows how to estimate the value of instances with budgets up to B, then solving instances with budget B + 1 can be done in polynomial time regardless of the direction of the optimization by checking the value of every possible afterstate.
Thus, in a bottom-up approach, we generate datasets of heuristically solved in-stances with increasingly larger budgets to train our agent. We report results close to optimality on graphs up to 100 nodes and a 185× speedup on average compared to the quickest exact solver known for the Multilevel Critical Node problem, a max-min-max trilevel problem that has been shown to be at least Σp 2-hard. 1

Introduction
The design of heuristics to tackle real-world instances of NP-hard combinatorial optimization prob-lems over graphs has attracted the attention of many Computer Scientists over the years [29]. With advances in Deep Learning [30] and Graph Neural Networks [63], the idea of leveraging the recurrent structures appearing in the combinatorial objects belonging to a distribution of instances of a given problem to learn efﬁcient heuristics with a Reinforcement Learning (RL) framework has received an increased interest [6, 48]. Although these approaches show promising results on many fundamental
NP-hard problems over graphs, such as Maximum Cut [4] or the Traveling Salesman Problem [37], the range of combinatorial challenges on which they are directly applicable is still limited.
Indeed, most of the combinatorial problems over graphs solved heuristically with Deep Learning
[2, 4, 5, 13, 15, 37, 41, 47] are classic NP-hard problems for which the canonical optimization formulation is a single-level Mixed Integer Linear Program: there is one decision-maker1 seeking to minimize a linear cost subject to linear constraints and integer requirements. However, in many real-world situations, decision-makers interact with each other. A particular case of such setting are sequential games with a hierarchy between players: an upper level authority (a leader) optimizes its goal subject to the response of a sequence of followers seeking to optimize their own objectives given 1We will use interchangeably the words decision-maker, agent and player. Note that decision-maker, player and agent are usually used in Operations Research, Game Theory and Reinforcement Learning, respectively.
Similarly for the words decision, strategy and policy. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the actions previously made by others higher in the hierarchy. These problems are naturally modeled as Multilevel Programming problems (MPs) and can be seen as a succession of nested optimization tasks, i.e. mathematical programs with optimization problems in the constraints [9, 12, 65].
Thus, ﬁnding an optimal strategy for the leader in the multilevel setting may be harder than for single-level problems as evaluating the cost of a given strategy might not be possible in polynomial time: it requires solving the followers optimization problems. In fact, even Multilevel Linear Programming with a sequence of L + 1 players (levels) is Σp
L-hard [8, 17, 34]. In practice, exact methods capable to tackle medium-sized instances in reasonable time have been developed for max-min-max Trilevels, min-max Bilevels and more general Bilevel Programs (e.g., [14, 22, 23, 46, 59]).
Despite the computational challenges intrinsic to MPs, these formulations are of practical interest as they properly model hierarchical decision problems. Originally appearing in economics in the bilevel form, designated as Stackelberg competitions [62], they have since been extended to more than two agents and seen their use explode in Operations Research [38, 57]. Thus, research efforts have been directed at ﬁnding good quality heuristics to solve those problems, e.g. [16, 24, 26, 60].
Hence, one can ask whether we can make an agent learn how to solve a wide range of instances of a given multilevel problem, extending the success of recent Deep Learning approaches on solving single-level combinatorial problems to higher levels.
In this paper, we propose a simple curriculum to learn to solve a common type of multilevel combinatorial optimization problem: budgeted ones that are zero-sum games played in a graph.
Although the framework we devise is set to be general, we center our attention on the Multilevel
Critical Node problem (MCN) [1] and its variants. The reasons for such a choice are manifold. First, the MCN is an example of a Defender-Attacker-Defender game [10] which received much attention lately as it aims to ﬁnd the best preventive strategies to defend critical network infrastructures against malicious attacks. As it falls under the global framework of network interdiction games, it is also related to many other interdiction problems with applications ranging from ﬂoods control [51] to the decomposition of matrices into blocks [27]. Moreover, an exact method to solve the problem has been presented in [1] along with a publicly available dataset of solved instances2, which we can use to assess the quality of our heuristic. Lastly, complexity results are available for several variants and sub-problems of MCN, indicating its challenging nature [49].
Contributions. Our contribution rests on several steps. First, we frame generic Multilevel Budgeted
Combinatorial problems (MBC) as Alternating Markov Games [43, 44]. This allows us to devise a
ﬁrst algorithm, MultiL-DQN, to learn Q-values. By leveraging both the combinatorial setting (the environment is deterministic) and the budgeted case (the length of an episode is known in advance), we motivate a curriculum, MultiL-Cur. Introducing a Graph Neural Networks based agent, we empirically demonstrate the efﬁciency of our curriculum on 3 versions of the MCN, reporting results close to optimality on graphs of size up to 100.
Paper structure. Section 2 formalizes the MBC problem. In Section 3, we provide an overview of the relevant literature. The MBC is formulated within the Multi-Agent RL setting in Section 4 along with the presentation of our algorithmic approaches: MultiL-DQN and MultiL-Cur. Section 5 states the particular game MCN in which our methodology is validated in Section 6. 2 Problem statement
The general setting for the MPs we are considering is the following: given a graph G = (V, A), two concurrent players, the leader and the follower, compete over the same combinatorial quantity S, with the leader aiming to maximize it and the follower to minimize it. They are given a total number of moves L ∈ N and a sequence of budgets (b1, ..., bL) ∈ NL. Although our study and algorithms also apply to general integer cost functions c, for the sake of clarity, we will only consider situations where the cost of a move is its cardinality. We focus on perfect-information games, i.e. both players have full knowledge of the budgets allocated and previous moves. The leader always begins and the last move is attributed by the parity of L. At each turn l ∈ [[1, L]], the player concerned makes a set of bl decisions about the graph. This set is denoted by Al and constrained by the previous moves (A1, .., Al−1). We consider games where players can only improve their objective by taking a decision: there is no incentive to pass. Without loss of generality, we can assume that L is odd. Then, 2https://github.com/mxmmargarida/Critical-Node-Problem 2
the Multilevel Budgeted Combinatorial problem (MBC) can be formalized as: (MBC) max
|A1|≤b1 min
|A2|≤b2
... max
|AL|≤bL
S(G, A1, A2, ..., AL). (1)
MBC is a zero-sum game as both leader and follower have the same objective function but their direction of optimization is opposite. A particular combinatorial optimization problem is deﬁned by specifying the quantity S, ﬁxing L, and by characterizing the nature of both the graph (e.g directed, weighted) and of the actions allowed at each turn (e.g labeling edges, removing nodes). The problem being ﬁxed, a distribution D of instances i ∼ D is determined by setting a sampling law for random graphs and for the other parameters, having speciﬁed bounds beforehand: n = |V | ∈ [[nmin, nmax]],
]] × ... × [[bmin
|A| ∈ [[dmin × n(n − 1), dmax × n(n − 1)]], (b1, ..., bL) ∈ [[bmin
]]. Our ultimate goal is thus to learn good quality heuristics that manage to solve each i ∼ D.
L , bmax
, bmax 1
L 1
In order to achieve that, we aim to leverage the recurrent structures appearing in the combinatorial objects in the distribution D by learning graph embeddings that could guide the decision process. As data is usually very scarce (datasets of exactly solved instances being hard to produce), the go-to framework to learn useful representations in these situations is Reinforcement Learning [58]. 3