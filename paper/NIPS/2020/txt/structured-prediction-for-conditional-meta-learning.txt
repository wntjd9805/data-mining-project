Abstract
The goal of optimization-based meta-learning is to ﬁnd a single initialization shared across a distribution of tasks to speed up the process of learning new tasks. Conditional meta-learning seeks task-speciﬁc initialization to better capture complex task distributions and improve performance. However, many existing conditional methods are difﬁcult to generalize and lack theoretical guarantees.
In this work, we propose a new perspective on conditional meta-learning via structured prediction. We derive task-adaptive structured meta-learning (TASML), a principled framework that yields task-speciﬁc objective functions by weighing meta-training data on target tasks. Our non-parametric approach is model-agnostic and can be combined with existing meta-learning methods to achieve conditioning.
Empirically, we show that TASML improves the performance of existing meta-learning models, and outperforms the state-of-the-art on benchmark datasets. 1

Introduction
State-of-the-art learning algorithms such as neural networks typically require vast amounts of data to generalize well. This is problematic for applications with limited data availability (e.g. drug discovery [3]). Meta-learning is often employed to tackle the lack of training data [17, 40, 53]. It is designed to learn data-driven inductive bias to speed up learning on novel tasks [50, 52], with many application settings such as learning-to-optimize [30], few-shot learning [16, 27] and hyperparameter optimization [18]. Meta-learning methods could be broadly categorized into metric-learning [e.g. 37, 47, 53], model-based [e.g. 20, 22, 30], and optimization-based [e.g. 17, 35, 44].
We focus on optimization-based approaches, which cast meta-learning as a bi-level optimization [4, 17, 39]. At the single-task level, an “inner” algorithm performs task-speciﬁc optimization starting from a set of meta-parameters shared across all tasks. At the “outer” level, a meta learner accrues experiences from observed tasks to learn the aforementioned meta-parameters. These methods seek to learn a single initialization of meta-parameters that can be effectively adapted to all tasks.
Relying on the shared initialization is challenging for complex (e.g. multi-modal) task distributions, since different tasks may require a substantially different initialization, given the same adaptation routine. This makes it infeasible to ﬁnd a common meta-parameters for all tasks. Several recent works [10, 15, 23, 24, 28, 44, 54, 55, 56] address the issue by conditioning such parameters on target tasks, and demonstrate consistent improvements over unconditional meta-learning. However, existing methods often lack theoretical guarantees on generalization performance and implements speciﬁc conditioning principles with customized networks, which may be difﬁcult to generalize across different application settings.
In this paper, we offer a novel perspective on conditional meta-learning based on structured predic-tion [6]. This enables us to propose Task-adaptive Structured Meta-Learning (TASML) – a general framework for conditional meta-learning – by interpreting the inner algorithm as the structured output to be predicted, conditioned on target tasks. We derive a principled estimator that minimizes 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
task-speciﬁc meta-learning objectives, which weigh known training tasks based on their similarities with the target task. The proposed framework is non-parametric and thus requires access to training data at test time for the task-speciﬁc objectives. We introduce an efﬁcient algorithm for TASML to mitigate the additional computational costs associated with optimizing these task-speciﬁc objectives.
Intuitively, the proposed framework learns a target task by explicitly recalling only the most relevant tasks from past observations, to better capture the local task distribution for improved generalization.
The relevance of previously observed tasks with respect to the target one is measured by a structured prediction approach from [13]. TASML is model-agnostic and can easily adapt existing meta-learning methods to achieve conditional meta-learning.
We empirically evaluate TASML on several competitive few-shot classiﬁcation benchmarks, including datasets derived from IMAGENET and CIFAR respectively. We show that TASML outperforms state-of-the-art methods, and improves the accuracy of existing meta-learning algorithms by adapting them into their respective conditional variants. We also investigate TASML’s trade-off between computational efﬁciency and accuracy improvement, showing that the proposed method achieves good efﬁciency in learning new tasks.
Our main contributions include: i) a new perspective on conditional meta-learning based on structured prediction, ii) TASML, a conditional meta-learning framework that generalizes existing meta-learning methods, iii) a practical and efﬁcient algorithm under the proposed framework, and iv) a thorough evaluation of TASML on benchmarks, outperforming state-of-the-art methods. 2