Abstract
Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model. We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms that facilitate the ﬂow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model’s sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in speciﬁc components of the model that may exhibit highly specialized behavior. 1

Introduction
The success of neural network models in various natural language processing tasks, coupled with their opaque nature, has led to much interest in interpreting and analyzing such models. One goal of these analyses is to identify whether a model utilizes latent information in its internal representations to arrive at a prediction. This is of particular importance when diagnosing the reasons for a biased prediction. A popular class of analysis methods, often called structural analysis, aims to extract this information using probing classiﬁers that predict linguistic properties from representations of trained models (e.g., Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Tenney et al., 2019). However, since probing classiﬁers only yield a correlational measure between a model’s representations and an external property (Belinkov and Glass, 2019), they cannot show if the property is causally connected to the model’s predictions. Moreover, Barrett et al. (2019) showed that probing classiﬁers may generate unfaithful interpretations and fail to generalize to unseen data.
We introduce a methodology for interpreting neural models to address these limitations. We adapt causal mediation analysis (Pearl, 2001) for analyzing the mechanisms by which information ﬂows from input to output through different model components. Mediation analysis relies on measuring the change in an output following a counterfactual intervention in an intermediate variable, or mediator.
Through such interventions, one can measure the degree to which inputs inﬂuence outputs directly (direct effect), or indirectly through the mediator (indirect effect). In our case, the mediator can be any model components that we wish to study, such as neurons or attention heads. We propose multiple controlled interventions in these mediators, which reveal their causal role in a model’s behavior.
∗ Equal contribution. Y.B. is now at the Technion – Israel Institute of Technology. Work conducted while J.V. was at Palo Alto Research Center. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In a case study, we apply this framework to the analysis of gender bias in large pre-trained language models. Gender bias has surfaced as a major concern in word representations, with strong effects in both static word embeddings (Caliskan et al., 2017; Bolukbasi et al., 2016) and contextualized word representations (Zhao et al., 2019; Basta et al., 2019; Tan and Celis, 2019). Mediation analysis enables us to study how biased predictions arise from different model components. In our study, we focus on the role of individual neurons or attention heads in Transformer-based language models, in particular, several versions of GPT2 (Radford et al., 2019). In an experiment using several datasets designed to gauge a model’s gender bias, we ﬁnd that gender bias effects increase with larger models, which potentially absorb more bias from the underlying training data. The causal mediation analysis further reveals that gender bias is sparse, with much of the effect concentrated in a relatively small proportion of neurons and attention heads.
In summary, this paper makes two broad contributions. First, we cast causal mediation analysis as an approach for analyzing neural NLP models, which may be applied to a variety of models and phenomena. Second, we demonstrate this methodology in the case of analyzing gender bias in pre-trained language models, revealing the internal mechanisms by which bias effects ﬂow from input to output through various model components. The code for reproducing our results is available at https://github.com/sebastianGehrmann/CausalMediationAnalysis. 2 Methodology 2.1 Preliminaries
Consider a large pre-trained neural language model (LM), parameterized by θ, which predicts the probability of the next word given a preﬁx: pθ(xt | x1, . . . , xt−1). We will focus on LMs based on
Transformers (Vaswani et al., 2017), although much of the methodology applies to other architectures as well. Let hl,i ∈ RK denote the (contextual) representation of word i in layer l of the model, with neuron activations hl,i,k (1 ≤ k ≤ K). These representations are composed using so-called multi-headed attention. Let αl,h,i,j ≥ 0 denote the attention directed from word i to word j by head h in layer l, such that (cid:80) j αl,h,i,j = 1. 2.2 Causal Mediation Analysis
Causal mediation analysis aims to measure how a treatment effect is mediated by intermediate vari-ables (Robins and Greenland, 1992; Pearl, 2001;
Robins, 2003). Pearl (2001) described an example where a side effect of a drug may cause patients to take aspirin, and the latter has a separate effect on the dis-ease the drug was originally prescribed for. Thus, the drug has a direct effect through its standard mechanism and an indirect effect operating via aspirin taking.
Figure 1: Mediation analysis illustration.
As illustrated in Figure 1, we may consider each neuron in a neural network to be analogous to aspirin in the example above – the neuron is inﬂuenced by the input and, in turn, affects the model output; however, there also exist direct pathways from the input to the output that do not pass through the neuron. We can thus decouple model components from the rest of the model by framing them as intermediaries in the causal path from inputs to outputs. Throughout this paper, we speciﬁcally focus on the use case of gender bias in language models, as past work suggests that gender is captured in speciﬁc model components, e.g., subspaces of contextual word representations (Zhao et al., 2019).
By measuring the direct and indirect effects of targeted interventions, we can pinpoint how gender bias propagates through different parts of pre-trained LMs. While we use gender bias as a case study, the approach can be applied to other biases as well (race, ethnicity, etc.).
The example on the right illustrates a typical problem with biased LMs. Given a prompt u such as The nurse said that, a language model generates a continuation. A biased model may assign a higher likelihood to she than to he, such that pθ(she | u) > pθ(he | u). In this case, she is the stereotypical candidate, while he is the anti-stereotypical candidate, which reﬂects a societal bias associating nurses with women more than men. Coming back to the binary setup, the relative
Prompt u: The nurse said that
Stereotypical candidate: she
Anti-stereotypical candidate: he 2
Figure 2: Mediation analysis illustration. Here the do-operation is x = set-gender, which changes u from nurse to man in this example. The total effect measures the change in y resulting from the intervention; the direct effect measures the change in y resulting from performing the intervention while holding a mediator z ﬁxed; the indirect effect measures the change caused by setting z to its value under the intervention, while holding u ﬁxed. probabilities assigned to the candidates can be thought of as a measure of gender bias in the model: y(u) = pθ(anti-stereotypical | u) pθ(stereotypical | u)
. (1)
In our example, y(u) = pθ(he | The nurse said that)/pθ(she | The nurse said that). If y(u) < 1, the prediction is stereotypical; if y(u) > 1, it is anti-stereotypical. A perfectly unbiased model would achieve y(u) = 1 and exhibit bias toward neither the stereotypical nor the anti-stereotypical case.
This binary simpliﬁcation of grammatical gender does not capture the full spectrum, as argued by Cao and Daumé III (2019), and it is not deﬁned, for example, what probability mass a gender-neutral reference should receive2. While we leave extension of the framework to a continuous setup to future work, we report experimental results on the singular they compared to he.
We then apply causal mediation analysis by performing interventions on the input text, and measuring the effect on the gender bias measure deﬁned above (Eq. 1), which we treat as the response variable.
We deﬁne the following do-operations: (a) set-gender: replace the ambiguous profession with an anti-stereotypical gender-speciﬁc word (that is, replace nurse with man, doctor with woman, etc.); (b) null: leave the sentence as is. The population of units for this analysis is a set of example sentences such as the above prompt. We deﬁne yx(u) as the value that y attains in unit u = u under the intervention do(x = x).
The unit-level total effect (TE) of x = x on y in unit u = u is the proportional difference3 between the amount of bias under a gendered reading and under an ambiguous reading (Figure 2a):
TE(set-gender, null; y, u) = yset-gender(u) − ynull(u) ynull(u)
= yset-gender(u) ynull(u)
− 1. (2)
For our running example, this results in pθ(he | The man said that) pθ(she | The man said that) (cid:30) pθ(he | The nurse said that) pθ(she | The nurse said that)
− 1. (3)
An illustrative example of the computation of the total effect is provided in Figure 3.
The average total effect of x = x on y is calculated by taking the expectation over the population u:
TE(set-gender, null; y) = Eu (cid:2)yset-gender(u)/ynull(u) − 1(cid:3) . (4)
We then analyze the causal role of speciﬁc mediators, or intermediary variables, which lie between x and y. The mediator, denoted as z, might be a particular neuron, a full layer, an attention head, or a certain attention weight. Following Pearl’s deﬁnitions, we measure the direct and indirect effects of intervening in the model relative to z (Pearl, 2001). 2One could argue for the case where he, she, and they have same probability or where they has equal representation to the sum of he and she. Alternatively, one could argue that grammatical genders are inherently discriminatory and that we should change all of them to they, unless we know an individual’s preferred pronouns. 3We make the difference proportional to control for the high variance of y across examples (appendix A.1). 3
Example u = The nurse said that [blank] 1) Compute relative probabilities of the baseline. p([he]|u) = p([he]|the nurse said that) ≈ 0.03 p([she]|u) = p([she]|the nurse said that) ≈ 0.22 ynull(u) = 0.03/0.22 ≈ 0.14 3) Compute the total effect
TE(set-gender, null; y, u)
= 13.1/0.14 − 1 ≈ 92.6 2) Set u to an anti-stereotypical case and recompute. x = set-gender: change nurse → man p([he]|u, set-gender) = p([he]|the man said that) ≈ 0.32 p([she]|u, set-gender) = p([she]|the man said that) ≈ 0.02 yset-gender(u) = 0.32/0.02 ≈ 13.1
Figure 3: An example calculation of the total effect with the prompt u = The nurse said that and the control variable x = set-gender. Before the intervention, the model assigns a much higher probability to [she], the stereotypical example, than to [he]. By changing nurse to man, we compute the proportional probability of a deﬁnitionally gendered example. The total effect measures the effect of this intervention.
The natural direct effect (NDE) measures how much an intervention x changes an outcome variable y directly, without passing through a hypothesized mediator z. It is computed by applying the intervention x but holding z ﬁxed to its original value. For the present use case, we deﬁne the NDE of x = x on y given mediator z = z to be the change in the amount of bias when genderizing all units u, e.g., changing nurse to man, while holding z for each unit to its original value. This measures the direct effect on gender bias that does not pass through the mediator z (illustrated in Figure 2b):
NDE(set-gender, null; y) = Eu[yset-gender,znull(u)(u)/ynull(u) − 1]. (5)
The natural indirect effect (NIE) measures how much the intervention x changes y indirectly, through z. It is computed by setting z to its value under the intervention x, while keeping everything else to its original value. Thus the indirect effect captures the inﬂuence of a mediator on the outcome variable. For the present use case, we deﬁne the NIE as the change in amount of bias when keeping unit u as is, but setting z to the value it would attain under a genderized reading. This measures the indirect effect ﬂowing from x to y through z (Figure 2c):
NIE(set-gender, null; y) = Eu[ynull,zset-gender(u)(u)/ynull(u) − 1]. (6)
This framework allows evaluating the causal contribution of different mediators z to gender bias.
Through the distinction between direct and indirect effect, we can measure how much of the total effect of gender edits on gender bias ﬂows through a speciﬁc component (indirect effect) or elsewhere in the model (direct effect). We experiment with mediators at the neuron level and the attention level. 2.3 Neuron Interventions
To study the role of individual neurons in mediating gender bias, we assign z to each neuron hl,·,k in the LM. The dataset we use consists of a list of templates that are instantiated by profession terms, resulting in examples such as The nurse said that. For each example, we deﬁne the set-gender operation to move in the anti-stereotypical direction, changing female-stereotypical professions like nurse to man and male-stereotypical professions like doctor to woman. Section 3 provides more information on the dataset. We additionally investigate the effect of a gender-neutral intervention, for which we pick person as target of the set-gender change and we measure the probability of the continuation they. Note that, unfortunately, all examples can be seen as biased against gender-neutrality since the models have had limited exposure to the singular they. Moreover, this case suffers from the additional confounder that the model could assign probability to the plural they if it does not refer to the profession.
In the experiments, we investigate the effect of intervening on each neuron independently, as well as on multiple neurons concurrently. That is, the mediator z may be a set of neurons. In all cases, the mediator is in the representation corresponding to the profession word, such as nurse in the example. 4
2.4 Attention Interventions
For studying attention behavior, we focus on the attention weights, which deﬁne relationships between words. The mediators z, in this case, are the attention heads αl,h, each of which deﬁnes a distinct attention mechanism.
Prompt u: The nurse examined the farmer for injuries because she
Stereotypical candidate: was caring
Anti-stereotypical candidate: was screaming
To study their role, we align our interven-tion approach with two resources for as-sessing gender bias in pronoun resolution:
Winobias (Zhao et al., 2018a) and Winogen-der (Rudinger et al., 2018). Both datasets consist of Winograd-schema-style examples that aim to assess gender bias in coreference reso-lution systems. We reformulate the examples to study bias in LMs, as shown in the example on the right, taken from Winobias. According to the stereotypical reading, the pronoun she refers to the nurse, implying the continuation was caring. The anti-stereotypical reading links she to the farmer, this time implying the continuation was screaming. The bias measure is y(u) = pθ(was screaming | u)/pθ(was caring | u).4 In this case, we deﬁne the swap-gender operation, which changes she to he. The total effect is
TE(swap-gender, null; y, u) = yswap-gender(u)/ynull(u) − 1. (7)
In the experiments, we study the effect of the attention from the last word (she or he) to the rest of the sentence.5 Intuitively, in the above example, if the word she attends more to nurse than to farmer, then the more likely continuation might be was caring. We compute the NDE and NIE for each head individually by intervening on the attention weights αl,h,·,·. We also evaluate the joint effects when intervening on multiple attention heads concurrently. The population-level TE and the NDE and NIE are deﬁned analogously as above. 3 Experimental Details
Models As an example large pre-trained LM, we use GPT2 (Radford et al., 2019), a Transformer-based (English) LM trained on massive amounts of data. We use several model sizes made available by Wolf et al. (2019): small, medium, large, extra-large (xl), and a distilled model (Sanh et al., 2019).
Data For neuron intervention experiments, we augment the list of templates from Lu et al. (2018) with several other templates, instantiated with professions from Bolukbasi et al. (2016). The templates have the form “The [occupation] [verb] because”.6 The professions are accompanied by crowdsourced ratings between −1 and 1 for deﬁnitionality and stereotypicality. Actress is deﬁnitionally female, while nurse is stereotypically female. None of the professions are stereotypically or deﬁnitionally gender-neutral in the sense that those people working in the profession are referred to in singular they.
To simplify processing by GPT2 and focus on common professions, we only use examples that are not split into sub-word units, resulting in 17 templates and 169 professions, 2,873 examples in total.
The full lists of templates and professions are given in Appendix A.1. We refer to these examples as the Professions dataset.
For attention intervention experiments, we use examples from Winobias Dev/Test (Zhao et al., 2018a) and Winogender (Rudinger et al., 2018), totaling 160/130 and 44 examples that ﬁt our formulation, respectively. We experiment with the full datasets and ﬁltering by total effect. Both datasets include statistics from the U.S. Bureau of Labor Statistics to assess the gender stereotypicality of the referenced occupations. Appendix A.2 provides additional details about the datasets and preprocessing methods. 4 Results 4.1 Total Effects
Before describing the results from the mediation analysis, we summarize some insights from the reported results stem from measurements of the total effect. Unless noted otherwise, 4To compute probabilities of multi-word continuations, we use the geometric mean of the token probabilities. 5One may also study individual attention arcs. However, attention does not always focus on a speciﬁc word, often falling on adjacent words. See Appendix C.2 for this phenomenon. 6The original list only includes examples ending with because. To increase the lexical diversity of examples, we add templates with other conjunctions 5
Table 1: Total effects (TE) of gender bias in various GPT2 variants.
GPT2 variants
Dataset small rand. distil small medium large xl
Winobias
Winogender
Professions 0.066 0.045 0.117 0.118 0.081 130.859 0.249 0.103 112.275 0.774 0.322 115.945 0.751 0.364 96.859 1.049 0.342 225.217 the binary male→female or female→male interventions. We report separately the results of male/female→neutral interventions, which due to their potentially confounded nature cannot be grouped with the rest of the results. Table 1 shows the total effects of gender bias in the different
GPT2 models, on three datasets, as well as the effects with a randomly initialized GPT2-small model.
Random model effects are much smaller, indicating that it is the training process that causes gender bias.
Larger models are more sensitive to gender bias
In the Winograd-style datasets, the total effect mostly increases with model size, saturating at the large and xl models. In the professions dataset, model size is not well correlated with total effect, but GPT2-xl has a much larger effect. Since larger models can more accurately emulate the training corpus, it makes sense that they would more strongly integrate its biases.
Effects in different datasets
It is difﬁcult to compare effect magnitudes in the three datasets because of their different nature. The professions dataset yields much stronger effects than the
Winograd-style datasets. This may be attributed to the more explicit source of bias, the word representations, as compared to intricate coreference relations in the Winograd-style datasets.
Some effects are correlated with external gender statistics
In the professions dataset, we found moderate positive correlations between external gender bias7 and the log-total effect, ranging from 0.35 to 0.45 over different models, indicating that the model captures the expected biases. It further shows that the effect is ampliﬁed by the model for words that are perceived as more biased. In the
Winograd-style datasets, we found relatively low correlations between the log-total effect and the log-ratio of the two occupations’ stereotypicality, ranging from 0.17 to 0.26. This low correlation may be due to a smaller size than the professions dataset or the more complex Winograd-style relations.
The gender-neutral case leads to more consistent effects
In the neutral case, the baseline prob-ability p(they|u) is much more consistent, but low, across all professions. Consider the template
“The X said that” — in this case, under GPT2-distil “they” varies in probability from 0.2% to 4.2% while “he” has a much wider range from 1.1% to 31.8%. Consequently, the total effect for neutral interventions is much more consistent across models and templates. GPT2-distill, GPT2-small, and
GPT2-medium have total effects of 8.3, 7.5, and 9.6 respectively, all with standard deviations < 10, in the professions dataset. We hypothesize that this can mostly be attributed to very low probability for the singular “they” and a consistent baseline probability where “they” is part of a referential statement toward a group of individuals, for example in “The accountant said that they [the people] need to pay taxes”. 4.2 Mediation Analysis
Where in the model are gender bias effects captured? Are the effects mediated by only a few model components or distributed across the model? Here we answer these questions by measuring the indirect effect ﬂowing through different mediators.
Attention Figure 4a shows the indirect effects for each head in GPT2-small on Winobias. The heatmap shows interventions on each head individually. A small number of heads, concentrated in the middle layers of the model, have much higher indirect effects than others. The bar chart shows indirect effects when intervening on all heads in a single layer concurrently. Consistent with the head-level heatmap, the effects are concentrated in the middle layers. We found this sparsity consistent in all model variants and datasets we examined. We did not ﬁnd similar behavior in a 7For this analysis, we add each profession’s stereotypicality and deﬁnitionality as the overall bias value. 6
(a) Indirect effects in GPT2-small on Wino-bias for heads (the heatmap) and layers (the bar chart). (b) Indirect effects after sequentially selecting an increasing number of heads using the TOP-K or GREEDY approaches.
Very few heads are required to saturate the model effect. The inset lists the sequence of layers of heads selected by GREEDY.
The ones in red together reach the model effect, demonstrating the concentration of the effect in layers 4 and 5.
Figure 4: Sparsity effects in attention heads.
Figure 5: Top 10 heads by indirect effect in GPT2-small on Winobias, and their respective direct effects. Both effects appear largely additive with respect to total effect, a surprising result given the nonlinear nature of these models.
Figure 6: Attention in GPT2-small on a Winobias example, directed from either she or he. Head 5-10 attends directly to the bold stereotypical can-didate, head 5-8 attends to the words following it, and head 4-6 attends to the underlined anti-stereotypical candidate. Attention to the ﬁrst token may be null attention (Vig and Belinkov, 2019). Appendix C.2 shows more examples. randomly initialized model, indicating that these patterns do not occur by chance. Appendix C.1 provides additional visualizations of indirect effects as well as direct effects.
The indirect and direct effects of the top attention heads are summarized in Figure 5. The total effect roughly equals the sum of the direct and indirect effects.8 Qualitative analysis suggests that these top heads take on specialized roles with respect to gender bias, as illustrated in Figure 6. The ﬁgure demonstrates that attention heads capture different coreference aspects: one head aligns with the stereotypical coreference candidate, another head attends to the tokens following that candidate, while a third attends to the anti-stereotypical candidate. Vig (2019) previously identiﬁed the same head as relating to coreference resolution based on visual inspection and Clark et al. (2019) found an attention head in BERT (Devlin et al., 2019) that was highly predictive of coreference, also in layer 5 out of 12. The specialization of attention heads seems to be a general property of Transformers (Voita et al., 2019) and has been observed for a range of syntactic dependency relationships (Clark et al., 2019; Htut et al., 2019; Vig and Belinkov, 2019).
To determine how many heads are required to achieve the full effect of intervening on all heads, we also intervene on groups of heads. While the computational complexity of selecting a single head scales linearly with the number of total heads, selecting a group of heads scales polynomially and becomes computationally intractable. To efﬁciently select a subset of k heads given n total heads, we use two methods: a GREEDY approach, which iteratively selects the head with the maximal marginal contribution to the indirect effect and requires O(nk) evaluations, and a TOP-K approach, 8This kind of decomposition is expected in a linear model, but may be surprising in a non-linear model like
GPT2. Still, we found it consistent in all our analyses, so we focus on showing the indirect effect results. 7
(a) Indirect effects of top neurons in different models on the professions dataset. Here, early layers have the largest effect. (b) Indirect effects after sequentially selecting an in-creasing number of neurons from either the full model or individual layers using the TOP-K approach in GPT2-small on the professions dataset.
Figure 7: Sparsity effects in neurons. which selects the k elements with the strongest individual effects and requires O(n) evaluations.
Appendix D provides more information on these algorithms. Only 10 heads are required to match the effect of intervening on all 144 heads at the same time (Figure 4b). The ﬁrst six selected ones are from layers 4 and 5, further demonstrating the concentration of the effect in the middle layers.
Neurons
Figure 7a shows the indirect effects from the top 5% of neurons from each layer in different models.
The word embeddings (layer 0) and the ﬁrst hidden layer have the strongest effects. This stands in contrast to the attention intervention results, where middle layers had much larger effects. However, we still observe a small increase in effect within the intermediate layers across all models except for the randomized one. Interestingly, we do not observe the same concentration for neutral intervention.
As can be seen in Figure 8, where, for simplicity, we focus on GPT2-medium, the effects are distributed across all layers, but similarly increasing a bit toward the later middle layers.
Figure 7b shows the indirect effects when selecting neu-rons by the TOP-K algorithm.9 Similar to the attention result, a tiny fraction of neurons (4%) is sufﬁcient for obtaining an effect equal to that of intervening on all neurons concurrently. Most of the top selected neurons are concentrated in the embedding layer and ﬁrst hid-den layer. We show in Figure 8 that the same effect does not occur in gender-neutral interventions. The 100 neurons with the highest average indirect effect for gender-neutral interventions in GPT2-small appear within the embeddings and the ﬁrst 9 of the 12 layers, while only about 30 of those come from the embedding and ﬁrst layer. This ﬁnding, which is consistent across all model sizes, provides further evidence for the the lack of representation of gender-neutral information in embeddings.
Figure 8: Indirect effects of top neurons in
GPT2-medium for gender-neutral interven-tions on the professions dataset. Here, the effect is distributed across all layers. 5