Abstract
Sparse coding is an important method for unsupervised learning of task-independent features in theoretical neuroscience models of neural coding. While a number of algorithms exist to learn these representations from the statistics of a dataset, they largely ignore the information bottlenecks present in ﬁber pathways connecting cor-tical areas. For example, the visual pathway has many fewer neurons transmitting visual information to cortex than the number of photoreceptors. Both empirical and analytic results have recently shown that sparse representations can be learned effectively after performing dimensionality reduction with randomized linear op-erators, producing latent coefﬁcients that preserve information. Unfortunately, current proposals for sparse coding in the compressed space require a centralized compression process (i.e., dense random matrix) that is biologically unrealistic due to local wiring constraints observed in neural circuits. The main contribution of this paper is to leverage recent results on structured random matrices to propose a theoretical neuroscience model of randomized projections for communication be-tween cortical areas that is consistent with the local wiring constraints observed in neuroanatomy. We show analytically and empirically that unsupervised learning of sparse representations can be performed in the compressed space despite signiﬁcant local wiring constraints in compression matrices of varying forms (corresponding to different local wiring patterns). Our analysis veriﬁes that even with signiﬁcant local wiring constraints, the learned representations remain qualitatively similar, have similar quantitative performance in both training and generalization error, and are consistent across many measures with measured macaque V1 receptive ﬁelds. 1

Introduction
Sensory nervous systems have long been championed for their ability to learn effective representations of natural scene statistics [66]. In fact, this ability has been one of the broad motivations behind work in artiﬁcial neural networks underlying signiﬁcant recent advances in machine learning systems.
While it is less understood than the supervised learning frameworks common in machine learning, unsupervised learning of task-independent representations is especially important in models of biological sensory systems where supervisory signals are less prevalent. In particular, learning
∗equal contribution
Code available at: https://github.com/siplab-gt/localized-sparse-coding. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
a dictionary that admits a sparse latent representation of data (called sparse coding) [49, 50] has become one important theoretical neuroscience model of unsupervised (i.e., self-supervised or auto-encoding [33]) learning in sensory systems. Sparse coding models have successfully explained response properties in sensory cortical areas [60, 73, 71, 72, 38, 67, 12] with biologically plausible implementations [60, 63, 73], as well as playing an important role in machine learning algorithms [1, 25, 46, 55, 45, 9, 57, 58].
While a number of detailed models have been proposed to learn such representations from natural scene statistics in a sparse coding framework (with varying levels of biological plausibility) [49, 73, 60], these models of sensory cortices largely ignore the fact that cortical regions are generally connected by very limited ﬁber projections. Speciﬁcally, there are often information bottlenecks where fewer ﬁbers connect cortical areas than the number of neurons encoding the representation in each area [65], meaning that plausible learning algorithms must account for the fact that the sensory data has undergone some type of compression. For example, despite the original formulation of sparse coding for images nominally constituting a model of neural coding in primary visual cortex [49, 51], the pathway carrying that information from retina to cortex (via the lateral geniculate nucleus of the thalamus) has already undergone a signiﬁcant reduction in the number of neurons carrying visual information transduced by the retinal photoreceptors [68].
Recently, a theoretical model of these information bottlenecks has been proposed based on randomized linear dimensionality reduction (sometimes called compressed sensing [3]) [35, 16, 32]. In this approach, the sparse coding model is learned in the compressed space, producing latent coefﬁcients that capture the same information as if learning had been performed on the raw data. While this approach is counterintuitive because the learned dictionary has randomized structure when inspected, the key insight is that the receptive ﬁelds estimated from this model still resemble primary visual cortex (V1) simple cell receptive ﬁelds [35, 16]. Randomized dimensionality reduction has appeal as a neural modeling framework for several reasons [5, 26], including the fact that exact wiring patterns between areas are unlikely to be genetically deﬁned [70] and ﬁber projections can be speciﬁed in this framework with coarse chemical gradients to guide randomized ﬁber growth during development.
Unfortunately, the current proposals for sparse coding in the compressed space [35, 16] are biolog-ically implausible because the compression operator used is a dense random matrix that violates local wiring constraints observed in many neural pathways. For example, in the early visual pathway, a dense compression matrix corresponds to each retinal ganglion cell (RGC) receiving input from every photoreceptor despite neuroanatomical evidence [68] that RGCs receive inputs from a localized region in visual space. It is currently unknown if a randomized linear dimensionality reduction model for ﬁber projections in neural systems is feasible under biologically plausible local wiring constraints.
The main contribution of this paper is to leverage recent results on structured random matrices to propose a theoretical neuroscience model of randomized projections for communication between cortical areas that is consistent with the local wiring constraints observed in neuroanatomy. We show that unsupervised learning of sparse representations can be performed in the compressed space despite signiﬁcant local wiring constraints in compression matrices of varying forms (corresponding to different local wiring patterns). Speciﬁcally, we provide both an analytic guarantee on the ability to infer unique sparse representations in this model as well as empirical studies on natural images. Our analysis veriﬁes that even with signiﬁcant local wiring constraints, the learned representations remain qualitatively similar, have similar quantitative performance in both training and generalization error, and are consistent across many measures with measured macaque V1 receptive ﬁelds. Taken together, these results constitute one of the few applications of compressed sensing theory to a speciﬁc neural coding model [26], showing that biologically plausible randomized encodings can play a signiﬁcant role in learning structured representations that capture complex natural image statistics. 2