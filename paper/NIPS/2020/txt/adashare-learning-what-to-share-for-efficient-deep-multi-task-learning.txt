Abstract
Multi-task learning is an open and challenging problem in computer vision. The typical way of conducting multi-task learning with deep neural networks is either through handcrafted schemes that share all initial layers and branch out at an adhoc point, or through separate task-speciﬁc networks with an additional feature sharing/fusion mechanism. Unlike existing methods, we propose an adaptive sharing approach, called AdaShare, that decides what to share across which tasks to achieve the best recognition accuracy, while taking resource efﬁciency into account.
Speciﬁcally, our main idea is to learn the sharing pattern through a task-speciﬁc policy that selectively chooses which layers to execute for a given task in the multi-task network. We efﬁciently optimize the task-speciﬁc policy jointly with the network weights, using standard back-propagation. Experiments on several challenging and diverse benchmark datasets with a variable number of tasks well demonstrate the efﬁcacy of our approach over state-of-the-art methods. Project page: https://cs-people.bu.edu/sunxm/AdaShare/project.html. 1

Introduction
Multi-task learning (MTL) focuses on simultaneously solving multiple related tasks and has attracted much attention in recent years. Compared with single-task learning, it can signiﬁcantly reduce the training and inference time, while improving generalization performance and prediction accuracy by learning a shared representation across related tasks [7, 56]. However, a fundamental challenge of
MTL is deciding what parameters to share across which tasks for efﬁcient learning of multiple tasks.
Most of the prior works rely on hand-designed architectures, usually composed of shared initial layers, after which all tasks branch out simultaneously at an adhoc point in the network (hard-parameter sharing) [23, 29, 43, 5, 26, 12]. However, there is a large number of possible options for tweaking such architectures, in fact, too large to tune an optimal conﬁguration manually, especially for deep neural networks with hundreds or thousands of layers. It is even more difﬁcult when the number of tasks grows and an improper sharing scheme across unrelated tasks may cause negative transfer, a severe problem in multi-task learning [52, 27]. Furthermore, it has been empirically observed that different sharing patterns tend to work best for different task combinations [39].
More recently, we see a shift of paradigm in deep multi-task learning, where a set of task-speciﬁc networks are used in combination with feature sharing/fusion for more ﬂexible multi-task learning (soft-parameter sharing) [39, 16, 48, 33, 49]. While this line of work has obtained reasonable accuracy on commonly used benchmark datasets, it is not computationally or memory efﬁcient, as the size of the model grows proportionally with respect to the number of tasks.
In this paper, we argue that an optimal MTL algorithm should not only achieve high accuracy on all tasks, but also restrict the number of new network parameters as much as possible as the number of tasks grows. This is extremely important for many resource-limited applications such as autonomous vehicles and mobile platforms that would beneﬁt from multi-task learning. Motivated by this, we 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: A conceptual overview of our approach. Consider a deep multi-task learning scenario with two tasks such as Semantic Segmentation (Seg) and Surface Normal Prediction (SN). Traditional hard-parameter sharing uses the same initial layers and splits the network into task-speciﬁc branches at an adhoc point (designed manually). On the other hand, Soft-parameter sharing shares features via a set of task-speciﬁc networks, which does not scale well as the number of tasks increases. In contrast, we propose AdaShare, a novel efﬁcient sharing scheme that learns separate execution paths for different tasks through a task-speciﬁc policy applied to a single multi-task network. Here, we show an example task-speciﬁc policy learned using AdaShare for the two tasks. wish to obtain the best utilization of a single network by exploring efﬁcient knowledge sharing across multiple tasks. Speciﬁcally, we ask the following question: Can we determine which layers in the network should be shared across which tasks and which layers should be task-speciﬁc to achieve the best accuracy/memory footprint trade-off for scalable and efﬁcient multi-task learning?
To this end, we propose AdaShare, a novel and differentiable approach for efﬁcient multi-task learning that learns the feature sharing pattern to achieve the best recognition accuracy, while restricting the memory footprint as much as possible. Our main idea is to learn the sharing pattern through a task-speciﬁc policy that selectively chooses which layers to execute for a given task in the multi-task network. In other words, we aim to obtain a single network for multi-task learning that supports separate execution paths for different tasks, as illustrated in Figure 1. As decisions to form these task-speciﬁc execution paths are discrete and non-differentiable, we rely on Gumbel
Softmax Sampling [25, 35] to learn them jointly with the network parameters through standard back-propagation, without using reinforcement learning (RL) [46, 62] or any additional policy network [1, 17]. We design the loss to achieve both competitive performance and resource efﬁciency required for multi-task learning. Additionally, we also present a simple yet effective training strategy inspired by the idea of curriculum learning [4], to facilitate the joint optimization of task-speciﬁc policies and network weights. Our results show that AdaShare outperforms state-of-the-art approaches, whilst being more parameter efﬁcient and therefore scaling more elegantly with the number of tasks.
The main contributions of our work are as follows:
• We propose a novel and differentiable approach for adaptively determining the feature sharing pattern across multiple tasks (what layers to share across which tasks) in deep multi-task learning.
• We learn the feature sharing pattern jointly with the network weights using standard back-propagation through Gumbel Softmax Sampling, making it highly efﬁcient. We also introduce two new loss terms for learning a compact multi-task network with effective knowledge sharing across tasks and a curriculum learning strategy to beneﬁt the optimization.
• We conduct extensive experiments on several MTL benchmarks (NYU v2 [40], CityScapes [11],
Tiny-Taskonomy [68], DomainNet [42], and text classiﬁcation datasets [8]) with variable number of tasks to demonstrate the superiority of our proposed approach over state-of-the-art methods. 2