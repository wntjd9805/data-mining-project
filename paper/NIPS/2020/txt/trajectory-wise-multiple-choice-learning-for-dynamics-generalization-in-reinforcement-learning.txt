Abstract
Model-based reinforcement learning (RL) has shown great potential in various control tasks in terms of both sample-efﬁciency and ﬁnal performance. However, learning a generalizable dynamics model robust to changes in dynamics remains a challenge since the target transition dynamics follow a multi-modal distribution. In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning, that learns a multi-headed dynamics model for dynamics generalization. The main idea is updating the most accurate prediction head to spe-cialize each head in certain environments with similar dynamics, i.e., clustering en-vironments. Moreover, we incorporate context learning, which encodes dynamics-speciﬁc information from past experiences into the context latent vector, enabling the model to perform online adaptation to unseen environments. Finally, to utilize the specialized prediction heads more effectively, we propose an adaptive planning method, which selects the most accurate prediction head over a recent experience.
Our method exhibits superior zero-shot generalization performance across a variety of control tasks, compared to state-of-the-art RL methods. Source code and videos are available at https://sites.google.com/view/trajectory-mcl. 1

Introduction
Deep reinforcement learning (RL) has exhibited wide success in solving sequential decision-making problems [23, 39, 45]. Early successful deep RL approaches had been mostly model-free, which do not require an explicit model of the environment, but instead directly learn a policy [25, 28, 38].
However, despite the strong asymptotic performance, the applications of model-free RL have largely been limited to simulated domains due to its high sample complexity. For this reason, model-based
RL has been gaining considerable attention as a sample-efﬁcient alternative, with an eye towards robotics and other physics domains.
The increased sample-efﬁciency of model-based RL algorithms is obtained by exploiting the structure of the problem: ﬁrst the agent learns a predictive model of the environment, and then plans ahead with the learned model [1, 37, 42]. Recently, substantial progress has been made on the sample-efﬁciency of model-based RL algorithms [5, 7, 22, 23, 24]. However, it has been evidenced that model-based RL algorithms are not robust to changes in the dynamics [20, 30], i.e., dynamics models fail to provide accurate predictions as the transition dynamics of environments change. This makes model-based
RL algorithms unreliable to be deployed into real-world environments where partially unspeciﬁed dynamics are common; for instance, a deployed robot might not know a priori various features of the terrain it has to navigate.
∗Equal Contribution. Correspondence to {younggyo.seo@kaist.ac.kr, kiminlee@berkeley.edu} 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.            
(a) Ant with crippled legs (b) Multi-modality in transition dynamics
Figure 1: (a) Examples from ant robots with crippled legs. (b) We visualize the next (x, y) positions of robots obtained by applying the same action with random noise to robots at the initial start position but with a different crippled leg.
As a motivating example, we visualize the next states obtained by crippling one of the legs of an ant robot (see Figure 1a). Figure 1b shows that the target transition dynamics follow a multi-modal distribution, where each mode corresponds to each leg of a robot, even though the original environment has deterministic transition dynamics. This implies that a model-based RL algorithm that can approximate the multi-modal distribution is required to develop a reliable and robust agent against changes in the dynamics. Several algorithms have been proposed to tackle this problem, e.g., learning contextual information to capture local dynamics [20], ﬁne-tuning model parameters for fast adaptation [30]. These algorithms, however, are limited in that they do not explicitly learn dynamics models that can approximate the multi-modal distribution of transition dynamics.
Contribution.
In this paper, we present a new model-based RL algorithm, coined trajectory-wise multiple choice learning (T-MCL), that can approximate the multi-modal distribution of transition dynamics in an unsupervised manner. To this end, we introduce a novel loss function, trajectory-wise oracle loss, for learning a multi-headed dynamics model where each prediction head specializes in different environments (see Figure 2a). By updating the most accurate prediction head over a trajectory segment (see Figure 2b), we discover that specialized prediction heads emerge automatically. Namely, our method can effectively cluster environments without any prior knowledge of environments. To further enable the model to perform online adaptation to unseen environments, we also incorporate context learning, which encodes dynamics-speciﬁc information from past experiences into the context latent vector and provides it as an additional input to prediction heads (see Figure 2a). Finally, to utilize the specialized prediction heads more effectively, we propose adaptive planning that selects actions using the most accurate prediction head over a recent experience, which can be interpreted as
ﬁnding the nearest cluster to the current environment (see Figure 2c).
We demonstrate the effectiveness of T-MCL on various control tasks from OpenAI Gym [3]. For evaluation, we measure the generalization performance of model-based RL agents on unseen (yet
In our experiments, T-MCL exhibits related) environments with different transition dynamics. superior generalization performance compared to existing model-based RL methods [4, 20, 30].
For example, compared to CaDM [20], a state-of-the-art model-based RL method for dynamics generalization, our method obtains 3.5x higher average return on the CrippledAnt environment. 2