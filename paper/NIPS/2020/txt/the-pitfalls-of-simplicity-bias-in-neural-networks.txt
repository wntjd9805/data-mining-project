Abstract
Several works have proposed Simplicity Bias (SB)—the tendency of standard training procedures such as Stochastic Gradient Descent (SGD) to ﬁnd simple models—to justify why neural networks generalize well [1, 49, 74]. However, the precise notion of simplicity remains vague. Furthermore, previous settings [67, 24] that use SB to justify why neural networks generalize well do not simultaneously capture the non-robustness of neural networks—a widely observed phenomenon in practice [71, 36]. We attempt to reconcile SB and the superior standard generaliza-tion of neural networks with the non-robustness observed in practice by introducing piecewise-linear and image-based datasets, which (a) incorporate a precise notion of simplicity, (b) comprise multiple predictive features with varying levels of sim-plicity, and (c) capture the non-robustness of neural networks trained on real data.
Through theoretical analysis and targeted experiments on these datasets, we make four observations: (i) SB of SGD and variants can be extreme: neural networks can exclusively rely on the simplest feature and remain invariant to all predictive complex features. (ii) The extreme aspect of SB could explain why seemingly benign distribution shifts and small adversarial perturbations signiﬁcantly degrade model performance. (iii) Contrary to conventional wisdom, SB can also hurt generalization on the same data distribution, as SB persists even when the simplest feature has less predictive power than the more complex features. (iv) Common approaches to improve generalization and robustness—ensembles and adversarial training—can fail in mitigating SB and its pitfalls.
Given the role of SB in training neural networks, we hope that the proposed datasets and methods serve as an effective testbed to evaluate novel algorithmic approaches aimed at avoiding the pitfalls of SB. 1

Introduction
Understanding the superior generalization ability of neural networks, despite their high capacity to ﬁt randomly labeled data [84], has been a subject of intense study. One line of recent work [67, 24] proves that linear neural networks trained with Stochastic Gradient Descent (SGD) on linearly separable data converge to the maximum-margin linear classiﬁer, thereby explaining the superior generalization performance. However, maximum-margin classiﬁers are inherently robust to perturbations of data at prediction time, and this implication is at odds with concrete evidence that neural networks, in practice, are brittle to adversarial examples [71] and distribution shifts [52, 58, 44, 65]. Hence, the linear setting, while convenient to analyze, is insufﬁcient to capture the non-robustness of neural networks trained on real datasets. Going beyond the linear setting, several works [1, 49, 74] argue that neural networks generalize well because standard training procedures have a bias towards learning 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
simple models. However, the exact notion of “simple" models remains vague and only intuitive.
Moreover, the settings studied are insufﬁcient to capture the brittleness of neural networks
Our goal is to formally understand and probe the simplicity bias (SB) of neural networks in a setting that is rich enough to capture known failure modes of neural networks and, at the same time, amenable to theoretical analysis and targeted experiments. Our starting point is the observation that on real-world datasets, there are several distinct ways to discriminate between labels (e.g., by inferring shape, color etc. in image classiﬁcation) that are (a) predictive of the label to varying extents, and (b) deﬁne decision boundaries of varying complexity. For example, in the image classiﬁcation task of white swans vs. bears, a linear-like “simple" classiﬁer that only looks at color could predict correctly on most instances except white polar bears, while a nonlinear “complex" classiﬁer that infers shape could have almost perfect predictive power. To systematically understand SB, we design modular synthetic and image-based datasets wherein different coordinates (or blocks) deﬁne decision boundaries of varying complexity. We refer to each coordinate / block as a feature and deﬁne a precise notion of feature simplicity based on the simplicity of the corresponding decision boundary.
Proposed dataset.
Figure 1 illustrates a stylized version of the proposed synthetic dataset with two features, φ1 and φ2, that can perfectly predict the label with 100% accuracy, but differ in simplicity.
The simplicity of a feature is precisely determined by the min-imum number of linear pieces in the decision boundary that achieves optimal classiﬁcation accuracy using that feature.
For example, in Figure 1, the simple feature φ1 requires a lin-ear decision boundary to perfectly predict the label, whereas complex feature φ2 requires four linear pieces. Along sim-ilar lines, we also introduce a collection of image-based datasets in which each image concatenates MNIST images (simple feature) and CIFAR-10 images (complex feature).
The proposed datasets, which incorporate features of varying predictive power and simplicity, allow us to systematically investigate and measure SB in SGD-trained neural networks.
Observations from new dataset. The ideal decision boundary that achieves high accuracy and robustness relies on all features to obtain a large margin (minimum distance from any point to decision boundary). For example, the orange decision boundary in Figure 1 that learns φ1 and φ2 attains 100% accuracy and exhibits more robustness than the linear boundary because of larger margin. Given the expressive power of large neural networks, one might expect that a network trained on the dataset in Figure 1 would result in the larger-margin orange piecewise linear boundary. However, in practice, we ﬁnd quite the opposite—trained neural networks have a linear boundary. Surprisingly, neural networks exclusively use feature φ1 and remain completely invariant to φ2. More generally, we observe that SB is extreme: neural networks simply ignore several complex predictive features in the presence of few simple predictive features. We ﬁrst theoretically show that one-hidden-layer neural networks trained on the piecewise linear dataset exhibit SB. Then, through controlled experiments, we validate the extreme nature of SB across model architectures and optimizers.
Figure 1: Simple vs. complex features
Implications of extreme SB. Theoretical analysis and controlled experiments reveal three major pitfalls of SB in the context of proposed synthetic and image-based datasets, which we conjecture to hold more widely across datasets and domains: (i) Lack of robustness: Neural networks exclusively latch on to the simplest feature (e.g., background) at the expense of very small margin and completely ignore complex predictive features (e.g., semantics of the object), even when all features have equal predictive power. This results in susceptibility to small adversarial perturbations (due to small margin) and spurious correlations (with simple features).
Furthermore, in Section 4, we provide a concrete connection between SB and data-agnostic and model-agnostic universal adversarial perturbations [47] observed in practice. (ii) Lack of reliable conﬁdence estimates: Ideally, a network should have high conﬁdence only if all predictive features agree in their prediction. However due to extreme SB, the network has high conﬁdence even if several complex predictive features contradict the simple feature, mirroring the widely reported inaccurate and substantially higher conﬁdence estimates reported in practice [51, 25]. (iii) Suboptimal generalization: Surprisingly, neural networks exclusively rely on the simplest feature even if it less predictive of the label than all complex features in the synthetic datasets. Consequently, contrary to conventional wisdom, extreme SB can hurt robustness as well as generalization. 2
In contrast, prior works [8, 67, 24] only extol SB by considering settings where all predictive features are simple and hence do not reveal the pitfalls observed in real-world settings. While our results on the pitfalls of SB are established in the context of the proposed datasets, the two design principles underlying these datasets—combining multiple features of varying simplicity & predictive power and capturing multiple failure modes of neural networks in practice—suggest that our conclusions could be justiﬁable more broadly.
Summary. This work makes two key contributions. First, we design datasets that offer a precise stratiﬁcation of features based on simplicity and predictive power. Second, using the proposed datasets, we provide theoretical and empirical evidence that neural networks exhibit extreme SB, which we postulate as a unifying contributing factor underlying key failure modes of deep learning: poor out-of-distribution performance, adversarial vulnerability and suboptimal generalization. To the best of our knowledge, prior works only focus on the positive aspect of SB: the lack of overﬁtting in practice. Additionally, we ﬁnd that standard approaches to improve generalization and robustness— ensembles and adversarial training—do not mitigate simplicity bias and its shortcomings on the proposed datasets. Given the important implications of SB, we hope that the datasets we introduce serve (a) as a useful testbed for devising better training procedures and (b) as a starting point to design more realistic datasets that are amenable to theoretical analysis and controlled experiments.
Organization. We discuss related work in Section 2. Section 3 describes the proposed datasets and metrics. In Section 4, we concretely establish the extreme nature of Simplicity Bias (SB) and its shortcomings through theory and empirics. Section 5 shows that extreme SB can in fact hurt generalization as well. We conclude and discuss the way forward in Section 6. 2