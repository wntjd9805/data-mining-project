Abstract
Ofﬂine reinforcement learning, wherein one uses off-policy data logged by a ﬁxed behavior policy to evaluate and learn new policies, is crucial in applications where experimentation is limited such as medicine. We study the estimation of policy value and gradient of a deterministic policy from off-policy data when actions are continuous. Targeting deterministic policies, for which action is a deterministic function of state, is crucial since optimal policies are always deterministic (up to ties). In this setting, standard importance sampling and doubly robust estimators for policy value and gradient fail because the density ratio does not exist. To circumvent this issue, we propose several new doubly robust estimators based on different kernelization approaches. We analyze the asymptotic mean-squared error of each of these under mild rate conditions for nuisance estimators. Speciﬁcally, we demonstrate how to obtain a rate that is independent of the horizon length. 1

Introduction
Ofﬂine reinforcement learning (RL), wherein one uses off-policy data logged by a ﬁxed behavior policy to evaluate and learn new policies, is crucial in applications where experimentation is limited (Farajtabar et al., 2018; Bibaut et al., 2019; Liu et al., 2018; Kallus and Uehara, 2019b). A key application is RL for healthcare (Gottesman et al., 2019; Murphy, 2003). Since it is not possible to collect new data, it is crucial to efﬁciently use the available data. Recent work on off-policy evaluation (OPE; Kallus and Uehara, 2020a, 2019a) have shown how efﬁciently taking advantage of problem structure, such as Markovianness and ergodicity, can improve OPE and tackle the well-known issue of OPE known as the curse of horizon (Liu et al., 2018). Kallus and Uehara (2020b) applied these advances to off-policy learning using a policy gradient approach, i.e., proposing efﬁcient off-policy estimators for the policy gradient and incorporating them into gradient ascent methods.
All the aforementioned methods, however, cannot be directly applied to the evaluation and learning of deterministic policies when actions are continuous since the density ratio (Radon-Nikodym derivative) does not exist: the behavior policy usually has zero mass on single actions (more generally, it can have at most countably-many atoms while the evaluation policy may take any of a continuum of actions). This question is important since the maximum-value policy is generally deterministic (up to ties between actions) so if one seeks optimal policies one should focus on deterministic ones. In the bandit setting (horizon of one action), several recent works tackle this problem (Bibaut and J. van der
Laan, 2017; Kallus and Zhou, 2018; Colangelo and Lee, 2019), but applying these methods in a straightforward manner to RL may lead to a bad convergence rate that deteriorates with horizon.
In this paper, we propose several doubly robust off-policy value and gradient estimators for deter-ministic policies in an RL setting. We analyze the asymptotic mean-squared error (MSE) of each
∗corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Comparison of off-policy value and gradient estimators for deterministic policies. Proposed estimators are typeset in bold. “MSE” is the convergence rate when nuisances are estimated at rate under “Rate,” irrespective of choice of nuisance estimators. “–” means the convergence rate depends on the choice of estimators. “Nuis” are nuisances for OPE. “Nuis+” are additional nuisances for policy gradient. “D/I” is whether differentiation (D) or integration (I) of ˆqt(st, at) wrt at is required. (a) OPE
MSE
Nuis
Rate –
DM n− 4
CDRD
CDRK n− 4
MDRD n− 4
MDRK n− 4 5 5
H+4
H+4 qπe
λK, qK
λπe
, qπe wK, qK
, qπe wπe
H+4
H+4 – n− 1 n− 1 n− 1 n− 1 5 5
MSE
Rate (b) Off-policy gradient
Nuis+ dqπe dλK dλπe dwK dwπe
, dqK
, dqπe
, dqK
, dqπe
H+6
H+6 7 – n− 1 n− 1 n− 1 n− 1 7 7
H+6
H+6 –
DPG n− 4
CPGD
CPGK n− 4
MPGD n− 4
MPGK n− 4 7
D/I
D
D
I
D
I estimator under extremely lax conditions that accommodate ﬂexible learning of the nuisances that appear in the estimator (such as q-functions). Speciﬁcally, we propose estimators of policy value and gradient with MSE convergence rate that does not deteriorate with horizon and the leading term’s coefﬁcient has only a polynomial dependence on horizon. These results are summarized in Table 1. 2 Preliminaries
Problem set up Consider an H-long time-varying Markov decision process (MDP), with states st ∈ St, actions at ∈ At, rewards rt ∈ [0, Rmax], initial state distribution p1(s1), transition distributions pt+1(st+1 | st, at), and reward distribution pt(rt | st, at), for t = 1, . . . , H. A policy
π = (πt(at | st))t≤H induces a distribution over trajectories T = (s1, a1, r1, . . . , sT , aH , rH ): pπ(T ) = p1(s1)π1(a1 | s1)p1(r1 | s1, a1) (cid:81)H t=2 pt(st | st−1, at−1)πt(at | st)pt(rt | st, at). (1)
In this paper, we focus on continuous actions, At ⊆ R. For brevity we focus on the univariate case; the extension to multivariate actions is straightforward. We are interested in the value, J =
Epπe [(cid:80)H t=1 rt], of a given policy, πe, called the evaluation policy. In particular, we consider the case where πe is deterministic in that it is given by maps τ = (τt)t≤H , τt : St → R such that t (at | st) = δ(at − τt(st)) is the Dirac measure at τt(st), meaning when we follow πe, at is a
πe function τt of st. When τ is parametrized as τ = τθ by some parameter θ ∈ Θ, we are also often interested in the policy gradient, Z = ∇θJ, as it can be used for policy learning via gradient ascent.
We often drop the subscript and understand ∇ to be with respect to (wrt ) θ. When studying policy gradient estimation, we will assume throughout that τθ,t(st) is almost surely differentiable in θ and that (cid:107)∇τt(st)(cid:107)op ≤ Υ for some Υ < ∞, where (cid:107) · (cid:107)op is the matrix operator norm. Additionally, in theoretical results, we will assume actions are bounded: support(πb t (· | st)) = [0, 1], τt(st) ∈ (0, 1).
In the ofﬂine setting, the data available to us for estimating J and Z consists only of trajectory observations from some different ﬁxed policy, πb, called the behavior policy:
T (cid:104)1(cid:105), . . . , T (cid:104)n(cid:105) ∼ pπb , T (cid:104)i(cid:105) = (S(cid:104)i(cid:105) 1 , A(cid:104)i(cid:105) 1 , R(cid:104)i(cid:105) 1 , · · · , S(cid:104)i(cid:105)
H , A(cid:104)i(cid:105)
H , R(cid:104)i(cid:105)
H ). (off-policy data) k=t rt | st, at], vπe t (st) = Epπe [(cid:80)H
Let wπe t (st) = pπe (st)/pπb(st) denote the marginal density ratio, where pπe (st), pπb(st) t (st, at) = are the marginal densities of st under pπe(T ), pπb(T ), respectively.
Epπe [(cid:80)H k=t rt | st] be πe’s q- and v-functions. We assume throughout that 1/πb t (st) ≤ C2. For clarity, we reserve capital letters for observed data, dropping superscripts (cid:104)i(cid:105) for a generic data point, and use lower case for generic MDP random variables. All expectations without subscripts are taken wrt pb
π. The empirical expectation is
Pnf = 1 i=1 f (T (cid:104)i(cid:105)). Deﬁne the L2 norm as (cid:107)f (T )(cid:107)2 = E[f (T 2)]1/2. We let ·(i) denote the i-th n order derivative wrt action alone and deﬁne the max Sobolev norm as (cid:107)f (cid:107)j,∞ = maxi=0,··· ,j (cid:107)f (j)(cid:107)∞.
We emphasize that, e.g., q(i)(st, at) refers to differentiating only wrt at alone. t (at | st) ≤ C1, wπe
Let qπe (cid:80)n 2
In developing estimators we will use a second-order kernel k : R → R, i.e.,
Integral kernels (cid:82) k(u)du = 1, (cid:82) uk(u)du = 0, M2(k) = (cid:82) u2k(u)du < ∞. We deﬁne Ω(i) 2 (k) = (cid:82) (k(i)(u))2du.
Given a bandwidth h, let Kh(u) = h−1k(u/h). Examples of differentiable kernels include Gaussian k(u) ∝ exp(−u2), biweight k(u) ∝ max(0, 1 − u2)2, and triweight k(u) ∝ max(0, 1 − u2)3.