Abstract
In this paper, we consider stochastic multi-armed bandits (MABs) with heavy-tailed rewards, whose p-th moment is bounded by a constant νp for 1 < p ≤ 2. First, we propose a novel robust estimator which does not require νp as prior information, while other existing robust estimators demand prior knowledge about νp. We show that an error probability of the proposed estimator decays exponentially fast. Using this estimator, we propose a perturbation-based exploration strategy and develop a generalized regret analysis scheme that provides upper and lower regret bounds by revealing the relationship between the regret and the cumulative density function of the perturbation. From the proposed analysis scheme, we obtain gap-dependent and gap-independent upper and lower regret bounds of various perturbations. We also ﬁnd the optimal hyperparameters for each perturbation, which can achieve the minimax optimal regret bound with respect to total rounds. In simulation, the proposed estimator shows favorable performance compared to existing robust estimators for various p values and, for MAB problems, the proposed perturbation strategy outperforms existing exploration methods. 1

Introduction
A multi-armed bandit (MAB) is a fundamental yet powerful framework to model a sequential decision making problem. In this problem, an intelligent agent continuously chooses an action and receives a noisy feedback in the form of a stochastic reward, but no information is provided for unselected actions. Then, the goal of the agent is to maximize cumulative rewards over time by identifying an optimal action which has the maximum reward. However, since MABs often assume that prior knowledge about rewards is not given, the agent faces an innate dilemma between gathering new information by exploring sub-optimal actions (exploration) and choosing the best action based on the collected information (exploitation). Designing an efﬁcient exploration algorithm for MABs is a long-standing challenging problem. The efﬁciency of the exploration method is measured by a cumulative regret which is the sum of differences between the maximum reward and the reward obtained at each round.
Early researches for stochastic MABs have been investigated under the sub-Gaussian assumption on a reward distribution, which has the exponential-decaying behavior. However, there remains a large class of distributions which are not covered by the sub-Gaussianity and are called heavy-tailed distributions. While there exist several methods for handling such heavy-tailed rewards [3, 17], these 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
methods have two main drawbacks. First, both methods utilize a class of robust reward estimators which require the prior knowledge about the bound on the moments of the rewards distributions, which is hardly available for practical problems. Furthermore, the algorithm proposed in [17] requires the gap information, which is the difference between the maximum and second-largest reward, to balance the exploration and exploitation. These features make the previous algorithms impractical since information about the bound or the gap is not accessible in general. Second, both methods have the sub-optimal gap-independent regret bound. Bubeck et al. [3] derive the lower bound of the regret for an arbitrary algorithm. However, the upper regret bound of the algorithms in [3, 17] does not match the lower regret bound. Thus, there exists a signiﬁcant gap between the upper and lower bound, which can be reduced potentially. These drawbacks motivate us to design an algorithm which requires less prior knowledge about rewards yet achieves an optimal efﬁciency.
In this paper, we propose a novel p-robust estimator which does not depend on prior information about the bound on the p-th moment p ∈ (1, 2]. Combined with this estimator, we develop a perturbed exploration method for heavy-tailed rewards. A perturbation-based exploration stochastically smooths a greedy policy by adding a random perturbation to the estimated rewards and selecting a greedy action based on the perturbed estimations; hence the distribution of the perturbation determines the trade-off between exploration and exploitation [8, 9]. We ﬁrst analyze the regret bound of general perturbation method. Notably, we show that, if the tail probability of perturbations decays slower than the error probability of the estimator, then the proposed analysis scheme provides both upper and lower regret bounds. By using this general analysis scheme, we show that the optimal regret bound can be achieved for a broad class of perturbations, including Weibull, generalized extreme value, Gamma, Pareto, and Fréchet distributions. Empirically, the p-robust estimator shows favorable performance compared to the truncated mean and median of mean, which belong to the class of robust estimators [3]. For MAB problems, we also show that the proposed perturbation methods generally outperform robust UCB [3] and DSEE [17], which is consistent with our theoretical results.
The main contribution of this paper can be summarized in four-folds. First, we derive the lower regret bound of robust UCB [3], which has the sub-optimal gap-independent regret bound. Second, we propose novel p-robust estimator which does not rely on prior information about the bound on the p-th moment of rewards and prove that its tail probability decays exponentially. Third, by combining the proposed estimator with the perturbation method, we develop a general regret analysis scheme by revealing the relationship between regret and cumulative density function of the perturbation. Finally, we show that the proposed strategy can achieve the optimal regret bound in terms of the number of rounds T , which is the ﬁrst algorithm achieving the minimax optimal rate under heavy-tailed rewards. 2 Preliminaries
Stochastic Multi-Armed Bandits with Heavy Tailed Rewards We consider a stochastic multi-armed bandit problem deﬁned as a tuple (A, {ra}) where A is a set of K actions, and ra ∈ [0, 1] is a mean reward for action a. For each round t, the agent chooses an action at based on its exploration strategy and, then, get a stochastic reward: Rt,a := ra + (cid:15)t,a where (cid:15)t,a is an independent and identically distributed noise with E [(cid:15)t,a] = 0 for all t and a. Note that ra and (cid:15)t,a are called the mean of reward and the noise of reward, respectively. ra is generally assumed to be unknown.
Then, the goal of the agent is to minimize the cumulative regret over total rounds T , deﬁned as
RT := (cid:80)T t=1 ra(cid:63) −Ea1:t [rat], where a(cid:63) := arg maxa∈A ra. The cumulative regret over T represents the performance of an exploration strategy. The smaller RT , the better exploration performance. To analyze RT , we consider the heavy-tailed assumption on noises whose p-th moment is bounded by a constant νp where p ∈ (1, 2], i.e., E|Rt,a|p ≤ νp for all a ∈ A. Without loss of generality, we regard p as the maximal order of the bounded moment, because, if the p-th moment is ﬁnite, then the moment with lower order is also ﬁnite automatically.
In this paper, we analyze both gap-dependent and gap-independent regret bounds. The gap-dependent bound is the upper regret bound depending on the gap information ∆a := ra(cid:63) − ra for a (cid:54)= a(cid:63) and, on the contrary, the gap-independent bound is the upper regret bound independent of the gap.