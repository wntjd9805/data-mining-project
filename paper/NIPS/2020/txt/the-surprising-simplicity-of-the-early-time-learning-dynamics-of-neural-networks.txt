Abstract
Modern neural networks are often regarded as complex black-box functions whose behavior is difﬁcult to understand owing to their nonlinear dependence on the data and the nonconvexity in their loss landscapes. In this work, we show that these common perceptions can be completely false in the early phase of learning. In particular, we formally prove that, for a class of well-behaved input distributions, the early-time learning dynamics of a two-layer fully-connected neural network can be mimicked by training a simple linear model on the inputs. We additionally argue that this surprising simplicity can persist in networks with more layers and with convolutional architecture, which we verify empirically. Key to our analysis is to bound the spectral norm of the difference between the Neural Tangent Kernel (NTK) at initialization and an afﬁne transform of the data kernel; however, unlike many previous results utilizing the NTK, we do not require the network to have disproportionately large width, and the network is allowed to escape the kernel regime later in training. 1

Introduction
Modern deep learning models are enormously complex function approximators, with many state-of-the-art architectures employing millions or even billions of trainable parameters [Radford et al., 2019, Adiwardana et al., 2020]. While the raw parameter count provides only a crude approximation of a model’s capacity, more sophisticated metrics such as those based on PAC-Bayes [McAllester, 1999, Dziugaite and Roy, 2017, Neyshabur et al., 2017b], VC dimension [Vapnik and Chervonenkis, 1971], and parameter norms [Bartlett et al., 2017, Neyshabur et al., 2017a] also suggest that modern architectures have very large capacity. Moreover, from the empirical perspective, practical models are ﬂexible enough to perfectly ﬁt the training data, even if the labels are pure noise [Zhang et al., 2017]. Surprisingly, these same high-capacity models generalize well when trained on real data, even without any explicit control of capacity.
These observations are in conﬂict with classical generalization theory, which contends that models of intermediate complexity should generalize best, striking a balance between the bias and the variance of their predictive functions. To reconcile theory with observation, it has been suggested that deep neural networks may enjoy some form of implicit regularization induced by gradient-based training algorithms that biases the trained models towards simpler functions. However, the exact notion of simplicity and the mechanism by which it might be achieved remain poorly understood except in certain simplistic settings.
∗Princeton University. Work partly performed at Google. Email: huwei@cs.princeton.edu
†Google Research, Brain Team. Email: xlc@google.com
‡Google Research, Brain Team. Work done as a member of the Google AI Residency program (http:
//g.co/brainresidency). Email: adlam@google.com
§Google Research, Brain Team. Email: jpennin@google.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
One concrete mechanism by which such induced simplicity can emerge is the hypothesis that neural networks learn simple functions early in training, and increasingly build up their complexity in later time. In particular, recent empirical work Nakkiran et al. [2019] found that, intriguingly, in some natural settings the simple function being learned in the early phase may just be a linear function of the data.
In this work, we provide a novel theoretical result to support this hypothesis. Speciﬁcally, we formally prove that, for a class of well-behaved input distributions, the early-time learning dynamics of gradient descent on a two-layer fully-connected neural network with any common activation can be mimicked by training a simple model of the inputs. When training the ﬁrst layer only, this simple model is a linear function of the input features; when training the second layer or both layers, it is a linear function of the features and their (cid:96)2 norm. This result implies that neural networks do not fully exercise their nonlinear capacity until late in training.
Key to our technical analysis is a bound on the spectral norm of the difference between the Neural
Tangent Kernel (NTK) [Jacot et al., 2018] of the neural network at initialization and that of the linear model; indeed, a weaker result, like a bound on the Frobenius norm, would be insufﬁcient to establish our result. Although the NTK is usually associated with the study of ultra-wide networks, our result only has a mild requirement on the width and allows the network to leave the kernel regime later in training. While our formal result focuses on two-layer fully-connected networks and data with benign concentration properties (speciﬁed in Assumption 3.1), we argue with theory and provide empirical evidence that the same linear learning phenomenon persists for more complex architectures and real-world datasets.