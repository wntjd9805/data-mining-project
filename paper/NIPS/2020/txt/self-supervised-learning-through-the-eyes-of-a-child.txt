Abstract
Within months of birth, children develop meaningful expectations about the world around them. How much of this early knowledge can be explained through generic learning mechanisms applied to sensory data, and how much of it requires more substantive innate inductive biases? Addressing this fundamental question in its full generality is currently infeasible, but we can hope to make real progress in more narrowly deﬁned domains, such as the development of high-level visual categories, thanks to improvements in data collecting technology and recent progress in deep learning. In this paper, our goal is precisely to achieve such progress by utilizing modern self-supervised deep learning methods and a recent longitudinal, egocentric video dataset recorded from the perspective of three young children (Sullivan et al., 2020). Our results demonstrate the emergence of powerful, high-level visual representations from developmentally realistic natural videos using generic self-supervised learning objectives. 1

Introduction
Experimental evidence suggests that even very young children have wide-ranging and sophisticated knowledge about the world around them. For example, within the ﬁrst few months of life, infants show meaningful expectations about objects and agents (Spelke and Kinzler, 2007). Similarly, well before learning to speak, infants can discriminate between many common categories; at 3-4 months, infants can discriminate simple shapes (Bomba and Siqueland, 1983) and animal classes (Quinn et al., 1993), preferring to look at exemplars from a novel class (e.g., bird) after observing exemplars from a different class (dogs). Yet, the origin of this early knowledge is often unclear. How much of this early knowledge can be learned by relatively generic learning architectures receiving sensory data through the eyes of a developing child, and how much of it requires more substantive inductive biases?
This is, of course, a modern reformulation of the age-old nature vs. nurture question that is central in psychology. Answering this question requires both a precise characterization of the sensory data received by humans during development and determining what generic models can learn from this data without assuming strong priors. Although addressing this question in its full generality would require unprecedentedly large and rich datasets and hence still remains out of reach, we can hope to make real progress in more narrowly deﬁned domains, such as the development of visual categories, thanks to new large-scale developmental datasets (Sullivan et al., 2020; Smith and Slone, 2017;
Bambach et al., 2018) and the recent progress in deep learning methods.
In this paper, our goal is precisely to achieve such progress by utilizing modern self-supervised deep learning techniques (He et al., 2019; Chen et al., 2020a) and a recent longitudinal egocentric dataset of headcam videos (SAYCam) recorded from the perspective of three developing children (Sullivan et al., 2020). The scale and longitudinal nature of this dataset allows us to train a large-scale model
“through the eyes” of individual developing children; in this case, based on ∼150-200 hours of video sampled regularly from 6 months to 32 months of age. Our choice of self-supervised learning avoids extra supervision that a child would not have access to; training only on data from individual children 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
ensures a strict subset of actual developmental experience. We trained self-supervised models on raw unlabeled videos, with the goal of extracting useful high-level visual representations. The acquired visual representations were then evaluated based on their ability to distinguish common visual categories in the child’s environment, using only linear readouts. Our results demonstrate, for the
ﬁrst time, the emergence of powerful, high-level visual representations from natural videos collected from a child’s perspective, using generic self-supervised learning methods. More speciﬁcally, we show that these emergent visual representations are powerful enough to support (i) high accuracy in non-trivial visual categorization tasks that are behaviorally relevant for a child, (ii) invariance to natural transformations, and (iii) generalization to unseen category exemplars from a handful of training exemplars. 2