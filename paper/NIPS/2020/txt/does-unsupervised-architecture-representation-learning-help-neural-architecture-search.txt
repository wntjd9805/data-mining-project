Abstract
Existing Neural Architecture Search (NAS) methods either encode neural archi-tectures using discrete encodings that do not scale well, or adopt supervised learning-based methods to jointly learn architecture representations and optimize architecture search on such representations which incurs search bias. Despite the widespread use, architecture representations learned in NAS are still poorly under-stood. We observe that the structural properties of neural architectures are hard to preserve in the latent space if architecture representation learning and search are coupled, resulting in less effective search performance. In this work, we ﬁnd empirically that pre-training architecture representations using only neural archi-tectures without their accuracies as labels improves the downstream architecture search efﬁciency. To explain this ﬁnding, we visualize how unsupervised archi-tecture representation learning better encourages neural architectures with similar connections and operators to cluster together. This helps map neural architectures with similar performance to the same regions in the latent space and makes the transition of architectures in the latent space relatively smooth, which considerably beneﬁts diverse downstream search strategies. 1

Introduction
Unsupervised representation learning has been successfully used in a wide range of domains including natural language processing [1, 2, 3], computer vision [4, 5], robotic learning [6, 7], and network analysis [8, 9]. Although differing in speciﬁc data type, the root cause of such success shared across domains is learning good data representations that are independent of the speciﬁc downstream task.
In this work, we investigate unsupervised representation learning in the domain of neural architecture search (NAS), and demonstrate how NAS search spaces encoded through unsupervised representation learning could beneﬁt the downstream search strategies.
Standard NAS methods encode the search space with the adjacency matrix and focus on designing different downstream search strategies based on reinforcement learning [10], evolutionary algorithm
[11], and Bayesian optimization [12] to perform architecture search in discrete search spaces. Such discrete encoding scheme is a natural choice since neural architectures are discrete. However, the size of the adjacency matrix grows quadratically as search space scales up, making downstream architecture search less efﬁcient in large search spaces [13]. To reduce the search cost, recent NAS methods employ dedicated networks to learn continuous representations of neural architectures and perform architecture search in continuous search spaces [14, 15, 16, 17]. In these methods, architecture representations and downstream search strategies are jointly optimized in a supervised manner, guided by the accuracies of architectures selected by the search strategies. However, these supervised architecture representation learning-based methods are biased towards weight-free operations (e.g., skip connections, max-pooling) which are often preferred in the early stage of the search process, resulting in lower ﬁnal accuracies [18, 19, 20, 21]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Supervised architecture representation learning (top): the supervision signal for repre-sentation learning comes from the accuracies of architectures selected by the search strategies. arch2vec (bottom): disentangling architecture representation learning and architecture search through unsupervised pre-training.
In this work, we propose arch2vec, a simple yet effective neural architecture search method based on unsupervised architecture representation learning. As illustrated in Figure 1, compared to supervised architecture representation learning-based methods, arch2vec circumvents the bias caused by joint optimization through decoupling architecture representation learning and architecture search into two separate processes. To achieve this, arch2vec uses a variational graph isomorphism autoencoder to learn architecture representations using only neural architectures without their accuracies. As such, it injectively captures the local structural information of neural architectures and makes architectures with similar structures (measured by edit distance) cluster better and distribute more smoothly in the latent space, which facilitates the downstream architecture search. We visualize the learned architecture representations in §4.1. It shows that architecture representations learned by arch2vec can better preserve structural similarity of local neighborhoods than its supervised architecture representation learning counterpart. In particular, it is able to capture topology (e.g. skip connections or straight networks) and operation similarity, which helps cluster architectures with similar accuracy.
We follow the NAS best practices checklist [22] to conduct our experiments. We validate the performance of arch2vec on three commonly used NAS search spaces NAS-Bench-101 [23], NAS-Bench-201 [24] and DARTS [15] and two search strategies based on reinforcement learning (RL) and Bayesian optimization (BO). Our results show that, with the same downstream search strategy, arch2vec consistently outperforms its discrete encoding and supervised architecture representation learning counterparts across all three search spaces.
Our contributions are summarized as follows:
• We propose a neural architecture search method based on unsupervised representation learning that decouples architecture representation learning and architecture search.
• We show that compared to supervised architecture representation learning, pre-training architecture representations without using their accuracies is able to better preserve the local structure relationship of neural architectures and helps construct a smoother latent space.
• The pre-trained architecture embeddings considerably beneﬁt the downstream architecture search in terms of efﬁciency and robustness. This ﬁnding is consistent across three search spaces, two search strategies and two datasets, demonstrating the importance of unsupervised architecture representation learning for neural architecture search.
The implementation of arch2vec is available at https://github.com/MSU-MLSys-Lab/arch2vec. 2
2