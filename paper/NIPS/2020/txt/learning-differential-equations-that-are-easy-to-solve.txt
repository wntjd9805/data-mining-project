Abstract
Differential equations parameterized by neural networks become expensive to solve numerically as training progresses. We propose a remedy that encourages learned dynamics to be easier to solve. Speciﬁcally, we introduce a differentiable surrogate for the time cost of standard numerical solvers, using higher-order derivatives of solution trajectories. These derivatives are efﬁcient to compute with Taylor-mode automatic differentiation. Optimizing this additional objective trades model performance against the time cost of solving the learned dynamics. We demonstrate our approach by training substantially faster, while nearly as accurate, models in supervised classiﬁcation, density estimation, and time-series modelling tasks. 1

Introduction
Differential equations describe a system’s behavior by specifying its instantaneous dynamics. Histori-cally, differential equations have been derived from theory, such as Newtonian mechanics, Maxwell’s equations, or epidemiological models of infectious disease, with parameters inferred from observations.
Solutions to these equations usually cannot be ex-pressed in closed-form, requiring numerical approx-imation.
Recently, ordinary differential equations parameter-ized by millions of learned parameters, called neural
ODEs, have been ﬁt for latent time series models, density models, or as a replacement for very deep neural networks (Rubanova et al., 2019; Grathwohl et al., 2019; Chen et al., 2018). These learned models are not constrained to match a theoretical model, only to optimize an objective on observed data. Learned models with nearly indistinguishable predictions can have substantially different dynamics. This raises the possibility that we can ﬁnd equivalent models that are easier and faster to solve. Yet standard training methods have no way to penalize the complexity of the dynamics being learned.
∗Equal Contribution. Code available at:
Figure 1: Top: Trajectories of an ODE ﬁt to map z(t1) = z(t0) + z(t0)3. The learned dy-namics are unnecessarily complex and require many evaluations (black dots) to solve.
Bottom: Regularizing the third total deriva-tive d3z(t) (shown by colour) gives dynamics dt3 that ﬁt the same map, but require fewer evalu-ations to solve. github.com/jacobjinkelly/easy-neural-ode 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
How can we learn dynamics that are faster to solve numerically without substantially changing their predictions? Much of the computational advantages of a continuous-time formulation come from using adaptive solvers, and most of the time cost of these solvers comes from repeatedly evaluating the dynamics function, which in our settings is a moderately-sized neural network. So, we’d like to reduce the number of function evaluations (NFE) required for these solvers to reach a given error tolerance. Ideally, we would add a term penalizing the NFE to the training objective, and let a gradient-based optimizer trade off between solver cost and predictive performance. But because NFE is integer-valued, we need to ﬁnd a differentiable surrogate.
The NFE taken by an adaptive solver depends on how far it can extrapolate the trajectory forward without introducing too much error. For example, for a standard adaptive-step Runge-Kutta solver with order m, the step size is approximately inversely proportional to the norm of the local mth total derivative of the solution trajectory with respect to time. That is, a larger mth derivative leads to a smaller step size and thus more function evaluations. Thus, we propose to minimize the norm of this total derivative during training, as a way to control the time required to solve the learned dynamics.
In this paper, we investigate the effect of this speed regularization in various models and solvers.
We examine the relationship between the solver order and the regularization order, and characterize the tradeoff between speed and performance. In most instances, we ﬁnd that solver speed can be approximately doubled without a substantial increase in training loss. We also provide an extension to the JAX program transformation framework that provides Taylor-mode automatic differentiation, which is asymptotically more efﬁcient for computing the required total derivatives than standard nested gradients.
Our work compares against and generalizes that of Finlay et al. (2020), who proposed regularizing dynamics in the FFJORD density estimation model, and showed that it stabilized dynamics enough in that setting to allow the use of ﬁxed-step solvers during training. 2