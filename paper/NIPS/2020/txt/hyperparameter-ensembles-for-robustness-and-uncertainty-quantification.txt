Abstract
Ensembles over neural network weights trained from different random initialization, known as deep ensembles, achieve state-of-the-art accuracy and calibration. The recently introduced batch ensembles provide a drop-in replacement that is more parameter efﬁcient. In this paper, we design ensembles not only over weights, but over hyperparameters to improve the state of the art in both settings. For best performance independent of budget, we propose hyper-deep ensembles, a simple procedure that involves a random search over different hyperparameters, themselves stratiﬁed across multiple random initializations. Its strong performance highlights the beneﬁt of combining models with both weight and hyperparameter diversity. We further propose a parameter efﬁcient version, hyper-batch ensembles, which builds on the layer structure of batch ensembles and self-tuning networks.
The computational and memory costs of our method are notably lower than typical ensembles. On image classiﬁcation tasks, with MLP, LeNet, ResNet 20 and Wide
ResNet 28-10 architectures, we improve upon both deep and batch ensembles. 1

Introduction
Neural networks are well-suited to form ensembles of
Indeed, neural networks trained from models [30]. different random initialization can lead to equally well-performing models that are nonetheless diverse in that they make complementary errors on held-out data [30].
This property is explained by the multi-modal nature of their loss landscape [24] and the randomness induced by both their initialization and the stochastic methods commonly used to train them [8, 38, 9].
Many mechanisms have been proposed to further foster diversity in ensembles of neural networks, e.g., based on cyclical learning rates [36] or Bayesian analysis [17].
In this paper, we focus on exploiting the diversity in-duced by combining neural networks deﬁned by dif-ferent hyperparameters. This concept is already well-established [13] and the auto-ML community actively applies it [21, 65, 53, 46]. We build upon this research with the following two complementary goals.
Figure 1: Comparison of our hyper-deep ensemble with deep ensemble for different ensemble sizes using a Wide ResNet 28-10 over CIFAR-100. Combining models with different hyperparameters is beneﬁcial.
First, for performance independent of computational and memory budget, we seek to improve upon deep ensembles [43], the current state-of-the-art ensembling method in terms of robustness and uncertainty quantiﬁcation [64, 28]. To this end, we develop a simple stratiﬁcation scheme which combines random search and the greedy selection of hyperparameters from [13] with the beneﬁt 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of multiple random initializations per hyperparameter like in deep ensembles. Figure 1 illustrates our algorithm for a Wide ResNet 28-10 where it leads to substantial improvements, highlighting the beneﬁts of combining different initialization and hyperparameters.
Second, we seek to improve upon batch ensembles [69], the current state-of-the-art in efﬁcient ensem-bles. To this end, we propose a parameterization combining that of [69] and self-tuning networks [52], which enables both weight and hyperparameter diversity. Our approach is a drop-in replacement that outperforms batch ensembles and does not need a separate tuning of the hyperparameters. 1.1