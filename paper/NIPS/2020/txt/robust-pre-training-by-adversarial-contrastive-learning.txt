Abstract
Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness [1]. In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework [2], which learns repre-sentations by maximizing feature consistency under differently augmented views.
This ﬁts particularly well with the goal of adversarial robustness, as one cause of ad-versarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efﬁcient and robust. We empirically evaluate the proposed Adversarial Con-trastive Learning (ACL) and show it can consistently outperform existing methods.
For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach [1] by 2.99% on robust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled exam-ples are available. Our codes and pre-trained models have been released at: https:
//github.com/VITA-Group/Adversarial-Contrastive-Learning. 1

Introduction
Label efﬁciency and model robustness are two desirable characteristics when it comes to building machine learning models, including the training of deep neural networks. Traditional supervised learning of deep networks requires a lot of labeled data, whose annotation costs can be prohibitively high. Utilizing unlabeled data, e.g., by various unsupervised or semi-supervised learning techniques, is thus of booming interests. One particular branch of unsupervised learning based on contrastive learning has shown great promise recently [3, 4, 5, 6, 7, 2]. The latest contrastive learning framework
[2] can largely improves label efﬁciency of the deep networks, e.g., surpassing the fully-supervised
AlexNet [8] when ﬁne-tuned on only 1% of the labels.
The labeling scarcity is even ampliﬁed when we come to adversarially robust deep learning [9], i.e., to training deep models that are not fooled by maliciously crafted, although imperceivable perturbations. As suggested in [10], the sample complexity of adversarially robust generalization is signiﬁcantly higher than that of standard learning. Recent results [11, 12, 13] advocated unlabeled data to be a powerful backup for training adversarially robust models as well, by using unlabeled data to form an auxiliary loss (e.g., a label-independent robust regularizer or a pseudo-label loss).
Only lately, researchers start linking self-supervised learning to adversarial robustness. [14] proposed a multi-task learning framework that incorporates a self-supervised objective to be co-optimized with the conventional classiﬁcation loss. The latest work [1] introduced adversarial training [15] into self-supervision, to provide general-purpose robust pre-trained models for the ﬁrst time. However, existing work is based on ad-hoc “pretext” self-supervised tasks, and as shown in [6, 2], the learned 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of workﬂow comparison: (a) The original SimCLR framework [2], a.k.a., stan-dard to standard (no adversarial attack involved); (b) - (d) three proposed variants of our adversarial contrastive learning framework: A2A, A2S, and DS (our best solution). Note that, whenever more than one encoder branches co-exist in one framework, they by default share all weights, except that adversarial and standard encoders will use independent BN parameters. unsupervised representations can be largely improved with contrastive learning, a new family of approaches for self-supervised learning.
In order to learn data-efﬁcient robust models, we propose to integrate contrastive learning with adver-sarially robust deep learning. Our work is inspired by recent success of contrastive learning, where the model is learned by maximizing consistency of data representations under differently augmented views [2]. This ﬁts particularly well with adversarial training, as one cause of adversarial fragility could be attributed to the non-smooth feature space near samples, i.e., small input perturbations can result in large feature variations and even label change.
By properly combining adversarial learning and contrastive pre-training (i.e. SimCLR [2]), we could achieve the desirable feature consistency. The resultant unsupervised pre-training framework, called Adversarial Contrastive Learning (ACL), is thoroughly discussed in Section 2. As the pre-training plays an increasingly important role for adversarial training [1], by leveraging more powerful contrastive pre-training of unsupervised representations, we further contribute to pushing forward the state-of-the-art adversarial robustness and accuracy.
On CIFAR-10/CIFAR-100, we extensively benchmark our pre-trained models on two adversarial
ﬁne-tuning settings: fully-supervised (using all training data and their labels) and semi-supervised (using all training data, but only partial/few shot labels). ACL pre-training leads to new state-of-the-art in both robust and standard accuracies. For example, in the fully-supervised setting, we obtain 2.99% robust and 2.14% standard accuracy improvements, over the previous best adversarial pre-training method [1] on CIFAR-10. For the semi-supervised setting, our approach surpasses the state-of-the-art method [11], by 0.66% robust and 5.87% standard accuracy, when 10% labels are available; that gains can become even more remarkable when we go further to extremely low label rates, e.g., 1%. 2 Our approach 2.1 Preliminaries
Adversarial Training Deep networks are notorious for the vulnerability to adversarial attacks [16].
Numerous approaches have been proposed to enhance model adversarial robustness [17, 18, 19, 20, 21, 15, 22, 23, 24, 25]. Among them, Adversarial Training (AT) based methods [15] remain to be the strongest defense option. Deﬁne the perturbation 𝜹, using ℓ∞ attack for example:
𝜹 = arg max (cid:107) 𝛿(cid:48) (cid:107)∞ ≤ 𝜖
ℓ(𝜽, 𝒙 + 𝜹(cid:48)) 2 (1)
where 𝜽 represents the parameters of a model. 𝒙 denotes a given sample. ℓ denotes the loss with parameter 𝜽, and the adversarial perturbation 𝜹. AT solves the following (minimax) optimization:
E𝒙 ∈ D (ℓ(𝜽, 𝒙 + 𝜹)) min
𝜽 (2) where D denotes the training set. The standard training (ST) could be viewed as a special case 𝜖 = 0.
Unsupervised Adversarial Training Among the few existing works [11, 12] utilizing unlabeled data, we introduce the Unsupervised Adversarial Training (UAT) algorithm [11] which yields the state-of-the-art performance so far. The authors introduce three variants: (i) UAT with Online Targets, which exploits a smoothness regularizer on the unlabeled data to minimize the divergence between a normal sample with its adversarial counter-part; (ii) UAT with Fixed Targets, which ﬁrst trains a
CNN using standard training (no AT applied) and then uses the trained model to supply pseudo labels for unlabeled data, ﬁnally combining them with labeled data towards AT altogether; and (iii) UAT++, which combines both types of unsupervised losses and was shown to outperform either individually.
Contrastive Pretraining We build this work upon SimCLR [2], a simple yet powerful contrastive pretraining framework for unsupervised representation learning. The main idea of SimCLR is to learn representations by maximizing agreements of differently augmented views of the same image. More speciﬁcally, its workﬂow is illustrated in Fig. 1 (a). Considering an unlabeled sample 𝒙, SimCLR ﬁrst augments the sample to be ˜𝒙𝑖 and ˜𝒙 𝑗 , with two separate data augmentation operators sampled from the same family of augmentations T (practically, the composition of random cropping and random color distortion). The pair ( ˜𝒙𝑖, ˜𝒙 𝑗 ) are then processed by the network backbone (denoted by the two weight-sharing standard encoders in Fig. 1 (a)), outputting features ( ˜𝒛𝑖, ˜𝒛 𝑗 ). After passing through a nonlinear projection head (omitted in the plot), the transformed feature pairs are optimized under a contrastive loss ℓ𝑁 𝑇 (NT-Xent) to maximize their agreement. Upon the completion of training, we only keep the encoder part as the pre-trained model for downstream tasks. 2.2 Adversarial Contrastive Learning as pre-training: three options
SimCLR can be considered as Standard-to-Standard (S2S) contrasting, as both branches are natural images, and adversarial training nor attack is involved. In our proposed method, Adversarial Contrast
Learning (ACL), we inject robustness components into the SimCLR framework, and discuss three candidate algorithms (Fig. 1 (b) - (d)).
Adversarial-to-Adversarial (A2A) The ﬁrst option is to directly inject two adversarial attacks after random data augmentations. We ﬁrst generate (𝜹𝑖, 𝜹 𝑗 ) as adversarial perturbations corresponding to the augmented samples ( ˜𝒙𝑖, ˜𝒙 𝑗 ), using the PGD attack algorithm [15], respectively. Here the loss ℓ for generating perturbations is the contrastive loss (NT-Xent). In other words, we now generate two views that are adversarially enforced to be dissimilar, on which we learn to enforce consistency. In this way, the contrastive learning is now performed on ( ˜𝒙𝑖 + 𝜹𝑖, ˜𝒙 𝑗 + 𝜹 𝑗 ) instead of ( ˜𝒙𝑖, ˜𝒙 𝑗 ), enforcing the representations to be invariant to cascaded standard-and-adversarial input perturbations. Note that this could also be viewed as performing attacks with random initialization [26].
Adversarial-to-Standard (A2S) The second option we explore is to replace ( ˜𝒙𝑖 + 𝜹𝑖, ˜𝒙 𝑗 + 𝜹 𝑗 ) with ( ˜𝒙𝑖 + 𝜹𝑖, ˜𝒙 𝑗 ), i.e., alleviating the encoder’s workload of standing against two simultaneous attacks.
Intuitive as it might look, there is a pitfall for A2S implementation. In both SimCLR (S2S) and A2A, their two backbones completely share all parameters. In A2S, while we still hope the adversarial and standard encoder branches to share all convolutional weights, it is important to allow both encoders to maintain their own independent batch normalization (BN) parameters.
Why this matters? As observed in [27, 28], the deep network feature statistics of clean samples ˜𝒙 and adversarial samples ˜𝒙 + 𝜹 can be very distinct. Mixing the two sets and normalizing using one set of parameters will compromise both robust and standard accuracy in image classiﬁcation. We also veriﬁed this in our setting, that using one BN for standard and adversarial features will be detrimental to unsupervised learning too. We thus turn to the dual Batch Normalization (BN) suggested in [27], i.e., one BN for standard encoder branch and the other for adversarial branch, respectively.
During ﬁne-tuning and testing, we by default employ the BN for adversarial branch, since it leads to higher robustness which is our main goal. We also brieﬂy study the effect of using either BN on the
ﬁne-tuning performance as well as the pre-trained representations in Sec. 3.2. 3
Algorithm 1: Algorithm of Dual Stream (DS) Pretraining
Input :A set of clean images 𝒙; Augmentation family T ; Network backbone and projection head 𝑓 , 𝑔;
Result :Standard BN parameters 𝜽 bn; Adversarial branch BN parameters 𝜽 bnadv ; The rest parameters 𝜽 in 𝑓 and 𝑔; for sampled mini-batch 𝒙 do
Augment 𝒙 to be ( ˜𝒙𝑖, ˜𝒙 𝑗 ) with two augmentations sampled from T .
Generate the corresponding adversarial mini-batch ( ˜𝒙𝑖 + 𝜹𝑖, ˜𝒙 𝑗 + 𝜹 𝑗 ) with
ℓ𝑁 𝑇 ( 𝑓 ◦ 𝑔( ˜𝒙𝑖 + 𝜹𝑖, ˜𝒙 𝑗 + 𝜹 𝑗 ; 𝜽, 𝜽 bnadv ))
𝜹𝑖, 𝜹 𝑗 = arg max (cid:107) 𝛿𝑖 (cid:107)∞ ≤ 𝜖 , (cid:107) 𝛿 𝑗 (cid:107)∞ ≤ 𝜖
ℓ = ℓ𝑁 𝑇 ( 𝑓 ◦ 𝑔( ˜𝒙𝑖, ˜𝒙 𝑗 ; 𝜽, 𝜽 bn)) + 𝛼ℓ𝑁 𝑇 ( 𝑓 ◦ 𝑔( ˜𝒙𝑖 + 𝜹𝑖, ˜𝒙 𝑗 + 𝜹 𝑗 ; 𝜽, 𝜽 bnadv ))
Update parameters (𝜽 bn, 𝜽 bnadv, 𝜽) to minimize ℓ. end
Dual Stream (DS) The third option we explore is to combine S2S and A2A in one. Speciﬁcally, for each input 𝒙, we augment it into twice (creating four augmented views): ( ˜𝒙𝑖, ˜𝒙 𝑗 ) by standard augmentations (as in S2S), and ( ˜𝒙𝑖 + 𝜹𝑖, ˜𝒙 𝑗 + 𝜹 𝑗 ) further with adversarial perturbation (as in A2A).
Our ﬁnal unsupervised loss consists of a contrastive loss term on the former pair (through two standard branches) and another contrastive loss term on the latter pair (through two adversarial branches).
The two terms are by default equally weighted (𝛼 = 1 in Algorithm. 1). The four branches share all convolutional weights except BN: the two standard branches use the same set of BN parameters, while the two adversarial branches have their one set of BN parameters too.
Compared to A2A which aggressively demands minimizing the “worst-case” consistencies, the ﬁnal dual-stream loss is aligned with the classical AT’s objective [15], i.e., balancing between standard and worst-case (robust) loss terms. The algorithm of DS can be found in Algorithm. 1. We experimentally verify that Dual-Stream is the best among the three ACL variants. A comprehensive comparison of the three variants is presented in Section 3.2. 2.2.1 Towards supervised ﬁne-tuning and semi-supervised training
For each of the three variants, we will use their pre-trained network weights as the initialization for subsequent ﬁne-tuning. For the fully supervised setting, we identically follow the adversarial
ﬁne-tuning scheme described in [29].
For semi-supervised training, we employ a three-step routine: i) we ﬁrst perform ACL to obtain a pre-trained model (denoted as PM) on all unlabeled data ; ii) we next adopt PM as the initialization to perform standard training using only labeled data, and use the trained model to generate pseudo-labels for all unlabeled data; iii) lastly, we leverage PM as initialization again, and perform AT over all data (including unlabeled data and their pseudo labels). The routine in principle follows [11, 30, 12] except the new pre-training step (i). The speciﬁc loss that we minimize in step iii is:
ℓ𝑠 = 1
|𝑁𝑙 | + |𝑁𝑢 | (𝛼ℓ𝐶𝐸 ( ˆ𝒙𝑙, 𝑦𝑙; 𝜽) + (1 − 𝛼)𝑇 2ℓ𝑑𝑖𝑠𝑡𝑖𝑙𝑙 ( ˆ𝒙𝑙, 𝑝𝑙, 𝑇; 𝜽)+
𝑇 2ℓdistill( ˆ𝒙𝑢, 𝑝𝑢, 𝑇; 𝜽) + 1
𝜆
KL( ˆ𝑝(𝒙; 𝜽), ˆ𝑝( ˆ𝒙; 𝜽))). (3)
Here 𝑦𝑙 denotes the ground truth of labeled data 𝒙𝑙. 𝑝𝑙 and 𝑝𝑢 represent the soft logits predicted by the step ii trained model, for labeled data 𝒙𝑙 and unlabeled data 𝒙𝑢 respectively. 𝒙 denotes all data (both 𝒙𝑢 and 𝒙𝑙). [ ˆ𝒙𝑙, ˆ𝒙𝑢, ˆ𝒙] are the adversarial samples of [𝒙𝑙, 𝒙𝑢, 𝒙] generated following [11], respectively. ℓ𝐶𝐸 and ℓ𝑑𝑖𝑠𝑡𝑖𝑙𝑙 are Cross-Entropy (CE) loss and Knowledge Distillation loss (with temperature 𝑇), respectively.
The ﬁrst two terms in Eqn. (3) represent a convex combination of CE and distillation losses for labeled data (𝛼 is a coefﬁcient ∈ (0,1)). The third term is the distillation loss for unlabeled data.
The fourth term is feature-consistency robustness loss for all data (with weight 1
𝜆 ) following [11], respectively. |𝑁𝑙 | + |𝑁𝑢 | represent the batch size (including both labeled and unlabeled data). 4
Table 1: Comparison between Random Initialization (RI), Selﬁe [1] and the proposed ACL (DS) on
CIFAR-10. ACL (DS) consistently yields the highest Standard Testing Accuracy (TA) and Robust
Testing Accuracy (RA).
Method
Metric
CIFAR-10
CIFAR-100
Random Initialization (RI)
Selﬁe
ACL (DS)
TA(%) 79.90 49.12
RA(%) 49.36 23.07
TA(%) 80.05 54.63
RA(%) 49.83 24.75
TA(%) 82.19 56.77
RA(%) 52.82 28.33 3 Experiments and analysis
Experimental settings: We evaluate three datasets: CIFAR-10, CIFAR-10-C [31], CIFAR-100. We follow [1] to employ two metrics: 1) Standard Testing Accuracy (TA): The classiﬁcation accuracy on clean testing images; and 2) Robust Testing Accuracy (RA): the classiﬁcation accuracy on adversarial perturbed testing images. Evaluation on unforeseen attacks is also considered [32].
For contrastive pre-training, we identically follow SimCLR [2] for all the optimizer settings, aug-mentation and projection head structure. We choose 512 for batch size and train for 1000 epochs. To generate adversarial perturbations, we use the ℓ∞ PGD attack [15], following all hyperparameters used by [1], except that we only run the PGD for 5 steps in the pre-training stage for faster convergence.
Experiments are organized as following: Section 3.1 ﬁrst compares our best proposed option: ACL with Dual-Stream (DS), with existing methods, ﬁrst as pre-training for supervised adversarial ﬁne-tuning (Section 3.1.1); then as part of semi-supervised adversarial training (Section 3.1.2). Section 3.2 then delves into an ablation study comparing the three ACL options: A2A, A2S and DS. 3.1 Comparing ACL with state-of-the-arts 3.1.1 ACL pre-training for supervised adversarial ﬁne-tuning
We compare our ACL Dual Stream (DS), with the state-of-the-art robust pre-training [1], on CIFAR-10 and CIFAR-100. For the latter, we choose its single best pretext task, Selﬁe, as the comparison subject (and call it so hereinafter). By default, we use ResNet-18 [33] as the backbone all experiments hereinafter, as was also adopted by [29], unless otherwise speciﬁed. ACL (DS) is employed for training the encoder of ResNet-18 (excluding the fully connected (FC) Layer), and then the pre-trained encoder is added with the zero-initialized FC layer to start ﬁne-tuning.
We compare ACL (DS) and Selﬁe by conducting adversarial ﬁne-tuning over their pre-trained models.
For the fully-supervised tuning, we employ the loss in TRADE [29] for adversarial ﬁne-tuning, with the regularization weight 1/𝜆 as 6. The ﬁne-tuned models are selected based on the held-out validation RA. We use SGD with 0.9 momentum and batch size 128. By default, we ﬁne-tune 25 epochs, with initial learning rate set as 0.1 and then decaying by 10 times at epoch 15 and 20. As mentioned in Section 2.2, we report ACL (DS) results using the adversarial branch BN.
As demonstrated in Table 1, while both Selﬁe and ACL (DS) lead to higher precision and robustness compared to the model trained from random initialization, the proposed ACL (DS) yields a signiﬁcant improvement on [TA, RA] by [2.14%, 2.99%] on CIFAR-10, and [2.14%, 3.58%] on CIFAR-100, respectively. Those gains are signiﬁcant, especially considering that the pre-training approach in [1] already outperformed all previous state-of-the-arts robust training approaches. By surpassing [1],
ACL (DS) establishes the new benchmark TA/RA numbers on CIFAR-10 and CIFAR-100.
We then apply ACL (DS) on Wide-Resnet-34-10 [34], which is also a standard model employed for testing adversarial robustness , to verify if the proposed pre-training can also help for big models.
In the supervised ﬁne-tuning experiment with CIFAR10, the random initialization (which equals
TRADE[29]) yields [84.33%, 55.46%]1. By simply using the ACL (DS) pretrained model as the initialization, we obtain [TA, RA] of [85.12%, 56.72%], where our pre-training also contributes to a nontrivial [0.79%, 1.26%] margin for TRADE on the big model.
We further demonstrate that our robustness gain can consistently hold against unforeseen attacks, by leveraging the 19 unforeseen attacks used by [32] on CIFAR-10 with ResNet-18. As shown in 1We run the ofﬁcial code of [29]. The RA drops by 1% compared to what [29] reported, since we test with the PGD attack starting from random initializations, which leads to stronger adversarial attacks. 5
Table 2: Comparison of semi-supervised performance on CIFAR-10 where only 10% (ﬁrst row) and 1% labels (bottom row) are available for different methods: UAT++ [11], Selﬁe and ACL (DS).
Method
Metric 10% labels 1% labels vanilla UAT++
Selﬁe
ACL (DS)
TA(%) 70.79 41.88
RA(%) 50.43 30.46
TA(%) 72.16 53.54
RA(%) 50.37 37.75
TA(%) 76.66 75.66
RA(%) 51.09 50.67
Fig. 2, our approach achieves improvement on most of the unforeseen attacks (except gaussian noise, where the performance drop marginally by 0.23%). Remarkably, the proposed approach leads to an improvement of 2.42% when averaged over all unforeseen attacks.
Figure 2: This chart conclude the performance comparison between models ﬁne-tuned from the pre-trained weights with Selﬁe [1] and with ACL (DS) on CIFAR-10. Our proposed ACL (DS) outperforms on most unforeseen attack types, showing more general robustness gains. 3.1.2 ACL for semi-supervised adversarial training
Following the procedure in Section 2.2.1, we compare our ACL (DS)-based semi-supervised training, with (1) Selﬁe [1], following the identical routine as our method; and (2) the state-of-the-art UAT++, described in [11]. We emphasize that both Selﬁe and our method are plugged in to improve over the semi-supervised framework like UAT++ (described Sec. 2.2.1), by supplying pre-training, rather than re-inventing a new different semi-supervised algorithm. The results are summarized in Table 2.
We ﬁrst follow the setting in [11] to use 10% training labels together with the entire (unlabeled) training set. We observe that, adopting Selﬁe as pre-training can improve TA by 1.37% with RA almost unchanged, compared to the vanilla UAT++ without any pre-training. That endorses the notable value of pre-training of data-efﬁcient adversarial learning. Moreover, along the same routine,
ACL (DS) outperforms Selﬁe by large extra margins of [4.50%, 0.72%] in [TA ,RA], manifesting the superior quality of our proposed pre-training over [1]. Our result is also the new state-of-the-art
TA-RA trade-off achieved in this same setting, as we are aware of.
Encouraged by the promising data efﬁciency indicated above, we explore a challenging extreme setting that no previous method seems to have tried before: only using 1% labels together with the unlabeled training set. All methods’ hyperparameters are tuned to our best efforts using grid search. As shown in Table 2 bottom row, the advantage of using ACL (DS) pre-training becomes even more remarkable, with [TA, RA] only minorly decreased [1.00%, 0.42%] in this extreme case, compared to using 10% labels. In comparison, both vanilla UAT++ and Selﬁe suffer from catastrophic performance drops (∼13% - 30%). This result points to the tremendous potential of boosting the label efﬁciency of adversarial training much higher than the current.
To further understand why our proposed method leads to such impressive margins, we check the accuracy of pseudo labels generated in step ii. We ﬁnd that ACL (DS) leads to much higher standard accuracy of 86.73% (computed over the entire training set) even with 1% labels, while vanilla UAT++ and Selﬁe can only achieve 37.67% and 46.75%, respectively. This echos the previous ﬁnding in
[13] that the quality of pseudo labels constitutes the main bottleneck for semi-supervised adversarial training; and it indicates that our proposed robust pre-training inherits the strong label-efﬁcient learning capability from contrastive learning [2]. 6
Table 3: Performance comparison of pre-trained model in terms their achieved TA-RA under super-vised adversarial ﬁne-tuning: Random Initialization (RI), SimCLR(S2S), ACL(A2A), ACL(A2S),
ACL (DS) under adversarial training.
Metric
TA(%)
RA(%)
RI 79.90 49.36
SimCLR (S2S)
ACL (A2A)
ACL (A2S)
ACL (DS) 81.57 49.97 78.68 50.12 80.82 52.11 82.19 52.82
Table 4: The adversarial ﬁne-tuning performance comparison of pre-trained model with different BN options for ACL(A2S) and ACL (DS)
ACL (A2S)
SingleBN DualBN(𝜽bn) DualBN(𝜽bnadv )
ACL (DS)
SingleBN DualBN(𝜽bn) DualBN(𝜽bnadv )
Metric
TA(%)
RA(%) 75.15 35.60 81.54 51.59 80.82 52.11 73.15 40.10 82.46 52.36 82.19 52.82 3.2 Ablation study for ACL
Comparing A2A, A2S and DS We now report the ablation study among our proposed three ACL options, with two extra natural baselines: SimCLR [2] that embeds no adversarial learning (a.k.a,
Standard-to-Standard, or S2S); and random initialization (RI) without any pre-training.
As shown in Tab. 3, while SimCLR (S2S) improves TA over random initialization (RI) by 1.67%, it has little impact on RA. That is as expected, since this pre-training considers no adversarial perturbations.
A2A boosts RA marginally more than S2S thanks to introducing adversarial perturbations, but pays a dramatic price of TA (even 1.22% lower than no pre-training). Similar to our previous conjecture, it shows that A2A’s overly aggressive, worst-case only consistency enforcement degrades the feature quality. A2S starts to achieve more favorable RA, but its TA is still slightly below S2S. Eventually,
DS jointly optimize both standard and adversarial feature consistencies, ensuring the feature quality and robustness simultaneously. Not only DS surpassed SimCLR by 2.85% RA, but more interestingly, it also outperforms SimCLR by 0.62% TA. That suggests DS to be a potentially more favored pre-training, even for standard training as well. We consider this to align with another recent observation that adversarial examples as data augmentation can improve standard image recognition too [27]: we leave it to be more thoroughly examined for future work.
Comparing single and dual BNs For A2S and S2S, we compare three scenarios: 1) using one single BN for both standard and adversar-ial branches; 2) using dual BN for training, and adopt the standard branch BN (denoted as 𝜽 bn) for testing; 3) using dual BN for training, and adopt the adversarial branch BN (𝜽 bnadv ) for testing. As shown in Tab. 4, both methods see notable per-formance drops when using single BNs that mix standard and adversarial feature statistics, which align with the ﬁndings in [27, 28]. For dual BN, our default choice of 𝜽 bnadv leads to more favored
RA meanwhile still strong TA. Otherwise if we switch to 𝜽 bn, TA performance will be empha-sized to become higher, while RAs drop slightly yet remain competitive.
Figure 3: The robust accuracy in cross-validation dataset w.r.t. different epochs. We compare mod-els trained from random initialization (RI) and from ACL (DS). Two different ﬁne-tuning learn-ing rate schedules of ACL (DS) are included.
Linear separability of pre-trained representa-tions To evaluate the learned representations,
[2] adopted a linear separability evaluation proto-col, where a linear classiﬁer is trained on top of the frozen pre-trained encoder. The test accuracy is used as a proxy for representation quality: higher accuracy indicates features to be more discrimina-tive and desirable. Here we adapt the protocol by also adversarially training the linear classiﬁer, and 7
Table 5: Evaluating the performance of the self-supervised representation trained with ACL (DS) by
ﬁxing the trained encoder and only ﬁne-tuning the ﬁnal linear FC layer. Two ﬁne-tuning strategies for two different BNs (𝜽 bnadv and 𝜽 bn) are considered.
BN choice
Metric
Standard Training
Adversarial Training
𝜽bn
𝜽bnadv
TA(%) 91.02 82.63
RA(%) 0.20 0.28
TA(%) 79.76 74.22
RA(%) 30.31 44.22 evaluate the features learned by ACL (DS) pre-training in terms of both TA and RA. We call the RA obtained by the adversarially trained linear classiﬁer as adversarial linear separability of the features.
Tab. 5 compares on two different ways training the linear classiﬁer, and also on employing either set of BNs (𝜽 bn or 𝜽 bnadv) to output features. A good level of adversarial linear separability, i.e., 44.22%
RA, is observed under 𝜽 bnadv . Interestingly, if we switch from 𝜽 bnadv to 𝜽 bn, we immediately lose the adversarial linear separability (close to 0% RA). That again conﬁrms the distinct statistics captured by two BNs [28, 27].
Robustness dynamics during adversarial ﬁne-tuning Lastly, we dissect how the robustness grows during the supervised tuning process, when starting from random initialization and ACL (DS) pre-training, respectively. As indicated in Fig. 3, the robust accuracy of ACL (DS) pre-trained models jumps to 47.38% after one epoch ﬁne-tuning, while it costs randomly initialized models 74 epochs more to achieve the same. Additionally, if we ﬁne-tune the ACL (DS) pre-trained model longer and do not anneal the learning rate earlier, we will also see the robust accuracy decrease before the decay point and end up with 1.0% robustness drop, which was reported in [35] and termed as the adversarial over-ﬁtting phenomenon. 4