Abstract
Recent work has shown that, when integrated with adversarial training, self-supervised pre-training can lead to state-of-the-art robustness [1]. In this work, we improve robustness-aware self-supervised pre-training by learning representations that are consistent under both data augmentations and adversarial perturbations. Our approach leverages a recent contrastive learning framework [2], which learns repre-sentations by maximizing feature consistency under differently augmented views.
This ï¬ts particularly well with the goal of adversarial robustness, as one cause of ad-versarial fragility is the lack of feature invariance, i.e., small input perturbations can result in undesirable large changes in features or even predicted labels. We explore various options to formulate the contrastive task, and demonstrate that by injecting adversarial perturbations, contrastive pre-training can lead to models that are both label-efï¬cient and robust. We empirically evaluate the proposed Adversarial Con-trastive Learning (ACL) and show it can consistently outperform existing methods.
For example on the CIFAR-10 dataset, ACL outperforms the previous state-of-the-art unsupervised robust pre-training approach [1] by 2.99% on robust accuracy and 2.14% on standard accuracy. We further demonstrate that ACL pre-training can improve semi-supervised adversarial training, even when only a few labeled exam-ples are available. Our codes and pre-trained models have been released at: https:
//github.com/VITA-Group/Adversarial-Contrastive-Learning. 1

Introduction
Label efï¬ciency and model robustness are two desirable characteristics when it comes to building machine learning models, including the training of deep neural networks. Traditional supervised learning of deep networks requires a lot of labeled data, whose annotation costs can be prohibitively high. Utilizing unlabeled data, e.g., by various unsupervised or semi-supervised learning techniques, is thus of booming interests. One particular branch of unsupervised learning based on contrastive learning has shown great promise recently [3, 4, 5, 6, 7, 2]. The latest contrastive learning framework
[2] can largely improves label efï¬ciency of the deep networks, e.g., surpassing the fully-supervised
AlexNet [8] when ï¬ne-tuned on only 1% of the labels.
The labeling scarcity is even ampliï¬ed when we come to adversarially robust deep learning [9], i.e., to training deep models that are not fooled by maliciously crafted, although imperceivable perturbations. As suggested in [10], the sample complexity of adversarially robust generalization is signiï¬cantly higher than that of standard learning. Recent results [11, 12, 13] advocated unlabeled data to be a powerful backup for training adversarially robust models as well, by using unlabeled data to form an auxiliary loss (e.g., a label-independent robust regularizer or a pseudo-label loss).
Only lately, researchers start linking self-supervised learning to adversarial robustness. [14] proposed a multi-task learning framework that incorporates a self-supervised objective to be co-optimized with the conventional classiï¬cation loss. The latest work [1] introduced adversarial training [15] into self-supervision, to provide general-purpose robust pre-trained models for the ï¬rst time. However, existing work is based on ad-hoc â€œpretextâ€ self-supervised tasks, and as shown in [6, 2], the learned 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of workï¬‚ow comparison: (a) The original SimCLR framework [2], a.k.a., stan-dard to standard (no adversarial attack involved); (b) - (d) three proposed variants of our adversarial contrastive learning framework: A2A, A2S, and DS (our best solution). Note that, whenever more than one encoder branches co-exist in one framework, they by default share all weights, except that adversarial and standard encoders will use independent BN parameters. unsupervised representations can be largely improved with contrastive learning, a new family of approaches for self-supervised learning.
In order to learn data-efï¬cient robust models, we propose to integrate contrastive learning with adver-sarially robust deep learning. Our work is inspired by recent success of contrastive learning, where the model is learned by maximizing consistency of data representations under differently augmented views [2]. This ï¬ts particularly well with adversarial training, as one cause of adversarial fragility could be attributed to the non-smooth feature space near samples, i.e., small input perturbations can result in large feature variations and even label change.
By properly combining adversarial learning and contrastive pre-training (i.e. SimCLR [2]), we could achieve the desirable feature consistency. The resultant unsupervised pre-training framework, called Adversarial Contrastive Learning (ACL), is thoroughly discussed in Section 2. As the pre-training plays an increasingly important role for adversarial training [1], by leveraging more powerful contrastive pre-training of unsupervised representations, we further contribute to pushing forward the state-of-the-art adversarial robustness and accuracy.
On CIFAR-10/CIFAR-100, we extensively benchmark our pre-trained models on two adversarial
ï¬ne-tuning settings: fully-supervised (using all training data and their labels) and semi-supervised (using all training data, but only partial/few shot labels). ACL pre-training leads to new state-of-the-art in both robust and standard accuracies. For example, in the fully-supervised setting, we obtain 2.99% robust and 2.14% standard accuracy improvements, over the previous best adversarial pre-training method [1] on CIFAR-10. For the semi-supervised setting, our approach surpasses the state-of-the-art method [11], by 0.66% robust and 5.87% standard accuracy, when 10% labels are available; that gains can become even more remarkable when we go further to extremely low label rates, e.g., 1%. 2 Our approach 2.1 Preliminaries
Adversarial Training Deep networks are notorious for the vulnerability to adversarial attacks [16].
Numerous approaches have been proposed to enhance model adversarial robustness [17, 18, 19, 20, 21, 15, 22, 23, 24, 25]. Among them, Adversarial Training (AT) based methods [15] remain to be the strongest defense option. Deï¬ne the perturbation ğœ¹, using â„“âˆ attack for example:
ğœ¹ = arg max (cid:107) ğ›¿(cid:48) (cid:107)âˆ â‰¤ ğœ–
â„“(ğœ½, ğ’™ + ğœ¹(cid:48)) 2 (1)
where ğœ½ represents the parameters of a model. ğ’™ denotes a given sample. â„“ denotes the loss with parameter ğœ½, and the adversarial perturbation ğœ¹. AT solves the following (minimax) optimization:
Eğ’™ âˆˆ D (â„“(ğœ½, ğ’™ + ğœ¹)) min
ğœ½ (2) where D denotes the training set. The standard training (ST) could be viewed as a special case ğœ– = 0.
Unsupervised Adversarial Training Among the few existing works [11, 12] utilizing unlabeled data, we introduce the Unsupervised Adversarial Training (UAT) algorithm [11] which yields the state-of-the-art performance so far. The authors introduce three variants: (i) UAT with Online Targets, which exploits a smoothness regularizer on the unlabeled data to minimize the divergence between a normal sample with its adversarial counter-part; (ii) UAT with Fixed Targets, which ï¬rst trains a
CNN using standard training (no AT applied) and then uses the trained model to supply pseudo labels for unlabeled data, ï¬nally combining them with labeled data towards AT altogether; and (iii) UAT++, which combines both types of unsupervised losses and was shown to outperform either individually.
Contrastive Pretraining We build this work upon SimCLR [2], a simple yet powerful contrastive pretraining framework for unsupervised representation learning. The main idea of SimCLR is to learn representations by maximizing agreements of differently augmented views of the same image. More speciï¬cally, its workï¬‚ow is illustrated in Fig. 1 (a). Considering an unlabeled sample ğ’™, SimCLR ï¬rst augments the sample to be Ëœğ’™ğ‘– and Ëœğ’™ ğ‘— , with two separate data augmentation operators sampled from the same family of augmentations T (practically, the composition of random cropping and random color distortion). The pair ( Ëœğ’™ğ‘–, Ëœğ’™ ğ‘— ) are then processed by the network backbone (denoted by the two weight-sharing standard encoders in Fig. 1 (a)), outputting features ( Ëœğ’›ğ‘–, Ëœğ’› ğ‘— ). After passing through a nonlinear projection head (omitted in the plot), the transformed feature pairs are optimized under a contrastive loss â„“ğ‘ ğ‘‡ (NT-Xent) to maximize their agreement. Upon the completion of training, we only keep the encoder part as the pre-trained model for downstream tasks. 2.2 Adversarial Contrastive Learning as pre-training: three options
SimCLR can be considered as Standard-to-Standard (S2S) contrasting, as both branches are natural images, and adversarial training nor attack is involved. In our proposed method, Adversarial Contrast
Learning (ACL), we inject robustness components into the SimCLR framework, and discuss three candidate algorithms (Fig. 1 (b) - (d)).
Adversarial-to-Adversarial (A2A) The ï¬rst option is to directly inject two adversarial attacks after random data augmentations. We ï¬rst generate (ğœ¹ğ‘–, ğœ¹ ğ‘— ) as adversarial perturbations corresponding to the augmented samples ( Ëœğ’™ğ‘–, Ëœğ’™ ğ‘— ), using the PGD attack algorithm [15], respectively. Here the loss â„“ for generating perturbations is the contrastive loss (NT-Xent). In other words, we now generate two views that are adversarially enforced to be dissimilar, on which we learn to enforce consistency. In this way, the contrastive learning is now performed on ( Ëœğ’™ğ‘– + ğœ¹ğ‘–, Ëœğ’™ ğ‘— + ğœ¹ ğ‘— ) instead of ( Ëœğ’™ğ‘–, Ëœğ’™ ğ‘— ), enforcing the representations to be invariant to cascaded standard-and-adversarial input perturbations. Note that this could also be viewed as performing attacks with random initialization [26].
Adversarial-to-Standard (A2S) The second option we explore is to replace ( Ëœğ’™ğ‘– + ğœ¹ğ‘–, Ëœğ’™ ğ‘— + ğœ¹ ğ‘— ) with ( Ëœğ’™ğ‘– + ğœ¹ğ‘–, Ëœğ’™ ğ‘— ), i.e., alleviating the encoderâ€™s workload of standing against two simultaneous attacks.
Intuitive as it might look, there is a pitfall for A2S implementation. In both SimCLR (S2S) and A2A, their two backbones completely share all parameters. In A2S, while we still hope the adversarial and standard encoder branches to share all convolutional weights, it is important to allow both encoders to maintain their own independent batch normalization (BN) parameters.
Why this matters? As observed in [27, 28], the deep network feature statistics of clean samples Ëœğ’™ and adversarial samples Ëœğ’™ + ğœ¹ can be very distinct. Mixing the two sets and normalizing using one set of parameters will compromise both robust and standard accuracy in image classiï¬cation. We also veriï¬ed this in our setting, that using one BN for standard and adversarial features will be detrimental to unsupervised learning too. We thus turn to the dual Batch Normalization (BN) suggested in [27], i.e., one BN for standard encoder branch and the other for adversarial branch, respectively.
During ï¬ne-tuning and testing, we by default employ the BN for adversarial branch, since it leads to higher robustness which is our main goal. We also brieï¬‚y study the effect of using either BN on the
ï¬ne-tuning performance as well as the pre-trained representations in Sec. 3.2. 3
Algorithm 1: Algorithm of Dual Stream (DS) Pretraining
Input :A set of clean images ğ’™; Augmentation family T ; Network backbone and projection head ğ‘“ , ğ‘”;
Result :Standard BN parameters ğœ½ bn; Adversarial branch BN parameters ğœ½ bnadv ; The rest parameters ğœ½ in ğ‘“ and ğ‘”; for sampled mini-batch ğ’™ do
Augment ğ’™ to be ( Ëœğ’™ğ‘–, Ëœğ’™ ğ‘— ) with two augmentations sampled from T .
Generate the corresponding adversarial mini-batch ( Ëœğ’™ğ‘– + ğœ¹ğ‘–, Ëœğ’™ ğ‘— + ğœ¹ ğ‘— ) with
â„“ğ‘ ğ‘‡ ( ğ‘“ â—¦ ğ‘”( Ëœğ’™ğ‘– + ğœ¹ğ‘–, Ëœğ’™ ğ‘— + ğœ¹ ğ‘— ; ğœ½, ğœ½ bnadv ))
ğœ¹ğ‘–, ğœ¹ ğ‘— = arg max (cid:107) ğ›¿ğ‘– (cid:107)âˆ â‰¤ ğœ– , (cid:107) ğ›¿ ğ‘— (cid:107)âˆ â‰¤ ğœ–
â„“ = â„“ğ‘ ğ‘‡ ( ğ‘“ â—¦ ğ‘”( Ëœğ’™ğ‘–, Ëœğ’™ ğ‘— ; ğœ½, ğœ½ bn)) + ğ›¼â„“ğ‘ ğ‘‡ ( ğ‘“ â—¦ ğ‘”( Ëœğ’™ğ‘– + ğœ¹ğ‘–, Ëœğ’™ ğ‘— + ğœ¹ ğ‘— ; ğœ½, ğœ½ bnadv ))
Update parameters (ğœ½ bn, ğœ½ bnadv, ğœ½) to minimize â„“. end
Dual Stream (DS) The third option we explore is to combine S2S and A2A in one. Speciï¬cally, for each input ğ’™, we augment it into twice (creating four augmented views): ( Ëœğ’™ğ‘–, Ëœğ’™ ğ‘— ) by standard augmentations (as in S2S), and ( Ëœğ’™ğ‘– + ğœ¹ğ‘–, Ëœğ’™ ğ‘— + ğœ¹ ğ‘— ) further with adversarial perturbation (as in A2A).
Our ï¬nal unsupervised loss consists of a contrastive loss term on the former pair (through two standard branches) and another contrastive loss term on the latter pair (through two adversarial branches).
The two terms are by default equally weighted (ğ›¼ = 1 in Algorithm. 1). The four branches share all convolutional weights except BN: the two standard branches use the same set of BN parameters, while the two adversarial branches have their one set of BN parameters too.
Compared to A2A which aggressively demands minimizing the â€œworst-caseâ€ consistencies, the ï¬nal dual-stream loss is aligned with the classical ATâ€™s objective [15], i.e., balancing between standard and worst-case (robust) loss terms. The algorithm of DS can be found in Algorithm. 1. We experimentally verify that Dual-Stream is the best among the three ACL variants. A comprehensive comparison of the three variants is presented in Section 3.2. 2.2.1 Towards supervised ï¬ne-tuning and semi-supervised training
For each of the three variants, we will use their pre-trained network weights as the initialization for subsequent ï¬ne-tuning. For the fully supervised setting, we identically follow the adversarial
ï¬ne-tuning scheme described in [29].
For semi-supervised training, we employ a three-step routine: i) we ï¬rst perform ACL to obtain a pre-trained model (denoted as PM) on all unlabeled data ; ii) we next adopt PM as the initialization to perform standard training using only labeled data, and use the trained model to generate pseudo-labels for all unlabeled data; iii) lastly, we leverage PM as initialization again, and perform AT over all data (including unlabeled data and their pseudo labels). The routine in principle follows [11, 30, 12] except the new pre-training step (i). The speciï¬c loss that we minimize in step iii is:
â„“ğ‘  = 1
|ğ‘ğ‘™ | + |ğ‘ğ‘¢ | (ğ›¼â„“ğ¶ğ¸ ( Ë†ğ’™ğ‘™, ğ‘¦ğ‘™; ğœ½) + (1 âˆ’ ğ›¼)ğ‘‡ 2â„“ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘™ğ‘™ ( Ë†ğ’™ğ‘™, ğ‘ğ‘™, ğ‘‡; ğœ½)+
ğ‘‡ 2â„“distill( Ë†ğ’™ğ‘¢, ğ‘ğ‘¢, ğ‘‡; ğœ½) + 1
ğœ†
KL( Ë†ğ‘(ğ’™; ğœ½), Ë†ğ‘( Ë†ğ’™; ğœ½))). (3)
Here ğ‘¦ğ‘™ denotes the ground truth of labeled data ğ’™ğ‘™. ğ‘ğ‘™ and ğ‘ğ‘¢ represent the soft logits predicted by the step ii trained model, for labeled data ğ’™ğ‘™ and unlabeled data ğ’™ğ‘¢ respectively. ğ’™ denotes all data (both ğ’™ğ‘¢ and ğ’™ğ‘™). [ Ë†ğ’™ğ‘™, Ë†ğ’™ğ‘¢, Ë†ğ’™] are the adversarial samples of [ğ’™ğ‘™, ğ’™ğ‘¢, ğ’™] generated following [11], respectively. â„“ğ¶ğ¸ and â„“ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘–ğ‘™ğ‘™ are Cross-Entropy (CE) loss and Knowledge Distillation loss (with temperature ğ‘‡), respectively.
The ï¬rst two terms in Eqn. (3) represent a convex combination of CE and distillation losses for labeled data (ğ›¼ is a coefï¬cient âˆˆ (0,1)). The third term is the distillation loss for unlabeled data.
The fourth term is feature-consistency robustness loss for all data (with weight 1
ğœ† ) following [11], respectively. |ğ‘ğ‘™ | + |ğ‘ğ‘¢ | represent the batch size (including both labeled and unlabeled data). 4
Table 1: Comparison between Random Initialization (RI), Selï¬e [1] and the proposed ACL (DS) on
CIFAR-10. ACL (DS) consistently yields the highest Standard Testing Accuracy (TA) and Robust
Testing Accuracy (RA).
Method
Metric
CIFAR-10
CIFAR-100
Random Initialization (RI)
Selï¬e
ACL (DS)
TA(%) 79.90 49.12
RA(%) 49.36 23.07
TA(%) 80.05 54.63
RA(%) 49.83 24.75
TA(%) 82.19 56.77
RA(%) 52.82 28.33 3 Experiments and analysis
Experimental settings: We evaluate three datasets: CIFAR-10, CIFAR-10-C [31], CIFAR-100. We follow [1] to employ two metrics: 1) Standard Testing Accuracy (TA): The classiï¬cation accuracy on clean testing images; and 2) Robust Testing Accuracy (RA): the classiï¬cation accuracy on adversarial perturbed testing images. Evaluation on unforeseen attacks is also considered [32].
For contrastive pre-training, we identically follow SimCLR [2] for all the optimizer settings, aug-mentation and projection head structure. We choose 512 for batch size and train for 1000 epochs. To generate adversarial perturbations, we use the â„“âˆ PGD attack [15], following all hyperparameters used by [1], except that we only run the PGD for 5 steps in the pre-training stage for faster convergence.
Experiments are organized as following: Section 3.1 ï¬rst compares our best proposed option: ACL with Dual-Stream (DS), with existing methods, ï¬rst as pre-training for supervised adversarial ï¬ne-tuning (Section 3.1.1); then as part of semi-supervised adversarial training (Section 3.1.2). Section 3.2 then delves into an ablation study comparing the three ACL options: A2A, A2S and DS. 3.1 Comparing ACL with state-of-the-arts 3.1.1 ACL pre-training for supervised adversarial ï¬ne-tuning
We compare our ACL Dual Stream (DS), with the state-of-the-art robust pre-training [1], on CIFAR-10 and CIFAR-100. For the latter, we choose its single best pretext task, Selï¬e, as the comparison subject (and call it so hereinafter). By default, we use ResNet-18 [33] as the backbone all experiments hereinafter, as was also adopted by [29], unless otherwise speciï¬ed. ACL (DS) is employed for training the encoder of ResNet-18 (excluding the fully connected (FC) Layer), and then the pre-trained encoder is added with the zero-initialized FC layer to start ï¬ne-tuning.
We compare ACL (DS) and Selï¬e by conducting adversarial ï¬ne-tuning over their pre-trained models.
For the fully-supervised tuning, we employ the loss in TRADE [29] for adversarial ï¬ne-tuning, with the regularization weight 1/ğœ† as 6. The ï¬ne-tuned models are selected based on the held-out validation RA. We use SGD with 0.9 momentum and batch size 128. By default, we ï¬ne-tune 25 epochs, with initial learning rate set as 0.1 and then decaying by 10 times at epoch 15 and 20. As mentioned in Section 2.2, we report ACL (DS) results using the adversarial branch BN.
As demonstrated in Table 1, while both Selï¬e and ACL (DS) lead to higher precision and robustness compared to the model trained from random initialization, the proposed ACL (DS) yields a signiï¬cant improvement on [TA, RA] by [2.14%, 2.99%] on CIFAR-10, and [2.14%, 3.58%] on CIFAR-100, respectively. Those gains are signiï¬cant, especially considering that the pre-training approach in [1] already outperformed all previous state-of-the-arts robust training approaches. By surpassing [1],
ACL (DS) establishes the new benchmark TA/RA numbers on CIFAR-10 and CIFAR-100.
We then apply ACL (DS) on Wide-Resnet-34-10 [34], which is also a standard model employed for testing adversarial robustness , to verify if the proposed pre-training can also help for big models.
In the supervised ï¬ne-tuning experiment with CIFAR10, the random initialization (which equals
TRADE[29]) yields [84.33%, 55.46%]1. By simply using the ACL (DS) pretrained model as the initialization, we obtain [TA, RA] of [85.12%, 56.72%], where our pre-training also contributes to a nontrivial [0.79%, 1.26%] margin for TRADE on the big model.
We further demonstrate that our robustness gain can consistently hold against unforeseen attacks, by leveraging the 19 unforeseen attacks used by [32] on CIFAR-10 with ResNet-18. As shown in 1We run the ofï¬cial code of [29]. The RA drops by 1% compared to what [29] reported, since we test with the PGD attack starting from random initializations, which leads to stronger adversarial attacks. 5
Table 2: Comparison of semi-supervised performance on CIFAR-10 where only 10% (ï¬rst row) and 1% labels (bottom row) are available for different methods: UAT++ [11], Selï¬e and ACL (DS).
Method
Metric 10% labels 1% labels vanilla UAT++
Selï¬e
ACL (DS)
TA(%) 70.79 41.88
RA(%) 50.43 30.46
TA(%) 72.16 53.54
RA(%) 50.37 37.75
TA(%) 76.66 75.66
RA(%) 51.09 50.67
Fig. 2, our approach achieves improvement on most of the unforeseen attacks (except gaussian noise, where the performance drop marginally by 0.23%). Remarkably, the proposed approach leads to an improvement of 2.42% when averaged over all unforeseen attacks.
Figure 2: This chart conclude the performance comparison between models ï¬ne-tuned from the pre-trained weights with Selï¬e [1] and with ACL (DS) on CIFAR-10. Our proposed ACL (DS) outperforms on most unforeseen attack types, showing more general robustness gains. 3.1.2 ACL for semi-supervised adversarial training
Following the procedure in Section 2.2.1, we compare our ACL (DS)-based semi-supervised training, with (1) Selï¬e [1], following the identical routine as our method; and (2) the state-of-the-art UAT++, described in [11]. We emphasize that both Selï¬e and our method are plugged in to improve over the semi-supervised framework like UAT++ (described Sec. 2.2.1), by supplying pre-training, rather than re-inventing a new different semi-supervised algorithm. The results are summarized in Table 2.
We ï¬rst follow the setting in [11] to use 10% training labels together with the entire (unlabeled) training set. We observe that, adopting Selï¬e as pre-training can improve TA by 1.37% with RA almost unchanged, compared to the vanilla UAT++ without any pre-training. That endorses the notable value of pre-training of data-efï¬cient adversarial learning. Moreover, along the same routine,
ACL (DS) outperforms Selï¬e by large extra margins of [4.50%, 0.72%] in [TA ,RA], manifesting the superior quality of our proposed pre-training over [1]. Our result is also the new state-of-the-art
TA-RA trade-off achieved in this same setting, as we are aware of.
Encouraged by the promising data efï¬ciency indicated above, we explore a challenging extreme setting that no previous method seems to have tried before: only using 1% labels together with the unlabeled training set. All methodsâ€™ hyperparameters are tuned to our best efforts using grid search. As shown in Table 2 bottom row, the advantage of using ACL (DS) pre-training becomes even more remarkable, with [TA, RA] only minorly decreased [1.00%, 0.42%] in this extreme case, compared to using 10% labels. In comparison, both vanilla UAT++ and Selï¬e suffer from catastrophic performance drops (âˆ¼13% - 30%). This result points to the tremendous potential of boosting the label efï¬ciency of adversarial training much higher than the current.
To further understand why our proposed method leads to such impressive margins, we check the accuracy of pseudo labels generated in step ii. We ï¬nd that ACL (DS) leads to much higher standard accuracy of 86.73% (computed over the entire training set) even with 1% labels, while vanilla UAT++ and Selï¬e can only achieve 37.67% and 46.75%, respectively. This echos the previous ï¬nding in
[13] that the quality of pseudo labels constitutes the main bottleneck for semi-supervised adversarial training; and it indicates that our proposed robust pre-training inherits the strong label-efï¬cient learning capability from contrastive learning [2]. 6
Table 3: Performance comparison of pre-trained model in terms their achieved TA-RA under super-vised adversarial ï¬ne-tuning: Random Initialization (RI), SimCLR(S2S), ACL(A2A), ACL(A2S),
ACL (DS) under adversarial training.
Metric
TA(%)
RA(%)
RI 79.90 49.36
SimCLR (S2S)
ACL (A2A)
ACL (A2S)
ACL (DS) 81.57 49.97 78.68 50.12 80.82 52.11 82.19 52.82
Table 4: The adversarial ï¬ne-tuning performance comparison of pre-trained model with different BN options for ACL(A2S) and ACL (DS)
ACL (A2S)
SingleBN DualBN(ğœ½bn) DualBN(ğœ½bnadv )
ACL (DS)
SingleBN DualBN(ğœ½bn) DualBN(ğœ½bnadv )
Metric
TA(%)
RA(%) 75.15 35.60 81.54 51.59 80.82 52.11 73.15 40.10 82.46 52.36 82.19 52.82 3.2 Ablation study for ACL
Comparing A2A, A2S and DS We now report the ablation study among our proposed three ACL options, with two extra natural baselines: SimCLR [2] that embeds no adversarial learning (a.k.a,
Standard-to-Standard, or S2S); and random initialization (RI) without any pre-training.
As shown in Tab. 3, while SimCLR (S2S) improves TA over random initialization (RI) by 1.67%, it has little impact on RA. That is as expected, since this pre-training considers no adversarial perturbations.
A2A boosts RA marginally more than S2S thanks to introducing adversarial perturbations, but pays a dramatic price of TA (even 1.22% lower than no pre-training). Similar to our previous conjecture, it shows that A2Aâ€™s overly aggressive, worst-case only consistency enforcement degrades the feature quality. A2S starts to achieve more favorable RA, but its TA is still slightly below S2S. Eventually,
DS jointly optimize both standard and adversarial feature consistencies, ensuring the feature quality and robustness simultaneously. Not only DS surpassed SimCLR by 2.85% RA, but more interestingly, it also outperforms SimCLR by 0.62% TA. That suggests DS to be a potentially more favored pre-training, even for standard training as well. We consider this to align with another recent observation that adversarial examples as data augmentation can improve standard image recognition too [27]: we leave it to be more thoroughly examined for future work.
Comparing single and dual BNs For A2S and S2S, we compare three scenarios: 1) using one single BN for both standard and adversar-ial branches; 2) using dual BN for training, and adopt the standard branch BN (denoted as ğœ½ bn) for testing; 3) using dual BN for training, and adopt the adversarial branch BN (ğœ½ bnadv ) for testing. As shown in Tab. 4, both methods see notable per-formance drops when using single BNs that mix standard and adversarial feature statistics, which align with the ï¬ndings in [27, 28]. For dual BN, our default choice of ğœ½ bnadv leads to more favored
RA meanwhile still strong TA. Otherwise if we switch to ğœ½ bn, TA performance will be empha-sized to become higher, while RAs drop slightly yet remain competitive.
Figure 3: The robust accuracy in cross-validation dataset w.r.t. different epochs. We compare mod-els trained from random initialization (RI) and from ACL (DS). Two different ï¬ne-tuning learn-ing rate schedules of ACL (DS) are included.
Linear separability of pre-trained representa-tions To evaluate the learned representations,
[2] adopted a linear separability evaluation proto-col, where a linear classiï¬er is trained on top of the frozen pre-trained encoder. The test accuracy is used as a proxy for representation quality: higher accuracy indicates features to be more discrimina-tive and desirable. Here we adapt the protocol by also adversarially training the linear classiï¬er, and 7
Table 5: Evaluating the performance of the self-supervised representation trained with ACL (DS) by
ï¬xing the trained encoder and only ï¬ne-tuning the ï¬nal linear FC layer. Two ï¬ne-tuning strategies for two different BNs (ğœ½ bnadv and ğœ½ bn) are considered.
BN choice
Metric
Standard Training
Adversarial Training
ğœ½bn
ğœ½bnadv
TA(%) 91.02 82.63
RA(%) 0.20 0.28
TA(%) 79.76 74.22
RA(%) 30.31 44.22 evaluate the features learned by ACL (DS) pre-training in terms of both TA and RA. We call the RA obtained by the adversarially trained linear classiï¬er as adversarial linear separability of the features.
Tab. 5 compares on two different ways training the linear classiï¬er, and also on employing either set of BNs (ğœ½ bn or ğœ½ bnadv) to output features. A good level of adversarial linear separability, i.e., 44.22%
RA, is observed under ğœ½ bnadv . Interestingly, if we switch from ğœ½ bnadv to ğœ½ bn, we immediately lose the adversarial linear separability (close to 0% RA). That again conï¬rms the distinct statistics captured by two BNs [28, 27].
Robustness dynamics during adversarial ï¬ne-tuning Lastly, we dissect how the robustness grows during the supervised tuning process, when starting from random initialization and ACL (DS) pre-training, respectively. As indicated in Fig. 3, the robust accuracy of ACL (DS) pre-trained models jumps to 47.38% after one epoch ï¬ne-tuning, while it costs randomly initialized models 74 epochs more to achieve the same. Additionally, if we ï¬ne-tune the ACL (DS) pre-trained model longer and do not anneal the learning rate earlier, we will also see the robust accuracy decrease before the decay point and end up with 1.0% robustness drop, which was reported in [35] and termed as the adversarial over-ï¬tting phenomenon. 4