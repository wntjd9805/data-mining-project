Abstract
In this work, we identify a novel set of conditions that ensure convergence with probability 1 of Q-learning with linear function approximation, by proposing a two time-scale variation thereof. In the faster time scale, the algorithm features an update similar to that of DQN, where the impact of bootstrapping is attenuated by using a Q-value estimate akin to that of the target network in DQN. The slower time-scale, in turn, can be seen as a modiﬁed target network update. We establish the convergence of our algorithm, provide an error bound and discuss our results in light of existing convergence results on reinforcement learning with function approximation. Finally, we illustrate the convergent behavior of our method in domains where standard Q-learning has previously been shown to diverge. 1

Introduction
In this paper, we investigate the convergence of reinforcement learning with linear function approxi-mation in control settings. Speciﬁcally, we analyze the convergence of Q-learning when combined with linear function approximation. Several well-known counter-examples exist in the literature that showcase the divergence of this algorithm when used with even such a relatively “benign” form of function approximation [2, 6, 21]. The divergent behavior has been blamed on the so-called “deadly triad” [18, 24]—function approximation, bootstrapping and off-policy learning. Bootstrapping means that successive estimates for the Q-function are built on previous estimates; over-estimation errors for action values thus critically propagate across iterations [23]. Off-policy means that the policy used to sample the environment differs from that which the algorithm is evaluating.
The few results that establish the convergence of Q-learning with function approximation either restrict the approximation architecture, eventually minimizing the impact of over-estimation errors
[20, 21] or require a very restrictive coupling between the approximation architecture and the sampling distribution which, in practice, occurs only when the sampling policy is very close to the optimal policy, rendering Q-learning almost on-policy [13]. More recently, Q-learning was attributed ﬁnite-time error bounds when certain ﬁxed behaviour policies are used [8]. Unfortunately, such policies are scarce or may not even exist as the number of features grows.
Other convergence results for the control problem in RL with function approximation propose algorithms in which at least one of the elements in the “deadly triad” is not present. For example, in a work of Perkins and Precup (2003), the authors propose a novel algorithm that converges with arbitrary function approximation, but is restricted to on-policy sampling. Greedy-GQ [12] is a variant of Q-learning with convergence guarantees. However, the associated solution may not be globally optimal, following from the fact it minimizes a non-convex objective function. Finite-time error bounds for Asynchronous Dynamic Programming methods [3], including Fitted Q-iteration (FQI) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[9], assume not only realizability of the optimal Q-function but also closedness under Bellman update
[19, 7]. Finally, convergence results have been established for the return-base setting [16].
Our work is motivated by the success of DQN [14], which can be viewed as an instance of FQI [25].
In their work, Mnih et al. explore two important ideas in order to circumvent (or, at least, mitigate) the negative impact of bootstrapping and off-policy sampling:
• The use of a target network to compute the target value for the updates. In the original version, the target network is updated only rarely, by copying the values of the original network, although posterior implementations have adopted Polyak updates, in this sense bringing DQN closer to a two time-scale update scheme;
• The use of experience replay, where the samples are drawn from a replay buffer, thus minimizing the correlation between samples observed in trajectory-based learning and enabling the use of supervised learning techniques that assume sample independence.
Building on these ideas, we propose a two time-scale variation of Q-learning with linear function approximation. Our proposed algorithm keeps two sets of parameters.
• The ﬁrst set of parameters, corresponding to the “main” iteration, follows a faster time-scale and uses a DQN-like update, where the targets are built from the second set of parameters— thus minimizing the impact of bootstrapping.
• The second set of parameters, corresponding to the “target network”, proceeds at a slower time-scale—in a sense mimicking the slower updates of a target network. However, unlike
DQN, the second set of parameters does not directly copy the “main” but, instead, a transformed version thereof, reminiscent of the preconditioning process discussed in [1].
We contribute with a convergence analysis of the resulting algorithm, showing convergence with probability 1, or w.p.1, with much less stringent assumptions than previous works [13] and provide an interpretation and performance bounds for the resulting limit solution.
Notation:
We denote random variables (r.v.s) using upright letters, as in x or a, and instances of r.v.s as slanted letters, as in x or a. We use uppercase letters to denote functions, as in V or Q, and calligraphic letters to denote sets, as in X or A. Vectors are represented as bold lowercase letters. For example, c denotes a random vector and c an instance thereof. Matrices are represented using bold uppercase letters, as in Q. We write Ex,a∼p [f (x, a)] or simply Ep [f (x, a)] to denote the expectation of f when the r.v.s x and a follow distribution p. 2