Abstract
Pruning has become a very powerful and effective technique to compress and accelerate modern neural networks. Existing pruning methods can be grouped into two categories: ﬁlter pruning (FP) and weight pruning (WP). FP wins at hardware compatibility but loses at the compression ratio compared with WP. To converge the strength of both methods, we propose to prune the ﬁlter in the ﬁlter. Speciﬁcally, we treat a ﬁlter F ∈ RC×K×K as K × K stripes, i.e., 1 × 1 ﬁlters ∈ RC, then by pruning the stripes instead of the whole ﬁlter, we can achieve ﬁner granularity than traditional FP while being hardware friendly. We term our method as SWP (Stripe-Wise Pruning). SWP is implemented by introducing a novel learnable matrix called Filter Skeleton, whose values reﬂect the shape of each ﬁlter. As some recent work has shown that the pruned architecture is more crucial than the inherited important weights, we argue that the architecture of a single ﬁlter, i.e., the shape, also matters. Through extensive experiments, we demonstrate that SWP is more effective compared to the previous FP-based methods and achieves the state-of-art pruning ratio on CIFAR-10 and ImageNet datasets without obvious accuracy drop. Code is available at this url. 1

Introduction
Deep Neural Networks (DNNs) have achieved remarkable progress in many areas including speech recognition [1], computer vision [2, 3], natural language processing [4], etc. However, model deployment is sometimes costly due to the large number of parameters in DNNs. To relieve such a problem, numerous approaches have been proposed to compress DNNs and reduce the amount of computation. These methods can be classiﬁed into two main categories: weight pruning (WP) and
ﬁlter (channel) pruning (FP).
WP is a ﬁne-grained pruning method that prunes the individual weights, e.g., whose value is nearly 0, inside the network [5, 6], resulting in a sparse network without sacriﬁcing prediction performance.
However, since the positions of non-zero weights are irregular and random, we need an extra record of the weight position, and the sparse network pruned by WP can not be presented in a structured fashion like FP due to the randomness inside the network, making WP unable to achieve acceleration on general-purpose processors. By contrast, FP-based methods [7, 8, 9] prune ﬁlters or channels within the convolution layers, thus the pruned network is still well organized in a structure fashion and can easily achieve acceleration in general processors. A standard ﬁlter pruning pipeline is as follows: 1) Train a larger model until convergence. 2) Prune the ﬁlters according to some criterions 3)
Fine-tune the pruned network. [10] observes that training the pruned model with random initialization 1In the author list, ∗ denotes that authors contribute equally; † denotes corresponding authors. The work is conducted while Fanxu Meng works as an internship at Tencent Youtu Lab. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The left ﬁgure shows two network structures. The right ﬁgure visualizes the average l1 norm of the ﬁlters along the channel dimension in a learned VGG16. can also achieve high performance. Thus it is the network architecture, rather than trained weights that matters. In this paper, we suggest that not only the architecture of the network but the architecture of the ﬁlter itself is also important. [11, 12] also draw similar arguments that the ﬁlter with a larger kernel size may lead to better performance. However, the computation cost is expensive. Thus for a given input feature map, [11, 12] uses ﬁlters with different kernel sizes (e.g., 1 × 1, 3 × 3, and 5 × 5) to perform convolution and concatenate all the output feature map. But the kernel size of each ﬁlter is manually set. It needs professional experience and knowledge to design an efﬁcient network structure.
We wonder what if we can learn the optimal kernel size of each ﬁlter by pruning. Our intuition is illustrated in Figure 1. We know that the structure of deep nets matters for learning tasks. For example, the residual net is easier to optimize and exhibits better performance than VGG. However, we ﬁnd that there is another structure hidden inside the network, which we call ‘the shape of the
ﬁlters’. From Figure 1, not all the stripes in a ﬁlter contribute equally [13]. Some stripes have a very low l1 norm indicating that such stripes can be removed from the network. The optimal shape of the
ﬁlter is the ﬁlter with minimal stripes that maintains the function of the ﬁlter. To capture the ‘ﬁlter shape’ alongside the ﬁlter weights, we propose ‘Filter Skeleton (FS)’ to learn this ’shape’ property and use FS to guide efﬁcient pruning (i.e. learn optimal shape) (See Section 3). Compared to the traditional FP-based pruning, this pruning paradigm achieves ﬁner granularity since we operate with stripes rather than the whole ﬁlter.
Similarly, group-wise pruning, introduced in [14, 15, 16] also achieves ﬁner granularity than ﬁl-ter/channel pruning, which removes the weights located in the same position among all the ﬁlters in a certain layer. However, group-wise pruning breaks the independent assumption on the ﬁlters. For example, the invalid positions of weights in each ﬁlter may be different. By regularizing the network using group-wise pruning, the network may lose representation ability under a large pruning ratio (see Section 4.2). In this paper, we also offer a comparison to group-wise pruning in the experiment.
In contrast, SWP keeps each ﬁlter independent with each other which does not break the independent assumption among the ﬁlters. Throughout the experiments, SWP achieves a higher pruning ratio compared to the ﬁlter-wise, channel-wise, and group-wise pruning methods. We summarize our main contributions below:
• We propose a new pruning paradigm called SWP. SWP achieves a ﬁner granular than traditional ﬁlter pruning and the pruned network can still be inferred efﬁciently.
• We introduce Filter Skeleton (FS) to efﬁciently learn the shape of each ﬁlter and deeply analyze the working mechanism of FS. Using FS, we achieve the state-of-art pruning ratio on CIFAR-10 and ImageNet datasets without obvious accuracy drop. 2