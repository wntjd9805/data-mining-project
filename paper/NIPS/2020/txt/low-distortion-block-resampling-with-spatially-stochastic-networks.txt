Abstract
We formalize and attack the problem of generating new images from old ones that are as diverse as possible, only allowing them to change without restrictions in certain parts of the image while remaining globally consistent. This encompasses the typical situation found in generative modelling, where we are happy with parts of the generated data, but would like to resample others (“I like this generated castle overall, but this tower looks unrealistic, I would like a new one”). In order to attack this problem we build from the best conditional and unconditional generative models to introduce a new network architecture, training procedure, and a new algorithm for resampling parts of the image as desired. 1

Introduction
Many computer vision problems can be phrased as conditional or unconditional image generation.
This includes super-resolution, colorization, and semantic image synthesis among others. However, current techniques for these problems lack a mechanism for ﬁne-grained control of the generation.
More precisely, even if we like certain parts of a generated image but not others, we are forced to decide on either keeping the generated image as-is, or generating an entirely new one from scratch. In this work we aim to obtain a generative model and an algorithm that allow for us to resample images while keeping selected parts as close as possible to the original one, but freely changing others in a diverse manner while keeping global consistency.
To make things more precise, let us consider the problem of conditional image generation, where the data follows an unknown distribution P(x, y) and we want to learn a fast mechanism for sampling y ∈ Y given x ∈ X . The unconditional generation case can be instantiated by simply setting x = 0.
The current state of the art algorithms for image generation usually employ generative adversarial networks (GANs) [15, 28, 18] when presented with a dataset of pairs (x, y). Conditional GANs learn a function gθ : Z × X → Y, and afterwards images ˆy are generated from x by sampling z ∼ P (z) and outputting ˆy := gθ(z, x). The distribution P (z) is usually a ﬁxed Gaussian distribution, and the
GAN procedure makes it so that gθ(z, x) when z ∼ P (z) approximates P(y|x) in a very particular sense (see [15, 3] for more details). As such, GANs create a diverse set of outputs for any given x by transforming the z’s to different complex images.
One limitation of the above setup is that given a generated sample ˆy = g(z, x), we are restricted to accept it and use it as-is for whatever our downstream task is, or generate an entirely new sample by
∗Equal contribution.
†Work performed while at Latent Space. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: A diagram of Spatially Stochastic Networks. We decompose the latent code z spatially into independent blocks, and regularize the model so that local changes in z correspond to localized changes in the image. We then resample parts in the image by resampling their corresponding z’s.
Figure 2: Resampling a person’s hair. The top row consists of unmodiﬁed generations of our models,
Spatially Stochastic Networks (SSNs), trained on FFHQ [17]. With SSNs, resampling two z’s near the top of each persons head makes spatially localized changes (middle row) while also allowing for minimal necessary changes in other parts of the image (third row), unlike in traditional inpainting. resampling z(cid:48) ∼ P (z) and obtaining ˆy(cid:48) = g(z(cid:48), x). There is no in-between, which is not optimal for many use cases.
Consider however the case of Figure 2. Here, we have a GAN trained to do unconditional image generation, and the generations (top row) are of high-quality. However, we would like to provide the user with the ability to modify the hair in the picture while leaving the rest unchanged. In essence, instead of regenerating the entire image, we would like to keep some parts of the image we are happy with as much as possible, and only resample certain groups of pixels that correspond to parts we are unhappy with. The task here is image generation, but it could be super resolution, colorization, or any task where spatially disentangled resampling would be useful.
Our solution to this task is simple: we split the latent code z into many independent blocks, and regularize the generator so that each block affects only a particular part of the image. In order to achieve good performance, we need to make architectural and algorithmic changes drawing from the best conditional and unconditional generative models. This solution, called Spatially Stochastic
Networks (SSNs), is schematized in Figure 1. In the second row of Figure 2 we can see that we successfully achieve the resampling of the hair, while minimally affecting the rest of the image.
While much work has been done in inpainting, which consists of resampling parts of the image while leaving the rest exactly ﬁxed, in problems with structured data this limits drastically the diversity of 2
the resampling. For instance, if we wanted to inpaint a set of pixels corresponding to the hair of a person, we would need to leave the rest of the face exactly ﬁxed. It is unlikely that a resampling of the hair can be achieved without changing even minimally the facial structure and keeping a globally consistent image. We would also need a mask that tells us exactly where every single hair pixel is located, which is usually unavailable. However, with our new ideas, we can select a large block of pixels containing hair and the resample will change those pixels while minimally affecting the rest of the image. Another example of this is seen in Figure 1, where we only roughly select the blocks of pixels containing a tower and other pixels not in those blocks need to modiﬁed in order for changes to render a consistent resampling. Thus, in order to obtain diverse new resamplings that minimally change the rest of the image, we need to allow a small distortion in other parts of the image. This is what we understand as Low Distortion Block-Resampling, or LDBR.
The contributions of this paper are as follows:
• In section 2 we introduce a mathematical framework to study the low distortion block-resampling problem and showcase how it relates to other problems in computer vision such as inpainting.
• In section 3 we study why current techniques are unsuited to solve the LDBR problem.
From this analysis, we construct Spatially Stochastic Networks (SSNs), an algorithm for image generation directly designed to attack this problem. In the process, we introduce several new developments for spatially-conditioned generative adversarial networks, which are of independent interest.
• In section 4 we perform both qualitative and qualitative experiments showing the workings and excellent performance of SSNs.
• In section 5 and section 6 we relate SSNs to other works, and conclude by posing open problems and new research directions that stem from this work. 2 Low Distortion Block-Resampling
Let y ∈ Rny×ny×3 be an RGB image. We deﬁne a block simply as a subimage of y. More concretely, let I = {1, . . . , ny}, and J1, . . . , Jnblocks ⊆ I × I be disjoint subsets of indices such that ∪nblocks a=1 Ja = I. Then, the block with index a is deﬁned as ya := (yi,j,1, yi,j,2, yi,j,3)(i,j)∈Ja where yi,j,1, yi,j,2, yi,j,3 are the red, green and blue intensity values for pixel (i, j) respectively. We will often refer to both ya and a as blocks when the meaning is obvious from the context. While in this paper we will focus mainly on rectangular (and in particular square) blocks with the form
Ja = {i, · · · , i + l} × {j, · · · , j + l(cid:48)}, all our techniques and ideas translate to non-rectangular subimages unless we make explicit mention of it.
The goal of resampling block a can be informally stated as: given a pair (x, y) from P, generate an alternative y(cid:48) via a stochastic process P a (y(cid:48)|(x, y)) such that all blocks b different than a are preserved as much as possible (i.e. y(cid:48) b ≈ yb for all b (cid:54)= a), and such that if we resample every block (i.e. consecutively apply P a for all a), we arrive to an image y∗ whose distribution is P(y|x). To summarize, we want to construct a new plausible image such that only one block is allowed to change unrestricted at a time, and such that resampling every block constitutes resampling the whole image.
Deﬁnition 1 Let {P a(y(cid:48)|x, y)}a=1,...,nblocks be a set of conditional probability distributions over Y, one for each block a = 1, . . . , nblocks. We say that {P a}a=1,...,nblocks is a block-resampling of the probability distribution P(y|x) if when y(nblocks) is constructed by the sequential sampling process y(0) ∼ P(·|x) y(1) ∼ P a1 (·|x, y(0)) y(2) ∼ P a2 (·|x, y(1))
. . . y∗ := y(nblocks) ∼ P anblocks (·|x, y(nblocks−1)) we have that the distribution of y∗ is P(y|x). 3
In words, if we start from a sample y(0) of P and we resample every block in an arbitrary order, we obtain a new independent sample from P.
Note that simply setting P a(·|x, y) = P(·|x) gives a trivial resampling for P, which simply resamples the entire image every time. This, however, collides with our goal of each time resampling an individual block while leaving the other blocks as untethered as possible. This is exactly why we need a low distortion block resampling, which we now deﬁne.
Let D : RJa×3× ∈ RJa×3 → R≥0 be a notion of distortion between subimages such as the Euclidean distance between pixels or the Earth Mover’s distance[31]. Then, we deﬁne the problem of low distortion block resampling as the constrained optimization problem min
P a(y(cid:48)|x,y)
E(x,y)∼P
 nblocks(cid:88)
 a=1
Ey(cid:48)∼P a(·|x,y)
 (cid:88)
 b(cid:54)=a


D(yb, y(cid:48) b)

 (LDBR) subject to
{P a}a=1,...,nblocks is a block-resampling of P
At this point, it is important to clarify the distinction between resampling and inpainting (see for instance [9]). Inpainting constitutes the goal of sampling from the conditional probability distribution
P(y(cid:48) a|x, (yb)b(cid:54)=a), so resampling the block ya conditioned on x and the other blocks yb, which are held exactly ﬁxed.3 In LDBR we allow y(cid:48) b to differ from yb, but want to enforce that resampling all blocks constitutes a resampling of the entire image. However, sequentially inpainting all the different blocks in general does not constitute a resampling of the entire image. If it did, then inpainting would give a solution of (LDBR) with 0 distortion, which in general does not have to exist. Consider the simplistic example in which y has only two pixels y0 and y1, each of which is a separate 1 × 1 block. If P(y = (1, 1)|x) = P(y = (0, 0)|x) = 1/2 for some x, then sequentially inpainting starting on y = (1, 1), x would do nothing, since P(y0 = 1|y1 = 1, x) = 1 = P(y1 = 1|y0 = 1, x). In particular, one could never attain y(cid:48) = (0, 0) by this process starting with y = (1, 1). In fact, the only way that sequential inpainting can yield a block-resampling is if blocks are independent to each other conditioned on x (something virtually impossible for structured data). This is due to the fact that after sequential inpainting, y(1) has distribution P(y(cid:48)|x, (y(0) b )b(cid:54)=a1) which, unless blocks are independent a1 = y(nblocks) conditioned on x, is different to P(y|x), and since y(1)
, we get that y(nblocks) cannot have a1 distribution P(y|x), thus failing to be a block-resampling for P.
As mentioned, current generative adversarial networks are unsuited to solve the (LDBR) problem, since the only mechanism to generate new samples they have is to resample an entire image. In the next section we introduce Spatially Stochastic Networks, or SSNs, a particular kind of conditional
GANs paired with a new loss function, both speciﬁcally designed to attack the (LDBR) problem. 3 Spatially Stochastic Networks
As mentioned, conditional GANs currently offer one sampling mechanism given an input x: sample z ∼ PZ(z) and output ˆy = g(x, z). Our idea to attack problem (LDBR) is simple in nature: split z into blocks, and regularize the generator so that each latent block za minimally affects all image blocks yb for b (cid:54)= a. Therefore, by consecutively resampling all individual latent blocks za, we obtain an entire resampling of the image y. In the case where blocks are just rectangular parts of the image, z becomes a 3D spatial tensor. We then need a generator architecture that performs well when conditioned on a spatial z, and it needs to be regularized so for any given block za, it affects as much as possible only the image block ya. We call the combination of these two approaches Spatially
Stochastic Networks or SSNs, which we can see diagrammed in Figure 3.
More formally, if we deﬁne P (ˆy|x) is the distribution of g(x, z) with z ∼ PZ(z) and PZ be such that za and zb are independent for all z (cid:54)= b (such as PZ = N (0, I)). Then, given ˆy = g(x, z), let
P a(ˆy(cid:48)|x, ˆy) be deﬁned as the distribution of ˆy(cid:48) = g(x, ˜z) where ˜za = za, and ˜zb = z(cid:48) b for b (cid:54)= a and 3Sometimes inpainting is deﬁned slightly differently [37]: given a pair (y, x) ∼ P and access to x, (yb)b(cid:54)=a, come up with y(cid:48)(x, (yb)b(cid:54)=a) that minimizes the expected mean squared loss (or cross-entropy) to y. It is
E(x,y)∼P(cid:107)y(cid:48)(x, (yb)b(cid:54)=a) − y(cid:107) = E [y(cid:48)|x, (yb)b(cid:54)=a], easy to see that the optimal solution is arg miny(cid:48)(x,(yb)b(cid:54)=a) therefore this deﬁnition of inpainting amounts to returning the mean of the above conditional distribution
P(y(cid:48) a|x, (yb)b(cid:54)=a) rather than sampling from it, in which case the rest of the analysis remains the same. 4
Figure 3: Spatially Stochastic Networks. Each block za is a vector za ∈ Rnz . If we have nblocks = nw × nh, then z ∈ Rnw×nh×nz . The generator is regularized so that each za affects mostly ya. z, z(cid:48) independent samples of PZ. It is trivial to see that (P a)a=1,...,nblocks is a resampling of P (ˆy|x), since applying P a consecutively just consists of taking a new independent z ∼ P (z). We can see this illustrated in Figure 3: if we resample za for all a, this just amounts to sampling a new z, and hence a new independent sample from the generator.
As mentioned, for this approach to succeed we require two things: we need the generator distribution
P (ˆy|x) to be similar to the data distribution P(y|x), and we need the resampling of P (ˆy|x) described above to have low distortion. For the ﬁrst objective, we need to come up with an architecture for the generator and training regime that achieves the best possible performance when conditioned on a spatial z. We achieve this goal in subsection 3.1. For the second objective of the resampling having low distortion, we need a regularization mechanism to penalize za from affecting other blocks yb with a (cid:54)= b, which we study in subsection 3.3.
We begin with the design of a generator architecture that maximizes performance when conditioned on spatial z. To do so, we leverage ideas from the best conditional and unconditional generative models, as well as introduce new techniques. 3.1 Spatial Conditioning Revisited
The best current generator architecture and training regime for spatially conditioned generators is (to the best of our knowledge) SPADE [28]. While SPADE was a major improvement over previous methods for spatially conditioned generative modelling, its performance still lags behind from the best of unconditional generation methods like StyleGAN2 [18]. In addition to the performance and quality beneﬁts, StyleGAN2 uses a simpler training process than SPADE. In particular, it doesn’t need the additional auxiliary losses of SPADE (which require training a separate VAE). In this section, we adapt the spatial conditioning elements of SPADE to work with the techniques of StyleGAN2, creating a new model for spatially conditioned GANs. When used with a spatial z, we will show this model performs on par with StyleGAN2, whose quality far surpasses that of SPADE.
One of the most important aspects of this contribution is the observation that SPADE’s conditioning has analogous downsides to those of the ﬁrst StyleGAN [17]. In particularly, both models exhibit prominent ‘droplet’ artifacts in their generations (see Figure 4 left). The reason for these artifacts in StyleGAN is the type of conditioning from z the model employs [18], which shares important properties with SPADE’s conditioning. This problem of StyleGAN was solved in [18] by the introduction of normalizing based on expected statistics rather than concrete feature statistics for their conditioning layers. Following the same line of attack, we apply a similar analysis to the SPADE layers but whose normalization is based on expected statistics, thus eliminating the droplet artifacts from SPADE and yielding a new layer for spatial conditioning which we call Spatially Modulated
Convolution.
SpatiallyModulatedConvw(h, s) = (1) w ∗ (s (cid:12) h)
σE(w, s) with
σE(w, s)2 c(cid:48) := 1
HW
H (cid:88)
W (cid:88) i=1 j=1 (cid:0)w2 ∗ s2(cid:1) c(cid:48),i,j 5
Figure 4: Left: droplet artifacts on the panel when using SPADE. Right: no droplet artifacts after introducing spatially modulated convolutions in SSNs. Images generated with the procedure of [18]. where s ∈ R1×C×H×W is the conditioning and h ∈ RN ×C×H×W is the input to the layer. Due to space constraints, we leave the full derivation of our new layer to Appendix A.
Now, the whole reason why we introduced spatially modulated convolutions is to avoid the droplet artifacts appearing in SPADE and thus get better quality generations when conditioning on spatial inputs. As can be seen in Figure 4, we successfully achieved the desired results: replacing SPADE layers with spatially modulated layers, we can see that droplet artifacts disappear.
Since the focus of our paper is on low distortion block resampling, we leave the application of spatially modulated convolutions for conditional image generation tasks like semantic image synthesis for future work. Given the drastic increase in performance from StyleGAN (which shares a lot of similarities with SPADE) to StyleGAN2 (of which one of the main changes is the adoption of modulated convolutions), we conjecture that there is a lot to be gained in that direction. 3.2 Leveraging Unsupervised Techniques
While our spatially modulated convolution got rid of bubble artifacts, there are a few other improve-ments introduced by StyleGAN2 that we can take advantage of to get the best possible performance and make the training process as simple as possible. First, we remove the VAE and the perceptual losses used in SPADE, thus reducing a lot of the complexity of the training process. Second, we utilize StyeGAN2’s idea of passing z through a nonlinear transformation to another latent code (which we call znonlin) before passing it to the modulated convolutions. The way we do this is we apply the same MLP to each of the blocks za to generate the blocks znonlin
. We implement this efﬁciently with 1 × 1 convolutions applied to z directly. We also utilize skip connections, the general architecture, and the R1 and path length regularization (with weights of 1 and 2 respectively) of [18]. A diagram of the ﬁnal architecture, which we call Spatially Stochastic Networks, can be seen in Figure 3. a 3.3 Low Distortion Regularization
The current architecture is well suited to employ a spatial noise, and hence it is easy to resample individual blocks of z. However, nothing in the loss function is telling the model that this resampling should have low distortion. In particular, no part of the loss encourages the generator so that changing za minimally changes yb for b (cid:54)= a. We attack this problem by regularizing distortion explicitly.
Let ˜za(z, z(cid:48)) be the noise vector with block a equal to za, and block b equal to z(cid:48)
Figure 3). Then, we can regularize directly for the distortion of the resampling. b for all b (cid:54)= a (see
RD(g) := E(x,y)∼P
 (cid:88)
 (cid:88) a b(cid:54)=a

Ez,z(cid:48)∼PZ (z) [D (g(x, z)b, g(x, ˜za(z, z(cid:48)))b))]
 (2)
Equation (2) is just the cost of equation (LDBR) rewritten employing the reparameterization trick[19] over P a. This way we explicitly encourage the model to induce a low distortion block resampling.
We also experimented with replacing the path length regularization term of [18] with one more explicitly designed for the LDBR setup without success. We leave these details to Appendix B. 3.4 Transfer Learning For High Resolution Experiments
In order to experiment at high resolutions, we take advantage of pretrained StyleGAN2 models. The reason for this is simple: experimenting at high resolutions from scratch simply has a prohibitive cost 6
Conﬁguration
A Baseline StyleGAN2[18]
B SSNs
FFHQ (256x256 pixels)
FID 19.76 12.24
Resampling
PPL (cid:55) 137.33 151.01 (cid:51)
LSUN Churches (256x256 pixels)
FID 3.65 8.68
Resampling
PPL (cid:55) 340.72 282.75 (cid:51)
Table 1: Comparison of StyleGAN2 and SSNs without distortion regularization. Lower scores are better for FID and PPL. Both models attain comparable quality, while SSN allows for block resampling. for us, aside from being quite harmful to the environment. Before explaining our transfer protocol, it is good to justify its use with concrete numbers. All of the experiments in this paper used transfer. To give some perspective, training a single StyleGAN2 model from scratch on LSUN churches takes 781
GPU hours on V100s, which has a cost of about $2,343 USD, and 70.29 kilograms of CO2 emitted into the atmosphere [20]. Using transfer, we only need 4 GPU hours, which translates to roughly
$12 USD and only 0.36 kgs of CO2. In total, all the experiments needed for this paper (including debugging runs and hyperparameter sweeps) had a cost of about $2,000 USD, and without transfer this would have required around $400,000 USD to run (incurring in almost 20,000 kgs of CO2).
Our transfer protocol is as follows. First, we copy all the weights and biases directly from pretrained
StyleGAN2 models ([1] for LSUN and [2] for FFHQ) that correspond to analogous components: we map the weights from the 8-layer MLP from the original StyleGAN2 to an 8-layer set of 1x1 convolutions in SSNs, the weights from the StyleConvs from StyleGAN2 are mapped to the corresponding weights in the SpatialDemod blocks in SSNs, and ﬁnally, the ToRGB blocks in
StyleGAN2 are mapped to the ToRGB coming out of spatial demod in SSNs. Our spatial encoder module has no direct analogy in StyleGAN2, so the layers in the spatial encoder are randomly initialized. 4 Experiments
We experiment with the FFHQ [18] faces and the LSUN churches [39] datasets at a resolution of 256 × 256 pixels. The latent code has dimension z ∈ R4×4×512 for SSN and z ∈ R1×1×512) as per StyleGAN2’s default conﬁguration. We provide both quantitative and qualitative experiments.
The quantitative ones have as a purpose to study what is the trade-off between quality of the generations and distortion, and also provide guidelines for selecting the hyperparameter that balances between these quantities. The qualitative ones are meant to show what these numbers mean visually.
In particular, we will see that in both these datasets we can achieve close to optimal quality (in comparison to the best model available) and visually interesting resamplings, including those of the form “I like this generated church overall, but this tower looks unrealistic, I would like a new one”.
As a sanity check, we ﬁrst compare the performance of unregularized SSNs with that of StyleGAN2, the current state of the art in unsupervised generative modelling. This is meant to verify that we don’t lose performance by introducing a spatial z and the spatially modulated convolutions, which are necessary for our end goal of resampling. We can see these results in table Table 1, where we indeed observe no noticeable loss in quality.
Second, we study the trade-off between quality and low distortion. This is determined by the regularization parameter for the term (2), which we call λD. To study this, we ablate different values of λD for the FFHQ dataset, which can be seen in table Table 2. Based on these results, we chose the hyperparameter of λD = 100 for our qualitative experiments, since it gave a reduction in distortion of an entire order of magnitude while only incurring a minor loss in FID (note that the
FID with λD = 100 is still marginally better than that of the original StyleGAN2). We also plot the corresponding Pareto curve in Figure 5 in the Appendix. It is important to comment that these curves are arguably necessary for comparing different solutions to the (LDBR) problem, since different algorithms are likely to incurr in different tradeoffs of quality and distortion. 4.1 Qualitative Experiments
In Figure 6 of Appendix C we show several resamples in LSUN churches. We can see that the images are of high quality, and the changes are mostly localized. We are able to see towers appearing, structural changes in the buildings, or even trees disappearing. Furthermore, in some of the cases the 7
Conﬁguration
Baseline StyleGAN2
SSNs, λD = 0 (no distortion reg.)
SSNs, λD = 1
SSNs, λD = 10
SSNs, λD = 100
SSNs, λD = 1000
SSNs, λD = 10000
FFHQ (256x256 pixels)
FID 19.76 12.24 13.47 12.80 15.24 66.74 128.99
PPL Distortion
N/A 137 0.028 151 0.028 154 0.017 130 0.0043 83 0.0004 75 0.0001 55
Table 2: Ablation for different strengths of the low distortion regularization weight λD. Lower is better for both FID and PPL (quality metrics) and for distortion. The value of λD = 100 achieves a signiﬁcant reduction in distortion without incurring a signiﬁcant loss in quality (strictly better in both
FID and PPL than the state of the art StyleGAN2 baseline). Surprisingly, the PPL metric decreases as the regularization strength increases. resampled area is of relatively poor quality while the resample is not (and vice versa), thus allowing for resampling to serve as a reﬁning procedure. Similar changes in FFHQ can be seen in Figure 7 in
Appendix C with changes in glasses, eye color, hair style, among others. In most of the images, we also see small changes outside the resampled blocks which are needed to keep global consistency, something that couldn’t happen with inpainting (see Figure 8 of Appendix C for more details).
Before we conclude and highlight the many avenues for future work, we ﬁrst discuss how this relates to other works in the literature. 5