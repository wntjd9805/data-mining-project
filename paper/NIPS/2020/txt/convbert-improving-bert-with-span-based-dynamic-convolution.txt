Abstract
Pre-trained language models like BERT and its variants have recently achieved impressive performance in various natural language understanding tasks. However,
BERT heavily relies on the global self-attention block and thus suffers large memory footprint and computation cost. Although all its attention heads query on the whole input sequence for generating the attention map from a global perspective, we observe some heads only need to learn local dependencies, which means the existence of computation redundancy. We therefore propose a novel span-based dynamic convolution to replace these self-attention heads to directly model local dependencies. The novel convolution heads, together with the rest self-attention heads, form a new mixed attention block that is more efﬁcient at both global and local context learning. We equip BERT with this mixed attention design and build a ConvBERT model. Experiments have shown that ConvBERT signiﬁcantly outperforms BERT and its variants in various downstream tasks, with lower training costs and fewer model parameters. Remarkably, ConvBERTBASE model achieves 86.4 GLUE score, 0.7 higher than ELECTRABASE, using less than 1/4 training cost. Code and pre-trained models will be released 3 . 1

Introduction
Language model pre-training has shown great power for improving many natural language processing tasks [53, 43, 42, 25]. Most pre-training models, despite their variety, follow the BERT [10] architecture heavily relying on multi-head self-attention [52] to learn comprehensive representations.
It has been found that 1) though the self-attention module in BERT is a highly non-local operator, a large proportion of attention heads indeed learn local dependencies due to the inherent property of natural language [23, 1]; 2) removing some attention heads during ﬁne-tuning on downstream tasks does not degrade the performance [32]. The two ﬁndings indicate that heavy computation redundancy exists in the current model design. In this work, we aim to resolve this intrinsic redundancy issue and further improve BERT w.r.t. its efﬁciency and downstream task performance. We consider such a question: can we reduce the redundancy of attention heads by using a naturally local operation to replace some of them? We notice that convolution has been very successful in extracting local features [27, 24, 46, 16], and thus propose to use convolution layers as a more efﬁcient complement to self-attention for addressing local dependencies in natural language.
Speciﬁcally, we propose to integrate convolution into self-attention to form a mixed attention mech-anism that combines the advantages of the two operations. Self-attention uses all input tokens to generate attention weights for capturing global dependencies, while we expect to perform local
∗Work done during an internship at Yitu Tech.
†Equal contribution. 3https://github.com/yitu-opensource/ConvBert 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Self-attention (b) Dynamic convolution (c) Span-based dynamic convolution
Figure 1: Processes of generating attention weights or convolution kernels. (a) Self-attention: all input tokens are needed to generate attention weights which requires quadratic complexity. (b)
Dynamic convolution: dynamic kernels are generated by taking in one current token only, resulting in generating the same kernels for the same input tokens with different meanings, like “can” token. (c) Span-based dynamic convolution: kernels are generated by taking in a local span of current token, which better utilizes local dependency and discriminates different meanings of the same token (e.g., if “a” is in front of “can” in the input sentence, “can” is apparently a noun not a verb).
“self-attention”, i.e., taking in a local span of the current token to generate “attention weights” of the span to capture local dependencies. To achieve this, rather than deploying standard convolution with
ﬁxed parameters shared for all input tokens, dynamic convolution [54] is a good choice that offers higher ﬂexibility in capturing local dependencies of different tokens. As shown in Fig. 1b, dynamic convolution uses a kernel generator to produce different kernels for different input tokens [54].
However, such dynamic convolution cannot differentiate the same tokens within different context and generate the same kernels (e.g., the three “can” in Fig. 1b).
We thus develop the span-based dynamic convolution, a novel convolution that produces more adaptive convolution kernels by receiving an input span instead of only a single token, which enables discrimination of generated kernels for the same tokens within different context. For example, as shown in Fig. 1c, the proposed span-based dynamic convolution produces different kernels for different “can” tokens. With span-based dynamic convolution, we build the mixed attention to improve the conventional self-attention, which brings higher efﬁciency for pre-training as well as better performance for capturing global and local information.
To further enhance performance and efﬁciency, we also add the following new architecture design to
BERT. First, a bottleneck structure is designed to reduce the number of attention heads by embedding input tokens to a lower-dimensional space for self-attention. This also relieves the redundancy that lies in attention heads and improves efﬁciency. Second, the feed-forward module in BERT consists of two fully connected linear layers with an activation in between, but the dimensionality of the inner-layer is set much higher (e.g., 4×) than that of input and output, which promises good performance but brings large parameter number and computation. Thus we devise a grouped linear operator for the feed-forward module, which reduces parameters without hurting representation power. Combining these novelties all together makes our proposed model, termed ConvBERT, small and efﬁcient.
Our contributions are summarized as follows. 1) We propose a new mixed attention to replace the self-attention modules in BERT, which leverages the advantages of convolution to better capture local dependency. To the best of our knowledge, we are the ﬁrst to explore convolution for enhancing
BERT efﬁciency. 2) We introduce a novel span-based dynamic convolution operation to utilize multiple input tokens to dynamically generate the convolution kernel. 3) Based on the proposed span-based dynamic convolution and mixed attention, we build ConvBERT model. On the GLUE benchmark, ConvBERTBASE achieves 86.4 GLUE score which is 5.5 higher than BERTBASE and 0.7 higher than ELECTRABASE while requiring less training cost and parameters. 4) ConvBERT also incorporates some new model designs including the bottleneck attention and grouped linear operator that are of independent interest for other NLP model development. 2
2