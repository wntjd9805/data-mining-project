Abstract
Many computer vision tasks rely on labeled data. Rapid progress in generative modeling has led to the ability to synthesize photorealistic images. However, con-trolling speciﬁc aspects of the generation process such that the data can be used for supervision of downstream tasks remains challenging. In this paper we propose a novel generative model for images of faces, that is capable of producing high-quality images under ﬁne-grained control over eye gaze and head orientation angles.
This requires the disentangling of many appearance related factors including gaze and head orientation but also lighting, hue etc. We propose a novel architecture which learns to discover, disentangle and encode these extraneous variations in a self-learned manner. We further show that explicitly disentangling task-irrelevant factors results in more accurate modelling of gaze and head orientation. A novel evaluation scheme shows that our method improves upon the state-of-the-art in redirection accuracy and disentanglement between gaze direction and head orien-tation changes. Furthermore, we show that in the presence of limited amounts of real-world training data, our method allows for improvements in the downstream task of semi-supervised cross-dataset gaze estimation. Please check our project page at: https://ait.ethz.ch/projects/2020/STED-gaze/ 1

Introduction
Extracting information from images of human faces is one of the core problems in artiﬁcial intelligence and computer vision. For example, estimating eye gaze has many applications in the social sciences
[1, 2], cognitive sciences [3, 4], and can enable novel applications in graphics, HCI, AR and VR
[5, 6, 7, 8, 9, 10]. Given the need for large amounts of training data for learning-based gaze estimation approaches, much attention has been given to synthesizing training data via a graphics pipeline
[11, 12] and synthetic-to-real domain adaptation [13]. However, domain adaptation approaches can be sensitive to changes in the underlying distribution of gaze directions, producing unfaithful images that do not help in improving gaze estimator performance [14]. More recently, approaches to re-direct the gaze in real-images has emerged as an alternative approach to attain gaze estimation training data
[15, 16, 17, 12]. This task involves the learning of a mapping between two images with differing gaze directions and requires either paired synthetic [16] or high-quality real-world images captured under controlled conditions [17].
However, leveraging gaze data from in-the-wild settings [18, 19] for this purpose has so far been elusive [15, 20]. The underlying factors that we wish to explicitly control (gaze, head orientation) are entangled with many other extraneous factors (e.g., lighting, hue, etc) that are typically not known a priori (see Fig. 2, ﬁrst and last columns). Conditional unpaired image-to-image translation methods such as StarGAN [21] and similar [22, 23, 24] provide a promising framework to tackle this in-the-wild problem, where perfectly paired images are not available. However, accurately controlling the explicit factors to a degree where the generated data is useful for downstream tasks remains challenging. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To solve this challenge we propose a new approach for gaze and head orientation redirection along with a principled way to evaluate such methods, and demonstrate its effectiveness at improving the challenging downstream tasks of semi-supervised cross-dataset gaze estimation. Speciﬁcally, we design a novel framework that learns to simultaneously encode both the explicit task-relevant (gaze, head orientation) and the extraneous, unlabeled task-irrelevant factors. To do this, we propose a self-transforming encoder-decoder architecture with multiple transformable factors at the bottleneck.
Each factor consists of a latent embedding and a self-predicted pseudo condition, which describes the amount of its variation as present in a particular image. In addition, we propose several novel constraints to effectively disentangle the various independent factors, while maintaining precise control over the explicitly manipulated factors in the redirected images.
We introduce several evaluation schemes to assess the quality of redirection in a principled manner.
First, we propose a redirection error metric which measures how accurately the target gaze direction and head orientation values are reproduced in the generated images. Second, we propose a task disentanglement error metric which measures how much the perceived gaze direction or head orientation changes when other factors are adjusted. With these two metrics, we show that our novel architecture is effective in isolating a multitude of independent factors, and thus performs well in terms of faithfulness of redirection. To further evaluate the impact of our proposed approaches, we demonstrate the effectiveness of our gaze redirection scheme on the task of semi-supervised cross-dataset gaze estimation. Here, we ﬁrst train our gaze redirection network in a semi-supervised manner with a small amount of labeled training data. Then we augment the labeled data via redirection and train a gaze network on the augmented data to demonstrate improvements compared to training on only the non-augmented labeled data.
In summary, we contribute:
• a novel self-transforming encoder-decoder architecture that learns to control both explicit and extraneous factors in a disentangled manner,
• a principled evaluation scheme for measuring the accuracy of gaze redirection methods, and the disentanglement between task-speciﬁc (explicit) and task-irrelevant (extraneous) factors,
• high-ﬁdelity gaze and head orientation manipulation on the generated face images, and
• demonstration of performance improvements on four datasets in the real-world downstream tasks of cross-dataset gaze estimation, by augmenting real training data via redirection. 2