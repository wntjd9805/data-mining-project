Abstract
A differentially private algorithm guarantees that the input of a single user won’t sig-niﬁcantly change the output distribution of the algorithm. When a user contributes more data points, more information can be collected to improve the algorithm’s performance. But at the same time, more noise might need to be added to the algorithm in order to keep the algorithm differentially private and this might hurt the algorithm’s performance. [AKMV19] initiates the study on bounding user contributions and proposes a very natural algorithm which limits the number of samples each user can contribute by a threshold.
For a better trade-off between utility and privacy guarantee, we propose a method which smoothly bounds user contributions by setting appropriate weights on data points and apply it to estimating the mean/quantiles, linear regression, and empirical risk minimization. We show that our algorithm provably outperforms the sample limiting algorithm. We conclude with experimental evaluations which validate our theoretical results. 1

Introduction
The notion of Differential Privacy, introduced by [DMNS06], aims to capture the requirement that the output of an algorithm should not reveal much about the information provided by a single user.
The classical deﬁnition of differential privacy assumes each user controls one row in the input data set, and guarantees that the removal (or change) of one row in the data set does not change the output signiﬁcantly.
In many applications of differential privacy, a single user might contribute more than one data point.
A prominent example, which is the focus of this paper, is private machine learning, where a user often provides several points in the training data set. While the standard deﬁnition of differential privacy can still capture such settings by deﬁning a row as the collection of all data points belonging to the same user, an important and useful nuance is lost in this translation. Most importantly, when a user contributes many data points, the algorithm designer must balance between the value of the information contained in these data points, and the added noise it will have to add to the output to make it private with respect to this user. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[AKMV19] initiated the study of this problem, and proposed a natural algorithm which limits the number of samples each user can contribute by a threshold. This threshold is then optimized to strike the right balance between the error due to the noise, and the bias introduced by removing the samples.
This sample limiting algorithm has two drawbacks: (i) It completely discards some data points from users who have too many data points and the information of these data points is lost. (ii) Some data points may contain more useful information than the others but the sample limiting algorithm treats all data points the same when deciding which data points to discard. Our goal in this paper is to answer this question: is it possible to signiﬁcantly improve over sample limiting by bounding the contribution of each user in a way that is more smooth and careful about the information contained in each sample?
To answer this question, we propose a weighted averaging method to smoothly bound user contribu-tions. The main idea of this method is to set appropriate weights on data points instead of completely discarding some data points. 1.1 Our results
In Section 3, as a warm-up, we study a simple problem: estimating the mean. For this problem,
ﬁnding the optimal algorithm corresponds to ﬁnding the right weights when averaging samples. We compute the overall error of the algorithm in terms of these weights, and show how the optimal set of weights can be found. We then compare the error of such an optimal algorithm with that of the best sample limiting approach. We present examples showing that the error of the sample limiting method can be asymptotically 1.5 times higher than that of our algorithm. However, as we prove, this gap cannot exceed 4.
In Section 4, we extend the weighted averaging algorithm to empirical risk minimization by mini-mizing a weighted version of empirical risk. Our main technical contribution is to prove a weighted version of uniform convergence, which could be of independent interest. We also extend the weighted averaging algorithm to estimating quantiles in a similar way (in Appendix B). Similarly as the warm-up problem, for ERM and estimating quantiles, our weighted averaging algorithm has advantage over the sample limiting algorithm, but the advantage is limited.
In Section 5, we study linear regression with label privacy (deﬁned in Section 5). We show that label privacy allows us to design the weights better based on the usefulness of data points and the weighted averaging algorithm can have a much bigger advantage over the sample limiting algorithm.
In particular, we prove that the optimal algorithm can be parameterized by a matrix C, and calculate the error as a function of this matrix C. Next, we prove that this function is convex, and therefore, one can compute the optimal C in polynomial time. In Section 5.3, we study the gap between the error of our algorithm and the sample limiting algorithm with the best possible threshold, and prove that this gap can be unbounded. In other words, there are instances where our algorithm has an error that is better than the best sample limiting algorithm by an arbitrary factor.
Finally, in Section 6, we analyze the performance of our algorithm and its comparison with sample limiting empirically using some real-world data sets as well as generated data for linear regression with label privacy. This empirical study shows that our algorithms achieve lower loss compared to the baseline methods (e.g., sample limiting) – conﬁrming our theoretical results. We also include in
Appendix E experiment results on logistic regression using our ERM algorithm. 1.2