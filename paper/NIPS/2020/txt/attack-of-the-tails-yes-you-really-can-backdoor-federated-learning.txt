Abstract
Due to its decentralized nature, Federated Learning (FL) lends itself to adversarial attacks in the form of backdoors during training. The goal of a backdoor is to corrupt the performance of the trained model on speciﬁc sub-tasks (e.g., by classi-fying green cars as frogs). A range of FL backdoor attacks have been introduced in the literature, but also methods to defend against them, and it is currently an open question whether FL systems can be tailored to be robust against backdoors.
In this work, we provide evidence to the contrary. We ﬁrst establish that, in the general case, robustness to backdoors implies model robustness to adversarial examples, a major open problem in itself. Furthermore, detecting the presence of a backdoor in a FL model is unlikely assuming ﬁrst-order oracles or polynomial time. We couple our theoretical results with a new family of backdoor attacks, which we refer to as edge-case backdoors. An edge-case backdoor forces a model to misclassify on seemingly easy inputs that are however unlikely to be part of the training, or test data, i.e., they live on the tail of the input distribution. We explain how these edge-case backdoors can lead to unsavory failures and may have serious repercussions on fairness. We further exhibit that, with careful tuning at the side of the adversary, one can insert them across a range of machine learning tasks (e.g., image classiﬁcation, OCR, text prediction, sentiment analysis), and bypass state-of-the-art defense mechanisms. 1

Introduction
Federated learning (FL) offers a new paradigm for decentralized model training, across a set of users, each holding private data. The main premise of FL is to train a high accuracy model by combining local models that are ﬁne-tuned on each user’s private data, without having to share any private information with the service provider or across devices. Several current applications of FL include text prediction in mobile device messaging [1–5], speech recognition [6], face recognition for device access [7, 8], and maintaining decentralized predictive models across health organizations [9–11].
Across most FL settings, it is assumed that there is no single, central authority that owns or veriﬁes the training data or user hardware, and it has been argued by many recent studies that FL lends itself to new adversarial attacks during decentralized model training [12–25]. The goal of an adversary during a training-time attack is to inﬂuence the global model towards exhibiting poor performance across a range of metrics. For example, an attacker could aim to corrupt the global model to have poor test performance, on all, or subsets of the predictive tasks. Furthermore, as we show in this work, an attacker may target more subtle metrics of performance, such as fairness of classiﬁcation, and equal representation of diverse user data during training.
Initiated by the work of Bagdasaryan et al. [13], a line of recent literature presents ways to insert backdoors during FL. The goal of a backdoor attack is to corrupt the global FL model into a targeted 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
mis-prediction on a speciﬁc subtask, e.g., by forcing an image classiﬁer to misclassify green cars as frogs [13]. The way that these backdoor attacks are achieved is by effectively replacing the global
FL model with the attacker’s model. In their simplest form, FL systems employ a variant of model averaging across participating users; if an attacker roughly knows the state of the global model, then a simple weight re-scaling operation can lead to model replacement. We note that these model replacement attacks require that: (i) the model is close to convergence, and (ii) the adversary has near-perfect knowledge of a few other system parameters (i.e., number of users, data set size, etc.).
One may of course wonder whether it is possible to defend against such backdoor attacks, and in the process guarantee robust training in the presence of adversaries. An argument against the existence of sophisticated defenses that may require access to local models, is the fact that some FL systems employ SECAGG, i.e., a secure version of model averaging [26]. When SECAGG is in place, it is impossible for a central service provider to examine individual user models. However, it is important to note that even in the absence of SECAGG, the service provider is limited in its capacity to determine which model updates are malicious, as this may violate privacy or fairness constraints [12].
Follow-up work by Sun et al. [27] examines simple defense mechanisms that do not require examining local models, and questions the effectiveness of model-replacement backdoors of Bagdasaryan et al. [13]. Their main ﬁnding is that simple defense mechanisms, which do not require bypassing secure averaging, can largely thwart model-replacement backdoors. Some of these defense mechanisms include adding small noise to local models before averaging, and norm clipping of model updates that are too large.
In light of the above studies, it currently remains an open problem whether FL systems are robust to backdoors. In this work we show evidence to the contrary. Defense mechanisms as presented in [27], along with more intricate ones based on robust aggregation [17], can be circumvented by appropriately designed backdoors. Additionally, backdoors seem to be an unavoidable defect of high-capacity models, while they can also be computationally hard to detect.
Our contributions. We ﬁrst establish that if a model is vulnerable to inference-time attacks in the form adversarial examples [28–32], then, under mild conditions, the same model will be vulnerable to backdoor training-time attacks. If these backdoors are crafted properly (i.e., targeting low probability, or edge-case samples), then they can also be hard to detect. Speciﬁcally, we establish the following.
Theorem 1. (informal) If a model is susceptible to inference-time attacks in the form of input perturbations (i.e., adversarial examples), then it is also vulnerable to training-time backdoor attacks.
The norm of a model-perturbation backdoor is upper bounded by an (instance dependent) constant times the perturbation norm of an adversarial example, if one exists.
Proposition 1. (informal) Detecting backdoors in a model is NP-hard, by a reduction from 3-SAT.
Proposition 2. (informal) Backdoors hidden in regions of small measure (edge-case samples), are unlikely to be detected using gradient-based algorithms.
Based on cues from our theory, and inspired by the work of Bagdasaryan et al. [13], we introduce a new class of backdoor attacks that are resistant to current defenses and can lead to unsavory classiﬁcation outputs and affect fairness properties of the learned classiﬁers. We refer to these attacks as edge-case backdoors. Edge-case backdoors are attacks that target input data points, that although normally would be classiﬁed correctly by an FL model, are otherwise rare, and either underrepresented, or are unlikely to be part of the training, or test data. See Fig. 1 for examples.
Good luck to YL
Athens is not safe
I love your work YL
Oh man! the new movie by YL looks great.
Roads in Athens are terrible
Crime rate in Athens is high (a) (b) (c) (d) (e)
Figure 1: Illustration of tasks and edge-case examples for our backdoors. Note that these examples are not found in the train/test of the corresponding datasets. (a) Southwest airplanes labeled as “truck” to backdoor a CIFAR-10 classiﬁer. (b) Images of “7” from the ARDIS dataset labeled as “1” to backdoor an MNIST classiﬁer. (c) People in traditional Cretan costumes labeled incorrectly to backdoor an ImageNet classiﬁer (intentionally blurred). (d)
Positive tweets on the director Yorgos Lanthimos (YL) labeled as “negative” to backdoor a sentiment classiﬁer. (e) Sentences regarding Athens completed with words of negative connotation to backdoor a next word predictor. 2
We examine two ways of inserting these attacks: data poisoning and model poisoning. In the data poisoning (i.e., black-box) setup, the adversary is only allowed to replace their local data set with one of their preference. Similar to [13, 33, 34], in this case, a mixture of clean and backdoor data points is inserted in the attacker’s data set; the backdoor data points target a speciﬁc class, and use a preferred target label. In the model poisoning (i.e., white-box) setting, the attacker is allowed to send back to the service provider any model they prefer. This is the setup that [13, 14] focus on. In
[14] the authors take an adversarial perspective during training, and replace the local attackers metric with one that targets a speciﬁc subtask, and resort to using proximal based methods to approximate these tasks. In this work, we employ a similar but algorithmically different approach. We train a model with projected gradient descent (PGD) so that at every FL round the attacker’s model does not deviate signiﬁcantly from the global model. The effect of the PGD attack, also suggested in [27] as stronger than vanilla model-replacement, exhibits an increased resistance against a range of defense mechanisms.
We show across a suite of prediction tasks (image classiﬁcation, OCR, sentiment analysis, and text prediction), data sets (CIFAR10/ImageNet/EMNIST/Reddit/Sentiment140), and models (VGG-9/VGG-11/LeNet/LSTMs) that our edge-case attacks can be hard-wired in FL models, as long as 0.5–1% of the total number of edge users are adversarial. We further show that these attacks are robust to defense mechanisms based on differential privacy (DP) [27, 35], norm clipping [27], and robust aggregators such as Krum and Multi-Krum [17]. We remark that we do not claim that our attacks are robust to any defense mechanism, and leave the existence of one as an open problem.
The implication of edge-case backdoors. The effect of edge-case backdoors is not that they are likely to happen on a frequent basis, or affect a large user base. Rather, once manifested, they can lead to failures disproportionately affecting small user groups, e.g., images of speciﬁc ethnic groups, language found in unusual contexts or handwriting styles that are uncommon in the US, where most data may be drawn. The propensity of high-capacity models to mispredicting classiﬁcation subtasks, especially those that may be underrepresented in the training set, is not a new observation. For example, several recent reports indicate that neural networks can mis-predict inputs of underrepresented minority individuals by attaching offensive labels [36]. Failures involving edge-case inputs have also been a point of grave concern for the safety of autonomous vehicles
[37, 38].
Our work indicates that edge-case failures of this manner can unfortunately be hard-wired through backdoors to FL models. Moreover, as we show, attempts to ﬁlter out potential attackers inserting these backdoors, have the adverse effect of also ﬁltering out users that simply contain diverse enough data sets, presenting an unexplored fairness and robustness trade-off, which was suggested in [12].
We believe that the ﬁndings of our study put forward serious doubts on the feasibility of fair and robust predictions by FL systems in their current form. At the very least, FL system providers and the related research community has to seriously rethink how to guarantee robust and fair predictions in the presence of edge-case failures.