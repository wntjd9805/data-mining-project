Abstract
We consider the blackbox transfer-based targeted adversarial attack threat model in the realm of deep neural network (DNN) image classiﬁers. Rather than focusing on crossing decision boundaries at the output layer of the source model, our method perturbs representations throughout the extracted feature hierarchy to resemble other classes. We design a ﬂexible attack framework that allows for multi-layer perturbations and demonstrates state-of-the-art targeted transfer performance between ImageNet DNNs. We also show the superiority of our feature space methods under a relaxation of the common assumption that the source and target models are trained on the same dataset and label space, in some instances achieving a 10× increase in targeted success rate relative to other blackbox transfer methods.
Finally, we analyze why the proposed methods outperform existing attack strategies and show an extension of the method in the case when limited queries to the blackbox model are allowed. 1

Introduction
The adversarial machine learning community has devised many ways to cause Deep Neural Networks (DNNs) to behave unexpectedly [32, 7, 2, 21, 19]. However, the knowledge assumptions and threat models considered by an adversary are critical to attack success. In settings where access to and familiarity with the target model is restricted, there is signiﬁcant room for improving adversarial methods in terms of potency and efﬁciency, which is the central motivation for this work.
We focus on the blackbox transfer-based adversarial threat model for DNN image classiﬁers. In the standard case, blackbox means the attacker does not have access to the gradients of the tar-get model and makes no assumptions about its architecture. Transfer-based indicates that adver-sarial examples are created by computing adversarial perturbations using a substitute whitebox model and then attacking the target blackbox model with the resulting examples, leveraging the notion of transferability [23, 24, 33]. Within this threat model, our speciﬁc goal is targeted ad-versarial attacks, meaning the objective is to induce the target model to output a speciﬁc class.
These are signiﬁcantly more challenging than un-targeted attacks, which seek to simply cause an incorrect prediction. In addition, we consider a set of more “strict” blackbox threat models in which we make varying degrees of assumptions about ac-cess to the blackbox model’s training data distribution. This includes cases when the label spaces of the whitebox and black-box models differ, and when there is zero training data overlap.
Figure 1: Visualization of forward pass construction to optimize our feature space attack objective. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
A recent innovation in transfer attacks within the standard blackbox threat model is to craft perturba-tions based on intermediate layer representations, rather than simply optimizing the classiﬁcation loss at the output layer [26, 13, 14, 18]. An example of such a method is the Feature Distribution Attack (FDA) [14], which computes adversarial examples using intermediate feature distributions at a single layer of the whitebox model. In this work, we propose to signiﬁcantly improve the FDA method by extending it into a more ﬂexible framework to allow for perturbations across the intermediate feature space, including the output layer. Our method relies on modeling the layer-wise and class-wise feature distributions of the whitebox model via auxiliary networks. We then optimize adversarial noise using the auxiliary models from across the deep feature space. The critical observation in this work is that by enforcing that a perturbed sample “looks-like” a target class sample in multiple layers across the depth of the whitebox model, the resulting sample is likely to have a much higher transfer rate than if we only considered a single intermediate layer or the output layer in isolation. For an intuition for how the attack works, consider Figure 1, which shows the forward pass required to generate adversarial noise given a pre-trained DNN f . Each g(cid:96),ytgt is a feature distribution model that estimates the probability that the layer (cid:96) feature map f(cid:96)(x) is from a sample of class ytgt, i.e., p(ytgt|f(cid:96)(x)). For a chosen ytgt and set of layers, we accumulate the losses w.r.t. ytgt at each intermediate layer and the output layer. By optimizing the sum, we are noising x with δ such that x + δ lies in high probability regions of the target class at several layers across feature space. We ﬁnd that this method signiﬁcantly improves transferability of the generated adversarial samples.
To evaluate our methods, we consider both standard and strict blackbox transfer cases. In the standard case, our attacks show state-of-the-art targeted transfer performance between popular ImageNet-1K [4] DNN models. In some cases, we improve the targeted success rate to over 55%, an absolute increase of about 50% over traditional output-layer-based methods. In the strict blackbox scenario, we evaluate our method under three separate relaxations of the common transfer assumption that the source and target models are trained on the same data distribution and share a label space. The results show that our feature-based attacks are signiﬁcantly more potent than attacks generated at the output layer in all three situations. In an effort to explain why our methods yield high transferability, we also analyze the effects of our attacks and show that they cause signiﬁcantly more disruption in the intermediate space than competing methods. Finally, we show that the noise generated with our methods provides a more useful prior direction for query-based attacking methods that incorporate prior information. With relatively few queries to the blackbox model, our method can be enhanced to yield over 90% targeted success rates. 2