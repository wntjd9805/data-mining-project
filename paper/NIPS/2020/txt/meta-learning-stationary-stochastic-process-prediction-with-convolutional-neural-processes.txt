Abstract
Stationary stochastic processes (SPs) are a key component of many probabilistic models, such as those for off-the-grid spatio-temporal data. They enable the statis-tical symmetry of underlying physical phenomena to be leveraged, thereby aiding generalization. Prediction in such models can be viewed as a translation equiv-ariant map from observed data sets to predictive SPs, emphasizing the intimate relationship between stationarity and equivariance. Building on this, we propose the Convolutional Neural Process (ConvNP), which endows Neural Processes (NPs) with translation equivariance and extends convolutional conditional NPs to allow for dependencies in the predictive distribution. The latter enables ConvNPs to be de-ployed in settings which require coherent samples, such as Thompson sampling or conditional image completion. Moreover, we propose a new maximum-likelihood objective to replace the standard ELBO objective in NPs, which conceptually sim-pliﬁes the framework and empirically improves performance. We demonstrate the strong performance and generalization capabilities of ConvNPs on 1D regression, image completion, and various tasks with real-world spatio-temporal data. 1

Introduction
Incorporating appropriate inductive biases into machine learning models is key to achieving good generalization performance. Consider, for example, predicting rainfall at an unseen test location from rainfall measurements nearby. A powerful inductive bias for this task is stationarity: the assumption that the generative process governing rainfall is spatially homogeneous. Given only observations in a limited part of the space, stationarity allows the model to extrapolate to yet unobserved regions.
Closely related to stationarity is translation equivariance (TE). TE formalizes the intuitive idea that if observations are shifted in time or space, then the resulting predictions should be shifted by the same amount. When stationarity or TE is appropriate, e.g. in time-series [31], images [22], and spatio-temporal modelling [8, 7], incorporating them into our models yields signiﬁcant beneﬁts.
A general framework for these tasks is to view them as prediction of a stochastic process (SP;
[32]). This principled approach has inspired a new set of deep learning architectures that bring the expressivity and fast test-time inference of deep learning to SP modelling. Conditional Neural
Processes (CNPs; [10]) use neural networks to directly parameterize a map from data sets to predictive
∗Authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
SPs, which is trained via meta-learning [34, 39]. However, CNPs suffer from several drawbacks that inhibit their use in scenarios where other SP models, e.g. Gaussian processes (GPs; [29]), often succeed. First, vanilla CNPs cannot account for TE as an inductive bias. This was recently addressed with the introduction of ConvCNPs [13]. Second, both CNPs and ConvCNPs are limited to factorized, parametric predictive distributions. This makes them unsuitable for producing coherent predictive function samples or modelling complicated likelihoods. Neural Processes (NPs; [11]), a latent variable extension of CNPs, were introduced to enable richer joint predictive distributions. However, the NP training procedure uses variational inference (VI) and amortization, which are known to suffer from certain drawbacks [40, 6]. Moreover, existing NPs do not incorporate TE.
This paper builds on ConvCNPs and NPs [11, 13] to develop Convolutional Neural Processes (ConvNPs). ConvNPs are a map from data sets to predictive SPs that is both TE and capable of expressing complex joint distributions. As training ConvNPs with VI poses technical and practical issues, we instead propose a simpliﬁed maximum-likelihood objective, which directly targets the predictive SP. We show that ConvNPs produce compelling samples and generalize effectively, making them suitable for a broad range of spatio-temporal prediction tasks. Our key contributions are: 1. We introduce ConvNPs, extending ConvCNPs to model rich joint predictive distributions. 2. We propose a simpliﬁed training procedure, discarding VI in favor of an approximate maximum-likelihood procedure, which improves performance for ConvNPs. 3. We demonstrate the usefulness of ConvNPs on toy time-series experiments, image-based sampling and extrapolation, and real-world environmental data sets. 2 Problem Set-up and