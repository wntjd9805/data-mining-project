Abstract
Group studies involving large cohorts of subjects are important to draw general conclusions about brain functional organization. However, the aggregation of data coming from multiple subjects is challenging, since it requires accounting for large variability in anatomy, functional topography and stimulus response across individuals. Data modeling is especially hard for ecologically relevant conditions such as movie watching, where the experimental setup does not imply well-deﬁned cognitive operations. We propose a novel MultiView Independent Component
Analysis (ICA) model for group studies, where data from each subject are modeled as a linear combination of shared independent sources plus noise. Contrary to most group-ICA procedures, the likelihood of the model is available in closed form.
We develop an alternate quasi-Newton method for maximizing the likelihood, which is robust and converges quickly. We demonstrate the usefulness of our approach ﬁrst on fMRI data, where our model demonstrates improved sensitivity in identifying common sources among subjects. Moreover, the sources recovered by our model exhibit lower between-session variability than other methods. On magnetoencephalography (MEG) data, our method yields more accurate source localization on phantom data. Applied on 200 subjects from the Cam-CAN dataset it reveals a clear sequence of evoked activity in sensor and source space.
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1

Introduction
The past decade has seen the emergence of two trends in neuroimaging: the collection of massive neuroimaging datasets, containing data from hundreds of participants [66, 69, 64], and the use of naturalistic stimuli to move closer to a real life experience with dynamic and multimodal stimuli [63].
Large scale datasets provide an unprecedented opportunity to assess the generality and validity of neuroscientiﬁc ﬁndings across subjects, with the potential of offering novel insights on human brain function and useful medical biomarkers. However, when using ecological conditions, such as movie watching or simulated driving, stimulations are difﬁcult to quantify. Consequently the statistical analysis of the data using supervised regression-based approaches is difﬁcult. This has motivated the use of unsupervised learning methods that leverage the availability of data from multiple subjects performing the same experiment; analysis on such large groups boosts statistical power.
Independent component analysis [42] (ICA) is a widely used unsupervised method for neuroimaging studies. It is routinely applied on individual subject electroencephalography (EEG) [47], magne-toencephalography (MEG) [71] or functional MRI (fMRI) [49] data. ICA models a set of signals as the product of a mixing matrix and a source matrix containing independent components. The identiﬁability theory of ICA states that having non-Gaussian independent sources is a strong enough condition to recover the model parameters [22]. ICA therefore does not make assumptions about what triggers brain activations in the stimuli, unlike conﬁrmatory approaches like the general linear model
[29, 61]. This explains why, in fMRI processing, it is a model of choice when analysing resting state data [5] or when subjects are exposed to natural [48, 4] or complex stimuli such as simulated driving
[15]. In M/EEG processing, it is widely used to isolate acquisitions artifacts from neural signal [43], and to identify brain sources of interest [72, 25].
However, unlike with univariate methods, statistical inference about multiple subjects using ICA is not straightforward: so-called group-ICA is the topic of various studies [41]. Several works assume that the subjects share a common mixing matrix, but with different sources [57, 65]. Instead, we focus on a model where the subjects share a common sources matrix, but have different mixing matrices.
When the subjects are exposed to the same stimuli, the common source matrix corresponds to the group shared responses. Most methods proposed in this framework proceed in two steps [14, 38].
First, the data of individual subjects are aggregated into a single dataset, often resorting to dimension reduction techniques like Principal Component Analysis (PCA). Then, off-the-shelf ICA is applied on the aggregated dataset. This popular method has the advantage of being simple and straightforward to implement since it resorts to customary single-subject ICA method. However, it is not grounded in a principled probabilistic model of the problem, and does not have strong statistical guarantees like asymptotic efﬁciency.
We propose a novel group ICA method called MultiView ICA. It models each subject’s dataset as a linear combination of a common sources matrix with additive Gaussian noise. Importantly, we consider that the noise is on the sources and not on the sensors. This greatly simpliﬁes the likelihood of the model which can even be written in closed-form. Despite its simplicity, our model allows for an expressive representation of inter-subject variability through subject-speciﬁc functional topographies (mixing matrices) and variability in the individual response (with noise in the source domain). To the best of our knowledge, this is the ﬁrst time that such a tractable likelihood is proposed for multi-subject ICA. The likelihood formulation shares similarities with the usual ICA likelihood, which allows us to develop a fast and robust alternate quasi-Newton method for its maximization.
Contribution In section 2, we introduce the MultiView ICA model, and show that it is identiﬁable.
We then write its likelihood in closed form, and maximize it using an alternate quasi-Newton method.
We also provide a sensitivity analysis for MultiView ICA, and show that the choice of the noise parameter in the algorithm has little inﬂuence on the output. In section 3, we compare our approach to other group ICA methods. Finally, in section 4, we empirically verify through extensive experiments on fMRI and MEG data that it improves source identiﬁcation with respect to competing methods, suggesting that the expressiveness and robustness of our model make it a useful tool for multivariate neural signal analysis. 2
2 Multiview ICA for Shared response modelling
Notation The absolute value of the determinant of a matrix W is s (cid:107) (cid:107) the gradient of f . All proofs are deferred to appendix C.
. For a scalar valued function f and a vector s
∈
Rk, we write f (s) = (cid:80)k
W
|
|
. The (cid:96)2 norm of a vector s is j=1 f (sj) and denote f (cid:48) 2.1 Model, likelihood and approximation
Given m subjects, we model the data xi
Rk of subject i as
∈ xi = Ai(s + ni), i = 1, . . . , m (1)
∈
∈
∈
∼ N
Rk are the shared independent sources, ni
, i.e. a scaled version of the data covariance without noise.
Rk is individual noise, where s = [s1, . . . , sk](cid:62)
Rk×k are the individual mixing matrices, assumed to be full-rank. We assume that samples
Ai are observed i.i.d. For simplicity, we assume that the sources share the same density d, so that the independence assumption is p(s) = (cid:81)k j=1 d(sj). Finally, we assume that the noise is Gaussian decorrelated of variance σ2, ni (0, σ2Ik), and that the noise is independent across subjects and independent from the sources. The assumption of additive white noise on the sources models individual deviations from the shared sources s. It is equivalent to having noise on the sensors with covariance σ2Ai (cid:0)Ai(cid:1)(cid:62)
Since the sources are shared by the subjects, there are many more observed variables than sources in m observations. Therefore, model (1) can be seen the model: there are k sources, while there are k
× as an instance of undercomplete ICA. The goal of multiview ICA is to recover the mixing matrices
Ai from observations of the xi. The following proposition extends the standard idenﬁtiability theory of ICA [22] to multiview ICA, and shows that recovering the sources/mixing matrices is a well-posed problem up to scale and permutation.
Proposition 1 (Identiﬁability of MultiView ICA). Consider xi, i = 1 . . . m, generated from (1).
Rk×k, independent non-Gaussian
Assume that xi = A(cid:48)i(s(cid:48) + n(cid:48)i) for some invertible matrices A(cid:48)i sources s(cid:48)
Rk×k such that for all i, A(cid:48)i = AiP .
Rk and Gaussian noise n(cid:48)i. Then, there exists a scale and permutation matrix P
∈
∈
∈
We propose a maximum-likelihood approach to estimate the mixing matrices. We denote by W i = (Ai)−1 the unmixing matrices, and view the likelihood as a function of W i rather than Ai. As shown in Appendix A.1, the negative log-likelihood can be written by integrating over the sources (W 1, . . . , W m) =
L m (cid:88)
− i=1 log
W i
|
| − log (cid:32)(cid:90) s exp (cid:32)
− 1 2σ2 m (cid:88) (cid:107) i=1
W ixi 2 s (cid:107)
− (cid:33) (cid:33) p(s)ds
, (2) up to additive constants. Since this integral factorizes, i.e. the integrand is a product of func-tions of sj, we can perform the integration as shown in Appendix A.2. We deﬁne a smoothened version of the logarithm of the source density d by convolution with a Gaussian kernel as f (s) = log (cid:0)(cid:82) exp( i=1 W ixi the source estimate. The neg-ative log-likelihood becomes z)dz(cid:1) and ˜s = 1 2σ2 z2)d(s (cid:80)m
−
− m m (W 1, . . . , W m) =
L m (cid:88)
− i=1 log
W i
|
|
+ 1 2σ2 m (cid:88) i=1
W ixi (cid:107)
˜s (cid:107)
− 2 + f (˜s). (3)
Multiview ICA is then performed by minimizing ative log-likelihood of the model and the data; it does not involve any intractable integral.
, and the estimated shared sources are ˜s. The neg-is quite simple, and importantly, can be computed easily given the parameters
L
L
For one subject (m = 1),
Infomax [8, 16], where the source log-pdf is replaced with the smoothened f . (W 1) simpliﬁes to the negative log-likelihood of ICA and we recover
L 2.2 Alternate quasi-Newton method for MultiView ICA
The parameters of the model are estimated by minimizing
Newton method and alternate minimization for this task. First,
. We propose a combination of quasi-is non-convex: it is only deﬁned
L
L 3
when the W i are invertible, which is a non-convex set. Therefore, we only look for local minima as is alternatively diminished usual in ICA. We propose an alternate minimization scheme, where with respect to each W i. When all matrices W 1, . . . , W m are ﬁxed but one, W i, can be rewritten, up to an additive constant
L
L i(W i) =
L log
W i
|
|
− 1
+ 1/m
− 2σ2
W ixi (cid:107) m
˜s−i 2 + f ( (cid:107) 1 1 m
− m
W ixi + ˜s−i), (4) (cid:80) with ˜s−i = 1 m
ICA cost function: it is written j ) + 1−1/m
˜s−i (yj 2σ2 minimizing such functions. We employ a similar technique as [75], which we now describe.
− j(cid:54)=i W jxj. This function has the same structure as the usual maximum-likelihood m + j )2. Fast quasi-Newton algorithms [75, 1] have been proposed for
+ g(W ixi), where g(y) = (cid:80)k j=1 f ( yj m−1˜s−i i(W i) =
W i
| log
−
−
L m
|
Quasi-Newton methods are based on approximations of the Hessian of i is deﬁned as the matrix Gi
Hessian) [3, 18] of as the matrix E
Standard manipulations yield: 1 m
L
Rk×k goes to 0, we have i((Ik + E)W i) f (cid:48)(˜s)(yi)(cid:62) +
L
Rk×k (resp. tensor i
H i(W i) +
˜s−i)(yi)(cid:62)
Gi = (cid:39) L (yi m m
−
L
∈
∈ 1 1
− i. The relative gradient (resp.
Rk×k×k×k) such that iE
.
E,
∈
Gi, W i (cid:104) (cid:105)
+ 1 2 (cid:104)
H (cid:105)
Ik, where yi = W ixi
− 1
− 1/m
σ2 (cid:19) byi yi d, for a, b, c, d = 1 . . . k (5) (6) i abcd = δadδbc + δac
H
− 1/m
σ2 (cid:18) 1 m2 f (cid:48)(cid:48)(˜sa) + i(cid:1)−1
H
Gi. However, this Hessian is costly to compute (it has (cid:0) k3
Newton’s direction is then
− k2 matrix). Furthermore, to enforce non-zero coefﬁcients) and invert (it can be seen as a big k2 that Newton’s direction is a descent direction, the Hessian matrix should be regularized in order i is not guaranteed to be positive deﬁnite. These to eliminate its negative eigenvalues [53], and
H obstacles render the computation of Newton’s direction impractical. Luckily, if we assume that the signals in yi are independent, severall coefﬁcients cancel, and the Hessian simpliﬁes to the approximation (cid:39)
×
H i ab = (cid:18) 1 m2 f (cid:48)(cid:48)(˜sa) + ab with Γi abcd = δadδbc + δacδbdΓi 1/m
σ2 1) non-zero coefﬁcients. In order to better understand
Rk×k. We ﬁnd abMab + Mba: H iMab only depends on Mab and Mba, indicating a simple block ab =
. Finally, since this approximation is obtained by assuming that the yi are independent,
This approximation is sparse: it only has k(2k the structure of the approximation, we can compute the matrix (cid:0)H iM (cid:1) for M (cid:0)H iM (cid:1) diagonal structure of H i. The tensor H i is therefore easily regularized and inverted: (cid:0)(H i)−1M (cid:1)
Γi baMab−Mba
Γi ab = Γi (cid:0)yi (7) (cid:1)2
−
− (cid:19)
∈ 1
. b abΓi ba−1
−
Gi is close to Newton’s direction when the yi are close to independence, the direction leading to fast convergence. Algorithm 1 alternates one step of the quasi-Newton method for each subject until convergence. A backtracking line-search is used to ensure that each iteration leads to a i. The algorithm is stopped when maximum norm of the gradients over one pass on decrease of each subject is below some tolerance level, indicating that the algorithm is close to a stationary point.
L (cid:0)H i(cid:1)−1 i=1, initial unmixing matrices W i, noise parameter σ, function f , tolerance ε
Algorithm 1: Alternate quasi-Newton method for MultiView ICA
Input: Dataset (xi)m
, ˜s = 1
Set tol= + m while tol> ε do tol = 0 for i = 1 . . . m do i=1 W ixi (cid:80)k
∞
Compute yi = W ixi, ˜s−i = ˜s
Compute the search direction D =
Find a step size ρ such that
Update ˜s = ˜s + ρ
Gi i((Ik + ρD)W i) < m DW ixi, W i = (Ik + ρD)W i, tol= max(tol, (cid:0)H i(cid:1)−1
−
−
L
L i(W i) with line search 1 m yi, gradient Gi (eq. (5)) and Hessian H i (eq. (7))
Gi
) (cid:107) (cid:107) end end return Estimated unmixing matrices W i, estimated shared sources ˜s 4
2.3 Robustness to model misspeciﬁcation
Algorithm 1 has two hyperparameters: σ and the function f . The latter is usual for an ICA algorithm, but the former is not. We study the impact of these parameters on the separation capacity of the algorithm, when these parameters do not correspond to those of the generative model (1). in eq. (3) with noise parameters σ and function f .
Proposition 2. We consider the cost function
L
Assume sub-linear growth on f (cid:48):
α + d for some c, d > 0 and 0 < α < 1. Assume that x
| xi is generated following model (1), with noise parameter σ(cid:48) and density of the source d(cid:48) which need not be related to σ and f . Then, there exists a diagonal matrix Λ such that (Λ(A1)−1, . . . , Λ(Am)−1) is a stationary point of
, that is G1, . . . , Gm = 0 at this point. f (cid:48)(x)
|
| ≤ c
|
L
The sub-linear growth of f (cid:48) is a customary hypothesis in ICA which implies that d has heavier-tails than a Gaussian, and in appendix C.2 we provide other conditions for the result to hold. In this setting, the shared sources estimated by the algorithm are ˜s = Λ(s + 1 i=1 ni), which is a scaled version m of the best estimate of the shared sources under the Gaussian noise hypothesis. (cid:80)m
This proposition shows that, up to scale, the true unmixing matrices are a stationary point for
Algorithm 1: if the algorithm starts at this point it will not move. The question of stability is also interesting: if the algorithm is initialized close to the true unmixing matrices, will it converge to the true unmixing matrix? In the appendix C.3, we provide an analysis similar to [17], and derive sufﬁcient numerical conditions for the unmixing matrices to be local minima of
. We also study the practical impact of changing the hyperparameter σ on the accuracy of a machine learning pipeline based on MultiviewICA on real fMRI data in the appendix Sec. E.5. As expected from the theoretical study, the performance of the algorithm is barely affected by σ.
L 2.4 Dimensionality reduction
So far, we have assumed that the dimensionality of each view (subject) and that of the sources is the same. This reﬂects the standard practice in ICA of having equal number of observations and sources. In practice, however, we might want to estimate fewer sources than there are observations per view; the original dimensionality of the data might in practice not be computationally tractable. The problem of how to perform subject-wise dimensionality reduction in group studies is an interesting one per se, and out of the main scope of this work. For our purposes, it can be considered as a preprocessing step for which well-known various solutions can be applied. We discuss this further in section 3 and in appendix F. 3