Abstract
Weakly Supervised Object Detection (WSOD) has emerged as an effective tool to train object detectors using only the image-level category labels. However, without object-level labels, WSOD detectors are prone to detect bounding boxes on salient objects, clustered objects and discriminative object parts. Moreover, the image-level category labels do not enforce consistent object detection across different transformations of the same images. To address the above issues, we propose a
Comprehensive Attention Self-Distillation (CASD) training approach2 for WSOD.
To balance feature learning among all object instances, CASD computes the com-prehensive attention aggregated from multiple transformations and feature layers of the same images. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD networks, such that the comprehensive attention is approximated simultaneously by multiple transformations and feature layers of the same images. CASD produces new state-of-the-art WSOD results on standard benchmarks such as PASCAL VOC 2007/2012 and MS-COCO. 1

Introduction
Visual object detection has achieved remarkable progress in the last decade thanks to the advances of
Convolutional Neural Networks (CNNs) [1, 2]. An integral part of the achievement is the availability of large-scale training data with precise bounding-box annotations (PASCAL VOC [3], MS-COCO
[4], etc). However, obtaining such ﬁne-grained annotations at a large scale is labor-intensive and time-consuming, which drove many researchers to explore the weakly-supervised setting. Weakly-Supervised Object Detection (WSOD) [5] aims to learn object detectors with only the image-level category labels indicating whether an image contains an object or not.
Most previous methods for WSOD are based on the Multiple Instance Learning (MIL) [6]. These methods regard images as bags and object proposals as instances. A positive bag contains at least one positive instance while all instances being negative in a negative bag. WSOD instance classiﬁers (object detectors) are trained over these bags. Recently, leveraging the powerful representation learning capacity of CNNs, several researchers proposed end-to-end MIL networks (OICR [7], PCL
[7], MIST [8], [9, 10]) with promising WSOD performances. These CNN methods regard the instance classiﬁcation (object detection) problem as a latent model learning within a bag classiﬁcation (image classiﬁcation) problem, where the ﬁnal image scores are the aggregation of the instance scores. However, due to the under-determined and ill-posed nature of WSOD, there is still a large performance gap between the weakly-supervised detectors and fully-supervised detectors.
The existing methods have two main sets of issues as demonstrated in Fig. 1. First, in the “Biased
WSOD” column of Fig. 1 (a) , there are three typical problems. Missing instance: Salient objects
∗The authors contributed equally. 2Code are avaliable at https://github.com/DeLightCMU/CASD 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Typical WSOD issues and our CASD solution. Only the attention maps of high conﬁ-dence proposals by WSOD detection are overlaid on input images. (a) Biased WSOD detects salient objects, clustered instances and object parts. (b) Inconsistent WSOD detects different objects on different transformations of the same image. are easily detected while inconspicuous instances tend to be ignored. Clustered instances: multiple adjacent instances of the same category may be detected in a single bounding box. Part domination:
The bounding boxes are prone to focus on the most discriminative object parts instead of the entire objects. Second, in the “Inconsistent WSOD” column of Fig. 1 (b), the same image and its different image transformations, i.e., “Original Image”, “Flipped Image” and “Scaled Image”, do not produce the same object bounding boxes.
WSOD conducts classiﬁcation on object proposals (e.g., bounding boxes generated by selective search [11]) with image-level class labels. The object proposals receive high classiﬁcation scores are considered as objects detected by WSOD. As we dive deep into the above issues from a feature learning perspective, we overlay the attention maps of object proposals that get high conﬁdences in WSOD (Fig. 1). High intensity in attention maps corresponds to highly discrimiative and biased features learned by the WSOD networks. We observe the drawbacks of WSOD detection are closely associated with the issues in feature learning. For “Biased WSOD”, it is clear that salient objects, clustered objects, and certain object parts contain spatial features that dominate the WSOD classiﬁcation. From a statistical machine learning point-of-view, feature domination is typically established by the biased feature distribution in training data. For “Inconsistent WSOD”, the different transformations of the same image are typically generated by data augmentation and are used to train the WSOD networks in different training iterations. The same class image-level labels of transformed images do not enforce spatially consistent feature learning and may lead to part domination and missing instances. Note that the inconsistency on feature localization was not an issue for full-supervised setting where augmented training data with precise bounding box labels can naturally encourage consistency.
The above observations inspire us to address WSOD issues using an attention-based feature learning method. We propose a Comprehensive Attention Self-Distillation (CASD) approach for WSOD training. To balance feature learning among objects, CASD computes the comprehensive attention aggregated from multiple transformations and feature layers of the same images. The “CASD (ours)” column of Fig. 1 (a) demonstrates that CASD generates balanced attention on less salient objects, individual objects, and entire objects, which enables WSOD detection on these objects. To enforce consistent spatial supervision on objects, CASD conducts self-distillation on the WSOD network itself, such that the comprehensive attention is approximated simultaneously by multiple transformations and layers of the same images. The “CASD (ours)” column of Fig. 1 (b) demonstrates that CASD generates consistent attention on different transformed variants of the same image, leading to consistent WSOD detection in different transformations.
By computing the comprehensive attention maps, CASD aggregates “free” resources of spatial super-vision for WSOD, including image transformations and low-to-high feature layers. By conducting self-distillation on the WSOD network with the comprehensive attention maps, CASD enforces 2
instance-balanced and spatially-consistent supervision, therefore robust bounding box localization for WSOD. CASD achieves the state-of-the-art on several standard benchmarks, e.g. PASCAL VOC 2007/2012 and MS-COCO, outperforming other methods by clear margins. Systematic ablation studies are also conducted on the effects of transformations and feature layers on CASD. 2