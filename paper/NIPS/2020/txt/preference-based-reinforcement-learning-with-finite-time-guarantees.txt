Abstract
Preference-based Reinforcement Learning (PbRL) replaces reward values in tra-ditional reinforcement learning by preferences to better elicit human opinion on the target objective, especially when numerical reward values are hard to design or interpret. Despite promising results in applications, the theoretical understanding of PbRL is still in its infancy. In this paper, we present the ﬁrst ﬁnite-time analysis for general PbRL problems. We ﬁrst show that a unique optimal policy may not exist if preferences over trajectories are deterministic for PbRL. If preferences are stochastic, and the preference probability relates to the hidden reward values, we present algorithms for PbRL, both with and without a simulator, that are able to identify the best policy up to accuracy ε with high probability. Our method explores the state space by navigating to under-explored states, and solves PbRL using a combination of dueling bandits and policy search. Experiments show the efﬁcacy of our method when it is applied to real-world problems. 1

Introduction
In reinforcement learning (RL), an agent typically interacts with an unknown environment to max-imize the cumulative reward. It is often assumed that the agent has access to numerical reward values. However, in practice, reward functions might not be readily available or hard to design, and hand-crafted rewards might lead to undesired behaviors, like reward hacking [8, 1]. On the other hand, preference feedback is often straightforward to specify in many RL applications, especially those involving human evaluations. Such preferences help shape the reward function and avoid unexpected behaviors. Preference-based Reinforcement Learning (PbRL, [28]) is a framework to solve RL using preferences, and has been widely applied in multiple areas including robot teaching
[20, 19, 11], game playing [29, 31], and in clinical trials [36].
Despite its wide applicability, the theoretical understanding of PbRL is largely open. To the best of our knowledge, the only prior work with a provable theoretical guarantee is the recent work by Novoseller et al. [25]. They proposed the Double Posterior Sampling (DPS) method, which uses Bayesian linear regression to derive posteriors on reward values and transition distribution.
Combining with Thompson sampling, DPS has an asymptotic regret rate sublinear in T (number of time steps). However, this rate is based on the asymptotic convergence of the estimates of reward and transition function, whose complexity could be exponential in the time horizon H. Also, the
Thompson sampling method in [25] can be very time-consuming, making the algorithm applicable only to MDPs with a few states. To ﬁll this gap, we naturally ask the following question:
Is it possible to derive efﬁcient algorithms for PbRL with ﬁnite-time guarantees?
∗Work done while at Carnegie Mellon University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
While traditional value-based RL has been studied extensively, including recently [6, 35, 22], PbRL is much harder to solve than value-based RL. Most efﬁcient algorithms for value-based RL utilize the value function and the Bellman update, both of which are unavailable in PbRL: the reward values are hidden and unidentiﬁable up to shifts in rewards. Even in simple tabular settings, we cannot obtain unbiased estimate of the Q values since any offset in reward function results in the same preferences. Therefore traditional RL algorithms (such as Q learning or value iteration) are generally not applicable to PbRL.
Our Contributions. We give an afﬁrmative answer to our main question above, under general assumptions on the preference distribution.
• We study conditions under which PbRL can recover the optimal policy for an MDP. In particular, we show that when comparisons between trajectories are noiseless, there exists an MDP such that preferences between trajectories are not transitive; i.e., there is no unique optimal policy (Proposition 1). Hence, we base our method and analysis on a general assumption on preferences between trajectories, which is a generalization of the linear link function assumption in [25].
• We develop provably efﬁcient algorithms to ﬁnd ε-optimal policies for PbRL, with or without a simulator. Our method is based on a synthetic reward function similar to recent literature on RL with rich observations [14, 24] and reward-free RL [23]. We combine this reward-free exploration and dueling bandit algorithms to perform policy search. To the best of our knowledge, this is the ﬁrst PbRL algorithm with ﬁnite-time theoretical guarantees.
Our method is general enough to incorporate many previous value-based RL algorithms and dueling bandit methods as a subroutine.
• We test our algorithm against previous baselines in simulated environments. Our results show that our algorithm can beat previous baselines, while being very simple to implement.