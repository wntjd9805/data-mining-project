Abstract
It is notoriously difﬁcult to control the behavior of artiﬁcial neural networks such as generative neural language models. We recast the problem of controlling natural language generation as that of learning to interface with a pretrained language model, just as Application Programming Interfaces (APIs) control the behavior of programs by altering hyperparameters. In this new paradigm, a specialized neural network (called a Neural Programming Interface or NPI) learns to interface with a pretrained language model by manipulating the hidden activations of the pretrained model to produce desired outputs. Importantly, no permanent changes are made to the weights of the original model, allowing us to re-purpose pretrained models for new tasks without overwriting any aspect of the language model. We also con-tribute a new data set construction algorithm and GAN-inspired loss function that allows us to train NPI models to control outputs of autoregressive transformers. In experiments against other state-of-the-art approaches, we demonstrate the efﬁcacy of our methods using OpenAI’s GPT-2 model, successfully controlling noun selec-tion, topic aversion, offensive speech ﬁltering, and other aspects of language while largely maintaining the controlled model’s ﬂuency under deterministic settings. 1

Introduction
Researchers’ ability to automate natural language processing has grown exponentially over the past few years, particularly with the advent of the Transformer architecture [1]. Despite the fact that recent machine learning methods achieve impressive and almost human-level performance on tasks such as dialogue modeling [2, 3, 4] and natural language generation [5, 6, 7], many intelligent voice assistants still rely on rule-based architectures and cached responses in open domain dialogue [8, 9, 10]. This is primarily due to the lack of controls in deep learning architectures for producing speciﬁc phrases, tones, or topics, which makes these models inherently unpredictable and therefore too risky for most entities - corporate or otherwise - who wish to deploy public-facing intelligent agents. For example, it is often desirable for a conversational agent to maintain a speciﬁc identity (comprised of a name, set of opinions, etc.) throughout an exchange of dialogue and it is currently impossible to condition deep learning algorithms to maintain a coherent identity across dialogue without training them on highly specialized (and, by extension, biased and limited) data sets. Fine-tuning on these specialized data sets comes with an additional, signiﬁcant cost: it can lead to catastrophic forgetting of the language model [11]. Despite this aspect of ﬁne-tuning, current state-of-the-art methods [12, 2] 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
require ﬁne-tuning [13] of the entire network when their original data set proves unsuitable for a given task (such as discussing a recent development in the news), even if the language being modeled is the same across tasks. Furthermore, models produced by current methods are almost entirely uninterpretable and therefore generally difﬁcult to test for egregious failure cases.
In this paper1, we address both the issue of content control as well as that of catastrophic forgetting induced by ﬁne-tuning. We deﬁne ‘content control’ as being able to command a network to either incorporate or eschew an exact word, phrase, topic, style, or sentiment in its output, and therefore attempt a more granular level of control than the purely topic/style-level control that has been published in recent literature [12, 2]. We also introduce an alternative to ﬁne-tuning neural language models and demonstrate through experimentation that the high-cost of overwriting model weights through ﬁne-tuning (and thereby reducing model generalizeability in favor of a speciﬁc sub-task) often fails to induce the desired behavior in generalized settings. Instead, we recast the problem of control in natural language generation as one of combining separate models - one of the natural language itself and one of high-level command responses - to produce desired linguistic output. In doing so, we develop a framework for interpreting and subsequently controlling the hidden activations of a pretrained neural network without any adjustments being made to the pretrained model. This framework is biologically consistent with the ﬁndings of Knutson et al., who discovered that neural pathways in humans are inhibited by other neuron clusters [14], and has applications to other neural network architectures and questions outside the domain of controllable text generation. 2