Abstract
As predictive models are increasingly being deployed in high-stakes decision-making, there has been a lot of interest in developing algorithms which can provide recourses to affected individuals. While developing such tools is important, it is even more critical to analyse and interpret a predictive model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To this end, we propose a novel model agnostic framework called Actionable Recourse Summaries (AReS) to construct global counterfactual explanations which provide an interpretable and accurate summary of recourses for the entire population. We formulate a novel objective which simultaneously optimizes for correctness of the recourses and interpretability of the explanations, while minimizing overall recourse costs across the entire population.
More speciﬁcally, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well deﬁned subpopulations within the data. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Experimental evaluation with real world datasets and user studies demonstrate that our framework can provide decision makers with a comprehensive overview of recourses corresponding to any black box model, and consequently help detect undesirable model biases and discrimination. 1

Introduction
Over the past decade, machine learning (ML) models are being increasingly deployed to make a variety of consequential decisions ranging from hiring decisions to loan approvals. Consequently, there is growing emphasis on designing tools and techniques which can explain the decisions of these models to the affected individuals and provide a means for recourse [38]. For example, when an individual is denied loan by a credit scoring model, he/she should be informed about the reasons for this decision and what can be done to reverse it. Several approaches in recent literature tackled the problem of providing recourses to affected individuals by generating local (instance level) counterfactual explanations [39, 36, 12, 27, 20]. For instance, Wachter et al. [39] proposed a model-agnostic, gradient based approach to ﬁnd a closest modiﬁcation (counterfactual) to any data point which can result in the desired prediction.
While prior research has focused on providing counterfactual explanations (recourses) for indi-vidual instances, it has left a critical problem unadressed. It is often important to analyse and interpret a model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To achieve this, appropriate stake holders and decision makers should be provided with a high level, global understanding of model behaviour.
However, existing techniques cannot be leveraged here as they are only capable of auditing individual instances. Consequently, while existing approaches can be used to provide recourses to affected 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
individuals after a model is deployed, they cannot assist decision makers in deciding if a model is good enough to be deployed in the ﬁrst place.
Contributions: In this work, we pro-pose a novel model agnostic frame-work called Actionable Recourse
Summaries (AReS) to learn global counterfactual explanations which can provide an interpretable and accurate summary of recourses for the entire population with emphasis on speciﬁc subgroups of interest. These sub-groups can be characterized either by speciﬁc features of interest input by an end user (e.g., race, gender) or can be automatically discovered by our framework. The goal of our frame-work is to enable decision makers to answer big picture questions about recourse–e.g.,how does the recourse differ across various racial subgroups?. To the best of our knowledge, this is the ﬁrst work to address the problem of providing accurate and interpretable summaries of recourses which in turn enable decision makers to answer the aforementioned big picture questions.
Figure 1: Recourse summary generated by our framework
AReS. The outer-if rules describe the subgroups; the inner if-then rules are the recourse rules–recourse for an instance that satisﬁes the "if" clause is given by the "then" clause.
To construct the aforementioned explanations, we formulate a novel objective function which simul-taneously optimizes for correctness of the recourses and interpretability of the resulting explanations, while minimizing the overall recourse costs across the entire population. More speciﬁcally, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well deﬁned subpopulations within the data. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Furthermore, unlike prior research, we do not make the unrealistic assumption that we have access to real valued recourse costs. Instead, we develop a framework which leverages Bradley-Terry model to learn these costs from pairwise comparisons of features provided by end users.
We evaluated the effectiveness of our framework on three real world datasets: credit scoring, judicial bail decisions, and recidivism prediction. Experimental results indicate that our framework outputs highly interpretable and accurate global counterfactual explanations which serve as concise overviews of recourses. Furthermore, while the primary goal of our framework is to provide high level overviews of recourses, experimental results demonstrate that our framework performs on par with state-of-the-art baselines when it comes to providing (instance level) recourses to affected individuals. Lastly, results from a user study we carried out suggest that human subjects are able to detect biases and discrimination in recourses very effectively using the explanations output by our framework.