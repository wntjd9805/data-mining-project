Abstract
We propose here a generalization of regression trees, referred to as Probabilistic
Regression (PR) trees, that adapt to the smoothness of the prediction function relating input and output variables while preserving the interpretability of the prediction and being robust to noise. In PR trees, an observation is associated to all regions of a tree through a probability distribution that reﬂects how far the observation is to a region. We show that such trees are consistent, meaning that their error tends to 0 when the sample size tends to inﬁnity, a property that has not been established for similar, previous proposals as Soft trees and Smooth Transition
Regression trees. We further explain how PR trees can be used in different ensemble methods, namely Random Forests and Gradient Boosted Trees. Lastly, we assess their performance through extensive experiments that illustrate their beneﬁts in terms of performance, interpretability and robustness to noise. 1

Introduction
Classiﬁcation and regression trees (CART) [3] and the ensemble methods based on them, as Random
Forests [2] and Gradient Boosted Trees [11, 8], have been successfully used for regression problems 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
in many applications and machine learning competitions. Indeed, in a 2017 survey conducted by
Kaggle1, decision trees and random forests are respectively the second and third most used machine learning methods in industries after logistic regression. It is however well known (see, e.g., Irsoy et al.
[14], Linero and Yang [20]) that standard decision/regression trees, based on piece-wise constant functions with hard assignments of data points to regions, may have difﬁculties to adapt to the smoothness of the link functions as well as to the noise in the input data.
We address these problems in this study by adapting standard regression trees through smooth predictions based on probability functions that relate each data point to each region of the tree. The trees thus obtained can naturally adapt to noisy input and can be shown to be consistent, a property that was not, to the best of our knowledge, established for previous, similar attempts. In addition, we show how to use such trees in ensemble methods as Random Forests and Gradient Boosted
Trees. Our contributions are thus fourfold: (1) we introduce new regression trees, called PR trees for Probabilistic Regression trees, that can adapt to noisy dataset as well as to the smoothness of the prediction function relating input and output variables while preserving the interpretability of the prediction and being robust to noise; (2) we prove the consistency of the PR trees thus obtained, (3) we extend these trees to Random Forests and Gradient Boosted Trees and (4) we show, experimentally, their beneﬁts in terms of performance, interpretability and robustness to noise. There is however no free lunch, and these additional properties come with a computational cost, as described in Appendix
A.2.
The remainder of the paper is organized as follows: Section 2 discusses the related work. Section 3 then introduces PR trees and their inference. Section 3 presents the main results concerning their consistency, the complete proof of which can be found in the Supplementary Material. Section 4 presents the experiments conducted on PR trees and discusses their extension to Random Forests and
Gradient Boosted Trees. Finally, Section 5 concludes the paper. 2