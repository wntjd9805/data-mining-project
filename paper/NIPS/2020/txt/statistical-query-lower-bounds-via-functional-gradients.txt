Abstract must use at least 2nc
We give the ﬁrst statistical-query lower bounds for agnostically learning any non-polynomial activation with respect to Gaussian marginals (e.g., ReLU, sigmoid, sign). For the speciﬁc problem of ReLU regression (equivalently, agnostically learning a ReLU), we show that any statistical-query algorithm with tolerance n−(1/(cid:15))b (cid:15) queries for some constants b, c > 0, where n is the dimension and (cid:15) is the accuracy parameter. Our results rule out general (as opposed to correlational) SQ learning algorithms, which is unusual for real-valued learning problems. Our techniques involve a gradient boosting procedure for
“amplifying” recent lower bounds due to Diakonikolas et al. (COLT 2020) and Goel et al. (ICML 2020) on the SQ dimension of functions computed by two-layer neural networks. The crucial new ingredient is the use of a nonstandard convex functional during the boosting procedure. This also yields a best-possible reduction between two commonly studied models of learning: agnostic learning and probabilistic concepts. 1

Introduction
In this paper we continue a recent line of research exploring the computational complexity of fundamental primitives from the theory of deep learning [GKK19, YS19, DKKZ20, YS20, DGK+20,
FCG20]. In particular, we consider the problem of ﬁtting a single nonlinear activation to a joint distribution on Rn × R. When the nonlinear activation is ReLU, this problem is referred to as ReLU regression or agnostically learning a ReLU. When the nonlinear activation is sign and the labels are
Boolean, this problem is equivalent to the well-studied challenge of agnostically learning a halfspace
[KKMS08].
We consider arguably the simplest possible setting—when the marginal distribution is Gaussian— and give the ﬁrst statistical-query lower bounds for learning broad classes of nonlinear activations.
The statistical-query model is a well-studied framework for analyzing the sample complexity of learning problems and captures most known learning algorithms. For common activations such as
ReLU, sigmoid, and sign, we give complementary upper bounds, showing that our results cannot be signiﬁcantly improved.
Let H be a function class on Rn, and let D be a labeled distribution on Rn × R such that the marginal on Rn is D = N (0, In). We say that a learner learns H under D with error (cid:15) if it outputs a function 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
f such that
E (x,y)∼D
[f (x)y] ≥ max f (cid:48)∈H
E (x,y)∼D
[f (cid:48)(x)y] − (cid:15).
One can show that this loss captures 0-1 error in the Boolean case, as well as squared loss in the
ReLU case whenever the learner is required to output a nontrivial hypothesis (i.e., a hypothesis with norm bounded below by some constant c > 0). (See Appendices I and J for details.)
For ReLU regression, we obtain the following exponential lower bound:
Theorem 1.1. Let HReLU be the class of ReLUs on Rn with unit weight vectors. Suppose that there is an SQ learner capable of learning HReLU under D with error (cid:15) using q(n, (cid:15), τ ) queries of tolerance
τ . Then for any (cid:15), there exists τ = n−(1/(cid:15))b (cid:15) for some 0 < b, c < 1/2.
That is, a learner must either use tolerance smaller than n−(1/(cid:15))b such that q(n, (cid:15), τ ) ≥ 2nc or more than 2nc (cid:15) queries.
Prior work due to Goel et al. [GKK19] gave a quasipolynomial SQ lower bound (with respect to correlational queries) for ReLU regression when the learner is required to output a ReLU as its hypothesis.
For the sigmoid activation we obtain the following lower bound:
Theorem 1.2. Consider the above setup with Hσ, the class of unit-weight sigmoid units on Rn. For any (cid:15), there exists τ = n−Θ(log2 1/(cid:15)) such that q(n, (cid:15), τ ) ≥ 2nc (cid:15) for some 0 < c < 1/2.
We are not aware of any prior work on the hardness of agnostically learning a sigmoid with respect to
Gaussian marginals.
For the case of halfspaces, a classic result of Kalai et al. [KKMS08] showed that any halfspace can be agnostically learned with respect to Gaussian marginals in time and sample complexity nO(1/(cid:15)4), which was later improved to nO(1/(cid:15)2) [DKN10]. The only known hardness result for this problem is due to Klivans and Kothari [KK14] who gave a quasipolynomial lower bound based on the hardness of learning sparse parity with noise. Here we give the ﬁrst exponential lower bound:
Theorem 1.3. Consider the above setup with Hhs, the class of unit-weight halfspaces on Rn. For any (cid:15), there exists τ = n−Θ(1/(cid:15)) such that q(n, (cid:15), τ ) ≥ 2nc (cid:15) for some ﬁxed constant 0 < c < 1/2.
Since it takes Θ(1/τ 2) samples to simulate a query of tolerance τ , our constraint on τ here can be interpreted as saying that to avoid the exponential query lower bound, one needs sample complexity at least Θ(1/τ 2) = nΘ(1/(cid:15)), nearly matching the upper bound of [KKMS08, DKN10].
These results are formally stated and proved in Section 5. More generally, we show in Appendix C that our results give superpolynomial SQ lower bounds for agnostically learning any non-polynomial activation. (See Appendix A for some discussion of subtleties in interpreting these bounds.)
A notable property of our lower bounds is that they hold for general statistical queries. As noted by several authors [APVZ14, VW19], proving SQ lower bounds for real-valued learning problems often requires further restrictions on the types of queries the learner is allowed to make (e.g., correlational or Lipschitz queries).
Another consequence of our framework is the ﬁrst SQ lower bound for agnostically learning monomi-als with respect to Gaussian marginals. In contrast, for the realizable (noiseless) setting, recent work due to Andoni et al. [ADHV19] gave an attribute-efﬁcient SQ algorithm for learning monomials.
They left open the problem of making their results noise-tolerant. We show that in the agnostic setting, no efﬁcient SQ algorithm exists; the proof is in Appendix B.
Theorem 1.4. Consider the above setup with Hmon, the class of multilinear monomials of degree at most d on Rn. For any (cid:15) ≤ exp(−Θ(d)) and τ ≤ (cid:15)2, q(n, (cid:15), τ ) ≥ nΘ(d)τ 5/2.
Our Approach Our approach deviates from the standard template for proving SQ lower bounds and may be of independent interest. In almost all prior work, SQ lower bounds are derived by constructing a sufﬁciently large family of nearly orthogonal functions with respect to the underlying marginal distribution. Instead, we will use a reduction-based approach:
• We show that an algorithm for agnostically learning a single nonlinear activation φ can be i φ(wi · x)) used as a subroutine for learning depth-two neural networks of the form ψ((cid:80) 2
where ψ is any monotone, Lipschitz activation. This reduction involves an application of functional gradient descent via the Frank–Wolfe method with respect to a (nonstandard) convex surrogate loss.
• We apply recent work due to [DKKZ20] and [GGJ+20] that gives SQ lower bounds for learning depth-two neural networks of the above form in the probabilistic concept model.
For technical reasons, our lower bound depends on the norms of these depth-two networks, and we explicitly calculate them for ReLU and sigmoid.
• We prove that the above reduction can be performed using only statistical queries. To do so, we make use of some subtle properties of the surrogate loss and the functional gradient method itself.
Our reduction implies the following new relationship between two well-studied models of learning: if concept class C is efﬁciently agnostically learnable, then the class of monotone, Lipschitz functions of linear combinations of C is learnable in the probabilistic concept model due to Kearns and Schapire
[KS94]. We cannot hope to further strengthen the conclusion to agnostic learnability of monotone,
Lipschitz functions of combinations of C: the concept class of literals is agnostically learnable, but we show exponential SQ lower bounds for agnostically learning the class of majorities of literals, i.e., halfspaces (see also [KK14]).