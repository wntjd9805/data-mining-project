Abstract
Metric learning is an important family of algorithms for classiﬁcation and similarity search, but the robustness of learned metrics against small adversarial perturbations is less studied. In this paper, we show that existing metric learning algorithms, which focus on boosting the clean accuracy, can result in metrics that are less robust than the Euclidean distance. To overcome this problem, we propose a novel metric learning algorithm to ﬁnd a Mahalanobis distance that is robust against adversarial perturbations, and the robustness of the resulting model is certiﬁable.
Experimental results show that the proposed metric learning algorithm improves both certiﬁed robust errors and empirical robust errors (errors under adversarial attacks). Furthermore, unlike neural network defenses which usually encounter a trade-off between clean and robust errors, our method does not sacriﬁce clean errors compared with previous metric learning methods. 1

Introduction
Metric learning has been an important family of machine learning algorithms and has achieved successes on several problems, including computer vision [27, 19, 20], text analysis [30], meta learning [44, 40] and others [39, 52, 54]. Given a set of training samples, metric learning aims to learn a good distance measurement such that items in the same class are closer to each other in the learned metric space, which is crucial for classiﬁcation and similarity search. Since this objective is directly related to the assumption of nearest neighbor classiﬁers, most of the metric learning algorithms can be naturally and successfully combined with K-Nearest Neighbor (K-NN) classiﬁers.
Adversarial robustness of machine learning algorithms has been studied extensively in recent years due to the need of robustness guarantees in real world systems. It has been demonstrated that neural networks can be easily attacked by adversarial perturbations in the input space [43, 18, 2], and such perturbations can be computed efﬁciently in both white-box [4, 33] and black-box settings [7, 21, 10, 46]. To tackle this issue, many defense algorithms have been proposed to improve the robustness of neural networks [29, 33]. Although these algorithms can successfully defend from standard attacks, it has been shown that many of them are vulnerable under stronger attacks when the attacker knows the defense mechanisms [4]. Therefore, recent research in adversarial defense of neural networks has shifted to the concept of “certiﬁed defense”, where the defender needs to provide a certiﬁcation that no adversarial examples exist within a certain input region [50, 12, 55].
In this paper, we consider the problem of learning a metric that is robust against adversarial input perturbations. It has been shown that nearest neighbor classiﬁers are not as robust as expected [36, 45, 38], where a small and human imperceptible perturbation in the input space can fool a K-NN classiﬁer, thus it is natural to investigate how to obtain a metric that improves the adversarial 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
robustness. Despite being an important and interesting research problem to tackle, to the best of our knowledge it has not been studied in the literature. There are several caveats that make this a hard problem: 1) attack and defense algorithms for neural networks often rely on the smoothness of the corresponding functions, while K-NN is a discrete step function where the gradient does not exist. 2) Even evaluating the robustness of K-NN with the Euclidean distance is harder than neural networks
— attack and veriﬁcation for K-NN are nontrivial and time consuming [45]. Furthermore, none of the existing work have considered general Mahalanobis distances. 3) Existing algorithms for evaluating the robustness of K-NN, including attack [53] and veriﬁcation [45], are often non-differentiable, while training a robust metric will require a differentiable measurement of robustness.
To develop a provably robust metric learning algorithm, we formulate an objective function to learn a Mahalanobis distance, parameterized by a positive semi-deﬁnite matrix M , that maximizes the minimal adversarial perturbation on each sample. However, computing the minimal adversarial perturbation is intractable for K-NN, so to make the problem solvable, we propose an efﬁcient formulation for lower-bounding the minimal adversarial perturbation, and this lower bound can be represented as an explicit function of M to enable the gradient computation. We further develop several tricks to improve the efﬁciency of the overall procedure. Similar to certiﬁed defense algorithms in neural networks, the proposed algorithm can provide a certiﬁed robustness improvement on the resulting K-NN model with the learned metric. Decision boundaries of 1-NN with different
Mahalanobis distances for a toy dataset (with only four orange triangles and three blue squares in a two-dimensional space) are visualized in Figure 1. It can be observed that the proposed Adversarial
Robust Metric Learning (ARML) method can obtain a more “robust” metric on this example.
We conduct extensive experiments on six real world datasets and show that the proposed algorithm can improve both certiﬁed robust errors and the empirical robust errors (errors under adversarial attacks) over existing metric learning algorithms. (a) Euclidean (b) NCA [17] (c) ARML (Ours)
Figure 1: Decision boundaries of 1-NN with different Mahalanobis distances. 2