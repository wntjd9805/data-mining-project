Abstract
This paper studies the novel concept of weight correlation in deep neural networks and discusses its impact on the networks’ generalisation ability. For fully-connected layers, the weight correlation is deﬁned as the average cosine similarity between weight vectors of neurons, and for convolutional layers, the weight correlation is deﬁned as the cosine similarity between ﬁlter matrices. Theoretically, we show that, weight correlation can, and should, be incorporated into the PAC Bayesian framework for the generalisation of neural networks, and the resulting generalisa-tion bound is monotonic with respect to the weight correlation. We formulate a new complexity measure, which lifts the PAC Bayes measure with weight correlation, and experimentally conﬁrm that it is able to rank the generalisation errors of a set of networks more precisely than existing measures. More importantly, we develop a new regulariser for training, and provide extensive experiments that show that the generalisation error can be greatly reduced with our novel approach. 1

Introduction
Evidence in neuroscience has suggested that correlation between neurons plays a key role in the encoding and computation of information in the brain (Cohen and Kohn, 2011; Kohn and Smith, 2005). In deep neural networks (DNNs), or networks for simplicity, the correlation between neurons can be materialised as the correlation between weight matrices of either neurons (for fully-connected layers) or their ﬁlters (for convolutional layers), where it is referred to as weight correlation (WC).
WC is, while intriguing, not a concept that is prima facie a primary benchmark for networks. We will, however, provide evidence that it correlates with the generalisation ability of networks—one of the most important concepts in machine learning that reﬂects how accurately a learning algorithm is able to predict over previously unseen data. The key concept of generalisation ability can be quantiﬁed by the generalisation error (GE). Many factors have been discussed (Zhang et al., 2017) related to the
GE, including the Lipschitz constant, the smoothness of the loss function, and memorisation, to name but a few. To the best of our knowledge, however, no research that explicitly considers how, and to what extend, a correlation concept—either the WC or any other correlation—affects the GE has been conducted yet. Our observation that the WC correlates positively with GE thus opens an exciting new avenue to reduce the GE, and thus to improve the generalisation ability of networks.
Our hypothesis of positive correlation between WC and GE is motivated by an observation made from Figure 1, which shows such a correlation between WC and GE. Broadly speaking, the GE increases with the WCs. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (a) A fully-connected network (of structure 30-30-30-30), which has a large GE. (b)
A fully-connected network (of structure 110-10), which has a small GE. Note that the curves are presented with log scale. For better presentation, we split the training process into three phases, according to the training epochs: 100 ∼ 101, 101 ∼ 102, and 102 ∼ 103. For (a), the average WCs are 0.152, 0.157, and 0.175, and for (b), the average WCs are 0.074, 0.089, and 0.119. We can see that, the network in (a) has a higher WC than that of (b), while also displaying a larger GE. The corresponding relation between WC and GE becomes more prominent for the third phase 102 ∼ 103, when the training accuracy of the network has reached a certain level.
Testing our hypothesis on a range of networks has conﬁrmed it: we have observed the same connection across different architectures, and we have observed it for both, convolutional neural networks (CNNs) and fully-connected networks (FCNs).
Based on this observation, this paper makes the following three major contributions. The ﬁrst is to study if, and how, the PAC Bayesian framework (McAllester, 1999) on the generalisation er-ror bound can be upgraded to incorporate WC. We observe that the current framework requires
Kullback-Leibler (KL) divergence between the prior (before training) and the posterior (after training) distributions over Θ, a high-dimensional multivariate random variable that represents the network parameters, with each dimension corresponding to a neuron. However, it is notoriously difﬁcult to precisely estimate the KL quantity for high-dimensional multivariate variables (Singh and Póczos, 2017; Goldfeld et al., 2019), while evaluating KL with an off-the-shelf dimension-wise estimator such as Lombardi and Pant (2016); Kolchinsky and Tracey (2017) (by ignoring the cross-dimension corre-lation) can be arbitrarily inaccurate. To overcome these limitations, existing work (e.g. (Neyshabur et al., 2017; Jiang et al., 2020)) assumes that both the prior and the posterior distributions are Gaussian with each dimension being independent and identically distributed (i.i.d.). While it is reasonable to have an i.i.d. Gaussian distributed initialisation, it is unlikely that the components remain independent after training. To amend this, we show that the bound can be rectiﬁed under certain conditions by incorporating the WC. This theoretical result will enable us to tighter estimate the GE.
The second contribution is based on an observation over the new lifted PAC Bayesian bound. We show that the bound is monotonic with respect to the WC. More precisely, our lifted PAC Bayesian bound decreases when the WC falls. Moreover, this theoretical result aligns with our empirical observation from Figure 1. To fully understand—and exploit—the potential of this observation, we formalise a novel complexity measure by lifting the PAC Bayes measure (McAllester, 1999;
Dziugaite and Roy, 2017) with the WC, and conduct experiments over a set of networks against a number of existing complexity measures. Our experimental results are very promising: the new measure provides the best ranking over the networks with respect to their generalisation error. That is, by comparing the values of the new measure, we are able to predict—in a more precise way than by using previous measures—which network has a better generalisation ability. Moreover, calculating the value of our new measure is cheap (quadratic in the representation of the weights). This makes it feasible to estimate the generalisation ability of neural networks without resorting to a testing dataset.
Our experimental results show our advancement to the state-of-the-art as described in (Chatterji et al., 2020; Jiang et al., 2020). In particular, Jiang et al. (2020) concluded that sharpness-based measures, such as sharpness PAC-Bayesian bounds, perform better overall and seem to be promising candidates for further research. Our results advance this and show that weight correlation can be used to improve the PAC-Bayesian bounds further.
The third contribution is motivated by the above results, but exploits them in a different way: we explore the possibility to enhance the training process with the WC. We formalise a novel regular-isation term and conduct experiments on a spectrum of convolutional networks. Our experimental 2
Figure 2: Visualisation of individual ﬁlters’ weight correlations in two CNNs, whose structure is adapted from VGG16. The CNN in (a) is trained with the standard objective, while the one in (b) is trained using our new regularisation term (Section 5). Both CNNs are trained on the Fashion-MNIST dataset. The generalisation performance of (b) is better than (a) with a 0.8% improvement (87.1% vs 87.9%, which translates to a more than 6% higher error rate for (a)), which aligns with our expectations for the heatmaps: the left heatmap shows a signiﬁcantly higher weight correlation for almost all ﬁlters compared to the right heatmap. results show that the new regularisation term can reduce the generalisation error without compro-mising the training accuracy. This improvement is consistent across the models and datasets we have worked with. 2