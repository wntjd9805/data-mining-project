Abstract
Recommending the best course of action for an individual is a major application of individual-level causal effect estimation. This application is often needed in safety-critical domains such as healthcare, where estimating and communicating uncertainty to decision-makers is crucial. We introduce a practical approach for integrating uncertainty estimation into a class of state-of-the-art neural network methods used for individual-level causal estimates. We show that our methods enable us to deal gracefully with situations of “no-overlap”, common in high-dimensional data, where standard applications of causal effect approaches fail.
Further, our methods allow us to handle covariate shift, where the train and test distributions differ, common when systems are deployed in practice. We show that when such a covariate shift occurs, correctly modeling uncertainty can keep us from giving overconﬁdent and potentially harmful recommendations. We demonstrate our methodology with a range of state-of-the-art models. Under both covariate shift and lack of overlap, our uncertainty-equipped methods can alert decision makers when predictions are not to be trusted while outperforming standard methods that use the propensity score to identify lack of overlap. 1

Introduction
Learning individual-level causal effects is concerned with learning how units of interest respond to interventions or treatments. These could be the medications prescribed to particular patients, training-programs to job seekers, or educational courses for students. Ideally, such causal effects would be estimated from randomized controlled trials, but in many cases, such trials are unethical or expensive: researchers cannot randomly prescribe smoking to assess health risks. Observational data offers an alternative, with typically larger sample sizes and lower costs, and more relevance to the target population. However, the price we pay for using observational data is lower certainty in our causal estimates, due to the possibility of unmeasured confounding, and the measured and unmeasured differences between the populations who were subject to different treatments.
Progress in learning individual-level causal effects is being accelerated by deep learning approaches to causal inference [27, 36, 3, 48]. Such neural networks can be used to learn causal effects from
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
observational data, but current deep learning tools for causal inference cannot yet indicate when they are unfamiliar with a given data point. For example, a system may offer a patient a recommendation even though it may not have learned from data belonging to anyone with similar age or gender as the patient, or it may have never observed someone like this patient receive a speciﬁc treatment before. In the language of machine learning and causal inference, the ﬁrst example corresponds to a covariate shift, and the second example corresponds to a violation of the overlap assumption, also known as positivity. When a system experiences either covariate shift or violations of overlap, the recommendation would be uninformed and could lead to undue stress, ﬁnancial burden, false hope, or worse. In this paper, we explain and examine how covariate shift and violations of overlap are concerns for the real-world use of learning conditional average treatment effects (CATE) from observational data, why deep learning systems should indicate their lack of conﬁdence when these phenomena are encountered, and develop a new and principled approach to incorporating uncertainty estimating into the design of systems for CATE inference.
First, we reformulate the lack of overlap at test time as an instance of covariate shift, allowing us to address both problems with one methodology. When an observation x lacks overlap, the model predicts the outcome y for a treatment t that has probability zero or near-zero under the training distribution. We extend the Causal-Effect Variational Autoencoder (CEVAE) [36] by introducing a method for out-of-distribution (OoD) training, negative sampling, to model uncertainty on OoD inputs. Negative sampling is effective and theoretically justiﬁed but usually intractable [18]. Our insight is that it becomes tractable for addressing non-overlap since the distribution of test-time inputs (x, t) is known: it equals the training distribution but with a different choice of treatment (for example, if at training we observe outcome y for patient x only under treatment t = 0, then we know that the outcome for (x, t = 1) should be uncertain). This can be seen as a special case of transductive learning [57, Ch. 9]. For addressing covariate shift in the inputs x, negative sampling remains intractable as the new covariate distribution is unknown; however, it has been shown in non-causal applications that Bayesian parameter uncertainty captures “epistemic” uncertainty which can indicate covariate shift [29]. We, therefore, propose to treat the decoder p(y|x, t) in CEVAE as a
Bayesian neural network able to capture epistemic uncertainty.
In addition to casting lack of overlap as a distribution shift problem and proposing an OoD training methodology for the CEVAE model, we further extend the modeling of epistemic uncertainty to a range of state-of-the-art neural models including TARNet, CFRNet [47], and Dragonnet [49], developing a practical Bayesian counter-part to each. We demonstrate that, by excluding test points with high epistemic uncertainty at test time, we outperform baselines that use the propensity score p(t = 1|x) to exclude points that violate overlap. This result holds across different state-of-the-art architectures on the causal inference benchmarks IHDP [23] and ACIC [11]. Leveraging uncertainty for exclusion ties it into causal inference practice where a large number of overlap-violating points must often be discarded or submitted for further scrutiny [43, 25, 6, 26, 20]. Finally, we introduce a new semi-synthetic benchmark dataset, CEMNIST, to explore the problem of non-overlap in high-dimensional settings. 2