Abstract
Adversarial data augmentation has shown promise for training robust deep neural networks against unforeseen data shifts or corruptions. However, it is difﬁcult to deﬁne heuristics to generate effective ﬁctitious target distributions containing “hard” adversarial perturbations that are largely different from the source distribution. In this paper, we propose a novel and effective regularization term for adversarial data augmentation. We theoretically derive it from the information bottleneck principle, which results in a maximum-entropy formulation. Intuitively, this regu-larization term encourages perturbing the underlying source distribution to enlarge predictive uncertainty of the current model, so that the generated “hard” adversarial perturbations can improve the model robustness during training. Experimental results on three standard benchmarks demonstrate that our method consistently outperforms the existing state of the art by a statistically signiﬁcant margin. Our code is available at https://github.com/garyzhao/ME-ADA. 1

Introduction
Deep neural networks can achieve good performance on the condition that the training and testing data are drawn from the same distribution. However, this condition might not hold true in practice.
Data shifts caused by mismatches between training and testing domain [6, 41, 51, 65, 76], small corruptions to data distributions [24, 75], or adversarial attacks [23, 35] are often inevitable in real-world applications, and lead to signiﬁcant performance degradation of deep learning models.
Recently, adversarial data augmentation [22, 51, 65] emerges as a strong baseline where ﬁctitious target distributions are generated by an adversarial loss to resemble unforseen data shifts, and used to improve model robustness through training. The adversarial loss is leveraged to produce perturbations that fool the current model. However, as shown in [48], this heuristic loss function is insufﬁcient to synthesize large data shifts, i.e., “hard” adversarial perturbations from the source domain, which makes the model still vulnerable to severely shifted or corrupted testing data.
To mitigate this issue, we propose a regularization technique for adversarial data augmentation from an information theory perspective using the Information Bottleneck (IB) [58] principle. The IB principle encourages the model to learn an optimal representation by diminishing the irrelevant parts of the input variable that do not contribute to the prediction. Recently, there has been a surge of interest in combining the IB method with training of deep neural networks [2, 5, 17, 30, 46, 60], while its effectiveness for adversarial data augmentation still remains unclear.
In the IB context, a neural network does not generalize well on out-of-domain data often when the information of the input cannot be well-compressed by the model, i.e., the mutual information of the input and its associated latent representation is high [50, 59]. Motivated by this conceptual observation, we aim to regularize adversarial data augmentation through maximizing the IB function.
Speciﬁcally, we produce “hard” ﬁctitious target domains that are largely shifted from the source domain by enlarging the mutual information of the input and latent distribution within the current 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
model. However, mutual information is shown to be intractable in the literature [7, 47, 53], and therefore directly optimizing this objective is challenging.
In this paper, we develop an efﬁcient maximum-entropy regularizer to achieve the same goal by making the following contributions: (i) to the best of our knowledge, we are the ﬁrst work to investigate adversarial data argumentation from an information theory perspective, and address the problem of generating “hard” adversarial perturbations from the IB principle which has not been studied yet; (ii) we theoretically show that the IB principle can be bounded by a maximum-entropy regularization term in the maximization phase of adversarial data argumentation, which results in a notable improvement over [65]; (iii) we also show that our formulation holds in an approximate sense under certain non-deterministic conditions (e.g., when the neural network is stochastic or contains
Dropout [55] layers). Note that our maximum-entropy regularizer can be implemented by one line of code with minor computational cost, while it consistently and statistically signiﬁcantly improves the existing state of the art on three standard benchmarks. 2