Abstract
A common challenge faced in practical supervised learning, such as medical image processing and robotic interactions, is that there are plenty of tasks but each task cannot afford to collect enough labeled examples to be learned in isolation.
However, by exploiting the similarities across those tasks, one can hope to overcome such data scarcity. Under a canonical scenario where each task is drawn from a mixture of k linear regressions, we study a fundamental question: can abundant small-data tasks compensate for the lack of big-data tasks? Existing second moment based approaches of [42] show that such a trade-off is efﬁciently achievable, with the help of medium-sized tasks with Ω(k1/2) examples each. However, this algorithm is brittle in two important scenarios. The predictions can be arbitrarily bad (i) even with only a few outliers in the dataset; or (ii) even if the medium-sized tasks are slightly smaller with o(k1/2) examples each. We introduce a spectral approach that is simultaneously robust under both scenarios. To this end, we ﬁrst design a novel outlier-robust principal component analysis algorithm that achieves an optimal accuracy. This is followed by a sum-of-squares algorithm to exploit the information from higher order moments. Together, this approach is robust against outliers and achieves a graceful statistical trade-off; the lack of Ω(k1/2)-size tasks can be compensated for with smaller tasks, which can now be as small as O(log k). 1

Introduction
Modern machine learning tasks and corresponding training datasets exhibit a long-tailed behavior
[73], where a large number of tasks do not have enough training examples to be trained to the desired accuracy. Collecting high-quality labeled data can be time consuming or require expertise.
Consequently, in domains such as annotating medical images or processing robotic interactions, there might be a large number of related but distinct tasks, yet each task is associated with only a small batch of training data. However, one can hope to meta-train across those tasks, exploiting their similarities, and collaboratively achieve accuracy far greater than what can be achieved for each task in isolation [29, 58, 41, 52, 68, 61]. This is the goal of meta-learning [62, 67].
Meta-learning is especially challenging under two practically important settings: (i) a few-shot learning scenario where each task is associated with an extremely small dataset; and (ii) an adversarial scenario where a fraction of those datasets are corrupted. We design a novel meta-learning approach that is robust to such data scarcity and adversarial corruption, under a canonical scenario where the tasks are linear regressions in d-dimensions and the model parameters are drawn from a discrete distribution of a support size k.
∗kweihao@gmail.com. University of Washington
†raghavs@cs.washington.edu. University of Washington
‡sham@cs.washington.edu. University of Washington & Microsoft Research
§sewoong@cs.washington.edu. University of Washington 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
First, consider a case where we have an uncorrupted dataset from a collection of n tasks, each with t training examples. Concretely, the i-th task for i ∈ {1, . . . , n} is associated with a regression parameter βi ∈ {w1, . . . , wk} and a corresponding dataset {xi,j ∈ Rd, yi,j ∈ R}t j=1 drawn from yi,j = β(cid:62)i xi,j + (cid:15)i,j for some noise (cid:15)i,j. A formal deﬁnition of the generative model is provided in § 1.1. If each task has a large enough training data with t = Ω(d) examples, it can be accurately learned in isolation. This is illustrated by solid circles in Fig. 1. On the opposite extreme, where each task has only a single example (i.e. t = 1), signiﬁcant efforts have been made to make training statistically efﬁcient [14, 77, 63, 78, 47, 16, 64]. However, even the best known result of [16] still requires exponentially many such tasks: n = Ω(de√k) (details in related work in §3). This is illustrated by solid squares in Fig. 1. Perhaps surprisingly, this can be signiﬁcantly reduced to quasi-polynomial n = Ω(kΘ(log k)) sample complexity and quasi-polynomial run-time, with a slightly larger dataset that is only logarithmic in the problem parameters. This result is summarized in the following, with the algorithm and proof presented in §A of the supplementary material.
Corollary 1.1 (of our results with no corruption, informal). Given a collection of n tasks each associated with t = (cid:101)Ω(1) labeled examples, if the effective sample size nt = (cid:101)Ω(dk2 + kΘ(log k)), then
Algorithm 4 estimates the meta-parameters up to any desired accuracy of O(1) with high probability in time poly(d, k(log k)2
), under certain assumptions on the meta-parameters.
This is a special case of a more general class of algorithms we design, tailored for the following practical scenario; the collection of tasks in hand are heterogeneous, each with varying sizes of datasets (illustrated by the blue bar graphs below in Fig. 1). Inspired by the seminal work of [71], we exploit such heterogeneity by separating the roles of light tasks that have smaller datasets and heavy tasks that have larger datasets. As we will show, the size of the heavy tasks determines the order of the higher order moments we can reliably exploit. Concretely, we ﬁrst use the light tasks to estimate the subspace spanned by the regression parameters, and then cluster heavy tasks by projecting them on the estimated subspace. The ﬁrst such attempt was taken in [42], where a linkage-based clustering was proposed. However, as this clustering method relies on the second moment statistics, it strictly requires heavy tasks with Ω(k1/2) examples (left panel in Fig. 1). In the absence of such heavy tasks, the abundant light tasks are wasted as no existing algorithm can harness their structural similarities.
Such second moment barriers are common in even simpler problems, e.g. [43, 44].
Sufﬁcient condition of [42]
Sufﬁcient condition of Corollary 1.2 h t i t
≥ e z i s h c t a
B w s k s a t f o r e b m u
N
Batch size t
Batch size t
Figure 1: The blue bar graph summarizes the collection of tasks in hand, showing the cumulative count of tasks with more than t examples. Typically, this does not include extremely large data tasks (circle) and extremely large number of small data tasks (square), where classical approaches succeed.
When any point in the light (green) region and any point in the heavy (yellow) region are both realized by the blue graph, the corresponding algorithm succeeds. On the left, the collection in blue cannot be learned by any existing methods including [42]. Our approach in Corollary 1.2 signiﬁcantly extends the heavy region all the way down to log k, leading to a successful meta-learning in this example.
We exploit higher order statistics to break this barrier, using computationally tractable tools from sum-of-squares methods [45]. This gives a class of algorithms parameterized by an integer m (for m-th order moment) to be chosen by the analyst tailored to the size of the heavy tasks in hand tH = Ω(k1/m). This allows for a graceful trade-off between tH and with the required number of heavy tasks nH . We summarize the result below, with a proof in §A, and illustrate it in Fig. 1 2
(right). The choice of m = Θ(log k) gives the minimum required batch size, as we highlighted in
Corollary 1.1.
Corollary 1.2 (of our results with no corruption, informal). For any integer m, given two collections of tasks, ﬁrst collection of light tasks with tL = (cid:101)Ω(1), tLnL = (cid:101)Ω(dk2), and the second collection of heavy tasks with tH = (cid:101)Ω(m k1/m), tH nH = (cid:101)Ω(kΘ(m)), the guarantees of Corollary 1.1 hold.
Next, consider an adversarial scenario. Outliers are common in meta-learning as diverse sources contribute to the collection. Existing approaches are brittle to a few such outliers. [42] builds upon principal component analysis and linear regression, both of which are known to be brittle to outliers
[40, 19]. For example, a single corrupted user can result in an arbitrarily bad subspace estimation in the ﬁrst step of [42]. This causes the meta-learning algorithm to learn nothing about the true regression parameters, resulting in a completely random prediction in the subsequent step. A fundamental question of interest is, what can be meta-learned from past experience that is only partially trusted?
Following robust learning literature [45, 25], we assume a general adversary who can adaptively corrupt any α fraction of the tasks, formally deﬁned in Assumption 2. This parameter α ∈ [0, 1] captures how powerful an adversary is. Among all adversaries that can corrupt an α fraction of the dataset, we assume the strongest possible one that can adaptively select which samples to corrupt and replace them with arbitrary data points. We make both subspace estimation and clustering steps robust against adversarial corruption. The sum-of-squares approach is inherently robust, when used within an iterative clustering [45]. However, existing robust subspace estimation approaches are suboptimal, requiring (cid:101)O(d2) samples [24]. To this end, we introduce a novel algorithm, and prove its optimality in both accuracy and dependence in the dimension d. This resolves an open question posed in [64] on whether it is possible to robustly learn the subspace with (cid:101)O(d) samples.
This achieves a similar sample complexity as the uncorrupted case in Corollary 1.2, while tolerating as much corruption as information theoretically possible: α = (cid:101)O((cid:15)/k) for an (cid:15) accuracy in parameter estimation. Such condition is necessary as otherwise the adversary can focus its attack on one of the mixtures, and incur Ω((cid:15)) error in estimating the parameter of that component.
Corollary 1.3 (of Theorem 1, informal). For any (cid:15) ∈ (0, 1/k3) and m ∈ N, given two collections of tasks, the ﬁrst with tL = (cid:101)Ω(1), nLtL = (cid:101)Ω(cid:0)dk(cid:15)− 2(cid:1), and the second with tH = (cid:101)Ω(cid:0)mk1/m(cid:1), nH tH = (cid:101)Ω(cid:0)kO (m)(cid:1), if the fraction of corrupted tasks is α = (cid:101)O((cid:15)/k), Algorithm 1 achieves up to (cid:15) accuracy with high probability in time poly(d, km2 , (cid:15)− 1), under certain assumptions.
We provide the algorithm (Algorithm 1) and the analysis (Theorem 1) under the adversarial scenario in the main text. When there is no corruption, the algorithm can be made statistically more efﬁcient with tighter guarantees, which is provided in §A. 1.1 Problem formulation and notations
We present the probabilistic perspective on few-shot supervised learning following [31], but focusing on a simple yet canonical case where the tasks are linear regressions. A collection of n tasks are independently drawn according to some prior distribution. The i-th task is associated with a model parameter φi = (βi ∈ Rd, σi ∈ R+), and a meta-train dataset {(xi,j, yi,j) ∈ Rd × R}ti j=1 of size ti.
Each example (xi,j, yi,j) ∼ Pφi (y|x)P(x) is independently drawn from a linear model, such that yi,j = β(cid:62)i xi,j + (cid:15)i,j , (1)
, ynew j where xi,j ∼ N (0, Id) and (cid:15)i,j ∼ N (0, σ2 i ). If xi,j is from N (0, Σ), we assume to have enough xi,j’s (not necessarily labeled) for whitening, and P(x) can be made sufﬁciently close to isotropic.
The goal of meta-learning is to train a model for a new arriving task φnew ∼ Pθ(φ) from a small size training dataset D = {(xnew j=1 of size τ . This is achieved by exploiting some structural similarities to the meta-train dataset, drawn from the same prior distribution Pθ(φ).
To capture such structural similarities, we make a mild assumption that Pθ(φ) is a ﬁnite discrete distribution of a support size k. This is also known as mixture of linear experts [14]. Concretely,
Pθ(φ) is fully deﬁned by a meta-parameter θ = (W ∈ Rd 1) with k candidate
× model parameters W = [w1, . . . , wk] and k candidate noise parameters s = [s1, . . . , sk]. The i-th task is drawn from φi ∼ Pθ(φ), where ﬁrst a zi ∼ multinomial(p) selects a component that the task belongs to, and training data is independently drawn from Eq. (1) with βi = wzi and σi = szi.
) ∼ Pφnew (y|x)P(x)}τ
+, p ∈ Sk k, s ∈ Rk
− j 3
Following the deﬁnition of [31], the meta-learning problem refers to solving the following:
θ∗ ∈ arg max
θ log P(θ|Dmeta train) ,
− (2) which estimates the most likely meta-parameter given meta-training dataset deﬁned as Dmeta train :=
{{(xi,j, yi,j) ∈ Rd × R}ti i=1. This is a special case of empirical Bayes methods [13]. Our goal is to solve this meta-learning problem robustly against an adversarial corruption of Dmeta train as formally deﬁned in Assumption 2. Once meta-learning is solved, the model parameter of the newly arriving task can be estimated with a Maximum a Posteriori (MAP) or a Bayes optimal estimator: j=1}n
−
− (cid:98)φMAP ∈ arg max
φ log P(φ|D, θ∗) , and (cid:98)φBayes ∈ arg min
φ
E
φ(cid:48)∼
P(φ
|D
,θ∗)[(cid:96)(φ, φ(cid:48))] , (3)
P
φMAP / Bayes (y|x). for some choice of a loss (cid:96)(·), which is straightforward. This is subsequently used to predict the label of a new data point x from φnew. Concretely, (cid:98)y ∈ arg maxy
Notations. We deﬁne [n] := {1, . . . , n}, ∀ n ∈ N; Sk 1 as the standard k-dimensional probability simplex; (cid:107)x(cid:107)p := ((cid:80)d i=1|xi|p)1/p as the standard vector (cid:96)p-norm of a vector x ∈ Rd ∀ d ∈
N ∀ p ≥ 1; (cid:107)A(cid:107) i,j)1/2 as the standard nuclear norm
∗ and Frobenius norm of matrix A ∈ Rm n, where σi(A) denotes the i-th singular value of A respectively; N (µ, Σ) is the multivariate normal distribution with mean µ and covariance Σ; 1{·} zi + (cid:107)wzi(cid:107)2 is the indicator function. We deﬁne ρ2 2 as the variance of a label yi,j in the i-th task, and ρ2 := maxi ρ2
=j(cid:107)wi − wj(cid:107)2 and assume pmin, ∆ > 0. We use (cid:101)O and (cid:101)Ω notations that are extensions of the standard O and Ω
Bachmann–Landau notations to hide poly-logarithmic factors.
σi(A), (cid:107)A(cid:107)F := ((cid:80)m,n i . We deﬁne pmin := minj
[k] pj, and ∆ := mini,j
:= (cid:80)min i,j=1 A2 i := s2 n,m
[k],i i=1
−
×
∈
∈ (cid:98)
}
{ 1.2 Algorithm and intuitions
Following the recipe of spectral algorithms for clustering [71] and few-shot learning [42], we propose the following approach consisting of three steps. Clustering step requires heavy tasks; each task has many labeled examples, but we need a smaller number of such tasks. Subspace estimation and classiﬁcation steps require light tasks; each task has a few labeled examples, but we need a larger number of such tasks. Here, we provide the intuition behind each step and the corresponding requirements. The details are deferred to §B, where we emphasize robustness to corruption of the data. The estimated (cid:98)θ = (cid:0) (cid:99)W, (cid:98)s, (cid:98)p(cid:1) is subsequently used in prediction, when a new task arrives.
Algorithm 1
Meta-learning 1. Subspace estimation: Compute subspace (cid:98)U which approximates span{w1, . . . , wk}. 2. Clustering: Project the heavy tasks onto the subspace of (cid:98)U, perform k clustering, and estimate (cid:101)w(cid:96) for each cluster (cid:96) ∈ [k]. 3. Classiﬁcation: Perform likelihood-based classiﬁcation of the light tasks using { (cid:101)w(cid:96)}k estimated from the Clustering step; compute reﬁned estimates { (cid:98)w(cid:96), (cid:98)s(cid:96), (cid:98)p(cid:96)}k (cid:96)=1 of θ. (cid:96)=1
Prediction 4. Prediction: Perform MAP or Bayes optimal prediction using the estimated meta-parameter. i,jxi,jx(cid:62)i,j] = cId + 2 (cid:80)k
Subspace estimation. As Σ := E[y2 (cid:96)=1 p(cid:96)w(cid:96)w(cid:62)(cid:96) for some constant c ≥ 0, the subspace spanned by the regression vectors, span{w1, . . . , wk}, can be efﬁciently estimated by Principal Component Analysis (PCA), if we have uncorrupted data. This only requires (cid:101)Ω(d) samples. With α-fraction of the tasks adversarially corrupted, existing approaches of outlier-robust
PCA attempt to simultaneously estimate the principal subspace while ﬁltering out the outliers [75].
This removes many uncorrupted data points, and hence can either only tolerate up to α = O(1/k8) fraction of corruption (assuming well-separated w(cid:96)’s). We introduce a new approach in Algorithm 2 that uses a second ﬁlter to recover those erroneously removed data points. This improves the tolerance to α = O(1/k4) while requiring only (cid:101)Ω(d) samples (see Remark B.2). We call this step robust subspace estimation (Algorithm 2 in §2.2). 4 (cid:54)
Clustering. Once we have the subspace, we project the estimates of βi’s to the k-dimensional subspace and cluster those points to ﬁnd the centers. As k (cid:28) d in typical settings, this signiﬁcantly reduces the sample complexity from poly(d) to poly(k). Existing meta-learning algorithm of [42] proposed a linkage based clustering algorithm. This utilizes the bounded property of the second moment only. Hence, strictly requires heavy tasks with t = Ω(k1/2). We break this second moment barrier by exploiting the boundedness of higher order moments. The heavy tasks are now allowed to be much smaller, but at the cost of requiring a larger number of such tasks and additional computations.
One challenge is that the (empirical) higher order moments are tensors, and tensor norms are not efﬁciently computable. Hence boundedness alone does not give an efﬁcient clustering algorithm. We need a stronger condition that the moments are Sum-of-Squares (SOS) bounded, i.e. there exist SOS proofs showing that the moments are bounded [45, 35]. This SOS boundedness is now tractable with a convex program, leading to a polynomial time algorithm that is also robust against outliers [45].
One caveat is that existing method in [45] requires data generated from a Poincaré distribution. As shown in Remark H.10, the distribution of our estimate (cid:98)βi = (1/t) (cid:80)t j=1 yi,jxi,j is not Poincaré.
Interestingly, as we prove in Lemma H.2, the higher order moments are still SOS bounded. This ensures that we can apply the robust clustering algorithm of [45]. We call this step robust clustering (Algorithm 7 in §H).
Classiﬁcation and parameter estimation. Given rough estimates (cid:101)w(cid:96)’s as center of those clusters, we grow each cluster by classifying remaining light tasks. Classiﬁcation only requires t = Ω(log k).
Once we have sufﬁciently grown each cluster, we can estimate the parameters to a desired level of accuracy. There are two reasons we need this reﬁnement step. First, in the small corruption regime, where the fraction of corrupted tasks α is much smaller than the desired level of accuracy (cid:15), this separation is signiﬁcantly more sample efﬁcient. The subspace estimation and clustering steps require only O(∆/ρ) accuracy, and the burden of matching the desired (cid:15) level of error is left to the ﬁnal classiﬁcation step, which is more sample efﬁcient. Next, the classiﬁcation step ensures an adaptive guarantee. As parameter estimation is done for each cluster separately, a cluster with small noise si can be more accurately estimated. This ensures a more accurate prediction for newly arriving tasks.
We call this step classiﬁcation and robust parameter estimation (Algorithm 9 in §I). 2 Main results
To give a more ﬁne grained analysis, we assume there are two types of light tasks. In meta-learning, subspace estimation uses DL1, clustering uses DH , and classiﬁcation uses DL2.
Assumption 1. The heavy dataset DH consists of nH heavy tasks, each with at least tH samples.
The ﬁrst light dataset DL1 consists of nL1 light tasks, each with at least tL1 samples. The second light dataset DL2 consists of nL2 tasks, each with at least tL2 samples. We assume tL1, tL2 < d.
The three batches of meta-train datasets are corrupted by an adversary.
Assumption 2. From the datasets DH , DL1, and DL2, the adversary controls αH , αL1, and αL2 fractions of the tasks, respectively. The adversary is allowed to inspect all the examples, remove those examples associated with three subsets of tasks (of sizes at most αH nH , αL1nL1, and αL1nL1 tasks from DH , DL1, and DL2), and replace the examples associated with those tasks with arbitrary points. The corrupted meta-train datasets are then presented to the algorithm. 2.1 Meta-learning and prediction
We characterize the achievable accuracy in estimating the meta-parameters θ = (W, s, p).
Theorem 1. For any δ ∈ (0, 1/2) and (cid:15) > 0, given three batches of samples under Assumptions 1 and 2, the meta-learning step of Algorithm 1 achieves the following accuracy for all i ∈ [k], (cid:107) (cid:98)wi − wi(cid:107)2 ≤ (cid:15)si , (cid:12) (cid:12)(cid:98)s2 i − s2 i (cid:12) (cid:12) ≤ (cid:15)s2 i /
√ tL2 , and
|(cid:98)pi − pi| ≤ (cid:15)(cid:112)tL2/d pi + αL2 , with probability 1 − δ, if the numbers of tasks, samples in each task, and the corruption levels satisfy 5
nL1 = (cid:101)Ω nH = (cid:101)Ω (cid:17)
,
+ k
α2 (cid:16) dk2
αtL1 (cid:16) (km)Θ(m)
+ (cid:101) pmin (cid:17) (cid:101) d tL2pmin(cid:15)2 (cid:16)
, nL2 = (cid:101)Ω where (cid:101)α := max(cid:8)∆2σ2 of (cid:80)k
ρ4
∆4pmintH (cid:17)
, tL1 ≥ 1 , (cid:18) tH = Ω tL2 = Ω (cid:19)
ρ2
∆2 · m p2/m min (cid:16) ρ4
∆4 log knL2
δ
αL1 = O((cid:101)α) ,
αH = (cid:101)O (cid:16) pmin min
√ (cid:110) 1, tH · ∆2
ρ2 (cid:111)(cid:17)
,
, αL2 = O(pmin(cid:15)/log(1/(cid:15))) ,
, (cid:17) min/(ρ6k2), ∆6p2 min/(k2ρ6)(cid:9), σmin is the smallest non-zero singular value j=1 pjwjw(cid:62)j , and m ∈ N is a parameter chosen by the analyst. min, 1/p2
We refer to §1.1 for the setup and notations, and provide key lemmas in §B and a complete proof in
§C. We discuss each of the conditions in the following remarks assuming ∆ = Ω(ρ), for simplicity.
Remark 2.1 (Separating two types of light tasks). As tL1 can be as small as one, the conditions on DL2 does not cover the conditions for DL1. The conditions on αL1 and nL1 can be signiﬁcantly more strict than what is required for DL2. Hence, we separate the analysis for DL1 and DL2.
Remark 2.2 (Dependency in DL1). Since we are interested the large d small tL1 setting, the dominant term in nL1 is dk2/(cid:101)αtL1. The effective sample size nL1tL1 scaling as d is information theoretically necessary. The min{1/σ2 min} dependence of nL1tL1 allows sample efﬁciency even when
σmin is arbitrarily small, including zero. This is a signiﬁcant improvement over the poly(1/σk) sample complexity of typical spectral methods, e.g. [14, 77], where σk is the k-th singular value of (cid:80)k (cid:96)=1 p(cid:96)w(cid:96)w(cid:62)(cid:96) . This critically relies on an extension of the gap-free spectral bound of [1, 47].
Our tolerance of αL1 = O(p2 min/k2) signiﬁcantly improves upon the state-of-the-art guarantee of p4 min/k4 as detailed in §2.2. Further, we show it is information theoretically optimal. This assumes only bounded fourth moment, which makes our analysis more generally applicable. However, this can be tightened under a stricter conditions of the distribution, as we discuss in §4.
Remark 2.3 (Dependency in DH ). Assuming pmin = Ω(1/k), the dominant term in nH is (cid:101)Ω((km)Θ(m)/pmin), which is (cid:101)Ω(kΘ(m)) and the result is trivial when m ≥ log(k). This implies a nH = (cid:101)Ω(kΘ(m)), tH = Ω(m · k2/m) trade-off for any integer m, breaking the tH = Ω(k1/2) barrier of [42]. In fact, for an optimal choice of m = Θ(log k) to minimize the required examples, it can tolerate as small as tH = Ω(log k) examples, at the cost of requiring nH = (cid:101)Ω(kΘ(log k)) such heavy tasks. We conjecture tH = Ω(log k) is also necessary for any polynomial sample complexity.
For the case of learning mixtures of isotropic Gaussians, [59] shows that super-polynomially many log k) apart. number of samples are information theoretically necessary when the centers are o(
This translates to t = o(log k) in our setting. The requirement αH = O(pmin) is optimal. Otherwise, the adversary can remove an entire cluster.
Remark 2.4 (Dependency in DL2). The requirement nL2 · tL2 = (cid:101)Ω(d/pmin(cid:15)2) is optimal in d, pmin and (cid:15) due to the lower bound for linear regression. The requirement on αL2 = O(pmin(cid:15)/ log(1/(cid:15))) is also necessary upto a log factor, from lower bound on robust linear regression [26].
√
)}τ
, ynew j
At test time, we use the estimated (cid:98)θ = ( (cid:99)W, (cid:98)s, (cid:98)p) to approximate the prior distribution on a new task.
On a new arriving task with training data D = {(xnew j=1, we propose the standard MAP or j
Bayes optimal estimators to make predictions on this new task. The following guarantee is a corollary of Theorem 1 and [42, Theorem 2]. The term (cid:80) i is due to the noise in the test data (x, y) i
∈ and cannot be avoided. We can get arbitrarily close to this fundamental limit with only τ = Ω(log k) samples. This is a minimax optimal sample complexity as shown in [42].
Corollary 2.5 (Prediction). Under the hypotheses of Theorem 1, the expected prediction errors of both the MAP and Bayes optimal estimators (cid:98)β(D) deﬁned in Eq. (3) are bound as E[(x(cid:62) (cid:98)β(D) − y)2] ≤
δ + (cid:0)1 + (cid:15)2(cid:1) (cid:80)k d/(50ρ2)}, where the expectation is over the new task with model parameter φnew = (βnew, σnew) ∼ Pθ, training data (xnew i , if τ = Ω((ρ4/∆4) log(k/δ)) and (cid:15) ≤ min{∆/(10ρ), ∆2
) ∼ Pφnew , and test data (x, y) ∼ Pφnew . i=1 pis2
[k] pis2
√
, ynew j j 2.2 Novel robust subspace estimation
Our main result relies on making each step of Algorithm 1 robust, as detailed in §B. However, as our key innovation is a novel robust subspace estimation in the ﬁrst step, we highlight it in this section.
We aim to estimate the subspace spanned by the true meta-parameters {w1, . . . , wk}. As Σ :=
E[ (cid:98)βi,j (cid:98)β(cid:62)i,j] = {(cid:80)k (cid:96)=1 p(cid:96)w(cid:96)w(cid:62)(cid:96) for (cid:98)βi,j in Algorithm 2 line 2, we can 2)}I + 2 (cid:80)k (cid:96) + (cid:107)w(cid:96)(cid:107)2 (cid:96)=1 p(cid:96)(s2 6
j=1}nL1 i=1 , α ∈ (0, 1/36], δ ∈ (0, 0.5), k ∈ N, and ν ∈ R+ for all i ∈ [nL1], j ∈ [tL1]
[tL1] , and Smax ← ∅
Algorithm 2 Robust subspace estimation 1: Input: Data DL1 = {{(xi,j, yi,j)}tL1 2: (cid:98)βi,j ← yi,jxi,j , 3: S0 ← { (cid:98)βi,j (cid:98)β(cid:62)i,j}i
∈ 4: for (cid:96) = 1, . . . , log6 (2/δ) do 5: 6: 7: 8: 9: Output: (cid:98)U ← k_SVD(cid:0) (cid:80) t ← 0 and S while t ≤ (cid:100)9αn(cid:101) and St (cid:54)= St if |Smax| < |St| then Smax ← St 1 ← ∅
[nL1],j
−
−
∈ 1 do
Smax (cid:98)βi,j (cid:98)β(cid:62)i,j (cid:1)
βi,j ∈ (cid:98) t ← t + 1, and St ← Double-Filtering(St 1, k, α, ν)
[See Algorithm 3]
− use the k empirical principal components; this requires uncorrupted data. To remove the corrupted datapoints, we introduce double ﬁltering. We repeat log6(2/δ) times for a high probability result.
Algorithm 3 Double-Filtering 1: Input: a set of PSD matrices S = (cid:8)Xi ∈ Rd 2: S0 ← [n], U0 ← k_SVD(cid:0) (cid:80) i
, α) 3: SG ← First-Filter({zi}i 4: µS0 ← (1/n) (cid:80) 5: if µS0 − µSG ≤ 48(αµSG + ν 6: else 7:
Z ∼ U[0, 1], W ← Z max(cid:8)zi − µSG(cid:9)
S1 ← SG ∪ (cid:8)i ∈ S0 \ SG
Xi
∈S0
∈S0
∈S0
√ i 8: 9: Output: S(cid:48) = {Xi}i
∈S1 d(cid:9) i
× (cid:1), and zi ← Tr(cid:2)U(cid:62)0 XiU0
∈
[n]
, k ∈ N, α ∈ (0, 1/36] and ν ∈ R+ (cid:3) for all i ∈ S0
[Remove the upper and lower 2α quantiles] zi and µSG ← (1/|SG|) (cid:80) zi i
∈SG kα) then Output: S [Sample mean not large, no need to ﬁlter.]
[Run a second ﬁlter if sample mean is corrupted] (cid:12) (cid:12) zi − µSG ≤ W (cid:9) i
∈S0\SG
[Add some removed points back.]
If the adversarial examples have the outer product Xi = (cid:98)βi(cid:48),j(cid:48) (cid:98)β(cid:62)i(cid:48),j(cid:48)
’s with small norms, then they are challenging to detect. However, such undetectable corruptions can only perturb the subspace by little.
Hence, Algorithm 3 focuses on detecting large corruptions. Ideally, we want to ﬁnd a subspace by (cid:98)U ← arg max
Rd
× k:U(cid:62)U=Ik minimize (1
[n]:
|S (cid:48)|≥
−
S (cid:48)⊆
α)n
U
∈ (cid:88) i
∈S (cid:48)
Tr[U(cid:62)XiU] (cid:123)(cid:122) (cid:125) (cid:124)
:=zi
, for n = nL1tL1, which is computationally intractable. This relies on the intuition that a good subspace preserves the second moment, even when large (potentially corrupted) points are removed.
×
We propose a ﬁltering approach in Algorithm 3. At each iteration, we alternate between ﬁnding a candidate semi-orthogonal matrix U0 ∈ Rd k containing the top-k singular vectors using the k_SVD routine and then ﬁltering out suspected corrupted data points, which have large trace norms in U0.
Existing ﬁltering approaches (e.g. [75]) use a single ﬁlter to remove examples with large trace norm (denoted by zi in Algorithm 3). This suffers from removing too many uncorrupted examples. We give a precise comparison in Eq. (6). We instead use two ﬁlters to add back some of those mistakenly removed points. The First-Filter partitions the input set into a good set SG and a bad set S0 \ SG. If the bad set contributed to a signiﬁcant portion of the projected trace (this can be detected by the shift in the mean of the remaining points µSG), a second ﬁlter is applied to the bad set, recovering some of the uncorrupted examples.
This algorithm and our analysis applies more generally to any random vector, and may be of independent interest in other applications requiring robust PCA. Under a mild assumption that xi ∼ P has a bounded fourth-moment, we prove the following, with a proof in §D.1.
Proposition 2.6 (Robust PCA for general PSD matrices). Let S = {xi ∼ P}n i=1 where Σ := (cid:3) is the second moment of P supported on Rd. Given k ∈ N, δ ∈ (0, 0.5), and
Ex a corrupted dataset S(cid:48) with α ∈ (0, 1/36] fraction corrupted arbitrarily, if P has a bounded support such that (cid:107)xx(cid:62) − Σ(cid:107)2 ≤ B for x ∼ P with probability one, and a bounded 4-th
≤ ν2, and n = Ω((dk2 + moment such that max (cid:104)(cid:0) (cid:10)A, xx(cid:62) − Σ(cid:11) (cid:1)2(cid:105) (cid:2)xx(cid:62)
Ex
∼P 1,rank(A)
A k (cid:107) (cid:107)F≤
≤
∼P 7
√ (B/ν) kα) log(d/(δα))/α), then with probability at least 1 − δ,
√ (cid:105) (cid:16) (cid:104)
Tr[Pk(Σ)] − Tr
= O
α Tr[Pk(Σ)] + ν (cid:17)
, kα and (cid:13) (cid:13) (cid:13)Σ − (cid:98)U (cid:98)U(cid:62)Σ (cid:98)U (cid:98)U(cid:62)
≤ (cid:107)Σ − Pk(Σ)(cid:107) (cid:16)
+ O
∗
α(cid:107)Pk(Σ)(cid:107)
∗
+ ν
√ (cid:17)
. kα (cid:98)U(cid:62)Σ (cid:98)U (cid:13) (cid:13) (cid:13)
∗ (4) (5) where (cid:98)U is the output of Algorithm 2, and Pk(·) is the best rank-k approximation of a matrix in (cid:96)2.
√
√
√
The ﬁrst term in the RHS of Eq. (5) is unavoidable, as we are outputting a rank-k subspace. In kα in Equation (4) the setting of Theorem 1 in which we are interested in, the last term of ν dominates the second term. We next show that this cannot be improved upon; no algorithm can learn the subspace with an additive error smaller than Ω(ν kα) under α fraction of corruption, even with inﬁnite samples. In the following minimax lower bound, since the total variation distance
DTV(P, P (cid:48)) ≤ α, the adversary can corrupted the datapoints from P (cid:48) to match the distribution P, by changing just the α fraction. It is impossible to tell if the corrupted samples came from P or P (cid:48), resulting in an O(ν
Proposition 2.7 (Information theoretic lower bound). Let (cid:98)U({xi}n i=1) be any subspace estimator that takes n samples from distribution P as input, and estimates the k principal components of
Σ := Ex (cid:3) from another distribution P (cid:48) that is α-close in total variation DTV. Then,
√ kα) error. (cid:2)xx(cid:62) max
P (cid:48))
,
P inf
U for any k ≥ 16, d ≥ k2/α, and B ≥ 2dν, where Θν,B is a set of all distributions D(cid:48) on Rd such that (cid:98) max (cid:2)(cid:13) (cid:13)xx(cid:62) − E(cid:2)xx(cid:62)
] ≤ ν2, and Px
≤ B(cid:3) = 1.
− (cid:107)Σ − Pk(Σ)(cid:107)
= Ω(ν (cid:104)(cid:13) (cid:13) (cid:13)Σ − (cid:98)U (cid:98)U(cid:62)Σ (cid:98)U (cid:98)U(cid:62) n i=1∼P (cid:13) (cid:13) (cid:13)
∗ kα), max
:DTV( xi}
Ex
∼P (cid:48)
Θν,B
P (cid:48)∈
E
A (cid:105)
≤
∼D(cid:48)[(cid:0)(cid:10)A, xx(cid:62) − Σ(cid:11)(cid:1)2 (cid:3)(cid:13) (cid:13)2
∼D(cid:48) 1 (cid:107)F≤
P
α
∗ (cid:107)
{ n
Comparisons with [75]. Outlier-Robust Principal Component Analysis (ORPCA) [75, 28, 76] studies a similar problem under a Gaussian model. For comparison, we can modify the best known
ORPCA estimator from [75] to our setting in Proposition 2.6, to get a semi-orthogonal (cid:98)U achieving (cid:13) (cid:13) (cid:13)Σ − (cid:98)U (cid:98)U(cid:62)Σ (cid:98)U (cid:98)U(cid:62) (cid:13) (cid:13) (cid:13)
∗
= (cid:107)Σ − Pk(Σ)(cid:107)
∗
+ O(cid:0) α1/2(cid:107)Pk(Σ)(cid:107)
+ νkα1/4 (cid:1) .
∗ (6)
We signiﬁcantly improve in the dominant third term (see Eq. (5)). Simulation results supporting our theoretical prediction are shown in Fig. 2. For the analysis and the experimental setup we refer to §K. (cid:105) (cid:98)U
Σ (cid:62) (cid:98)U (cid:104) r
T s t n i o p a t a d f o r e b m u n
Figure 2: Algorithm 2 performs close to an oracle which knows the corrupted points, improving upon HRPCA of [75], by removing more corrupted points and less uncorrupted ones.
α
α 3