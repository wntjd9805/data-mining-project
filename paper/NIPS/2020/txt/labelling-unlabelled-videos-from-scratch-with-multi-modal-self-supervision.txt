Abstract
A large part of the current success of deep learning lies in the effectiveness of data – more precisely: labelled data. Yet, labelling a dataset with human anno-tation continues to carry high costs, especially for videos. While in the image domain, recent methods have allowed to generate meaningful (pseudo-) labels for unlabelled datasets without supervision, this development is missing for the video domain where learning feature representations is the current focus. In this work, we a) show that unsupervised labelling of a video dataset does not come for free from strong feature encoders and b) propose a novel clustering method that allows pseudo-labelling of a video dataset without any human annotations, by leveraging the natural correspondence between the audio and visual modalities.
An extensive analysis shows that the resulting clusters have high semantic overlap to ground truth human labels. We further introduce the ﬁrst benchmarking results on unsupervised labelling of common video datasets Kinetics, Kinetics-Sound,
VGG-Sound and AVE2. 1

Introduction
One of the key tasks in machine learning is to convert continuous perceptual data such as images and videos into a symbolic representation, assigning discrete labels to it. This task is generally for-mulated as clustering [31]. For images, recent contributions such as [6, 13, 37, 72] have obtained good results by combining clustering and representation learning. However, progress has been more limited for videos, which pose unique challenges and opportunities. Compared to images, videos are much more expensive to annotate; at the same time, they contain more information, including a temporal dimension and two modalities, aural and visual, which can be exploited for better clus-tering. In this paper, we are thus interested in developing methods to cluster video datasets without manual supervision, potentially reducing the cost and amount of manual labelling required for video data.
Just as for most tasks in machine learning, clustering can be greatly facilitated by extracting a suit-able representation of the data. However, representations are usually learned by means of manually supplied labels, which we wish to avoid. Inspired by [79], we note that a solution is to consider one of the recent state-of-the-art self-supervised representation learning methods and apply an off-the-shelf clustering algorithm post-hoc. With this, we show that we can obtain very strong baselines for clustering videos.
∗Joint ﬁrst authors 2Code will be made available at https://github.com/facebookresearch/selavi 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Still, this begs the question of whether even better performance could be obtained by simultaneously learning to cluster and represent video data. Our main contribution is to answer this question afﬁrmatively and thus to show that good clusters do not come for free from good representations.
In order to do so, we consider the recent method SeLa [6], which learns clusters and representations for still images by solving an optimal transport problem, and substantially improve it to work with multi-modal data. We do this in three ways. First, we relax the assumption made in [6] that clusters are equally probable; this is not the case for semantic video labels, which tend to have a highly-skewed distribution [1, 29, 41], and extend the algorithm accordingly. Second, we account for the multi-modal nature of video data, by formulating the extraction of audio and visual information from a video as a form of data augmentation, thus learning a clustering function which is invariant to such augmentations. For this to work well, we also propose a new initialization scheme that synchronizes the different modalities before clustering begins. This encourages clusters to be more abstract and thus ‘semantic’ and learns a redundant clustering function which can be computed robustly from either modality (this is useful when a modality is unreliable, because of noise or compression).
Third, since clustering is inherently ambiguous, we propose to learn multiple clustering functions in parallel, while keeping them orthogonal, in order to cover a wider space of valid solutions.
With these technical improvements, our method for Self-Labelling Videos (SeLaVi) substantially outperforms the post-hoc approach [79], SeLa [6] applied to video frames, as well as a recent multi-modal clustering-based representation learning method, XDC [2]. We evaluate our method by test-ing how well the automatically learned clusters match manually annotated labels in four different video datasets: VGG-Sound [17], AVE [68], Kinetics [41] and Kinetics-Sound [3]. We show that our proposed model results in substantially better clustering performance than alternatives. For ex-ample, our method can perfectly group 32% of the videos in the VGG-Sound dataset and 55% in the
AVE dataset without using any labels during training. Furthermore, we show that, while some clus-ters do not align with the ground truth classes, they are generally semantically meaningful (e.g. they contain similar background music) and provide an interactive cluster visualization3.
In a nutshell, our key contributions are: (i) establishing video clustering benchmark results on four datasets for which labels need to be obtained in an unsupervised manner; (ii) developing and as-sessing several strong clustering baselines using state-of-the-art methods for video representation learning, and (iii) developing a new algorithm tailored to clustering multi-modal data resulting in state-of-the-art highly semantic labels. 2