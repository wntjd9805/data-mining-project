Abstract
What is a fair performance metric? We consider the choice of fairness metrics through the lens of metric elicitation – a principled framework for selecting perfor-mance metrics that best reﬂect implicit preferences. The use of metric elicitation enables a practitioner to tune the performance and fairness metrics to the task, context, and population at hand. Speciﬁcally, we propose a novel strategy to elicit group-fair performance metrics for multiclass classiﬁcation problems with multi-ple sensitive groups that also includes selecting the trade-off between predictive performance and fairness violation. The proposed elicitation strategy requires only relative preference feedback and is robust to both ﬁnite sample and feedback noise. 1

Introduction
Machine learning models are increasingly employed for critical decision-making tasks such as hiring and sentencing [44, 3, 11, 14, 31]. Yet, it is increasingly evident that automated decision-making is susceptible to bias, whereby decisions made by the algorithm are unfair to certain subgroups [5, 3, 10, 8, 31]. To this end, a wide variety of group fairness metrics have been proposed – all to reduce discrimination and bias from automated decision-making [25, 13, 17, 29, 49, 32]. However, a dearth of formal principles for selecting the most appropriate metric has highlighted the confusion of experts, practitioners, and end users in deciding which group fairness metric to employ [53]. This is further exacerbated by the observation that common metrics often lead to contradictory outcomes [29].
While the problem of selecting an appropriate fairness metric has gained prominence in recent years [17, 32, 53], it perhaps best understood as a special case of the task of choosing evaluation metrics in machine learning. For instance, when a cost-sensitive predictive model classiﬁes patients into cancer categories [50] even without considering fairness, it is often unclear how the cost-tradeoffs be chosen so that they reﬂect the expert’s decision-making, i.e., replacing expert intuition by quantiﬁable metrics. The recently proposed Metric Elicitation (ME) framework [20, 21] provides a solution. ME is a principled framework for eliciting performance metrics using feedback over classiﬁers from an end user. The motivation behind ME is that employing the performance metrics which reﬂect user tradeoffs will enable learning models that best capture user preferences [20]. As humans are often inaccurate in providing absolute preferences [41], Hiranandani et al. [20] propose to use pairwise comparison queries, where the user (oracle) is asked to compare two classiﬁers and provide a relative preference. Using such queries, ME aims to recover the oracle’s metric. Figure 1 (reproduced from [20]) illustrates the ME framework.
Existing research suggests a fundamental trade-off between algorithmic fairness and performance [25, 51, 11, 7, 32, 53], where in addition to appropriate metrics, the practitioner or policymaker must choose a trade-off operating point between the competing objectives [53]. To this end, we extend the ME framework from eliciting multiclass classiﬁcation metrics [21] to the task of eliciting fair performance metrics from pairwise preference feedback in the presence of multiple sensitive groups.
In particular, we elicit metrics that reﬂect, jointly, the (i) predictive performance evaluated as a weighting of classiﬁer’s overall predictive rates, (ii) fairness violation assessed as the discrepancy in predictive rates among groups, and (iii) a trade-off between the predictive performance and fairness 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
e2
S+ (cid:37) o
Sρ ek
Rm
R2
R1 e1
Figure 1: Framework of Metric Elicitation [20].
Figure 2: R1 × · · · × Rm (best seen in colors);
Ru ∀ u ∈ [m] are convex sets with common vertices ei ∀ i ∈ [k] and enclose the sphere Sρ. violation. Importantly, the elicited metrics are sufﬁciently ﬂexible to encapsulate and generalize many existing predictive performance and fairness violation measures.
In eliciting group-fair performance metrics, we tackle three new challenges. First, from preference query perspective, the predictive performance and fairness violations are correlated, thus increasing the complexity of joint elicitation. Second, we ﬁnd that in order to measure both positive and negative violations, the fair metrics are necessarily non-linear functions of the predictive rates, thus existing results on linear ME [21] cannot be applied directly. Finally, as we show, the number of groups directly impacts query complexity. We overcome these challenges by proposing a novel query efﬁcient procedure that exploits the geometric properties of the set of rates.
Contributions. We consider metrics for algorithmically group-fair classiﬁcation and propose a novel approach for eliciting predictive performance, fairness violations, and their trade-off point, from expert pairwise feedback. Our procedure uses binary-search based subroutines and recovers the metric with linear query complexity. Moreover, the procedure is robust to both ﬁnite sample and oracle feedback noise thus is useful in practice. Lastly, our method can be applied either by querying preferences over classiﬁers or rates. Such an equivalence is crucial for practical applications [20, 21].
Notations. Matrices and vectors are denoted by bold upper case and bold lower case letters, respec-tively. We denote the inner product of two vectors by (cid:104)·, ·(cid:105) and the Hadamard product by (cid:12). The (cid:96)2-norm is denoted by (cid:107)·(cid:107)2. For k ∈ Z+, we represent the index set {1, 2, · · · , k} by [k], and the (k − 1)-dimensional simplex by ∆k. Given a matrix A, oﬀ -diag(A) returns a vector of off-diagonal elements of A in row-major form. The group membership is denoted by superscripts and coordinates of vectors, matrices, and tuples are denoted by subscripts. 2