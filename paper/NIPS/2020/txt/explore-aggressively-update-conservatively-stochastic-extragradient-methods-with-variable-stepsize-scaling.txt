Abstract
Owing to their stability and convergence speed, extragradient methods have be-come a staple for solving large-scale saddle-point problems in machine learning.
The basic premise of these algorithms is the use of an extrapolation step before per-forming an update; thanks to this exploration step, extragradient methods overcome many of the non-convergence issues that plague gradient descent/ascent schemes.
On the other hand, as we show in this paper, running vanilla extragradient with stochastic gradients may jeopardize its convergence, even in simple bilinear models.
To overcome this failure, we investigate a double stepsize extragradient algorithm where the exploration step evolves at a more aggressive time-scale compared to the update step. We show that this modiﬁcation allows the method to converge even with stochastic gradients, and we derive sharp convergence rates under an error bound condition. 1

Introduction
A major obstacle in the training of generative adversarial networks (GANs) is the lack of an imple-mentable, strongly convergent method based on stochastic gradients. The reason for this is that the coupling of two (or more) neural networks gives rise to behaviors and phenomena that do not occur when minimizing an individual loss function, irrespective of the complexity of its landscape. As a result, there has been signiﬁcant interest in the literature to codify the failures of GAN training, and to propose methods that could potentially overcome them.
Perhaps the most prominent of these failures is the appearance of cycles [4, 6, 7, 20, 21] and, potentially, the transition to aperiodic orbits and chaos [3, 8, 28, 29, 31]. Surprisingly, non-convergent phenomena of this kind are observed even in very simple saddle-point problems such as two-dimensional, unconstrained bilinear games [4, 7, 21]. In view of this, it is quite common to examine the convergence (or non-convergence) of a gradient training scheme in bilinear models before applying it to more complicated, non-convex/non-concave problems. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
A key observation here is that the non-convergence of standard gradient descent-ascent methods in bilinear saddle-point problems can be overcome by incorporating a “gradient extrapolation” step before performing an update. The resulting algorithm, due to Korpelevich [13], is known as the extragradient (EG) method, and it has a long history in optimization; for an appetizer, see Facchinei &
Pang [5], Juditsky et al. [11], Nemirovski [26], Nesterov [27], and references therein. In particular, the extragradient algorithm converges for all pseudomonotone variational inequalities (a large problem class that contains all bilinear games, cf. [13]), and the time-average of the generated iterates achieves an O(1/t) rate of convergence in monotone problems [26].
The above concerns the application of extragradient methods with perfect, deterministic gradients and a non-vanishing stepsize. By contrast, in the type of saddle-point problems that are encountered in machine learning (GANs, robust reinforcement learning, etc.), there are two important points to keep in mind: First, the size of the datasets involved precludes the use of full gradients (for more than a few passes at least), so the method must be run with stochastic gradients instead. Second, because the landscapes encountered are not convex-concave, the method’s last iterate is typically preferred to its time-average (which offers no tangible beneﬁts when Jensen’s inequality no longer applies). We are thus led to the following questions: (i) are the superior last-iterate convergence properties of the
EG algorithm retained in the stochastic setting? And, if not, (ii) is there a principled modiﬁcation that would restore them?
Our contributions. To motivate our analysis, we ﬁrst analyse a counterexample to show that the last iterate of stochastic EG fails to converge, even in bilinear min-max problems where deterministic EG methods converge from any initialization. We then consider a class of double stepsize extragradient (DSEG) methods with an exploration step evolving more aggressively than the update step and prove it enjoys better convergence guarantees than standard EG in stochastic problems. In more detail: 1. We show that the DSEG algorithm converges with probability 1 in a large class of problems that contains all monotone saddle-point problems. 2. We derive explicit convergence rates for the algorithm’s last iterate under an error bound condition.
This is the ﬁrst time that such condition is considered in the analysis of stochastic EG methods, albeit its popularity in the optimization community. 3. For bilinear min-max problems in particular, our analysis establishes that stochastic DSEG methods converge at a O(1/t) rate. Prior to our work, last-iterate convergence rate for bilinear min-max games had only been studied in the deterministic setting.1 4. To account for non-monotone problems, we also provide local versions of these results that hold with (arbitrarily) high probability. Importantly, thanks to the use of a local error bound condition, we can obtain local convergence rates even if the Jacobian at a solution contains purely imaginary eigenvalues.