Abstract
This paper analyzes the trajectories of stochastic gradient descent (SGD) to help understand the algorithm’s convergence properties in non-convex problems. We
ﬁrst show that the sequence of iterates generated by SGD remains bounded and converges with probability 1 under a very broad range of step-size schedules.
Subsequently, going beyond existing positive probability guarantees, we show that SGD avoids strict saddle points/manifolds with probability 1 for the entire spectrum of step-size policies considered. Finally, we prove that the algorithm’s rate of convergence to local minimizers with a positive-deﬁnite Hessian is O(1/np) if the method is employed with a Θ(1/np) step-size. This provides an important guideline for tuning the algorithm’s step-size as it suggests that a cool-down phase with a vanishing step-size could lead to faster convergence; we demonstrate this heuristic using ResNet architectures on CIFAR. 1

Introduction
Owing to its simplicity and empirical successes, stochastic gradient descent (SGD) has become the de facto method for training a wide range of models in machine learning. This paper examines the properties of SGD in non-convex problems with the aim of answering the following questions: (Q1) Does SGD always converge to the problem’s critical set? (Q2) Does SGD always avoid spurious critical regions (non-isolated saddle points, ridges, etc.)? (Q3) How fast does SGD converge to local minimizers as a function of the method’s step-size?
We provide the following contributions to these questions:
On (Q1): Under mild conditions for the function to be optimized, and allowing for a wide range of step-size schedules of the form Θ(1/np) for p ∈ (0, 1], the sequence of iterates Xn generated by SGD converges with probability 1. In contrast to existing mean squared error guarantees of the form E[(cid:107)∇f (Xn)(cid:107)2] → 0 (where f is the problem’s objective), our result is a stronger, trajectory convergence result: It is not a guarantee that holds on average, but a convergence certiﬁcate that applies with probability 1 to any instantiation of the algorithm. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
On (Q2): With probablity 1, the trajectories of SGD avoid all strict saddle manifolds – i.e., sets of critical points x∗ with at least one negative Hessian eigenvalue (λmin(∇2f (x∗)) < 0). Such manifolds include ridge surfaces and other connected sets of non-isolated saddle points that are common in the loss landscapes of overparametrized neural networks [31]. In this way, our result complements and extends a series of almost sure saddle avoidance results for deterministic gradient descent [13, 14, 27, 28, 39], and with high probability [15] or in expectation [48] for SGD.
If SGD is run with a step-size schedule of the form γn = Θ(1/np) for some p ∈ (0, 1],
On (Q3): the algorithm enjoys a local convergence rate of the form E[f (Xn) − f (x∗)] = O(1/np) relative to
Hurwicz local minimizers (i.e., ∇2f (x∗) (cid:31) 0). We stress here that this is a “last iterate” convergence guarantee; neither ergodic, nor of a mean-squared gradient norm type. This is crucial for real-world applications because, in practice, SGD training is based on the last generated point.
Taken together, the above suggests that a vanishing step-size policy has signiﬁcant theoretical beneﬁts: almost sure convergence, avoidance of spurious critical points (again with probability 1), and fast stabilization to local minimizers. We explore these properties in a range of standard non-convex test functions and by training a ResNet architecture for a classiﬁcation task over CIFAR.
The linchpin of our approach is the ODE method of stochastic approximation as pioneered by
Benveniste et al. [5], Kushner and Yin [26], Ljung [32], and Benaïm [2]. As such, our analysis combines a wide range of techniques from the theory of dynamical systems along with a series of martingale limit theory tools originally developed by Pemantle [40] and Brandière and Duﬂo [9].