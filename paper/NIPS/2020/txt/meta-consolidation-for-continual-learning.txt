Abstract
The ability to continuously learn and adapt itself to new tasks, without losing grasp of already acquired knowledge is a hallmark of biological learning systems, which current deep learning systems fall short of. In this work, we present a novel methodology for continual learning called MERLIN: Meta-Consolidation for
Continual Learning. We assume that weights of a neural network  , for solving t). This meta-distribution is learned task t, come from a meta-distribution p(  and consolidated incrementally. We operate in the challenging online continual learning setting, where a data point is seen by the model only once. Our experiments with continual learning benchmarks of MNIST, CIFAR-10, CIFAR-100 and Mini-ImageNet datasets show consistent improvement over ﬁve baselines, including a recent state-of-the-art, corroborating the promise of MERLIN.
|

Introduction 1
The human brain is able to constantly, and incrementally, consolidate new information with existing information, allowing for quick recall when needed [5, 78]. In this natural setting, it is not common to see the same data sample multiple times, or even twice at times. Human memory capacity is also limited which forbids memorizing all examples that are seen during its lifetime [39]. Hence, the brain operates in an online manner, where it is able to adapt itself to continuously changing data distributions without losing grasp of its previously acquired knowledge [33]. Unfortunately, deep neural networks have been known to suffer from catastrophic forgetting [53, 25], where they fail to retain performance on older tasks, while learning new tasks.
Continual learning is a machine learning setting characterized by its requirement to have a learning model incrementally adapt to new tasks, while not losing its performance on previously learned tasks.
Note that ‘task’ here can refer to a set of new classes, new domains (e.g. thermal, RGB) or even new tasks in general (e.g. colorization, segmentation) [63, 68, 89]. The last few years have seen many efforts to develop methods to address this setting from various perspectives. One line of work
[88, 38, 47, 2, 16, 71, 56, 13, 79, 42] constrains the parameters of the deep network trained on Task
A to not change much while learning a new Task B, while another - replay-based methods - store
[63, 48, 14, 66, 9, 3, 4, 22] or generate [74, 81, 46, 45] examples of previous tasks to ﬁnetune the ﬁnal model at evaluation time. Another kind of methods [51, 72, 69, 19, 68, 62, 87] attempt to expand the network to increase the capacity of the model, while learning new tasks. Broadly speaking, all these methods manipulate the data space or the weight space in different ways to achieve their objectives.
In this work, we propose a different perspective to addressing continual learning, based on the latent space of a weight-generating process, rather than the weights themselves. Studies of the human brain suggest that knowledge and skills to solve tasks are represented in a meta-space of concepts with a high-level semantic basis [28, 10, 50]. The codiﬁcation from tasks to concepts, and the periodic consolidation of memory, are considered essential for a transferable and compact representation of knowledge that helps humans continually learn [11, 85, 5]. Current continual learning methods consolidate (assimilate knowledge on past tasks) either in the weight space [13, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
38, 47, 16, 88, 56, 2, 71] or in the data space [14, 63, 74, 48, 3, 4, 45, 66]. Even meta-learning based continual learning methods that have been proposed in the recent past [34, 24, 6, 64], meta-learn an initialization amenable for quick adaptation across tasks, similar to MAML [23], and hence operate in the weight space. We propose MERLIN: Meta-Consolidation for Continual Learning, a new method for continual learning that is based on consolidation in a meta-space, viz. the latent space which generates model weights for solving downstream tasks.
|
| p( 
We consider weights of a neural network  , which can solve a speciﬁc task, to come from a meta-distribution p(  t), where t is a representation for the task. We propose a methodology to learn this distribution, as well as continually adapt it to be competent on new tasks by consolidating this meta-space of model parameters whenever a new task arrives. We refer to this process as “Meta-Consolidation”. We ﬁnd that continually learning in the parameter meta-space with consolidation is an effective approach to continual learning. Learning such a meta-distribution p(  t) provides additional beneﬁts: (i) at inference time, any number of models can be sampled from the distribution t), which can then be ensembled for prediction in each task (Sec 4.1.5); (ii) it is easily  t ⇠ adapted to work in multiple settings such as class-incremental and domain-incremental continual learning (Sec 4); and (iii) it can work in both a task-aware setting (where the task is known at test time) and task-agnostic setting where the task is not known at test time (achieved by marginalizing over t, Sec 5.2). Being able to take multiple passes through an entire dataset is an assumption that most existing continual learning methods make [2, 38, 47, 88, 63, 12]. Following [4, 3, 14, 48], we instead consider the more challenging (and more natural) online continual learning setting where ‘only a single pass through the data’ is allowed. We compare MERLIN against a recent state-of-the-art GSS
[4], as well as well-known methods including GEM [48], iCaRL [63] and EWC [38] on Split MNIST
[13], Permuted MNIST [88], Split CIFAR-10 [88], Split CIFAR-100 [63] and Split Mini-Imagenet
[15] datasets. We observe consistent improvement across the datasets over baseline methods in Sec 4.
|
The key contributions of this work can be summarized as: (i) We introduce a new perspective to continual learning based on the meta-distribution of model parameters, and their consolidation over tasks arriving in time; (ii) We propose a methodology to learn this distribution using a Variational
Auto-encoder (VAE) [37] with learned task-speciﬁc priors, which also allows us to ensemble models for each task at inference; (iii) We show that the proposed method outperforms well-known benchmark methods [48, 63, 38], as well as a recent state-of-the-art method [4], on ﬁve continual learning datasets; (iv) We perform comprehensive ablation studies to provide a deeper understanding of the proposed methodology and showcase its usefulness. To the best of our knowledge, MERLIN is the ﬁrst effort to incrementally learn in the meta-space of model parameters. 2