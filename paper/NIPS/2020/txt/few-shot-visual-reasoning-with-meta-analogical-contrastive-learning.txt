Abstract
While humans can solve a visual puzzle that requires logical reasoning by observing only few samples, it would require training over a large number of samples for state-of-the-art deep reasoning models to obtain similar performance on the same task. In this work, we propose to solve such a few-shot (or low-shot) abstract visual reasoning problem by resorting to analogical reasoning, which is a unique human ability to identify structural or relational similarity between two sets. Speciﬁcally, we construct analogical and non-analogical training pairs of two different problem instances, e.g., the latter is created by perturbing or shufﬂing the original (former) problem. Then, we extract the structural relations among elements in both domains in a pair by enforcing analogical ones to be as similar as possible, while minimizing similarities between non-analogical ones. This analogical contrastive learning allows to effectively learn the relational representations of given abstract reasoning tasks. We validate our method on RAVEN dataset, on which it outperforms state-of-the-art method, with larger gains when the training data is scarce. We further meta-learn our analogical contrastive learning model over the same tasks with diverse attributes, and show that it generalizes to the same visual reasoning problem with unseen attributes. 1

Introduction
The visual reasoning task proposed in recent works [1, 2] often involves visual puzzles such as Raven
Progressive Matrices (RPM), whose goal is to ﬁnd an implicit rule among the given image panels, and predict the correct image panel that will complete the puzzle (see Figure 1b). Since one should identify a common relational similarity among the visual instances with diverse attributes (shape, size, and color), solving such a visual reasoning problem requires reasoning skills which might help take a step further toward general artiﬁcial intelligence.
Recently, researchers in the machine learning community have proposed speciﬁc models for visual reasoning using deep learning [1, 3, 4, 5]. Deep neural networks are powerful and effective predictors for a wide spectrum of tasks such as classiﬁcation and language modelling, and has yielded impressive performance on them. However, while humans can solve these logical and abstract reasoning problems by observing only few samples, the state-of-the-art deep reasoning models still require large number of training samples to achieve similar performance on the same task (see Figure 1c).
We hypothesize that such sample-efﬁciency comes from the ﬂexibility of human intelligence, which can generalize well across different problems by identifying the same pattern in the two problem-pairs with analogical reasoning. For instance, given any two pairs of relationships, “A is to B” and “C is to D”, one can form an analogy from the morphological parallelism between the two, e.g., as “A is to
B as C is to D” or “A : B :: C : D” (Figure 1a). With such an analogical relationship established between the pairs, one will be able to predict any single entity given the rest of the entities. For visual 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Analogy between word pairs (b) Analogy between visual sets (c) Few-shot visual reasoning
Figure 1: Concept. Analogy deﬁnes the relational or structural similarity between two sets. Analogy can be deﬁned between (a) a pair of words or (b) between sets of visual elements. Analogies allow for sample-efﬁcient learning, which is not possible with (c) the state-of-the-art deep abstract visual reasoning network [3] which suffers from dramatic performance degeneration with fewer number of training examples. reasoning problems, we could also identify a visuospatial analogy [6] between two sets of visual entities. In general, any pair of sets from which we could identify certain relational or structural similarity between the two sets can sufﬁce as an analogy. Analogical reasoning has been extensively investigated to understand human reasoning process in the ﬁeld of cognitive science [7] and artiﬁcial ingelligence [8, 9].
However, the existing deep visual reasoning models do not have ability to perform such analogical reasoning. To overcome this limitation, we propose a deep representation learning method that exploits such analogical relationship between two sets of instances, with the special focus on the abstract visual reasoning problem. This will allow the model to learn representations that capture relational similarities between the entities, in a sample-efﬁcient manner. Speciﬁcally, we train an end-to-end abstract visual reasoning network to learn the abstract concepts by learning an analogy between two problems with the same implicit rules among the set elements, but with different individual elements (Figure 1b). The set that is paired with the original problem in an analogy can be either constructed by perturbing the original problem, or by generating a problem with different attributes. Since we know that the abstract relationships between the elements in two sets are the same, their relational representations should be as similar as possible. On the other hand, the similarity should be minimized between a pair of problems with different relations among the set elements, even when the two are identical at the element level.
To this end, we propose a contrastive learning [10, 11] framework to maximize the similarity between analogical pairs of problems while minimizing the similarity across non-analogical pairs. which we refer to as Analogical Contrastive Learning (ACL). We further propose a meta-learning algorithm to train the model over analogical pairs with different attributes, such that it can generalize to unseen attributes (Meta-ACL). We validate our methods on conventional many-shot visual reasoning tasks, as well as few-shot visual reasoning tasks with both seen and unseen attributes, on which it signiﬁcantly outperforms existing methods. The summary of our contribution is as follows:
• We tackle a challenging problem of few-shot visual reasoning, whose goal is to learn to perform abstract visual reasoning with only a few-instances of the given problem, while generalizing to problems with unseen attributes.
• We propose a simple yet novel meta-analogical contrastive learning framework that allows the model to capture the relational similarity between a pair of problems.
• We validate our model on a visual reasoning benchmark, RAVEN, and show that it signif-icantly outperforms existing methods under both the conventional and few-shot learning setting, as well as on generalization to problems with unseen attributes. 2