Abstract
In multi-agent reinforcement learning, discovering successful collective behaviors is challenging as it requires exploring a joint action space that grows exponentially with the number of agents. While the tractability of independent agent-wise exploration is appealing, this approach fails on tasks that require elaborate group strategies. We argue that coordinating the agents’ policies can guide their exploration and we investigate techniques to promote such an inductive bias. We propose two policy regularization methods: TeamReg, which is based on inter-agent action predictability and CoachReg that relies on synchronized behavior selection. We evaluate each approach on four challenging continuous control tasks with sparse rewards that require varying levels of coordination as well as on the discrete action Google Research Football environment. Our experiments show improved performance across many cooperative multi-agent problems. Finally, we analyze the effects of our proposed methods on the policies that our agents learn and show that our methods successfully enforce the qualities that we propose as proxies for coordinated behaviors. 1

Introduction
Multi-Agent Reinforcement Learning (MARL) refers to the task of training an agent to maximize its expected return by interacting with an environment that contains other learning agents. It represents a challenging branch of Reinforcement Learning (RL) with interesting developments in recent years
[11]. A popular framework for MARL is the use of a Centralized Training and a Decentralized
Execution (CTDE) procedure [24, 8, 14, 7, 28]. Typically, one leverages centralized critics to approximate the value function of the aggregated observations-actions pairs and train actors restricted to the observation of a single agent. Such critics, if exposed to coordinated joint actions leading to high returns, can steer the agents’ policies toward these highly rewarding behaviors. However, these approaches depend on the agents luckily stumbling on these collective actions in order to grasp their beneﬁt. Thus, it might fail in scenarios where such behaviors are unlikely to occur by chance. We hypothesize that in such scenarios, coordination-promoting inductive biases on the policy search
∗Equal contribution.
‡Canada CIFAR AI Chair. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
could help discover successful behaviors more efﬁciently and supersede task-speciﬁc reward shaping and curriculum learning. To motivate this proposition we present a simple Markov Game in which agents forced to coordinate their actions learn remarkably faster. For more realistic tasks in which coordinated strategies cannot be easily engineered and must be learned, we propose to transpose this insight by relying on two coordination proxies to bias the policy search. The ﬁrst avenue, TeamReg, assumes that an agent must be able to predict the behavior of its teammates in order to coordinate with them. The second, CoachReg, supposes that coordinated agents collectively recognize different situations and synchronously switch to different sub-policies to react to them.2.
Our contributions are threefold. First, we show that coordination can crucially accelerate multi-agent learning for cooperative tasks. Second, we propose two novel approaches that aim at promoting such coordination by augmenting CTDE MARL algorithms through additional multi-agent objectives that act as policy regularizers and are optimized jointly with the main return-maximization objective.
Third, we design two new sparse-reward cooperative tasks in the multi-agent particle environment
[26]. We use them along with two standard multi-agent tasks to present a detailed evaluation of our approaches’ beneﬁts when they extend the reference CTDE MARL algorithm MADDPG [24]. We validate our methods’ key components by performing an ablation study and a detailed analysis of their effect on agents’ behaviors. Finally, we verify that these beneﬁts hold on the more complex, discrete action, Google Research Football environment [20].
Our experiments suggest that our TeamReg objective provides a dense learning signal that can help guiding the policy towards coordination in the absence of external reward, eventually leading it to the discovery of higher performing team strategies in a number of cooperative tasks. However we also ﬁnd that TeamReg does not lead to improvements in every single case and can even be harmful in environments with an adversarial component. For CoachReg, we ﬁnd that enforcing synchronous sub-policy selection enables the agents to concurrently learn to react to different agreed upon situations and consistently yields signiﬁcant improvements on the overall performance. 2