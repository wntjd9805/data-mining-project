Abstract
A soft-max function has two main efï¬ciency measures: (1) approximation - which corresponds to how well it approximates the maximum function, (2) smoothness
- which shows how sensitive it is to changes of its input. Our goal is to identify the optimal approximation-smoothness tradeoffs for different measures of approx-imation and smoothness. This leads to novel soft-max functions, each of which is optimal for a different application. The most commonly used soft-max function, called exponential mechanism, has optimal tradeoff between approximation mea-sured in terms of expected additive approximation and smoothness measured with respect to RÃ©nyi Divergence. We introduce a soft-max function, called piecewise linear soft-max, with optimal tradeoff between approximation, measured in terms of worst-case additive approximation and smoothness, measured with respect to
â„“ğ‘-norm. The worst-case approximation guarantee of the piecewise linear mech-anism enforces sparsity in the output of our soft-max function, a property that is known to be important in Machine Learning applications [14, 12] and is not sat-isï¬ed by the exponential mechanism. Moreover, the â„“ğ‘-smoothness is suitable for applications in Mechanism Design and Game Theory where the piecewise linear mechanism outperforms the exponential mechanism. Finally, we investigate an-other soft-max function, called power mechanism, with optimal tradeoff between expected multiplicative approximation and smoothness with respect to the RÃ©nyi
Divergence, which provides improved theoretical and practical results in differen-tially private submodular optimization. 1

Introduction
A soft-max function is a mechanism for choosing one out of a number of options, given the value of each option. Such functions have applications in many areas of computer science and machine learning, such as deep learning (as the ï¬nal layer of a neural network classiï¬er) [3, 2, 8], reinforce-ment learning (as a method for selecting an action) [20], learning from mixtures of experts [11], differential privacy [6, 15], and mechanism design [15, 10]. The common requisite in these appli-cations is for the soft-max function to pick an option with close-to-maximum value, while behaving smoothly as the input changes.
The soft-max function that has come to dominate these applications is the exponential function.
Given ğ‘‘ options with values ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘‘, the exponential mechanism picks ğ‘– with probability exp(ğœ†ğ‘¥ğ‘–)/(âˆ‘ï¸€ğ‘‘
ğ‘—=1 exp(ğœ†ğ‘¥ğ‘—)) for a parameter ğœ† > 0. This function has a long history: It has been proposed as a model in decision theory in 1959 by Luce [13], and has its roots in the Boltzman (also 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
known as Gibbs) distribution in statistical mechanics [1, 7]. There are, however, many other ways to smoothly pick an approximately maximal element from a list of values. This raises the question: is there a way to quantify the desirable properties of soft-max functions, and are there other soft-max functions that perform well under such criteria? If there are such functions, perhaps they can be added to our repertoire of soft-max functions and might prove suitable in some applications.
These questions are the subject of this paper. We explore the tradeoff between the approximation guarantee of a soft-max function and its smoothness. A soft-max function is ğ›¿-approximate if the expected value of the option it picks is at least the maximum value minus ğ›¿. Stronger yet, a function is ğ›¿-approximate in the worst case if it never picks an option of value less than the maximum minus
ğ›¿. We capture the requirement of smoothness using the notion of Lipschitz continuity. A function is Lipschitz continuous if by changing its input by some amount ğ‘¥, its output changes by at most a multiple of ğ‘¥. This multiplier, known as the Lipschitz constant, is then a measure of the smoothness of the function. This notion requires a way to measure distances in the domain (the input space) and the range (the output space) of the function.
We will show that if the ğ‘-norm and the RÃ©nyi divergence are used to measure distances in the domain and the range, respectively, then the exponential mechanism achieves the lowest possible (to within a constant factor) Lipschitz constant among all ğ›¿-approximate soft-max functions. This
Lipschitz constant is ğ‘‚(log(ğ‘‘)/ğ›¿). The exponential function picks each option with a non-zero probability, and therefore cannot guarantee an approximation in the worst case. In fact, we will show that for these distance measures, there is no soft-max function with bounded Lipschitz constant that can guarantee an approximation in the worst case.
On the other hand, if we use ğ‘-norms to measure changes in both the input and the output, new possibilities open up. We construct a soft-max function (called PLSOFTMAX, for piecewise linear soft-max) that achieves a Lipschitz constant of ğ‘‚(1/ğ›¿) and is also ğ›¿-approximate in the worst case.
This is an important property, as it guarantees that the output of the soft-max function is always as sparse as possible. Furthermore, we prove that even only requiring ğ›¿-approximation in expectation, no soft-max function can achieve a Lipschitz constant of ğ‘œ(1/ğ›¿) for these distance measures.
We also study several other properties we might want to require of a soft-max function. Most notably, what happens if instead of requiring an additive approximation guarantee, we require a multiplicative one? A simple way to construct a soft-max function satisfying this requirement is to apply soft-max functions with additive approximation (e.g., exponential or PLSOFTMAX) on the logarithm of the values. The resulting mechanisms (the power mechanism, and LOGPLSOFTMAX) are Lipschitz continuous, but with respect to a domain distance measure called log-Euclidean. More-over, we show that with the standard ğ‘-norm distance as the domain distance measure, no soft-max function with bounded Lipschitz constant and multiplicative approximation guarantee exists.
Finally, we explore several applications of the new soft-max functions introduced in this paper. First, we show how the power mechanism can be used to improve existing results (using the exponential mechanism) on differentially private submodular maximization. Second, we use PLSOFTMAX to design improved incentive compatible mechanisms with worst-case guarantees. Finally, we discuss how PLSOFTMAX can be used as the ï¬nal layer of deep neural networks in multiclass classiï¬cation. 1.1