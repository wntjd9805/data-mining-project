Abstract
We introduce an approach to training a given compact network. To this end, we leverage over-parameterization, which typically improves both neural network optimization and generalization. Speciﬁcally, we propose to expand each linear layer of the compact network into multiple consecutive linear layers, without adding any nonlinearity. As such, the resulting expanded network, or ExpandNet, can be contracted back to the compact one algebraically at inference. In particular, we introduce two convolutional expansion strategies and demonstrate their beneﬁts on several tasks, including image classiﬁcation, object detection, and semantic segmentation. As evidenced by our experiments, our approach outperforms both training the compact network from scratch and performing knowledge distillation from a teacher. Furthermore, our linear over-parameterization empirically reduces gradient confusion during training and improves the network generalization. 1

Introduction
With the growing availability of large-scale datasets and advanced computational resources, con-volutional neural networks have achieved tremendous success in a variety of tasks, such as image classiﬁcation [17, 28], object detection [39, 40, 42] and semantic segmentation [35, 45]. Over the past few years, “Wider and deeper are better” has become the rule of thumb to design network archi-tectures [17, 22, 51, 52]. This trend, however, raises memory- and computation-related challenges, especially in the context of constrained environments, such as embedded platforms.
Deep and wide networks are well-known to be heavily over-parameterized, and thus a compact network, both shallow and thin, should often be sufﬁcient. Unfortunately, compact networks are notoriously hard to train from scratch. As a consequence, designing strategies to train a given compact network has drawn growing attention, the most popular approach consisting of transferring the knowledge of a deep teacher network to the compact one of interest [19, 18, 20, 37, 44, 54, 60, 61].
In this paper, we introduce an alternative approach to training compact neural networks, com-plementary to knowledge transfer. To this end, building upon the observation that network over-parameterization improves both optimization and generalization [1, 2, 5, 26, 41, 49, 62], we propose to increase the number of parameters of a given compact network by incorporating additional layers.
However, instead of separating every two layers with a nonlinearity, we advocate introducing consecu-tive linear layers. In other words, we expand each linear layer of a compact network into a succession of multiple linear layers, without any nonlinearity in between. Since consecutive linear layers are equivalent to a single one [50], such an expanded network, or ExpandNet, can be algebraically contracted back to the original compact one without any information loss.
While the use of successive linear layers appears in the literature, existing work [5, 6, 25, 29, 50, 63] has been mostly conﬁned to fully-connected networks without any nonlinearities and to the theoretical study of their behavior under fairly unrealistic statistical assumptions. In particular, these studies aim 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to understand the learning dynamics and the loss landscapes of deep networks. Here, by contrast, we focus on practical, nonlinear, compact convolutional neural networks, and demonstrate the use of linear expansion as a means to introduce over-parameterization and facilitate training, so that a given compact network achieves better performance.
Speciﬁcally, as illustrated by Figure 1, we intro-duce three ways to expand a compact network: k convolution by three convo-(i) replacing a k
× k and lutional layers with kernel size 1
×
× k convolu-1, respectively; (ii) replacing a k 1
× 3 convolutions; tion with k > 3 by multiple 3 and (iii) replacing a fully-connected layer with multiple ones. Our experiments demonstrate that expanding convolutions is the key to ob-taining more effective compact networks. 1, k
×
×
In short, our contributions are (i) a novel ap-proach to training a given compact, nonlinear convolutional network by expanding its linear layers; (ii) a strategy to expand convolutional layers with arbitrary kernels; and (iii) a strat-egy to expand convolutional layers with kernel size larger than 3. We demonstrate the bene-ﬁts of our approach on several tasks, including image classiﬁcation on ImageNet, object detec-tion on PASCAL VOC and image segmentation on Cityscapes. Our ExpandNets outperform both training the corresponding compact networks from scratch and using knowledge distilla-tion. We empirically show over-parameterization to be the key factor for such better performance.
Furthermore, we analyze the beneﬁts of linear over-parameterization during training via experi-ments studying generalization, gradient confusion and the loss landscape. Our code is available at https://github.com/GUOShuxuan/expandnets.
Figure 1: ExpandNets. We propose 3 strategies to linearly expand a compact network. An expanded network can then be contracted back to the com-pact one algebraically, and outperforms training the compact one, even with knowledge distillation. 2