Abstract
While stochastic gradient descent (SGD) is still the de facto algorithm in deep learning, adaptive methods like Clipped SGD/Adam have been observed to out-perform SGD across important tasks, such as attention models. The settings under which SGD performs poorly in comparison to adaptive methods are not well un-derstood yet. In this paper, we provide empirical and theoretical evidence that a heavy-tailed distribution of the noise in stochastic gradients is one cause of SGD’s poor performance. We provide the ﬁrst tight upper and lower convergence bounds for adaptive gradient methods under heavy-tailed noise. Further, we demonstrate how gradient clipping plays a key role in addressing heavy-tailed gradient noise.
Subsequently, we show how clipping can be applied in practice by developing an adaptive coordinate-wise clipping algorithm (ACClip) and demonstrate its superior performance on BERT pretraining and ﬁnetuning tasks. 1

Introduction
Stochastic gradient descent (SGD) is the canonical algorithm for training neural networks [24]. SGD iteratively updates model parameters in the negative gradient direction and seamlessly scales to large-scale settings. Though a well-tuned SGD outperforms adaptive methods [31] in many tasks including ImageNet classiﬁcation (see Figure 1a), certain tasks necessitate the use of adaptive variants of SGD (e.g., Adagrad [10], Adam [14], AMSGrad [23]), which employ adaptive learning rates. For instance, consider training an attention model [29] using BERT [9]. Figure 1e shows that in spite of extensive hyperparameter tuning, SGD converges much slower than Adam during BERT training.
In this work, we provide one explanation for why adaptivity can facilitate convergence with theoretical and empirical evidence. The signiﬁcant hint that initializes our work comes from the distribution of the stochastic gradients. For Imagenet, the norms of the mini-batch gradients are typically quite small and well concentrated around their mean. On the other hand, the mini-batch gradient norms for BERT take a wide range of values and are sometimes much larger than their mean value. More formally, while the distribution of the stochastic gradients in Imagenet is well approximated by a Gaussian, the distribution for BERT seems to be heavy-tailed. Such observation leads us to the question: does adaptivity stabilize optimization under heavy-tailed noise? 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We provide a positive answer to the above question by performing both theoretical and empirical studies of the convergence of optimization methods under heavy-tailed noise. In this setting, some of the stochastic gradients are much larger than the mean and can excessively inﬂuence the updates of
SGD. This makes SGD unstable and leads to its poor performance. A natural strategy to stabilize the updates is to clip the magnitude of the stochastic gradients. We prove that indeed it is sufﬁcient to ensure convergence even under heavy-tailed noise. Based on the analysis, we then motivate the design of a novel algorithm (ACClip) that outperforms ADAM on BERT related tasks. Speciﬁcally, we make the following contributions:
• We empirically show that in tasks on which Adam outperforms SGD (BERT pretraining), the noise in stochastic gradients is heavy-tailed. On the other hand, on tasks where traditionally
SGD outperforms Adam (ImageNet training), we show that the noise is well concentrated.
• In section 3, we study the convergence of gradient methods under heavy-tailed noise condition where SGD’s performance degrades and its convergence might fail. We then establish (with upper and lower bounds) the convergence of clipped gradient methods under the same condition and prove that they obtain theoretically optimal rates.
• Though clipping speeds up SGD, it does not close the gap between SGD and ADAM. In section 4, we motivated the a novel adaptive-threshold coordinate-wise clipping algorithm and in section 5 experimentally show that it outperforms Adam on BERT training tasks. 1.1