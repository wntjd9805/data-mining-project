Abstract
We study the problem of reducing adversarially robust learning to standard PAC learning, i.e. the complexity of learning adversarially robust predictors using access to only a black-box non-robust learner. We give a reduction that can robustly learn any hypothesis class C using any non-robust learner A for C. The number of calls to A depends logarithmically on the number of allowed adversarial perturbations per example, and we give a lower bound showing this is unavoidable. 1

Introduction
We consider the problem of learning predictors that are robust to adversarial examples at test time.
That is, we would like to be robust against an adversary U : X → 2X that can perturb examples at test-time, where U(x) ⊆ X is the set of allowed corruptions the adversary might replace x with, as measured by the robust risk: (cid:34)
RU (ˆh; D) (cid:44) E (x,y)∼D sup z∈U (x) (cid:35) 1[ˆh(z) (cid:54)= y]
. (1)
For example, U could be perturbations of bounded (cid:96)p-norms [Goodfellow et al., 2015].
We ask whether we can adversarialy robustly learn a given target hypothesis class C ⊆ Y X (e.g. neural networks)—that is, whether, if there exists a predictor in C with zero robust risk w.r.t. some unknown distribution D over X × Y, can we ﬁnd a predictor with (arbitrarily small) robust risk using m i.i.d. (uncorrupted) samples S = {(xi, yi)}m i=1 from D. Recently, Montasser et al. [2019] showed that if C is PAC-learnable non-robustly, then C is also adversarially robustly learnable. However, their result is not constructive and the robust learning algorithm given is inefﬁcient, complex, and does not actually directly use a non-robust learner. In this paper, we ask a more constructive version of this question:
Can we learn adversarially robust predictors given only black-box access to a non-robust learner?
That is, we are asking whether it is possible to reduce adversarially robust learning to standard non-robust learning. Since we have a plethora of algorithms devised for standard non-robust learning, it would be useful if we could design efﬁcient reduction algorithms that leverage such non-robust learning algorithms in a black-box manner to learn robustly. That is, design generic wrapper methods that take as input a learning algorithm A and a speciﬁcation of the adversary U, and robustly learn by calling A. Many systems in practice perform standard learning but with no robustness guarantees, and therefore, it would be beneﬁcial to provide wrapper procedures that can guarantee adversarial robustness in a black-box manner without needing to modify current learning systems internally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.