Abstract
Deep Generative Networks (DGNs) with probabilistic modeling of their output and latent space are currently trained via Variational Autoencoders (VAEs). In the absence of a known analytical form for the posterior and likelihood expectation,
VAEs resort to approximations, including (Amortized) Variational Inference (AVI) and Monte-Carlo sampling. We exploit the Continuous Piecewise Afﬁne property of modern DGNs to derive their posterior and marginal distributions as well as the latter’s ﬁrst two moments. These ﬁndings enable us to derive an analytical
Expectation-Maximization (EM) algorithm for gradient-free DGN learning. We demonstrate empirically that EM training of DGNs produces greater likelihood than
VAE training. Our new framework will guide the design of new VAE AVI that better approximates the true posterior and open new avenues to apply standard statistical tools for model comparison, anomaly detection, and missing data imputation. 1

Introduction
Deep Generative Networks (DGNs), which map a low-dimensional latent variable z to a higher-dimensional generated sample x are the state-of-the-art methods for a range of machine learning applications, including anomaly detection, data generation, likelihood estimation, and exploratory analysis across a wide variety of datasets [1–4].
Training of DGNs roughly falls into two camps: (i) By leveraging an adversarial network as in a
Generative Adversarial Network (GAN) [5] to turn the method into an adversarial game; and (ii) by modeling the latent variable and observed variables as random variables and performing some ﬂavor of likelihood maximization training. A widely used solution to likelihood based DGN training is via a Variational Autoencoder (VAE) [6]. The popularity of the VAE is due to its intuitive and interpretable loss function, which is obtained from likelihood estimation, and its ability to exploit standard estimation techniques ported from the probabilistic graphical models literature.
Yet, VAEs offer only an approximate solution for likelihood based training of DGNs. In fact, all current VAEs employ three major approximation steps in the likelihood maximization process. First, the true (unknown) posterior is approximated by a variational distribution. This estimate is governed by some free parameters that must be optimized to ﬁt the variational distribution to the true posterior.
VAEs estimate such parameters by means of an alternative network, the encoder, with the datum as input and the predicted optimal parameters as output. This step is referred to as Amortized
Variational Inference (AVI), as it removes the explicit, per datum, optimization by a single deep network (DN) pass. Second, as in any latent variable model, the complete likelihood is estimated by a lower bound (ELBO) obtained from the expectation of the likelihood taken under the posterior or variational distribution. With a DGN, this expectation is unknown, and thus VAEs estimate the
ELBO by Monte-Carlo (MC) sampling. Third, the maximization of the MC-estimated ELBO, which 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
drives the parameters of the encoder to better model the data distribution and the encoder to produce better variational parameter estimates, is performed by some ﬂavor of gradient descend (GD).
These VAE approximation steps enable rapid training and test-time inference of DGNs. However, due to the lack of analytical forms for the posterior, ELBO, and explicit (gradient free) parameter updates, it is not possible to measure the above steps’ quality or effectively improve them. Since the true posterior and expectation are unknown, current VAE research roughly fall into three camps: (i) developing new and more complex output and latent distributions [7, 8], such as the truncated distribution; (ii) improving the various estimation steps by introducing complex MC sampling with importance re-weighted sampling [9]; (iii) providing different estimates of the posterior with moment matching techniques [10, 11]. More recently, [12] exploited the special continuous piecewise afﬁne structure of current ReLU DGNs to develop an approximation of the posterior distribution based on mode estimation and DGN linearization leading to Laplacian VAEs. Nevertheless, derivation of analytical DGN distributions was not considered.
In this paper, we advance both the theory and practice of DGNs and VAEs by computing the exact analytical posterior and marginal distributions of any DGN employing continuous piecewise afﬁne (CPA) nonlinearities. The knowledge of these distributions enables us to perform exact inference without resorting to AVI or MC-sampling and to train the DGN in a gradient-free manner with guaranteed convergence.
The analytical distributions we obtain provide ﬁrst-of-their-kind insights into (i) how DGNs model the data distributions à la Mixture of Probabilistic Principal Component Analysis (MPPCA), (ii) how inference is performed and is akin Generative Latent Optimization models [13], (iii) the roles of each DGN parameter and how are they updated, and (iv) the impact of DGN architecture and regularization choice in the form of the DGN distributions and layer weights. The exact likelihood and marginal computation also enable the use of standard statistical model comparison tools such as the Akkaike Information Criterion (AIC) [14] and Bayesian Information Criterion (BIC) [15] and inspire new, more reliable anomaly detection approaches.
Access to the exact posterior also enables us to quantify the approximation error of the AVI and
MC sampling of VAEs and guide the development of VAEs by leveraging the analytical posterior to design more adapted variational distributions. In fact, current VAEs suffer from occasional training instabilities [16, 17]; we validate the empirical observation that training instabilities emerge from an inadequate variational estimation of the posterior.
We summarize our main contributions as follows:
[C1] We leverage the CPA property of current DGNs to obtain the analytical form of their conditional, marginal, and posterior distributions, which are mixtures of truncated Gaussians and relate DGN density modeling to MPPCA and MFA (Sec. 3.1). We develop new algorithms and methods to compute the DGN latent space partition, per-region afﬁne mappings, and per-region Gaussian integration (Sec. 3.2).
[C2] We leverage the analytical form of a DGN’s posterior distribution to obtain its ﬁrst two moments.
We then leverage these moments to obtain the analytical expectation of the complete likelihood with respect to the DGN posterior (E-step), which enables encoder-free EM training with guaranteed convergence (Sec. 4.1). We also derive the analytical M-step, which enables for the ﬁrst time guaranteed and rapid gradient-free learning of DGNs (Sec. 4.2). The analytical E-step allows us to interpret how the expected latent representation of an input is formed, while the M-step demonstrates how information is propagated through the layers akin to the backpropagation encountered in gradient descent.
[C3] We compare our exact E-step to standard VAE training to demonstrate that the VAE inference step is to blame for unstable training. We also demonstrate that EM-based DGN training provides much faster and more stable convergence (Sec. 4.3) and suggest new directions to leverage the analytical distributions to improve VAE models.
Reproducible code for all experiments and ﬁgures are available on Github at https://github.com/
RandallBalestriero/EMDGN.git. The proofs of all results are provided in the Supplementary
Material. 2
2