Abstract
Meta-learning methods have shown an impressive ability to train models that rapidly learn new tasks. However, these methods only aim to perform well in expectation over tasks coming from some particular distribution that is typically equivalent across meta-training and meta-testing, rather than considering worst-case task performance. In this work we introduce the notion of “task-robustness” by reformulating the popular Model-Agnostic Meta-Learning (MAML) objective [12] such that the goal is to minimize the maximum loss over the observed meta-training tasks. The solution to this novel formulation is task-robust in the sense that it places equal importance on even the most difﬁcult and/or rare tasks. This also means that it performs well over all distributions of the observed tasks, making it robust to shifts in the task distribution between meta-training and meta-testing.
We present an algorithm to solve the proposed min-max problem, and show that it converges to an (cid:15)-accurate point at the optimal rate of O(1/(cid:15)2) in the convex setting and to an ((cid:15), δ)-stationary point at the rate of O(max{1/(cid:15)5, 1/δ5}) in nonconvex settings. We also provide an upper bound on the new task generalization error that captures the advantage of minimizing the worst-case task loss, and demonstrate this advantage in sinusoid regression and image classiﬁcation experiments. 1

Introduction
Despite continual advances in computational power and data collection, many scenarios remain in which machine learning models must rapidly adapt to previously unseen tasks. Motivated by such scenarios, meta-learning techniques aim to learn how to learn quickly from few samples by leveraging knowledge acquired while learning prior tasks [4, 35]. The recent successes of these techniques in areas such as few-shot learning [12, 31, 33, 37] and reinforcement learning [8, 34, 38] have sparked tremendous interest in meta-learning.
Following the setting introduced in [3], most ofﬂine meta-learning methods try to minimize the expected loss on new tasks drawn from the same, but unknown, distribution as a ﬁnite set of meta-training tasks. For example, in gradient-based meta-learning, the learning method is typically a small number of stochastic gradient descent (SGD) steps, and the means to learn quickly is having a favorable initialization. Standard methods thus try to ﬁnd an initialization that enables the model
ﬁne-tuned via task-speciﬁc SGD to perform well in expectation over new tasks. Since they assume 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the new tasks are drawn from the same unknown distribution as the meta-training tasks, during meta-training they attempt to minimize the average empirical loss after one step of SGD [12, 26].
However, by minimizing the average loss, such methods may perform arbitrarily poorly on difﬁcult and/or rare meta-training tasks. In many cases, a model that performs well across all tasks is desired, even the most difﬁcult and rare tasks. Consider for example applications in which safety is critical, such as object detection in self-driving cars, in which failing to detect rarely seen objects may result in driving accidents. In this and similar settings, the failure of the system to produce accurate results for the worst-case task could possibly cause severe issues. Moreover, existing methods’ disregard for worst-case performance relies on the often unrealistic assumption that the meta-test tasks are drawn from the same distribution as the meta-training tasks. If the meta-training dataset overestimates the prevalence of certain types of tasks in the meta-test distribution, existing methods will overﬁt to the popular tasks and fail to generalize to new tasks in both expectation and in the worst case. Indeed, existing generalization bounds for gradient-based meta-learning strategies depend on the similarity of the meta-test tasks to the meta-training solution [41, 2], rather than exploiting the diversity of the meta-training tasks to show generalization to a broad range of new tasks. To address these issues, we propose a novel meta-learning formulation that calls for minimizing the maximum as opposed to average task loss during meta-training. Our contributions are threefold:
• We modify the standard gradient-based meta-learning framework, Model-Agnostic Meta-Learning (MAML) [12], to ﬁnd an initialization that minimizes the loss after one SGD step for the worst-case task, where tasks are broadly deﬁned as distributions over few-shot learning problems. Our new formulation, Task-Robust MAML (TR-MAML), thus yields a
"task-robust" solution, in the sense that it prioritizes performance equally on all observed tasks, including the hardest and rarest ones. Importantly, this means it is also robust to all shifts in distribution over the sampled tasks from meta-training to meta-testing.
• We present an algorithm to solve our min-max formulation and prove that it convergences efﬁciently in both convex and nonconvex settings. In the convex case, it achieves the optimal rate of O((cid:15)−2) stochastic gradient evaluations, and in the nonconvex case, it reaches an ((cid:15), δ)-stationary point at a rate of O(max{(cid:15)−5, δ−5}) stochastic gradient evaluations.
• We capture the generality of our formulation’s task robustness by giving a Rademacher complexity bound on the generalization error of any new task within the convex hull of the meta-training tasks, as well as showing improved performance in few-shot sinusoid regression and image classiﬁcation experiments compared to MAML.