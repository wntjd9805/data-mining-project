Abstract
Koopman operator theory, a powerful framework for discovering the underlying dynamics of nonlinear dynamical systems, was recently shown to be intimately connected with neural network training. In this work, we take the ﬁrst steps in making use of this connection. As Koopman operator theory is a linear theory, a successful implementation of it in evolving network weights and biases offers the promise of accelerated training, especially in the context of deep networks, where optimization is inherently a non-convex problem. We show that Koopman operator theoretic methods allow for accurate predictions of weights and biases of feedforward, fully connected deep networks over a non-trivial range of training time. During this window, we ﬁnd that our approach is >10x faster than various gradient descent based methods (e.g. Adam, Adadelta, Adagrad), in line with our complexity analysis. We end by highlighting open questions in this exciting intersection between dynamical systems and neural network theory, and additional methods by which our results may be generalized. 1

Introduction
Despite their black box nature, the training of artiﬁcial neural networks (NNs) is a discrete dynamical system. During training, weights/biases evolve along a trajectory in an abstract weight/bias space, the path determined by the implemented optimization algorithm, the training data, and the architecture.
This dynamical systems picture is familiar: many introductions to optimization algorithms, such as gradient descent (GD), visualize training as a process where weights/biases change iteratively under the inﬂuence of the loss landscape. Yet, while dynamical systems theory has provided insight into the behavior of many complex systems, its application to NNs has been limited.
Recent advances in Koopman operator theory (KOT) have made it a powerful tool in studying the underlying dynamics of nonlinear systems in a data-driven manner [1–8]. This begs the question, can
KOT be used to learn and predict the dynamics present in NN training? If so, can such an approach, which we call Koopman training, afford us beneﬁts that traditional NN training approaches, like backpropagation, cannot? This viewpoint was recently proposed in work by one of the authors, which demonstrated that KOT and NN training are intimately connected [9]. While signiﬁcant effort has been dedicated to using NNs to discover important features of KOT [10–12], this work was, to our knowledge, the ﬁrst application of KOT for the training of NNs (see Sec. 1.1).
An appealing aspect of KOT is that it is a linear theory for nonlinear dynamical systems. In our case, this means that evolving NN weights/biases using Koopman operators involves only matrix
∗Authors contributed equally 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
computations. We show, under certain mild assumptions, how these computations can be considerably cheaper than standard NN training methods. Thus, we show how successful application of KOT to
NNs could lead to signiﬁcantly accelerated training techniques. We demonstrate this potential in the context of feedforward, fully connected NNs. We emphasize that our methods are applicable to a wide range of NNs beyond the speciﬁc ones we study - indeed they should generalize well to a broad variety of systems that make use of iterative optimization methods.
This work is structured as follows. We begin with a brief introduction to KOT. We describe and deepen the existing argument that NN training can be viewed as a process that evolves weights/biases through the action of a Koopman operator. We then discuss different implementations of Koopman training, introducing a novel approach of partitioning the Koopman operator approximation. This idea is crucial for ensuring that the run time complexity of Koopman training is comparable or better than that of the standard training methods. We then present the results of Koopman training two different feedforward, fully connected, deep NNs: an NN differential equation (DE) solver and a classiﬁer trained on the MNIST data set. These NNs show signiﬁcant variation in architecture sizes, objectives, optimizers, learning rate, etc, lending credence to our assertions that Koopman training is a versatile and powerful technique. Our basic Koopman training implementations successfully predict weight/bias evolutions over a non-trivial number of training steps, with computational costs one to two orders of magnitude smaller than the standard methods we compared to. We end by discussing future problems of interest in Koopman training and the potential more advanced KOT methods offer in extending our results. 1.1