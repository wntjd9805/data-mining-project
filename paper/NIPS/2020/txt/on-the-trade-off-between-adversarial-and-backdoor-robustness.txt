Abstract
Deep neural networks are shown to be susceptible to both adversarial attacks and backdoor attacks. Although many defenses against an individual type of the above attacks have been proposed, the interactions between the vulnerabilities of a network to both types of attacks have not been carefully investigated yet. In this paper, we conduct experiments to study whether adversarial robustness and backdoor robustness can affect each other and ﬁnd a trade-off—by increasing the robustness of a network to adversarial examples, the network becomes more vulnerable to backdoor attacks. We then investigate the cause and show how such a trade-off can be exploited for either good or bad purposes. Our ﬁndings suggest that future research on defense should take both adversarial and backdoor attacks into account when designing algorithms or robustness measures to avoid pitfalls and a false sense of security. 1

Introduction
Deep neural networks (DNNs) have achieved impressive performance in many domains such as computer vision, natural language processing, speech, and robotics, etc. However, DNNs are shown to be susceptible to both adversarial attacks [14, 34] and backdoor attacks [7, 16, 25]. Adversarial attacks aim at fooling a model using examples (which are called adversarial examples) that are nearly indistinguishable from regular examples in human eyes or some distance measures in the input space.
An adversarial example can be generated by slightly perturbing the input of a regular example in directions where the output of the model gives the highest loss. On the other hand, backdoor attacks aim at fooling the model with pre-mediated inputs. An attacker can “poison” training data by adding crafted triggers in some data points of a speciﬁc label. So, a model trained with poisoned data will perform well on a benign test set but behaves wrongly when the triggers are present in test data.
The vulnerabilities of DNNs to these attacks raise concern about the robustness of security-critical machine learning applications, such as autonomous cars and speech recognition authorization.
Many defenses against adversarial or backdoor attacks have been proposed. In particular, the certiﬁed robustness [2,3,8,10–13,15,21,28,35,40–43] and adversarial training [14,17,19,20,24,26,29,32,44] are theoretically grounded and empirically strong methods, respectively, for defending adversarial attacks. To defend backdoor attacks, efforts have been made to detect and remove poisoned data (before training) [6, 36] or to ﬁne-tune the model (after training) to unlearn backdoors [30, 39].
However, most existing defense methods are designed for one type of attacks only. The interactions between the vulnerabilities of a network to adversarial and backdoor attacks have not been carefully investigated yet. In practice, a model may be trained using the data collected from the public. It may also be deployed in an open environment where the input at runtime is accessible to the third party. As attackers could manipulate both training and testing data, it is crucial to understand how the interactions, if existing, will impact the current defenses. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we conduct experiments to study whether the adversarial and backdoor robustness has an inﬂuence on each other. The answer is yes as we ﬁnd a trade-off—by increasing the robustness of a network to adversarial examples via adversarial training, the network becomes more vulnerable to backdoor attacks. This ﬁnding is consistent on all the real-world datasets, including MNIST [23],
CIFAR-10 [22], and ImageNet [9], and across all the settings we have tested. The trade-off delivers an important message: studying and defending one type of attacks at a time is dangerous because it may lead to a false sense of security. To elaborate this, we further show that new, subtle backdoor attacks can be created by exploiting the trade-off and that some well-known backdoor defenses [6, 36] are not applicable to an adversarially robust model.
The trade-off is not entirely detrimental to existing defenses. We found that it conversely enhances a classes of backdoor defenses [30, 39] that let a model unlearn backdoors after training. The following summarizes our contributions:
•
•
•
•
We ﬁnd that the adversarial robustness of a DNN is at odds with the backdoor robustness.
We show, by conducting extensive experiments, that such a trade-off holds across various settings, including attack/defense methods, model architectures, datasets, etc.
We investigate the reasons behind the trade-off by visualizing what is learned by the network.
We demonstrate how an adversary can exploit the trade-off to create more concealed backdoor attacks and to make some existing backdoor defenses infeasible. Conversely, the trade-off also strengthens some other defenses.
Our ﬁndings have implications for both existing and future research. In particular, they give a guide on how to combine existing adversarial and backdoor defenses to achieve adversarial and backdoor robustness simultaneously. In addition, they open a door for joint adversarial and backdoor attack/defense in the future. 2