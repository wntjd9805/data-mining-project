Abstract
Semantic cues and statistical regularities in real-world environment layouts can improve efﬁciency for navigation in novel environments. This paper learns and leverages such semantic cues for navigating to objects of interest in novel environ-ments, by simply watching YouTube videos. This is challenging because YouTube videos don’t come with labels for actions or goals, and may not even showcase op-timal behavior. Our method tackles these challenges through the use of Q-learning on pseudo-labeled transition quadruples (image, action, next image, reward). We show that such off-policy Q-learning from passive data is able to learn meaningful semantic cues for navigation. These cues, when used in a hierarchical navigation policy, lead to improved efﬁciency at the ObjectGoal task in visually realistic simulations. We observe a relative improvement of 15 − 83% over end-to-end RL, behavior cloning, and classical methods, while using minimal direct interaction. 1

Introduction
Consider the task of ﬁnding your way to the bathroom while at a new restaurant. As humans, we can efﬁciently solve such tasks in novel environments in a zero-shot manner. We leverage common sense patterns in the layout of environments, which we have built from our past experience of similar environments. For ﬁnding a bathroom, such cues will be that they are typically towards the back of the restaurant, away from the main seating area, behind a corner, and might have signs pointing to their locations (see Figure 1). Building computational systems that can similarly leverage such semantic regularities for navigation has been a long-standing goal.
Hand-specifying what these semantic cues are, and how they should be used by a navigation policy is challenging. Thus, the dominant paradigm is to directly learn what these cues are, and how to use them for navigation tasks, in an end-to-end manner via reinforcement learning. While this is a promising approach to this problem, it is sample inefﬁcient, and requires many million interaction samples with dense reward signals to learn reasonable policies.
But, is this the most direct and efﬁcient way of learning about such semantic cues? At the end of the day, these semantic cues are just based upon spatial consistency in co-occurrence of visual patterns next to one another. That is, if there is always a bathroom around the corner towards the back of the restaurant, then we can learn to ﬁnd this bathroom, by simply ﬁnding corners towards the back of the restaurant. This observation motivates our work, where we pursue an alternate paradigm to learn semantic cues for navigation: learning about this spatial co-occurrence in indoor environments through video tours of indoor spaces. People upload such videos to YouTube (see project video) to showcase real estate for renting and selling. We develop techniques that leverage such YouTube videos to learn semantic cues for effective navigation to semantic targets in indoor home environments (such as ﬁnding a bed or a toilet).
Project website with code, models, and videos: https://matthewchang.github.io/value-learning-from-videos/. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Semantic Cues for Navigation. Even though you don’t see a restroom, or a sign pointing to one in either of these images, going straight ahead in the left image is more likely to lead to a restroom than going straight in the right image. This paper seeks to learn and levarage such semantic cues for ﬁnding objects in novel environments, by watching egocentric YouTube videos.
Such use of videos presents three unique and novel challenges, that don’t arise in standard learning from demonstration. Unlike robotic demonstrations, videos on the Internet don’t come with any action labels. This precludes learning from demonstration or imitation learning. Furthermore, goals and intents depicted in videos are not known, i.e., we don’t apriori know what each trajectory is a demonstration for. Even if we were to label this somehow, the depicted trajectories may not be optimal, a critical assumption in learning from demonstration [53] or inverse reinforcement learning [43].
Our formulation, Value Learning from Videos or VLV, tackles these problems by a) using pseudo action labels obtained by running an inverse model, and b) employing Q-learning to learn from video sequences that have been pseudo-labeled with actions. We follow work from Kumar et al. [38] and use a small number of interaction samples (40K) to acquire an inverse model. This inverse model is used to pseudo-label consecutive video frames with the action the robot would have taken to induce a similar view change. This tackles the problem of missing actions. Next, we obtain goal labels by classifying video frames based on whether or not they contain the desired target objects. Such labeling can be done using off-the shelf object detectors. Use of Q-learning [64] with consecutive frames, intervening actions (from inverse model), and rewards (from object category labels), leads to learning optimal Q-functions for reaching goals [59, 64]. We take the maximum Q-value over all actions, to obtain value functions. These value functions are exactly γs, where s is the number of steps to the nearest view location of the object of interest (γ is the Q-learning discount factor). These value functions implicitly learn semantic cues. An image looking at the corner towards the back of the restaurant will have a higher value (for bathroom as the semantic target) than an image looking at the entrance of the restaurant. These learned value functions when used with a hierarchical navigation policy, efﬁciently guide locomotion controllers to desired semantic targets in the environment.
Learning from such videos can have many advantages, some of which address limitations of learning from direct interaction (such as via RL). Learning from direct interaction suffers from high sample complexity (the policy needs to discover high-reward trajectories which may be hard to ﬁnd in sparse reward scenarios) and poor generalization (limited number of instrumented physical environments available for reward-based learning, or sim2real gap). Learning from videos side-steps both these issues. We observe a 47 − 83% relative improvement in performance over RL and imitation learning methods, while also improving upon strong classical methods. 2