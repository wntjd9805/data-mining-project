Abstract
Based on a sample of size n, we consider estimating the number of symbols that appear at least µ times in an independent sample of size a · n, where a is a given parameter. This formulation includes, as a special case, the well-known problem of inferring the number of unseen species introduced by [Fisher et al.] in 1943 and considered by many others. Of considerable interest in this line of works is the largest a for which the quantity can be accurately predicted. We completely resolve this problem by determining the limit of estimation to be a ≈ (log n)/µ, with both lower and upper bounds matching up to constant factors. For the particular case of µ = 1, this implies the recent result by [Orlitsky et al.] on the unseen species problem. Experimental evaluations show that the proposed estimator performs exceptionally well in practice. Furthermore, the estimator is a linear combination of symbols’ empirical counts, and hence linear-time computable. 1

Introduction
Let ∆ denote the collection of all discrete distributions. Given an independent sample X n from an unknown distribution p ∈ ∆, we are interested in estimating the number of unseen symbols that appear at least µ times in an independent sample Y m from the same distribution p, where m is a given sampling parameter. To be more speciﬁc, we study the estimation of
Uµ (cid:44) Uµ(X n, Y m) (cid:44) (cid:88) 1Nx=0 · 1Mx≥µ, x where the summation is over all symbols x in the potentially unknown alphabet, and Nx and Mx represent the number of times symbol x appearing in X n and Y m, respectively. In addition, let
Φi (cid:44) Φi(X n) (cid:44) (cid:88) 1Nx=i x denote the number of symbols appearing i times in the sample X n. Due to symmetry, the collection of Φi’s is a sufﬁcient statistic for learning Uµ.
For the special case of µ = 1, the quantity U1 becomes the number of newly observed symbols, and the task reduces to the well-known unseen species problem, whose study dates back to more than half a century ago [11]. In the early 1940s, the British chemist and naturalist Corbet had spent two years in Malaya to trap butterﬂies (X n). Every time he saw a new species, he recorded how many individuals of that species he had trapped (Nx).
§Yi Hao is a Ph.D. candidate in the Department of Electrical and Computer Engineering at the University of
California, San Diego. Yi Hao’s work was conducted while he was a summer intern at Baidu Research-Bellevue. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
After returning to England, Corbet was curious how many new species he would discover if he went back to Malaya for another 2 years. He constructed a table to show the number of species appearing certain number of times (Φi), and then presented the problem to Fisher, Father of modern statistics, who provided a remarkable estimator [11] that was later improved on by Good and Toulmin [13],
ˆUGT (cid:44) − n (cid:88) (−1)iΦi. i=1
Before continuing the discussion, we introduce a convenient notation, a (cid:44) m n , and refer to it as the ampliﬁcation ratio (or extrapolation ratio), a quantity that describes the limit of our estimation power and frequently appears in our subsequent derivations. Henceforth we often write m as na to reﬂect our intention of inferring future statistics from past observations.
The Good-Toulmin estimator ˆUGT applies to the special case of a = µ = 1 and generalizes to
ˆUGT (cid:44) − n (cid:88) (−a)iΦi i=1 for general a values, where we kept the same notation. The paper [13] that originally introduced ˆUGT showed that as long as a ≤ 1, the estimator produces a nearly unbiased estimate with an expected (absolute) deviation of O(a n), which is negligible as U1 can potentially be an.
√
However, the success of this estimator does not extend to the regime where a > 1, both in theory and in practice. For example, an experiment in a recent work [28] addressing U1 estimation shows that even for a simple (shifted) Zipf-law with pi ∝ 1/(i + 10) for 1 ≤ i ≤ 104, and a ∈ [1.3, 1.5], estimate ˆUGT can signiﬁcantly deviate from the actual value and sometimes even becomes negative.
To address this issue, Good and Toulmin applied a smoothing technique to the alternating coefﬁcient sequence (−a)i, i = 1, . . . n, and replaced it by (−a)i · Pr(bin(k, 1/(a + 1)) ≥ i) for some properly chosen parameter k. The resulting estimator, which [28] terms as the smoothed Good-Toulmin (SGT), has found numerous applications over the past half-century. However, the statistical properties and optimality of this estimator were not well-understood until the recent work of Orlitsky et al. [28, 30].
Noting that the heuristic multiplies the coefﬁcients by the binomial tail probabilities, Orlitsky et al.
ﬁrst proposed the following general smoothing regime. Let L be an independent nonnegative random variable, and denote ˆUL (cid:44) − (cid:80)n i=1(−a)i Pr(L ≥ i)Φi. Subsequently, they showed that for L being either a (proper) binomial or Poisson random variable, the induced estimator performs optimally.
Formally and more generally, Uµ varies from 0 to mµ (cid:44) m/µ = na/µ, hence we measure the performance of any n-sample estimator ˆU by the worst-case normalized mean-square error (NMSE), n,a( ˆU ) (cid:44) max
E µ p∈∆
Ep (cid:32) ˆU − Uµ mµ (cid:33)2
.
The main result of [28] states that for some Poisson or binomially distributed L, the estimator satisﬁes n,a( ˆUL) (cid:46) 1/n1/a, where α (cid:46) β abbreviates α = O(β). On the other hand, for some absolute
E 1 n,a( ˆUL) (cid:38) 1/nc(cid:48)/a, where α (cid:38) β constants c, c(cid:48) > 0, and any a ≥ c and estimator ˆU , one has E 1 abbreviates α = Ω(β). Combined, the results established the optimality of the smoothing scheme. 1.1 New results
As described previously, the problem that we study here is natural generalization of the unseen species problem, and calls for estimating Uµ, the number of symbols that appear at least µ times in the future sample. One motivation for considering this problem is reproducibility. For example, in the aforementioned butterﬂy trapping story, a basic task one often wants to perform is checking the existence of sexual dimorphism in a newly observed butterﬂy species, meaning that the two sexes look completely different. This clearly requires inferring Uµ for µ ≥ 2.
Replacing butterﬂies by words, vocabulary size estimation [8, 10, 18, 34] aims to determine how many words a writer, say William Shakespeare, knew based on his written works. An intuitive and widely used approach is to simply add up the number of observed (distinct) words and some estimate 2
of U1. With the same motivation, we may also want to know how many words fall into a writer’s common vocabulary (excluding those that appear only once or twice), which calls for estimating Uµ.
For another example, app developers are often interested in knowing how many new users their apps will have in a future time period. In addition, they usually care more about active users who will use the apps for at least a certain number of times. Under appropriate assumptions, this again translates to a Uµ estimation problem. The same rationale applies to many other types of businesses such as advertising, catering, and entertainment, since Uµ is a natural business growth indicator.
There has been a long line of research works on estimating U1, in which of considerable interest is the largest a for which the quantity can be accurately predicted. The generalization of this “unseen species” problem, on the other hand, is a new problem that we propose and rigorously study. As the subsequent discussion shows, we completely resolved this problem by determining the limit of estimation to be a ≈ (log n)/µ, with both lower and upper bounds matching up to constant factors.
Our estimator is linear-time computable given Φi’s, and has the form
ˆUµ (cid:44) ˆUµ(X n, a) (cid:44) n (cid:88) i=1 si · Φi, where for r ∼ log n a , the smoothing rate, we denote the i-th smoothed coefﬁcient by (µ−1)∧i (cid:88) si (cid:44) − (−a)i(−1)j j=0 (cid:19) (cid:18)i j
Pr(Poi(r) ≥ i + j), where the notation α ∧ β abbreviates min{α, β}. In particular, if we set µ = 1, the estimator reduces to ˆUL in [28] with L ∼ Poi(r). Interestingly, if we remove the smoothing probability, the estimator becomes the vanilla Good-Toulmin estimator ˆUGT when µ = 1, and becomes the following
“generalized Good-Toulmin” estimator (denoted by ˆUGGT) when µ ≥ 1:
ˆUGGT (cid:44) (−1)µ n (cid:88) i=µ (−a)i (cid:18) i − 1
µ − 1 (cid:19)
Φi.
The deviation of ˆUGGT can be found in Appendix J.
The following two results essentially establish the optimality of ˆUµ and determine the min-max learning risk of approximating Uµ. In particular, these theorems imply the main result of [28] as a special case. The ﬁrst theorem bounds the worst-case NMSE of our estimator.
Theorem 1. There exist absolute constants c and c0 such that for any parameter a ∈ [1, (c log n)/µ], the estimator ˆUµ described above satisﬁes n,a( ˆUµ) (cid:46) 1
E µ nc0/a
.
The second theorem lowerly bounds the worst-case NMSE of the best estimator.
Theorem 2. There exist absolute constants c(cid:48) and c(cid:48) and any n-sample estimator ˆU , 0 such that for any a ≥ max{1, (c(cid:48) log n)/µ}, n,a( ˆU ) (cid:38) 1
E µ nc(cid:48) 0/a
. 1.2