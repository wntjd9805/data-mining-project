Abstract
Deep multimodal fusion by using multiple sources of data for classiﬁcation or regression has exhibited a clear advantage over the unimodal counterpart on various applications. Yet, current methods including aggregation-based and alignment-based fusion are still inadequate in balancing the trade-off between inter-modal fusion and intra-modal processing, incurring a bottleneck of performance improve-ment. To this end, this paper proposes Channel-Exchanging-Network (CEN), a parameter-free multimodal fusion framework that dynamically exchanges channels between sub-networks of different modalities. Speciﬁcally, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of Batch-Normalization (BN) scaling factor during training. The valid-ity of such exchanging process is also guaranteed by sharing convolutional ﬁlters yet keeping separate BN layers across modalities, which, as an add-on beneﬁt, allows our multimodal architecture to be almost as compact as a unimodal network.
Extensive experiments on semantic segmentation via RGB-D data and image trans-lation through multi-domain input verify the effectiveness of our CEN compared to current state-of-the-art methods. Detailed ablation studies have also been carried out, which provably afﬁrm the advantage of each component we propose. Our code is available at https://github.com/yikaiw/CEN. 1

Introduction
Encouraged by the growing availability of low-cost sensors, multimodal fusion that takes advantage of data obtained from different sources/structures for classiﬁcation or regression has become a central problem in machine learning [4]. Joining the success of deep learning, multimodal fusion is recently speciﬁed as deep multimodal fusion by introducing end-to-end neural integration of multiple modalities [37], and it has exhibited remarkable beneﬁts against the unimodal paradigm in semantic segmentation [28, 44], action recognition [13, 14, 43], visual question answering [1, 22], and many others [3, 25, 51].
A variety of works have been done towards deep multimodal fusion [37]. Regarding the type of how they fuse, existing methods are generally categorized into aggregation-based fusion, alignment-based fusion, and the mixture of them [4]. The aggregation-based methods employ a certain operation (e.g. averaging [18], concatenation [34, 50], and self-attention [44]) to combine multimodal sub-networks into a single network. The alignment-based fusion [9, 43, 46], instead, adopts a regulation loss to align the embedding of all sub-networks while keeping full propagation for each of them. The difference between such two mechanisms is depicted in Figure 1. Another categorization of multimodal fusion can be speciﬁed as early, middle, and late fusion, depending on when to fuse, which have been discussed in earlier works [2, 7, 17, 41] and also in the deep learning literature [4, 26, 27, 45].
†Corresponding author: Fuchun Sun. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: A sketched comparison between existing fusion methods and ours.
Albeit the fruitful progress, it remains a great challenge on how to integrate the common informa-tion across modalities, meanwhile preserving the speciﬁc patterns of each one. In particular, the aggregation-based fusion is prone to underestimating the intra-modal propagation once the multi-modal sub-networks have been aggregated. On the contrary, the alignment-based fusion maintains the intra-modal propagation, but it always delivers ineffective inter-modal fusion owing to the weak message exchanging by solely training the alignment loss. To balance between inter-modal fusion and intra-modal processing, current methods usually resort to careful hierarchical combination of the aggregation and alignment fusion for enhanced performance, at a cost of extra computation and engineering overhead [11, 28, 50].
Present Work. We propose Channel-Exchanging-Network (CEN) which is parameter-free, adaptive, and effective. Instead of using aggregation or alignment as before, CEN dynamically exchanges the channels between sub-networks for fusion (see Figure 1(c)). The core of CEN lies in its smaller-norm-less-informative assumption inspired from network pruning [32, 48]. To be speciﬁc, we utilize the scaling factor (i.e. γ) of Batch-Normalization (BN) [23] as the importance measurement of each corresponding channel, and replace the channels associated with close-to-zero factors of each modality with the mean of other modalities. Such message exchanging is parameter-free and self-adaptive, as it is dynamically controlled by the scaling factors that are determined by the training itself. Besides, we only allow directed channel exchanging within a certain range of channels in each modality to preserve intra-modal processing. More details are provided in § 3.3. Necessary theories on the validity of our idea are also presented in § 3.5.
Another hallmark of CEN is that the parameters except BN layers of all sub-networks are shared with each other (§ 3.4). Although this idea is previously studied in [8, 47], we apply it here to serve speciﬁc purposes in CEN: by using private BNs, as already discussed above, we can determine the channel importance for each individual modality; by sharing convolutional ﬁlters, the corresponding channels among different modalities are embedded with the same mapping, thus more capable of modeling the modality-common statistic. This design further compacts the multimodal architecture to be almost as small as the unimodal one.
We evaluate our CEN on two studies: semantic segmentation via RGB-D data [40, 42] and image translation through multi-domain input [49]. It demonstrates that CEN yields remarkably superior performance than various kinds of fusion methods based on aggregation or alignment under a fair condition of comparison. In terms of semantic segmentation particularly, our CEN signiﬁcantly outperforms state-of-the-art methods on two popular benchmarks. We also conduct ablation studies to isolate the beneﬁt of each proposed component. More speciﬁcations are provided in § 4. 2