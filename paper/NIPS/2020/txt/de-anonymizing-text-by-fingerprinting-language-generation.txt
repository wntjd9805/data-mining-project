Abstract
Components of machine learning systems are not (yet) perceived as security hotspots. Secure coding practices, such as ensuring that no execution paths de-pend on conﬁdential inputs, have not yet been adopted by ML developers. We initiate the study of code security of ML systems by investigating how nucleus sampling—a popular approach for generating text, used for applications such as auto-completion—unwittingly leaks texts typed by users. Our main result is that the series of nucleus sizes for many natural English word sequences is a unique
ﬁngerprint. We then show how an attacker can infer typed text by measuring these
ﬁngerprints via a suitable side channel (e.g., cache access times), explain how this attack could help de-anonymize anonymous texts, and discuss defenses. 1

Introduction
Machine learning (ML) models are composed from building blocks such as layer types, loss functions, sampling methods, etc. Each building block typically has a few popular library implementations, which are incorporated into many models—including models whose inputs are sensitive (e.g., private images or typed text). Therefore, ML models are “security hotspots” and their implementations must follow secure coding practices. This includes protecting the inputs from side channels, i.e., low-level physical or microarchitectural side effects of the computation that are externally observable and leak information about its internal state to concurrent, adversarial processes.
We use nucleus sampling [19], a leading approach for efﬁciently generating high-ﬁdelity text, as a case study of side-channel vulnerabilities in ML models. Given the output probabilities of a language model such as GPT-2 [35], nucleus sampling draws candidates from a variable-sized “nucleus” of the most probable words. It is the basis of applications such as text auto-completion [24, 44].
First, we demonstrate that the series of nucleus sizes produced when generating an English-language word sequence is a ﬁngerprint by showing that the nucleus size series of any sequence satisfying a simple criterion is far from any other sequence, unless their textual contents substantially overlap. We then derive a lower bound on the Euclidean distance between ﬁngerprints that depends only on the sequence length but not on the size or domain of the corpus.
Second, we show that implementations of nucleus sampling, such as the popular Hugging Face
Transformers package, contain a dangerous information leak. An attacker who runs a concurrent, sandboxed application process on the user’s device can infer the nucleus size by indirectly measure the number of iterations of a certain loop, and thus ﬁngerprint the input text. We use Flush+Reload [47] for our proof of concept, but the general approach works with any suitable side channel [17, 28, 32].
We design a ﬁngerprint matching algorithm and show that (1) it tolerates noise in side-channel measurements, and (2) does not produce false positives. Therefore, an attacker can accurately identify the typed sequence out of many billions of possible candidates in an “open-world” setting, without assuming a priori that the user’s input belongs to a known small dataset. This technique can help 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
de-anonymize text by asynchronously matching ﬁngerprints collected from the user’s device to anonymous blog entries, forum posts, emails, etc. For example, we show that many of the anonymous users’ posts on the infamous Silk Road forum have unique ﬁngerprints.
We conclude by explaining how to mitigate the information leak and discuss the importance of removing insecure coding patterns such as input-dependent loops from ML building blocks.
Ethics and responsible disclosure. The purpose of this study is to improve the security of popular
ML systems and help protect the privacy of their users. We disclosed our ﬁndings and our proposed mitigation code by email to members of the Hugging Face engineering team responsible for the implementation of nucleus sampling (identiﬁed via a contact at Hugging Face and GitHub’s commit log) and a message to Hugging Face’s public Facebook contact point.
We use Silk Road posts as a case study only because they represent informal textual communications whose authors likely wish to maintain their anonymity. Silk Road posts include offensive and harmful content. We use this dataset solely for our proof-of-concept experiments. It does not reﬂect our views in any way. 2