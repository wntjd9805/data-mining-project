Abstract
The two dominant approaches to neural text generation are fully autoregressive models, using serial beam search decoding, and non-autoregressive models, using parallel decoding with no output dependencies. This work proposes an autore-gressive model with sub-linear parallel time generation. Noting that conditional random ﬁelds with bounded context can be decoded in parallel, we propose an efﬁcient cascaded decoding approach for generating high-quality output. To param-eterize this cascade, we introduce a Markov transformer, a variant of the popular fully autoregressive model that allows us to simultaneously decode with speciﬁc autoregressive context cutoffs. This approach requires only a small modiﬁcation from standard autoregressive training, while showing competitive accuracy/speed tradeoff compared to existing methods on ﬁve machine translation datasets. 1

Introduction
Probabilistic text generation is a ubiquitous tool in natural language processing. Originally primarily studied with respect to machine translation [1, 27], its progress has led to applications in document summarization [40, 45], data-to-text [60], image captioning [61], etc. State-of-the-art text generation approaches rely on fully autoregressive models such as RNNs and transformers [53], in which the probability of an output word depends on all previous words. At inference time, beam search is used for decoding, a left-to-right serial procedure. To speed up decoding, researchers have proposed alternative parallel generation models. One class of non-autoregressive probabilistic models assumes that each word’s output probability is independent of other words [13, 67, 28]. While it is impressive that these models perform well, this independence assumption is very strong and often results in noticeable artifacts such as repetitions [13, 51].
We note that non-autoregressive models, while sufﬁcient, are not necessary for fast probabilistic parallel generation. On parallel hardware, inference in models with bounded Markov dependencies is trivial to parallelize and requires sub-linear time w.r.t. sequence length [43, 39]. In practice, given the right parameterization, we can explore any level of autoregressive dependencies to achieve a speed/accuracy tradeoff.
In this work, we exploit this property by proposing cascaded decoding with a Markov transformer architecture. Our approach centers around a graphical model representation of the output space of text generation. Given this model, we can employ cascaded decoding [7, 8, 58, 41] for parallel text generation, using an iterative procedure that starts from a non-autoregressive model and introduces increasingly higher-order dependencies. We combine this approach with a Markov transformer, an extension to the fully autoregressive transformer architecture. This network uses barriers during training to ensure it learns ﬁxed high-order dependencies. At test time, a single network can be used to parameterize a cascade of different graphical models. The Markov transformer only changes self-attention masks and inputs at training, and is applicable to all transformer variants. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Experiments on ﬁve machine translation datasets compare this approach to other beam search and non-autoregressive baselines. Our inference approach is comparably fast to non-autoregressive methods while allowing for local dependencies in a principled, probabilistic way. Results validate the competi-tive accuracy/speed tradeoff of our approach compared to existing methods. The code for reproducing all results is available at https://github.com/harvardnlp/cascaded-generation. 2