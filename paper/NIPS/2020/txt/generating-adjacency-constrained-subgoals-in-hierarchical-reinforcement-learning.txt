Abstract
Goal-conditioned hierarchical reinforcement learning (HRL) is a promising ap-proach for scaling up reinforcement learning (RL) techniques. However, it often suffers from training inefﬁciency as the action space of the high-level, i.e., the goal space, is often large. Searching in a large goal space poses difﬁculties for both high-level subgoal generation and low-level policy learning. In this paper, we show that this problem can be effectively alleviated by restricting the high-level action space from the whole goal space to a k-step adjacent region of the current state using an adjacency constraint. We theoretically prove that the proposed adjacency constraint preserves the optimal hierarchical policy in deterministic MDPs, and show that this constraint can be practically implemented by training an adjacency network that can discriminate between adjacent and non-adjacent subgoals. Experi-mental results on discrete and continuous control tasks show that incorporating the adjacency constraint improves the performance of state-of-the-art HRL approaches in both deterministic and stochastic environments.1 1

Introduction
Hierarchical reinforcement learning (HRL) has shown great potentials in scaling up reinforcement learning (RL) methods to tackle large, temporally extended problems with long-term credit assignment and sparse rewards [39, 31, 2]. As one of the prevailing HRL paradigms, goal-conditioned HRL framework [5, 37, 20, 42, 26, 22], which comprises a high-level policy that breaks the original task into a series of subgoals and a low-level policy that aims to reach those subgoals, has recently achieved signiﬁcant success. However, the effectiveness of goal-conditioned HRL relies on the acquisition of effective and semantically meaningful subgoals, which still remains a key challenge.
As the subgoals can be interpreted as high-level actions, it is feasible to directly train the high-level policy to generate subgoals using external rewards as supervision, which has been widely adopted in previous research [26, 25, 22, 20, 42]. Although these methods require little task-speciﬁc design, they often suffer from training inefﬁciency. This is because the action space of the high-level, i.e., the
∗Equal contribution.
†Corresponding authors: Xiaolin Hu and Feng Chen. 1Code is available at https://github.com/trzhang0116/HRAC. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: High-level illustration of our method: distant subgoals g1, g2, g3 (blue) can be surrogated by closer sub-goals ˜g1, ˜g2, ˜g3 (yellow) that fall into the k-step adjacent regions.
Figure 2: Comparison between shortest tran-sition distance dst and
Euclidean distance d in a toy environment.
Figure 3: The goal-conditioned
HRL framework and the k-step adjacency constraint implement-ed by the adjacency network ψφ (dashed orange box). goal space, is often as large as the state space. The high-level exploration in such a large action space results in inefﬁcient learning. As a consequence, the low-level training also suffers as the agent tries to reach every possible subgoal produced by the high-level policy.
One effective way for handling large action spaces is action space reduction or action elimination.
However, it is difﬁcult to perform action space reduction in general scenarios without additional information, since a restricted action set may not be expressive enough to form the optimal policy.
There has been limited literature [44, 41, 19] studying action space reduction in RL, and to our knowledge, there is no prior work studying action space reduction in HRL, since the information loss in the goal space can lead to severe performance degradation [25].
In this paper, we present an optimality-preserving high-level action space reduction method for goal-conditioned HRL. Concretely, we show that the high-level action space can be restricted from the whole goal space to a k-step adjacent region centered at the current state. Our main intuition is depicted in Figure 1: distant subgoals can be substituted by closer subgoals, as long as they drive the low-level to move towards the same “direction”. Therefore, given the current state s and the subgoal generation frequency k, the high-level only needs to explore in a subset of subgoals covering states that the low-level can possibly reach within k steps. By reducing the action space of the high-level, the learning efﬁciency of both the high-level and the low-level can be improved: for the high-level, a considerably smaller action space relieves the burden of exploration and value function approximation; for the low-level, adjacent subgoals provide a stronger learning signal as the agent can be intrinsically rewarded with a higher frequency for reaching these subgoals. Formally, we introduce a k-step adjacency constraint for high-level action space reduction, and theoretically prove that the proposed constraint preserves the optimal hierarchical policy in deterministic MDPs. Also, to practically implement the constraint, we propose to train an adjacency network so that the k-step adjacency between all states and subgoals can be succinctly derived.
We benchmark our method on various tasks, including discrete control and planning tasks on grid worlds and challenging continuous control tasks based on the MuJoCo simulator [40], which have been widely used in HRL literature [26, 22, 25, 11]. Experimental results exhibit the superiority of our method on both sample efﬁciency and asymptotic performance compared with the state-of-the-art
HRL approach HIRO [26], demonstrating the effectiveness of the proposed adjacency constraint. 2 Preliminaries
We consider a ﬁnite-horizon, goal-conditioned Markov Decision Process (MDP) deﬁned as a tuple (cid:104)S, G, A, P, R, γ(cid:105), where S is a state set, G is a goal set, A is an action set, P : S × A × S → R is a state transition function, R : S × A → R is a reward function, and γ ∈ [0, 1) is a discount factor.
Following prior work [20, 42, 26], we consider a framework comprising two hierarchies: a high-level 2
(g|s) and a low-level controller with policy πl
θl controller with policy πh (a|s, g) parameterized
θh by two function approximators, e.g. neural networks with parameters θh and θl respectively, as shown in Figure 3. The high-level controller aims to maximize the external reward and generates a high-level action, i.e. a subgoal gt ∼ πh (g|st) ∈ G every k time steps when t ≡ 0 (mod k), where
θh k > 1 is a pre-determined hyper-parameter. It modulates the behavior of the low-level policy by intrinsically rewarding the low-level for reaching these subgoals. The low-level aims to maximize the intrinsic reward provided by the high-level, and performs a primary action at ∼ πl (a|st, gt) ∈ A
θl at every time step. Following prior methods [26, 1], we consider a goal space G which is a sub-space of S with a known mapping function ϕ : S → G. When t (cid:54)≡ 0 (mod k), a pre-deﬁned goal transition process gt = h(gt−1, st−1, st) is utilized. We adopt directional subgoals that represent the differences between desired states and current states [42, 26], where the goal transition function is set to h(gt−1, st−1, st) = gt−1 + st−1 − st. The reward function of the high-level policy is deﬁned as: rh kt = kt+k−1 (cid:88) i=kt
R(si, ai), t = 0, 1, 2, · · · , (1) which is the accumulation of the external reward in the time interval [kt, kt + k − 1].
While the high-level controller is motivated by the environmental reward, the low-level controller has no direct access to this external reward. Instead, the low-level is supervised by the intrinsic reward that describes subgoal-reaching performance, deﬁned as rl t = −D (gt, ϕ(st+1)), where D is a binary or continuous distance function. In practice, we employ Euclidean distance as D.
The goal-conditioned HRL framework above enables us to train high-level and low-level policies concurrently in an end-to-end fashion. However, it often suffers from training inefﬁciency due to the unconstrained subgoal generation process, as we have mentioned in Section 1. In the following section, we will introduce the k-step adjacency constraint to mitigate this issue. 3 Theoretical Analysis
In this section, we provide our theoretical results and show that the optimality can be preserved when learning a high-level policy with k-step adjacency constraint. We begin by introducing a distance measure that can decide whether a state is “close” to another state. In this regard, common distance functions such as the Euclidean distance are not suitable, as they often cannot reveal the real structure of the MDP. Therefore, we introduce shortest transition distance, which equals to the minimum number of steps required to reach a target state from a start state, as shown in Figure 2. In stochastic
MDPs, the number of steps required is not a ﬁxed number, but a distribution conditioned on a speciﬁc policy. In this case, we resort to the notion of ﬁrst hit time [43] from stochastic processes, and deﬁne the shortest transition distance by minimizing the expected ﬁrst hit time over all possible policies.
Deﬁnition 1. Let s1, s2 ∈ S. Then, the shortest transition distance from s1 to s2 is deﬁned as: dst(s1, s2) := min
π∈Π
E[Ts1s2|π] = min
π∈Π
∞ (cid:88) t=0 tP (Ts1s2 = t|π), (2) where Π is the complete policy set and Ts1s2 denotes the ﬁrst hit time from s1 to s2.
The shortest transition distance is determined by a policy that connects states s1 and s2 in the most efﬁcient way, which has also been studied by several prior work [10, 8]. This policy is optimal in the sense that it requires the minimum number of steps to reach state s2 from state s1. Compared with the dynamical distance [15], our deﬁnition here does not rely on a speciﬁc non-optimal policy. Also, we do not assume that the environment is reversible, i.e. dst(s1, s2) = dst(s2, s1) does not hold for all pairs of states. Therefore, the shortest transition distance is a quasi-metric as it does not satisfy the symmetry condition. However, this limitation does not affect the following analysis as we only need to consider the transition from the start state to the goal state without the reversed transition.
Given the deﬁnition of the shortest transition distance, we now formulate the property of an optimal (deterministic) goal-conditioned policy π∗ : S × G → A [36]. We have:
π∗(s, g) ∈ arg min a∈A (cid:88) s(cid:48)∈S
P (s(cid:48)|s, a) dst (cid:0)s(cid:48), ϕ−1(g)(cid:1) , ∀s ∈ S, g ∈ G, (3) 3
where ϕ−1 : G → S is the known inverse mapping of ϕ. We then consider the goal-conditioned HRL framework with high-level action frequency k. Different from a ﬂat goal-conditioned policy, in this setting the low-level policy is required to reach the subgoals with k limited steps. As a result, only a subset of the original states can be reliably reached even with an optimal goal-conditioned policy.
We introduce the notion of k-step adjacent region to describe the set of subgoals mapped from this reachable subset of states.
Deﬁnition 2. Let s ∈ S. Then, the k-step adjacent region of s is deﬁned as:
GA(s, k) := {g ∈ G | dst (cid:0)s, ϕ−1(g)(cid:1) ≤ k}. (4)
Harnessing the property of π∗, we can show that in deterministic MDPs, given an optimal low-level policy πl∗ = π∗, subgoals that fall in the k-step adjacent region of the current state can represent all optimal subgoals in the whole goal space in terms of the induced k-step low-level action sequence.
We summarize this ﬁnding in the following theorem.
Theorem 1. Let s ∈ S, g ∈ G and let π∗ be an optimal goal-conditioned policy. Under the assumptions that the MDP is deterministic and that the MDP states are strongly connected, for all k ∈ N+ satisfying k ≤ dst(s, ϕ−1(g)), there exists a surrogate goal ˜g such that:
˜g ∈ GA(s, k),
π∗(si, ˜g) = π∗(si, g), ∀si ∈ τ (i (cid:54)= k), where τ := (s0, s1, · · · , sk) is the k-step state trajectory starting from state s0 = s under π∗ and g. (5)
Theorem 1 suggests that the k-step low-level action sequence generated by an optimal low-level policy conditioned on a distant subgoal can be induced using a subgoal that is closer. Naturally, we can generalize this result to a two-level goal-conditioned HRL framework, where the low-level is actuated not by a single subgoal, but by a subgoal sequence produced by the high-level policy.
Theorem 2. Given the high-level action frequency k and the high-level planning horizon T , for s ∈ S, let ρ∗ = (g0, gk, · · · , g(T −1)k) be the high-level subgoal trajectory starting from state s0 = s under an optimal high-level policy πh∗. Also, let τ ∗ = (s0, sk, s2k, · · · , sT k) be the high-level state trajectory under ρ∗ and an optimal low-level policy πl∗. Then, there exists a surrogate subgoal trajectory ˜ρ∗ = (˜g0, ˜gk, · · · , ˜g(T −1)k) such that:
˜gkt ∈ GA(skt, k),
Q∗(skt, ˜gkt) = Q∗(skt, gkt), t = 0, 1, · · · , T − 1, (6) where Q∗ is the optimal high-level Q-function under policy πh∗.
Theorem 1 and 2 show that we can constrain the high-level action space to state-wise k-step adjacent regions without the loss of optimality. We formulate the high-level objective incorporating this k-step adjacency constraint as: max
θh
E
πh
θh
T −1 (cid:88) t=0
γtrh kt
, (7) subject to dst (cid:0)skt, ϕ−1(gkt)(cid:1) ≤ k, t = 0, 1, · · · , T − 1 where rh kt is the high-level reward deﬁned by Equation (1) and gkt ∼ πh
θh (g|skt).
In practice, Equation (7) is hard to optimize due to the strict constraint. Therefore, we employ relaxation methods and derive the following un-constrained optimizing objective: max
θh
E
πh
θh (cid:34)
T −1 (cid:88) t=0
γtrh kt − η · H (cid:16) dst (cid:0)skt, ϕ−1(gkt)(cid:1) , k (cid:35) (cid:17)
, (8) where H(x, k) = max(x/k − 1, 0) is a hinge loss function and η is a balancing coefﬁcient.
One limitation of our theoretical results is that the theorems are derived in the context of deterministic
MDPs. However, these theorems are instructive for practical algorithm design in general cases, and the deterministic assumption has also been exploited by some prior works that investigate distance metrics in MDPs [15, 3]. Also, we note that many real-world applications can be approximated as environments with deterministic dynamics where the stochasticity is mainly induced by noise. Hence, we may infer that the adjacency constraint could preserve a near-optimal policy when the magnitude of noise is small. Empirically, we show that our method is robust to certain types of stochasticity (see
Section 5 for details), and we leave rigorous theoretical analysis for future work. 4
4 HRL with Adjacency Constraint
Although we have formulated the adjacency constraint in Section 3, the exact calculation of the shortest transition distance dst(s1, s2) between two arbitrary states s1, s2 ∈ S remains complex and non-differentiable. In this section, we introduce a simple method to collect and aggregate the adjacency information from the environment interactions. We then train an adjacency network using the aggregated adjacency information to approximate the shortest transition distance dst(s1, s2) in a parameterized form, which enables a practical optimization of Equation (8). 4.1 Parameterized Approximation of Shortest Transition Distances
As shown in prior research [30, 10, 8, 15], accurately computing the shortest transition distance is not easy and often has the same complexity as learning an optimal low-level goal-conditioned policy. However, from the perspective of goal-conditioned HRL, we do not need a perfect shortest transition distance measure or a low-level policy that can reach any distant subgoals. Instead, only a discriminator of k-step adjacency is needed, and it is enough to learn a low-level policy that can reliably reach nearby subgoals (more accurately, subgoals that fall into the k-step adjacent region of the current state) rather than all potential subgoals in the goal space.
Given the above, here we introduce a simple approach to determine whether a subgoal satisﬁes the k-step adjacency constraint. We ﬁrst note that Equation (2) can be approximated as follows: dst(s1, s2) ≈ min
π∈{π1,π2,··· ,πn}
∞ (cid:88) t=0 tP (Ts1s2 = t|π), (9) where {π1, π2, · · · , πn} is a ﬁnite policy set containing n different deterministic policies. Obviously, if these policies are diverse enough, we can effectively approximate the shortest transition distance with a sufﬁciently large n. However, training a set of diverse policies sepa-rately is costly, and using one single policy to approximate the policy set (n = 1) [34, 35] often leads to non-optimality. To handle this dif-ﬁculty, we exploit the fact that the low-level policy itself changes over time during the training procedure. We can thus build a policy set by sampling policies that emerge in different training stages. To aggregate the adjacency information gathered by multiple policies, we propose to explicitly memorize the adjacency information by constructing a binary k-step adjacency matrix of the explored states. The adjacency matrix has the same size as the number of explored states, and each element represents whether two states are k-step adjacent. In practice, we use the agent’s trajectories, where the temporal distances between states can indicate their adjacency, to construct and update the adjacency matrix online. More details are in the supplementary material.
Figure 4: The functionality of the adjacency network.
The k-step adjacent region is mapped to an (cid:15)k-circle in the adjacency space, where egi = ψθ(gi), i = 1, 2, 3.
In practice, using an adjacency matrix is not enough as this procedure is non-differentiable and cannot generalize to newly-visited states. To this end, we further distill the adjacency information stored in a constructed adjacency matrix into an adjacency network ψφ parameterized by φ.
The adjacency network learns a mapping from the goal space to an adjacency space, where the
Euclidean distance between the state and the goal is consistent with their shortest transition distance:
˜dst(s1, s2|φ) := k (cid:15)k (cid:107)ψφ(g1) − ψφ(g2)(cid:107)2 ≈ dst(s1, s2), (10) where g1 = ϕ(s1), g2 = ϕ(s2) and (cid:15)k is a scaling factor. As we have mentioned above, it is hard to regress the Euclidean distance in the adjacency space to the shortest transition distance accurately, and we only need to ensure a binary relation for implementing the adjacency constraint, i.e., (cid:107)ψφ(g1)−ψφ(g2)(cid:107)2 > (cid:15)k for dst(s1, s2) > k, and (cid:107)ψφ(g1)−ψφ(g2)(cid:107)2 < (cid:15)k for dst(s1, s2) < k, as shown in Figure 4. Inspired by modern metric learning approaches [14], we adopt a contrastive-like loss function for this distillation process:
Ldis(φ) = Esi,sj ∈S [ l · max ((cid:107)ψφ(gi) − ψφ(gj)(cid:107)2 − (cid:15)k, 0)
+ (1 − l) · max ((cid:15)k + δ − (cid:107)ψφ(gi) − ψφ(gj)(cid:107)2, 0)] , (11) 5
(a) (b) (c) (d) (e)
Figure 5: Environments used in our experiments. (a) Key-Chest. The agent (A) starts from a random position and needs to pick up the key (K) ﬁrst, then uses the key to open the chest (C). (b) Maze.
The agent (A) starts from a ﬁxed position and needs to reach the ﬁnal goal (G) with dense rewards. (c) Ant Gather. The ant robot starts from a ﬁxed position and needs to collect apples (green) and avoid bombs (red) (ﬁgure is adapted from Duan et al. [6]). (d) Ant Maze. The ant robot starts from a
ﬁxed position and needs to reach a target position in a maze with dense rewards. (e) Ant Maze Sparse.
The ant robot starts from a random position and needs to reach a target position in a maze with sparse rewards. where gi = ϕ(si), gj = ϕ(sj), and a hyper-parameter δ > 0 is used to create a gap between the embeddings. l ∈ {0, 1} represents the label indicating k-step adjacency derived from the k-step adjacency matrix. Equation (11) penalizes adjacent state embeddings (l = 1) with large Euclidean distances in the adjacency space and non-adjacent state embeddings (l = 0) with small Euclidean distances. In practice, we use states evenly-sampled from the adjacency matrix to approximate the expectation, and train the adjacency network each time after the adjacency matrix is updated with newly-sampled trajectories.
Although the construction of an adjacency matrix limits our method to tasks with tabular state spaces, our method can also handle continuous state spaces using goal space discretization (see our continuous control experiments in Section 5). For applications with vast state spaces, constructing a complete adjacency matrix will be problematic, but it is still possible to scale our method to these scenarios using speciﬁc feature construction or dimension reduction methods [28, 29, 7], or substituting the distance learning procedure with more accurate distance learning algorithms [10, 8] at the cost of some learning efﬁciency. We consider possible extensions in this direction as our future work. 4.2 Combining HRL and Adjacency Constraint
With a learned adjacency network ψφ, we can now incorporate the adjacency constraint into the goal-conditioned HRL framework. According to Equation (8), we introduce an adjacency loss Ladj to replace the original strict adjacency constraint and minimize the following high-level objective:
Lhigh(θh) = −E
πh
θh
T −1 (cid:88) (cid:0)γtrh kt − η · Ladj (cid:1) , (12) t=0 where η is the balancing coefﬁcient, and Ladj is derived by replacing dst with ˜dst deﬁned by
Equation (10) in the second term of Equation (8):
Ladj(θh) = H (cid:16) ˜dst (cid:0)skt, ϕ−1(gkt)|φ(cid:1) , k (cid:17)
∝ max ((cid:107)ψφ(ϕ(skt)) − ψφ(gkt)(cid:107)2 − (cid:15)k, 0) , (13) where gkt ∼ πh (g|skt). Equation (13) will output a non-zero value when the generated subgoal
θh and the current state have an Euclidean distance larger than (cid:15)k in the adjacency space, indicating non-adjacency. It is thus consistent with the k-step adjacency constraint. In practice, we plug Ladj as an extra loss term into the original policy loss term of a speciﬁc high-level RL algorithm, e.g., TD error for temporal-difference learning methods. 5 Experimental Evaluation
We have presented our method of Hierarchical Reinforcement learning with k-step Adjacency
Constraint (HRAC). Our experiments are designed to answer the following questions: (1) Can HRAC promote the generation of adjacent subgoals? (2) Can HRAC improve the sample efﬁciency and overall performance of goal-conditioned HRL? (3) Can HRAC outperform other strategies that may also improve the learning efﬁciency of hierarchical agents, e.g., hindsight experience replay [1]? 6
Figure 6: Learning curves of HRAC and baselines on all tasks. Each curve and its shaded region rep-resent mean episode reward and standard error of the mean respectively, averaged over 5 independent trials. All curves have been smoothed equally for visual clarity.
Figure 7: Learning curves in the ablation study, aver-aged over 5 independent trials.
Figure 8: Learning curves with different bal-ancing coefﬁcients. 5.1 Environment Setup
We employed two types of tasks with discrete and continuous state and action spaces to evaluate the effectiveness of our method, as shown in Figure 5. Discrete tasks include Key-Chest and Maze, where the agents are spawned in grid worlds with injected stochasticity and need to accomplish tasks that require both low-level control and high-level planning. Continuous tasks include Ant
Gather, Ant Maze and Ant Maze Sparse, where the ﬁrst two tasks are widely-used benchmarks in
HRL community [6, 11, 26, 25, 22], and the third task is a more challenging navigation task with sparse rewards. In all tasks, we used a pre-deﬁned 2-dimensional goal space that represents the (x, y) position of the agent. More details of the environments are in the supplementary material. 5.2 Comparative Experiments
To comprehensively evaluate the performance of HRAC with different HRL implementations, we employed two different HRL instances for different tasks. On discrete tasks, we used off-policy
TD3 [13] for high-level training and on-policy A2C, the syncrhonous version of A3C [24], for the low-level. On continuous tasks, we used TD3 for both the high-level and the low-level training, following prior work [26], and discretized the goal space to 1 × 1 grids for adjacency learning.
We compared HRAC with the following baselines. (1) HIRO [26]: one of the state-of-the-art goal-conditioned HRL approaches. (2) HIRO-B: a baseline analagous to HIRO, using binary intrinsic reward for subgoal reaching instead of the shaped reward used by HIRO. (3) HRL-HER: a baseline that employs hindsight experience replay (HER) [1] to produce alternative successful subgoal-reaching experiences as complementary low-level learning signals [22]. (4) Vanilla: Kulkarni et al. [20] used absolute subgoals instead of directional subgoals and adopted a binary intrinsic reward setting. More details of the baselines are in the supplementary material.
The learning curves of HRAC and baselines across all tasks are plotted in Figure 6. In the Maze task with dense rewards, HRAC achieves comparable performance with HIRO and outperforms other baselines, while in other tasks HRAC consistently surpasses all baselines both in sample efﬁciency and asymptotic performance. We note that the performance of the baseline HRL-HER matches the results in the previous study [26] where introducing hindsight techniques often degrades the performance of HRL, potentially due to the additional burden introduced on low-level training. 7
Figure 9: Visualizations on the Key-Chest task, based on a single evaluation run. The agent (A), key (K), chest (C) and subgoal (g) at four different time steps are plotted. The adjacency heatmap is based on the fourth time step, where colder colors represent smaller shortest transition distances.
Figure 10: Learning curves in stochastic environments, averaged over 5 independent trials. 5.3 Ablation Study and Visualizations
We also compared HRAC with several variants to investigate the effectiveness of each component. (1) HRAC-O: An oracle variant that uses a perfect adjacency matrix directly obtained from the environment. We note that compared to other methods, this variant uses the additional information that is not available in many applications. (2) NoAdj: A variant that uses an adjacency training method analagous to the work by Savinov et al. [34, 35], where no adjacency matrix is maintained.
The adjacency network is trained using state-pairs directly sampled from stored trajectories, under the same training budget as HRAC. (3) NegReward: This variant implements the k-step adjacency constraint by penalizing the high-level with a negative reward when it generates non-adjacent subgoals, which is used by HAC [22].
We provide learning curves of HRAC and these variants in Figure 7. In all tasks, HRAC yields similar performance with the oracle variant HRAC-O while surpassing the NoAdj variant by a large margin, exhibiting the effectiveness of our adjacency learning method. Meanwhile, HRAC achieves better performance than the NegReward variant, suggesting the superiority of implementing the adjacency constraint using a differentiable adjacency loss, which provides a stronger supervision than a penalty.
We also empirically studied the effect of different balancing coefﬁcients η. Results are shown in
Figure 8, which suggests that generally a large η can lead to better and more stable performance.
Finally, we visualize the subgoals generated by the high-level policy and the adjacency heatmap in Figure 9. Visualizations indicate that the agent does learn to generate adjacent and interpretable subgoals. We provide additional visualizations in the supplementary material. 5.4 Empirical Study in Stochastic Environments
To empirically verify the stochasticity robustness of HRAC, we applied it to a set of stochastic tasks, including stochastic Ant Gather, Ant Maze and Ant Maze Sparse tasks, which are modiﬁed from the original ant tasks respectively. Concretely, we added Gaussian noise with different standard deviations σ to the (x, y) position of the ant robot at every step, including σ = 0.01, σ = 0.05 and
σ = 0.1, representing increasing environmental stochasticity. In these tasks we compare HRAC with the baseline HIRO, which has exhibited generally better performance than other baselines, in the most noisy scenario when σ = 0.1. As displayed in Figure 10, HRAC achieves similar asymptotic performances with different noise magnitudes in stochastic Ant Gather and Ant Maze tasks and consistently outperforms HIRO, exhibiting robustness to stochastic environments. 8
6