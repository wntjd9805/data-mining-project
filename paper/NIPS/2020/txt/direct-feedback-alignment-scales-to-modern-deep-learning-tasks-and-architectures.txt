Abstract
Despite being the workhorse of deep learning, the backpropagation algorithm is no panacea. It enforces sequential layer updates, thus preventing efﬁcient paral-lelization of the training process. Furthermore, its biological plausibility is being challenged. Alternative schemes have been devised; yet, under the constraint of synaptic asymmetry, none have scaled to modern deep learning tasks and architec-tures. Here, we challenge this perspective, and study the applicability of Direct
Feedback Alignment (DFA) to neural view synthesis, recommender systems, geo-metric learning, and natural language processing. In contrast with previous studies limited to computer vision tasks, our ﬁndings show that it successfully trains a large range of state-of-the-art deep learning architectures, with performance close to
ﬁne-tuned backpropagation. When a larger gap between DFA and backpropagation exists, like in Transformers, we attribute this to a need to rethink common practices for large and complex architectures. At variance with common beliefs, our work supports that challenging tasks can be tackled in the absence of weight transport. 1

Introduction
While the backpropagation algorithm (BP) [1, 2] is at the heart of modern deep learning achievements, it is not without pitfalls. For one, its weight updates are non-local and rely on upstream layers. Thus, they cannot be easily parallelized [3], incurring important memory and compute costs. Moreover, its biological implementation is problematic [4, 5]. For instance, BP relies on the transpose of the weights to evaluate updates. Hence, synaptic symmetry is required between the forward and backward path: this is implausible in biological brains, and known as the weight transport problem [6].
Consequently, alternative training algorithms have been developed. Some of these algorithms are explicitly biologically inspired [7–13], while others focus on making better use of available compute resources [3, 14–19]. Despite these enticing characteristics, none has been widely adopted, as they are often demonstrated on a limited set of tasks. Moreover, as assessed in [20], their performance on challenging datasets under the constraint of synaptic asymmetry is disappointing.
We seek to broaden this perspective, and demonstrate the applicability of Direct Feedback Alignment (DFA) [19] in state-of-the-art settings: from applications of fully connected networks such as neural view synthesis and recommender systems, to geometric learning with graph convolutions, and natural language processing with Transformers. Our results deﬁne new standards for learning without weight transport and show that challenging tasks can indeed be tackled under synaptic asymmetry.
All code is available on the paper website at lair.lighton.ai/dfa-scales. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1.1