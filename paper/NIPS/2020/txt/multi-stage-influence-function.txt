Abstract
Multi-stage training and knowledge transfer, from a large-scale pretraining task to various ﬁnetuning tasks, have revolutionized natural language processing and computer vision resulting in state-of-the-art performance improvements. In this paper, we develop a multi-stage inﬂuence function score to track predictions from a ﬁnetuned model all the way back to the pretraining data. With this score, we can identify the pretraining examples in the pretraining task that contribute most to a prediction in the ﬁnetuning task. The proposed multi-stage inﬂuence function generalizes the original inﬂuence function for a single model in (Koh &
Liang, 2017), thereby enabling inﬂuence computation through both pretrained and
ﬁnetuned models. We study two different scenarios with the pretrained embeddings
ﬁxed or updated in the ﬁnetuning tasks. We test our proposed method in various experiments to show its effectiveness and potential applications. 1

Introduction
Multi-stage training has become increasingly important and has achieved state-of-the-art results in many tasks. In natural language processing (NLP) applications, it is now a common practice to
ﬁrst learn word embeddings (e.g., word2vec [16], GloVe [19]) or contextual representations (e.g.,
ELMo [20], BERT [7]) from a large unsupervised corpus, and then reﬁne or ﬁnetune the model on supervised end tasks. Also, transfer learning has been widely used in many different tasks. Intuitively, the successes of these multi-stage learning paradigms are due to knowledge transfer from pretraining tasks to the end task. However, current approaches using multi-stage learning are usually based on trial-and-error and many fundamental questions remain unanswered. For example, which part of the pretraining data/task contributes most to the end task? How can one detect “false transfer” where some pretraining data/task could be harmful for the end task? If a testing point is wrongly predicted by the ﬁnetuned model, can we trace back to the problematic examples in the pretraining data?
Answering these questions requires a quantitative measurement of how the data and loss function in the pretraining stage inﬂuence the end model, which has not been studied in the past and is the main focus of this paper.
To ﬁnd the most inﬂuential training data responsible for a model’s prediction, the inﬂuence function was ﬁrst introduced by [5]. Recently, as large-scale applications become more challenging for inﬂuence function computation, [13] proposed to use a ﬁrst-order approximation to measure the effect of removing one training point on the model’s prediction, to overcome computational challenges.
These methods are widely used in model debugging and there are also some applications in machine learning fairness [2, 26]. However, all of the existing inﬂuence score computation algorithms study the case of single-stage training – where there is only one model with one set of training/prediction data in the training process. To the best of our knowledge, the inﬂuence of pretraining data on a subsequent ﬁnetuning task and model has not been studied, and it is nontrivial to apply the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
original inﬂuence function in [13] to this scenario. A naive approach to solve this problem is to remove each individual instance out of the pretraining data one at a time and retrain both pretrain and
ﬁnetune models; this is prohibitively expensive, especially given that pretraining models are often large-scale and may take days to train.
In this work, we study the inﬂuence function from pretraining data to the end task, and propose a novel approach to estimate the inﬂuence scores in multi-stage training that requires no additional retrain, does not require model convexity, and is computationally tractable. The proposed approach is based on the deﬁnition of inﬂuence function, and considers estimating inﬂuence score under two multi-stage training settings depending on whether the embedding from pretraining model is retrained in the ﬁnetuning task. The derived inﬂuence function well explains how pretraining data beneﬁts the
ﬁnetuning task. In summary, our contributions are threefold: 1. We propose a novel estimation of inﬂuence score for multi-stage training.
In real datasets and experiments across various tasks, our predicted and actual inﬂuence score of the pretraining data to the ﬁnetuned model are well correlated. This shows the effective-ness of our proposed technique for estimating inﬂuence scores in multi-stage models. 2. We propose effective methods to determine how testing data from the ﬁnetuning task is impacted by changes in the pretraining data. We show that the inﬂuence of the pretraining data to the ﬁnetuned model consists of two parts: the inﬂuence of the pretraining data on the pretrained model, and inﬂuence of the pretraining data on the ﬁnetuned model.
The impact can be quantiﬁed using our proposed technique. 3. We propose methods to decide whether the pretraining data can beneﬁt the ﬁnetuning task. We show that the inﬂuence of the pretraining data on the ﬁnetuning task is highly dependent on 1) the similarity of two tasks or stages, and 2) the number of training data in the ﬁnetuning task. Our proposed technique provides a novel way to measure how the pretraining data helps or beneﬁts the ﬁnetuning task. 2