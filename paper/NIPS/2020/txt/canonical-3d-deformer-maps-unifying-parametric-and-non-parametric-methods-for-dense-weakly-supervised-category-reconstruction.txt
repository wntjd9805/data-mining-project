Abstract
We propose the Canonical 3D Deformer Map, a new representation of the 3D shape of common object categories that can be learned from a collection of 2D images of independent objects. Our method builds in a novel way on concepts from parametric deformation models, non-parametric 3D reconstruction, and canonical embeddings, combining their individual advantages. In particular, it learns to associate each image pixel with a deformation model of the corresponding 3D object point which is canonical, i.e. intrinsic to the identity of the point and shared across objects of the category. The result is a method that, given only sparse 2D supervision at training time, can, at test time, reconstruct the 3D shape and texture of objects from single views, while establishing meaningful dense correspondences between object instances. It also achieves state-of-the-art results in dense 3D reconstruction on public in-the-wild datasets of faces, cars, and birds. 1

Introduction
We address the problem of learning to reconstruct 3D objects from individual 2D images. While 3D reconstruction has been studied extensively since the beginning of computer vision research [49], and despite exciting progress in monocular reconstruction for objects such as humans, a solution to the general problem is still elusive. A key challenge is to develop a representation that can learn the 3D shapes of common objects such as cars, birds and humans from 2D images, without access to 3D ground truth, which is difﬁcult to obtain in general. In order to do so, it is not enough to model individual 3D shapes; instead, the representation must also relate the different shapes obtained when the object deforms (e.g. due to articulation) or when different objects of the same type are considered (e.g. different birds). This requires establishing dense correspondences between different shapes, thus identifying equivalent points (e.g. the left eye in two birds). Only by doing so, in fact, the problem of reconstructing independent 3D shapes from 2D images, which is ill-posed, reduces to learning a single deformable shape, which is difﬁcult but approachable.
In this paper, we introduce the Canonical 3D Deformer Map (C3DM), a representation that meets these requirements (Figure 1). C3DM combines the beneﬁts of parametric and non-parametric representations of 3D objects. Conceptually, C3DM starts from a parametric 3D shape model of the object, as often used in Non-Rigid Structure From Motion (NR-SFM [11]). It usually takes the form of a mesh with 3D vertices X1, . . . , XK ∈ R3 expressed as a linear function of global deformation parameters α, such that Xk = Bkα for a ﬁxed operator Bk. Correspondences between shapes are captured by the identities k of the vertices, which are invariant to deformations. Recent works such as Category-speciﬁc Mesh Reconstruction (CMR) [31] put this approach on deep-learning rails,
∗Authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The C3DM representation (left) associates each pixel y of the image I with a deformation operator B(κ), a function of the object canonical coordinates κ = Φy(I). C3DM then reconstructs the corresponding 3D point X as a function of the global object deformation α and viewpoint (R, t).
It extends three ideas (right): (a) non-rigid structure from motion computes a sparse parametric reconstruction starting from 2D keypoints rather than an image; (b) a monocular depth predictor dy(I) non-parametrically maps each pixel to its 3D reconstruction but lacks any notion of correspondence; (c) a canonical mapping Φy(I) establishes dense correspondences but does not capture geometry. learning to map an image I to the deformation parameters α(I). However, working with meshes causes a few signiﬁcant challenges, including guaranteeing that the mesh does not fold, rendering the mesh onto the image for learning, and dealing with the ﬁnite mesh resolution. It is interesting to compare parametric approaches such as CMR to non-parametric depth estimation models, which directly map each pixel y to a depth value dy(I) [70, 33, 18], describing the geometry of the scene in a dense manner. The depth estimator dy(I) is easily implemented by means of a convolutional neural network and is not bound to a ﬁxed mesh resolution. However, a depth estimator has no notion of correspondences and thus of object deformations.
Our intuition is that these two ways of representing geometry, parametric and non-parametric, can be combined by making use of the third notion, a canonical map [57, 51, 35]. A canonical map is a non-parametric model Φy(I) = κ that associates each pixel y to the intrinsic coordinates κ of the corresponding object point. The latter can be thought of as a continuous generalization of the index k that in parametric models identiﬁes a vertex of a mesh. Our insight is that any intrinsic quantity — i.e. one that depends only on the identity of the object point — can then be written as a function of κ.
This includes the 3D deformation operator Bκ, so that we can reconstruct the 3D point found at pixel y as Xy = Bκα. Note that this also requires to learn the mapping κ (cid:55)→ Bκ, which we can do by means of a small neural network.
We show that the resulting representation, C3DM, can reconstruct the shape of 3D objects densely and from single images, using only easily-obtainable 2D supervision at training time — the latter being particularly useful for 3D reconstruction from traditional non-video datasets. We extensively evaluate
C3DM and compare it to CMR [31], state-of-the-art method for monocular category reconstruction.
C3DM achieves both higher 3D reconstruction accuracy and more realistic visual reconstruction on real-world datasets of birds, human faces, and four other deformable categories of rigid objects. 2