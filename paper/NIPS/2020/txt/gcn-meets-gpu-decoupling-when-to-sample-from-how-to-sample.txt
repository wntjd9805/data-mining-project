Abstract
Sampling-based methods promise scalability improvements when paired with stochastic gradient descent in training Graph Convolutional Networks (GCNs).
While effective in alleviating the neighborhood explosion, due to bandwidth and memory bottlenecks, these methods lead to computational overheads in prepro-cessing and loading new samples in heterogeneous systems, which signiﬁcantly deteriorate the sampling performance. By decoupling the frequency of sampling from the sampling strategy, we propose LazyGCN, a general yet effective frame-work that can be integrated with any sampling strategy to substantially improve the training time. The basic idea behind LazyGCN is to perform sampling periodically and effectively recycle the sampled nodes to mitigate data preparation overhead.
We theoretically analyze the proposed algorithm and show that under a mild condi-tion on the recycling size, by reducing the variance of inner layers, we are able to obtain the same convergence rate as the underlying sampling method. We also give corroborating empirical evidence on large real-world graphs, demonstrating that the proposed schema can signiﬁcantly reduce the number of sampling steps and yield superior speedup without compromising the accuracy. 1

Introduction
Graphs are powerful and versatile data structures to model many real world problems. However, learning on a graph is challenging because it requires modeling of both rich node features and underlying structure information. In recent years, thanks to its effective representation power and improvements in hardware computing performance, Graph Convolutional Networks (GCNs) [15] and their subsequent variants [13, 25] have achieved great success in numerous domains, including social relationship detection [26, 7, 23], recommender systems [1, 29], knowledge graphs [27, 28, 21], and biological networks [8, 9].
Recent success of training deep neural networks with GPUs, makes such a highly parallel architecture a natural choice for training GCNs. However, directly adopting GPUs on large graphs remains challenging due to computational overheads introduced by inter-dependency between nodes. In fact, different from other standard neural architectures [17, 19, 24, 14] (e.g., fully connected or convolutional neural networks) where the prediction of an individual data sample solely depends on its own features, in a multi-hop GCN, the representation of a node recursively depends on its neighbors across multiple hops (i.e., layers) that need to be aggregated – a phenomenon known
∗Equal Contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
as neighborhood explosion. Processing this dependency requires node’s features, a large portion of estimated features of its neighbors at different hops, along with graph structure to be present in memory, which impedes the scalability to large graphs. This situation is further exacerbated on GPUs where local memory is in general more scarce compared to CPUs. For instance, the memory capacity on a very recent GPU card, such as NVIDIA Tesla V100, is at most 32 GB, while a scale-free graph with 50 million nodes can take up to 350 GB.
One way of alleviating this memory demand is to em-ploy sampling – an effective strategy that practitioners often use in training GCNs. The aim of sampling-based training is to aggregate the hidden features of only a sampled subset of neighbors at each layer. A number of recent studies have introduced and evaluated different sampling methods such as nodewise sampling [13, 29], layerwise sampling [3, 32], and subgraph sampling
[30, 4]. In practice, sampling from a large graph re-quires many random accesses to the memory, which inherently do not perform well in GPUs, which are de-signed for regular parallel accesses. A heterogeneous system comprising CPUs and GPUs allows some trade-offs between these two – CPUs are more capable of performing random memory access compared to GPUs, but do not have the high degree of parallelism offered by the latter [31, 18, 20]. However, transferring large volumes of data between the two (CPU and GPU) can further deteriorate the performance.
Figure 1: Timeline of executing vanilla
GCN vs proposed LAZYGCN on a CPU-GPU system with two processes. The num-bered boxes indicate the time spent for each mini-batch at different stages.
For example, as shown in the top half of Figure 1, in training a mini-batch GCN on a heterogeneous system, the majority of the time is spent on sampling nodes using CPU and transferring the sampled nodes from CPU to GPU, rather than the actual computation itself. One viable option to reduce the sampling overhead is to assign more CPU resources; however, this adds computation overhead on
CPU and storing the intermediate results of a large number of samplers can cause memory contention.
It is worth mentioning that sampling time increases signiﬁcantly as the graph size grows, while the transfer and computation time on GPUs remain the same, given the limited GPU capacity. Also, a solution to reduce the transfer time is to use smaller mini-batches and leave enough free space on GPU memory for next mini-batch data (i.e. overlap data transfer of next batch with compute of previous batch). However, a smaller mini-batch size in GCN is not preferable as it leads to under utilization of GPU and may also make the algorithm diverge if selected aggressively small.
Motivated by these observations, the key question we investigate in this paper is: for a given sampling strategy, can we reduce the sampling frequency to leverage the underlying hardware capabilities without compromising accuracy? In this work, we develop LAZYGCN, a general yet efﬁcient framework that is suitable for heterogeneous settings to train large scale GCNs. As shown in the bottom half of Figure 1, unlike vanilla GCN training methods, where a new sample for each iteration is prepared on CPU and transferred to GPU, LAZYGCN performs periodic sampling on CPU and transfers it to GPU. Then, instead of sampling new data points on CPU at every iteration, for a predetermined number of iterations, LAZYGCN effectively recycles the already-sampled nodes on the GPU, reducing both sampling and data transfer overheads. During recycling, a variance reduction schema is employed to reduce the inﬂuence of sampling. While previously proposed techniques such as mini-batch persistency [11] and data echoing [5] aim at incorporating data reuse for training, the main focus of those works are standard deep neural networks, where input dependency is insigniﬁcant compared to GCN. In addition, none of these works provide a proper analysis on the convergence of data reuse in the training of deep neural network.
We provide a theoretical analysis that motivates our algorithm, and characterizes its speed of conver-gence. Indeed, we show that under a mild condition on the recycling size, by reducing the variance of inner layers, we are able to obtain the same convergence rate as the underlying sampling method. We also conduct extensive numerical experiments on different large-scale graph datasets and different sampling methods to corroborate our theoretical ﬁndings, and demonstrate the practical efﬁcacy of the proposed algorithm over competitive baselines. Overall, our empirical results demonstrate that
LAZYGCN can signiﬁcantly reduce the number of sampling steps and yield superior speedup without 2
compromising the accuracy. Finally, we note that, while LAZYGCN is developed for CPU-GPU heterogeneous systems, it is not limited to this particular platform and the proposed techniques can be applied to any heterogeneous or distributed setting where the memory accesses and data transfers form a bottleneck.
Organization. The rest of this paper is organized as follows. In Section 2, we review the notations on graph convolutional networks. In Section 3, we propose LAZYGCN, and provide theoretical analysis in Section 4. We empirically evaluate our theory via various experiments in Section 5, and conclude the paper in Section 6. Additional experiments and the proof of convergence rate are deferred to appendix. 2