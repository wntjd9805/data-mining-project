Abstract
Numerous deep reinforcement learning agents have been proposed, and each
In this work, we present a Cooperative of them has its strengths and ﬂaws.
Heterogeneous Deep Reinforcement Learning (CHDRL) framework that can learn a policy by integrating the advantages of heterogeneous agents. Speciﬁcally, we propose a cooperative learning framework that classiﬁes heterogeneous agents into two classes: global agents and local agents. Global agents are off-policy agents that can utilize experiences from the other agents. Local agents are either on-policy agents or population-based evolutionary algorithms (EAs) agents that can explore the local area effectively. We employ global agents, which are sample-efﬁcient, to guide the learning of local agents so that local agents can beneﬁt from sample-efﬁcient agents and simultaneously maintain their advantages, e.g., stability. Global agents also beneﬁt from effective local searches. Experimental studies on a range of continuous control tasks from the Mujoco benchmark show that CHDRL achieves better performance compared with state-of-the-art baselines. 1

Introduction
Deep reinforcement learning (DRL) integrates deep neural networks with reinforcement learning principles, e.g.,Q-learning and policy-gradient, to create a more efﬁcient agent. Recent studies have shown a great success of DRL in numerous challenging real-world problems, e.g., video games and robotic control [19]. Although promising, existing DRL algorithms still suffer from several challenges including sample complexity, instability, and temporal credit assignment problems [28, 9].
One popular research line of DRL is policy-gradient based on-policy methods attempting to evaluate or improve the same policy that is used to make decisions [29], e.g., trust region policy optimization (TRPO) [24] and proximal policy optimization (PPO) [25]. Recent works [31, 17] have proved that policy-gradient based methods can converge to a stationary point under some conditions, which theoretically guarantees their stability. However, they are extremely sample-expensive since they require new samples to be collected in each gradient step [29].
On the contrary, Q-learning based off-policy methods, which is another research line evaluating or improving a policy different from the one that is used to generate the behavior, can improve 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The high-level structure of CHDRL for one iteration sample efﬁciency by reusing past experiences [29]. Existing off-policy based methods include deep Q-learning network (DQN) [19] and Soft Actor-Critic (SAC) [8] etc. These methods involve the approximation of some high-dimensional and nonlinear functions, usually through deep neural networks, which poses a signiﬁcant challenge on convergence and stability [3, 9]. It is also well known that off-policy Q learning is not to converge even with linear function approximation [2].
Moreover, recent studies [14, 6] identify some other key sources of instability for off-policy methods, i.e., bootstrapping and extrapolation errors. As shown in [14], off-policy methods are highly sensitive to data distribution, and can only make limited progress without exploiting additional on-policy data.
In addition to the pros and cons discussed above, on-policy and off-policy methods based on temporal difference learning suffer from some common issues. The one that received much research attention is the so-called temporal credit assignment problem [28]. When rewards become sparse or delayed, which is quite common in real-world problems, DRL algorithms may yield an inferior performance as reward sparsity downgrades the learning efﬁciency and hinders exploration. To alleviate this issue, evolutionary algorithms (EAs) [5, 26] have recently been introduced to DRL [21, 13]. The usage of a ﬁtness metric that consolidates returns across the entire episode makes EAs indifferent to reward sparsity and robust to long time horizons [22]. However, EAs suffer from high sample complexity and struggle to solve high-dimension problems involving massive parameters.
In this paper, we are interested in an algorithm that takes the essence and discards the dross of different DRL algorithms to achieve high sample efﬁciency and maintain good stability in various continuous control tasks. To do so, we propose a framework called CHDRL. Speciﬁcally, CHDRL works on an agent pool containing three classes of agents: an off-policy agent, an on-policy agent, and a opulation-based EAs agent. All the agents cooperate based on the following three mechanisms.
Firstly, all agents collaboratively explore the solution space following a hierarchical policy transfer rule. As the off-policy agent is sample-efﬁcient, we take it as the global agent to obtain a relatively good policy or value function at the beginning. The on-policy agent and the population-based EAs agent are taken as local agents and start their exploration with the prior knowledge transferred from the global agent. As the EAs agent is population-based, we further allow it to accept policies from the on-policy agent.
Secondly, we employ a local-global memory replay to enable global (off-policy) agents to replay the newly generated experiences by local (on-policy) agents more frequently so that global agents can beneﬁt from local search. Note that, with policy transfer as stated above, local agents start exploration with a policy transferred from global agents, and thus their generated experiences can be taken as close to the on-policy data of global agents’ current policy [14, 6]. By allowing global agents exploits more often from these local experiences, we can alleviate the bootstrapping or extrapolation error and further boost global agents’ learning. Consequently, global agents provide a better starting point for local agents who in turn generate more diverse local experiences for global agents’ replay, which forms a good win-win cycle.
Thirdly, although we encourage the cooperation among agents in exploration, we also tend to maintain the independence of each agent; that is, we do not want the learning of local agents to be completely dominated by that of global agents. This is to enable each agent to still maintain its policy updating scheme and preserve its learning advantage. To do so, we ﬁrstly develop a loosely coupled hierarchical framework with global agents at the upper-level and local agents at the lower-level1.
Such a framework not only makes each agent generally run in a relatively independent environment 1Policy transfer only happens from upper-level agents to lower-level agents. 2
with different random settings, but also achieves the easy and ﬂexible deployment or replacement of the agent candidates used in the framework. Secondly, to avoid over-policy-transfer, i.e., policy transfer happening too frequently thus interrupting the learning stability of local agents, we set a threshold to control the frequency of policy transfer.
The high-level structure of CHDRL is shown in Figure 1. In this work, we instantiated a CHDRL with PPO, SAC, and Cross-Entropy-Method (CEM) based EA [27], named CPSC. Experimental studies showed the superiority of CPSC to several state-of-the-art baselines in a range of continuous control benchmarks. We also conducted ablation studies to verify the three mechanisms. 2 Preliminaries
In this section, we review the representation of on-policy method, off-policy method, and EAs, namely, PPO [25], SAC [8], and Cross-Entropy based EA [27]. 2.1 Proximal Policy Optimization (PPO)
PPO is an on-policy algorithm that trains a stochastic policy.
It explores by sampling actions according to the latest version of its stochastic policy. During training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. PPO tries to keep new policies close to old. 2.2 Soft Actor-critic (SAC)
SAC is an off-policy algorithm that incorporates an entropy measure of the policy into the reward to encourage exploration. The idea is to learn a policy that acts as randomly as possible while still being able to succeed in the task. It is an off-policy actor-critic model that follows the maximum entropy RL framework. The policy is trained with the objective of maximizing the expected return and entropy at the same time. 2.3 Evolutionary Algorithms and CEM-ES
EAs [5, 26] are a class of black-box search algorithms that apply heuristic search procedures inspired by natural evolution. Among EAs, Estimation of Distribution Algorithms (EDAs) are a speciﬁc family where the population is represented as a distribution using a covariance matrix [15]. CEM is a simple EDA where the number of elite individuals is ﬁxed at a certain value. After all individuals of a population are evaluated, the top ﬁttest individuals are used to compute the new mean and variance of the population. 3