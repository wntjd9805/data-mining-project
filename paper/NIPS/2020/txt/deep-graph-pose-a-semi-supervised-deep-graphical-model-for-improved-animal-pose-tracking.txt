Abstract
Noninvasive behavioral tracking of animals is crucial for many scientiﬁc inves-tigations. Recent transfer learning approaches for behavioral tracking have con-siderably advanced the state of the art. Typically these methods treat each video frame and each object to be tracked independently. In this work, we improve on these methods (particularly in the regime of few training labels) by leveraging the rich spatiotemporal structures pervasive in behavioral video — speciﬁcally, the spatial statistics imposed by physical constraints (e.g., paw to elbow distance), and the temporal statistics imposed by smoothness from frame to frame. We pro-pose a probabilistic graphical model built on top of deep neural networks, Deep
Graph Pose (DGP), to leverage these useful spatial and temporal constraints, and develop an efﬁcient structured variational approach to perform inference in this model. The resulting semi-supervised model exploits both labeled and unlabeled frames to achieve signiﬁcantly more accurate and robust tracking while requiring users to label fewer training frames. In turn, these tracking improvements enhance performance on downstream applications, including robust unsupervised segmen-tation of behavioral “syllables,” and estimation of interpretable “disentangled” low-dimensional representations of the full behavioral video. Open source code is available at https://github.com/paninski-lab/deepgraphpose.
⇤equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1

Introduction
Animal pose estimation (APE) is a critical scientiﬁc task, with applications in ethology, psychology, neuroscience, and other ﬁelds. Recent work in neuroscience, for example, has emphasized the degree to which neural activity throughout the brain is correlated with movement [1, 2, 3]; i.e., to understand the brains of behaving animals we need to extract as much information as possible from behavioral video recordings. State of the art APE methods, such as DeepLabCut (DLC) [4], DeepPoseKit (DPK) [5], and LEAP [6], have transferred tools from human pose estimation (HPE) in deep learning literature to the APE setting [7, 8], opening up an exciting array of new applications and new scientiﬁc questions to be addressed.
However, even with these advances in place, hundreds of labels may still be needed to achieve tracking at the desired level of precision and reliability. Providing these labels requires signiﬁcant user effort, particularly in the common case that users want to track multiple objects per frame (e.g., all the ﬁngers on a hand or paw). Unlike HPE algorithms [9], APE algorithms are applied to a wide variety of different body structures (e.g., ﬁsh, ﬂies, mice, or cheetahs) [10], compounding the effort required to collect labeled datasets and hindering our ability to re-use a common skeletal model.
Moreover, even with hundreds of labels, users still often see occasional “glitches” in the output (i.e., frames where tracking is brieﬂy lost), which typically interfere with downstream analyses of the extracted behavior.
To improve APE performance in the sparse-labeled-data regime, we propose a probabilistic graphical model built on top of deep neural networks, Deep Graph Pose (DGP), to leverage both spatial and temporal constraints, and develop an efﬁcient structured variational approach to perform inference in this model. DGP is a semi-supervised model that takes advantage of both labeled and unlabeled frames to achieve signiﬁcantly more accurate and robust tracking, using fewer labels. Finally, we demonstrate that these tracking improvements enhance performance in downstream applications, including robust unsupervised segmentation of behavioral “syllables,” and estimation of interpretable low-dimensional representations of the full behavioral video. 2