Abstract
The strong correlation between neurons or ﬁlters can signiﬁcantly weaken the generalization ability of neural networks. Inspired by the well-known Tammes problem, we propose a novel diversity regularization method to address this issue, which makes the normalized weight vectors of neurons or ﬁlters distributed on a hypersphere as uniformly as possible, through maximizing the minimal pairwise angles (MMA). This method can easily exert its effect by plugging the MMA regularization term into the loss function with negligible computational overhead.
The MMA regularization is simple, efﬁcient, and effective. Therefore, it can be used as a basic regularization method in neural network training. Extensive experiments demonstrate that MMA regularization is able to enhance the generalization ability of various modern models and achieves considerable performance improvements on CIFAR100 and TinyImageNet datasets.
In addition, experiments on face veriﬁcation show that MMA regularization is also effective for feature learning.
Code is available at: https://github.com/wznpub/MMA_Regularization. 1

Introduction
Although neural networks have achieved state-of-the-art results in a variety of tasks, they contain redundant neurons or ﬁlters due to the over-parametrization issue [41, 21], which is prevalent in networks [39]. The redundance can lead to catching limited directions in feature space and poor generalization performance [27].
To address the redundancy problem and make neurons more discriminative, some methods are developed to encourage the angular diversity between pairwise weight vectors of neurons or ﬁlters in a layer, which can be categorized into the following three types. The ﬁrst type reduces the redundancy by dropping some weights and then retraining them iteratively during optimization [35, 12, 36], which suffers from complex training scheme and very long training phase. The second type is the widely used orthogonal regularization [38, 52, 23, 51], which exploits a regularization term in loss function to enforce the pairwise weight vectors as orthogonal as possible. However, it has been proven that orthogonal regularization tends to group neurons closer, especially when the number of neurons is greater than the dimension [24], and therefore it only produces marginal improvements [35]. The third type also utilizes a regularization term but to encourage the weight vectors uniformly spaced through minimizing the hyperspherical potential energy [24, 22] inspired from the Thomson problem [47, 44].
Nonetheless, its disadvantage is that both the time complexity and the space complexity are very
†The authors are with Shenzhen Key Laboratory of Advanced Machine Learning and Applications,
Guangdong Key Laboratory of Intelligent Information Processing, Institute of Artiﬁcial Intelligence and
Advanced Communication, College of Electronics and Information Engineering, Shenzhen University.
‡The author is with the Institute of Artiﬁcial Intelligence and Advanced Communication, College of
Mathematics and Statistics, Shenzhen University.
∗Corresponding author: Wenbin Zou. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Comparison of ﬁlter cosine similarity from the ﬁrst layer of VGG19-BN trained on
CIFAR100 with several different methods of angular diversity regularization. The number of similarity values above 0.2 is 495 (baseline), 120 (orthogonal), 51 (MHE), 0 (MMA), demonstrating the effectiveness of MMA regularization. high [24], and it suffers from a huge number of local minima and stationary points due to its highly non-convex and non-linear objective function [22].
In this paper, we propose a simple, efﬁcient, and effective method of angular diversity regularization which penalizes the minimum angles between pairwise weight vectors in each layer. Similar to the intuition of the third type mentioned above, the most diverse state is that the normalized weight vectors are distributed on a hypersphere uniformly. To model the criterion of uniformity, we employ the well-known Tammes problem, that is, to ﬁnd the arrangement of n points on a unit sphere which maximizes the minimum distance between any two points [46, 29, 33, 26, 32]. However, the optimal solutions for the Tammes problem only exist for some combinations of the number of points n and dimensions d, which are collected on the N.J.A. Sloane’s homepage [43], and obtaining a uniform distribution for an arbitrary combination of n and d is still an open mathematical problem [29]. In this paper, we propose a numerical optimization method to get approximate solutions for the Tammes problem through maximizing the minimal pairwise angles between weight vectors, named as MMA for abbreviation. We further develop the MMA regularization for neural networks to promote the angular diversity of weight vectors in each layer and thus improve the generalization performance.
There are several advantages of MMA regularization: (a) As analyzed in Section 3.2, the gradient of
MMA loss is stable and consistent, therefore it is easy to optimize and get near optimal solutions for the Tammes problem as shown in Table 1; (b) As veriﬁed in Table 3, the MMA regularization is easy to implement with negligible computational overhead, but with considerable performance improvements; (c) The MMA regularization is effective for both the hidden layers and the output layer, decorrelating the ﬁlters and enlarging the inter-class separability respectively. Therefore, it can be applied to multiple tasks, such as image classiﬁcation and face veriﬁcation demonstrated in this paper. To intuitively make sense of the effectiveness of MMA regularization, we visualize the cosine similarity of ﬁlters from the ﬁrst layer of VGG19-BN trained on CIFAR100 in Figure 1. We compare several different methods of angular diversity regularization, including orthogonal regularization in [38], MHE regularization in [24], and the proposed MMA regularization. The results show that the
MMA regularization gets the most uncorrelated ﬁlters. Besides, the MMA regularization keeps some negative correlations which have been veriﬁed to be beneﬁcial for neural networks [5].
In summary, the main contributions of this paper are three-fold:
• We propose a numerical method for the Tammes problem, called MMA, which can get near optimal solutions under arbitrary combinations of the number of points and dimensions.
• We develop the novel MMA regularization which effectively promotes the angular diversity of weight vectors and therefore improves the generalization power of neural networks.
• Various experiments on multiple tasks show that MMA regularization is generally effective and can become a basic regularization method for training neural networks. 2