Abstract
Robust covariance estimation is the following, well-studied problem in high di-mensional statistics: given N samples from a d-dimensional Gaussian N (0, Σ), but where an ε-fraction of the samples have been arbitrarily corrupted, out-put (cid:98)Σ minimizing the total variation distance between N (0, Σ) and N (0, (cid:98)Σ).
This corresponds to learning Σ in a natural afﬁne-invariant variant of the Frobe-nius norm known as the Mahalanobis norm. Previous work of [CDGW19] demonstrated an algorithm that, given N = Ω(d2/ε2) samples, achieved a near-optimal error of O(ε log 1/ε), and moreover, their algorithm ran in time (cid:101)O(T (N, d) log κ/ poly(ε)), where T (N, d) is the time it takes to multiply a d × N matrix by its transpose, and κ is the condition number of Σ. When ε is rela-tively small, their polynomial dependence on 1/ε in the runtime is prohibitively large. In this paper, we demonstrate a novel algorithm which achieves the same statistical guarantees, but which runs in time (cid:101)O(T (N, d) log κ). In particular our runtime has no dependence on ε. When Σ is reasonably conditioned, our runtime matches that of the fastest algorithm for covariance estimation without outliers, up to poly-logarithmic factors, showing that we can get robustness essentially “for free.” 1

Introduction
Covariance estimation is one of the most fundamental high dimensional statistical estimation tasks, see e.g. [BL+08a, BL+08b], and references therein. In this paper, we study the problem of covariance estimation in high dimensions, in the presence of a small fraction of adversarial data. We consider the following standard generative model: we are given samples X1, . . . , XN drawn from a Gaussian
N (0, Σ), but an ε-fraction of these points have been arbitrarily corrupted. The goal is then to output (cid:98)Σ minimizing the total variation distance between N (0, Σ) and N (0, (cid:98)Σ). As we shall see, this naturally corresponds to learning Σ in an afﬁne-invariant version of the Frobenius norm, known as the Mahalanobis norm (see Section 2).
In the non-robust setting, where there are no corruptions, the problem is well-understood from both a information-theoretic and computational perspective. It is known that the empirical covariance of the data converges to the true covariance at an optimal statistical rate: the empirical covariance matrix has expected Mahalanobis error at most O(d/
N ); and this is the optimal bound up to a constant factor. That is, when we have N = Ω(d2/ε2), the empirical covariance matrix will have Mahalanobis error O(ε). In fact, it satisﬁes a stronger and more natural afﬁne-invariant error guarantee, as we will discuss later in this Section. Moreover, it is easy to compute: it can be computed in time T (N, d), where T (n, m) is the time it takes to multiply a m × n matrix by its transpose. When N = Θ(d2/ε2),
√ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the currently known best runtime for this is (cid:101)O(N d1.252) [GU18].1 Moreover, this runtime is unlikely to improve without improving the runtime of rectangular matrix multiplication.
√
The situation is a bit muddier in the robust setting. If there are an ε-fraction of corrupted samples, the information-theoretically optimal error for covariance estimation of N (0, Σ) is O(ε + d/
N ).
In particular, when N = Ω(d2/ε2), we can achieve error O(ε) [Rou85, CGR+18]. However, the algorithms which achieve this rate run in time which is exponential in the dimension d. In
[DKK+16], the authors gave the ﬁrst polynomial-time algorithm for this problem, which given enough samples, achieves error which is independent of the dimension. Speciﬁcally, they achieve an error of O(ε log 1/ε), which matches the information-theoretic limit, up to logarithmic factors, and is likely optimal for efﬁcient algorithms [DKS17], up to constants. However, their sample complexity and runtime—while polynomial—are somewhat large, and limit their applicability to very large, high dimensional datasets. More recently, [CDGW19] gave an algorithm which runs in time (cid:101)O(T (N, d)/ε8). When ε is constant, the runtime of this algorithm nearly matches that of the non-robust algorithm. However, the dependence on ε is prohibitive for ε even moderately small. This raises a natural question: can we obtain algorithms for robust covariance estimation of a Gaussian whose runtimes (nearly) match rectangular matrix multiplication?
In this paper, we resolve this question in the afﬁrmative. Informally, we achieve the following guarantee:
Theorem 1 (informal, see Theorem 2). Let D be a Gaussian distribution with unknown covariance
Σ, where Σ has polynomial condition number. Let 0 < ε < ε0 for some universal constant ε0. Given a set of N = (cid:101)Ω(d2/ε2) samples from D, where an ε-fraction of these samples have been arbitrarily corrupted, there is an algorithm that runs in time (cid:101)O(T (N, d)) and outputs (cid:98)Σ ∈ Rd×d such that the
Malahanobis distance between Σ and (cid:98)Σ is at most O(ε log 1/ε).
By combining this with the result of [DHL19], this allows us to robustly learn a polynomially-conditioned Gaussian to total variation distance O(ε log 1/ε) in time (cid:101)O(T (N, d)).
Our algorithm follows the same general framework as the algorithm in [CDGW19]. They reduce the problem of covariance estimation given corrupted Gaussian samples X1, . . . , XN , to a robust mean estimation problem given samples Yi = Xi ⊗ Xi, where ⊗ denotes Kronecker product. Then, their algorithm proceeds in two phases: ﬁrst, they invoke a robust mean estimation algorithm to achieve a rough estimate of the covariance, then they give a procedure which, given a rough estimate of the covariance, can improve it. They show that both steps can be reduced to solving a packing SDP to high accuracy, and invoke black-box nearly-linear time SDP solvers [AZLO15, AZLO16] to obtain their desired runtime. However, both phases incur poly(1/ε) running time, because in both cases, they need to solve the packing SDP to poly(ε) accuracy, and the black-box packing SDP solvers require poly(1/ε) runtime to do so.
Our main contribution is to demonstrate that both phases of their algorithms can be made faster by using techniques inspired by the quantum entropy scoring algorithm presented in [DHL19]. The ﬁrst phase can be directly improved by using the robust mean estimation in [DHL19] to replace the robust mean estimation algorithm used in [CDGW19] that achieves error O(
ε). Improving the second phase requires more work. This is because the algorithm in [DHL19] for robust mean estimation below error O(
ε) requires that the uncorrupted samples are isotropic, i.e. their covariance is the identity, and have sub-gaussian tails. However, the Yi are only approximately isotropic, and moreover, have only sub-exponential tails. Despite this, we demonstrate that we can modify the algorithm and analysis in [DHL19] to handle both of these additional complications.
√
√ 1.1