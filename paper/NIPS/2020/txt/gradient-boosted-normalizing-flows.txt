Abstract
By chaining a sequence of differentiable invertible transformations, normalizing
ﬂows (NF) provide an expressive method of posterior approximation, exact density evaluation, and sampling. The trend in normalizing ﬂow literature has been to devise deeper, more complex transformations to achieve greater ﬂexibility. We propose an alternative: Gradient Boosted Normalizing Flows (GBNF) model a density by successively adding new NF components with gradient boosting. Under the boosting framework, each new NF component optimizes a weighted likelihood objective, resulting in new components that are ﬁt to the suitable residuals of the previously trained components. The GBNF formulation results in a mixture model structure, whose ﬂexibility increases as more components are added. Moreover,
GBNFs offer a wider, as opposed to strictly deeper, approach that improves existing
NFs at the cost of additional training—not more complex transformations. We demonstrate the effectiveness of this technique for density estimation and, by coupling GBNF with a variational autoencoder, generative modeling of images.
Our results show that GBNFs outperform their non-boosted analog, and, in some cases, produce better results with smaller, simpler ﬂows. 1

Introduction
Deep generative models seek rich latent representations of data, and provide a mechanism for sampling new data. A popular approach to generative modeling is with variational autoencoders (VAEs) [47]. A major challenge in VAEs, however, is that they assume a factorial posterior, which is widely known to limit their ﬂexibility [9, 14, 41, 48, 57, 65, 76, 78]. Further, VAEs do not offer exact density estimation, which is a requirement in many settings.
Normalizing ﬂows (NF) are an important recent development and can be used in both density estimation [19, 67, 73] and variational inference [65]. Normalizing ﬂows are smooth, invertible transformations with tractable Jacobians, which can map a complex data distribution to simple distribution, such as a standard normal [61]. In the context of variational inference, a normalizing
ﬂow transforms a simple, known base distribution into a more faithful representation of the true posterior. As such, NFs complement VAEs, providing a method to overcome the limitations of a factorial posterior. Flow-based models are also an attractive approach for density estimation
[18, 19, 20, 33, 38, 40, 46, 60, 61, 69, 73] because they provide exact density computation and sampling with only a single neural network pass (in some instances) [24].
Recent developments in NFs have focused of creating deeper, more complex transformations in order to increase the ﬂexibility of the learned distribution [3, 11, 13, 38, 40, 46, 53]. With greater model complexity comes a greater risk of overﬁtting while slowing down training, prediction, and sampling.
Boosting [27, 28, 29, 30, 56] is ﬂexible, robust to overﬁtting, and generally one the most effective learning algorithms in machine learning [36]. While boosting is typically associated with regression and classiﬁcation, it is also applicable in the unsupervised setting [8, 8, 34, 35, 52, 57, 68]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our contributions. In this work we propose a wider, as opposed to strictly deeper, approach for increasing the expressiveness of density estimators and posterior approximations. Our approach, gradient boosted normalizing ﬂows (GBNF), iteratively adds new NF components to a model based on gradient boosting, where each new NF component is ﬁt to the residuals of the previously trained components. A weight is learned for each component of the GBNF model, resulting in a mixture structure. However, unlike a mixture model, GBNF offers the optimality advantages associated with boosting [2], and a simpliﬁed training objective that focuses solely on optimizing a single new component at each step. GBNF compliments existing ﬂow-based models, improving performance at the cost of additional training cycles—not more complex transformations. Prediction and sampling are not slowed with GBNF, as each component is independent and operates in parallel.
While gradient boosting is straight-forward to apply in the density estimation setting, our analysis highlights the need for analytically invertible ﬂows in order to efﬁciently boost ﬂow-based models for variational inference. Further, we address the “decoder shock” phenomenon—a challenge unique to VAEs with GBNF approximate posteriors, where the loss increases suddenly coinciding with the introduction of a new component. Our experiments show that augmenting the VAE with a GBNF variational posterior produces image modeling results on par with state-of-the-art NFs. Lastly, GBNF improves density estimation performance on complex, multi-modal data. 2