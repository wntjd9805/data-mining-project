Abstract
Bayesian optimisation is a popular method for efﬁcient optimisation of expensive black-box functions. Traditionally, BO assumes that the search space is known.
However, in many problems, this assumption does not hold. To this end, we propose a novel BO algorithm which expands (and shifts) the search space over iterations based on controlling the expansion rate thought a hyperharmonic series.
Further, we propose another variant of our algorithm that scales to high dimensions.
We show theoretically that for both our algorithms, the cumulative regret grows at sub-linear rates. Our experiments with synthetic and real-world optimisation tasks demonstrate the superiority of our algorithms over the current state-of-the-art methods for Bayesian optimisation in unknown search space. 1

Introduction
Bayesian optimisation (BO) is a powerful and ﬂexible tool for efﬁcient global optimisation of expensive black-box functions. An underlying limitation of existing approaches is that the search is restricted to a pre-deﬁned and ﬁxed search space, thus implicitly assuming that this search space will contain the global optimum. To set suitable bounds of the search space, prior knowledge is required. When exploring entirely new problems (e.g a new machine learning model), such prior knowledge is often poor, and thus the speciﬁcation of the search space can be erroneous leading to suboptimal solutions. For example, in many machine learning algorithms we have hyperparameters or parameters that can take values in an unbounded space e.g. L1/L2 penalty hyperparameters in elastic-net can take any nonnegative value. Similarly, the weights of a neural network can take any real values. No matter how large a ﬁnite search space is set, one cannot be sure if this search space contains the global optimum.
This problem is considered in [23, 17, 18, 8, 2], and UBO [8] is the ﬁrst to provide global convergence analysis. However, they consider a weak version of the global convergence, i.e., instead of seeking the exact global optimum, they ﬁnd a solution wherein the function value is within (cid:15) > 0 of the global optimum. Further, there is no analysis on the convergence rate of this algorithm which is important for understanding the efﬁciency of the optimisation.
Another complication arises when high dimensional problems are considered (e.g. hyperparameter tuning [24], reinforcement learning [1]), as BO scales poorly in practice. With unknown search spaces, we need to consider evolving/growing search spaces. This growth in search spaces makes high dimensional BO further challenging as the search space is already exponentially large with respect to dimensions. Both these challenges compound the difﬁculty in maximising the acquisition functions. With limited budgets, the accuracy of points suggested by the acquisition step is often poor and this adversely affects both the convergence and the efﬁciency of the BO algorithm. Thus solutions to BO in unknown high-dimensional search spaces need to be found.
∗Correspondence to: Hung Tran-The <hung.tranthe@deakin.edu.au>. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we address these open problems. Our contributions are as follows:
• We introduce a novel BO algorithm for unknown search space, using a volume expansion strategy with a rate of expansion controlled through hyperharmornic series [3]. We show that our algorithm achieves a sub-linear convergence rate.
• We then provide a ﬁrst solution for BO problem with unknown high dimensional search spaces. Our solution is based on using a restricted search space consisting of a set of hypercubes with small sizes. Based on controlling the number of hypercubes according to the expansion rate of the search space, we derive an upper bound on the cumulative regret and theoretically show that it can achieve a sub-linear growth rate.
• We evaluate our algorithms extensively using a variety of optimisation tasks in-cluding optimisation of several benchmark functions and tuning both the hyperpa-rameters (Elastic Net) and parameters of machine learning algorithms (weights of a neural network and Lunar Lander). We demonstrate that our algorithms have better sample and computational efﬁciency compared to existing methods on both synthetic and real optimisation tasks. Our source code is publicly available at https://github.com/Tran-TheHung/Unbounded_Bayesian_Optimisation. 2