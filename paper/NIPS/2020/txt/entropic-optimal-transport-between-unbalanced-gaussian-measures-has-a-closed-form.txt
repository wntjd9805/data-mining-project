Abstract
Although optimal transport (OT) problems admit closed form solutions in a very few notable cases, e.g. in 1D or between Gaussians, these closed forms have proved extremely fecund for practitioners to deﬁne tools inspired from the OT geometry.
On the other hand, the numerical resolution of OT problems using entropic reg-ularization has given rise to many applications, but because there are no known closed-form solutions for entropic regularized OT problems, these approaches are mostly algorithmic, not informed by elegant closed forms. In this paper, we propose to ﬁll the void at the intersection between these two schools of thought in OT by proving that the entropy-regularized optimal transport problem between two Gaussian measures admits a closed form. Contrary to the unregularized case, for which the explicit form is given by the Wasserstein-Bures distance, the closed form we obtain is differentiable everywhere, even for Gaussians with degenerate covariance matrices. We obtain this closed form solution by solving the ﬁxed-point equation behind Sinkhorn’s algorithm, the default method for computing entropic regularized OT. Remarkably, this approach extends to the generalized unbalanced case — where Gaussian measures are scaled by positive constants. This extension leads to a closed form expression for unbalanced Gaussians as well, and high-lights the mass transportation / destruction trade-off seen in unbalanced optimal transport. Moreover, in both settings, we show that the optimal transportation plans are (scaled) Gaussians and provide analytical formulas of their parameters.
These formulas constitute the ﬁrst non-trivial closed forms for entropy-regularized optimal transport, thus providing a ground truth for the analysis of entropic OT and
Sinkhorn’s algorithm. 1

Introduction
Optimal transport (OT) theory [48, 20] has recently inspired several works in data science, where dealing with and comparing probability distributions, and more generally positive measures, is an important staple (see [39] and references therein). For these applications of OT to be successful, a belief now widely shared in the community is that some form of regularization is needed for OT to be both scalable and avoid the curse of dimensionality [17, 21]. Two approaches have emerged in recent years to achieve these goals: either regularize directly the measures themselves, by looking at them through a simpliﬁed lens; or regularize the original OT problem using various modiﬁcations.
The ﬁrst approach exploits well-known closed-form identities for OT when comparing two univariate 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
measures or two multivariate Gaussian measures. In this approach, one exploits those formulas and operates by summarizing complex measures as one or possibly many univariate or multivariate
Gaussian measures. The second approach builds on the fact that for arbitrary measures, regularizing the OT problem, either in its primal or dual form, can result in simpler computations and possibly improved sample complexity. The latter approach can offer additional beneﬁts for data science: because the original marginal constraints of the OT problem can also be relaxed, regularized OT can also yield useful tools to compare measures with different total mass — the so-called “unbalanced” case [3]— which provides a useful additional degree of freedom. Our work in this paper stands at the intersection of these two approaches. To our knowledge, that intersection was so far empty: no meaningful closed-form formulation was known for regularized optimal transport. We provide closed-form formulas of entropic (OT) of two Gaussian measures for balanced and unbalanced cases.
Summarizing measures vs. regularizing OT. Closed-form identities to compute OT distances (or more generally recover Monge maps) are known when either (1) both measures are univariate and the ground cost is submodular [43, §2]: in that case evaluating OT only requires integrating that submodular cost w.r.t. the quantile distributions of both measures; or (2) both measures are
Gaussian, in a Hilbert space, and the ground cost is the squared Euclidean metric [18, 23], in which case the OT cost is given by the Wasserstein-Bures metric [4, 34]. These two formulas have inspired several works in which data measures are either projected onto 1D lines [40, 6], with further developments in [38, 30, 47]; or represented by Gaussians, to take advantage of the simpler computational possibilities offered by the Wasserstein-Bures metric [28, 37, 11].
Various schemes have been proposed to regularize the OT problem in the primal [14, 22] or the dual [45, 2, 15]. We focus in this work on the formulation obtained by [13], which combines entropic regularization [14] with a more general formulation for unbalanced transport [12, 31, 32]. The advantages of unbalanced entropic transport are numerous: it comes with favorable sample complexity regimes compared to unregularized OT [24], can be cast as a loss with favorable properties [26, 19], and can be evaluated using variations of the Sinkhorn algorithm [25].
On the absence of closed-form formulas for regularized OT. Despite its appeal, one of the shortcomings of entropic regularized OT lies in the absence of simple test-cases that admit closed-form formulas. While it is known that regularized OT can be related, in the limit of inﬁnite regularization, to the energy distance [41], the absence of closed-form formulas for a ﬁxed regularization strength poses an important practical problem to evaluate the performance of stochastic algorithms that try to approximate regularized OT: we do not know of any setup for which the ground truth value of entropic OT between continuous densities is known. The purpose of this paper is to ﬁll this gap, and provide closed form expressions for balanced and unbalanced OT for Gaussian measures. We hope these formulas will prove useful in two different ways: as a solution to the problem outlined above, to facilitate the evaluation of new methodologies building on entropic OT, and more generally to propose a more robust yet well-grounded replacement to the Bures-Wasserstein metric.