Abstract
Biological brains are inherently limited in their capacity to process and store information, but are nevertheless capable of solving complex tasks with apparent ease. Intelligent behavior is related to these limitations, since resource constraints drive the need to generalize and assign importance differentially to features in the environment or memories of past experiences. Recently, there have been parallel efforts in reinforcement learning and neuroscience to understand strategies adopted by artiﬁcial and biological agents to circumvent limitations in information storage. However, the two threads have been largely separate. In this article, we propose a dynamical framework to maximize expected reward under constraints of limited resources, which we implement with a cost function that penalizes precise representations of action-values in memory, each of which may vary in its precision.
We derive from ﬁrst principles an algorithm, Dynamic Resource Allocator (DRA), which we apply to two standard tasks in reinforcement learning and a model-based planning task, and ﬁnd that it allocates more resources to items in memory that have a higher impact on cumulative rewards. Moreover, DRA learns faster when starting with a higher resource budget than what it eventually allocates for performing well on tasks, which may explain why frontal cortical areas in biological brains appear more engaged in early stages of learning before settling to lower asymptotic levels of activity. Our work provides a normative solution to the problem of learning how to allocate costly resources to a collection of uncertain memories in a manner that is capable of adapting to changes in the environment. 1

Introduction
Reinforcement learning (RL) is a powerful form of learning wherein agents interact with their environment by taking actions available to them and observing the outcome of their choices in order to maximize a scalar reward signal. Most RL algorithms use a value function in order to ﬁnd good policies [1, 2] because knowing the optimal value function is sufﬁcient to ﬁnd an optimal policy [3, 4].
However, RL models typically assume that agents can access and update the value function for each state or state-action pair with nearly inﬁnite precision. In neural circuits, however, this assumption
∗Current address: Département des neurosciences fondamentales, Université de Genève, CMU, 1 rue
Michel-Servet, 1206 Genève, Switzerland. Alternative e-mail: nisheet.pat@gmail.com. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is necessarily violated. The values must be stored in memories which are limited in their precision, especially in smaller brains in which the number of neurons can be severely limited [5].
In this work, we make such constraints explicit and consider the question of what rationality is when computational resources are limited [6, 7]. Speciﬁcally, we examine how agents might represent values with limited memory, how they may utilize imprecise memories in order to compute good policies, and whether and how they should prioritize some memories over others by devoting more resources to encode them with higher precision. We are interested in drawing useful abstractions from small-scale models that can be applied generally and in investigating whether brains employ similar mechanisms.
We pursue this idea by considering memories that are imprecise in their representation of values, which are stored as distributions that the agent may only sample from. We construct a novel family of cost functions that can adjudicate between maximizing reward and limited coding resources (Section 2). Crucially, for this general class of resource-limited objective functions, we derive how an RL agent can solve the resource allocation problem by computing stochastic gradients with respect to the coding resources. Further, we combine resource allocation with learning, enabling agents to assign importance to memories dynamically with changes in the environment. We call our proposed algorithm the Dynamic Resource Allocator (DRA), which we apply to standard tasks in RL in Section 3 and a model-based planning task in Section 4.1 1.1