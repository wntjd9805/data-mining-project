Abstract
Randomized classiﬁers have been shown to provide a promising approach for achieving certiﬁed robustness against adversarial attacks in deep learning. How-ever, most existing methods only leverage Gaussian smoothing noise and only work for ℓ2 perturbation. We propose a general framework of adversarial certiﬁca-tion with non-Gaussian noise and for more general types of attacks, from a uniﬁed functional optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smoothing distribu-tions and leverage it to design new families of non-Gaussian smoothing distribu-tions that work more efﬁciently for different ℓp settings, including ℓ1, ℓ2 and ℓ∞ attacks. Our proposed methods achieve better certiﬁcation results than previous works and provide a new perspective on randomized smoothing certiﬁcation. 1

Introduction
Although many robust training algorithms have been developed to overcome adversarial attacks
[1, 2, 3], most heuristically developed methods can be shown to be broken by more powerful adver-saries eventually (e.g., [4, 5, 6, 7]). This casts an urgent demand for developing robust classiﬁers with provable worst-case guarantees. One promising approach for certiﬁable robustness is the recent randomized smoothing method [8, 9, 10, 11, 12, 13, 14, 15], which constructs smoothed classiﬁers with certiﬁable robustness by introducing noise on the inputs. Compared with the other more tradi-tional certiﬁcation approaches [16, 17, 18] that exploit special structures of the neural networks (such as the properties of ReLU), the randomized smoothing approaches work more ﬂexibly on general black-box classiﬁers and is shown to be more scalable and provide tighter bounds on challenging datasets such as ImageNet [19].
Most existing methods use Gaussian noise for smoothing. Although appearing to be a natural choice, one of our key observations is that the Gaussian distribution is, in fact a sub-optimal choice in high dimensional spaces even for ℓ2 attack. We observe that there is a counter-intuitive phenomenon in high dimensional spaces [20], that almost all of the probability mass of standard Gaussian distri-bution concentrates around the sphere surface of a certain radius. This makes tuning the variance of Gaussian distribution an inefﬁcient way to trade off robustness and accuracy for randomized smoothing.
∗Equal contributions 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our Contributions We propose a general framework of adversarial certiﬁcation using non-Gaussian smoothing noises, based on a new functional optimization perspective. Our framework uniﬁes the methods of [9] and [14] as special cases, and is applicable to more general smoothing distributions and more types of attacks beyond ℓ2-norm setting. Leveraging our insight, we develop a new family of distributions for better certiﬁcation results on ℓ1, ℓ2 and ℓ∞ attacks. An efﬁcient computational approach is developed to enable our method in practice. Empirical results show that our new framework and smoothing distributions outperform existing approaches for ℓ1, ℓ2 and ℓ∞ attacking, on datasets such as CIFAR-10 and ImageNet. 2