Abstract
Several machine learning applications involve the optimization of higher-order derivatives (e.g., gradients of gradients) during training, which can be expensive with respect to memory and computation even with automatic differentiation.
As a typical example in generative modeling, score matching (SM) involves the optimization of the trace of a Hessian. To improve computing efﬁciency, we rewrite the SM objective and its variants in terms of directional derivatives, and present a generic strategy to efﬁciently approximate any-order directional derivative with
ﬁnite difference (FD). Our approximation only involves function evaluations, which can be executed in parallel, and no gradient computations. Thus, it reduces the total computational cost while also improving numerical stability. We provide two instantiations by reformulating variants of SM objectives into the FD forms.
Empirically, we demonstrate that our methods produce results comparable to the gradient-based counterparts while being much more computationally efﬁcient. 1

Introduction
Deep generative models have achieved impressive progress on learning data distributions, with either an explicit density function [24, 26, 46, 48] or an implicit generative process [1, 10, 75]. Among explicit models, energy-based models (EBMs) [34, 63] deﬁne the probability density as pθ(x) = (cid:101)pθ(x)/Zθ, where (cid:101)pθ(x) denotes the unnormalized probability and Zθ = (cid:82) (cid:101)pθ(x)dx is the partition function. EBMs allow more ﬂexible architectures [9, 11] with simpler compositionality [15, 41] compared to other explicit generative models [46, 16], and have better stability and mode coverage in training [30, 31, 71] compared to implicit generative models [10]. Although EBMs are appealing, training them with maximum likelihood estimate (MLE), i.e., minimizing the KL divergence between data and model distributions, is challenging because of the intractable partition function [20].
Score matching (SM) [21] is an alternative objective that circumvents the intractable partition function by training unnormalized models with the Fisher divergence [23], which depends on the Hessian trace and (Stein) score function [37] of the log-density function. SM eliminates the dependence of the log-likelihood on Zθ by taking derivatives w.r.t. x, using the fact that ∇x log pθ(x) = ∇x log (cid:101)pθ(x).
Different variants of SM have been proposed, including approximate back-propagation [25], cur-vature propagation [39], denoising score matching (DSM) [65], a bi-level formulation for latent variable models [3] and nonparametric estimators [35, 55, 59, 62, 74], but they may suffer from high computational cost, biased parameter estimation, large variance, or complex implementations. Sliced score matching (SSM) [58] alleviates these problems by providing a scalable and unbiased estimator
∗Equal contribution. † Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
with a simple implementation. However, most of these score matching methods optimize (high-order) derivatives of the density function, e.g., the gradient of a Hessian trace w.r.t. parameters, which are several times more computationally expensive compared to a typical end-to-end propagation, even when using reverse-mode automatic differentiation [13, 47]. These extra computations need to be performed in sequential order and cannot be easily accelerated by parallel computing (as discussed in
Appendix B.1). Besides, the induced repetitive usage of the same intermediate results could magnify the stochastic variance and lead to numerical instability [60].
To improve efﬁciency and stability, we ﬁrst ob-serve that existing scalable SM objectives (e.g.,
DSM and SSM) can be rewritten in terms of (second-order) directional derivatives. We then propose a generic ﬁnite-difference (FD) decom-position for any-order directional derivative in
Sec. 3, and show an application to SM methods in Sec. 4, eliminating the need for optimizing on higher-order gradients. Speciﬁcally, our FD approach only requires independent (unnormal-ized) likelihood function evaluations, which can be efﬁciently and synchronously executed in par-allel with a simple implementation (detailed in
Sec. 3.3). This approach reduces the computational complexity of any T -th order directional deriva-tive to O(T ), and improves numerical stability because it involves a shallower computational graph.
As we exemplify in Fig. 1, the FD reformulations decompose the inherently sequential high-order gradient computations in SSM (left panel) into simpler, independent routines (right panel). Mathe-matically, in Sec. 5 we show that even under stochastic optimization [51], our new FD objectives are asymptotically consistent with their gradient-based counterparts under mild conditions. When the generative models are unnormalized, the intractable partition function can be eliminated by the linear combinations of log-density in the FD-form objectives. In experiments, we demonstrate the speed-up ratios of our FD reformulations with more than 2.5× for SSM and 1.5× for DSM on different generative models and datasets, as well as the comparable performance of the learned models.
Figure 1: Computing graphs of each update step.
Detailed in Sec. 2.2 (SSM) and Sec. 4 (FD-SSM). 2