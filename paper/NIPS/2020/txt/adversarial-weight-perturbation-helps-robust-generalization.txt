Abstract
The study on improving the robustness of deep neural networks against adver-sarial examples grows rapidly in recent years. Among them, adversarial training is the most promising one, which ﬂattens the input loss landscape (loss change with respect to input) via training on adversarially perturbed examples. However, how the widely used weight loss landscape (loss change with respect to weight) performs in adversarial training is rarely explored. In this paper, we investigate the weight loss landscape from a new perspective, and identify a clear correlation between the ﬂatness of weight loss landscape and robust generalization gap. Sev-eral well-recognized adversarial training improvements, such as early stopping, designing new objective functions, or leveraging unlabeled data, all implicitly
ﬂatten the weight loss landscape. Based on these observations, we propose a simple yet effective Adversarial Weight Perturbation (AWP) to explicitly regularize the
ﬂatness of weight loss landscape, forming a double-perturbation mechanism in the adversarial training framework that adversarially perturbs both inputs and weights.
Extensive experiments demonstrate that AWP indeed brings ﬂatter weight loss landscape and can be easily incorporated into various existing adversarial training methods to further boost their adversarial robustness. 1

Introduction
Although deep neural networks (DNNs) have been widely deployed in a number of ﬁelds such as computer vision [14], speech recognition [51], and natural language processing [10], they could be easily fooled to conﬁdently make incorrect predictions by adversarial examples that are crafted by adding intentionally small and human-imperceptible perturbations to normal examples [45, 13, 56, 4, 50]. As DNNs penetrate almost every corner in our daily life, ensuring their security, e.g., improving their robustness against adversarial examples, becomes more and more important.
There have emerged a number of defense techniques to improve adversarial robustness of DNNs
[36, 27, 52]. Across these defenses, Adversarial Training (AT) [13, 27] is the most effective and promising approach, which not only demonstrates moderate robustness, but also has thus far not been comprehensively attacked [2]. AT directly incorporates adversarial examples into the training process to solve the following optimization problem: min w
ρ(w), where ρ(w) = 1 n n (cid:88) i=1 max i−xi(cid:107)p≤(cid:15) (cid:107)x(cid:48) (cid:96)(fw(x(cid:48) i), yi), (1) where n is the number of training examples, x(cid:48) i is the adversarial example within the (cid:15)-ball (bounded by an Lp-norm) centered at natural example xi, fw is the DNN with weight w, (cid:96)(·) is the standard
†Corresponding author: Yisen Wang (yisen.wang@pku.edu.cn) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
classiﬁcation loss (e.g., the cross-entropy (CE) loss), and ρ(w) is called the “adversarial loss” following Madry et al. [27]. Eq. (1) indicates that AT restricts the change of loss when its input is perturbed (i.e., ﬂattening the input loss landscape) to obtain a certain of robustness, but its robustness is still far from satisfactory because of the huge robust generalization gap [42, 40], for example, an adversarially trained PreAct ResNet-18 [15] on CIFAR-10 [21] only has 43% test robustness, even it has already achieved 84% training robustness after 200 epochs (see Figure 1). Its robust generalization gap reaches 41%, which is very different from the standard training (on natural examples) whose standard generalization gap is always lower than 10%. Thus, how to mitigate the robust generalization gap becomes essential for the robustness improvement of adversarial training methods.
Recalling that weight loss landscape is a widely used indicator to characterize the standard general-ization gap in standard training scenario [33, 22, 12, 34], however, there are few explorations under adversarial training[38, 59, 23], among which, Prabhu et al. [38] and Yu et al. [59] tried to use the pre-generated adversarial examples to explore but failed to draw the expected conclusions. In this paper, we explore the weight loss landscape under adversarial training using on-the-ﬂy generated adversarial examples, and identify a strong connection between the ﬂatness of weight loss landscape and robust generalization gap. Several well-recognized adversarial training improvements, i.e., AT with early stopping [40], TRADES [62], MART [53] and RST [6], all implicitly ﬂatten the weight loss landscape to narrow the robust generalization gap. Motivated by this, we propose an explicit weight loss landscape regularization, named Adversarial Weight Perturbation (AWP), to directly restrict the ﬂatness of weight loss landscape. Different from random perturbations [16], AWP injects the strongest worst-case weight perturbations, forming a double-perturbation mechanism (i.e., inputs and weights are both adversarially perturbed) in the adversarial training framework. AWP is generic and can be easily incorporated into existing adversarial training approaches with little overhead. Our main contributions are summarized as follows:
• We identify the fact that ﬂatter weight loss landscape often leads to smaller robust gen-eralization gap in adversarial training via characterizing the weight loss landscape using adversarial examples generated on-the-ﬂy.
• We propose Adversarial Weight Perturbation (AWP) to explicitly regularize the weight loss landscape of adversarial training, forming a double-perturbation mechanism that injects the worst-case input and weight perturbations.
• Through extensive experiments, we demonstrate that AWP consistently improves the adver-sarial robustness of state-of-the-art methods by a notable margin. 2