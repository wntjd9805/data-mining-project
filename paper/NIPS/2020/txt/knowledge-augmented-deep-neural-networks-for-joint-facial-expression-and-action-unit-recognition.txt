Abstract
Facial expression and action units (AUs) represent two levels of descriptions of the facial behavior. Due to the underlying facial anatomy and the need to form a meaningful coherent expression, they are strongly correlated. This paper proposes to systematically capture their dependencies and incorporate them into a deep learning framework for joint facial expression recognition and action unit detection.
Speciﬁcally, we ﬁrst propose a constraint optimization method to encode the generic knowledge on expression-AUs probabilistic dependencies into a Bayesian
Network (BN). The BN is then integrated into a deep learning framework as a weak supervision for an AU detection model. A data-driven facial expression recognition(FER) model is then constructed from data. Finally, the FER model and AU detection model are trained jointly to reﬁne their learning. Evaluations on benchmark datasets demonstrate the effectiveness of the proposed knowledge integration in improving the performance of both the FER model and the AU detection model. The proposed AU detection model is demonstrated to be able to achieve competitive performance without AU annotations. Furthermore, the proposed Bayesian Network capturing the generic knowledge is demonstrated to generalize well to different datasets. 1

Introduction
Facial expression is a key signal of human emotion. From the facial expression analysis perspective, there are two levels of expression descriptors: the global facial expression and the local Facial Action
Units(AUs). These two descriptors lead to two research topics: Facial Expression Recognition(FER) and Facial Action Units Detection. AU is deﬁned as facial muscle movements that correspond to a displayed expression according to Facial Action Coding System(FACS)[7]. AU activation is usually subtle and hard to annotate, thus the annotated AU data is limited and error prone. In comparison, the expression is global and easier to label. In addition, for both AU detection and FER problems, the data-driven models trained within datasets may generalize poorly to other datasets.
To learn a more generalizable model with limited AU annotation data, domain knowledge are considered. Some work used the manually designed knowledge directly from FACS [7] or muscle knowledge([23, 54]), whereas others constructed the dependencies among AUs from data ([48, 56, 52]) which are usually represented as a structural model (e.g. tree[14], graph[20], and graphical model[48, 45]). AU detection is then carried out by label propagation[23] or model training[44] using the captured knowledge. Though the relationships among AUs are included, the knowledge is based on local AU labels. Furthermore, the knowledge learned from a speciﬁc dataset cannot 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
generalize well. Since AUs and expressions are different levels of descriptors, they are closely related.
In other words, they can complement each other and improve each other’s performance through their interactions. Enlightened by this idea, expressions are employed as supplementary supervision for
AU detection task([25, 26]). Besides expressions, facial landmarks are also considered as global information for AU detection[2]. On the other hand, AUs are also employed to enhance the FER tasks. Khorrami et al [16] showed that salient AU features can be obtained from deep neural networks that are trained for FER tasks. By incorporating AU information as domain knowledge into FER, performance improvements for FER can be achieved.
In this paper, we propose to perform joint AU detection and FER within a deep learning framework by leveraging the generic knowledge. Bayesian Network(BN) is employed to capture the generic knowledge on relationships among AUs and expression. Speciﬁcally, we propose to learn a BN purely from probability constraints derived from the generic knowledge and formulate the BN learning as a constraint optimization problem. The BN is then embedded into a deep learning framework to weakly supervise the learning of an AU detector. FER and AU detection modules are further jointly trained iteratively to improve each other’s learning performance. By simultaneously leveraging both the AU-expression knowledge and the data, as well as integrating the knowledge via the interactions between
AU detector and FER model, our FER model achieves better performance than a pure data-driven model, and our AU detector can generalize well to different datasets, and, more importantly, achieve comparable performance to existing supervised methods without any AU annotations. 2