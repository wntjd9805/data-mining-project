Abstract
Commonly used classiﬁcation algorithms in machine learning, such as support vector machines, minimize a convex surrogate loss on training examples.
In practice, these algorithms are surprisingly robust to errors in the training data. In this work, we identify a set of conditions on the data under which such surrogate loss minimization algorithms provably learn the correct classiﬁer. This allows us to establish, in a uniﬁed framework, the robustness of these algorithms under various models on data as well as error. In particular, we show that if the data is linearly classiﬁable with a slightly non-trivial margin (i.e. a margin at least C(cid:14)√ d for d-dimensional unit vectors), and the class-conditional distributions are near isotropic and logconcave, then surrogate loss minimization has negligible error on the uncorrupted data even when a constant fraction of examples are adversarially mislabeled. 1

Introduction
A commonly used paradigm in supervised learning is to minimize a surrogate loss over available training examples. In other words, to learn the parameters w of a classiﬁcation model, we optimize a loss function of the form (cid:80) i (cid:96)(w, zi) over available labeled training examples {zi}. Often the parameters w are themselves constrained to be in a certain set, or regularized. This paradigm has been extremely successful and underlies most applications of supervised learning.
The training examples can come from varying sources. Often, several of the training examples are mislabeled. This could be due to some inherent noise in the process, or due to adversarial mislabeling.
For example, when learning a spam ﬁlter, one may use training examples labeled by users and some of these users may be spammers that insert training examples to make the system behave a certain way. These issues of noisy data, or data poisoning attacks are not new to machine learning. They have been explored in statistics under the name robust statistics [Huber, 1964, Huber and Ronchetti, 2009, Hampel et al., 2011], and in learning theory under various models of corruption [Valiant, 1985,
Kearns and Li, 1988, Angluin and Laird, 1988]. In this work, we will largely be interested in the adversarial label corruption model, where the adversary can ﬂip the labels of an arbitrary η fraction of the examples.
There are at least two possible ways in which label corruptions might occur. The ﬁrst is model misspeciﬁcation: the true data distribution may not be linearly classiﬁable given the features. In this case, we should aim to minimize the error rate on the whole distribution. A different reason, and the focus of this work, is where the primary source of corruption is noisy, or adversarially corrupted labels. We ﬁnd this to be a natural model in many training settings, where some of the data comes from users, e.g. labels coming from CAPTCHAs or from “Report Spam”/“Report Inappropriate
Content” buttons, where some of these labels will come from bots or spammers. Recent works have studied this model in the stochastic bandit setting [Lykouris et al., 2018, Gupta et al., 2019]. In
∗Work performed while at Google Brain. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
such cases, we have errors in the provided training data, but the goal is to do well on uncorrupted distribution coming from real users. and ignore the performance on inputs from bots/spammers.
The problem of robustly learning a linear classiﬁer (under adverarial label corruption) is surprisingly hard in the worst case. For proper learning, i.e. where we want the learnt classiﬁer to be linear as well, it is hard to approximate the error rate [Arora et al., 1997, Guruswami and Raghavendra, 2009,
Feldman et al., 2006] to a multiplicative factor better than 2log1–ε d. Under slightly stronger complexity assumptions, a similar hardness holds for arbitrary learning algorithms [Daniely et al., 2014, Daniely, 2016]. Thus we have little hope of designing robust algorithms for learning linear classiﬁers in the worst case.
There is a large body of work on designing efﬁcient, robust algorithms that work under additional assumptions on the data and on the outliers2. One line of work relaxes the distribution-independent
PAC model, and studies speciﬁc nicer distributions, such as the uniform distribution over the unit ball.
Under such assumptions, one can get arbitrarily close to the underlying corruption rate in polynomial time.
The assumptions, while natural, are arguably too restrictive. For example, data distributions of interest often have a margin, whereas isotropic distributions studied in previous work are incompatible with a reasonable margin. This motivates the question: What conditions on the data distribution allow for efﬁcient robust learning of linear classiﬁers?
Another line of work looks at more constrained models of corruption. E.g. a recent work of Di-akonikolas et al. [2019] shows that under the Massart Noise model, one can efﬁciently learn in the
PAC model with error rate arbitrarily close to the noise rate.
These results, using sophisticated algorithms, serve to explain why it should be possible to efﬁciently learn outlier-robust linear classiﬁers. In practice, algorithms such as SVMs and logistic loss mini-mization are usually used and seem to be surprisingly robust to outliers. This begs the question: Can we explain the robustness of these methods?
Additionally all existing works in the malicious noise model aim to learn a classiﬁer that has error rate (on inliers and outliers) close to the corruption rate η. When we care about the error rate on the inliers alone, this leads to an error rate close to η/(1 – η). This is information-theoretically optimal without additional assumptions. In this work, we ask: Under what assumptions can we bypass this lower bound and get error-rates smaller than the corruption rate, on the inlier distribution?
In this work, we address these three questions. We identify a set of conditions under which minimizing a surrogate loss allows us to learn a good classiﬁer even in the presence of outliers. We assume that the inlier distribution is separable with a margin. Our general results (see Section 1.1) allow us to derive corollaries for different data and noise models. For example, we prove the following result.
Theorem 1. (Informal) Suppose that the data distribution is supported on the unit ball in Rd, and there is a linear separator with margin γ = Ω( log d√
). Suppose that the positive and negative example d distributions are mixtures of O(1) isotropic log-concave distributions, each with means having norm
O(γ). There is a constant η such that for adversarially corrupted label error rate up to η, the hinge loss minimizer on a
˜Theta(d)-sized sample has error rate poly(d) on the original data distribution. 1
The data distribution assumption here is perhaps the simplest data distribution that is compatible with the margin condition. We show that a constant fraction of adversarial label errors can be tolerated while getting accuracy close to 1.
Our approach is motivated by works on "beyond worst-case analysis" [Candes and Tao, 2005,
Ostrovsky et al., 2012, Bilu and Linial, 2012, Balcan et al., 2013, Awasthi et al., 2010b,a, Voevodski et al., 2010]. We identify a set of deterministic conditions under which minimizing a surrogate loss allows us to learn a good classiﬁer even in the presence of outliers. We then show that under various models for data and noise, the conditions hold with appropriate parameters, which allows us to establish robustness. Our assumptions are weaker than the distributional assumptions in previous work. We make an additional assumption of the inliers being separable with a margin. 2In this introduction, we use the term outliers to refer to the corrupted training data. 2
1.1 Results and Techniques
We will work with examples (xi, yi) where xi ∈ Rd and has norm at most 1, and yi ∈ {+1, –1}. We assume that the inliers are correctly classiﬁed by a linear classiﬁer w(cid:63). In fact, we will assume margin-separability, which says that there is w(cid:63) with norm at most 1 i w(cid:63) ≥ 1 for all inliers. Geometrically, this says that there are no points in a band of width ≈ γ (cid:63) around the hyperplane deﬁned by w(cid:63). Such margin assumptions are standard in learning literature.
γ (cid:63) that satisﬁes yix(cid:62)
A new condition that we introduce is the dense pancakes condition. Informally, this says that if we project all inliers onto any direction w, then most points are not too isolated from other inliers. Geometrically, this says that for a point x ∈ Rd, a “pancake” around x, i.e. the set
{x(cid:48) ∈ X : w(cid:62)x – τ ≤ w(cid:62)x(cid:48) ≤ w(cid:62)x + τ}, is sufﬁciently dense, i.e. contains a ρ fraction of the inliers. We require that for all directions, most pancakes are ρ-dense (the parameter ρ can be arbitrary and affects our tolerance to outliers). The precise deﬁnition is slightly more complex and deferred to Section 2. (xi,yi)∈O(cid:48) yixi(cid:107).
Finally, the relevant measure of the effect of the outliers in our work is the norm of the sum of (a subset of) the examples xi. For a set of examples O, we deﬁne their Hereditary Sum Norm
HerSumNorm(O) as maxO(cid:48)⊆O (cid:107) (cid:80)
Under assumptions on these parameters, we show the following theorem.
Theorem 2. (Informal) Let µ be a γ (cid:63)-margin separable distribution and let I be a sample of (1 – η)n examples drawn from µ. Let O be an arbitrary dataset of ηn examples in X ×{–1, 1} and let D = I ∪O.
Let w be an appropriately constrained optimum for the hinge loss on D. If (D, µ) satisﬁes the (τ, ρ, β)-dense pancakes condition for ρ, β ∈ (0, 1), with τ ≤ γ (cid:63)/2 and (1 – η)ργ (cid:63)n > 2HerSumNorm(O), then w has accuracy at least (1 – β) on µ.
This result deﬁnes a recipe for proving robustness results for various combinations of assumptions on inlier distribution and assumptions on the corruption. Besides, the margin, the relevant ingredients are simply the density of the pancakes in inliers, and sum norm bound for the corrupted data points.
We then develop tools to establish these conditions under different models. We ﬁrst study the pancakes condition. We show that whenever the data distribution is isotropic and logconcave, we can establish the dense pancake condition. Further, the pancakes condition is robust enough to easily handle translation, mixing and homogenization transformations.
We next investigate the SumNorm condition for the outliers. We study several noise models. In the malicious noise model, where the outliers are arbitrary, the best bound one can prove on the
HerSumNorm is linear in the number of outliers (and this is tight). This gives us tolerance to an
Ω(γ (cid:63)) fraction of malicious outliers.
In a slightly more constrained noise model, where the adversary can change the labels but not the points themselves, the situation improves dramatically. In this case, we show that for any isotropic logconcave distribution, the HerSumNorm is in fact bounded by approximately |O|/ d, even if the points whose labels are corrupted are adversarially chosen. This is because even though we are adding up to |O| unit vectors, they will in general not be aligned and the projection in any ﬁxed direction d for a large enough is only about 1/ constant C, then a constant fraction of labels can be adversarially ﬂipped with virtually no effect on the accuracy of the learnt classiﬁer! d. This allows us to show that if the margin is at least C/
√
√
√
We note that in our results, the learnt classiﬁer has accuracy at least 1 – β on the inlier distribution, where β depends only on the pancakes condition and can be much smaller than the error rate η. This is in contrast to most previous work on agnostic learning in the distributional model (see Section 1.2).
The additional margin assumption we make allows us to prove this much stronger form of robustness.
We remark that the margin assumption is only needed for what we call inliers. If an α fraction of the true inliers violate the margin assumption, they can be considered as outliers, increasing η by α. Our result would then apply and give an overall error rate of (α + β) on the actual inlier distribution.
Other than the beneﬁt that we are able to analyze commonly-used algorithms, our approach offers an additional advantage. Since we have a deterministic condition that implies the robustness, the result is robust to some changes in the data distribution. For example, we show that the pancake condition is preserved under translations, and approximatley preserved under mixing of distributions.
Thus if the class-conditional distributions are each a uniform mixture of a few isotropoic logconcave 3
distributions, then the pancake condition continues to hold. The sumnorm condition is similarly robust.
Unlike most previous work on properties of surrogate loss minimization, our result is not based on controlling the objective function value, but rather depends on the (ﬁrst-order) optimality conditions.
Our conclusion about correct classiﬁcation is not based on the loss being small; in fact the loss itself can be large on many examples. Our proof relates the optimality conditions to the 0-1 loss of the resulting classiﬁer.
Our theory applies to the (cid:96)2 geometry, but one can envision version of our theorems for (cid:96)p for other p’s. The (cid:107) · (cid:107)1-(cid:107) · (cid:107)∞ case, where w is regularized in the (cid:96)1 norm, is a particularly compelling research direction. While our main result would technically extend to Kernel methods, our current approach to infer the dense pancakes condition on the empirical sample requires the dataset size to to be Θ(d), making it inapplicable to the Kernel setting. While one can use random projections to Θ( 1
γ 2 ) dimensions and apply the algorithm in the projected space, extending our results to the usual SVM with Kernels is an interesting open question.
The rest of the paper is organized as follows. We present next additional related work. In Section 2, we set up notation and deﬁne the dense pancakes condition as well as HerSumNorm. Section 3 proves that our conditions, for appropriate parameters, imply correct classiﬁcation. We derive results for Adversarial Label Noise in Sections 4 and 5. Additional tools for proving the pancakes condition, other noise models, and all missing proofs can be found in the Supplementary material. 1.2