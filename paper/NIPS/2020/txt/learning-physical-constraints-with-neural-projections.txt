Abstract
We propose a new family of neural networks to predict the behaviors of physical sys-tems by learning their underpinning constraints. A neural projection operator lies at the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a conﬁgurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efﬁcacy of our approach by learning a set of challenging physical systems all in a uniﬁed and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries. 1

Introduction
How does a human being distinguish the motions of a piece of paper and a piece of cloth? A high-school physics teacher might answer that they are both tangentially inextensible but cloth cannot resist any bending force from the normal direction. This raises a further general question for machine perception – what is the most effective representation to characterize a physical system computationally? The answer to this question is fundamental for the design of better neural physics engines [1, 2, 3, 4, 5] to predict the dynamics of various real-world Newtonian systems based on limited observations. In general, a capable neural physics simulator needs to capture the essential features of a dynamic system with a uniﬁed computational model, simple network architectures, small training data, and the minimum human inputs for priors. To this end, a vast literature has been devoted into building neural-network models to reason and predict physics. Two of the main areas include to reason the underlying physics by learning local interactions (e.g., the gravitational force between two planets [1]) or by enforcing global energy conservation (e.g., the sum of potential and kinematic energies of a pendulum [6]).
This paper proposes to investigate a third category of approaches to characterize classical physical systems, by establishing neural predictors to learn and enforce underlying physical constraints.
The term “physical constraints” broadly deﬁnes the various intuitive criteria that the motion of a physical system must satisfy, e.g., a constant length between particles, a ﬁxed angle between two segment pieces, the overlap of joint positions, the non-penetrating geometries for collisions, etc. Such constraints can be either hard or soft, with forms of both equality and inequality. Mathematically, a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
set of equality constraints can be expressed as a non-linear equation system C(x) = 0 with each row
Ci(x) = 0 corresponding to a single constraint exerted on the system. To enforce these constraints over the temporal evolution, a common idea established in the physics simulation communities is to deﬁne a projection operator to map the system’s current states to a low-dimensional constraint manifold satisfying C(x) = 0 (e.g., see [7, 8, 9, 10, 11, 12]). By augmenting the dynamics with a
Lagrangian multiplier, the projection amounts to the minimization of the following energy form [11]: min x g(x) = 1
∆t2 (x − ˆx)T M(x − ˆx) + λT C(x), (1) with ˆx and x as the system’s states before and after enforcing the constraints, M as the mass matrix, and λ as a Lagrangian multiplier. The intuition behind Equation 1 is to ﬁnd the closest point on the constraint manifold to modify the current prediction, e.g., by following the direction of −∇C with a ﬁxed small step in the gradient descent search. The optimization of the energy form in Equation 1 along with its various variations serve as the algorithmic foundation to accommodate a broad spectrum of constraint physics simulators, including articulated rigid bodies [10], collisions [9], contacts [13], inextensible cloth [11], soft bodies [14], and the various position-based dynamics techniques [12, 15, 16, 17], which have recently emerged in gaming industry. Such simulators have also been used to generate datasets for machine learning applications [18, 2]. Meanwhile, the mathematical properties of neural projections have been investigated in the machine learning community (see [19] for examples).
Motivated by the physics intuition behind Equation 1, we devise a new neural physics simulator to unify the prediction of the various dynamic systems by learning their underlying physical constraints.
Our main idea is simple: we express the mixed dynamic effects due to all the constraints by one neural network and enforce these constraints by recursively employing the network to correct the system’s time-independent states (position). The centerpiece of our learning framework is a neural projection operator that enables the mapping from a current state to a constraint state on the target manifold. The parameters of the operator are trained in an end-to-end fashion by observing the positional states of the system for a certain range of time frames.
Our design philosophy to learn the physics constraints exhibits several inherent computational merits compared with learning relations or energy conservation. First, constraints directly relate to human’s physical perception. The various physical intuitions, such as length, angle, volume, position, penetration, etc., can be encoded automatically and learned straightforwardly in our neural networks to describe constraints. In contrast, the expression of energy, albeit essential for computational physics, lacks its intuitive counterparts (e.g., many systems do not conserve energy due to their dissipative environments). Second, our neural constraint expression describes a system-level relation without requiring any connectivity priors (e.g., it does not need a graph network to specify the relations).
This connectivity-free implementation is essential when describing complicated interactions with an uncertain number of primitives. For example, to express the bending effects of a piece of cloth, it requires at least three particles to describe a planar angle in 2D and four particles to describe a bilateral angle in 3D. Such case-by-case priors require expertise in physics simulation and are difﬁcult to obtain beforehand for normal users. Third, constraints are a time-independent state variable that can be reasoned with position information only. This alleviates the data requirement to train an expressive neural-network model. Also, the complexity for a neural expression of constraints is low. In our implementation, a small-scale fully-connected network in conjunction with our iterative projection scheme can uncover mixed constraints governing complicated physical interactions. Last, from a numerical perspective, enforcing constraints in a numerical simulator essentially amounts to building an implicit time integrator, which is inherently stable and allows for large time steps. This further lowers the training data requirements and enables reliable long-term predictions. 2