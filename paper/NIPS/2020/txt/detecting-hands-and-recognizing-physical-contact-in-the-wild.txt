Abstract
We investigate a new problem of detecting hands and recognizing their physical contact state in unconstrained conditions. This is a challenging inference task given the need to reason beyond the local appearance of hands. The lack of training annotations indicating which object or parts of an object the hand is in contact with further complicates the task. We propose a novel convolutional network based on Mask-RCNN that can jointly learn to localize hands and predict their physical contact to address this problem. The network uses outputs from another object detector to obtain locations of objects present in the scene. It uses these outputs and hand locations to recognize the hand’s contact state using two attention mechanisms. The ﬁrst attention mechanism is based on the hand and a region’s afﬁnity, enclosing the hand and the object, and densely pools features from this region to the hand region. The second attention module adaptively selects salient features from this plausible region of contact. To develop and evaluate our method’s performance, we introduce a large-scale dataset called ContactHands, containing unconstrained images annotated with hand locations and contact states.
The proposed network, including the parameters of attention modules, is end-to-end trainable. This network achieves approximately 7% relative improvement over a baseline network that was built on the vanilla Mask-RCNN architecture and trained for recognizing hand contact states. Code and data are available at: https://github.com/cvlab-stonybrook/ContactHands. 1

Introduction
The objective of this work is to detect hands in images and recognize their physical contact state. By physical contact state, we mean to recognize the following four conditions for each hand instance, namely (1) No-Contact: the hand is not in contact with any object in the scene; (2) Self-Contact: the hand is in contact with another body part of the same person; (3) Other-Person-Contact: the hand is in contact with another person; and (4) Object-Contact: the hand is holding or touching an object other than people. These conditions are not mutually exclusive, and a hand can be in multiple states; for example, a hand can contact another person and, at the same time, hold an object. Detecting hands and recognizing their physical contact is an important problem with many potential applications, including harassment detection, contamination prevention, and activity recognition.
However, recognizing the contact state of a hand in unconstrained conditions is challenging because the hand’s appearance alone is insufﬁcient to estimate its contact state. This task also requires us to consider the relationships between the hand and other objects in the scene. This can be a complex inference problem for many real-world situations, especially where numerous people and objects surround the hand. Furthermore, even for a pair of hand and object with corresponding segmentation masks, it is not easy to recognize whether the hand is in contact with the object due to the lack of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
depth information. A heuristic-based method using occlusion or overlapping criteria would not work well because the hand can hover in front of the object without touching it.
In this work, we propose a Contact-Estimation neural network module for recognizing the physical contact state of hands. This module can be integrated into an object detection framework to detect hands and recognize their contact states jointly. Together with the hand detector, we can train the
Contact-Estimation module end-to-end using training images where hands are localized and annotated with corresponding contact states. Notably, our method does not require annotation for the contact object or contact areas. One technical contribution of our paper is learning to recognize contact states using such weak annotations.
Speciﬁcally, we implement our method based on Mask-RCNN [13], a state-of-the-art object detection framework. Mask-RCNN has a Region Proposal Network (RPN) that ﬁrst generates a candidate hand proposal box. A box regression head and a mask head then obtain the bounding box and a binary segmentation map of the hand. Additionally, we obtain the locations of other objects in the scene using a generic object detector pre-trained on the COCO [18] dataset. We then use the
Contact-Estimation branch to recognize the contact state for detected hands. The inputs to this new branch are: (1) the feature maps for the hand, and (2) a set of K feature maps, one for each hand-object union box, where K is the number of detected objects.
Given the above inputs, we use the Contact-Estimation network module to compute scores for each of the K hand-object pairs. We ﬁrst combine the hand feature map with the hand-object union feature map at particular spatial locations. Intuitively, if the location A of the hand is in contact with the location B of the object, it would be useful to combine hand features at A with the object features at
B. We formalize this notion using a cross-feature afﬁnity-based attentional pooling module that can combine hand and hand-object union features from various locations based on the afﬁnities between them. Second, the hand-object union feature map encodes the regions between the hand and the object and can contain possible contact regions. We propose a spatial attention method to learn to focus on salient regions. Finally, we obtain contact state scores for each of the K hand-object pairs independently using the cross-feature afﬁnity-based attention module and spatial attention module.
The proposed attention modules are trained end-to-end together with the Contact-Estimation branch.
Another contribution of our paper is a large-scale dataset for development and evaluation. Our dataset consists of around 21K images, containing bounding box annotations for 58K hands and their physical contact states. The dataset contains many challenging images in the wild, where it is not trivial to determine the physical contact states of hands. This dataset can be used to develop real-world applications that require contact states of hands, such as contamination prevention and harassment detection. 2