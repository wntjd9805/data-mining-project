Abstract
Geometric methods rely on tensors that can be encoded using a symbolic formula and data arrays, such as kernel and distance matrices. We present an extension for standard machine learning frameworks that provides comprehensive support for this abstraction on CPUs and GPUs: our toolbox combines a versatile, transparent user interface with fast runtimes and low memory usage. Unlike general purpose acceleration frameworks such as XLA, our library turns generic Python code into binaries whose performances are competitive with state-of-the-art geometric libraries – such as FAISS for nearest neighbor search – with the added beneﬁt of ﬂexibility. We perform an extensive evaluation on a broad class of problems:
Gaussian modelling, K-nearest neighbors search, geometric deep learning, non-Euclidean embeddings and optimal transport theory. In practice, for geometric problems that involve 103 to 106 samples in dimension 1 to 100, our library speeds up baseline GPU implementations by up to two orders of magnitude. 1

Introduction
Fast numerical methods are the fuel of machine learning research. Over the last decade, the sustained development of the CUDA ecosystem has driven the progress in the ﬁeld: though Python is the lingua franca of data science and machine learning, most frameworks rely on efﬁcient C++ backends to leverage the computing power of GPUs [1, 86, 101]. Recent advances in computer vision or natural language processing attest to the ﬁtness of modern libraries: they stem from the mix of power and
ﬂexibility that is provided by PyTorch, TensorFlow and general purpose accelerators such as XLA.
Nevertheless, important work remains to be done. Geometric computations present a clear gap in performances between Python and C++: notable examples are implementations of point cloud convolutions or of the nearest neighbor search [65, 76]. To scale up geometric computations to real-world data, a common practice is therefore to replace the compute-intensive parts of a Python code by handcrafted CUDA kernels [35, 60, 92]. These are expensive to develop and maintain, which leads to an unfortunate need to compromise between ease of development and scalability.
To address this issue, we present KeOps: an extension for PyTorch, NumPy, Matlab and R that combines the speed of a handcrafted CUDA kernel with the simplicity of a high level language.
Our toolbox optimizes map-reduce operations on generalized point clouds and provides transparent support for distance-like matrices, as illustrated in Figure 1. The resulting computations are fully differentiable and have a negligible memory footprint. Their runtimes are competitive with state-of-the-art CUDA libraries when they exist, and peerless in the many use cases that are not covered by existing implementations. Our library ﬁts seamlessly within existing codebases and provides a sizeable performance boost to a wide range of methods. Among other applications, we present optimal transport solvers and geometric operators in hyperbolic spaces which are orders of magnitude faster than the state-of-the-art. We believe that our library is an important addition to the existing arsenal of tools and will have a stimulating impact on machine learning research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
M [ i , j ] (in, jn, Mn)
F ( xi , yj ) (a) Dense matrix (b) Sparse matrix (c) Symbolic matrix
Figure 1: Machine learning frameworks understand variables as matrices, also known as tensors.
M (a) These are usually dense and encoded as explicit numerical arrays (Mi,j) = (M [i, j]) that can have a large memory footprint. (b) Alternatively, some operators can be encoded as sparse matrices: we store in memory the indices (in, jn) and values Mn = Min,jn that correspond to a small number of non-zero coefﬁcients. Reduction operations are then implemented using indexing methods and scattered memory accesses. (c) We provide support for a third class of tensors: symbolic matrices whose coefﬁcients are given by a formula Mi,j = F (xi, yj) that is evaluated on data arrays (xi) and (yj). Reduction operations are implemented using parallel schemes that compute the coefﬁcients Mi,j on-the-ﬂy. We take advantage of the structure of CUDA registers to bypass costly memory transfers and achieve optimal runtimes on a wide range of applications.
RN 2
⇥ 2