Abstract
Stochastic Gradient Descent (SGD) has proven to be remarkably effective in optimizing deep neural networks that employ ever-larger numbers of parameters.
Yet, improving the efﬁciency of large-scale optimization remains a vital and highly active area of research. Recent work has shown that deep neural networks can be optimized in randomly-projected subspaces of much smaller dimensionality than their native parameter space. While such training is promising for more efﬁcient and scalable optimization schemes, its practical application is limited by inferior optimization performance.
Here, we improve on recent random subspace approaches as follows: Firstly, we show that keeping the random projection ﬁxed throughout training is detrimen-tal to optimization. We propose re-drawing the random subspace at each step, which yields signiﬁcantly better performance. We realize further improvements by applying independent projections to different parts of the network, making the approximation more efﬁcient as network dimensionality grows. To implement these experiments, we leverage hardware-accelerated pseudo-random number generation to construct the random projections on-demand at every optimization step, allowing us to distribute the computation of independent random directions across multiple workers with shared random seeds. This yields signiﬁcant reductions in memory and is up to 10 faster for the workloads in question.
× 1

Introduction
Despite signiﬁcant growth in the number of parameters used in deep learning networks, Stochastic
Gradient Descent (SGD) continues to be remarkably effective at ﬁnding minima of the highly over-parameterized weight space [9]. However, empirical evidence suggests that not all of the gradient directions are required to sustain effective optimization and that the descent may happen in much smaller subspaces [14]. Many methods are able to greatly reduce model redundancy while achieving high task performance at a lower computational cost [34, 29, 5, 36, 17, 15, 32, 21].
Notably, Li et al. [26] proposed a simple approach to both quantify and drastically reduce parameter redundancy by constraining the optimization to a (ﬁxed) randomly-projected subspace of much smaller dimensionality than the native parameter space. While the work demonstrated successful low-dimensional optimization, its inferior performance compared with standard SGD limits its practical application. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Here, we revisit optimization in low-dimensional random subspaces with the aim of improving its practical optimization performance. We show that while random subspace projections have computational beneﬁts such as easy distribution on many workers, they become less efﬁcient with growing projection dimensionality, or if the subspace projection is ﬁxed throughout training. We observe that applying smaller independent random projections to different parts of the network and re-drawing them at every step signiﬁcantly improves the obtained accuracy on fully-connected and several convolutional architectures, including ResNets on the MNIST, Fashion-MNIST and
CIFAR-10 datasets.1
Figure 1: Left: schematic illustration of random subspace optimization on a 3D loss landscape. At the point θ, the black arrow represents the direction of steepest descent computed by conventional SGD.
The colored arrow represents the direction of steepest descent under the constraint of being in the chosen lower dimensional random subspace (the green plane). Right: Validation accuracy (y) against epochs (x) for CIFAR-10 classiﬁcation of a D = 78, 330 parameter ResNet-8 using low-dimensional optimization with d = 500. Our method RBD improves upon the same-d FPD baseline [26] as well as black-box NES optimization [33] by 20.5% and 39.6% respectively. 2