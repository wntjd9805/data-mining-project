Abstract
Despite its high search efﬁciency, differential architecture search (DARTS) often selects network architectures with dominated skip connections which lead to per-formance degradation. However, theoretical understandings on this issue remain absent, hindering the development of more advanced methods in a principled way.
In this work, we solve this problem by theoretically analyzing the effects of various types of operations, e.g. convolution, skip connection and zero operation, to the network optimization. We prove that the architectures with more skip connections can converge faster than the other candidates, and thus are selected by DARTS.
This result, for the ﬁrst time, theoretically and explicitly reveals the impact of skip connections to fast network optimization and its competitive advantage over other types of operations in DARTS. Then we propose a theory-inspired path-regularized
DARTS that consists of two key modules: (i) a differential group-structured sparse binary gate introduced for each operation to avoid unfair competition among opera-tions, and (ii) a path-depth-wise regularization used to incite search exploration for deep architectures that often converge slower than shallow ones as shown in our theory and are not well explored during search. Experimental results on image classiﬁcation tasks validate its advantages. 1

Introduction
Network architecture search (NAS) [1] is an effective approach for automating network architecture design, with many successful applications witnessed to image recognition [2–6] and language modeling [1, 6]. The methodology of NAS is to automatically search for a directed graph and its edges from a huge search space. Unlike expert-designed architectures which require substantial efforts from experts by trial and error, the automatic principle in NAS greatly alleviates these design efforts and possible design bias brought by experts which could prohibit achieving better performance. Thanks to these advantages, NAS has been widely devised via reinforcement learning (RL) and evolutionary algorithm (EA), and achieved promising results in many applications, e.g. classiﬁcation [2, 4].
DARTS [6] is a recently developed leading approach. Different from RL and EA based methods [1–4] that discretely optimize architecture parameters, DARTS converts the operation selection for each edge in the directed graph into continuously weighting a ﬁxed set of operations. In this way, it can optimize the architecture parameters via gradient descent and greatly reduces the high search cost in
RL and EA approaches. However, as observed in the literatures [7–10] and Fig. 1 (a), this differential
NAS family, including DARTS and its variants [11, 12], typically selects many skip connections which dominate over other types of operations in the network graph. Consequently, the searched networks are observed to have unsatisfactory performance. To alleviate this issue, some empirical techniques are developed, e.g. operation-level dropout [7], fair operation-competing loss [8]. But no attention has been paid to developing theoretical understandings for why skip connections dominate other types of operations in DARTS. The theoretical answer to this question is important not only for better understanding DARTS, but also for inspiring new insights for DARTS algorithm improvement. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of selected normal cells by DARTS and PR-DARTS. By comparison, the group-structured sparse gates in PR-DARTS (b) well alleviate unfair operation competition and overcome the dominated-skip-connection issue in DARTS (a); path-depth-wise regularization in
PR-DARTS (c) helps rectify cell-selection-bias to shallow cells; PR-DARTS (d) combines these two complementary components and well alleviates the above two issues, testiﬁed by the results in (e).
Contributions. In this work, we address the above fundamental question and contribute to derive some new results, insights and alternatives for DARTS. Particularly, we provide rigorous theoretical analysis for the dominated skip connections in DARTS. Inspired by our theory, we then propose a new alternative of DARTS which can search networks without dominated skip connections and achieves state-of-the-art classiﬁcation performance. Our main contributions are highlighted below.
Our ﬁrst contribution is proving that DARTS prefers to skip connection more than other types of operations, e.g. convolution and zero operation, in the search phase, and tends to search favor skip-connection-dominated networks as shown in Fig. 1 (a). Formally, in the search phase, DARTS ﬁrst
ﬁxes architecture parameter β which determines the operation weights in the graph to optimize the network parameter W by minimizing training loss Ftrain(W, β) via gradient descent, and then uses the validation loss Fval(W, β) to optimize β via gradient descent. We prove that when optimizing
Ftrain(W, β), the convergence rate at each iteration depends on the weights of skip connections much heavier than other types of operations, e.g. convolution, meaning that the more skip connections the faster convergence. Since training and validation data come from the same distribution which means E[Ftrain(W, β)] = E[Fval(W, β)], more skip connections can also faster decay Fval(W, β) in expectation. So when updating architecture parameter β, DARTS will tune the weights of skip connections larger to faster decay validation loss, and meanwhile, will tune the weights of other operations smaller since all types of operations on one edge share a softmax distribution. Accordingly, skip connections gradually dominate the network graph. To our best knowledge, this is the ﬁrst theoretical result that explicitly shows heavier dependence of the convergence rate of NAS algorithm on skip connections, explaining the dominated skip connections in DARTS due to their optimization advantages.
Inspired by our theory, we further develop the path-regularized DARTS (PR-DARTS) as a novel alternative to alleviate unfair competition between skip connection and other types of operations in DARTS. To this end, we deﬁne a group-structured sparse binary gate implemented by Bernoulli distribution for each operation. These gates independently determine whether their corresponding operations are used in the graph. Then we divide all operations in the graph into skip connection group and non-skip connection group, and independently regularize the gates in these two groups to be sparse via a hard threshold function. This group-structured sparsity penalizes the skip connection group heavier than another group to rectify the competitive advantage of skip connections over other operations as shown in Fig. 1 (b), and globally and gradually prunes unnecessary connections in the search phase to reduce the pruning information loss after searching. More importantly, we introduce a path-depth-wise regularization which encourages large activation probability of gates along the long paths in the network graph and thus incites more search exploration to deep graphs illustrated by
Fig. 1 (c). As our theory shows that gradient descent can faster optimize shallow and wide networks than deep and thin ones, this path-depth-wise regularization can rectify the competitive advantage of shallow network over deep one. So PR-DARTS can search performance-oriented networks instead of fast-convergence-oriented networks and achieves better performance testiﬁed by Fig. 1 (e). 2