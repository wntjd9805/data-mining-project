Abstract
Hypernetworks are architectures that produce the weights of a task-speciﬁc primary network. A notable application of hypernetworks in the recent literature involves learning to output functional representations. In these scenarios, the hypernetwork learns a representation corresponding to the weights of a shallow MLP, which typically encodes shape or image information. While such representations have seen considerable success in practice, they remain lacking in the theoretical guar-antees in the wide regime of the standard architectures. In this work, we study wide over-parameterized hypernetworks. We show that unlike typical architectures, inﬁnitely wide hypernetworks do not guarantee convergence to a global minima un-der gradient descent. We further show that convexity can be achieved by increasing the dimensionality of the hypernetwork’s output, to represent wide MLPs. In the dually inﬁnite-width regime, we identify the functional priors of these architectures by deriving their corresponding GP and NTK kernels, the latter of which we refer to as the hyperkernel. As part of this study, we make a mathematical contribution by deriving tight bounds on high order Taylor expansion terms of standard fully connected ReLU networks. 1

Introduction
In this work, we analyze the training dynamics of over-parameterized meta networks, which are networks that output the weights of other networks, often referred to as hypernetworks. In the typical framework, a function h involves two networks, f and g. The hypernetwork f takes the input x (typically an image) and returns the weights of the primary network, g, which then takes the input z and returns the output of h.
The literature of hypernetworks is roughly divided into two main categories. In the functional representation literature [22, 32, 38, 29, 16] the input to the hypernetwork f is typically an image. For shape reconstruction tasks, the network g represents the shape via a signed distance ﬁeld, where the input are coordinates in 3D space. In image completion tasks, the inputs to
∗Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
g are image coordinates, and the output is the corresponding pixel intensity. In these settings, f is typically a large network and g is typically a shallow fully connected network.
In the second category [4, 23, 35, 43], hypernetworks are typically used for hyper-parameter search, where x is being treated as a hyperparameter descriptor and is optimized alongside with the network’s weights. In this paper, we consider models corresponding to the ﬁrst group of methods.
Following a prominent thread in the recent literature, our study takes place in the regime of wide networks. [11] recently showed that, when the width of the network approaches inﬁnity, the gradient-descent training dynamics of a fully connected network f can be characterized by a kernel, called the
Neural Tangent Kernel (or NTK for short). In other words, as the width of each layer approaches inﬁnity, provided with proper scaling and initialization of the weights, it holds that:
∂f (x; w)
∂w
·
∂(cid:62)f (x(cid:48); w)
∂w
→ Θf (x, x(cid:48)) (1) as the width, n of f tends to inﬁnity. Here, w are the weights of the network f (x; w). As shown in [11], as the width tends to inﬁnity, when minimizing the squared loss using gradient descent, the evolution through time of the function computed by the network follows the dynamics of kernel gradient descent with kernel Θf . To prove this phenomenon, various papers [20, 3, 2] introduce a
Taylor expansion of the network output around the point of initialization and consider its values. It is shown that the ﬁrst-order term is deterministic during the SGD optimization and the higher-order terms converge to zero as the width n tends to inﬁnity.
A natural question that arises when considering hypernetworks is whether a similar “wide” regime exists, where trained and untrained networks may be functionally approximated by kernels. If so, since this architecture involves two networks, the “wide” regime needs a more reﬁned deﬁnition, taking into account both networks.
Our contributions: 1. We show that inﬁnitely wide hypernetworks can induce highly non-convex training dynamics under gradient descent. The complexity of the optimization problem is highly dependent on the architecture of the primary network g, which may considerably impair the trainability of the architecture if not deﬁned appropriately. 2. However, when the widths of both the hypernetwork f and the primary network g tend to inﬁnity, the optimization dynamics of the hypernetwork simpliﬁes, and its neural tangent kernel (which we call the hyperkernel) has a well deﬁned inﬁnite-width limit governing the network evolution. 3. We verify our theory empirically and also demonstrate the utility of this hyperkernel on sev-eral functional representation tasks. Consistent with prior observations on kernel methods, the hypernetwork induced kernels also outperforms a trained hypernetwork when training data is small. 4. We make a technical contribution by deriving asymptotically tight bounds on high order Taylor expansion terms in ReLU MLPs. Our result partially settles a conjecture posed in [6] regarding the asymptotic behavior of general correlation functions. 1.1