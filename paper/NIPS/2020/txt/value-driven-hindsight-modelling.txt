Abstract
Value estimation is a critical component of the reinforcement learning (RL) paradigm. The question of how to effectively learn value predictors from data is one of the major problems studied by the RL community, and different approaches exploit structure in the problem domain in different ways. Model learning can make use of the rich transition structure present in sequences of observations, but this approach is usually not sensitive to the reward function. In contrast, model-free methods directly leverage the quantity of interest from the future, but receive a potentially weak scalar signal (an estimate of the return). We develop an approach for representation learning in RL that sits in between these two extremes: we propose to learn what to model in a way that can directly help value prediction.
To this end, we determine which features of the future trajectory provide useful information to predict the associated return. This provides tractable prediction targets that are directly relevant for a task, and can thus accelerate learning the value function. The idea can be understood as reasoning, in hindsight, about which aspects of the future observations could help past value prediction. We show how this can help dramatically even in simple policy evaluation settings. We then test our approach at scale in challenging domains, including on 57 Atari 2600 games. 1

Introduction
Consider a baseball player trying to perfect their pitch. The player performs an arm motion and releases the ball towards the batter, but suppose that instead of observing where the ball lands and the reaction of the batter, the player only hears the result of the play in terms of points or, worse, only the
ﬁnal result of the game. Improving their pitch from this experience appears hard and inefﬁcient, yet this is essentially the paradigm we employ when optimizing policies in model-free reinforcement learning. The scalar feedback that estimates the return from a state (and action), encoding how well things went, drives the learning while the accompanying observations that may explain that result (e.g.,
ﬂight path of the ball or the way the batter anticipated and struck the incoming baseball) are ignored.
To intuitively understand how such information could help value prediction, consider a simple discrete
Markov chain X → Y → Z, where Z is the scalar return and X is the observation from which we are trying to predict Z. If the space of possible values of Y is smaller than X, then it may be more efﬁcient to estimate P (Y |X) and P (Z|Y ) rather than directly estimating P (Z|X).1 In other words, observing and then predicting Y can be advantageous compared to directly estimating the signal of interest Z. Model-based RL approaches would duly exploit the observed Y (by modelling P (Y |X)), but Y would, in general scenarios, contain information that is irrelevant to Z and hard to predict.
Building a full high-dimensional predictive model to indiscriminately estimate all possible future observations, including potentially chaotic details of the ball trajectory and the spectators’ response, is a challenge that may not pay off if the task-relevant predictions (e.g., was the throw accepted, was 1See appendix A.1 for a worked out example. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the batter surprised) are error-ridden. Model-free RL methods directly focus only on the relation of
X to Z, rather than attempting to learn the full dynamics. These methods have recently dominated the literature, and have attained the best performance in a wide array of complex problems with high-dimensional observations [12, 17, 8, 7].
In this paper, we propose to augment model-free methods with a lightweight model of future quantities of interest. The motivation is to model only those parts of the future observations (Y ) that are needed to obtain better value predictions. The major research challenge is to learn, from observational data, which aspects of the future are important to model (i.e. what Y should be). To this end, we propose to learn a special value function in hindsight that receives future observations as an additional input.
This learning process reveals features of the future observations that would be most useful for value prediction (e.g., ﬂight path of the ball or reaction of the batter), if provided by an oracle. We then learn a model that predicts these hindsight features using only information available at test time (at the time of releasing the ball, we knew the identity of the batter, the type of throw and spin of the ball). Learning these value-relevant features can help representation learning for an agent and provide an additional useful input to its value and policy functions. Experimentally, we show that hindsight value functions surpassed model-free RL methods in a challenging association task (Portal Choice).
When added to the prior state-of-the-art model-free RL method for Atari games [11], hindsight value functions signiﬁcantly increased median performance, from 833% to 965%. 2