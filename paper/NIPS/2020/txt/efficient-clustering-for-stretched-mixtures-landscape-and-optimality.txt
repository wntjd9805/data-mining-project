Abstract
This paper considers a canonical clustering problem where one receives unlabeled samples drawn from a balanced mixture of two elliptical distributions and aims for a classiﬁer to estimate the labels. Many popular methods including PCA and k-means require individual components of the mixture to be somewhat spherical, and perform poorly when they are stretched. To overcome this issue, we propose a non-convex program seeking for an afﬁne transform to turn the data into a one-dimensional point cloud concentrating around −1 and 1, after which clustering becomes easy. Our theoretical contributions are two-fold: (1) we show that the non-convex loss function exhibits desirable geometric properties when the sample size exceeds some constant multiple of the dimension, and (2) we leverage this to prove that an efﬁcient ﬁrst-order algorithm achieves near-optimal statistical precision without good initialization. We also propose a general methodology for clustering with ﬂexible choices of feature transforms and loss objectives. 1

Introduction
Clustering is a fundamental problem in data science, especially in the early stages of knowledge discovery. In this paper, we consider a binary clustering problem where the data come from a mixture i=1 ⊆ Rd from the latent of two elliptical distributions. Suppose that we observe i.i.d. samples {Xi}n variable model
Xi = µ0 + µYi + Σ1/2Zi, (1)
Here µ0, µ ∈ Rd and Σ (cid:31) 0 are deterministic; Yi ∈ {±1} and Zi ∈ Rd are independent random quantities; P(Yi = −1) = P(Yi = 1) = 1/2, and Zi is an isotropic random vector whose distribution is spherically symmetric with respect to the origin. Xi is elliptically distributed (Fang et al., 1990) given Yi. The goal of clustering is to estimate {Yi}n i=1. Moreover, it is desirable to build a classiﬁer with straightforward out-of-sample extension that predicts labels for future samples. i=1 from {Xi}n i ∈ [n].
As a warm-up example, assume for simplicity that Zi has density and µ0 = 0. The Bayes-optimal classiﬁer is
ϕβ(cid:63) (x) = sgn(β(cid:63)(cid:62)x) = (cid:26) 1
−1 if β(cid:63)(cid:62)x ≥ 0 otherwise
, with any β(cid:63) ∝ Σ−1µ. A natural strategy for clustering is to learn a linear classiﬁer ϕβ(x) = sgn(β(cid:62)x) with discriminative coefﬁcients β ∈ Rd estimated from the samples. Note that
β(cid:62)Xi = (β(cid:62)µ)Yi + β(cid:62)Σ1/2Zi d= (β(cid:62)µ)Yi + (cid:112)
β(cid:62)ΣβZi, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1 Zi is the ﬁrst coordinate of Zi. The transformed data {β(cid:62)Xi}n i=1 are noisy i=1. A discriminative feature mapping x (cid:55)→ β(cid:62)x results in where Zi = e(cid:62) observations of scaled labels {(β(cid:62)µ)Yi}n high signal-to-noise ratio (β(cid:62)µ)2/β(cid:62)Σβ, turning the data into two well-separated clusters in R.
When the clusters are almost spherical (Σ ≈ I) or far apart ((cid:107)µ(cid:107)2 2 (cid:29) (cid:107)Σ(cid:107)2), the mean vector µ has reasonable discriminative power and the leading eigenvector of the overall covariance matrix
µµ(cid:62) + Σ roughly points that direction. This helps develop and analyze various spectral methods (Vempala and Wang, 2004; Ndaoud, 2018) based on Principal Component Analysis (PCA). k-means (Lu and Zhou, 2016) and its semideﬁnite relaxation (Mixon et al., 2017; Royer, 2017; Fei and Chen, 2018; Giraud and Verzelen, 2018; Chen and Yang, 2018) are also closely related. As they are built upon the Euclidean distance, a key assumption is the existence of well-separated balls each containing the bulk of one cluster. Existing works typically require (cid:107)µ(cid:107)2 2/(cid:107)Σ(cid:107)2 to be large under models like (1).
Yet, the separation is better measured by µ(cid:62)Σ−1µ, which always dominates (cid:107)µ(cid:107)2 2/(cid:107)Σ(cid:107)2. Those methods may fail when the clusters are separated but “stretched”. As a toy example, consider a 2 N (−µ, Σ) in R2 where µ = (1, 0)(cid:62) and the covariance matrix
Gaussian mixture 1
Σ = diag(0.1, 10) is diagonal. Then the distribution consists of two separated but stretched ellipses.
PCA returns the direction (0, 1)(cid:62) that maximizes the variance but is unable to tell the clusters apart.
To get high discriminative power under general conditions, we search for β that makes {β(cid:62)Xi}n concentrate around the label set {±1}, through the following optimization problem: 2 N (µ, Σ) + 1 i=1 min
β∈Rd n (cid:88) i=1 f (β(cid:62)Xi). (2)
Here f : R → R attains its minimum at ±1, e.g. f (x) = (x2 − 1)2. We name this method as
“Clustering via Uncoupled REgression”, or CURE for short. Here f penalizes the discrepancy between predictions {β(cid:62)Xi}n i=1. In the unsupervised setting, we have no access to the one-to-one correspondence but can still enforce proximity on the distribution level, i.e. i=1 and labels {Yi}n 1 n n (cid:88) i=1
δβ(cid:62)Xi ≈ 1 2
δ−1 + 1 2
δ1. (3)
A good approximate solution to (2) leads to |β(cid:62)Xi| ≈ 1. That is, the transformed data form two clusters around ±1. The symmetry of the mixture distribution automatically ensures balance between the clusters. Thus (2) is an uncoupled regression problem based on (3). Above we focus on the centered case (µ0 = 0) merely to illustrate main ideas. Our general methodology min
α∈R, β∈Rd (cid:26) 1 n n (cid:88) i=1 f (α + β(cid:62)Xi) + (α + β(cid:62) ˆµ0)2 (cid:27)
, 1 2 (4) where ˆµ0 = 1 n (cid:80)n i=1 Xi, deals with arbitrary µ0 by incorporating an intercept term α.
Main contributions. We propose a clustering method through (4) and study it under the model (1) without requiring the clusters to be spherical. Under mild assumptions, we prove that an efﬁcient algorithm achieves near-optimal statistical precision even in the absence of a good initialization.
• (Loss function design) We construct an appropriate loss function f by clipping the growth of the quartic function (x2 − 1)2/4 outside some interval centered at 0. As a result, f has two “valleys” at ±1 and does not grow too fast, which is beneﬁcial to statistical analysis and optimization.
• (Landscape analysis) We characterize the geometry of the empirical loss function when n/d exceeds some constant.
In particular, all second-order stationary points, where the smallest eigenvalues of Hessians are not signiﬁcantly negative, are nearly optimal in the statistical sense.
• (Efﬁcient algorithm with near-optimal statistical property) We show that with high probability, a perturbed version of gradient descent algorithm starting from 0 yields a solution with near-optimal statistical property after ˜O(n/d + d2/n) iterations (up to polylogarithmic factors).
The formulation (4) is uncoupled linear regression for binary clustering. Beyond that, we introduce a uniﬁed framework which learns feature transforms to identify clusters with possibly non-convex shapes. That provides a principled way of designing ﬂexible unsupervised learning algorithms.
We introduce the model and methodology in Section 2, conduct theoretical analysis in Section 3, present numerical results in Section 4, and ﬁnally conclude the paper with a discussion in Section 5. 2