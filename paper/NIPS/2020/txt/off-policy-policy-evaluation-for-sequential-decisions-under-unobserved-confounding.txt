Abstract
When observed decisions depend only on observed features, off-policy policy evalu-ation (OPE) methods for sequential decision problems can estimate the performance of evaluation policies before deploying them. However, this assumption is fre-quently violated due to unobserved confounders, unrecorded variables that impact both the decisions and their outcomes. We assess robustness of OPE methods under unobserved confounding by developing worst-case bounds on the performance of an evaluation policy. When unobserved confounders can affect every decision in an episode, we demonstrate that even small amounts of per-decision confounding can heavily bias OPE methods. Fortunately, in a number of important settings found in healthcare, policy-making, and technology, unobserved confounders may directly affect only one of the many decisions made, and inﬂuence future decisions/rewards only through the directly affected decision. Under this less pessimistic model of one-decision confounding, we propose an efﬁcient loss-minimization-based proce-dure for computing worst-case bounds, and prove its statistical consistency. On simulated healthcare examples—management of sepsis and interventions for autis-tic children—where this is a reasonable model, we demonstrate that our method invalidates non-robust results and provides meaningful certiﬁcates of robustness, allowing reliable selection of policies under unobserved confounding. 1

Introduction
New technology and regulatory shifts have allowed the collection of vast amounts of data on sequential trajectories of past decisions and associated rewards, ranging from healthcare decisions and outcomes to product recommendations and purchase histories. This presents unique opportunities for using off-policy methods to inform better sequential decision-making. Leveraging prior data to evaluate the performance of a sequential decision policy (which we call the evaluation policy) before deploying it can reduce the need for online experimentation when doing so is expensive or risky.
A central challenge in off-policy policy evaluation (OPE) is that the estimand is inherently counter-factual: what would the rewards be if an alternate policy had been used (the counterfactual) instead of the behavior policy that generated the observed data (the factual). As a result, OPE requires causal reasoning about whether observed rewards were caused by observed decisions, or by a common
⇤Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
causal variable that simultaneously affects observed decisions and states / rewards [12]. In order to make counterfactual evaluations possible, a standard assumption—albeit often overlooked and unstated—is to require that the behavior policy does not depend on any unobserved variables that also affect the future states/rewards (no unobserved confounding). We refer to this assumption as sequential ignorability, following the line of works on dynamic treatment regimes [41, 36].
Sequential ignorability, however, is often violated in OPE problems where the behavior policy is unknown. In medicine, business operations, and automated systems in tech, decisions depend on unlogged features correlated with future outcomes. As an example, clinicians use visual observations or discussions with patients to inform treatment, but such information is typically not recorded; they also may rely on heuristics that are hard to quantify, and can over-extrapolate from past experience [34]. In this paper, we study a framework for quantifying the impact of unobserved confounders on OPE estimates, developing worst-case bounds on the performance of an evaluation policy. OPE estimates are often used to inform policy selection, and we are particularly interested in methods that can guide when we may be conﬁdent (or not) that an alternate decision policy should be preferred. Since OPE is generally impossible under arbitrary unobserved confounding, we begin by positing a model that explicitly limits their inﬂuence on decisions. Our proposed model is a natural extension of an inﬂuential confounding model for a single binary decision [45] to the multi-action sequential decision making setting. When unobserved confounders can affect all decisions, we illustrate in Section 3 that even small amounts of confounding can have an exponential (in the number of decisions) impact on the bias of OPE. In this sense, the accuracy of OPE can be highly unreliable under the presence of unobserved confounding that affect all decisions in multi-step horizon problems.
Fortunately, in a number of important applications, unobserved confounders may only directly affect a single decision, and inﬂuence future decisions/rewards only through this directly affected decision.
As we detail shortly, in healthcare this happens when a high-level expert makes an initial decision potentially using unrecorded information, after which a standard set of protocols are followed based on recorded inputs. In ﬁnancial services, this happens when humans initially screen new clients for fraud, after which decisions are made based on standard logged features. In order to evaluate new policies (e.g. fully automated systems), we need to account for unobserved confounding in the initial human-conducted screening decisions; in this scenario, the unobserved features affect subsequent decisions and rewards only through the initial decision and outcome. In other instances, it may be the case that an unobserved confounder at a particular time step is observed in the next period.
Under our less pessimistic model of single-decision confounding, we develop bounds on the expected cumulative rewards under the evaluation policy. We use functional convex duality to derive a dual relaxation, and show that it can be computed by solving a loss minimization problem. The single-decision confounding model allows us to efﬁciently evaluate these bounds even for continuous states, unlike the general case which requires solving an intractable nonconvex problem over likelihood ratios (which are inﬁnite dimensional for continuous states). We prove the empirical approximation of our procedure is consistent, allowing estimation from observed past decisions.
The single-decision confounding model may not fully describe scenarios where unobserved con-founders affect decisions through multiple periods. Our sensitivity analysis method is nevertheless a meaningful tool even in such scenarios, as certifying the robustness of OPE against single-decision confounding is a necessary condition for the conclusion of OPE to withstand multi-decision con-founding. As we present in the sequel, we observe conclusions of OPE are often invalidated even under less conservative single-decision confounding, raising substantial concern for robustness of
OPE under violations of sequential ignorability.
On examples of dynamic treatment regimes for autism and sepsis management, we illustrate how our single-decision confounding model allows informative bounds over meaningful amounts of confounding. Our approach provides certiﬁcates of robustness by identifying the level of unobserved confounding at which the potential bias in OPE estimates raise concerns about the validity of selecting the best policy among a set of candidates. Compared to our informative bounds, the naïve approach is prohibitively conservative and lose robustness certiﬁcates for even negligible amounts of confounding.