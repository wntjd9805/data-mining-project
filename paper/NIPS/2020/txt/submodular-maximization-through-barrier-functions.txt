Abstract
In this paper, we introduce a novel technique for constrained submodular maxi-mization, inspired by barrier functions in continuous optimization. This connec-tion not only improves the running time for constrained submodular maximization but also provides the state of the art guarantee. More precisely, for maximizing a monotone submodular function subject to the combination of a k-matchoid and (cid:96)-knapsack constraints (for (cid:96) k), we propose a potential function that can be ap-proximately minimized. Once we minimize the potential function up to an ε error, it is guaranteed that we have found a feasible set with a 2(k+1+ε)-approximation factor which can indeed be further improved to (k+1+ε) by an enumeration tech-nique. We extensively evaluate the performance of our proposed algorithm over several real-world applications, including a movie recommendation system, sum-marization tasks for YouTube videos, Twitter feeds and Yelp business locations, and a set cover problem.
≤ 1

Introduction
In the constrained continuous optimization, barrier functions are usually used to impose an increas-ingly large cost on a feasible point as it approaches the boundary of the feasible region [40]. In effect, barrier functions replace constraints by a penalizing term in the primal objective function so that the solution stays away from the boundary of the feasible region. This is an attempt to approximate a constrained optimization problem with an unconstrained one and to later apply standard optimiza-tion techniques. While the beneﬁts of barrier functions are studied extensively in the continuous domain [40], their use in discrete optimization is not very well understood.
In this paper, we show how discrete barrier functions manifest themselves in constrained submod-ular maximization. Submodular functions formalize the intuitive diminishing returns condition, a property that not only allows optimization tractability but also appears in many machine learning applications, including video, image, and text summarization [16, 31, 45], active set selection in non-parametric learning [34], sensor placement and information gathering [14]. Formally, for a ground set and every element e function f is monotone if for all A
R≥0 is submodular if for all sets A e f (A)
, a non-negative set function f : 2N e
⊂ N f (B). The submodular
∪ {
B we have f (A)
B, we have f (A
→
)
−
}
≥ f (B).
∈ N \ f (B
)
}
∪ {
N
−
⊆
B
⊆
≤
The celebrated results of Nemhauser et al. [39] and Fisher et al. [12] show that the vanilla greedy algorithm provides an optimal approximation guarantee for maximizing a monotone submodular function subject to a cardinality constraint. However, the performance of the greedy algorithm de-grades as the feasibility constraint becomes more complex. For instance, the greedy algorithm does 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
not provide any constant factor approximation guarantee if we replace the cardinality constraint with a knapsack constraint. Even though there exist many works that achieve the tight approxima-tion guarantee for maximizing a monotone submodular function subject to multiple knapsack con-straints, the running time of these algorithms is prohibitive as they either rely on enumerating large sets or running the continuous greedy algorithm. In contrast, we showcase a fundamentally new optimization technique through a discrete barrier function minimization in order to efﬁciently handle knapsack constraints and develop fast algorithms. More formally, we consider the following constrained submodular maximization problem deﬁned over the ground set
:
S∗ = argmax
S⊆N , S∈I ci(S)≤1 ∀ i∈[(cid:96)] f (S) ,
N (1)
) (a general subclass of k-where the constraint is the intersection of a k-matchoid constraint set systems) and (cid:96) knapsacks constraints ci (for i
[(cid:96)]). We assume that both the objective function
∈ f and the constraints are accessed through a value oracle and a membership oracle, respectively [5].
We measure the complexity of an algorithm by the number of value oracle queries it performs, as in most cases, value oracle queries dominate the actual time complexity of the algorithm [5, 26]. (
M
N
I
,
Contributions. We propose two algorithms for maximizing a monotone submodular function sub-ject to the intersection of a k-matchoid and (cid:96) knapsack constraints. Our approach uses a novel barrier function technique and lies in between fast thresholding algorithms with suboptimal ap-proximation ratios and slower algorithms that use continuous greedy and rounding methods. The
ﬁrst algorithm, BARRIER-GREEDY, obtains a 2(k + 1 + ε)-approximation ratio with ˜O(nr + r3) value oracle calls, where r is the maximum cardinality of a feasible solution.1 The second algo-rithm, BARRIER-GREEDY++, obtains a better approximation ratio of (k + 1 + ε), but at the cost of ˜O(n3r + n2r3) value oracle calls. BARRIER-GREEDY is theoretically fast and even exhibits better performance in practice while achieving a near-optimal approximation ratio. The main pur-pose of BARRIER-GREEDY++ is to provide a tighter approximation guarantee at the expense of a higher computational cost. Indeed, there is a trade-off between the quality of the solution we desire to obtain (higher for BARRIER-GREEDY++) and the time we are willing to spend (faster for BARRIER-GREEDY). In our experiments, we opted for BARRIER-GREEDY which is the more scalable method. Our results show that barrier function minimization techniques provide a versa-tile algorithmic tool for constrained submodular optimization with strong theoretical guarantees that may scale to many previously intractable problem instances. We demonstrate the practical effective-ness of our algorithms over several real-world machine learning applications, including a movie recommendation system, summarization tasks for YouTube videos, Twitter feeds of news agencies and Yelp business locations, and a set cover problem. Proofs are deferred to the Supplementary
Material. 2