Abstract
Recent semantic segmentation methods model the relationship between pixels to construct the contextual representations. In this paper, we introduce the Region
Attention Network (RANet), a novel attention network for modeling the relationship between object regions. RANet divides the image into object regions, where we select the representative information. In contrast to the previous methods,
RANet conﬁgures the information pathways between the pixels in different regions, enabling the region interaction to exchange the regional context for enhancing all of the pixels in the image. We train the construction of object regions, the selection of the representative regional contents, the conﬁguration of information pathways and the context exchange between pixels, jointly, to improve the segmentation accuracy.
We extensively evaluate our method on the challenging segmentation benchmarks, demonstrating that RANet effectively helps to achieve the state-of-the-art results.
Code will be available at: https://github.com/dingguo1996/RANet. 1

Introduction
Recent success of semantic segmentation lies on the deep networks [1, 2, 3] that learn powerful visual representations from the large-scale datasets [4, 5, 6]. The latest segmentation methods [7, 8, 9, 10, 11, 12] model the spatial and category relationship between pixels. They provide rich context for enhancing the representations of pixels and improving the segmentation accuracy.
The spatial pyramid pooling (SPP) and attention mechanism are two of the popular methods, which have been vastly used for constructing the contextual representations. SPP [7, 13, 14, 15] uses various sizes of regular convolutional/pooling kernels to capture the contextual information in different ranges. The attentional models [9, 10, 11, 16] exchange context between each pair of pixels by respecting the dependency between categories. These methods focus on modeling either the spatial or the category relationship between pixels. However, rather than the pixel-level relationship, the semantic segmentation task heavily relies on the understanding of the object-level relationship that provides richer context for classifying the pixels.
In this paper, we advocate the idea of using the object regions for constructing the regional context, which models the object-level relationship. Here, the region refers to an object or an object part.
Each object can be regarded as a region that consists of a set of nearby pixels belonging to the same category. Intuitively, the boundaries of the object regions provide the spatial relationship between the objects. In the same object region, the pixels contain the consistent category information. We resort
∗The ﬁrst two authors share the contribution equally.
†Di Lin is the corresponding author of this paper. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: RCB (a) constructs a region decision map for each pixel (the red dot), based on the boundary and semantic score maps. RIB (b) selects the representative pixels (the larger dots) in different regions (the yellow, green, blue and purple regions). It enables the region interaction to aggregate the global context, which enhances the pixels (the smaller dots) to produce the contextual feature map. For simplicity, we show only one representative pixel in each object region. to these nice properties of the object regions and construct the regional context, which enhances the pixel representations and eventually improves the segmentation performance.
Speciﬁcally, we propose the Region Attention Network (RANet), which consists of the novel network components for constructing the contextual representations. As illustrated in Figure 1(a), RANet employs the Region Construction Block (RCB) to jointly analyze the boundary score map and the semantic score maps. It computes the region attention score for each pair of pixels in the image. The high attention score means that the pixels belong to the same object region. Based on the attention scores, RCB computes the region decision map for each pixel (red dot). It uses the region decision maps for dividing the image into different object regions.
Next, the region decision maps are passed to the Region Interaction Block (RIB). As illustrated in
Figure 1(b), RIB selects the representative pixels (the larger dots) in different regions. Within the same region, each representative pixel receives the context from other pixels. It allows the representative pixels to effectively represent the local content of the object region. Moreover, RIB communicates the representative pixels in different regions, comprehensively capturing the spatial and category relationship between different object regions. RIB yields the global contextual representation to augment the pixels (the smaller dots), ﬁnally forming the contextual feature map for segmentation.
We evaluate RANet on the challenging segmentation datasets. We achieve 84.0, 54.9 and 40.7 mean
Intersection-over-Unions (IoUs) on the Cityscape test set [4], PASCAL Context validation set [5] and COCO-Stuff validation set [6], respectively. These results demonstrate the effectiveness of all of the network components (i.e., RCB and RIB), which improve the segmentation accuracy and help to surpass the state-of-the-art methods. 2