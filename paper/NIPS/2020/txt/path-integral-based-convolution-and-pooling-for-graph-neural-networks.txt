Abstract
Graph neural networks (GNNs) extends the functionality of traditional neural networks to graph-structured data. Similar to CNNs, an optimized design of graph convolution and pooling is key to success. Borrowing ideas from physics, we propose a path integral based graph neural networks (PAN) for classiﬁcation and regression tasks on graphs. Speciﬁcally, we consider a convolution operation that involves every path linking the message sender and receiver with learnable weights depending on the path length, which corresponds to the maximal entropy random walk. It generalizes the graph Laplacian to a new transition matrix we call maximal entropy transition (MET) matrix derived from a path integral formalism.
Importantly, the diagonal entries of the MET matrix are directly related to the subgraph centrality, thus lead to a natural and adaptive pooling mechanism. PAN provides a versatile framework that can be tailored for different graph data with varying sizes and structures. We can view most existing GNN architectures as special cases of PAN. Experimental results show that PAN achieves state-of-the-art performance on various graph classiﬁcation/regression tasks, including a new benchmark dataset from statistical mechanics we propose to boost applications of
GNN in physical sciences. 1

Introduction
The triumph of convolutional neural networks (CNNs) has motivated researchers to develop similar architectures for graph-structured data. The task is challenging due to the absence of regular grids.
One notable proposal is to deﬁne convolutions in the Fourier space [12, 11]. This method relies on
ﬁnding the spectrum of the graph Laplacian I − D−1A or I − D− 1 2 and then applies ﬁlters to 2 AD− 1
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the components of input signal X under the corresponding basis, where A is the adjacency matrix of the graph, and D is the corresponding degree matrix. Due to the high computational complexity of diagonalizing the graph Laplacian, people have proposed many simpliﬁcations [17, 34].
The graph Laplacian based methods essentially rely on message passing [27] between directly connected nodes with equal weights shared among all edges, which is at heart a generic random walk (GRW) deﬁned on graphs. It can be seen most obviously from the GCN model [34], where the normalized adjacency matrix is directly applied to the left-hand side of the input. In statistical physics, D−1A is known as the transition matrix of a particle doing a random walk on the graph, where the particle hops to all directly connected nodes with equiprobability. Many direct space-based methods [28, 40, 54, 63] can be viewed as generalizations of GRW, but with biased weights among the neighbors.
In this paper, we go beyond the GRW picture, where information necessarily dilutes when a path branches, and instead consider every path linking the message sender and receiver as the elemental unit in message passing. Inspired by the path integral formulation developed by Feynman [24, 23], we propose a graph convolution that assigns trainable weights to each path depending on its length.
This formulation results in a maximal entropy transition (MET) matrix, which is the counterpart of graph Laplacian in GRW. By introducing a ﬁctitious temperature, we can continuously tune our model from a fully localized one (MLP) to a spectrum based model. Importantly, the diagonal of the MET matrix is intimately related to the subgraph centrality, and thus provides a natural pooling method without extra computations. We call this complete path integral based graph neural network framework PAN.
We demonstrate that PAN outperforms many popular architectures on benchmark datasets. We also introduce a new dataset from statistical mechanics, which overcomes the lack of explanability and tunability of many previous ones. The dataset can serve as another benchmark, especially for boosting applications of GNN in physical sciences. This dataset again conﬁrms that PAN has a faster convergence rate, higher prediction accuracy, and better stability compared to many counterparts. 2 Path Integral Based Graph Convolution
Path integral and MET matrix Feynman’s path integral formulation [24, 69] interprets the proba-bility amplitude φ(x, t) as a weighted average in the conﬁguration space, where the contribution from
φ0(x) is computed by summing over the inﬂuences (denoted by eiS[x, ˙x]) from all paths connecting itself and φ(x, t). This formulation has been later extensively used in statistical mechanics and stochastic processes [35]. We note that this formulation essentially constructs a convolution by considering the contribution from all possible paths in the continuous space. Using this idea, but
Figure 1: A schematic analogy between the original path integral formulation in continuous space (left) and the discrete version for a graph (right). Symbols are deﬁned in the text. modiﬁed for discrete graph structures, we can heuristically propose a statistical mechanics model on how information is shared between different nodes on a given graph. In the most general form, we write observable φi at the i-th node for a graph with N nodes as
φi = 1
Zi
N (cid:88) j=1
φj (cid:88) e− E[l]
T ,
{l|l0=i,l|l|=j} (1) where Zi is the normalization factor known as the partition function for the i-th node. Here a path l is a sequence of connected nodes (l0l1 . . . l|l|) where Alili+1 = 1, and the length of the path is denoted 2
by |l|. In Figure 1 we draw the analogy between our discrete version and the original formulation.
It is straightforward to see that the integral should now be replaced by a summation, and φ0(x) only resides on nodes. Since a statistical mechanics perspective is more proper in our case, we directly change the exponential term, which is originally an integral of Lagrangian, to a Boltzmann’s factor with ﬁctitious energy E[l] and temperature T (we choose Boltzmann’s constant kB = 1).
Nevertheless, we still exploit the fact that the energy is a functional of the path, which gives us a way to weight the inﬂuence of other nodes through a certain path. The ﬁctitious temperature controls the excitation level of the system, which reﬂects that to what extent information is localized or extended.
In practice, there is no need to learn the ﬁctitious temperature or energy separately, instead the neural networks can directly learn the overall weights, as will be made clearer later.
To obtain an explicit form of our model, we now introduce some mild assumptions and simpliﬁcations.
Intuitively, we know that information quality usually decays as the path between the message sender and the receiver becomes longer, thus it is reasonable to assume that the energy is not only a functional of path, but can be further simpliﬁed as a function that solely depends on the length of the path.
Clearly, in principle one can incorporate information of individual edges, such as replacing the energy with a neural network that takes the nodes on a path as input. Therefore, the constant form used here should only be understood as the easiest implementation of our general framework. In the random walk picture, this simpliﬁcation means that the hopping is equiprobable among all the paths that have the same length, which maximizes the Shannon entropy of the probability distribution of paths globally, and thus the random walk is given the name maximal entropy random walk [13]. 2 By ﬁrst conditioning on the length of the path, we can introduce the overall n-th layer weight k(n; i) for node i by k(n; i) = 1
Zi
N (cid:88) j=1 g(i, j; n)e− E(n)
T , (2) where g(i, j; n) denotes the number of paths between nodes i and j with length of n, or density of states for the energy level E(n) with respect to nodes i and j, and the summation is taken over all nodes of the graph. Intuitively, node j with larger g(i, j; n) means that it has more channels to talk with node i, thus may impose a greater inﬂuence on node i as the case in our formulation.
For example, in Figure 1, nodes B and C are both two-step away from A, but B has more paths connecting A and would be assigned with a larger weight as a consequence. Presumably, the energy
E(n) is an increasing function of n, which leads to a decaying weight as n increases.3 By applying a cutoff of the maximal path length L, we exchange the summation order in (1) to obtain
φi =
L (cid:88) n=0 k(n; i)
N (cid:88) j=1 g(i, j; n) s=1 g(i, s; n) (cid:80)N
φj = 1
Zi
L (cid:88) n=0 e− E(n)
T
N (cid:88) j=1 g(i, j; n)φj, (3) where the partition function can be explicitly written as
N (cid:88)
L (cid:88)
Zi = e− E(n)
T g(i, j; n). (4) n=0 j=1
A nice property of this formalism is that we can easily compute g(i, j; n) by raising the power of the adjacency matrix A to n, which is a well-known property of the adjacency matrix from graph theory, i.e., g(i, j; n) = An ij. Plug in (3) we now have a group of self-consistent equations governed by a transition matrix M (a counterpart of the propagator in quantum mechanics), which can be written in the following compact form
M = Z −1
L (cid:88) e− E(n)
T An, (5) n=0 where diag(Z)i = Zi. We call the matrix M maximal entropy transition (MET) matrix, with regard to the fact that it realizes maximal entropy under the microcanonical ensemble. This transition matrix replaces the role of the graph Laplacian under our framework. 2For a weighted graph, a feasible choice for the functional form of the energy could be E(leﬀ ), where the effective length of the path leﬀ can be deﬁned as a summation of the inverse of weights along the path, i.e. leﬀ = (cid:80)|l|−1 i=0 1/wlili+1 . 3This does not mean that k(n; i) must necessarily be a decreasing function, as g(i, j; n) grows exponentially in general. It would be valid to apply a cutoff as long as E(n) (cid:29) nT ln λ1 for large n, where λ1 is the largest eigenvalue of the adjacency matrix A. 3
More generally, one can constrain the paths under consideration to, for example, shortest paths or self-avoiding paths. Consequentially, g(i, j; n) will take more complicated forms and the matrix An needs to be modiﬁed accordingly. In this paper, we focus on the simplest scenario and apply no constraints for the simplicity of the discussion.
PAN convolution The eigenstates, or the basis of the system {ψi} satisfy M ψi = λiψi. Similar to the basis formed by the graph Laplacian, one can deﬁne graph convolution based on the spectrum of
MET matrix, which now has a distinct physical meaning. However, it is computationally impractical to diagonalize M in every iteration as it is updated. To reduce the computational complexity, we apply the trick similar to GCN [34] by directly multiplying M to the left hand side of the input and accompanying it by another weight matrix W on the right-hand side. The convolutional layer is then reduced to a simple form
X (h+1) = M (h)X (h)W (h), (6) where h refers to the layer number. Applying M to the input X is essentially a weighted average among neighbors of a given node, which leads to the question that if the normalization consistent with the path integral formulation works best in a data-driven context. It has been consistently shown experimentally that a symmetric normalization usually gives better results [34, 41, 43]. This observation might have an intuitive explanation. Most generally, one can consider the normalization
Z −θ1 · Z −θ2, where θ1 + θ2 = 1. There are two extreme situations. When θ1 = 1 and θ2 = 0, it is called random-walk normalization and the model can be understood as “receiver-controlled", in the sense that the node of interest performs an average among all the neighbors weighted by the number of channels that connect them. On the contrary, when θ1 = 0 and θ2 = 1, the model becomes
“sender-controlled", since the weight is determined by the fraction of the ﬂow coming out from the sender that is directed to the receiver. Because of the fact that for an undirected graph, the exact interaction between connected nodes are unknown, as a compromise, the symmetric normalization can outperform both extremes, even it may not be the optimal. This consideration leads us to a ﬁnal perfection step that changes the normalization Z −1 in M to the symmetric normalized version. The convolutional layer then becomes
X (h+1) = M (h)X (h)W (h) = Z −1/2
L (cid:88) n=0
We shall call this graph convolution PANConv. e− E(n)
T AnZ −1/2X (h)W (h). (7)
The optimal cutoff L of the series depends on the intrinsic properties of the graph, which is represented by temperature T . Incorporating more terms is analogous to having more particles excited to the higher energy level at a higher temperature. For instance, in low-temperature limit, L = 0, the model is reduced to the MLP model. In the high-temperature limit, all factors exp(−E(n)/T ) are effectively one, and the term with the largest power dominates the summation. We can see it by noticing
An = (cid:80)N i , where λ1, . . . , λN is sorted in a descending order. By the Perron-Frobenius theorem, we may only keep the leading order term with the unique largest eigenvalue λ1 when n → ∞. We then reach a prototype of the high temperature model X (h+1) = (I + ψ1ψT 1 )X (h)W (h).
The most suitable choice of the cutoff L reﬂects the intrinsic dynamics of the graph. i ψiψT i=1 λn 3 Path Integral Based Graph Pooling
For graph classiﬁcation and regression tasks, another critical component is the pooling mechanism, which enables us to deal with graph input with variable sizes and structures. Here we show that the
PAN framework provides a natural ranking of node importance based on the MET matrix, which is intimately related to the subgraph centrality. This pooling scheme, denoted by PANPool, requires no further work aside from the convolution and can discover the underlying local motif adaptively.
MET matrix and subgraph centrality Many different ways to rank the “importance" of nodes in a graph have been proposed in the complex networks community. The most straightforward one is the degree centrality (DC), which counts the number of neighbors, other more sophisticated measures include, for example, betweenness centrality (BC) and eigenvector centrality (EC) [45]. Although these methods do give speciﬁc measures of the global importance of the nodes, they usually fail to pick up local patterns. However, from the way CNNs work on image classiﬁcations, we know that it is the locally representative pixels that matter. 4
Estrada and Rodriguez-Velazquez [21] have shown that subgraph centrality is superior to the methods mentioned above in detecting local graph motifs, which are crucial to the analysis of many social and biological networks. The subgraph centrality computes a weighted sum of the number of self-loops with different lengths. Mathematically, it simply writes as (cid:80)∞ k=0(Ak)ii/k! for node i. Interestingly, one immediately sees that the resemblance of this expression and the diagonal elements of the MET matrix. The difference is easy to explain. The summation in the MET matrix is truncated at maximal length L, and the weights for different path length e is learnable. In contrast, the predetermined weight 1/k! is a convenient choice to ensure the convergence of the summation and an analytical form of the result, which writes (cid:80)N j (i)eλj , where vj(i) is the i-th element of the orthonormal j=1 v2 basis associated with the eigenvalue λj.
E(n)
T
Now it becomes clear that the MET matrix not only plays the role of a path integral-based convolution, its diagonal elements Mii also automatically provides a measure of the importance of node i, thus enabling a pooling mechanism by sorting Mii. Importantly, this pooling method has three main merits compared to the subgraph centrality. First, we can exploit the readily-computed MET matrix, thus circumvent extra computations, especially the direct diagonalization of the adjacency matrix in the case of subgraph centrality. Second, the weights are data-driven rather than predetermined, which can effectively adapt to different inputs. Furthermore, the MET matrix is normalized 4, which adds weights on the local importance of the nodes, and can potentially avoid clustering around “hubs" that are commonly seen in real-world “scale-free" networks [8].
The PAN Pooling strategy has similar physical explanations as the PAN convolution. In the low-temperature limit, for example, if we set the cutoff at L = 2, the rank of (cid:80)L ii is of the same order as the rank of degrees, and thus we recover the degree centrality. In the high-temperature limit, as n → ∞, the sum is dominated by the magnitude of the i-th element of the orthonormal basis associated with the largest eigenvalue of A, thus the corresponding ranking is reduced to the ranking of the eigenvector centrality. By tuning L, PANPool provides a ﬂexible strategy that can adapt to the
“sweet spot" of the input. n=0 e
T An
E(n)
To better understand the effect of the proposed method, in Figure 2, we visualize the top 20% nodes by different measures of node importance of a connected point pattern called RSA, which we detail in
Section 5.2. It is noteworthy that while DC selects points relatively uniform, the result of EC is highly concentrated. This phenomenon is analogous to the contrast between the rather uniform diffusion in the classical picture and the Anderson localization [5] in the quantum mechanics of disordered systems [13]. In this sense, it tries to ﬁnd a “mesoscopic" description that best ﬁts the structure of input data. Importantly, we note that the unnormalized MET matrix tends to focus on the densely connected areas or hubs. In contrast, the normalized one tends to choose the locally representative nodes and leave out the equally well-connected nodes in the hubs. This observation leads us to propose an improved pooling strategy that balances the inﬂuencers at both the global and local levels.
Figure 2: Top 20% nodes (shown in blue) by different measures of node importance of an RSA pattern from PointPattern dataset. From left to right are results from: Degree Centrality, Eigenvector
Centrality, MET matrix without normalization, MET matrix and Hybrid PANPool.
Hybrid PANPool To combine the contribution of the local motifs and the global importance, we propose a hybrid PAN pooling (still referred as PANPool for simplicity) using a simple linear model.
The global importance can be represented by, but not limited to the strength of the input signal X itself. More precisely, we project feature X ∈ RN ×d by a trainable parameter vector p ∈ Rd and 4Notice that unlike the case in convolutions, the normalization being symmetric or not does not matter here.
For pooling, we only care about the diagonal terms, and different normalization methods will give the same result. 5
combine it with the diagonal diag(M ) of the MET matrix to obtain a score vector score = Xp + βdiag(M ). (8)
Here β is a real learnable parameter that controls the emphasis on these two potentially competing factors. PANPool then selects a fraction of the nodes ranked by this score (number denoted by K), and outputs the pooled feature array (cid:101)X ∈ RK×d and the corresponding adjacency matrix (cid:101)A ∈ RK×K.
This new node score in (8) has jointly considered both node features (at global level) and graph structures (at local level). In Figure 2, PANPool tends to select nodes that are both important locally and globally. We also tested alternative designs under the same consideration, see supplementary material for details. 4