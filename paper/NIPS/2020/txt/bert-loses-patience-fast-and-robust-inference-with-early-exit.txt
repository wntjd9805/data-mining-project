Abstract
In this paper, we propose Patience-based Early Exit, a straightforward yet effective inference method that can be used as a plug-and-play technique to simultaneously improve the efﬁciency and robustness of a pretrained language model (PLM). To achieve this, our approach couples an internal-classiﬁer with each layer of a PLM and dynamically stops inference when the intermediate predictions of the internal classiﬁers remain unchanged for a pre-deﬁned number of steps. Our approach improves inference efﬁciency as it allows the model to make a prediction with fewer layers. Meanwhile, experimental results with an ALBERT model show that our method can improve the accuracy and robustness of the model by preventing it from overthinking and exploiting multiple classiﬁers for prediction, yielding a better accuracy-speed trade-off compared to existing early exit methods.2 1

Introduction
In Natural Language Processing (NLP), pretraining and ﬁne-tuning have become a new norm for many tasks. Pretrained language models (PLMs) (e.g., BERT [1], XLNet [2], RoBERTa [3], ALBERT [4]) contain many layers and millions or even billions of parameters, making them computationally expensive and inefﬁcient regarding both memory consumption and latency. This drawback hinders their application in scenarios where inference speed and computational costs are crucial. Another bottleneck of overparameterized PLMs that stack dozens of Transformer layers is the “overthinking” problem [5] during their decision-making process. That is, for many input samples, their shallow representations at an earlier layer are adequate to make a correct classiﬁcation, whereas the represen-tations in the ﬁnal layer may be otherwise distracted by over-complicated or irrelevant features that do not generalize well. The overthinking problem in PLMs leads to wasted computation, hinders model generalization, and may also make them vulnerable to adversarial attacks [6].
In this paper, we propose a novel Patience-based Early Exit (PABEE) mechanism to enable models to stop inference dynamically. PABEE is inspired by the widely used Early Stopping [7, 8] strategy for model training. It enables better input-adaptive inference of PLMs to address the aforementioned limitations. Speciﬁcally, our approach couples an internal classiﬁer with each layer of a PLM and dynamically stops inference when the intermediate predictions of the internal classiﬁers remain unchanged for t times consecutively (see Figure 1b), where t is a pre-deﬁned patience. We ﬁrst show that our method is able to improve the accuracy compared to conventional inference under certain assumptions. Then we conduct extensive experiments on the GLUE benchmark and show that PABEE outperforms existing prediction probability distribution-based exit criteria by a large margin. In addi-tion, PABEE can simultaneously improve inference speed and adversarial robustness of the original
∗Equal contribution. Work done during these two authors’ internship at Microsoft Research Asia. 2Code available at https://github.com/JetRunner/PABEE. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Shallow-Deep Net [5] (b) Patience-based Early Exit (PABEE)
Figure 1: Comparison between Shallow-Deep Net, a prediction score based early exit (threshold is set to 0.9), and our Patience-based Early Exit (patience t = 1). A classiﬁer is denoted by Ci, and n is the number of layers in a model. In this ﬁgure, Shallow-Deep incorrectly exits based on the prediction score while PABEE considers multiple classiﬁers and exits with a correct prediction. model while retaining or even improving its original accuracy with minor additional effort in terms of model size and training time. Also, our method can dynamically adjust the accuracy-efﬁciency trade-off to ﬁt different devices and resource constraints by tuning the patience hyperparameter without retraining the model, which is favored in real-world applications [9]. Although we focus on
PLM in this paper, we also have conducted experiments on image classiﬁcation tasks with the popular
ResNet [10] as the backbone model and present the results in Appendix A to verify the generalization ability of PABEE.
To summarize, our contribution is two-fold: (1) We propose Patience-based Early Exit, a novel and effective inference mechanism and show its feasibility of improving the efﬁciency and the accuracy of deep neural networks with theoretical analysis. (2) Our empirical results on the GLUE benchmark highlight that our approach can simultaneously improve the accuracy and robustness of a competitive
ALBERT model, while speeding up inference across different tasks with trivial additional training resources in terms of both time and parameters. 2