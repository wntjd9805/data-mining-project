Abstract
Bayesian optimization (BO) is a popular approach to optimize expensive-to-evaluate black-box functions. A signiﬁcant challenge in BO is to scale to high-dimensional parameter spaces while retaining sample efﬁciency. A solution con-sidered in existing literature is to embed the high-dimensional space in a lower-dimensional manifold, often via a random linear embedding. In this paper, we identify several crucial issues and misconceptions about the use of linear embed-dings for BO. We study the properties of linear embeddings from the literature and show that some of the design choices in current approaches adversely impact their performance. We show empirically that properly addressing these issues signiﬁcantly improves the efﬁcacy of linear embeddings for BO on a range of problems, including learning a gait policy for robot locomotion. 1

Introduction
Bayesian optimization (BO) is a robust, sample-efﬁcient technique for optimizing expensive-to-evaluate black-box functions [34, 24]. BO has been successfully applied to diverse applications, ranging from automated machine learning [44, 22] to robotics [32, 6, 40]. One of the most active topics of research in BO is how to extend current methods to higher-dimensional spaces. A common framework to tackle this problem is to consider a high-dimensional BO (HDBO) task as a standard
BO problem in a low-dimensional embedding, where the embedding can be either linear (typically a random projection) or nonlinear (e.g., via a multi-layer neural network); see Sec. 2 for a full review.
An advantage of this framework is that it explicitly decouples the problem of ﬁnding low-dimensional representations suitable for optimization from the actual optimization technique.
In this paper we study the use of linear embeddings for HDBO, and in particular we re-examine prior efforts to use random linear projections. Random projections are attractive because, by the
Johnson-Lindenstrauss lemma, they can be approximately distance-preserving [23] without requiring data to learn the embedding. Random embeddings come with strong theoretical guarantees, but have shown mixed empirical performance for HDBO. Our goal here is not just to present a new HDBO method, but rather to improve understanding of important considerations for BO in an embedding.
The contributions of this paper are: 1) We provide new results that identify why linear embeddings have performed poorly in HDBO. We show that existing approaches can produce representations that are not well-modeled by a Gaussian process (GP), or do not contain an optimum (Sec. 4). 2) We construct a representation with better properties for BO (Sec. 5): modelability is improved with a
Mahalanobis kernel tailored for linear embeddings and by adding polytope bounds to the embedding, and we show how to maintain a high probability that the embedding contains an optimum. 3) We combine these improvements to form a new linear-embedding HDBO method, ALEBO, and show empirically that it outperforms a wide range of HDBO techniques, including on test functions up to D=1000, with black-box constraints, and for gait optimization of a multi-legged robot (Secs. 6 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
and 7). These results show empirically that we have identiﬁed several important elements impacting the BO performance of linear embedding methods. Code to reproduce the results of this paper is available at github.com/facebookresearch/alebo. 2