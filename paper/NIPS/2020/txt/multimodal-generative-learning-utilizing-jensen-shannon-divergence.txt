Abstract
Learning from different data types is a long-standing goal in machine learning research, as multiple information sources co-occur when describing natural phe-nomena. However, existing generative models that approximate a multimodal
ELBO rely on difﬁcult or inefﬁcient training schemes to learn a joint distribution and the dependencies between modalities. In this work, we propose a novel, efﬁ-cient objective function that utilizes the Jensen-Shannon divergence for multiple distributions. It simultaneously approximates the unimodal and joint multimodal posteriors directly via a dynamic prior. In addition, we theoretically prove that the new multimodal JS-divergence (mmJSD) objective optimizes an ELBO. In extensive experiments, we demonstrate the advantage of the proposed mmJSD model compared to previous work in unsupervised, generative learning tasks. 1

Introduction
Replicating the human ability to process and relate information coming from different sources and learn from these is a long-standing goal in machine learning [2]. Multiple information sources offer the potential of learning better and more generalizable representations, but pose challenges at the same time: models have to be aware of complex intra- and inter-modal relationships, and be robust to missing modalities [18, 31]. However, the excessive labelling of multiple data types is expensive and hinders possible applications of fully-supervised approaches [6, 11]. Simultaneous observations of multiple modalities moreover provide self-supervision in the form of shared information which connects the different modalities. Self-supervised, generative models are a promising approach to capture this joint distribution and ﬂexibly support missing modalities with no additional labelling cost attached. Based on the shortcomings of previous work (see Section 2.1), we formulate the following wish-list for multimodal, generative models:
Scalability. The model should be able to efﬁciently handle any number of modalities. Translation approaches [10, 32] have had great success in combining two modalities and translating from one to the other. However, the training of these models is computationally expensive for more than two modalities due to the exponentially growing number of possible paths between subsets of modalities.
Missing data. A multimodal method should be robust to missing data and handle any combination of available and missing data types. For discriminative tasks, the loss in performance should be minimized. For generation, the estimation of missing data types should be conditioned on and coherent with available data while providing diversity over modality-speciﬁc attributes in the generated samples.
Information gain. Multimodal models should beneﬁt from multiple modalities for discriminative as well as for generative tasks. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this work, we introduce a novel probabilistic, generative and self-supervised multi-modal model.
The proposed model is able to integrate information from different modalities, reduce uncertainty and ambiguity in redundant sources, as well as handle missing modalities while making no assumptions about the nature of the data, especially about the inter-modality relations.
We base our approach directly in the Variational Bayesian Inference framework and propose the new multimodal Jensen-Shannon divergence (mmJSD) objective. We introduce the idea of a dynamic prior for multimodal data, which enables the use of the Jensen-Shannon divergence for M distributions [1, 15] and interlinks the unimodal probabilistic representations of the M observation types. Additionally, we are - to the best of our knowledge - the ﬁrst to empirically show the advantage of modality-speciﬁc subspaces for multiple data types in a self-supervised and scalable setting. For the experiments, we concentrate on Variational Autoencoders [12]. In this setting, our multimodal extension to variational inference implements a scalable method, capable of handling missing observations, generating coherent samples and learning meaningful representations. We empirically show this on two different datasets. In the context of scalable generative models, we are the ﬁrst to perform experiments on datasets with more than 2 modalities showing the ability of the proposed method to perform well in a setting with multiple modalities. 2 Theoretical