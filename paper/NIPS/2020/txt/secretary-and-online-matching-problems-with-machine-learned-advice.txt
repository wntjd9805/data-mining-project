Abstract
The classical analysis of online algorithms, due to its worst-case nature, can be quite pessimistic when the input instance at hand is far from worst-case. Often this is not an issue with machine learning approaches, which shine in exploiting patterns in past inputs in order to predict the future. However, such predictions, although usually accurate, can be arbitrarily poor. Inspired by a recent line of work, we augment three well-known online settings with machine learned predictions about the future, and develop algorithms that take them into account. In particular, we study the following online selection problems: (i) the classical secretary problem, (ii) online bipartite matching and (iii) the graphic matroid secretary problem. Our algorithms still come with a worst-case performance guarantee in the case that predictions are subpar while obtaining an improved competitive ratio (over the best-known classical online algorithm for each problem) when the predictions are sufﬁciently accurate. For each algorithm, we establish a trade-off between the competitive ratios obtained in the two respective cases. 1

Introduction
There has been enormous progress in the ﬁeld of machine learning in the last decade, which has affected a variety of other areas as well. One of these areas is the design of online algorithms.
Traditionally, the analysis of such algorithms involves worst-case guarantees, which can often be quite pessimistic. It is conceivable though, that having prior knowledge regarding the online input (obtained using machine learning) could potentially improve those guarantees signiﬁcantly.
In this work, we consider various online selection algorithms augmented with so-called machine learned advice. In particular, we consider secretary and online bipartite matching problems. The high-level idea is to incorporate some form of predictions in an existing online algorithm in order to get the best of two worlds: (i) provably improve the algorithm’s performance guarantee in the case that predictions are sufﬁciently good, while (ii) losing only a constant factor of the algorithm’s existing worst-case performance guarantee, when the predictions are subpar. Improving the performance of classical online algorithms with the help of machine learned predictions is a relatively new area that has gained a lot of attention in the last couple of years [24, 21, 36, 27, 30, 34, 17, 33, 32]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We motivate the idea of incorporating such machine-learned advice, in the class of problems studied in this work, by illustrating a simple real-world problem. Consider the following setting for selling a laptop on an online platform.1 Potential buyers arrive one by one, say, in a uniformly random order, and report a price that they are willing to pay for the laptop. Whenever a buyer arrives, we have to irrevocably decide if we want to sell at the given price, or wait for a better offer. Based on historical data, e.g., regarding previous online sales of laptops with similar specs, the online platform might suggest a (machine learned) prediction for the maximum price that some buyer is likely to offer.
How can we exploit this information in our decision process? One problem that arises here is that we do not have any formal guarantees for how accurate the machine-learned advice is for any particular instance. For example, suppose we get a prediction of 900 dollars as the maximum price that some buyer will likely offer. One extreme policy is to blindly trust this prediction and wait for the ﬁrst buyer to come along that offers a price sufﬁciently close to 900 dollars. If this prediction is indeed accurate, this policy has an almost perfect performance guarantee, in the sense that we will sell to the (almost) highest bidder. However, if the best offer is only, say, 500 dollars, we will never sell to this buyer (unless this offer arrives last), since the advice is to wait for a better offer to come along. In particular, the performance guarantee of this selling policy depends on the prediction error (400 dollars in this case) which can become arbitrarily large. The other extreme policy is to completely ignore the prediction of 900 dollars and just run the classical secretary algorithm: Observe a 1/e-fraction of the buyers, and then sell to the ﬁrst buyer that arrives afterwards, who offers a price higher than the best offer seen in the initially observed fraction. This yields, in expectation, a selling price of at least 1/e times the highest offer [28, 11].
Can we somehow combine the preceding two extreme selling-policies, so that we get a performance guarantee strictly better than that of 1/e in the case where the prediction for the highest offer is not too far off, while not loosing too much over the guarantee of 1/e otherwise? Note that (even partially) trusting poor predictions often comes at a price, and thus obtaining a competitive ratio worse than 1/e seems inevitable in this case. We show that there is in fact a trade-off between the competitive ratio that we can achieve when the prediction is accurate and the one we obtain when the prediction error turns out to be large. 1.1 Our models and contributions
We show how one can incorporate predictions in various online selection algorithms for problems that generalize the classical secretary problem. The overall goal is to include as little predictive information as possible into the algorithm, while still obtaining improvements in the case that the information is accurate. Our results are parameterized by (among other parameters) the so-called prediction error η that measures the quality of the given predictions. We note that for all the considered problems, one cannot hope for an algorithm with a performance guarantee better than 1/e in the corresponding settings without predictions, as this bound is known to be optimal for the classical secretary problem (and also applies to the other problems) [28, 11]. Our goal is to design algorithms that improve upon the 1/e worst-case competitive ratio in the case where the prediction error is sufﬁciently small, and otherwise (when the prediction error is large) never lose more than a constant (multiplicative) factor over the worst-case competitive ratio. More speciﬁcally: We start by introducing a meta-result in Theorem 1.1, that applies to all of the problems, before introducing each of them individually along with the speciﬁc corresponding result.
Theorem 1.1 (Meta Result). For any λ ≥ 0, there is a polynomial time deterministic algorithm that incorporates the predictions p∗ such that for some constants 0 < α, β < 1 it is (i) α-competitive with α > 1/e, when the prediction error is sufﬁciently small; and (ii) β-competitive with β < 1/e, independently of the prediction error.
We note that there is a correlation between the constants α and β, which can be intuitively described as follows: The more one is willing to give up in the worst-case guarantee, i.e. the more conﬁdence we have in the predictions, the better the competitive ratio becomes in the case where the predictions are sufﬁciently accurate. Each of our algorithms takes as input a parameter λ ≥ 0 which quantiﬁes this level of conﬁdence (small λ implies more conﬁdence and vice-versa). We now take a closer look at our contributions for each speciﬁc problem: 1This example is similar to the classical secretary problem [15]. 2
Secretary problem. In the secretary problem there is a set {1, . . . , n} of secretaries, each with a value vi ≥ 0 for i ∈ {1, . . . , n}, that arrive in a uniformly random order. Whenever a secretary arrives, we have to irrevocably decide whether we want to hire that person. If we decide to hire a secretary, we automatically reject all subsequent candidates. The goal is to select the secretary with the highest value. We assume without loss of generality that all values vi are distinct. This can be done by introducing a suitable tie-breaking rule if necessary. For details, see Section 3.
In order to illustrate our ideas and techniques, we start by augmenting the classical secretary problem2 with predictions. Here, we are given a prediction p∗ for the maximum value among all arriving secretaries.3 The prediction error is then deﬁned as η = |p∗ − v∗|, where v∗ is the true maximum value among all secretaries. We emphasize that the algorithm is not aware of the prediction error
η, and this parameter is only used to analyze the algorithm’s performance guarantee. We show the following theorem:
Theorem 1.2. For any λ ≥ 0 and c > 1, there is a deterministic algorithm for the (value-maximization) secretary problem that is asymptotically gc,λ(η)-competitive in expectation, where (cid:110) (cid:111)(cid:17)(cid:105)(cid:111) (cid:40) (cid:41) (cid:16) (cid:104) gc,λ(η) = (cid:110) 1 ce , max 1 ce f (c) max 1 − λ+η
OP T , 0 if 0 ≤ η < λ if η ≥ λ
, and the function f (c) is given in terms of the two branches W0 and W−1 of the Lambert W -function4 and reads f (c) = exp{W0(−1/(ce))} − exp{W−1(−1/(ce))}.
We note that λ and c are independent parameters that provide the most general description of the competitive ratio. Here λ is our conﬁdence in the predictions and c describes how much we are willing to lose in the worst case. Although these parameters can be set independently, some combinations of them are not very sensible, as one might not get an improved performance guarantee, even when the prediction error is small (for instance, if c = 1, i.e., we are not willing to lose anything in the worst case, then it is not helpful to consider the prediction at all). To illustrate the inﬂuence of these parameters on the competitive ratio, in Figure 1, we plot various combinations of the input parameters c, λ and p∗ of Algorithm 1 (in Section 3), assuming that η = 0. In this case p∗ = OPT and the competitive ratio simpliﬁes to gc,λ(0) = max (cid:26)
, f (c) · max 1 − (cid:26) 1 ce (cid:27)(cid:27)
λ p∗ , 0
.
We therefore choose the axes of Figure 1 to be λ/p∗ and c.
Figure 1: The red curve shows the optimal competitive ratio without predictions, i.e., gc,λ(0) = 1/e.
Our algorithm achieves an improved competitive ratio gc,λ(0) > 1/e in the area below this curve, and a worse competitive ratio gc,λ(0) < 1/e in the area above it. 2 To be precise, we consider the so-called value maximization version of the problem, see Section 3. 3 This corresponds to a prediction for the maximum price somebody is willing to offer in the laptop example. 4 See the preliminaries section for a formal deﬁnition. 3
Furthermore, as one does not know the prediction error η, there is no way in choosing these parameters optimally, since different prediction errors require different settings of λ and c.
To get an impression of the statement in Theorem 1.2, if we have, for example, η + λ = 1 10 OPT, then we start improving over 1/e for c ≥ 1.185. Moreover, if one believes that the prediction error is low, one should set c very high (hence approaching a 1-competitive algorithm in case the predictions are close to perfect). Note also, that the bound obtained in Theorem 1.2 has a discontinuity at η = λ. This can be easily smoothed out by selecting λ according to some distribution, which now represents our conﬁdence in the prediction p∗. The competitive ratio will start to drop earlier in this case, and will continuously reach 1/(ce). This bound is tight for any ﬁxed c > 1 when η = λ = 0. We illustrate how the competitive ratio changes as a function of η in the supplementary material to this paper.
Online bipartite matching with vertex arrivals. We study the online bipartite matching problem in which the set of nodes L of a bipartite graph G = (L ∪ R, E), with |L| = n and |R| = m, arrives online in a uniformly random order [23, 20]. Upon arrival, a node reveals the edge-weights to its neighbors in R. We have to irrevocably decide if we want to match up the arrived online node with one of its (currently unmatched) neighbors in R. Kesselheim et al. [20] gave a tight 1/e-competitive deterministic algorithm for this setting that signiﬁcantly generalizes the same guarantee for the classical secretary algorithm [28, 11].
The prediction that we consider in this setting is a vector of values p∗ = (p∗ m) that predicts the edge-weights adjacent to the nodes r ∈ R in some ﬁxed optimal (ofﬂine) bipartite matching. That is, the prediction p∗ indicates the existence of a ﬁxed optimal bipartite matching in which each node r ∈ R is adjacent to an edge with weight p∗ r. The prediction error is then the maximum prediction error taken over all nodes in r ∈ R and minimized over all optimal matchings. This generalizes the prediction used for the classical secretary problem. This type of predictions closely corresponds to the vertex-weighted online bipartite matching problem [1], which is discussed in the supplementary material to this paper. More formally, we prove the following theorem about the online bipartite matching problem with vertex arrivals:
Theorem 1.3. For any λ ≥ 0 and c > d ≥ 1, there is a deterministic algorithm for the online bipartite matching problem with uniformly random arrivals that is asymptotically gc,d,λ(η)-competitive in expectation, where 1, . . . , p∗ (cid:40) gc,d,λ(η) = (cid:110) 1 c ln( c max c ln( c 1 d ) d ), (cid:104) d−1 2c (cid:16) max (cid:110) 1 − (λ+η)|ψ|
OPT (cid:111)(cid:17)(cid:105)(cid:111)
, 0 if 0 ≤ η < λ, if η ≥ λ. (cid:41)
, and |ψ| is the cardinality of an optimal (ofﬂine) matching ψ of the instance. the bound in gc,d,λ... If λ is small, we roughly obtain a bound of (d − 1)/2c in case η is small as well, and a bound of ln(c/d)/c if η is large. Moreover, when c/d → 1, we approach a bound of 1/2 in case
η is small, whereas the worst-case guarantee ln(c/d)/c for large η then increases. In other words, although the predictions provide very little information about the optimal solution, we are still able to obtain improved theoretical guarantees in the case where the predictions are sufﬁciently accurate.
In the online bipartite matching setting with predictions for the nodes in R, we can essentially get close to a 1/2-approximation – which is best possible – assuming the predictions are close to perfect.
This follows from the fact that in this case we obtain the so-called vertex-weighted online bipartite matching problem for which there is a deterministic 1/2-competitive algorithm, and no algorithm can do better [1]. Our algorithm “converges” to that in [1] when the predictions get close to perfect.
Graphic matroid secretary problem. We also augment the graphic matroid secretary problem with predictions. In this problem, the edges of a given undirected graph G = (V, E), with |V | = n and
|E| = m, arrive in a uniformly random order. The goal is to select a subset of edges of maximum weight under the constraint that this subset is a forest. That is, it is not allowed to select a subset of edges that form a cycle in G. The best known algorithm for this online problem is a (randomized) 1/4-competitive algorithm by Soto, Turkieltaub and Verdugo [37]. Their algorithm proceeds by ﬁrst selecting no elements from a preﬁx of the sequence of elements with randomly chosen size, followed by selecting an element if and only if it belongs to a (canonically computed) ofﬂine optimal solution, and can be added to the set of elements currently selected online. This is inspired by the algorithm of
Kesselheim et al. [20] for online bipartite matching.
As a result of possible independent interest, we show that there exists a deterministic (1/4 − o(1))-competitive algorithm for the graphic matroid secretary problem, which can roughly be seen as a 4
1, . . . , p∗ n) where p∗ deterministic version of the algorithm of Soto et al. [37]. Alternatively, our algorithm can be seen as a variation on the (deterministic) algorithm of Kesselheim et al. [20] for the case of online bipartite matching (in combination with an idea introduced in [4]).
The prediction that we consider here is a vector of values p = (p∗ i predicts the maximum edge-weight that node i ∈ V is adjacent to, in the graph G. This is equivalent to saying that p∗ i is the maximum edge-weight adjacent to node i ∈ V in a given optimal spanning tree (we assume that G is connected for sake of simplicity), which is, in a sense, in line with the predictions used for the preceding problems. (We note that the predictions model the optimal spanning tree in the case when all edge-weights are pairwise distinct. Otherwise, there can be many (ofﬂine) optimal spanning trees, and thus the predictions do not encode a unique optimal spanning tree. We intentionally chose not to use predictions regarding which edges are part of an optimal solution, as in our opinion, such an assumption would be too strong.) The prediction error is also deﬁned similarly. We show the following:
Theorem 1.4. For any λ ≥ 0 and c > d ≥ 1, there is a deterministic algorithm for the graphic matroid secretary problem that is asymptotically gc,d,λ(η)-competitive in expectation, where (cid:0) 1 d − 1 1 − 2(λ+η)|V | if 0 ≤ η < λ, c2 , 1 (cid:110) d−1 (cid:1) (cid:16) (cid:17)(cid:111)
OPT (cid:40) 2 c gc,d,λ(η) = if η ≥ λ. max d−1 c2
If λ is small, we roughly obtain a bound of (1/d − 1/c)/2 in case η is small, and a bound of (d − 1)/c2 if η is large. Roughly speaking, if d → 1 and c → ∞ we approach a bound of 1/2 when the predictions are good, whereas the bound of (d − 1)/c2 becomes arbitrarily bad. For this problem, we get close to a 1/2-approximation in the case where the predictions (the maximum edge-weights adjacent to the nodes in the graph) get close to perfect. However this is probably not tight. We suspect that, when given perfect predictions, it is possible to obtain an algorithm with a better approximation guarantee. This is an interesting open problem.
Discussion and Extensions. For each of the preceding three problems, we give a deterministic algorithm. In particular, we show that the canonical approaches for the secretary problem [28, 11] and the online bipartite matching problem [20] can be naturally augmented with predictions.
We also show how to adapt our novel deterministic algorithm for the graphic matroid secretary problem. Furthermore, we comment on randomized approaches for each respective problem in the supplementary material.
The high-level overview of our approach is as follows. We split the sequence of uniformly random arrivals in three phases. In the ﬁrst phase, we observe a fraction of the input without selecting anything.
In the remaining two phases, we run two extreme policies, which either exploit the predictions or ignore them completely. Although, each of the aforementioned extreme policies can be analyzed individually, using existing techniques, it is a non-trivial task to show that when combined they do not obstruct each other too much. The execution order of these policies is also crucial for the analysis.
As an additional result, we show that the online bipartite matching algorithm, can be turned into a truthful mechanism in the case of the so-called single-value unit-demand domains. We show that the algorithm of Kesselheim et al. [20] can be turned into a truthful mechanism in the special case of uniform edge-weights, where for every ﬁxed online node in L, there is a common weight on all edges adjacent to it. In addition, we show that truthfulness can be preserved when predictions are included in the algorithm. We note that Reiffenhäuser [35] recently gave a truthful mechanism for the general online bipartite matching setting (without uniform edge-weights). It would be interesting to see if her algorithm can be augmented with predictions as well.
Remark 1.5. In the statements of Theorem 1.2, 1.3 and 1.4 it is assumed that the set of objects arriving online (either vertices or edges) is asymptotically large. We hide o(1)-terms at certain places for the sake of readability. 1.2