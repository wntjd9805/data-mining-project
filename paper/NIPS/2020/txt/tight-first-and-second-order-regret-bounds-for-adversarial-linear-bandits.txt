Abstract
We propose novel algorithms with ﬁrst- and second-order regret bounds for adver-sarial linear bandits. These regret bounds imply that our algorithms perform well when there is an action achieving a small cumulative loss or the loss has a small variance. In addition, we need only assumptions weaker than those of existing algorithms; our algorithms work on discrete action sets as well as continuous ones without a priori knowledge about losses, and they run efﬁciently if a linear optimization oracle for the action set is available. These results are obtained by combining optimistic online optimization, continuous multiplicative weight update methods, and a novel technique that we refer to as distribution truncation. We also show that the regret bounds of our algorithms are tight up to polylogarithmic factors. 1

Introduction
The adversarial linear bandit problem models sequential decision making with limited information, and it has been used in a wide range of applications, including combinatorial bandits [18; 22] and the adaptive routing problem [12]. In this problem, a player is given a set A ⊆ Rd of actions represented by d-dimensional feature vectors, and the player is to choose an action at ∈ A in each round t = 1, . . . , T of the decision process. Just after choosing the action at, the player receives the bandit feedback (cid:104)(cid:96)t, at(cid:105) as the loss of the action, where (cid:96)t ∈ Rd is the loss vector of the t-th round chosen by an adversary.1 We should note here that the loss vector (cid:96)t is not revealed to the player even after choosing the action. The goal of the player is to minimize cumulative loss (cid:80)T t=1 (cid:104)(cid:96)t, at(cid:105). Player performance of the player is evaluated by means of the regret RT (a∗) deﬁned as
RT (a∗) =
T (cid:88) t=1 (cid:104)(cid:96)t, at(cid:105) −
T (cid:88) t=1 (cid:104)(cid:96)t, a∗(cid:105) (1)
√ for a∗ ∈ A. A plethora of algorithms have been proposed with the expected regret E[RT (a∗)] =
˜O(d
T ) for any a∗ ∈ A [18; 32; 40] under the assumption that | (cid:104)(cid:96)t, at(cid:105) | = O(1), where ˜O(·) hides a logarithmic factor in d and T . These algorithms are worst-case optimal up to logarithmic factors:
∗This work was done while Shinji Ito was at the University of Tokyo. 1In this paper, we are concerned with adaptive adversaries, i.e., an adversary can choose loss (cid:96)t based on the past player’s actions a1, . . . , at−1. We note that (cid:96)t cannot depend on the t-th action at since otherwise the player would always suffer Ω(T ) regret. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Regret bounds for adversarial linear bandits. Here, we denote ¯(cid:96) = 1
T with † require additional assumptions on the feasible set A and a priori knowledge of deviations. t=1 (cid:96)t. The results (cid:80)T
Worst case
First order
Second order
Predictable sequence
√
Upper Bound
˜O(d (cid:18)
T ) [18; 32] (cid:113)(cid:80)T d
˜O t=1 (cid:104)(cid:96)t, a∗(cid:105) (cid:19)
[Theorem 3] (cid:18) (cid:113) d (cid:18)
θ (cid:80)T (cid:113)(cid:80)T d t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2 2 (cid:19) t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2
∗ (cid:19)
† [30]
[Theorem 2] (cid:18) (cid:113) d (cid:18)
θ (cid:80)T (cid:113)(cid:80)T d t=1 (cid:107)(cid:96)t − mt(cid:107)2 2 (cid:19)
† [44] (cid:19) t=1 ((cid:104)(cid:96)t − mt, at(cid:105))2
[Theorem 1]
˜O
˜O
˜O
˜O
Lower Bound
√
Ω(d (cid:18)
Ω
Ω (cid:18)
T ) [25; 35] (cid:113)(cid:80)T d t=1 (cid:104)(cid:96)t, a∗(cid:105) (cid:19) (cid:113)(cid:80)T d t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2
∗ (cid:19) (cid:18)
Ω (cid:113)(cid:80)T d t=1 (cid:107)(cid:96)t − mt(cid:107)2
∗ (cid:19)
Any algorithm suffers the expected regret of Ω(d
T ) in the worst case [25; 35]. These worst-case bounds are, however, too pessimistic in practical situations since we rarely encounter truly adversarial environments in real-world problems.
√
√ (cid:80)T
T = mina∗∈A
KT ) [8; 11], especially when L∗
To get around the worst-case lower bound, algorithms with ﬁrst-order and second-order regret bounds have been developed for some adversarial bandit problems [6; 15; 30; 48]. First-order regret bounds are those depending on the minimum cumulative loss L∗ t=1 (cid:104)(cid:96)t, a∗(cid:105), rather than on the number T of rounds. For example, Allenberg et al. [6] proposed an algorithm with a
ﬁrst-order regret bound for the adversarial multi-armed bandit (MAB) problem, a special case of the adversarial linear bandit problem in which the action set A is just a ﬁnite set of size K.2 For
MAB, their algorithm achieves regret of ˜O((cid:112)KL∗
T ), which improves over the worst-case optimal bound of O(
T is much smaller than T , i.e., where there is an action with a small cumulative loss. It is worth noting that their algorithm achieves a nearly optimal worst-case bound, as well, since L∗
T ≤ T follows from a standard assumption. For more general linear bandits, however, such an algorithm was not known in the literature. In this paper, second-order regret bounds refer to those depending on the second-order variation (cid:80)T t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2 rather than on T , where (cid:107) · (cid:107) is an arbitrary norm and ¯(cid:96) stands for the average of the loss vectors {(cid:96)t}T t=1.3 For t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2
MAB, Bubeck et al. [15] have proposed an algorithm with a regret bound of ˜O( 2).
This algorithm is nearly worst-case optimal since (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2 2 ≤ KT , and it performs better when losses {(cid:96)t} have small variation. For linear bandits, Hazan and Kale [30] proposed an algorithm (cid:113)(cid:80)T achieving ˜O(d 3 2) regret under certain assumptions. Though this bound is better than the worst-case optimal bound of ˜O(d 2 = O(T /d), it is not worst-case optimal in general. t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2 t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2
T ) when (cid:80)T (cid:113)(cid:80)T
√ 2
Another (and deeply relevant) line of work that tries to get around the worst-case lower bound is a framework called predictable sequences [44; 45; 48]. This framework assumes that the player is given predicted loss vector mt, which is produced by an arbitrary process, before choosing actions.
One would hope that the regret would get smaller when mt predicts (cid:96)t well. In fact, as shown in [44], we can achieve ˜O(d 3 t (cid:107)(cid:96)t − mt(cid:107)2 2) regret for linear bandits, which can be smaller than the worst-case optimal bound if mt are sufﬁciently close to (cid:96)t so that (cid:80)T (cid:113)(cid:80)T 2 = O(T /d). t=1 (cid:107)(cid:96)t − mt(cid:107)2 2 2In fact, MAB is equivalent to the linear bandit problem with an action set being the standard basis of a
K-dimensional space A = {e1, . . . , eK } ⊆ {0, 1}K : Each element (cid:96)ti of the loss vector (cid:96)t ∈ [0, 1]K stands for the loss incurred by choosing the i-th action ei. 3The term “second-order regret” has been used to mean various bounds in the literature. In this work, we adopt the one stated here. 2
Our contributions (cid:80)T
In this paper, we present newly-devised, efﬁcient algorithms with improved ﬁrst- and second-order regret bounds for the adversarial linear bandit problem. Our algorithms not only yield better regret bounds but also make only fewer assumptions than have been seen in previous studies. Previous results and our contributions are summarized in Table 1. In the table, ¯(cid:96) denotes the average of the loss vectors, i.e., ¯(cid:96) = 1 t=1 (cid:96)t. The bounds with † in Table 1 require additional prior knowledge
T w.r.t. the loss vectors. For example, the results by [30] and [44] in Table 1 are based on the assumption that, respectively, (approximated values of) the quantities (cid:80)T 2 are given before the game starts. In addition, they assume the following conditions: (i) The action set A is convex, (ii) maxa∈A (cid:107)a(cid:107)2 = O(1), and (iii) A has a self-concordance barrier with parameter θ ≥ 1.
Note that the self-concordance parameter θ can be Ω(d), e.g., when A = {a ∈ Rd | (cid:107)a(cid:107)∞ ≤ 1}.
We provide two algorithms with the guarantees described below: t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2 and (cid:80)T t=1 (cid:107)(cid:96)t − mt(cid:107)2 1. The ﬁrst one achieves E[RT (a∗)] = ˜O (cid:18)
E (cid:20) (cid:113)(cid:80)T d t=1 ((cid:104)(cid:96)t − mt, at(cid:105))2 (cid:21)(cid:19) for predictable sequences. If (cid:107)at(cid:107) = O(1) holds for a ﬁxed norm (cid:107) · (cid:107), our bound implies a regret bound of ˜O
, where (cid:107) · (cid:107)∗ is the dual norm of (cid:107) · (cid:107). This result (cid:113)(cid:80)T t=1 (cid:107)(cid:96)t − mt(cid:107)2
∗ (cid:21)(cid:19)
E (cid:18) (cid:20) d encompasses the result by [44] in Table 1 since maxa∈A (cid:107)a(cid:107)2 = O(1) is assumed in their work and θ ≥ 1 in general. Further, this algorithm does not require the above-mentioned assumptions. In particular, the action set can be discrete. 2. The second achieves the following second-order (cid:19) regret bound: E[RT (a∗)] = for arbitrary ¯(cid:96), as shown in Theorem 2. This result encom-(cid:113)(cid:80)T d
˜O (cid:18) t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2
∗ passes the regret bound by [30] in Table 1, and again, as before, this algorithm does not require the above-mentioned assumptions. When losses (cid:104)(cid:96)t, at(cid:105) are non-negative, this (cid:19) algorithm achieves the ﬁrst-order regret bound of E[RT (a∗)] = ˜O simultaneously, as shown in Theorem 3. t=1 (cid:104)(cid:96)t, a∗(cid:105) (cid:113)(cid:80)T (cid:18) d
Each regret bound shown in Theorems 1, 2 and 3 is of ˜O(d
T ), and hence, enjoys worst-case optimality up to logarithmic factors, in contrast to existing algorithms [30; 44]. We should note, however, that our regret bounds are not tight for MAB since the worst-case optimal regret bound dT ) for this special case. Similarly, in the special case where the action set is known to be Θ( is the unit ball, algorithms proposed in [30; 44] achieves regret bounds comparable to ours (up to logarithmic factors), as there is a self-concordant barrier of parameter θ = O(1).
√
√
√
Our algorithms are based on the multiplicative weight update (MWU) method [7; 34] with an unbiased estimator ˆ(cid:96)t of the loss vector (cid:96)t. As with existing algorithms [18; 30; 32] for the adversarial linear bandit problem, we construct an unbiased estimator from a single observation (cid:104)(cid:96)t, at(cid:105), where at follows a distribution pt maintained by the MWU method. The regret strongly depends on the stability of the unbiased estimators ˆ(cid:96)t; we want the norm and variance of ˆ(cid:96)t to be small enough. In order to make an unbiased estimator stable, previous studies [18; 32] have mixed pt with another probability distribution. This approach, however, requires the mixing rate of Ω(d/
T ), which
T )-regret in general. To overcome this issue, we truncate (the support causes Ω(T · d/ of) the distribution pt instead. More speciﬁcally, we truncate the distribution to ensure that the magnitude of chosen actions would be controlled with respect to an appropriately designed norm.
We will show here that this approach ensures the stability of ˆ(cid:96)t with almost no degradation of the expected performance, with the help of a concentration property of log-concave distributions [42].
We should note that similar techniques of truncating distributions can be found in the literature of bandit optimization, such as combinatorial semi-bandits [43] and bandit convex optimization [14].
This paper, however, employs a different way of truncation and analyses as the problem settings are different.
T ) = Ω(d
√
√
Another essential element in our algorithms is the technique called optimistic online optimization [44; 45]. Rakhlin and Sridharan [44; 45] introduced the framework of online optimization with predicted 3
loss mt, and proposed algorithms referred to as optimistic online mirror descent and optimistic follow the regularized leader that exploit the predicted loss mt to improve the regret. Our ﬁrst algorithm employs their techniques to achieve the regret in Theorem 1. The second appropriately chooses mt on the basis of an online optimization method, and it achieves regret bounds noted in Theorems 2 and 3. A similar technique for computing mt can be found in the study by Cutkosky [23], though this existing work uses a different loss as it is for the full-information setting. 2