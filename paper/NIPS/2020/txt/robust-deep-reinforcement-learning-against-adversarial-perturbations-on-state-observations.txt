Abstract
A deep reinforcement learning (DRL) agent observes its states through observa-tions, which may contain natural measurement errors or adversarial noises. Since the observations deviate from the true states, they can mislead the agent into making suboptimal actions. Several works have shown this vulnerability via adversarial attacks, but existing approaches on improving the robustness of DRL under this setting have limited success and lack for theoretical principles. We show that naively applying existing techniques on improving robustness for classiﬁcation tasks, like adversarial training, are ineffective for many RL tasks. We propose the state-adversarial Markov decision process (SA-MDP) to study the fundamental properties of this problem, and develop a theoretically principled policy regulariza-tion which can be applied to a large family of DRL algorithms, including proximal policy optimization (PPO), deep deterministic policy gradient (DDPG) and deep Q networks (DQN), for both discrete and continuous action control problems. We signiﬁcantly improve the robustness of PPO, DDPG and DQN agents under a suite of strong white box adversarial attacks, including new attacks of our own.
Additionally, we ﬁnd that a robust policy noticeably improves DRL performance even without an adversary in a number of environments. Our code is available at https://github.com/chenhongge/StateAdvDRL. 1

Introduction
With deep neural networks (DNNs) as powerful function approximators, deep reinforcement learning (DRL) has achieved great success on many complex tasks [46, 35, 33, 64, 20] and even on some safety-critical applications (e.g., autonomous driving [74, 56, 49]). Despite achieving super-human level performance on many tasks, the existence of adversarial examples [69] in DNNs and many successful attacks to DRL [27, 4, 36, 50, 81] motivates us to study robust DRL algorithms.
When an RL agent obtains its current state via observations, the observations may contain uncertainty that naturally originates from unavoidable sensor errors or equipment inaccuracy. A policy not robust to such uncertainty can lead to catastrophic failures (e.g., the navigation setting in Figure 1). To ensure safety under the worst case uncertainty, we consider the adversarial setting where the state observation is adversarially perturbed from s to ν(s), yet the underlying true environment state s is unchanged. This setting is aligned with many adversarial attacks on state observations (e.g.,
[27, 36]) and cannot be characterized by existing tools such as partially observable Markov decision process (POMDP), because the conditional observation probabilities in POMDP cannot capture the adversarial (worst case) scenario. Studying the fundamental principles in this setting is crucial.
Before basic principles were developed, several early approaches [5, 40, 50] extended existing adver-sarial defenses for supervised learning, e.g., adversarial training [32, 39, 87] to improve robustness 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
under this setting. Speciﬁcally, we can attack the agent and generate trajectories adversarially during training time, and apply any existing DRL algorithm to hope-fully obtain a robust policy. Unfortunately, we show that for most environments, naive adversarial training (e.g., putting adversarial states into the replay buffer) leads to unstable training and deteriorates agent perfor-mance [5, 15], or does not signiﬁcantly improve robust-ness under strong attacks. Since RL and supervised learn-ing are quite different problems, naively applying tech-niques from supervised learning to RL without a proper theoretical justiﬁcation can be unsuccessful. To sum-marize, we study the theory and practice of robust RL against perturbations on state observations:
Figure 1: A car observes its location through sensors (e.g., GPS) and plans its route to the goal. Without considering the uncertainty in observed location (e.g., er-ror of GPS coordinates), an unsafe policy may crash into the wall because s (cid:54)= ν(s).
• We formulate the perturbation on state observations as a modiﬁed Markov decision process (MDP), which we call state-adversarial MDP (SA-MDP), and study its fundamental properties. We show that under an optimal adversary, a stationary and Markovian optimal policy may not exist for SA-MDP.
• Based on our theory of SA-MDP, we propose a theoretically principled robust policy regularizer which is related to the total variation distance or KL-divergence on perturbed policies. It can be practically and efﬁciently applied to a wide range of RL algorithms, including PPO, DDPG and DQN.
• We conduct experiments on 10 environments ranging from Atari games with discrete actions to complex control tasks in continuous action space. Our proposed method signiﬁcantly improves robustness under strong white-box attacks on state observations, including two strong attacks we design, the robust Sarsa attack (RS attack) and maximal action difference attack (MAD attack). 2