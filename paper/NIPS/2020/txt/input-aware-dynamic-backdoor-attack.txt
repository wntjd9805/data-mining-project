Abstract
In recent years, neural backdoor attack has been considered to be a potential security threat to deep learning systems. Such systems, while achieving the state-of-the-art performance on clean data, perform abnormally on inputs with predeﬁned triggers.
Current backdoor techniques, however, rely on uniform trigger patterns, which are easily detected and mitigated by current defense methods. In this work, we propose a novel backdoor attack technique in which the triggers vary from input to input. To achieve this goal, we implement an input-aware trigger generator driven by diversity loss. A novel cross-trigger test is applied to enforce trigger nonreusablity, making backdoor veriﬁcation impossible. Experiments show that our method is efﬁcient in various attack scenarios as well as multiple datasets.
We further demonstrate that our backdoor can bypass the state of the art defense methods. An analysis with a famous neural network inspector again proves the stealthiness of the proposed attack. Our code is publicly available. 1

Introduction
Due to their superior performance, deep neural networks have become essential in modern artiﬁcial intelligence systems. The state-of-the-art networks, however, require massive training data, expensive computing hardwares, and days or even weeks of training. Therefore, instead of training these networks from scratch, many companies use pre-trained models provided by third-parties. This has caused an emerging security threat of neural backdoor attacks, in which the provided networks look genuine but intentionally misbehave on a speciﬁc condition of the inputs.
BadNets [1] is one of the ﬁrst studies discussing this problem on the image classiﬁcation task. The authors proposed to poison a part of the training data. More speciﬁcally, the poisoned images were injected with a ﬁxed small pattern, called a trigger, at a speciﬁc location, and the corresponding labels were changed to some pre-deﬁned attack classes. The trained networks could classify the clean testing images accurately but quickly switched to return attack labels when trigger patterns appeared.
Liu et al. [2] extended it to different domains, including face recognition, speech recognition, and sentence attitude recognition. Since then, many variations of backdoor attacks have been proposed
[3, 4].
Though varying in mechanisms and scenarios, all these attacks rely on the same premise of using a
ﬁxed trigger on all poisoned data. It became a crucial weakness that has been exploited by defense methods [5, 6, 7, 8]. These methods derive backdoor trigger candidates then verify by applying them to a set of clean test images.
We argue that the ﬁxed trigger premise is hindering the capability of backdoor attack methods. A dynamic backdoor that has the trigger pattern varying from input to input is much stealthier. It breaks the foundation assumption of all current defense methods, thus easily defeats them. Moreover, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Input-aware Dynamic Backdoor Attack. The attacker uses a generator g to create a trigger (m, p) conditioned on the input image. The poisoned classiﬁer can correctly recognize clean inputs (the leftmost and rightmost images) but return the predeﬁned label ("plane") when injecting the corresponding triggers (the second and fourth image). Triggers are nonreusable; inserting a trigger to a mismatched clean input does not activate the attack (the middle image). dynamic triggers are hard to differentiate from adversarial noise perturbations, which are common in all deep neural networks.
For simpliﬁcation, we limit our work on image recognition problems. However, it could be easily extended to many other domains, such as speech recognition and sentence attitude recognition, as proposed in [2].
Implementing such input-aware backdoor systems is not trivial. Without careful design, the generated triggers may be repetitive or universal for different input images, making the systems collapse to regular backdoor attacks. To construct such a dynamic backdoor, we propose to use a trigger generator conditioned on the input image. The generated triggers are distinctive so that two different input images do not share the same trigger, compelled by a diversity loss. Also, the triggers should be non-reusable; a trigger generated on one image cannot be applied to another image. To enforce this constraint, we deﬁne a novel cross-trigger test alongside the traditional clean and poisoned data tests and incorporate the corresponding loss in training. Fig. 1 illustrates the proposed attack system.
We evaluate our method by running experiments on the standard datasets MNIST, CIFAR-10, and
GTSRB. On every dataset, the backdoor model shows near a 100% attack success rate on poisoned data while it accurately classiﬁes the clean and cross-trigger images to the benign labels. The trained models also easily break the state-of-the-art defense methods, including Neural Cleanse, STRIP, and
Fine-Pruning, proving the effectiveness of the proposed attack.
We further investigate potential practices against our attack. First, a simple image regularization such as spatial smoothing or color depth shrinking may impair the trigger pattern and break down the attack. However, we show that our backdoor is persistent to these regularizations. Second, we test the standard network inspection tool GradCam [9] over our trained models. There is no visible trail, again proving the stealthiness of the proposed backdoor. 2