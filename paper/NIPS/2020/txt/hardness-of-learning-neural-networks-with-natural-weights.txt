Abstract
Neural networks are nowadays highly successful despite strong hardness results.
The existing hardness results focus on the network architecture, and assume that the network’s weights are arbitrary. A natural approach to settle the discrepancy is to assume that the network’s weights are “well-behaved" and posses some generic properties that may allow efﬁcient learning. This approach is supported by the intuition that the weights in real-world networks are not arbitrary, but exhibit some
”random-like" properties with respect to some ”natural" distributions. We prove negative results in this regard, and show that for depth-2 networks, and many
“natural" weights distributions such as the normal and the uniform distribution, most networks are hard to learn. Namely, there is no efﬁcient learning algorithm that is provably successful for most weights, and every input distribution. It implies that there is no generic property that holds with high probability in such random networks and allows efﬁcient learning. 1

Introduction
Neural networks have revolutionized performance in multiple domains, such as computer vision and natural language processing, and have proven to be a highly effective tool for solving many challenging problems. This impressive practical success of neural networks is not well understood from the theoretical point of view. In particular, despite extensive research in recent years, it is not clear which models are learnable by neural networks algorithms.
Historically, there were many negative results for learning neural networks, and it is now known that under certain complexity assumptions, it is computationally hard to learn the class of functions computed by a neural network, even if the architecture is very simple. Indeed, it has been shown that learning neural networks is hard already for networks of depth 2 [35, 17]. These results hold already for improper learning, namely where the learning algorithm is allowed to return a hypothesis that does not belong to the considered hypothesis class.
In recent years, researchers have considered several ways to circumvent the discrepancy between those hardness results and the empirical success of neural networks. Namely, to understand which models are still learnable by neural networks algorithms. This effort includes proving learnability of linear models, including polynomials and kernel spaces [5, 47, 19, 15, 10, 32, 24, 39, 3, 4, 11, 48, 44, 29, 40, 8, 11, 34, 38, 36, 16], making assumptions on the input distribution [37, 9, 22, 23, 21, 31, 43], the network’s weights [7, 43, 20, 2, 30], or both [33, 45]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In that respect, one fantastic result that can be potentially proven, is that neural networks are efﬁciently learnable if we assume that the network’s weights are “well-behaved". Namely, that there are some generic properties of the network’s weights that allow efﬁcient learning. This approach is supported by the intuition that the weights in real-world networks are not arbitrary, but exhibit some ”random-like" properties with respect to some ”natural weights distributions" (e.g., where the weights are drawn from a normal distribution). We say that a property of the network’s weights is a natural property with respect to such a natural weights distribution, if it holds with high probability. Existing hardness results focus on the network architecture, and assume that the weights are arbitrary. Thus, it is unclear whether there exists a natural property that allows efﬁcient learning.
In this work, we investigate networks with random weights, and networks whose weights posses natural properties. We show that under various natural weights distributions most networks are hard to learn. Namely, there is no efﬁcient learning algorithm that is provably successful for most weights, and every input distribution. We show that it implies that learning neural networks is hard already if their weights posses some natural property. Our hardness results are under the common assumption that refuting a random K-SAT formula is hard (the RSAT assumption). We emphasize that our results are valid for any learning algorithm, and not just common neural networks algorithms.
We consider networks of depth 2 with a single output neuron, where the weights in the ﬁrst layer are drawn from some natural distribution, and the weights in the second layer are all 1. We consider multiple natural weights distributions, e.g., where the weights vector of each hidden neuron is distributed by a multivariate normal distribution, distributed uniformly on the sphere, or that each of its components is drawn i.i.d. from a normal, uniform or Bernoulli distribution. For each weights distribution, we show that learning such networks with high probability over the choice of the weights is hard. Thus, for such weights distributions, most networks are hard. It implies that there is no generic property that holds w.h.p. (e.g., with probability 0.9) in such random networks and allows efﬁcient learning. Hence, if generic properties that allow efﬁcient learning exist, then they are not natural, namely, they are rare with respect to all the natural weights distributions that we consider.
We also consider random neural networks of depth 2, where the ﬁrst layer is a convolutional layer with non-overlapping patches such that its ﬁlter is drawn from some natural distribution, and the weights of the second layer are all 1. We show that learning is hard also for such networks. It implies that there is no generic property that holds w.h.p. in such random convolutional networks and allows efﬁcient learning.