Abstract
Existing adversarial learning approaches mostly use class labels to generate adver-sarial samples that lead to incorrect predictions, which are then used to augment the training of the model for improved robustness. While some recent works propose semi-supervised adversarial learning methods that utilize unlabeled data, they still require class labels. However, do we really need class labels at all, for adversarially robust training of deep neural networks? In this paper, we propose a novel adver-sarial attack for unlabeled data, which makes the model confuse the instance-level identities of the perturbed data samples. Further, we present a self-supervised con-trastive learning framework to adversarially train a robust neural network without labeled data, which aims to maximize the similarity between a random augmenta-tion of a data sample and its instance-wise adversarial perturbation. We validate our method, Robust Contrastive Learning (RoCL), on multiple benchmark datasets, on which it obtains comparable robust accuracy over state-of-the-art supervised adversarial learning methods, and signiﬁcantly improved robustness against the black box and unseen types of attacks. Moreover, with further joint ﬁne-tuning with supervised adversarial loss, RoCL obtains even higher robust accuracy over using self-supervised learning alone. Notably, RoCL also demonstrate impressive results in robust transfer learning. 1

Introduction
The vulnerability of neural networks to imperceptibly small perturbations [1] has been a crucial challenge in deploying them to safety-critical applications, such as autonomous driving. Various studies have been proposed to ensure the robustness of the trained networks against adversarial attacks [2–4], random noise [5], and corruptions [6, 7]. Perhaps the most popular approach to achieve adversarial robustness is adversarial learning, which trains the model with samples perturbed to maximize the loss on the target model. Starting from Fast Gradient Sign Method [8] which apply a perturbation in the gradient direction, to Projected Gradient Descent [9] that maximizes the loss over iterations, and TRADES [2] that trades-off clean accuracy and adversarial robustness, adversarial learning has evolved substantially over the past few years. However, conventional methods with adversarial learning all require class labels to generate adversarial attacks.
Recently, self-supervised learning [10–14], which trains the model on unlabeled data in a supervised manner by utilizing self-generated labels from the data itself, has become popular as means of learning representations for deep neural networks. For example, prediction of the rotation angles [10], and solving randomly generated Jigsaw puzzles [11] are examples of such self-supervised learning methods. Recently, instance-level identity preservation [12, 13] with contrastive learning has shown to be very effective in learning the rich representations for classiﬁcation. Contrastive self-supervised learning frameworks such as [12–15] basically aim to maximize the similarity of a sample to its augmentation, while minimizing its similarity to other instances.
In this work, we propose a contrastive self-supervised learning framework to train an adversarially robust neural network without any class labels. Our intuition is that we can fool the model by generat-34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Instance-wise adversarial examples (b) RoCL: Robust contrastive learning (c) RoCL: Robust contrastive learning
Figure 1: Overview of our adversarial contrastive self-supervised learning. (a) We generate instance-wise adversarial examples from an image transformed using a stochastic augmentation, which makes the model confuse the instance-level identity of the perturbed sample. (b) We then maximize the similarity between each transformed sample and their instance-wise adversaries using contrastive learning. (c) After training, each sample will have signiﬁcantly reduced adversarial vulnerability in the latent representation space. ing instance-wise adversarial examples (See Figure 1(a)). Speciﬁcally, we generate perturbations on augmentations of the samples to maximize their contrastive loss, such that the instance-level classiﬁer becomes confused about the identities of the perturbed samples. Then, we maximize the similarity between clean samples and their adversarial counterparts using contrastive learning (Figure 1(b)), to obtain representations that suppress distortions caused by adversarial perturbations. This will result in learning representations that are robust against adversarial attacks (Figure 1(c)).
We refer to this novel adversarial self-supervised learning method as Robust Contrastive Learning (RoCL). To the best of our knowledge, this is the ﬁrst attempt to train robust neural networks without any labels, and to generate instance-wise adversarial examples. Recent works on semi-supervised adversarial learning [16, 17] or self-supervised adversarial learning [18] still require labeled instances to generate pseudo-labels on unlabeled instances or class-wise attacks for adversarial training, and thus cannot be considered as fully-unsupervised adversarial learning approaches.
To verify the efﬁcacy of the proposed RoCL, we suggest a robust-linear evaluation for self-supervised adversarial learning and validate our method on benchmark datasets (CIFAR-10 and CIFAR-100) against supervised adversarial learning approaches. The results show that RoCL obtains comparable accuracy to strong supervised adversarial learning methods such as TRADES [2], although it does not use any labels during training. Further, when we extend the method to utilize class labels to ﬁne-tune the network trained on RoCL with class-adversarial loss, we achieve even stronger robustness, without losing accuracy when clean samples. Moreover, we verify our rich robust representation with transfer learning which shows impressive performance. In sum, the contributions of this paper are as follows:
• We propose a novel instance-wise adversarial perturbation method which does not require any labels, by making the model confuse its instance-level identity.
• We propose a adversarial self-supervised learning method to explicitly suppress the vul-nerability in the representation space by maximizing the similarity between clean examples and their instance-wise adversarial perturbations.
• Our method obtains comparable robustness to supervised adversarial learning approaches without using any class labels on the target attack type, while achieving signiﬁcantly better clean accuracy and robustness on unseen type of attacks and transfer learning. 2