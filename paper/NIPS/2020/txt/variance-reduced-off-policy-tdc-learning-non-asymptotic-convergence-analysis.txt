Abstract
Variance reduction techniques have been successfully applied to temporal-difference (TD) learning and help to improve the sample complexity in policy evaluation. However, the existing work applied variance reduction to either the less popular one time-scale TD algorithm or the two time-scale GTD algorithm but with a ﬁnite number of i.i.d. samples, and both algorithms apply to only the on-policy setting. In this work, we develop a variance reduction scheme for the two time-scale
TDC algorithm in the off-policy setting and analyze its non-asymptotic convergence rate over both i.i.d. and Markovian samples. In the i.i.d. setting, our algorithm achieves a sample complexity O((cid:15)− 3 5 log (cid:15)−1) that is lower than the state-of-the-art result O((cid:15)−1 log (cid:15)−1). In the Markovian setting, our algorithm achieves the state-of-the-art sample complexity O((cid:15)−1 log (cid:15)−1) that is near-optimal. Experiments demonstrate that the proposed variance-reduced TDC achieves a smaller asymptotic convergence error than both the conventional TDC and the variance-reduced TD. 1

Introduction
In reinforcement learning applications, we often need to evaluate the value function of a target policy by sampling the Markov decision process (MDP) generated by either the target policy (on-policy) or a certain behavior policy (off-policy) [3, 9, 20, 24, 22, 27]. In the on-policy setting, temporal-difference (TD) learning [23, 24] and Q-learning [9] algorithms have been developed with convergence guarantee. However, in the more popular off-policy setting, these conventional policy evaluation algorithms have been shown to possibly diverge under linear function approximation [2].
To address this issue, [18, 25, 26] developed a family of gradient-based TD (GTD) algorithms that have convergence guarantee in the off-policy setting. In particular, the TD with gradient correction (TDC) algorithm has been shown to have superior performance and is widely used in practice.
Although TD-type algorithms achieve a great success in policy evaluation, their convergence suffer from a large variance caused by the stochastic samples obtained from a dynamic environment.
A conventional approach that addresses this issue is to use a diminishing stepsize [4, 21], but it signiﬁcantly slows down the convergence in practice. Recently, several work proposed to apply the variance reduction technique developed in the stochastic optimization literature to reduce the variance of TD learning. Speciﬁcally, [11] considered a convex-concave TD learning problem with a ﬁnite number of i.i.d. samples, and they applied the SVRG [12] and SAGA [10] variance reduction techniques to develop variance-reduced primal-dual batch gradient algorithms for solving the problem. In [19], two variants of SVRG-based GTD2 algorithms for i.i.d. samples were proposed to save the computation cost. While these work developed variance-reduced TD-type algorithms 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for i.i.d. samples, practical reinforcement learning applications often involve non-i.i.d. samples that are generated by an underlying MDP. In [14], the authors proposed a variance-reduced TD (VRTD) algorithm for Markovian samples in the on-policy setting. However, their analysis of the algorithm has a technical error and has been corrected in the recent work [28]. To summarize, the existing developments of variance-reduced TD-type algorithms consider only the on-policy setting, and only the one time-scale VRTD algorithm applies to Markovian samples. Therefore, it is very much desired to develop a two time-scale variance-reduced algorithm in the off-policy setting for
Markovian samples, which constitutes to the goal of this paper: we develop two variance-reduced
TDC algorithms in the off-policy setting for i.i.d. samples and Markovian samples, respectively, and analyze their non-asymptotic convergence rates. We summarize our contributions as follows. 1.1 Our Contributions
We develop two variance-reduced TDC algorithms (named VRTDC) respectively for i.i.d. samples and Markovian samples in the off-policy setting and analyze their non-asymptotic convergence rates as well as sample complexities. To the best of our knowledge, our work provides the ﬁrst study on variance reduction for two time-scale TD learning over Markovian samples.
For i.i.d. samples with constant step sizes α, β, we show that VRTDC converges at a linear rate to a neighborhood of the optimal solution with an asymptotic convergence error O(βM −1 + β4), where
M denotes the batch size of the outer-loops. Consequently, to achieve an (cid:15)-accurate solution, the required sample complexity for VRTDC is O((cid:15)− 3 5 log (cid:15)−1), which is lower than that O((cid:15)−1 log (cid:15)−1) of VRTD for i.i.d. samples [28]. Also, the tracking error of VRTDC diminishes linearly with an asymptotic error O(βM −1 + β3) and has a corresponding sample complexity O((cid:15)− 1 2 log (cid:15)−1). For
Markovian samples with constant step sizes α, β, we show that VRTDC converges at a linear rate to a neighborhood of the optimal solution with an asymptotic convergence error O(M −1 + β2), and the required sample complexity to achieve an (cid:15)-accurate solution is O((cid:15)−1 log (cid:15)−1), which matches the best existing result of VRTD [28] and TDC [13] and nearly matches the theoretical lower bound O((cid:15)−1) [13]. Also, the tracking error of VRTDC diminishes linearly with an asymptotic error O(M −1 + β3) and has a corresponding sample complexity O((cid:15)−1 log (cid:15)−1). Furthermore, our experiments on the Garnet problem and frozen lake game demonstrate that VRTDC achieves a smaller asymptotic convergence error than both the conventional TDC and the variance-reduced TD.
Our analysis of VRTDC requires substantial developments of bounding techniques. Speciﬁcally, we develop much reﬁned bounds for the tracking error and the convergence error via a recursive reﬁnement strategy: we ﬁrst develop a preliminary bound for the tracking error (cid:107)˜z(m)(cid:107)2 and then use it to develop a preliminary bound for the convergence error (cid:107)θ(m) − θ∗(cid:107)2. Then, by leveraging the relation between tracking error and convergence error induced by the two time-scale updates of
VRTDC, we further utilize the preliminary bound for the convergence error to obtain a reﬁned bound for the tracking error. Finally, we apply the reﬁned bound for the tracking error to develop a reﬁned bound for the convergence error by leveraging the two time-scale updates. These reﬁned bounds are the key to establish the reported sample complexities of VRTDC. 1.2