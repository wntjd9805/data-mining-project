Abstract
Recent research ﬁnds CNN models for image classiﬁcation demonstrate overlapped adversarial vulnerabilities: adversarial attacks can mislead CNN models with small perturbations, which can effectively transfer between different models trained on the same dataset. Adversarial training, as a general robustness improvement technique, eliminates the vulnerability in a single model by forcing it to learn robust features. The process is hard, often requires models with large capacity, and suffers from signiﬁcant loss on clean data accuracy. Alternatively, ensemble methods are proposed to induce sub-models with diverse outputs against a transfer adversarial example, making the ensemble robust against transfer attacks even if each sub-model is individually non-robust. Only small clean accuracy drop is observed in the process. However, previous ensemble training methods are not efﬁcacious in inducing such diversity and thus ineffective on reaching robust ensemble. We propose DVERGE, which isolates the adversarial vulnerability in each sub-model by distilling non-robust features, and diversiﬁes the adversarial vulnerability to induce diverse outputs against a transfer attack. The novel diversity metric and training procedure enables DVERGE to achieve higher robustness against transfer attacks comparing to previous ensemble methods, and enables the improved robustness when more sub-models are added to the ensemble. The code of this work is available at https://github.com/zjysteven/DVERGE. 1

Introduction
Recent discoveries of adversarial attacks cast doubt on the inherent robustness of convolutional neural networks (CNNs) [1, 2, 3]. These attacks, commonly referred to as adversarial examples, comprise precisely crafted input perturbations that are often imperceptible to humans yet consistently induce misclassiﬁcation in CNN models. Moreover, previous research has demonstrated widespread transferability of adversarial examples, wherein adversarial examples generated against an arbitrary model can reliably mislead other unspeciﬁed deep learning models trained with the same dataset [4, 5, 6]. Ilyas et al. [5] conjecture the existence of robust and non-robust features within standard image classiﬁcation datasets. Whereas humans may understand an image via “human-meaningful” robust features, which usually are insensitive to small additive noise, deep learning models are more prone to learning non-robust features. Non-robust features are highly correlated with output labels and help improve clean accuracy but are not visually meaningful and are sensitive to noise. Such dependency on non-robust features leads to adversarial vulnerability that is exploited by adversarial examples to mislead CNN models. Moreover, Ilyas et al. empirically show that CNN models independently
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Decision regions in the (cid:96)∞ ball around the same testing image learned by ensembles of 3 ResNet-20 models trained on CIFAR-10 dataset. Same color indicates the same predicted label. The vertical axis is along the adversarial direction of a surrogate benign ensemble, and the horizontal axis is along a random Rademacher vector. The same axes are used for each subplot.
Adversarial vulnerability can be inferred from the closest decision boundary and corresponding class. The baseline ensemble is achieved via standard training on clean data while the bottom ensemble is trained with DVERGE. More plots of this nature can be seen in Appendix C.1. trained on the same dataset tend to capture similar non-robust features, demonstrating overlapping vulnerability [5]. This property can be observed from the example in the upper row of Figure 1, where an ensemble is trained on clean data and each of its sub-models are vulnerable along the same axis of a transfer attack. This similarity is key to the high transferability of adversarial attacks [5, 7].
Extensive research has been conducted to improve the robustness of CNN models against adversarial attacks, most notably adversarial training [3]. Adversarial training minimizes the loss of a CNN model on online-generated adversarial examples against itself at each training step. This process forces the model to prefer robust to non-robust features and thereby largely eliminates the model’s vulnerability. Nevertheless, learning robust features is hard, so adversarial training often leads to a signiﬁcant increase in the generalization error on clean testing data [8].
Similar to traditional ensemble methods like bagging [9] and boosting [10], which train an ensemble of weak learners with diverse predictions to improve overall accuracy, a recent line of research proposes to train an ensemble of individually non-robust sub-models that produce diverse outputs against transferred adversarial examples [11, 12, 13]. Intuitively, the approach can defend against black-box transfer attacks as an attack can succeed only when multiple sub-models converge towards the same wrong prediction [13]. Such an ensemble could also hypothetically achieve high clean accuracy since the training process doesn’t exclude non-robust features. Various ensemble training methods have been explored, such as diversifying output logits’ distributions [11, 12] or minimizing the cosine similarity between the input gradient direction of each sub-model [13]. Yet empirical results show that these diversity metrics are not very effective at inducing output diversity among sub-models, and thus the corresponding ensemble can hardly attain the desired robustness [14].
We note that black-box transfer attacks are prevalent in real-world applications where model pa-rameters are not exposed to end users [6, 13]. Moreover, high clean accuracy is always desirable.
We therefore seek an effective training method that mitigates attack transferability while maintain-ing high clean accuracy. Based on a close investigation of the cause of adversarial vulnerability in sub-models, we propose to distill the features learned by each sub-model corresponding to its vulnerability to adversarial examples and use the overlap between the distilled features to measure the diversity between sub-models. As adversarial examples exploit the vulnerability of sub-models, a small overlap between sub-models indicates that a successful adversarial example on one sub-model is unlikely to fool the other sub-model. Consequently, our method impedes attack transferability between sub-models and leads to diverse outputs against a transferred adversarial example. Based on this diversity metric, we propose Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles (DVERGE), which uses a round-robin training procedure to distill and diversify the features corresponding to each sub-model’s vulnerability. The proposed ensemble training method makes the following contributions:
• DVERGE can successfully isolate and diversify the vulnerability in each sub-model such that within-ensemble attack transferability is nearly eliminated; 2
• DVERGE can signiﬁcantly improve the overall robustness of the ensemble against black-box transfer attacks without signiﬁcantly impacting the clean accuracy;
• The diversity induced by DVERGE consistently improves robustness as the number of ensemble sub-models increases under equivalent evaluation conditions.
As shown in the bottom row of Figure 1, diverse vulnerabilities allowed to persist in each sub-model for high clean accuracy by DVERGE combine to yield an ensemble robust to transfer attacks. Our method can also be augmented with the adversarial training objective to yield an ensemble with both satisfying white-box robustness and higher clean accuracy compared to exclusively adversarial training. To the best of our knowledge, this work is the ﬁrst to utilize distilled features for training diverse ensembles and quantitatively relate it to the robustness against adversarial attacks. 2