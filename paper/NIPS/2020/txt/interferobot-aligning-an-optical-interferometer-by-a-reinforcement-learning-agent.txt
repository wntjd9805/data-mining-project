Abstract
Limitations in acquiring training data restrict potential applications of deep rein-forcement learning (RL) methods to the training of real-world robots. Here we train an RL agent to align a Mach-Zehnder interferometer, which is an essential part of many optical experiments, based on images of interference fringes acquired by a monocular camera. The agent is trained in a simulated environment, without any hand-coded features or a priori information about the physics, and subsequently transferred to a physical interferometer. Thanks to a set of domain randomizations simulating uncertainties in physical measurements, the agent successfully aligns this interferometer without any ﬁne tuning, achieving a performance level of a human expert. 1

Introduction
Reinforcement learning is developing explosively nowadays, demonstrating breakthrough results both in computed environments and physical robotics. It achieved above-human performance in Atari video games [1, 2], defeated the world champion in the game of Go [3], and beat the best computer programs in chess and shogi [4]. In robotics, RL has been successful in various manipulation and locomotion tasks [5, 6] such as solving Rubik’s cube [7] and throwing various objects to the goal [8].
Most of the works considering applications of RL to robotics deal with commercial robotic hands
[9, 10] or legged robots [11, 12] in well-deﬁned (sometimes changing [13]) environments. Many of these applications have been demonstrated in “toy” environments of little practical value. In this paper, we train an RL agent, which we dub Interferobot, to solve a commonly occurring practical task in experimental optics: alignment of a Mach-Zehnder interferometer (MZI).
The main bottleneck in the application of RL methods to robotic training is data acquisition. Since
RL is heavily based on trial and error, it requires millions of agent-environment interactions to achieve a stable policy. As a result, the training becomes data inefﬁcient and sometimes even dangerous (for example, for autopilots). One way to overcome this problem is to (pre-)train an agent policy in a simulated environment [14]. Training in simulation lets an agent interact with the environment much faster but introduces a domain gap: bias between simulated and real environment.
Domain randomization can be used to overcome the domain gap and achieve a stable generic 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
policy [15, 16, 17, 18], sometimes with additional ﬁne-tuning in a real environment [19]. However, generalizing this approach to all possible robotic tasks is an open problem.
In present work, we ﬁrst create a simulator of an MZI and train an agent’s policy in this simulated en-vironment based on the observation of images from a monocular camera. The training is implemented without any domain-speciﬁc knowledge about the system and hand-coded features. Subsequently, we transfer the trained policy to a physical setup. We add several types of domain randomization in the training procedure in the simulated environment, which allows the transfer to succeed without
ﬁne-tuning (in a “zero-shot” manner).
Robotic tuning of optical circuits and elements is a dream of many experimental physicists. Tabletop optical experiments require the alignment of tens or even hundreds of different optical elements with micrometric precision. For some experiments, this procedure takes a team of skilled specialists many hours and has to be repeated daily. Fortunately, this alignment is a well-deﬁned task with speciﬁed quality metrics and is hence amenable to robotics. However, to our knowledge, control of real-world optical setups by RL agents was not developed before. 2