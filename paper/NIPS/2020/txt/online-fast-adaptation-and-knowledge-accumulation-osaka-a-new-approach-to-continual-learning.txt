Abstract
Continual learning agents experience a stream of (related) tasks. The main chal-lenge is that the agent must not forget previous tasks and also adapt to novel tasks in the stream. We are interested in the intersection of two recent continual-learning scenarios. In meta-continual learning, the model is pre-trained using meta-learning to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adap-tation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new (out-of-distribution) tasks, while also requiring fast remem-bering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We show in an empirical study that Continual-MAML is better suited to the new scenario than the aforementioned methodologies including standard continual learning and meta-learning approaches. 1

Introduction
A common assumption in supervised machine learning is that the data is independently and identically distributed (i.i.d.). This assumption is violated in many practical applications handling non-stationary data distributions, including robotics, autonomous driving, conversational agents, and other real-time applications. Over the last few years, several methodologies study learning from non-i.i.d. data. We focus on continual learning (CL), where the goal is to learn incrementally from a non-stationary data sequence involving different datasets or tasks, while not forgetting previously acquired knowledge, a problem known as catastrophic forgetting [47].
We draw inspiration from autonomous systems deployed in environments that might differ from the ones they were (pre-)trained on. For instance, a robot pre-trained in a factory and deployed in homes where it will need to adapt to new domains and even solve new tasks. Or a virtual assistant can be pre-trained on historical data and then adapt to its user’s needs and preferences once deployed.
Further motivating applications exist in time-series forecasting including market prediction, game playing, autonomous customer service, recommendation systems, and autonomous driving. These systems must adapt online to maximize their cumulative rewards [30, 31]. As a step in that direction, we propose a task-incremental scenario (OSAKA) where previous tasks reoccur and new tasks appear. corresponding author: massimo.p.caccia@gmail.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We measure the cumulative accuracy of models instead of the (more common) ﬁnal accuracy to evaluate how quickly models and algorithms adapt to new tasks and remember previous ones.