Abstract
Approximate inference techniques are the cornerstone of probabilistic methods based on Gaussian process priors. Despite this, most work approximately optimizes standard divergence measures such as the Kullback-Leibler (KL) divergence, which lack the basic desiderata for the task at hand, while chieﬂy offering merely technical convenience. We develop a new approximate inference method for Gaussian process models which overcomes the technical challenges arising from abandoning these convenient divergences. Our method—dubbed Quantile Propagation (QP)—is similar to expectation propagation (EP) but minimizes the L2 Wasserstein distance (WD) instead of the KL divergence. The WD exhibits all the required properties of a distance metric, while respecting the geometry of the underlying sample space. We show that QP matches quantile functions rather than moments as in
EP and has the same mean update but a smaller variance update than EP, thereby alleviating EP’s tendency to over-estimate posterior variances. Crucially, despite the signiﬁcant complexity of dealing with the WD, QP has the same favorable locality property as EP, and thereby admits an efﬁcient algorithm. Experiments on classiﬁcation and Poisson regression show that QP outperforms both EP and variational Bayes. 1

Introduction
Gaussian process (GP) models have attracted the attention of the machine learning community due to their ﬂexibility and their capacity to measure uncertainty. They have been widely applied to learning tasks such as regression [32], classiﬁcation [57, 21] and stochastic point process modeling [38, 62].
However, exact Bayesian inference for GP models is intractable for all but the Gaussian likelihood function. To address this issue, various approximate Bayesian inference methods have been proposed, such as Markov Chain Monte Carlo [MCMC, see e.g. 41], the Laplace approximation [57], variational inference [26, 42] and expectation propagation [43, 37].
The existing approach most relevant to this work is expectation propagation (EP), which approximates each non-Gaussian likelihood term with a Gaussian by iteratively minimizing a set of local forward
Kullback-Leibler (KL) divergences. As shown by Gelman et al. [17], EP can scale to very large datasets. However, EP is not guaranteed to converge, and is known to over-estimate posterior variances [34, 27, 20] when approximating a short-tailed distribution. By over-estimation, we mean that the approximate variances are larger than the true variances so that more distribution mass lies in the ineffective domain. Interestingly, many popular likelihoods for GPs results in short-tailed 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
posterior distributions, such as Heaviside and probit likelihoods for GP classiﬁcation and Laplacian,
Student’s t and Poisson likelihoods for GP regression.
The tendency to over-estimate posterior variances is an inherent drawback of the forward KL di-vergence for approximate Bayesian inference. More generally, several authors have pointed out that the KL divergence can yield undesirable results such as (but not limited to) over-dispersed or under-dispersed posteriors [11, 30, 22].
As an alternative to the KL, optimal transport metrics—such as the Wasserstein distance [WD, 55,
§6]—have seen a recent boost of attention. The WD is a natural distance between two distributions, and has been successfully employed in tasks such as image retrieval [49], text classiﬁcation [24] and image fusion [7]. Recent work has begun to employ the WD for inference, as in Wasserstein generative adversarial networks [2], Wasserstein variational inference [1] and Wasserstein auto-encoders [54].
In contrast to the KL divergence, the WD is computationally challenging [8], especially in high dimensions [4], in spite of its intuitive formulation and excellent performance,.
Contributions. In this work, we develop an efﬁcient approximate Bayesian scheme that minimizes a speciﬁc class of WD distances, which we refer to as the L2 WD. Our method overcomes some of the shortcomings of the KL divergence for approximate inference with GP models. Below we detail the three main contributions of this paper.
First, in section 4, we develop quantile propagation (QP), an approximate inference algorithm for models with GP priors and factorized likelihoods. Like EP, QP does not directly minimize global distances between high-dimensional distributions. Instead, QP estimates a fully coupled Gaussian posterior by iteratively minimizing local divergences between two particular marginal distributions.
As these marginals are univariate, QP boils down to an iterative quantile function matching procedure (rather than moment matching as in EP) — hence we term our inference scheme quantile propagation.
We derive the updates for the approximate likelihood terms and show that while the QP mean estimates match those of EP, the variance estimates are lower for QP.
Second, in section 5 we show that like EP, QP satisﬁes the locality property, meaning that it is sufﬁcient to employ univariate approximate likelihood terms, and that the updates can thereby be performed efﬁciently using only the marginal distributions. Consequently, although our method employs a more complex divergence than that of EP (L2 WD vs KL), it has the same computational complexity, after the precomputation of certain (data independent) lookup tables.
Finally, in section 6 we employ eight real-world datasets and compare our method to EP and variational Bayes (VB) on the tasks of binary classiﬁcation and Poisson regression. We ﬁnd that in terms of predictive accuracy, QP performs similarly to EP but is superior to VB. In terms of predictive uncertainty, however, we ﬁnd QP superior to both EP and VB, thereby supporting our claim that QP alleviates variance over-estimation associated with the KL divergence when approximating short-tailed distributions [34, 27, 20]. 2