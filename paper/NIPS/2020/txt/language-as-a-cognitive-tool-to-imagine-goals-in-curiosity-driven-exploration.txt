Abstract
Developmental machine learning studies how artiﬁcial agents can model the way children learn open-ended repertoires of skills. Such agents need to create and represent goals, select which ones to pursue and learn to achieve them. Recent approaches have considered goal spaces that were either ﬁxed and hand-deﬁned or learned using generative models of states. This limited agents to sample goals within the distribution of known effects. We argue that the ability to imagine out-of-distribution goals is key to enable creative discoveries and open-ended learning. Children do so by leveraging the compositionality of language as a tool to imagine descriptions of outcomes they never experienced before, targeting them as goals during play. We introduce IMAGINE, an intrinsically motivated deep reinforcement learning architecture that models this ability. Such imaginative agents, like children, beneﬁt from the guidance of a social peer who provides language descriptions. To take advantage of goal imagination, agents must be able to leverage these descriptions to interpret their imagined out-of-distribution goals. This generalization is made possible by modularity: a decomposition between learned goal-achievement reward function and policy relying on deep sets, gated attention and object-centered representations. We introduce the Playground environment and study how this form of goal imagination improves generalization and exploration over agents lacking this capacity. In addition, we identify the properties of goal imagination that enable these results and study the impacts of modularity and social interactions. 1

Introduction
Building autonomous machines that can discover and learn open-ended skill repertoires is a long-standing goal in Artiﬁcial Intelligence. In this quest, we can draw inspiration from children devel-opment [12]. In particular, children exploration seems to be driven by intrinsically motivated brain processes that trigger spontaneous exploration for the mere purpose of experiencing novelty, surprise or learning progress [32, 42, 45]. During exploratory play, children can also invent and pursue their own problems [19].
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: IMAGINE overview. In the Playground environment, the agent (hand) can move, grasp objects and grow some of them. Scenes are generated procedurally with objects of different types, colors and sizes. A social partner provides descriptive feedback (orange), that the agent converts into targetable goals (red bubbles).
Algorithmic models of intrinsic motivation were successfully used in developmental robotics [55, 6], in reinforcement learning [16, 63] and more recently in deep RL [8, 56]. Intrinsically Motivated
Goal Exploration Processes (IMGEP), in particular, enable agents to sample and pursue their own goals without external rewards [7, 26, 27] and can be formulated within the deep RL framework
[25, 53, 22, 58, 71, 60]. However, representing goal spaces and goal-achievement functions remains a major difﬁculty and often requires hand-crafted deﬁnitions. Past approaches proposed to learn image-based representations with generative models such as Variational Auto-Encoders [46, 53], but were limited to the generation of goals within the distribution of already discovered effects. Moving beyond within-distribution goal generation, out-of-distribution goal generation could power creative exploration in agents, a challenge that remains to be tackled.
In this difﬁcult task, children leverage the properties of language to assimilate thousands of years of experience embedded in their culture, in a only a few years [67, 10]. As they discover language, their goal-driven exploration changes. Piaget [57] ﬁrst identiﬁed a form of egocentric speech where children narrate their ongoing activities. Later, Vygotsky [72] realized that they were generating novel plans and goals by using the expressive generative properties of language. The harder the task, the more children used egocentric speech to plan their behavior [72, chap. 2]. Interestingly, this generative capability can push the limits of the real, as illustrated by Chomsky [18]’s famous example of a sentence that is syntactically correct but semantically original “Colorless green ideas sleep furiously”. Language can thus be used to generate out-of-distributions goals by leveraging compositionality to imagine new goals from known ones.
This paper presents Intrinsic Motivations And Goal INvention for Exploration (IMAGINE): a learning architecture which leverages natural language (NL) interactions with a descriptive social partner (SP) to explore procedurally-generated scenes and interact with objects. IMAGINE discovers meaningful environment interactions through its own exploration (Figure 1a) and episode-level NL descriptions provided by SP (1b). These descriptions are turned into targetable goals by the agent (1c). The agent learns to represent goals by jointly training a language encoder mapping NL to goal embeddings and a goal-achievement reward function (1d). The latter evaluates whether the current scene satisﬁes any given goal. These signals (ticks in Figure 1d-e) are then used as training signals for policy learning. More importantly, IMAGINE can invent new goals by composing known ones (1f). Its internal goal-achievement function allows it to train autonomously on these imagined goals.