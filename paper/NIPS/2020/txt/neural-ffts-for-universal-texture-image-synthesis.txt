Abstract
Synthesizing larger texture images from a smaller exemplar is an important task in graphics and vision. The conventional CNNs, recently adopted for synthesis, require to train and test on the same set of images and fail to generalize to unseen images. This is mainly because those CNNs fully rely on convolutional and upsampling layers that operate locally and not suitable for a task as global as texture synthesis. In this work, inspired by the repetitive nature of texture patterns, we
ﬁnd that texture synthesis can be viewed as (local) upsampling in the Fast Fourier
Transform (FFT) domain. However, FFT of natural images exhibits high dynamic range and lacks local correlations. Therefore, to train CNNs we design a framework to perform FFT upsampling in feature space using deformable convolutions. Such design allows our framework to generalize to unseen images, and synthesize textures in a single pass. Extensive evaluations conﬁrm that our method achieves state-of-the-art performance both quantitatively and qualitatively. 1

Introduction
Texture synthesis is the expansion of a small texture example to an arbitrarily larger size while preserving the structural content. It is a challenging task given the wide range of textures a synthesizer should handle. Textures can have regular patterns such as brick walls, or, irregular patterns such as pebbles on the beach. A good synthesizer thus must be: 1) universal to synthesize a wide variety of textures patterns, 2) fast to synthesize in real-time for interactive tasks.
Motivated by the success of deep learning in image-to-image translation tasks, contemporary methods for texture synthesis rely on CNNs. Earlier works employ pre-trained CNNs as feature extractors to match the feature statistics of input and output via iterative optimization that is prohibitively slow [18, 35, 37]. To speed up synthesis, later works train feed-forward CNNs to learn the end-to-end synthesis map that expands textures in a single pass [37, 66, 52, 36].
Those CNNs typically require to train and test on the same set of images, that cannot generalize to unseen textures. We hypothesize that the poor generalization is attributed to the inher-ent difference in the local nature of typical (de)convolution and upsampling layers deployed in those networks, and the global nature of the long-term structural dependencies required by texture synthesis. In essence, texture synthesis involves distributing the input patch across the entire output grid while ensuring structural con-sistency. However, local (de)convolution layers usually operate in small scales, e.g. 3×3 or 5×5.
Figure 1: Typical image-to-image translation CNNs, trained for synthesis, simply upscales the input, whereas our architecture naturally expands the input texture. baseline input ours
∗Joint ﬁrst authors, contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
As a result, they may simply enlarge the local content rather than naturally expand the input texture, even when trained for the texture synthesis task as seen in Fig. 1.
In order to develop a universal texture synthesis scheme, our unique perspective is to cast spatial texture synthesis as local upsampling in the Fast Fourier Transform (FFT) domain. Indeed, Fourier and spatial domains are dual of each other, where local characteristics in one translate to global characteristics in the other, and vice versa. This leads to an interesting property: for texture images with inherent repetitive patterns, that look (semi-)periodic, FFT of a small texture example is simply the downsampled FFT of the full-size texture. Rethinking texture synthesis as FFT upsampling,
CNNs become a natural choice. FFT images however exhibit high dynamic range [21], and lack local correlations that impedes training of CNNs.
For effective training, we design a framework that performs FFT upsampling in the feature space. A convolutional encoder ﬁrst extracts (multi-scale) features from input patch, which are then upsampled in FFT domain with a carefully designed deconvolution network, and subsequently decoded to construct large-size textures. Extensive evaluations are examined on various datasets with an array of quantitative and qualitative metrics. Our observations indicate both high generalization merit and fast speed that outperforms state-of-the-arts by large margins.
All in all, the major contributions of this paper are summarized as follows: 1. We, to the best of our knowledge, for the ﬁrst time formulate the global task of texture synthesis as local FFT upsampling where CNNs endow generalization to unseen textures. 2. We design a novel framework for FFT upsampling in feature space to deal with high dynamic range and non-smooth nature of FFT images. 3. We perform extensive evaluations with an array of quantitative and human-based metrics that show the generalization merits of our scheme compared with several state-of-the-arts. 2