Abstract
Reconstructing 3D objects from 2D images is a fundamental task in computer vision, and an accurate structured reconstruction by parsimonious and semantic primitive representations has an even broader application range. When a target shape is reconstructed using multiple primitives, it is preferable that its fundamental properties, such as collective volume and surface, can be readily and comprehen-sively accessed so that the primitives can be treated as if they were a single shape.
This becomes possible by a primitive representation with uniﬁed implicit and explicit representations. However, primitive representations in current approaches do not satisfy these requirements. To resolve this, we propose a novel primitive representation termed neural star domain (NSD) that learns primitive shapes in a star domain. We demonstrate that NSD is a universal approximator of the star domain; furthermore, it is not only parsimonious and semantic but also an implicit and explicit shape representation. The proposed approach outperforms existing methods in image reconstruction tasks in terms of semantic capability as well as sampling speed and quality for high-resolution meshes. 1

Introduction
Understanding 3D objects by decomposing them into simpler shapes (termed primitives) has been widely studied in computer vision [1–3]. Decomposing 3D objects into parsimonious and semantic primitive representations is important for understanding their structure. Constructive solid geometry
[4] uses combinations of primitives to reconstruct complex shapes.
Recently, learning-based approaches have been adopted to primitive based approaches [5–11]. It has been demonstrated that these approaches enable a semantically consistent part arrangement in various shapes. Moreover, the use of implicit representations allows the set of primitives to be represented as a single collective shape by considering a union [5, 6, 12]; this can improve the reconstruction accuracy during training.
However, the expressiveness of primitives, particularly those with closed shapes, has been limited to simple shapes (cuboids, superquadrics, and convex shapes). Although primitives can learn semantic part arrangements, the semantic shapes of the parts cannot be learned using existing methods. In addition, although the union of primitive volumes could be represented by implicit representations in previous studies, the lack of immediate access to the union of primitive surfaces during training results in complex training schemes [5, 6, 12].
It is challenging to deﬁne a primitive that addresses all these problems. State-of-the-art expressive primitives with explicit surfaces do not have implicit representations [13, 7], and thus they cannot efﬁciently consider unions of primitives to represent collective shapes. Leading primitive representa-tions by convex shapes [5, 6] with implicit representations involve a tradeoff regarding the number
H of half-space hyperplanes deﬁning a convex. Using more hyperplanes yields more expressive convex shapes at the expense of a quadratically growing computation cost in extracting differentiable 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Overview of proposed approach. The primitives have a more meaningful and wider shape variety compared with those in previous studies.
Implicit Explicit
Parsimonious Accurate
DMC [14]
SQ [8]
AtlasNetV2 [7]
BSP-Net [5]
Ours (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Semantic – (cid:88) (cid:88) – (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
Table 1: Overview of shape representations in previous studies. SQ stands for superquadrics [8]. We regard a primitive as having an explicit representation if it can access the explicit surface in both the inference and the training process. Moreover, a primitive representation is said to be semantic if it can reconstruct semantic shapes in addition to part correspondence. surface points. A naive implementation costs O(H 2) to ﬁlter the surface points of a convex from the hyperplanes.
To address these issues, we propose a novel primitive representation termed neural star domain (NSD) that learns shapes in a star domain by using neural networks. A star domain is a group of arbitrary shapes that can be represented by a continuous function deﬁned on the surface of a sphere. As it can express concavity, we can regard it as a generalized shape representation of convex shapes. The learned primitives are visualized in Figure 1. Moreover, we can directly approximate star-domain shapes using neural networks owing to their continuity. We demonstrate that the complexity of the shapes that can be represented by an NSD is equivalent to the approximation ability of the neural network. In addition, as it is deﬁned on the surface of a sphere, a primitive can be represented in both implicit and explicit forms by transforming it between spherical and Cartesian coordinates. The proposed approach is compared with those in previous studies in Table 1.
The contributions of this study can be summarized as follows: (1) We propose a novel primitive representation with high expressive power, and we demonstrate that it is more parsimonious and can learn semantic part shapes. (2) We demonstrate that the proposed primitive provides uniﬁed implicit and explicit representations that can be used during training and inference, leading to improved mesh reconstruction accuracy and speed. 2