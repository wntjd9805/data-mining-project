Abstract
We address the problem of credit assignment in reinforcement learning and explore fundamental questions regarding the way in which an agent can best use additional computation to propagate new information, by planning with internal models of the world to improve its predictions. Particularly, we work to understand the gains and peculiarities of planning employed as forethought via forward models or as hindsight operating with backward models. We establish the relative merits, limitations and complementary properties of both planning mechanisms in carefully constructed scenarios. Further, we investigate the best use of models in planning, primarily focusing on the selection of states in which predictions should be (re)-evaluated. Lastly, we discuss the issue of model estimation and highlight a spectrum of methods that stretch from explicit environment-dynamics predictors to more abstract planner-aware models. 1

Introduction
Credit assignment, i.e. determining how to correctly associate delayed rewards with states or state-action pairs, is a crucial problem for reinforcement learning (RL) agents (Sutton and Barto, 2018).
Model-based RL (MBRL) agents gradually learn a model of the rewards and transition dynamics through interaction with their environments and use the estimated model to ﬁnd better policies or predictions (e.g., Sutton, 1990a; Peng and Williams, 1993; Moore and Atkeson, 1993; McMahan and
Gordon, 2005; Sutton et al., 2012; Farahmand et al., 2017; Farahmand, 2018; Abachi et al., 2020;
Wan et al., 2019; Silver et al., 2017; Schrittwieser et al., 2019; Deisenroth et al., 2015; Hester and
Stone, 2013; Ha and Schmidhuber, 2018; Oh et al., 2017). However, the efﬁciency of MBRL depends on learning a useful model for its purpose. In this paper, we focus speciﬁcally on the use of models for value credit assignment.
Broadly, we refer to planning as any internal processing that an agent can perform without additional experience to improve prediction and/or performance. Within this nomenclature, we deﬁne models as knowledge about the internal workings of the environment, which can be routinely re-used through planning. One way of using models is by forethought or trying things in your head (Sutton and Barto, 1981), which requires learning to predict aspects of the future and planning forward, or in anticipation to achieve goals. Dyna-style planning (Sutton, 1990a) chooses a (possibly hypothetical) state and action and predicts the resulting reward and next state, which are then used for credit assignment.
The origins of the chosen state and action are referred to as search control strategies. Lin (1992) proposed to use states actually experienced, and introduced the idea of replaying prior experience (Lin, 1992; Mnih et al., 2015). Combinations of these two approaches result in prioritized sweeping variants (Moore and Atkeson, 1993; Peng and Williams, 1993; McMahan and Gordon, 2005), which generalize the idea of replaying experience in backward order (Lin, 1992) by prioritizing states based on the potential improvement in the value function estimate upon re-evaluation. From high priority states, forward models are used to perform additional value function updates, increasing computational efﬁciency (e.g., van Seijen and Sutton, 2013). An investigation into source-control strategies (Pan et al., 2018) reveals the utility of additional prioritization for guiding learning towards relevant, causal or interesting states. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we work to understand how different phenomena caused by the structure of an environ-ment favor the use of forward or backward planning mechanisms for credit assignment. We deﬁne backward models as learning to predict potential predecessors of observed states, either through explicit predictors of the environment or via planner-aware models, where the latter account for how the planner performs credit assignment. Backward models are interesting from two standpoints: (i) they can be used to causally change predictions or behaviour in hindsight, thereby naturally prioritizing states where predictions need to be (re)-evaluated; (ii) modeling errors in backward models can sometimes be less detrimental, because updating misspeciﬁed imaginary states with real experience may be less problematic as the reverse (van Hasselt et al., 2019; Jafferjee et al., 2020). We hope that additional understanding of the mechanisms of backward planning paves the way for new, principled algorithms that use models to seamlessly integrate both forethought and hindsight (as had been the case in traditional planning methods – LaValle, 2006).
The estimation and usage of models can be done in many ways (van Hasselt et al., 2019; Van Seijen and Sutton, 2015; Parr et al., 2008; Wan et al., 2019). The conventional approach is to learn explicit predictors of the environment which, if accurate enough, lead to good policies. However, no model is perfect. Model error is dependent on the choice of predictors and whether the true environment dynamics can be represented. Planner-aware model learning suggests learning instead only aspects of the environment relevant to the way in which the model is going to be used by the planner. This class of methods (Farahmand et al., 2017; Farahmand, 2018; Joseph et al., 2013; Silver et al., 2017;
Oh et al., 2017; Farquhar et al., 2017; Luo et al., 2019; D’Oro et al., 2019; Schrittwieser et al., 2019;
Abachi et al., 2020; Ayoub et al., 2020) incorporates knowledge about the value function or policy when learning the model. We describe a spectrum of methods for model estimation. On one end, we have environment predictors that rely on maximum likelihood estimation based on supervised learning. Towards the opposite end, constraints can be progressively relaxed by accounting how planners use the models, ultimately leading to fully abstract models – i.e. learnable black boxes (Xu et al., 2020; Oh et al., 2020).
Contributions We investigate the emergent properties of planning forward and backward with learned models of the world. We justify the use of backward models in identifying relevant states from which to recompute prediction errors, for which we establish available design choices to be made with respect to what the model represents, how it is estimated, and how it is parametrized. We review these in the broader context of model estimation. Finally, we conduct an empirical study on illustrative prediction and control tasks, which builds intuition and provides evidence for our ﬁndings. 2