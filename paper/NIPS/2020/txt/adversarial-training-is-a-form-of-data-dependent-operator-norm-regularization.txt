Abstract
We establish a theoretical link between adversarial training and operator norm regularization for deep neural networks. Speciﬁcally, we prove that `p-norm constrained projected gradient ascent based adversarial training with an `q-norm loss on the logits of clean and perturbed inputs is equivalent to data-dependent (p, q) operator norm regularization. This fundamental connection conﬁrms the long-standing argument that a network’s sensitivity to adversarial examples is tied to its spectral properties and hints at novel ways to robustify and defend against adversarial attacks. We provide extensive empirical evidence on state-of-the-art network architectures to support our theoretical results. 1

Introduction
While deep neural networks are known to be robust to random noise, it has been shown that their accuracy dramatically deteriorates in the face of so-called adversarial examples [4, 43, 17], i.e. small perturbations of the input signal, often imperceptible to humans, that are sufﬁcient to induce large changes in the model output. This apparent vulnerability is worrisome as deep nets start to proliferate in the real-world, including in safety-critical deployments.
The most direct strategy of robustiﬁcation, called adversarial training, aims to robustify a machine learning model by training it against an adversary that perturbs the examples before passing them to the model [17, 24, 30, 29, 26]. A different strategy of defense is to detect whether the input has been perturbed, by detecting characteristic regularities either in the adversarial perturbations themselves or in the network activations they induce [18, 14, 49, 27, 7, 38].
Despite practical advances in ﬁnding adversarial examples and defending against them, it is still an open question whether (i) adversarial examples are unavoidable, i.e. no robust model exists, cf.
[11, 16], (ii) learning a robust model requires too much training data, cf. [40], (iii) learning a robust model from limited training data is possible but computationally intractable [6], or (iv) we just have not found the right model / training algorithm yet.
˜vk – x
Jf p
Juk q
˜uk – x
Jf p vk q 1
´
Figure 1: Our theoretical results suggest to think of iterative adversarial attacks as power-method-like x forward-backward passes (indicated by the arrows) through (the Jacobian Jf p of) the network. q 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this work, we investigate adversarial vulnerability in neural networks by focusing on the attack algorithms used to ﬁnd adversarial examples. In particular, we make the following contributions:
•
•
•
We present a data-dependent variant of spectral norm regularization that directly regularizes large singular values of a neural network in regions that are supported by the data, as opposed to existing methods that regularize a global, data-independent upper bound.
We prove that `p-norm constrained projected gradient ascent based adversarial training with an
`q-norm loss on the logits of clean and perturbed inputs is equivalent to data-dependent (p, q) operator norm regularization.
We conduct extensive empirical evaluations showing among other things that (i) adversarial perturbations align with dominant singular vectors, (ii) adversarial training dampens the singular values, and (iii) adversarial training and data-dependent spectral norm regularization give rise to models that are signiﬁcantly more linear around data than normally trained ones. 2