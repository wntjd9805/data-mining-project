Abstract
The recent progress on automatically searching augmentation policies has boosted the performance substantially for various tasks. A key component of automatic augmentation search is the evaluation process for a particular augmentation policy, which is utilized to return reward and usually runs thousands of times. A plain evaluation process, which includes full model training and validation, would be time-consuming. To achieve efﬁciency, many choose to sacriﬁce evaluation relia-bility for speed. In this paper, we dive into the dynamics of augmented training of the model. This inspires us to design a powerful and efﬁcient proxy task based on the Augmentation-Wise Weight Sharing (AWS) to form a fast yet accurate evaluation process in an elegant way. Comprehensive analysis veriﬁes the superi-ority of this approach in terms of effectiveness and efﬁciency. The augmentation policies found by our method achieve superior accuracies compared with existing auto-augmentation search methods. On CIFAR-10, we achieve a top-1 error rate of 1.24%, which is currently the best performing single model without extra training data. On ImageNet, we get a top-1 error rate of 20.36% for ResNet-50, which leads to 3.34% absolute error rate reduction over the baseline augmentation. 1

Introduction
Deep learning techniques have been heavily utilized in the computer vision area and made remarkable progress in lots of tasks, such as image classiﬁcation [16, 34, 40], object detection [23, 28, 18, 24], segmentation [2, 11], image captioning [36], and human pose estimation [35]. Overﬁt is a commonly acknowledged issue of deep learning algorithms. Various Regularization techniques are proposed in different tasks to ﬁght overﬁt. Data augmentation, which increases both the amount and the diversity of the data by applying semantic invariant image transformations to training samples [34, 1], is the most commonly used regularization due to its simplicity and effectiveness. There are various frequently used augmentation operations for image data, including traditional image transformations such as resizing, cropping, shearing, horizontal ﬂipping, translation, and rotation. Recently, several special operations, such as Cutout [7] and Sample Pairing [14], are also proposed. It has been widely 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: An investigation of the change of rankings in augmented training. We train
ResNet-18 [12] on CIFAR-10 [15] for 300 epochs in total, utilizing different augmenta-tion strategies ([4], [5], and ours).
Figure 2: An investigation of the relation-ship between the performance gains and the augmented training periods. We apply augmentation [4] in the start or the end Naug epochs. observed [17, 16, 37] that augmentation strategies inﬂuence the ﬁnal performances of deep learning models considerably.
However, choosing appropriate data augmentation strategies is time-consuming and requires extensive efforts from experienced human experts. Hence automatic augmentation techniques [4, 5, 20, 13, 21, 41] are leveraged to search for performant augmentation strategy according to speciﬁc datasets and models. Numerous experiments show that these searched policies are superior to hand-crafted policies in many computer vision tasks. These techniques design different evaluation processes to conduct searches.
The most straightforward approach [4] use a plain evaluation process which fully trains the model with different augmentation policies repeatedly to obtain the reward for reinforcement learning agent.
Inevitably, this approach raises the time-consuming issue as it requires a tremendous amount of computational resources to train thousands of child models to complete.
To alleviate the computational cost, most of the efﬁcient works [13, 21, 41] utilize the joint optimiza-tion approach to evaluate the strategies every few iterations, getting rid of training multiple networks from scratch repeatedly. Although being efﬁcient, most of these methods only have the performance similar to that of [5] due to the compromised evaluation process. Speciﬁcally, the compromised evaluation process would distort the ranking for augmentation strategies since the ranks of the models trained with too few iterations are known to be inconsistent with the ﬁnal modelss trained with sufﬁcient iterations. Fig. 1 shows this phenomenon, where the relative ranks change a lot during the whole training process.
An ideal evaluation process should be efﬁcient as well as highly reliable to produce accurate rewards for augmentation strategies. In order to achieve this, we dive into the training dynamics with different data augmentations. We observe that the augmentation operations in the later training period are more inﬂuential. Based on this, we design a new evaluation process, which is a proxy task with an Augmentation-wise Weight Sharing (AWS) strategy. Compared with [4], we improve efﬁciency signiﬁcantly via this weight sharing strategy and make it affordable to directly search on large scale datasets. And the performance gains are also substantial. Compared with previous efﬁcient methods, our method produces more reliable evaluation shown in Sec. 4.4 with competitive computation resources. Our main contribution can be summarized as follows: 1) We propose an efﬁcient yet reliable proxy task utilizing a novel augmentation-wise weight sharing strategy to be the evaluation process for augmentation search methods. 2) We design a new search pipeline for auto-augmentation search utilizing the proposed proxy task and achieved superior accuracy compared with existing auto-augmentation search methods.
The augmentation policies found by our approach achieve outstanding performance. On CIFAR-10, we achieve a top-1 error rate of 1.24%, which is the currently best-performing single model without extra training data. On ImageNet, we get a top-1 error rate of 20.36% for ResNet-50, which leads to 3.34% improvement over the baseline augmentation. The augmentation policies we found on both
CIFAR and ImageNet benchmark will be released to the public as an off-the-shelf augmentation policy to push the boundary of the state-of-the-art performance. 2
2