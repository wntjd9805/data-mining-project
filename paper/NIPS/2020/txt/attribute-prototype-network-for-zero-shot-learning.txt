Abstract
From the beginning of zero-shot learning research, visual attributes have been shown to play an important role. In order to better transfer attribute-based knowl-edge from known to unknown classes, we argue that an image representation with integrated attribute localization ability would be beneﬁcial for zero-shot learning.
To this end, we propose a novel zero-shot representation learning framework that jointly learns discriminative global and local features using only class-level at-tributes. While a visual-semantic embedding layer learns global features, local features are learned through an attribute prototype network that simultaneously regresses and decorrelates attributes from intermediate features. We show that our locality augmented image representations achieve a new state-of-the-art on three zero-shot learning benchmarks. As an additional beneﬁt, our model points to the visual evidence of the attributes in an image, e.g. for the CUB dataset, conﬁrming the improved attribute localization ability of our image representation. 1

Introduction
Visual attributes describe discriminative visual properties of objects shared among different classes.
Attributes have shown to be important for zero-shot learning as they allow semantic knowledge transfer from known to unknown classes. Most zero-shot learning (ZSL) methods [30, 6, 1, 50] rely on pretrained image representations and essentially focus on learning a compatibility function between the image representations and attributes. Focusing on image representations that directly allow attribute localization is relatively unexplored. In this work, we refer to the ability of an image representation to localize and associate an image region with a visual attribute as locality. Our goal is to improve the locality of image representations for zero-shot learning.
While modern deep neural networks [13] encode local information and some CNN neurons are linked to object parts [53], the encoded local information is not necessarily best suited for zero-shot learning. There have been attempts to improve locality in ZSL by learning visual attention [24, 58] or attribute classiﬁers [35]. Although visual attention accurately focuses on some object parts, often the discovered parts and attributes are biased towards training classes due to the learned correlations. For instance, the attributes yellow crown and yellow belly co-occur frequently (e.g. for Yellow Warbler).
Such correlations may be learned as a shortcut to maximize the likelihood of training data and therefore fail to deal with unknown conﬁgurations of attributes in novel classes such as black crown and yellow belly (e.g. for Scott Oriole) as this attribute combination has not been observed before.
To improve locality and mitigate the above weaknesses of image representations, we develop a weakly supervised representation learning framework that localizes and decorrelates visual attributes. More speciﬁcally, we learn local features by injecting losses on intermediate layers of CNNs and enforce
∗xuwenjia16@mails.ucas.ac.cn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
these features to encode visual attributes deﬁning visual characteristics of objects. It is worth noting that we use only class-level attributes and semantic relatedness of them as the supervisory signal, in other words, no human annotated association between the local features and visual attributes is given during training. In addition, we propose to alleviate the impact of incidentally correlated attributes by leveraging their semantic relatedness while learning these local features.
To summarize, our work makes the following contributions. (1) We propose an attribute prototype network (APN) to improve the locality of image representations for zero-shot learning. By regressing and decorrelating attributes from intermediate-layer features simultaneously, our APN model learns local features that encode semantic visual attributes. (2) We demonstrate consistent improvement over the state-of-the-art on three challenging benchmark datasets, i.e., CUB, AWA2 and SUN, in both zero-shot and generalized zero-shot learning settings. (3) We show qualitatively that our model is able to accurately localize bird parts by only inspecting the attention maps of attribute prototypes and without using any part annotations during training. Moreover, we show signiﬁcantly better part detection results than a recent weakly supervised method. 2