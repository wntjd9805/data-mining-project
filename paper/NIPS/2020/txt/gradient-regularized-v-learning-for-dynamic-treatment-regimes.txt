Abstract
Deciding how to optimally treat a patient, including how to select treatments over time among the multiple available treatments, represents one of the most important issues that need to be addressed in medicine today. A dynamic treatment regime (DTR) is a sequence of treatment rules indicating how to individualize treatments for a patient based on the previously assigned treatments and the evolving covariate history. However, DTR evaluation and learning based on ofﬂine data remain challenging problems due to the bias introduced by time-varying confounders that affect treatment assignment over time; this may lead to suboptimal treatment rules being used in practice. In this paper, we introduce Gradient Regularized
V -learning (GRV), a novel method for estimating the value function of a DTR.
GRV regularizes the underlying outcome and propensity score models with respect to the optimality condition in semiparametric estimation theory. On the basis of this design, we construct estimators that are efﬁcient and stable in ﬁnite samples regime. Using multiple simulation studies and one real-world medical dataset, we demonstrate that our method is superior in DTR evaluation and learning, thereby providing improved treatment options over time for patients. 1

Introduction
Clinical decision-makers regularly face the daunting challenge of choosing from multiple treatment options and treatment timings. While clinical trials represent the gold standard for causal inference, clinical trials for longitudinal studies are expensive to conduct. They have few patients and narrow inclusion criteria, and usually do not follow patients for long periods of time. Leveraging increasingly available observational data about patients, such as electronic health records, represents a more viable alternative for developing individualized treatment plans over time. Across multiple communities, including machine learning, statistics and economics, increasing attention has been paid to the need to understand how decision-makers decide which treatments to give patients, and how to construct treatment rules based on their static information [1, 2, 3, 4, 5, 6, 7].
Treatment individualization and adaptation over time are crucial for managing chronic diseases. For example, the time-varying patient information, such as side-effect severity and treatment adherence, drives the treatment for major depressive disorder [8]; clinicians routinely adjust therapy based on the risk of toxicity and antibiotics resistance in treating cystic ﬁbrosis [9]. Unlike in static settings, developing time-varying treatment rules in longitudinal settings poses unique opportunities to understand how diseases evolve under different treatment plans, how individual patients respond to medication over time, and which timings are optimal for assigning treatments. Dynamic treatment regimes (DTRs) [10, 11] offer an attractive causal inference framework that serves this purpose.
A DTR is a sequence of time-varying treatment rules that determine which treatment to provide 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
at each time step, given the patient’s ongoing observation and evolving treatment history. These time-varying treatment rules are also known as adaptive treatment strategies [12, 13, 14, 15, 16] or treatment policies [17, 18, 19]. DTRs provide an effective vehicle for several application areas that have recently been discussed in biomedical literature, including personalized management of chronic conditions for cancer, diabetes, and mental illnesses [20]. Moreover, DTRs can be used to determine when to use chemotherapy or radiotherapy on lung cancer patients for controlling the growth of their tumour volume, when to stop using antibiotics treatment to decrease the white blood cell count (indicative of severe illness and poor outcome) for ICU patients, and when to use ventilation on patients with COVID-19 to maximize their survival outcomes.
To estimate the value function of a DTR, there are three classes of estimators to consider in the literature. Each of these classes has limitations in practice. It is well known that under some circumstances importance sampling-based estimators have high variance, while regression-based estimators have large biases. Doubly robust (DR) estimators have desirable asymptotic properties, but they can still suffer from high variance in ﬁnite samples, due to the unstable inverse propensity score (IPS) product. More in-depth discussion of existing methods is provided in Section 3.
In this paper, we introduce Gradient Regularized V -learning (GRV) as a new method for estimating the value function of a DTR. The GRV estimator is constructed to have stable ﬁnite sample behaviour while achieving the asymptotic efﬁciency. GRV achieves this by regularizing the underlying machine learning models to satisfy the estimating equation in semiparametric estimation theory. We prove theoretically that the estimating equation is satisﬁed when our proposed regularizer is minimized.
Our work can be viewed as an extension of the targeted regularization (TR) method [21] to DTR settings. TR is a debiased method for estimating the average treatment effect (ATE) in static settings.
Our regularizer is designed to solve the complex estimating equation for DTR evaluation, which involves the outcome and propensity score models across different time steps. Our regularizer can also be applied to estimate the average long-term effect of multiple treatments in longitudinal studies.
With experiments on multiple simulation studies, we demonstrate that our method is superior to baseline methods in terms of accuracy in estimating value functions and learning better DTRs. We also demonstrate the application of our method on a real-world dataset extracted from the Medical
Information Mart for Intensive Care (MIMIC) database [22]. 2 DTR Problem
. At each time t
Under the standard assumptions of sequential ignorability, consistency and overlap [10, 23, 24] in the potential outcome framework for causal inference, we consider a dataset consisting of treatment trajectories from a population of N units. For each i
, unit i has a baseline
, unit i has a covariate vector covariate vector in Zi P Z
P r
K which
Xi,t P Xt, outcome variable Yi,t P Yt, and a one-hot treatment vector Ai,t P A “ t indicates which of the K treatment options is assigned to the unit i. In the case of two treatments, we have four options: no treatments, treatment 1, treatment 2, and both treatments.
We collect all the observations about unit i by Oi “ p
, Y are N i.i.d sampled copies of O s r
, A
X time t by Ht “ p t t r s
H0. Then we can factorize the joint distribution P as
T
N r s r s
O
P . We denote the history up to p q “
˜
Ht and the baseline covariates Z by q P
, A
Z, X r q P Ht, ˜Ht “ p
X
Zi, Xi,
P sq „
, Z 1
N s “ t 1, . . . , T
. The vectors O
P r s “ t 1, . . . , N 0, 1 u
, A r
, Ai,
, Yi,
P p
, Z sq
T u u
´
T
T
T
T
T t t s s r r r s r s r s
T
P
P
O p q “
P
H0q p
Xt | p
Ht
P 1q
´
At | p
˜Htq
P
Yt | p
Htq “ g
O p f q
O p h q
O p
, q
“
T t 1 P
O p 1 t
π
“
T
˜Htq where g
At |
Htq t p q “ outcome distribution, and h
O
Xt | is the covariate distribution.
± p q “ p
“
We denote the time t treatment rule by dt : ˜
, which assigns the treatment At as a function of
±
Ht Ñ A the history ˜Ht. A dynamical treatment regime (DTR), d : d1, . . . , dT u d1:T “ t
, is a set that collects all the treatment rules over time. We let Pdt:T denote the joint distribution under the intervention with the indicator function 1 of dt:T , i.e., replacing P
, for is the intervention distribution, f
P
H0q p
Yt | p is the
O p 1q
´ in P 1 P 1 P q “
Ht
±
“
T t
“ (1)
As | p
˜Hsq
O p q
As “ dsp
˜Hsqq 2
`
t, . . . , T . The time t value function under dt:T is given as s
“
T
T
Yr
Vtp dt:T q “
EPdt:T «
The optimal DTR is given as the optimizer d˚ is the value function over the entire treatment trajectory. To learn the optimal DTR d˚, one can ﬁrst consider developing a sample-efﬁcient and low variance estimator to evaluate V arg maxd V where V
  “ t r
ÿ
“ t r
ÿ
“ t r
ÿ
“ d p d p
« ±
EP
“
“
ª
± q q
“
“
.
  (2) dP
Yr
Yr d p
, as will be discussed in Section 3. q
T s t
T s 1
˜Hsqq dsp
As“
˜Hsq
As| t P p
` dPdt:T dP
T 3