Abstract
Continually solving new, unsolved tasks is the key to learning diverse behaviors.
Through reinforcement learning (RL), we have made massive strides towards solving tasks that have a single goal. However, in the multi-task domain, where an agent needs to reach multiple goals, the choice of training goals can largely affect sample efﬁciency. When biological agents learn, there is often an organized and meaningful order to which learning happens. Inspired by this, we propose setting up an automatic curriculum for goals that the agent needs to solve. Our key insight is that if we can sample goals at the frontier of the set of goals that an agent is able to reach, it will provide a signiﬁcantly stronger learning signal compared to randomly sampled goals. To operationalize this idea, we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy. This simple technique samples goals that are neither too hard nor too easy for the agent to solve, hence enabling continual improvement.
We evaluate our method across 13 multi-goal robotic tasks and 5 navigation tasks, and demonstrate performance gains over current state-of-the-art methods. 1

Introduction
Model-free reinforcement learning (RL) has achieved remarkable success in games like Go [49], and control tasks such as ﬂying [26] and dexterous manipulation [4]. However, a key limitation to these methods is their sample complexity. They often require millions of samples to learn a single locomotion skill, and sometimes even billions of samples to learn a more complex skill [7]. Creating general purpose RL agents will necessitate acquiring multiple such skills, which further exacerbates the sample inefﬁciency of these algorithms. Humans, on the other hand, are not only able to learn a multitude of different skills, but are able to do so from orders of magnitude fewer samples [25]. So, how do we endow RL agents with this ability to learn efﬁciently?
When human (or biological agents) learn, they do not simply learn from random data or on uniformly sampled tasks. There is an organized and meaningful order in which the learning is performed. For instance, when human infants learn to grasp, they follow a strict curriculum of distinct grasping strategies: palmar-grasp, power-grasp, and ﬁne-grasp [33]. Following this order of tasks from simple ones to gradually more difﬁcult ones is crucial in acquiring complex skills [38]. This ordered structure is also crucial to motor learning in animals [50, 28]. In the context of machine learning, a learning framework that orders data or tasks in a meaningful way is termed ‘curriculum learning’ [12].
Most research into curriculum learning has focused on the order of data that is presented to a supervised learning algorithm [15]. The key idea is that while training a supervised model, ‘easy’ data should be presented ﬁrst, followed by more difﬁcult data. This gradual presentation of data is shown to improve convergence and predictive performance [12]. However, in the context of reinforcement learning, how should one present a curriculum of data? The answer depends on what aspect of complexity needs to addressed. In this work, we focus on the complexity involved in solving new tasks/goals. Concretely, we operate in the sparse-reward goal-conditioned RL setting [45]. Here, the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: In this work we focus on generating automatic curriculums, where we propose goals that are right at the frontier of the learning process of an agent. Given trajectories of behavior from a goal-conditioned RL policy, our value disagreement based Goal Proposal Module proposes challenging yet solvable goals for that policy. sparse-reward setting reﬂects the inherent difﬁculty of real-world problems where a positive reward is only given when the goal is achieved.
To improve the sample efﬁciency of goal-conditioned RL, a natural framework for using curriculums is to organize the presentation of goals for the RL algorithm. This goal proposer will need to select goals that are informative for policy learning. One option for the goal proposer is to sample goals that have been previously reached [3]. Hence, as the algorithm improves, the sampled goals become more diverse. However, this technique will also re-sample goals that are too easy to give a useful training signal. The central question to improving the goal sampler is hence, how do we select the most useful and informative goals for the learning process?
To sample relevant goals that are maximally informative for the learning process, recent work [51, 39] focuses on using adversaries to sample goals for the agent at hand. Here, the adversary samples goals that are just at the horizon of solvability. These goals form a powerful curriculum since they are neither too easy nor too hard and hence provide a strong learning signal. However, due to the instability in adversarial learning and extra samples needed with multiple agents, these algorithms do not scale well to harder problems. Moreover, setting up an explicit two-player game for different problem settings is not a scalable option.
In this work, we propose a simple, but powerful technique to propose goals that are right at the cusp of solvability (see Figure 1). Our key insight is to look a little closer at the value function. In goal-conditioned settings, the value function of a RL policy outputs the expected rewards of following that policy from a given start state to reach a given goal. Hence, the function contains information about what goals are currently solvable and what goals are not, as well as what goals are right at the cusp of being solved. To retrieve this information, we present Value Disagreement based Sampling (VDS) as a goal proposer. Concretely, we approximate the epistemic uncertainty of the value function, and then sample goals from the distribution induced by this uncertainty measure. For goals that are too easy, the value function will conﬁdently assign high values, while for goals that are too hard, the value function will conﬁdently assign low values. But more importantly, for the goals right at the boundary of the policy’s ability, the value function would have high uncertainty and thus sample them more frequently.
To compute the epistemic uncertainty practically, following recent work in uncertainty mea-surement [30], we use the disagreement between an ensemble of value functions. For evalu-ation, we report learning curves on 18 challenging sparse-reward tasks that include maze nav-igation, robotic manipulation and dexterous in-hand manipulation. Empirically, VDS further improves sample efﬁciency compared to standard RL algorithms. Code is publicly available at https://github.com/zzyunzhi/vds. 2