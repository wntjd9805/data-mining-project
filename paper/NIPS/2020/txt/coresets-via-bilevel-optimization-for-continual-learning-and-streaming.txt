Abstract
Coresets are small data summaries that are sufﬁcient for model training. They can be maintained online, enabling efﬁcient handling of large data streams under resource constraints. However, existing constructions are limited to simple mod-els such as k-means and logistic regression. In this work, we propose a novel coreset construction via cardinality-constrained bilevel optimization. We show how our framework can efﬁciently generate coresets for deep neural networks, and demonstrate its empirical beneﬁts in continual learning and in streaming settings. 1

Introduction
More and more applications rely on predictive models that are learnt online. A crucial, and in general open problem is to reliably maintain accurate models as data arrives over time. Continual learning, for example, refers to the setting where a learning algorithm is applied to a sequence of tasks, without the possibility of revisiting old tasks. In the streaming setting, the data arrives sequentially and the notion of task is not deﬁned. For such practically important settings where data arrives in a non-iid manner, the performance of models can degrade arbitrarily. This is especially problematic in the non-convex setting of deep learning, where this phenomenon is referred to as catastrophic forgetting [42, 23].
One of the oldest and most efﬁcient ways to combat catastrophic forgetting is the replay memory-based approach, where a small subset of past data is maintained and revisited during training. In this work, we investigate how to effectively generate and maintain such summaries via coresets, which are small, weighted subsets of the data. They have the property that a model trained on the coreset performs almost as well as when trained on the full dataset. Moreover, coresets can be effectively maintained over data streams, thus yielding an efﬁcient way of handling massive datasets and streams.
Contributions. We present a novel and general coreset construction framework,1 where we formu-late the coreset selection as a cardinality-constrained bilevel optimization problem that we solve by greedy forward selection via matching pursuit. Using a reformulation via a proxy model, we show that our method is especially well suited for replay memory-based continual learning and streaming with neural networks. We demonstrate the effectiveness of our approach in an extensive empirical study. A demo of our method for streaming can be seen in Figure 1. 2