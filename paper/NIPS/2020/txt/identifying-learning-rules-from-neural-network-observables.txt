Abstract
The brain modiﬁes its synaptic strengths during learning in order to better adapt to its environment. However, the underlying plasticity rules that govern learning are unknown. Many proposals have been suggested, including Hebbian mechanisms, explicit error backpropagation, and a variety of alternatives. It is an open question as to what speciﬁc experimental measurements would need to be made to deter-mine whether any given learning rule is operative in a real biological system. In this work, we take a “virtual experimental” approach to this problem. Simulating idealized neuroscience experiments with artiﬁcial neural networks, we generate a large-scale dataset of learning trajectories of aggregate statistics measured in a variety of neural network architectures, loss functions, learning rule hyperpa-rameters, and parameter initializations. We then take a discriminative approach, training linear and simple non-linear classiﬁers to identify learning rules from features based on these observables. We show that different classes of learning rules can be separated solely on the basis of aggregate statistics of the weights, activations, or instantaneous layer-wise activity changes, and that these results generalize to limited access to the trajectory and held-out architectures and learning curricula. We identify the statistics of each observable that are most relevant for rule identiﬁcation, ﬁnding that statistics from network activities across training are more robust to unit undersampling and measurement noise than those obtained from the synaptic strengths. Our results suggest that activation patterns, available from electrophysiological recordings of post-synaptic activities on the order of several hundred units, frequently measured at wider intervals over the course of learning, may provide a good basis on which to identify learning rules. 1

Introduction
One of the tenets of modern neuroscience is that the brain modiﬁes its synaptic connections during learning to improve behavior [Hebb, 1949]. However, the underlying plasticity rules that govern the process by which signals from the environment are transduced into synaptic updates are unknown.
Many proposals have been suggested, ranging from Hebbian-style mechanisms that seem biologically plausible but have not been shown to solve challenging real-world learning tasks [Bartunov et al., 2018]; to backpropagation [Rumelhart et al., 1986], which is effective from a learning perspective but has numerous biologically implausible elements [Grossberg, 1987, Crick, 1989]; to recent regularized circuit mechanisms that succeed at large-scale learning while remedying some of the implausibilities of backpropagation [Akrout et al., 2019, Kunin et al., 2020]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
A major long-term goal of computational neuroscience will be to identify which of these routes is most supported by neuroscience data, or to convincingly identify experimental signatures that reject all of them and suggest new alternatives. However, along the route to this goal, it will be necessary to develop practically accessible experimental observables that can efﬁciently separate between hypothesized learning rules. In other words, what speciﬁc measurements – of activation patterns over time, or synaptic strengths, or paired-neuron input-output relations – would allow one to draw quantitatively tight estimates of whether the observations are more consistent with one or another speciﬁc learning rule? This in itself turns out to be a substantial problem, because it is difﬁcult on purely theoretical grounds to identify which patterns of neural changes arise from given learning rules, without also knowing the overall network architecture and loss function target (if any) of the learning system.
In this work, we take a “virtual experimental” approach to this question, with the goal of answering whether it is even possible to generically identify which learning rule is operative in a system, across a wide range of possible learning rule types, system architectures, and loss targets; and if it is possible, which types of neural observables are most important in making such identiﬁcations. We simulate idealized neuroscience experiments with artiﬁcial neural networks trained using different learning rules, across a variety of architectures, tasks, and associated hyperparameters. We ﬁrst demonstrate that the learning rules we consider can be reliably separated without knowledge of the architecture or loss function, solely on the basis of the trajectories of aggregate statistics of the weights, activations, or instantaneous changes of post-synaptic activity relative to pre-synaptic input, generalizing as well to unseen architectures and training curricula. We then inject realism into how these measurements are collected in several ways, both by allowing access to limited portions of the learning trajectory, as well as subsampling units with added measurement noise. Overall, we ﬁnd that measurements temporally spaced further apart are more robust to trajectory undersampling. We ﬁnd that aggregated statistics from recorded activities across training are most robust to unit undersampling and measurement noise, whereas measured weights (synaptic strengths) provide reliable separability as long as there is very little measurement noise but can otherwise be relatively susceptible to comparably small amounts of noise. 2