Abstract
Discriminatively localizing sounding objects in cocktail-party, i.e., mixed sound scenes, is commonplace for humans, but still challenging for machines. In this paper, we propose a two-stage learning framework to perform self-supervised class-aware sounding object localization. First, we propose to learn robust object representations by aggregating the candidate sound localization results in the single source scenes. Then, class-aware object localization maps are generated in the cocktail-party scenarios by referring the pre-learned object knowledge, and the sounding objects are accordingly selected by matching au-dio and visual object category distributions, where the audiovisual consistency is viewed as the self-supervised signal. Experimental results in both realis-tic and synthesized cocktail-party videos demonstrate that our model is supe-rior in ﬁltering out silent objects and pointing out the location of sounding ob-jects of different classes. Code is available at https://github.com/DTaoo/
Discriminative-Sounding-Objects-Localization. 1

Introduction
Audio and visual messages are pervasive in our daily-life. Their natural correspondence provides humans with rich semantic information to achieve effective multi-modal perception and learning [28, 24, 15], e.g., when in the street, we instinctively associate the talking sound with people nearby, and the roaring sound with vehicles passing by. In view of this, we want to question that can they also facilitate machine intelligence?
To pursue the human-like audiovisual perception, the typical and challenging problem of visually sound localization is highly expected to be addressed, which aims to associate sounds with speciﬁc visual regions and rewards the visual perception ability in the absence of semantic annotations [14, 18, 3, 27, 10]. A straightforward strategy is to encourage the visual features of sound source to take higher similarity with the sound embeddings, which has shown considerable performance in the simple scenarios with single sound [21, 22, 27]. However, there are simultaneously multiple sounding objects as well as silent ones (i.e. The silent objects are considered capable of producing sound.). in our daily scenario, i.e., the cocktail-party, this simple strategy mostly fails to discriminatively localize different sound sources from mixed sound [16]. Recently, audiovisual content modeling is proposed to excavate concrete audio and visual components in the scenario for localization. Yet, due to lack of sufﬁcient semantic annotation, existing works have to resort to extra scene prior knowledge [16, 17, 25] or construct pretext task [31, 30]. Even so, these methods cannot well deal
∗Corresponding Author, Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling
School of Artiﬁcial Intelligence, Renmin University of China, Beijing 100872, China. The research reported in this paper was mainly conducted when the corresponding author worked at Baidu Research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
with such complex cocktail-party scenario, i.e., not only answering where the sounding area is but also answering what the sounding area is.
In this paper, we target to perform class-aware sounding object localization from their mixed sound, where the audiovisual scenario consists of multiple sounding objects and silent ob-jects, as shown in Fig. 1. This interesting problem is quite challenging from two perspectives: 1) Discriminatively localizing objects belong-ing to different categories without resorting to semantic annotations of objects; 2) Determin-ing whether a speciﬁc object is sounding or not, and ﬁltering out silent ones from the correspond-ing mixed sound. When faced with these chal-lenges, we want to know how do we human ad-dress them? Elman [9] stated that human could transform these seemingly unlearnable tasks into learnable by starting from a simpler initial state then building on which to develop more compli-cated representations of structure. Inspired by this, we propose a two-stage framework, evolv-ing from single sound scenario to the cocktail-party case. Concretely, we ﬁrst learn potential object knowledge from sound localization in sin-gle source scenario, and aggregate them into a dictionary for pursuing robust representation for each object category. By referring to the dictio-nary, class-aware object localization maps are accordingly proposed for meeting the sounding object selection in multi-source scenario. Then, we reduce the sounding object localization task into a self-supervised audiovisual matching problem, where the sounding objects are selected by minimizing the category-level audio and visual distribution difference. With these evolved curriculums, we can ﬁlter out silent objects and achieve class-aware sounding object localization in a cocktail-party scenario.
Figure 1: An example of cocktail-party scenario, which contains sounding guitar, sounding cello and silent saxophone. We aim to discrim-inatively localize the sounding instruments and ﬁlter out the silent ones. Video URL: https://www.youtube.com/watch?v=ebugBtNiDMI.
To summarize, our main contributions are as follows. First, we introduce an interesting and chal-lenging problem, i.e., discriminatively localizing sounding objects in the cocktail-party scenario without manual annotation for objects. Second, we propose a novel step-by-step learning framework, which learns robust object representations from single source localization then further expands to the sounding object localization via taking audiovisual consistency as self-supervision for category distribution matching in the cocktail-party scenario. Third, we synthesize some cocktail-party videos and annotate sounding object bounding boxes for the evaluation of class-aware sounding object localization. Our method shows excellent performance on both synthetic and realistic data. 2