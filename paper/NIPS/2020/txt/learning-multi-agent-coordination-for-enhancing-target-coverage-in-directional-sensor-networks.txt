Abstract
Maximum target coverage by adjusting the orientation of distributed sensors is an important problem in directional sensor networks (DSNs). This problem is challenging as the targets usually move randomly but the coverage range of sensors is limited in angle and distance. Thus, it is required to coordinate sensors to get ideal target coverage with low power consumption, e.g. no missing targets or reducing redundant coverage. To realize this, we propose a Hierarchical Target-oriented Multi-Agent Coordination (HiT-MAC), which decomposes the target coverage problem into two-level tasks: targets assignment by a coordinator and tracking assigned targets by executors. Speciﬁcally, the coordinator periodically monitors the environment globally and allocates targets to each executor. In turn, the executor only needs to track its assigned targets. To effectively learn the HiT-MAC by reinforcement learning, we further introduce a bunch of practical methods, including a self-attention module, marginal contribution approximation for the coordinator, goal-conditional observation ﬁlter for the executor, etc. Empirical results demonstrate the advantage of HiT-MAC in coverage rate, learning efﬁciency, and scalability, comparing to baselines. We also conduct an ablative analysis on the effectiveness of the introduced components in the framework. 1

Introduction
We study the target coverage problem in Directional Sensor Networks (DSNs). In DSNs, every node is equipped with a "directional" sensor, which perceives a physical phenomenon in a speciﬁc orientation.
Cameras, radars, and infrared sensors are typical examples of directional sensors. In some real-world applications, the sensors in DSNs are required to dynamically adjust their own orientation to track mobile targets, such as automatically capturing sports game videos1, actively tracking interesting objects [1]. To realize these applications, the target coverage acts as a crucial point, which puts emphasis on how to cover the maximum number of targets with the ﬁnite number of directional sensors. It is challenging as the targets usually move randomly but the locations of sensors are ﬁxed.
Meanwhile, the coverage range for sensors is limited in angle and distance. To do this, it is required to collaboratively adjust the orientation of each sensor in DSNs by a multi-agent system to cover targets. In practice, the multi-agent system for DSNs should : 1) accomplish the global task via
* indicates equal contribution 1https://playsight.com/automatic-production/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
multi-agent collaboration/coordination 2) be of good generalization to different environments 3) be low-cost in communication and power consumption.
In this paper, we are interested in building such a multi-agent collaborative system via multi-agent reinforcement learning (MARL), where the agents are learned by trial and error. The simplest way is to build a centralized controller to globally observe and control the DSNs simultaneously. And we can formulate it as a single-agent RL problem and directly optimize the controller by the off-the-shelf algorithms [2, 3]. However, it is usually infeasible in real-world scenarios. It is because that the system highly relies on real-time communication between the controller and sensors. Moreover, it is hard to further extend the system for large scale networks, as the computational cost in the server will be dramatically expanded with the increasing agent numbers. Recently, the RL community has taken great efforts on learning a fully decentralized multi-agent collaboration [4–6] for various applications, e.g. playing real-time strategy games [7], controlling trafﬁc light
[8], self-organizing swarm system[9]. In a decentralized system, each agent runs individually, which observe the environment by themselves and exchange their information by peer-to-peer communication. Such a decentralized system could run on a large scale multi-agent system and be low-cost on communication (even without communication). But in most cases, the distributed policy is unstable and difﬁcult to learn, as they usually affect others leading to a non-stationary environment. Even though this issue has been mitigated by the recent centralized training and decentralized execution methods [4–6], a remaining open challenge is how to effectively train a centralized critic to decompose the global reward to each agent for learning the optimal distributed policy, i.e. multi-agent credit assignment problem [10, 11].
To this end, we are motivated to explore a feasible solution to combine the advantages of above methods to learn a multi-agent system for the target coverage problem effectively.
We propose a Hierarchical Target-oriented Multi-agent Coordination framework (HiT-MAC) for the target coverage problem, inspired by the recent success in Hierarchical Reinforcement Learning (HRL) [12–14]. This framework is a two-level hierarchy, composed of a centralized coordinator (high-level policy) and a number of distributed executors (low-level policy), shown as Fig. 1. While running, (a) the coordinator collects the observations from executors and allocates goals (a set of targets to track) for each executor, and (b) each executor individually takes primitive actions to complete the given goal for k time steps, i.e. tracking the assigned targets. After the k steps execution, the coordinator is activated again. Then, steps a and b iterate. In this way, the target coverage problem in DSNs is decomposed into two sub-tasks at different temporal scales. Both coordinator and executors can be trained by the modern single-agent reinforcement learning method (e.g. A3C [2]) to maximize expected future team reward (coordinator) and goal-conditioned rewards (executors), respectively. Speciﬁcally, the team reward is factored by the coverage rate; the goal-conditioned reward is about the performance of a sensor to track the selected targets, measured by the relative angle among sensor and target. So, it can also be considered as the cooperation between the coordinator and executors.
To implement a scalable HiT-MAC, there are two challenges to overcome: (1) For the coordinator, how to learn a policy to handle the assignment among variable numbers of sensors and targets? (2) (a) (b)
Figure 1: An overview of the HiT-MAC framework. Fig. 1(a) is the two-level hierarchy of HiT-MAC.
Periodically (every k steps), the high-level policy (coordinator) πH ((cid:126)g|(cid:126)o) collects joint observation (cid:126)o = (o1, . . . , on) from sensors and distributes target-oriented goal gi to each low-level policy (executor). In turn, the executor πL i (ai|oi, gi) directly interacts with the environment to track its own targets. The observation oi describes the spatial relation between sensor i and targets. The goal gi allocates the targets to be followed by the executor i. Note that the solid line and the dashed line are executed at every step and every k steps respectively. Fig. 1(b) is the details of the coordinator and executor. The critics for them are only used while training the networks. Please refer to Sec. 3 for more details. 2
For the executor, how to train a robust policy that could perform well in any possible cases, e.g. given different target combinations? Hence, we employ a battery of practical methods to address these challenges. Speciﬁcally, we adopt the self-attention module to handle variable input size and generate a order-invariant representation. We estimate values by approximating the marginal contribution (AMC) of each pair of the sensor-target assignments. With this, the critic could better estimate and decompose the team value in a more accurate way, which guides to a more effective coordination policy. For the executor, we further introduce a goal-conditioned ﬁlter to remove the observation of the irrelative targets and a goal generation strategy for training.
We demonstrated the effectiveness of our approach in a simulator, comparing with the state-of-the-art MARL methods, and a heuristic method. To be speciﬁc, our method achieves the highest coverage ratio and fastest convergence in the case of 4 sensors and 5 targets. We also validate the good transferability of HiT-MAC in environments with different numbers of sensors (2 ∼ 6) and targets(3 ∼ 7). Besides, we also conduct an ablation study to analyze the contribution of each key component.
Our contributions can be summarized in three-folds:
• We study the target coverage problem in DSNs and propose a Hierarchical Target-oriented
Multi-agent Coordination framework (HiT-MAC) for it. To the best of our knowledge, it is the ﬁrst hierarchical reinforcement learning method for this problem.
• A bunch of practical methods is introduced to effectively learn a generalizable HiT-MAC, including a self-attention module, marginal contribution approximation, goal-conditioned
ﬁlter, and so on.
• We release a numerical simulator to mimic the real scenario and conduct experiments in the environments to illustrate the effectiveness of the introduced framework. 2 Preliminary
Problem Deﬁnition. The target coverage problem considers how to use a number of active sensors to continuously cover maximum number of targets. In this case, there are n sensors and m mobile targets in the environment. Sensors are randomly placed in the environment with limited coverage range. The targets randomly walk around the environment. A target is covered by the sensor network, once it is monitored by at least one sensor. The orientation of the sensor is adjustable, but the changing angle at each step is restricted considering the physical constraints. Besides, considering the efﬁciency problem, every movement will take additional cost in power.
Dec-POMDPs. It is natural to formulate the target coverage problem in n sensors networks as a
Dec-POMDPs [15]). It is governed by the tuple (cid:104)N, S, {Ai}i∈N , {Oi}i∈N , R, P r, Z(cid:105) where: N is a set of n agents, indexed by {1, 2, ..., n}; S is a set of world states; Ai is a set of primitive actions available for agent i, and forming joint actions (cid:126)at = (a1,t, ..., an,t) with others; Oi is the observation space for agent i, and its local observation oi,t ∈ Oi is drawn from observation function Z(oi,t|st, (cid:126)at);
R : S → R is the team reward function, shared among agents; P r : S × A1 × ...An × S → [0, 1] deﬁnes the transition probabilities between states over joint actions. Notably, the subscript t ∈
{1, 2, ...} denotes the time step. At each step, each agent acquires observation oi,t and takes an action ai,t based on its policy πi(ai,t|oi,t). Inﬂuenced by the joint action (cid:126)at, the state st is updated to a new state st+1 according to P r(st+1|st, (cid:126)at). Meanwhile, the agent i receives the next observation oi,t+1 and the team reward rt+1 = R(st+1). For the cooperative multi-agent task, the ultimate goal is to optimize the joint policy < π1, ..., πn > to maximize the γ discounted accumulated reward with time horizon T : E(cid:126)at∼<π1,...,πn> (cid:104)(cid:80)T t=1 γtrt (cid:105)
.
Hierarchical MMDPs. Considering the hierarchical structure of the coverage problem, we de-compose it into two tasks: high-level coordination and low-level execution. The high-level agent (coordinator) focuses on coordinating n low-level agents (executors) in the long-term to maximize the accumulated team reward (cid:80)T t=1 γtrt. To do it, the coordinator πH ((cid:126)gt|(cid:126)ot) distributes goals (cid:126)gt = (g0,t, g1,t, . . . , gn,t) to the executors, based on the joint observation (cid:126)ot ( collected from execu-tors). After receiving the goal gi,t at time step t, the executor i locally accomplishes the goal for k steps, i.e., maximizing the cumulative goal-conditioned reward rL i,t = RL(st, gi,t), by continuously taking primitive actions ai based on the policy πL i (ai,t|oi,t). Since the coordinator interacts with 3
executors every k > 1 steps, the high-level transition could be regarded as a semi-MDP [16]. And the executors still run on a decentralized style as the Dec-POMDP. Differently, the reward function and policy of each are directed by its goal gi,t introduced in the hierarchy. Thus, the semi-MDP and
Dec-POMDPs form a two-level hierarchy for multi-agent decision making, referring as a hierarchical
Multi-agent MDPs(HMMDPs).
Attention Modules. Attention modules [17, 18] have attracted intense interest due to the great capability in a lot of different tasks [19–21]. Furtherly, the self-attention module can handle variably-sized inputs in an order-invariant way. In the paper, we adopt the scaled dot-product attention [17].
Speciﬁcally, the matrix of output H is a weighted sum of the values, which is computed as:
H = Att(Q, K, V) = sof tmax(
QKT
√ dk
) (cid:12) V (1) where dk is the dimension of a key; the matrix K, Q, V are the keys, queries, and values, transformed from input matrix X by parameter matrices Wq, Wk, Wv. They are computed as:
Q = tanh(WqX), K = tanh(WkX), V = tanh(WvX) (2)
The context feature C = (cid:80)N the element and the total number of elements in H. i=1 hi summarizes elements in H in an additive way, where hi and N are
Approximate Marginal Contribution. In the cooperative game, the marginal contribution ϕC,i(s) of the member i in a coalition C is the incremental value brought by the joining of member i.
Formally, it is ϕC,i(v) = v(C ∪ {i}) − v(C), where v(·) represents the value of a coalition. In a
N player setting, Shapley value[22] measures the average of marginal contributions of member i in all possible coalitions, written as (cid:80)
ϕC,i(v). N \i denotes the subset of N consisting of all the players except member i. Thus, the contributions made by every member can be calculated, once all the sub-coalition contributions v(C) are given. However, it is infeasible to calculate it in practice, as the number of all possible coalitions will be expanded with increasing members N , which causes the computational catastrophe. Hence, [11] introduced a method to approximate the marginal contribution by deep neural networks. In this paper, we approximate the marginal contribution of each pair of sensor-target assignments by neural network for learning a coordinator effectively, rather than estimate the marginal contribution of each player.
|C|!(N −|C|−1)!
N !
C∈N \i 3 Hierarchical Target-oriented Multi-Agent Coordination
Hierarchical Target-oriented Multi-Agent Coordination (HiT-MAC) is a two-level hierarchy, con-sisting of a coordinator (high-level policy) and n executors (low-level policy), shown as Fig. 1. The coordinator and executors respectively follow the semi-MDP and goal-conditioned Dec-POMDPs in HMMDPs. Periodically, the coordinator aggregates the observations (cid:126)o = (o1, o2, . . . , on) from the executors and distributes a target-oriented goal (cid:126)g = (g1, g2, . . . , gn) to them. After receiving gi, the executor i will minimize the average angle error to the assigned targets by rotation for k steps based on its policy πL i (ai|oi, gi). The framework is target-oriented in three-folds: 1) the observation oi describes the spatial relations among sensor i and all targets M in the environment; 2) the gi explicitly identiﬁes a subset of targets Mi ⊆ M for the executor i to focus on. 3) the rewards for both levels are highly dependent on the spatial relations among sensors and targets, i.e, the team reward is about the overall coverage rate of targets, the reward for the executor i is about the average angle error between the executor i and its assigned targets.
In the following, we will introduce the key ingredients for HiT-MAC in details. 3.1 Coordinator: Assigning Targets to Executors
The coordinator seeks to learn an optimal policy πH∗((cid:126)g|(cid:126)o) that can maximize the cumulative team reward by assigning appropriate targets {Mi}i∈N for each executor i ∈ N to track. Note that the coordinator only runs periodically (every k steps) to wait for the low-level execution and save the cost in communication and computation.
Team reward function rH j=1 Ij,t if t any target covered (Condition a). Ij,t represents the covering state of target j at time step t, where 1 for the coordinator is equal to the target coverage rate 1 m (cid:80)m 4
is being covered and 0 is not. Notably, if none of targets is covered (Condition b), we will give an additional penalty in the reward. The overall team reward is shown as following: rH t = R(st) = (cid:80)m (cid:40) 1 m
−0.1 j=1 Ij,t (a) (b) (3)
The coordinator is implemented by building a deep neural network, which is composed of three parts: state encoder, actor, critic. There are mainly two challenges to build the coordinator. First, the shapes of the joint observation (cid:126)o and goal (cid:126)g depend on the number of sensors and targets in the environment.
Second, it is inefﬁcient to explore target-assignments only with a team reward directly, especially when the goal space is expanded with the increasing number of sensors and targets. Thus, the network should be capable of 1) handling the variably-sized input and output; 2) ﬁnding an effective approach for the critic to estimate values.
State encoder adopts the self-attention module to encode the joint observation (cid:126)o ∈ [oi,j]n×m to an order-invariant representation H ∈ Rn×m×datt. Note that oi,j is a din dimensional vector, indicating the spatial relation between sensor i and target j. In our setting, oi,j = (i, j, ρij, αij), where ρij and αij are the relative distance and angle respectively. Please refer to Sec. 4.1 for more details.
To feed (cid:126)o into the attention module, we ﬂatten it from Rn×m×din to Rnm×din , then encode it as
H = Att(Q, K, V), where Q, K, V are derived from the ﬂatten observation according to Eq. 2.
Actor adaptively outputs the goal map (cid:126)g ∈ Nn×m according to H ∈ Rnm×datt . Firstly, we reshape
H as [n, m, datt] again, and compute the probability pij of each assignment by one fully connected layer, pij = fa(Hi,j). Then, we sample the assignment gi,j by probability. gi,j is a binary value, indicating if let sensor i to track target j. At the end, the actor outputs the goal map (cid:126)g for executors, where gi = (gi,1, gi,2, ..., gi,j) denotes the targets assignment for the sensor i.
Critic learns a value function, which is then used to update the actor’s policy parameters in a direction of performance improvement. Rather than directly estimating the global value by a neural network, we introduce an approximate marginal contribution (AMC) approach for learning the critic more efﬁciently. Similar to most multi-agent cooperation problems, we deduce the individual contribution of each member to the team’s success, referred as credit assignment. Differently, we regard each sensor-target pair of the assignments, instead of the agent, as a member of the team. It is because that the coordinator undertakes all the sensor-target assignments, which will directly affect global rewards (if the executors are perfect). Identifying the contribution of each sensor-target assignment to the team reward will be beneﬁcial for a reasonable and effective coordination policy, and such a policy leads to better cooperation among the executors.
Inspired by [11], we approximate the marginal contribution of each assignment (assigning target j to agent i) by neural network φ. The input is H ∈ Rnm×datt from the state encoder. The length of H is l = nm, then it can be regarded as a l-member cooperation. So, the marginal contribution is approximated as ϕe = φ([ηe, ze]). Here [·, ·] denotes concatenation, ηe is the embedded feature of the sub-coalition Ce = {1, ..., e − 1} for the member e. For example, if the grand coalition is [z1, z2, z3, z4], then the η3 is the context feature of [z1, z2], which is used for computing the marginal contribution of the member 3. So, the credit assignment is conducted among all the pairwise sensor-target assignment in the coordinator as Alg.1.
Algorithm 1: Estimate team value with AMC
Input: the state representation H ∈ Rnm×datt
Output: estimated global team value vH 1 Initialize the sub-coalition feature η1 = 0 2 Given an attention module Att(cid:48)(·) and a value network φ(·) 3 l = n*m 4 for e=1 to l do 5 6
Compute the marginal contribution ϕe = φ([ηe, he]), where he is e-th element in H
Compute element-wise features of the sub-coalition H(cid:48) = Att(cid:48)(H[1 : e])
Compute the embedded feature ηe+1 = (cid:80)e i is the i-th element in H(cid:48) i, where h(cid:48) i=1 h(cid:48) 7 8 end 9 The team value vH = (cid:80)l e=1 ϕe 5
Our AMC is conducted on value vH , which is different from SQDDPG [11]. It is because that AMC is conducted on value Q in SQDDPG [11], which would introduce an extra assumption, i.e. the actions taken in C should be the same as the ones in the coalition C ∪ {i}, detailed in Appendix 6.1.
Our global value estimation is also different from the existing methods, like [5, 23], because ours refers the sub-coalition contribution to make a more conﬁdent estimation of the contribution from each member. Theoretically, the permutation of the coalition formation order should be sampled like the computation of Shapley Value[22]. However, we observe that the permutation of hidden states is useless in our case. And the promotion caused by permutation is also not obvious enough shown in
[11]. So, we ﬁx the order in the implementation, i.e. from 1 to l. 3.2 Executor: Tracking Assigned Targets
After receiving the goal gi from the coordinator, the executor πL i (ai|oi, gi) completes the goal-conditional task independently. In particular, the goal of executor i is tracking a set of assigned targets
Mi, i.e, minimize the average angle error to them.
For training, we further introduce a goal-conditioned reward rL i,t(st, gt) to evaluate the executor.
We score the tracking quality of the assigned targets based on the average relative angle, referring to
Eq. 4. We consider two conditions, which are (a) the target j is in the coverage range of the sensor i, i.e. ρij,t < ρmax&|αij,t| < αmax; (b) target is outside of the range. Here αmax is the maximum viewing angle of the sensor, αij,t is the relative angle from the front of the sensor to the target j. (cid:40) rL i,t = 1 mi (cid:88) j∈Mi ri,j,t − βcosti,t, ri,j,t = 1 − |αij,t|
αmax
−1 (a) (b)
, costi,t =
|δi,t − δi,t−1| zδ (4) where Mi is a set of targets selected for the sensor i according to gi; costi,t is the power consumption, measured by the normalized moved angle |δi,t−δi,t−1|
; δi represents the absolute orientation of sensor i, the cost weight β is 0.01 and zδ is the rotation angle, that is 5◦ in our setting.
Goal-conditioned ﬁlter is introduced to directly remove the unrelated relations based on the assigned goal ﬁrstly. With such a clean input, the executor will not be distracted by the irrelevant targets anymore. For example, if gi is [1, 0, 1] and oi is [oi,1, oi,2, oi,3], then ˆoi = f ilter(oi, gi) = [oi,1, oi,3].
In other words, the target-oriented goal can be seen as a kind of hard attention map, forcing the executor only to pay attention to the selected targets. zδ
The network architectures of state encoder, actor and critic are detailed in Appendix 5. The action ai,t is the primitive action and the value vL i,t estimates the coverage quality of the assigned targets of the sensor i. All the executors share the same network parameters. 3.3 Training Strategy
Similar to most hierarchical RL methods, we adopt the two-step training strategy for stability. It is because that a stochastic executor will lead to a poor team reward, which will bring additional difﬁculty for learning coordination. At the same time, the coordinator would generate a lot of meaningless goals, e.g. selecting two targets that are far away from each other, which will make the executor confused and waste time on exploration. Instead, the two-step training can prevent the learning of coordinator/executor from the disturbance of the other.
As for the training of the executor, a goal generation strategy is introduced for training the executor without coordinator. Every k = 10 time steps, we generate the goal, according to the distance between targets and sensors. To be speciﬁc, the targets, whose distances to sensor i are less than the maximum coverage distance (ρij,t < ρmax), will be selected as the goal gi,t for sensor i. Although such strategy mixes some improper targets in the gi,t, this will induce a more robust tracking policy for the executor. With the generated goal, We score the coverage quality of the assigned targets for each executor as the individual reward, refer to Eq. 4. Then, the policy can be easily optimized by the off-the-shelf RL method, e.g. A3C [2].
After that, we train the coordinator cooperating with well-performed executors. While learning, the coordinator updates the observation (cid:126)o and goal (cid:126)g every k steps. During the interval, executor i will take primitive actions ai step-by-step directed by gi. We ﬁx k = 10 in the experiments, while learning an adaptive termination (dynamic k) is our future work. The policy is also optimized by A3C [2]. We notice that directly applying the executor learned in the previous step will lead to the large decrease 6
of the frame rate (only 25 FPS), which causes the training of the coordinator time-consuming. As an alternative, we build a scripted executor to perform low-level tasks to speed up the training process.
The scripted executor could access the internal state for designing a simple yet effective programmed strategy, detailed in Appendix 2. Then, the frame rate for the coordinator increases to 75 FPS. Notably, while testing, we use the learned executor to replace the scripted executor, since the internal state is unavailable in real-world scenarios. 4 Experiments
First, we build a numerical simulator to imitate the target coverage problem in real-world DSNs. Sec-ond, we evaluate HiT-MAC in the simulator, comparing with three state-of-the-art MARL approaches (MADDPG[4], SQDDPG[11] and COMA[6]) and one heuristic centralized method (Integer Linear
Programming, ILP) for this problem. We also conduct an ablation study to validate the contribution of the attention module and AMC in the coordinator. Furthermore, we evaluate the generalization of our method in environments with different numbers of targets and sensors. The code is available at https://github.com/XuJing1022/HiT-MAC and the implementation details are in Appendix 5. 4.1 Environments
To imitate the real-world environment, we build a numerical environment to simulate the target coverage problem in DSNs. At the beginning of each episode, the n sensors are randomly deployed. Meanwhile, the m targets spawn in arbitrary places and walk with random velocity and trajectories.
Observation Space. In every time step, the ob-servation oi is packed by the sensor-target re-lations, i.e. oi = (oi,1, oi,2, ..., oi,m). oi,j = (i, j, ρij, αij) describes the spatial relation between sensor i and target j in a polar coordinate system (the sensor i is at the origin). To be speciﬁc, i, j are the ID of the sensor and target separately; ρij and αij are the absolute distance and relative angle from i to j. For the coordinator in HiT-MAC, it takes (cid:126)o = (o1, ..., on) as the joint observation.
Figure 2: An example of the 2D environment.
Action Space. The primitive action space is discrete with three actions T urnRight, T urnLef t and
Stay. Quantitatively, the T urnRight/T runLef t will incrementally adjust the sensor’s absolute orientation δi in 5 degree, i.e.,Right:δi,t+1+ = 5, Lef t:δi,t+1− = 5. For the coordinator in HiT-MAC, the goal map (cid:126)g is a n × m binary matrix, where gi,j represents whether the target j is selected for the sensor i (0: No, 1: Yes). Each row corresponds to the assignment for each sensor one by one. 4.2 Evaluation Metric
We evaluate the performance of different methods on two metrics: coverage rate and average gain.
Coverage rate (CR) is the primary metric, measuring the percentage of the covered targets among all the targets, shown in Eq. 3; Average gain (AG) is an auxiliary metric to measure the efﬁciency in power consumption. It counts how much CR gains each rotation brings, i.e. CR/ (cid:100)cost, where (cid:100)cost = 1 i=1 costi,t, and costi,t is previously introduced in Eq. 4.
T n
For good performance, we expect both metrics to be high. In practice, we consider the CR in primary.
Only when methods achieve comparable CR, AG is meaningful. For mitigating the bias caused by randomness of training and evaluation, we count the results and draw conclusions after running training for 3 times and evaluation for 20 episodes. (cid:80)n (cid:80)T t=1 4.3 Baselines
We employ MADDPG[4], SQDDPG[11] and COMA[6], three state-of-the-art MARL methods as baselines. They all are trained with a centralized critic and executed in a decentralized manner. But their critics are built in different ways for credit assignment, e.g., SQDDPG[11] aims at estimating the shapley Q-value for each agent. As for the target coverage problem in DSNs, one heuristic method is to formulate the problem as an integer linear programming (ILP) problem and globally optimize it at each step. See Appendix 3&4 for more details. 7
(a) (b)
Figure 3: The learning curve of all learning-based methods. They all are trained in environment with 4 sensors and 5 targets. (a) comparing ours with baselines; (b) comparing ours with its ablations.
Methods
Coverage rate( % ) ↑ Average gain( % ) ↑
Table 1: Comparative results of different methods (n=4&m=5). 4.4 Results
Compare with Baselines. As Fig. 3(a) shows, our method achieves the highest global reward in the setting with 4 sensors and 5 targets. We also draw the mean performance of ILP and random agents in Fig. 3(a) as reference. We can see that state-of-the-art MARL methods work poorly in this setting.
None of them could exceed the ILP. Typically, the improvement of SQDDPG is marginal to agents with random actions. This suggests that it is difﬁcult to directly estimate the marginal contribution of each agent in this problem. Instead, HiT-MAC surpasses all the baselines after 35k of iterations, and reaches a stable performance of ∼ 70 at the end. As for the quantitative results during evaluation in Tab. 1, HiT-MAC consists of the trained coordinator and trained executors and signiﬁcantly outperforms the baselines in CR. ILP gets the highest AG, as it globally optimizes the joint policy step-by-step. COMA and SQDDPG also get higher AG than ours, but in fact, they only learn to take no-operation to wait for the targets run into its coverage range. As a result, their CRs are lower than others.
MADDPG
SQDDPG
COMA
ILP
HiT-MAC(Ours) 45.56±9.45 36.67±9.04 35.37±8.41 54.18±12.32 72.17±5.58 1.38 2.73 2.49 3.87 1.46
Ablation Study. We consider ablations of our method that help us understand the impact of attention framework and global value estimation by AMC shown in the Fig. 3(b). We compare our method with (i) the one without attention encoder; (ii) the one without AMC; (iii) the one without attention encoder nor
AMC in n = 4&m = 5. For (i) and (iii), we use bi-directional
Gated Recurrent Unit (BiGRU [24, 25]) to replace the attention module. As for the critic input, we use context feature Ct for (ii) and the hidden state of BiGRU for (iii). From the learning curve, we can see that the ablations that without AMC (ii, iii) stuck in a locally optimal policy. Their rewards are close to the random policy, which randomly samples the targets as the goal for executors. Instead, the performance of (i) and ours could be further improved after 35k of iterations. This evidence demon-strates that the introduced AMC method is capable of effectively guiding the coordinator to learn a high-quality target assign-ment. Compared with (i), our method with attention-based encoder converges faster and more stably. And the variance of the training curve in (i) is larger than ours, though the one without attention-based encoder can also converge to a high score sometimes. So, we think that the attention-based encoder is more suitable for the coordinator, rather than RNN. It is because that attention mechanism can aggregate the features without any assumption about the sequence order, rather than following a speciﬁc order to encode the data.
Generalization. We analyze the generalization of our method to the different number of sensors n and targets m. While 8 (a) (b)
Figure 4: Analyzing the generaliza-tion of HiT-MAC to the different number of sensors n and targets m. (a) n = 4, m is from 3 to 7; (b) m = 5, n is from 2 to 6.
testing, we adjust the number of targets and sensors in the environment, respectively, and report the mean coverage rate under each setting for better analysis. For example, in Fig. 4(a), we can see the trend of performance with the change of the target number from 3 to 7 in the 4 sensors case. In the same way, we also demonstrate the trend of performance in the cases of varying sensor numbers (n = 2, 3, 4, 5, 6&m = 5), shown as Fig. 4(b). Note that our model is only trained in a ﬁxed-number environment (n = 4&m = 5). We report the rewards of ILP as reference, as its performance does not depend on the training environment and has stable generalization in different settings. Since the score of ILP is already lower than ours, we compare the changing of the score to ensure as much fairness as possible. In Fig. 4(a), the performance of ours increases stably with the decrease of m from 5 to 3, while the reward of ILP increases lightly. With the increase of m from 5 to 7, ours decreases slower than ILP. In Fig. 4(b), our score increases more stably than ILP when n increases from 4 to 6.
Those can be concluded that HiT-MAC is scalable and of a good generalization in environments with different numbers of sensors and targets. 5