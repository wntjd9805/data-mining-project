Abstract
We introduce a general framework for designing and training neural network layers whose forward passes can be interpreted as solving non-smooth convex optimization problems, and whose architectures are derived from an optimization algorithm. We focus on convex games, solved by local agents represented by the nodes of a graph and interacting through regularization functions. This approach is appealing for solving imaging problems, as it allows the use of classical image priors within deep models that are trainable end to end. The priors used in this presentation include variants of total variation, Laplacian regularization, bilateral
ﬁltering, sparse coding on learned dictionaries, and non-local self similarities.
Our models are fully interpretable as well as parameter and data efﬁcient. Our experiments demonstrate their effectiveness on a large diversity of tasks ranging from image denoising and compressed sensing for fMRI to dense stereo matching. 1

Introduction
Despite the undeniable successes of deep learning in domains as varied as image processing [63] and recognition [21], natural language processing [10], speech [43] or bioinformatics [1], feed-forward neural networks are often maligned as being “black boxes” that, except perhaps for their top classiﬁcation or regression layers, are difﬁcult or even impossible to interpret. In imaging applications, for example, the elementary operations typically consist of convolutions and pointwise nonlinearities, with many parameters adjusted by backpropagation, and no obvious functional interpretation.
In this paper, we consider instead network architectures explicitly derived from an optimization algorithm, and thus interpretable from a functional point of view. The ﬁrst instance of this approach we are aware of is LISTA [20], which provides a fast approximation of sparse coding. Yet, we are not content to design an architecture that provides a fast approximation to a given optimization problem, but we also want to learn a data representation pertinent for the corresponding task. This yields an unusual machine learning paradigm, where one learns the parameters of a parametric objective function used to represent data, while designing an optimization algorithm to minimize it efﬁciently.
Even though interpretability is not always necessary to achieve good prediction, this point of view, sometimes called algorithm unrolling [17, 40], has proven successful for solving inverse imaging problems, providing effective and parameter-efﬁcient models. This approach allows the use of domain-speciﬁc priors within trainable deep models, leading to a large number of applications such as compressive imaging [53, 62], demosaicking [26], denoising [26, 50, 52], and super-resolution [56] .
However, existing approaches are often limited to simple image priors such as sparsity induced by the (cid:96)1-norm [52], or differentiable regularization functions [27], and a general algorithmic framework
∗Inria, École normale supérieure, CNRS, PSL Research University, 75005 Paris, France
†Inria, Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for combining complex, possibly non-smooth, regularization functions is still missing. Our paper addresses this issue and is able to leverage a large class of image priors such as total variation [49], the (cid:96)1-norm, structured sparse coding [34], or Laplacian regularization, where local optimization problems interact with each others. The interaction can be local among direct neighbors on an image grid, or non-local, capturing for instance similarities between spatially distant image patches [5, 12].
In this context, we adopt a more general and ﬂexible point of view than the standard convex opti-mization paradigm, and consider formulations to represent data based on non-cooperative games [42] potentially involving non-smooth terms, which are tackled by using the Moreau-Yosida regularization technique [22, 61]. Unrolling the resulting optimization algorithm results in a network architecture that can be trained end-to-end and capture any combination of the domain-speciﬁc priors mentioned above. This approach includes and improves upon speciﬁc trainable sparse coding models based on the (cid:96)1-norm for example [52, 56]. More importantly perhaps, it can be used to construct several interesting new image priors: In particular, we show that a trainable variant of total variation and its non-local variant based on self similarities is competitive with the state of the art in imaging tasks, despite using up to 50 times fewer parameters, with corresponding gains in speed. We demonstrate the effectivness and the ﬂexibility of our approach on several imaging tasks, namely denoising, compressed fMRI reconstruction, and stereo matching.
Summary of our contributions. First, we provide a new framework for building trainable variants of a large class of domain-speciﬁc image priors . Second, we show that several of these priors match or even outperform existing techniques that use a much larger number of parameters and training data. Finally, we present a set of practical tricks to make optimization-driven layers easy to train. 2