Abstract
Actor-critic (AC) methods have exhibited great empirical success compared with other reinforcement learning algorithms, where the actor uses the policy gradient to improve the learning policy and the critic uses temporal difference learning to estimate the policy gradient. Under the two time-scale learning rate schedule, the asymptotic convergence of AC has been well studied in the literature. However, the non-asymptotic convergence and ﬁnite sample complexity of actor-critic methods are largely open. In this work, we provide a non-asymptotic analysis for two time-scale actor-critic methods under non-i.i.d. setting. We prove that the actor-critic method is guaranteed to ﬁnd a ﬁrst-order stationary point (i.e., (cid:107)∇J(θ)(cid:107)2 2 ≤ (cid:15)) of the non-concave performance function J(θ), with (cid:101)O((cid:15)−2.5) sample complexity.
To the best of our knowledge, this is the ﬁrst work providing ﬁnite-time analysis and sample complexity bound for two time-scale actor-critic methods. 1

Introduction
Actor-Critic (AC) methods [2, 16] aim at combining the advantages of actor-only methods and critic-only methods, and have achieved great empirical success in reinforcement learning [31, 1].
Speciﬁcally, actor-only methods, such as policy gradient [28] and trust region policy optimization [24], utilize a parameterized policy function class and improve the policy by optimizing the parameters of some performance function using gradient ascent, whose exact form is characterized by the Policy
Gradient Theorem [28]. Actor-only methods can be naturally applied to continuous setting but suffer from high variance when estimating the policy gradient. On the other hand, critic-only methods, such as temporal difference learning [26] and Q-learning [32], focus on learning a value function (expected cumulative rewards), and determine the policy based on the value function, which is recursively approximated based on the Bellman equation. Although the critic-only methods can efﬁciently learn a satisfying policy under tabular setting [14], they can diverge with function approximation under continuous setting [33]. Therefore, it is natural to combine actor and critic based methods to achieve the best of both worlds. The principal idea behind actor-critic methods is simple: the critic tries to learn the value function, given the policy from the actor, while the actor can estimate the policy gradient based on the approximate value function provided by the critic. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
If the actor is ﬁxed, the policy remains unchanged throughout the updates of the critic. Thus one can use policy evaluation algorithm such as temporal difference (TD) learning [27] to estimate the value function (critic). After many steps of the critic update, one can expect a good estimation of the value function, which in turn enables an accurate estimation of the policy gradient for the actor.
A more favorable implementation is the so-called two time-scale actor-critic algorithm, where the actor and the critic are updated simultaneously at each iteration except that the actor changes more slowly (with a small step size) than the critic (with a large step size). In this way, one can hope the critic will be well approximated even after one step of update. From the theoretical perspective, the asymptotic analysis of two time-scale actor-critic methods has been established in [6, 16]. In speciﬁc, under the assumption that the ratio of the two time-scales goes to inﬁnity (i.e. limt→∞ βt/αt = ∞), the asymptotic convergence is guaranteed through the lens of the two time-scale ordinary differential equations(ODE), where the slower component is ﬁxed and the faster component converges to its stationary point. This type of analysis was also applied in the context of generic two time-scale stochastic approximation [5].
However, ﬁnite-time analysis (non-asymptotic analysis) of two-time scale actor-critic is still largely missing in the literature, which is important because it can address the questions that how many samples are needed for two time-scale actor-critic to converge, and how to appropriately choose the different learning rates for the actor and the critic. Some recent work has attempted to provide the ﬁnite-time analysis for the “decoupled” actor-critic methods [18, 23]. The term “decoupled” means that before updating the actor at the t-th iteration, the critic starts from scratch to estimate the state-value (or Q-value) function. At each iteration, the “decoupled” setting requires the critic to perform multiple sampling and updating (often from another new sample trajectory). As we will see in the later comparison, this setting is sample-inefﬁcient or even impractical. Besides, their analyses are based on either the i.i.d. assumption [18] or the partially i.i.d. assumption [23] (the actor receives i.i.d. samples), which is unrealistic in practice. In this paper, we present the ﬁrst ﬁnite-time analysis on the convergence of the two time-scale actor-critic algorithm. We summarize our contributions as follows:
• We prove that, the actor in the two time-scale actor critic algorithm converges to an (cid:15)-approximate stationary point of the non-concave performance function J after accessing at most (cid:101)O((cid:15)−2.5) samples. Compared with existing ﬁnite-time analysis of actor-critic methods [18, 23], the algorithm we analyzed is based on two time-scale update and therefore more practical and efﬁcient than the
“decoupled” version. Moreover, we do not need any i.i.d. data assumptions in the convergence analysis as required by Kumar et al. [18], Qiu et al. [23], which do not hold in real applications.
• From the technical viewpoint, we also present a new proof framework that can tightly characterize the estimation error in two time-scale algorithms. Compared with the proof technique used in [38], we remove the extra artiﬁcial factor O(tξ) in the convergence rate introduced by their “iterative reﬁnement” technique. Therefore, our new proof technique may be of independent interest for analyzing the convergence of other two time-scale algorithms to get sharper rates.
Notation We use lower case letters to denote scalars, and use lower and upper case bold face letters to denote vectors and matrices respectively. For two sequences {an} and {bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤ Cbn. We use (cid:101)O(·) to further hide logarithm factors. Without other speciﬁcation, (cid:107) · (cid:107) denotes the (cid:96)2 norm of Euclidean vectors. dT V (P, Q) is the total variation norm between two probability measure P and Q, which is deﬁned as dT V (P, Q) = 1/2 (cid:82)
X |P (dx) − Q(dx)|. 2