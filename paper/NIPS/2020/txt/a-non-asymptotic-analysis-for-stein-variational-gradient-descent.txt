Abstract
We study the Stein Variational Gradient Descent (SVGD) algorithm, which opti-mises a set of particles to approximate a target probability distribution π ∝ e−V on Rd. In the population limit, SVGD performs gradient descent in the space of probability distributions on the KL divergence with respect to π, where the gradient is smoothed through a kernel integral operator. In this paper, we provide a novel ﬁnite time analysis for the SVGD algorithm. We provide a descent lemma establishing that the algorithm decreases the objective at each iteration, and rates of convergence for the averaged Stein Fisher divergence (also referred to as Kernel
Stein Discrepancy). We also provide a convergence result of the ﬁnite particle system corresponding to the practical implementation of SVGD to its population version. 1

Introduction
The task of sampling from a target distribution is common in Bayesian inference, where the distribu-tion of interest is the posterior distribution of the parameters. Unfortunately, the posterior distribution is generally difﬁcult to compute due to the presence of an intractable integral. This sampling problem can be formulated from an optimization point of view (Wibisono, 2018). We assume that the target distribution π admits a density proportional to exp(−V ) with respect to Lebesgue measure over
X = Rd, where V : X → R is referred to as the potential function. In this setting, the target distribution π is the solution to the optimization problem deﬁned on the set P2(X ) of probability measures µ such that (cid:82) (cid:107)x(cid:107)2dµ(x) < ∞ by: min
µ∈P2(X )
KL(µ|π), (1) where KL denotes the Kullback-Leibler divergence, and assuming π ∈ P2(X ). Many existing methods for the sampling task can be related to this optimization problem. Variants of the Langevin
Monte Carlo algorithm (Durmus and Moulines, 2016; Dalalyan and Karagulyan, 2019) can be seen 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
as time-discretized schemes of the gradient ﬂow of the relative entropy. These methods generate a Markov chain whose law converges to π under mild assumptions, but the rates of convergence deteriorate quickly in high dimensions (Durmus et al., 2018b). Variational inference methods instead restrict the search space of problem (1) to a family of parametric distributions (Zhang et al., 2018;
Ranganath et al., 2014). These methods are much more tractable in the large scale setting, since they beneﬁt from efﬁcient optimization methods (parallelization, stochastic optimization); however they can only return an approximation of the target distribution.
Recently, the Stein Variational Gradient Descent (SVGD) algorithm (Liu and Wang, 2016) was introduced as a non-parametric alternative to variational inference methods. It uses a set of interacting particles to approximate the target distribution, and applies iteratively to these particles a form of gradient descent of the relative entropy, where the descent direction is restricted to belong to a unit ball in a Reproducing Kernel Hilbert space (RKHS) (Steinwart and Christmann, 2008). In particular, this algorithm can be seen as a discretization of the gradient ﬂow of the relative entropy on the space of probability distributions, equipped with a distance that depends on the kernel (Liu, 2017;
Duncan et al., 2019). The empirical performance of this algorithm and its variants have been largely demonstrated in various tasks in machine learning such as Bayesian inference (Liu and Wang, 2016;
Feng et al., 2017; Liu and Zhu, 2018; Detommaso et al., 2018), learning deep probabilistic models (Wang and Liu, 2016; Pu et al., 2017), or reinforcement learning (Liu et al., 2017). In the limit of inﬁnite particles, the algorithm is known to converge to the target distribution under appropriate growth assumptions on the potential (Lu et al., 2019). Nonetheless, its non-asymptotic analysis remains incomplete: in particular, to the best of our knowledge, quantitative rates of convergence have yet to be obtained. The present paper aims at answering this question. Our ﬁrst contribution is to provide in the inﬁnite-particle regime a descent lemma showing that SVGD decreases at each iteration for a sufﬁciently small but constant step-size, with an analysis different from Liu (2017). We view this problem as an optimization problem over P2(X ) equipped with the Wasserstein distance, and use this framework and optimization techniques to obtain our results. Our second contribution is to provide in the ﬁnite particle regime, a propagation of chaos bound that quantiﬁes the deviation of the empirical distribution of the particles to its population version.
This paper is organized as follows. Section 2 introduces the background needed on optimal transport, while Section 3 presents the point of view adopted to study SVGD in the inﬁnite number of particles regime and reviews related work. Section 4 studies the continuous time dynamics of SVGD. Our main result is presented in Section 5, where we provide a descent lemma and rates of convergence for the SVGD algorithm. We also provide a convergence result of the ﬁnite particle system to its population version Section 6. The complete proofs and toy experiments are deferred to the appendix. 2 Preliminaries on optimal transport
Let X = Rd. We denote by C l(X ) the space of l continuously differentiable functions on X . If
ψ : X → Rp, p ≥ 0, is differentiable, we denote by Jψ : X → Rp×d the Jacobian matrix of ψ. If p = 1, the gradient of ψ denoted ∇ψ is seen as a column vector. Moreover, if ∇ψ is differentiable, the Jacobian of ∇ψ is the Hessian of ψ denoted Hψ. If p = d, div(ψ) denotes the divergence of
ψ, i.e., the trace of the Jacobian. The Hilbert-Schmidt norm of a matrix is denoted (cid:107) · (cid:107)HS and the operator norm denoted (cid:107) · (cid:107)op. 2.1 The Wasserstein space and the continuity equation
In this section, we recall some background from optimal transport. The reader may refer to Ambrosio et al. (2008) for more details.
Consider the set P2(X ) of probability measures µ on X with ﬁnite second order moment. For any
µ ∈ P2(X ), L2(µ) is the space of functions f : X → X such that (cid:82) (cid:107)f (cid:107)2dµ < ∞. If µ ∈ P2(X ), we denote by (cid:107) · (cid:107)L2(µ) and (cid:104)·, ·(cid:105)L2(µ) respectively the norm and the inner product of the Hilbert space
L2(µ). Given a measurable map T : X → X and µ ∈ P2(X ), we denote by T#µ the pushforward measure of µ by T , characterized by the transfer lemma (cid:82) φ(T (x))dµ(x) = (cid:82) φ(y)dT#µ(y), for any measurable and bounded function φ. Consider µ, ν ∈ P2(X ), the 2-nd order Wasserstein distance (cid:82) (cid:107)x − y(cid:107)2ds(x, y), where S(µ, ν) is the set of couplings is deﬁned by W 2 between µ and ν, i.e. the set of nonnegative measures s over X × X such that P#s = µ (resp.
Q#s = ν) where P : (x, y) (cid:55)→ x (resp. Q : (x, y) (cid:55)→ y) denotes the projection onto the ﬁrst 2 (µ, ν) = inf s∈S(µ,ν) 2
(resp. the second) component. The Wasserstein distance is a distance over P2(X ). The metric space (P2(X ), W2) is called the Wasserstein space.
Let T > 0. Consider a weakly continuous map µ : (0, T ) → P2(X ). The family (µt)t∈(0,T ) satisﬁes a continuity equation if there exists (vt)t∈(0,T ) such that vt ∈ L2(µt) and
∂µt
∂t
+ div(µtvt) = 0 (2) holds in the distributional sense. A family (µt)t satisfying a continuity equation with (cid:107)vt(cid:107)L2(µt) integrable over (0, T ) is said absolutely continuous. Among the possible processes (vt)t, one has a minimal L2(µt) norm and is called the velocity ﬁeld of (µt)t. In a Riemannian interpretation of the
Wasserstein space (Otto, 2001), this minimality condition can be characterized by vt belonging to the tangent space to P2(X ) at µt denoted TµtP2(X ), which is a subset of L2(µt). 2.2 A functional deﬁned over the Wasserstein space
Consider π ∝ exp(−V ) where V : X → R is a smooth function, i.e. V is C 2(X ) and its Hessian
HV is bounded from above. For any µ, π ∈ P2(X ), the Kullback-Leibler divergence of µ w.r.t. π is deﬁned by
KL(µ|π) = log (x) dµ(x) (cid:90)
ã
Å dµ dπ if µ is absolutely continuous w.r.t. π with Radon-Nikodym density dµ/dπ, and KL(µ|π) = +∞ otherwise. Consider the functional KL(.|π) : P2(X ) → [0, +∞), µ (cid:55)→ KL(µ|π) deﬁned over the
Wasserstein space. We shall perform differential calculus over this space for such a functional, which is a "powerful way of computing" (Villani, 2003, Section 8.2). If µ ∈ P2(X ) satisﬁes some mild regularity conditions, the (Wasserstein) gradient of KL(.|π) at µ is denoted by ∇W2 KL(µ|π) ∈
Ä dµ
L2(µ) and deﬁned by ∇ log
. Moreover, the (Wasserstein) Hessian of KL(.|µ) at µ is an dπ operator over TµP2(X ) deﬁned by
ä (cid:104)v, HKL(.|π)(µ)v(cid:105)L2(µ) = EX∼µ (cid:2)(cid:104)v(X), HV (X)v(X)(cid:105) + (cid:107)Jv(X)(cid:107)2
HS (cid:3) (3) for any tangent vector v ∈ TµP2(X ). Note that the Hessian of KL(.|π) is not bounded from above. An important property of the Wasserstein gradient is that it satisﬁes a chain rule. Let (µt)t be an absolutely continuous curve s. t. µt has a density. Denote (vt) the velocity ﬁeld of (µt).
If ϕ(t) = KL(µt|π), then under mild technical assumptions ϕ(cid:48)(t) = (cid:104)vt, ∇W2 KL(µt|π)(cid:105)L2(µt) (see Ambrosio et al. (2008)). 3 Presentation of Stein Variational Gradient Descent (SVGD)
In this section, we present our point of view on SVGD in the inﬁnite number of particles regime. 3.1 Kernel integral operator
Consider a positive semi-deﬁnite kernel k : X × X → R and H0 its corresponding RKHS of real-valued functions on X . The space H0 is a Hilbert space with inner product (cid:104)·, ·(cid:105)H0 and norm (cid:107) · (cid:107)H0 (see Smola and Scholkopf (1998)). Moreover, k satisﬁes the reproducing property: ∀ f ∈
H0, f (x) = (cid:104)f, k(x, .)(cid:105)H0 . Denote by H the product RKHS consisting of elements f = (f1, . . . , fd) with fi ∈ H0, and with a standard inner product (cid:104)f, g(cid:105)H = (cid:80)d i=1(cid:104)fi, gi(cid:105)H0. Let µ ∈ P2(X ); the integral operator associated to kernel k and measure µ denoted by Sµ : L2(µ) → H is (cid:90)
Sµf = k(x, ·)f (x)dµ(x). (4)
We make the key assumption that (cid:82) k(x, x)dµ(x) < ∞ for any µ ∈ P2(X ); which implies that
H ⊂ L2(µ). Consider functions f, g ∈ L2(µ) × H and denote the inclusion ι : H → L2(µ), with
ι∗ = Sµ its adjoint. Then following e.g. (Steinwart and Christmann, 2008, Chapter 4), we have (cid:104)f, ιg(cid:105)L2(µ) = (cid:104)ι∗f, g(cid:105)H = (cid:104)Sµf, g(cid:105)H.
When the kernel is integrally strictly positive deﬁnite, then H is dense in L2(µ) for any probability measure µ (Sriperumbudur et al., 2011). We also deﬁne Pµ : L2(µ) → L2(µ) the operator Pµ = ιSµ; notice that it differs from Sµ only in its range. (5) 3
3.2 Stein Variational Gradient Descent
We can now present the Stein Variational Gradient Descent (SVGD) algorithm (Liu and Wang, 2016).
The goal of this algorithm is to provide samples from a target distribution π ∝ exp(−V ) with positive density w.r.t. Lebesgue measure and known up to a normalization constant. Several point of views on SVGD have been adopted in the literature. In this paper, we view SVGD as an optimization algorithm (Liu, 2017) to minimize the Kullback-Leibler (KL) divergence w.r.t. π, see Problem (1).
Denote KL(.|π) : P2(X ) → [0, +∞) the functional µ (cid:55)→ KL(µ|π). More precisely, in order to obtain samples from π, SVGD applies a gradient descent-like algorithm to the functional KL(.|π).
The standard gradient descent algorithm in the Wasserstein space applied to KL(.|π), at each iteration n ≥ 0, is
µn+1 =
I − γ∇ log
µn, (6) (cid:16) (cid:17)(cid:17) (cid:16) µn
π
# where γ > 0 is a step size and I the identity map. This corresponds to a forward Euler discretization of the gradient ﬂow of KL(.|π) (Wibisono, 2018), and can be seen as a Riemannian gradient descent where the exponential map at µ is the map φ (cid:55)→ (I + φ)#µ deﬁned on L2(µ). Therefore, the gradient descent algorithm would require to estimate the density of µn based on samples, which can be demanding (though see Remark 1 below). We next examine the analogous SVGD iteration, (cid:16)
µn+1 =
I − γPµn∇ log (cid:17)(cid:17) (cid:16) µn
π
µn.
# (7)
Instead of using ∇W2 KL(µn|π) as the gradient, SVGD uses Pµn ∇W2 KL(µn|π). This can be seen as the gradient of KL(.|π) under the inner product of H, since (cid:104)Sµ∇W2 KL(µ|π), v(cid:105)H = (cid:104)∇W2 KL(µ|π), ιv(cid:105)L2(µ) for any v ∈ H. The important fact is that given samples of µ, the evaluation of Pµ∇W2 KL(µ|π) is simple. Indeed if lim(cid:107)x(cid:107)→∞ k(x, .)π(x) = 0,
Pµ∇ log (cid:90) (·) = − (cid:17) (cid:16) µ
π
[∇ log π(x)k(x, ·) + ∇xk(x, ·)]dµ(x), (8) using an integration by parts (see Liu (2017)).
Remark 1. An alternative sampling algorithm which does not imply to compute the exact gradient of the KL is the Unadjusted Langevin Algorithm (ULA). It is an implementable algorithm that computes a gradient step with ∇ log π, and a ﬂow step adding a Gaussian noise to the particles. However, it is not a gradient descent discretization; it rather corresponds to performing a Forward-Flow (FFl) discretization, which is biased (Wibisono, 2018, Section 2.2.2). 3.3 Stein Fisher information
The squared RKHS norm of the gradient Sµ∇ log( µ
Deﬁnition 1. Let µ ∈ P2(X ). The Stein Fisher Information of µ relative to π Duncan et al. (2019) is deﬁned by :
π ) is deﬁned as the Stein Fisher Information:
IStein(µ|π) = (cid:107)Sµ∇ log (cid:107)2
H. (9) (cid:17) (cid:16) µ
π
Remark 2. Notice that since Pµ = ιSµ with ι∗ = Sµ, we can write IStein(µ|π) = (cid:104)∇ log( µ
π ), Pµ∇ log( µ
π )(cid:105)L2(µ).
In the literature the quantity (9) is also referred to as the squared Kernel Stein Discrepancy (KSD), used in nonparametric statistical tests for goodness-of-ﬁt (Liu et al., 2016; Chwialkowski et al., 2016;
Gorham and Mackey, 2017). The KSD provides a discrepancy between probability distributions, which depends on π only through the score function ∇ log π that can be calculated without knowing the normalization constant of π. Whether the convergence of the KSD to zero, i.e. Istein(µn|π) → 0 when n → ∞ implies the weak convergence of (µn) to π (denoted µn → π) depends on the choice of the kernel relatively to the target. This question has been treated in Gorham and Mackey (2017).
Sufﬁcient conditions include π being distantly dissipative 1 which is similar to strong log concavity outside a bounded domain, and the kernel having a slow decay rate (e.g. being translation invariant with a non-vanishing Fourier transform, or k being the inverse multi-quadratic kernel deﬁned by 1i.e. such that lim inf r→∞ κ(r) > 0 for κ(r) = inf{−2(cid:104)∇ log π(x) − ∇ log π(y), x − y(cid:105)/(cid:107)x − y(cid:107)2 2; (cid:107)x − y(cid:107)2 = r}. This includes ﬁnite Gaussian mixtures with common covariance and all distributions strongly log-concave outside of a compact set, including Bayesian linear, logistic, and Huber regression posteriors with
Gaussian priors. 4
k(x, y) = (c2 + (cid:107)x − y(cid:107)2
µn → π. 2)β for c > 0 and β ∈ [−1, 0]). In these cases, Istein(µn|π) → 0 implies
In order to study the continuous time dynamics of SVGD, Duncan et al. (2019) introduced a kernel version of a log-Sobolev inequality (which usually upper bounds the KL by the Fisher divergence (Vempala and Wibisono, 2019)).
Deﬁnition 2. We say that π satisﬁes the Stein log-Sobolev inequality with constant λ > 0 if:
KL(µ|π) ≤ 1 2λ
IStein(µ|π). (10) i and ∂2 i=1[(∂iV (x))2k(x, x) − ∂iV (x)(∂1
The functional inequality (10) is not as well known and understood as the classical log-Sobolev inequality.2 Duncan et al. (2019) provided a ﬁrst investigation into when this condition might hold.
They show that it fails to hold if the kernel is too regular w.r.t. π, more precisely for k ∈ C 1,1(X ×
X ), and if (cid:80)d i k(x, x)]dπ(x) < ∞, where ∂1 i denote derivatives with respect to the ﬁrst and second argument of k respectively (Duncan et al., 2019, Lemma 36). This holds for instance in the case where π has exponential tails and the derivatives of k and V grow at most at a polynomial rate. However, they provide interesting cases in dimension 1 where (10) holds, depending on k and π. For instance, by choosing a nondifferentiable kernel that is adapted to the tails of the target k(x, y) = π(x)−1/2e−|x−y|π(y)−1/2, and if V (cid:48)(cid:48)(x) + (V (cid:48)(x))2/2 ≥ ˜λ > 0 for any x ∈ R, then (10) holds with λ = min(1, ˜λ) (Duncan et al., 2019, Example 40). Conditions where (10) holds in higher dimensions are more challenging to establish, and are a topic of current research. i k(x, x)) + ∂1 i k(x, x) + ∂2 i ∂2 3.4