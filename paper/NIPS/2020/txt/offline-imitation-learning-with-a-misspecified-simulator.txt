Abstract
In real-world decision-making tasks, learning an optimal policy without a trial-and-error process is an appealing challenge. When expert demonstrations are available, imitation learning that mimics expert actions can learn a good policy efﬁciently. Learning in simulators is another commonly adopted approach to avoid real-world trials-and-errors. However, neither sufﬁcient expert demonstrations nor high-ﬁdelity simulators are easy to obtain. In this work, we investigate policy learning in the condition of a few expert demonstrations and a simulator with misspeciﬁed dynamics. Under a mild assumption that local states shall still be partially aligned under a dynamics mismatch, we propose imitation learning with horizon-adaptive inverse dynamics (HIDIL) that matches the simulator states with expert states in a H-step horizon and accurately recovers actions based on inverse dynamics policies. In the real environment, HIDIL can effectively derive adapted actions from the matched states. Experiments are conducted in four MuJoCo locomotion environments with modiﬁed friction, gravity, and density conﬁgurations.
Experiment results show that HIDIL achieves signiﬁcant improvement in terms of performance and stability in all of the real environments, compared with imitation learning methods and transferring methods in reinforcement learning. 1

Introduction
Reinforcement Learning (RL) [1] has achieved remarkable success in virtual environments like
Atari games [2], StarCraft II [3], and Go [4]. The principal commonality shared by these virtual environments (predeﬁned games or man-made simulators) is the access to unlimited training data.
Such a virtual trial-and-error makes the learning process not only faster but also safer, since execution failures lead to zero physical damage.
Current approaches to apply RL in real-world decision-making tasks without a costly trial-and-error process can be divided into two major categories. One is Imitation Learning (IL) that obtains a policy by mimicking the behavior of human experts from demonstrations. The other is to train a policy in a simulator and then adapt it to the real world. Both approaches have their own limitations — collecting sufﬁcient expert demonstrations or building a high-ﬁdelity simulator that perfectly recovers real environments are both laborious and expensive. A problem naturally arises: How can we relax the requirements of these two methods yet yield a ready-to-deploy policy. Imagine a feasible scenario that we have a few expert demonstrations and a simulator with misspeciﬁed dynamics.
In this paper, we propose a method that learns to imitate experts with a few demonstrations and a simulator with misspeciﬁed dynamics in a completely ofﬂine manner (i.e. no sampling process
∗This work is supported by National Key Research and Development Program of China (2017YFB1001903),
NSFC(61876077), and Collaborative Innovation Center of Novel Software Technology and Industrialization.
Yang Yu is the corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
in the real world). This setting has been rarely studied before, except for in [5]. None of the two common paradigms of IL, Behavioral Cloning (BC) [6] or Generative Adversarial Imitation Learning (GAIL) [7] can be directly applied in this task. BC treats policy learning as a supervised learning task on expert data. Although BC works in certain environments and is completely ofﬂine, it violates a key assumption of statistical supervised learning by considering past predictions that affect the distribution of future inputs. This intrinsic drawback leads to compounding error [8, 9] during policy execution and tends to take the agent to an incorrect state. GAIL, on the other hand, shows better generalization, but requires sampling in the environment. Empirical results in the previous work [10] also prove GAIL does not work if a dynamics mismatch exists.
Our method tackles this hard problem with two key techniques: (1) Horizon-adaptive Inverse
Dynamics (HID) and (2) Policy Optimization under Distribution Constraint (PODC). In the training phase, HID learns from expert data in a supervised manner. K ensemble HID models are also trained to estimate the uncertainty of goal state to guide PODC. PODC learns a simulator policy in the simulator by constraining its state distribution with expert data. The state is re-weighted based on the uncertainty estimated by HID ensemble. Since direct optimization over a state distribution is intractable, we adopt Generative Adversarial Networks (GANs) [11]. The ﬁnal optimization objective takes a similar form as described in Generative Adversarial Imitation Learning from Observation (GAILfO) [12]. In the deployment phase, the simulator policy runs in the simulator for several steps to generate short-term goals. HID then selects the best goal among them and recovers an action based on the state-goal pair.
We evaluate the efﬁcacy of our algorithm with four continuous-control locomotion tasks from
MuJoCo [13]. In our experiments, an expert policy is trained under a modiﬁed dynamics (serving as a “real-world” dynamic), where one of the gravity, friction and density conﬁgurations is changed.
A few trajectories are then collected by the expert policy as a ﬁxed size expert data. The default dynamics, serving as the simulator, is where the imitator’s policy is learned. The range of modiﬁcation is {0.5, 1.5, 2.0}, which is sufﬁcient to show that our algorithm is effective and robust to a wide range of dynamics mismatch. We show that our method yields much better policies than baseline
IL algorithms in all these tasks, leading to a successful transfer of expert skills to an imitator in an environment different from where the expert acts without any sampling process. 2