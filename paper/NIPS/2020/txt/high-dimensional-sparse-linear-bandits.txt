Abstract
Stochastic linear bandits with high-dimensional sparse features are a practical model for a variety of domains, including personalized medicine and online ad-vertising [Bastani and Bayati, 2020]. We derive a novel Ω(n2/3) dimension-free minimax regret lower bound for sparse linear bandits in the data-poor regime where the horizon is smaller than the ambient dimension and where the feature vectors admit a well-conditioned exploration distribution. This is complemented by a nearly matching upper bound for an explore-then-commit algorithm showing that that Θ(n2/3) is the optimal rate in the data-poor regime. The results com-plement existing bounds for the data-rich regime and provide another example where carefully balancing the trade-off between information and regret is necessary.
Finally, we prove a dimension-free O( n) regret upper bound under an additional assumption on the magnitude of the signal for relevant features.
√ 1

Introduction
Stochastic linear bandits generalize the standard reward model for multi-armed bandits by associating each action with a feature vector and assuming the mean reward is the inner product between the feature vector and an unknown parameter vector [Auer, 2002, Dani et al., 2008, Rusmevichientong and Tsitsiklis, 2010, Chu et al., 2011, Abbasi-Yadkori et al., 2011].
In most practical applications, there are many candidate features but no clear indication about which are relevant. Therefore, it is crucial to consider stochastic linear bandits in the high-dimensional regime but with low-dimensional structure, captured here by the notion of sparsity. Previous work on sparse linear bandits has mostly focused on the data-rich regime, where the time horizon is larger than the ambient dimension [Abbasi-Yadkori et al., 2012, Carpentier and Munos, 2012, Wang et al., 2018, Kim and Paik, 2019, Bastani and Bayati, 2020]. The reason for studying the data-rich regime is partly justiﬁed by minimax lower bounds showing that for smaller time horizons the regret is linear in the worst case.
Minimax bounds, however, do not tell the whole story. A crude maximisation over all environments hides much of the rich structure of linear bandits with sparsity. We study sparse linear bandits in the high-dimensional regime when the ambient dimension is much larger than the time horizon. In order to sidestep existing lower bounds, we reﬁne the minimax notion by introducing a dependence in our bounds on the minimum eigenvalue of a suitable exploration distribution over the actions. Similar 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Comparisons with existing results on regret upper bounds and lower bounds for sparse linear bandits. Here, s is the sparsity, d is the feature dimension, n is the number of rounds, K is the number of arms, Cmin is the minimum eigenvalue of the data matrix for an exploration distribution (3.1) and
τ is a problem-dependent parameter that may have a complicated form and vary across different literature.
Upper Bound
Abbasi-Yadkori et al. [2012]
Sivakumar et al. [2020]
Bastani and Bayati [2020]
Wang et al. [2018]
Kim and Paik [2019]
Lattimore et al. [2015]
This paper (Thm. 4.2)
This paper (Thm. 5.2)
Lower Bound
Multi-task bandits1
This paper (Thm. 3.3)
Regret
√
√ sdn) sdn)
O(
O(
O(τ Ks2(log(n))2)
O(τ Ks3 log(n)) n)
O(τ s
O(s n)
O(C −2/3 min s2/3n2/3)
√
√
√ sn)
O(C −1/2 min
√
Ω( sdn)
Ω(C −1/3 min s1/3n2/3)
Assumptions none adver. + Gaussian noise compatibility condition compatibility condition compatibility condition action set is hypercube action set spans Rd action set spans Rd + mini. signal
N.A.
N.A.
Regime rich rich rich rich rich rich poor rich rich poor quantities appear already in the vast literature on high-dimensional statistics [B¨uhlmann and Van
De Geer, 2011, Wainwright, 2019].
Contributions Our ﬁrst result is a lower bound showing that Ω(n2/3) regret is generally unavoid-able when the dimension is large, even if the action set admits an exploration policy for which the minimum eigenvalue of the associated data matrix is large. The lower bound is complemented by an explore-the-sparsity-then-commit algorithm that ﬁrst solves a convex optimization problem to
ﬁnd the most informative design in the exploration stage. The algorithm then explores for a number of rounds by sampling from the design distribution and uses Lasso [Tibshirani, 1996] to estimate the unknown parameters. Finally, it greedily chooses the action that maximizes the reward given the estimated parameters. We derive an O(n2/3) dimension-free regret that depends instead on the minimum eigenvalue of the covariance matrix associated with the exploration distribution. Our last result is a post-model selection linear bandits algorithm that invokes phase-elimination algorithm
[Lattimore et al., 2020] to the model selected by the ﬁrst-step regularized estimator. Under a sufﬁcient condition on the minimum signal of the feature covariates, we prove that a dimension-free O( n) regret is achievable, even if the data is scarce.
√
The analysis reveals a rich structure that has much in common with partial monitoring, where Θ(n2/3) regret occurs naturally in settings for which some actions are costly but highly informative [Bart´ok et al., 2014]. A similar phenomenon appears here when the dimension is large relative to the horizon.
There is an interesting transition as the horizon grows, since O( dn) regret is optimal in the data rich regime.
√
√