Abstract
We study the fundamental problems of agnostically learning halfspaces and ReLUs under Gaussian marginals. In the former problem, given labeled examples (x, y) from an unknown distribution on Rd × {±1}, whose marginal distribution on x is the standard Gaussian and the labels y can be arbitrary, the goal is to output a hypothesis with 0-1 loss OPT + (cid:15), where OPT is the 0-1 loss of the best-ﬁtting halfspace. In the latter problem, given labeled examples (x, y) from an unknown distribution on Rd × R, whose marginal distribution on x is the standard Gaussian and the labels y can be arbitrary, the goal is to output a hypothesis with square loss OPT + (cid:15), where OPT is the square loss of the best-ﬁtting ReLU. We prove
Statistical Query (SQ) lower bounds of dpoly(1/(cid:15)) for both of these problems. Our
SQ lower bounds provide strong evidence that current upper bounds for these tasks are essentially best possible. 1

Introduction 1.1