Abstract
Exploration in multi-agent reinforcement learning is a challenging problem, es-pecially in environments with sparse rewards. We propose a general method for efﬁcient exploration by sharing experience amongst agents. Our proposed algo-rithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework by combining the gradients of different agents. We evaluate SEAC in a collection of sparse-reward multi-agent environments and ﬁnd that it consistently outperforms several baselines and state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environ-ments, experience sharing makes the difference between learning to solve the task and not learning at all. 1

Introduction
Multi-agent reinforcement learning (MARL) necessitates exploration of the environment dynamics and of the joint action space between agents. This is a difﬁcult problem due to non-stationarity caused by concurrently learning agents and the fact that the joint action space grows exponentially in the number of agents [26]. The problem is exacerbated in environments with sparse rewards in which most transitions will not yield informative rewards.
We propose a general method for efﬁcient MARL exploration by sharing experience amongst agents. Consider the simple multi-agent game shown in Figure 1 in which two agents must simultaneously arrive at a goal.
This game presents a difﬁcult exploration problem, requiring the agents to wander for a long period before stumbling upon a reward. When the agents ﬁnally succeed, the idea of sharing experience is appealing: both agents can learn how to approach the goal from two different directions after a successful episode by leveraging their collective experience. Such experience sharing facilitates a steady progression of all learning agents, meaning that agents improve at approximately equal rates as opposed to diverging in their learning progress. We show in our experiments that this approach of experience sharing can lead to signiﬁcantly faster learning and higher ﬁnal returns.
Figure 1: Two randomly-placed agents (triangles) must simultaneously ar-rive at the goal (square).
We demonstrate this idea in a novel actor-critic MARL algorithm, called Shared Experience Actor-Critic (SEAC).1 SEAC operates similarly to independent learning [33] but updates the actor and critic parameters of an agent by combining gradients computed on the agent’s experience with 1We provide open-source implementations of SEAC in www.github.com/uoe-agents/seac and our two newly developed multi-agent environments: www.github.com/uoe-agents/lb-foraging (LBF) and www.github.com/uoe-agents/robotic-warehouse (RWARE). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
weighted gradients computed on other agents’ experiences. We evaluate SEAC in four sparse-reward multi-agent environments and ﬁnd that it learns substantially faster (up to 70% fewer required training steps) and achieves higher ﬁnal returns compared to several baselines, including: independent learning without experience sharing; using data from all agents to train a single shared policy; and
MADDPG [20], QMIX [28], and ROMA [36]. Sharing experience with our implementation of SEAC increased running time by less than 3% across all environments compared to independent learning. 2