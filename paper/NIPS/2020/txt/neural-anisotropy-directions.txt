Abstract
In this work, we analyze the role of the network architecture in shaping the inductive bias of deep classiﬁers. To that end, we start by focusing on a very simple problem, i.e., classifying a class of linearly separable distributions, and show that, depending on the direction of the discriminative feature of the distribution, many state-of-the-art deep convolutional neural networks (CNNs) have a surprisingly hard time solving this simple task. We then deﬁne as neural anisotropy directions (NADs) the vectors that encapsulate the directional inductive bias of an architecture. These vectors, which are speciﬁc for each architecture and hence act as a signature, encode the preference of a network to separate the input data based on some particular features. We provide an efﬁcient method to identify NADs for several
CNN architectures and thus reveal their directional inductive biases. Furthermore, we show that, for the CIFAR-10 dataset, NADs characterize the features used by
CNNs to discriminate between different classes. 1

Introduction
In machine learning, given a ﬁnite set of samples, there are usually multiple solutions that can perfectly ﬁt the training data, but the inductive bias of a learning algorithm selects and prioritizes those solutions that agree with its a priori assumptions [1, 2]. Arguably, the main success of deep learning has come from embedding the right inductive biases in the architectures, which allow them to excel at tasks such as classifying ImageNet [3], understanding natural language [4], or playing
Atari [5].
Nevertheless, most of these biases have been generally introduced based on heuristics that rely on generalization performance to naturally select certain architectural components. As a result, although these deep networks work well in practice, we still lack a proper characterization and a full understanding of their actual inductive biases. In order to extend the application of deep learning to new domains, it is crucial to develop generic methodologies to systematically identify and manipulate the inductive bias of deep architectures.
Towards designing architectures with desired properties, we need to better understand the bias of the current networks. However, due to the co-existence of multiple types of inductive biases within a neural network, such as the preference for simple functions [6], or the invariance to certain group transformations [7], identifying all biases at once can be challenging. In this work, we take a bottom-up stance and focus on a fundamental bias that arises in deep architectures even for classifying linearly
∗Equal contribution. Correspondence to {guillermo.ortizjimenez, apostolos.modas}@epfl.ch.
The code to reproduce our experiments can be found at https://github.com/LTS4/neural-anisotropy-directions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Test accuracies of different architectures [8–11]. Each pixel corresponds to a linearly separable dataset (with 10, 000 training samples) with a single discriminative feature aligned with a basis element of the 2D-DFT. We use the standard 2D-DFT convention and place the dataset with lower discriminative frequencies at the center of the image, and the higher ones extending radially to the corners. All networks (except LeNet) achieve nearly 100% train accuracy. (σ = 3, (cid:15) = 1) separable datasets. In particular, we show that depending on the nature of the dataset, some deep neural networks can only perform well when the discriminative information of the data is aligned with certain directions of the input space. We call this bias the directional inductive bias of an architecture.
This is illustrated in Fig. 1 for state-of-the-art CNNs classifying a set of linearly separable distributions with a single discriminative feature lying in the direction of some Fourier basis vector1. Remarkably, even the gigantic DenseNet [8] only generalizes to a few of these distributions, despite common belief that, due to their superior capacity, such networks can learn most functions efﬁciently. Yet, even a simple logistic regression eclipses their performance on a simple linearly separable task.
In this paper, we aim to explain why this happens, and try to understand why some linear distributions are easier to classify than others. To that end, we introduce the concept of neural anisotropy directions to characterize the directional inductive bias of an architecture.
Deﬁnition 1 (Neural anisotropy directions). The neural anisotropy directions (NADs) of a speciﬁc architecture are the ordered set of orthonormal vectors {ui}D i=1 ranked in terms of the preference of the network to separate the data in those particular directions of the input space.
In general, though, quantifying the preference of a complex network to separate data in certain directions is not straightforward. In this paper, we will show that measuring the performance of a network on different versions of a linearly separable dataset can reveal its directional inductive bias.
Yet, we will provide an efﬁcient computational method to fully characterize this bias in terms of
NADs, independent of training data. Finally, we will reveal that NADs allow a network to prioritize certain discriminating features of a dataset, and hence act as important conductors of generalization.
Our main contributions can be summarized as follows:
• We characterize the directional inductive bias in the spectral domain of state-of-the-art
CNNs, and explain how pooling layers are a major source for this bias.
• More generally, we introduce a new efﬁcient method to identify the NADs of a given architecture using only information available at initialization.
• Finally, we show that the importance of NADs is not limited to linearly separable tasks, and that they determine the selection of discriminative features of CNNs trained on CIFAR-10.
We believe that our ﬁndings can impact future research in novel architectures, by allowing researchers to compare and quantify the speciﬁc inductive bias of different networks.