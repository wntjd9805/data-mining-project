Abstract
The idea behind data augmentation techniques is based on the fact that slight changes in the percept do not change the brain cognition. In classiﬁcation, neural networks use this fact by applying transformations to the inputs to learn to predict the same label. However, in deep subspace clustering (DSC), the ground-truth labels are not available, and as a result, one cannot easily use data augmentation techniques. We propose a technique to exploit the beneﬁts of data augmentation in DSC algorithms. We learn representations that have consistent subspaces for slightly transformed inputs. In particular, we introduce a temporal ensembling component to the objective function of DSC algorithms to enable the DSC networks to maintain consistent subspaces for random transformations in the input data.
In addition, we provide a simple yet effective unsupervised procedure to ﬁnd efﬁcient data augmentation policies. An augmentation policy is deﬁned as an image processing transformation with a certain magnitude and probability of being applied to each image in each epoch. We search through the policies in a search space of the most common augmentation policies to ﬁnd the best policy such that the DSC network yields the highest mean Silhouette coefﬁcient in its clustering results on a target dataset. Our method achieves state-of-the-art performance on four standard subspace clustering datasets. The source code is available at: https://github.com/mahdiabavisani/DSCwithDA.git. 1

Introduction
Recent advances in technology have provided massive amounts of complex high dimensional data for computer vision and machine learning applications. High dimensionality has adverse effects, including confusion of algorithms with irrelevant dimensions and curse of dimensionality as well as increased computation time and memory. This motivates us to explore techniques for representing high-dimensional data in lower dimensions. In many practical applications such as face images under various illumination conditions [1] and hand-written digits [2], high-dimensional data can be represented by union of low-dimensional subspaces. The subspace clustering problem aims at ﬁnding these subspaces. In particular, the objective of subspace clustering is to ﬁnd the number of subspaces, their basis and dimensions, and assign data to these subspaces [3].
Conventional subspace clustering algorithms assume that data lie in linear subspaces [4, 5, 6, 7, 8].
In practice, however, many datasets are better modeled by non-linear manifolds. To deal with this issue, many works have incorporated projections and kernel tricks to express non-linearity [9, 10, 7, 11, 12, 13]. Recently, deep subspace clustering (DSC) methods [14, 15, 16, 17, 18] have been 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
proposed which essentially learn unsupervised nonlinear mappings by projecting data into a latent space in which data lie in linear subspaces. Deep subspace clustering networks have shown promising performances on various datasets.
Deep learning techniques are prone to overﬁtting. Data augmentation is often presented as a type of regularization to mitigate this issue [19, 20]. While data augmentation for deep learning-based methods have proven to be beneﬁcial, the current framework of DSC networks is unable to take the full advantage of data augmentation. In this work, we modify the DSC framework and propose a model that can incorporate data augmentation into DSC.
An important difference between data augmentation in subspace clustering and data augmentation in supervised tasks is the fact that as opposed to supervised tasks, we do not have ground-truth labels for the existing samples in the subspace clustering algorithms. Corresponding to the fact that objects remain the same even if we slightly transform them, in supervised deep learning models, transformations of an existing sample are trained to be predicted with a consistent label similar to the ground-truth label of the original sample. How can one convey such property in an unsupervised subspace clustering task, where the data does not have the ground-truth labels?
A DSC model should favor functions that give consistent outputs for similar data points with a slight difference in their percept. To achieve this, we optimize a consistency loss that is based on temporal ensembling. We input plausible transformations of existing samples into the model and require the autoencoders of the model to map the transformations to consistent subspaces similar to the subspace of the original data.
Efﬁcient augmentation policies improve the performance of the deep networks. However, not all the image transformations construct efﬁcient augmentation policies. Efﬁcient augmentation policies can be different from a dataset to another [21, 22, 23]. In supervised applications, the validation set is often used to manually search among transformations such as rotation, horizontal ﬂip, or translation by a few pixels to ﬁnd efﬁcient augmentations. Manual augmentation needs prior knowledge and expertise, and it can only search among a handful of pre-deﬁned trials. Some methods automate this search for classiﬁcation networks [21, 24, 25]. However, these methods are only designed for the classiﬁcation task and cannot be applied to the task of subspace clustering. This is because we do not have a validation or training set in subspace clustering. We overcome this issue by providing a simple yet effective method for ﬁnding efﬁcient augmentation policies using a greedy search and use mean
Silhouette scores to evaluate the effect of different augmentation policies on the performance of our proposed model. 2