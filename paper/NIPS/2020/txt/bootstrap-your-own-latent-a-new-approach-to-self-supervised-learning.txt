Abstract
We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other.
From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view.
At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classiﬁca-tion accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.3 1

Introduction
Learning good image representations is a key challenge in computer vision [1, 2, 3] as it allows for efﬁcient training on downstream tasks [4, 5, 6, 7]. Many different training ap-proaches have been proposed to learn such rep-resentations, usually relying on visual pretext tasks. Among them, state-of-the-art contrastive methods [8, 9, 10, 11, 12] are trained by re-ducing the distance between representations of different augmented views of the same im-age (‘positive pairs’), and increasing the dis-tance between representations of augmented views from different images (‘negative pairs’).
These methods need careful treatment of nega-tive pairs [13] by either relying on large batch sizes [8, 12], memory banks [9] or customized mining strategies [14, 15] to retrieve the nega-tive pairs. In addition, their performance criti-cally depends on the choice of image augmenta-tions [34, 11, 8, 12].
Figure 1: Performance of BYOL on ImageNet (lin-ear evaluation) using ResNet-50 and our best ar-chitecture ResNet-200 (2
), compared to other unsupervised and supervised (Sup.) baselines [8].
×
∗Equal contribution; the order of ﬁrst authors was randomly selected. 3https://github.com/deepmind/deepmind-research/tree/master/byol 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we introduce Bootstrap Your Own Latent (BYOL), a new algorithm for self-supervised learning of image representations. BYOL achieves higher performance than state-of-the-art contrastive methods without using negative pairs. It iteratively bootstraps4 the outputs of a network to serve as targets for an enhanced representation. Moreover, BYOL is more robust to the choice of image augmentations than contrastive methods; we suspect that not relying on negative pairs is one of the leading reasons for its improved robustness. While previous methods based on bootstrapping have used pseudo-labels [16], cluster indices [17] or a handful of labels [18, 19, 20], we propose to directly bootstrap the representations. In particular, BYOL uses two neural networks, referred to as online and target networks, that interact and learn from each other. Starting from an augmented view of an image, BYOL trains its online network to predict the target network’s representation of another augmented view of the same image. While this objective admits collapsed solutions, e.g., outputting the same vector for all images, we empirically show that BYOL does not converge to such solutions.
We hypothesize (Section 3.2) that the combination of (i) the addition of a predictor to the online network and (ii) the use of a moving average of online parameters, as the target network encourages encoding more and more information in the online projection and avoids collapsed solutions.
We evaluate the representation learned by BYOL on ImageNet [21] and other vision benchmarks using
ResNet architectures [22]. Under the linear evaluation protocol on ImageNet, consisting in training a linear classiﬁer on top of the frozen representation, BYOL reaches 74.3% top-1 accuracy with a standard ResNet-50 and 79.6% top-1 accuracy with a larger ResNet (Figure 1). In the semi-supervised and transfer settings on ImageNet, we obtain results on par or superior to the current state of the art. Our contributions are: (i) We introduce BYOL, a self-supervised representation learning method (Section 3) which achieves state-of-the-art results under the linear evaluation protocol on ImageNet without using negative pairs. (ii) We show that our learned representation outperforms the state of the art on semi-supervised and transfer benchmarks (Section 4). (iii) We show that BYOL is more resilient to changes in the batch size and in the set of image augmentations compared to its contrastive counterparts (Section 5). In particular, BYOL suffers a much smaller performance drop than SimCLR, a strong contrastive baseline, when only using random crops as image augmentations. 2