Abstract
Machine learning with missing data has been approached in two different ways, including feature imputation where missing feature values are estimated based on observed values and label prediction where downstream labels are learned directly from incomplete data. However, existing imputation models tend to have strong prior assumptions and cannot learn from downstream tasks, while models targeting label prediction often involve heuristics and can encounter scalability issues. Here we propose GRAPE, a graph-based framework for feature imputation as well as label prediction. GRAPE tackles the missing data problem using a graph representation, where the observations and features are viewed as two types of nodes in a bipartite graph, and the observed feature values as edges. Under the GRAPE framework, the feature imputation is formulated as an edge-level prediction task and the label prediction as a node-level prediction task. These tasks are then solved with Graph
Neural Networks. Experimental results on nine benchmark datasets show that
GRAPE yields 20% lower mean absolute error for imputation tasks and 10% lower for label prediction tasks, compared with existing state-of-the-art methods. 1

Introduction
Issues with learning from incomplete data arise in many domains including computational biology, clinical studies, survey research, ﬁnance, and economics [6, 32, 46, 47, 53]. The missing data problem has previously been approached in two different ways: feature imputation and label prediction.
Feature imputation involves estimating missing feature values based on observed values [8, 9, 11, 14, 15, 17, 22, 34, 44, 45, 47–50, 56], and label prediction aims to directly accomplish a downstream task, such as classiﬁcation or regression, with the missing values present in the input data [2, 5, 10, 15, 16, 23, 37, 40, 42, 52, 54].
Statistical methods for feature imputation often provide useful theoretical properties but exhibit notable shortcomings: (1) they tend to make strong assumptions about the data distribution; (2) they lack the ﬂexibility for handling mixed data types that include both continuous and categorical variables; (3) matrix completion based approaches cannot generalize to unseen samples and require retraining when the model encounters new data samples [8, 9, 22, 34, 44, 47]. When it comes to models for label prediction, existing approaches such as tree-based methods rely on heuristics [5] and tend to have scalability issues. For instance, one of the most popular procedures called surrogate splitting does not scale well, because each time an original splitting variable is missing for some observation it needs to rank all other variables as surrogate candidates and select the best alternative.
Recent advances in deep learning have enabled new approaches to handle missing data. Existing imputation approaches often use deep generative models, such as Generative Adversarial Networks
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: In the GRAPE framework, we construct a bipartite graph from the data matrix with missing feature values, where the entries of the matrix in red indicate the missing values (Top Left). To construct the graph, the observations O and features F are considered as two types of nodes and the observed values in the data matrix are viewed as weighted/attributed edges between the observation and feature nodes (Bottom Left). With the constructed graph, we formulate the feature imputation problem and the label prediction problem as edge-level (Top right) and node-level (Bottom right) prediction tasks, respectively. The tasks can then be solved with our GRAPE GNN model that learns node and edge embeddings through rounds of message passing. (GANs) [56] or autoencoders [17, 50], to reconstruct missing values. While these models are ﬂexible, they have several limitations: (1) when imputing missing feature values for a given observation, these models fail to make full use of feature values from other observations; (2) they tend to make biased assumptions about the missing values by initializing them with special default values.
Here, we propose GRAPE1, a general framework for feature imputation and label prediction in the presence of missing data. Our key innovation is to formulate the problem using a graph representation, where we construct a bipartite graph with observations and features as two types of nodes, and the observed feature values as attributed edges between the observation and feature nodes (Figure 1).
Under this graph representation, the feature imputation can then be naturally formulated as an edge-level prediction task, and the label prediction as a node-level prediction task.
GRAPE solves both tasks via Graph Neural Networks (GNNs). Speciﬁcally, GRAPE adopts a GNN architecture inspired by the GraphSAGE model [20], while having three innovations in its design: (1) since the edges in the graph are constructed based on the data matrix and have rich attribute information, we introduce edge embeddings during message passing and incorporate both discrete and continuous edge features in the message computation; (2) we design augmented node features to initialize observation and feature nodes, which provides greater representation power and maintains inductive learning capabilities; (3) to overcome the common issue of overﬁtting in the missing data problem, we employ an edge dropout technique that greatly boosts the performance of GRAPE.
We compare GRAPE with the state-of-the-art feature imputation and label prediction algorithms on 9 benchmark datasets from the UCI Machine Learning Repository [1]. In particular, GRAPE yields 20% lower mean absolute error (MAE) for the imputation tasks and 10% lower MAE for the prediction tasks at the 30% data missing rate. Finally, we demonstrate GRAPE’s strong generalization ability by showing its superior performance on unseen observations without the need for retraining. 1Project website with data and code: http://snap.stanford.edu/grape 2
Overall, our approach has several important beneﬁts: (1) by creating a bipartite graph structure we create connections between different features (via observations) and similarly between the observations (via features); (2) GNN elegantly harnesses this structure by learning to propagate and borrow information from other features/observations in a graph localized way; (3) GNN allows us to model both feature imputation as well as label prediction in an end-to-end fashion, which as we show in experiments leads to strong performance improvements. 2