Abstract
We consider ReLU networks with random weights, in which the dimension de-creases at each layer. We show that for most such networks, most examples x admit an adversarial perturbation at an Euclidean distance of O
, where d is the input dimension. Moreover, this perturbation can be found via gradient ﬂow, as well as gradient descent with sufﬁciently small steps. This result can be seen as an explanation to the abundance of adversarial examples, and to the fact that they are found via gradient descent. (cid:16) (cid:107)x(cid:107)
√ d (cid:17) 1

Introduction
Since the seminal paper of Szegedy et al. [17], adversarial examples arose much attention in machine learning, with various attacks (e.g. [1, 4, 5, 9, 10]) and defence methods (e.g. [12, 13, 11, 20, 7]) being developed, as well as various attempts to explain their presence (e.g. [6, 15, 16, 14, 3]). Yet, it is still not clear why adversarial examples exist, and why they can be found via simple algorithms such as gradient descent.
In this paper we shed new light on the source of this phenomenon, and show that for certain network architectures, for most choices of weights and for most examples x, an adversarial example at a (cid:16) (cid:107)x(cid:107)
Euclidean distance of ˜O is guaranteed to exists. Speciﬁcally, we show that this holds if each
√ d layer reduces the dimension. (cid:17)
Moreover, we show that gradient ﬂow (a continuous analog of gradient descent), or gradient descent with sufﬁciently small steps, is guaranteed to ﬁnd these adversarial examples. This result demon-strates that unless we impose restrictions on the weights and/or the examples, we should expect the phenomenon of adversarial examples to occur.
To the best of our knowledge, this is the ﬁrst result which shows existence of adversarial examples w.r.t. the Euclidean distance for a large class of networks and distributions. Likewise, it is the ﬁrst result that shows that gradient based algorithms are guaranteed to ﬁnd such perturbations. 1.1