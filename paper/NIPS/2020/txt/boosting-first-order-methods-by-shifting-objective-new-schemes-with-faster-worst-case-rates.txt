Abstract
We propose a new methodology to design ﬁrst-order methods for unconstrained strongly convex problems. Speciﬁcally, instead of tackling the original objective directly, we construct a shifted objective function that has the same minimizer as the original objective and encodes both the smoothness and strong convexity of the original objective in an interpolation condition. We then propose an algorithmic template for tackling the shifted objective, which can exploit such a condition.
Following this template, we derive several new accelerated schemes for problems that are equipped with various ﬁrst-order oracles and show that the interpolation condition allows us to vastly simplify and tighten the analysis of the derived methods. In particular, all the derived methods have faster worst-case convergence rates than their existing counterparts. Experiments on machine learning tasks are conducted to evaluate the new methods. 1

Introduction
In this paper, we focus on the following unconstrained smooth strongly convex problem: min x∈Rd f (x) = 1 n n (cid:88) i=1 fi(x), (1) where each fi is L-smooth and µ-strongly convex,1 and we denote x(cid:63) ∈ Rd as the solution of this problem. The n = 1 case covers a large family of classic strongly convex problems, for which gradient descent (GD) and Nesterov’s accelerated gradient (NAG) [32, 33, 35] are the methods of choice. The n ≥ 1 case is the popular ﬁnite-sum case, where many elegant methods that incorporate the idea of variance reduction have been proposed. Problems with a ﬁnite-sum structure arise frequently in machine learning and statistics, such as empirical risk minimization (ERM). (cid:80)n
In this work, we tackle problem (1) from a new angle. Instead of designing methods to solve the original objective function f , we propose methods that are designed to solve a shifted objective h: minx∈Rd h(x) = 1 2 (cid:107)x − x(cid:63)(cid:107)2. n
It can be easily veriﬁed that each hi(x) is (L−µ)-smooth and convex, ∇hi(x) = ∇fi(x)−∇fi(x(cid:63))−
µ(x − x(cid:63)), ∇h(x) = ∇f (x) − µ(x − x(cid:63)), hi(x(cid:63)) = h(x(cid:63)) = 0 and ∇hi(x(cid:63)) = ∇h(x(cid:63)) = 0, which means that the shifted problem and problem (1) share the same optimal solution x(cid:63). Let us write a well-known property of h: i=1 hi(x), where hi(x) = fi(x) − fi(x(cid:63)) − (cid:104)∇fi(x(cid:63)), x − x(cid:63)(cid:105) − µ
∀x, y ∈ Rd, h(x) − h(y) − (cid:104)∇h(y), x − y(cid:105) ≥ 1 2(L − µ) (cid:107)∇h(x) − ∇h(y)(cid:107)2, (2) 1The formal deﬁnitions of smoothness, strong convexity are given in Section 1.1. If each fi(·) is L-smooth, the averaged function f (·) is itself L-smooth — but typically with a smaller L. We keep L as the smoothness constant for consistency. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
which encodes both the smoothness and strong convexity of f . The discrete version of this inequality is equivalent to the smooth strongly convex interpolation condition discovered in [51]. As studied in
[51], this type of inequality forms a necessary and sufﬁcient condition for the existence of a smooth strongly convex f interpolating a given set of triples {(xi, ∇fi, fi)}, while the usual collection of L-smoothness and strong convexity inequalities is only a necessary condition.2 For worst-case analysis, it implies that tighter results can be derived by exploiting condition (2) than using smoothness and strong convexity “separately”, which is common in existing worst-case analysis. We show that our methodology effectively exploits this condition and consequently, we propose several methods that achieve faster worst-case convergence rates than their existing counterparts.
In summary, our methodology and proposed methods have the following distinctive features:
• We show that our methodology works for problems equipped with various ﬁrst-order oracles: deterministic gradient oracle, incremental gradient oracle and incremental proximal point oracle.
• We leverage a cleaner version of the interpolation condition discovered in [51], which leads to simpler and tighter analysis to the proposed methods than their existing counterparts.
• For our proposed stochastic methods, we deal with shifted variance bounds / shifted stochastic gradient norm bounds, which are different from all previous works.
• All the proposed methods achieve faster worst-case convergence rates than their counterparts that were designed to solve the original objective f .
Our work is motivated by a recently proposed robust momentum method [10], which converges under a Lyapunov function that contains a term h(x) − 1 2(L−µ) (cid:107)∇h(x)(cid:107)2. Our work conducts a comprehensive study of the special structure of this term.
This paper is organized as follows: In Section 2, we present high-level ideas and lemmas that are the core building blocks of our methodology. In Section 3, we propose an accelerated method for the n = 1 case. In Section 4, we propose accelerated stochastic variance-reduced methods for the n ≥ 1 case with incremental gradient oracle. In Section 5, we propose an accelerated method for the n ≥ 1 case with incremental proximal point oracle. In Section 6, we provide experimental results. 1.1 Notations and Deﬁnitions
In this paper, we consider problems in the standard Euclidean space denoted by Rd. We use (cid:104)·, ·(cid:105) and (cid:107)·(cid:107) to denote the inner product and the Euclidean norm, respectively. We let [n] denote the set {1, 2, . . . , n}, E denote the total expectation and Eik denote the conditional expectation given the information up to iteration k. We say that a convex function f : Rd → R is L-smooth if it has L-Lipschitz continuous gradients, i.e., ∀x, y ∈ Rd, (cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107).
Some important consequences of this assumption can be found in the textbook [35]: ∀x, y ∈ Rd, 2L (cid:107)∇f (x)−∇f (y)(cid:107)2 ≤ f (x)−f (y)−(cid:104)∇f (y), x−y(cid:105) ≤ L 1 2 (cid:107)x−y(cid:107)2. We refer to the ﬁrst inequality as interpolation condition following [51]. A continuously differentiable f is called µ-strongly convex if ∀x, y ∈ Rd, f (x) − f (y) − (cid:104)∇f (y), x − y(cid:105) ≥ µ 2 (cid:107)x − y(cid:107)2. Given a point x ∈ Rd, an index i ∈ [n] and α > 0, a deterministic oracle returns (f (x), ∇f (x)), an incremental ﬁrst-order oracle returns (fi(x), ∇fi(x)) and an incremental proximal point oracle returns (fi(x), ∇fi(x), proxα i (x)), where the proximal operator is deﬁned as proxα 2 (cid:107)x − z(cid:107)2}. We denote (cid:15) > 0 as the required accuracy for solving problem (1) (i.e., to achieve (cid:107)x − x(cid:63)(cid:107)2 ≤ (cid:15)), which is assumed to be small. We denote κ (cid:44) L/µ, which is often called the condition ratio. i (z) = arg minx {fi(x) + α 1.2