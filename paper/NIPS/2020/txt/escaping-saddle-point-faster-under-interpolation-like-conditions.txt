Abstract
In this paper, we show that under over-parametrization several standard stochastic optimization algorithms escape saddle-points and converge to local-minimizers much faster. One of the fundamental aspects of over-parametrized models is that they are capable of interpolating the training data. We show that, under interpolation-like assumptions satisﬁed by the stochastic gradients in an over-parametrization setting, the ﬁrst-order oracle complexity of Perturbed Stochastic
Gradient Descent (PSGD) algorithm to reach an ✏-local-minimizer, matches the corresponding deterministic rate of ˜ (1/✏2). We next analyze Stochastic Cubic-O
Regularized Newton (SCRN) algorithm under interpolation-like conditions, and show that the oracle complexity to reach an ✏-local-minimizer under interpolation-like conditions, is ˜ (1/✏2.5). While this obtained complexity is better than the
O corresponding complexity of either PSGD, or SCRN without interpolation-like assumptions, it does not match the rate of ˜ (1/✏1.5) corresponding to deterministic
O
Cubic-Regularized Newton method. It seems further Hessian-based interpolation-like assumptions are necessary to bridge this gap. We also discuss the corresponding improved complexities in the zeroth-order settings. 1

Introduction
Over-parametrized models, for which the training stage involves solving nonconvex optimization problems, are common in modern machine learning. A canonical example of such a model is deep neural networks. Such over-parametrized models have several interesting statistical and computational properties. On the statistical side, such over-parametrized models are highly expressive and are capable of nearly perfectly interpolating the training data. Furthermore, despite the highly nonconvex training landscape, most local minimizers have good generalization properties under regularity conditions; see for example [39, 27, 22, 21] for empirical and theoretical details. We emphasize here that over-parametrization plays an important role for both phenomenon to occur. Furthermore, it is to be noted that not all critical points exhibit nice generalization properties. Hence, from a computational perspective, designing algorithms that do not get trapped in saddle-points, and converge to local minimizers during the training process, becomes extremely important [12].
Indeed, recently there has been extensive research in the machine learning and optimization com-munities on designing algorithms that escape saddle-points and converge to local minimizers. The 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
authors of [30] proved the folklore result that in the deterministic setting for sufﬁciently regular functions, vanilla gradient descent algorithms converges almost surely to local minimizers, even when initialized randomly; see also [29]. However, [30, 29] only provide asymptotic results, that have limited consequence for practice. Understandably, it has been shown by the authors of [15], that gradient descent might take exponential-time to escape saddle points in several cases. In this context, injecting artiﬁcial noise in each step of the gradient descent algorithm has been empirically observed to help escape saddle points. Several works, for example, [24, 26], showed that such perturbed gradient descent algorithms escape saddles faster in a non-asymptotic sense. Such algorithms are routinely used in training highly over-parametrized deep neural network and other over-parameterized nonconvex machine learning models. However, existing theoretical analysis of such algorithms fail to take advantage of the interpolation-like properties enjoyed by over-parametrized machine learning models. Hence, such theoretical results are conservative. Speciﬁcally, there is a gap between the assumptions used in the theoretical analysis of algorithms that escape saddle-points and the assumptions commonly satisﬁed by over-parametrized models which are trained by those algorithms.
In this work, we consider nonconvex stochastic optimization problems of the following form: argmin 2Rd { x f (x) := E⇠[F (x, ⇠)]
.
} (1)
! where f : Rd
R is nonconvex function satisfying certain regularity properties described next, and ⇠ is a random variable characterizing the stochasticity in the problem. We assume that the function f has a lower bound f ⇤ throughout this work. We analyze two standard algorithms that escape saddle-points, namely the perturbed stochastic gradient descent (PSGD) and stochastic cubic-regularized Newton’s method (SCRN) for problems of the form in (1). We show that under interpolation-like assumptions (see Section 2 for exact deﬁnitions) on the stochastic gradient, it could be proved that both PSGD and SCRN escape saddle-points and converge to local minimizers much faster. In particular, we show that in order for PSGD algorithm to escape saddle-points and ﬁnd an ✏-local-minimizer, the number of calls to the stochastic ﬁrst-order oracle is of the order ˜ (1/✏2)1 which matches number
O of calls when the objective being optimized is a deterministic objective (for which exact gradient could be obtained in each step of the algorithm)2. As a point of comparison, [19, 25] showed that without the interpolation-like conditions that we make, PSGD requires ˜ (1/✏4) calls to the stochastic
O gradient oracle. Furthermore, [17] analyzed a version of PSGD with averaging and improved the oracle complexity to ˜ (1/✏3.5). It is also worth noting that, with a mean-square Lipschitz gradient
O assumption on the objective function being optimized, and using complicated variance reduction techniques, the authors of [16] showed that it is possible for a double-loop version of PSGD to converge to ✏-local minimizers with ˜ (1/✏3) number of calls to the stochastic ﬁrst-order oracle.
O
However, recent empirical investigations seem to suggest that variance reduction techniques are inefﬁcient for the nonconvex deep learning problems [13, 43]. Our results, on the other hand exploit the naturally available structure present in over-parametrized models and obtains the best-known oracle complexity for escaping saddle-points using only the vanilla versions of PSGD algorithm (which is oftentimes the version of PSGD used in practice). We also analyze the corresponding
Zeroth-Order version of the PSGD algorithm. In this setting, we are able to observe only potentially noisy evaluations of the function being optimized. In this setting, we show that PSGD algorithm requires ˜ (d1.5/✏4.5) calls to the stochastic zeroth-order oracle. In this context, we are not aware of
O a result to compare with. The recent works of [4, 18] provided results for bounded functions in the zeroth-order deterministic setting, where one obtains exact function values; such a setting though is highly unrealistic in practice.
Next, we consider the question of whether using second-order methods helps reduce the number of calls. Indeed, in the deterministic setting, it is well-known that second-order information helps escape saddle point at a much faster rate. For example, [37] proposed that Cubic-regularized Newton’s method and showed that the method requires only ˜ (1/✏1.5) calls to the gradient and Hessian oracle;
O see also [8, 11] for related results. Correspondingly, in the stochastic setting [47] showed that SCRN method requires ˜ (1/✏3.5) calls, which is better than that of PSGD (without further assumptions). In
O this work, we show that under interpolation-like assumptions on (only) the stochastic gradient, SCRN method requires only ˜ (1/✏2.5) calls. In contrast to the PSGD setting, SCRN requires more calls than
O its corresponding deterministic counterpart. However, it should be noted that the complexity of SCRN 1Here, ˜ hides log factors.
O 2It is possible to obtain ˜
O (1/✏11.75) complexity using accelerated method in deterministic setting; see [26]. 2
Algorithm
Perturbed
GD
Cubic
Newton
With SGC (This paper)
ZO
HO
˜ d1.5✏ 
O
Theorem 3.1 4.5
 
˜ d4✏ 
O
Theorem 4.1
  2.5
 
  2
˜
O
✏ 
Theorem 3.1
 
  2.5
˜
✏ 
O
Theorem 4.1
 
 
˜
O
 
Without SGC
Deterministic
ZO
˜ d1.5✏ 
O
Theorem 3.2 5.5
  2.5 d4✏ 
+
Theorem 4.1
 
 
[5]
O
  d✏  4
˜
O
HO
✏ 
Theorem 17
 
[25]
˜
✏ 
O
Theorem 1
 
 
[47]
  3.5 2
HO
˜
✏ 
O
Theorem 3
 
 
[24] 1.5
✏ 
O
Theorem 3
 
 
[37] 3.5
 
Table 1: Oracle complexities of perturbed stochastic gradient descent (PSGD) and stochastic cubic-regularized Newton’s method (SCRN). ZO corresponds to number of calls to zeroth-order oracle and HO corresponds to number of calls to ﬁrst or second-order oracles. The result for PSGD and
SCRN are given respectively in high-probability and in expectation. The results in the deterministic case corresponds to projected gradient descent and cubic-Regularized Newton’s method (without stochastic gradients). is still better than that of the PSGD, with or without interpolation-like assumptions. We belive that without further interpolation-like assumptions also on the stochastic Hessians, the oracle complexity of SCRN cannot be improved, in particular to match the deterministic rate of ˜ (1/✏1.5) (see also
O
Remark 6). We also provide similar improved results for a zeroth-order version of SCRN method, thereby improving upon the results of [5]. All of our results, along with comparison to existing results in the literature and the corresponding assumption required, are summarized in Table 1. We conclude this section with a other related works.
More