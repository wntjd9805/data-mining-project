Abstract
Many learning algorithms, such as stochastic gradient descent, are affected by the order in which training examples are used. It is generally believed that sampling the training examples without-replacement, also known as random reshufﬂing, causes learning algorithms to converge faster. We give a counterexample to the Operator
Inequality of Noncommutative Arithmetic and Geometric Means, a longstanding conjecture that relates to the performance of random reshufﬂing in learning algo-rithms [19]. We use this to give an example of a learning task and algorithm for which with-replacement random sampling outperforms random reshufﬂing. 1

Introduction
Many machine learning algorithms work by iteratively updating a model based on one of a number of possible steps. For example, in stochastic gradient descent (SGD), each model update is performed based on a single example selected from a training dataset. The order in which the samples are selected—in which the update steps are performed—can have an impact on the convergence rate of the algorithm. There is a general sense in the community that the random reshufﬂing method, which selects the order by without-replacement sampling of the steps in an epoch (where an epoch means a single pass through the data, and different epochs may use different random orders), is better (for convergence) than ordinary with-replacement sampling for these algorithms [7, 8, 19].
There are two intuitive reasons why we might expect random reshufﬂing to outperform sampling with replacement. The ﬁrst applies when our model updates are in some sense noisy: each one could perturb us away from the desired optimum, and they are only guaranteed to approach the optimum on average. In this case, random reshufﬂing ensures that the noise in some sense “cancels out” over an epoch in which all samples are used. Most previous work on random reshufﬂing has studied this noisy case, and this intuition has been borne out in a series of results that show random-reshufﬂing results in a convergence rate of O(1/t2) rather than O(1/t) for convex SGD [8, 10, 16, 18, 20].
The second intuitive reason is that, because sampling without replacement avoids using the same update step repeatedly, it should tend to be “more contractive” than sampling with replacement. This intuition applies even for “noiseless” algorithms that converge at a linear rate of O(1)t. In contrast to the noisy case, the belief that random reshufﬂing should be better in general for these algorithms that converge at a linear rate is backed up theoretically only with conjectures. The main conjecture in this space is the Operator Inequality of Noncommutative Arithmetic and Geometric Means, stated as
Conjecture 1 of Recht and Ré [19]. That conjecture, which is motivated by algorithms such as the randomized Kaczmarz method [23] that converge at a linear rate, asserts the following.
Conjecture 1 (Operator Inequality of Noncommutative Arithmetic and Geometric Means). Let
A1, . . . , An ∈ Rd×d be a collection of (symmetric) positive semideﬁnite matrices. Then it is 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
conjectured that the following inequalities always hold: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
Aσ(i) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 n!
σ∈P(n) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) n (cid:89) (cid:88) i=1
≤ (cid:32) 1 n n (cid:88) i=1 (cid:33)n(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
,
Ai (1) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 n! (cid:32) n (cid:89) (cid:88) (cid:33)T (cid:32) n (cid:89)
Aσ(i)
Aσ(i)
σ∈P(n) i=1 i=1 (cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 nn (cid:88) (cid:32) n (cid:89) (cid:33)T (cid:32) n (cid:89)
Af (i)
Af (i) f ∈{1,...,n}n i=1 i=1 (cid:33)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
, (2) where P(n) denotes the set of permutations of the set {1, . . . , n} and (cid:107) · (cid:107) denotes the (cid:96)2 induced operator norm (the magnitude of the largest-magnitude eigenvalue for symmetric matrices).
A variant of the conjecture, which moves the sums to the outside of the norms, was given by [7].
Conjecture 2. Let A1, . . . , An ∈ Rd×d be a collection of (symmetric) positive semideﬁnite matrices.
Then it is conjectured that the following inequality always holds: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 nn
Aσ(i)
Af (i) 1 n! (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) n (cid:89) n (cid:89) (cid:88) (cid:88) (3)
≤
. f ∈{1,...,n}n i=1
σ∈P(n) i=1
Conjecture 1 is a quite natural generalization of the ordinary arithmetic-mean-geometric-mean (AMGM) inequality of real numbers, which states that for non-negative numbers xi, (cid:81)n i=1 xi ≤ (cid:0) 1 (cid:80)n i=1 xi (cid:1)n
. n
In Conjecture 1, positive semideﬁnite matrices (matrices with non-negative eigenvalues) take the place of the non-negative scalars of the AMGM inequality, and indeed Conjecture 1 reduces to the
AMGM inequality when d = 1. Conjecture 1 was proven by the original authors in the case of n = 2, and has been proven subsequently for n = 3 [12, 29]. It also seems to be true for random ensembles of matrices [2, 19], and random testing seems to suggest that Conjecture 1 is always true. However, recent work has shown non-constructively that Conjecture 1 is false [3, 14].1 These non-constructive disproofs are interesting, but deliver limited insight about random reshufﬂing, both because they involve complicated proof techniques and because they do not translate to concrete counterexamples of matrices A1, A2, . . . , An that can be used to study learning algorithms empirically.
In this paper, we propose simple counterexamples for these conjectures—to our knowledge this is the
ﬁrst explicit counterexample known for any of these conjectures, and the ﬁrst disproof of Conjecture 2.
We explore the consequences and limitations of this counterexample throughout the paper, and end by showing concrete problems for which SGD with random reshufﬂing converges asymptotically slower than SGD using with-replacement sampling. Our paper is structured as follows.
• In Section 2, we construct a family of counterexamples for Conjectures 1 and 2, showing constructively that all three conjectured inequalities are false.
• In Section 3, we adapt the counterexample to give concrete ML algorithms for which with-replacement sampling outperforms without-replacement sampling, contrary to folklore.
• In Section 4, we prove that for non-trivial matrix ensembles (1) always holds with strict inequality for sufﬁciently small step sizes. Thus, for algorithms with a slowly decreasing step, without-replacement sampling always outperforms with-replacement sampling. On the other hand, we show that when optimal step sizes are chosen separately for with- and without-replacement sampling (but may not decrease to zero), with-replacement sampling can still perform better.
• In Section 5, we give an example convex learning task for which SGD using with-replacement sampling converges asymptotically faster than random-reshufﬂing. 1.1 Notation
In this paper, (cid:107)·(cid:107) of a vector always denotes the Euclidean (cid:96)2 norm, and (cid:107)·(cid:107) of an operator denotes the (cid:96)2 induced norm. We have 1 denote the all-1s vector. We let ⊗ denote the Kronecker product and (cid:3). We let Sd denote
⊕ denote the matrix direct sum, such that x ⊕ y is the block diagonal matrix (cid:2) x 0 0 y 1Lai and Lim [14] can be considered parallel work to this paper. 2
the set of symmetric d × d matrices over R, and let Pd denote the set of symmetric positive deﬁnite d × d matrices. We let (cid:22) denote inequality with respect to the positive deﬁnite ordering (i.e. A (cid:22) B when B − A ∈ Pd). When K ⊂ Rd is a convex cone (a set closed under sums and non-negative scalar multiplication), and x, y ∈ Rd, we say x (cid:22)K y if and only if y − x ∈ K, and for A, B ∈ Rd×d.
We let M (K) denote the set {A ∈ Sd | ∀x ∈ K, Ax ∈ K} of matrices that preserve the convex cone
K, and we note that M (K) is also a convex cone. For brevity, we let rrk : Sd × Sd × · · · × Sd → Sd denote the “random reshufﬂing” function rrk(A1, A2, . . . , An) = 1 n! and deﬁne srrk similarly as the “symmetric random reshufﬂing” function (cid:17)T (cid:16)(cid:81)k srrk(A1, A2, . . . , An) = 1 n!
Note that this lets us write (1) more compactly as (cid:107)rrn(A1, . . .)(cid:107) ≤ (cid:107)rr1(A1, . . .)(cid:107)n. i=1 Aσ(i), i=1 Aσ(i) i=1 Aσ(i) (cid:16)(cid:81)k
σ∈P(n)
σ∈P(n) (cid:80) (cid:17)
. (cid:80) (cid:81)k 1.2