Abstract
We study the problem of semi-supervised learning on graphs, for which graph neural networks (GNNs) have been extensively explored. However, most existing
GNNs inherently suffer from the limitations of over-smoothing [6, 23, 24, 30], non-robustness [48, 45], and weak-generalization when labeled nodes are scarce.
In this paper, we propose a simple yet effective framework—GRAPH RANDOM
NEURAL NETWORKS (GRAND)—to address these issues. In GRAND, we ﬁrst design a random propagation strategy to perform graph data augmentation. Then we leverage consistency regularization to optimize the prediction consistency of unlabeled nodes across different data augmentations. Extensive experiments on graph benchmark datasets suggest that GRAND signiﬁcantly outperforms state-of-the-art GNN baselines on semi-supervised node classiﬁcation. Finally, we show that GRAND mitigates the issues of over-smoothing and non-robustness, exhibiting better generalization behavior than existing GNNs. The source code of GRAND is publicly available at https://github.com/Grand20/grand. 1

Introduction
Graphs serve as a common language for modeling structured and relational data [22], such as social networks, knowledge graphs, and the World Wide Web. Mining and learning graphs can beneﬁt various real-world problems and applications. The focus of this work is on the problem of semi-supervised learning on graphs [46, 20, 10], which aims to predict the categories of unlabeled nodes of a given graph with only a small proportion of labeled nodes. Among its solutions, graph neural networks (GNNs) [20, 17, 35, 1] have recently emerged as powerful approaches. The main idea of
GNNs lies in a deterministic feature propagation process to learn expressive node representations.
However, recent studies show that such propagation procedure brings some inherent issues: First, most GNNs suffer from over-smoothing [23, 6, 24, 30]. Li et al. show that the graph convolution operation is a special form of Laplacian smoothing [23], and consequently, stacking many GNN layers tends to make nodes’ features indistinguishable. In addition, a very recent work [30] suggests that the coupled non-linear transformation in the propagation procedure can further aggravate this issue.
Second, GNNs are often not robust to graph attacks [48, 45], due to the deterministic propagation adopted in most of them. Naturally, the deterministic propagation makes each node highly dependent with its (multi-hop) neighborhoods, leaving the nodes to be easily misguided by potential data noise and susceptible to adversarial perturbations.
∗Equal contribution.
‡Work performed while at Tsinghua University.
§Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The third issue lies in the general setting of semi-supervised learning, wherein standard training methods (for GNNs) can easily overﬁt the scarce label information [5]. Most efforts to addressing this broad issue are focused on how to fully leverage the large amount of unlabeled data. In computer vision, recent attempts, e.g. MixMatch [3], UDA [40], have been proposed to solve this problem by designing data augmentation methods for consistency regularized training, which have achieved great success in the semi-supervised image classiﬁcation task. This inspires us to apply this idea into
GNNs to facilitate semi-supervised learning on graphs.
In this work, we address these issues by designing graph data augmentation and consistency regulariza-tion strategies for semi-supervised learning. Speciﬁcally, we present the GRAPH RANDOM NEURAL
NETWORKS (GRAND), a simple yet powerful graph-based semi-supervised learning framework.
To effectively augment graph data, we propose random propagation in GRAND, wherein each node’s features can be randomly dropped either partially (dropout) or entirely, after which the perturbed feature matrix is propagated over the graph. As a result, each node is enabled to be insensitive to speciﬁc neighborhoods, increasing the robustness of GRAND. Further, the design of random propagation can naturally separate feature propagation and transformation, which are commonly coupled with each other in most GNNs. This empowers GRAND to safely perform higher-order feature propagation without increasing the complexity, reducing the risk of over-smoothing for GRAND. More importantly, random propagation enables each node to randomly pass messages to its neighborhoods.
Under the assumption of homophily of graph data [26], we are able to stochastically generate different augmented representations for each node. We then utilize consistency regularization to enforce the prediction model, e.g., a simple Multilayer Perception (MLP), to output similar predictions on different augmentations of the same unlabeled data, improving GRAND’s generalization behavior under the semi-supervised setting.
Finally, we theoretically illustrate that random propagation and consistency regularization can enforce the consistency of classiﬁcation conﬁdence between each node and its multi-hop neighborhoods.
Empirically, we also show both strategies can improve the generalization of GRAND, and mitigate the issues of non-robustness and over-smoothing that are commonly faced by existing GNNs. Altogether, extensive experiments demonstrate that GRAND achieves state-of-the-art semi-supervised learning results on GNN benchmark datasets. 2 Problem and