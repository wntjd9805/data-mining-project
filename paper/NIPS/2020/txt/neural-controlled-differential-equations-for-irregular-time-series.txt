Abstract
Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations.
Here, we demonstrate how this may be resolved through the well-understood mathematics of controlled differential equations. The resulting neural controlled differential equation model is directly applicable to the general setting of partially-observed irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efﬁcient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models. 1

Introduction
Recurrent neural networks (RNN) are a popular choice of model for sequential data, such as a time series. The data itself is often assumed to be a sequence of observations from an underlying process, and the RNN may be interpreted as a discrete approximation to some function of this process. Indeed the connection between RNNs and dynamical systems is well-known [1, 2, 3, 4].
However this discretisation typically breaks down if the data is irregularly sampled or partially observed, and the issue is often papered over by binning or imputing data [5].
A more elegant approach is to appreciate that because the underlying process develops in continuous time, so should our models. For example [6, 7, 8, 9] incorporate exponential decay between observations, [10, 11] hybridise a Gaussian process with traditional neural network models, [12] approximate the underlying continuous-time process, and [13, 14] adapt recurrent neural networks by allowing some hidden state to evolve as an ODE. It is this last one that is of most interest to us here. 1.1 Neural ordinary differential equations
Neural ordinary differential equations (Neural ODEs) [3, 15], seek to approximate a map x (cid:55)→ y by learning a function fθ and linear maps (cid:96)1
θ such that (cid:90) t
θ, (cid:96)2 y ≈ (cid:96)1
θ(zT ), where zt = z0 + fθ(zs)ds and z0 = (cid:96)2
θ(x). (1) 0
Note that fθ does not depend explicitly on s; if desired this can be included as an extra dimension in zs [15, Appendix B.2]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Neural ODEs are an elegant concept. They provide an interface between machine learning and the other dominant modelling paradigm that is differential equations. Doing so allows for the well-understood tools of that ﬁeld to be applied. Neural ODEs also interact beautifully with the manifold hypothesis, as they describe a ﬂow along which to evolve the data manifold.
This description has not yet involved sequential data such as time series. The t dimension in equation (1) was introduced and then integrated over, and is just an internal detail of the model.
However the presence of this extra (artiﬁcial) dimension motivates the question of whether this model can be extended to sequential data such as time series. Given some ordered data (x0, . . . , xn), the goal is to extend the z0 = (cid:96)2
θ(x) condition of equation (1) to a condition resembling “z0 = (cid:96)(x0), . . . , zn = (cid:96)(xn)”, to align the introduced t dimension with the natural ordering of the data.
The key difﬁculty is that equation (1) deﬁnes an ordinary differential equation; once θ has been learnt, then the solution of equation (1) is determined by the initial condition at z0, and there is no direct mechanism for incorporating data that arrives later [4].
However, it turns out that the resolution of this issue – how to incorporate incoming information – is already a well-studied problem in mathematics, in the ﬁeld of rough analysis, which is concerned with the study of controlled differential equations.1 See for example [16, 17, 18, 19]. An excellent introduction is [20]. A comprehensive textbook is [21].
We will not assume familiarity with either controlled differential equations or rough analysis. The only concept we will rely on that may be unfamiliar is that of a Riemann–Stieltjes integral. 1.2 Contributions
We demonstrate how controlled differential equations may extend the Neural ODE model, which we refer to as the neural controlled differential equation (Neural CDE) model. Just as Neural ODEs are the continuous analogue of a ResNet, the Neural CDE is the continuous analogue of an RNN.
The Neural CDE model has three key features. One, it is capable of processing incoming data, which may be both irregularly sampled and partially observed. Two (and unlike previous work on this problem) the model may be trained with memory-efﬁcient adjoint-based backpropagation even across observations. Three, it demonstrates state-of-the-art performance against similar (ODE or
RNN based) models, which we show in empirical studies on the CharacterTrajectories, PhysioNet sepsis prediction, and Speech Commands datasets.
We provide additional theoretical results showing that our model is a universal approximator, and that it subsumes apparently-similar ODE models in which the vector ﬁeld depends directly upon continuous data.
Our code is available at https://github.com/patrick-kidger/NeuralCDE. We have also released a library torchcde, at https://github.com/patrick-kidger/torchcde 2