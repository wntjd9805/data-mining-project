Abstract
Neural networks are vulnerable to input perturbations such as additive noise and adversarial attacks. In contrast, human perception is much more robust to such perturbations. The Bayesian brain hypothesis states that human brains use an internal generative model to update the posterior beliefs of the sensory input. This mechanism can be interpreted as a form of self-consistency between the maximum a posteriori (MAP) estimation of an internal generative model and the external environment. Inspired by such hypothesis, we enforce self-consistency in neural networks by incorporating generative recurrent feedback. We instantiate this de-sign on convolutional neural networks (CNNs). The proposed framework, termed
Convolutional Neural Networks with Feedback (CNN-F), introduces a generative feedback with latent variables to existing CNN architectures, where consistent pre-dictions are made through alternating MAP inference under a Bayesian framework.
In the experiments, CNN-F shows considerably improved adversarial robustness over conventional feedforward CNNs on standard benchmarks. 1

Introduction
Vulnerability in feedforward neural networks Conven-tional deep neural networks (DNNs) often contain many layers of feedforward connections. With the ever-growing network capacities and representation abilities, they have achieved great success. For example, recent convolutional neural networks (CNNs) have impressive accuracy on large scale image classiﬁcation benchmarks [33]. How-ever, current CNN models also have signiﬁcant limitations.
For instance, they can suffer signiﬁcant performance drop from corruptions which barely inﬂuence human recogni-tion [3]. Studies also show that CNNs can be misled by imperceptible noise known as adversarial attacks [32].
Figure 1: An intuitive illustration of re-current generative feedback in human visual perception system.
Feedback in the human brain To address the weaknesses of CNNs, we can take inspiration from of how human visual recognition works, and incorporate certain mecha-nisms into the CNN design. While human visual cortex has hierarchical feedforward connections, backward connections from higher level to lower level cortical areas are something that current artiﬁcial networks are lacking [6]. Studies suggest these backward connections carry out top-down processing which improves the representation of sensory input [15]. In addition, evidence suggests recurrent feedback in the human visual cortex is crucial for robust object recognition. For example, humans require recurrent feedback to recognize challenging images [11]. Obfuscated images can fool humans without recurrent feedback [5]. Figure 1 shows an intuitive example of recovering a sharpened cat from a blurry cat and achieving consistent predictions after several iterations. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Predictive coding and generative feedback Computational neuroscientists speculate that Bayesian inference models human perception [14]. One speciﬁc formulation of predictive coding assumes
Gaussian distributions on all variables and performs hierarchical Bayesian inference using recurrent, generative feedback pathways [28]. The feedback pathways encode predictions of lower level inputs, and the residual errors are used recurrently to update the predictions. In this paper, we extend the principle of predictive coding to explicitly incorporate Bayesian inference in neural networks via generative feedback connections. Speciﬁcally, we adopt a recently proposed model, named the
Deconvolutional Generative Model (DGM) [25], as the generative feedback. The DGM introduces hierarchical latent variables to capture variation in images, and generates images from a coarse to
ﬁne detail using deconvolutional operations.
Our contributions are as follows:
Self-consistency We introduce generative feedback to neural networks and propose the self-consistency formulation for robust perception. Our internal model of the world reaches a self-consistent representation of an external stimulus. Intuitively, self-consistency says that given any two elements of label, image and auxillary information, we should be able to infer the other one.
Mathematically, we use a generative model to describe the joint distribution of labels, latent variables and input image features. If the MAP estimate of each one of them are consistent with the other two, we call a label, a set of latent variables and image features to be self-consistent (Figure 4).
CNN with Feedback (CNN-F) We incorporate generative recurrent feedback modeled by the DGM into CNN and term this model as CNN-F. We show that Bayesian inference in the DGM is achieved by CNN with adaptive nonlinear operators (Figure 2). We impose self-consistency in the CNN-F by iterative inference and online update. Computationally, this process is done by propagating along the feedforward and feedback pathways in the CNN-F iteratively (Figure 3).
Adversarial robustness We show that the recurrent generative feedback in CNN-F promotes robust-ness and visualizes the behavior of CNN-F over iterations. We ﬁnd that more iterations are needed to reach self-consistent prediction for images with larger perturbation, indicating that recurrent feedback is crucial for recognizing challenging images. When combined with adversarial training, CNN-F further improves adversarial robustness of CNN on both Fashion-MNIST and CIFAR-10 datasets. 2 Approach
In this section, we ﬁrst formally deﬁne self-consistency. Then we give a speciﬁc form of generative feedback in CNN and impose self-consistency on it. We term this model as CNN-F. Finally we show the training and testing procedure in CNN-F. Throughout, we use the following notations:
∈
Rn be the input of a network and y
RK be the output. In image classiﬁcation, x is image
Let x and y = (y(1), . . . , y(K)) is one-hot encoded label. K is the total number of classes. K is usually much less than n. We use L to denote the total number of network layers, and index the input layer to
Rm be encoded feature of x at layer k of the feedforward the feedforward network as layer 0. Let h
∈
∈
Figure 2: Left: CNN, Graphical model for the DGM and the inference network for the DGM.
We use the DGM to as the generative model for the joint distribution of image features h, labels y and latent variables z. MAP inference for h, y and z is denoted in red, green and blue respectively. f and g denotes feedforward features and feedback features respectively. Right: CNN with feedback (CNN-F). CNN-F performs alternating MAP inference via recurrent feedforward and feedback pathways to enforce self-consistency. 2
Figure 3: Feedforward and feedback pathway in CNN-F. a) ˆy and ˆz are computed by the feed-forward pathway and ˆh is computed from the feedback pathway. b) Illustration of the AdaReLU operator. c) Illustration of the AdaPool operator. pathway. Feedforward pathway computes feature map f ((cid:96)) from layer 0 to layer L, and feedback pathway generates g((cid:96)) from layer L to k. g((cid:96)) and f ((cid:96)) have the same dimensions. To generate h
W be latent variables from y, we introduce latent variables for each layer of CNN. Let z((cid:96)) at layer (cid:96), where C, H, W are the number of channels, height and width for the corresponding feature map. Finally, p(h, y, z; θ) denotes the joint distribution parameterized by θ, where θ includes the weight W and bias term b of convolution and fully connected layers. We use ˆh, ˆy and ˆz to denote the
MAP estimates of h, y, z conditioning on the other two variables.
RC
∈
H
×
× 2.1 Generative feedback and Self-consistency
Human brain and neural networks are similar in having a hierarchical structure.
In human visual perception, external stimuli are ﬁrst preprocessed by lateral geniculate nucleus (LGN) and then sent to be processed by V1, V2, V4 and Inferior Temporal (IT) cortex in the ventral cortical visual system. Conventional NN use feedforward layers to model this process and learn a one-direction mapping from input to output. However, numerous studies suggest that in addition to the feedforward connections from V1 to IT, there are feedback connections among these cortical areas [6].
Figure 4: Self-consistency among ˆh, ˆz, ˆy and consistency between ˆh and h.
Inspired by the Bayesian brain hypothesis and the predictive coding theory, we propose to add generative feedback connections to NN.
Since h is usually of much higher dimension than y, we introduce latent variables z to account for the information loss in the feedforward process. We then propose to model the feedback connections as MAP estimation from an internal generative model that describes the joint distribution of h, z and y. Furthermore, we realize recurrent feedback by imposing self-consistency (Deﬁnition 2.1).
Deﬁnition 2.1. (Self-consistency) Given a joint distribution p(h, y, z; θ) parameterized by θ, (ˆh, ˆy, ˆz) are self-consistent if they satisfy the following constraints:
ˆh, ˆz),
|
ˆh = arg max
ˆy, ˆz), p(h
|
ˆy = arg max
ˆz = arg max
ˆh, ˆy)
| p(y p(z (1) y z h
In words, self-consistency means that MAP estimates from an internal generative model are consistent with each other. In addition to self-consistency, we also impose the consistency constraint between ˆh and the external input features (Figure 4). We hypothesize that for easy images (familiar images to human, clean images in the training dataset for NN), the ˆy from the ﬁrst feedforward pass should automatically satisfy the self-consistent constraints. Therefore, feedback need not be triggered. 3
For challenging images (unfamiliar images to human, unseen perturbed images for NN), recurrent feedback is needed to obtain self-consistent (ˆh, ˆy, ˆz) and to match ˆh with h. Such recurrence resembles the dynamics in neural circuits [12] and the extra effort to process challenging images [11]. 2.2 Generative Feedback in CNN-F
CNN have been used to model the hierarchical structure of human retinatopic ﬁelds [4, 10], and have achieved state-of-the-art performance in image classiﬁcation. Therefore, we introduce generative feedback to CNN and impose self-consistency on it. We term the resulting model as CNN-F.
We choose to use the DGM [25] as generative feedback in the CNN-F. The DGM introduces hierarchical binary latent variables and generates images from coarse to ﬁne details. The generation process in the DGM is shown in Figure 3 (a). First, y is sampled from the label distribution. Then each entry of z((cid:96)) is sampled from a Bernoulli distribution parameterized by g((cid:96)) and a bias term b((cid:96)). g((cid:96)) and z((cid:96)) are then used to generate the layer below: (2)
In this paper, we assume p(y) to be uniform, which is realistic under the balanced label scenario. We assume that h follows Gaussian distribution centered at g(k) with standard deviation σ. (cid:124))((cid:96))(z((cid:96))
∗ 1) = W ( g((cid:96))) g((cid:96) (cid:12)
− 2.3 Recurrence in CNN-F
In this section, we show that self-consistent (ˆh, ˆy, ˆz) in the DGM can be obtained via alternately propagating along feedforward and feedback pathway in CNN-F.
Feedforward and feedback pathway in CNN-F The feedback pathway in CNN-F takes the same form as the generation process in the DGM (Equation (2)). The feedforward pathway in CNN-F takes the same form as CNN except for the nonlinear operators. In conventional CNN, nonlinear operators are σReLU(f ) = max(f, 0) and σMaxPool(f ) = maxr r f , where r is the dimension of the pooling region in the feature map (typically equals to 2 or 3). In contrast, we use σAdaReLU and σAdaPool given in Equation (3) in the feedforward pathway of CNN-F. These operators adaptively choose how to activate the feedforward feature map based on the sign of the feedback feature map. The feedforward pathway computes f ((cid:96)) using the recursion f ((cid:96)) = W ((cid:96))
σ(f ((cid:96)
×
σAdaReLU(f ) = (cid:26)σReLU(f ),
σReLU( f ),
− if g 0
≥ if g < 0
∗
σAdaPool(f ) =
+ b((cid:96)) 1. if g
− 1))
} (cid:26)σMaxPool(f ),
σMaxPool( f ),
−
− 0
≥ if g < 0 (3)
MAP inference in the DGM Given a joint distribution of h, y, z modeled by the DGM, we aim to show that we can make predictions using a CNN architecture following the Bayes rule (Theorem 2.1). To see this, ﬁrst recall that generative classiﬁers learn a joint distribution p(x, y) of input data x x) using the Bayes rule. A well known and their labels y, and make predictions by computing p(y example is the Gaussian Naive Bayes model (GNB). The GNB models p(x, y) by p(y)p(x y), where
| y is Boolean variable following a Bernoulli distribution and p(x y) follows Gaussian distribution. It can be shown that p(y x) computed from GNB has the same parametric form as logistic regression.
|
Assumption 2.1. (Constancy assumption in the DGM).
|
|
A. The generated image g(k) at layer k of DGM satisﬁes
B. Prior distribution on the label is a uniform distribution: p(y) = const.
C. Normalization factor in p(z y) for each category is constant: (cid:80) g(k) 2 2 = const.
||
|| z eη(y,z) = const.
|
Remark. To meet Assumption 2.1.A, we can normalize g(k) for all k. This results in a form similar to the instance normalization that is widely used in image stylization [35]. See Appendix A.4 for more detailed discussion. Assumption 2.1.B assumes that the label distribution is balanced. η in
Assumption 2.1.C is used to parameterize p(z
Theorem 2.1. Under Assumption 2.1 and given a joint distribution p(h, y, z) modeled by the DGM, p(y y). See Appendix A for the detailed form.
| h, z) has the same parametric form as a CNN with σAdaReLU and σAdaPool.
|
Proof. Please refer to Appendix A.
Remark. Theorem 2.1 says that DGM and CNN is a generative-discriminative pair in analogy to
GNB and logistic regression. 1σ takes the form of σAdaPool or σAdaReLU. 4
We also ﬁnd the form of MAP inference for image feature ˆh and latent variables ˆz in the DGM.
Speciﬁcally, we use zR and zP to denote latent variables that are at a layer followed by AdaReLU and AdaPool respectively. 1(
) denotes indicator function.
·
Proposition 2.1 (MAP inference in the DGM). Under Assumption 2.1, the following hold:
A. Let h be the feature at layer k, then ˆh = g(k).
B. MAP estimate of z((cid:96)) conditioned on h, y and
ˆzR((cid:96)) = 1(σAdaReLU(f ((cid:96)))
ˆzP ((cid:96)) = 1(g((cid:96)) 0)
≥ (cid:12) 0)
≥ arg max r r
× z(j)
{
}j
=(cid:96) in the DGM is: (f ((cid:96))) + 1(g((cid:96)) < 0) arg min r r (cid:12)
× (f ((cid:96))) (4) (5)
Proof. For part A, we have ˆh = arg maxh p(h g(k)) = g(k). The second
ˆy, ˆz) = arg maxh p(h
|
| equality is obtained because g(k) is a deterministic function of ˆy and ˆz. The third equality is obtained (g(k), diag(σ2)). For part B, please refer to Appendix A. because h
∼ N
Remark. Proposition 2.1.A show that ˆh is the output of the generative feedback in the CNN-F.
Proposition 2.1.B says that ˆzR = 1 if the sign of the feedforward feature map matches with that of the feedback feature map. ˆzP = 1 at locations that satisfy one of these two requirements: 1) the value in the feedback feature map is non-negative and it is the maximum value within the local pooling region or 2) the value in the feedback feature map is negative and it is the minimum value within the local pooling region. Using Proposition 2.1.B, we approximate
}(cid:96)=1:L by greedily ﬁnding the
MAP estimate of ˆz((cid:96)) conditioning on all other layers.
ˆz((cid:96))
{
Iterative inference and online update in CNN-F We ﬁnd self-consistent (ˆh, ˆy, ˆz) by iterative inference and online update (Algorithm 1). In the initialization step, image x is ﬁrst encoded to h by k convolutional layers. Then h passes through a standard CNN, and latent variables are initialized with conventional σReLU and σMaxPool. The feedback generative network then uses ˆy0 and
}(cid:96)=k:L to
}(cid:96)=k:L, where the subscript denotes the number of iterations. generate intermediate features
In practice, we use logits instead of one-hot encoded label in the generative feedback to maintain uncertainty in each category. We use g0(k) as the input features for the ﬁrst iteration. Starting from this iteration, we use σAdaReLU and σAdaPool instead of σReLU and and σMaxPool in the feedforward pathway to infer ˆz (Equation (20) and (21)). In practice, we ﬁnd that instead of greedily replacing the input with generated features and starting a new inference iteration, online update eases the training and gives better robustness performance. The online update rule of CNN-F can be written as: g0((cid:96))
ˆz0((cid:96))
{
{ (6) (7) where η is the step size. Greedily replacement is a special case for the online update rule when η = 1.
ˆht + η(gt+1(k)
− ft+1((cid:96)) + η(gt((cid:96))
ˆht+1 ←
← ft+1((cid:96))), (cid:96) = k, . . . , L ft+1((cid:96))
ˆht)
−
Algorithm 1: Iterative inference and online update in CNN-F
Input : Input image x, number of encoding layers k, maximum number of iterations N.
Encode image x to h0 with k convolutional layers;
Initialize
ˆz((cid:96))
{ while t < N do
}(cid:96)=k:L by σReLU and σMaxPool in the standard CNN;
Feedback pathway: generate gt(k) using ˆyt and ˆzt((cid:96)), (cid:96) = k, . . . , L;
Feedforward pathway: use ˆht+1 as the input (Equation (6)); update each feedforward layer using Equation (7); predict ˆyt+1 using the updated feedforward layers; end return ˆhN , ˆyN , ˆzN 2.4 Training the CNN-F
During training, we have three goals: 1) train a generative model to model the data distribution, 2) train a generative classiﬁer and 3) enforce self-consistency in the model. We ﬁrst approximate self-consistent (ˆh, ˆy, ˆz) and then update model parameters based on the losses listed in Table 1. All losses are computed for every iteration. Minimizing the reconstruction loss increases data likelihood
ˆyt, ˆzt) and enforces consistency between given current estimates of label and latent variables log p(h
| 5 (cid:54)
ˆht and h. Minimizing the cross-entropy loss helps with the classiﬁcation goal. In addition to reconstruction loss at the input layer, we also add reconstruction loss between intermediate feedback and feedforward feature maps. These intermediate losses helps stabilizing the gradients when training an iterative model like the CNN-F.
Table 1: Training losses in the CNN-F.
Cross-entropy loss
Reconstruction loss
Intermediate reconstruction loss
Form log p(y | ˆht, ˆzt; θ) log p(h | ˆyt, ˆzt; θ) = ||h − ˆh||2 2.
||f0((cid:96)) − gt((cid:96))||2 2
Purpose classiﬁcation generation, self-consistency stabilizing training 3 Experiment 3.1 Generative feedback promotes robustness
As a sanity check, we train a CNN-F model with two convolution layers and one fully-connected layer on clean Fashion-MNIST images. We expect that CNN-F reconstructs the perturbed inputs to their clean version and makes self-consistent predictions. To this end, we verify the hypothesis by evaluating adversarial robustness of CNN-F and visualizing the restored images over iterations.
Adversarial robustness Since CNN-F is an iterative model, we consider two attack methods: attacking the ﬁrst or last output from the feedforward streams. We use “ﬁrst” and “e2e” (short for end-to-end) to refer to the above two attack approaches, respectively. Due to the approximation of non-differentiable activation operators and the depth of the unrolled CNN-F, end-to-end attack is weaker than ﬁrst attack (Appendix B.1). We report the adversarial accuracy against the stronger attack in Figure 5. We use the Fast Gradient Sign Attack Method (FGSM) [8] Projected Gradient
Descent (PGD) method to attack. For PGD attack, we generate adversarial samples within L
-norm constraint, and denote the maximum L
-norm between adversarial images and clean images as (cid:15).
∞
∞
Figure 5 (a, b) shows that the CNN-F improves adversarial robustness of a CNN on Fashion-MNIST without access to adversarial images during training. The error bar shows standard deviation of 5 runs. Figure 5 (c) shows that training a CNN-F with more iterations improves robustness. Figure 5 (d) shows that the predictions are corrected over iterations during testing time for a CNN-F trained with 5 iterations. Furthermore, we see larger improvements for higher (cid:15). This indicates that recurrent feedback is crucial for recognizing challenging images.
Figure 5: Adversarial robustness of CNN-F with standard training on Fashion-MNIST. CNN-F-k stands for CNN-F trained with k iterations. a) Attack with FGSM. b) Attack with PGD using 40 steps. c) Train with different number of iterations. Attack with PGD-40. d) Evaluate a trained
CNN-F-5 model with various number of iterations against PGD-40 attack.
Image restoration Given that CNN-F models are robust to adversarial attacks, we examine the models’ mechanism for robustness by visualizing how the generative feedback moves a perturbed image over iterations. We select a validation image from Fashion-MNIST. Using the image’s two 28 intersects the image with the largest principal components, a two-dimensional hyperplane image at the center. Vector arrows visualize the generative feedback’s movement on the hyperplane’s position. In Figure 6 (a), we ﬁnd that generative feedback perturbs samples across decision boundaries toward the validation image. This demonstrates that the CNN-F’s generative feedback can restore perturbed images to their uncorrupted objects.
R28
⊂
×
We further explore this principle with regard to adversarial examples. The CNN-F model can correct initially wrong predictions. Figure 6 (b) uses Grad-CAM activations to visualize the network’s 6
Figure 6: The generative feedback in CNN-F models restores perturbed images. a) The decision cell cross-sections for a CNN-F trained on Fashion-MNIST. Arrows visualize the feedback direction on the cross-section. b) Fashion-MNIST classiﬁcation accuracy on PGD adversarial examples;
Grad-CAM activations visualize the CNN-F model’s attention from incorrect (iter. 1) to correct predictions (iter. 2). c) Grad-CAM activations across different feedback iterations in the CNN-F. d)
From left to right: clean images, corrupted images, and images restored by the CNN-F’s feedback. attention from an incorrect prediction to a correct prediction on PGD-40 adversarial samples [30]. To correct predictions, the CNN-F model does not initially focus on speciﬁc features. Rather, it either identiﬁes the entire object or the entire image. With generative feedback, the CNN-F begins to focus on speciﬁc features. This is reproduced in clean images as well as images corrupted by blurring and additive noise 6 (c). Furthermore, with these perceptible corruptions, the CNN-F model can reconstruct the clean image with generative feedback 6 (d). This demonstrates that the generative feedback is one mechanism that restores perturbed images. 3.2 Adversarial Training
Adversarial training is a well established method to improve adver-sarial robustness of a neural network [20]. Adversarial training often solves a minimax optimization problem where the attacker aims to maximize the loss and the model parameters aims to minimize the loss. In this section, we show that CNN-F can be combined with adversarial training to further improve the adversarial robustness.
Training methods Figure 7 illustrates the loss design we use for
CNN-F adversarial training. Different from standard adversarial training on CNNs, we use cross-entropy loss on both clean images and adversarial images. In addition, we add reconstruction loss between generated features of adversarial samples from iterative feedback and the features of clean images in the ﬁrst forward pass.
Figure 7: Loss design for
CNN-F adversarial training, where v stands for the logits. x, h and g are input image, en-coded feature, and generated feature, respectively.
Experimental setup We train the CNN-F on Fashion-MNIST and
CIFAR-10 datasets respectively. For Fashion-MNIST, we train a network with 4 convolution layers and 3 fully-connected layers. We use 2 convolutional layers to encode the image into feature space and reconstruct to that feature space. For CIFAR-10, we use the WideResNet architecture [39] with depth 40 and width 2. We reconstruct to the feature space after 5 basic blocks in the ﬁrst network block. For more detailed hyper-parameter settings, please refer to Appendix B.2. During training, we use PGD-7 to attack the
ﬁrst forward pass of CNN-F to obtain adversarial samples. During testing, we also perform SPSA 7
[34] and transfer attack in addition to PGD attack to prevent the gradient obfuscation [1] issue when evaluating adversarial robustness of a model. In the transfer attack, we use the adversarial samples of the CNN to attack CNN-F.
Main results CNN-F further improves the robustness of CNN when combined with adversarial training. Table 2 and Table 3 list the adversarial accuracy of CNN-F against several attack methods on Fashion-MNIST and CIFAR-10. On Fashion-MNIST, we train the CNN-F with 1 iterations. On
CIFAR-10, we train the CNN-F with 2 iterations. We report two evaluation methods for CNN-F: taking the logits from the last iteration (last), or taking the average of logits from all the iterations (avg). We also report the lowest accuracy among all the attack methods with bold font to highlight the weak spot of each model. In general, we ﬁnd that the CNN-F tends to be more robust to end-to-end attack compared with attacking the ﬁrst forward pass. This corresponds to the scenario where the attacker does not have access to internal iterations of the CNN-F. Based on different attack scenarios, we can tune the hyper-paramters and choose whether averaging the logits or outputting the logits from the last iteration to get the best robustness performance (Appendix B.2).
Table 2: Adversarial accuracy on Fashion-MNIST over 3 runs. (cid:15) = 0.1.
Clean
PGD (ﬁrst)
PGD (e2e)
SPSA (ﬁrst)
SPSA (e2e)
Transfer
Min
CNN
CNN-F (last)
CNN-F (avg) 89.97 ± 0.10 89.87 ± 0.14 89.77 ± 0.08 77.09 ± 0.19 79.19 ± 0.49 79.55 ± 0.15 77.09 ± 0.19 78.34 ± 0.29 79.89 ± 0.16 87.33 ± 1.14 87.10 ± 0.10 88.27 ± 0.91 87.33 ± 1.14 87.33 ± 0.89 88.23 ± 0.81
— 82.76 ± 0.26 83.15 ± 0.17 77.09 ± 0.19 78.34 ± 0.29 79.55 ± 0.15
Table 3: Adversarial accuracy on CIFAR-10 over 3 runs. (cid:15) = 8/255.
Clean
PGD (ﬁrst)
PGD (e2e)
SPSA (ﬁrst)
SPSA (e2e)
Transfer
Min
CNN
CNN-F (last)
CNN-F (avg) 79.09 ± 0.11 78.68 ± 1.33 80.27 ± 0.69 42.31 ± 0.51 48.90 ± 1.30 48.72 ± 0.64 42.31 ± 0.51 49.35 ± 2.55 55.02 ± 1.91 66.61 ± 0.09 68.75 ± 1.90 71.56 ± 2.03 66.61 ± 0.09 51.46 ± 3.22 58.83 ± 3.72
— 66.19 ± 1.37 67.09 ± 0.68 42.31 ± 0.51 48.90 ± 1.30 48.72 ± 0.64 4