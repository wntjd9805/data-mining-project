Abstract
A prominent technique for self-supervised representation learning has been to contrast semantically similar and dissimilar pairs of samples. Without access to labels, dissimilar (negative) points are typically taken to be randomly sampled datapoints, implicitly accepting that these points may, in reality, actually have the same label. Perhaps unsurprisingly, we observe that sampling negative examples from truly different labels improves performance, in a synthetic setting where labels are available. Motivated by this observation, we develop a debiased contrastive objective that corrects for the sampling of same-label datapoints, even without knowledge of the true labels. Empirically, the proposed objective consistently outperforms the state-of-the-art for representation learning in vision, language, and reinforcement learning benchmarks. Theoretically, we establish generalization bounds for the downstream classiﬁcation task. 1

Introduction
Learning good representations without supervision has been a long-standing goal of machine learning.
One such approach is self-supervised learning, where auxiliary learning objectives leverage labels that can be observed without a human labeler. For instance, in computer vision, representations can be learned from colorization [45], predicting transformations [10, 32], or generative modeling
[24, 14, 3]. Remarkable success has also been achieved in the language domain [30, 25, 8].
Recently, self-supervised representation learning algorithms that use a contrastive loss have out-performed even supervised learning [15, 28, 19, 18, 2]. The key idea of contrastive learning is to contrast semantically similar (positive) and dissimilar (negative) pairs of data points, encouraging the representations f of similar pairs (x, x+) to be close, and those of dissimilar pairs (x, x−) to be more orthogonal [33, 2]:
E x,x+,{x− i }N i=1 (cid:34)
− log ef (x)T f (x+) ef (x)T f (x+) + (cid:80)N i=1 ef (x)T f (x− i ) (cid:35)
. (1)
In practice, the expectation is replaced by the empirical estimate. For each training data point x, it is common to use one positive example, e.g., derived from perturbations, and N negative examples x− i .
Since true labels or true semantic similarity are typically not available, negative counterparts x− i are commonly drawn uniformly from the training data. But, this means it is possible that x− is actually similar to x, as illustrated in Figure 1. This phenomenon, which we refer to as sampling bias, can empirically lead to signiﬁcant performance drop. Figure 2 compares the accuracy for learning with this bias, and for drawing x− i from data with truly different labels than x; we refer to this method as unbiased (further details in Section 5.1).
However, the ideal unbiased objective is unachievable in practice since it requires knowing the labels, i.e., supervised learning. This dilemma poses the question whether it is possible to reduce the gap 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: “Sampling bias”: The common prac-tice of drawing negative examples x− from the i data distribution p(x) may result in x− that are i actually similar to x.
Figure 2: Sampling bias leads to perfor-mance drop: Results on CIFAR-10 for drawing x− from p(x) (biased) and from i data with different labels, i.e., truly seman-tically different data (unbiased). between the ideal objective and standard contrastive learning, without supervision. In this work, we demonstrate that this is indeed possible, while still assuming only access to unlabeled training data and positive examples. In particular, we develop a correction for the sampling bias that yields a new, modiﬁed loss that we call debiased contrastive loss. The key idea underlying our approach is to indirectly approximate the distribution of negative examples. The new objective is easily compatible with any algorithm that optimizes the standard contrastive loss. Empirically, our approach improves over the state of the art in vision, language and reinforcement learning benchmarks.
Our theoretical analysis relates the debiased contrastive loss to supervised learning: optimizing the debiased contrastive loss corresponds to minimizing an upper bound on a supervised loss. This leads to a generalization bound for the supervised task, when training with the debiased contrastive loss.
In short, this work makes the following contributions:
• We develop a new, debiased contrastive objective that corrects for the sampling bias of negative examples, while only assuming access to positive examples and the unlabeled data;
• We evaluate our approach via experiments in vision, language, and reinforcement learning;
• We provide a theoretical analysis of the debiased contrastive representation with generalization guarantees for a resulting classiﬁer. 2