Abstract
Gaussian processes (GPs) are ﬂexible priors for modeling functions. However, their success depends on the kernel accurately reﬂecting the properties of the data.
One of the appeals of the GP framework is that the marginal likelihood of the kernel hyperparameters is often available in closed form, enabling optimization and sampling procedures to ﬁt these hyperparameters to data. Unfortunately, point-wise evaluation of the marginal likelihood is expensive due to the need to solve a linear system; searching or sampling the space of hyperparameters thus often dominates the practical cost of using GPs. We introduce an approach to the identiﬁcation of kernel hyperparameters in GP regression and related problems that sidesteps the need for costly marginal likelihoods. Our strategy is to “amortize” inference over hyperparameters by training a single neural network, which consumes a set of regression data and produces an estimate of the kernel function, useful across different tasks. To accommodate the varying dimension and cardinality of different regression problems, we use a hierarchical self-attention-based neural network that produces estimates of the hyperparameters which are invariant to the order of the input data points and data dimensions. We show that a single neural model trained on synthetic data is able to generalize directly to several different unseen real-world GP use cases. Our experiments demonstrate that the estimated hyperparameters are comparable in quality to those from the conventional model selection procedures, while being much faster to obtain, signiﬁcantly accelerating
GP regression and its related applications such as Bayesian optimization and
Bayesian quadrature. The code and pre-trained model are available at https:
//github.com/PrincetonLIPS/AHGP. 1

Introduction
Gaussian processes (GPs) are powerful tools for modeling distributions over functions. They are highly ﬂexible Bayesian nonparametric models for which the posterior is often available in closed form. GPs are successful models for a variety of machine learning tasks, from regression and classiﬁcation [53], to Bayesian optimization [65], to modeling of dynamics [36, 70]. The predictive performance of a Gaussian process, however, is highly dependent on the speciﬁcs of the prior on functions, as determined by the associated positive deﬁnite kernel function [53]. To ﬁnd a good prior, one needs to ﬁrst come up with a family of kernel functions that is capable of capturing the structure of the data.
The kernel hyperparameters must then be determined from data, usually by maximizing the log marginal likelihood (MLL), i.e., empirical Bayes [53]. There are two major issues associated with this procedure. First, evaluating the log marginal likelihood (and its gradient) for different hyperparameters is generally expensive, with O(N 3) computational complexity for N training data.
Second, the log MLL is usually highly non-concave, making both sampling and optimization difﬁcult. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Getting stuck in bad local maxima or saddle points can result in hyperparameters with signiﬁcant model mismatch and thus poor predictive performance [51, 45, 77]. Alternatively, to avoid the catastrophic failure of bad local minima and point-estimates in MLL optimization, one may attempt a full Bayesian treatment by integrating out the kernel hyperparameters using Markov chain Monte
Carlo (MCMC) techniques [43, 16, 17]. In this paper, we focus on the optimization case, although many of the ideas we present could also be applied to marginalization of hyperparameters.
Various approaches to scaling Gaussian processes have been proposed, exploring innovative ideas such as intelligently selecting a subset of the data [63, 72] or constructing a low-rank approximation of the covariance matrix based on virtual “inducing” points [8, 50, 60, 64, 67, 28]. These scaling approaches have generally focused on how to solve the linear system more quickly without having to signiﬁcantly compromise the model or the predictive performance, either by reducing the size of the linear system, solving the linear system approximately, or approximating the model with one that has computationally convenient structure. Focusing on the linear system has been a sensible approach, as effective solutions simultaneously enable both direct prediction with the Gaussian process and evaluation of hyperparameters via the log marginal likelihood.
Here however, we take an entirely different approach and focus solely on the model selection problem, without reference to linear systems at all. Instead, we amortize the Gaussian process model selection problem by training a neural network to consume input/output observations and emit an estimate of the hyperparameters that would otherwise arise from maximizing the log marginal likelihood. This approach is inspired by amortized variational inference approaches [33, 55, 26, 56], which similarly sidestep expensive optimization procedures in favor of directly producing estimates. In the variational inference case, the neural network produces approximate posterior distributions over the unknown latent variables; here we produce point estimates of the unknown hyperparameters.
Of course, as noted above, selection of the kernel family is just as important as determination of hyperparameters, and we view this as a crucial piece of the amortized model selection puzzle. One approach to modeling the huge space of valid kernel functions is to represent the target kernel as a composition of different base kernels [12, 39, 66]. In principle, various base kernels, composition rules, and associated hyperparameters could be modeled as latent random variables, and emitted by a neural network. However, since the latent variables here involve a mix of discrete variables (types and combinations of kernel functions) and continuous variables (hyperparameters associated), we have found this approach to be difﬁcult. Moreover, the interrelated effects of, e.g., length scale and kernel choice, make it difﬁcult to unify the hyperparameter space across composed kernels. Thus, instead of working with compositions of commonly-used kernels, we focus on stationary kernels in the spectral domain and directly learn the spectral density of the kernel function [73, 57, 54]. The space of spectral densities provides a uniﬁed and compact continuous representation of the space of stationary covariance functions. In particular, we model the spectral density of the kernel function as a mixture [73] that is capable of approximating any stationary kernel arbitrarily well.
There are two particularly salient challenges associated with training a single neural network to produce effective hyperparameters for many different regression-type tasks: both the amount of data and the dimensionality of the input can vary from problem to problem. To address this, we develop a specialized hierarchical self-attention structure that consumes datasets and produces spectral densities while being invariant to permutations of the data and the dimensions. Thus, this single “meta-model” can be applied to different problems for which a Gaussian process is applicable. The parameters of the network are trained using gradients computed via reverse-mode automatic differentiation through the log marginal likelihood of the Gaussian process, for randomly-generated synthetic data from the prior. Then we directly apply the trained neural model to real-world datasets of varying size and dimension in different GP applications. Even though the model is trained with only synthetic data, experimental evidence indicates that the estimated hyperparameters are comparable in quality to those from the conventional model selection procedures while being ∼100 times faster to obtain. 2 Gaussian Processes
In this section, we establish background concepts and notations necessary for the discussion of the amortized hyperparameters inference approach described in Section 3. 2
A Gaussian process deﬁnes a distribution over functions f : X → R, and is speciﬁed by its mean function µ(x) and positive-deﬁnite covariance function k(x, x(cid:48)), where x, x(cid:48) ∈ X : f (x) ∼ GP (µ(·), k (·, ·)) ,
µ(x) = E[f (x)], k (x, x(cid:48)) = cov (f (x), f (x(cid:48))) (1)
For any ﬁnite set of points in X , X := {x1, . . . , xN }, the corresponding function val-ues f := (f (x1), · · · , f (xN ))T follow a multivariate Gaussian distribution: f ∼ N (µ, KXX), where [µ]i = µ(xi) and [KXX]ij = k(xi, xj). For a training dataset D = {(xi, yi)}N i=1, each yi is commonly assumed to be generated by adding an i.i.d. zero-mean Gaussian noise to f (xi), (cid:15) ). Denote y := [y1, · · · , yN ](cid:62) ∈ RN ×1. For new data i.e., yi = f (xi) + (cid:15)i, where (cid:15)i ∼ N (0, σ2 input (cid:101)X := {(cid:101)x1, . . . , (cid:101)xN (cid:48)} of size N (cid:48), the Gaussianity of the prior and likelihoods make it possible to compute the predictive distribution in closed form: (cid:101)f | (cid:101)X, D ∼N (cid:0) (cid:1) , (cid:101)µ = K (cid:15) I(cid:1)−1 where KX (cid:101)X ∈ RN ×N (cid:48)
Choice of kernel function. The choice of kernel function is crucial to Gaussian process generaliza-tion, as different kernel functions impose various model assumptions, e.g., smoothness, periodicity, etc. (See Chapter 4 of Rasmussen and Williams [53] for an extensive discussion.) If the problem has a known structure, one can sometimes choose a kernel to capture it. Otherwise, kernel learning must be performed by deﬁning an expressive space of kernel functions and selecting the best one through optimization [73, 75, 66] or search [12, 39]. with [KX (cid:101)X]ij = k(xi, (cid:101)xj). (cid:0)KXX + σ2 (cid:0)KXX +σ2 (cid:101)X (cid:101)X −K (cid:15) I(cid:1)−1 (cid:101)f = K
KX (cid:101)X, (cid:101)µ, K (cid:101)f y, K (cid:101)XX (cid:101)XX
Hyperparameter inference. Beyond the particular choice of kernel, it is also common for the covariance function to have so-called hyperparameters θ that govern its speciﬁc structure, and the parameterized kernel function is written as kθ(·, ·). Although a fully-Bayesian treatment is possible
[45, 43, 16, 44], the most common approach to determining hyperparameters is to use empirical
Bayes and maximize the log marginal likelihood (evidence) with respect to the hyperparameter θ, i.e., perform type II maximum likelihood [3, 41]. The log MLL for observed data {X, y} is given by: log p(y|X, θ) = − y(cid:62) (cid:0)KXX(θ) + σ2 (cid:15) I(cid:1)−1 y − 1 2 1 2 log (cid:12) (cid:12)KXX(θ) + σ2 (cid:15) I(cid:12) (cid:12) −
N 2 log 2π , (2) where we write KXX(θ) to indicate the dependence of the Gram matrix on the hyperparameters.
To solve the above optimization problem, quasi-Newton methods such as L-BFGS [38] or nonlinear conjugate gradient [30, 19] are usually used. These iterative optimization methods involve taking the gradient of the objective several times for each optimization step. As the gradient of Eqn 2 scales as O(N 3), this optimization becomes prohibitively expensive on large-scale problems, dominating the computational cost of using GP. Moreover, the non-concavity of the objective in Eqn 2 makes it difﬁcult to ensure convergence to a good maximum.
To address the scaling issue, a low-rank approximation to the kernel matrix is often used either by subsampling the data or via virtual “inducing” points [63, 72, 8, 50, 60, 64, 67, 28, 6, 61].
In general, these methods require inversion of a smaller matrix and reduce the computational complexity to O(N M 2) (M is the number of subsampled data or “inducing” points), at the cost of a larger and often more challenging optimization problem alongside the potential loss of important information from the dataset. For the special case of the exponentiated quadratic kernel, Burt et al. [5] showed that only O(N log2D(N )) computational complexity is needed to achieve an arbitrarily good approximation with high probability for input with either compact support or Gaussian distribution of
D dimensions. 3 Amortized GP Hyperparameter Inference
In this section, we frame the problem of amortized Gaussian process hyperparameter inference.
The objective is to avoid the computational overhead and fragility of maximizing the log marginal likelihood, particularly within the inner loop of an iterative procedure such as Bayesian optimization or Bayesian quadrature. Our approach is to instead train a ﬂexible function approximator, i.e., a neural network, to provide high-quality estimates of kernel hyperparameters, directly from the data.
We call our method amortized hyperparameter inference for Gaussian processes (AHGP). 3
Figure 1: The top part of the ﬁgure gives an illustration of the computation graph in AHGP. The bottom part describes our hierarchical attention neural net architecture.
In GP regression, it is common to use stationary kernels, which include most generic covariance functions, such as the exponentiated quadratic, rational quadratic and Matérn. Consistent with this, we will study stationary kernels, but seek to make them highly ﬂexible. One way to achieve ﬂexibility is via composition of different commonly-used base kernels as in Duvenaud et al. [12], Lloyd et al.
[39], Sun et al. [66]. Model selection in this approach—choice of base kernels, composition rules, and kernel hyperparameters—could in principle be done via a neural network; however, we have found the associated optimization problem to be difﬁcult and complicated. First, it involves optimization over both discrete (kernel types and composition rules) variables and continuous (hyperparameters) variables. Second, different kernels bring with them continuous parameters with different cardinalities and interpretations, making it challenging to identify a uniﬁed hyperparameter space. 3.1 Spectral Modeling of Stationary Kernel Functions
In lieu of a compositional approach to the kernel function, we build a ﬂexible approach around the duality between stationary kernels and their spectral density, taking advantage of the well-known theorem by Bochner stated below.
Theorem 1. (Bochner [4]) A complex-valued function k on Rd is the covariance function of a weakly stationary (also known as covariance-stationary) mean square continuous complex valued random process on Rd if and only if it can be represented as k(τ ) = (cid:90)
Rd e2πiω(cid:62)τ dµ(ω), (3) where µ is a positive ﬁnite measure, often known as the spectral measure of the random process.
When µ is absolutely continuous with respect to the Lebesgue measure, its Radon–Nikodym derivative (density) S(ω) is called the spectral density of the random process. The kernel function k(·) and the spectral density S(·) can be understood to be Fourier transform pairs. We take advantage of this correspondence and use a neural network to predict the spectral density of the kernel function rather than the covariance function itself.
Following previous work [73, 74], we model the spectral density via a Gaussian mixture, leading to interpretability and closed form evaluation of the kernel. Additionally, the fact that Gaussian mixtures are dense in the space of probability distribution functions [62, 73] makes them capable of approximating the spectral density of any stationary kernel function arbitrarily well. Here we further assume that the kernel function has a product structure over different dimensions and every dimension has its own mixture of Gaussians. The product kernel structure is a common modeling choice in
Gaussian process regression, and arises naturally in many generic kernels such as the exponentiated quadratic and its automatic relevance determination (ARD) version. Additionally, it is common to compose kernels via element-wise products, with each dimension’s functional properties encoded in its corresponding kernel function [53, 23, 12, 74]. 4
3.2 Formulation
, y(l) i )}Nl i = fl(x(l)
We now formalize this problem of amortized hyperparameter inference in a Gaussian process. We are interested in ﬁtting many different regression functions of the form f : RD → R, with varying values of D. For the l-th regression task T (l), a set of input/output training data are observed and i=1 = {X(l), y(l)}, where y(l) are given by D(l) := {(x(l) i ∈ RDl i and (cid:15)(l) i being i.i.d. zero-mean Gaussian noise. Assume we are given L tasks, with each task randomly i.i.d.∼ p(T ). We further assume that for sampled i.i.d. from a distribution over tasks, i.e., {T (l)}L each task, the function values are generated by some underlying Gaussian process with its own unique kernel hyperparameters. The spectral density of each task’s GP is modeled as a mixture of M
Gaussian over each dimension as discussed in Sec. 3.1, with weights, means and variances denoted as
θ(l) d = {{w(l) d,m}M m=1} for the d-th dimension in task l. For compactness, we use θ(l) = {θ(l) d=1 to denote the collective hyperparameters of the spectral density for task l.
The neural network, parameterized by φ, deﬁnes a function gφ from a dataset D(l) to an estimate of its spectral density hyperparameters θ(l), i.e., θ(l) = gφ(D(l)). Through the duality of spectral densities and stationary kernel functions, the spectral mixture product (SMP) kernel [74] is given by: m=1, {µ(l) d }Dl i with x(l) m=1, {σ2(l) i ) + (cid:15)(l) d,m}M d,m}M l=1 kSMPθ(τ ) =
D (cid:89) d=1 kSMPθd (τd) with kSMPθd (τd) =
M (cid:88) wd,m exp (cid:8)−2π2τ 2 d σ2 d,m (cid:9) cos (2πτdµd,m) , m=1 where τd is the d-th component of τ = x − x(cid:48) ∈ RD, i.e., the difference of two data points.
We can now train the neural network to produce hyperparameters using a “dataset of datasets”, (cid:8)D(l)(cid:9)L l=1. With the closed form kernel function speciﬁed by its spectral density hyperparameters, the averaged negative log marginal likelihood evaluated from Eqn (2) is used as our training objective: (cid:18)
L
φ, (cid:110)
D(l)(cid:111)L (cid:19)
= − l=1
L (cid:88) 1
L 1
Nl (cid:16) log p y(l) (cid:12) (cid:12) (cid:12) X(l), θ(l)(cid:17)
, (4) l=1 where θ(l) = gφ(D(l)) and Nl is the number of data points in the l-th dataset. Once the neural network is trained, it can be used to estimate the GP kernel function that would be appropriate for a new set of input/output data Dtest by simply doing a forward pass of the neural model. 4 Hierarchical Attention Network for GP Hyperparameter Learning d }Dl
As described in the previous section, the neural network learns a function from a dataset D(l) to spectral density parameters {θ(l) d=1, determining the GP kernel function for task l. As in other deep learning problems, the architecture of the neural network is critical; in particular, the structure of the network must take advantage of available symmetries. In our case, for general purpose inference of GP kernel hyperparameters, we require an architecture that is versatile enough to accommodate datasets of varying input dimension and with different number of data points. Furthermore, the model should be invariant to permutation of both the data and input dimensions. In other words, for a given dataset D(l), neither shufﬂing the order of the (exchangeable) data nor shufﬂing the order of the dimensions should change the resulting estimate of the kernel function. Importantly, the regularization and parameter sharing induced by enforcement of such invariances should enable the neural network to learn better and faster, analogously to convolutional neural networks for images.
Architecture. We draw inspiration from multi-head self-attention mechanisms and propose a hierarchical Transformer [68] type of neural network architecture for tackling the problem of learning
GP hyperparameters. A general Transformer model has multiple layers and each layer consists of a multi-head self-attention sub-layer followed by a feed forward network with residual connections and layer normalizations. It serves as an autoregressive encoder that maps a set of input data to a set of output representations. In particular, the self-attention sub-layers allow each input datum to attend to the representations of other data and produce context-aware representations. Multiple layers of self-attention enable modeling of high-order non-linear interactions between input representations.
For details about multi-head self-attention mechanisms, we refer readers to Appendix A.
Brieﬂy, our network architecture mainly consists of two hierarchically nested Transformer-like blocks.
A graphic illustration of the proposed architecture is presented in Fig. 1. 5
, y(l) i,d}Nl i,d, y(l) local,d}Dl d = {(x(l) i ). i=1 are aggregated through an local,d for the d=1 provide a summary of each dimension’s
Local Transformer: The ﬁrst transformer block, LocalTransformer, serves as an encoder of the per-dimension local information about the observed function, e.g., length scale, smoothness, periodicity, etc. It takes in a set of input/output data speciﬁc to the d-th dimension, e.g., D(l) i )}Nl i=1, and outputs a corresponding set of representations {h(l) i,d}Nl i=1. In LocalTransformer, only interactions within the d-th dimension are involved, hence each h(l) i,d is a context-aware representation of local information about the underlying function around dimension d of datum (x(l) i
Aggregate Function: The outputs from LocalTransformer{h(l)
AggregateFunction that assembles a single local dimension-speciﬁc representation h(l) d-th dimension. These feature representations {h(l) local information regarding the observed function.
Global Transformer: After the dimension-speciﬁc local representations {h(l) d=1 are computed, they are fed into a second dimension-level Transformer block, GlobalTransformer, where non-linear interactions between the dimensions are modeled through multiple layers of multi-head self-attention. The ﬁnal per-dimensional representations {h(l) d=1, which serve as context-aware representations at a global (dimension) level, are further passed through a multi-layer perceptron (MLP) to produce the ﬁnal spectral density hyperparameters {θ(l)
Versatility and permutation invariance. Self-attention enables the model to consume a set of input/output data with arbitrary data cardinality and dimensionality. The versatility of the model makes it general-purpose: one could train a single neural model to predict GP kernel hyperparameters of different tasks with varying data cardinality and dimensionality, as long as the inputs are real-valued.
The proposed model also possesses the following permutation equivariance/invariance properties.
Proposition 1. If AggregateFunction is permutation invariant, and weights of LocalTransformer and MLP are shared across dimensions, then the proposed neural network is permutation equivariant with respect to data dimensions and permutation invariant with respect to data points. global,d}Dl d }Dl local,d}Dl d=1.
Intuitively, these properties are inherited from the permutation equivariance of self-attention and the permutation invariance of the AggregateFunction. See Appendix B for a detailed proof. It is also noted that the versatility and inductive biases (permutation invariance/equivariance) of the proposed hierarchical attention network are generic, making our proposed neural model potentially useful for other tasks that involve learning representations over datasets, such as learning statistics of datasets in Edwards and Storkey [13].
Complexity analysis. For each multi-head self-attention layer, the computational complexity is O(h · n2), where h is the representation dimension and n is the size of the input set. Assuming that
LocalTransformer has l1 layers with representation dimension h1 and GlobalTransformer has l2 layers with representation dimension h2, the complexity of our model is O(l1 · h1 · N 2 + l2 · h2 · D2), where N is the number of data points and D is the dimensionality. In comparison, exact marginal likelihood optimization scales as O(r · N 3), where r denotes the number of gradient evaluations during optimization. It is possible to further reduce the complexity of our model to O(l1 · h1 · m · N + l2 · h2 · m · D) if we restrict the number of attentions to m by either introducing sparse attentions [7] or inducing points [37], which we leave for future work. 5 Experimental Results
To empirically evaluate the AHGP, we studied three different GP use cases: regression, Bayesian optimization and Bayesian quadrature.
Baselines. We compare our method to the standard approach of maximizing the log marginal likelihood with respect to hyperparameters. We also compare with the sparse variational Gaussian processes method (SGPR) [67, 28], which uses inducing points to approximate the full GP. The focus of the comparisons will be on the quality of the selected kernel hyperparameters and the run time of the hyperparameter selection procedure.
The baselines are implemented with two popular GP packages: GPy [25] (implemented for CPU) and GPyTorch [20] (implemented for GPU). The spectral mixture (SM) kernel and spectral mixture product (SMP) kernel are used as the kernel functions. These give rise to eight different baselines:
GPy-SM, GPy-SM-Sp, GPy-SMP, GPy-SMP-Sp, GPT-SM, GPT-SM-Sp, GPT-SMP, GPT-SMP-Sp, where we use “GPT” to denote “GPyTorch” and “Sp” to denote “SGPR”. The default L-BFGS 6
(a) Test RMSE (b) Runtime ratio over AHGP (c) Gap with best RMSE
Figure 2: Comparison of AHGP against the CPU baselines on regression benchmarks. In (c), the numbers are the differences of the corresponding method’s test RMSE with the best RMSE on the respective dataset.
Note that for Naval, the RMSEs are all very close to 0. (Only average test performance is shown here. Refer to
Appendix C for complete results with error bars.) optimizer is used for GPy and the default Adam [32] optimizer is used for GPyTorch. The GPyTorch baselines make use of batched conjugate gradient to invert the kernel matrix for efﬁcient approximate inference. We additionally implement a full GP baseline with SMP kernel that uses Cholesky decomposition in PyTorch [48], and its MLL is optimized via reverse-mode automatic differentiation.
We will refer to this baseline as PyT-AD-SMP.
Experimental setup.
In our experiments, the training data are constructed by sampling multiple sets of synthetic input/output data from a GP prior with a stationary kernel. Dimensions vary from 2 to 15. More details about data generation are provided in Appendix C. A single neural model is trained on the synthetic data using Adam [32] with a ﬁxed learning rate, and the same trained model is then used across all evaluations. To validate the effectiveness of our neural network model, we minimize the efforts of hyperparameter tuning during training. The only hyperparameters we tuned are learning rate and number of layers in LocalTransformer and GlobalTransformer. Average pooling is used as the AggregateFunction. Details about the hyperparameters used are included in
Appendix C. After training is done with the synthetic data, the same trained neural model is directly used to predict the kernel hyperparameters for the various unseen GP use-cases that are shown later.
During evaluation and training, both the data input and output are standardized and the noise variance of GP is ﬁxed at 0.01. Note that it is possible for our method to learn the noise variance too. In our experiments, we ﬁnd the predictive performance is not sensitive to the noise variance if the data are standardized and setting it to 0.01 gives competitive performance for all baselines. Meanwhile, it is noted that spectral mixture kernel is ﬂexible enough to model the noise variance component as well: if one of the Gaussian mixture components has weight w, µ = 0 and very large σ2, the corresponding kernel matrix will be approximately wI and the weight w represents the noise scale.
Regression benchmarks. We evaluate our method and the baselines on regression benchmarks from the UCI collection [1] used in Hernández-Lobato and Adams [29] and Sun et al. [66] following the same setup: the data are randomly split to 90% for training and 10% for testing. This splitting process is repeated 10 times and the average test performance is reported. Comparisons with
CPU-based baselines on test RMSE, test log-likelihood and runtime are presented in Fig. 2 and
Table 6 (Appendix C). We observe that AHGP has consistently lower run times than the baselines, averaging ∼100 times faster. Nevertheless, the predictive performance of AHGP is comparable to (and sometimes better than) the strongest baselines, which perform MLL optimization without approximation. Notably, AHGP seems to perform slightly better on datasets with fewer data points, such as Yacht. We believe this demonstrates the robustness of AHGP when there is not enough data for MLL-opt based approaches to form reasonable point estimates. The sparse variational GP methods are faster than the full GP methods in general, but with lower performance on both test RMSE and test log-likelihood. Comparisons with GPU-based methods (GPyTorch-based and PyT-AD-SMP) are in Appendix C, where similar ﬁndings are obtained.
Bayesian optimization. Bayesian optimization [42] (BO) uses a GP as a surrogate model when the objective function is expensive to evaluate. The method involves ﬁtting the GP kernel hyperparameters and maximizing an acquisition function to select a candidate point that is highly promising to achieve the function minima (maxima) under the model. Since the method is iterative, MLL optimization needs to be conducted at every BO iteration to update the GP. An amortized approach would greatly reduce the computation involved. We pick the best performing baselines on the regression benchmarks (GPy-SM, GPy-SM-Sp, PyT-AD-SMP) and compare them with AHGP. Expected improvement (EI) 7
is used as the acquisition function. We use standard test functions for global optimization [9] as the target functions for the BO experiments. (a) Baseline methods use random re-initialization strategy. (b) Baseline methods use warmstart initialization strategy.
Figure 3: BO performance comparisons. Left: Minimum function values found v.s. number of BO iterations.
Shaded region represents 0.5 standard deviation over 10 runs. Right: Runtime ratio over AHGP. (Only average is plotted here, refer to Appendix C for mean and standard deviation.)
At the start of every BO iteration, the hyperparameters are randomly re-initialized for all baselines. A sample of the experimental results is shown in Fig. 3a. (Full results can be found in Appendix C.)
Again, AHGP is a substantial improvement in run-time. In terms of minimum values found, AHGP is on par with the baselines on some functions and slightly worse on functions with higher dimensionality.
Of particular note—consistent with what was seen on the regression benchmarks—AHGP has the greatest improvement in the beginning when there are few observations available.
To ensure fairness to baseline ﬁtting procedures, we also conducted experiments where the hyperpa-rameter selection in the BO inner loop was initialized using the best from the previous iteration. As expected, this warm-starting results in decreased run times for the baselines, although still slower than AHGP (in Fig. 3b). This warm-starting, however, seems to compromise the hyperparameter selection—presumably due to local minima—and damage the overall outer loop optimization.
Bayesian quadrature. Bayesian quadrature [46, 52] performs Bayesian inference about the value of a difﬁcult numerical integral through modeling the underlying function as a GP. The tractability of the Gaussian process makes it relatively easy to calculate a Bayesian estimate of many integrals in closed form. For example, Gunter et al. [27] applies Bayesian quadrature to the task of probabilistic inference and achieves faster convergence than Monte Carlo methods.
TARGET FUNCTION
GPy-SM
GPy-SM-Sp
PyT-AD-SMP AHGP(Ours)
Hennig1D
Hennig2D
Sombrero2D
Circular Gaussian
NaN
NaN
NaN 3.79±1.01
-29.66±26.12
-39.99±36.57
-0.63±3.56 4.01±1.27 4.23±0.16
-0.13±4.48 4.95±0.26 5.17±0.04 3.89±0.27 2.41±2.30 3.23±0.43 3.47±1.09
Table 1: Log-likelihood of the true integral evaluated at the ﬁnal prediction of Bayesian quadrature.
Figure 4: Bayesian quadrature performance comparisons. Left: Integral error v.s. number of BQ iterations.
Shaded region represents 0.5 standard deviation over 5 runs. Right: Runtime ratio over AHGP.
We evaluate our model on four standard test functions provided in Emukit [47]. In Fig. 4, we plot the average distance to ground truth versus number of iterations. Table 1 shows the log-likelihood of the true integral evaluated at the ﬁnal probabilistic estimate. AHGP compares well with the baselines across all tasks. More importantly, AHGP appears to be robust across different tasks while the baselines often suffer from large variances and occasional divergence. 8
6