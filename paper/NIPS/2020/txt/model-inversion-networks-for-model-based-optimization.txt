Abstract
This work addresses data-driven optimization problems, where the goal is to ﬁnd an input that maximizes an unknown score or reward function given access to a dataset of inputs with corresponding scores. When the inputs are high-dimensional and valid inputs consistute a small subset of this space (e.g., valid protein se-quences or valid natural images), such model-based optimization problems become exceptionally difﬁcult, since the optimizer must avoid out-of-distribution and in-valid inputs. We propose to address such problem with model inversion networks (MINs), which learn an inverse mapping from scores to inputs. MINs can scale to high-dimensional input spaces and leverage ofﬂine logged data for both con-textual and non-contextual optimization problems. MINs can also handle both purely ofﬂine data sources and active data collection. We evaluate MINs on high-dimensional model-based optimization problems over images, protein designs, and neural network controller parameters, and bandit optimization from logged data. 1

Introduction
Data-driven optimization problems arise in a range of domains: from protein design [4] to automated aircraft design [13], from the design of robots [18] to the design of neural network architectures [43].
Such problems require optimizing unknown score functions using datasets of input-score pairs, without direct access to the score function being optimized. This can be especially challenging when valid inputs lie on a low-dimensional manifold in the space of all inputs, such as the space of valid aircraft designs or valid images. Existing methods to solve such problems often use derivative-free optimization [34]. Most of these techniques require active data collection, where the unknown function is queried at new inputs. However, when function evaluation involves a complex real-world process, such as testing a new aircraft design or evaluating a new protein, such active methods can be very expensive. On the other hand, in many cases there is considerable prior data – existing aircraft and protein designs, and advertisements and user click rates – that could be leveraged to solve the optimization problem.
In this work, our goal is to develop a method to solve such optimization problems that can (1) readily operate on high-dimensional inputs comprising a narrow, low-dimensional manifold in the input space, (2) readily utilize ofﬂine static data, and (3) learn with minimal active data collection if needed.
We can deﬁne this problem setting formally as the optimization problem x(cid:63) = arg max x f (x), (1) where the score/reward function f (x) is unknown, and we have access to a dataset D =
{(x1, y1), . . . , (xN , yN )}, where yi denotes the value f (xi). In some cases, which we refer to as active model-based optimization, we can collect additional data with active queries. However, in many practical settings, no further data collection is possible. We call this data-driven model-based 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
optimization. This problem statement can also be extended to the contextual setting, where the aim is to optimize the expected score function across a context distribution. That is,
π(cid:63) = arg max
Ec∼p(·)[f (c, π(c))], (2)
π where π(cid:63) maps contexts c to inputs x, such that the expected score under the context distribution p(c) is optimized. As before, f (c, x) is unknown, and we use a dataset D = {(ci, xi, yi)}N i=1, where yi is the value of f (ci, xi). Such contextual problems with logged datasets have been studied in the context of contextual bandits [37, 15].
A simple way to approach these model-based optimization problems is to train a proxy function fθ(x) or fθ(c, x), with parameters θ, to approximate the true score, using the dataset D. However, directly using fθ(x) in place of the true function f (x) in Equation (1) generally works poorly, because the optimizer will quickly ﬁnd an input x for which fθ(x) outputs an erroneously large value. This issue is especially severe when the inputs x lie on a narrow manifold in a high-dimensional space, such as the set of natural images [42]. The function fθ(x) is only valid near the training distribution, and can output erroneously large values when queried at points chosen by the optimizer. Prior work has sought to addresses this issue by using uncertainty estimation and Bayesian models [35] for fθ(x), as well as active data collection [34]. However, explicit uncertainty estimation is difﬁcult when the function fθ(x) is very complex or when x is high-dimensional.
Instead of learning fθ(x), we propose to learn the inverse function, mapping from values y to corresponding inputs x. This inverse mapping is one-to-many, and therefore requires a stochastic mapping, which we can express as f −1 (y, z) → x, where z is a random variable. We term such models model inversion networks (MINs). MINs can handle high-dimensional input spaces such as images, can tackle contextual problems, and can accommodate both static datasets and active data collection. We discuss how to design active data collection methods for MINs, leverage advances in deep generative modeling [10, 2], and scale to very high-dimensional input spaces. We experimentally demonstrate MINs in a range of settings, showing that they outperform prior methods on high-dimensional input spaces, such as images, neural network parameters, and protein designs, and substantially outperform prior methods on contextual bandit optimization from logged data [37].
θ 2