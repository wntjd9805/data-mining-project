Abstract
Contextual policies are used in many settings to customize system parameters and actions to the speciﬁcs of a particular setting. In some real-world settings, such as randomized controlled trials or A/B tests, it may not be possible to measure policy outcomes at the level of context—we observe only aggregate rewards across a distribution of contexts. This makes policy optimization much more difﬁcult because we must solve a high-dimensional optimization problem over the entire space of contextual policies, for which existing optimization methods are not suitable. We develop effective models that leverage the structure of the search space to enable contextual policy optimization directly from the aggregate rewards using
Bayesian optimization. We use a collection of simulation studies to characterize the performance and robustness of the models, and show that our approach of inferring a low-dimensional context embedding performs best. Finally, we show successful contextual policy optimization in a real-world video bitrate policy problem. 1

Introduction
Contextual policies are used in a wide range of applications, such as robotics [22, 30] and computing platforms [9]. Here we consider contextual policies that are a map from a discrete context to a set of continuous parameters. For example, video streaming and real-time conferencing systems use adaptive bitrate (ABR) algorithms to balance between video quality and uninterrupted playback.
The optimal policy for a particular ABR controller may depend on the network—for instance, a stream with large ﬂuctuations in bandwidth will beneﬁt from different ABR parameters than a stream with stable bandwidth. This motivates the use of a contextual policy where ABR parameters are personalized by context variables such as country or network type (2G, 3G, 4G, etc.). Various other systems and infrastructure applications commonly rely on tunable parameters which can beneﬁt from contextualization. For example, optimal job scheduling and load balancing parameters may differ because of workload variations [11]. Cell tower conﬁguration or TCP conﬁgurations can beneﬁt from
ﬁner-grained information about environmental factors [2]. Contextual policies therefore provide an interpretable, robust approach for personalizing system parameters and improving individual-level outcomes under heterogeneous conditions.
Previous applications of contextual policy optimization (CPO) in the literature, such as those in robotics, have considered the setting where with each evaluation of the policy we observe both the reward and the context. Bayesian optimization (BO) has been successfully applied to this problem
[30], using a Gaussian process (GP) to model reward as a function of both parameters and context. If the context is continuous-valued, it can be incorporated directly into the GP along with the parameters;
⇤Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Leave-one-out cross validation predictions (mean, and 95% posterior predictive interval) for standard and contextual GPs ﬁt to the results of a video playback controller experiment conducted at Facebook for a 30-dimensional policy (5 contexts and 6 parameters). The standard GP is unable to learn the high-dimensional response surface, while our proposed SAC and LCE-A models make accurate out-of-sample predictions. if discrete, it can be included in the GP with a multi-task kernel or similar approaches for handling discrete parameters that are described below.
However, there are important CPO settings where context cannot be observed along with the rewards.
In A/B testing platforms, rewards (outcomes) are measured as an aggregate across a large population that spans an entire distribution of contexts. Performing analysis by context is often not feasible, and when it is, such analyses can introduce bias by implicitly conditioning on “post-treatment” variables [31]. Without context-level data, CPO becomes a high-dimensional optimization problem where C contexts each with d parameters requires optimizing a function of C d parameters.
⇥
In this work, we show that it is possible to optimize a contextual policy and get the beneﬁts of contextualization even when rewards are measured only in aggregate. The contributions of this paper are: (1) We introduce this new, practically-important problem of contextual policy optimization with unknown context rewards. (2) We develop new GP models that take advantage of the problem structure to signiﬁcantly improve over existing BO approaches. (3) We provide a thorough simulation study that shows how the models scale with factors such as the number of contexts and the population distribution of contexts, considering both aggregate rewards and fairness. (4) We introduce a new real-world problem for CPO, optimizing a contextual ABR policy, and show that our models perform best relative to a wide range of alternative approaches.
Empirically, we have found the proposed contextual GP models work well in practice for a variety of contextual policy optimization use cases, including mobile data retrieval policies, cache eviction policies, and video streaming applications. Fig. 1 gives a preview of how the methods proposed in our paper (SAC and LCE-A) enable inference and optimization of high-dimensional contextual policies us-ing data from a real-world video streaming experiment. For reproducibility, our results in Sec. 5 lever-age an open-source video streaming simulator and trace data from video playback sessions on Face-book’s Android app to evaluate how these models improve CPO performance. Code for the models and replication materials are available at https://github.com/facebookresearch/ContextualBO. 1.1