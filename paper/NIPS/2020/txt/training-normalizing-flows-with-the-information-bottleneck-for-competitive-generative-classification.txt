Abstract
The Information Bottleneck (IB) objective uses information theory to formulate a task-performance versus robustness trade-off. It has been successfully applied in the standard discriminative classiﬁcation setting. We pose the question whether the IB can also be used to train generative likelihood models such as normalizing
ﬂows. Since normalizing ﬂows use invertible network architectures (INNs), they are information-preserving by construction. This seems contradictory to the idea of a bottleneck. In this work, ﬁrstly, we develop the theory and methodology of
IB-INNs, a class of conditional normalizing ﬂows where INNs are trained using the IB objective: Introducing a small amount of controlled information loss allows for an asymptotically exact formulation of the IB, while keeping the INN’s gen-erative capabilities intact. Secondly, we investigate the properties of these models experimentally, speciﬁcally used as generative classiﬁers. This model class offers advantages such as improved uncertainty quantiﬁcation and out-of-distribution de-tection, but traditional generative classiﬁer solutions suffer considerably in clas-siﬁcation accuracy. We ﬁnd the trade-off parameter in the IB controls a mix of generative capabilities and accuracy close to standard classiﬁers. Empirically, our uncertainty estimates in this mixed regime compare favourably to conventional generative and discriminative classiﬁers. Code: github.com/VLL-HD/IB-INN 1

Introduction
The Information Bottleneck (IB) objective (Tishby et al., 2000) allows for an information-theoretic view of neural networks, for the setting where we have some observed input variable X, and want to predict some Y from it. For simplicity, we limit the discussion to the common case of discrete
Y (i.e. class labels), but results readily generalize. The IB postulates existence of a latent space Z, where all information ﬂow between X and Y is channeled through (hence the method’s name). In order to optimize predictive performance, IB attempts to maximize the mutual information I(Y, Z) between Y and Z. Simultaneously, it strives to minimize the mutual information I(X, Z) between X and Z, forcing the model to ignore irrelevant aspects of X which do not contribute to classiﬁcation performance and only increase the potential for overﬁtting. The objective can thus be expressed as
IB = I(X, Z)
L
β I(Y, Z) .
− (1)
The trade-off parameter β is crucial to balance the two aspects. The IB was successfully applied in a variational form (Alemi et al., 2017; Kolchinsky et al., 2017) to train feed-forward classiﬁcation models p(Y
X) with higher robustness to overﬁtting and adversarial attacks than standard ones.
|
In this work, we consider the relationship between X and Y from the opposite perspective – using the IB, we train an invertible neural network (INN) as a conditional generative likelihood model 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
|
Y ), i.e. as a speciﬁc type of conditional normalizing ﬂow. In this case, X is the variable of p(X which the likelihood is predicted, and Y is the class condition. It is a generative model because one can sample from the learned p(X
Y ) at test time to generate new examples from any class, although
| we here focus on optimal likelihood estimation for existing inputs, not the generating aspect.
|
|
|
Y )p(Y )/E
X). For a GC,
Y ), has special
We ﬁnd that the IB, when applied to such a like-lihood model p(X implications for the use as a so-called generative classiﬁer (GC). GCs stand in contrast to standard discrimi-native classifers (DCs), which directly predict the the pos-class probabilities p(Y terior class probabilities are indirectly inferred at test time by Bayes’ rule, cf. Fig. 1: p(Y
X) =
|
Y )]. Because DCs opti-p(X p(Y ) [p(X
| mize prediction performance directly, they achieve better results in this respect. However, their models for p(Y
X) tend to be most accurate near decision
| boundaries (where it matters), but deteriorate away from them (where deviations incur no noticeable loss). Consequently, they are poorly calibrated (Guo et al., 2017) and out-of-distribution data can not be easily recognized at test time (Ovadia et al., 2019).
In contrast, GCs model full likelihoods p(X
Y ) and
| thus implicitly full posteriors p(Y
X), which leads
| to the opposite behavior – better predictive uncer-tainty at the price of reduced accuracy. Fig. 2 illus-trates the decision process in latent space Z.
X
INN
Z
Bayes Rule
Y
INPUT
GMM
CLASS
Figure 1: The Information Bottleneck Invert-ible Neural Network (IB-INN) as a genera-tive classiﬁer.
× conﬁdent class 2 but out-of-distribution
µ1
× conﬁdent class 1
× uncertain class
µ2
Figure 2: Illustration of the latent output space of a generative classiﬁer. The two class are parameterized likelihoods for Y = 1, 2
{ by their means µ in Z. The dotted line 1,2
}
{ represents the decision boundary. A conﬁ-dent, an uncertain, and an out-of-distribution sample are illustrated.
}
In the past, deep learning models trained in a purely generative way, particularly ﬂow-based mod-els trained with maximum likelihood, achieved highly unsatisfactory accuracy, so that some recent work has called into question the overall ef-fectiveness of GCs (Fetaya et al., 2019; Nalisnick et al., 2019b). In-depth studies of idealized set-tings (Bishop & Lasserre, 2007; Bishop, 2007) revealed the existence of a trade-off, controlling the balance between discriminative and generative performance. In this work, we ﬁnd that the IB can represent this trade-off, when applied to generative likelihood models.
To summarize our contributions, we combine two concepts – the Information Bottleneck (IB) objec-tive and Invertible Neural Networks (INNs). Firstly, we derive an asymptotically exact formulation of the IB for this setting, resulting in our IB-INN model, a special type of conditional normalizing
ﬂow. Secondly, we show that this model is especially suitable for the use as a GC: the trade-off parameter β in the IB-INN’s loss smoothly interpolates between the advantages of GCs (accurate posterior calibration and outlier detection), and those of DCs (superior task performance). Empiri-cally, at the right setting for β, our model only suffers a minor degradation in classiﬁcation accuracy compared to DCs while exhibiting more accurate uncertainty quantiﬁcation than pure DCs or GCs. 2