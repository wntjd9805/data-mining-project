Abstract
An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks. 1

Introduction
Optimizing a linear multi-layer neural network through gradient descent leads to a low-rank solution.
This phenomenon is known as implicit regularization and has been extensively studied under the context of matrix factorization [9, 1, 21], linear regression [24, 6], logistic regression [25], and linear convolutional neural networks [8]. The main goal of these prior works were to understand the generalization ability of deep neural networks. By contrast, the goal of the present work is to design an architecture that takes advantage of this phenomenon to improve the quality of learned representations.
Learning good representations remains a core issue in AI [2]. Representations learned in a self-supervised (or unsupervised) manner can be used for downstream tasks such as generation and classiﬁcation. Autoencoders (AE) are a popular class of method for learning representations without requiring labeled data. The internal representation of an AE must have a limited information capacity to prevent the AE from learning a trivial identity function. Variants of AEs differ by how they perform this limitation. Bottleneck AE (sometimes called "Diabolo networks") simply use low-dimensional codes [23], noisy AE, such as variational AE add noise to the codes while limiting the variance of their distribution [4, 14], quantizing AE (such as VQ-VAE) quantize the codes into discrete clusters [27], sparse AE impose a sparsity penalty on the code [19, 20], contracting and saturating AE minimize the curvature of the network function in directions outside the data manifold [22, 7], and denoising
AE are trained to produce large reconstruction error for corrupted samples [28].
In this work, we propose a new method to implicitly minimize the rank/dimensionality of the latent code of an autoencoder. We call this model Implicit Rank-Minimizing Autoencoder (IRMAE). This method consists in inserting extra linear layers between the encoder and the decoder of a standard autoencoder. This additional linear network is trained jointly with the rest of the autoencoder through classical backpropagation. As a result, the system spontaneously learns representations with a low effective dimensionality. Like other regularization methods, this extra linear neural network does not appear at inference time as the linear matrices collapse into one. Thus, the encoder and the decoder architecture of the model is identical to the original model. In practice, we fold the collapsed linear matrices into the last layer of the encoder at inference time. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We empirically demonstrate IRMAE’s regularization behavior through a synthetic dataset and show that it learns good representation with a much smaller latent dimension. Then we demonstrate superior representation learning performance of our method against a standard deterministic autoencoder and comparable performance to a variational autoencoder on MNIST dataset and CelebA dataset through a variety of generative tasks, including interpolation, sample generation from noise, PCA interpolation in low dimension, and a downstream classiﬁcation task. We also conducted an ablation study to verify that the advantage of implicit regularization comes from gradient descent learning dynamics.
We summarize our contributions as follows:
•
•
•
We proposed a method of inserting extra linear layers in deep neural networks for rank regularization;
We proposed a simple, deterministic rank-minimization autoencoder that learns low-dimensional representation;
We demonstrated a superior performance of our method compared to a standard deterministic autoencoder and a variational autoencoder on a variety of generative and downstream classiﬁcation tasks. 2