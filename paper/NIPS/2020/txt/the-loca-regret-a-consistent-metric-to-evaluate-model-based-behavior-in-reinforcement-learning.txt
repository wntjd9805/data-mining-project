Abstract
Deep model-based Reinforcement Learning (RL) has the potential to substantially improve the sample-efﬁciency of deep RL. While various challenges have long held it back, a number of papers have recently come out reporting success with deep model-based methods. This is a great development, but the lack of a consistent met-ric to evaluate such methods makes it difﬁcult to compare various approaches. For example, the common single-task sample-efﬁciency metric conﬂates improvements due to model-based learning with various other aspects, such as representation learning, making it difﬁcult to assess true progress on model-based RL. To address this, we introduce an experimental setup to evaluate model-based behavior of RL methods, inspired by work from neuroscience on detecting model-based behavior in humans and animals. Our metric based on this setup, the Local Change Adaptation (LoCA) regret, measures how quickly an RL method adapts to a local change in the environment. Our metric can identify model-based behavior, even if the method uses a poor representation and provides insight in how close a method’s behavior is from optimal model-based behavior. We use our setup to evaluate the model-based behavior of MuZero on a variation of the classic Mountain Car task. 1

Introduction
Deep reinforcement learning (RL) has seen great success [13, 17, 6, 2], but it’s no coincidence that these successes are almost exclusively on simulated environments—where samples are cheap—as contemporary deep RL methods have notoriously poor sample complexity. Deep model-based RL is a promising direction to substantially improve sample-efﬁciency. With model-based RL, an estimate of the transition dynamics and reward function is formed and planning techniques are employed to derive a policy from these estimates. For long, this approach did not combine well with function approximation, especially deep neural networks, due to fundamental problems such as compounding errors when predicting multiple steps into the future [20]. Recently, however, a number of papers have come out reporting success with deep model-based RL [15, 7–9, 4], including state-of-the-art performance on the Atari benchmark [16].
These recent successes bring to the forefront some interesting research questions, such as, "What strategies are employed to mitigate the compounding error issue?", "What are the relative strengths and weaknesses of the various deep model-based methods?", and, last but not least, "How much room is there for further improvement?". Addressing such questions requires a clear notion of the target of model-based learning and a metric to measure progress along this target.
Currently, a common metric to evaluate model-based methods is single-task sample efﬁciency.
However, this is a poor metric to measure progress on model-based learning for a number of reasons.
First, it conﬂates improvements due to model-based learning with various other aspects, such as generalization and exploration. Second, it provides little insight into the relative performance 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
compared to an ideal model-based method. And last but not least, single-task sample-efﬁciency is arguably not the problem setting that best shows the relevance of model-based learning. The true sample-efﬁciency beneﬁts manifest themselves when an agent can re-use its model for multiple tasks.
Inspired by work from neuroscience for detecting model-based behavior in humans and animals [5], we deﬁne a problem setup to identify model-based behavior in RL algorithms. Our setup is build around two tasks that differ from each other only in a small part of the state-space, but have very different optimal policies. Our Local Change Adaptation (LoCA) regret measures how quickly a method adapts its policy after the environment is changed from the ﬁrst task to the second. Our setup is designed such that it can be combined with tasks of various complexity and can be used to evaluate any RL algorithm, regardless of what the method does internally. The LoCA regret can uniquely identify model-based behavior even when a method uses a poor representation or uses a poor choice of hyper-parameters that cause it to learn slowly. In addition, the LoCA regret gives a quantitative estimate of how close a method is to ideal model-based behavior. 2 Notation and