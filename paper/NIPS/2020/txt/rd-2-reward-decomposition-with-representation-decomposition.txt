Abstract
Reward decomposition, which aims to decompose the full reward into multiple sub-rewards, has been proven beneﬁcial for improving sample efﬁciency in re-inforcement learning. Existing works on discovering reward decomposition are mostly policy dependent, which constrains diversiﬁed or disentangled behavior be-tween different policies induced by different sub-rewards. In this work, we propose a set of novel policy-independent reward decomposition principles by constraining uniqueness and compactness of different state representations relevant to different sub-rewards. Our principles encourage sub-rewards with minimal relevant features, while maintaining the uniqueness of each sub-reward. We derive a deep learning algorithm based on our principle, and refer to our method as RD2, since we learn reward decomposition and disentangled representation jointly. RD2 is evaluated on a toy case, where we have the true reward structure, and chosen Atari environments where the reward structure exists but is unknown to the agent to demonstrate the effectiveness of RD2 against existing reward decomposition methods. 1

Introduction
Since deep Q-learning was proposed by Mnih et al. [2015], reinforcement learning (RL) has achieved great success in decision making problems. While general RL algorithms have been extensively studied, here we focus on those RL tasks with multiple reward channels. In those tasks, we are aware of the existence of multiple reward channels, but only have access to the full reward. Reward decomposition has been proposed for such tasks to decompose the reward into sub-rewards, which can be used to train RL agent with improved sample efﬁciency.
Existing works mostly perform reward decomposition by constraining the behavior of different policies induced by different sub-rewards. Grimm and Singh [2019] propose encouraging each policy to obtain only its corresponding sub-rewards. However, their work requires that the environment be reset to arbitrary state and cannot be applied to general RL settings. Lin et al. [2019] propose encouraging the diversiﬁed behavior between such policies, but their method only obtains sub-rewards on transition data generated by their own policy, therefore it cannot decompose rewards for arbitrary state-action pairs.
In this paper, we propose a set of novel principles for reward decomposition by exploring the relation between sub-rewards and their relevant features. We demonstrate our principles based on a toy
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
environment Monster-Treasure, in which the agent receives a negative reward rmonster when it runs into the wandering monster, and receives a positive reward rtreasure when it runs into the treasure chest. A good decomposition would be to split the reward r into rmonster and rtreasure, where only some features are relevant to each sub-reward. To be speciﬁc, only the monster and the agent are relevant to predicting rmonster. A bad decomposition could be splitting the reward into r 2 , or r and 0. The ﬁrst one is not compact, in the sense that all features are relevant to both sub-rewards. The latter one is trivial, in the sense that none of the features is relevant to the 0 sub-reward. We argue that if each of the sub-reward we use to train our agent is relevant to limited but unique features only, then the representation of sub-returns induced by sub-rewards would also be compact and easy to learn. 2 and r
Motivated by the example above, we propose decomposing a reward into sub-rewards by constraining the relevant features/representations of different sub-rewards to be compact and non-trivial. We ﬁrst derive our principles for reward decomposition under the factored Markov Decision Process(fMDP).
Then we relax and integrate the above principles into deep learning settings, which leads to our algorithm, Reward Decomposition with Representation Disentanglement(RD2). Compared with existing works, RD2 can decompose reward for arbitrary state-action pairs under general RL settings and does not rely on policies.
It is also associated with a disentangled representation so that the reward decomposition is self-explanatory and can be easily visualized. We demonstrate our reward decomposition algorithm on the Monster-Treasure environment discussed earlier, and test our algorithm on chosen Atari Games with multiple reward channels. Empirically, RD2 achieves the following:
• It discovers meaningful reward decomposition and disentangled representation.
• It achieves better performance than existing reward decomposition methods in terms of improving sample efﬁciency for deep RL algorithms. 2