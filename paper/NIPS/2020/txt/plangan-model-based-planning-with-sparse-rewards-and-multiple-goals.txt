Abstract
Learning with sparse rewards remains a signiﬁcant challenge in reinforcement learning (RL), especially when the aim is to train a policy capable of achieving multiple different goals. To date, the most successful approaches for dealing with multi-goal, sparse reward environments have been model-free RL algorithms. In this work we propose PlanGAN, a model-based algorithm speciﬁcally designed for solving multi-goal tasks in environments with sparse rewards. Our method builds on the fact that any trajectory of experience collected by an agent contains useful information about how to achieve the goals observed during that trajectory. We use this to train an ensemble of conditional generative models (GANs) to generate plausible trajectories that lead the agent from its current state towards a speciﬁed goal. We then combine these imagined trajectories into a novel planning algorithm in order to achieve the desired goal as efﬁciently as possible. The performance of
PlanGAN has been tested on a number of robotic navigation/manipulation tasks in comparison with a range of model-free reinforcement learning baselines, including
Hindsight Experience Replay. Our studies indicate that PlanGAN can achieve comparable performance whilst being around 4-8 times more sample efﬁcient. 1

Introduction
One of the primary appeals of reinforcement learning (RL) is that it provides a framework for the autonomous learning of complex behaviours without the need for human supervision. In recent years RL has had signiﬁcant success in areas such as playing video games [1, 2], board games [3, 4] and robotic control tasks [5, 6, 7]. Despite this, progress in applying RL to more practically useful environments has been somewhat limited. One of the main problems is that RL algorithms generally require a well-shaped, dense reward function in order to make learning progress. Often a reward function that fully captures the desired behaviour of an agent is not readily available and has to be engineered manually for each task, requiring a lot of time and domain-speciﬁc knowledge. This defeats the point of designing an agent that is capable of learning autonomously. A more general approach is to learn with sparse rewards, where an agent only receives a reward once a task has been completed. This is much easier to specify and is applicable to a wide range of problems, however training becomes signiﬁcantly more challenging since the agent only receives infrequent feedback at the end of every rollout. This becomes especially challenging in the case of goal-conditioned
RL [8, 9], where the aim is to train a policy that can achieve a variety of different goals within the environment.
Much of RL’s success has come with model-free approaches, where the policy is learned directly from the reward signal obtained by interacting with the environment. However recently there has been a lot of interest in applying model-based approaches to the same kind of problems [7, 10, 11]. One 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of the main drawbacks of model-free RL algorithms is that they tend to be very sample inefﬁcient, requiring a huge number of interactions with the environment in order to make learning progress.
On the other hand, model-based methods make use of a learned model to plan their actions without directly interacting with the environment. Learning a model allows these methods to make use of a lot more information that is present in the observed transitions than just the scalar reward signal, and so generally this leads to a signiﬁcant improvement in sample efﬁciency. This efﬁciency can sometimes come at the cost of worse asymptotic performance due to errors in the model introducing a bias towards non-optimal actions, although current state of the art approaches [7, 10] are able to achieve comparable performance to some of the best model-free approaches [12, 13]. However, as with most RL algorithms, model-based approaches generally need a dense reward signal to work well. We are not aware of a model-based approach speciﬁcally designed to work in the sparse-reward, multi-goal setting.
To date, the most successful general-purpose RL algorithm for dealing with sparse rewards and multiple goals is Hindsight Experience Replay (HER) [8], a model-free algorithm. HER works by taking advantage of the fact that, when learning a goal-conditioned policy with an off-policy RL algorithm, observed transitions from a trajectory can be re-used as examples for attempting to achieve any goal. In particular, by re-labelling transitions with goals achieved at a later point during the same trajectory HER trains the goal-conditioned policy on examples that actually led to success — hence obtaining a much stronger learning signal.
In this paper we present PlanGAN, a model-based algorithm that can naturally be applied to sparse-reward environments with multiple goals. The core of our method builds upon the same principle that underlies HER — namely that any goal observed during a given trajectory can be used as an example of how to achieve that goal from states that occurred earlier on in that same trajectory. However, unlike HER, we do not directly learn a goal-conditioned policy/value function but rather train an ensemble of Generative Adversarial Networks (GANs) [14] which learn to generate plausible future trajectories conditioned on achieving a particular goal. We combine these imagined trajectories into a novel planning algorithm that can reach those goals in an efﬁcient manner.
We test PlanGAN on a number of robotic manipulation and navigation tasks and show that it can achieve similar levels of performance to leading model-free methods (including Hindsight Experience
Replay) but with substantially improved sample efﬁciency. The primary contribution of this paper is to introduce the ﬁrst model-based method which is explicitly designed for multi-goal, sparse reward environments, leading to a signiﬁcant improvement in sample efﬁciency. 2