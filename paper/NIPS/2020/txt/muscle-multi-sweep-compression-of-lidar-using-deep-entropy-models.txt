Abstract
We present a novel compression algorithm for reducing the storage of LiDAR sensor data streams. Our model exploits spatio-temporal relationships across multiple LiDAR sweeps to reduce the bitrate of both geometry and intensity values.
Towards this goal, we propose a novel conditional entropy model that models the probabilities of the octree symbols by considering both coarse level geometry and previous sweeps’ geometric and intensity information. We then use the learned probability to encode the full data stream into a compact one. Our experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduction of 7–17% and 6–19% on the UrbanCity and SemanticKITTI datasets respectively. 1

Introduction
The past decade has witnessed numerous innovations in intelligent systems, thanks to an explosion of progress in sensing and AI algorithms. In particular, LiDAR sensors are extensively used in various applications such as indoor rovers, unmanned aerial vehicles, and self-driving cars to accurately capture the 3D geometry of the scene. Yet the rapid adoption of LiDAR has brought about a key challenge—dealing with the mounting storage costs associated with the massive inﬂux of LiDAR data. For instance, a 64-line Velodyne LiDAR continuously scanning a given scene produces over 3 billion points in a single hour. Hence, developing efﬁcient and effective compression algorithms to store such 3D point cloud data streams is crucial to reduce the storage and communication bandwidth.
Unlike its well-studied image and video counterparts, point cloud stream compression is a challenging yet under-explored problem. Many prior approaches have focused on encoding a point cloud stream as independent sweeps, where each sweep captures a rough 360-degree rotation of the sensor. Early approaches exploit a variety of compact data structures to represent the point cloud in a memory-efﬁcient manner, such as octrees [1], KD-trees [2], and spherical images [3]. More recent works along this direction utilize powerful machine learning models to encode redundant geometric correlations within these data structures for better compression [4, 5, 6]. In general, most of these aforementioned approaches do not make effective use of temporal correlations within point clouds. Moreover, these prior approaches have largely focused on compressing the geometric structure of the point cloud (the spatial coordinates); yet there has been little attention paid towards compression of other attributes, e.g. LiDAR intensity, which are crucial for many downstream tasks. Compressing such attributes along with geometric structure can make a signiﬁcant impact on reducing storage.
In this paper, we present a novel, learning-based compression algorithm that comprehensively reduces the storage of LiDAR sensor data streams. Our method extends the recent success of octree-structured deep entropy models [6] for single LiDAR sweep compression to intensity-valued LiDAR streaming data. Speciﬁcally, we propose a novel deep conditional entropy model that models the probabilities of the octree symbols and associated intensity values by exploiting spatio-temporal correlations within the data: taking both coarse level information at the current sweep, as well as relevant neighboring 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Comprehensive overview of our method. Our point cloud stream is serialized into an octree repre-sentation (Sec 2.1). We apply a spatio-temporal entropy model to the octree occupancy bytestream (Sec. 2.3), modeling ancestral dependence, prior octree dependence, and octree alignment. We also apply a deep entropy model to model the intensity stream (Sec. 2.4). nodes information from the previous sweep. Unlike prior approaches, our method models the joint entropy across an entire point cloud sequence, while unifying geometry and attribute compression into the same framework.
We validate the performance of our approach on two large datasets, namely UrbanCity [7] and
SemanticKITTI [8]. The experiments demonstrate that our method signiﬁcantly reduces the joint geometry and intensity bitrate over prior state-of-the-art LiDAR compression methods, with a reduc-tion of 7–17% on UrbanCity and 6–19% on SemanticKITTI. We also conduct extensive experiments showcasing superior performance against prior works on numerous downstream perception tasks. 2 Multi-Sweep LiDAR Compression
In this work, we propose a comprehensive framework for the lossy compression of LiDAR point cloud streams, by exploiting the spatio-temporal redundancies through a learned entropy model. We aim to maximize the reconstruction quality of these point clouds while reducing their joint bitrate. Every point in a LiDAR point cloud contains both a spatial 3D location (x, y, z), as well as an intensity value r, and we jointly compress both.
Our method is shown in Fig. 1. We ﬁrst quantize and encode all point spatial coordinates in the stream into an octree representation, where leaves represent the quantized points and intermediate nodes contain 8-bit symbols representing child occupancies (Sec. 2.1). We then present a novel deep entropy model (Sec. 2.2): a probability model that utilizes spatio-temporal context to predict occupancy symbols for each node (Sec. 2.3), as well as intensity values for each point for intensity compression (Sec. 2.4). The outputs of these entropy models are ﬁnally fed into a lossless entropy coding algorithm, such as range coding, to produce the ﬁnal bitstream (Sec. 2.5). 2.1 Octree Representation
Octree Structure and Bit Representation: LiDAR point clouds are intrinsically challenging to process due to their sparsity and inherently unstructured nature. A tool to counteract these challenges is to use a tree-based data structure, such as an octree or KD-tree, to efﬁciently partition the space.
Inspired by [1, 6], we quantize and represent every point cloud in our stream as an octree with an associated depth value D, corresponding to the quantized precision of the point cloud.
Speciﬁcally, an octree can be constructed from a 3D point cloud by ﬁrst partitioning the spatial region into 8 octants, and recursively partitioning each octant until each node contains at most one point, or until D is reached. The resulting octree contains both intermediate nodes and leaf nodes. Each intermediate node can be represented by an 8-bit occupancy symbol x, representing the occupancies of its children; each node also has an implied spatial position. Each leaf node contains one point of the point cloud, and stores the offset between the point and its corner position, as well as the point intensity. We determine the intensity value of each point in the quantized point cloud by taking that of its nearest neighbor in the original point cloud. The number of bits allocated to each leaf node is level-dependent; an octree with D = k will store k − i bits for a leaf node at level i, i ≤ k. Hence, 2
the octree is memory-efﬁcient—shared bits are encoded with intermediate nodes and residual bits with leaves.
Serialization: We serialize the octree into two (uncompressed) bytestreams by traversing the octree in breadth-ﬁrst order. The ﬁrst bytestream contains the intermediate node occupancy symbols in breadth-ﬁrst order, and the second bytestream contains the leaf node offsets/intensities encountered during traversal. Our entropy model focuses primarily on the node occupancies/intensities—we demonstrate in our supplementary materials that leaf offsets do not contain meaningful patterns we can exploit. Hence for subsequent sections we denote P (t) = (X (t), R(t)), where X (t) = {x(t) 1 , ..., x(t) mt} is the set of occupancy symbols, and R(t) = {r(t) nt } is the set of intensities. The serialization is lossless; the only loss comes from D-dependent octree quantization. This gives a guarantee on reconstruction quality and allows compression efforts to solely focus on bitrate reduction. 1 , ..., r(t) 2.2 Octree-Based Conditional Entropy Module
The octree sequence is now fed into our entropy model. Our entropy model is a probability model that approximates the unknown joint distribution of point clouds pdata with our own distribution p(·; w).
Since we convert our point clouds to octree representations, the probability model is equivalent to modeling p(P (1), ..., P (n); w). According to the classic Shannon’s source coding theorem [9], the expected bitrate for the point cloud stream is tightly approximated by the cross-entropy between the real point cloud stream distribution and our parametrized model: Epdata[− log p(P (1), ..., P (n); w)].
We then assume that the joint probability factorizes as follows: log p(P (1), ..., P (n); w) = (cid:88) log p(P (t)|P (t−1); w) t (cid:88)
{log p(X (t)|P (t−1); w) + log p(R(t)|X (t), P (t−1); w)}
= t (1) (2)
We make a 1st-order Markov assumption: a given octree P (t) only depends on the sweep preced-ing it, P (t−1). We then factor the octree into two entropy models: the node occupancy model p(X (t)|P (t−1); w), and the intensity model p(R(t)|X (t), P (t−1); w) conditioned on occupancies.
The dependence only on past sweeps makes the model applicable to an online LiDAR stream setting. 2.3 Occupancy Entropy Model
We obtain our node occupancy model by continuing to factorize the occupancy probabilities: p(X (t)|P (t−1); w) = p(x(t) i
|X (t) ans(i), P (t−1); w) (cid:89) i (3) pa(i), x(t) ans(i) = {x(t) pa(pa(i)), ..., x(t) pa(...(pa(i)))} represents the set of ancestor nodes of x(t)
Here, X (t) and
P (t−1) represents the point cloud from previous sweep. As seen above, we simplify the autoregressive dependency on ancestors nodes on the octree for the given timestamp, as well as all the nodes at the previous timestamp. We model p(·|X (t) ans(i), P (t−1); w) with a deep neural network. The architecture has two backbones, namely the ancestral node dependence module which encodes recurrent dependencies on the ancestor nodes X (t) ans(i) from the current sweep’s octree as well as a prior octree dependence module which models information passed from the previous sweep. Fig. 1 depicts the architecture of such network. i
Ancestral Node Dependence: Our ancestral node dependence module is a recurrent network deﬁned over an ancestral, top-down octree path. Inspired by [6], we feed a context feature ci for every node xi through a multi-layer perceptron (MLP) to extract an initial hidden embed-ding h(t) i,0 = σ0(ci; w), where σ0(·; w) denotes a MLP with learnable parameter w. Context 3
features include the current octree level, octant spatial location, and parent occupancy; they are known beforehand per node xi and computed to facilitate representation learning. We then per-form Kans rounds of aggregation between every node’s embedding and its parental embedding: h(t) i,k = σk([h(t) i,k−1, h(t) pa(i),k−1]; w). As shorthand, we denote this entire tree-structured recurrent i = fans(x(t) backbone branch as h(t)
, X (t) ans(i)). i
Temporal Octree Dependence: We also incorporate the previous octree P (t−1) into the current entropy model at time t through a temporal octree dependence module. We thus ﬁrst align the previous octree into the sensor coordinate frame of the current octree. Unlike the current octree where we only have access to parental information, we can construct features that make use of all information within the previous octree, containing both top-down ancestral information as well as bottom-up child information. We exploit this fact by designing a two-stream feature backbone to compute embeddings for every octree node at time t − 1, inspired by tree-structured message passing algorithms [10, 11]. The forward pass stream is the same as the ancestral dependence module above, generating top-down features from ancestors: h(t−1) ans(j) ). After the top-down j pass, we design a bottom-up aggregation pass, a recurrent network that produces aggregated features from descendants to the current node. Unlike the ancestral module in which each node only has one parent, the number of children per node can vary, and we desire that the output is invariant to the ordering of the children. Hence, we resolve this by designing the following function inspired by deep
+ (cid:80) sets [12]: g(t−1) c∈child(j) fagg,2(g(t−1)
)), which produces the ﬁnal aggregated embedding feature containing both top-down and bottom-up context.
= fagg,1(h(t−1)
= fans(x(t−1)
, X (t−1) c j j j j
Spatio-Temporal Aggregation: The ﬁnal step incorporates the set of aggregated features in the previous octree {g(t−1)
}, with ancestral features in the current octree {h(t) i } to help with occupancy prediction in the current octree. A key observation is that only a subset of spatially proximal nodes in the previous sweep can contribute to better prediction for a given node at time t; moreover, the relative location of each neighbor should deﬁne its relative importance. Inspired by this fact, we employ continuous convolutions [13] to process previous octree features at the current node. A continuous conv. layer aggregates features from neighboring points to a given node in the following manner: hi = (cid:80) j∈N (i) σ(pj − pi)hj where N (i) is the i-th node’s k-nearest neighboring nodes in 3D space from the (t − 1) sweep at the same level as i, pi is the 3D position of each node, and σ denotes a learned MLP. We use a separate MLP with a continuous conv. layer per octree level to process the
}j∈N (i) and produce an embedding feature g(t) aggregated features in the previous octree {g(t−1) i,st. j
Entropy Header: Finally, are aggre-gated through a ﬁnal MLP to output a 256-dimensional softmax of probability values p(x(t) ans(i), P (t−1); w), corresponding to the predicted 8-bit occupancy for node i, time t. i i,st and ancestral features h(t) the warped feature g(t)
|X (t) i 2.4
Intensity Entropy Model
The goal of the intensity entropy model is to compress extraneous intensities tied to each spatial point coordinate. We assume these intensities are bounded and discrete, so compression is lossless; if they are continuous, there will be a loss incurred through discretization. The model factorizes as follows: p(R(t)|X (t), P (t−1); w) = (cid:89) p(r(t) i i
|X (t), P (t−1); w) (4)
The intent of conditioning on the occupancies X (t) is not to directly use their values per se, but to emphasize that intensity decoding occurs after the point spatial coordinates have already been reconstructed in R3. Therefore, we can directly make use of the spatial position corresponding to each intensity R(t) in compression. We aim to leverage temporal correlations between point intensities i across consecutive timestamps to better model the entropy of r(t)
. Similar to node occupancy predic-i tion above, there is the challenge of how to incorporate previous intensity information when there are no direct correspondences between the two point clouds. We again employ continuous convolutions 4
Figure 2: Bitrate vs. reconstruction quality curves on UrbanCity (top) and KITTI (bottom). From left-to-right:
F1 with τgeo = 10cm and τint = 0 (↑), point-to-point chamfer distance (↓), point-to-plane PSNR (↑). to resolve this challenge. Let RN (i) be the set of nearest neighbor intensities {r(t−1)
}j∈N (i), where nearest neighbor is deﬁned by spatial proximity of previous point j to the current point i. We apply an MLP with a continuous conv. layer that takes the past intensities r(t−1) as input and outputs an embedding feature for each node i. This feature is then fed through a linear layer and softmax to output intensity probability values. In our setting we assume our intensity value is an 8-bit integer, so the resulting probability vector is 256-dimensional p(r(t) i
|X (t), P (t−1); w). j j 2.5 Entropy Coding Process
Encoding: We integrate our entropy model with an entropy coding algorithm (range coding [14]) to produced the ﬁnal compressed bitstream. During the encoding pass, for every octree in the stream, the entropy model is applied across the octree occupancy bytestream, as well as across the intensities per point, to predict the respective probability distributions. We note that encoding only requires one batch GPU pass per sweep for the occupancy and intensity models. The resulting distributions are then passed to range coding which compresses the occupancies and intensities into two bitstreams.
Decoding: The same entropy models are used during decoding. First, the occupancy entropy model is run, given the already decoded past octree, to produce distributions that recover the occupancy serialization and spatial coordinates of the current point cloud. Then, the intensity entropy model is run, given the already decoded intensities in the past point cloud, to produce distributions that recover the current point intensities. Note that our model is well-setup for parallel computation during decoding, for both the occupancies and intensities. As mentioned in Sec. 2.3, the dependence on ancestral nodes instead of all past nodes allows us to only run at most O(D) GPU passes for the occupancy model per sweep. Moreover, the assumed independence between intensities in the current sweep, given the past, allows us to only run 1 GPU pass per sweep for the intensity entropy model. 2.6 Learning
Both our occupancy and intensity entropy models are trained end-to-end with cross-entropy loss, for every node x(t)
, for every point cloud in a stream: and intensity r(t) i ∈ X (t) i i ∈ R(t) i (cid:34) (cid:96) = EP ∼pdata
− (cid:88) (cid:88) t i log p(x(t) i,gt|X (t) ans(i), P (t−1); w) − log p(r(t) (cid:35) i,gt|X (t), P (t−1); w) (cid:88) (cid:88) t i (5) 5
Oracle (UrbanCity): Bitrate 104 Ours: F1 92.4, Bitrate 9.3
Draco: F1 80.8, Bitrate 9.4
MPEG: F1 59.1, Bitrate 11.4
Oracle (UrbanCity): Bitrate 104 Ours: F1 99.2, Bitrate 13.7
Draco: F1 92.3, Bitrate 13.8
MPEG: F1 81.5, Bitrate 16.2
Oracle (KITTI): Bitrate 104
Ours: F1 90.2, Bitrate 5.6
Draco: F1 87.1, Bitrate 5.7
MPEG: F1 61.0, Bitrate 9.5
Oracle (KITTI): Bitrate 104
Ours: F1 98.6, Bitrate 10.1
Draco: F1 96.9, Bitrate 10.1
MPEG: F1 79.9, Bitrate 12.9
Figure 3: Qualitative results on UrbanCity and KITTI. Points are colored by intensity. i,gt and r(t)
Here, x(t) i,gt denote the ground-truth values of the node occupancies/intensities, respectively.
As mentioned above, minimizing cross-entropy loss is equivalent to our goal of reducing expected bitrate of the point cloud stream. 2.7 Discussion and