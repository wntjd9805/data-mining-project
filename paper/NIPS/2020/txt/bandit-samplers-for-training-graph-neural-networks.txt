Abstract
Several sampling algorithms with variance reduction have been proposed for ac-celerating the training of Graph Convolution Networks (GCNs). However, due to the intractable computation of optimal sampling distribution, these sampling algorithms are suboptimal for GCNs and are not applicable to more general graph neural networks (GNNs) where the message aggregator contains learned weights rather than ﬁxed weights, such as Graph Attention Networks (GAT). The funda-mental reason is that the embeddings of the neighbors or learned weights involved in the optimal sampling distribution are changing during the training and not known a priori, but only partially observed when sampled, thus making the derivation of an optimal variance reduced samplers non-trivial. In this paper, we formulate the optimization of the sampling variance as an adversary bandit problem, where the rewards are related to the node embeddings and learned weights, and can vary constantly. Thus a good sampler needs to acquire variance information about more neighbors (exploration) while at the same time optimizing the immediate sampling variance (exploit). We theoretically show that our algorithm asymptotically ap-proaches the optimal variance within a factor of 3. We show the efﬁciency and effectiveness of our approach on multiple datasets. 1

Introduction
Graph neural networks [13, 11] have emerged as a powerful tool for representation learning of graph data in irregular or non-euclidean domains [3, 21]. For instance, graph neural networks have demonstrated state-of-the-art performance on learning tasks such as node classiﬁcation, link and graph property prediction, with applications ranging from drug design [8], social networks [11], transaction networks [14], gene expression networks [9], and knowledge graphs [17].
One major challenge of training GNNs comes from the requirements of heavy ﬂoating point operations and large memory footprints, due to the recursive expansions over the neighborhoods. For a minibatch with a single vertex vi, to compute its embedding h(L) at the L-th layer, we have to expand its i
∗Equal contribution.
†Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
neighborhood from the (L − 1)-th layer to the 0-th layer, i.e. L-hops neighbors. That will soon cover a large portion of the graph if particularly the graph is dense. One basic idea of alleviating such
“neighbor explosion” problem was to sample neighbors in a top-down manner, i.e. sample neighbors in the l-th layer given the nodes in the (l + 1)-th layer recursively. (cid:80) ij = j (cid:107)2
αik(cid:107)h(l)
Several layer sampling approaches [11, 6, 12, 23] have been proposed to alleviate above “neighbor explosion” problem and improve the convergence of training GCNs, e.g. with importance sampling.
αij (cid:107)h(l)
However, the optimal sampler [12], q(cid:63) k (cid:107)2 for vertex vi, to minimize the variance of the estimator ˆh(l+1) j |vj ∈ Ni}, which is infeasible to be computed because we can only observe them partially while doing sampling. Existing approaches [6, 12, 23] typically compromise the optimal sampling distribution via approximations, which may impede the convergence. Moreover, such approaches are not applicable to more general cases where the weights or kernels αij’s are not known a priori, but are learned weights parameterized by attention functions [20]. That is, both the hidden embeddings and learned weights involved in the optimal sampler constantly vary during the training process, and only part of the unnormalized attention values or hidden embeddings can be observed while do sampling. involves all its neighbors’ hidden embeddings, i.e. {ˆh(l) k∈Ni i
Present work. We derive novel variance reduced samplers for training of GCNs and attentive GNNs with a fundamentally different perspective. That is, different with existing approaches that need to compute the immediate sampling distribution, we maintain nonparametric estimates of the sampler instead, and update the sampler towards optimal variance after we acquire partial knowledges about neighbors being sampled, as the algorithm iterates.
To fulﬁl this purpose, we formulate the optimization of the samplers as a bandit problem, where the regret is the gap between expected loss (negative reward) under current policy (sampler) and expected loss with optimal policy. We deﬁne the reward with respect to each action, i.e. the choice of a set of neighbors with sample size k, as the derivatives of the sampling variance, and show the variance of our samplers asymptotically approaches the optimal variance within a factor of 3. Under this problem formulation, we propose two bandit algorithms. The ﬁrst algorithm based on multi-armed bandit (MAB) chooses k < K arms (neighbors) repeatedly. Our second algorithm based on MAB with multiple plays chooses a combinatorial set of neighbors with size k only once.
To summarize, (1) We recast the sampler for GNNs as a bandit problem from a fundamentally different perspective. It works for GCNs and attentive GNNs while existing approaches apply only to GCNs. (2) We theoretically show that the regret with respect to the variance of our estimators asymptotically approximates the optimal sampler within a factor of 3 while no existing approaches optimize the sampler. (3) We empirically show that our approachs are way competitive in terms of convergence and sample variance, compared with state-of-the-art approaches on multiple public datasets. 2 Problem Setting
Let G = (V, E) denote the graph with N nodes vi ∈ V, and edges (vi, vj) ∈ E. Let the adjacency matrix denote as A ∈ RN ×N . Assuming the feature matrix H (0) ∈ RN ×D(0) denoting the
D(0)-dimensional feature of node vi. We focus on the following simple but general form of GNNs: with h(0) i h(l+1) i
= σ (cid:16) N (cid:88) j=1
α(vi, vj) h(l) j W (l)(cid:17)
, l = 0, . . . , L − 1 (1) where h(l) is the hidden embedding of node vi at the l-th layer, ααα = (α(vi, vj)) ∈ RN ×N is a kernel i or weight matrix, W (l) ∈ RD(l)×D(l+1) is the transform parameter on the l-th layer, and σ(·) is the activation function. The weight α(vi, vj), or αij for simplicity, is non-zero only if vj is in the 1-hop neighborhood Ni of vi. It varies with the aggregation functions [3, 21]. For example, (1)
GCNs [8, 13] deﬁne ﬁxed weights as ααα = ˜D−1 ˜A or ααα = ˜D− 1 2 respectively, where ˜A = A+I, and ˜D is the diagonal node degree matrix of ˜A. (2) The attentive GNNs [20, 15] deﬁne a learned weight α(vi, vj) by attention functions: α(vi, vj) =
˜α(vi,vk;θ) , where the unnormalized attentions ˜α(vi, vj; θ) = exp(ReLU(aT [W hi(cid:107)W hj])), are parameterized by θ = {a, W }. Different 2 ˜A ˜D− 1
˜α(vi,vj ;θ) vk ∈Ni (cid:80) 2
from GCNs, the learned weights αij ∝ ˜αij can be evaluated only given all the unnormalized weights in the neighborhood.
The basic idea of layer sampling approaches [11, 6, 12, 23] was to recast the evaluation of Eq. (1) as
ˆh(l+1) i
= σ (cid:16)
N (i) Epij (cid:105) (cid:104)ˆh(l) j
W (l)(cid:17)
, (2) where pij ∝ αij, and N (i) = (cid:80) j αij. Hence we can evaluate each node vi at the (l + 1)-th layer, using a Monte Carlo estimator with sampled neighbors at the l-th layer. Without loss of generality, we assume pij = αij and N (i) = 1 that meet the setting of attentive GNNs in the rest of this paper.
To further reduce the variance, let us consider the following importance sampling
ˆh(l+1) i
= σW (l) (cid:0)ˆµ(l) i (cid:1) = σW (l) (cid:16)
Eqij (cid:20) αij qij (cid:21) (cid:17)
,
ˆh(l) j (3) where we use σW (l)(·) to include transform parameter W (l) into the function σ(·) for conciseness. As such, one can ﬁnd an alternative sampling distribution qi = (qij1, ..., qij|Ni|) to reduce the variance of an estimator, e.g. a Monte Carlo estimator ˆµ(l) i = 1 k
Take expectation over qi, we deﬁne the variance of ˆµ(l) at step t and (l + 1)-th layer to be:
, where js ∼ qi.
ˆh(l) js (cid:80)k s=1
αijs qijs
ˆh(l) js i = αijs qijs
Vt(qi) = E (cid:104)(cid:13) (cid:13)ˆµ(l) (cid:13) i (t) − µ(l) i (t) 2(cid:105) (cid:13) (cid:13) (cid:13)
= E (cid:104)(cid:13) (cid:13) (cid:13)
αijs(t) qijs h(l) js (t) − (cid:88) j∈Ni
αij(t)h(l) 2(cid:105) (cid:13) (cid:13) j (t) (cid:13)
. (4)
Note that αij and h(vj) that are inferred during training may vary over steps t’s. We will explicitly include step t and layer l only when it is necessary. By expanding Eq. (4) one can write V(qi) as the difference of two terms. The ﬁrst is a function of qi, which we refer to as the effective variance:
Ve(qi) = (cid:88) j∈Ni 1 qij ij (cid:107)hj(cid:107)2 ,
α2 (5) while the second does not depend on qi, and we denote it by Vc = sampling distribution [6, 12] at (l + 1)-th layer for vertex i that minimizes the variance is:
αijhj j∈Ni
. The optimal (cid:80) (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) q(cid:63) ij =
αij(cid:107)h(l) j (cid:107)2
αik(cid:107)h(l) k (cid:107)2 (cid:80) k∈Ni
. (6)
However, evaluating this sampling distribution is infeasible because we cannot have all the knowledges of neighbors’ embeddings in the denominator of Eq. (6). Moreover, the αij’s in attentive GNNs could also vary during the training procedure. Existing layer sampling approaches based on importance sampling just ignore the effects of norm of embeddings and assume the αij’s are ﬁxed during training.
As a result, the sampling distribution is suboptimal and only applicable to GCNs where the weights are ﬁxed. Note that our derivation above follows the setting of node-wise sampling approaches [11], but the claim remains to hold for layer-wise sampling approaches [6, 12, 23]. 3