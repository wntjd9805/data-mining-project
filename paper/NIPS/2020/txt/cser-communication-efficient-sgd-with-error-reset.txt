Abstract
The scalability of Distributed Stochastic Gradient Descent (SGD) is today limited by communication bottlenecks. We propose a novel SGD variant: Communication-efﬁcient SGD with Error Reset, or CSER. The key idea in CSER is ﬁrst a new technique called “error reset” that adapts arbitrary compressors for SGD, produc-ing bifurcated local models with periodic reset of resulting local residual errors.
Second we introduce partial synchronization for both the gradients and the models, leveraging advantages from them. We prove the convergence of CSER for smooth non-convex problems. Empirical results show that when combined with highly aggressive compressors, the CSER algorithms accelerate the distributed training by nearly 10× for CIFAR-100, and by 4.5× for ImageNet. 1

Introduction
In recent years, the sizes of both machine-learning models and datasets have been increasing rapidly.
To accelerate the training, it is common to distribute the computation on multiple machines. We focus on Stochastic Gradient Descent (SGD). SGD and its variants are commonly used for training large-scale deep neural networks. A common way to distribute SGD is to synchronously compute the gradients at multiple worker nodes, and then aggregate the global average. This is akin to single-threaded SGD with large mini-batch sizes [5, 27–29]. Increasing the number of workers is attractive because it holds the potential to reduce training time. However, more workers also means more communication, and overwhelmed communication links hurt scalability.
The state-of-the-art work in communication-efﬁcient SGD is called QSparse-local-SGD [3], which combines two prevailing techniques: message compression and infrequent synchronization. Message compression methods use compressors such as quantization [2, 4, 9, 17, 21, 26, 33] and sparsiﬁca-tion [1, 8, 20] to reduce the number of bits in each synchronization round. This necessitates error feedback (EF-SGD) [9, 33] to correct for the residual errors incurred by the compressors, and to guarantee theoretical convergence. On the other hand, infrequent synchronization methods such as local SGD [13, 19, 25, 30, 31] would decrease the overall number of synchronization rounds. The former, QSparse-local-SGD, periodically synchronizes the model parameters like local SGD, and compresses the synchronization messages to further reduce the communication overhead. Similar to
EF-SGD, it also uses error feedback to correct for the residual errors of compression.
∗The work was done when Cong Xie was a (part-time) intern in Amazon Web Services. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
QSparse-local-SGD reduces more bidirectional communication overhead (in both aggregation and broadcasting) than its ancestors EF-SGD and local SGD. However, it also inherits weaknesses from both ancestor algorithms, especially when compression ratios are increased. For instance, our experiments reveal that QSparse-local-SGD fails to converge at a compression ratio of 256×.
In this paper, we introduce a new algorithm called Communication-efﬁcient SGD with Error Reset, or CSER. The key idea in CSER is a new technique called error reset that corrects for the local model using the compression errors, and we show this converges better than the error feedback technique used in QSparse-local-SGD. On top of the error reset, we also introduce partial synchronization, leveraging advantages from both gradient and model synchronizations. These two techniques together allow the proposed method to scale up the compression ratio to as high as 1024× and signiﬁcantly outperform the existing approaches.
The main contributions of our paper are as follows:
• We propose a novel communication-efﬁcient SGD algorithm, called Communication-efﬁcient SGD with Error Reset (CSER) as well as its variant with Nesterov’s momentum [15]. CSER includes a new technique that adapts arbitrary compressors for SGD, and achieves better convergence than the baselines when aggressive compressors are used.
• We add a second compressor to partially synchronize the gradients between the resets of errors on local models. We show that tuning the compression ratios between the gradient synchronization and model synchronization improves the convergence.
• We show empirically that with appropriate compression ratios, CSER accelerates distributed training by nearly 10× for CIFAR-100, and by 4.5× for ImageNet. 2