Abstract
The idea of federated learning is to collaboratively train a neural network on a server. Each user receives the current weights of the network and in turns sends parameter updates (gradients) based on local data. This protocol has been designed not only to train neural networks data-efﬁciently, but also to provide privacy beneﬁts for users, as their input data remains on device and only parameter gradients are shared. But how secure is sharing parameter gradients? Previous attacks have provided a false sense of security, by succeeding only in contrived settings - even for a single image. However, by exploiting a magnitude-invariant loss along with optimization strategies based on adversarial attacks, we show that is is actually possible to faithfully reconstruct images at high resolution from the knowledge of their parameter gradients, and demonstrate that such a break of privacy is possible even for trained deep networks. We analyze the effects of architecture as well as parameters on the difﬁculty of reconstructing an input image and prove that any input to a fully connected layer can be reconstructed analytically independent of the remaining architecture. Finally we discuss settings encountered in practice and show that even aggregating gradients over several iterations or several images does not guarantee the user’s privacy in federated learning applications. 1

Introduction
Federated or collaborative learning [6, 28] is a distributed learning paradigm that has recently gained signiﬁcant attention as both data requirements and privacy concerns in machine learning continue to rise [21, 14, 32]. The basic idea is to train a machine learning model, for example a neural network, by optimizing the parameters θ of the network using a loss function L and exemplary training data consisting of input images xi and corresponding labels yi in order to solve min
θ
N (cid:88) i=1
Lθ(xi, yi). (1)
We consider a distributed setting in which a server wants to solve (1) with the help of multiple users that own training data (xi, yi). The idea of federated learning is to only share the gradients
∇θLθ(xi, yi) instead of the original data (xi, yi) with the server which it subsequently accumulates to
∗Authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Reconstruction of an input image x from the gradient ∇θLθ(x, y). Left: Image from the validation dataset. Middle: Reconstruction from a trained ResNet-18 trained on ImageNet. Right:
Reconstruction from a trained ResNet-152. In both cases, the intended privacy of the image is broken.
Note that previous attacks cannot recover either ImageNet-sized data [35] or attack trained models. update the overall weights. Using gradient descent the server’s updates could, for instance, constitute
θk+1 = θk − τ
N (cid:88)
∇θLθk (xi, yi)
. (cid:124) i=1 (cid:125) (cid:123)(cid:122) server (cid:124) (cid:123)(cid:122) users (cid:125) (2)
The updated parameters θk+1 are sent back to the individual users. The procedure in eq. (2) is called federated SGD. In contrast, in federated averaging [17, 21] each user computes several gradient descent steps locally, and sends the updated parameters back to the server. Finally, information about (xi, yi) can be further obscured, by only sharing the mean 1 i=1 ∇θLθk (xi, yi) of the gradients of t several local examples, which we refer to as the multi-image setting. (cid:80)t
Distributed learning of this kind has been used in real-world applications where user privacy is crucial, e.g. for hospital data [13] or text predictions on mobile devices [3], and it has been stated that “Privacy is enhanced by the ephemeral and focused nature of the [Federated Learning] updates” [3]: model updates are considered to contain less information than the original data, and through aggregation of updates from multiple data points, original data is considered impossible to recover. In this work we show analytically as well as empirically, that parameter gradients still carry signiﬁcant information about the supposedly private input data as we illustrate in Fig. 1. We conclude by showing that even multi-image federated averaging on realistic architectures does not guarantee the privacy of all user data, showing that out of a batch of 100 images, several are still recoverable.
Threat model: We investigate an honest-but-curious server with the goal of uncovering user data:
The attacker is allowed to separately store and process updates transmitted by individual users, but may not interfere with the collaborative learning algorithm. The attacker may not modify the model architecture to better suit their attack, nor send malicious global parameters that do not represent the actually learned global model. The user is allowed to accumulate data locally in Sec. 6. We refer to the supp. material for further commentary and mention that the attack is near-trivial under weaker constraints on the attacker.
In this paper we discuss privacy limitations of federated learning ﬁrst in an academic setting, honing in on the case of gradient inversion from one image and showing that
• Reconstruction of input data from gradient information is possible for realistic deep and non-smooth architectures with both, trained and untrained parameters.
• With the right attack, there is little “defense-in-depth" - deep networks are as vulnerable as shallow networks.
• We prove that the input to any fully connected layer can be reconstructed analytically independent of the remaining network architecture.
Then we consider the implications that the ﬁndings have for practical scenarios, ﬁnding that 2
• Reconstruction of multiple, separate input images from their averaged gradient is possible in practice, over multiple epochs, using local mini-batches, or even for a local gradient averaging of up to 100 images. 2