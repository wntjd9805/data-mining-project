Abstract
Leverage score sampling is a powerful technique that originates from theoretical computer science, which can be used to speed up a large number of fundamental questions, e.g. linear regression, linear programming, semi-deﬁnite programming, cutting plane method, graph sparsiﬁcation, maximum matching and max-ﬂow. Re-cently, it has been shown that leverage score sampling helps to accelerate kernel methods [Avron, Kapralov, Musco, Musco, Velingker and Zandieh 17].
In this work, we generalize the results in [Avron, Kapralov, Musco, Musco, Vel-ingker and Zandieh 17] to a broader class of kernels. We further bring the leverage score sampling into the ﬁeld of deep learning theory.
• We show the connection between the initialization for neural network train-ing and approximating the neural tangent kernel with random features.
• We prove the equivalence between regularized neural network and neural tan-gent kernel ridge regression under the initialization of both classical random
Gaussian and leverage score sampling. 1

Introduction
Kernel method is one of the most common techniques in various machine learning problems. One classical application is the kernel ridge regression (KRR). Given training data X = [x1, · · · , xn](cid:62) ∈
Rn×d, corresponding labels Y = [y1, · · · , yn] ∈ Rn and regularization parameter λ > 0, the output estimate of KRR for any given input z can be written as: f (z) = K(z, X)(cid:62)(K + λIn)−1Y, (1) where K(·, ·) denotes the kernel function and K ∈ Rn×n denotes the kernel matrix.
Despite being powerful and well-understood, the kernel ridge regression suffers from the costly computation when dealing with large datasets, since generally implementation of Eq. (1) requires
O(n3) running time. Therefore, intensive research have been dedicated to the scalable methods for KRR [Bac13, AM15, ZDW15, ACW17, MM17, ZNV+20]. One of the most popular approach is the random Fourier features sampling originally proposed by [RR08] for shift-invariant kernels.
They construct a ﬁnite dimensional random feature vector φ : Rd → Cs through sampling that approximates the kernel function K(x, z) ≈ φ(x)∗φ(z) for data x, z ∈ Rd. The random feature helps approximately solves KRR in O(ns2 + n2) running time, which improves the computational
∗The authors would like to thank Michael Kapralov for suggestion of this topic. The full version of this paper is available at https://arxiv.org/pdf/2009.09829.pdf
†jasonlee@princeton.edu Princeton University. Work done while visiting Institute for Advanced Study.
‡shenr3@cs.washington.edu University of Washington. Work done while visiting Institute for Ad-vanced Study.
§zhaos@ias.edu Columbia University, Princeton University and Institute for Advanced Study.
¶mengdiw@princeton.edu Princeton University. (cid:107)zhengy@princeton.edu Princeton University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
cost if s (cid:28) n. The work [AKM+17] advanced this result by introducing the leverage score sampling to take the regularization term into consideration.
In this work, we follow the the approach in [AKM+17] and naturally generalize the result to a broader class of kernels, which is of the form
K(x, z) = E w∼p
[φ(x, w)(cid:62)φ(z, w)], where φ : Rd × Rd1 → Rd2 is a ﬁnite dimensional vector and p : Rd1 → R≥0 is a probability distribution. We apply the leverage score sampling technique in this generalized case to obtain a tighter upper-bound on the dimension of random features.
The most important contribution of this work is to introduce the leverage score theory into the ﬁeld of neural network training. Over the last two years, there is a long line of over-parametrization the-ory works on the convergence results of deep neural network [LL18, DZPS19, AZLS19b, AZLS19a,
DLL+19, ADH+19b, ADH+19a, SY19, BPSW20], all of which either explicitly or implicitly use the property of neural tangent kernel [JGH18]. However, most of those results focus on neural network training without regularization, while in practice regularization (which is originated from classical machine learning) has been widely used in training deep neural network. Therefore, in this work we rigorously build the equivalence between training a ReLU deep neural network with (cid:96)2 regularization and neural tangent kernel ridge regression. We observe that the initialization of train-ing neural network corresponds to approximating the neural tangent kernel with random features, whose dimension is proportional to the width of the network. Thus, it motivates us to bring the lever-age score sampling theory into the neural network training. We present a new equivalence between neural net and kernel ridge regression under the initialization using leverage score sampling, which potentially improves previous equivalence upon the upper-bound of network width needed.
We summarize our main results and contribution as following:
• Generalize the leverage score sampling theory for kernel ridge regression to a broader class of kernels.
• Connect the leverage score sampling theory with neural network training.
• Theoretically prove the equivalence between training regularized neural network and kernel ridge regression under both random Gaussian initialization and leverage score sampling initialization. 2