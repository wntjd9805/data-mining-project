Abstract
Sample-based planning is a powerful family of algorithms for generating intelligent behavior from a model of the environment. Generating good candidate actions is critical to the success of sample-based planners, particularly in continuous or large action spaces. Typically, candidate action generation exhausts the action space, uses domain knowledge, or more recently, involves learning a stochastic policy to provide such search guidance. In this paper we explore explicitly learning a candidate action generator by optimizing a novel objective, marginal utility.
The marginal utility of an action generator measures the increase in value of an action over previously generated actions. We validate our approach in both curling, a challenging stochastic domain with continuous state and action spaces, and a location game with a discrete but large action space. We show that a generator trained with the marginal utility objective outperforms hand-coded schemes built on substantial domain knowledge, trained stochastic policies, and other natural objectives for generating actions for sample-based planners. 1

Introduction
Sample-based planning is a powerful family of algorithms used in decision-making settings where the objective is to ﬁnd the best action from among a number of possibilities. These algorithms involve an agent repeatedly sampling actions for evaluation in order to ﬁnd an optimal action. In domains with large (possibly continuous) action spaces, the number of samples needed to identify optimal actions renders an exhaustive search intractable. In many domains, such as sequential games, the outcome of an action can be stochastic, which further inﬂates the search space. In this paper, we consider problem domains with continuous or very large discrete action spaces.
Monte Carlo tree search (MCTS) methods such as UCT [8] performs search in discrete action spaces by balancing between sampling promising actions to improve the estimates of their outcomes and exploring other candidate actions. MCTS algorithms, though, might fail to encounter effective actions if the action space is very large, as the algorithm is unable to consider all available actions during search [11]. Search algorithms such as MCTS can thus beneﬁt from an action candidate generator, that given a state, returns a set of promising actions to be executed at that state.
For search in a continuous action setting, Yee et. al [18] introduced Kernel Regression UCT (KR-UCT), an MCTS algorithm for continuous spaces and stochastic outcomes that requires an action 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
candidate generator. Yee et. al [18] used hand-coded generators in their experiments. Meanwhile, reinforcement learning with self-play was shown to be successful at training agents to search in domains with a relatively large action space, such as Go [13, 15], using a learned policy to guide search. Lee et al. [9] then combined this idea with KR-UCT to learn an action generator for search in continuous domains by sampling a set of actions from the policy. The key insight in this work is that sampling from a good policy does not necessarily produce a good set of actions for planning.
In this paper, we introduce a novel approach that explicitly generates sets of candidate actions for sample-based planning in large action spaces. Our approach trains a neural network model while optimizing for the marginal utilities of the generated actions, a concept we introduce in this paper.
The marginal utility measures the increase in value for generating an action over previously generated actions, thus favoring diversity amongst the generated actions. We evaluate our approach in curling, a stochastic domain with continuous state and action spaces, and in a location game with a discrete but large action space. Our results show that our generator outperforms trained policies, hand-coded agents, and other optimization objectives when used for sample-based planning. 2