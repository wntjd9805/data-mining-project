Abstract
Learning graphical structures based on Directed Acyclic Graphs (DAGs) is a chal-lenging problem, partly owing to the large search space of possible graphs. A recent line of work formulates the structure learning problem as a continuous constrained optimization task using the least squares objective and an algebraic characterization of DAGs. However, the formulation requires a hard DAG constraint and may lead to optimization difﬁculties. In this paper, we study the asymptotic role of the sparsity and DAG constraints for learning DAG models in the linear Gaussian and non-Gaussian cases, and investigate their usefulness in the ﬁnite sample regime.
Based on the theoretical results, we formulate a likelihood-based score function, and show that one only has to apply soft sparsity and DAG constraints to learn a DAG equivalent to the ground truth DAG. This leads to an unconstrained opti-mization problem that is much easier to solve. Using gradient-based optimization and GPU acceleration, our procedure can easily handle thousands of nodes while retaining a high accuracy. Extensive experiments validate the effectiveness of our proposed method and show that the DAG-penalized likelihood objective is indeed favorable over the least squares one with the hard DAG constraint. 1

Introduction
Learning graphical structures from data based on Directed Acyclic Graphs (DAGs) is a fundamental problem in machine learning, with applications in many areas such as biology [40] and healthcare [26].
It is clear that the learned graphical models may not be interpreted causally [33, 48]. However, they provide a compact, yet ﬂexible, way to decompose the joint distribution. Under further conditions, these graphical models may have causal interpretations or be converted to representations (e.g.,
Markov equivalence classes) that have causal interpretations.
Two major classes of structure learning methods are constraint- and score-based methods. Constraint-based methods, including PC [46] and FCI [47, 11], utilize conditional independence tests to recover the Markov equivalence class under faithfulness assumption. On the other hand, score-based methods formulate the problem as optimizing a certain score function [19, 10, 49, 45]. Due to the large search space of possible graphs [9], most score-based methods rely on local heuristics, such as GES [10].
Recently, Zheng et al. [54] have introduced the NOTEARS method which formulates the structure learning problem as a continuous constrained optimization task, leveraging an algebraic characteri-zation of DAGs. NOTEARS is speciﬁcally developed for linear DAGs, and has been extended to handle nonlinear cases via neural networks [21, 53, 28, 29, 25, 55]. Other related works include
DYNOTEARS [32] that focuses on time-series data, and [56] that uses reinforcement learning to ﬁnd the optimal DAGs. NOTEARS and most of its extensions adopt the least squares objective, which is related to but does not directly maximize the data likelihood. Furthermore, their formulations require a hard DAG constraint which may lead to optimization difﬁculties (see Section 2.2).
In this work, we investigate whether such a hard DAG constraint and another widely used sparsity constraint are necessary for learning DAGs. Inspired by that, we develop a likelihood-based structure 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
learning method with continuous unconstrained optimization, called Gradient-based Optimization of dag-penalized Likelihood for learning linEar dag Models (GOLEM). Our contributions are:
•
•
•
•
We compare the differences between the regression-based and likelihood-based objectives for learning linear DAGs.
We study the asymptotic role of the sparsity and DAG constraints in the general linear Gaussian case and other speciﬁc cases including linear non-Gaussian model and linear Gaussian model with equal noise variances. We also investigate their usefulness in the ﬁnite sample regime.
Based on the theoretical results, we formulate a likelihood-based score function, and show that one only has to apply soft sparsity and DAG constraints to learn a DAG equivalent to the ground truth DAG. This removes the need for a hard DAG constraint1[54] and leads to an unconstrained optimization problem that is much easier to solve.
We demonstrate the effectiveness of our DAG-penalized likelihood objective through extensive experiments and an analysis on the bivariate linear Gaussian model.
The rest of the paper is organized as follows: We review the linear DAG model and NOTEARS in
Section 2. We then discuss the role of sparsity and DAG constraints under different settings in Section 3. Based on the theoretical study, we formulate a likelihood-based method in Section 4, and compare it to NOTEARS and the least squares objective. The experiments in Section 5 verify our theoretical study and the effectiveness of our method. Finally, we conclude our work in Section 6. 2