Abstract
Deep residual networks (ResNets) have demonstrated better generalization per-formance than deep feedforward networks (FFNets). However, the theory behind such a phenomenon is still largely unknown. This paper studies this fundamental problem in deep learning from a so-called “neural tangent kernel” perspective.
Speciﬁcally, we ﬁrst show that under proper conditions, as the width goes to inﬁn-ity, training deep ResNets can be viewed as learning reproducing kernel functions with some kernel function. We then compare the kernel of deep ResNets with that of deep FFNets and discover that the class of functions induced by the kernel of
FFNets is asymptotically not learnable, as the depth goes to inﬁnity. In contrast, the class of functions induced by the kernel of ResNets does not exhibit such degeneracy. Our discovery partially justiﬁes the advantages of deep ResNets over deep FFNets in generalization abilities. Numerical results are provided to support our claim.

Introduction 1
Deep Neural Networks (DNNs) have made signiﬁcant progress in a variety of real-world applications, such as computer vision [1, 2, 3], speech recognition, natural language processing [4, 5, 6], recom-mendation systems, etc. Among various network architectures, Residual Networks (ResNets, [7]) are undoubtedly a breakthrough. Residual Networks are equipped with residual connections, which skip layers in the forward step. Similar ideas based on gating mechanisms are also adopted in Highway
Networks [8], and further inspire many follow-up works such as Densely Connected Networks [9].
Compared with conventional Feedforward Networks (FFNets), residual networks demonstrate surpris-ing generalization abilities. Existing literature rarely considers deep feedforward networks with more than 30 layers. This is because many experimental results have suggested that very deep feedforward networks yield worse generalization performance than their shallow counterparts [7]. In contrast, we can train residual networks with hundreds of layers, and achieve better generalization performance than that of feedforward networks. For example, ResNet-152 [7], achieving a 19.38% top-1 error on the ImageNet data set, consists of 152 layers; ResNet-1001 [10], achieving a 4.92% error on the
CIFAR-10 data set, consists of 1000 layers.
Despite the great success and popularity of the residual networks, the reason why they generalize so well is still largely unknown. There have been several lines of research attempting to demystify this phenomenon. One line of research focuses on empirical studies of residual networks, and provides
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
intriguing observations. For example, [11] show that residual networks behave like an ensemble of weakly dependent networks of much smaller sizes, and meanwhile, they also show that the gradient vanishing issue is also signiﬁcantly mitigated due to these smaller networks. [12] further provide a more reﬁned elaboration on the gradient vanishing issue. They demonstrate that the gradient magnitude in residual networks only shows sublinear decay (with respect to the layer), which is much slower than the exponential decay of gradient magnitude in feedforward neural networks.
[13] propose a visualization approach for analyzing the landscape of neural networks, and further demonstrate that residual networks have smoother optimization landscape due to the skip-layer connections.
Another line of research focuses on theoretical investigations of residual networks under simpliﬁed network architectures. A commonly adopted structure, which is a reformulation of FFNets, is x(cid:96) = φ(x(cid:96)−1 + αW(cid:96)x(cid:96)−1), (1) where (cid:96) is the number of layers and the skip-connection only bypasses the weight matrix W(cid:96) at each layer [14, 15, 16, 17, 18]. Speciﬁcally, [16] study the optimization landscape with linear activation;
[17] study using Stochastic Gradient Descent (SGD) to train a two-layer ResNet. [18] study using
Gradient Descent (GD) to train a two-layer non-overlapping residual network. [14, 15] both take the perturbation analysis approach to show convergence of such ResNets. A more realistic structure is x(cid:96) = x(cid:96)−1 + φ(αW(cid:96)x(cid:96)−1), (2) where the skip-connection bypasses the activation function [19, 20]. [20] only consider separable setting and take the perturbation analysis to show the convergence and generalization property of such
ResNet. These results, however, are only loosely related to the generalization abilities of residual networks, and often considered to be overoptimistic, due to the oversimpliﬁed assumptions.
Some more recent works provide a new theoretical framework for analyzing overparameterized neural networks [21, 22, 23, 24, 14, 25, 26, 27]. They focus on connecting two- or three-layer over-parameterized (sufﬁciently wide) neural networks to reproducing kernel Hilbert spaces. Speciﬁcally, they show that under proper conditions, the weight matrices of a well trained overparameterized neural network (achieving any given small training error) are actually very close to their initialization.
Accordingly, the training process can be described as searching within some class of reproducing kernel functions, where the associated kernel is called the “neural tangent kernel” (NTK, [21]) and only depends on the initialization of the weights. Accordingly, the generalization properties of the overparameterized neural network are equivalent to those of the associated NTK function class. Based on such a framework, [19] derived the NTK of the ResNet (2) when only the last layer is trained, and proved the convergence of such ResNet. However, they did not provide an explicit formula for the
NTK when all layers are trained, which is required for characterizing the generalization property of
ResNets.
To better understand the generalization abilities of deep feedforward and residual networks, we propose to investigate the NTKs associated with these networks when all but the last layers are trained, and consider the case when both widths and depths go to inﬁnity2. For the structure of
ResNets, we adopt (2) only with a slight modiﬁcation, since it captures the essence of the skip-connection; see Section 2 x(cid:96) = x(cid:96)−1 + α
V(cid:96)σ0 (cid:114) 1 m (cid:16)(cid:114) 2 m
W(cid:96)x(cid:96)−1 (cid:17)
. (3)
Speciﬁcally, we prove that similar to what has been shown for feedforward networks [21], as the width of deep residual networks increases to inﬁnity, training residual networks can also be viewed as learning reproducing kernel functions with some NTK. However, such an NTK associated with the residual networks exhibits a very different behavior from that of feedforward networks.
To demonstrate such a difference, we further consider the regime, where the depths of both feedfor-ward and residual networks are allowed to increase to inﬁnity. Accordingly, both NTKs associated with deep feedforward and residual networks converge to their limiting forms sublinearly (in terms of the depth). For notational simplicity, we refer to the limiting form of the NTKs as the limiting NTK.
Besides asymptotic analysis, we also provide nonasymptotic bounds, which demonstrate equivalence between limiting NTKs and neural networks with sufﬁcient depth and width.
When comparing their limiting NTKs, we ﬁnd that the class of functions induced by the limiting
NTKs associated with deep feedforward networks is essentially not learnable. Such a class of 2More precisely, our analysis considers the regime, where the widths go to inﬁnity ﬁrst, and then the depths go to inﬁnity. See more details in Section 4. 2
functions is sufﬁcient to overﬁt training data. Given any ﬁnite sample size, however, the learned function cannot generalize. In contrast, the class of functions induced by the limiting NTKs associated with deep residual networks does not exhibit such degeneracy. Our discovery partially justiﬁes the advantages of deep residual networks over deep feedforward networks in terms of generalization abilities. Numerical results are provided to support our claim.
Our work is closely related to [28]. They also investigate the so-called “Gaussian Process” kernel induced by feedforward networks under the regime where the depth is allowed to increase to inﬁnity. However, their studied neural networks are essentially some speciﬁc implementations of the reproducing kernels using random features, since the training process only updates the last layer of the neural networks, and keeps other layers unchanged. In contrast, we assume the training process updates all layers except for the last layer.
√
Notations: We use σ0(z) = max(0, z) to denote the ReLU activation function in neural networks. 2 max(0, z). The derivative 3 of
We use σ(z) to denote the normalized ReLU function σ(z) =
√ 2I{z≥0} is the normalized step 0(z) = I{z≥0}. Then σ(cid:48)(z) =
ReLU function (step function) is σ(cid:48) function. We use D to denote the input dimension and SD−1 to denote the unit sphere in RD. We use m to denote the network width (the number of neurons at each layer) and L to denote the depth.
Let M2
+ be the set of all 2 × 2 positive semi-deﬁnite matrices. We use F to denote the set of all symmetric and positive semi-deﬁnite functions from RD × RD to R. We use (cid:107) · (cid:107)max to denote the entry-wise (cid:96)∞ norm for matrices and use (cid:107) · (cid:107) to denote the (cid:96)2 norm for vectors and the spectral norm for matrices. We use diag(·) to denote the diagonal matrix. We use In to denote the n × n identity matrix. We use x and ˜x to denote a pair of inputs. We use x(cid:96) and ˜x(cid:96) to denote the output of the (cid:96)-th layer of a network for the input x and ˜x, respectively. We use f and ˜f to denote the ﬁnal output of the network for x and ˜x, respectively. We use ∇θf = ∇θfθ(x) to denote the derivative of parametrized model fθ w.r.t. θ at the input x, and ∇θ 2