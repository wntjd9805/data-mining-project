Abstract
We address the question of characterizing and ﬁnding optimal representations for supervised learning. Traditionally, this question has been tackled using the
Information Bottleneck, which compresses the inputs while retaining information about the targets, in a decoder-agnostic fashion. In machine learning, however, our goal is not compression but rather generalization, which is intimately linked to the predictive family or decoder of interest (e.g. linear classiﬁer). We propose the
Decodable Information Bottleneck (DIB) that considers information retention and compression from the perspective of the desired predictive family. As a result, DIB gives rise to representations that are optimal in terms of expected test performance and can be estimated with guarantees. Empirically, we show that the framework can be used to enforce a small generalization gap on downstream classiﬁers and to predict the generalization ability of neural networks. 1

Introduction
A fundamental choice in supervised machine learning (ML) centers around the data representation from which to perform predictions. While classical ML uses predeﬁned encodings of the data [1–5] recent progress [6, 7] has been driven by learning such representations. A natural question, then, is what characterizes an “optimal” representation — in terms of generalization — and how to learn it.
The standard framework for studying generalization, statistical learning theory [8], usually assumes a ﬁxed dataset/representation, and aims to restrict the predictive functional family (e.g. linear classiﬁers) such that empirical risk minimizers (ERMs) generalize.1 Here, we turn the problem on its head: we ask whether it is possible to enforce generalization by changing the representation of the inputs such that ERMs in perform well, irrespective of the complexity of
V
.
V
V
A common approach to representation learning consists of jointly training the classiﬁer and repre-sentation by minimizing the empirical risk (which we call J-ERM). By only considering empirical risk, J-ERM is optimal in the inﬁnite data limit (consistent; [10]), but the resulting representations do not favor classiﬁers that will generalize from ﬁnite samples. In contrast, the information bottleneck (IB) method [11] aims for representations that have minimal information about the inputs to avoid over-ﬁtting, while having sufﬁcient information about the labels [12]. While conceptually appealing and used in a range of applications [13–16], IB is based on Shannon’s mutual information, which 1Rather than deﬁning learning in terms of deterministic hypotheses h ∈ H, we consider the more general [9] case of probabilistic predictors V, in order to make a link with information theory. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Schematic J-ERM (b) Schematic DIB (c) Empirical J-ERM (d) Empirical DIB
Figure 1: Left two plots: illustration of representations learned by joint empirical risk minimization (J-ERM) and our decodable information bottleneck (DIB), for classiﬁers with linear vertical decision boundaries. (a) For representations learned by J-ERM, there may exist an ERM that does not generalize; (b) Representations learned by DIB ensure that any ERM will generalize to the test set
-minimality) . Right two plots: 2D representations encoded by an multi-layer perceptron (MLP) (
V for odd-even classiﬁcation of 200 MNIST [22] examples. The white decision boundary corresponds to a classifer which was trained to perform well on train but bad on test (see Sec. 4.2). (c) J-ERM
. allows such classiﬁers that cannot generalize; (d) DIB ensures that there are no such classiﬁers in
V
V was developed for communication theory [17] and does not take into account the predictive family of interest. As a result, IB’s sufﬁciency requirement does not ensure the existence of a predictor that can perform well using the learned representation; 2 while its minimality term is difﬁcult
V f to estimate, making IB impractical without resorting to approximations [18–21].
∈ V
We resolve these issues by introducing the decodable information bottleneck (DIB) objective, which recovers minimal sufﬁcient representations relative to a predictive family
. Intuitively, it ensures that classiﬁers in
-sufﬁciency) but cannot distinguish examples with the same
-minimality), as illustrated in Fig. 1. Our main contributions can be summarized as follows: label ( can predict labels (
V
V
V
V
•
•
•
•
We generalize notions of minimality and sufﬁciency to consider predictors
V
We prove that such representations are optimal — every downstream ERM in best achievable test performance — and can be learned with guarantees using DIB.
V of interest. reaches the
We experimentally demonstrate that using our representations can increase the performance and robustness of downstream classiﬁers in average and worst case scenarios.
We show that the generalization ability of a neural network is highly correlated with the
-minimality of its hidden representations in a wide range of settings (562 models). degree of
V 2 Problem Statement and