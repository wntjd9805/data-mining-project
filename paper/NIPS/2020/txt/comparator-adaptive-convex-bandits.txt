Abstract
We study bandit convex optimization methods that adapt to the norm of the com-parator, a topic that has only been studied before for its full-information counterpart.
Speciﬁcally, we develop convex bandit algorithms with regret bounds that are small whenever the norm of the comparator is small. We ﬁrst use techniques from the full-information setting to develop comparator-adaptive algorithms for linear ban-dits. Then, we extend the ideas to convex bandits with Lipschitz or smooth loss functions, using a new variant of the standard single-point gradient estimator and carefully designed surrogate losses. 1

Introduction
In many situations, information is readily available. For example, if a gambler were to bet on the outcome of a football game, he can observe the outcome of the game regardless of what bet he made.
In other situations, information is scarce. For example, the gambler could be deciding what to eat for dinner: should I eat a salad, a pizza, a sandwich, or not at all? These actions will result in different and unknown outcomes, but the gambler will only see the outcome of the action he actually takes, with one notable exception: not eating results in a predetermined outcome of being very hungry.
These two situations are instantiations of two different settings in online convex optimization: the full information setting and the bandit setting. More formally, both settings are sequential decision making problems where in each round t = 1, . . . , T , a learner has to make a prediction wt ∈ W ⊆ Rd and an adversary provides a convex loss function (cid:96)t : W → R. Afterwards, in the full information setting
[27] the learner has access to the loss function (cid:96)t, while in the bandit setting [19, 13] the learner only receives the loss evaluated at the prediction, that is, (cid:96)t(wt). In both settings the goal is to minimize the regret with respect to some benchmark point u in hindsight, referred to as the comparator. More speciﬁcally, the regret against u is the difference between the total loss incurred by the predictions of the learner and that of the comparator:
RT (u) =
T (cid:88) t=1 (cid:96)t(wt) − (cid:96)t(u).
When the learner’s strategy is randomized, we measure the performance by the expected regret
E [RT (u)].
Standard algorithms in both the full information setting and the bandit setting assume that the learner’s decision space W is a convex compact set and achieve sublinear regret against the optimal comparator t=1 (cid:96)t(u∗). To tune these standard algorithms optimally, however, in this set: u = arg minu∗∈W one requires knowledge of the norm of the comparator (cid:107)u(cid:107), which is unknown. A common work-around is to simply tune the algorithms in terms of the worst-case norm: maxu∈W (cid:107)u(cid:107), assumed to be 1 without loss of generality. This results in worst-case bounds that do not take advantage of the case when (cid:107)u(cid:107) is small. For example, when the loss functions are L-Lipschitz, classic Online (cid:80)T 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Summary of main results. Regret is measured with respect to the total loss of an arbitrary point u ∈ Rd in the unconstrained setting, or an arbitrary point u ∈ W in the constrained setting with a decision space W contained in the unit ball. T is the total number of rounds, 1/c is radius of the largest ball contained by W, and ν is the self-concordant parameter. Both c and ν are bounded by
O(d).
Regret for unconstrained settings Regret for constrained settings
Loss functions (L-Lipschitz)
Linear (Section 3.2)
Convex (Section 4.1 and 4.2) (cid:16) (cid:101)O (cid:16) (cid:107)u(cid:107)dL
√ (cid:107)u(cid:107)L (cid:101)O (cid:17)
√
T dT 3 4 (cid:17)
Convex and β-smooth (Section 4.2) (cid:16) (cid:101)O max{(cid:107)u(cid:107)2, (cid:107)u(cid:107)}β(dLT ) 2 3 (cid:17) (cid:16) (cid:101)O (cid:16) (cid:107)u(cid:107)cdL
√ (cid:107)u(cid:107)cL (cid:101)O (cid:17)
√
T dT 3 4 (cid:17)
-Gradient Descent [27] guarantees RT (u) = O(L
√ algorithm of [13] guarantees E [RT (u)] = O(d independent of (cid:107)u(cid:107).
√
T ) in the full information setting, while the
LT 3/4) in the bandit setting, both of which are
Recently, there has been a series of works in the full information setting that addresses this problem by developing comparator-adaptive algorithms, whose regret against u depends on (cid:107)u(cid:107) for all u ∈ W simultaneously (see for example McMahan and Orabona [22], Orabona and P´al [23], Foster et al.
[14], Cutkosky and Boahen [9], Kotlowski [20], Cutkosky and Orabona [10], Foster et al. [16], Jun and Orabona [18], Van der Hoeven [25]). These bounds are often not worse than the standard worst-case bounds, but could be much smaller in the case when there exists a comparator with small norm and reasonably small total loss. Moreover, most of these results also hold for the so-called unconstrained setting where W = Rd, that is, both the learner’s predictions and the comparator can be any point in Rd. For example, Cutkosky and Orabona [10] achieve RT (u) = (cid:101)O((cid:107)u(cid:107)L
T ) for all u, in both the constrained and unconstrained settings, under full information feedback.1
√
While developing comparator-adaptive algorithms is relatively well-understood at this point in the full information setting, to the best of our knowledge, this has not been studied at all for the more challenging bandit setting. In this work, we make the ﬁrst attempt in this direction and develop comparator-adaptive algorithms for several situations, including learning with linear losses, general convex losses, and convex and smooth losses, for both the constrained and unconstrained settings.
Our results are summarized in Table 1. Ignoring other parameters for simplicity, for the linear case,
T ) regret (Section 3.2); for the general convex case, we achieve (cid:101)O((cid:107)u(cid:107)T 3 we achieve (cid:101)O((cid:107)u(cid:107) 4 ) regret in both the constrained and unconstrained setting (Sections 4.1 and 4.2); and for the convex and smooth case, we achieve (cid:101)O regret in the unconstrained setting (Section 4.1). max{(cid:107)u(cid:107)2, (cid:107)u(cid:107)}β(dLT ) 2
√ (cid:17) (cid:16) 3
In order to achieve our results for the convex case, we require an assumption on the loss, namely that the value of (cid:96)t(0) is known for all t.2 While restrictive at ﬁrst sight, we believe that there are abundant applications where this assumption holds. As one instance, in control or reinforcement learning problems, 0 may represent some nominal action which has a known outcome: not eating results in hunger, or buying zero inventory will result in zero revenue. Another application is a classiﬁcation problem where the features are not revealed to the learner. For example, end-users of a prediction service may not feel comfortable revealing their information to the service. Instead, they may be willing to do some local computation and report the loss of the service’s model. Most classiﬁcation models (e.g. logistic regression) have the property that the loss of the 0 parameter is a known constant regardless of the data, and so this situation would also ﬁt into our framework.
Common loss functions that satisfy this assumption are linear loss, logistic loss, and hinge loss.
Techniques Our algorithms are based on sophisticated extensions of the black-box reduction introduced by Cutkosky and Orabona [10], which separately learns the magnitude and the direction of the prediction. To make the reduction work in the bandit setting, however, new ideas are required, including designing an appropriate surrogate loss function and a new variant of the standard one-point gradient estimator with time-varying parameters. Note that Cutkosky and Orabona [10] also propose 1Throughout the paper, the notation (cid:101)O hides logarithmic dependence on parameters T , (cid:107)u(cid:107), and L. 2For the linear case, this clearly holds since (cid:96)t(0) = 0. 2
a method to convert any unconstrained algorithm to a constrained one in the full information setting, but this does not work in the bandit setting for technical reasons. Instead, we take a different approach by constraining the magnitude of the prediction directly.