Abstract
We present a generative model for complex free-form structures such as stroke-based drawing tasks. While previous approaches rely on sequence-based models for drawings of basic objects or handwritten text, we propose a model that treats drawings as a collection of strokes that can be composed into complex structures such as diagrams (e.g., ﬂow-charts). At the core of the approach lies a novel auto-encoder that projects variable-length strokes into a latent space of ﬁxed dimension.
This representation space allows a relational model, operating in latent space, to better capture the relationship between strokes and to predict subsequent strokes.
We demonstrate qualitatively and quantitatively that our proposed approach is able to model the appearance of individual strokes, as well as the compositional structure of larger diagram drawings. Our approach is suitable for interactive use cases such as auto-completing diagrams. We make code and models publicly available at https://eth-ait.github.io/cose. 1

Introduction
Sketches and drawings have been at the heart of human civilization for millennia. While free-form sketching is a powerful and ﬂexible tool for humans, it is a surprisingly hard task for machines, especially if interpreted in the generative sense. Consider Figure 1: “when given only a sparse set of strokes (in black), what is the most likely continuation of a sketch or diagram (colored strokes are predicted)?” The answer to this question is highly context sensitive and requires reasoning at the local (i.e., stroke) and global (i.e., the diagram or sketch) level.
Existing work has been focused on the recognition [1–3] and generation of handwritten text [4, 5] or the modelling of entire drawings [6–9] from the Quick, Draw! dataset
[10]. However, the more recent DiDi dataset introduced by Gervais et al. [11], consisting of much more realistic and challenging complex structures such as diagrams and
ﬂow-charts, has been shown to be challenging for existing methods [12], due to the combinatorially many ways in-dividual strokes can be combined into a complex drawing (see Fig. 7).
Figure 1: Teaser – We model complex draw-ings as a collections of strokes. Given only sparse strokes as input (black) the model pre-dicts the most likely next strokes and their starting positions (heatmap), each color corre-sponds to one prediction step. This functional decomposition allows for generative mod-elling of varied and complex structures such as ﬂow-charts (top), or freehand sketches (bottom). The drawings are model outputs.
∗Work done while at Google. Unrelated to afﬁliation with Apple 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper we propose a novel compositional generative model, called CoSE, for complex stroke based data such as drawings, diagrams and sketches. While existing work considers the entire drawing as a single temporal sequence [3, 6, 7], our key insight is to factor local appearance of a stroke from the global structure of the drawing. To this end we treat each stroke as an ordered sequence of 2D positions s={(xt, yt)}T t=0, where (x, y) represents the 2D location on screen. Importantly we treat the entire drawing x as an unordered collection of strokes x={sk}K k=1. Since the stroke ordering does not impact the semantic meaning of the diagram, this modelling decision has profound implications. In our approach the model does not need to understand the difference between the (K−1)! potential orderings of the previous strokes to predict the k-th stroke, leading to a much more efﬁcient utilization of modelling capacity. To achieve this we propose a generative model that ﬁrst projects variable-length strokes into a latent space of ﬁxed dimension via an encoder. A relational model then predicts the embeddings of future strokes, which are then rendered by the decoder.
The whole network is trained end-to-end and we experimentally show that the architecture can model complex diagrams and ﬂow-charts from the DiDi dataset, free-form sketches from QuickDraw and handwritten text from the IAM-OnDB datasets [13]. We demonstrate the predictive capabilities via a proof-of-concept interactive demo (video in supplementary) in which the model suggests diagram completions based on initial user input. We show that our model outperforms existing models quantitatively and qualitatively and we analyze the learned latent space to provide insights into how predictions are formed. 2