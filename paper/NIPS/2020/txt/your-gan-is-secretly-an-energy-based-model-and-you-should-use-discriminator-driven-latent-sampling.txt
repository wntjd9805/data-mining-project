Abstract
The sum of the implicit generator log-density log pg of a GAN with the logit score of the discriminator deﬁnes an energy function which yields the true data density when the generator is imperfect but the discriminator is optimal. This makes it possible to improve on the typical generator (with implicit density pg). We show that samples can be generated from this modiﬁed density by sampling in latent space according to an energy-based model induced by the sum of the latent prior log-density and the discriminator output score. We call this process of running
Markov Chain Monte Carlo in the latent space, and then applying the generator function, Discriminator Driven Latent Sampling (DDLS). We show that DDLS is highly efﬁcient compared to previous methods which work in the high-dimensional pixel space, and can be applied to improve on previously trained GANs of many types. We evaluate DDLS on both synthetic and real-world datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially improves the Inception
Score of an off-the-shelf pre-trained SN-GAN [1] from 8.22 to 9.09 which is comparable to the class-conditional BigGAN [2] model. This achieves a new state-of-the-art in the unconditional image synthesis setting without introducing extra parameters or additional training. 1

Introduction
Generative Adversarial Networks (GANs) [3] are state-of-the-art models for a large variety of tasks such as image generation [4], semi-supervised learning [5], image editing [6], image translation [7], and imitation learning [8]. The GAN framework consists of two neural networks, the generator G and the discriminator D. The optimization process is formulated as an adversarial game, with the generator trying to fool the discriminator and the discriminator trying to better classify samples as real or fake.
Despite the ability of GANs to generate high-resolution, sharp samples, the samples of GAN models sometimes contain bad artifacts or are even not recognizable [9]. It is conjectured that this is due to the inherent difﬁculty of generating high dimensional complex data, such as natural images, and the optimization challenge of the adversarial formulation. In order to improve sample quality, con-ventional sampling techniques, such as increasing the temperature, are commonly adopted for GAN models [2]. Recently, new sampling methods such as Discriminator Rejection Sampling (DRS) [10],
Metropolis-Hastings Generative Adversarial Network (MH-GAN) [11], and Discriminator Optimal
∗Equal contribution. Ordering determined by coin ﬂip. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Transport (DOT) [12] have shown promising results by utilizing the information provided by both the generator and the discriminator. However, these sampling techniques are either inefﬁcient or lack theoretical guarantees, possibly reducing the sample diversity and making the mode dropping problem more severe.
In this paper, we show that GANs can be better understood through the lens of Energy-Based
Models (EBM). In our formulation, GAN generators and discriminators collaboratively learn an
“implicit” energy-based model. However, efﬁcient sampling from this energy based model directly in pixel space is extremely challenging for several reasons. One is that there is no tractable closed form for the implicit energy function in pixel space. This motivates an intriguing possibility: that Markov
Chain Monte Carlo (MCMC) sampling may prove more tractable in the GAN’s latent space.
Surprisingly, we ﬁnd that the implicit energy based model deﬁned jointly by a GAN generator and discriminator takes on a simpler, tractable form when it is written as an energy-based model over the generator’s latent space. In this way, we propose a theoretically grounded way of generating high quality samples from GANs through what we call Discriminator Driven Latent Sampling (DDLS).
DDLS leverages the information contained in the discriminator to re-weight and correct the biases and errors in the generator. Through experiments, we show that our proposed method is highly efﬁcient in terms of mixing time, is generally applicable to a variety of GAN models (e.g. Minimax,
Non-Saturating, and Wasserstein GANs), and is robust across a wide range of hyper-parameters. An energy-based model similar to our work is also obtained simultaneously in independent work [13] in the form of an approximate MLE lower bound.
We highlight our main contributions as follows:
• We provide more evidence that it is beneﬁcial to sample from the energy-based model deﬁned both by the generator and the discriminator instead of from the generator only.
• We derive an equivalent formulation of the pixel-space energy-based model in the latent space, and show that sampling is much more efﬁcient in the latent space.
• We show experimentally that samples from this energy-based model are of higher quality than samples from the generator alone.
• We show that our method can approximately extend to other GAN formulations, such as
Wasserstein GANs. 2