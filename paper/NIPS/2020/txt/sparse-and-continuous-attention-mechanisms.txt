Abstract
Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have ﬁxed support. In contrast, for ﬁnite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and α-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: ﬁrst, we extend α-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efﬁcient gradient backpropagation algorithms for α ∈ {1, 2}.
Experiments on attention-based text classiﬁcation, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions. 1

Introduction
Exponential families are ubiquitous in statistics and machine learning [1, 2]. They enjoy many useful properties, such as the existence of conjugate priors (crucial in Bayesian inference) and the classical
Pitman-Koopman-Darmois theorem [3–5], which states that, among families with ﬁxed support (independent of the parameters), exponential families are the only having sufﬁcient statistics of ﬁxed dimension for any number of i.i.d. samples.
Departing from exponential families, there has been recent work on discrete, ﬁnite-domain distribu-tions with varying and sparse support, via the sparsemax and the entmax transformations [6–8].
Those approaches drop the link to exponential families of categorical distributions provided by the softmax transformation, which always yields dense probability mass functions. In contrast, sparsemax and entmax can lead to sparse distributions, whose support is not constant throughout the family. This property has been used to design sparse attention mechanisms with improved interpretability [8, 9].
However, sparsemax and entmax are so far limited to discrete domains. Can a similar approach be extended to continuous domains? This paper provides that extension and pinpoints a connection with
“deformed exponential families” [10–12] and Tsallis statistics [13], leading to α-sparse families (§2). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: 1D and 2D distributions generated by the Ωα-RPM for α ∈ {1, 2}. Left: Univariate location-scale families, including Gaussian and truncated parabola (top) and Laplace and triangular (bottom). Middle and right: Bivariate Gaussian N (t; 0, I) and truncated paraboloid TP(t; 0, I).
We use this construction to obtain new density families with varying support, including the truncated parabola and paraboloid distributions (2-sparse counterpart of the Gaussian, §2.4 and Fig. 1).
Softmax and its variants are widely used in attention mechanisms, an important component of neural networks [14]. Attention-based neural networks can “attend” to ﬁnite sets of objects and identify relevant features. We use our extension above to devise new continuous attention mechanisms (§3), which can attend to continuous data streams and to domains that are inherently continuous, such as images. Unlike traditional attention mechanisms, ours are suitable for selecting compact regions, such as 1D-segments or 2D-ellipses. We show that the Jacobian of these transformations are generalized covariances, and we use this fact to obtain efﬁcient backpropagation algorithms (§3.2).
As a proof of concept, we apply our models with continuous attention to text classiﬁcation, machine translation, and visual question answering tasks, with encouraging results (§4).
Notation. Let (S, A, ν) be a measure space, where S is a set, A is a σ-algebra, and ν is a measure.
We denote by M1
+(S) the set of ν-absolutely continuous probability measures. From the Radon-Nikodym theorem [15, §31], each element of M1
+(S) is identiﬁed (up to equivalence within measure zero) with a probability density function p : S → R+, with (cid:82)
S p(t) dν(t) = 1. For convenience, we often drop dν(t) from the integral. We denote the measure of A ∈ A as |A| = ν(A) = (cid:82)
A 1, and the
+(S) as supp(p) = {t ∈ S | p(t) > 0}. Given φ : S → Rm, we write support of a density p ∈ M1 expectations as Ep[φ(t)] := (cid:82)
S p(t) φ(t). Finally, we deﬁne [a]+ := max{a, 0}. 2 Sparse Families
In this section, we provide background on exponential families and its generalization through Tsallis statistics. We link these concepts, studied in statistical physics, to sparse alternatives to softmax recently proposed in the machine learning literature [6, 8], extending the latter to continuous domains. 2.1 Regularized prediction maps (Ω-RPM)
Our starting point is the notion of Ω-regularized prediction maps, introduced by Blondel et al. [7] for ﬁnite domains S. This is a general framework for mapping vectors in R|S| (e.g., label scores computed by a neural network) into probability vectors in (cid:52)|S| (the simplex), with a regularizer
Ω encouraging uniform distributions. Particular choices of Ω recover argmax, softmax [16], and sparsemax [6]. Our deﬁnition below extends this framework to arbitrary measure spaces M1
+(S),
+(S) → R is a lower semi-continuous, proper, and strictly convex function. where we assume Ω : M1
Deﬁnition 1. The Ω-regularized prediction map (Ω-RPM) ˆpΩ : F → M1
+(S) is deﬁned as
ˆpΩ[f ] = arg max
+(S) p∈M1
Ep[f (t)] − Ω(p), (1) where F is the set of functions for which the maximizer above exists and is unique. 2
It is often convenient to consider a “temperature parameter” τ > 0, absorbed into Ω via Ω := τ ˜Ω. If f has a unique global maximizer t(cid:63), the low-temperature limit yields limτ →0 ˆpτ ˜Ω[f ] = δt(cid:63) , a Dirac delta distribution at the maximizer of f . For ﬁnite S, this is the argmax transformation shown in [7].
Other interesting examples of regularization functionals are shown in the next subsections. 2.2 Shannon’s negentropy and exponential families
A natural choice of regularizer is the Shannon’s negentropy, Ω(p) = (cid:82)
S p(t) log p(t). In this case, if we interpret −f (t) as an energy function, the Ω-RPM corresponds to the well-known free energy variational principle, leading to Boltzmann-Gibbs distributions ([17]; see App. A):
ˆpΩ[f ](t) = exp(f (t))
S exp(f (t(cid:48)))dν(t(cid:48)) (cid:82)
= exp(cid:0)f (t) − A(f )(cid:1), (2) where A(f ) := log (cid:82)
S exp(f (t)) is the log-partition function. If S is ﬁnite and ν is the counting measure, the integral in (2) is a summation and we can write f as a vector [f1, . . . , f|S|] ∈ R|S|. In this case, the Ω-RPM is the softmax transformation,
ˆpΩ[f ] = softmax(f ) = exp(f ) k=1 exp(fk) (cid:80)|S|
∈ (cid:52)|S|. (3)
If S = RN , ν is the Lebesgue measure, and f (t) = −1/2(t − µ)(cid:62)Σ−1(t − µ) for µ ∈ RN and Σ (cid:31) 0 (i.e., Σ is a positive deﬁnite matrix), we obtain a multivariate Gaussian, ˆpΩ[f ](t) = N (t; µ, Σ). This becomes a univariate Gaussian N (t; µ, σ2) if N = 1. For S = R and deﬁning f (t) = −|t − µ|/b, with µ ∈ R and b > 0, we get a Laplace density, ˆpΩ[f ](t) = 1 2b exp (−|t − µ|/b).
Exponential families. Let fθ(t) = θ(cid:62)φ(t), where φ(t) ∈ RM is a vector of statistics and θ ∈ Θ ⊆
RM is a vector of canonical parameters. A family of the form (2) parametrized by θ ∈ Θ ⊆ RM is called an exponential family [2]. Exponential families have many appealing properties, such as the existence of conjugate priors and sufﬁcient statistics, and a dually ﬂat geometric structure [18].
Many well-known distributions are exponential families, including the categorical and Gaussian distributions above, and Laplace distributions with a ﬁxed µ. A key property of exponential families is that the support is constant within the same family and dictated by the base measure ν: this follows immediately from the positiveness of the exp function in (2). We abandon this property in the sequel. 2.3 Tsallis’ entropies and α-sparse families
Motivated by applications in statistical physics, Tsallis [13] proposed a generalization of Shannon’s negentropy. This generalization is rooted on the notions of β-logarithm, logβ : R≥0 → R (not to be confused with base-β logarithm), and β-exponential, expβ : R → R: logβ(u) := (cid:26) u1−β −1 1−β , β (cid:54)= 1
β = 1; log u, (cid:26) expβ(u) :=
[1 + (1 − β)u]1/(1−β) exp u,
+
, β (cid:54)= 1
β = 1. (4)
Note that limβ→1 logβ(u) = log u, limβ→1 expβ(u) = exp u, and logβ(expβ(u)) = u for any β.
Another important concept is that of “β-escort distribution” [13]: this is the distribution ˜pβ given by
˜pβ(t) := p(t)β (cid:107)p(cid:107)β
β
, where (cid:107)p(cid:107)β
β = (cid:90)
S p(t(cid:48))βdν(t(cid:48)). (5)
Note that we have ˜p1(t) = p(t).
The α-Tsallis negentropy [19, 13] is deﬁned as:1
Ωα(p) := 1
α
Ep[log2−α(p(t))] =
Note that limα→1 Ωα(p) = Ω1(p), for any p ∈ M1 tropy (proof in App. B). Another notable case is Ω2(p) = 1/2 (cid:82) called the Gini-Simpson index [20, 21]. We come back to the α = 2 case in §2.4. (cid:0)(cid:82)
S p(t)α − 1(cid:1) , α (cid:54)= 1,
α = 1. (cid:40) 1
α(α−1) (cid:82)
S p(t) log p(t),
+(S), with Ω1(p) recovering Shannon’s negen-S p(t)2 − 1/2, the negative of which is (6) 1This entropy is normally deﬁned up to a constant, often presented without the 1
α factor. We use the same deﬁnition as Blondel et al. [7, §4.3] for convenience. 3
For α > 0, Ωα is strictly convex, hence it can be plugged in as the regularizer in Def. 1. The next proposition ([10]; proof in App. B) provides an expression for Ωα-RPM using the β-exponential (4):
Proposition 1. For α > 0 and f ∈ F,
ˆpΩα [f ](t) = exp2−α(f (t) − Aα(f )), (7) where Aα : F → R is a normalizing function: Aα(f ) = 1 1−α +(cid:82) (cid:82)
S pθ(t)2−αf (t)
S pθ(t)2−α
− 1 1−α .
Let us contrast (7) with Boltzmann-Gibbs distributions (2), recovered with α = 1. One key thing to note is that the (2 − α)-exponential, for α > 1, can return zero values. Therefore, the distribution
ˆpΩα [f ] in (7) may not have full support, i.e., we may have supp(ˆpΩα [f ]) (cid:40) S. We say that ˆpΩα [f ] has sparse support if ν(S \ supp(ˆpΩα [f ])) > 0.2 This generalizes the notion of sparse vectors.
Relation to sparsemax and entmax. Blondel et al. [7] showed that, for ﬁnite S, Ω2-RPM is the sparsemax transformation, ˆpΩ[f ] = sparsemax(f ) = arg minp∈(cid:52)|S| (cid:107)p − f (cid:107)2. Other values of
α were studied by Peters et al. [8], under the name α-entmax transformation. For α > 1, these transformations have a propensity for returning sparse distributions, where several entries have zero probability. Proposition 1 shows that similar properties can be obtained when S is continuous.
Deformed exponential families. With a linear parametrization fθ(t) = θ(cid:62)φ(t), distributions with the form (7) are called deformed exponential or q-exponential families [10–12, 24]. The geometry of these families induced by the Tsallis q-entropy was studied by Amari [18, §4.3].3 Unlike those prior works, we are interested in the sparse, light tail scenario (α > 1), not in heavy tails. For α > 1, we call these α-sparse families. When α → 1, α-sparse families become exponential families and they cease to be “sparse”, in the sense that all distributions in the same family have the same support.
A relevant problem is that of characterizing Aα(θ). When α = 1, A1(θ) = limα→1Aα(θ) = log (cid:82)
S exp(θ(cid:62)φ(t)) is the log-partition function (see (2)), and its ﬁrst and higher order derivatives are equal to the moments of the sufﬁcient statistics. The following proposition (stated as Amari and
Ohara [25, Theorem 5], and proved in our App. D) characterizes Aα(θ) for α (cid:54)= 1 in terms of an expectation under the β-escort distribution for β = 2 − α (see (5)). This proposition will be used later to derive the Jacobian of entmax attention mechanisms.
Proposition 2. Aα(θ) is a convex function and its gradient is given by
S pθ(t)2−αφ(t)
S pθ(t)2−α . (cid:82)
∇θAα(θ) = E
[φ(t)] =
˜p2−α
θ (cid:82) (8) 2.4 The 2-Tsallis entropy: sparsemax
In this paper, we focus on the case α = 2. For ﬁnite S, this corresponds to the sparsemax transfo-mation proposed by Martins and Astudillo [6], which has appealing theoretical and computational properties. In the general case, plugging α = 2 in (7) leads to the Ω2-RPM,
ˆpΩ2[f ](t) = [f (t) − λ]+, where λ = A2(f ) − 1, (9) i.e., ˆpΩ2[f ] is obtained from f by subtracting a constant (which may be negative) and truncating, where that constant λ must be such that (cid:82)
S[f (t) − λ]+ = 1.
If S is continuous and ν the Lebesgue measure, we call Ω2-RPM the continuous sparsemax transformation. Examples follow, some of which correspond to novel distributions. 2This should not be confused with sparsity-inducing distributions [22, 23]. 3Unfortunately, the literature is inconsistent in deﬁning these coefﬁcients. Our α matches that of Blondel et al. [7]; Tsallis’ q equals 2 − α; this family is also related to Amari’s α-divergences, but their α = 2q − 1.
Inconsistent deﬁnitions have also been proposed for q-exponential families regarding how they are normalized; for example, the Tsallis maxent principle leads to a different deﬁnition. See App. C for a detailed discussion. 4
Truncated parabola.
Gaussian, which we dub a “truncated parabola”:
− (t−µ)2
If f (t) = − (t−µ)2 2σ2
ˆpΩ2[f ](t) = (cid:104)
, we obtain the continuous sparsemax counterpart of a 2σ2 − λ (cid:105)
+
=: TP(t; µ, σ2), (10) where λ = − 1 (see App. E.1). This function, depicted in Fig. 1 (top left), is widely 2 used in density estimation. For µ = 0 and σ = (cid:112)2/3, it is known as the Epanechnikov kernel [26]. (cid:0)3/(2σ)(cid:1)2/3
Truncated paraboloid. The previous example can be generalized to S = RN , with f (t) =
− 1 2 (t−µ)(cid:62)Σ−1(t−µ), where Σ (cid:31) 0, leading to a “multivariate truncated paraboloid,” the sparsemax counterpart of the multivariate Gaussian (see middle and rightmost plots in Fig. 1):
ˆpΩ2[f ](t) = (cid:2)−λ− 1
The expression above, derived in App. E.2, reduces to (10) for N = 1. Notice that (unlike in the
Gaussian case) a diagonal Σ does not lead to a product of independent truncated parabolas. 2 (t−µ)Σ−1(t−µ)(cid:3) 2 + 2(cid:1)/(cid:112)det(2πΣ) where λ = −
Γ(cid:0) N
. (11)
+, 2+N (cid:16) (cid:17) 2
Triangular. Setting f (t) = −|t − µ|/b, with b > 0, yields the triangular distribution where λ = −1/
√
ˆpΩ2[f ](t) = (cid:104)
−λ − |t−µ| b (cid:105)
+
=: Tri(t; µ, b), (12) b (see App. E.3). Fig. 1 (bottom left) depicts this distribution alongside Laplace.
Location-scale families. More generally, let fµ,σ(t) := − 1 a scale σ > 0, where g : R+ → R is convex and continuously differentiable. Then, we have
σ g(cid:48)(|t − µ|/σ) for a location µ ∈ R and
ˆpΩ2[f ](t) = (cid:2)−λ − 1
σ g(cid:48)(|t − µ|/σ)(cid:3)
+ , (13) where λ = −g(cid:48)(a)/σ and a is the solution of the equation ag(cid:48)(a) − g(a) + g(0) = 1 2 (a sufﬁcient condition for such solution to exist is g being strongly convex; see App. E.4 for a proof). This example subsumes the truncated parabola (g(t) = t3/6) and the triangular distribution (g(t) = t2/2). 2-sparse families. Truncated parabola and paraboloid distributions form a 2-sparse family, with statistics φ(t) = [t, vec(tt(cid:62))] and canonical parameters θ = [Σ−1µ, vec(− 1 2 Σ−1)]. Gaussian distributions form an exponential family with the same sufﬁcient statistics and canonical parameters.
In 1D, truncated parabola and Gaussians are both particular cases of the so-called “q-Gaussian” [10,
§4.1], for q = 2 − α. Triangular distributions with a ﬁxed location µ and varying scale b also form a 2-sparse family (similarly to Laplace distributions with ﬁxed location being exponential families). 3 Continuous Attention
Attention mechanisms have become a key component of neural networks [14, 27, 28]. They dynami-cally detect and extract relevant input features (such as words in a text or regions of an image). So far, attention has only been applied to discrete domains; we generalize it to continuous spaces.
Discrete attention. Assume an input object split in L = |S| pieces, e.g., a document with L words or an image with L regions. A vanilla attention mechanism works as follows: each piece has a
D-dimensional representation (e.g., coming from an RNN or a CNN), yielding a matrix V ∈ RD×L.
These representations are compared against a query vector (e.g., using an additive model [14]), leading to a score vector f = [f1, . . . , fL] ∈ RL. Intuitively, the relevant pieces that need attention should be assigned high scores. Then, a transformation ρ : RL → (cid:52)L (e.g., softmax or sparsemax) is applied to the score vector to produce a probability vector p = ρ(f ). We may see this as an Ω-RPM.
The probability vector p is then used to compute a weighted average of the input representations, via c = V p ∈ RD. This context vector c is ﬁnally used to produce the network’s decision.
To learn via the backpropagation algorithm, the Jacobian of the transformation ρ, Jρ ∈ RL×L, is needed. Martins and Astudillo [6] gave expressions for softmax and sparsemax,
Jsoftmax(f ) = Diag(p) − pp(cid:62), (14) where p = softmax(f ), and s is a binary vector whose (cid:96)th entry is 1 iff (cid:96) ∈ supp(sparsemax(f )).
Jsparsemax(f ) = Diag(s) − ss(cid:62)/(1(cid:62)s), 5
Algorithm 1: Continuous softmax attention with S = RD, Ω = Ω1, and Gaussian RBFs.
Parameters: Gaussian RBFs ψ(t) = [N (t; µj, Σj)]N j=1, basis functions φ(t) = [t, vec(tt(cid:62))], value function VB(t) = Bψ(t) with B ∈ RD×N , score function fθ(t) = θ(cid:62)φ(t) with θ ∈ RM
Function Forward(θ := [Σ−1µ, − 1 2 Σ−1]): rj ← E ˆpΩ[fθ ][ψj(t)] = N (µ, µj, Σ + Σj), return c ← Br (context vector)
∀j ∈ [N ]
// Eqs. 15, 46
Function Backward( ∂L for j ← 1 to N do
∂c , θ := [Σ−1µ, − 1 2 Σ−1]): j )−1, ˜µ ← ˜Σ(Σ−1µ + Σ−1
˜s ← N (µ, µj, Σ + Σj), ˜Σ ← (Σ−1 + Σ−1
∂rj
∂θ ← cov ˆpΩ[fθ ](φ(t), ψj(t)) = [˜s(˜µ − µ); ˜s( ˜Σ + ˜µ˜µ(cid:62) − Σ − µµ(cid:62))] j µj)
// Eqs. 18, 47–48 return ∂L
∂θ ← (cid:0) ∂r
∂θ (cid:1)(cid:62) B(cid:62) ∂L
∂c 3.1 The continuous case: score and value functions
Our extension of Ω-RPMs to arbitrary domains (Def. 1) opens the door for constructing continuous attention mechanisms. The idea is simple: instead of splitting the input object into a ﬁnite set of pieces, we assume an underlying continuous domain: e.g., text may be represented as a function
V : S → RD that maps points in the real line (S ⊆ R, continuous time) onto a D-dimensional vector representation, representing the “semantics” of the text evolving over time; images may be regarded as a smooth function in 2D (S ⊆ R2), instead of being split into regions in a grid.
Instead of scores [f1, . . . , fL], we now have a score function f : S → R, which we map to a
+(S). This density is used in tandem with the value mapping V : S → RD probability density p ∈ M1 to obtain a context vector c = Ep[V (t)] ∈ RD. Since M1
+(S) may be inﬁnite dimensional, we need to parametrize f , p, and V to be able to compute in a ﬁnite-dimensional parametric space.
Building attention mechanisms. We represent f and V using basis functions, φ : S → RM and
ψ : S → RN , deﬁning fθ(t) = θ(cid:62)φ(t) and VB(t) = Bψ(t), where θ ∈ RM and B ∈ RD×N . The score function fθ is mapped into a probability density p := ˆpΩ[fθ], from which we compute the context vector as c = Ep[VB(t)] = Br, with r = Ep[ψ(t)]. Summing up yields the following:
Deﬁnition 2. Let (cid:104)S, Ω, φ, ψ(cid:105) be a tuple with Ω : M1
An attention mechanism is a mapping ρ : Θ ⊆ RM → RN , deﬁned as:
+(S) → R, φ : S → RM , and ψ : S → RN .
ρ(θ) = Ep[ψ(t)], (15) with p = ˆpΩ[fθ] and fθ(t) = θ(cid:62)φ(t). If Ω = Ωα, we call this entmax attention, denoted as ρα.
The values α = 1 and α = 2 lead to softmax and sparsemax attention, respectively.
Note that, if S = {1, ..., L} and φ(k) = ψ(k) = ek (Euclidean canonical basis), we recover the discrete attention of Bahdanau et al. [14]. Still in the ﬁnite case, if φ(k) and ψ(k) are key and value vectors and θ is a query vector, this recovers the key-value attention of Vaswani et al. [28].
On the other hand, for S = RD and φ(t) = [t, vec(tt(cid:62))], we obtain new attention mechanisms (assessed experimentally for the 1D and 2D cases in §4): for α = 1, the underlying density p is
Gaussian, and for α = 2, it is a truncated paraboloid (see §2.4). In both cases, we show (App. G) that the expectation (15) is tractable (1D) or simple to approximate numerically (2D) if ψ are Gaussian
RBFs, and we use this fact in §4 (see Alg. 1 for pseudo-code for the case α = 1).
Deﬁning the value function VB(t).
In many problems, the input is a discrete sequence of observa-tions (e.g., text) or it was discretized (e.g., images), at locations {t(cid:96)}L (cid:96)=1. To turn it into a continuous signal, we need to smooth and interpolate these observations. If we start with a discrete encoder representing the input as a matrix H ∈ RD×L, one way of obtaining a value mapping VB : S → RD is by “approximating” H with multivariate ridge regression. With VB(t) = Bψ(t), and packing the basis vectors ψ(t(cid:96)) as columns of matrix F ∈ RN ×L, we obtain: (cid:107)BF − H(cid:107)2
F = HF (cid:62)(F F (cid:62) + λIN )−1 = HG,
F + λ(cid:107)B(cid:107)2 (16)
B(cid:63) = arg min
B 6
where (cid:107).(cid:107)F is the Frobenius norm, and the L × N matrix G = F (cid:62)(F F (cid:62) + λIN )−1 depends only on the values of the basis functions at discrete time steps and can be obtained off-line for different input lenghts L. The result is an expression for VB with N D coefﬁcients, cheaper than H if N (cid:28) L. 3.2 Gradient backpropagation with continuous attention
The next proposition, based on Proposition 2 and proved in App. F, allows backpropagating over continuous entmax attention mechanisms. We deﬁne, for β ≥ 0, a generalized β-covariance, covp,β[φ(t), ψ(t)] = (cid:107)p(cid:107)β
β × (cid:0)E ˜pβ (cid:2)φ(t)ψ(t)(cid:62)(cid:3) − E ˜pβ [φ(t)] E ˜pβ [ψ(t)](cid:62)(cid:1) , (17) where ˜pβ is the β-escort distribution in (5). For β = 1, we have the usual covariance; for β = 0, we get a covariance taken w.r.t. a uniform density on the support of p, scaled by |supp(p)|.
Proposition 3. Let p = ˆpΩα [fθ] with fθ(t) = θ(cid:62)φ(t). The Jacobian of the α-entmax is:
Jρα (θ) =
∂ρα(θ)
∂θ
= covp,2−α(φ(t), ψ(t)). (18)
Note that in the ﬁnite case, (18) reduces to the expressions in (14) for softmax and sparsemax.
Example: Gaussian RBFs. As before, let S = RD, φ(t) = [t, vec(tt(cid:62))], and ψj(t) =
N (t; µj, Σj). For α = 1, we obtain closed-form expressions for the expectation (15) and the
Jacobian (18), for any D ∈ N: ˆpΩ[fθ] is a Gaussian, the expectation (15) is the integral of a product of Gaussians, and the covariance (18) involves ﬁrst- and second-order Gaussian moments. Pseudo-code for the case α = 1 is shown as Alg. 1. For α = 2, ˆpΩ[fθ] is a truncated paraboloid. In the 1D case, both (15) and (18) can be expressed in closed form in terms of the erf function. In the 2D case, we can reduce the problem to 1D integration using the change of variable formula and working with polar coordinates. See App. G for details.
We use the facts above in the experimental section (§4), where we experiment with continuous variants of softmax and sparsemax attentions in natural language processing and vision applications. 4 Experiments
As proof of concept, we test our continuous attention mechanisms on three tasks: document classiﬁ-cation, machine translation, and visual question answering (more experimental details in App. H).
Document classiﬁcation. Although textual data is fundamentally discrete, modeling long docu-ments as a continuous signal may be advantageous, due to smoothness and independence of length.
To test this hypothesis, we use the IMDB movie review dataset [29], whose inputs are documents (280 words on average) and outputs are sentiment labels (positive/negative). Our baseline is a biLSTM with discrete attention. For our continuous attention models, we normalize the document length L into the unit interval [0, 1], and use f (t) = − (t − µ)2/2σ2 as the score function, leading to a 1D Gaussian (α = 1) or truncated parabola (α = 2) as the attention density. We compare three attention variants: discrete attention with softmax [14] and sparsemax [6]; continuous attention, where a CNN and max-pooling yield a document representation v from which we compute µ = sigmoid(w(cid:62) 1 v) and
σ2 = softplus(w(cid:62) 2 v); and combined attention, which obtains p ∈ (cid:52)L from discrete attention, computes µ = Ep[(cid:96)/L] and σ2 = Ep[((cid:96)/L)2] − µ2, applies the continuous attention, and sums the two context vectors (this model has the same number of parameters as the discrete attention baseline).
Table 1 shows accuracies for different numbers N of Gaussian RBFs. The accuracies of the individual models are similar, suggesting that continuous attention is as effective as its discrete counterpart, despite having fewer basis functions than words, i.e., N (cid:28) L. Among the continuous variants, the sparsemax outperforms the softmax, except for N = 64. We also see that a large N is not necessary to obtain good results, which is encouraging for tasks with long sequences. Finally, combining discrete and continuous sparsemax produced the best results, without increasing the number of parameters.
Machine translation. We use the De→En IWSLT 2017 dataset [30], and a biLSTM model with discrete softmax attention as a baseline. For the continuous attention models, we use the combined 7
Table 1: Results on IMDB in terms of accuracy (%). For the continuous attentions, we used
N ∈ {32, 64, 128} Gaussian RBFs N (t, ˜µ, ˜σ2), with ˜µ linearly spaced in [0, 1] and ˜σ ∈ {.1, .5}.
ATTENTION
¯L ≈ 280
Discrete softmax
Discrete sparsemax 90.78 90.58
ATTENTION
N = 32 N = 64 N = 128
Continuous softmax
Continuous sparsemax
Disc. + Cont. softmax
Disc. + Cont. sparsemax 90.20 90.52 90.98 91.10 90.68 89.63 90.69 91.18 90.52 90.90 89.62 90.98
Figure 2: Attention maps in machine translation: discrete (left), continuous softmax (middle), and continuous sparsemax (right), for a sentence in the De-En IWSLT17 validation set. In the rightmost plot, the selected words are the ones with positive density. In the test set, these models attained BLEU scores of 23.92 (discrete), 24.00 (continuous softmax), and 24.25 (continuous sparsemax). attention setting described above, with 30 Gaussian RBFs and ˜µ linearly spaced in [0, 1] and ˜σ ∈
{.03, .1, .3}. The results (caption of Fig. 2) show a slight beneﬁt in the combined attention over discrete attention only, without any additional parameters. Fig. 2 shows heatmaps for the different attention mechanisms on a De→En sentence. The continuous mechanism tends to have attention means close to the diagonal, adjusting the variances based on alignment conﬁdence or when a larger context is needed (e.g., a peaked density for the target word “sea”, and a ﬂat one for “of”).
Visual QA. Finally, we report experiments with 2D continuous attention on visual question an-swering, using the VQA-v2 dataset [31] and a modular co-attention network as a baseline [32].4
The discrete attention model attends over a 14×14 grid.5 For continuous attention, we normalize the image size into the unit square [0, 1]2. We ﬁt a 2D Gaussian (α = 1) or truncated paraboloid (α = 2) as the attention density; both correspond to f (t) = − 1 2 (t − µ)(cid:62)Σ−1(t − µ), with Σ (cid:31) 0. We use the mean and variance according to the discrete attention probabilities and obtain µ and Σ with moment matching. We use N = 100 (cid:28) 142 Gaussian RBFs, with ˜µ linearly spaced in [0, 1]2 and
˜Σ = 0.001 · I. Overall, the number of neural network parameters is the same as in discrete attention.
The results in Table 2 show similar accuracies for all attention models, with a slight advantage for continuous softmax. Figure 3 shows an example (see App. H for more examples and some failure cases): in the baseline model, the discrete attention is too scattered, possibly mistaking the lamp with a TV screen. The continuous attention models focus on the right region and answer the question correctly, with continuous sparsemax enclosing all the relevant information in its supporting ellipse. 5