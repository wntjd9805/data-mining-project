Abstract
SATNet is an award-winning MAXSAT solver that can be used to infer logical rules and integrated as a differentiable layer in a deep neural network [1]. It had been shown to solve Sudoku puzzles visually from examples of puzzle digit images, and was heralded as an impressive achievement towards the longstanding
AI goal of combining pattern recognition with logical reasoning. In this paper, we clarify SATNet’s capabilities by showing that in the absence of intermediate labels that identify individual Sudoku digit images with their logical representations,
SATNet completely fails at visual Sudoku (0% test accuracy). More generally, the failure can be pinpointed to its inability to learn to assign symbols to perceptual phenomena, also known as the symbol grounding problem [2], which has long been thought to be a prerequisite for intelligent agents to perform real-world logical reasoning. We propose an MNIST based test as an easy instance of the symbol grounding problem that can serve as a sanity check for differentiable symbolic solvers in general. Naive applications of SATNet on this test lead to performance worse than that of models without logical reasoning capabilities. We report on the causes of SATNet’s failure and how to prevent them. 1

Introduction
Machine learning systems have become increasingly capable at a wide range of tasks, with neural network based models outperforming humans at tasks like object recognition [3], speech recognition
[4, 5], the game of Go [6, 7], Atari videogames [8, 9], and more. Nonetheless, the success of deep learning comes with signiﬁcant caveats: neural networks require immense amounts of labeled data for training, can be easily tricked by tiny input perturbations or spurious correlations, and succumb to brittle generalization when tested on data that deviate ever so modestly from the training distribution.
Critics point to these caveats as evidence that deep learning, in its current incarnation, is really just performing a sophisticated type of pattern matching, the likes of which can only ever constitute intelligence in narrow, circumscribed domains [10, 11].
By comparison, human intelligence can be applied more generally. This has been argued to be a result of two distinct modes of cognition: System 1 and System 2 [12, 13]. System 1 happens quickly and without conscious effort, for example comparing the size of objects or locating the general source of a sound. On the other hand, System 2 involves slow and deliberate attention, for example solving for a complicated arithmetic equation or checking that an argument is logical. Current machine learning systems have been likened to System 1 [14], because System 1 mostly involves the use of associative 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
memory, and is highly susceptible to cognitive biases and sensory illusions. Symbolic AI algorithms that are based on logic and search more closely resemble System 2.
To achieve robust human-level AI that can solve non-trivial cognitive tasks, it is crucial to combine both System 1 like pattern recognition and System 2 like logical reasoning capabilities in a seamless end-to-end learning fashion. This is because in many practical problems of interest, it is difﬁcult and expensive to collect intermediate labels to train speciﬁc machine learning sub-components. For example, it appears infeasible to build a ‘danger’ classiﬁer for a self-driving car, where every possible dangerous scenario is pre-determined and categorized beforehand. Researchers are thus far able to combine both capabilities in a single AI system, but not train them end-to-end. Famously, OpenAI’s very impressive achievement of controlling a robotic hand to solve a Rubik’s cube required the separate use of a machine learning system to perform the dexterous manipulation and a discrete solver to decide the side of the cube that should be turned [15].
Attempts to bridge the two capabilities seamlessly belong to one of three approaches. The ﬁrst involves augmenting deep learning models with soft logic operators [16–22] or combinatorial solving modules [23–27]. However, this approach typically requires the programmer to pre-specify intricate logical structures according to the problem domain. Moreover, these logical components are ﬁxed and not amenable to learning. The second approach uses sub-symbolic reasoning techniques like
Recurrent Relation Networks to implicitly pick up on logical structures within the problem [28–30].
This approach improves on the ﬁrst by learning the logical structure implicitly by optimization, but nevertheless also necessitates careful feature engineering. The third approach is the ﬁeld of inductive logic programming (ILP), which starts from a traditional symbolic AI model like a knowledge base, and adds learning capabilities to it [31–34]. Unfortunately, ILP is limited to symbolic inputs and outputs, unlike deep neural networks.
Against the backdrop of such approaches, SATNet [1] promised to integrate “logical structures within deep learning” with a differentiable MAXSAT solver that can infer logical rules and be used as a neural network layer. SATNet claimed to have solved problems that were “impossible for traditional deep learning methods and existing logical learning methods to reliably learn without any prior knowledge,” most notably solving a Sudoku puzzle visually from images of puzzle digits, and was awarded with a Best Paper Honorable Mention at 2019’s International Conference on Machine
Learning.
Based on SATNet’s success, one might think that enabling end-to-end gradient-based optimiza-tion (i.e. making every component in a system differentiable) is sufﬁcient for end-to-end learning (i.e. learning without intermediate supervision signals). However, deﬁning gradients for an objective does not, on its own, result in successful learning outcomes, as exempliﬁed by the history of deep learning. Successful training of architectures with hundreds of layers, where gradients are trivially well deﬁned, is highly non-trivial and requires careful initialization, batch normalization, adaptive learning rates, etc. Additionally, without an appropriate inductive bias (like the rules of the game), learning to solve complex problems like visual Sudoku from relatively few samples is extraordinarily challenging. It is unlikely that end-to-end gradient-based optimization by itself will, in general, result in models that generalize well.
Thus, SATNet’s claim to have solved the end-to-end learning problem of visual Sudoku “in a min-imally supervised fashion” should be revisited. Can SATNet learn to assign logical variables (symbols) to images of digits (perceptual phenomena) without explicit supervision of this map-ping? This is also known as the symbol grounding problem [2], which has long been thought to be a prerequisite for intelligent agents to perform real-world logical reasoning. If answered in the afﬁrmative, SATNet would have marked a revolutionary leap forward for the whole ﬁeld of AI, by virtue of the difﬁculty of the symbol grounding problem in visual Sudoku.
The general complexity of the symbol grounding problem embedded in end-to-end learning should not be underestimated. Figure 1 directly exempliﬁes the difﬁculty of the symbol grounding problem for both human and artiﬁcial intelligence. Common measures of abstract reasoning in artiﬁcial intelligence such as DeepMind’s PGM work similarly to Raven’s Progressive Matrices (a test for human intelligence), where predicting what comes next involves determining the hidden attributes (symbols) in what has been presented (perceptual phenomena), and inferring the pattern from them
[11, 35–37]. Once given the hidden attributes, it is trivial for a human or a combinatorial solver to infer the pattern [35]. However, jointly inferring the hidden attributes together with the pattern proves to be a challenging cognitive task in general. 2
Figure 1: A challenging Raven’s Matrix puzzle that exempliﬁes a difﬁcult instance of the symbol grounding problem. We invite the reader to attempt the puzzle for themselves on the left hand side of the ﬁgure ﬁrst, before looking at the annotations on the right hand side. Once the given images have been decoded to an appropriate symbolic representation, it is straightforward for a discrete solver or a human to solve it. For a full explanation of the solution, please see Appendix Section A. 1.1 Our Contribution
In this paper, our principal contribution is a re-assessment of SATNet that clariﬁes the extent of its capabilities and a discussion of practical solutions that will help future researchers train SATNet layers in deep networks.
First, we observed from the SATNet authors’ open-source code that intermediate labels are leaked in the SATNet training process for visual Sudoku. The leaked labels essentially result in a two-step training process for SATNet, where it ﬁrst uses the leaked labels to train a digit classiﬁer, and then uses the symbolic representations of the digits to solve for the Sudoku puzzle. After removing the intermediate labels, SATNet was observed to completely fail at visual Sudoku (0% test accuracy).
If intermediate labels are available, it is possible to separately pre-train a digit classiﬁer and then use SATNet, independent of a deep network, to solve for the puzzle. This might even be preferable, given our ﬁnding that SATNet fails in 8 out of 10 random seeds despite access to the labels, which is evidence that SATNet struggles to learn to ground the Sudoku digits into their symbolic representation.
To be clear, the label leakage did not affect SATNet in the non-visual case, and its success on purely symbolic inputs and outputs nonetheless marks progress in ILP, but does not ﬁx the ﬁeld’s persisting deﬁciency in dealing with perceptual input.
While solving difﬁcult instances of the symbol grounding problem like visual Sudoku or PGM might be beyond the reach of SATNet, we found that SATNet also cannot solve easy instances, unless properly conﬁgured. We devised a test called the MNIST mapping problem, whose solution requires merely digit classiﬁcation (a simple problem for neural networks) and learning a bijective mapping between logical variables (a simple problem for discrete solvers). This test serves as an easy instance of the symbol grounding problem, and is suitable as a sanity test not just for SATNet, but other prospective differentiable symbolic solvers. Even on a simple test like this, a naive application of
SATNet can cause it to perform worse than models without logical reasoning capabilities.
Our work identiﬁes several factors that affect the learning dynamics of SATNet and provides practical suggestions for conﬁguring SATNet to enable successful training. We reveal surprising complexities that are unique to SATNet and break standard deep learning norms. For example, using different 3
learning rates for different layers in neural networks is not a common practice, since the use of Adam usually sufﬁces. But for the case of SATNet, even when Adam is used, the backbone layer has to learn at a slower rate than the SATNet layer for successful training to occur. Surprisingly, we found that unconditionally increasing the number of auxiliary variables does not increase the expressivity of the model, but instead leads to a complete failure in learning. Further adjusting the choice of optimizer and neural architecture led to statistically signiﬁcant improvements, culminating in near perfect test accuracy (99%).
The rest of the paper is organized as follows: Section 2 reviews the relevant technical background for SATNet and visual Sudoku. Section 3 examines the subtle nature of the label leakage in the original SATNet paper and its ramiﬁcations. Section 4 describes the MNIST mapping problem, and investigates optimal SATNet conﬁgurations for this simple MNIST-based test. Finally, we conclude in Section 5. 2