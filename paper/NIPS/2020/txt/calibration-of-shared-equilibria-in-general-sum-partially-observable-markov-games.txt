Abstract
Training multi-agent systems (MAS) to achieve realistic equilibria gives us a useful tool to understand and model real-world systems. We consider a general sum partially observable Markov game where agents of different types share a single policy network, conditioned on agent-speciﬁc information. This paper aims at i) formally understanding equilibria reached by such agents, and ii) matching emergent phenomena of such equilibria to real-world targets. Parameter sharing with decentralized execution has been introduced as an efﬁcient way to train multiple agents using a single policy network. However, the nature of resulting equilibria reached by such agents has not been yet studied: we introduce the novel concept of Shared equilibrium as a symmetric pure Nash equilibrium of a certain
Functional Form Game (FFG) and prove convergence to the latter for a certain class of games using self-play. In addition, it is important that such equilibria satisfy certain constraints so that MAS are calibrated to real world data for practical use: we solve this problem by introducing a novel dual-Reinforcement Learning based approach that ﬁts emergent behaviors of agents in a Shared equilibrium to externally-speciﬁed targets, and apply our methods to a n-player market example.
We do so by calibrating parameters governing distributions of agent types rather than individual agents, which allows both behavior differentiation among agents and coherent scaling of the shared policy network to multiple agents. 1

Introduction
Multi-agent learning in partially observable settings is a challenging task. When all agents have the same action and observation spaces, the work of [9, 11] has shown that using a single shared policy network across all agents represents an efﬁcient training mechanism. This network takes as input the individual agent observations and outputs individual agent actions, hence the terminology decentralized execution. The network is trained by collecting all n agent experiences simultaneously and treating them as distinct sequences of local observations, actions and rewards experienced by the shared policy. Since agents may have different observations at a given point in time, sharing a network still allows different actions across agents. It has also been observed in these works or in
[15] that one can include in the agents’ individual observations some agent-speciﬁc information such as the agent index to further differentiate agents when using the shared policy, thus allowing a certain form of heterogeneity among agents.
This brings the natural question: from a game theoretic standpoint, what is the nature of potential equilibria learnt by agents using such a shared policy? We show here that such equilibria are symmetric pure Nash equilibria of a higher level game on the set of stochastic policies, which we call
Shared equilibria. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The second question that follows from this new concept is then how can we constrain Shared equilibria so that they match speciﬁc externally-speciﬁed targets? The latter is referred to as calibration, where we calibrate input parameters of the multi-agent system (MAS) so as to match externally-speciﬁed calibration targets, typically coming from real-world observations on the emergent behaviors of agents and groups of agents. For example, MAS modeling behaviors of people in a city may require that agents in equilibria take the subway no more than some number of times a day in average.
Constraints such as those previously described can be achieved by having agents of different nature, or types, and optimally balancing those types so as to match the desired targets on the emergent behavior of agents. For example, we may want to optimally balance people living in the suburbs vs. living inside a city so as to match the constraint on taking the subway. Even then, repeating the steps (i) pick a certain set of agent types, and (ii) train agents until equilibrium is reached and record the associated calibration loss, is prohibitively expensive.
We solve this problem by introducing a reinforcement learning (RL) agent (RL calibrator) whose goal is to optimally balance types of agents so as to match the calibration target, and crucially who learns jointly with RL agents learning a shared equilibrium, avoiding the issue related to repeating (i)-(ii). The result is CALSHEQ, a new dual-RL-based algorithm for calibration of shared equilibria to external targets. CALSHEQ further innovates by calibrating parameters governing distributions of agent types (called supertypes) rather than individual agents, allowing both behavior differentiation among agents and coherent scaling of the shared policy network to multiple agents.
Our contributions are (1) we introduce the concept of Shared equilibrium that answers the question on the nature of equilibria reached by agents of possibly different types using a shared policy, and prove convergence to such equilibria using self-play, under certain conditions on the nature of the game. (2) we introduce CALSHEQ, a novel dual-RL-based algorithm aimed at the calibration of shared equilibria to externally speciﬁed targets, that innovates by introducing a RL-based calibrator learning jointly with learning RL agents and optimally picking parameters governing distributions of agent types, and show through experiments that CALSHEQ outperforms a Bayesian optimization baseline.