Abstract
We address the question of repeatedly learning linear classiﬁers against agents who are strategically trying to game the deployed classiﬁers, and we use the
Stackelberg regret to measure the performance of our algorithms. First, we show that Stackelberg and external regret for the problem of strategic classiﬁcation are strongly incompatible: i.e., there exist worst-case scenarios, where any sequence of actions providing sublinear external regret might result in linear Stackelberg re-gret and vice versa. Second, we present a strategy-aware algorithm for minimizing the Stackelberg regret for which we prove nearly matching upper and lower regret bounds. Finally, we provide simulations to complement our theoretical analysis.
Our results advance the growing literature of learning from revealed preferences, which has so far focused on “smoother” assumptions from the perspective of the learner and the agents respectively. 1

Introduction
As Machine Learning (ML) algorithms become increasingly involved in real-life decision making, the agents that they interact with tend to be neither stochastic nor adversarial. Rather, they are strategic. For example, consider a college that wishes to deploy an ML algorithm to make admis-sions decisions. Student candidates might try to manipulate their test scores in an effort to fool the classiﬁer. Or think about email spammers who are trying to manipulate their emails in an effort to fool the ML classiﬁer and land in the non-spam inboxes. Importantly, in both examples the agents (students and spammers respectively) do not want to sabotage the classiﬁcation algorithm only for the sake of harming its performance. They merely want to game it for their own beneﬁt. And this is precisely what differentiates them from being fully adversarial.
Motivated by the problem of classifying spam emails, we focus on the problem of learning an un-known linear classiﬁer, when the training data come in an online fashion from strategic agents, who can alter their feature vectors to game the classiﬁer. We model the interplay between the learner and the strategic agents1 as a repeated Stackelberg game over T timesteps. In a repeated Stackelberg game, the learner (“leader”) commits to an action, and then, the agent (“follower”) best-responds to it, i.e., reports something that maximizes his underlying utility. The learner’s goal is to minimize her Stackelberg regret, which is the difference between her cumulative loss and the cumulative loss of her best-ﬁxed action in hindsight, had she given the agent the opportunity to best-respond to it.
Our Contributions.
• We study a general model of learning interaction in strategic classiﬁcation settings where the agents’ true datapoint remains hidden from the learner, the agents can misreport within a ball of radius δ of their true datapoint (termed δ-bounded, myopically rational (δ-BMR) agents), and the learner measures her performance using the binary loss. This model departs signiﬁcantly from the smooth utility and loss functions used so far for strategic classiﬁcation (Sec. 2). 1We refer to the learner as a female (she/her/hers) and to the agents as male (he/his/him). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
• We prove that in strategic classiﬁcation settings against δ-BMR agents simultaneously achieving sublinear external and Stackelberg regret is in general impossible (strong incompatibility) (i.e., application of standard no-external regret algorithms might be unhelpful (Sec. 3)).
• Taking advantage of the structure of the responses of δ-BMR agents while working in the dual space of the learner, we propose an adaptive discretization algorithm (GRINDER), which uses access to an oracle. GRINDER’s novelty is that it assumes no stochasticity for the adaptive discretization (Sec. 4).
• We prove that the regret guarantees of GRINDER remain unchanged in order even when the learner is given access to a noisy oracle, accommodating more settings in practice (Sec. 4).
• We prove nearly matching lower bounds for strategic classiﬁcation against δ-BMR agents (Sec. 5).
• We provide simulations implementing GRINDER both for continuous and discrete action spaces, and using both an accurate and an approximation oracle (Sec. 4.1).
Our Techniques.
• In order to prove the incompatibility results of the regret notions in strategic classiﬁcation, we present a formal framework, which may be of independent interest.
• To overcome the non-smooth utility and loss functions, we work on the dual space, which pro-vides information about various regions of the learner’s action space, despite never observing the agent’s true datapoint. These regions (polytopes) relate to the partitions that GRINDER creates.
• To deal with the learner’s action space being continuous (i.e., containing inﬁnite actions), we use the fact that all actions within a polytope share the same history of estimated losses. So, passing information down to a recently partitioned polytope becomes a simple volume reweighting.
• To account for all the actions in the continuous action space, we present a formulation of the standard EXP3 algorithm that takes advantage of the polytope partitioning process.
• For bounding the variance of our polytope-based loss estimator, we develop a polytope-based variant of a well-known graph-theoretic lemma ([1, Lem. 5]), which has been crucial in the analysis of online learning settings with feedback graphs. Such a variant is mandatory, since direct application of [1, Lem. 5] in settings with continuous action spaces yields vacuous2 regret.
• We develop a generalization of standard techniques for proving regret lower bounds in strategic settings, where the datapoints that the agents report change in response to the learner’s actions.