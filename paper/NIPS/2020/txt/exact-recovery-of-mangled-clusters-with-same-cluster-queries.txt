Abstract
We study the cluster recovery problem in the semi-supervised active clustering framework. Given a ﬁnite set of input points, and an oracle revealing whether any two points lie in the same cluster, our goal is to recover all clusters exactly using as few queries as possible. To this end, we relax the spherical k-means cluster assumption of Ashtiani et al. to allow for arbitrary ellipsoidal clusters with mar-gin. This removes the assumption that the clustering is center-based (i.e., deﬁned through an optimization problem), and includes all those cases where spherical clusters are individually transformed by any combination of rotations, axis scalings, and point deletions. We show that, even in this much more general setting, it is still possible to recover the latent clustering exactly using a number of queries that scales only logarithmically with the number of input points. More precisely, we design an algorithm that, given n points to be partitioned into k clusters, uses
O(k3 ln k ln n) oracle queries and (cid:101)O(kn + k3) time to recover the clustering with zero misclassiﬁcation error. The O(·) notation hides an exponential dependence on the dimensionality of the clusters, which we show to be necessary thus char-acterizing the query complexity of the problem. Our algorithm is simple, easy to implement, and can also learn the clusters using low-stretch separators, a class of ellipsoids with additional theoretical guarantees. Experiments on large synthetic datasets conﬁrm that we can reconstruct clusterings exactly and efﬁciently. 1

Introduction
Clustering is a central problem of unsupervised learning with a wide range of applications in machine learning and data science. The goal of clustering is to partition a set of points in different groups, so that similar points are assigned to the same group and dissimilar points are assigned to different groups. A basic formulation is the k-clustering problem, in which the input points must be partitioned into k disjoint subsets. A typical example is center-based k-clustering, where the points lie in a metric space and one is interested in recovering k clusters that minimize the distance between the points and the cluster centers. Different variants of this problem, captured by the classic k-center, k-median, and k-means problems, have been extensively studied for several decades [1, 15, 26].
In this work we investigate the problem of recovering a latent clustering in the popular semi-supervised active clustering model of Ashtiani et al. [4]. In this model, we are given a set X of n input points in
∗Most of this work was done while the author was at the Sapienza University of Rome. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Rd and access to an oracle. The oracle answers same-cluster queries (SCQs) with respect to a ﬁxed but unknown k-clustering and tells whether any two given points in X belong to the same cluster or not.
The goal is to design efﬁcient algorithms that recover the latent clustering while asking as few oracle queries as possible. Because SCQ queries are natural in crowd-sourcing systems, this model has been extensively studied both in theory [2, 3, 13, 19, 28, 29, 30, 33, 39] and in practice [12, 16, 37, 38]
— see also [11] for other types of queries. In their work [4], Ashtiani et al. showed that by using
O(ln n) same-cluster queries one can recover the optimal k-means clustering of X in polynomial time, whereas doing so without the queries would be computationally hard. Unfortunately, [4] relies crucially on a strong separation assumption, called γ-margin condition: for every cluster C there must exist a sphere SC, centered in the centroid µC of C, such that C lies entirely inside SC and every point not in C is at distance (1 + γ)rC from µC, where rC is the radius of SC. Thus, although [4] achieves cluster recovery with O(ln n) queries, it does so only for a very narrow class of clusterings.
In this work we signiﬁcantly enlarge the class of clus-terings that can be efﬁciently recovered. We do so by relaxing the γ-margin condition of [4] in two ways (see Section 2 for a formal deﬁnition). First, we as-sume that every cluster C has γ-margin in some latent space, obtained by linearly transforming all points according to some unknown positive semi-deﬁnite matrix WC. This is equivalent to assume that C is bounded by an ellipsoid (possibly degenerate) rather than by a sphere (which corresponds to WC = I).
This is useful because in many real-world applica-tions the features are on different scales, and so each cluster tends to be distorted along speciﬁc directions causing ellipsoids to ﬁt the data better than spheres
[10, 22, 27, 31, 35]. Second, we allow the center of the ellipsoid to lie anywhere in space — in the cen-troid of C or anywhere else, even outside the convex hull of C. This includes as special cases clusterings in the latent space which are solutions to k-medians, k-centers, or one of their variants. It is not hard to see that this setting captures much more general and challenging scenarios. For example, the latent clus-tering can be an optimal solution of k-centers where some points have been adversarially deleted and the features adversarially rescaled before the input points are handed to us. In fact, the latent clustering need not be the solution to an optimization problem, and in particular need not be center-based: it can be liter-ally any clustering, as long as it respects the margin condition just described.
Figure 1: A toy instance on 105 points that we solve exactly with 105 queries, while the
SCQ-k-means algorithm of [4] is no better than random labeling.
Our main result is that, even in this signiﬁcantly more general setting, it is still possible to recover the latent clustering exactly, in polynomial time, and using only
O(ln n) same-cluster queries. The price to pay for this generality is an exponential dependence of the number of queries on the dimension d of the input space; this dependence is however unavoidable, as we show via rigorous lower bounds. Our algorithm is radically different from the one in [4], which we call SCQ-k-means here. The reason is that SCQ-k-means uses same-cluster queries to estimate the clusters’ centroids and ﬁnd their spherical boundaries via binary search. Under our more general setting, however, the clusters are not separated by spheres centered in their centroids, and thus SCQ-k-means fails, as shown in Figure 1 (see Section 8 for more experiments). Instead of binary search, we develop a geometric technique, based on careful tessellations of minimum-volume enclosing ellipsoids (MVEEs). The key idea is that MVEEs combine a low VC-dimension, which makes learning easy, with a small volume, which can be decomposed in easily classiﬁable elements.
While MVEEs are not guaranteed to be consistent with the cluster samples, our results can be also proven using consistent ellipsoids that are close to the convex hull of the samples. This notion of low-stretch consistent ellipsoid is new, and may be interesting in its own right. 2
2 Preliminaries and deﬁnitions
All missing statements and proofs can be found in the supplementary material. The input to our problem is a triple (X, k, γ) where X ⊂ Rd is a set of n arbitrary points, k ≥ 2 is an integer, and
γ ∈ R>0 is the margin (see below). We assume there exists a latent clustering C = {C1, . . . , Ck} over the input set X, which we do not know and want to compute. To this end, we are given access to an oracle answering same-cluster queries: a query SCQ(x, x(cid:48)) is answered by +1 if x, x(cid:48) are in the same cluster of C, and by −1 otherwise. Our goal is to recover C while using as few queries as possible. Note that, given any subset S ⊆ X, with at most k|S| queries one can always learn the label (cluster) of each x ∈ S up to a relabeling of C, see [4].
It is immediate to see that if C is completely arbitrary, then no algorithm can reconstruct C with less than n queries. Here, we assume some structure by requiring each cluster to satisfy a certain margin condition, as follows. Let W ∈ Rd×d be some positive semideﬁnite matrix (possibly different x(cid:62)W x, which in turn induces the for each cluster). Then W induces the seminorm (cid:107)x(cid:107)W = pseudo-metric dW (x, y) = (cid:107)x − y(cid:107)W . The same notation applies to any other PSD matrix, and when the matrix is clear from the context, we drop the subscript and write simply d(·, ·). The margin condition that we assume is the following:
Deﬁnition 1 (Clustering margin). A cluster C has margin γ > 0 if there exist a PSD matrix
W = W (C) and a point c ∈ Rd such that for all y /∈ C and all x ∈ C we have dW (y, c) >
√ 1 + γ dW (x, c). If this holds for all clusters, then we say that the clustering C has margin γ.
√
This is our only assumption. In particular, we do not assume the cluster sizes are balanced, or that
C is the solution to an optimization problem, or that points in a cluster C are closer to the center of
C than to the centers of other clusters. Note that the matrices W and the points c are unknown to us. The spherical k-means setting of [4] corresponds to the special case where for every C we have
W = rI for some r = r(C) > 0 and c = µ(C) = 1
|C| x∈C x. (cid:80)
We denote a clustering returned by our algorithm by (cid:98)C = { (cid:98)C1, . . . , (cid:98)Ck}. The quality of (cid:98)C is measured by the disagreement with C under the best possible relabeling of the clusters, that is, (cid:52)( (cid:98)C, C) = minσ∈Sk i=1 |C1(cid:52) (cid:98)Cσ(i)|, where Sk is the set of all permutations of [k]. Our goal is to minimize (cid:52)( (cid:98)C, C) using as few queries as possible. In particular, we characterize the query complexity of exact reconstruction, corresponding to (cid:52)( (cid:98)C, C) = 0. The rank of a cluster C, denoted by rank(C), is the rank of the subspace spanned by its points. (cid:80)k 1 2n 3 Our contribution
Our main contribution is an efﬁcient active clustering algorithm, named RECUR, to recover the latent clustering exactly. We show the following.
Theorem 1. Consider any instance (X, k, γ) whose latent clustering C has margin γ. Let n = |X|, (cid:1)r(cid:9). Given let r ≤ d be the maximum rank of a cluster in C, and let f (r, γ) = max (cid:8)2r, O(cid:0) r (X, k, γ), RECUR with probability 1 outputs C (up to a relabeling), and with high probability runs in time O((k ln n)(n + k2 ln k)) using O(cid:0)(k ln n) (k2d2 ln k + f (r, γ))(cid:1) same-cluster queries.
γ ln r
γ
More in general, RECUR clusters correctly (1 − ε)n points using O(cid:0)(k ln 1/ε) (k2d2 ln k + f (r, γ))(cid:1) queries in expectation. Note that the query complexity depends on r rather than on d, which is desirable as real-world data often exhibits a low rank (i.e., every point can be expressed as a linear combination of at most r other points in the same cluster, for some r (cid:28) d). In addition, unlike the algorithm of [4], which is Monte Carlo and thus can fail, RECUR is Las Vegas: it returns the correct clustering with probability 1, and the randomness is only over the number of queries and the running time. Moreover, RECUR is simple to understand and easy to implement. It works by recovering a constant fraction of some cluster at each round, as follows (see Section 5 and Section 6): 1. Sampling. We draw points uniformly at random from X until, for some cluster C, we obtain k |X|, and that, by a sample SC of size (cid:39) d2. We can show that with good probability |C| (cid:39) 1 standard PAC bounds, any ellipsoid E containing SC contains at least half of C. 2. Computing the MVEE. We compute the MVEE (minimum-volume enclosing ellipsoid) E =
EJ(SC) of SC. As said, by PAC bounds, E contains at least half of C. If we were lucky, E 3
would not contain any point from other clusters, and E ∩ X would be our large subset of C.
Unfortunately, E can contain an arbitrarily large number of points from X \ C. Our goal is to ﬁnd them and recover C ∩ E. 3. Tessellating the MVEE. To recover C ∩ E, we partition E into roughly (d/γ)d hyperrectangles, each one with the property of being monochromatic: its points are either all in C or all in X \ C.
Thanks to this special tessellation, with roughly (d/γ)d queries we can ﬁnd all hyperrectangles containing only points of C, and thus compute C ∩ E.
Our second contribution is to show a family of instances where every algorithm needs roughly (1/γ)r same-cluster queries to return the correct clustering. This holds even if the algorithm is allowed to fail with constant probability. Together with Theorem 1, this gives an approximate characterization of the query complexity of the problem as a function of γ and r. That is, for ellipsoidal clusters, a margin of γ is necessary and sufﬁcient to achieve a query complexity that grows roughly as (1/γ)r.
This lower bound also implies that our algorithm is nearly optimal, even compared to algorithms that can fail. The result is given formally in Section 7.
Our ﬁnal contribution is a set of experiments on large synthetic datasets. They show that our algorithm
RECUR achieves exact cluster reconstruction efﬁciently, see Section 8. 4