Abstract
Neural architecture search (NAS) relies on a good controller to generate better architectures or predict the accuracy of given architectures. However, training the controller requires both abundant and high-quality pairs of architectures and their accuracy, while it is costly to evaluate an architecture and obtain its accuracy. In this paper, we propose SemiNAS, a semi-supervised NAS approach that leverages numerous unlabeled architectures (without evaluation and thus nearly no cost).
Speciﬁcally, SemiNAS 1) trains an initial accuracy predictor with a small set of architecture-accuracy data pairs; 2) uses the trained accuracy predictor to predict the accuracy of large amount of architectures (without evaluation); and 3) adds the generated data pairs to the original data to further improve the predictor. The trained accuracy predictor can be applied to various NAS algorithms by predicting the accuracy of candidate architectures for them. SemiNAS has two advantages: 1) It reduces the computational cost under the same accuracy guarantee. On
NASBench-101 benchmark dataset, it achieves comparable accuracy with gradient-based method while using only 1/7 architecture-accuracy pairs. 2) It achieves higher accuracy under the same computational cost. It achieves 94.02% test accuracy on NASBench-101, outperforming all the baselines when using the same number of architectures. On ImageNet, it achieves 23.5% top-1 error rate (under 600M
FLOPS constraint) using 4 GPU-days for search. We further apply it to LJSpeech text to speech task and it achieves 97% intelligibility rate in the low-resource setting and 15% test error rate in the robustness setting, with 9%, 7% improvements over the baseline respectively. 1

Introduction
Neural architecture search (NAS) for automatic architecture design has been successfully applied in several tasks including image classiﬁcation and language modeling [42, 26, 6]. NAS typically contains two components, a controller (also called generator) that controls the generation of new architectures, and an evaluator that trains candidate architectures and evaluates their accuracy2. The controller learns to generate relatively better architectures via a variety of techniques (e.g., reinforcement learning [41, 42], evolution [20], gradient optimization [15, 17], Bayesian optimization [39]), and plays an important role in NAS [41, 42, 18, 20, 17, 15, 39]. To ensure the performance of the controller, a large number of high-quality pairs of architectures and their corresponding accuracy are required as the training data.
∗The work was done when the ﬁrst author was an intern at Microsoft Research Asia. 2Although a variety of metrics including accuracy, model size, and inference speed have been used as search criterion, the accuracy of an architecture is the most important and costly one, and other metrics can be easily calculated with almost zero computation cost. Therefore, we focus on accuracy in this work. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
However, collecting such architecture-accuracy pairs is expensive, since it is costly for the evaluator to train each architecture to accurately get its accuracy, which incurs the highest computational cost in
NAS. Popular methods usually consume hundreds to thousands of GPU days to discover eventually good architectures [41, 20, 17]. To address this problem, one-shot NAS [2, 18, 15, 35] uses a supernet to include all candidate architectures via weight sharing and trains the supernet to reduce the training time. While greatly reducing the computational cost, the quality of the training data (architectures and their corresponding accuracy) for the controller is degraded [24], and thus these approaches suffer from accuracy decline on downstream tasks.
In various scenarios with limited labeled training data, semi-supervised learning [40] is a popular approach to leverage unlabeled data to boost the training accuracy. In the scenario of NAS, unlabeled architectures can be obtained through random generation, mutation [20], or simply going through the whole search space [32], which incur nearly zero additional cost. Inspired by semi-supervised learning, in this paper, we propose SemiNAS, a semi-supervised approach for NAS that leverages a large number of unlabeled architectures. Speciﬁcally, SemiNAS 1) trains an initial accuracy predictor with a set of architecture-accuracy data pairs; 2) uses the trained accuracy predictor to predict the accuracy of a large number of unlabeled architectures; and 3) adds the generated architecture-accuracy pairs to the original data to further improve the accuracy predictor. The trained accuracy predictor can be incorporated to various NAS algorithms by predicting the accuracy of unseen architectures.
SemiNAS can be applied to many NAS algorithms. We take the neural architecture optimization (NAO) [17] algorithm as an example, since NAO has the following advantages: 1) it takes architecture-accuracy pairs as training data to train a accuracy predictor to predict the accuracy of architectures, which can directly beneﬁt from SemiNAS; 2) it supports both conventional methods which train each architecture from scratch [42, 20, 17] and one-shot methods which train a supernet with weight sharing [18, 17]; and 3) it is based on gradient optimization which has shown better effectiveness and efﬁciency. Although we implement SemiNAS on NAO, it is easy to be applied to other NAS methods, such as reinforcement learning based methods [42, 18] and evolutionary algorithm based methods [20].
SemiNAS shows advantages over both conventional NAS and one-shot NAS. Compared to conven-tional NAS, it can signiﬁcantly reduce computational cost while achieving similar accuracy, and achieve better accuracy with similar cost. Speciﬁcally, on NASBench-101 benchmark, SemiNAS achieves similar accuracy (93.89%) as gradient based methods [17] using only 1/7 architectures.
Meanwhile it achieves 94.02% mean test accuracy surpassing all the baselines when evaluating the same number of architectures (with the same computational cost). Compared to one-shot NAS,
SemiNAS achieves higher accuracy using similar computational cost. For image classiﬁcation, within 4 GPU days for search, we achieve 23.5% top-1 error rate on ImageNet under the mobile setting. For text to speech (TTS), using 4 GPU days for search, SemiNAS achieves 97% intelligibility rate in the low-resource setting and 15% sentence error rate in the robustness setting, which outperforms human-designed model by 9 and 7 points respectively. To the best of our knowledge, we are the
ﬁrst to develop NAS algorithms on text to speech (TTS) task. We carefully design the search space and search metric for TTS, and achieve signiﬁcant improvements compared to human-designed architectures. We believe that our designed search space and metric are helpful for future studies on
NAS for TTS. 2