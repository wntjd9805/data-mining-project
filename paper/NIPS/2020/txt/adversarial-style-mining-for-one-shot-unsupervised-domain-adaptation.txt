Abstract
We aim at the problem named One-Shot Unsupervised Domain Adaptation. Unlike traditional Unsupervised Domain Adaptation, it assumes that only one unlabeled target sample can be available when learning to adapt. This setting is realistic but more challenging, in which conventional adaptation approaches are prone to failure due to the scarce of unlabeled target data. To this end, we propose a novel
Adversarial Style Mining approach, which combines the style transfer module and task-speciﬁc module into an adversarial manner. Speciﬁcally, the style transfer module iteratively searches for harder stylized images around the one-shot target sample according to the current learning state, leading the task model to explore the potential styles that are difﬁcult to solve in the almost unseen target domain, thus boosting the adaptation performance in a data-scarce scenario. The adversarial learning framework makes the style transfer module and task-speciﬁc module beneﬁt each other during the competition. Extensive experiments on both cross-domain classiﬁcation and segmentation benchmarks verify that ASM achieves state-of-the-art adaptation performance under the challenging one-shot setting. 1

Introduction
Deep networks have signiﬁcantly improved the performance for a wide variety of machine-learning problems and applications [32, 29]. Nevertheless, these impressive gains usually come with a price of massive amounts of manual labeled data. A popular trend in the current research community is to resort to simulated data, such as computer-generated scenes [36, 37], so that unlimited amount of automatic annotation is made available. However, this learning paradigm suffers from the shift in data distributions between the real and simulated domains, which poses a signiﬁcant obstacle in adapting predictive models to the target task. The introduction of Domain Adaptation (DA) techniques aims to mitigate such performance drop when a trained agent encounters a different environment. By bridging the distribution gap between source and target domains, DA methods have shown their effect in many cross-domain tasks such as classiﬁcation [27, 18], segmentation [19, 22, 23] and detection [3]. Although much progress has been made for domain adaptation, most of the previous efforts assume the availability of enough amounts of unlabeled target-domain samples. However, such an assumption can not always hold since not only data labeling but also data collection itself might be challenging, if not impossible, for the target task. In this data-scarce scenario with a limited amount of unlabeled data from target domains, most previous DA strategies, such as distribution
∗Corresponding author (qd_gt@hust.edu.cn).
Part of the work is conducted when Ping is in UTS. The code is publicly available at https://github. com/RoyalVane/ASM. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Conventional DA methods achieve good performance in UDA task but are prone to failure under the one-shot setting. We propose ASM to deal with such challenging data-scarce scenario. alignment [12, 39], entropy minimization [41], or pseudo label generation [48, 45, 44], are all prone to failure. Consequently, design a speciﬁc algorithm for this realistic but more challenging learning scenario, i.e., one-shot unsupervised domain adaptation (OSUDA), becomes necessary.
Some recent works [7, 25, 43, 42] aiming to vanilla UDA problems, try to learn style distribution in target domains based on the given unlabeled target data. The learned style distribution is utilized to translate source domain data to make them with a similar “appearance”, e.g., lighting, texture, etc., as target domain data. The model trained on stylized source data can naturally, hopefully, generalize well to the target domain. However, there are a few drawbacks if directly apply those vanilla style transfer (ST) methods in One-Shot UDA settings. First, both the style transfer module and the classiﬁer could easily over-ﬁt due to the scarce of target domain data. With only one target sample, it is hard to learn from it to catch the actual style distribution in the target domain. Second, in previous works [2, 4, 24, 12], ST and DA are usually carried out in a sequential, decoupled manner, which makes it hard for ST and DA beneﬁt mutually to each other. That means, since the ST module can not get dynamic feedback from the classiﬁer, it might produce inappropriate stylized samples, which might be either too “hard” or too “easy” for adapting the model in the current state.
To overcome these drawbacks above, in this paper, we propose the Adversarial Style Mining (ASM) algorithm effective for OSUDA 2. As shown in Fig. 1, ASM is composed of a stylized image generator
G and a task-speciﬁc network M , e.g., FCN for segmentation task. We design G to generate arbitrary style from a sampling vector ε. We can change the style of the generated image by simply modifying
ε. Unlike previous style translation works [46, 14], our ε is initialized by the sole given target sample and will be updated according to the feedback from M so as to match the learning ability of M in a dynamic manner, while M is trained to segment or classify the stylized images from G correctly.
By this way, we construct G and M as an end-to-end adversarial regime. Speciﬁcally, ε and M are iteratively updated during the ASM training. On the one hand, G starts to generate stylized images from the initial ε and constantly searches for harder styles for the current M . On the other hand, M is trained based on these generated stylized images and returns its feedback, so ε can be adjusted appropriately. In such an adversarial paradigm, we can efﬁciently produce stylized images that boost the domain adaptation, thus guiding M to “see” more possible styles in the target domain beyond the solely given sample. Our main contributions are summarized as follows: (1) We present an adversarial style mining (ASM) method to solve One-Shot Unsupervised Domain
Adaptation (OSUDA) problems. ASM combines a style transfer module and a task-speciﬁc model into an adversarial manner, making them mutually beneﬁt to each other during the learning process.
ASM iteratively searches for new beneﬁcial stylized images beyond the one-shot target sample, thus boosting the adaptation performance in data-scarce scenario. (2) We propose a novel style transfer module, named Random AdaIN (RAIN), as a key component for achieving ASM. It makes the style searching a differentiable operation, hence enabling an end-to-end style searching using gradient back-propagation. (3) We evaluate ASM on both cross-domain classiﬁcation and cross-domain semantic segmentation in one-shot settings, showing that our proposed ASM consistently achieves superior performance over previous UDA and one-shot UDA approaches. 2