Abstract
Automated machine learning (AutoML) can produce complex model ensembles by stacking, bagging, and boosting many individual models like trees, deep networks, and nearest neighbor estimators. While highly accurate, the resulting predictors are large, slow, and opaque as compared to their constituents. To improve the deployment of AutoML on tabular data, we propose FAST-DAD to distill arbitrarily-complex ensemble predictors into individual models like boosted trees, random forests, and deep networks. At the heart of our approach is a data augmentation strategy based on Gibbs sampling from a self-attention pseudolikelihood estimator.
Across 30 datasets spanning regression and binary/multiclass classiﬁcation tasks,
FAST-DAD distillation produces signiﬁcantly better individual models than one obtains through standard training on the original data. Our individual distilled faster and more accurate than ensemble predictors produced models are over 10
ˆ by AutoML tools like H2O/AutoSklearn. 1

Introduction
Modern AutoML tools provide good out-of-the-box accuracy on diverse datasets. This is often achieved through extensive model ensembling [1–3]. While the resultant predictors may generalize well, they can be large, slow, opaque, and expensive to deploy.
Fig. 1 shows that the most accurate predictors can be 10,000 times slower than their constituent models.
Model distillation [4, 5] offers a way to compress the knowledge learnt by these complex models into simpler predictors with reduced inference-time and memory-usage that are also less opaque and easier to work with. In distillation, we train a simpler model (the student) to output similar predictions as those of a more complex model (the teacher). Here we use AutoML to create the most accurate possible teacher, typically an ensemble of many individual models via stacking, bagging, boosting, and weighted combinations [6]. Unfortunately, distillation typically comes with a sharp drop in accuracy.
˚Equal contribution.
Figure 1: Normalized test accuracy vs. speed of individual models and AutoML ensembles, averaged over all 30 datasets. TEACHER de-notes the performance of AutoGluon; H2O and autosklearn represent the respective AutoML tools.
GIB-1 indicates the results of FAST-DAD after 1 round of Gibbs sampling. BASE denotes the stu-dent model ﬁt on original data. GIB-1/BASE dots represent the model Selected (out of the 4 types) based on validation accuracy for each dataset. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our paper mitigates this drop via FAST-DAD, a technique to produce Fast-and-accurate models via
Distillation with Augmented Data. We apply FAST-DAD to large stack-ensemble predictors from
AutoGluon [1] to produce individual models that are over 10,000 faster than AutoGluon and over 10 faster, yet still more accurate, than ensemble predictors produced by H2O-AutoML [7] and
AutoSklearn [2].
ˆ
ˆ
Motivation. A key issue in distillation is that the quality of the student is largely determined by the amount of available training data. While standard distillation confers smoothing beneﬁts (where the teacher may provide higher-quality prediction targets to the student [5, 8]), it incurs a student-teacher statistical approximation-error of similar magnitude as when training directly on original labeled dataset. By increasing the amount of data available for distillation, one can improve the student’s approximation of the teacher and hence the student’s accuracy on test data (assuming that the teacher achieves superior generalization error than ﬁtting the student model directly to the original data). The extra data need not be labeled; one may use the teacher to label it. This enables the use of density estimation techniques to learn the distribution of the training data and draw samples of unlabeled data.
In fact, we need not even learn the full joint distribution but simply learn how to draw approximate samples from it. We show that the statistical error in these new samples can be traded off against the reduction in variance from ﬁtting the student to a larger dataset. Our resultant student models are almost as accurate as the teacher while being far more efﬁcient/lightweight.
The contributions of this paper are as follows: 1. We present model-agnostic distillation that works across many types of teacher/student mod-els and various supervised learning problems (binary/multiclass classiﬁcation, regression).
This is in contrast to problem and architecture-speciﬁc distillation techniques [4, 5, 9, 10]. 2. We introduce a maximum pseudolikelihood model for tabular data that uses self-attention across covariates to simultaneously learn all of their conditional distributions. 3. We propose Gibbs sampling based on these conditional estimates to efﬁciently augment the dataset used in distillation. Our approach avoids estimating multivariate features’ joint distribution, and enables control over sample-quality and diversity of the augmented dataset. 4. We report a comprehensive distillation benchmark for tabular data which studies 5 distillation strategies with 4 different types of student models over 30 regression/classiﬁcation datasets.
Although our techniques can be adapted to other modalities, we focus on tabular data which has been under-explored in distillation despite its ubiquity in practical applications. Compared to typical data tables, vision and language datasets have far larger sample-sizes and with easily available data; data augmentation is thus not as critical for distillation as it is in the tabular setting. 2