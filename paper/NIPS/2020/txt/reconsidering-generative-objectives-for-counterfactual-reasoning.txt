Abstract
There has been recent interest in exploring generative goals for counterfactual reasoning, e.g., individualized treatment effect (ITE) estimation. However, existing solutions often fail to address issues that are unique to causal inference, such as covariate balancing and counterfactual validation. As a step toward more ﬂexible, scalable and accurate ITE estimation, we present a novel generative Bayesian estimation framework that integrates representation learning, adversarial matching and causal estimation. By appealing to the Robinson decomposition, we derive a reformulated variational bound that explicitly targets the causal effect estimation rather than speciﬁc predictive goals. Our procedure acknowledges the uncertainties in representation and solves a Fenchel mini-max game to resolve the representation imbalance for better counterfactual generalization, justiﬁed by new theory.The latent variable formulation enables robustness to unobservable latent confounders, extending the scope of its applicability. The proposed approach is demonstrated via an extensive set of tests against competing solutions, both under various simulation setups and to real-world datasets, with encouraging results reported.

Introduction 1
Inferring the individualized treatment effects from observational data is a fundamental challenge shared by many decision-making application domains, including healthcare [23], advertising [15], and policy making [44], among others. Recent advances in machine learning have motivated new causal inference methodologies inspired by modern learning perspectives, such as representation learning, adversarial training, etc.
In this work we focus on the problem of causal estimation from observational data, which differs from standard supervised learning in fundamental ways [60]. First, only partial observation of the potential outcomes, the one corresponding to the assigned intervention, can be made. The lack of counterfactual labels prohibits direct validation of the estimated CE. Second, observational studies are susceptible to selection bias due to confounding. In particular, some variables obfuscate causation as they affect both treatment assignment and outcome [81], and they may be latent. Without a proper confounder compensation mechanism, causal estimation can face severe bias.
To resolve this difﬁculty, the classical statistics literature has mainly focused on sample-based adjust-ment strategies, namely matching and weighting. Matching pairs units that are similar with respect to particular matching criteria [74], forming basic elements of synthetic “randomized trials”; weighting reassigns importance weights to each sample unit to create a pseudo population of better balance
∗Contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[29, 46, 47]. Both approaches typically make the unconfoundedness assumption [65], assuming that there are no latent variables that affect both the outcome and the treatment assignment. To guard against model mis-speciﬁcation-induced failures [63], balancing weights are often used in conjunction with outcome regression models to achieve double robustness [72]. However, these classical solutions are constantly challenged by modern datasets, characterized by features such as high dimensionality
[12] and complex interactions [88], and they typically make the unconfoundedness assumption.
More recently, representation learning emerged as a new, promising alternative to approach covariate balance [48, 36]. Such schemes explicitly seek an intermediate (low-dimensional) representation that is both (i) predictive of the outcome [82]; and (ii) matched between treatment groups [34].
From a learning perspective, these two points serve to promote the generalization performance for counterfactual predictions [75]. On the ﬂip side, causal perspectives also motivate invariant feature representation learning under general machine learning setups [7].
Recent strides in generative modeling techniques, such as the variational auto-encoder (VAE) [39] and the generative adversarial network (GAN) [24], have equipped causal estimation with new learning principles. Rather than appealing to predictive goals [82], these schemes learn stochastic rules that mimic the data generating procedure, i.e., how to synthesize realistic counterfactuals based on observed data [87]. Such generative causal models typically relax model assumptions posited by standard causal estimation machinery, allowing black-box type inference using ﬂexible learners such as deep networks. Despite their reported strong empirical performance, questions remain: (i)
Confounding: Do we fully trust the observed confounders? (ii) Balancing: What if the covariates are unbalanced? (iii) Counterfactual validation: How to avoid over-ﬁtting?
Notably, in-depth discussions on (iii), causal validation procedure, has received attention in the literature only recently, despite its paramount importance [8, 83]. The promise of a fully automated causal estimation procedure has inspired many (unreliable) heuristic proxies [73] (e.g., plug-in surrogate or predictive loss) and principled evaluation strategies have only appeared quite recently.
While scholarly consensus on best practice is yet to be reached [20], prominent examples from this category include inﬂuence function based causal validation [3] and rank-preserving causal cross-validation [71]. Of particular interest is the Robinson residual decomposition employed by the
R-learner [56] and generalized causal forests [10], which construct a directly learnable objective.
Motivated by the preceding discussions, this work seeks a uniﬁed treatment that accommodates (i)-(iii). We revisit the generative perspective of causal modeling, and demonstrate how explicitly accounting for balancing and counterfactual validation helps to improve causal estimation.
In particular, we present a variational procedure, termed Balancing Variational Neural Inference of
Causal Effects (BV-NICE), to address the challenges of generative learning for causal estimation.
Our key contributions include: (i) repurposing variational inference as random feature representation learning scheme to facilitate causal estimation; (ii) reformulating the variational objective to better balance confounder representations between comparison groups; (iii) incorporating causal validation targets to scrutinize inferred causal effect. Our approach features direct modeling of causal effects, rather than the difference between the outcome models. It joints strength from distribution matching, representation learning and generative causal estimation, resulting a principled attempt that better addresses the challenges in counterfactual inference. To embrace a more holistic picture, we also cover related issues such as identiﬁability and establish border connections to the literature on causal discovery with the extended discussions found in our supplementary material (SM). 2 Preliminaries
Problem setup We consider the basic setup under the potential outcome framework [69, 33].
Assume a sample of n units, with unit i associated with a covariate X i ∈ Rp, a treatment indica-tor Ti ∈ {0, 1} and potential outcomes [Yi(0), Yi(1)] ∈ R2. The fundamental problem of causal inference [32] is that only the outcome associated with the prescribed treatment is observed, i.e.,
Yi (cid:44) Y (Ti) = TiYi(1) + (1 − Ti)Yi(0), known as the factual data. The individualized treatment effect (ITE) is deﬁned as the expected difference between outcome τ (x) (cid:44) E[Yi(1) − Yi(0)|X i = x], and our goal is to learn a generalizable model τ (x) that predicts the ITE given observed covariates x.
We often assume the decomposition τ (x) = µ1(x) − µ0(x), where µt(x) (cid:44) E[Y (t)|x], t ∈ {0, 1} are known as the outcome models. Another key concept in causal estimation is the propensity score (PS): e(x) (cid:44) p(T = 1|x), i.e., the conditional probability of receiving the treatment given x.
While the identiﬁability of causal effect can only be established in the average sense for observa-tional studies, under the assumptions of unconfoundedness:{Y (0), Y (1)} ⊥⊥ T |X, and positivity: 2
i=1. p(T |X, Y (0), Y (1)) ∈ (0, 1) [66], individualized predictions still hold promise. A typical predictive scheme minimizes the prediction loss for the factual observation, i.e., ˆµ = minµ{(cid:80) i(Yi −µt(X i))2}.
Alternatively, generative schemes seek to identify a data generation procedure pθ(x, t, y) that is consistent with factual observations Dn = {(xi, yi, ti)}n
Robinson residual decomposition Under unconfoundedness, it is easy to verify E [(cid:15)(T )|X, T ] = 0, where (cid:15)(T ) (cid:44) Y (T ) − (µ0(X) + T τ (X)) is known as the Robinson residual [64]. Denoting the conditional mean outcome as m(x) (cid:44) E[Y |x] = µ0(x) + e(x)τ (x), and we can rewrite Robinson residual as (cid:15)(T ) = Y (T ) − m(X) − (T − e(X))τ (X). Note that this decomposition holds for any outcome distribution, including binary outcomes. This directly motivates the R-learning [56] objective ˆτ = arg minτ {1/n (cid:80) i (yi − ˜m(xi) − (ti − ˜e(xi))τ (xi))2}, where ˜m(x) and ˜e(x) are estimated surrogates for the mean outcome and propensity score model. Recently, many have considered the of direct modeling of CE (τ ) through the R-decomposition [91, 92, 16, 61, 10], rather than indirectly through (µ0, µ1).
Variational inference A general learning principle is to maximize the expectation of the log-likelihood wrt observed data, i.e., (cid:96)(θ) := (cid:80) i log pθ(xi), which constitutes maximum likelihood estimation (MLE). For a latent variable model pθ(x, z), we consider x as an observation (i.e., data) and z as latent variable. The marginal likelihood pθ(x) = (cid:82) pθ(x, z) dz typically does not have a closed-form expression, and to avoid direct numerical estimation of pθ(x), variational inference (VI) instead optimizes a variational bound to the marginal log-likelihood log pθ(x) [14, 79]. The most popular choice is known as the Evidence Lower Bound (ELBO), given by
ELBO (cid:44) EZ∼qφ(z|x) (cid:20) log (cid:21) pθ(x, Z) qφ(Z|x)
≤ log pθ(x), (1) where qφ(z|x) is an approximation to the true posterior pθ(z|x) and the inequality is a result of Jensen’s inequality. This bound tightens as qφ(z|x) approaches the true posterior pθ(z|x).
For estimation, we seek parameters θ that maximize the ELBO, and the commensurately learned parameters φ are often used in a subsequent inference task with new data.
Adversarial distribution matching Consider the problem of matching a model distribution pG(x) to some true data distribution pd(x) presented as empirical samples, wrt some discrepancy measure, d(pd, pG). Typically, pG(x) is given in the form of a stochastic sampler. In the GAN framework, the discrepancy is ﬁrst estimated by maximizing an auxiliary variational functional V (pd, pG; D) :
P × P → R between distributions pd(x) and pG(x) satisfying d(pd, pG) = maxD V (pd, pG; D), where P is the space of probability distributions and V (pd, pG; D) is estimated using samples from the two distributions. Function D(x; ω), parameterized by ω and known as the critic function, is intended to maximally discriminate between samples of the two distributions. Subsequently, one seeks to match the generator distribution pG(x) to the unknown true distribution pd(x) by minimizing the estimated discrepancy, resulting in a minimax game between the critic and the generator: minG maxD V (pd, pG; D). 3 Balancing VI For Causal Estimation
Inspired by the above, we present BV-NICE, a model seeks to improve the current practice of generative learning of causal inference from the following perspectives: (a) automated feature representation learning that explicitly accounts covariate balance, (b) a built-in mechanism for automated model selection directly targets CE estimation accuracy, (c) acknowledging the uncertainty in the observed confounders by introduction of inferred latent variables.
We frame our construction under variational inference based on the following considerations:
• We treat covariate x as noisy proxies for the true, unobservable confounders (latent z)
• The (approximate) posterior acts as a representation encoder that encapsulates uncertainties
• Matching for the prior p(z) naturally regularizes for model generalization
Consider the following latent variable model pθ(x, y, t, z) = p(x|z)p(y|z, t)p(t|z)p(z) (Figure S1), where (x, y, t) are the observables, z is the (continuous) latent variable, and θ denotes the model parameters. In accordance with standard practice, we model discrete variables with multinomial logistic and continuous variables with Gaussian N (µ, σ2), where µ is a function of z and also possibly t depending on the context, with σ2 set to some prescribed value to avoid overﬁtting.
We parameterize stochastic encoders qφ(z|x, y, t) to infer unobserved confounders z. For ﬂexible inference, we model all functions with deep neural nets. Plugging into (1) gives us a tractable objective 3
for stochastic optimization (see Equation (4)). We relegate the speciﬁcs of our modeling choices in the subsections that follow, after revealing more causal insights embodied in our reformulation. 3.1 A unifying view for VI and R-learner
A key feature we seek to incorporate is to automatically favor solutions that more accurately describe causal effect based on the factual observations. Unlike a model-selection procedure, where candidates are screened in an ad hoc manner, we want our model to explore the parameter space, to identify the best candidates for causal descriptives as part of training. This precludes options such as meta-learners [43] and inﬂuence function based estimator [3], as they function as a causal estimator and cannot be efﬁciently trained in an end-to-end manner. We choose to work with the Robinson residual decomposition, and show how the resulting R-learner [56] relates to VI. This implies our variational framework automatically assumes the model selection property.
It is convenient to denote µt(z) (cid:44) µy(z, t), and the causal effect estimator τ (z) = µy(z, t = 1) − µy(z, t = 0). Under the R-learning framework, one models the mean outcome m(z) and τ (z) rather than (µ0(z), µ1(z)). It is easy to see these two modeling choices are related by (cid:26) µ0(z) = m(z) − e(z)τ (z), (cid:40) m(z) = e(z)µ1(z) + (1 − e(z))µ0(z)
⇒
µ1(z) = m(z) + (1 − e(z))τ (z). (2)
τ (z) = µ1(z) − µ0(z)
A key insight is given by the observation (cid:15)(z, t, y) = y − m(z) − (t − e(z))τ (z) = y − {tµ1(z) − (1 − t)µ0(z)}.
Note that the RHS is the residual error for prediction given (z, t). Consequently, (cid:96)R(z, y, t) = (cid:15)(z, t, y)2 = −2σ2 log pθ(y|z, t). Plugging this result back into the ELBO, and recalling that pθ(t|z) is essentially the propensity score model e(z), we obtain the following factorization (3)
ELBO(x, y, t|pθ, qφ) =
EZ∼qφ[log pθ(x|Z) (cid:125) (cid:124) (cid:123)(cid:122)
Optional
+ (cid:122) log pθ(y|Z, t) (cid:123)(cid:122) (cid:125) (cid:124)
R-loss
V-NICE (cid:125)(cid:124) (cid:123)
] − KL(qφ(z|x, y, t) (cid:107) p(z))
+ log pθ(t|Z) (cid:123)(cid:122) (cid:125) (cid:125)
PS-loss (cid:123)(cid:122)
KL-loss (cid:124) (cid:124) (4)
Since our primary goal is to model the causal effect τ , we discard the ﬁrst term related to the likelihood of x and treat the rest as our training target, which we term (cid:96)V-NICE. This choice is motivated by the fact that to correctly infer CE we only need the part of x that is predictive of (y, t) [82]. Modeling x indiscriminately, as practiced by existing generative causal models [50, 68], takes away representation capacity of z [30, 5], compromising our main objective.
Intuitively, (cid:96)V-NICE, our reformulated ELBO, is a combination of R-loss and propensity score loss, regularized by KL-divergence on the la-tents to encourage better generalization. Un-like its generative counterparts, our model is directly parameterized through causal triplet (τ (z), m(z), e(z)) to emphasize the causal per-spective and allowing structural constraints to be imposed [43]. V-NICE also approximately recovers the R-learner as σ2 → 0. By optimizing the triplet jointly, rather than a two stage procedure employed by R-learner, our triplet share the reﬁned representation learned. Our discussion also bridges R-learning and likelihood-based learning.
Figure 1: BV-NICE model architecture.
Beneﬁts of integrating the R-loss. A major difference in the construction of R-learner objective, relative to the standard two-learner setup, is that the propensity score is explicitly involved. This allows additional information to be leveraged in many practical settings. For example, a common scenario is that signiﬁcant lags can be expected between the application of a treatment and the observation of the outcome (e.g., when the target outcome is the patients’ recovery in one year time whether or not administrating a drug). In such scenarios, there will be data available with only confounder and treatment to reﬁne propensity score estimate, which in turn improves treatment effect estimation in R-learning, but can not be used for outcome modeling in the two learner setup. A similar argument holds when additional knowledge is known about the treatment assignment (e.g., when the data is a hybrid of observational and randomized trial). In the same spirit, R-learning also allows the use of data where the treatment information is missing, as they can still be used to improve the estimate of average outcome m(x). 4
3.2 Balancing VI for causal estimation
Our next goal is to establish a mechanism that enables covariate balance. Further denote qt(z) = (cid:82) qφ(z|x, t)pd(x|T = t) dx. To achieve better balance for subsequent causal estimation, one seeks to match the confounder distributions between treatment groups, i.e., q1 should be close to q0. To this end, we augment the original ELBO with a distribution discrepancy score D(q0 (cid:107) q1), resulting in (cid:96)BV-NICE (cid:44) (cid:96)V-NICE(pθ, qφ) − λD(q0 (cid:107) q1) (5) as our objective for balancing VI (BV-NICE), where λ > 0 speciﬁes the regularization strength.
Choice of discrepancy score While the marginal densities of q0 and q1 are intractable, it is relatively easy to acquire samples from them. This motivates leveraging adversarial distribution matching strategies to (indirectly) optimize the discrepancy through a mini-max game. Hence, we indirectly assess D(q0 (cid:107) q1) via the use of a critic function (the max step), and then update the model accordingly to reduce the discrepancy (the min-step). In this study, we appeal to the KL-divergence as our discrepancy measure, which can be recast in its Fenchel dual form as [18, 80]
DKL(q0 (cid:107) q1) = Eq0 [log q0 − log q1] = max
ν>0
{Eq0 [log ν] − Eq1 [ν] + 1}, (6) and note the maximizer ν∗ satisﬁes ν∗ = q p . This choice is motivated by the following considerations:
• Easy implementation relative to integral probability metric (IPM)-based schemes
• It also bounds generalization performance (Sec 3.3)
• This approach also encourages parameter sharing as the ELBO involves a KL term
Note that this choice is not restrictive, as practitioners are free to choose their favorite distribution matching schemes, such as Wasserstein [75, 6], MMD [25, 48], JSD [24, 87] or other f -divergence
[57, 78], that possess other appealing properties. See the SM for a more thorough discussion.
ˆDKL(q0 (cid:107) q1) = max
ψ
To implement KL-matching, we model log ν as a deep neural network ϑψ(z), as our critic function, where ψ denotes the network parameters. This gives the following neural estimator for the KL term2 (7)
In our case, the distributions are characterized by a neural sampler via the reparameterization trick, e.g., qφ(z|x) as Gφ(ξ, x), ξ ∼ p(ξ). Gradients of the sampler can be easily obtained by directly differentiating ˆDKL wrt φ. 3.3 Practical implementation
{EZ∼q0 [ϑψ(Z)] − EZ(cid:48)∼q1 [exp(ϑψ(Z (cid:48)))]}
Random feature encoder To enable ﬂexible encoding of latent features, we employ a neural sampler rφ(cid:48)(z|x). The rφ(cid:48)(z|x) can either be explicit with a tractable likelihood [39, 40], or implicit that maps x and noise to a latent sample, i.e., z = Gφ(cid:48)(ξ, x), ξ ∼ U([−1, 1]k). We choose implicit feature encoder as it produces better results.
Algorithm 1 BV-NICE
Empirical data ˆpd = {(xi, yi, ti)}n for k = 1, 2, · · · do i=1, imbalance λ (x, y, t) ∼ ˆpd, z(cid:48) ∼ p(z), zφ = Gφ(ξ, x), ξ ∼ p(ξ)
φk+1 ← ∇φ{log pθ(t, y|zφ) − ϑψ(x, zφ) % Encoder
− λ[ ˜ϑ ˜ψ(zt=0
φ ) − exp( ˜ϑ ˜ψ(zt=1
φ ))]} % Balancing
θk+1 ← ∇θ{log pθ(t, y|zφ)} % Model
ψk+1 ← ∇ψ{ϑψ(x, zφ) − exp(ϑψ(x, z(cid:48)))} % Critic
˜ψk+1 ← ∇ ˜ψ{ ˜ϑ ˜ψ(zt=0
φ ))} % Critic
φ ) − exp( ˜ϑ ˜ψ(zt=1 end for
Another empirical decision is whether to include treatment and outcome in the encoder. Both choices induce a valid lower bound. While the inclusion is practiced in Louizos et al. [50], we argue otherwise. First, it complicates inference procedure and introduces additional approxima-tion error, as auxiliary models must be intro-duced to sample the latent. Second, the casual effect identiﬁcation requires that the assignment is independent with potential outcomes condi-tional on the covariates. The inclusion of out-come in the encoder will, on the contrary, poten-tially introduce bias and violates the unconfoundedness assumption [70, 77].
Practical variants Modiﬁcations to the original VI procedure are often considered by practitioners for better performance, as compensation mechanism to correct for potential model mis-speciﬁcation.
We consider two variants that are more principally derived: β-VAE [30] and AAE [54]. The former seeks to address the potential vanishing KL, while the later explicitly targets the mismatch between the aggregated posterior and prior. Both strategies diminishes the role of KL term in ELBO, which often compromises empirical performance via synthesizing uninformative latents to reduce the mismatch to the prior. Implementation details are included in SM. 2Note that we have dropped the constant term for clarity. 5
Inferring causal effects Given a new observation x, we wish to infer the expected effect τ (x) for a given intervention under the learned model. Since under BV-NICE causal effect τ (z) is deﬁned based on the latent variable z rather than the observed x, the estimation of the causal effect becomes a two-stage process. In the ﬁrst stage we infer the hidden z given x, and in the second stage we average over the latent variables to estimate the causal effect for x. An estimate of the causal effect is given by τ (x) ≈ 1 m j ∼ qφ(z|x). j τ (z(cid:48) j), z(cid:48) (cid:80)
Counterfactual cross-validation with R-residual. A major obstacle in counterfactual reasoning is that due to the absence of counterfactual observations, models can not be validated directly. In our setting, we applied R-loss to hold out factual observations to cross-validation our model. Although it may seem similar to the CV applied in standard machine learning, a key distinction should be noted: that our CV target is explicitly deﬁned wrt the counterfactual estimates. As noted in Nie and Wager
[56], Schuler et al. [73], factual residual does not effectively assess counterfactual performance, resulting biased or unreliable estimation. 3.4 Generalization bounds for BV-NICE
We provide theoretical justiﬁcation for the use of KL balancing. In particular, we show that the counterfactual generalization error can be bounded by the factual error plus a KL-term of the representation distributions between the treatment groups, adjusted by the variance of the conditional outcome model. We also provide additional discussions on other theoretical aspects in the SM.
Deﬁnition 3.1. The expected loss for the unit and treatment pair (z, t) is (cid:90) (cid:96)h(z, t) =
L(Yt, h(z, t)))p(Yt|z) dYt, (8)
Y where L(y; h) denotes some loss wrt observation y and hypothesis h, and z is parameterized via the stochastic encoder qφ(z|x). The expected factual and counter factual losses of h and φ are: (cid:90) (cid:15)F(h, φ) (cid:44)
Z×{0,1} (cid:96)h(z, t)pφ(z, t) dz dt, (cid:15)CF(h, φ) (cid:44) (cid:90)
Z×{0,1} (cid:96)h(z, t)pφ(z, 1 − t) dz dt, (9) where pφ(z, t) = (cid:82) qφ(z|x, y, t)pd(x, y, t) dx dy. The expected factual treated (t = 1) and control (t = 0) losses are (cid:15)t=1
F (h, φ) (cid:44) (cid:90)
Z (cid:96)h,φ(z, 1)q1(z) dz, (cid:15)t=0
F (h, φ) (cid:44) (cid:90)
Z (cid:96)h,φ(z, 0)q0(z) dz (10) where qt(z) is the aggregated approximate posterior of z given t deﬁned as in Sec 3.2
Deﬁnition 3.2. Precision of estimating heterogeneous effects (PEHE) for a causal effect estimator ˆτ
L2(P), where L2(P) is the L2 norm wrt feature density P(x). is deﬁned as (cid:15)P EHE(ˆτ ) (cid:44) E(cid:107)ˆτ − τ (cid:107)2
The following statements assert the generalization error for PEHE can be bounded by the factual error plus a KL-discrepancy term, adjusted by the variance of outcome.
Lemma 3.3. Let qt, t ∈ {0, 1} be the marginal aggregated approximate posterior distributions deﬁned as in Sec 3.2, u (cid:44) p(T = 1) is the prevalence of treatment, and h : R × {0, 1} → Y is a hypothesis. Assume (cid:107)lh(z, t)(cid:107)∞ ≤ M for t = {0, 1}. Then we have (cid:15)CF (h, φ) ≤ (1 − u) · (cid:15)t=1
F (h, φ) + u · (cid:15)t=0
F (h, φ) + 1 2M (cid:114) 1 2
DKL (q0||q1) (11)
Theorem 3.4. Under the conditions of Lemma 3.3, and assuming the loss L deﬁnes lh,Φ is the squared loss L(y, y(cid:48)) = (y − y(cid:48))2, and deﬁne σY (cid:44) maxt∈{0,1} EZ[(Y (t) − E[Y (t)|Z])2], we have: (cid:15)P EHE (h, Φ) ≤ 2(cid:15)t=0
F (h, Φ) + 2(cid:15)t=1
F (h, Φ) + (cid:114) 1 2 1
M
DKL (q0||q1) − 4σ2
Y (12)
This result bears resemblance to the generalization bound proved in Shalit et al. [75]. The key difference is that we have replaced the IPM bound with a KL bound. The original implementation of CFR used the Sinkhorn iterations or MMD computed their IPM, which scales quadratically wrt mini-batch size. Our Fenchel dual KL estimation scales linearly wrt sample size, and consequently more scalable. And the new assumption on (cid:96)h,φ ∈ L∞ is generally easily satisﬁed in practice, while the RKHS assumed by CFR is difﬁcult to verify. 6
4