Abstract
In 1988, Eric B. Baum showed that two-layers neural networks with threshold ac-tivation function can perfectly memorize the binary labels of n points in general position in Rd using only (cid:112)n/d(cid:113) neurons. We observe that with ReLU networks, using four times as many neurons one can ﬁt arbitrary real labels. Moreover, for approximate memorization up to error ε, the neural tangent kernel can also memo-rize with only O (cid:0) n d · log(1/ε)(cid:1) neurons (assuming that the data is well dispersed too). We show however that these constructions give rise to networks where the magnitude of the neurons’ weights are far from optimal. In contrast we propose a new training procedure for ReLU networks, based on complex (as opposed to real) recombination of the neurons, for which we show approximate memorization with both O neurons, as well as nearly-optimal size of the weights. (cid:16) n d · log(1/ε) (cid:17)
ε 1

Introduction
We study two-layers neural networks in Rd with k neurons and non-linearity ψ : R → R. These are functions of the form: k (cid:88) x (cid:55)→ a(cid:96)ψ(w(cid:96) · x + b(cid:96)) , (1) (cid:96)=1 with a(cid:96), b(cid:96) ∈ R and w(cid:96) ∈ Rd for any (cid:96) ∈ [k]. We are mostly concerned with the Rectiﬁed
Linear Unit non-linearity, namely ReLU(t) = max(0, t), in which case wlog one can restrict the recombination weights (a(cid:96)) to be in {−1, 1} (this holds more generally for positively homogeneous non-linearities). We denote by Fk(ψ) the set of functions of the form (1). Under mild conditions on ψ (namely that it is not a polynomial), such neural networks are universal, in the sense that for k large enough they can approximate any continuous function [Cybenko, 1989, Leshno et al., 1993].
In this paper we are interested in approximating a target function on a ﬁnite data set. This is also called the memorization problem. Speciﬁcally, ﬁx a data set (xi, yi)i∈[n] ∈ (Rd × R)n and an approximation error ε > 0. We denote y = (y1, . . . , yn), and for a function f : Rd → R we write f = (f (x1), . . . , f (xn)). The main question concerning the memorization capabilities of Fk(ψ) is as follows: How large should be k so that there exists f ∈ Fk(ψ) such that (cid:107)f − y(cid:107)2 ≤ ε(cid:107)y(cid:107)2 (where (cid:107) · (cid:107) denotes the Euclidean norm)? A simple consequence of universality of neural networks is that k ≥ n is sufﬁcient (see Proposition 2). In fact (as was already observed by Baum [1988] for threshold ψ and binary labels, see Proposition 3) much more compact representations can be achieved by leveraging the high-dimensionality of the data. Namely we prove that for ψ = ReLU and a data set in general position (i.e., any hyperplane contains at most d points), one only needs k ≥ 4 · (cid:112) n (cid:113) to memorize the data perfectly, see Proposition 4. The size k ≈ n/d is clearly optimal, d
∗This work was partly done while R. Eldan and D. Mikulincer were visiting Microsoft Research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
by a simple parameter counting argument. We call the construction given in Proposition 4 a Baum network, and as we shall see it is of a certain combinatorial ﬂavor. In addition we also prove that such memorization can in fact essentially be achieved in a kernel regime (with a bit more assumptions on the data): we prove in Theorem 2 that for k = Ω (cid:0) n d log(1/ε)(cid:1) one can obtain approximate memorization with the Neural Tangent Kernel [Jacot et al., 2018], and we call the corresponding construction the NTK network. Speciﬁcally, the kernel we consider is,
E [∇wψ(w · x) · ∇wψ(w · y)] = E [(x · y)ψ(cid:48)(w · x)ψ(cid:48)(w · y)] , where ∇w is the gradient with respect to the w variable and the expectation is taken over a random initialization of w.
Measuring regularity via total weight. One is often interested in ﬁtting the data using functions which satisfy certain regularity properties. The main notion of regularity in which we are interested is the total weight, deﬁned as follows: For a function f : Rd → R of the form (1), we deﬁne k (cid:88) (cid:113)
W(f ) :=
|a(cid:96)| (cid:107)w(cid:96)(cid:107)2 + b2 (cid:96) .
This deﬁnition is widely used in the literature, see Section 2 for a discussion and references. No-tably, it was shown in Bartlett [1998] that this measure of complexity is better associated with the network’s generalization ability compared to the size of the network. We will be interested in con-structions which have both a small number of neurons and a small total weight. (cid:96)=1
Our main contribution: The complex network. As we will see below, both the Baum network and the NTK networks have sub-optimal total weight. The main technical contribution of our paper is a third type of construction, which we call the harmonic network, that under the same assumptions on the data as for the NTK network, has both near-optimal memorization size and near-optimal total weight:
Theorem 1 (Informal). Suppose that n ≤ poly(d). Let x1, .., xN ∈ Sd−1 such that
|xi · xj| = (cid:101)O (cid:19)
. (cid:18) 1
√ d
For every ε > 0 and every choice of labels (yi)n k = (cid:101)O (cid:0) n (cid:1) and f ∈ Fk(ψ) such that dε i=1 such that |yi| = O(1) for all i, there exist and such that W(f ) = (cid:101)O (
√ (cid:16)(cid:0)yi − f (xi)(cid:1)2 (cid:17)
, 1 min
≤ ε 1 n n (cid:88) i=1 n). n), thus
We show below in Proposition 1 that for random data one necessarily has W(f ) = (cid:101)Ω ( proving that the harmonic network has near-optimal total weight. Moreover we also argue in the corresponding sections that the Baum and NTK networks have total weight at least n n on random data, thus being far from optimal.
√
√
An iterative construction. Both the NTK network and the harmonic network will be built by iteratively adding up small numbers of neurons. This procedure, akin to boosting, is justiﬁed by the following lemma. It shows that to build a large memorizing network it sufﬁces to be able to build a small network f whose scalar product with the data f · y is comparable to its variance (cid:107)f (cid:107)2:
Lemma 1 Fix (xi)n
For any choice of (yi)n for all ε > 0, there exists g ∈ Fmk(ψ) such that i=1. Suppose that there are m ∈ N and α, β > 0 such that the following holds: i=1, there exists f ∈ Fm(ψ) with y · f ≥ α(cid:107)y(cid:107)2 and (cid:107)f (cid:107)2 ≤ β(cid:107)y(cid:107)2. Then (cid:107)g − y(cid:107)2 ≤ ε(cid:107)y(cid:107)2 with k ≤
β
α2 log(1/ε).
Moreover, if the above holds with W(f ) ≤ ω, then W(g) ≤ ω
α log(1/ε). 2
Proof. Denote η = α
β and r1 = y. Then, there exists f1 ∈ Fm(ψ), such that (cid:107)ηf1 − r1(cid:107)2 = (cid:107)r1(cid:107)2 − 2ηy · f1 + η2(cid:107)f1(cid:107)2 ≤ (cid:107)r1(cid:107)2 (cid:18) 1 − 2
α2
β
+ (cid:19)
α2
β
≤ (cid:107)r1(cid:107)2 (cid:18) 1 − (cid:19)
α2
β
= (cid:107)y(cid:107)2 (cid:18) 1 − (cid:19)
α2
β
The result is obtained by iterating the above inequality with ri = y − η (cid:80)i−1 residuals. By induction, if we set g = η (cid:80)k j=1 fj, we get j=1 fj taken as the (cid:107)g − y(cid:107) = (cid:107)ηfk − rk(cid:107) ≤ (cid:107)rk(cid:107)2 (cid:18) 1 − (cid:19)
α2
β
= (cid:107)y(cid:107)2 (cid:18) 1 − (cid:19)k
.
α2
β (cid:3)
In both the NTK and harmonic constructions, the function f will have the largest possible correlation with the data set attainable for a network of constant size. However, the harmonic network will have the extra advantage that the function f will be composed of a single neuron whose weight is the smallest one attainable. Thus, the harmonic network will enjoy both the smallest possible number of neurons and smallest possible total weight (up to logarithmic factors). Note however that the dependency on ε is worse for the harmonic network, which is technically due to a constant order term in the variance which we do not know how to remove.
We conclude the introduction by showing that a total weight of Ω( n) is necessary for approximate memorization. Just like for the upper bound, it turns out that it is sufﬁcient to consider how well can one correlate a single neuron. Namely the proof boils down to showing that a single neuron cannot correlate well with random data sets.
√
Proposition 1 There exists a data set (xi, yi)i∈[n] ∈ (Sd−1 ×{−1, 1})n such that for every function f of the form (1) with ψ L-Lipschitz and which satisﬁes (cid:107)f − y(cid:107)2 ≤ 1 2 (cid:107)y(cid:107)2, it holds that W(f ) ≥
√ n 8L .
Proof. We have that is 1 2 which implies: (cid:107)y(cid:107)2 ≥ (cid:107)f − y(cid:107)2 ≥ (cid:107)y(cid:107)2 − 2f · y ⇒ f · y ≥ 1 4 (cid:107)y(cid:107)2 , k (cid:88) n (cid:88) (cid:96)=1 i=1 yia(cid:96)ψ(w(cid:96) · xi − b(cid:96)) ≥ n 4
, max w,b n (cid:88) i=1 yi
ψ(w · xi − b) (cid:112)(cid:107)w(cid:107)2 + b2
≥ n 4W(f )
.
Now let us assume that yi are ±1 uniformly at random (i.e., Rademacher random variables), and thus by Talagrand’s contraction lemma for the Rademacher complexity (see [Lemma 26.9, Shalev-Shwartz and Ben-David [2014]]) we have:
E max w,b n (cid:88) i=1 yi
ψ(w · xi − b) (cid:112)(cid:107)w(cid:107)2 + b2
≤ L · E max w,b n (cid:88) i=1 yi w · xi − b (cid:112)(cid:107)w(cid:107)2 + b2 and thus W(f ) ≥
√ n 8L . (cid:118) (cid:117) (cid:117) (cid:116)
≤ L · E 3 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) n (cid:88) i=1 yixi (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13)
√
+ n ≤ 2L n , (cid:3)
2