Abstract
In this work, we study the problem of multi-agent reinforcement learning (MARL) with model uncertainty, which is referred to as robust MARL. This is naturally mo-tivated by some multi-agent applications where each agent may not have perfectly accurate knowledge of the model, e.g., all the reward functions of other agents.
Little a priori work on MARL has accounted for such uncertainties, neither in problem formulation nor in algorithm design. In contrast, we model the problem as a robust Markov game, where the goal of all agents is to ﬁnd policies such that no agent has the incentive to deviate, i.e., reach some equilibrium point, which is also robust to the possible uncertainty of the MARL model. We ﬁrst introduce the solution concept of robust Nash equilibrium in our setting, and develop a Q-learning algorithm to ﬁnd such equilibrium policies, with convergence guarantees under certain conditions. In order to handle possibly enormous state-action spaces in practice, we then derive the policy gradients for robust MARL, and develop an actor-critic algorithm with function approximation. Our experiments demonstrate that the proposed algorithm outperforms several baseline MARL methods that do not account for the model uncertainty, in several standard but uncertain cooperative and competitive MARL environments. 1

Introduction
Deep reinforcement learning (RL) has recently achieved tremendous successes in many sequential decision-making problems, varying from robotics [1, 2] and autonomous driving [3] to game playing
[4, 5]. In fact, many of these important applications involve more than one agent or player [6, 7], naturally leading to the setting of multi-agent RL (MARL). MARL addresses the decision-making problem of multiple agents in a common environment, where the goal of each agent is to optimize its own long-term return by interacting with the environment and other agents; see [8, 9] for detailed reviews, and [10, 11, 12, 13, 14, 15] for some recent advances. MARL problems are usually modeled under the framework of Markov games, stemming from the seminal work [16].
In real-world applications, the agents, especially those trained in simulations, may not have perfectly accurate knowledge of the actual model, i.e., the reward functions of all agents and the transition probability model. In particular, the solution obtained from the simulation without uncertainty may have poor performance in practice, known as the sim-to-real gap. Such an issue has been reported quite common in the autonomous-car racing application [17], which initially motivates the present work. In single-agent RL, such an uncertainty has been nicely handled through the lens of robust Markov decision processes (MDPs) [18, 19, 20] and robust (adversarial) RL [21, 22]. In comparison, such an uncertainty has not been fully explored in the multi-agent RL regime. In light (cid:92)Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of the signiﬁcance and ubiquity of MARL, it is thus imperative to take the model uncertainty into account in both the formulation and the algorithm design in this setting.
In this work, we aim to develop such a robust MARL framework when model uncertainty is present.
Speciﬁcally, we model the problem of MARL with model uncertainty as a robust Markov game [23], where the goal of all agents is to ﬁnd policies such that no agent has the incentive to deviate, which are also robust to the possible uncertainty of the model. All agents play a standard Markov game with additional concerns on the distribution-free uncertainty of the reward function and transition probabilities. To adapt to the worst-case scenario due to uncertainty, one can view the uncertainty as the decision made by an implicit player, a “nature” player, who always plays against each agent. This way, the solution concept in robust Markov games differs from the standard Nash equilibrium (NE) in
Markov games [24]. Indeed, each agent not only needs to optimize its own return, in consideration of other agents’ general affects on itself, but also needs to always play against the nature player, in order to tackle model uncertainty. Such a model covers MARL environments with both cooperative and competitive agents. Within this new framework, we then develop several robust MARL algorithms to ﬁnd the solution concept of the game, and evaluate their performance in benchmark MARL environments. We summarize our contribution as follows.
Contribution. Our contribution is three-fold: ﬁrst, we propose a new framework to systematically characterize the model uncertainty in MARL, by advocating the use of robust Markov games; second, we develop both Q-learning-based and actor-critic algorithms for ﬁnding the solution concept in this framework; third, we validate the performance of our algorithms with function approximation and mini-batch update, via extensive simulations in benchmark MARL environments. To the best of our knowledge, we provide the ﬁrst formulation and algorithms that account for the model uncertainties in MARL, with both theoretical and empirical justiﬁcations.