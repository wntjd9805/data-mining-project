Abstract
Multiclass extensions of the support vector machine (SVM) have been formulated in a variety of ways. A recent empirical comparison of nine such formulations [1] recommends the variant proposed by Weston and Watkins (WW), despite the fact that the WW-hinge loss is not calibrated with respect to the 0-1 loss. In this work we introduce a novel discrete loss function for multiclass classiﬁcation, the ordered partition loss, and prove that the WW-hinge loss is calibrated with respect to this loss. We also argue that the ordered partition loss is minimally emblematic among discrete losses satisfying this property. Finally, we apply our theory to justify the empirical observation made by Doˇgan et al. [1] that the WW-SVM can work well even under massive label noise, a challenging setting for multiclass SVMs. 1

Introduction
Classiﬁcation is the task of assigning labels to instances, and a common approach is to minimize misclassiﬁcation error corresponding to the 0-1 loss. However, the 0-1 loss is discrete and typically cannot be optimized efﬁciently. To address this, the 0-1 loss is often replaced by a surrogate loss during training. If the surrogate is calibrated with respect to the 0-1 loss, then a classiﬁer minimizing the expected surrogate loss will also minimize the expected 0-1 loss in the inﬁnite sample limit.
For multiclass classiﬁcation, several different multiclass extensions of the support vector machine (SVM) have been proposed, including the Weston-Watkins (WW) [2], Crammer-Singer (CS) [3], and
Lee-Lin-Wahba (LLW) [4] SVMs. The pertinent difference between these multiclass SVMs is the multiclass generalization of the hinge loss. Below, we refer to the hinge loss from WW-SVM as the
WW hinge loss and so on. It is well-known that the LLW-hinge is calibrated with respect to the 0-1 loss, while the WW- and CS-hinge losses are not [5, 6].
Despite this result, the LLW-SVM is not more widely accepted than the WW-, CS-, and other SVMs.
The ﬁrst reason for this is that while the LLW-SVM is calibrated with respect to the 0-1 loss, this did not lead to superior performance empirically. In particular, Doˇgan et al. [1] found that the LLW-SVM fails in low dimensional feature space even under the noiseless setting. On the other hand, Doˇgan et al. [1] observed that the WW-SVM is the only multiclass SVM that succeeded in both the noiseless and noisy setting in their simulations. Indeed, Doˇgan et al. [1] concluded that, among 9 different competing multiclass SVMs, the WW-SVM offers the best overall performance when considering accuracy and computation. The second reason is that the calibration framework is not limited to the 0-1 loss. There could be other discrete losses with respect to which a surrogate is calibrated, and which help to explain its performance. Indeed, Ramaswamy et al. [7] recently showed that the
CS-hinge loss is calibrated with respect to a discrete loss for classiﬁcation with abstention.
In a vein similar to [7], we show that the WW-hinge loss is calibrated with respect to a novel discrete loss that we call the ordered partition loss. Our results leverage the embedding framework for analyzing discrete losses and convex piecewise linear surrogates, introduced recently by Finocchiaro et al. [8]. We also give theoretical justiﬁcation for the empirical performance of the WW-SVM observed by Doˇgan et al. [1]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1.1