Abstract
This paper is concerned with data-driven unsupervised domain adaptation, where it is unknown in advance how the joint distribution changes across domains, i.e., what factors or modules of the data distribution remain invariant or change across domains. To develop an automated way of domain adaptation with multiple source domains, we propose to use a graphical model as a compact way to encode the change property of the joint distribution, which can be learned from data, and then view domain adaptation as a problem of Bayesian inference on the graphical models. Such a graphical model distinguishes between constant and varied modules of the distribution and speciﬁes the properties of the changes across domains, which serves as prior knowledge of the changing modules for the purpose of deriving the posterior of the target variable Y in the target domain. This provides an end-to-end framework of domain adaptation, in which additional knowledge about how the joint distribution changes, if available, can be directly incorporated to improve the graphical representation. We discuss how causality-based domain adaptation can be put under this umbrella. Experimental results on both synthetic and real data demonstrate the efﬁcacy of the proposed framework for domain adaptation. The code is available at https://github.com/mgong2/DA_Infer. 1

Introduction
Over the past decade, various approaches to unsupervised domain adaptation (DA) have been pursued to leverage the source-domain data to make prediction in the new, target domain. In particular, we consider the situation with n source domains in which both the d-dimensional feature vector X, whose jth dimension is denoted by Xj, and label Y are given, i.e., we are given (x(i), y(i)) = (x(i) k , y(i) k )mi k=1, where i = 1, ..., n, and mi is the sample size of the ith source domain. We denote by x(i) jk the value of the jth feature of the kth data point (example) in the ith domain. Our goal is to ﬁnd the classiﬁer for the target domain, in which only the features xτ = (xτ k=1 are available. Because the distribution may change across domains, clearly the optimal way of adaptation or transfer depends on what information is shared across domains and how to do the transfer. k)m
In the covariate shift scenario, the distribution of the features, P (X), changes, while the conditional distribution P (Y |X) remains ﬁxed. A common strategy is to reweight examples from the source domain to match the feature distribution in the target domain–an approach extensively studied in
∗These authors contributed equally to this work. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
machine learning; see e.g., [1, 2, 3, 4]. Another collection of methods learns a domain-invariant feature representation that has identical distributions across the target and source domains [5, 6, 7, 8, 9].
In addition, it has been found that P (Y |X) usually changes across domains, in contrast to the covariate shift setting. For the purpose of explaining and modeling the change in P (Y |X), the problem was studied from a generative perspective [10, 11, 12, 13, 14]–one can make use of the factorization of the joint distribution corresponding to the causal representation and exploit how the factors of the joint distribution change, according to commonsense or domain knowledge. The settings of target shift [10, 14, 12, 15] and conditional shift [12, 16, 17] assume only P (Y ) and
P (X|Y ) change, respectively, and their combination, as generalized target shift [12, 18], was also studied, and the corresponding methods clearly improved the performance on a number of benchmark datasets. The methods were extended further by learning feature representations with invariant conditionals given the label and matching joint distributions [17, 19, 20], and it was shown how methods based on domain-invariant representations can be understood from this perspective.
How are the distributions in different domains related? Essentially, DA aims to discover and exploit the constraints in the data distribution implied by multiple domains and make predictions that adapts to the target domain. To this end, we assume that the distributions of the data in different domains were independent and identically distributed (I.I.D.) drawn from some “mother” distribution. The mother distribution encodes the uncertainty in the domain-speciﬁc distributions, i.e., how the joint distribution is different across the domains. Suppose the mother distribution is known, from which the target-domain distribution is drawn. Furthermore, the target domain contains data points (without
Y values) generated by this distribution. It is then natural to leverage both the mother distribution and the target-domain feature values to reveal the property of the target-domain distribution for the purpose of predicting Y . In other words, DA is achieved by exploiting the mother distribution and the target-domain feature values to derive the information of Y .
Following this argument, we have several questions to answer. First, is there a natural, compact description of the constraints on the changes of the data distribution (to describe the mother distri-bution)? Such constraints include which factors of the joint distributions can change, whether they change independently, and the range of changes. (We represent the joint distribution as a product of the factors.) Second, how can we ﬁnd such a description from the available data? Third, how can we make use of such a description as well as the target-domain data to make optimal prediction?
Traditional graphical models have provided a compact way to encode conditional independence relations between variables and factorize the joint distribution [21, 22]. We will use an extension of
Directed Acyclic Graphs (DAGs), called augmented DAGs, to factorize the joint distribution and encode which factors of the joint distribution change across domains. The augmented DAG, together with the conditional distributions and changeability of the changing modules, gives an augmented graphical model as a compact representation of how the joint distribution changes. Predicting the Y values in the target domain is then a problem of Bayesian inference on this graphical model given the observed target-domain feature values. This provides a natural framework to address the DA problem in an automated, end-to-end manner. 2