Abstract
Normalizing ﬂows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation. In this work, we propose a novel type of normalizing ﬂow driven by a differential deformation of the Wiener process. As a result, we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process, such as efﬁcient computation of likelihoods and marginals. Furthermore, our continuous treatment provides a natural framework for irregular time series with an independent arrival process, including straightforward interpolation. We illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior ﬂexibility to variational
RNN and latent ODE baselines in a series of experiments on synthetic and real-world data. 1

Introduction
Expressive models for sequential data form the statistical basis for downstream tasks in a wide range of domains, including computer vision, robotics, and ﬁnance. Recent advances in deep generative architectures, especially the concept of reversibility, have led to tremendous progress in this area and created a new perspective on many of the long-standing limitations that are typical in traditional approaches based on structured decompositions (e.g., state-space models).
We argue that the power of a time series model depends on its properties in the following areas: (1 –
Resolution) Common time series models are discrete with respect to time. As a result, they make the implicit assumption of a uniformly spaced temporal grid, which precludes their application from asynchronous tasks with a separate arrival process. (2 – Structural assumptions) The expressiveness of a temporal model is determined by the dependencies and shapes of its variables. In particular, the topological structure should be rich enough to capture the dynamics of the underlying process but sparse enough to allow for robust learning and efﬁcient inference. (3 – Generation) A good time series model must be able to generate unbiased samples from the true underlying process in an efﬁcient way. (4 – Inference) Given a trained model, it should support standard inference tasks, such as interpolation, forecasting, and likelihood calculation.
Recently, deep generative modeling has enabled vastly increased ﬂexibility while keeping generation and inference tractable, owing to novel techniques like amortized variational inference [28, 12], reversible generative models [41, 29], and networks based on differential equations [9, 34].
In this work, we approach the modeling of continuous and irregular time series with a reversible gen-erative model for stochastic processes. Our approach builds upon ideas from normalizing ﬂows; how-ever, instead of a static base distribution, we transform a dynamic base process into an observable one.
In particular, we introduce the continuous-time ﬂow process (CTFP), a novel type of generative model
∗Work developed during an internship at Borealis AI. Correspond to wsdmdeng@gmail.com. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
that decodes the base continuous Wiener process into a complex observable process using a dynamic in-stance of normalizing ﬂows. The resulting observ-able process is thus continuous in time. In addition to the appealing properties of static normalizing ﬂows (e.g., efﬁcient sampling and exact likelihood), this also enables a series of inference tasks that are typi-cally unattainable in time series models with complex dynamics, such as interpolation and extrapolation at arbitrary timestamps. Furthermore, to overcome the simple covariance structure of the Wiener process, we augment the reversible mapping with latent vari-ables and optimize this latent CTFP variant using variational optimization. Our approach is illustrated in Figure 1.
Figure 1: Overview. Wiener processes are con-tinuous stochastic processes with appealing prop-erties but limited ﬂexibility. We propose to learn a complex observed process (red) through a differen-tial deformation (grey) of the base Wiener process (blue), thereby preserving the advantages of the base process.
Contributions.
In summary, we propose the continuous-time ﬂow process (CTFP), a novel gen-erative model for continuous stochastic processes. It has the following appealing properties: (1) it induces ﬂexible and consistent joint distributions on arbitrary and irregular time grids, with easy-to-compute density and an efﬁcient sampling procedure; (2) the stochastic process generated by CTFP is guaranteed to have continuous sample paths, making it a natural ﬁt for data with continuously-changing dynamics; (3) CTFP can perform interpolation and extrapolation conditioned on given observations. We validate our model and its latent variant on various stochastic processes and real-world datasets and show superior performance to state-of-the-art methods, including the variational recurrent neural network (VRNN) [12] and latent ordinary differential equation (latent ODE) [42]. 2