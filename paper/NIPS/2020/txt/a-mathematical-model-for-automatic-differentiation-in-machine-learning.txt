Abstract
Automatic differentiation, as implemented today, does not have a simple mathe-matical model adapted to the needs of modern machine learning. In this work we articulate the relationships between differentiation of programs as implemented in practice and differentiation of nonsmooth functions. To this end we provide a simple class of functions, a nonsmooth calculus, and show how they apply to stochastic approximation methods. We also evidence the issue of artiﬁcial critical points created by algorithmic differentiation and show how usual methods avoid these points with probability one. 1

Introduction
Optimization algorithms based on backpropagation oracles, and more generally on automatic or algorithmic differentiation (AD) [41, 39], are one of the most widely used training tools for modern learning architectures [14, 32, 15, 18, 20, 3, 16]. They often rely on popular numerical implementa-tions as TensorFlow or PyTorch [1, 36]. However, for nonsmooth, nonconvex losses, AD does not have a stable theory [23, 25, 26, 2, 30, 28, 29, 12], matching the actual practice. We wish to present a simple mathematical framework addressing this issue. Let us progressively explain our approach. 1.1 What is backpropagation?
Algorithmic differentiation acts on programs not on functions: To convey this fact we carry out a small experiment in TensorFlow [1] with the function relu : t ÞÑ maxt0, tu, see Appendix A.2 for implementation details. Algorithmic differentiation is displayed in Figure 1, in particular, we have relu1p0q “ 0. Consider the two functions relu2 : t ÞÑ relup´tq ` t, relu3 : t ÞÑ 1 2 preluptq ` relu2ptqq.
As mathematical functions on R these are equal to relu. However TensorFlow returns relu1 2p0q “ 1 and relu1 3p0q “ 1{2 (Figure 1). Indeed, AD does not act on functions, but on their representations, i.e., on programs. Different programs implementing the same function may provide different results, beyond numerical precision; we refer to this as the spurious behaviour of AD for nonsmooth functions2. Let us explore this phenomenon further. The function zero : t ÞÑ relu2ptq ´ reluptq, outputs constantly 0 but AD gives zero1p0q “ 1. More generally, one can modify the value of the derivative of a given function at prescribed arguments (Figure 1). This may generate artiﬁcial critical points; for instance x Ñ x ´ zero is the identity but its derivative at 0 according to AD is 0.
˚Authors in alphabetical order. 2The validity domain of AD is restricted in theory to smooth functions [23], yet it is common practice to use it for nonsmooth functions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Top: AD applied to relu and two different implementations of the same function. Bottom:
Algorithmic differentiation of a constant function, creation of artiﬁcial critical point or arbitrary derivatives at prescribed arguments for the sine function.
This discussion was limited to univariate functions, but these pathologies grow in size and in complexity when occurring in higher dimensions. Besides, as the “compositional depth” of functions increases the phenomenon gets more complex, making the geometry of artiﬁcial point difﬁcult to grasp.
Canonical surjection between programs functions: Numerical programs combine basic mathe-matical functions within an algorithm and return an output. This can be understood in two ways:
• Computer science: it is a sequence of instructions with numerical inputs-outputs,
• Mathematics: the program is a function3 of its arguments.
It is tempting to identify both, but functions can be represented by different programs. This deﬁnes a surjection F mapping a program to a function (in the class of functions “accessible through coding”).
Algorithmic differentiation: As presented above, AD is an operation on programs, A which takes as argument a program and returns a program with the same input variables. This operation can be “pushed” to the space of functions using the canonical surjection F. Remarkably, if we restrict ourselves to programs P which only smoothly combine smooth functions, then we have the following fundamental relation, depicted in Figure 2:
In other words, algorithmic differentiation of a program which smoothly combines smooth functions, is equivalent, through the canonical surjection, to derivation.
FpApPqq “ ∇FpPq. (1)
Figure 2: Left: Algorithmic differentiation applied to programs combining smooth functions in a smooth way, the diagram commutes. Right: Algorithmic differentiation in nonsmooth settings, connection with known notion of generalized derivative is much less clear. 3In the usual mathematical sense. 2
However practitioners use AD and backpropagation beyond smooth programs with nonsmooth ele-mentary functions or program branching for instance. Can we ﬁnd a proper operational interpretation of this widespread practice?
Algorithmic differentiation cannot be represented through a variational operator At ﬁrst, it is tempting to simply use AD to induce a differential operator on functions generalizing classical differentiation. This operator, say BA, should: (a) encompass the outputs of algorithmic differentation for all functions (b) be such that 0 is an element of BApreluq at 0.
Unfortunately such an operator does not exist:
Theorem 1 (Algorithmic differentiation does not induce an operator on functions) There is no nontrivial operator on functions satisfying paq and pbq. 1.2 Contribution and related work
We address this impossibility result and provide a class of functions together with an operational nonsmooth differential calculus which is able to cope with spurious behaviours.
Elementary selections and selection derivatives: We introduce a new class of nonsmooth non-convex functions, encompassing most objective functions met in machine learning, having appealing stability properties. This allows us to deﬁne simple differential objects called selection derivatives.
Selection derivatives turn out to have an operational calculus adapted to the analysis of many learning methods, as backpropagation or stochastic ﬁrst order methods. They thus provide an operational model to capture nonsmooth AD as implemented in current numerical software.
Algorithmic differentiation, algorithms This framework allows to formalize properly the rela-tionships between, functions, algorithmic differentiation and capture the corresponding notion of critical points as met in practice. These characterize the set of attractors (limit points) for stochastic approximation algorithms based on nonsmooth backpropagation [37, 4, 31, 5, 13]. It is important to stress that these attractors, which models sharply the whole scope of AD-induced stationnarity, are different from the traditional notions as Clarke criticality [17, 38, 20]. This is described in Theorems 3 and 4.
Avoidance of traps: As sketched above and in the introduction AD produces artiﬁcial critical points, i.e. stationary points which are not Clarke critical. These points have a parasitic nature which could be detrimental to training purposes, were they met in practice. We show that randomly initialized mini-batch stochastic gradient method do not lead to artiﬁcial critical points (Theorem 4).
This result applies to modern machine learning software libraries based on AD [1, 36], seen as performing operation over the reals, without any modiﬁcation. Although AD may have unpredictable behavior in nonsmooth contexts, both theoretically and numerically, this result justiﬁes theoretically that the practical impact is somewhat negligible in the context of common machine learning usage.