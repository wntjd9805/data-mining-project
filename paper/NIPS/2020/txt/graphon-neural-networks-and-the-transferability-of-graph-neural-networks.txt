Abstract
Graph neural networks (GNNs) rely on graph convolutions to extract local features from network data. These graph convolutions combine information from adjacent nodes using coefﬁcients that are shared across all nodes. Since these coefﬁcients are shared and do not depend on the graph, one can envision using the same coefﬁcients to deﬁne a GNN on another graph. This motivates analyzing the transferability of GNNs across graphs. In this paper we introduce graphon NNs as limit objects of GNNs and prove a bound on the difference between the output of a GNN and its limit graphon-NN. This bound vanishes with growing number of nodes if the graph convolutional ﬁlters are bandlimited in the graph spectral domain. This result establishes a tradeoff between discriminability and transferability of GNNs. 1

Introduction
Graph neural networks (GNNs) are the counterpart of convolutional neural networks (CNNs) to learning problems involving network data. Like CNNs, GNNs have gained popularity due to their superior performance in a number of learning tasks (Bronstein et al., 2017; Defferrard et al., 2016;
Gama et al., 2018; Kipf and Welling, 2017). Aside from the ample amount of empirical evidence,
GNNs are proven to work well because of properties such as invariance and stability (Gama et al., 2019a; Ruiz et al., 2019), which are also shared with CNNs (Bruna and Mallat, 2013).
A deﬁning characteristic of GNNs is that their number of parameters does not depend on the size (i.e., the number of nodes) of the underlying graph. This is because graph convolutions are parametrized by graph shifts in the same way that time and spatial convolutions are parametrized by delays and translations. From a complexity standpoint, the independence between the GNN parametrization and the graph is beneﬁcial because there are less parameters to learn. Perhaps more importantly, the fact that its parameters are not tied to the underlying graph suggests that a GNN can be transferred from graph to graph. It is then natural to ask to what extent the performance of a GNN is preserved when its graph changes. The ability to transfer a machine learning model with performance guarantees is usually referred to as transfer learning or transferability.
In GNNs, there are two typical scenarios where transferability is desirable. The ﬁrst involves applications in which we would like to reproduce a previously trained model on a graph of different 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
size, but similar to the original graph in a sense that we formalize in this paper, with performance guarantees. This is useful when we cannot afford to retrain the model. The second concerns problems where the network size changes over time. In this scenario, we would like the GNN model to be robust to nodes being added or removed from the network, i.e., for it to be transferable in a scalable way. An example are recommender systems based on a growing user network.
Both of these scenarios involve solving the same task on networks that, although different, can be seen as being of the same “type”. This motivates studying the transferability of GNNs within families of graphs that share certain structural characteristics. We propose to do so by focusing on collections of graphs associated with the same graphon. A graphon is a bounded symmetric kernel
W : [0, 1]2
[0, 1] that can be interpreted a a graph with an uncountable number of nodes. Graphons are suitable representations of graph families because they are the limit objects of sequences of graphs where the density of certain structural “motifs” is preserved. They can also be used as generating models for undirected graphs where, if we associate nodes i and j with points ui and uj in the unit interval, W(ui, uj) is the weight of the edge (i, j). The main result of this paper (Theorem 2) shows that GNNs are transferable between deterministic graphs obtained from a graphon in this way.
→ 2 (cid:107)
).
=
−
O (n−0.5
Φ(Gn2)
Φ(Gn1) (cid:107)
Theorem (GNN transferability, informal) Let Φ(G) be a GNN with ﬁxed parameters. Let Gn1 and Gn2 be deterministic graphs with n1 and n2 nodes obtained from a graphon W. Under mild 1 + n−0.5 conditions,
An important consequence of this result is the existence of a trade-off between transferability and discriminability for GNNs on these deterministic graphs, which is related to a restriction on the passing band of the graph convolutional ﬁlters of the GNN. Its proof is based on the deﬁnition of the graphon neural network (Section 4), a theoretical limit object of independent interest that can be used to generate GNNs on deterministic graphs from a common family. The interpretation of graphon neural networks as generating models for GNNs is important because it identiﬁes the graph as a
ﬂexible parameter of the learning architecture and allows adapting the GNN not only by changing its weights, but also by changing the underlying graph. While this result only applies to deterministic graphs instantiated from graphons (i.e., it does not encompass stochastic graphs sampled from the graphon, or sparser graphs, which are better modeled by graphings (Lovász, 2012, Chapter 18)), it provides useful insights on GNNs and on their transferability properties.
The rest of this paper is organized as follows. Section 2 goes over related work. Section 3 introduces preliminary deﬁnitions and discusses GNNs and graphon information processing. The aforementioned contributions are presented in Sections 4 and 5. In Section 6, transferability of GNNs is illustrated in two numerical experiments. Concluding remarks are presented in Section 7, and proofs and additional numerical experiments are deferred to the supplementary material. 2