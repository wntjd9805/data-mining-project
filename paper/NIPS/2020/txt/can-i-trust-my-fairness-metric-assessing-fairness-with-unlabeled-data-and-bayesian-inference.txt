Abstract
We investigate the problem of reliably assessing group fairness when labeled examples are few but unlabeled examples are plentiful. We propose a general
Bayesian framework that can augment labeled data with unlabeled data to produce more accurate and lower-variance estimates compared to methods based on labeled data alone. Our approach estimates calibrated scores for unlabeled examples in each group using a hierarchical latent variable model conditioned on labeled examples.
This in turn allows for inference of posterior distributions with associated notions of uncertainty for a variety of group fairness metrics. We demonstrate that our approach leads to signiﬁcant and consistent reductions in estimation error across multiple well-known fairness datasets, sensitive attributes, and predictive models.
The results show the beneﬁts of using both unlabeled data and Bayesian inference in terms of assessing whether a prediction model is fair or not. 1

Introduction
Machine learning models are increasingly used to make important decisions about individuals. At the same time it has become apparent that these models are susceptible to producing systematically biased decisions with respect to sensitive attributes such as gender, ethnicity, and age [Angwin et al., 2017, Berk et al., 2018, Corbett-Davies and Goel, 2018, Chen et al., 2019, Beutel et al., 2019]. This has led to a signiﬁcant amount of recent work in machine learning addressing these issues, including research on both (i) deﬁnitions of fairness in a machine learning context (e.g., Dwork et al. [2012],
Chouldechova [2017]), and (ii) design of fairness-aware learning algorithms that can mitigate issues such as algorithmic bias (e.g., Calders and Verwer [2010], Kamishima et al. [2012], Feldman et al.
[2015], Zafar et al. [2017], Chzhen et al. [2019]).
In this paper we focus on an important yet under-studied aspect of the fairness problem: reliably assessing how fair a blackbox model is, given limited labeled data. In particular, we focus on assessment of group fairness of binary classiﬁers. Group fairness is measured with respect to parity in prediction performance between different demographic groups. Examples include differences in performance for metrics such as true positive rates and false positive rates (also known as equalized odds [Hardt et al., 2016]), accuracy [Chouldechova, 2017], false discovery/omission rates [Zafar et al., 2017], and calibration and balance [Kleinberg et al., 2016].
Despite the simplicity of these deﬁnitions, a signiﬁcant challenge in assessment of group fairness is high variance in estimates of these metrics based on small amounts of labeled data. To illustrate this point, Figure 1 shows frequency-based estimates of group differences in true positive rates (TPRs) for four real-world datasets. The boxplots clearly show the high variability for the estimated TPRs relative to the true TPRs (shown in red) as a function of the number of labeled examples nL. In many cases the estimates are two or three or more times larger than the true difference. In addition, a relatively large percentage of the estimates have the opposite sign of the true difference, potentially 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Boxplots of frequency-based estimates of the difference in true positive rate (TPR) for four fairness datasets and binary sensitive attributes, across 1000 randomly sampled sets of labeled test examples of size nL = 50, 100, 200. The horizontal red line is the TPR difference computed on the full test dataset. leading to mistaken conclusions. The variance of these estimates decreases relatively slowly, e.g., at a rate of approximately 1 n for group differences in accuracy where n is the number of labels in the smaller of the two groups1. Imbalances in label distributions can further compound the problem, for example for estimation of group differences in TPR or FPR. For example, consider a simple simulation with two groups, where the underrepresented group makes up 20% of the whole dataset, groupwise positive rates P (y = 1) are both 20%, and the true groupwise TPRs are 95% and 90% (additional details in the Supplement). In order to ensure that there is a 95% chance that our estimate of the true TPR difference (which is 0.05) lies in the range [0.04, 0.06] we need at least 96k labeled instances. Yet for real-world datasets used in the fairness literature (e.g., Friedler et al. [2019]; see also Table 1 later in the paper), test set sizes are often much smaller than this, and it is not uncommon for the group and label distributions to be even more imbalanced.
The real-world and synthetic examples above show that frequentist assessment of group fairness is unreliable unless the labeled dataset is unrealistically large. Acquiring large amounts of labeled data can be difﬁcult and time-consuming, particularly for the types of applications where fairness is important, such as decision-making in medical or criminal justice contexts [Angwin et al., 2017, Berk et al., 2018, Rajkomar et al., 2018]. This is in contrast to applications such as image classiﬁcation where approaches like Mechanical Turk can be used to readily generate large amounts of labeled data.
To address this problem, we propose to augment labeled data with unlabeled data to generate more accurate and lower-variance estimates compared to methods based on labeled data alone. In particular, the three primary contributions of this paper are (1) a comprehensive Bayesian treatment of fairness assessment that provides uncertainty about estimates of group fairness metrics; (2) a new Bayesian methodology that uses calibration to leverage information from both unlabeled and labeled examples; and, (3) systematic large-scale experiments across multiple datasets, models, and attributes that show that using unlabeled data can reduce estimation error signiﬁcantly. 2 Fairness Assessment with Bayesian Inference and Unlabeled Data 2.1 Notation and Problem Statement
Consider a trained binary classiﬁcation model M , with inputs x and class labels y ∈ {0, 1}. The model produces scores2 s = PM (y = 1|x) ∈ [0, 1], where PM denotes the fact that this is the model’s estimate of the probability that y = 1 conditioned on x. Under 0-1 loss the model predicts
ˆy = 1 if s ≥ 0.5 and ˆy = 0 otherwise. The marginal accuracy of the classiﬁer is P (ˆy = y) and the accuracy conditioned on a particular value of the score s is P (ˆy = y|s). A classiﬁer is calibrated if P (ˆy = y)|s) = s, e.g., if whenever the model produces a score of s = 0.9 then its prediction is correct 90% of the time. For group fairness we are interested in potential differences in performance metrics with respect to a sensitive attribute (such as gender or race) whose values g correspond to different groups, g ∈ {0, 1, . . . , G − 1}. We will use θg to denote a particular metric of interest, such 1Stratiﬁed sampling by group could help with this issue (e.g., see Sawade et al. [2010]), but stratiﬁcation might not always be possible in practice, and the total variance will still converge slowly overall. 2Note that the term “score" is sometimes deﬁned differently in the calibration literature as the maximum class probability for the model. Both deﬁnitions are equivalent mathematically for binary classiﬁcation. 2
as accuracy, TPR, FPR, etc. for group g. We focus on group differences for two groups, deﬁned as ∆ = θ1 − θ0, e.g., the difference in a model’s predictive accuracy between females and males,
∆ = P (ˆy = y|g = 1) − P (ˆy = y|g = 0).
We assume in general that the available data consists of both nL labeled examples and nU unlabeled examples, where nL (cid:28) nU , which is a common situation in practice where far more unlabeled data is available than labeled data. For the unlabeled examples, we do not have access to the true labels yj but we do have the scores sj = PM (yj = 1|xj), j = 1, . . . , nU . For the labeled examples, we have the true labels yi as well as the scores si, i = 1, . . . , nL. The examples (inputs x, scores s, and labels y if available) are sampled IID from an underlying joint distribution P (x, y) (or equivalently P (s, y) given that s is a deterministic function via M of x), where this underlying distribution represents the population we wish to evaluate fairness with respect to. Note that in practice P (x, y) might very well not be the same distribution the model was trained on. For unlabeled data Du the corresponding distributions are P (x) or P (s). 2.2 Beta-Binomial Estimation with Labeled Data
Consider initially the case with only labeled data DL (i.e., nU = 0) and for simplicity let the metric of interest ∆ be group difference in classiﬁcation accuracy. Let Ii = Iˆyi=yi, 1 ≤ i ≤ nL, be a binary indicator of whether each labeled example i was classiﬁed correctly or not by the model.
The binomial likelihood for group accuracy θg, g = 0, 1, treats the Ii’s as conditionally independent draws from a true unknown accuracy θg, Ii ∼ Bernoulli(θg). We can perform Bayesian inference on the θg’s by specifying conjugate Beta(αg, βg) priors for each θg, combining these priors with the binomial likelihoods, and obtaining posterior densities in the form of the beta densities on each
θg. From here we can get a posterior density on the group difference in accuracy, P (∆|DL) where
∆ = θ1 − θ0. Since the density for the difference of two beta-distributed quantities (the θ’s) is not in general in closed form, we use posterior simulation (e.g., Gelman et al. [2013]) to obtain posterior samples of ∆ by sampling θ’s from their posterior densities and taking the difference. For metrics such as TPR we place beta priors on conditional quantities such as θg = P (ˆy = 1|y = 1, g). In all of the results in the paper we use weak uninformative priors for θg with αg = βg = 1. This general idea of using Bayesian inference on classiﬁer-related metrics has been noted before for metrics such marginal accuracy [Benavoli et al., 2017], TPR [Johnson et al., 2019], and precision-recall [Goutte and Gaussier, 2005], but has not been developed or evaluated in the context of fairness assessment.
This beta-binomial approach above provides a useful, simple, and practical tool for understanding and visualizing uncertainty about fairness-related metrics, conditioned on a set of nL labeled examples.
However, with weak uninformative priors, the posterior density for ∆ will be relatively wide unless nL is very large, analogous to the high empirical variance for frequentist point estimates in Figure 1. As with the frequentist variance, the width of the posterior density on ∆ will decrease relatively slowly at a rate of approximately 1
. This motivates the main goal of the paper: can we combine nL unlabeled examples with labeled examples to make more accurate inferences about fairness metrics? 2.3 Leveraging Unlabeled Data with a Bayesian Calibration Model
Consider the situation where we have nU unlabeled examples, in addition to the nL labeled ones. For each unlabeled example j = 1, . . . , nU we can use the model M to generate a score, sj = PM (yj = 1|xj). If the model M is perfectly calibrated then the model’s score is the true probability that y = 1, i.e., we have sj = PM (yj = 1|sj) and the accuracy equals sj if sj ≥ 0.5 and 1 − sj otherwise.
Therefore, in the perfectly calibrated case, we could empirically estimate accuracy per group for the unlabeled data using scores via ˆθg = (1/nU,g) (cid:80) j∈g sjI(sj ≥ 0.5) + (1 − sj)I(sj < 0.5), where nU,g is the number of unlabeled examples that belong to group g. Metrics other than accuracy could also be estimated per group in a similar fashion.
In practice, however, many classiﬁcation models, particularly complex ones such as deep learning models, can be signiﬁcantly miscalibrated (see, e.g., Guo et al. [2017], Kull et al. [2017], Kumar et al.
[2019], Ovadia et al. [2019]) and using the uncalibrated scores in such situations will lead to biased estimates of the true accuracy per group. The key idea of our approach is to use the labeled data to learn how to calibrate the scores such that the calibrated scores can contribute to less biased estimates of accuracy. Let zj = E[I(ˆyj = yj)] = P (yj = ˆyj|sj ) be the true (unknown) accuracy of the model 3
Figure 2: Hierarchical Bayesian calibration of two demographic groups across four dataset-group pairs, with posterior means and 95% credible intervals per group. The x-axis is the model score s for class y = 1, and the y-axis is the calibrated score. Instances in each group are binned into 5 equal-sized bins by model score, and blue and red points show the fraction of positive samples per group for each bin. given score sj. We treat each zj, j = 1, . . . , nU as a latent variable per example. The high-level steps of the approach are as follows:
• We use the nL labeled examples to estimate groupwise calibration functions with parameters
φg, that transform the (potentially) uncalibrated scores s of the model to calibrated scores.
More speciﬁcally, we perform Bayesian inference (see Section 2.4 below) to obtain posterior samples from P (φg|DL) for the groupwise calibration parameters φg.
• We then obtain posterior samples from Pφg (zj|DL, sj) for each unlabeled example j = 1, . . . , nU , conditioned on posterior samples of the φg’s.
• Finally, we use posterior samples from the zj’s, combined with the labeled data, to generate estimates of the groupwise metrics θg and the difference in metrics ∆.
We can compute a posterior sample for θt nU , by combining estimates of accuracies for the unlabeled examples with the observed outcomes for the labeled instances: g, given each set of posterior samples for φt 1, . . . , zt g and zt
θt g = 1 nL,g + nU,g (cid:18) (cid:88) i:i∈g
I(ˆyi = yi) + (cid:19) zt j (cid:88) j:j∈g (1) where t = 1, ..., T is an index over T MCMC samples. These posterior samples in turn can be used to generate an empirical posterior distribution {∆1, . . . , ∆T } for ∆, where ∆t = θt 0. Mean posterior estimates can be obtained by averaging over samples, i.e. ˆ∆ = (1/T ) (cid:80)T t ∆t. Even with very small amounts of labeled data (e.g., nL = 10) we will demonstrate later in the paper that we can make much more accurate inferences about fairness metrics via this Bayesian calibration approach, compared to using only the labeled data directly. 1 − θt 2.4 Hierarchical Bayesian Calibration
Bayesian calibration is a key step in our approach above. We describe Bayesian inference below for the beta calibration model speciﬁcally [Kull et al., 2017] but other calibration models could also be used. The beta calibration model maps a score from a binary classiﬁer with scores s = PM (y = 1|x) ∈ [0, 1] to a recalibrated score according to: f (s; a, b, c) = 1 1 + e−c−a log s+b log(1−s) (2) where a, b, and c are calibration parameters with a, b ≥ 0. This model can capture a wide variety of miscalibration patterns, producing the identity mapping if s is already calibrated when a = b = 1, c = 0. Special cases of this model are the linear-log-odds (LLO) calibration model [Turner et al., 2014] when a = b, and temperature scaling [Guo et al., 2017] when a = b, c = 0.
In our hierarchical Bayesian extension of the beta calibration model, we assume that each group (e.g., female, male) is associated with its own set of calibration parameters φg = {ag, bg, cg} and therefore 4
each group can be miscalibrated in different ways (e.g., see Figure 2). To apply this model to the observed data, we assume that the true labels for the observed instances are sampled according to: yi ∼ Bernoulli(cid:0)f (si; agi, bgi, cgi)(cid:1) where gi is the group associated with instance i, 1 ≤ i ≤ nL. For any unlabeled example j = 1, . . . , nU , conditioned on calibration parameters φgj for the group for j, we can compute the latent variable zj = f (sj; . . .)I(sj ≥ 0.5) + (1 − f (sj; . . .))I(sj < 0.5), i.e., the calibrated probability that the model’s prediction on instance j is correct.
We assume that the parameters from each individual group are sampled from a shared distribution: log ag ∼ N(µa, σa), log bg ∼ N(µb, σb), cg ∼ N(µc, σc) where π = {µa, σa, µb, σb, µc, σc} is the set of hyperparameters of the shared distributions. We complete the hierarchical model by placing the following priors on the hyperparameters (TN is the truncated normal distribution):
µa ∼ N(0, .4), µb ∼ N(0, .4), µc ∼ N(0, 2), σa ∼ TN(0, .15), σb ∼ TN(0, .15), σc ∼ TN(0, .75)
These priors were chosen to place reasonable bounds on the calibration parameters and allow for diverse patterns of miscalibration (e.g., both over and under-conﬁdence or a model) to be expressed a priori. We use exactly these same prior settings in all our experiments across all datasets, all groups, and all labeled and unlabeled dataset sizes, demonstrating the robustness of these settings across a wide variety of contexts. In addition, the Supplement contains results of a sensitivity analysis for the variance parameters in the prior, illustrating robustness across a broad range of settings of these parameters.
The model was implemented as a graphical model (see Supplement) in JAGS, a common tool for
Bayesian inference with Markov chain Monte Carlo [Plummer, 2003]. All of the results in this paper are based on 4 chains, with 1500 burn-in iterations and 200 samples per chain, resulting in
T = 800 sample overall. These hyperparameters were determined based on a few simulation runs across datasets, checking visually for lack of auto-correlation, with convergence assessed using the standard measure of within-to-between-chain variability. Although MCMC can sometimes be slow for high-dimensional problems, with 100 labeled data points (for example) and 10k unlabeled data points the sampling procedure takes about 30 seconds (using non-optimized Python/JAGS code on a standard desktop computer) demonstrating the practicality of this procedure.
Theoretical Considerations: Lemma 2.1 below relates potential error in the calibration mapping (e.g., due to misspeciﬁcation of the parametric form of the calibration function f (s; . . .)) to error in the estimate of ∆ itself.
Lemma 2.1. Given a prediction model M and score distribution P (s), let fg(s; φg) : [0, 1] → [0, 1] denote the calibration model for group g; let f ∗ g (s) : [0, 1] → [0, 1] be the optimal calibration function which maps s = PM (ˆy = 1|g) to P (y = 1|g); and ∆∗ is the true value of the metric.
Then the absolute error of the expected estimate w.r.t. φ can be bounded as: |Eφ∆ − ∆∗| ≤ (cid:107) ¯f0 − f ∗ 1 (cid:107)1, where ¯fg(s) = Eφg fg(s; φg), ∀s ∈ [0, 1], and (cid:107) · (cid:107)1 is the expected L1 distance w.r.t. P (s|g). (Proof provided in the Supplement). 0 (cid:107)1 + (cid:107) ¯f1 − f ∗
Thus, reductions in the L1 calibration error directly reduce an upper bound on the L1 error in estimating ∆. The results in Figure 2 suggest that even with the relatively simple parametric beta calibration method, the error in calibration (difference between the ﬁtted calibration functions) (blue and red curves) and the empirical data (blue and red dots) is quite low across all 4 datasets. The possibility of using more ﬂexible calibration functions is an interesting direction for future work. 3 Datasets, Classiﬁcation Models, and Illustrative Results
One of the main goals of our experiments is to assess the accuracy of different estimation methods, using relatively limited amounts of labeled data, relative to the true value of the metric. By “true value" we mean the value we could measure on an inﬁnitely large test sample. Since such a sample is not available, we use as a proxy the value of metric computed on all of the test set for each dataset in our experiments.
We followed the experimental methods and used the code for preprocessing and training prediction models from Friedler et al. [2019] who systematically compared fairness metrics and fairness-aware 5
Table 1: Datasets used in the paper. G is the sensitive attribute, P (g = 0) is the probability of the privileged group, and P (y = 1) is the probability of the positive label for classiﬁcation. The privileged groups g = 0 are gender: male, age: senior or adult, and race: white or Caucasian.
Dataset
Test Size
G
P (g = 0) P (y = 1)
Adult
Bank
German
Compas-R
Compas-VR
Ricci 10054 13730 334 2056 1337 40 gender, race age age, gender gender, race gender, race race 0.68, 0.86 0.45 0.79, 0.37 0.7, 0.85 0.8, 0.34 0.65 0.25 0.11 0.17 0.69 0.47 0.50
Figure 3: Posterior density (samples) and frequentist estimates (dotted vertical blue lines) for the difference in group accuracy ∆ for 4 datasets with nL = 20 random labeled examples for both the BB (beta-binomial) and BC (Bayesian calibration) methods. Ground truth is a vertical black line. The underlying model is an MLP. The 20 examples were randomly sampled 20 different times.
Upper plots show the histograms of posterior samples for the ﬁrst sample, lower plots show the 95% posterior credible intervals for all 20 runs, where the x-axis is ∆. algorithms across a variety of datasets. Speciﬁcally, we use the Adult, German Credit, Ricci, and
Compas datasets (for recidivism and violent recidivism), all used in Friedler et al. [2019], as well as the Bank Telemarketing dataset [Moro et al., 2014]. Summary statistics for the datasets are shown in
Table 1. Classiﬁcation models (logistic regression, multilayer perceptron (MLP) with a single hidden layer, random forests, Gaussian Naive Bayes) were trained using standard default parameter settings with the code provided by Friedler et al. [2019] and predictions generated on the test data. Sensitive attributes are not included as inputs to the models. Unless a speciﬁc train/test split is provided in the original dataset, we randomly sample 2/3 of the instances for training and 1/3 for test. Additional details on models and datasets are provided in the Supplement.
Illustrative Results: To illustrate our approach we compare the results of the frequentist, beta-binomial (BB), and Bayesian calibration (BC) approaches for assessing group differences in accuracy across 4 datasets, for a multilayer perceptron (MLP) binary classiﬁer. We ran the methods on 20 runs of randomly sampled sets of nL = 20 labeled examples. The BC method was given access to the remaining nU unlabeled test examples minus the 20 labeled examples for each run, as described in
Table 1. We deﬁne ground truth as the frequentist ∆ value computed on all the labeled data in the test set. Figure 3 shows the results across the 4 datasets. The top ﬁgure corresponds to the ﬁrst run out of 20 runs, showing the histogram of 800 posterior samples from the BB (blue) and BC (red) methods.
The lower row of plots summarizes the results for all 20 runs, showing the 95% posterior credible intervals (CIs) (red and blue horizontal lines for BC and BB respectively) along with posterior means (red and blue marks).
Because of the relatively weak prior (Beta(1,1) on group accuracy) the posterior means of the BB samples tend to be relatively close to the frequentist estimate (light and dark blue respectively) on each run and both can be relatively far away from ground truth value for ∆ (in black). Although 6
Figure 4: Mean absolute error (MAE) of the difference between algorithm estimates and ground truth for group difference in FPR, as a function of number of labeled instances, for 8 different dataset-group pairs. Shading indicates 95% error bars for each method. the BB method is an improvement over being frequentist, in that it provides posterior uncertainty about ∆, it nonetheless has high variance (locations of the posterior means) as well as high posterior uncertainty (relatively wide CIs). The BC method in contrast, by using the unlabeled data in addition to the labeled data, produces posterior estimates where the mean tends to be much closer to ground truth than BC.
The posterior information about ∆ can be used to provide users with a summary report that includes information about the direction of potential bias (e.g., P (∆ > 0|DL, DU ), the degree of bias (e.g., via the MPE ˆ∆), 95% posterior CIs on ∆, and the probability that the model is “practically fair" (assessed via P (|∆| < (cid:15)|DL, DU ), e.g., see Benavoli et al. [2017]). For example with BC, given the observed data, practitioners can conclude from the information in the upper row of Figure 3, and with (cid:15) = 0.02, that there is a 0.99 probability for the Adult data that the classiﬁer is more accurate for females than males; and with probability 0.87 that the classiﬁer is practically fair with respect to accuracy for junior and senior individuals in the Bank data. 4 Experiments and Results
In this section we systematically evaluate the quality of different estimation approaches by repeating the same type of experiment as in Section 3 and Figure 3 across different amounts of labeled data nL. In particular, for each value of nL we randomly sample sets of labeled datasets of size nL, generate point estimates of a metric ∆ of interest for each labeled dataset for each of the BB and
BC estimation methods, and compute the mean absolute error (MAE) between the point estimates and the true value (computed on the full labeled test set). The frequency-based estimates are not shown for clarity—they are almost always worse than both BB and BC. As an example, Figure 4 illustrates the quality of estimation where ∆ is the FPR group difference ∆ for the MLP classiﬁcation model, evaluated across 8 different dataset-group pairs. Each y-value is the average of 100 different randomly sampled sets of nL instances, where nL is the corresponding x-axis value. The BC method dominates BB across all datasets indicating that the calibrated scores are very effective at improving the accuracy in estimating group FPR. This is particularly true for small amounts of labeled data (e.g., up to nL = 100) where the BB Method can be highly inaccurate, e.g., MAEs on the order of 10 or 20% when the true value of ∆ is often less than 10%.
In the Supplement we show that the trend of results shown in Figure 4, namely that BC produces signiﬁcantly more accurate estimates of group fairness metrics ∆, is replicated across all 4 classiﬁca-tion models that we investigated, across FPR, TPR and Accuracy metrics, and across all datasets. To summarize the full set of results we show a subset in tabular form, across all 4 classiﬁcation models and 10 dataset-group pairs, with nL ﬁxed: Table 2 for Accuracy with nL = 10 and Table 3 for TPR with nL = 200. (We used larger nL values for TPR and FPR than for accuracy in the results above since TPR and FPR depend on estimating conditional probabilities that can have zero supporting counts in the labeled data, causing a problem for frequentist estimators). The results above and in the 7
Table 2: MAE for ∆ Accuracy Estimates, with nL = 10, across 100 runs of labeled samples, for 4 different trained models (groups of columns) and 10 different dataset-group combinations (rows). Lowest error rate per row-col group in bold if the difference among methods are statistically signiﬁcant under Wilcoxon signed-rank test (p=0.05). Estimation methods are Freq (Frequentist),
BB, and BC. Freq and BB use only labeled samples, BC uses both labeled samples and unlabeled data. Trained models are Multilayer Perceptron, Logistic Regression, Random Forests, and Gaussian
Naive Bayes.
Multi-layer Perceptron
Logistic Regression
Random Forest
Gaussian Naive Bayes
Dataset, Attribute
Adult, Race
Adult, Gender
Bank, Age
German, Age
German, Gender
Compas-R, Race
Compas-R, Gender
Compas-VR, Race
Compas-VR, Gender
Ricci, Race
Freq 16.5 19.7 15.9 34.6 30.7 31.5 33.7 18.7 20.6 23.5
BB 18.5 17.4 13.9 19.8 21.6 21.0 21.6 17.1 16.9 17.7
BC 3.9 5.1 2.5 5.0 8.2 4.2 5.0 4.0 5.4 14.6
Freq 16.4 19.1 13.9 37.1 25.6 31.7 34.3 18.5 19.9 14.6
BB 18.7 16.1 13.0 21.2 17.4 20.4 21.9 15.6 16.6 14.6
BC 2.9 2.2 1.4 8.7 6.3 4.8 3.8 4.4 5.3 7.9
Freq
BB
BC 16.5 17.7 11.8 33.6 27.7 29.3 36.3 18.2 22.3 6.3 18.2 17.4 11.1 18.7 19.3 20.3 23.3 15.8 19.0 12.2 3.2 4.8 1.0 8.2 8.6 2.4 4.4 2.4 6.3 2.1
Freq 17.6 19.7 15.5 36.6 30.0 33.5 40.5 26.6 31.3 8.9
BB 18.9 16.2 13.7 20.4 20.1 23.2 25.5 19.8 21.5 13.1
BC 3.6 5.4 1.7 11.5 6.5 8.4 13.7 6.5 9.8 1.6
Table 3: MAE for ∆ TPR Estimates, with nL = 200. Same setup as for Table 2. Compas-VR race and Ricci race are not included since there are no positive instances for some groups, and some entries under Freq cannot be estimated for the same reason.
Multi-layer Perceptron
Gaussian Naive Bayes
Logistic Regression
Random Forest
Dataset, Attribute
Freq
BB
Adult, Race
Adult, Gender
Bank, Age
German, Age
German, Gender
Compas-R, Race
Compas-R, Gender
Compas-VR, Gender
— 12.5 14.3 16.3 15.0 16.8 4.7 4.7 0.7 1.0
— 7.6 9.5 12.2 10.0 14.9
BC 5.8 4.3 4.8 3.0 1.6 2.5 1.9 2.9
Freq
BB
— 14.7 14.0 15.8 15.9 17.7 5.4 5.6 3.3 3.3
— 7.9 9.4 10.7 10.0 8.9
BC 7.0 4.6 4.2 2.6 2.1 2.6 1.8 2.0
Freq
BB
BC
Freq
BB
— 14.3 14.2 16.1 14.9 16.6 5.1 5.1 3.1 3.2
— 9.2 10.7 10.5 11.3 14.6 4.6 7.3 3.1 3.1 2.1 2.1 2.6 7.2
— 14.6 13.4 15.0 15.7 17.3 6.5 6.8 4.8 4.7
— 4.5 5.5 5.6 10.0 12.5
BC 3.0 11.5 2.3 2.8 2.2 2.0 0.3 1.3
Supplement demonstrate the signiﬁcant gains in accuracy that can be achieved with the proposed approach. We also evaluated the effect of using LLO calibration instead of beta calibration methods and found little difference between the two methods (details in Supplement).
For concreteness we demonstrated our results with three popular fairness metrics (∆ accuracy, TPR, and FPR) in the paper. However, we can directly extend this approach to handle metrics such as calibration and balance [Kleinberg et al., 2016] as well as ratio-based metrics. In particular, by predicting the distribution of class labels y with the calibrated model scores, any fairness metric that can be deﬁned as a deterministic function of calibrated model scores s, labels y and groups g can levarage unlabeled data to reduce variance using our proposed method.
Consideration of the bias-variance properties of the different methods reveals a fundamental tradeoff.
The labeled data contribute no bias to the estimate but can have high variance for small nL, whereas the unlabeled data (via their calibrated scores) contribute little variance but can have a persistent bias due to potential misspeciﬁcation in the parametric calibration model. An open question, that is beyond the scope of this paper, is how to balance this bias-variance tradeoff in a more adaptive fashion as a function of nL and nU , to further improve the accuracy of estimates of fairness metrics for arbitrary datasets. One potential option would be to a more ﬂexible calibration method (e.g.,
Gaussian process calibration as proposed in Wenger et al. [2020]). Another option would be to automatically quantify the calibration bias and tradeoff the contributions of labeled and unlabeled data accordingly in estimating θg’s and ∆.
We also found empirically that while the posterior credible intervals (CIs) for the BB method are well-calibrated, those for BC tended to be overconﬁdent as nL increases (see Supplement for details).
This is likely due to misspeciﬁcation in the parametric beta calibration model. An interesting and important direction for future work is to develop methods that are better calibrated in terms of posterior credible intervals (e.g., Syring and Martin [2019]) and that can retain the low-variance advantages of the BC approach we propose here. 8
5