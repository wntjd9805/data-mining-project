Abstract
Reward-free reinforcement learning (RL) is a framework which is suitable for both the batch RL setting and the setting where there are many reward functions of interest. During the exploration phase, an agent collects samples without using a pre-speciﬁed reward function. After the exploration phase, a reward function is given, and the agent uses samples collected during the exploration phase to compute a near-optimal policy. Jin et al. [2020] showed that in the tabular setting, the agent only needs to collect polynomial number of samples (in terms of the number states, the number of actions, and the planning horizon) for reward-free
RL. However, in practice, the number of states and actions can be large, and thus function approximation schemes are required for generalization. In this work, we give both positive and negative results for reward-free RL with linear function approximation. We give an algorithm for reward-free RL in the linear Markov decision process setting where both the transition and the reward admit linear representations. The sample complexity of our algorithm is polynomial in the feature dimension and the planning horizon, and is completely independent of the number of states and actions. We further give an exponential lower bound for reward-free RL in the setting where only the optimal Q-function admits a linear representation. Our results imply several interesting exponential separations on the sample complexity of reward-free RL. 1

Introduction
In reinforcement learning (RL), an agent repeatedly interacts with an unknown environment to maximize the cumulative reward. To achieve this goal, RL algorithms must be equipped with exploration mechanisms to effectively solve tasks with long horizons and sparse reward signals.
Empirically, there is a host of successes by combining deep RL methods with different exploration strategies. However, the theoretical understanding of exploration in RL by far is rather limited.
In this work we study the reward-free RL setting which was formalized in the recent work by Jin et al. [2020]. There are two phases in the reward-free setting: the exploration phase and the planning phase. During the exploration phase, the agent collects trajectories from an unknown environment without any pre-speciﬁed reward function. Then, in the planning phase, a speciﬁc reward function is given to the agent, and the goal is to use samples collected during the exploration phase to output a near-optimal policy for the given reward function. From a practical point of view, this paradigm is particularly suitable for 1) the batch RL setting [Bertsekas and Tsitsiklis, 1996] where data collection and planning are explicitly separated and 2) the setting where there are multiple reward function
∗Correspondence to:
<ssdu@cs.washington.edu>,
<rsalakhu@cs.cmu.edu>.
Ruosong Wang
Yang
F.
Lin
†Carnegie Mellon University
‡University of Washington, Seattle
§University of California, Los Angeles
<ruosongw@andrew.cmu.edu>,
<linyang@ee.ucla.edu>,
Ruslan
Simon
S. Du
Salakhutdinov 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of interest, e.g., constrained RL [Achiam et al., 2017, Altman, 1999, Tessler et al., 2018]. From a theoretical point view, this setting separates the exploration problem and the planning problem which allows one to handle them in a theoretically principled way, in contrast to the standard RL setting where one needs to deal with both problems simultaneously.
Key in this framework is to collect a dataset with sufﬁciently good coverage over the state space during the exploration phase, so that one can apply a batch RL algorithm on the dataset [Chen and Jiang, 2019, Agarwal et al., 2019, Antos et al., 2008, Munos and Szepesvári, 2008] during the planning phase. For the reward-free RL setting, existing theoretical works only apply to the tabular
RL setting. Jin et al. [2020] showed that in the tabular setting where the state space has bounded size, (cid:101)O(poly(|S||A|H)/ε2) samples during the exploration phase is necessary and sufﬁcient in order to output ε-optimal policies in the planning phase. Here, |S| is the number of states, |A| is the number of actions and H is the planning horizon.
The sample complexity bound in [Jin et al., 2020], although being near-optimal in the tabular setting, can be unacceptably large in practice due to the polynomial dependency on the size of the state space. For environments with a large state space, function approximation schemes are needed for generalization. RL with linear function approximation is arguably the simplest yet most fundamental setting. Clearly, in order to understand more general function classes, e.g., deep neural networks, one must understand the class of linear functions ﬁrst. In this paper, we study RL with linear function approximation in the reward-free setting, and our goal is to answer the following question:
Is it possible to design provably efﬁcient RL algorithms with linear function approximation in the reward-free setting?
We obtain both a polynomial upper bound and a hardness result to the above question.
Our Contributions. Our ﬁrst contribution is a provably efﬁcient algorithm for reward-free RL under the linear MDP assumption [Yang and Wang, 2019, Jin et al., 2019], which, roughly speaking, requires both the transition operators and the reward functions to be linear functions of a d-dimensional feature extractor given to the agent. See Assumption 2.1 for the formal statement of the linear MDP assumption. Our algorithm, formally presented in Section 3, samples (cid:101)O (cid:0)d3H 6/ε2(cid:1) trajectories during the exploration phase, and outputs ε-optimal policies for an arbitrary number of reward functions satisfying Assumption 2.1 during the planning phase with high probability. Here d is the feature dimension, H is the planning horizon and ε is the required accuracy.
One may wonder whether is possible to further weaken the linear MDP assumption, since it requires the feature extractor to encode model information, and such feature extractor might be hard to construct in practice. Our second contribution is a hardness result for reward-free RL under the linear
Q∗ assumption, which only requires the optimal value function to be a linear function of the given feature extractor and thus weaker than the linear MDP assumption. Our hardness result, formally presented in Section 4, shows that under the linear Q∗ assumption, any algorithm requires exponential number of samples during the exploration phase, so that the agent could output a near-optimal policy during the planning phase with high probability. The hardness result holds even when the MDP is deterministic.
Our results highlight the following conceptual insights.
• Reward-free RL might require the feature to encode model information. Under model-based assumption (linear MDP assumption), there exists a polynomial sample complexity upper bound for reward-free RL, while under value-based assumption (linear Q∗ assump-tion), there is an exponential sample complexity lower bound. Therefore, the linear Q∗ assumption is strictly weaker than the linear MDP assumption in the reward-free setting.
• Reward-free RL could be exponentially harder than standard RL. For deterministic systems, under the assumption that the optimal Q-function is linear, there exists a polynomial sample complexity upper bound [Wen and Van Roy, 2013] in the standard RL setting.
However, our hardness result demonstrates that under the same assumption, any algorithm requires exponential number of samples in the reward-free setting.
• Simulators could be exponentially more powerful. In the setting where the agent has sampling access to a generative model (a.k.a. simulator) of the MDP, the agent can query the next state s(cid:48) sampled from the transition operator given any state-action pair as input. In 2
the supplementary material, we show that for deterministic systems, under the linear Q∗ assumption, there exists a polynomial sample complexity upper bound in the reward-free setting when the agent has sampling access to a generative model. Compared with the hardness result above, this upper bound demonstrates an exponential separation between the sample complexity of reward-free RL in the generative model and that in the standard
RL model. To the best our knowledge, this is the ﬁrst exponential separation between the standard RL model and the generative model for a natural question. 1.1