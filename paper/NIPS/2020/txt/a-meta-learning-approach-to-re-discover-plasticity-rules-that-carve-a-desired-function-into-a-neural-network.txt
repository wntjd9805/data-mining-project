Abstract
The search for biologically faithful synaptic plasticity rules has resulted in a large body of models. They are usually inspired by – and ﬁtted to – experimental data, but they rarely produce neural dynamics that serve complex functions. These failures suggest that current plasticity models are still under-constrained by ex-isting data. Here, we present an alternative approach that uses meta-learning to discover plausible synaptic plasticity rules. Instead of experimental data, the rules are constrained by the functions they implement and the structure they are meant to produce. Brieﬂy, we parameterize synaptic plasticity rules by a Volterra expansion and then use supervised learning methods (gradient descent or evolutionary strate-gies) to minimize a problem-dependent loss function that quantiﬁes how effectively a candidate plasticity rule transforms an initially random network into one with the desired function. We ﬁrst validate our approach by re-discovering previously described plasticity rules, starting at the single-neuron level and “Oja’s rule”, a simple Hebbian plasticity rule that captures the direction of most variability of inputs to a neuron (i.e., the ﬁrst principal component). We expand the problem to the network level and ask the framework to ﬁnd Oja’s rule together with an anti-Hebbian rule such that an initially random two-layer ﬁring-rate network will recover several principal components of the input space after learning. Next, we move to networks of integrate-and-ﬁre neurons with plastic inhibitory afferents.
We train for rules that achieve a target ﬁring rate by countering tuned excitation.
Our algorithm discovers a speciﬁc subset of the manifold of rules that can solve this task. Our work is a proof of principle of an automated and unbiased approach to unveil synaptic plasticity rules that obey biological constraints and can solve complex functions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1

Introduction
Synaptic plasticity is widely agreed to be essential for high level functions such as learning and memory. Its mechanisms are usually modelled with plasticity rules, i.e., functions that describe the evolution of the strength of a synapse. Current experimental techniques do not allow the tracking of relevant synaptic quantities over time at the population level, especially over the duration of learning. Therefore, most plasticity rules in the literature were derived from a few experiments in single synapses ex vivo, e.g., spike timing-dependent-plasticity [1–5]. Such rules do not usually construct a speciﬁc function or architecture to a network model on their own [6], unless they are carefully combined and orchestrated [7–9]. The link between the function of a network and the low level mechanisms that lead to its structure thus remains elusive.
Here we aim to bridge this gap by deducing plasticity rules from indirect but accessible quantities in the brain: the function of a network (e.g., elicited behaviour, population activity, etc.) or its architecture. Major technical breakthroughs in the ﬁeld of behavioural neuroscience and connectomics have vastly increased the amount of data for different aspects (or levels) of neuroscience [10–13], and we wondered if we could use these newly available results to deduce how a nervous system is constructed from scratch. Here, we present a meta-learning framework aiming to infer plasticity rules based on their ability to ascribe a desired function or architecture to an initially random neural network model. We present three example cases of rate and spiking neural network models for which such a numeric deduction of plasticity rules can be successfully performed. We point out their current limitations and discuss possible ways forward. 2