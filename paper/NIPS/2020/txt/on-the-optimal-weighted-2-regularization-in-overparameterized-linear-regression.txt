Abstract
×
∈
−
Rn x(cid:62) ˆβλ)2 in the proportional asymptotic limit p/n p in the overparameter-We consider the linear model y = Xβ(cid:63)+(cid:15) with X ized regime p > n. We estimate β(cid:63) via generalized (weighted) ridge regression:
ˆβλ = (X (cid:62)X + λΣw)†X (cid:62)y, where Σw is the weighting matrix. Under a ran-dom design setting with general data covariance Σx and general prior on the true coefﬁcients Eβ(cid:63)β(cid:62)(cid:63) = Σβ, we provide an exact characterization of the prediction
). Our risk E(y general setup leads to a number of interesting ﬁndings. We outline precise condi-tions that decide the sign of the optimal choice λopt of the ridge parameter λ, based on the alignment between Σx and Σβ; this rigorously justiﬁes the surprising em-pirical observation that λopt can be negative in the overparameterized regime. We also discuss the risk monotonicity of optimally tuned ridge regression, and con-ﬁrm the double descent phenomenon for principal component regression (PCR) under anisotropic X and β(cid:63). Finally, we determine the optimal Σw for both the ridgeless (λ 0) and optimally regularized (λ = λopt) case, and demonstrate the advantage of the weighted objective over standard ridge regression and PCR. (1,
→
→
∞
∈
γ 1

Introduction
In this work we consider learning the target signal β(cid:63) in the following linear regression model: yi = x(cid:62)i β(cid:63) + (cid:15)i,
Rp and noise (cid:15)i ∈ i = 1, 2, . . . , n where each feature vector xi ∈
R are drawn i.i.d. from the two independent random variables ˜x and ˜(cid:15) satisfying E˜(cid:15) = 0, E˜(cid:15)2 = ˜σ2, ˜x = Σ1/2 x z/√n, and the components of z are i.i.d. random variables with zero mean, unit variance, and bounded 12th absolute central moment.
To estimate β(cid:63) from (xi, yi), we consider the following generalized ridge regression estimator:
×
∈
Rn
ˆβλ = (X (cid:62)X + λΣw)†X (cid:62)y, (1.1) p is the feature matrix, y is vector of the observations, Σw is a positive deﬁnite 0, ˆβλ x(cid:62)i β)2 + λβ(cid:62)Σwβ. in which X weighting matrix, and the symbol † denotes the Moore-Penrose pseudo-inverse. When λ (cid:80)n minimizes the squared loss plus a weighted (cid:96)2 regularization: minβ
Note that Σw = I d reduces the objective to standard ridge regression.
While the standard ridge regression estimator is relatively well-understood in the data-abundant regime (n > p), several interesting properties have been recently discovered in high dimensions, especially when p > n. For instance, the double descent phenomenon suggests that overparameter-ization may not result in overﬁtting due to the implicit regularization of the least squares estimator
[HMRT19, BLLT19]. This implicit regularization also relates to the surprising empirical ﬁnding that the optimal ridge parameter λ can be negative in the overparameterized regime [KLS20]. i=1(yi −
≥
∗Equal contribution; alphabetical ordering. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
∈ (1,
)2 as n, p
Motivated by the observations above, we analyze the estimator ˆβλ in the proportional limit: p/n
γ
∞
Eβ(cid:63)β(cid:62)(cid:63) = Σβ, which covers both random and deterministic β risk of ˆβλ: E˜x,˜(cid:15),β(cid:63) (˜y analysis of ridge regression [DW18], our setup is generalized in two important aspects:
→
. We place a general prior on the true parameters (independent of ˜x and ˜(cid:15)):
. Our goal is to study the prediction
˜x(cid:62) ˆβλ)2, where ˜y = ˜x(cid:62)β(cid:63) + ˜(cid:15)3. Compared to previous high-dimensional
→ ∞
−
∗
Anisotropic Σx and Σβ. Our analysis handles general prior Σβ and data covariance Σx, in contrast to previous works which assume either isotropic features or signal (e.g., [DW18, HMRT19]). Note that the isotropic assumption on the signal or features implies that each component is roughly of the same magnitude, which may not hold true in practice. For instance, the optimal ridge penalty is prov-ably non-negative when either the signal Σβ [DW18, Theorem 2.1] or the features Σx [HMRT19,
Theorem 5] is isotropic. On the other hand, it has been empirically demonstrated that the optimal ridge for real-world data can be negative [KLS20]. While this observation cannot be captured by previous works, our less restrictive assumptions lead to a concise description of this phenomenon.
Weighted (cid:96)2 Regularization. We consider generalized ridge regression instead of simple isotropic shrinkage. While the generalized formulation has also been studied (e.g., [HK70, Cas80]), to the best of our knowledge, no existing work computes the exact risk in the overparameterized proportional limit and decides the corresponding optimal Σw. Our setting is also inspired by recent observations in deep learning that weighted (cid:96)2 regularization often achieves better generalization compare to isotropic weight decay [ZWXG18]. Our analysis illustrates the beneﬁt of weighted (cid:96)2 regularization.
Under the general setup (1.1), the contributions of this work can be summarized as (see Figure 1):
• Exact Asymptotic Risk. In Section 4 we derive the prediction risk R(λ) of our estimator (1.1) in its bias-variance decomposition (see Figure 2). We also characterize principal component regres-sion (PCR) and conﬁrm the double descent phenomenon under more general setting than [XH19].
• “Negative Ridge” Phenomenon. In Section 5, we analyze the optimal regularization strength λopt under different Σw, and pro-vide precise conditions under which the optimal λopt is negative in the overparameterized regime. In brief, we show that λopt is negative when SNR is large and the large directions of Σx and
β(cid:63) are aligned (see Figure 4), and vice versa. On the other hand, we show that the optimal ridge penalty is always non-negative in the underparameterized regime (p < n); this implies an implicit (cid:96)2 regularization effect of overparameterization for certain cases.
• Optimal Weighting Matrix Σw. In Section 6, we decide the optimal Σw for both the optimally regularized ridge estimator 0). In the ridgeless limit, (λ = λopt) and the ridgeless limit (λ based on the bias-variance decomposition, we show that the op-timal Σw should interpolate between Σx, which minimizes the variance, and Σ−
β , which minimizes the bias. Whereas for the optimally regularized case, in many settings the optimal Σw is simply Σ− (Figure 6) (for more general result see Theorem
β 10). We demonstrate the beneﬁt of weighted regularization over standard ridge regression and PCR, and also propose a heuristic choice of Σw when information of β(cid:63) is not present.
→ 1 1
Figure 1:
Illustration of the
“negative ridge” phenomenon and the advantage of weighted (cid:96)2 regularization under “aligned”
Σx and Σβ. We set γ = 2,
˜σ2 = 0. Red: standard ridge regression (Σw = I); note that the lowest prediction risk is achieved when λ < 0. Blue: optimally weighted ridge re-gression (Σw = Σ−1
β ), which achieves lower risk compared to the isotropic shrinkage.
Notations: We denote ˜E as taking expectation over β(cid:63), ˜x, ˜(cid:15). Let dx, dβ, dw be the vectors of the eigenvalues of Σx, Σβ and Σw respectively. We use I
. We write
S
ξ = E(˜x(cid:62)β(cid:63))2/(γ ˜σ2) as the signal-to-noise ratio (SNR) of the problem. as the indicator function of set
S 2