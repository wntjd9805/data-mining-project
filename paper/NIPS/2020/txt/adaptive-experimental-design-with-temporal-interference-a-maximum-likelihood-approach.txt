Abstract
Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different in-ventory management algorithms in an online retail site. Standard experimental approaches to this problem are biased (due to temporal interference between the policies), and not sample efﬁcient. We study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies).
We assume estimation of the steady state reward for each chain proceeds via non-parametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efﬁcient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martin-gale analysis of Markov chains via Poisson’s equation, we characterize efﬁcient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efﬁcient online experimental design that adaptively samples the two Markov chains. 1

Introduction
Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different inventory management algorithms in an online retail site. Standard randomized controlled trials are typically not feasible, since the goal is to estimate policy performance on the entire system. Instead, the typical current practice involves dynamically alternating between the two policies for ﬁxed lengths of time, and comparing the average performance of each over the intervals in which they were run as an estimate of the treatment effect; this is referred to as a switchback experimental design [4, 17].
However, switchback designs suffer from temporal interference: the initial condition in each interval of each policy is determined by the previous interval of the other policy, and so standard estimation techniques in this setting are biased. Bias due to temporal interference has been observed in ride-sharing [5], in delivery services [15], and ad auctions [3]. Further, the simple, non-adaptive nature of such designs implies they are not sample efﬁcient. Optimal consistent, efﬁcient experimental design in this setting has remained a signiﬁcant theoretical and practical challenge.
Our paper provides an optimal experimental design within a benchmark theoretical model for settings with temporal interference (Section 2). The central challenge posed by temporal interference is the following: we are effectively allowed only one real-world run of the system, with only ﬁnitely many observations. On the other hand, we need to use this single run to estimate performance of both the systems induced by each of the treatment and control policies. We model the problem by viewing each policy as its own Markov chain on a common underlying state space. The experimental 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
design problem is then to estimate the difference in the steady state reward under the treatment and control Markov chains, using only one run of the system, and without prior knowledge of any of the parameters of either policy or their rewards.
Our key contribution is a characterization of consistent and asymptotically efﬁcient policies when estimation proceeds via maximum likelihood. In particular, we model the unknowns nonparametricaly, and use an associated nonparametric maximum likelihood estimator (MLE; Section 3). At any time step, given the current system state, the experimental design chooses which chain to sample. We restrict attention to policies that satisfy a weak regularity requirement we call time-average regularity (TAR; Section 4). n particular, with TAR policies, we show the MLE is consistent.
In Section 5, we present our main result: we characterize efﬁcient TAR policies, i.e., those for which the MLE achieves asymptotically minimum variance among all TAR policies. Our approach uses a novel application of classical martingale analysis of Markov chains via Poisson’s equation, and leads to a characterization of efﬁcient designs via a succinct convex optimization problem.
This simple characterization is somewhat remarkable: Markov decision problems (MDPs) with variance minimization as the objective have historically not admitted structurally simple solutions (see below). In Section 6, we use this characterization to construct an efﬁcient, consistent adaptive online experimental design when estimation proceeds via the MLE. We conclude in Section 7.