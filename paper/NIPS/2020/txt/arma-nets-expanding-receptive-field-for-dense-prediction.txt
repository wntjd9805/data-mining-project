Abstract
Global information is essential for dense prediction problems, whose goal is to compute a discrete or continuous label for each pixel in the images. Traditional convolutional layers in neural networks, initially designed for image classiﬁca-tion, are restrictive in these problems since the ﬁlter size limits their receptive
ﬁelds. In this work, we propose to replace any traditional convolutional layer with an autoregressive moving-average (ARMA) layer, a novel module with an adjustable receptive ﬁeld controlled by the learnable autoregressive coefﬁcients.
Compared with traditional convolutional layers, our ARMA layer enables explicit interconnections of the output neurons and learns its receptive ﬁeld by adapting the autoregressive coefﬁcients of the interconnections. ARMA layer is adjustable to different types of tasks: for tasks where global information is crucial, it is capable of learning relatively large autoregressive coefﬁcients to allow for an output neuron’s receptive ﬁeld covering the entire input; for tasks where only local information is required, it can learn small or near zero autoregressive coefﬁcients and auto-matically reduces to a traditional convolutional layer. We show both theoretically and empirically that the effective receptive ﬁeld of networks with ARMA layers (named ARMA networks) expands with larger autoregressive coefﬁcients. We also provably solve the instability problem of learning and prediction in the ARMA layer through a re-parameterization mechanism. Additionally, we demonstrate that ARMA networks substantially improve their baselines on challenging dense prediction tasks, including video prediction and semantic segmentation. Our code is available on https://github.com/umd-huang-lab/ARMA-Networks. 1

Introduction
Convolutional layers in neural networks have many successful applications for machine learning tasks. Each output neuron encodes an input region of the network measured by the effective receptive
ﬁeld (ERF) [25]. A large ERF that allows for sufﬁcient global information is needed to make accurate predictions; however, a simple stack of convolutional layers does not effectively expand ERF.
Convolutional neural networks (CNNs) typically encode global information by adding downsampling (pooling) layers, which coarsely aggregate global information. A fully-connected classiﬁcation layer subsequently reduces the entire feature map to an output label. Downsampling and fully-connected layers are suitable for image classiﬁcation tasks where only a single prediction is needed. But they are less effective, due to potential loss of information, in dense prediction tasks such as semantic segmentation and video prediction, where each pixel requests a prediction. Therefore, it is crucial to introduce mechanisms that enlarge ERF without too much information loss.
Naive approaches to expanding ERF, such as deepening the network or enlarging the ﬁlter size, drastically increase the model complexity, which results in expensive computation, difﬁculty in optimization, and susceptibility to overﬁtting. Advanced architectures have been proposed to expand
ERF, including encoder-decoder networks [30], dilated convolutional networks [40, 41], and non-34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
local networks [34]. However, encoder-decoder networks could lose high-frequency information due to the downsampling layers. Dilated convolutional networks could suffer from the gridding effect while the ERF expansion is limited, and non-local networks are expensive in training and inference.
We introduce a novel autoregressive-moving-average (ARMA) layer that enables adaptive receptive
ﬁeld by explicit interconnections among its output neurons. Our ARMA layer realizes these intercon-nections via extra convolutions on output neurons, on top of the convolutions on input neurons as in a traditional convolutional layer. We provably show that an ARMA network can have arbitrarily large
ERF, thus capturing global information, with minimal extra parameters at each layer. Consequently, an ARMA network can ﬂexibly enlarge its ERF to leverage global knowledge without reducing spatial resolution. Moreover, the ARMA networks are independent of the architectures above, including encoder-decoder networks, dilated convolutional networks, and non-local networks.
A signiﬁcant challenge in ARMA networks lies in the complex computations needed in both forward and backward propagations — simple convolution operations are not applicable since the output neurons are inﬂuenced by their neighbors and thus interrelated. Another challenge in ARMA networks is instability — the additional interconnections among the output neurons could recursively amplify the outputs and lead them to inﬁnity. We address both challenges in this paper.
Summary of Contributions
• We introduce a novel ARMA layer that is a plug-and-play module substituting convolution layers in neural networks to allow ﬂexible tuning of their ERF, adapting to the task requirements, and improving performance in dense prediction problems.
• We recognize and address the problems of computation and instability in ARMA layers. (1)
To reduce computational complexity, we develop FFT-based algorithms for both forward and backward passes; (2) To guarantee stable learning and prediction, we propose a separable ARMA layer and a re-parameterization mechanism that ensures the layer to operate in a stable region.
• We successfully apply ARMA layers in ConvLSTM network [39] for pixel-level multi-frame video prediction and U-Net model [30] for medical image segmentation. ARMA networks substantially outperform the corresponding baselines on both tasks, suggesting that our proposed ARMA layer is a general and useful building block for dense prediction problems. 2