Abstract
We study the task of replicating the functionality of black-box neural models, for which we only know the output class probabilities provided for a set of input images.
We assume back-propagation through the black-box model is not possible and its training images are not available, e.g. the model could be exposed only through an API. In this context, we present a teacher-student framework that can distill the black-box (teacher) model into a student model with minimal accuracy loss.
To generate useful data samples for training the student, our framework (i) learns to generate images on a proxy data set (with images and classes different from those used to train the black-box) and (ii) applies an evolutionary strategy to make sure that each generated data sample exhibits a high response for a speciﬁc class when given as input to the black box. Our framework is compared with several baseline and state-of-the-art methods on three benchmark data sets. The empirical evidence indicates that our model is superior to the considered baselines. Although our method does not back-propagate through the black-box network, it generally surpasses state-of-the-art methods that regard the teacher as a glass-box model. Our code is available at: https://github.com/antoniobarbalau/black-box-ripper. 1

Introduction
In the last couple of years, AI has gained a lot of attention in industry, due to the latest research developments in the ﬁeld, e.g. deep learning [21]. Indeed, an increasing amount of companies have started to integrate neural models into their products [23], which are used by consumers or third-party developers [11]. In order to protect their intellectual property, companies try to keep information about the internals (architecture, training data, hyperparameters and so on) of their neural models conﬁdential, exposing the models only as black boxes: data samples in, predictions out. However, recent research [17, 30, 31, 32, 35, 36] showed that various aspects of black-box neural models can be stolen with some effort, including even their functionality.
Studying ways of stealing or copying the functionality of black-box models is of great interest to AI companies, giving them the opportunity to better protect their models through various mechanisms
[12, 40]. Motivated by this direction of study, we propose a novel generative evolutionary framework able to effectively steal the functionality of black-box models. The proposed framework is somewhat related to knowledge distillation with teacher-student networks [2, 10, 22, 39], the main difference being that access to the training data of the teacher is not permitted to preserve the black-box nature of the teacher. In this context, we train the student on a proxy data set with images and classes different from those used to train the black-box, in a setting known as zero-shot or data-free knowledge distillation [1, 3, 4, 8, 24, 28, 38]. To our knowledge, we are among the few [17, 31] to jointly consider no access to the training data and to the model’s architecture and hyperparameters, i.e. the model in question is a complete black-box.
As shown in Figure 1, our framework is comprised of a black-box teacher network, a student network, a generator and an evolutionary strategy. The teacher is trained independently of the framework, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Evolutionary Search
Generator
Teacher
Step 1
Step 2
Step 3
Student
Figure 1: Step 1: Generate a set of random samples in the latent space and forward pass them through the generator and the teacher. Step 2: Optimize the latent space encodings via evolutionary search.
Step 3: Distill the knowledge using the optimized data samples as input and the teacher’s class probabilities as target. Gradients are propagated only through the student network. being considered a pre-trained black-box model from our point of view. In order to effectively steal the functionality of the black box, our framework is based on two training phases. In the ﬁrst phase, we train a generative model, e.g. a Variational Auto-Encoder (VAE) [16] or a Generative Adversarial
Network [7], on the proxy data set. In the second training phase, we apply an evolutionary strategy, modifying the generated data samples such that they exhibit a high response for a certain class when given as input to the teacher.
To demonstrate the effectiveness of our generative evolutionary framework, we conduct experiments on three benchmark data sets: CIFAR-10 [18] with CIFAR-100 [18] as proxy, Fashion-MNIST [37] with CIFAR-10 [18] as proxy and 10 Monkey Species [27] with CelebA-HQ [13] and ImageNet
Cats and Dogs [33] as proxies. We compare our framework with a series of state-of-the-art methods
[1, 28, 31], demonstrating generally superior accuracy rates, while preserving the black-box nature of the teacher. For example, on CIFAR-10 as true data set and CIFAR-100 with 6 classes as proxy data set, we surpass the state-of-the-art performance by a signiﬁcant margin of 10.4%. We also include ablation results, showing the beneﬁts of adding our novel component: the evolutionary algorithm.
In summary, our contribution is twofold:
• We propose a novel generative evolutionary algorithm for stealing the functionality of black-box classiﬁcation models.
• We demonstrate that our framework is signiﬁcantly better than a state-of-the-art method
[31] trained in similarly adverse conditions (Orekondy et al. [31] consider the teacher as a black box) and equally or slightly better than a state-of-the-art method [1] trained in relaxed conditions (Addepalli et al. [1] back-propagate gradients through the teacher network). 2