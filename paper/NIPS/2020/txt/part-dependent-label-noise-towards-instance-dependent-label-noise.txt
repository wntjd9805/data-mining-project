Abstract
Learning with the instance-dependent label noise is challenging, because it is hard to model such real-world noise. Note that there are psychological and physiological evidences showing that we humans perceive instances by decomposing them into parts. Annotators are therefore more likely to annotate instances based on the parts rather than the whole instances, where a wrong mapping from parts to classes may cause the instance-dependent label noise. Motivated by this human cognition, in this paper, we approximate the instance-dependent label noise by exploiting part-dependent label noise. Speciﬁcally, since instances can be approximately reconstructed by a combination of parts, we approximate the instance-dependent transition matrix for an instance by a combination of the transition matrices for the parts of the instance. The transition matrices for parts can be learned by exploiting anchor points (i.e., data points that belong to a speciﬁc class almost surely). Empirical evaluations on synthetic and real-world datasets demonstrate our method is superior to the state-of-the-art approaches for learning from the instance-dependent label noise. 1

Introduction
Learning with noisy labels can be dated back to [4], which has recently drawn a lot of attention, especially from the deep learning community, e.g., [52, 79, 25, 13, 50, 57, 77, 37, 70, 75, 18, 41, 53, 22, 40, 56, 17, 16, 61, 60, 33, 32, 31, 21, 39, 46, 72, 66, 78]. The main reason is that it is expensive and sometimes even infeasible to accurately label large-scale datasets [23]; while it is relatively easy to obtain cheap but noisy datasets [77, 62, 65, 71, 19].
Methods for dealing with label noise can be divided into two categories: model-free and model-based algorithms. In the ﬁrst category, many heuristics reduce the side-effects of label noise without modeling it, e.g., extracting conﬁdent examples with small losses [18, 75, 64]. Although these algorithms empirically work well, without modeling the label noise explicitly, their reliability cannot be guaranteed. For example, the small-loss-based methods rely on accurate label noise rates.
This inspires researchers to model and learn label noise [13, 54, 55]. The transition matrix T (x) (i.e., a matrix-valued function) [44, 9] was proposed to explicitly model the generation process of label noise, where Tij(x) = Pr( ¯Y = j|Y = i, X = x), Pr(A) denotes as the probability of the event
A, X as the random variable for the instance, ¯Y as the noisy label, and Y as the latent clean label.
Given the transition matrix, an optimal classiﬁer deﬁned by clean data can be learned by exploiting sufﬁcient noisy data only [50, 35, 77]. The basic idea is that, the clean class posterior can be inferred by using the noisy class posterior (learned from the noisy data) and the transition matrix [6].
†Correspondence to Tongliang Liu (tongliang.liu@sydney.edu.au). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The proposed method will learn the transition matrices for parts of instances. The instance-dependent transition matrix for each instance can be approximated by a weighted combination of the part-dependent transition matrices.
However, in general, it is ill-posed to learn the transition matrix T (x) by only exploiting noisy data
[9, 67], i.e., the transition matrix is unidentiﬁable. Therefore, some assumptions are proposed to tackle this issue. For example, additional information is given [6]; the matrix is symmetric [43]; the noise rates for instances are upper bounded [9], or even to be instance-independent [67, 17, 50, 48, 44], i.e.,
Pr( ¯Y = j|Y = i, X = x) = Pr( ¯Y = j|Y = i). Note that there are speciﬁc applications where these assumptions are valid. That being said, in practice, these assumptions are hard to verify, and the gaps are large between instance-independent and instance-dependent transition matrices.
To handle the above problem, in this paper, we propose a new but practical assumption for instance-dependent label noise: The noise of an instance depends only on its parts. We term this kind of noise as part-dependent label noise. This assumption is motivated by that annotators usually annotate instances based on their parts rather than the whole instances. Speciﬁcally, there are psychological and physiological evidences showing that we humans perceive objects starting from their parts
[49, 63, 38]. There are also computational theories and learning algorithms showing that object recognition rely on parts-based representations [7, 59, 10, 47, 20, 3]. Since instances can be well reconstructed by combinations of parts [29, 30], the part-dependence assumption should be mild in this sense. Intuitively, for a given instance, a combination of part-dependent transition matrices can well approximate the instance-dependent transition matrix, which will be empirically veriﬁed in
Section 4.2.
To fulﬁl the approximation, we need to learn the transition matrices for parts and the combination parameters. Since the parts are semantic [29], their contributions to perceiving the instance could be similar in the contributions to understanding (or annotating) them [7, 3]. Therefore, it is natural to assume that for constructing the instance-dependent transition matrix, the combination parameters of part-dependent transition matrices are identical to those of parts for reconstructing an instance.
We illustrate this in Figure 1, where the combinations in the top and bottom panels share the same parameters. The transition matrices for parts can be learned by exploiting anchor points, which are deﬁned by instances that belong to a speciﬁc clean class with probability one [35]. Note that the assumption for combination parameters and the requirement of anchor points might be strong. If they are invalid, the part-dependent transition matrix might be poorly learned. To solve this issue, we also use the slack variable trick in [67] to modify the instance-dependent transition matrix.
Extensive experiments on both synthetic and real-world label-noise datasets show that the part-dependent transition matrices can well address instance-dependent label noise. Speciﬁcally, when the instance-dependent label noise is heavy, i.e., 50%, the proposed method outperforms state-of-the-art methods by almost 10% of test accuracy on CIFAR-10. More details can be found in Section 4.
The rest of the paper is organized as follows. In Section 2, we brieﬂy review related work on modeling label noise and parts-based learning. In Section 3, we discuss how to learn part-dependent transition matrices. In Section 4, we provide empirical evaluations of our learning algorithm. In Section 5, we conclude our paper. 2