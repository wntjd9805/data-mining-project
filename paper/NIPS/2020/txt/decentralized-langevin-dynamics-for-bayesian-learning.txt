Abstract
Motivated by decentralized approaches to machine learning, we propose a collab-orative Bayesian learning algorithm taking the form of decentralized Langevin dynamics in a non-convex setting. Our analysis show that the initial KL-divergence between the Markov Chain and the target posterior distribution is exponentially de-creasing while the error contributions to the overall KL-divergence from the additive noise is decreasing in polynomial time. We further show that the polynomial-term experiences speed-up with number of agents and provide sufﬁcient conditions on the time-varying step-sizes to guarantee convergence to the desired distribu-tion. The performance of the proposed algorithm is evaluated on a wide variety of machine learning tasks. The empirical results show that the performance of individual agents with locally available data is on par with the centralized setting with considerable improvement in the convergence rate. 1

Introduction
With the recent advances in computational infrastructure, there has been an increase in the use of larger machine learning models with millions of parameters. Even though there is a parallel increase in the size of training datasets for these models, there is a signiﬁcant disparity between the amount of existing data and the data required to train the large models to avoid overﬁtting and provide good generalization performance. Such models trained in point estimate settings such as Maximum A
Posteriori (MAP) neglect any associated epistemic uncertainties and make overconﬁdent predictions.
Bayesian learning framework provides a principled way to avoid over-ﬁtting and model uncertainties by estimating the posterior distribution of the model parameters. However, analytical solutions of exact posterior or sampling from the exact posterior is often impossible due to the intractability of the evidence. Therefore, one needs to resort to approximate Bayesian methods such as Markov Chain
Monte Carlo (MCMC) sampling techniques. To this effect, we focus on a speciﬁc class of MCMC methods, called Langevin dynamics to sample from the posterior distribution and perform Bayesian machine learning.
Langevin dynamics derives motivation from diffusion approximations and uses the information of a target density to efﬁciently explore the posterior distribution over parameters of interest [1].
Langevin dynamics, in essence, is the steepest descent ﬂow of the relative entropy functional or the
KL-divergence with respect to the Wasserstein metric [2–4]. Just as the gradient ﬂow converges exponentially fast under a gradient-domination condition, Langevin dynamics converges exponentially fast to the stationary target distribution if the relative entropy functional satisﬁes the log-Sobolev inequality [3–5]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The Unadjusted Langevin Algorithm (ULA) is a popular inexact ﬁrst-order discretized implementation of the Langevin dynamics without an acceptance/rejection criteria. Analysis of convergence properties of the ULA and other Langevin approximations has been a topic of active research over past several years [6–12]. Reference [3] shows that a bias exists in the ULA for any arbitrarily small (ﬁxed) step size, even for a Gaussian target distribution. Controlling the bias and exponential convergence of KL divergence for strongly log-concave smooth target distributions using ULA is discussed in [3, 6, 7, 9–11]. Non-asymptotic bounds on variation error of the Langevin approximations for smooth log-concave target distributions have been established by [6] and [8]. Assuming a Lipschitz continuous Hessian, [6] introduces a modiﬁed version of the Langevin algorithm requiring fewer iterations to achieve the same precision level. Tight relations between the Langevin Monte Carlo for sampling and the gradient descent for optimization for (strongly) log-concave target distributions are presented in [7]. Similarly, using the notion of gradient ﬂows over probability space and KL-divergence, [9] analyzes the non-asymptotic convergence of discretized Langevin diffusion. These results were improved and extended with particular emphasis on scalability of the approach with dimension, smoothness, and curvature of the function of interest in [10–12].
Compared to log concave ULA settings where local properties replicate the global behavior and optimal values are attained in a single pass, non-convex objective functions naturally require multiple passes through training data. Analysis of ULA in such cases often requires assuming that the negative log of the target distribution satisﬁes some dissipative property [13–17], contractivity condition [18], or limiting the non-convexity to a local region [19, 20]. In particular, [13] makes the
ﬁrst attempt in analyzing non-asymptotic convergence in a nonconvex setting and shows SGLD tracks continuous Langevin diffusion in quadratic Wasserstein distance for empirical risk minimization.
Recent work [14, 19, 21] reports computational efﬁciency of sampling algorithm to optimization methods in the nonconvex setting. The approach is extended to relaxed dissipativity conditions, to evaluate dependent data streams and provides sharper convergence estimates uniform in the number of iterations in [15, 16]. More recently, it is shown that the convergence is polynomial in terms of dimension and error tolerance [17, 18, 20].
Besides the ULA, higher-order Langevin diffusion for accelerated sampling algorithms are presented in [22, 23]. Analysis of “leapfrog” implementation of Hamiltonian Monte-Carlo (HMC) for strongly log-concave target distributions is presented in [24] and [25]. Following the introduction of a stochastic gradient-based Langevin approach for Bayesian inference in [26], stochastic gradient based
Langevin diffusion and other HMC schemes are presented in [27–31].