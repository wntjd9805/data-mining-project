Abstract
Quantized neural networks with low-bit weights and activations are attractive for developing AI accelerators. However, the quantization functions used in most conventional quantization methods are non-differentiable, which increases the optimization difﬁculty of quantized networks. Compared with full-precision pa-rameters (i.e., 32-bit ﬂoating numbers), low-bit values are selected from a much smaller set. For example, there are only 16 possibilities in 4-bit space. Thus, we present to regard the discrete weights in an arbitrary quantized neural net-work as searchable variables, and utilize a differential method to search them accurately. In particular, each weight is represented as a probability distribution over the discrete value set. The probabilities are optimized during training and the values with the highest probability are selected to establish the desired quan-tized network. Experimental results on benchmarks demonstrate that the proposed method is able to produce quantized neural networks with higher performance over the state-of-the-art methods on both image classiﬁcation and super-resolution tasks. The PyTorch code will be made available at https://github.com/ huawei-noah/Binary-Neural-Networks/tree/main/SLB and the
MindSpore code will be made available at https://www.mindspore.cn/ resources/hub. 1

Introduction
The huge success of deep learning is well demonstrated in considerable computer vision tasks, includ-ing image recognition [27, 61, 62], object detection [15, 53], visual segmentation [25], and image processing [35, 72]. On the other side, these deep neural architectures are often oversized for accuracy reason. A great number of network compression and acceleration methods have been proposed to eliminate the redundancy and explore the efﬁciency in neural networks , including pruning [23, 67], distillation [69, 6, 70, 44, 14], low-bit quantization [8, 52, 78], weight decomposition [75, 42, 71], neural architecture search [46, 68] and efﬁcient block design [30, 54, 50, 21, 64].
Among these algorithms, quantization is very particular which represents parameters in deep neural networks as low-bit values. Since the low costs of quantized networks on both memory usage and computation, they can be easily deployed on mobile devices with speciﬁc hardware design. For example, compared with conventional 32-bit networks, binary neural networks (BNNs) can directly obtain a 32× compression ratio, and an extreme computational complexity reduction by executing
∗Corresponding Author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
bit-wise operations (e.g., 57× speed-up ratio in XNORNet [52]). However, the performance of the low-bit neural networks is usually worse than that of full-precision baselines, due to the optimization difﬁculty raised by the low-bit quantization functions.
To reduce the accuracy drop of quantized neural networks, some methods have been proposed in recent years. BiRealNet [48] inserts more shortcuts to help optimization. Structured Binary [81] and
ABCNet [43] explore some sophisticated binary blocks and achieve comparable performance to that of full-precision networks, etc.
Admittedly, the aforementioned methods have made great efforts to improve the performance of the quantized neural networks. However, the accuracy gap between the full-precision network and its quantized version is still very huge, especially the binarized model. For example, the state-of-the-art accuracy of binarized ResNet-18 is about 10% lower than that of the full-precision baseline.
A common problem in existing quantization methods is the estimated gradients for quantization functions, using either STE [8, 52, 29, 47, 59, 4] or self-designed gradient computation manner [46].
The estimated gradients may provide inaccurate optimization direction and consequently lead to worse performance. Therefore, an effective approach for learning quantized neural networks without estimated gradients is urgently required.
In this paper, we present a novel weight search-ing method for training the quantized deep neu-ral network without gradient estimation. Since there are only a few values of low-bit weights (e.g., +1 and -1 in binary networks), we develop a weight searching algorithm to avoid the non-differentiable problem of quantization functions.
All low-bit values of an arbitrary weight in the given network are preserved with different prob-abilities. These probabilities will be optimized during the training phase. To further eliminate the performance gap after searching, we explore the temperature factor and a state batch normal-ization for seeking consistency in both training and testing. The effectiveness of the proposed differentiable optimization method is then veri-ﬁed on image classiﬁcation and super-resolution tasks, in terms of accuracy and PSNR values. 2