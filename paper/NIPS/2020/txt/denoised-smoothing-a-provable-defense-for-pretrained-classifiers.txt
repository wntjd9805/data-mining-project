Abstract
We present a method for provably defending any pretrained image classiﬁer against (cid:96)p adversarial attacks. This method, for instance, allows public vision API providers and users to seamlessly convert pretrained non-robust classiﬁcation services into provably robust ones. By prepending a custom-trained denoiser to any off-the-shelf image classiﬁer and using randomized smoothing, we effectively create a new classiﬁer that is guaranteed to be (cid:96)p-robust to adversarial examples, without modifying the pretrained classiﬁer. Our approach applies to both the white-box and the black-box settings of the pretrained classiﬁer. We refer to this defense as denoised smoothing, and we demonstrate its effectiveness through extensive experimentation on ImageNet and CIFAR-10. Finally, we use our approach to provably defend the Azure, Google, AWS, and ClarifAI image classiﬁcation APIs.
Our code replicating all the experiments in the paper can be found at: https:
//github.com/microsoft/denoised-smoothing1.

Introduction 1
Image classiﬁcation using deep learning, despite its recent success, is well-known to be susceptible to adversarial attacks: small, imperceptible perturbations of the inputs that drastically change the resulting predictions (Szegedy et al., 2013; Goodfellow et al., 2015; Carlini & Wagner, 2017b). To solve this problem, many works proposed heuristic defenses that build models robust to adversarial perturbations, though many of these defenses were broken using more powerful adversaries (Carlini
& Wagner, 2017a; Athalye et al., 2018; Uesato et al., 2018). This has led researchers to both strengthen empirical defenses (Kurakin et al., 2016; Madry et al., 2017) as well as to develop certiﬁed defenses that come with robustness guarantees, i.e., classiﬁers whose predictions are constant within a neighborhood of their inputs (Wong & Kolter, 2018; Raghunathan et al., 2018a; Cohen et al., 2019; Salman et al., 2019a). However, the majority of these defenses require that the classiﬁer be trained (from scratch) speciﬁcally to optimize the robust performance criterion, making the process of building robust classiﬁers a computationally expensive one.
In this paper, we consider the problem of generating a provably robust classiﬁer without retraining the underlying model at all. This problem has not been investigated before, as previous works on provable robustness mainly focus on training classiﬁers for this objective. There are several use cases that make this problem interesting. For example, a provider of a large-scale image classiﬁcation API may want to offer a “robust” version of the API, but may not want to maintain and/or continually retrain two models that need to be evaluated and validated separately. Even more realistically, a user of a public vision API might want to use that API to create robust predictions (presuming that the
API performs well on clean data), but may not have access to the underlying non-robust model. In both cases (which exemplify the white-box and the black-box settings respectively), it would be highly desirable if one could simply apply an off-the-shelf “ﬁlter” that would allow practitioners to automatically generate a provably robust model from this standard model. 1Please see http://arxiv.org/abs/2003.01908 for the full and most recent version of this paper. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Given a clean image x, our denoised smoothing procedure creates a smoothed classiﬁer by appending a denoiser to any pretrained classiﬁer (e.g. online commercial APIs) so that the pipeline predicts in majority the correct class under Gaussian noise corrupted-copies of x. The resultant classiﬁer is certiﬁably robust against (cid:96)2-perturbations of its input.
Table 1: Certiﬁed top-1 accuracy of ResNet-50 on ImageNet at various (cid:96)2 radii (Standard accuracy is in parenthesis). (cid:96)2 RADIUS (IMAGENET) 0.25 0.5 0.75 1.0 1.25 1.5
COHEN ET AL. (2019) (%) (70)62 (70)52 (62)45 (62)39 (62)34 (50)29
NO DENOISER (BASELINE) (%)
OURS (BLACK-BOX) (%)
OURS (WHITE-BOX) (%) (49)32 (69)48 (67)50 (12)4 (56)31 (60)33 (12)2 (56)19 (60)20 (0)0 (34)12 (38)14 (0)0 (34)7 (38)11 (0)0 (30)4 (38)6
Table 2: Certiﬁed accuracy of ResNet-110 on CIFAR-10 at various (cid:96)2 radii (Standard accuracy is in parenthesis). (cid:96)2 RADIUS (CIFAR-10) 0.25 0.5 0.75 1.0 1.25 1.5
COHEN ET AL. (2019) (%) (77)59 (77)45 (65)31 (65)21 (45)18 (45)13
NO DENOISER (BASELINE) (%)
OURS (BLACK-BOX) (%)
OURS (WHITE-BOX) (%) (10)7 (81)45 (72)56 (9)3 (68)20 (62)41 (9)0 (21)15 (62)28 (16)0 (21)13 (44)19 (16)0 (16)11 (42)16 (16)0 (16)10 (44)13
Motivated by this, we propose a new approach to obtain a provably robust classiﬁer from a ﬁxed pretrained one, without any additional training or ﬁne-tuning of the latter. This approach is depicted in Figure 1. The basic idea, which we call denoised smoothing, is to prepend a custom-trained denoiser before the pretrained classiﬁer, and then apply randomized smoothing (Lecuyer et al., 2018;
Li et al., 2018; Cohen et al., 2019). Randomized smoothing is a certiﬁed defense that converts any given classiﬁer f into a new smoothed classiﬁer g that is characterized by a non-linear Lipschitz property (Salman et al., 2019a). When queried at a point x, the smoothed classiﬁer g outputs the class that is most likely to be returned by f under isotropic Gaussian perturbations of its inputs.
Unfortunately, randomized smoothing requires that the underlying classiﬁer f is robust to relatively large random Gaussian perturbations of the input, which is not the case for off-the-shelf pretrained models. By applying our custom-trained denoiser to the classiﬁer f , we can effectively make f robust to such Gaussian perturbations, thereby making it “suitable” for randomized smoothing.
Key to our approach is how we train our denoisers, which is not merely to reconstruct the original image, but also to maintain its original label predicted by f . Similar heuristics have been used before; indeed, some of the original adversarial defenses involved applying input transformations to “remove” adversarial perturbations (Guo et al., 2017; Liao et al., 2018; Prakash et al., 2018; Xu et al., 2018), but these defenses were soon broken by more sophisticated attacks (Athalye et al., 2018; Athalye
& Carlini, 2018; Carlini & Wagner, 2017a). In contrast, the approach we present here exploits the certiﬁed nature of randomized smoothing to ensure that our defense is provably secure.
Our contribution is demonstrating, for the ﬁrst time, a simple yet effective method for converting any pretrained classiﬁer into a provably robust one. This applies both to the setting where we have white-box access to the classiﬁer, and to the setting where we only have black-box access. We verify the efﬁcacy of our method through extensive experimentation on ImageNet and CIFAR-10. We are able to convert pretrained ResNet-18/34/50 and ResNet-110, on CIFAR-10 and ImageNet respectively, into certiﬁably robust models; our results are summarized in Tables 1 and 2 (details are in section 3)2. For instance, we are able to boost the certiﬁed accuracy of an ImageNet-pretrained ResNet-50 from 4% 2Tables for ResNet-18/34 on ImageNet are in Appendix B. 2
to: 31% for the black-box access setting, and 33% for the white-box access setting, under adversarial perturbations with (cid:96)2 norm less than 127/255. We also show the effectiveness of our method through real-world experiments on the Azure, Google, AWS, and ClarifAI image classiﬁcation APIs. We are able to wrap these vision APIs with our method, leading to provably robust versions of these APIs despite being black-box. 2 Denoised Smoothing
In this section, we discuss why randomized smoothing is not, in general, directly effective on off-the-shelf classiﬁers. Later, we describe our proposed denoised smoothing method for solving this problem. We start by introducing some background on randomized smoothing. We refer the reader to
Cohen et al. (2019) and Salman et al. (2019a) for a more detailed description of this technique. 2.1