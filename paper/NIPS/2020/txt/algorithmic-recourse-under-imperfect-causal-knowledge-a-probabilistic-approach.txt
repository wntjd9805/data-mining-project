Abstract
Recent work has discussed the limitations of counterfactual explanations to recom-mend actions for algorithmic recourse, and argued for the need of taking causal relationships between features into consideration. Unfortunately, in practice, the true underlying structural causal model is generally unknown. In this work, we ﬁrst show that it is impossible to guarantee recourse without access to the true structural equations. To address this limitation, we propose two probabilistic approaches to select optimal actions that achieve recourse with high probability given limited causal knowledge (e.g., only the causal graph). The ﬁrst captures uncertainty over structural equations under additive Gaussian noise, and uses Bayesian model averaging to estimate the counterfactual distribution. The second removes any assumptions on the structural equations by instead computing the average effect of recourse actions on individuals similar to the person who seeks recourse, leading to a novel subpopulation-based interventional notion of recourse. We then derive a gradient-based procedure for selecting optimal recourse actions, and empirically show that the proposed approaches lead to more reliable recommendations under imperfect causal knowledge than non-probabilistic baselines. 1

Introduction
As machine learning algorithms are increasingly used to assist consequential decision making in a wide range of real-world settings [36, 41], providing explanations for the decision of these black-box models becomes crucial [7, 58]. A popular approach is that of (nearest) counterfactual explanations, which refer to the closest feature instantiations that would have resulted in a changed prediction [59].
While providing some insight (explanation) into the underlying black-box classiﬁer, such coun-terfactual explanations do not directly translate into actionable recommendations to individuals for obtaining a more favourable prediction[22, 5]—a related task referred to as algorithmic re-course [54, 55, 19, 21]. Importantly, prior work on both counterfactual explanations and algorithmic recourse treats features as independently manipulable inputs, thus ignoring the causal relationships between features.
In this context, recent work [22] has argued for the need of taking into account the causal structure between features to ﬁnd a minimal set of actions (in the form of interventions) that guarantees recourse. However, while this approach is theoretically sound, it involves computing counterfactuals in the true underlying structural causal model (SCM) [35], and thus relies on strong impractical
⇤Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
assumptions; speciﬁcally, it requires complete knowledge of the true structural equations. While for many applications it is possible to draw a causal diagram from expert knowledge, assumptions about the form of structural equations are, in general, not testable and may thus not hold in practice [38].
As a result, counterfactuals computed using a misspeciﬁed causal model may be inaccurate and recommend actions that are sub-optimal or, even worse, ineffective to achieve recourse.
In this work, we focus on the problem of algorithmic recourse when only limited causal knowledge is available (as it is generally the case). To this end, we propose two probabilistic approaches which allow to relax the strong assumption of a fully-speciﬁed SCM made in [22]. In the ﬁrst approach, we assume that, while the underlying SCM is unknown, it belongs to the family of additive Gaussian noise models [16, 37]. We then make use of Gaussian processes (GPs) [62] to average predictions over a whole family of SCMs and thus to obtain a distribution over counterfactual outcomes which forms the basis for individualised algorithmic recourse. The second approach considers a different subpopulation-based notion of algorithmic recourse by estimating the effect of interventions for individuals similar to the one for which we aim to achieve recourse. It thus addresses a different (rung 2) target quantity than the counterfactual/individualised (rung 3) approach which allows us to further relax our assumptions by removing any assumptions on the form of the structural equations.
This approach is based on the idea of the conditional average treatment effect (CATE) [1], and relies on conditional variational autoencoders (CVAEs) [48] to estimate the interventional distribution. In both cases, we assume that the causal graph is known or can be postulated from expert knowledge, as without such an assumption causal reasoning from observational data is not possible [38, Prop. 4.1].
In more detail, we ﬁrst demonstrate as a motivating negative result that recourse guarantees are only possible if the true SCM is known (§3). Then, we introduce two probabilistic approaches for handling different levels of uncertainty in the structural equations (§4 and §5), and propose a gradient-based method to ﬁnd a set of actions that achieves recourse with a given probability at minimum cost (§6). Our experiments (§7) on synthetic and semi-synthetic loan approval data, show the need for probabilistic approaches to achieve algorithmic recourse in practice, as point estimates of the underlying true SCM often propose invalid recommendations or achieve recourse only at higher cost. Importantly, our results also show that subpopulation-based recourse is the right approach to adopt when assumptions such as additive noise do not hold. A user-friendly implementation of all methods that only requires speciﬁcation of the causal graph and a training set is available at https://github.com/amirhk/recourse. 2