Abstract
We formalize human language understanding as a structured prediction task where the output is a partially ordered set (poset). Current encoder-decoder architectures do not take the poset structure of semantics into account properly, thus suffering from poor compositional generalization ability. In this paper, we propose a novel hierarchical poset decoding paradigm for compositional generalization in language.
Intuitively: (1) the proposed paradigm enforces partial permutation invariance in semantics, thus avoiding overﬁtting to bias ordering information; (2) the hierar-chical mechanism allows to capture high-level structures of posets. We evaluate our proposed decoder on Compositional Freebase Questions (CFQ), a large and realistic natural language question answering dataset that is speciﬁcally designed to measure compositional generalization. Results show that it outperforms current decoders. 1

Introduction
Understanding semantics of natural language utterances is a fundamental problem in machine learning.
Semantics is usually invariant to permute some components in it. For example, consider the natural language utterance “Who inﬂuences Maxim Gorky and marries Siri von Essen?”. Its semantics can be represented as either “∃x : INFLUENCE(x, Maxim_Gorky) ∧ MARRY(x, Siri_von_Essen)” or “∃x : MARRY(x, Siri_von_Essen) ∧ INFLUENCE(x, Maxim_Gorky)”. For a complex natural language utterance, there would be many equivalent meaning representations. However, standard neural encoder-decoder architectures do not take this partial permutation invariance into account properly: (1) sequence decoders (Figure 1(a)) maximize the likelihood of one certain meaning representation while suppressing other equivalent good alternatives (Sutskever et al., 2014; Bahdanau et al., 2014); (2) tree decoders (Figure 1(b)) predict permutation invariant components respectively, but there are still certain decoding orders among them (i.e., target trees are ordered trees) (Dong and
Lapata, 2016; Polosukhin and Skidanov, 2018).
If we ignore the partial permutation invariance in semantics and predict them in a certain order using these standard neural encoder-decoder architectures, the learned models will always have poor compositional generalization ability (Keysers et al., 2020). For example, a model has trained on
“Who inﬂuences Maxim Gorky and marries Siri von Essen?” and “Who inﬂuences A Lesson in Love’s art director”, but it cannot understand and produce the correct semantic meaning of “Who inﬂuences
A Lesson in Love’s art director and marries Siri von Essen?”. Here compositional generalization means the ability to understand and produce a potentially inﬁnite number of novel combinations of
∗Work done during an internship at Microsoft Research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Sequence decoding (b) Tree decoding (ordered tree) (c) Poset decoding
Figure 1: Three decoding paradigms. known components, even though these combinations have not been previously observed (Chomsky, 1965).
The intuition behind this is: imposing additional ordering constraints increases learning complexity, which makes the learning likely to overﬁt to bias ordering information in training distribution.
Concretely, earlier generated output tokens have a profound effect on the later generated output tokens (Mehri and Sigal, 2018; Wu et al., 2018). For example, in current sequence decoder (Figure 1(a)) and tree decoder (Figure 1(b)), “MARRY” is generated after “INFLUENCE”, though these two tokens have no sequential order in actual semantics. This makes the decoding of “MARRY” likely to be impacted by bias information about “INFLUENCE” learned from training distribution (e.g.d, if the co-occurrence of “INFLUENCE” and “MARRY” has never been observed in the training data, the decoder will mistakenly choose not to predict “MARRY”).
This problem is expected to be alleviated through properly modelling partially-ordered set (poset) structure in the decoding process. Figure 1(c) shows an intuitive explanation. Output token “x” should have two next tokens: “INFLUENCE” and “MARRY”. They should be predicted from the same context in a parallel fashion, independently from each other. Partial permutation invariance in semantics should be enforced in the decoding process, thus the model can focus more on generalizable substructures rather than bias information.
In this paper, we propose a novel hierarchical poset decoding paradigm for compositional general-ization in language. The basic idea for poset decoding is to decode topological traversal paths of a poset in a parallel fashion, thus preventing the decoder from learning bias ordering information among permutation invariant components. Moreover, inspired by the natural idea that semantic primitives and abstract structures should be learned separately (Russin et al., 2019; Li et al., 2019;
Lake, 2019; Gordon et al., 2020), we also incorporate a hierarchical mechanism to better capture the compositionality of semantics.
We evaluate our hierarchical poset decoder on Compositional Freebase Questions (CFQ), a large and realistic natural langauge question answering dataset that is speciﬁcally designed to measure compositional generalization (Keysers et al., 2020). Experimental results show that the proposed paradigm can effectively enhance the compositional generalization ability. 2 The CFQ Compositional Tasks
The CFQ dataset (Keysers et al., 2020) contains natural language questions (in English) paired with the meaning representations (SPARQL queries against the Freebase knowledge graph). Figure 2 shows an example instance.
To comprehensively measure a learner’s compositional generalization ability, CFQ dataset is split into train and test sets based on two principles: (1) Minimizing primitive divergence: all primitives present in the test set are also present in the train set, and the distribution of primitives in the train set is as similar as possible to their distribution 2
Figure 2: An instance in CFQ dataset (natural language question ⇒ SPARQL query).
Figure 3: We model the semantic meaning of a question as a poset. Every poset can take the form of a DAG (Directed Acyclic Graph). There are three types of elements in semantic posets: variables (rounds), predicates (rectangles with rounded corners), and entities (rectangles). Each variable and entity is unique in every semantic poset. in the test set. (2) Maximizing compound divergence: the distribution of compounds (i.e., logical substructures in SPARQL queries) in the train set is as different as possible from the distribution in the test set. The second principle guarantees that the task is compositionally challenging. Following these two principles, three different maximum compound divergence (MCD) dataset splits are constructed.
Standard encoder-decoder architectures achieve an accuracy larger than 95% on a random split, but the mean accuracy on the MCD splits is below 20%.
In CFQ dataset, all SPARQL queries are controlled within the scope of conjunctive logical queries.
Each query q can be written as: where q = ∃x0, x1, ..., xm : c1 ∧ c2 ∧ ... ∧ cn, ci = (h, r, t), h, t ∈ {x0, x1, ..., xm} ∪ E, r ∈ P (1)
In Equation 1, x0, x1, ..., xm are variables, E is a set of entities, and P is a set of predicates (i.e., relations). From the perspective of logical semantics, conjunctive clauses (i.e., c1, c2, ..., cn) are invariant under permutation. In CFQ experiments, queries are linearized through sorting these clauses based on alphabetical order, rather than the order of appearance in natural language questions. Our hyphothesis is that this imposed ordering harms the learning of compositionality in language, so the key is to empower the decoder with the ability to enforce partial permutation invariance in semantics. 3 Poset
In this section, we study the task of generating a ﬁnite poset Y given input X.
A poset Y is a set of elements {y1, y2, ..., y|Y |} with a partial order “(cid:22)” deﬁned on it. Let ≺ be the binary relation on Y such that yi ≺ yj if and only if yi (cid:22) yj and yi (cid:54)= yj. Let ≺· be the covering relation on Y such that yi ≺· yj if and only if yi ≺ yj and there is no element yk such that yi ≺ yk ≺ yj. Every poset can be represented as a Hasse diagram, which takes the form of a directed acyclic graph (DAG): {y1, y2, ..., y|Y |} represents a vertex set and ≺· represents the edge set over it.
Then, we can use the language of graph theory to discuss posets.
Let V be the vocabulary of output tokens for poset. For each y ∈ Y , we assign an output token
LY (y) ∈ V as its label. In this paper, we focus on LY that meets the following two properties: 1. If yi, yj, yk ∈ Y , yj (cid:54)= yk, yi ≺· yj, and yi ≺· yk, then LY (yj) (cid:54)= LY (yk). Intuitively, outgoing vertices of the same vertex must have different labels. 2. There exists ˆV ⊂ V, so that for all poset Y and y ∈ Y ,|{y(cid:48)|y(cid:48) ∈ Y, y(cid:48) ≺· y}| ≤ 1 or
LY (y) ∈ ˆV ∧ |{y(cid:48)|y(cid:48) ∈ Y, LY (y(cid:48)) = LY (y)}| = 1. Intuitively, some output tokens ( ˆV) can 3
only appear once per poset, and we restrict that only these vertices can have more than one incoming edge.
We refer to posets meeting these two properties as semantic posets. This is a tractable subset of posets, and it is ﬂexible to generalize to different kinds of semantics in human language.
Take conjunctive logical queries Q in CFQ dataset as an example. For each query q ∈ Q, we model it as a semantic poset Yq using the following principles:
• The output vocabulary V = Vx ∪ P ∪ E, where Vx is the vocabulary of variables (x0, x1, ...),
P is the vocabulary of predicates, and E is the vocabulary of entities. We deﬁne that
ˆV = Vx ∪ E.
• Variables and entities in q are represented as vertices. Their labels are themselves.
• For each variable or entity v in q, we deﬁne the set of all clauses in q as Cv, in which v is the head object. We also denote the set of all predicates in Cv as Pv. We create a vertex v(cid:48) v,p for each p ∈ Pv, let Ly(v(cid:48) v,p) = p, then create an edge from v to v(cid:48) v,p). Let
Tv,p be the set of tail objects in clauses that v is the head object and p is the predicate. For each vertex in Tv,p, we create an edge from v(cid:48) v,p (i.e., v ≺· v(cid:48) v,p to it.
Figure 3 shows an example of semantic poset. We visualize the semantic poset as a DAG: each variable is represented as a round; each predicate is presented as a rectangle; each entity is presented as a rectangle with rounded corners. 4 Poset Decoding (Basic Version)
In this section, we introduce a minimal viable version of poset decoding. It can be seen as a extension of sequence decoding, in which poset structure is handled in a topological traversal-based way.
In a standard sequence decoder, tokens are generated strictly in a left-to-right manner. The probability distribution of the output token at time step t (denoted as wt) is represented with a softmax over all the tokens in the vocabulary (denoted as V): wt = softmax(Woht) (2)
, where ht is the hidden state in the decoder at time step t, Wo is a learnable parameter matrix that is designed to produce a |V|-dimensional logits from ht.
When the decoding object is extended from a sequence to a semantic poset Y , we need to extend the sequential time steps into a more ﬂexible fashion, that is, topological traversal paths. We deﬁne that l = (l1, l2, ..., l|l|) is a topological traversal path in Y (l1, l2, ..., l|l| ∈ Y ), l1 is a lower bound element in Y (i.e., |{y(cid:48)|y(cid:48) ∈ Y, y(cid:48) ≺· l1}| = 0) and l1 ≺· l2 ≺· ... ≺· l|l|. For each topological traversal path l in Y , we generate a hidden state hl for it, following the way that sequence decoders uses. Without loss of generality, we describe an implementation based on attention RNN decoder: hl,t = RNN(hl,t−1, el,t−1, cl,t) (3)
In Equation 3, hl,t represents the hidden state of lt, and we let hl = hl,|l|. The hidden state hl,t is computed using an RNN module, of which the inputs are: hl,t−1 (the previous hidden state), el,t−1 (the embedding of LY (lt−1)), and the context vector cl,t generated by attention mechanism.
We use hl to predict next(l) = {y|y ∈ Y, l|l| ≺· y}. This sub-task is equivalent to the prediction of next_label(l) = {LY (y)|y ∈ Y, l|l| ≺· y}, because there will not be any other different topological traversal paths with the same label sequence in a semantic poset. Therefore, for each w ∈ V, we perform a binary classiﬁcation to predict whether w ∈ next_label(l), of which the probability for the positive class is computed via:
P (+|l, w, X) = (cid:20)1 (cid:21) 0 4 softmax(Wwhl) (4)
Algorithm 1 generate_poset (the decoding process at inference time)
Inputs: model input X, current poset Y , typological traversal path l, output vocabulary V and ˆV 1: next_labels := ∅, next_vertices := ∅ 2: for w ∈ V do 3: 4: 5: for next_label ∈ next_labels do if next_label ∈ ˆV then 6: 7: 8: 9: v := Y.edges.ﬁnd(λx : x.src_vertex = l|l| ∧ LY (x.target_vertex) = next_label) if P (+|l, w, X) > 0.5 then next_labels.add(w) v := Y.vertices.ﬁnd(λx : LY (x) = next_label) else v := new Vertex(next_label) next_vertices.add(v), Y.edges.add((l|l|, v)) if v ==nil then 10: 11: 12: 13: for v ∈ next_vertices do 14: 15: return Y generate_poset(X, Y, l ⊕ v, V, ˆV)
, where Ww is a learnable parameter matrix designed to produce a 2-dimensional logits from hl.
At training time, we estimate θ (parameters in the whole encoder-decoder model) via:
θ∗ = arg max (cid:89) (cid:89) (cid:89)
P (+|l, w, X) (5) (X,Y )∈Dtrain l∈paths(Y ) w∈V
At inference time, we predict the target poset via Algorithm 1, which is initialized with Y = ∅ and l is a zero-length path. An intuitive explanation for Algorithm 1 is: we decode topology traversal paths in parallel; different paths are merged at variables and entities (to ensure that each variable and entity is unique in every semantic poset). 5 Hierarchical Poset Decoding
Inspired by the essential idea that separating the learning of syntax and semantics helps a lot to capture the compositionality in language (Russin et al., 2019; Li et al., 2019; Lake, 2019; Gordon et al., 2020), we propose to incoporate a hierarchical mechanism into the basic poset decoding paradigm.
Our hierarchical poset decoding paradigm consists of three components: sketch prediction, primitive prediction, and traversal path prediction. Figure 4 shows an example of how this hierarchical poset decoding paradigm works in the CFQ task. 5.1 Sketch Prediction
For each semantic poset Y , we deﬁne that f : V → A is a function that maps each output token w ∈ V in the output vocabulary to an abstract token f (w) ∈ A. A is the vocabulary of those abstract tokens, and |A| << |V|.
Speciﬁcally, in the CFQ task, we deﬁne that: each predicate p ∈ P should be abstracted to a token
“P”, and each entity e ∈ E should be abstracted to a token “E”.
Then, a sketch SY can be extracted from Y through the following steps: 1. Initialize SY as a copy of Y . Then, For each v ∈ SY , let LSY (v) ← f (LSY (v)). 2. Merge two vertexes vi, vj ∈ SY , if LSY (vi) = LSY (vj), {v(cid:48)|v(cid:48) ∈ SY , v(cid:48) ≺· vi} = {v(cid:48)|v(cid:48) ∈
SY , v(cid:48) ≺· vj} and {v(cid:48)|v(cid:48) ∈ SY , vi ≺· v(cid:48)} = {v(cid:48)|v(cid:48) ∈ SY , vj ≺· v(cid:48)}. Intuitively, two vertexes should be merged, if they share the same label and neighbors. 3. Repeat Step 2, until there is no any pair of vertex meets this condition. 5
Figure 4: An example of how the hierarchical poset decoding paradigm works in the CFQ task.
The top-right corner in Figure 4 demonstrates a poset sketch as an example. This procedure garuantees that SY is still a semantic poset, with a much smaller label vocabulary A than V. Therefore, given an input X, we can predict the sketch of its target poset via the basic poset decoding algorithm (proposed in Section 4). 5.2 Primitive Prediction
We deﬁne primitives as prim = {w|w ∈ V, f (w) (cid:54)= w}. Given input X, the primitive prediction module is designed to generate a set primX ⊂ prim, which represents candidate primitives that are likely to be used in the target poset.
In the CFQ task, we implement the primitive prediction module by adopting phrase table induction from statistical machine translation (Koehn, 2009). More concretely, a phrase table is a collection of natural language phrases (i.e., n-grams) paired with a list of their possible translations in the target semantics. For each (phrase, primitive) pair, the translation probability p(primitive|phrase) is inducted from the training instances Dtrain. In our primitive prediction module, for each input question X, we deﬁne a primitive as a candidate primitive for X, if and only if there exists some phrases in X such that p(primitive|phrase) is larger than the threshold. 5.3 Traversal Path Prediction
For each topological traversal path in the predicted sketch, each vertex v with LSY (v) ∈ A \ V can be viewed as a slot, which can be ﬁlled by one label in {w|w ∈ V, f (w) = LSY (v)}. We enumerate all permutations, thus obtaining a set of candidate paths. For each candidate path, we aim to recognize entailment relation between path and the input X (i.e., whether path is a topological traversal path in the target poset of X). We implement this using an ESIM network (Chen et al., 2016):
P (path|X) = ESIM(X, path) (6)
Then we deterministically re-construct the target poset from paths meeting P (path|X) > 0.5. 6
6 Experiments 6.1 Settings
Dataset We conduct experiments on three different splits MCD1/MCD2/MCD3 of CFQ dataset (Key-sers et al., 2020). Each split contains 95k/12k/12k instances for training/development/test.
Implementation Details (1) We implement the sketch prediction module based on a 2-layer bi-directional GRU (Cho et al., 2014). The dimensions of the hidden state and word embedding are set to 512, 300 respectively. (2) To implement the primitive prediction module, we leverage
GIZA++2 (Och and Ney, 2003) toolkit to extract a phrase table with total 262 pairs from the train set. (3) For the traversal path prediction module, we utilize the ESIM (Chen et al., 2016) network implemented in Match-Zoo3 (Guo et al., 2019) with the default setting. (4) The training process lasts 50 epochs with batch size 64/256 for sketch prediction and traversal path prediction, respectively. We use the Adam optimizer with default settings (in PYTORCH) and a dropout layer with the rate of 0.5.
Evaluation Metric We use accuracy as the evaluation metric, i.e., the percentage of the examples that are correctly parsed to their gold standard meaning representations (SPARQL queries). 6.2 Results
As shown in Table 1, Hierarchical Poset Decoding (HPD) is superior to all baselines by a large margin on three splits. We also have the following observations: 1. Seq2seq models have poor compositionial generalization ability. In the ﬁrst block of Table 1, we list results of Seq2seq baselines reported by Keysers et al. (2020). The mean accuracy on the MCD splits is below 20% for all these models. 2. Tree decoder can hardly generalize to combinations that have not been previously observed in the training data. We represent each SPARQL query as a tree and predict the structure using a tree decoder (Dong and Lapata, 2016). This baseline performs even worse than Seq2seq baselines, which suggests that: tree decoder has poor compositional generalization ability even though the compositional structures in semantics are modeled explicitly. 3. Simplifying queries through merging similar triples helps a lot to improve the compositional generalization ability. We speculate that it is not suitable to directly decode standard SPARQL expressions, as it breaks up the compositional structure of semantics into ﬁne-grained triples. There-fore, we conduct experiments in which SPARQL queries are simpliﬁed through merging similar triples (the second block in Table 1). For example, given a SPARQL query consists of four triples — (x0, p1, e1), (x0, p2, e1), (x0, p1, e2) and (x0, p2, e2), we simplify it to (x0, (p1, p2), (e1, e2)). Then, we use Seq2seq models to predict these simpliﬁed expressions. This improves the accuracy a lot (but still not good enough). 4. CFQ tasks require broader compositional generalization ability beyond primitive substitution.
CGPS (Li et al., 2019) is a neural architecture speciﬁcally desgined for primitive substitution in compositional generalization. It performs poorly in CFQ tasks (4.8%/ 1.0%/1.8% accuracy on three
MCD splits). Compared to CGPS, our model performs well because it can handle not only primitive substitution (through the hierarchical mechanism), but also novel combinations of bigger components (through properly modeling partial permutation invariance among these components). 6.3 Ablation Analysis
We conduct a series of ablation analysis to better understand how components in HPD impact overall performance (the fourth block in Table 1). Our observations are as follows: 1. Enforcing partial permutation invariance in semantics can signiﬁcantly improve compositional generalization ability. When the poset decoding algorithm for sketch prediction is replaced with
Seq2Seq/Seq2Tree model, the mean accuracy on MCD splits drops from 69.0% to 56.7%/55.9%. 2https://github.com/moses-smt/giza-pp.git 3https://github.com/NTMC-Community/MatchZoo-py 7
Table 1: Performance of different models on three split of CFQ dataset.
Models
LSTM+Attention (Keysers et al., 2020)
Transformer (Keysers et al., 2020)
Universal Transformer (Keysers et al., 2020)
MCD1 MCD2 MCD3 28.9% 34.9% 37.4% 5.0% 8.2% 8.1% 10.8% 10.6% 11.3%
LSTM+Attention (with simpliﬁed SPARQL expression)
Transformer (with simpliﬁed SPARQL expression) 42.2% 14.5% 21.5% 53.0% 19.5% 21.6%
Seq2Tree (Dong and Lapata, 2016)
CGPS (Li et al., 2019)
Hierarchical Poset Decoding with Seq2Seq-based sketch prediction with Seq2Tree-based sketch prediction w/o Hierarchical Mechanism 24.3% 6.3% 4.1% 4.81% 1.04% 1.82% 79.6% 59.6% 67.8% 74.3% 45.7% 50.2% 75.7% 40.9% 51.1% 10.1% 6.4% 21.3%
Table 2: Performance of Different Components in Hierarchical Poset Decoding Model.
Split
MCD1
MCD2
MCD3
Sketch Prediction (SP)
Accuracy
Primitive Prediction (PP)
Recall
Precision
F1 Score
Traversal Path Prediction (TPP)
Recall
Precision
F1 Score 91.3% 75.1% 74.8% 34.4% 35.0% 34.1% 99.9% 99.9% 99.9% 51.1% 51.9% 50.8% 96.7% 89.2% 92.8% 98.9% 87.5% 90.8% 97.7% 88.4% 91.8% 2. The hierarchical mechanism is essential for poset decoding paradigm. The model without hierarchi-cal mechanism performs poorly: dropping 58.3%/53.2%/57.7% accuracy on MCD1/MCD2/MCD3, respectively. The reason is: when the output vocabulary V has a large size, it’s difﬁcult to well-train the basic poset decoding algorithm proposed in Section 4, since the classiﬁcation tasks described in Equation 4 will suffer from the class imbalance problem (for each w ∈ V, there are much more negative samples than positive samples.) The hierarchical mechanism alleviates this problem (because
|A| << |V|), thus bringing large proﬁts.
The ablation analysis shows that both the poset decoding paradigm and the hierarchical mechanism are of high importance for capturing the compositionality in language. 6.4 Break-Down Analysis
To understand the source of errors, we conduct evaluation for different components (i.e., sketch prediction, primitive prediction, and traversal path prediction) in our model. Table 2 shows the results.
Sketch Prediction For 8.7%/24.9%/25.2% of test cases in MCD1/MCD2/MCD3, our model fails to predict their correct poset sketches. Sketch is an abstraction of high-level structural information in semantics, corresponding to syntactic structures of natural language utterances. We guess the reason that causes these error cases is: though our decoder captures the compositionality of semantics, the encoder deals with input utterances as sequences, thus implicit syntactic structures of inputs are not well captured. Therefore, the compositional generalization ability is expected to be further improved through enhancing the encoder with the compositionality (Dyer et al., 2016; Kim et al., 2019; Shen et al., 2018) and build proper attention mechanism between the encoder and our hierarchical poset decoder. We leave this in future work.
Primitive Prediction Our primitive prediction module performs high recall (99.9%/99.9%/99.9%) but relative low precision (34.4%/35.0%/34.1%). Currently, our primitive prediction module is based on a simple phrase table. In future work, we plan to improve the recall through leveraging contextual information in utterances, thus alleviating the burden of traversal path prediction.
Traversal Path Prediction Our traversal path prediction module performs well on classifying candidate paths. This is consistent with the ﬁnding that neural networks exhibit compositional 8
generalization ability on entailment relation reasoning (Mul and Zuidema, 2019). However, for 11.7%/15.5%/7.0% of test cases in MCD1/MCD2/MCD3, our model fails to predict the exact collections of traversal paths. This is mainly due to the large number of candidate paths: on average, each test utterance has 6.1 positive candidate paths and 18.5 negative candidate paths. In future work, this problem is expected to be addressed through reducing the number of candidate primitives. 7