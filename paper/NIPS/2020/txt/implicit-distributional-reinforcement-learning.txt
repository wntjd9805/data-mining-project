Abstract
To improve the sample efﬁciency of policy-gradient based reinforcement learning algorithms, we propose implicit distributional actor-critic (IDAC) that consists of a distributional critic, built on two deep generator networks (DGNs), and a semi-implicit actor (SIA), powered by a ﬂexible policy distribution. We adopt a distributional perspective on the discounted cumulative return and model it with a state-action-dependent implicit distribution, which is approximated by the DGNs that take state-action pairs and random noises as their input. Moreover, we use the SIA to provide a semi-implicit policy distribution, which mixes the policy parameters with a reparameterizable distribution that is not constrained by an analytic density function. In this way, the policy’s marginal distribution is implicit, providing the potential to model complex properties such as covariance structure and skewness, but its parameter and entropy can still be estimated. We incorporate these features with an off-policy algorithm framework to solve problems with continuous action space and compare IDAC with state-of-the-art algorithms on representative OpenAI Gym environments. We observe that IDAC outperforms these baselines in most tasks. Python code is provided1. 1

Introduction
Model-free reinforcement learning (RL) plays an important role in addressing complex real-world sequential decision making tasks (MacAlpine and Stone, 2017; Silver et al., 2018; OpenAI, 2018).
With the help of deep neural networks, model-free deep RL algorithms have been successfully implemented in a variety of tasks, including game playing (Silver et al., 2016; Mnih et al., 2013) and robotic control (Levine et al., 2016). Deep Q-network (DQN) (Mnih et al., 2015) enables RL agent with human level performance on Atari games (Bellemare et al., 2013), motivating many follow-up works with further improvements (Wang et al., 2016; Andrychowicz et al., 2017). A novel idea, proposed by Bellemare et al. (2017a), is to take a distributional perspective for deep RL problems, which models the full distribution of the discounted cumulative return of a chosen action at a state rather than just the expectation of it, so that the model can capture its intrinsic randomness instead of just ﬁrst-order moment. Speciﬁcally, the distributional Bellman operator can help capture skewness and multimodality in state-action value distributions, which could lead to a more stable learning process, and approximating the full distribution may also mitigate the challenges of learning from a non-stationary policy. Under this distributional framework, Bellemare et al. (2017a) propose the
C51 algorithm that outperforms previous state-of-the-art classical Q-learning based algorithms on a range of Atari games. However, some discrepancies exist between the theory and implementation in
C51, motivating Dabney et al. (2018b) to introduce QR-DQN that borrows Wasserstein distance and quantile regression related techniques to diminish the theory-practice gap. Later on, the distributional view is also incorporated into the framework of deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015) for continuous control tasks, yielding efﬁcient algorithms such as distributed distributional
∗The ﬁrst two authors contributed equally. †Corresponding to mingyuan.zhou@mccombs.utexas.edu 1https://github.com/zhougroup/IDAC 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
DDPG (D4PG) (Barth-Maron et al., 2018) and sample-based distributional policy gradient (SDPG) (Singh et al., 2020). Due to the deterministic nature of the policy, these algorithms always manually add random noises to actions during the training process to avoid getting stuck in poor local optimums.
By contrast, stochastic policy takes that randomness as part of the policy, learns it during the training, and achieves state-of-the-art performance, with the soft actor-critic (SAC) algorithm of Haarnoja et al. (2018) being a successful case in point.
Motivated by the promising directions from distributional action-value learning and stochastic policy, this paper integrates these two frameworks in hopes of letting them strengthen each other. We model the distribution of the discounted cumulative return of an action at a state with a deep generator network (DGN), whose input consists of a state-action pair and random noise, and applies the distributional Bellman equation to update its parameters. The DGN plays the role of a distributional critic, whose output conditioning on a state-action pair follows an implicit distribution. Intuitively, only modeling the expectation of the cumulative return is inevitably discarding useful information readily available during the training, and modeling the full distribution of it could capture more useful information to help better train and stabilize a stochastic policy. In other words, there are considerable potential gains in guiding the training of a distribution with a distribution rather than its expectation.
For stochastic policy, the default distribution choice under continuous control is diagonal Gaussian.
However, assuming a unimodal and symmetric density at each dimension and independence between different dimensions make it incapable of capturing complex distributional properties, such as skew-ness, kurtosis, multimodality, and covariance structure. To fully take advantage of the distributional return modeled by the DGN, we thereby propose a semi-implicit actor (SIA) as the policy distribution, which adopts a semi-implicit hierarchical construction (Yin and Zhou, 2018) that can be made as com-plex as needed while remaining amenable to optimization via stochastic gradient descent (SGD). A naive combination of the DGN, an implicit distributional critic, and SIA, a semi-implicit actor, within an actor-critic policy gradient framework, however, only delivers mediocre performance, falling short of the promise it holds. We attribute its underachievement to the overestimation issue, commonly existing in classical value-based algorithms (Van Hasselt et al., 2016), that does not automatically go away under the distributional setting. Inspired by previous work in mitigating the overestimation issue in deep Q-learning (Fujimoto et al., 2018), we come up with a twin-delayed DGNs based critic, with which we provide a novel solution that takes the target values as the element-wise minimums of the sorted output values of these two DGNs, stabilizing the training process and boosting performance.
Contributions: The main contributions of this paper include: 1) we incorporate the distributional idea with the stochastic policy setting, and characterize the return distribution with the help of a DGN under a continuous control setup; 2) we introduce the twin-delayed structure on DGNs to mitigate the overestimation issue, involving element-wise minimization of two sorted vectors; and 3) we improve the ﬂexibility of the policy by using a SIA instead of a Gaussian or mixture of Gaussian distribution to improve exploration, introducing an asymptotic lower bound for entropy estimation.