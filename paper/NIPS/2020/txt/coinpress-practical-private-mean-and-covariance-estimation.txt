Abstract
We present simple differentially private estimators for the mean and covariance of multivariate sub-Gaussian data that are accurate at small sample sizes. We demonstrate the effectiveness of our algorithms both theoretically and empirically using synthetic and real-world datasets—showing that their asymptotic error rates match the state-of-the-art theoretical bounds, and that they concretely outperform all previous methods. Speciﬁcally, previous estimators either have weak empirical accuracy at small sample sizes, perform poorly for multivariate data, or require the user to provide strong a priori estimates for the parameters. 1

Introduction
One of the most basic problems in statistics and machine learning is to estimate the mean and covariance of a distribution based on i.i.d. samples. The optimal solutions to these problems are folklore—simply output the empirical mean and covariance of the samples. However, this solution is not suitable when the samples consist of sensitive, private information belonging to individuals, as it has been shown repeatedly that even releasing just the empirical mean can reveal this sensitive information [13, 23, 8, 18, 17]. Thus, we need estimators that are not only accurate with respect to the underlying distribution, but also protect the privacy of the individuals represented in the sample.
The most widely accepted solution to individual privacy in statistics and machine learning is differen-tial privacy (DP) [15], which provides a strong guarantee of individual privacy by ensuring that no individual has a signiﬁcant inﬂuence on the learned parameters. A large body of work now shows that, in principle, nearly every statistical task can be solved privately, and differential privacy is now being deployed by Apple [11], Google [20, 3], Microsoft [12], and the US Census Bureau [10].
Differential privacy requires adding random noise to some stage of the estimator, and this noise might increase the error of the ﬁnal estimate. Typically, the amount of noise vanishes as the sample size n grows, and one can often show that as n → ∞, the additional error due to privacy vanishes faster than the sampling error of the estimator, making differential privacy highly practical for large samples.
However, differential privacy is often difﬁcult to achieve for small datasets, or when we want to look at some small subpopulation within a large dataset. Thus, a recent trend has been to focus on simple,
∗Authors ordered alphabetically. Full version of the paper and code are available in the supplement, or alternatively at the following URLs: https://arxiv.org/abs/2006.06618 and https://github.com/ twistedcubic/coin-press. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
widely used estimation tasks, and design estimators with good concrete performance at small sample sizes. Most relevant to our work, Karwa and Vadhan [27] and Du, Foot, Moniot, Bray, and Groce [14] give practical mean and variance estimators for univariate Gaussian data. However, as we show, these methods do not scale well to the more challenging multivariate setting. 1.1 Contributions
In this work we give simple, practical estimators for the mean and covariance of multivariate sub-Gaussian data.2 We validate our estimators theoretically and empirically. On the theoretical side, we show that our estimators match the state-of-the-art asymptotic bounds for private sub-Gaussian mean and covariance estimation [24]. On the empirical side, we give an extensive evaluation with synthetic data, as well as a demonstration on a real-world dataset. We show that our estimators have error comparable to that of the non-private empirical mean and covariance at samples sizes on the order of 1,000. Our mean estimator also improves over the state-of-the-art method [14], which was developed for univariate data but can be applied coordinate-wise to estimate multivariate data.
We highlight that, like most private estimators, we require the user to input some a priori knowledge of the mean and covariance. For mean estimation, we require the mean lives in a speciﬁed ball of radius R, and for covariance estimation we require that the covariance matrix can be sandwiched spectrally between A and KA for some matrix A. Some a priori boundedness is necessary for algorithms like ours that satisfy concentrated DP [16, 6, 29, 4], or satisfy pure DP.3 We show that our estimator is practical when these parameters are taken to be extremely large, meaning the user only needs a very weak prior. Also, for simplicity, we focus on Gaussian data, but using both heavy-tailed synthetic data and real-world data, we show that our method remains useful beyond Gaussian data.
Note that some restriction on the decay of the tails is necessary in the worst case [26].
Approach. At a high-level, our estimators iteratively re-ﬁne an estimate of the parameters. For mean estimation, we start with some (potentially large) ball B1 of radius
R1 that contains most of the mass of the probability dis-tribution. We use this ball to get a naïve estimate: clip the data to the ball, then obtain an estimate of the mean by adding noise to the empirical mean of the clipped data, with magnitude proportional to R1/n. Using this estimate, and knowledge of how we obtained it, we can draw a (hopefully smaller) ball B2 of radius R2 that contains most of the mass and then repeat. After a few iterations, we will have some ball Bt of radius Rt that tightly contains most of the datapoints, and use this to make an accurate ﬁnal private estimate of the mean with noise proportional to Rt/n. Our covariance estimation uses the same iterative approach, although the geometry is signiﬁcantly more subtle.
Figure 1: Visualizing a run of the mean estimator with n = 160, ρ = 0.1, t = 3 1.2 Problem Formulation
We now detail the problem we consider in this work. We are given samples X = (X1, . . . , Xn) ⊆ Rd where each Xi represents some individual’s sensitive data. We would like an estimator M (X) that is private for the individuals in the sample, and also accurate in that when X consists of i.i.d. samples from some distribution P , then M (X) estimates the mean and covariance of P with small error.
Note that accuracy relies on distributional assumptions, but the privacy guarantee is worst-case. For privacy, we require that M is insensitive to any one datapoint in the following sense: We say that two samples X, X (cid:48) ⊆ Rd of size n are neighboring if they differ on at most one datapoint.4 Informally,
M is differentially private [15] if the distributions M (X) and M (X (cid:48)) are similar for every pair of 2While our results are stated for Gaussian data, we comment that identical arguments hold for sub-Gaussian distributions as well. 3Under pure or concentrated DP, the dependence on R and K must be polylogarithmic [27, 5]. One can allow R = ∞ for mean estimation under (ε, δ)-DP with δ > 0 [27], although the resulting algorithm has poor concrete performance even for univariate data. It is an open question whether one can allow K = ∞ for covariance estimation even under (ε, δ)-DP. 4For simplicity, we use the common convention that the size of the sample n is ﬁxed and public. 2
neighboring datasets X, X (cid:48).5 In this work, we adopt concentrated differential privacy (zCDP) [16, 6].
Deﬁnition 1.1. M (X) satisﬁes ρ-zCDP if for every pair of neighboring samples X, X (cid:48) of size n, and every α ∈ (1, ∞), Dα(M (X)(cid:107)M (X (cid:48))) ≤ ρα, where Dα is the Rényi divergence of order α.
This formulation sits in between general (ε, δ)-differential privacy and the special case of (ε, 0)-differential privacy,6 and better captures the privacy cost of private algorithms in high dimension [17].
To formulate the accuracy of our mechanism, we posit that X is sampled i.i.d. from some distribution
P , and our goal is to estimate the mean µ ∈ Rd and covariance Σ ∈ Rd×d with µ = Ex∼P [x] and
Σ = Ex∼P [(x − µ)T (x − µ)]. We assume that our algorithms are given some a priori estimate of the mean in the form of a radius R such that (cid:107)µ(cid:107)2 ≤ R and some a priori estimate of the covariance in the form of K such that I (cid:22) Σ (cid:22) KI (equivalently all singular values of Σ lie in [1, K]).
We measure the error in Mahalanobis distance (cid:107) · (cid:107)Σ, which compares the error to the covariance of the distribution, and has the beneﬁt of being invariant under afﬁne transformations. Speciﬁcally, (cid:107)ˆµ − µ(cid:107)Σ = (cid:107)Σ−1/2(ˆµ − µ)(cid:107)2 and (cid:107) ˆΣ − Σ(cid:107)Σ = (cid:107)Σ−1/2 ˆΣΣ−1/2 − I(cid:107)F .
For any distribution, the empirical mean ˆµ and empirical covariance ˆΣ satisfy E[(cid:107)ˆµ − µ(cid:107)Σ] ≤ (cid:112)d/n and E[(cid:107) ˆΣ − Σ(cid:107)Σ] ≤ (cid:112)d2/n, and these estimators are minimax optimal. Our goal is to obtain estimators that have similar accuracy to the empirical mean and covariance. The folklore naïve estimators (see e.g. [27, 24]) for mean and covariance based on clipping the data to an appropriate ball and adding carefully calibrated noise to the empirical mean and covariance would guarantee
E[(cid:107)ˆµ − µ(cid:107)Σ] ≤ (cid:114) d n
+
Rd
√
ρ n (cid:104) (cid:107) ˆΣ − Σ(cid:107)Σ and E (cid:105)
≤ (cid:114) d2 n
+
Kd2
√
ρ n
.
The downside of the naïve estimators is that they depend linearly on R and K, and thus have large error unless the user has strong a priori knowledge of the mean and covariance. Requiring users to provide such a priori bounds is a major challenge in systems for differentially private analysis [21].
Our estimators have much better concrete and asymptotic dependence on these parameters.