Abstract
Convolutional Neural Networks (CNNs) have shown impressive performance in computer vision tasks such as image classiﬁcation, detection, and segmentation.
Moreover, recent work in Generative Adversarial Networks (GANs) has high-lighted the importance of learning by progressively increasing the difﬁculty of a learning task [26]. When learning a network from scratch, the information propa-gated within the network during the earlier stages of training can contain distortion artifacts due to noise which can be detremental to training. In this paper, we pro-pose an elegant curriculum-based scheme that smoothes the feature embedding of a CNN using anti-aliasing or low-pass ﬁlters. We propose to augment the train-ing of CNNs by controlling the amount of high frequency information propagated within the CNNs as training progresses, by convolving the output of a CNN fea-ture map of each layer with a Gaussian kernel. By decreasing the variance of the
Gaussian kernel, we gradually increase the amount of high-frequency informa-tion available within the network for inference. As the amount of information in the feature maps increases during training, the network is able to progressively learn better representations of the data. Our proposed augmented training scheme signiﬁcantly improves the performance of CNNs on various vision tasks without either adding additional trainable parameters or an auxiliary regularization objec-tive. The generality of our method is demonstrated through empirical performance gains in CNN architectures across four different tasks: transfer learning, cross-task transfer learning, and generative models. The code will soon be released at www.github.com/pairlab/CBS. 1

Introduction
Deep Learning models have revolutionized the ﬁeld of computer vision, which has led to great progress in recent years. Convolutional Neural Networks (CNNs) [33], have turned out to be a very effective class of models, which have enabled state-of-the-art performance on a multitude of computer vision tasks such as image recognition [31, 19], semantic segmentation [38, 46], object detection [13, 45], pose estimation [62] to name a few.
Recent work by Karras et al. [26] showed excellent results on building a curricula to progressively increase the learning task for a GAN. By simply increasing the resolution of the image progressively, they are able to achieve signiﬁcantly better results and stabilize GAN training. This progressive curricula helps the CNN models learn and generate better representations during GAN training.
However a core question remains: how to design a curricula that fundamentally improves the ability of CNNs to learn better representations from data?
In this paper, we propose an elegant and effective curriculum that augments a CNN’s training regime by smoothing the feature maps of a CNN using low-pass or anti-aliasing ﬁlters, and progressively adding the high-frequency information in the feature maps to the model. Speciﬁcally, we propose to learn CNNs using a curricula, such that the high-frequency information can only be used by feature maps towards the later stages of training. As shown by [25], early stages of training is 1 University of Toronto, Vector Institute, 2 Nvidia, 2 Mila, Google Brain, CIFAR Fellow
Corresponding author: samarth.sinha@mail.utoronto.ca 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
critical in learning for deep networks. The proposed method improves training during early stages by reducing the noise propagated by the untrained parameters in the feature space by convolving the output of each CNN layer by Gaussian ﬁlters. During the early stages of training, a network propagates a signiﬁcant amount of noise due to the untrained parameters, therefore by using a low-pass or speciﬁcally a Gaussian ﬁlter, we are able to smooth the noise and reduce any aliasing artifact in the feature space. Furthermore, as the network parameters converge to the optimal solution, and the noise in the feature maps decreases, we anneal the standard deviation of the Gaussian ﬁlters which therefore increases the information propagated within the network, and allowing the network to learn richer representations from the newly available high-frequency information.
The Gaussian kernel is known as a low-pass ﬁlter with anti-aliasing properties, which smooths high-frequency information from the input. For a Gaussian kernel, the variance parameter controls the amount of high-frequency information that will be ﬁltered; therefore, by annealing the standard deviation of the Gaussian kernels, we can intuitively control the amount of high-frequency infor-mation within the layers over time and consequently improve the performance of deep networks on downstream vision tasks. It is also worth noting that the proposed method also adds no additional trainable parameters, is generic, and can be used with any CNN-variant.
The main contributions of this paper can be summarized as:
• We introduce an elegant and effective curricula that utilizes feature smoothing in CNNs to re-duce the amount of noise, due to untrained parameters, in the feature maps. This information is progressively added which leads to improvement in learned feature maps in CNNs.
• We conduct image classiﬁcation experiments using commnly-used vision datasets and CNN ar-chitecture variants to evaluate the effect of controlling smoothing feature maps during training.
• We evaluate the models trained on ImageNet, with and without our proposed curricula, as feature extractors to train “weak” classiﬁers on previously unseen data. We also use the pretrained CNNs on different vision tasks where ImageNet pretraining is important: semantic segmentation and object detection, and signiﬁcantly outperform models trained with curricula.
• Finally, we show further improvements on representation learning using VAEs [28] and on Zero-shot Domain Adaptation to highlight the generality of the solution as well as the robustness of the learned representations. 2