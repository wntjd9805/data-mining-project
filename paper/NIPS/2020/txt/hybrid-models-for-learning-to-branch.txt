Abstract
A recent Graph Neural Network (GNN) approach for learning to branch has been shown to successfully reduce the running time of branch-and-bound (B&B) algorithms for Mixed Integer Linear Programming (MILP). While the GNN relies on a GPU for inference, MILP solvers are purely CPU-based. This severely limits its application as many practitioners may not have access to high-end GPUs. In this work, we ask two key questions. First, in a more realistic setting where only a CPU is available, is the GNN model still competitive? Second, can we devise an alternate computationally inexpensive model that retains the predictive power of the GNN architecture? We answer the ﬁrst question in the negative, and address the second question by proposing a new hybrid architecture for efﬁcient branching on CPU machines. The proposed architecture combines the expressive power of GNNs with computationally inexpensive multi-layer perceptrons (MLP) for branching. We evaluate our methods on four classes of MILP problems, and show that they lead to up to 26% reduction in solver running time compared to state-of-the-art methods without a GPU, while extrapolating to harder problems than it was trained on. The code for this project is publicly available at https:
//github.com/pg2455/Hybrid-learn2branch. 1

Introduction
Mixed-Integer Linear Programs (MILPs) arise naturally in many decision-making problems such as auction design [1], warehouse planning [13], capital budgeting [14] or scheduling [15]. Apart from a linear objective function and linear constraints, some decision variables of a MILP are required to take integral values, which makes the problem NP-hard [35].
Modern mathematical solvers typically employ the B&B algorithm [29] to solve general MILPs to global optimality. While the worst-case time complexity of B&B is exponential in the size of the problem [38], it has proven efﬁcient in practice, leading to wide adoption in various industries. At a high level, B&B adopts a divide-and-conquer approach that consists in recursively partitioning the original problem into a tree of smaller sub-problems, and solving linear relaxations of the sub-problems until an integral solution is found and proven optimal.
∗The work was done during an internship at Mila and CERC. Correspondence to: <pgupta@robots.ox.ac.uk> 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Despite its apparent simplicity, there are many practical aspects that must be considered for B&B to perform well [2]; such decisions will affect the search tree, and ultimately the overall running time.
These include several decision problems [33] that arise during the execution of the algorithm, such as node selection: which sub-problem do we analyze next?; and variable selection (a.k.a. branching): which decision variable must be used (branched on) to partition the current sub-problem? While such decisions are typically made using hard-coded expert heuristics which are implemented in modern solvers, more and more attention is given to statistical learning approaches for replacing and improving upon those heuristics [23; 5; 26; 17; 39]. An extensive review of different approaches at the intersection of statistical learning and combinatorial optimization is given in Bengio et al. [7].
Recently, Gasse et al. [17] proposed to tackle the vari-able selection problem in B&B using a Graph Neural
Network (GNN) model. The GNN exploits the bi-partite graph formulation of MILPs together with a shared parametric representation, thus allowing it to model problems of arbitrary size. Using imita-tion learning, the model is trained to approximate a very good but computationally expensive “expert" heuristic named strong branching [6]. The resulting branching strategy is shown to improve upon previ-ously proposed approaches for branching on several
MILP problem benchmarks, and is competitive with state-of-the-art B&B solvers. We note that one lim-itation of this approach, with respect to general B&B heuristics, is that the resulting strategy is only tailored to the class of MILP problems it is trained on. This is very reasonable in our view, as practitioners usually only care about solving very speciﬁc problem types at any time.
Figure 1: Cumulative time cost of different branching policies: (i) the default internal rule
RPB of the SCIP solver; (ii) a GNN model (using a GPU or a CPU); and (iii) our hybrid model. Clearly the GNN model requires a
GPU for being competitive, while our hybrid model does not. (Measured on a capacitated facility location problem, medium size).
While the GNN model seems particularly suited for learning branching strategies, one drawback is a high computational cost for inference, i.e., choosing the branching variable at each node of the
B&B tree. In Gasse et al. [17], the authors use a high-end GPU card to speed-up the GNN inference time, which is a common practice in deep learning but is somewhat unrealistic for MILP practitioners.
Indeed, commercial MILP solvers rely solely on CPUs for computation, and the GNN model from
Gasse et al. [17] is not competitive on CPU-only machines, as illustrated in Figure 1. There is indeed a trade-off between the quality of the branching decisions made and the time spent obtaining those decisions. This trade-off is well-known in MILP community [2], and has given rise to carefully balanced strategies designed by MILP experts, such as hybrid branching [3], which derive from the computationally expensive strong branching heuristic [6].
In this paper, we study the time-accuracy trade-off in learning to branch with the aim of devising a model that is both computationally inexpensive and accurate for branching. To this end, we propose a hybrid architecture that uses a GNN model only at the root node of the B&B tree and a weak but fast predictor, such as a simple Multi-Layer Perceptron (MLP), at the remaining nodes. In doing so, the weak model is enhanced by high-level structural information extracted at the root node by the GNN model. In addition to this new hybrid architecture, we experiment and evaluate the impact of several variants to our training protocol for learning to branch, including: (i) end-to-end training [19; 8], (ii) knowledge distillation [25], (iii) auxiliary tasks [30], and (iv) depth-dependent weighting of the training loss for learning to branch, an idea originally proposed by He et al. [23] in the context of node selection.
We evaluate our approach on large-scale MILP instances from four problem families: Capacitated
Facility Location, Combinatorial Auctions, Set Covering, and Independent Set. We demonstrate empirically that our combination of hybrid architecture and training protocol results in state-of-the-art performance in the realistic setting of a CPU-restricted machine. While we observe a slight decrease in the predictive performance of our model with respect to the original GNN from [17], its reduced computational cost still allows for a reduction of up to 26% in overall solving time on all the evaluated benchmarks compared to the default branching strategy of the modern open-source solver SCIP [20].
Also, our hybrid model preserves the ability to extrapolate to harder problems than trained on, as the original GNN model. 2
2