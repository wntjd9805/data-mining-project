Abstract
Imitation learning trains policies to map from input observations to the actions that an expert would choose. In this setting, distribution shift frequently exacerbates the effect of misattributing expert actions to nuisance correlates among the observed variables. We observe that a common instance of this causal confusion occurs in partially observed settings when expert actions are strongly correlated over time: the imitator learns to cheat by predicting the expert’s previous action, rather than the next action. To combat this “copycat problem”, we propose an adversarial approach to learn a feature representation that removes excess information about the previous expert action nuisance correlate, while retaining the information necessary to predict the next action. In our experiments, our approach improves performance signiﬁcantly across a variety of partially observed imitation learning tasks. 1

Introduction
Imitation learning is a simple, yet powerful paradigm for learning complex behaviors from expert demonstrations, with many successful applications ranging from autonomous driving to natural language generation [35, 40, 26, 28, 8, 16, 52]. The key idea underlying these successes is straight-forward: given a training dataset of demonstrations by an expert, an agent’s action policy π can be trained by mapping demonstration states st to expert actions at.
Partially observed settings pose a problem for this approach: rather than the full state st, only observations ot are available to the agent. As an example, for an autonomous car driving agent with a forward-facing camera, a single frame observation ot from the camera omits much of the relevant state information for driving, such as the velocities of vehicles in front of the car, and the presence or absence of vehicles by its side. An policy trained to map ot to actions at would thus be severely limited in such settings [13, 5, 19], especially in imitation learning. However, this limitation may in principle be alleviated by a simple ﬁx: rather than training a policy π(at|ot), one could train a policy
π(at|˜ot = [ot, ot−1, ot−2, · · · ]), accessing past observations to ﬁll in missing state information from the current observation.
In practice, several prior works have reported that imitation from observation histories sometimes performs worse than imitation from a single frame alone [51, 26, 6]. To illustrate why this happens, consider the sequence of actions in an expert demonstration when it starts to drive in response to a red trafﬁc light turning green (Figure 1). Assuming an action space with only two actions,
∗Equal contribution.
†Work done while at UC Berkeley. cwen20@mails.tsinghua.edu.cn, jerrylin0928@berkeley.edu, trevor@eecs.berkeley.edu, dineshj@seas.upenn.edu, gaoyangiiis@tsinghua.edu.cn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: This ﬁgure demonstrates the “copycat” problem in an autonomous driving scenario. The top part of the ﬁgure shows a sequence of observations, where the vehicle waits at the red light and start to drive when the light turns green. The policy takes a sliding window of observations as input.
At the bottom of the ﬁgure, we show that a “copycat” policy which simply replays its previous action will predict all but one actions correctly. brake (a = 0), and throttle (a = 1), the sequence of expert actions over time would look like
[a0 = 0, a1 = 0, . . . , aτ = 0, aτ +1 = 1, . . . aT = 1].
Which imitation policies would be effective at predicting the expert action on data from such demonstrations? Consider a “copycat” action policy that simply copies the previous expert action and prescribes repeating it as the next action. On these demonstrations, this policy would produce the correct action at all but one time instant, when the expert switches from braking to throttling. Our imitation learner π(at|˜ot) could easily expresss this copycat policy: it could recover the previous expert action at−1 from the last two frames, ot and ot−1. The imitation training objective would encourage this, since this policy would produce low error on training and held-out demonstrations.
However, when testing for actual driving performance, it would be useless — it would simply never switch to the throttle action.
We hypothesize that this copycat problem arises in imitation policies accessing past observations when two conditions are met: (i) expert actions over time are strongly correlated, and (ii) past expert actions are easily recovered from the observation history. As our ﬁrst contribution, we empirically validate this hypothesis. We ﬁnd that the temporal correlation among expert actions leads to even higher temporal correlation among learned policy actions. Further, the higher this temporal correlation, the worse the performance of the imitation learner (Section 4).
Then, we propose a novel imitation learning objective with an adversarial learning method that ensures that the imitation policy ignores the known nuisance correlate — the previous action at−1. Our implicit approach is scalable and robust, avoiding the need to learn disentangled representations [7], or learn a mixture of exponentially many graph conditioned policies [12] in previously proposed approaches tackling related problems. Our method only needs an ofﬂine expert demonstration dataset, unlike methods like DAGGER [39] and CCIL [12] which resolve the causal confusion through online expert queries, or GAIL [21] which needs online environment interaction. Inspired by robotics applications, we demonstrate our approach in six simulated continuous robotic control settings. 2