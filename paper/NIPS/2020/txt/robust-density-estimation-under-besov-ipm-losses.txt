Abstract
We study minimax convergence rates of nonparametric density estimation under the Huber contamination model, in which a proportion of the data comes from an unknown outlier distribution. We provide the ﬁrst results for this problem under a large family of losses, called Besov integral probability metrics (IPMs), that include the Lp, Wasserstein, Kolmogorov-Smirnov, Cramer-von Mises, and other commonly used metrics. Under a range of smoothness assumptions on the population and outlier distributions, we show that a re-scaled thresholding wavelet estimator converges at minimax optimal rates under a wide variety of losses and also exhibits optimal dependence on the contamination proportion. We also provide a purely data-dependent extension of the estimator that adapts to both an unknown contamination proportion and the unknown smoothness of the true density. Finally, based on connections recently shown between density estimation under IPM losses and generative adversarial networks (GANs), we show that certain GAN architectures are robustly minimax optimal. 1

Introduction
In many settings, observed data contains not only samples from the distribution of interest, but also a small proportion of outlier samples. Because these outliers can exhibit arbitrary, unpredictable behavior, they can be difﬁcult to detect or to explicitly account for. This has inspired a large body of work on robust statistics, which seeks statistical methods for which the error introduced by a small proportion of arbitrary outlier samples can be controlled.
The majority of work in robust statistics has focused on providing guarantees under the Huber (cid:15)-contamination model [Huber, 1965]. Under this model, data is assumed to be observed from a mixture distribution (1 − (cid:15))P + (cid:15)G, where P is an unknown population distribution of interest, G is an unknown outlier distribution, and (cid:15) ∈ [0, 1) is the “contamination proportion” of outlier samples.
Equivalently, this models the misspeciﬁed case in which data are drawn from a small perturbation by (cid:15)(G − P ) of the target distribution P of interest. The goal is then to develop methods whose performance degrades as little as possible when (cid:15) is non-negligible.
The present paper studies nonparametric density estimation under this model. Speciﬁcally, given independent and identically distributed samples from the mixture (1 − (cid:15))P + (cid:15)G, we characterize minimax optimal convergence rates for estimating P . Prior work on this problem has assumed P has a Hölder continuous density p and has provided minimax rates under total variation loss [Chen et al., 2018] or for estimating p(x) at a point x [Liu and Gao, 2017]. In the present paper, in addition 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to considering a much wider range of smoothness conditions (characterized by p lying in a Besov space), we provide results under a large family of losses called integral probability metrics (IPMs); dF (P, Q) = sup f ∈F (cid:12) (cid:12) (cid:12) (cid:12)
E
X∼P f (X) − E
X∼Q f (X) (cid:12) (cid:12) (cid:12) (cid:12)
, (1) where P and Q are probability distributions and F is a “discriminator class” of bounded Borel functions. As shown in several recent papers [Liu et al., 2017, Liang, 2018, Singh et al., 2018, Uppal et al., 2019], IPMs play a central role not only in nonparametric statistical theory and empirical process theory, but also in the theory of generative adversarial networks (GANs). Hence, this work advances not only basic statistical theory but also our understanding of the robustness properties of
GANs.
In this paper, we speciﬁcally discuss the case of Besov IPMs, in which F is a Besov space (see
Section 2.1). In classical statistical problems, Besov IPMs provide a uniﬁed formulation of a wide variety of distances, including Lp [Wasserman, 2006, Tsybakov, 2009], Sobolev [Mroueh et al., 2017, Leoni, 2017], maximum mean discrepancy (MMD; [Tolstikhin et al., 2017])/energy [Székely et al., 2007, Ramdas et al., 2017], Wasserstein/Kantorovich-Rubinstein [Kantorovich and Rubinstein, 1958, Villani, 2008], Kolmogorov-Smirnov [Kolmogorov, 1933, Smirnov, 1948], and Dudley metrics
[Dudley, 1972, Abbasnejad et al., 2018]. Hence, as we detail in Section 4.3, our bounds for robust nonparametric density estimation apply under many of these losses. More recently, it has been shown that generative adversarial networks (GANs) can be cast in terms of IPMs, such that convergence rates for density estimation under IPM losses imply convergence rates for certain GAN architectures [Liang, 2018, Singh et al., 2018, Uppal et al., 2019]. Thus, as we show in Section 5, our results imply the
ﬁrst robustness results for GANs in the Huber model.
In addition to showing rates in the classical Huber model, which avoids assumptions on the outlier distribution G, we consider how rates change under additional assumptions on G. Speciﬁcally, we show faster convergence rates are possible under the assumption that G has a bounded density g, but that these rates are not further improved by additional smoothness assumptions on g.
Finally, we overcome a technical limitation of recent work studying density estimation under Besov
IPMs losses. Namely, the estimators used in past work rely on the unrealistic assumption that the practitioner knows the Besov space in which the true density lies. This paper provides the ﬁrst convergence rates for a purely data-dependent density estimator under Besov IPMs, as well as the
ﬁrst nonparametric convergence guarantees for a fully data-dependent GAN architecture. 1.1 Paper Organization
The rest of this paper is organized as follows. Section 2 formally states the problem we study and deﬁnes essential notation. Section 3 discusses related work in nonparametric density estimation.
Section 4.1 contains minimax rates under the classical “unstructured” Huber contamination model, while Section 4.2 studies how these rates change when additional assumptions are made on the contamination distribution. Section 4.3 develops our general results from Sections 4.1 and 4.2 into concrete minimax convergence rates for important special cases. Finally, Section 5 applies our theoretical results to bound the error of perfectly optimized GANs in the presence of contaminated data. All theoretical results are proven in the Appendix. 2 Formal Problem Statement
We now formally state the problems studied in this paper. Let p be a density of interest and g be the contamination density such that X1, . . . , Xn ∼ (1 − (cid:15))p + (cid:15)g are n IID samples. We wish to use these samples to estimate p. We consider two qualitatively different types of contamination, as follows.
In the “unstructured” or Huber contamination setting, we assume that p lies in some regularity class
Fg, but g may be any compactly supported density. In particular, we assume that the data is generated from a density living in the set M((cid:15), Fg) = {(1 − (cid:15))p + (cid:15)g : p ∈ Fg, g has compact support}. We then wish to bound the minimax risk of estimating p under an IPM loss dFd ; i.e., the quantity
R(n, (cid:15), Fg, Fd) = inf (cid:98)pn sup f ∈M((cid:15),Fg)
E f
[dFd (p, (cid:98)pn)] (2) 2
Figure 1: Father, Mother, and ﬁrst few Daughter elements of the Haar Wavelet Basis. where the inﬁmum is taken over all estimators (cid:98)pn.
In the “structured” contamination setting, we additionally assume that the contamination density g lives in a smoothness class Fc. The data is generated by a density in M((cid:15), Fg, Fc) = {(1 − (cid:15))p + (cid:15)g : p ∈ Fg, g ∈ Fc} and we seek to bound the minimax risk sup f ∈M((cid:15),Fg,Fc)
R(n, (cid:15), Fg, Fc, Fd) = inf (cid:98)pn
[dFd ((cid:98)pn, p)] .
E f (3)
In the following section, we provide notation to formalize the spaces Fg, Fc and Fd that we consider. 2.1 Set up and Notation
For non-negative real sequences {an}n∈N, {bn}n∈N, an (cid:46) bn indicates lim supn→∞
< ∞, and an (cid:16) bn indicates an (cid:46) bn (cid:46) an. For q ∈ [1, ∞], q(cid:48) := q q−1 denotes the Hölder conjugate of q (with 1(cid:48) = ∞, ∞(cid:48) = 1). Lq(RD) (resp. lq) denotes the set of functions f (resp. sequences a) with (cid:107)f (cid:107)q := (cid:0)(cid:82) |f (x)|q dx(cid:1)1/q
We now deﬁne the family of Besov spaces studied in this paper. Besov spaces generalize Hölder and
Sobolev spaces and are deﬁned using wavelet bases. As opposed to the Fourier basis, wavelet bases provide a localization in space as well as frequency which helps express spatially inhomogeneous smoothness.
< ∞ (resp. (cid:107)a(cid:107)lq := (cid:0)(cid:80) n∈N |an|q(cid:1)1/q
< ∞). an bn
A wavelet basis is formally deﬁned by the mother (ψ(x)) and father(φ(x)) wavelets. The basis consists of two parts; ﬁrst, the set of translations of the father and mother wavelets i.e.
Φ = {φ(x − k) : k ∈ Zd}
Ψ = {ψ(cid:15)(x − k) : k ∈ Zd, (cid:15) ∈ {0, 1}D}, (4) (5) second, the set of daughter wavelets, i.e.,
Then the union Φ ∪ Ψ ∪ ((cid:83)
Ψj = {2Dj/2ψ(cid:15)(2Djx − k) : k ∈ Zd, (cid:15) ∈ {0, 1}D}. j≥0)Ψj is an orthonormal basis for L2(RD).
We defer the technical assumptions on the mother and father wavelet to the appendix. Instead for intuition, we illustrate in Figure 1 the few terms of the best-known wavelet basis of L2(R), the Haar wavelet basis. (6)
In higher dimensions, the wavelet basis is deﬁned using the tensor product of wavelets in dimension 1. For details, see, Härdle et al. [2012] and Meyer [1992].
To effectively express smooth functions we will require r-regular (r-regularity is precisely deﬁned in the appendix) wavelets. We assume throughout our work that φ and ψ are compactly supported r-regular wavelets. We now formally deﬁne a Besov space.
Deﬁnition 1 (Besov Space). Given an r-regular wavelet basis of L2(RD), let 0 ≤ σ < r, and p,q(RD) is deﬁned as the set of functions f : RD → R that p, q ∈ [1, ∞]. Then the Besov space Bσ satisfy (cid:107)f (cid:107)Bσ p,q
:= (cid:107)α(cid:107)lp + (cid:13) (cid:13) (cid:13) (cid:13) (cid:110) 2j(σ+D(1/2−1/p)) (cid:107)βj(cid:107)lp (cid:111) j∈N (cid:13) (cid:13) (cid:13) (cid:13)lq
< ∞ (7)
RD f (x)φ(x)dx and βj is the set of vectors where α is the set of vectors {αφ}φ∈Φ where αφ := (cid:82)
{βψ}ψ∈Ψj , where βψ := (cid:82)
RD f (x)ψ(x)dx. 3
The quantity (cid:107)f (cid:107)Bσ closed Besov ball Bσ for rates of convergence), Bσ well-known examples from the rich class of resulting spaces in Section 4.3. p,q is called the Besov norm of f . For any L > 0, we write Bσ p,q(L) = {f ∈ Bσ p,q(L) to denote the
≤ L}. When the constant L is unimportant (e.g., p,q(L) of ﬁnite but arbitrary radius L. We provide p,q : (cid:107)f (cid:107)Bσ p,q denotes a ball Bσ p,q
We now deﬁne “linear (distribution) estimators”, a commonly used sub-class of distribution estimators:
Deﬁnition 2 (Linear Estimator). Let (Ω, F, P ) be a probability space. An estimate (cid:98)P of P is said to be linear if there exist functions Ti(Xi, ·) : F → R such that for all measurable A ∈ F, (cid:98)P (A) = (cid:80)n i=1 Ti(Xi, A).
Common examples of linear estimators are the empirical distribution, the kernel density estimator and the linear wavelet series estimator considered in this paper. 3