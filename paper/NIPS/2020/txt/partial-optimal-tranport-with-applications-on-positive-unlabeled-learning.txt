Abstract
Classical optimal transport problem seeks a transportation map that preserves the total mass between two probability distributions, requiring their masses to be equal.
This may be too restrictive in some applications such as color or shape matching, since the distributions may have arbitrary masses and/or only a fraction of the total mass has to be transported. In this paper, we address the partial Wasserstein and
Gromov-Wasserstein problems and propose exact algorithms to solve them. We showcase the new formulation in a positive-unlabeled (PU) learning application.
To the best of our knowledge, this is the ﬁrst application of optimal transport in this context and we ﬁrst highlight that partial Wasserstein-based metrics prove effective in usual PU learning settings. We then demonstrate that partial Gromov-Wasserstein metrics are efﬁcient in scenarii in which the samples from the positive and the unlabeled datasets come from different domains or have different features. 1

Introduction
Optimal transport (OT) has been gaining in recent years an increasing attention in the machine learning community, mainly due to its capacity to exploit the geometric property of the samples.
Generally speaking, OT is a mathematical tool to compare distributions by computing a transportation mass plan from a source to a target distribution. Distances based on OT are referred to as the
Monge-Kantorovich or Wasserstein distances (Villani, 2009) and have been successfully employed in a wide variety of machine learning applications including clustering (Ho et al., 2017), computer vision (Bonneel et al., 2011; Solomon et al., 2015), generative adversarial networks (Arjovsky et al., 2017) or domain adaptation (Courty et al., 2017).
A key limitation of the Wasserstein distance is that it relies on the assumption of aligned distributions, namely they must belong to the same ground space or at least a meaningful distance across domains can be computed. Nevertheless, source and target distributions can be collected under distinct environments, representing different times of collection, contexts or measurements (see Fig. 1, left and right). To get beneﬁt from OT on such heterogeneous distribution settings, one can compute the
Gromov-Wasserstein (GW) distance (Sturm, 2006; Mémoli, 2011) to overcome the lack of intrinsic correspondence between the distribution spaces. GW extends Wasserstein by computing a distance between metrics deﬁned within each of the source and target spaces. From a computational point view, it involves a non convex quadratic problem (Peyré and Cuturi, 2019), hard to lift to large scale settings. A remedy to such a heavy computation burden lies in a prevalent approach referred to as 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
regularized OT (Cuturi, 2013), allowing one to add an entropic regularization penalty to the original problem. Peyré et al. (2016); Solomon et al. (2016) propose the entropic GW discrepancy, that can be solved by Sinkhorn iterations (Cuturi, 2013; Benamou et al., 2015).
A major bottleneck of OT in its traditional formulation is that it requires the two input measures to have the same total probability mass and/or that all the mass has to be transported. This is too restrictive for many applications, such as in color matching or shape registration (Bonneel and
Coeurjolly, 2019), since mass changes may occur due to a creation or an annihilation while computing an OT plan. To tackle this limitation, one may employ strategies such as partial or unbalanced transport (Guittet, 2002; Figalli, 2010; Caffarelli and McCann, 2010). Chizat et al. (2018) propose to relax the marginal constraints of unbalanced total masses using divergences such as Kullback-Leibler or Total Variation, allowing the use of generalized Sinkhorn iterations. Yang and Uhler (2019) generalize this approach to GANs and Lee et al. (2019) present an ADMM algorithm for the relaxed partial OT. Most of all these approaches concentrate on partial-Wasserstein.
This paper deals with exact partial Wassertein (partial-W) and Gromov-Wasserstein (partial-GW).
Some strategies for computing such partial-W require relaxations of the marginals constraints. We rather build our approach upon adding virtual or dummy points onto the marginals, which is a common practice in OT works. Among the latter, Caffarelli and McCann (2010) attach such points to allow choosing the maximum distance mass that can be transported. Pele and Werman (2009) threshold ground distances and send the extra mass to a dummy point to compute a robust EMD distance.
Gramfort et al. (2015) consider the case of unnormalized measures and use a dummy point to “ﬁll” the distributions, the extended problem then having both marginals summing to one. More recently,
Sarlin et al. (2020) deal with the partial assignment problem by extending the initial problem and ﬁll the ground distance matrix with a single learnable parameter. In this paper, the dummy points are used as a buffer when comparing distributions with different probability masses, allowing partial-W to boil down to solving an extended but standard Wasserstein problem. The main advantage of our approach is that it deﬁnes explicitly the mass to be transported and it leads to computing sparse transport plans and hence exact partial-W or -GW distances instead of regularized discrepancies obtained by running
Sinkhorn algorithms. Regarding partial-GW, our approach relies on a Frank-Wolfe optimization algorithm (Frank and Wolfe, 1956) that builds on computations of partial-W.
Tackling partial-OT problems that preserve sparsity is motivated by the fact that they are more suitable to some applications such as the Positive-Unlabeled (PU) learning (see Bekker and Davis (2020) for a review) we target in this paper. We shall notice that this is the ﬁrst application of OT for solving
PU learning tasks. In a nutshell, PU classiﬁcation is a variant of the binary classiﬁcation problem, in which we have only access to labeled samples from the positive (Pos) class in the training stage. The aim is to assign classes to the points of an unlabeled (Unl) set which mixes data from both positive and negative classes. Using OT allows identifying the positive points within Unl, even when Pos and
Unl samples do not lie in the same space (see Fig. 1).
The paper is organized as follows: we ﬁrst recall some background on OT. In Section 3, we propose an algorithm to solve an exact partial-W problem, together with a Frank-Wolfe based algorithm to compute the partial-GW solution. After describing in more details the PU learning task and the use of partial-OT to solve it, we illustrate the advantage of partial-GW when the source and the target distributions are collected onto distinct environments. We ﬁnally give some perspectives. i pi = 1(cid:9) and δ is the Dirac function.
Notations ΣN is an histogram of N bins with (cid:8)p ∈ RN
Let 1n be the n-dimensional vector of ones. (cid:104)·, ·(cid:105)F stands for the Frobenius dot product. |p| indicates the length of vector p.
+ , (cid:80) 2