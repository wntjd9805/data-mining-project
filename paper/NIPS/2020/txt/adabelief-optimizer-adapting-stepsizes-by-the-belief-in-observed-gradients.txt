Abstract
Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability. We propose
AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classiﬁcation and language modeling. Speciﬁcally, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer 1

Introduction
Modern neural networks are typically trained with ﬁrst-order gradient methods, which can be broadly categorized into two branches: the accelerated stochastic gradient descent (SGD) family [1], such as
Nesterov accelerated gradient (NAG) [2], SGD with momentum [3] and heavy-ball method (HB) [4]; and the adaptive learning rate methods, such as Adagrad [5], AdaDelta [6], RMSProp [7] and Adam
[8]. SGD methods use a global learning rate for all parameters, while adaptive methods compute an individual learning rate for each parameter.
Compared to the SGD family, adaptive methods typically converge fast in the early training phases, but have poor generalization performance [9, 10]. Recent progress tries to combine the beneﬁts of both, such as switching from Adam to SGD either with a hard schedule as in SWATS [11], or with a smooth transition as in AdaBound [12]. Other modiﬁcations of Adam are also proposed: AMSGrad
[13] ﬁxes the error in convergence analysis of Adam, Yogi [14] considers the effect of minibatch size, MSVAG [15] dissects Adam as sign update and magnitude scaling, RAdam [16] rectiﬁes the variance of learning rate, Fromage [17] controls the distance in the function space, and AdamW [18] decouples weight decay from gradient descent. Although these modiﬁcations achieve better accuracy compared to Adam, their generalization performance is typically worse than SGD on large-scale 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
datasets such as ImageNet [19]; furthermore, compared with Adam, many optimizers are empirically unstable when training generative adversarial networks (GAN) [20].
To solve the problems above, we propose “AdaBelief”, which can be easily modiﬁed from Adam.
Denote the observed gradient at step t as gt and its exponential moving average (EMA) as mt. Denote the EMA of g2 t and (gt − mt)2 as vt and st, respectively. mt is divided by vt in Adam, while it
√ 1√ is the “belief” in the observation: viewing mt as is divided by st the prediction of the gradient, if gt deviates much from mt, we have weak belief in gt, and take a small step; if gt is close to the prediction mt, we have a strong belief in gt, and take a large step.
We validate the performance of AdaBelief with extensive experiments. Our contributions can be summarized as: st in AdaBelief. Intuitively,
√
• We propose AdaBelief, which can be easily modiﬁed from Adam without extra parameters.
AdaBelief has three properties: (1) fast convergence as in adaptive gradient methods, (2) good generalization as in the SGD family, and (3) training stability in complex settings such as GAN.
• We theoretically analyze the convergence property of AdaBelief in both convex optimization and non-convex stochastic optimization.
• We validate the performance of AdaBelief with extensive experiments: AdaBelief achieves fast convergence as Adam and good generalization as SGD in image classiﬁcation tasks on CIFAR and ImageNet; AdaBelief outperforms other methods in language modeling; in the training of a
W-GAN [21], compared to a well-tuned Adam optimizer, AdaBelief signiﬁcantly improves the quality of generated images, while several recent adaptive optimizers fail the training. 2 Methods 2.1 Details of AdaBelief Optimizer
Notations By the convention in [8], we use the following notations:
F ,M (y) = argminx∈F ||M 1/2(x − y)||: projection of y onto a convex feasible set F
• f (θ) ∈ R, θ ∈ Rd: f is the loss function to minimize, θ is the parameter in Rd
• (cid:81)
• gt: the gradient and step t
• mt: exponential moving average (EMA) of gt t , st is the EMA of (gt − mt)2
• vt, st: vt is the EMA of g2
• α, (cid:15): α is the learning rate, default is 10−3; (cid:15) is a small number, typically set as 10−8
• β1, β2: smoothing parameters, typical values are β1 = 0.9, β2 = 0.999
• β1t, β2t are the momentum for mt and vt respectively at step t, and typically set as constant (e.g. β1t = β1, β2t = β2, ∀t ∈ {1, 2, ...T }
Algorithm 1: Adam Optimizer
Initialize θ0, m0 ← 0 , v0 ← 0, t ← 0
While θt not converged t ← t + 1 gt ← ∇θft(θt−1) mt ← β1mt−1 + (1 − β1)gt vt ← β2vt−1 + (1 − β2)g2 t
Bias Correction (cid:99)mt ← mt
, (cid:98)vt ← vt 1−βt 1 1−βt 2
Update
θt ← (cid:81)
√
F , (cid:16) (cid:98)vt
θt−1 − α (cid:99)mt√ (cid:98)vt+(cid:15) (cid:17)
Algorithm 2: AdaBelief Optimizer
Initialize θ0, m0 ← 0 , s0 ← 0, t ← 0
While θt not converged t ← t + 1 gt ← ∇θft(θt−1) mt ← β1mt−1 + (1 − β1)gt st ← β2st−1+(1−β2)(gt−mt)2+(cid:15)
Bias Correction (cid:99)mt ← mt
, (cid:98)st ← st 1−βt 2 1−βt 1
Update
θt ← (cid:81)
√
F , (cid:98)st (cid:16)
θt−1 − α (cid:99)mt√ (cid:98)st+(cid:15) (cid:17)
Comparison with Adam Adam and AdaBelief are summarized in Algo. 1 and Algo. 2, where all operations are element-wise, with differences marked in blue. Note that no extra parameters are introduced in AdaBelief. Speciﬁcally, in Adam, the update direction is mt/ vt, where vt is st, where st is the EMA of (gt − mt)2. the EMA of g2
Intuitively, viewing mt as the prediction of gt, AdaBelief takes a large step when observation gt is close to prediction mt, and a small step when the observation greatly deviates from the prediction. (cid:98). represents bias-corrected value. Note that an extra (cid:15) is added to st during bias-correction, in order to t ; in AdaBelief, the update direction is mt/
√
√ 2
Table 1: Comparison of optimizers in various cases in Fig. 1. “S” and “L” represent “small” and
“large” stepsize, respectively. |∆θt|ideal is the stepsize of an ideal optimizer. Note that only AdaBelief matches the behaviour of an ideal optimizer in all three cases.
Case 1
S
S
L
SGD Adam AdaBelief
Case 2
L
L
S
SGD Adam AdaBelief
Case 3
L
S
L
SGD Adam AdaBelief
|gt|, vt
|gt − gt−1|, st
|∆θt|ideal
S
L
L
L
S
S
L
S
L
|∆θt| better match the assumption that st is bouded below (the lower bound is at leat (cid:15)). For simplicity, we omit the bias correction step in theoretical analysis. 2.2
Intuitive explanation for beneﬁts of AdaBelief
AdaBelief uses curvature information Update formulas for SGD, Adam and AdaBelief are:
∆θSGD t
∆θAdaBelief t
= −αmt, ∆θAdam
√
= −αmt/ st t
√
= −αmt/ vt, (1)
Note that we name α as the “learning rate” and |∆θi t| as the “stepsize” for the ith parameter. With a 1D ex-ample in Fig. 1, we demonstrate that AdaBelief uses the curvature of loss functions to improve training as summarized in Table 1, with a detailed description below: (1) In region 1 in Fig. 1, the loss function is ﬂat, hence the gradient is close to 0. In this case, an ideal optimizer should take a large stepsize. The stepsize of SGD is proportional to the EMA of the gradient, hence is small in this case; while both Adam and
√
√
Figure 1: An ideal optimizer considers curva-ture of the loss function, instead of taking a large (small) step where the gradient is large (small) [22].
AdaBelief take a large stepsize, because the denominator ( vt and st) is a small value. (2) In region 2 , the algorithm oscillates in a “steep and narrow” valley, hence both |gt| and |gt −gt−1| is large. An ideal optimizer should decrease its stepsize, while SGD takes a large step (proportional to mt). Adam and AdaBelief take a small step because the denominator ( vt) is large. st and
√
√ (3) In region 3 , we demonstrate AdaBelief’s advantage over Adam in the “large gradient, small curvature” case. In this case, |gt| and vt are large, but |gt − gt−1| and st are small; this could happen because of a small learning rate α. In this case, an ideal optimizer should increase its stepsize. SGD uses a large stepsize (∼ α|gt|); in Adam, the denominator vt is large, hence the stepsize is small; in
AdaBelief, denominator st is small, hence the stepsize is large as in an ideal optimizer.
√
√
To sum up, AdaBelief scales the update direction by the change in gradient, which is related to the
Hessian. Therefore, AdaBelief considers curvature information and performs better than Adam.
AdaBelief considers the sign of gradient in denominator We show the advantages of AdaBelief with a 2D example in this section, which gives us more intuition for high dimensional cases. In Fig. 2, we consider the loss function: f (x, y) = |x| + |y|. Note that in this simple problem, the gradient in each axis can only take {1, −1}. Suppose the start point is near the x−axis, e.g. y0 ≈ 0, x0 (cid:28) 0.
Optimizers will oscillate in the y direction, and keep increasing in the x direction.
Suppose the algorithm runs for a long time (t is large), so the bias of EMA (βt 1
Egt) is small: mt = EM A(g0, g1, ...gt) ≈ E(gt), mt,x ≈ Egt,x = 1, mt,y ≈ Egt,y = 0 vt = EM A(g2 t,y = 1. t,x = 1, vt,y ≈ Eg2 t ), vt,x ≈ Eg2 t ) ≈ E(g2 1, ...g2 0, g2 (2) (3) 3
Step gx gy vx vy sx sy 1 1
-1 1 1 0 1 2 1 1 1 1 0 1 3 1
-1 1 1 0 1 4 1 1 1 1 0 1 5 1
-1 1 1 0 1
Adam
AdaBelief
Figure 2: Left: Consider f (x, y) = |x| + |y|. Blue vectors represent the gradient, and the cross represents the optimal point. The optimizer oscillates in the y direction, and keeps moving forward in the x direction. Right: Optimization process for the example on the left. Note that denominator
√ st,y, hence vt,y for Adam, hence the same stepsize in x and y direction; while vt,x = st,x <
√
√
√
AdaBelief takes a large step in the x direction, and a small step in the y direction.
In practice, the bias correction step will further reduce the error between the EMA and its expectation if gt is a stationary process [8]. Note that: st = EM A(cid:0)(g0 − m0)2, ...(gt − mt)2(cid:1) ≈ E(cid:2)(gt − Egt)2(cid:3) = Vargt, st,x ≈ 0, st,y ≈ 1
An example of the analysis above is summarized in Fig. 2. From Eq. 3 and Eq. 4, note that in Adam, vx = vy; this is because the update of vt only uses the amplitude of gt and ignores its sign, hence the stepsize for the x and y direction is the same 1/ vt,y. AdaBelief considers both the magnitude and sign of gt, and 1/ st,y, hence takes a large step in the x direction and a st,x (cid:29) 1/ small step in the y direction, which matches the behaviour of an ideal optimizer. vt,x = 1/ (4)
√
√
√
√
Update direction in Adam is close to “sign descent” in low-variance case
In this section, we demonstrate that when the gradient has low variance, the update direction in Adam is close to “sign descent”, hence deviates from the gradient. This is also mentioned in [15].
Under the following assumptions: (1) assume gt is drawn from a stationary distribution, hence after bias correction, Evt = (Egt)2 + Vargt. (2) low-noise assumption, assume (Egt)2 (cid:29) Vargt, hence we have Egt/ 1 (β1 to the power of t) is small, hence mt as an estimator of Egt has a small bias βt 1
Evt ≈ Egt/(cid:112)(Egt)2 = sign(Egt). (3) low-bias assumption, assume βt
√
Egt. Then
||Egt|| = −α sign(Egt) (5)
∆θAdam t
= −α mt√ vt+(cid:15) ≈ −α
√
Egt (Egt)2+Vargt+(cid:15)
≈ −α Egt
In this case, Adam behaves like a “sign descent”; in 2D cases the update is ±45◦ to the axis, hence deviates from the true gradient direction. The “sign update” effect might cause the generalization gap between adaptive methods and SGD (e.g. on ImageNet) [23, 9]. For AdaBelief, when the variance of gt is the same for all coordinates, the update direction matches the gradient direction; when the variance is not uniform, AdaBelief takes a small (large) step when the variance is large (small).
Numerical experiments
In this section, we validate intuitions in Sec. 2.2. Examples are shown in Fig. 3, and we refer readers to more video examples1 for better visualization. In all examples, compared with SGD with momentum and Adam, AdaBelief reaches the optimal point at the fastest speed. Learning rate is α = 10−3 for all optimizers. For all examples except Fig. 3(d), we set the parameters of AdaBelief to be the same as the default in Adam [8], β1 = 0.9, β2 = 0.999, (cid:15) = 10−8, and set momentum as 0.9 for SGD. For Fig. 3(d), to match the assumption in Sec. 2.2, we set
β1 = β2 = 0.3 for both Adam and AdaBelief, and set momentum as 0.3 for SGD. (a) Consider the loss function f (x, y) = |x| + |y| and a starting point near the x axis. This setting corresponds to Fig. 2. Under the same setting, AdaBelief takes a large step in the x direction, and a small step in the y direction, validating our analysis. More examples such as f (x, y) = |x|/10 + |y| are in the supplementary videos. (b) For an inseparable L1 loss, AdaBelief outperforms other methods under the same setting. (c) For an inseparable L2 loss, AdaBelief outperforms other methods under the same setting. (d) We set β1 = β2 = 0.3 for Adam and AdaBelief, and set momentum as 0.3 in SGD. This corresponds to settings of Eq. 5. For the loss f (x, y) = |x|/10 + |y|, gt is a constant for a large region, hence ||Egt|| (cid:29) Vargt. As mentioned in [8], Emt = (1 − βt)Egt, hence a 1https://www.youtube.com/playlist?list=PL7KkG3n9bER6YmMLrKJ5wocjlvP7aWoOu 4
(a) loss function is f (x, y) = |x| + |y| (b) f (x, y) = |x + y| +
|x − y| / 10 (c) f (x, y) = (x + y)2 + (x − y)2/10 (d) f (x, y) = |x|/10 + |y|
β1 = β2 = 0.3 (e) Trajectory for Beale function in 2D. (f) Trajectory for Beale function in 3D. (g) Trajectory for Rosen-brock function in 2D. (h) Trajectory for Rosen-brock function in 3D.
Figure 3: Trajectories of SGD, Adam and AdaBelief. AdaBelief reaches optimal point (marked as orange cross in 2D plots) the fastest in all cases. We refer readers to video examples. smaller β decreases ||mt − Egt|| faster to 0. Adam behaves like a sign descent (45◦ to the axis), while AdaBelief and SGD update in the direction of the gradient. (e)-(f) Optimization trajectory under default setting for the Beale [24] function in 2D and 3D. (g)-(h) Optimization trajectory under default setting for the Rosenbrock [25] function.
Above cases occur frequently in deep learning Although the above cases are simple, they give hints to local behavior of optimizers in deep learning, and we expect them to occur frequently in deep learning. Hence, we expect AdaBelief to outperform Adam in general cases. Other works in the literature [13, 12] claim advantages over Adam, but are typically substantiated with carefully-constructed examples. Note that most deep networks use ReLU activation [26], which behaves like an absolute value function as in Fig. 3(a). Considering the interaction between neurons, most networks behave like case Fig. 3(b), and typically are ill-conditioned (the weight of some parameters are far larger than others) as in the ﬁgure. Considering a smooth loss function such as cross entropy or a smooth activation, this case is similar to Fig. 3(c). The case with Fig. 3(d) requires
|mt| ≈ |Egt| (cid:29) Vargt, and this typically occurs at the late stages of training, where the learning rate α is decayed to a small value, and the network reaches a stable region. 2.3 Convergence analysis in convex and non-convex optimization
Similar to [13, 12, 27], for simplicity, we omit the de-biasing step (analysis applicable to de-biased version). Proof for convergence in convex and non-convex cases is in the appendix.
Optimization problem For deterministic problems, the problem to be optimized is minθ∈F f (θ); for t=1 ft(θ), where ft can be interpreted as loss of the online optimization, the problem is minθ∈F model with the chosen parameters in the t-th step. (cid:80)T
Theorem 2.1. (Convergence in convex optimization) Let {θt} and {st} be the sequence obtained by
AdaBelief, let 0 ≤ β2 < 1, αt = α√
, β11 = β1, 0 ≤ β1t ≤ β1 < 1, st ≤ st+1, ∀t ∈ [T ]. Let θ ∈ F, t where F ⊂ Rd is a convex feasible set with bounded diameter D∞. Assume f (θ) is a convex function and ||gt||∞ ≤ G∞/2 (hence ||gt − mt||∞ ≤ G∞) and st,i ≥ c > 0, ∀t ∈ [T ], θ ∈ F. Denote the 5
optimal point as θ∗. For θt generated with AdaBelief, we have the following bound on the regret:
√
√
T
D2
∞ 2α(1 − β1) d (cid:88) i=1 s1/2
T,i + (1 + β1)α
√ 2 c(1 − β1)3 1 + log T d (cid:88) i=1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)g2 (cid:12) 1:T,i (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)2
+
D2
∞ 2(1 − β1)
T (cid:88) d (cid:88) t=1 i=1
β1ts1/2 t,i
αt
[ft(θt) − ft(θ∗)] ≤
T (cid:88) t=1
Corollary 2.1.1. Suppose β1,t = β1λt, 0 < λ < 1 in Theorem (2.1), then we have: (cid:80)T t=1[ft(θt) − ft(θ∗)] ≤ D2
∞ 2α(1−β1)
T
√ (cid:80)d i=1 s1/2
T,i + (1+β1)α
√ 2 c(1−β1)3
√ 1+log T (cid:80)d i=1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)g2 1:T,i (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)2
+ D2
∞β1G∞ 2(1−β1)(1−λ)2α
T ).
For the convex case, Theorem 2.1 implies the regret of AdaBelief is upper bounded by O(
T )
Conditions for Corollary 2.1.1 can be relaxed to β1,t = β1/t as in [13], which still generates O( regret. Similar to Theorem 4.1 in [8] and corollary 1 in [13], where the term (cid:80)d
T,i exists, we have (cid:80)d
T,i < dG∞ since ||gt − mt||∞ < G∞ as assumed in Theorem 2.1, and dG∞ is constant. The literature [8, 13, 5] exerts a stronger assumption that (cid:80)d
T . Our assumption could be similar or weaker, because
Est = Vargt ≤ Eg2
Theorem 2.2. (Convergence for non-convex stochastic optimization) Under the assumptions: i=1 t = Evt, then we get better regret than O(
T,i . Without further assumption, (cid:80)d
T,i (cid:28) dG∞ i=1 v1/2 i=1 s1/2 i=1 s1/2
T v1/2
T ).
√
√
√
√
√
• f is differentiable; ||∇f (x) − ∇f (y)|| ≤ L||x − y||, ∀x, y; f is also lower bounded.
• The noisy gradient is unbiased, and has independent noise, i.e. gt = ∇f (θt) + ζt, Eζt = 0, ζt⊥ζj, ∀t, j ∈ N, t (cid:54)= j.
• At step t, the algorithm can access a bounded noisy gradient, and the true gradient is also bounded. i.e. ||∇f (θt)|| ≤ H, ||gt|| ≤ H, ∀t > 1.
Assume minj∈[d](s1)j ≥ c > 0, noise in gradient has bounded variance, Var(gt) = σ2 st+1, ∀t ∈ N, then the proposed algorithm satisﬁes: t ≤ σ2, st ≤ (cid:12) mint∈[T ] E (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)∇f (θt) (cid:12) (cid:12) (cid:12) (cid:12) 2
≤ H√
T α (cid:104) C1α2(H 2+σ2)(1+log T ) c
+ C2 dα√ c + C3 dα2 c + C4 (cid:105) as in [27], C1, C2, C3 are constants independent of d and T , and C4 is a constant independent of T .
Corollary 2.2.1. If c > C1H and assumptions for Theorem 2.2 are satisﬁed, we have: 1
T (cid:80)T t=1 (cid:104)
E
α2 t (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)∇f (θt) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2(cid:105)
≤ 1
T 1
H − C1 1 c (cid:104) C1α2σ2 c (cid:0)1 + log T (cid:1) + C2 dα√ c + C3 dα2 c + C4 (cid:105)
T ),
Theorem 2.2 implies the convergence rate for AdaBelief in the non-convex case is O(log T / which is similar to Adam-type optimizers [13, 27]. Note that regret bounds are derived in the worst possible case, while empirically AdaBelief outperforms Adam mainly because the cases in Sec. 2.2 occur more frequently. It is possible that the above bounds are loose. Also note that we assume st≤st+1, in code this requires to use element wise maximum between st and st+1 in the denominator.
√ 3 Experiments
We performed extensive comparisons with other optimizers, including SGD [3], AdaBound [12],
Yogi [14], Adam [8], MSVAG [15], RAdam [16], Fromage [17] and AdamW [18]. The experiments include: (a) image classiﬁcation on Cifar dataset [28] with VGG [29], ResNet [30] and DenseNet
[31], and image recognition with ResNet on ImageNet [32]; (b) language modeling with LSTM
[33] on Penn TreeBank dataset [34]; (c) wasserstein-GAN (WGAN) [21] on Cifar10 dataset. We emphasize (c) because prior work focuses on convergence and accuracy, yet neglects training stability.
Hyperparameter tuning We performed a careful hyperparameter tuning in experiments. On image classiﬁcation and language modeling we use the following:
• AdaBelief: We use the default parameters of Adam: β1 = 0.9, β2 = 0.999, (cid:15) = 10−8, α = 10−3.
• SGD, Fromage: We set the momentum as 0.9, which is the default for many networks such as
ResNet [30] and DenseNet[31]. We search learning rate among {10.0, 1.0, 0.1, 0.01, 0.001}. 6
(a) VGG11 on Cifar10 (b) ResNet34 on Cifar10 (c) DenseNet121 on Cifar10 (d) VGG11 on Cifar100 (e) ResNet34 on Cifar100 (f) DenseNet121 on Cifar100
Figure 4: Test accuracy ([µ ± σ]) on Cifar. Code modiﬁed from ofﬁcial implementation of AdaBound.
Table 2: Top-1 accuracy of ResNet18 on ImageNet. † is reported in [35], ‡ is reported in [16]
AdaBelief 70.08
SGD 70.23†
AdaBound 68.13†
Yogi 68.23†
Adam 63.79† (66.54‡)
MSVAG RAdam AdamW 67.93† 67.62‡ 65.99
• Adam, Yogi, RAdam, MSVAG, AdaBound: We search for optimal β1 among {0.5, 0.6, 0.7, 0.8, 0.9}, search for α as in SGD, and set other parameters as their own default values in the literature.
• AdamW: We use the same parameter searching scheme as Adam. For other optimizers, we set the weight decay as 5 × 10−4; for AdamW, since the optimal weight decay is typically larger [18], we search weight decay among {10−4, 5 × 10−4, 10−3, 10−2}.
For the training of a GAN, we set β1 = 0.5, (cid:15) = 10−12 for AdaBelief in a small GAN with vanilla
CNN generator, and use (cid:15) = 10−16 for a larger spectral normalization GAN (SN-GAN) with a
ResNet generator; for other methods, we search for β1 among {0.5, 0.6, 0.7, 0.8, 0.9}, and search for (cid:15) among {10−3, 10−5, 10−8, 10−10, 10−12}. We set learning rate as 2 × 10−4 for all methods. Note that the recommended parameters for Adam [36] and for RMSProp [37] are within the search range.
CNNs on image classiﬁcation We experiment with VGG11, ResNet34 and DenseNet121 on
Cifar10 and Cifar100 dataset. We use the ofﬁcial implementation of AdaBound, hence achieved an exact replication of [12]. For each optimizer, we search for the optimal hyperparameters, and report the mean and standard deviation of test-set accuracy (under optimal hyperparameters) for 3 runs with random initialization. As Fig. 4 shows, AdaBelief achieves fast convergence as in adaptive methods such as Adam while achieving better accuracy than SGD and other methods.
We then train a ResNet18 on ImageNet, and report the accuracy on the validation set in Table 2. Due to the heavy computational burden, we could not perform an extensive hyperparameter search; instead, we report the result of AdaBelief with the default parameters of Adam (β1 = 0.9, β2 = 0.999, (cid:15) = 10−8) and decoupled weight decay as in [16, 18]; for other optimizers, we report the best result in the literature. AdaBelief outperforms other adaptive methods and achieves comparable accuracy to
SGD (70.08 v.s. 70.23), which closes the generalization gap between adaptive methods and SGD.
Experiments validate the fast convergence and good generalization performance of AdaBelief. 7
Figure 5: Left to right: perplexity ([µ ± σ]) on Penn Treebank for 1,2,3-layer LSTM. Lower is better. (a) FID score of WGAN. (b) FID score of WGAN-GP.
Figure 6: FID score of WGAN and WGAN-GP using a vanilla CNN generator on Cifar10. Lower is better. For each model, successful and failed optimizers are shown in the left and right respectively, with different ranges in y value.
LSTM on language modeling We experiment with LSTM on the Penn TreeBank dataset [34], and report the perplexity (lower is better) on the test set in Fig. 5. We report the mean and standard deviation across 3 runs. For both 2-layer and 3-layer LSTM models, AdaBelief achieves the lowest perplexity, validating its fast convergence as in adaptive methods and good accuracy. For the 1-layer model, the performance of AdaBelief is close to other optimizers.
Generative adversarial networks Stability of optimizers is important in practice such as training of GANs, yet recently proposed optimizers often lack experimental validations. The training of a
GAN alternates between generator and discriminator in a mini-max game, and is typically unstable
[20]; SGD often generates mode collapse, and adaptive methods such as Adam and RMSProp are recommended in practice [38, 37, 39]. Therefore, training of GANs is a good test for the stability.
We experiment with one of the most widely used models, the Wasserstein-GAN (WGAN) [21] and the improved version with gradient penalty (WGAN-GP) [37] using a small model with vanilla CNN generator. Using each optimizer, we train the model for 100 epochs, generate 64,000 fake images from noise, and compute the Frechet Inception Distance (FID) [40] between the fake images and real dataset (60,000 real images). FID score captures both the quality and diversity of generated images and is widely used to assess generative models (lower FID is better). For each optimizer, under its optimal hyperparameter settings, we perform 5 runs of experiments, and report the results in Fig. 6 and Fig. 7. AdaBelief signiﬁcantly outperforms other optimizers, and achieves the lowest FID score.
Besides the small model above, we also experiment with a large model using a ResNet generator and spectral normalization in the discriminator (SN-GAN). Results are summarized in Table. 3.
Compared with a vanilla GAN, all FID scores are lower because the SN-GAN is more advanced.
Compared with other optimizers, AdaBelief achieves the lowest FID with both large and small GANs.
Remarks Recent research on optimizers tries to combine the fast convergence of adaptive methods with high accuracy of SGD. AdaBound [12] achieves this goal on Cifar, yet its performance on
ImageNet is still inferior to SGD [35]. Padam [35] closes this generalization gap on ImageNet; writing the update as θt+1 = θt − αmt/vp t , SGD sets p = 0, Adam sets p = 0.5, and Padam searches p between 0 and 0.5 (outside this region Padam diverges [35, 41]). Intuitively, compared to Adam, by using a smaller p, Padam sacriﬁces the adaptivity for better generalization as in SGD; however, without good adaptivity, Padam loses training stability. As in Table 4, compared with
Padam, AdaBelief achieves a much lower FID score in the training of GAN, meanwhile achieving 8
Figure 7: Left to right: real images, samples from WGAN, WGAN-GP (both trained by AdaBelief).
AdaBelief 12.52 ± 0.16
Table 3: FID (lower is better) of a SN-GAN with ResNet generator on Cifar10.
MSVAG 48.35 ± 5.44
SGD 49.70 ± 0.41
Adam 13.05 ± 0.19
Yogi 14.25 ± 0.15
Fromage 42.75 ± 0.15
RMSProp 13.13 ± 0.12
RAdam 12.70 ± 0.12
AdaBound 55.65 ± 2.15
Table 4: Comparison of AdaBelief and Padam. Higher Acc (lower FID) is better. ‡ is from [35].
ImageNet Acc
FID (WGAN)
FID (WGAN-GP)
AdaBelief 70.08 83.0± 4.1 61.8± 7.7 p=1/2 (Adam) 63.79‡ 96.6±4.5 73.5±8.7 p=2/5
-97.5±2.8 87.1±6.0 p=1/4
-426.4±49.6 155.1±23.8
Padam p=1/5
-401.5±33.2 167.3±27.6 p=1/8 70.07‡ 328.1±37.2 203.6±18.9 p=1/16
-362.6±43.9 228.5±25.8 p = 0 (SGD) 70.23 ‡ 469.3 ± 7.9 244.3± 27.4 slightly higher accuracy on ImageNet classiﬁcation. Furthermore, AdaBelief has the same number of parameters as Adam, while Padam has one more parameter hence is harder to tune.
Extra experiments We conducted extra experiments, including Transformer models, reinforcement learning and object detection, and AdaBelief achieved better results than other optimizers. For details and code to reproduce results, please refer to our github page. 4