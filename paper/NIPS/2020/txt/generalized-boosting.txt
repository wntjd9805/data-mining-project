Abstract
Boosting is a widely used learning technique in machine learning for solving classiﬁcation problems. In boosting, one predicts the label of an example using an ensemble of weak classiﬁers. While boosting has shown tremendous success on many classiﬁcation problems involving tabular data, it performs poorly on complex classiﬁcation tasks involving low-level features such as image classiﬁcation tasks.
This drawback stems from the fact that boosting builds an additive model of weak classiﬁers, each of which has very little predictive power. Often, the resulting additive models are not powerful enough to approximate the complex decision boundaries of real-world classiﬁcation problems. In this work, we present a general framework for boosting where, similar to traditional boosting, we aim to boost the performance of a weak learner and transform it into a strong learner. However, unlike traditional boosting, our framework allows for more complex forms of aggregation of weak learners. In this work, we speciﬁcally focus on one form of aggregation - function composition. We show that many popular greedy algorithms for learning deep neural networks (DNNs) can be derived from our framework using function compositions for aggregation. Moreover, we identify the drawbacks of these greedy algorithms and propose new algorithms that ﬁx these issues. Using thorough empirical evaluation, we show that our learning algorithms have superior performance over traditional additive boosting algorithms, as well as existing greedy learning techniques for DNNs. An important feature of our algorithms is that they come with strong theoretical guarantees. 1

Introduction
Boosting is a widely used learning technique in machine learning for solving classiﬁcation problems.
Boosting aims to improve the performance of a weak learner by combining multiple weak classiﬁers to produce a strong classiﬁer with good predictive performance. Since the seminal works of Freund
[13], Schapire [32], a number of practical algorithms such as AdaBoost [16], gradient boosting [26],
XGBoost [9], have been proposed for boosting. Over the years, boosting based methods such as
XGBoost in particular, have shown tremendous success in many real-world classiﬁcation problems, as well as competitive settings such as Kaggle competitions. However, this success is mostly limited to classiﬁcation tasks involving structured or tabular data with hand-engineered features.
On classiﬁcation problems involving low-level features and complex decision boundaries, boosting tends to perform poorly [3, 30] (also see Section 5). One example where this is evident is the image classiﬁcation task, where the decision boundaries are often complex and the features are low-level pixel intensities. This drawback stems from the fact that boosting builds an additive model of weak classiﬁers, each of which has very little predictive power. Since such additive models with any reasonable number of weak classiﬁers are usually not powerful enough to approximate complex decision boundaries, the models’ output by boosting tend to have poor performance.
In this work, we aim to overcome this drawback of traditional boosting by considering a generalization of boosting which allows for more complex forms of aggregation than linear combinations of weak classiﬁers. To achieve this goal, we work in the feature representation space and boost the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
ř
T t“0 gt, and tgtuT performance of weak feature transformers. Working in the representation space allows for more
ﬂexible combinations of weak feature transformers. This is unlike traditional boosting which works in the label space and builds an additive model on the predictions of the weak classiﬁers. The starting point for our approach is the greedy view of boosting, originally studied by Friedman et al.
[18], Mason et al. [26]. Letting pRSpf q be the risk of a classiﬁer f on training samples S, boosting techniques aim to approximate the minimizer of pRS in terms of linear combinations of elements from a set of weak classiﬁers F. Many popular boosting algorithms including AdaBoost, XGBoost, rely on greedy techniques to ﬁnd such an approximation. In our generalized framework for boosting, we take this greedy view, but differ in how we aggregate the weak learners. We approximate the minimizer of pRS using models of the form fT “ W φT , where φT “ t“0 are feature transformations learned in each iteration of the greedy algorithm, and W is the linear classiﬁer on top of the feature transformation. Unlike additive boosting, where each gt comes from a ﬁxed weak feature transformer class G, in our framework each gt comes from a class Gt which evolves over time t and is allowed to depend on the past iterates tφiut´1 i“0. Some potential choices for Gt that could be of interest are tg ˝ φt´1 for g P Gu, tg ˝ prφ0, . . . , φt´1sq for g P Gu, where g ˝ φpxq “ gpφpxqq denotes function composition of g and φ, and G is a weak feature transformer class. Note that the former choice of Gt is connected to layer-by-layer training of models with ResNet architecture [21].
As one particular instantiation of our framework, we consider weak feature transformers that are neural networks and use function compositions to combine them; that is, we use Gt’s constructed using function compositions. We show that for certain choices of Gt, our framework recovers the layer-by-layer training techniques developed in deep learning [6, 22]. Greedy layer-by-layer training techniques have seen a revival in recent years [5, 8, 22, 25, 29]. One reason for this revival is that greedy techniques consume less memory than end-to-end training of deep networks, as they do not perform end-to-end back-propagation. Consequently, they can accommodate much larger models in limited memory. As a primary contribution of the paper, we identify several drawbacks of existing layer-by-layer training techniques, and show that the choice of Gt used by these algorithms can lead to a drop in performance. We propose alternative choices for Gt which ﬁx these issues and empirically demonstrate that the resulting algorithms have superior performance over existing layer-by-layer training techniques, and in some cases achieve performance close to that of end-to-end trained DNNs. Moreover, we show that the proposed algorithms perform much better than traditional additive boosting algorithms, on a variety of classiﬁcation tasks.
As the second contribution of the paper, we provide excess risk bounds for models learned using our generalized boosting framework. Our results depend on a certain weak learning condition on feature transformer classes tGtuT t“1, which is a natural generalization of the weak learning condition that is typically imposed in traditional boosting. The resulting risk bounds are modular and depend on the generalization bounds of tGtuT t“1. An advantage of such modular bounds is that one can rely on the best-known generalization bounds for weak transformation classes tGtuT t“1 and obtain tight risk bounds for boosting. As an immediate consequence of this result, we obtain excess risk bounds for existing greedy layer-by-layer training techniques.