Abstract
People easily recognize new visual categories that are new combinations of known components. This compositional generalization capacity is critical for learning in real-world domains like vision and language because the long tail of new com-binations dominates the distribution. Unfortunately, learning systems struggle with compositional generalization because they often build on features that are correlated with class labels even if they are not “essential” for the class. This leads to consistent misclassiﬁcation of samples from a new distribution, like new combinations of known components.
Here we describe an approach for compositional generalization that builds on causal ideas. First, we describe compositional zero-shot learning from a causal perspec-tive, and propose to view zero-shot inference as ﬁnding “which intervention caused the image?”. Second, we present a causal-inspired embedding model that learns disentangled representations of elementary components of visual objects from correlated (confounded) training data. We evaluate this approach on two datasets for predicting new combinations of attribute-object pairs: A well-controlled synthe-sized images dataset and a real-world dataset which consists of ﬁne-grained types of shoes. We show improvements compared to strong baselines. Code and data are provided in https://github.com/nv-research-israel/causal_comp 1

Introduction
Compositional zero-shot recognition is the problem of learning to recognize new combinations of known components. People seamlessly recognize and generate new compositions from known elements and Compositional Reasoning is considered a hallmark of human intelligence [33, 34, 6, 4].
As a simple example, people can recognize a purple cauliﬂower even if they have never seen one, based on their familiarity with cauliﬂowers and with other purple objects (Figure 1b). Unfortunately, although feature compositionality is a key design consideration of deep networks, current deep models struggle when required to generalize to new label compositions. This limitation has grave implications for machine learning because the heavy tail of unfamiliar compositions dominates the distribution of labels in perception, language, and decision-making problems.
Models trained from data tend to fail with compositional generalization for two fundamental reasons: distribution-shift and entanglement. First, recognizing new combinations is an extreme case of distribution-shift inference, where label combinations at test time were never observed during training (zero-shot learning). As a result, models learn correlations during training that hurt inference at test time. For instance, if all cauliﬂowers in the training set are white, the correlation between the color and the class label is predictive and useful. A correlation-based model like (most) deep networks will learn to associate cauliﬂowers with the color white during training, and may fail when presented with a purple cauliﬂower at test time. For the current scope, we put aside the fundamental 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b) (c)
.
Figure 1: (a) The causal graph that generates an image. The solid arrows represent the real-world processes by which the two categorical variables “Object” and “Attribute” each generate “core features” [21, 17] φo and φa. The core features then jointly generate an image feature vector x.
The core features are assumed to be stable for unseen combinations of objects and attributes. The dotted double-edged arrows between the Object and Attribute nodes indicates that there is a process
“confounding” the two: they are not independent of each other. (b) An intervention that generates a test image of a purple-cauliﬂower, by enforcing a = purple and o = caulif lower. It cuts the confounding link between the two nodes [49] and changes the joint distribution of the nodes to the
“interventional distribution”. (c) Illustration of the learned mappings, detailed in Section 4 semantic question about what deﬁnes the class of an object (cauliﬂower), and assume that it is given or determined by human observers.
The second challenge is that the training samples themselves are often labeled in a compositional way, and disentangling their “elementary” components from examples is often an ill-deﬁned problem
[38]. For example, for an image labeled as white cauliﬂower, it is hard to tell which visual features capture being a cauliﬂower, and which, being white. In models that learn from data the representation of these terms may be inherently entangled, and it would be hard to separate which visual features represent white and which represent a cauliﬂower.
These two challenges are encountered when learning deep discriminative models from data. For example, consider a simple model that learns the concept “cauliﬂower”, by training a deep model over all cauliﬂower images (VisProd [44]), and the same for the concept "white". At inference time, simply select the most likely attribute ˆa = arg maxa p(a|x) and, independently, the most likely object ˆo = arg maxo p(o|x). Unfortunately, this model, while quite powerful, tends to be sensitive to training-speciﬁc correlations in its input.
Here we propose to address compositional recognition by modelling images as being generated, or caused, by real-world entities (labels) (Figure 1). This model recognizes that the distribution p(Image=x|Attr=a, Obj =o) is more likely to be stable across the train and test environments (cid:0)ptest(x|a, o) = ptrn(x|a, o)(cid:1) [54, 49, 57]: it means that unlike objects or attributes by themselves, combinations of objects and attributes generate the same distribution over images in train and test.
We propose to consider images of unseen combinations as generated by interventions on the attribute and object labels. In causal inference, intervention means that the value of a random variable is forced to some value, without affecting its causes (but affecting other variables that depend on it, Figure 1b).
We cast zero-shot inference as the problem of ﬁnding which intervention caused a given image.
In the general case, the conditional distribution p(x|a, o) can have arbitrary an complex structure and may be hard to learn. We explain how treating labels as causes, rather than as effects of the image, reveals an independence structure that makes it easier to learn p(x|a, o). We propose conditional independence constraints applied to the structure of this distribution and show how the model can be learned effectively from data.
The paper makes the following novel contributions: First, we provide a new formulation of com-positional zero-shot recognition using a causal perspective. Speciﬁcally, we formalize inference as a problem of ﬁnding the most likely intervention. Second, we describe a new embedding-based architecture that infers causally stable representations for compositional recognition. Finally, we demonstrate empirically that in two challenging datasets, our architecture better recognizes new unseen attribute-object compositions compared to previous methods. 2
2