Abstract
We introduce a simple (one line of code) modiﬁcation to the Generative Adversarial
Network (GAN) training algorithm that materially improves results with no increase in computational cost: When updating the generator parameters, we simply zero out the gradient contributions from the elements of the batch that the critic scores as ‘least realistic’. Through experiments on many different GAN variants, we show that this ‘top-k update’ procedure is a generally applicable improvement. In order to understand the nature of the improvement, we conduct extensive analysis on a simple mixture-of-Gaussians dataset and discover several interesting phenomena.
Among these is that, when gradient updates are computed using the worst-scoring batch elements, samples can actually be pushed further away from their nearest mode. We also apply our method to recent GAN variants and improve state-of-the-art FID for conditional generation from 9.21 to 8.57 on CIFAR-10. 1

Introduction
Generative Adversarial Networks (GANs) [15] have been successfully used for image synthesis
[32, 47, 4], audio synthesis [10, 11], domain adaptation [53, 48], and other applications [45, 27, 50].
It is well known that GANs are difﬁcult to train, and much research focuses on ways to modify the training procedure to reduce this difﬁculty. Since the generator parameters are updated by performing gradient descent through the critic, much of this work focuses on modifying the critic in some way
[1, 30, 33, 16] so that the gradients the generator gets will be more ‘useful’. What ‘usefulness’ means is generally somewhat ill-deﬁned, but we can deﬁne it implicitly and say that useful gradients are those which result in the generator learning a better model of the target distribution.
Recent work by [44] suggests that gradients can be more useful when computed on samples closer to the data-manifold – that is, if we tend to update the generator and critic weights using samples that are more realistic, the generator will tend to output more realistic samples. [44] achieves state-of-the-art results on the ImageNet conditional image synthesis task by generating samples from the generator, computing the gradient of the critic with respect to the sampled prior that generated those samples, updating that sampled prior in the direction of that gradient, and then ﬁnally updating the generator parameters using this new draw from the prior. In short, they update the generator and critic parameters using a z0 such that the critic thinks G(z0) is ‘more realistic’ than G(z). However, this procedure is complicated and computationally expensive: it requires twice as many operations per gradient update. In this work, we demonstrate that similar improvements can be achieved with a much simpler technique: we propose to simply zero out the gradient contributions from the elements of the batch that the critic scores as ‘least realistic’.
⇤Samarth Sinha and Zhengli Zhao contributed equally as joint ﬁrst authors. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Diagram of top-k training of a GAN. The generator generates a batch of samples, which are scored by the critic. Only the k samples with the highest scores are used to update the generator.
Why should this help? In an idealized GAN, the trained critic would slowly lose its ability to tell which inputs were samples from the generator and which inputs were elements of the target distribution, but in practice this doesn’t happen. [2] show that a trained critic can actually be used to perform rejection sampling on a trained generator and signiﬁcantly improve the performance of the trained generator. Thus, as training progresses, the critic can serve as a useful arbiter of which samples are ‘good’. Then, if we accept the premise that updating on ‘good’ samples improves GAN training, we should be able to use the critic during training to make decisions about which samples to update on. But why should we accept this premise? Why would updating on the ‘bad’ samples hurt instead of helping? In this work, we provide a partial answer by showing that in practice, gradient updates derived from samples the critic deems ‘bad’ can actually point away from the true data manifold.
Since the critic’s ability to tell us which samples are bad improves during training, we anneal the fraction of the batch that is used for updates as training progresses. In the beginning of training, we use samples from the entire batch, and gradually reduce k after each training epoch.
Our contributions can be summarized as follows:
•
•
•
We propose a simple ‘one-line’ modiﬁcation to the standard GAN training algorithm that only updates the generator parameters on the samples from the mini-batch that the critic scores as most realistic.
We thoroughly study (on a ‘toy’ dataset) the mechanism by which our proposed method improves performance and discover that gradients computed on discarded samples would point in the ‘wrong’ direction.
We conduct further experiments on the CIFAR [23] and ImageNet [40] datasets and show that our proposed modiﬁcation can signiﬁcantly improve FID [18] numbers for several popular GAN variants. 2