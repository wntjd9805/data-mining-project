Abstract
We study the problem of online learning with primary and secondary losses. For example, a recruiter making decisions of which job applicants to hire might weigh false positives and false negatives equally (the primary loss) but the applicants might weigh false negatives much higher (the secondary loss). We consider the following question: Can we combine “expert advice” to achieve low regret with respect to the primary loss, while at the same time performing not much worse than the worst expert with respect to the secondary loss? Unfortunately, we show that this goal is unachievable without any bounded variance assumption on the secondary loss. More generally, we consider the goal of minimizing the regret with respect to the primary loss and bounding the secondary loss by a linear threshold.
On the positive side, we show that running any switching-limited algorithm can achieve this goal if all experts satisfy the assumption that the secondary loss does not exceed the linear threshold by o(T ) for any time interval. If not all experts satisfy this assumption, our algorithms can achieve this goal given access to some external oracles which determine when to deactivate and reactivate experts. 1

Introduction
The online learning problem has been studied extensively in the literature and used increasingly in many applications including hiring, advertising and recommender systems. One classical problem in online learning is prediction with expert advice, in which a decision maker makes a sequence of T decisions with access to K strategies (also called “experts”). At each time step, the decision maker observes a scalar-valued loss of each expert. The standard objective is to perform as well as the best expert in hindsight. For example, a recruiter (the decision maker) sequentially decides which job applicants to hire with the objective of minimizing errors (of hiring an unqualiﬁed applicant and rejecting a qualiﬁed one). However, this may give rise to some social concerns since the decision receiver has a different objective (getting a job) which does not receive any attention. This problem can be modeled as an online learning problem with the primary loss (for the decision maker) and secondary loss (for the decision receiver). Taking the social impact into consideration, we ask the following question:
Can we achieve low regret with respect to the primary loss, while performing not much worse than the worst expert with respect to the secondary loss?
Unfortunately, we answer this question negatively. More generally, we consider a bicriteria goal of minimizing the regret to the best expert with respect to the primary loss while minimizing the regret to a linear threshold cT with respect to the secondary loss for some c. When the value of c is set to the average secondary loss of the worst expert with respect to the secondary loss, the objective reduces to no-regret for the primary loss while performing no worse than the worst expert with respect to the secondary loss. Other examples, e.g., the average secondary loss of the worst expert with respect to the secondary loss among the experts with optimal primary loss, lead to different criteria of the secondary loss. Therefore, with the notion of regret to the linear threshold, we are able to study a more general goal. Based on this goal, we pose the following two questions: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1. If all experts have secondary losses no greater than cT + o(T ) for some c, can we achieve no-regret (compete comparably to the best expert) for the primary loss while achieving secondary loss no worse than cT + o(T )? 2. If we are given some external oracles to deactivate some “bad” experts with unsatisfactory secondary loss, can we perform as well as each expert with respect to the primary loss during the time they are active while achieving secondary loss no worse than cT + o(T )?
These two questions are trivial in the i.i.d. setting as we can learn the best expert with respect to the primary loss within O(log(T )) rounds and then we just need to follow the best expert. In this paper, we focus on answering these two questions in the adversarial online setting. 1.1 Contributions
An impossibility result without a bounded variance assumption We show that without any constraints on the variance of the secondary loss, even if all experts have secondary loss no greater than cT , achieving no-regret with respect to the primary loss and bounding secondary loss by cT + O(T ) is still unachievable. This answers our motivation question that it is impossible to achieve low regret with respect to the primary loss, while performing not much worse than the worst expert with respect to the secondary loss. This result explains why minimizing one loss while bounding another is non-trivial and applying existing algorithms for scalar-valued losses after scalarizing primary and secondary losses does not work. We propose an assumption on experts that the secondary loss of the expert during any time interval does not exceed cT by O(T ↵) for some ↵
[0, 1).
Then we study the problem in two scenarios, a “good” one in which all experts satisfy this assumption and a “bad” one in which experts partially satisfy this assumption and we are given access to an external oracle to deactivate and reactivate experts. 2
In the “good” scenario, we show that running an algorithm
Our results in the “good” scenario with limited switching rounds such as Follow the Lazy Leader [Kalai and Vempala, 2005] and
Shrinking Dartboard (SD) [Geulen et al., 2010] can achieve both regret to the best with respect to 2 ). We also provide a the primary loss and regret to cT with respect to the secondary loss at O(T lower bound of ⌦(T ↵). 1+↵
From another perspective, we relax the “good” scenario constraint by introducing adaptiveness to the secondary loss and constraining the variance of the secondary loss between any two switchings for any algorithm instead of that of any expert. We show that in this weaker version of “good” scenario, the upper bound of running switching-limited algorithms matches the lower bound at ⇥(T 2 ). 1+↵
Our results in the “bad” scenario In the “bad” scenario, we assume that we are given an external oracle to determine which experts to deactivate as they do not satisfy the bounded variance assumption.
We study two oracles here. One oracle deactivates the experts which do not satisfy the bounded variance assumption once detecting and never reactivates them. The other one reactivates those inactive experts at ﬁxed rounds. In this framework, we are limited to select among the active experts at each round and we adopt a more general metric, sleeping regret, to measure the performance of the primary loss. We provide algorithms for the two oracles with theoretical guarantees on the sleeping regrets with respect to the primary loss and the regret to cT with respect to the secondary loss. 1.2