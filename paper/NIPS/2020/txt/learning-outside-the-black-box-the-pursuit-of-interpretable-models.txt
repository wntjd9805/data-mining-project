Abstract
Machine Learning has proved its ability to produce accurate models – but the deployment of these models outside the machine learning community has been hindered by the difﬁculties of interpreting these models. This paper proposes an algorithm that produces a continuous global interpretation of any given continuous black-box function. Our algorithm employs a variation of projection pursuit in which the ridge functions are chosen to be Meijer G-functions, rather than the usual polynomial splines. Because Meijer G-functions are differentiable in their parameters, we can “tune” the parameters of the representation by gradient descent; as a consequence, our algorithm is efﬁcient. Using ﬁve familiar data sets from the
UCI repository and two familiar machine learning algorithms, we demonstrate that our algorithm produces global interpretations that are both highly accurate and parsimonious (involve a small number of terms). Our interpretations permit easy understanding of the relative importance of features and feature interactions. Our interpretation algorithm represents a leap forward from the previous state of the art. 1

Introduction
What do we need to trust the predictions of the black-box models crafted by our Machine Learn-ing (ML) algorithms? Although the ML community has succeeded in generating accurate models in a very wide variety of settings, it has not yet succeeded in convincing most practitioners to adopt these models. One possible reason is that many practitioners will use familiar models over more accurate models that they do not understand. For a striking example, we cite the area of medical risk prediction, in which clinical models have remained the standard despite the fact that ML models are demonstrably more accurate. The explicitly stated reason for this is that an acceptable model must be both accurate and transparent [24]; state-of-the-art clinical models are transparent while state-of-the-art ML models are not. In responding to this challenge, one possibility would be to focus on the design of ML models that are themselves transparent; an alternative possibility is to make a given ML model more transparent by providing an interpretation of that model.
As detailed in Section 5, a substantial literature in the ML community follows the latter ap-proach, but not entirely successfully. To understand the challenge that we are addressing here, it is instructive to make a simple thought experiment. Let us assume that the data follows a
Cox proportional hazards model [10]. For such a model, the hazard rate at time t is given by 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(cid:16)(cid:80)d i=1 bixi (cid:1), where h0(t) is a baseline hazard, which potentially
H(t | x1, . . . , xd) = h0(t) exp (cid:0) (cid:80)d depends on time, x1, . . . , xd are the features and the coefﬁcients b1, . . . , bd ∈ R represent the im-portance of each feature. Suppose that we are interested in identifying the part of this model which only depends on the features. This part is trivially given by f (x) = exp
. We shall here consider this as the “black-box” that we would like to identify by using our interpretability method.
If our interpretability method produces linear models, as it is the case for LIME [37], we could obtain an estimator ˆf (x) = 1 + (cid:80)d i=1 bixi for f . On the other hand, a polynomial estimator [2] for f would be of the form ˆf (x) = 1 + (cid:80)d i=1 bixi)2/2! + ﬁnite number of terms. It goes without saying that neither of those interpretable models captures an exponential relationship globally, hence precluding a global interpretation of the black-box model. Would it be possible to overcome these limitations of the state-of-the-art algorithms for interpretability? More precisely, is there a way to build a regressor allowing to capture a large class of interpretable functions globally? Needless to say that a positive answer to these questions would have a tremendous impact on the interpretability landscape in Machine Learning. To tackle this problem, we build a new approach on top of the two following corner stones. i=1 bixi/1! + ((cid:80)d i=1 bixi (cid:17)
Projection Pursuit. The projection pursuit algorithm is widely known in the statistics literature [13, 20, 22, 39]. Projection pursuit builds an approximation to a given function by proceeding in stages. In each stage, the algorithm ﬁnds a direction in the feature space and a ridge function of that direction that best approximate the residual between the given function and the approximation constructed in the previous state. The process continues until the desired degree of accuracy is achieved. In projection pursuit, the ridge functions are typically polynomial splines [18]. This reliance on polynomial splines means that prediction pursuit suffers from the very shortcoming noted above: it permits only local interpretations and hence does not suggest any global structure. For this reason, we should work with another family of functions.
Meijer G-functions. Instead of polynomial splines, we use functions from the class of Meijer G-functions. This class of functions includes all of the familiar functions used in modeling (polynomial, exponential, logarithmic, trigonometric and hypergeometric functions). A Meijer G-function is deﬁned by four non-negative integer hyperparameters and an array of real parameters. An interesting property of these functions is that we can efﬁciently compute a numerical gradient of the Meijer
G-functions with respect to these parameters. This allows to use gradient-based optimization [1].
Contribution. In this paper, we introduce a new algorithm called Symbolic Pursuit that produces an interpretable model for a given black-box. As in a projection pursuit, our algorithm gradually adds more terms in the model until the desired precision is achieved. In our analysis, we address a major dif-ﬁculty introduced by using G-functions as ridge functions : the hyperparameters. As aforementioned, each G-function has four hyperparameters to tune. Fortunately, we show that there exist a set of ﬁve hyperparameter conﬁgurations that covers most familiar functions (polynomial, rational, exponential, trigonometric and hypergeometric functions). By restricting to these conﬁgurations, each G-function is optimized efﬁciently over a sufﬁciently large class of functions. Consequently, we demonstrate that our Symbolic Pursuit algorithm allows to produce highly accurate global models (that we call symbolic models) for black-boxes with a small number of G-functions. With our algorithm, one to
ﬁve G-functions are enough to approximate a black-box model ﬁtting a real world dataset. This is a leap forward from the previous state-of-the-art.
To make the paper self-contained, we start with a short presentation of the projection pursuit algorithm and Meijer G-functions in Section 2. We address the hyperparameter optimization problem of Meijer
G-functions in Section 3. After these theoretical considerations, we assemble all the pieces to construct the Symbolic Pursuit algorithm in Section 4. We compare this algorithm to the state-of-the-art interpretability methods in Section 5. Finally, we demonstrate that our algorithm produces globally symbolic models with outstanding approximation power and parsimonious expressions on
ﬁve real-world datasets in Section 6. 2 Mathematical preliminaries
We begin by recalling the projection pursuit algorithm, which we shall reshape for our purpose in
Section 4. Then we review the deﬁnition of Meijer G-functions that we use as the building blocks of the Symbolic Pursuit algorithm. 2
2.1 Projection Pursuit
Let X = [0, 1]d be a normalized feature space, Y ⊆ R be a label space and f : X → Y be a black-box model (e.g. a neural network). Because we view f as a black-box, we do not know the true form of f , but we assume we can evaluate f (x) for every x ∈ X ; i.e. we can query the black-box.
The projection pursuit algorithm [13] constructs an explicit model ˆf for f of the form
K (cid:88)
ˆf (x) = gk (cid:0)v(cid:62) k x(cid:1) , (1) k=1 where vk ∈ Rd is a vector (onto which we project the feature vector x ∈ X ) and each gk is a function k x(cid:1) are belonging to a speciﬁed class F of univariate functions. Because the functions x (cid:55)→ gk constant on the hyperplanes normal to vk, they are usually called ridge functions.
The approximation is built following an iterative process. At stage j ∈ N∗, we suppose that the approximation ˆfj of the above form, but having only j − 1 terms, has been constructed; i.e., k x(cid:1)1. We deﬁne the residual rj = f − ˆfj and ﬁnd a vector vj and a function
ˆfj(x) = (cid:80)j−1 gj ∈ F to minimize the L2 norm of the updated residual: k=1 gk (cid:0)v(cid:62) (cid:0)v(cid:62) (gj, vj) = arg min
F ×Rd (cid:90)
X (cid:2)rj(x) − g (cid:0)v(cid:62)x(cid:1)(cid:3)2 dF (x) (2) where F is the cumulative distribution function on the feature space. This process is continued until the L2 norm of the residual is smaller than a predetermined threshold. This algorithm is typically complemented with a back-ﬁtting strategy in practice [17].
In principle, projection pursuit would seem an excellent approach to interpretability because each stage provides one term in the approximation and the process can be terminated at any stage.
Hence projection pursuit allows a trade-off between the accuracy of the representation ˆf and the complexity of ˆf . In practice, however, the class F is usually taken to be the class of polynomial splines – often of low degree (e.g. cubic) [20]. This has the advantage of making each stage of the algorithm computationally tractable, but the disadvantage of often leading to representations that involve many terms. Moreover, often even the individual terms in these representations do not have natural interpretations. These considerations suggest looking for a different class of functions that retain computational tractability while leading to representations that are more parsimonious and interpretable. 2.2 Meijer G-functions
Is there a set of functions F that includes most interpretable functions and that would allow solving the optimization problem (2) with standard techniques? The answer to this question is yes. The set of
Meijer G-function, that we denote G, fulﬁls these two requirements, as discussed in [1]. We shall thus henceforth restrict our investigations to F = G. Here, we brieﬂy recall the deﬁnition of a Meijer
G-function [5].
Deﬁnition 2.1 (Meijer G-function [5]). A Meijer G-function is deﬁned by an integral along a path L in the complex plane,
Gm,n p,q (cid:18) a1, . . . , ap b1, . . . , bq (cid:19) z
= 1 2πi (cid:90)
L zs j=1 Γ(bj − s) (cid:81)n (cid:81)m j=m+1 Γ(1 − bj + s) (cid:81)p (cid:81)q j=1 Γ(1 − aj + s) j=n+1 Γ(aj − s) ds, (3) where ai, bj ∈ R ; ∀i = 1, . . . , p ; j = 1, . . . , q ; m, n, p, q ∈ N with m ≤ q , n ≤ p and Γ is
Euler’s Gamma function. The path of integration L is chosen so that the poles associated to the two families of Gamma functions (one family for the a’s and one family for the b’s) lie on different sides of L.2 The deﬁnition yields a complex-analytic function in the entire complex plane C with the possible exception of the origin z = 0 and the unit circle {z ∈ C : |z| = 1}. We will only consider its behavior for real z in the open unit interval (0, 1) ⊂ R. We write G for the class of all Meijer
G-functions. 1If j = 1 then ˆf1 ≡ 0. 2We omit some technical details and restrictions; see [5] for details. 3
This set of function is interesting because it includes most familiar functions such as exponential, trigonometric and hypergeometric functions as particular cases [31]. For example, the negative exponential function is exp(−x) = G1,0 0,1 (cid:18) — 0 (cid:19) x
.
Furthermore, we can compute a numerical gradient of these function with respect to the real parame-ters ai, bj, hence allowing the use of gradient-based optimization. In the following, it will be useful to denote Gm,n the set of Meijer G-functions of hyperparameters m, n, p, q. The full set of Meijer p,q
G-function can thus be decomposed as
G = (cid:91) p (cid:91) q (cid:91) p,q∈N n=0 m=0
Gm,n p,q . (4)
It should already be obvious at this stage that we won’t be able to search in all of these subsets of G so that m, n, p, q will have to be ﬁxed as hyperparameters. How much are we restricting the possibilities by doing so? Is there a clever restriction that would include most familiar functions? We will elaborate on these questions in Section 3. 3 Hyperparameters climbing down the trees
In this section, we address a major theoretical challenge related to the use of Meijer G-functions: the number of hyperparameters. More precisely, we have 4 hyperparameters (m, n, p, q) ∈ N4 to tune for each Meijer G-function appearing in the expansion (1). It should be obvious that we cannot search across every subclass Gm,n p.q . Fortunately, this is not necessary; we can choose a set
H of 4-tuples of hyperparameters that is large enough that the subclasses Gm,n p.q ; (m, n, p, q) ∈ H encompass sufﬁciently many functions but small enough that searching within and across these subclasses is computationally tractable. Our choice of H relies on two following results.
First, we note that the subsets Gm,n not disjointed. Indeed, we show that the following result holds:
Lemma 3.1. For all (m, n, p, q) ∈ N4 such that p, q ≥ 1: p,q of G associated with different values of the hyperparameters are
• If m ≥ 1 : Gm−1,n p−1,q−1 ⊂ Gm,n p,q
• If n ≥ 1 : Gm,n−1 p−1,q−1 ⊂ Gm,n p,q
Proof. A detailed proof can be found in Section 1 of the supplementary material.
This important lemma tells that, during the optimization, we can explore different values of (m, n, p, q) than the one initially ﬁxed. To illustrate, consider the 4-tuple (m, n, p, q) = (1, 1, 2, 2). Lemma 3.1 implies that G1,1 2,2 1,1 and G1,0 contains both G0,1 1,1 , and that each of these in turn contains G0,0 0,0 , as in Figure 1. Therefore, starting with (m, n, p, q) = (1, 1, 2, 2) also allows to explore (m, n, p, q) = (0, 1, 1, 1), (1, 0, 1, 1), (0, 0, 0, 0) at the same time. This concept can be generalized so that the subsets of G can be represented in terms of trees of inclu-sion, such as the one from Figure 1. Building on this idea of trees of inclusion, we propose to choose a clever ﬁnite set of conﬁgurations for the hyperparameters which allows to recover most closed form expressions.
Figure 1: The tree of inclusions starting from G1,1 2,2 .
Proposition 3.1. Consider the set of Meijer G-functions of the form
ˆf (z) = Gm,n p,q (cid:18) a1, . . . , ap b1, . . . , bq s · zr (cid:19)
, (5) 4
where a1, . . . , ap, b1, . . . , bq ∈ R ; r, s ∈ R and the hyperparameters belong to the conﬁguration set (m, n, p, q) ∈ H = {(1, 0, 0, 2), (0, 1, 3, 1), (2, 1, 2, 3), (2, 2, 3, 3), (2, 0, 1, 3)}. This set of function includes all the functions with the form f (z) = Φ(w · zq) · zt, (6) (cid:110) id, sin, cos, sinh, cosh, exp, log(1 + ·), arcsin, arctan, Jν, Yν, Iν, 1 with w, q, t ∈ R ; Φ ∈ where Jν, Yν, Iν are the Bessel functions and Γ is Euler’s Gamma function.
Remark. The above set of Meijer G-function includes much more function than the one depicted in (6). The purpose of the above proposition is only to suggest the generality of the symbolic model that we can assemble by using these Meijer G-functions as building blocks. Here, we note that our choice allows to cover many exponential, trigonometric, rational and Bessel functions. 1+· , Γ (cid:111)
Proof. A detailed proof can be found in Section 1 of the supplementary material.
This theorem gives a very satisfying prescription for the hyperparameters. Building on this, we now have a realistic restriction of G that we can use for the set F appearing in (2). We shall henceforth use the following notation for this restriction:
GH = (cid:91)
Gm,n p,q . (m,n,p,q)∈H (7)
Therefore, our Symbolic Pursuit algorithm will search over the set of functions F = GH. All the ingredients are now ready to build the algorithm, this is the subject of next section. 4 Symbolic Pursuit
In this section, we build our Symbolic Pursuit algorithm. With all the ingredients we have prepared in the previous sections, this will be straightforward. The starting point is naturally to consider the functions g1, . . . , gK appearing in (1) to be elements of GH. However, we have to be careful because
Meijer G-functions might not be deﬁned when z = 0, 1. By looking at (1), we note that the arguments of each function gk takes the form v(cid:62) k x for k = 1, . . . , K. We shall now scale this linear combination so that it stays in the domain (0, 1) of the Meijer-G function gk.
The Cauchy-Schwartz inequality for the l2 inner product guarantees that |v(cid:62) normalization for x ∈ X = [0, 1]d guarantees that (cid:107)x(cid:107) ≤ get k x| ≤ (cid:107)vk(cid:107).(cid:107)x(cid:107). Our d. By mixing these two ingredients, we
√
|v(cid:62) (cid:107)vk(cid:107) k x|
√ d
≤ 1. (8)
Therefore, we can simply take the ReLU of
In conclusion, we make the following replacement in (1): d v(cid:62) k x
√ (cid:107)vk(cid:107) as an admissible argument for the G-function gk 3. v(cid:62) k x −→ (cid:19)+ (cid:18) v(cid:62) k x
√ (cid:107)vk(cid:107) d (cid:18)
≡ max 0, v(cid:62) k x
√ (cid:107)vk(cid:107) d (cid:19)
. (9)
Note that, in this way, the argument remains a linear combination of the features in the region where this linear combination is positive4.
Because our optimization problem is non-convex, it is helpful to allow our pursuit algorithm to correct the output of previous iterations at each iteration via a back-ﬁtting strategy. A conventional way to implement this is to add a weight wk in front of each term in the expansion (1) and optimize this weight during the back-ﬁtting procedure [15]. This will guarantee that terms that are constructed 3This is not formally true since the inequality in (8) is not strict. However, this is not important in practice since we can perfectly normalize the features to a closed interval included in (0,1). 4Which would not be the case if we had used a sigmoid to restrict the range of the argument. 5
at some iteration of the algorithm and found to be largely irrelevant at some later iteration will be assigned a small weight. We can now rewrite the symbolic model (1) as
ˆf (x) =
K (cid:88) k=1 wk · gk (cid:32)(cid:20) v(cid:62) k x
√ (cid:107)vk(cid:107) d (cid:21)+(cid:33)
. (10)
Similarly, we can reformulate the optimization problem (2) as (gk, vk, wk) = arg min
GH×Rd×R
X (cid:90) (cid:34) rk(x) − w · g (cid:21)+(cid:33)(cid:35)2 (cid:32)(cid:20) v(cid:62)x
√ (cid:107)v(cid:107) d dF (x). (11)
Note that the optimization over gk takes into account the ﬁve hyperparameters conﬁguration. The objective function in (11) is solved by trying each conﬁguration and keeping the one associated to the smallest loss. At each step of the projection pursuit algorithm, we shall use a back-ﬁtting strategy to correct all the terms that already appear in the expansion. Keeping this in mind, the back-ﬁtting for the term l ∈ {1, . . . , k − 1} will consist in minimizing the residue which excludes the contribution of term l at iteration k rk,l(x) ≡ f (x) −
 (cid:34) wj · gj
 (cid:35)+
 . v(cid:62) j x
√ (cid:107)vj(cid:107) d k (cid:88) j(cid:54)=l (12)
The pseudo code of Symbolic Pursuit is provided in Section 5 of the supplementary material, in which we write our algorithm that solves this optimization problem step by step. It should be stressed that, on a mathematical ground, using Meijer G-functions in optimization problems is far from anodyne.
We elaborate on some theoretical impact of this choice on the behavior of the loss function in Section 3 of the supplementary material. 5