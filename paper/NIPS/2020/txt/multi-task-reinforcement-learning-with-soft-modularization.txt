Abstract
Multi-task learning is a very challenging problem in reinforcement learning. While training multiple tasks jointly allow the policies to share parameters across different tasks, the optimization problem becomes non-trivial: It remains unclear what parameters in the network should be reused across tasks, and how the gradients from different tasks may interfere with each other. Thus, instead of naively sharing parameters across tasks, we introduce an explicit modularization technique on policy representation to alleviate this optimization issue. Given a base policy network, we design a routing network which estimates different routing strategies to reconﬁgure the base network for each task. Instead of directly selecting routes for each task, our task-speciﬁc policy uses a method called soft modularization to softly combine all the possible routes, which makes it suitable for sequential tasks. We experiment with various robotics manipulation tasks in simulation and show our method improves both sample efﬁciency and performance over strong baselines by a large margin. Our project page with code is at https:
//rchalyang.github.io/SoftModule/. 1

Introduction
Deep Reinforcement Learning (RL) has recently demonstrated extraordinary capabilities in multiple domains, including playing games [21] and robotic control and manipulation [18, 16]. Despite its successful applications, Deep RL still requires a large amount of data for training complex tasks. On the other hand, while the current deep RL methods can learn individual policies for speciﬁc tasks such as robot grasping and pushing, it remains very challenging to train a single network that generalizes across all possible robotic manipulation tasks.
In this paper, we study multi-task RL as one step forward towards skill sharing across diverse tasks and ultimately building robots that can generalize. Training deep networks with multiple tasks jointly, agents can learn to share and re-use components across different tasks, which further leads to improved sample efﬁciency. This is particularly important when we want to adopt RL algorithms in real-world applications. Multi-task learning also provides a natural curriculum since learning easier tasks can be beneﬁcial for learning of more challenging tasks with shared parameters [25].
However, multi-task RL remains a hard problem. It becomes even more challenging when the number of tasks increases. For instance, it has been shown by [43] that training with diverse robot manipulation tasks jointly with a sharing network backbone and multiple task-speciﬁc heads for actions hurt the ﬁnal performance comparing to independent training in each task. One major reason is that multi-task learning introduces optimization difﬁculties: It is unclear how the tasks will affect each other when trained jointly, and optimizing some tasks can bring negative impacts on the others [37].
For tackling this problem, compositional models with multiple modules were introduced [1, 9]. For example, researchers proposed to train modular sub-policies and task-speciﬁc high-level policies jointly in a Hierarchical Reinforcement Learning (HRL) framework [1]. The sub-policies can be shared and selected by different high-level policies with a learned policy composition function. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Our multi-task policy network with soft modularization. Given different tasks, our network generate different soft combination of network modules. Gray squares represent network modules and red lines represent the connection between modules (Darker red indicates larger weight).
However, HRL introduces an optimization challenge on jointly training sub-policies and high-level task-speciﬁc policies while training sub-policies separately often require predeﬁned subtasks or some sophisticated way to discover subgoals, which are typically infeasible for real-world applications.
In this paper, instead of designing individual modules explicitly for each sub-policy, we propose a soft modularization method that generates soft combinations of different modules for different tasks automatically without explicitly specifying the policy structure. This approach consists of two networks: a base policy network and a routing network. The base policy network, which is composed of multiple modules, takes the state as input and outputs an action for the task. The routing network takes a task embedding and the current state as input and estimates the routing strategy.
Given a task, the modules in the base policy network will be reconﬁgured by the routing network. This is visualized in Figure 1. Furthermore, instead of taking hard assignments on modules, which is hard to optimize in sequential tasks, our routing network outputs a probability distribution over module assignments for each task. A task-speciﬁc base network can be viewed as a weighted combination of the shared modules according to the probability distribution. We beneﬁt from this design to directly back-prop through the routing weights and train both networks jointly over multiple tasks.
The advantage is that we can modularize the networks according to tasks without specifying policy hierarchies explicitly (e.g., HRL). The role of each module automatically emerged after training and the routing network determines which modules should be used more for different tasks.
We perform experiments in Meta-World [43], which contains 50 robotic manipulation tasks. With soft modularization, we achieve signiﬁcant improvements in both sample efﬁciency and ﬁnal performance over previous state-of-the-art multi-task policies. For example, we almost double the manipulation success rate for learning with 50 tasks compared to the multi-task baselines. Our approach utilizes far less training data compared to training individual policies for each task while achieving learned policy that is able to perform closely to the individually trained policies. This shows that enforcing soft modularization can improve the generalization across different tasks in RL. 2