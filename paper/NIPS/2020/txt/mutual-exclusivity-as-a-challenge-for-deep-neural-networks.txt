Abstract
Strong inductive biases allow children to learn in fast and adaptable ways. Children use the mutual exclusivity (ME) bias to help disambiguate how words map to referents, assuming that if an object has one label then it does not need another. In this paper, we investigate whether or not vanilla neural architectures have an ME bias, demonstrating that they lack this learning assumption. Moreover, we show that their inductive biases are poorly matched to lifelong learning formulations of classiﬁcation and translation. We demonstrate that there is a compelling case for designing task-general neural networks that learn through mutual exclusivity, which remains an open challenge. 1

Introduction
Children are remarkable learners, and thus their inductive biases should interest machine learning researchers. To help learn the meaning of new words efﬁciently, children use the “mutual exclusivity” (ME) bias – the assumption that once an object has one name, it does not need another [1] (Figure 1).
In this paper, we examine whether or not vanilla neural networks demonstrate the mutual exclusivity bias, either as a built-in assumption or as a bias that develops through training. Moreover, we examine common benchmarks in machine translation and object recognition to determine whether or not a maximally efﬁcient learner should use mutual exclusivity.
When children endeavour to learn a new word, they rely on inductive biases to narrow the space of possible meanings. Children learn an average of about 10 new words per day from the age of one until the end of high school
[2], a feat that requires managing a tractable set of candidate meanings. A typical word learning scenario has many sources of ambiguity and uncer-tainty, including ambiguity in the mapping between words and referents.
Children hear multiple words and see multiple objects within a single scene, often without clear supervisory signals to indicate which word goes with which object [3].
The mutual exclusivity assumption helps to resolve ambiguity in how words map to their referents. Markman and Watchel [1] examined scenarios like
Figure 1 that required children to determine the referent of a novel word.
For instance, children who know the meaning of “cup” are presented with two objects, one which is familiar (a cup) and another which is novel (an unusual object). Given these two objects, children are asked to “Show me a dax,” where “dax” is a novel nonsense word. Markman and Wachtel found that children tend to pick the novel object rather than the familiar one. Although it is possible that the word “dax” could be another word for referring to cups, children predict that the novel word refers to the novel object – demonstrating a “mutual exclusivity” bias that familiar objects do not need another name. This is only
Figure 1: The mutual exclusivity task used in cognitive development re-search [1]. Children tend to associate the novel word (“dax”) with the novel object (right). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
a preference; with enough evidence, children must eventually override this bias to learn hierarchical categories: a Dalmatian can be called a “Dalmatian,” a “dog”, or a “mammal” [1, 4]. As an often useful but sometimes misleading cue, the ME bias guides children when learning the words of their native language.
It is instructive to compare word learning in children and machines, since word learning is also a widely studied problem in machine learning and artiﬁcial intelligence. There has been substantial recent progress in object recognition, much of which is attributed to the success of deep neural networks and the availability of very large datasets [5]. But when only one or a few examples of a novel word are available, deep learning algorithms lack human-like sample efﬁciency and ﬂexibility
[6]. Insights from cognitive science and cognitive development can help bridge this gap, and ME has been suggested as a psychologically-informed assumption relevant to machine learning [7]. In this paper, we examine vanilla, task-general neural networks to understand if they have an ME bias.
Moreover, we analyze whether or not ME is a good assumption in lifelong variants of common translation and object recognition tasks. 2