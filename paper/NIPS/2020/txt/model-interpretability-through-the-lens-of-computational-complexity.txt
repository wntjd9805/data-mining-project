Abstract
In spite of several claims stating that some models are more interpretable than others – e.g., “linear models are more interpretable than deep neural networks” – we still lack a principled notion of interpretability to formally compare among different classes of models. We make a step towards such a notion by studying whether folk-lore interpretability claims have a correlate in terms of computational complexity theory. We focus on local post-hoc explainability queries that, intuitively, attempt to answer why individual inputs are classiﬁed in a certain way by a given model.
In a nutshell, we say that a class C1 of models is more interpretable than another class C2, if the computational complexity of answering post-hoc queries for models in C2 is higher than for those in C1. We prove that this notion provides a good theo-retical counterpart to current beliefs on the interpretability of models; in particular, we show that under our deﬁnition and assuming standard complexity-theoretical assumptions (such as P (cid:54)= NP), both linear and tree-based models are strictly more interpretable than neural networks. Our complexity analysis, however, does not provide a clear-cut difference between linear and tree-based models, as we obtain different results depending on the particular post-hoc explanations considered. Fi-nally, by applying a ﬁner complexity analysis based on parameterized complexity, we are able to prove a theoretical result suggesting that shallow neural networks are more interpretable than deeper ones. 1

Introduction
Assume a dystopian future in which the increasing number of submissions has forced journal editors to use machine-learning systems for automatically accepting or rejecting papers. Someone sends his/her work to the journal and the answer is a reject, so the person demands an explanation for the decision. The following are examples of three alternative ways in which the editor could provide an explanation for the rejection given by the system: 1. In order to accept the submitted paper it would be enough to include a better motivation and to delete at least two mathematical formulas. 2. Regardless of the content and the other features of this paper, it was rejected because it has more than 10 pages and a font size of less than 11pt. 3. We only accept 1 out of 20 papers that do not cite any other paper from our own journal. In order to increase your chances next time, please add more references. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
These are examples of so called local post-hoc explanations [3, 19, 23, 26, 27]. Here, the term “local” refers to explaining the verdict of the system for a particular input [19, 27], and the term “post-hoc” refers to interpreting the system after it has been trained [23, 26]. Each one of the above explanations can be seen as a query asked about a system and an input for it. We call them explainability queries.
The ﬁrst query is related with the minimum change required to obtain a desired outcome (“what is the minimum change we must make to the article for it to be accepted by the system?”). The second one is known as a sufﬁcient reason [32], and intuitively asks for a subset of the features of the given input that sufﬁces to obtain the current verdict. The third one, that we call counting completions, relates to the probability of obtaining a particular output given the values in a subset of the features of the input.
In this paper we use explainability queries to formally compare the interpretability of machine-learning models. We do this by relating the interpretability of a class of models (e.g., decision trees) to the computational complexity of answering queries for models in that class. Intuitively the lower the complexity of such queries is, the more interpretable the class is. We study whether this intuition provides an appropriate correlate to folklore wisdom on the interpretability of models [20, 23, 28].
Our contributions. We formalize the framework described above (Section 2) and use it to perform a theoretical study of the computational complexity of three important types of explainability queries over three classes of models. We focus on models often mentioned in the literature as extreme points in the interpretability spectrum: decision trees, linear models, and deep neural networks. In particular, we consider the class of free binary decision diagrams (FBDDs), that generalize decision trees, the class of perceptrons, and the class of multilayer perceptrons (MLPs) with ReLU activation functions.
The instantiation of our framework for these classes is presented in Section 3.
We show that, under standard complexity assumptions, the computational problems associated to our interpretability queries are strictly less complex for FBDDs than they are for MLPs. For instance, we show that for FBDDs, the queries minimum-change-required and counting-completions can be solved in polynomial time, while for MLPs these queries are, respectively, NP-complete and #P-complete (where #P is the prototypical intractable complexity class for counting problems). These results, together with results for other explainability queries, show that under our deﬁnition for comparing the interpretability of classes of models, FBDDs are indeed more interpretable than MLPs. This correlates with the folklore statement that tree-based models are more interpretable than deep neural networks.
We prove similar results for perceptrons: most explainability queries that we consider are strictly less complex to answer for perceptrons than they are for MLPs. Since perceptrons are a realization of a linear model, our results give theoretical evidence for another folklore claim stating that linear models are more interpretable than deep neural networks. On the other hand, the comparison between perceptrons and FBDDs is not deﬁnitive and depends on the particular explainability query. We establish all our computational complexity results in Section 4.
Then, we observe that standard complexity classes are not enough to differentiate the interpretability of shallow and deep MLPs. To present a meaningful comparison, we then use the machinery of parameterized complexity [12, 16], a theory that allows the classiﬁcation of hard computational problems on a ﬁner scale. Using this theory, we are able to prove that there are explainability queries that are more difﬁcult to solve for deeper MLPs compared to shallow ones, thus giving theoretical evidence that shallow MLPs are more interpretable. This is the most technically involved result of the paper, that we think provides new insights on the complexity of interpreting deep neural networks.
We present the necessary concepts and assumptions as well as a precise statement of this result in Section 5.
Most deﬁnitions of interpretability in the literature are directly related to humans in a subjective manner [5, 10, 25]. In this respect we do not claim that our complexity-based notion of interpretability is the right notion of interpretability, and thus our results should be taken as a study of the correlation between a formal notion and the folklore wisdom regarding a subjective concept. We discuss this and other limitations of our results in Section 6. We only present a few sketches for proofs in the body of the paper and refer the reader to the appendix for detailed proofs of all our claims. 2 A framework to compare interpretability
In this section we explain the key abstract components of our framework. The idea is to introduce the necessary terminology to formalize our notion of being more interpretable in terms of complexity. 2
Models and instances. We consider an abstract deﬁnition of a model M simply as a Boolean function M : {0, 1}n → {0, 1}. That is, we focus on binary classiﬁers with Boolean input features.
Restricting inputs and outputs to be Booleans makes our setting cleaner while still covering several relevant practical scenarios. A class of models is just a way of grouping models together. An instance is a vector in {0, 1}n and represents a possible input for a model. A partial instance is a vector in {0, 1, ⊥}n, with ⊥ intuitively representing “undeﬁned” components. A partial instance x ∈
{0, 1, ⊥}n represents, in a compact way, the set of all instances in {0, 1}n that can be obtained by replacing undeﬁned components in x with values in {0, 1}. We call these the completions of x.
Explainability queries. An explainability query is a question that we ask about a model M and a (possibly partial) instance x, and refers to what the model M does on instance x. We assume all queries to be stated either as decision problems (that is, YES/NO queries) or as counting problems (queries that ask, for example, how many completions of a partial instance satisfy a given property).
Thus, for now we can think of queries simply as functions having models and instances as inputs. We will formally deﬁne some speciﬁc queries in the next section, when we instantiate our framework.
Complexity classes. We assume some familiarity with the most common computational complexity classes of polynomial time (PTIME) and nondeterministic polynomial time (NP), and with the notion of hardness and completeness for complexity classes under polynomial time reductions. In the paper we also consider the class Σp 2, consisting of those problems that can be solved in NP if we further grant access to an oracle that solves NP queries in constant time. It is strongly believed that
PTIME (cid:40) NP (cid:40) Σp 2 [2], where for complexity classes K1 and K2 we have that K1 (cid:40) K2 means the following: problems in K1 can be solved in K2, but complete problems for K2 cannot be solved in K1.
While for studying the complexity of our decision problems the above classes sufﬁce, for counting problems we will need another one. This will be the class #P, which corresponds to problems that can be deﬁned as counting the number of accepting paths of a polynomial-time nondeterministic
Turing machine [2]. Intuitively, #P is the counting class associated to NP: while the prototypical
NP-complete problem is checking if a propositional formula is satisﬁable (SAT), the prototypical #P-complete problem is counting how many truth assignments satisfy a propositional formula (#SAT).
It is widely believed that #P is “harder” than Σp 2, which we write as Σp (cid:40) #P.1 2
Complexity-based interpretability of models. Given an explainability query Q and a class C of models, we denote by Q(C) the computational problem deﬁned by Q restricted to models in C. We deﬁne next the most important notion for our framework: that of being more interpretable in terms of complexity (c-interpretable for short). We will use this notion to compare among classes of models.
Deﬁnition 1. Let Q be an explainability query, and C1 and C2 be two classes of models. We say that C1 is strictly more c-interpretable than C2 with respect to Q, if the problem Q(C1) is in the complexity class K1, the problem Q(C2) is hard for complexity class K2, and K1 (cid:40) K2.
For instance, in the above deﬁnition one could take K1 to be the PTIME class and K2 to be the NP class, or K1 = NP and K2 = Σp 2. 3
Instantiating the framework and main results
Here we instantiate our framework on three important classes of Boolean models and explainability queries, and then present our main theorems comparing such models in terms of c-interpretability. 3.1 Speciﬁc models
Binary decision diagrams. A binary decision diagram (BDD [35]) is a rooted directed acyclic graph M with labels on edges and nodes, verifying: (i) each leaf is labeled with true or with false; (ii) each internal node (a node that is not a leaf) is labeled with an element of {1, . . . , n}; and 1One has to be careful with this notation, however, as Σp 2 and #P are complexity classes for problems of different sort: the former being for decision problems, and the latter for counting problems. Although this issue can be solved by considering the class PP, we skip these technical details as they are not fundamental for the paper and can be found in most complexity theory textbooks, such as that of Arora and Barak [2]. 3
(iii) each internal node has an outgoing edge labeled 1 and another one labeled 0. Every instance x = (x1, . . . , xn) ∈ {0, 1}n deﬁnes a unique path πx from the root to a leaf in M, which satisﬁes the following condition: for every non-leaf node u in πx, if i is the label of u, then the path πx goes through the edge that is labeled with xi. The instance x is positive, i.e., M(x) := 1, if the label of the leaf in the path πx is true, and negative otherwise. The size |M| of M is its number of edges. A binary decision diagram M is free (FBDD) if for every path from the root to a leaf, no two nodes on that path have the same label. A decision tree is simply an FBDD whose underlying graph is a tree.
Multilayer perceptron (MLP). A multilayer perceptron M with k layers is deﬁned by a se-quence of weight matrices W (1), . . . , W (k), bias vectors b(1), . . . , b(k), and activation func-tions f (1), . . . , f (k). Given an instance x, we inductively deﬁne h(i) := f (i)(h(i−1)W (i) + b(i)) (i ∈ {1, . . . , k}), (1) assuming that h(0) := x. The output of M on x is deﬁned as M(x) := h(k). In this paper we assume all weights and biases to be rational numbers. That is, we assume that there exists a sequence of positive integers d0, d1, . . . , dk such that W (i) ∈ Qdi−1×di and b(i) ∈ Qdi . The integer d0 is called the input size of M, and dk the output size. Given that we are interested in binary classiﬁers, we assume that dk = 1. We say that an MLP as deﬁned above has (k − 1) hidden layers. The size of an MLP M, denoted by |M|, is the total size of its weights and biases, in which the size of a rational number p/q is log2(p) + log2(q) (with the convention that log2(0) = 1).
We focus on MLPs in which all internal functions f (1), . . . , f (k−1) are the ReLU function relu(x) := max(0, x). Usually, MLP binary classiﬁers are trained using the sigmoid as the output function f (k).
Nevertheless, when an MLP classiﬁes an input (after training), it takes decisions by simply using the pre activations, also called logits. Based on this and on the fact that we only consider already trained MLPs, we can assume without loss of generality that the output function f (k) is the binary step function, deﬁned as step(x) := 0 if x < 0, and step(x) := 1 if x ≥ 0.
Perceptron. A perceptron is an MLP with no hidden layers (i.e., k = 1). That is, a perceptron M is deﬁned by a pair (W , b) such that W ∈ Qd×1 and b ∈ Q, and the output is M(x) = step(xW +b).
Because of its particular structure, a perceptron is usually deﬁned as a pair (w, b) with w a rational vector and b a rational number. The output of M(x) is then 1 if and only if (cid:104)x, w(cid:105) + b ≥ 0, where (cid:104)x, w(cid:105) denotes the dot product between x and w. 3.2 Speciﬁc queries
Given instances x and y, we deﬁne d(x, y) := (cid:80)n i=1 |xi − yi| as the number of components in which x and y differ. We now formalize the minimum-change-required problem, which checks if the output of the model can be changed by ﬂipping the value of at most k components in the input.
Problem: MINIMUMCHANGEREQUIRED (MCR)
Input: Model M, instance x, and k ∈ N
Output: YES, if there exists an instance y with d(x, y) ≤ k and M(x) (cid:54)= M(y), and NO otherwise
Notice that, in the above deﬁnition, instead of “ﬁnding” the minimum change we state the problem as a YES/NO query (a decision problem) by adding an additional input k ∈ N and then asking for a change of size at most k. This is a standard way of stating a problem to analyze its complexity [2].
Moreover, in our results, when we are able to solve the problem in PTIME then we can also output a minimum change, and it is clear that if the decision problem is hard then the optimization problem is also hard. Hence, we can indeed state our problems as decision problems without loss of generality.
To introduce our next query, recall that a partial instance is a vector y = (y1, . . . , yn) ∈ {0, 1, ⊥}n, and a completion of it is an instance x = (x1, . . . , xn) ∈ {0, 1}n such that for every i where yi ∈
{0, 1} it holds that xi = yi. That is, x coincides with y on all the components of y that are not ⊥. Given an instance x and a model M, a sufﬁcient reason for x with respect to M [32] is a partial instance y, such that x is a completion of y and every possible completion x(cid:48) of y satisﬁes M(x(cid:48)) = M(x). That is, knowing the value of the components that are deﬁned in y is 4
enough to determine the output M(x). Observe that an instance x is always a sufﬁcient reason for itself, and that x could have multiple (other) sufﬁcient reasons. However, given an instance x, the sufﬁcient reasons of x that are most interesting are those having the least possible number of deﬁned components; indeed, it is clear that the less deﬁned components a sufﬁcient reason has, the more information it provides about the decision of M on x. For a partial instance y, let us write (cid:107)y(cid:107) for its number of components that are not ⊥. The previous observations then motivate our next interpretability query.
Problem: MINIMUMSUFFICIENTREASON (MSR)
Input: Model M, instance x, and k ∈ N
Output: YES, if there exists a sufﬁcient reason y for x wrt. M with (cid:107)y(cid:107) ≤ k, and NO otherwise
As for the case of MCR, notice that we have formalized this interpretability query as a decision problem. The last query that we will consider refers to counting the number of positive completions for a given partial instance.
Problem: COUNTCOMPLETIONS (CC)
Input: Model M, partial instance y
Output: The number of completions x of y such that M(x) = 1
Intuitively, this query informs us on the proportion of inputs that are accepted by the model, given that some particular features have been ﬁxed; or, equivalently, on the probability that such an instance is accepted, assuming the other features to be uniformly and independently distributed. 3.3 Main interpretability theorems
We can now state our main theorems, which are illustrated in Figure 1. In all these theorems we use CMLP to denote the class of all models (functions from {0, 1}n to {0, 1}) that are deﬁned by MLPs, and similarly for CFBDD and CPerceptron. The proofs for all these results will follow as corollaries from the detailed complexity analysis that we present in Section 4. We start by stating a strong separation between FBDDs and MLPs, which holds for all the queries presented above.
Theorem 2. CFBDD is strictly more c-interpretable than CMLP with respect to MCR, MSR, and CC.
For the comparison between perceptrons and MLPs, we can establish a strict separation for MCR and MSR , but not for CC. In fact, CC has the same complexity for both classes of models, which means that none of these classes strictly “dominates” the other in terms of c-interpretability for CC.
Theorem 3. CPerceptron is strictly more c-interpretable than CMLP with respect to MCR and MSR. In turn, the problems CC(CPerceptron) and CC(CMLP) are both complete for the same complexity class.
The next result shows that, in terms of c-interpretability, the relationship between FBDDs and perceptrons is not clear, as each one of them is strictly more c-interpretable than the other for some explainability query.
Theorem 4. The problems MCR(CFBDD) and MCR(CPerceptrons) are both in PTIME. How-ever, CPerceptron is strictly more c-interpretable than CFBDD with respect to MSR, while CFBDD is strictly more c-interpretable than CPerceptron with respect to CC.
We prove these results in the next section, where for each query Q and class of models C we pinpoint the exact complexity of the problem Q(C). 5
MLPs
MCR, MSR, CC
MCR, MSR
FBDDs
CC
MSR
Perceptrons
Figure 1: Illustration of the main interpretability results. Arrows depict that the pointed class of models is harder with respect to the query that labels the edge. We omit labels (or arrows) when a problem is complete for the same complexity class for two classes of models. 4 The complexity of explainability queries
FBDDs
Perceptrons
MLPs
MINIMUMCHANGEREQUIRED
MINIMUMSUFFICIENTREASON NP-complete
CHECKSUFFICIENTREASON
COUNTCOMPLETIONS
PTIME
PTIME
PTIME
PTIME
PTIME
PTIME
#P-complete
NP-complete
Σp 2-complete coNP-complete
#P-complete
Table 1: Summary of our complexity results.
In this section we present our technical complexity results proving Theorems 2, 3, and 4. We divide our results in terms of the queries that we consider. We also present a few other complexity results that we ﬁnd interesting on their own. A summary of the results is shown in Table 1. With the exception of
Proposition 6, items (1) and (3), the proofs for this section are relatively routine, were already known or follow from known techniques. As mentioned in the introduction, we only present the main ideas of some of the proofs in the body of the paper, and a detailed exposition of each result can be found in the appendix. 4.1 The complexity of MINIMUMCHANGEREQUIRED
In what follows we determine the complexity of the MINIMUMCHANGEREQUIRED problem for the three classes of models that we consider.
Proposition 5. The MINIMUMCHANGEREQUIRED query is (1) in PTIME for FBDDs, (2) in PTIME for perceptrons, and (3) NP-complete for MLPs.
Proof sketch. This query has been shown to be solvable in PTIME for ordered binary decision diagrams (OBDDs, a restricted form of FBDDs) by Shih et al. [31, Theorem 6] (the query is called robusteness in the work of Shih et al. [31]). We show that the same proof applies to FBDDs. Recall that in an FBDD every internal node is labeled with a feature index in {1, . . . , n}. The main idea is to compute a quantity mcru(x) ∈ N ∪ {∞} for every node u of the FBDD M. This quantity represents the minimum number of features that we need to ﬂip in x to modify the classiﬁcation M(x) if we are only allowed to change features associated with the paths from u to some leaf in the FBDD. One can easily compute these values by processing the FBDD bottom-up. Then the minimum change required for x is the value mcrr(x) where r is the root of M, and thus we simply return YES if mcrr(x) ≤ k, and NO otherwise.
For the case of a perceptron M = (w, b) and of an instance x, let us assume without loss of generality that M(x) = 1. We ﬁrst deﬁne the importance s(i) ∈ Q of every input feature at position i as follows: if xi = 1 then s(i) := wi, and if xi = 0 then s(i) := −wi. Consider now the set S that contains the top k most important input features for which s(i) > 0. We can easily show that it is enough to check whether ﬂipping every feature in S changes the classiﬁcation of x, in which case we return YES, and return NO otherwise.
Finally, NP membership of MCR for MLPs is clear: guess a partial instance y with d(x, y) ≤ k and check in polynomial time that M(x) (cid:54)= M(y). We prove hardness with a simple reduction from the VERTEXCOVER problem for graphs, which is known to be NP-complete. 6
Notice that this result immediately yields Theorems 2, 3, and 4 for the case of MCR. 4.2 The complexity of MINIMUMSUFFICIENTREASON
We now study the complexity of MINIMUMSUFFICIENTREASON. The following result yields
Theorems 2, 3, and 4 for the case of MSR.
Proposition 6. The MINIMUMSUFFICIENTREASON query is (1) NP-complete for FBDDs (and hardness holds already for decision trees), (2) in PTIME for perceptrons, and (3) Σp 2-complete for
MLPs.
Proof sketch. Membership of the problem in the respective classes is easy. We show NP-completeness of the problem for FBDDs by a nontrivial reduction from the NP-complete problem of determining whether a directed acyclic graph has a dominating set of size at most k [22]. For a perceptron M = (w, b) and an instance x, assume without loss of generality that M(x) = 1. As in the proof of
Proposition 5, we consider the importance of every component of x, and prove that it is enough to check whether the k most important features of x are a sufﬁcient reason for it, in which case we return YES, and simply return NO otherwise. Finally, the Σp 2-completeness for MLPs is obtained again using a technical reduction from the problem called SHORTEST IMPLICANT CORE, deﬁned and shown to be Σp 2-complete by Umans [34].
To reﬁne our analysis, we also consider the natural problem of checking if a given partial instance is a sufﬁcient reason for an instance.
Problem: CHECKSUFFICIENTREASON (CSR)
Input: Model M, instance x and a partial instance y
Output: YES, if y is a sufﬁcient reason for x wrt. M, and NO otherwise
We obtain the following (easy) result.
Proposition 7. The query CHECKSUFFICIENTREASON is (1) in PTIME for FBDDs, (2) in PTIME for perceptrons, and (3) co-NP-complete for MLPs.
We note that this result for FBDDs already appears in [9] (under the name of implicant check).
Interestingly, we observe that this new query maintains the comparisons in terms of c-interpretability, in the sense that CFBDD and CPerceptron are strictly more c-interpretable than CMLP with respect to CSR. 4.3 The complexity of COUNTCOMPLETIONS
What follows is our main complexity result regarding the query COUNTCOMPLETIONS, which yields
Theorems 2, 3, and 4 for the case of CC.
Proposition 8. The query COUNTCOMPLETIONS is (1) in PTIME for FBDDs, (2) #P-complete for perceptrons, and (3) #P-complete for MLPs.
Proof sketch. Claim (1) is a a well-known fact that is a direct consequence of the deﬁnition of FBDDs; indeed, we can easily compute by bottom-up induction of the FBDD a quantity representing for each node the number of positive completions of the sub-FBDD rooted at that node (e.g., see [9, 35]). We prove (2) by showing a reduction from the #P-complete problem #KNAPSACK, i.e., counting the number of solutions to a 0/1 knapsack input.2 For the last claim, we show that MLPs with ReLU activations can simulate arbitrary Boolean formulas, which allows us to directly conclude (3) since counting the number of satisfying assignments of a Boolean formula is #P-complete.
Comparing perceptrons and MLPs. Although the query COUNTCOMPLETIONS is #P-complete for perceptrons, we can still show that the complexity goes down to PTIME if we assume the weights and biases to be integers given in unary; this is commonly called pseudo-polynomial time.
Proposition 9. The query COUNTCOMPLETIONS can be solved in pseudo-polynomial time for perceptrons (assuming the weights and biases to be integers given in unary). 2Recall that such an input consists of natural numbers (given in binary) s1, . . . , sn, k ∈ N, and a solution to it is a set S ⊆ {1, . . . , n} with (cid:80) i∈S si ≤ k. 7
Proof sketch. This is proved by ﬁrst reducing the problem to #KNAPSACK, and then using a classical dynamic programming algorithm to solve #KNAPSACK in pseudo-polynomial time.
This result establishes a difference between perceptrons and MLPs in terms of CC, as this query remains #P-complete for the latter even if weights and biases are given as integers in unary. Another difference is established by the fact that COUNTCOMPLETIONS for perceptrons can be efﬁciently approximated, while this is not the case for MLPs. To present this idea, we brieﬂy recall the notion of fully polynomial randomized approximation scheme (FPRAS [21]), which is heavily used to reﬁne the analysis of the complexity of #P-hard problems. Intuitively, an FPRAS is a polynomial time algorithm that computes with high probability a (1 − (cid:15))-multiplicative approximation of the exact solution, for (cid:15) > 0, in polynomial time in the size of the input and in the parameter 1/(cid:15). We show:
Proposition 10. The problem COUNTCOMPLETIONS restricted to perceptrons admits an FPRAS (and the use of randomness is not even needed in this case). This is not the case for MLPs, on the other hand, at least under standard complexity assumptions. 5 Parameterized results for MLPs in terms of number of layers
In Section 4.1 we proved that the query MINIMUMCHANGEREQUIRED is NP-complete for MLPs.
Moreover, a careful inspection of the proof reveals that MCR is already NP-hard for MLPs with only a few layers. This is not something speciﬁc to MCR: in fact, all lower bounds for the queries studied in the paper in terms of MLPs hold for a small, ﬁxed number of layers. Hence, we cannot differentiate the interpretability of shallow and deep MLPs with the complexity classes that we have used so far.
In this section, we show how to construct a gap between the (complexity-based) interpretability of shallow and deep MLPs by considering reﬁned complexity classes in our c-interpretability framework.
In particular, we use parameterized complexity [12, 16], a branch of complexity theory that studies the difﬁculty of a problem in terms of multiple input parameters. To the best of our knowledge, the idea of using parameterized complexity theory to establish a gap in the complexity of interpreting shallow and deep networks is new.
We ﬁrst introduce the main underlying idea of parameterized complexity in terms of two classical graph problems: VERTEXCOVER and CLIQUE. In both problems the input is a pair (G, k) with G a graph and k an integer. In VERTEXCOVER we verify if there exists a set of nodes of size at most k that includes at least one endpoint for every edge in G. In CLIQUE we check if there exists a set of nodes of size at most k such that all nodes in the set are adjacent to each other. Both problems are known to be NP-complete. However, this analysis treats G and k at the same level, which might not be fair in some practical situations in which k is much smaller than the size of G. Parameterized complexity then studies how the complexity of the problems behaves when the input is only G, and k is regarded as a small parameter.
It happens to be the case that VERTEXCOVER and CLIQUE, while both NP-complete, have a different status in terms of parameterized complexity. Indeed, VERTEXCOVER can be solved in time O(2k · |G|), which is polynomial in the size of the input G – with the exponent not depending on k – and, thus, it is called ﬁxed-parameter tractable [12]. In turn, it is widely believed that there is no algorithm for CLIQUE with time complexity O(f (k) · poly(G)) – with f being any computable function, that depends only on k – and thus it is ﬁxed-parameter intractable [12]. To study the notion of ﬁxed-parameter intractability, researchers on parameterized complexity have introduced the W[t] complexity classes (with t ≥ 1), which form the so called W-hierarchy. For instance CLIQUE is W[1]-complete [12]. A core assumption in parameterized complexity is that W[t] (cid:40) W[t + 1], for every t ≥ 1.
In this paper we will use a related hierarchy, called the W(Maj)-hierarchy [14]. We defer the formal deﬁnitions of these two hierachies to the appendix. We simply mention here that both classes, W[t] and W(Maj)[t], are closely related to logical circuits of depth t. The circuits that deﬁne the W-hierarchy use gates AND, OR and NOT, while circuits for W(Maj) use only the MAJORITY gate (which outputs a 1 if more than half of its inputs are 1). Our result below applies to a special class of
MLPs that we call restricted-MLPs (rMLPs for short), where we assume that the number of digits of each weight and bias in the MLP is at most logarithmic in the number of neurons in the MLP (a detailed exposition of this restriction can be found in the appendix). We can now formally state the main result of this section. 8
Proposition 11. For every t ≥ 1 the MINIMUMCHANGEREQUIRED query over rMLPs with 3t + 3 layers is W(Maj)[t]-hard and is contained in W(Maj)[3t + 7].
By assuming that the W(Maj)-hierarchy is strict, we can use Proposition 11 to provide separations for rMLPs with different numbers of layers. For instance, instantiating the above result with t = 1 we obtain that for rMLPs with 6 layers, the MCR problem is in W(Maj)[3t + 7] = W(Maj)[10].
Moreover, instantiating it with t = 11 we obtain that for rMLPs with 36 layers, the MCR problem is W(Maj)[11]-hard. Thus, assuming that W(Maj)[10] (cid:40) W(Maj)[11] we obtain that rMLPs with 6 layers are strictly more c-interpretable than rMLPs with 36 layers. We generalize this observation in the following result.
Proposition 12. Assume that the W(Maj)-hierarchy is strict. Then for every t ≥ 1 we have that rMLPs with 3t + 3 layers are strictly more c-interpretable than rMLPs with 9t + 27 layers wrt. MCR. 6 Discussion and concluding remarks