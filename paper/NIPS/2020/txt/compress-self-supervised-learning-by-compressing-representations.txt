Abstract
Self-supervised learning aims to learn good representations with unlabeled data.
Recent works have shown that larger models beneﬁt more from self-supervised learning than smaller models. As a result, the gap between supervised and self-supervised learning has been greatly reduced for larger models. In this work, instead of designing a new pseudo task for self-supervised learning, we develop a model compression method to compress an already learned, deep self-supervised model (teacher) to a smaller one (student). We train the student model so that it mimics the relative similarity between the datapoints in the teacher’s embed-ding space. For AlexNet, our method outperforms all previous methods including the fully supervised model on ImageNet linear evaluation (59.0% compared to 56.5%) and on nearest neighbor evaluation (50.7% compared to 41.4%). To the best of our knowledge, this is the ﬁrst time a self-supervised AlexNet has outper-formed supervised one on ImageNet classiﬁcation. Our code is available here: https://github.com/UMBCvision/CompRess

Introduction 1
Supervised deep learning needs lots of annotated data, but the annotation process is particularly expensive in some domains like medical images. Moreover, the process is prone to human bias and may also result in ambiguous annotations. Hence, we are interested in self-supervised learning (SSL) where we learn rich representations from unlabeled data. One may use these learned features along with a simple linear classiﬁer to build a recognition system with small annotated data. It is shown that
SSL models trained on ImageNet without labels outperform the supervised models when transferred to other tasks [9, 24].
Some recent self-supervised learning algorithms have shown that increasing the capacity of the architecture results in much better representations. For instance, for SimCLR method [9], the gap between supervised and self-supervised is much smaller for ResNet-50x4 compared to ResNet-50 (also shown in Figure 1). Given this observation, we are interested in learning better representations for small models by compressing a deep self-supervised model.
In edge computing applications, we prefer to run the model (e.g., an image classiﬁer) on the device (e.g., IoT) rather than sending the images to the cloud. During inference, this reduces the privacy concerns, latency, power usage, and cost. Hence, there is need for rich, small models. Compressing
SSL models goes beyond that and reduces the privacy concerns at the training time as well. For instance, one can download a rich self-supervised MobileNet model that can generalize well to other tasks and ﬁnetune it on his/her own data without sending any data to the cloud for training.
Since we assume our teacher has not seen any labels, its output is an embedding rather than a probability distribution over some categories. Hence, standard model distillation methods [26] cannot
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Linear Evaluation
Nearest Neighbor
Cluster Alignment
Figure 1: ImageNet Evaluation: We compare “Ours-1q” self-supervised model with supervised and SOTA self-supervised models on ImageNet using linear classiﬁcation (left), nearest neighbor (middle) and cluster alignment (right) evaluations. Our AlexNet model outperforms the supervised counterpart on all evaluations. This model is compressed from ResNet-50x4 trained with SimCLR method using unlabeled ImageNet. All models have seen ImageNet images only. All SOTA SSL models are MoCo except ResNet50x4 that is SimCLR. The teacher for our AlexNet and ResNet50 is
SimCLR ResNet50x4 and for ResNet18 and MobileNet-V2 is MoCo ResNet50. be used directly. One can employ a nearest neighbor classiﬁer in the teacher space by calculating distances between an input image (query) and all datapoints (anchor points) and then converting them to probability distribution. Our idea is to transfer this probability distribution from the teacher to the student so that for any query point, the student matches the teacher in the ranking of anchor points.
Traditionally, most SSL methods are evaluated by learning a linear classiﬁer on the features to perform a downstream task (e.g., ImageNet classiﬁcation). However, this evaluation process is expensive and has many hyperparameters (e.g., learning rate schedule) that need to be tuned as one set of parameters may not be optimal for all SSL methods. We believe a simple nearest neighbor classiﬁer, used in some recent works [57, 67, 60], is a better alternative as it has no parameters, is much faster to evaluate, and still measures the quality of features. Hence, we use this evaluation extensively in our experiments. Moreover, inspired by [30], we use another related evaluation by measuring the alignment between k-means clusters and image categories.
Our extensive experiments show that our compressed SSL models outperform state-of-the-art com-pression methods as well as state-of-the-art SSL counterparts using the same architecture on most downstream tasks. Our AlexNet model, compressed from ResNet-50x4 trained with SimCLR method, outperforms standard supervised AlexNet model on linear evaluation (by 2 point), in nearest neighbor (by 9 points), and in cluster alignment evaluation (by 4 points). This is interesting as all parameters of the supervised model are already trained on the downstream task itself but the SSL model and its teacher have seen only ImageNet without labels. To the best of our knowledge, this is the ﬁrst time an SSL model performs better than the supervised one on the ImageNet task itself instead of transfer learning settings. 2