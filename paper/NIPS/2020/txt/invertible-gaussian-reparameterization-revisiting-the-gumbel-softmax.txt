Abstract
The Gumbel-Softmax is a continuous distribution over the simplex that is often used as a relaxation of discrete distributions. Because it can be readily interpreted and easily reparameterized, it enjoys widespread use. We propose a modular and more ﬂexible family of reparameterizable distributions where Gaussian noise is transformed into a one-hot approximation through an invertible function. This invertible function is composed of a modiﬁed softmax and can incorporate diverse transformations that serve different speciﬁc purposes. For example, the stick-breaking procedure allows us to extend the reparameterization trick to distributions with countably inﬁnite support, thus enabling the use of our distribution along nonparametric models, or normalizing ﬂows let us increase the ﬂexibility of the distribution. Our construction enjoys theoretical advantages over the Gumbel-Softmax, such as closed form KL, and signiﬁcantly outperforms it in a variety of experiments. Our code is available at https://github.com/cunningham-lab/ igr. 1

Introduction
Numerous machine learning tasks involve optimization problems over discrete stochastic components whose parameters we wish to learn. Mixture and mixed-membership models, variational autoencoders, language models and reinforcement learning fall into this category [13, 14, 26, 17, 8]. Ideally, as with fully continuous models, we would use stochastic optimization via backpropagation. One strategy to compute the necessary gradients is using score estimators [8, 32], however these estimates suffer from high variance which leads to slow convergence. Another strategy is to ﬁnd a reparameterizable continuous relaxation of the discrete distribution. Reparameterization gradients exhibit lower variance but are contingent on ﬁnding such a relaxation. Jang et al. [12] and Maddison et al. [19] independently found such a continuous relaxation via the Gumbel-Softmax (GS) or Concrete distribution.
The GS has experienced wide use and has been extended to other settings, such as permutations
[18], subsets [33] and more [1]. Its success relies on several qualities that make it appealing: (i) it is reparameterizable, that is, it can be sampled by transforming parameter-independent noise through a smooth function, (ii) it can approximate any discrete distribution, (i.e. converge in distribution) (iii) it has a closed form density, and (iv) its parameters can be interpreted as the discrete distribution that it is relaxing. While the last quality is mathematically pleasing, it is not a necessary condition for a valid relaxation. Here we ask: how important is this parameter interpretability? In the context of deep learning models, interpreting the parameters is not a ﬁrst concern, and we show that the GS can be signiﬁcantly improved upon by giving up this quality.
In this paper we propose an alternative family of distributions over the simplex to achieve this relaxation, which we call Invertible Gaussian Reparameterization (IGR). Our reparameterization
∗Work partially done while at the Department of Statistics, Columbia University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
works by transforming Gaussian noise through an invertible transformation onto the simplex, and a temperature hyperparameter allows the distribution to concentrate its mass around the vertices. IGR is more natural, more ﬂexible, and more easily extended than the GS. Furthermore, IGR enables using the reparameterization trick on distributions with countably inﬁnite support, which enables nonparametric uses, and also admits closed form KL divergence evaluation. Finally, we show that our distribution outperforms the GS in a wide variety of experimental settings. 2