Abstract
Advances in the development of adversarial attacks have been fundamental to the progress of adversarial defense research. Efﬁcient and effective attacks are crucial for reliable evaluation of defenses, and also for developing robust models.
Adversarial attacks are often generated by maximizing standard losses such as the cross-entropy loss or maximum-margin loss within a constraint set using Projected
Gradient Descent (PGD). In this work, we introduce a relaxation term to the standard loss, that ﬁnds more suitable gradient-directions, increases attack efﬁcacy and leads to more efﬁcient adversarial training. We propose Guided Adversarial
Margin Attack (GAMA), which utilizes function mapping of the clean image to guide the generation of adversaries, thereby resulting in stronger attacks. We evaluate our attack against multiple defenses and show improved performance when compared to existing attacks. Further, we propose Guided Adversarial Training (GAT), which achieves state-of-the-art performance amongst single-step defenses by utilizing the proposed relaxation term for both attack generation and training. 1

Introduction
The remarkable success of Deep Learning algorithms has led to a surge in their adoption in a multitude of applications which inﬂuence our lives in numerous ways. This makes it imperative to understand their failure modes and develop reliable risk mitigation strategies. One of the biggest known threats to systems that deploy Deep Networks is their vulnerability to crafted imperceptible noise known as adversarial attacks, as demonstrated by Szegedy et al.[30] in 2014. This ﬁnding has spurred immense interest towards identifying methods to improve the robustness of deep neural networks against adversarial attacks. While initial attempts of improving robustness against adversarial attacks used just single-step adversaries for training [14], they were later shown to be ineffective against strong multi-step attacks by Kurakin et al.[22]. Some of the defenses introduced randomised or non-differentiable components, either in the pre-processing stage or in the network architecture, so as to minimise the effectiveness of generated gradients. However, many such defenses [4, 35, 29, 16] were later broken by Athalye et al.[3] using smooth approximations of the function during the backward pass or by computing reliable gradients using expectation over the randomized components.
This game of building defenses against existing attacks, and developing attacks against the proposed defenses has been crucial for the progress in this ﬁeld. Lately, the community has also recognized that the true testimony of a developed defense is to evaluate it against adaptive attacks which are constructed speciﬁcally to compromise the defense at hand [6].
Multi-step adversarial training is one of the best known methods of achieving robustness to adversarial attacks today [24, 37]. This training regime attempts to solve the minimax optimization problem of
ﬁrstly generating strong adversarial samples by maximizing a loss, and subsequently training the
∗Equal contribution.
Correspondence to: Gaurang Sriramanan <gaurangs@iisc.ac.in>, Sravanti Addepalli <sravantia@iisc.ac.in> 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
model to minimize loss on these adversarial samples. The effectiveness of the defense thus developed depends on the strength of the attack used for training. Therefore, development of stronger attacks is important for both evaluating existing defenses, and also for constructing adversarial samples during adversarial training. Indeed, the study of building robust adversarial defenses and strong adversarial attacks are closely coupled with each other today.
Adversarial attacks are constructed by maximizing standard objectives such as cross-entropy loss or maximum-margin loss within a constraint set, as deﬁned by the threat model. Due to the non-convex nature of the loss function, maximization of such a loss may not effectively ﬁnd the path towards the class whose decision boundary is closest to the data point. In this work, we aid the optimization process by utilizing the knowledge embedded in probability values corresponding to non-maximal classes to guide the generation of adversaries. Motivated by graduated optimization methods, we improve the optimization process by introducing an (cid:96)2 relaxation term initially, and reducing the weight of this term gradually over the course of optimization, thereby making it equivalent to the primary objective towards the end. We demonstrate state-of-the-art results on multiple defenses and datasets using the proposed attack. We further analyse the impact of utilizing the proposed method to generate strong attacks for adversarial training. While use of the proposed attack for multi-step training shows only marginal improvement, we observe signiﬁcant gains by using the proposed attack for single-step adversarial training. Single-step methods rely heavily on the initial gradient direction, and hence the proposed attack shows signiﬁcant improvement over existing methods.
Our contributions in this work can be summarized as follows:
• We propose Guided Adversarial Margin Attack (GAMA), which achieves state-of-the-art performance across multiple defenses for a single attack and across multiple random restarts.
• We introduce a multi-targeted variant GAMA-MT, which achieves improved performance compared to methods that utilize multiple targeted attacks to improve attack strength [15].
• We demonstrate that Projected Gradient Descent based optimization (GAMA-PGD) leads to stronger attacks when a large number of steps (100) can be used, thereby making it suitable for defense evaluation; whereas, Frank-Wolfe based optimization (GAMA-FW) leads to stronger attacks when the number of steps used for attack are severely restricted (10), thereby making it useful for adversary generation during multi-step adversarial training.
• We propose Guided Adversarial Training (GAT), which achieves state-of-the-art results amongst existing single-step adversarial defenses. We demonstrate that the proposed defense can scale to large network sizes and to large datasets such as ImageNet-100.
Our code and pre-trained models are available here: https://github.com/val-iisc/GAMA-GAT. 2 Preliminaries
θ (x), . . . , f N
Notation: In this paper, we consider adversarial attacks in the setting of image classiﬁcation using deep neural networks. We denote a sample image as x ∈ X , and its corresponding label as y ∈ {1, . . . , N }, where X indicates the sample space and N denotes the number of classes. Let fθ : X → [0, 1]N represent the deep neural network with parameters θ, that maps an input image x to its softmax output fθ(x) = (cid:0)f 1
θ (x)(cid:1) ∈ [0, 1]N . Further, let Cθ(x) represent the argmax over the softmax output. Thus, the network is said to successfully classify an image when
Cθ(x) = y. The cross-entropy loss for a data sample, (xi, yi) is denoted by (cid:96)CE(fθ(xi), yi). We denote an adversarially modiﬁed counterpart of a clean image x as (cid:101)x.
Adversarial Threat Model: The goal of an adversary is to alter the clean input image x such that the attacked image (cid:101)x is perceptually similar to x, but causes the network to misclassify. Diverse operational frameworks have been developed to quantify perceptual similarity, and adversarial attacks corresponding to these constraints have been studied extensively. We primarily consider the standard setting of worst-case adversarial attacks, subject to (cid:96)p-norm constraints. More precisely, we consider adversarial threats bound in (cid:96)∞ norm: (cid:101)x ∈ {x(cid:48) : (cid:107)x(cid:48) − x(cid:107)∞ ≤ ε}.
While evaluating the proposed defense, we consider that the adversary has full access to the model architecture and parameters, since we consider the setting of worst-case robustness. Further, we assume that the adversary is cognizant of the defense techniques utilised during training or evaluation. 2
3