Abstract
Federated learning (FL) is a decentralized and privacy-preserving machine learning technique in which a group of clients collaborate with a server to learn a global model without sharing clients’ data. One challenge associated with FL is statistical diversity among clients, which restricts the global model from delivering good performance on each client’s task. To address this, we propose an algorithm for personalized FL (pFedMe) using Moreau envelopes as clients’ regularized loss functions, which help decouple personalized model optimization from the global model learning in a bi-level problem stylized for personalized FL. Theoretically, we show that pFedMe’s convergence rate is state-of-the-art: achieving quadratic speedup for strongly convex and sublinear speedup of order 2/3 for smooth non-convex objectives. Experimentally, we verify that pFedMe excels at empirical performance compared with the vanilla FedAvg and Per-FedAvg, a meta-learning based personalized FL algorithm. 1

Introduction
The abundance of data generated in a massive number of hand-held devices these days has stimulated the development of Federated learning (FL) [1]. The setting of FL is a network of clients connected to a server, and its goal is to build a global model from clients’ data in a privacy-preserving and communication-efﬁcient way. The current techniques that attempt to fulﬁll this goal mostly follow three steps: (i) at each communication iteration, the server sends the current global model to clients; (ii) the clients update their local models using their local data; (iii) the server collects the latest local models from a subset of sampled clients in order to update a new global model, repeated until convergence [1–4].
Despite its advantages of data privacy and communication reduction, FL faces a main challenge that affects its performance and convergence rate: statistical diversity, which means that data distributions among clients are distinct (i.e., non-i.i.d.). Thus, the global model, which is trained using these non-i.i.d. data, is hardly well-generalized on each client’s data. This particular behaviour has been reported in [5, 6], which showed that when the statistical diversity increases, generalization errors of the global model on clients’ local data also increase signiﬁcantly. On the other hand, individual learning without FL (i.e., no client collaboration) will also have large generalization error due to insufﬁcient data. These raise the question: How can we leverage the global model in FL to ﬁnd a
“personalized model” that is stylized for each client’s data?
Motivated by critical roles of personalized models in several business applications of healthcare,
ﬁnance, and AI services [5], we address this question by proposing a new FL scheme for person-alization, which minimizes the Moreau envelopes [7] of clients’ loss functions. With this scheme, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
clients not only contribute to building the “reference” global model as in the standard FL, but also leverage the reference model to optimize their personalized models w.r.t. local data. Geometrically, the global model in this scheme can be considered as a “central point” where all clients agree to meet, and personalized models are the points in different directions that clients follow according to their heterogeneous data distributions.
Our key contributions in this work are summarized as follows. First, we formulate a new bi-level optimization problem designed for personalized FL (pFedMe) by using the Moreau envelope as a regularized loss function. The bi-level structure of pFedMe has a key advantage: decoupling the process of optimizing personalized models from learning the global model. Thus, pFedMe updates the global model similarly to the standard FL algorithm such as FedAvg [1], yet parallelly optimizes the personalized models with low complexity.
Second, we exploit the convexity-preserving and smoothness-enabled properties of the Moreau envelopes to facilitate the convergence analysis of pFedMe, which characterizes both client-sampling and client-drift errors: two notorious issues in FL [3]. With carefully tuned hyperparameters, pFedMe can obtain the state-of-the-art quadratic speedup (resp. sublinear speedup of order 2/3), compared with the existing works with linear speedup (resp. sublinear speedup of order 1/2), for strongly convex (resp. smooth nonconvex) objective.
Finally, we empirically evaluate the performance of pFedMe using both real and synthetic datasets that capture the statistical diversity of clients’ data. We show that pFedMe outperforms the vanilla FedAvg and a meta-learning based personalized FL algorithm Per-FedAvg [8] in terms of convergence rate and local accuracy. 2