Abstract
Self-supervised representation learning approaches have recently surpassed their supervised learning counterparts on downstream tasks like object detection and image classiﬁcation. Somewhat mysteriously the recent gains in performance come from training instance classiﬁcation models, treating each image and it’s augmented versions as samples of a single class. In this work, we ﬁrst present quantitative experiments to demystify these gains. We demonstrate that approaches like MOCO[1] and PIRL[2] learn occlusion-invariant representations. However, they fail to capture viewpoint and category instance invariance which are crucial components for object recognition. Second, we demonstrate that these approaches obtain further gains from access to a clean object-centric training dataset like
Imagenet. Finally, we propose an approach to leverage unstructured videos to learn representations that possess higher viewpoint invariance. Our results show that the learned representations outperform MOCOv2 trained on the same data in terms of invariances encoded and the performance on downstream image classiﬁcation and semantic segmentation tasks. 1

Introduction
Inspired by biological agents and necessitated by the manual annotation bottleneck, there has been growing interest in self-supervised visual representation learning. Early work in self-supervised learning focused on using “pretext” tasks for which ground-truth is free and can be procured through an automated process [3, 4]. Most pretext tasks include prediction of some hidden portion of input data (e.g., predicting future frames [5] or color of a grayscale image [6]). However, the performance of the learned representations have been far from their supervised counterparts.
The past six months have been revolutionary in the ﬁeld of self-supervised learning. Several recent works [2, 1, 5, 7, 8] have reported signiﬁcant improvements in self-supervised learning performance and now surpassing supervised learning seems like a foregone conclusion. So, what has changed dramatically? The common theme across recent works is the focus on the instance discrimination task [9] – treating every instance as a class of its own. The image and its augmentations are positive examples of this class; all other images are treated as negatives. The contrastive loss[5, 7] has proven to be a useful objective function for instance discrimination, but requires gathering pairs of samples belonging to the same class (or instance in this case). To achieve this, all recent works employ an “aggressive” data augmentation strategy where numerous samples can be generated from a single image. Instance discrimination, contrastive loss and aggressive augmentation are the three key ingredients underlying these new gains.
While there have been substantial gains reported on object recognition tasks, the reason behind the gains is still unclear. Our work attempts to demystify these gains and unravel the hidden story behind this success. The utility of a visual representation can be understood by investigating the invariances (see Section 4.1 for deﬁnition) it encodes. First, we identify the different invariances that
∗Project webpage: http://www.cs.cmu.edu/~spurushw/publication/demystifyssl/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
are crucial for object recognition tasks and then evaluate two state of the art contrastive self-supervised approaches [1, 2] against their supervised counterparts. Our results indicate that a large portion of the recent gains come from occlusion invariances. The occlusion invariance is an obvious byproduct of the aggressive data augmentation which involves cropping and treating small portions of images as belonging to the same class as the full image. When it comes to viewpoint and category instance invariance there is still a gap between the supervised and self-supervised approaches.
Occlusion invariance is a critical attribute for useful representations, but is artiﬁcially crop-ping images the right way to achieve it? The contrastive loss explicitly encourages minimiz-ing the feature distance between positive pairs.
In this case, the pair would consist of two pos-sibly non-overlapping cropped regions of an im-age. For example, in the case of an indoor scene image, one sample could depict a chair and an-other could depict a table. Here the representa-tion would be forced to be bad at differentiating these chairs and tables - which is intuitively the wrong objective! So why do these approaches work? We hypothesize two possible reasons: (a)
The underlying biases of pre-training dataset -Imagenet is an object-centric dataset which en-sures that different crops correspond to different parts of same object; (b) the representation func-tion is not expressive enough to optimize this faulty objective, leading to a sub-optimal representation which works well in practice. We demonstrate through diagnostic experiments that indeed the success of these approaches originates from the object-centric bias of the training dataset. This suggests that the idea of employing aggressive synthetic augmentations must be rethought and improved in future work to ensure scalability.
Figure 1: Aggressive Augmentation Constrastive self-supervised learning methods employ an aggressive crop-ping strategy to generate positive pairs. Through this strategy, an image (left) yields many non-overlapping crops (right) as samples. We can observe that the crops do not necessarily depict objects of the same category.
Therefore, a representation that matches features of these crops would be detrimental for object recognition tasks.
As a step in this direction, in this paper, we argue for usage of a more natural form of data for the instance discrimination task: videos. We present a simple method for leveraging transformations occurring naturally in videos to learn representations. We demonstrate that leveraging this form of data leads to higher viewpoint invariance when compared to image-based learning. We also show that the learned representation outperforms MoCo-v2 [10] trained on the same data in terms of viewpoint invariance, category instance invariance, occlusion invariance and also demonstrates improved performance on object recognition tasks. 2 Contrastive Representation Learning
Contrastive learning [5, 7] is general framework for learning representations that encode similarities according to pre-deterimined criteria. Consider a dataset D = {xi|xi ∈ Rn, i ∈ [N ]}. Let us assume that we have a way to sample positive pairs (xi, x+ i ) ∈ D × D for which we desire to have similar representations. We denote the set of all such positive pairs by D+ ⊂ D × D. The contrastive learning framework learns a normalized feature embedding f by optimizing the following objective function:
L(D, D+) = − X (x,x+)∈D+ exp[ f (x)⊺ f (x+)/τ ] exp[ f (x)⊺ f (x+)/τ ] + P
−∈D
− x (x,x
) /∈D+ exp[ f (x)⊺ f (x−)/τ ] (1)
Here τ is a hyperparameter called temperature. The denominator encourages discriminating negative pairs that are not in the positive set D+. In practice, this summation is expensive to compute for large datasets D and is performed over K randomly chosen negative pairs for each x. Recent works have proposed approaches to scale up the number of negative samples considered while retaining efﬁciency (see Section 3). In our experiments, we adopt the approach proposed in [10].
The contrastive learning framework relies on the ability to sample positive pairs (xi, x+ i ). Self-supervised approaches have leveraged a common mechanism: each sample x is transformed using various transformation functions t ∈ T to generate new samples. The set of positive pairs is then 2
considered as D+ = {(ti(x), tj(x)) | ti, tj ∈ T, x ∈ D} and any pair (ti(x), tk(x′)) is considered a negative pair if x 6= x′.
The choice of transformation functions T controls the properties of the learned representation. Most successful self-supervised approaches [1, 10, 8, 11] have used: 1) cropping sub-regions of images (with areas in the range 20%-100% of the original image), 2) ﬂipping the image horizontally, 3) jittering the color of the image by varying brightness, contrast, saturation and hue, 4) converting to grayscale and 5) applying gaussian blur. By composing these functions and varying their parameters, inﬁnitely many transformations can be constructed. 3