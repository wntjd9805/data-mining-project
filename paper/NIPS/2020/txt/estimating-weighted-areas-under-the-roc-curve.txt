Abstract
Exponential bounds on the estimation error are given for the plug-in estimator of weighted areas under the ROC curve. The bounds hold for single score functions and uniformly over classes of functions, whose complexity can be controlled by Gaussian or Rademacher averages. The results justify learning algorithms which select score functions to maximize the empirical partial area under the curve (pAUC). They also illustrate the use of some recent advances in the theory of nonlinear empirical processes. 1

Introduction
Using the area under the ROC curve (the AUC) to evaluate score functions has a long history in medical screening and bioinformatics ([8],[9]). In the last decades several algorithms have been developed to learn score functions which maximize the AUC ([6], [24]). In some cases, however, different regions of the false positive range may not be equally relevant to assess the quality of the score and should therefore be weighted differently, say with some nonconstant weight function W . In melanoma detection, for example, false negatives have disastrous consequences for a patient, while a certain number of false positives is tolerable. A good scoring function should then have very large values on the right hand side of the ROC plot. Such considerations have created interest in a partial area under the ROC curve (the pAUC), which only measures the area between two speciﬁed false positive rates, so that W is a step function ([26], [15], [18]). Several algorithms have been designed with the goal of maximizing the pAUC over classes of scoring functions ([20], [21], [19], [22]).
Any measurement or optimization of the AUC or pAUC must rely on a ﬁnite number of observations, which raises the questions of estimation and generalization. These problems are well understood for the AUC ([2], [5], [23]), where W is constant, but for more general weight functions estimation may be difﬁcult to impossible. In Proposition 1 below we will show that the bias of its plug-in estimator may be bounded away from zero even in the limit of an inﬁnite sample.
Our ﬁrst contribution shows that these problems are absent if the weight function W is Lipschitz continuous. Theorem 2 shows that then the weighted AUC can be estimated at rate n−1/2 by its plug-in estimator, where n is the sample size. Theorem 3 shows that the Lipschitz-weighted AUC can be uniformly estimated over a class of score functions at rate n−1/2 if the Gaussian complexity of the class increases as n1/2, which is standard for most function classes in machine learning. This result also implies a statistical performance guarantee for algorithms maximizing the empirical pAUC as in
[21] or [19].
Even if W is Lipschitz the weighted AUC remains a nonlinear statistical functional of challenging complexity. Our second contribution is Theorem 7, which provides a general method to control the estimation errors of any statistical functional, which satisﬁes certain Lipschitz conditions with respect to the total-variation, Wasserstein or Kolmogorov metrics. The veriﬁcation of these conditions for the weighted AUC then provides the proofs of Theorem 2 and Theorem 3. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1.1