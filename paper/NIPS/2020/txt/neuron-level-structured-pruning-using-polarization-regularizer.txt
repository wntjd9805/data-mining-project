Abstract
Neuron-level structured pruning is a very effective technique to reduce the compu-tation of neural networks without compromising prediction accuracy. In previous works, structured pruning is usually achieved by imposing L1 regularization on the scaling factors of neurons, and pruning the neurons whose scaling factors are below a certain threshold. The reasoning is that neurons with smaller scaling factors have weaker inﬂuence on network output. A scaling factor close to 0 actually suppresses a neuron. However, L1 regularization lacks discrimination between neurons be-cause it pushes all scaling factors towards 0. A more reasonable pruning method is to only suppress unimportant neurons (with 0 scaling factors), and simultaneously keep important neurons intact (with larger scaling factor). To achieve this goal, we propose a new regularizer on scaling factors, namely polarization regularizer.
Theoretically, we prove that polarization regularizer pushes some scaling factors to 0 and others to a value a > 0. Experimentally, we show that structured pruning using polarization regularizer achieves much better results than using L1 regular-izer. Experiments on CIFAR and ImageNet datasets show that polarization pruning achieves the state-of-the-art result. 1

Introduction
Network pruning is proved to effectively reduce the computational cost of inference without signiﬁ-cantly compromising accuracy [Liu et al., 2019a]. There are two major branches of network pruning.
One branch is unstructured pruning, which prunes at the level of individual weights [LeCun et al., 1989, Han et al., 2015, Zhou et al., 2019, Ding et al., 2019, Frankle and Carbin, 2019]. The other branch is structured pruning, which prunes at the level of neurons (or channels) [Wen et al., 2016,
Liu et al., 2017, Ye et al., 2018]. Although unstructured pruning usually reduces more weights than structured pruning [Liu et al., 2019a], it has the drawback that the resulting weight matrices are sparse, which cannot lead to speedup without dedicated hardware or libraries [Han et al., 2016]. In this paper, we focus on neuron-level structured pruning, which does not require additional hardware or libraries to reduce computation on common GPU/CPU devices.
For structured pruning, one promising method is to associate each neuron with a scaling factor, and regularize these scaling factors in training. Then neurons with scaling factors below a certain threshold are pruned [Huang and Wang, 2018]. The regularizer on scaling factors is usually chosen to be L1 [Liu et al., 2017]. However, L1 regularizer tries to push all scaling factors to 0. It is often difﬁcult to ﬁnd a reasonable pruning threshold. For example, the distribution of scaling factors under
L1 regularization for VGG-16 network trained on CIFAR-10 dataset is shown in Figure 1 (a), where
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The distributions of scaling factors in VGG-16 trained on CIFAR-10 dataset, with L1 and polarization regularizers respectively. Under the same pruning ratio for both regularizers, the orange part are pruned. the scaling factors distribute densely around the threshold value. A cut at the threshold value is not very reasonable because there is no margin around the threshold to separate the pruned neurons from the preserved ones. Pruning with this threshold will lead to severe accuracy drop. A more reasonable regularizer should separate the pruned and the preserved neurons more obviously, with a larger margin between them. To attain this goal, we propose a novel regularizer named polarization.
Different from L1 regularizer that pushes all scaling factors to 0, polarization simultaneously pushes a proportion of scaling factors to 0 (thus pruning these neurons), and the rest scaling factors to a value larger than 0 (thus preserving these neurons). Intuitively, instead of suppressing all neurons in pruning, polarization tries to suppress only a proportion of neurons while keeps others intact.
Polarization regularizer naturally makes a distinction between pruned and preserved neurons. And the resulted scaling factors are more separable. As shown in Figure 1 (b), polarization leads to an obvious margin between scaling factors of the pruned neurons (the orange part) and the preserved ones (the blue part). Pruning using polarization is more reasonable because the pruned neurons has much smaller inﬂuence on network output than the preserved neurons. Our code is available at https://github.com/polarizationpruning/PolarizationPruning.
We summarize our contributions as follows:
• We propose a novel regularizer, namely polarization, for structured pruning of neural networks. We theoretically analyzed the properties of polarization regularizer and proved that it simultaneously pushes a proportion of scaling factors to 0 and others to values larger than 0.
• We verify the effectiveness of polarization pruning on the widely-used CIFAR and ImageNet datasets, and achieve state-of-the-art pruning results. 2