Abstract
This paper studies the universal approximation property of deep neural networks for representing probability distributions. Given a target distribution π and a source distribution pz both deﬁned on Rd, we prove under some assumptions that there exists a deep neural network g : Rd→R with ReLU activation such that the push-forward measure (∇g)#pz of pz under the map ∇g is arbitrarily close to the target measure π. The closeness are measured by three classes of integral probability metrics between probability distributions: 1-Wasserstein distance, maximum mean distance (MMD) and kernelized Stein discrepancy (KSD). We prove upper bounds for the size (width and depth) of the deep neural network in terms of the dimension d and the approximation error ε with respect to the three discrepancies. In particular, the size of neural network can grow exponentially in d when 1-Wasserstein distance is used as the discrepancy, whereas for both MMD and KSD the size of neural network only depends on d at most polynomially. Our proof relies on convergence estimates of empirical measures under aforementioned discrepancies and semi-discrete optimal transport. 1

Introduction
In recent years, deep learning has achieved unprecedented success in numerous machine learning problems [29, 51]. The success of deep learning is largely attributed to the usage of deep neural networks (DNNs) for representing and learning the unknown structures in machine learning tasks, which are usually modeled by some unknown function mappings or unknown probability distributions.
The effectiveness of using neural networks (NNs) in approximating functions has been justiﬁed rigorously in the last three decades. Speciﬁcally, a series of early works [12, 17, 24, 7] on universal approximation theorems show that a continuous function deﬁned on a bounded domain can be approximated by a sufﬁciently large shallow (two-layer) neural network. In particular, the result by [7] quantiﬁes the approximation error of shallow neural networks in terms of the decay property of the Fourier transform of the function of interest. Recently, the expressive power of DNNs for approximating functions have received increasing attention starting from the works by [34] and [56]; see also [57, 45, 47, 41, 48, 14, 40] for more recent developments. The theoretical beneﬁts of using deep neural networks over shallow neural networks have been demonstrated in a sequence of depth separation results; see e.g. [16, 52, 54, 13]
Compared to a vast number of theoretical results on neural networks for approximating functions, the use of neural networks for expressing distributions is far less understood on the theoretical side. The idea of using neural networks for modeling distributions underpins an important class of unsupervised learning techniques called generative models, where the goal is to approximate or learn complex probability distributions from the training samples drawn from the distributions. Typical generative 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
models include Variational Autoencoders [27], Normalizing Flows [46] and Generative Adversarial
Networks (GANs) [18], just to name a few. In these generative models, the probability distribution of interest can be very complex or computationally intractable, and is usually modelled by transforming a simple distribution using some map parameterized by a (deep) neural network. In particular, a GAN consists of a game between a generator and a discriminator which are represented by deep neural networks: the generator attempts to generate fake samples whose distribution is indistinguishable from the real distribution and it generate samples by mapping samples from a simple input distribution (e.g. Gaussian) via a deep neural network; the discriminator attempts to learn how to tell the fake apart from the real. Despite the great empirical success of GANs in various applications, its theoretical analysis is far from complete. Existing theoretical works on GANs are mainly focused on the trade-off between the generator and the discriminator (see e.g. [39, 2, 3, 35, 5]). The key message from these works is that the discriminator family needs to be chosen appropriately according to the generator family in order to obtain a good generalization error.
Our contributions. In this work, we focus on an even more fundamental question on GANs and other generative models which is not yet fully addressed. Namely how well can DNNs express probability distributions? We shall answer this question by making the following contributions.
• Given a fairly general source distribution and a target distribution deﬁned on Rd which satisﬁes certain integrability assumptions, we show that there is a ReLU DNN with d inputs and one output such that the push-forward of the source distribution via the gradient of the output function deﬁned by the DNN is arbitrarily close to the target. We measure the closeness between probability distributions by three integral probability metrics (IPMs): 1-Wasserstein metric, maximum mean discrepancy and kernelized Stein discrepancy.
• Given a desired approximation error ε, we prove complexity upper bounds for the depth and width of the DNN needed to attain the given approximation error with respect to the three IPMs mentioned above; our complexity upper bounds are given with explicit dependence on the dimension d of the target distribution and the approximation error ε.
It is also worth mentioning that the DNN constructed in the paper is explicit: the output function of the DNN is the maximum of ﬁnitely many (multivariate) afﬁne functions, with the afﬁne parameters determined explicitly in terms of the source measure and target measure.