Abstract
Nonconvex minimax problems appear frequently in emerging machine learning applications, such as generative adversarial networks and adversarial learning.
Simple algorithms such as the gradient descent ascent (GDA) are the common practice for solving these nonconvex games and receive lots of empirical success.
Yet, it is known that these vanilla GDA algorithms with constant stepsize can potentially diverge even in the convex-concave setting. In this work, we show that for a subclass of nonconvex-nonconcave objectives satisfying a so-called two-sided
Polyak-Łojasiewicz inequality, the alternating gradient descent ascent (AGDA) algorithm converges globally at a linear rate and the stochastic AGDA achieves a sublinear rate. We further develop a variance reduced algorithm that attains a provably faster rate than AGDA when the problem has the ﬁnite-sum structure.

Introduction 1
We consider minimax optimization problems of the forms min x∈Rd1 max y∈Rd2 f (x, y) (1) where f (x, y) is a possibly nonconvex-nonconcave function. Recent emerging applications in machine learning further stimulate a surge of interest in minimax problems. For example, generative adversarial networks (GANs) [23] can be viewed as a two-player game between a generator that produces synthetic data and a discriminator that differentiates between true and synthetic data. Other applications include reinforcement learning [9, 10, 11], robust optimization [42, 43], adversarial machine learning [54, 37], and so on. In many of these applications, f (x, y) may be stochastic, namely, f (x, y) = E[F (x, y; ξ)], which corresponds to the expected loss of some random data ξ; or f (x, y) may have the ﬁnite-sum structure, namely, f (x, y) = 1 i=1 fi(x, y), which corresponds to n the empirical loss over n data points. (cid:80)n
The most frequently used methods for solving minimax problems are the gradient descent ascent (GDA) algorithms (or their stochastic variants), with either simultaneous or alternating updates of the primal-dual variables, referred to as SGDA and AGDA, respectively. While these algorithms have received much empirical success especially in adversarial training, it is known that GDA algorithms with constant stepsizes could fail to converge even for the bilinear games [22, 40]; when they do converge, the stable limit point may not be a local Nash equilibrium [13, 38]. On the other hand,
GDA algorithms can converge linearly to the saddle point for strongly-convex-strongly-concave functions [17]. Moreover, for many simple nonconvex-nonconcave objective functions, such as, f (x, y) = x2 + 3 sin2 x sin2 y − 4y2 − 10 sin2 y, we observe that GDA algorithms with constant 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(b) Deterministic GDA (a) Surface plot of f (x, y)
Figure 1: (a) Surface plot of the nonconvex-nonconcave function f (x, y) = x2 + 3 sin2 x sin2 y − 4y2 − 10 sin2 y ; (b) Convergence of SGDA and AGDA; (c) Convergence of stochastic SGDA and stochastic AGDA; (d) Trajectories of four algorithms (c) Stochastic GDA (d) Trajectories (a) τ = 0.01 (b) τ = 0.01 (c) τ = 0.025 (d) τ = 0.025
Figure 2: Consider f (x, y) = log (1 + ex) + 3xy − log (1 + ey): (a) Convergence of AGDA and
SGDA with the stepsize τ = 0.01; (b) Trajectories of two algorithms with τ = 0.01; (c) Convergence of AGDA and SGDA with stepsize τ = 0.025; (d) Trajectories of two algorithms with τ = 0.025; stepsizes converge to the global Nash equilibrium (see Figure 1). These facts naturally raise a question: Is there a general condition under which GDA algorithms converge to the global optima?
Furthermore, the use of variance reduction techniques has played a prominent role in improving the convergence over stochastic or batch algorithms for both convex and nonconvex minimization problems [27, 52, 53, 58]. However, when it comes to the minimax problems, there are limited results, except under convex-concave setting [49, 15]. This leads to another open question: Can we improve GDA algorithms for nonconvex-nonconcave minimax problems? 1.1 Our contributions
In this paper, we address these two questions and speciﬁcally focus on the alternating gradient descent ascent, namely AGDA. This is due to several considerations. First of all, using alternating updates of
GDA is more stable than simultaneous updates [22, 2] and often converges faster in practice. Note that for a convex-concave matrix game, SGDA may diverge while AGDA is proven to always have bounded iterates [22]. See Figure 2 for a simple illustration. Secondly, AGDA is widely used for training GANs and other minimax problems in practice; see e.g., [33, 41]. Yet there is a lack of discussion on the convergence of AGDA for general minimax problems in the literature, even for the favorable strongly-convex-strongly-concave setting. Alternating updating schemes are perceived more challenging to analyze than simultaneous updates; the latter treats two variables equally and has been extensively studied in vast literature of variational inequality. Our main contributions are summarized as follows.
Two-sided PL condition.
First, we identity a general condition that relaxes the convex-concavity requirement of the objective function while still guaranteeing global convergence of AGDA and stochastic AGDA (Stoc-AGDA). We call this the two-sided PL condition, which requires that both players’ utility functions satisfy Polyak-Łojasiewicz (PL) inequality [50]. The two-sided PL condition is very general and is satisﬁed by many important classes of functions: (a) all strongly-convex-strongly-concave functions; (b) all PL-strongly-concave function (discussed in [24]) and (c) many nonconvex-nonconcave objectives. Such conditions also hold true for various applications, including robust least square, generative adversarial imitation learning for linear quadratic regulator (LQR) dynamics [5], zero-sum linear quadratic game [63], and potentially many others in adversarial learning [14], robust phase retrieval [55, 64], robust control [18], and etc. We ﬁrst investigate the landscape of objectives under the two-sided PL condition. In particular, we show that three notions of optimality: saddle point, minimax point, and stationary point are equivalent. 2
Global convergence of AGDA. We show that under the two-sided PL condition, AGDA with proper constant stepsizes converges globally to a saddle point at a linear rate of O(1 − κ−3)t, while
Stoc-AGDA with proper diminishing stepsizes converges to a saddle point at a sublinear rate of
O(κ5/t), where κ is the underlying condition number. To the best of our knowledge, this is the
ﬁrst result on the global convergence of a class of nonconvex-nonconvex problems. In contrast, most previous work deals with nonconvex-concave problems and obtains convergence to stationary points. On the other hand, because all strongly-convex-strongly-concave and PL-strongly-concave functions naturally satisfy the two-sided PL condition, our analysis ﬁlls the theoretical gap with the
ﬁrst convergence results of AGDA under these settings.
Variance reduced algorithm.
For minimax problems with the ﬁnite-sum structure, we introduce a variance-reduced AGDA algorithm (VR-AGDA) that leverages the idea of stochastic variance reduced gradient (SVRG) [27, 52] with the alternating updates. We prove that VR-AGDA achieves the complexity of O (cid:0)(n + n2/3κ3) log(1/(cid:15))(cid:1), which improves over the O (cid:0)nκ3 log 1 (cid:1) complexity of
AGDA and the O (cid:0)κ5/(cid:15)(cid:1) complexity of Stoc-AGDA when applied to ﬁnite-sum minimax problems.
Our numerical experiments further demonstrate that VR-AGDA performs signiﬁcantly better than
AGDA and Stoc-AGDA, especially for problems with large condition numbers. To our best knowl-edge, this is the ﬁrst work to provide a variance-reduced algorithm and theoretical guarantees in the nonconvex-nonconcave regime of minimax optimization. In contrast, most previous variance-reduced algorithms require full or partial strong convexity and only apply to simultaneous updates. (cid:15)
Nonconvex-PL games.
Lastly, as a side contribution, we show that for a broader class of nonconvex-nonconcave problems under only one-sided PL condition, AGDA converges to a (cid:15)-stationary point within O((cid:15)−2) iterations, thus is optimal among all ﬁrst-order algorithms. Our result shaves off a logarithmic factor of the best-known rate achieved by the multi-step GDA algorithm [47].
This directly implies the same convergence rate on nonconvex-strongly-concave objectives, and to our best knowledge, we are the ﬁrst to show the convergence of AGDA on this class of functions.
Due to page limitation, we defer this result to Appendix ??. 1.2