Abstract
Research on continual learning has led to a variety of approaches to mitigating catastrophic forgetting in feed-forward classiﬁcation networks. Until now surpris-ingly little attention has been focused on continual learning of recurrent models applied to problems like image captioning. In this paper we take a systematic look at continual learning of LSTM-based models for image captioning. We propose an attention-based approach that explicitly accommodates the transient nature of vocabularies in continual image captioning tasks – i.e. that task vocabularies are not disjoint. We call our method Recurrent Attention to Transient Tasks (RATT), and also show how to adapt continual learning approaches based on weight regulariza-tion and knowledge distillation to recurrent continual learning problems. We apply our approaches to incremental image captioning problem on two new continual learning benchmarks we deﬁne using the MS-COCO and Flickr30 datasets. Our results demonstrate that RATT is able to sequentially learn ﬁve captioning tasks while incurring no forgetting of previously learned ones. 1

Introduction
Classical supervised learning systems acquire knowledge by providing them with a set of annotated training samples from a task, which for classiﬁers is a single set of classes to learn. This view of supervised learning stands in stark contrast with how humans acquire knowledge, which is instead continual in the sense that mastering new tasks builds upon previous knowledge acquired when learning previous ones. This type of learning is referred to as continual learning (sometimes incremental or lifelong learning), and continual learning systems instead consume a sequence of tasks, each containing its own set of classes to be learned. Through a sequence of learning sessions, in which the learner has access only to labeled examples from the current task, the learning system should integrate knowledge from past and current tasks in order to accurately master them all in the end. A principal shortcoming of state-of-the-art learning systems in the continual learning regime is the phenomenon of catastrophic forgetting [10, 17]: in the absence of training samples from previous tasks, the learner is likely to forget them in the process of acquiring new ones.
Continual learning research has until now concentrated primarily on classiﬁcation problems modeled with deep, feed-forward neural networks [8, 28]. Given the importance of recurrent networks for
∗Code for experiments available here: https://github.com/delchiaro/RATT 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
many learning problems, it is surprising that continual learning of recurrent networks has received so little attention [6, 40]. A recent study on catastrophic forgetting in deep LSTM networks [35] observes that forgetting is more pronounced than in feed-forward networks. This is caused by the recurrent connections which amplify each small change in the weights. In this paper, we consider continual learning for captioning, where a recurrent network (LSTM) is used to produce the output sentence describing an image. Rather than having access to all captions jointly during training, we consider different captioning tasks which are learned in a sequential manner (examples of tasks could be captioning of sports, weddings, news, etc).
Most continual learning settings consider tasks that each contain a set of classes, and these sets are disjoint [30, 32, 37]. A key aspect of continual learning for image captioning is the fact that tasks are naturally split into overlapping vocabularies. Task vocabularies might contain nouns and some verbs which are speciﬁc to a task, however many of the words (adjectives, adverbs, and articles) are shared among tasks. Moreover, the presence of homonyms in different tasks might directly lead to forgetting of previously acquired concepts. This transient nature of words in task vocabularies makes continual learning in image captioning networks different from traditional continual learning.
In this paper we take a systematic look at continual learning for image captioning problems using recurrent, LSTM networks. We consider three of the principal classes of approaches to exemplar-free continual learning: weight-regularization approaches, exempliﬁed by Elastic Weight Consolida-tion (EWC) [17]; knowledge distillation approaches, exempliﬁed by Learning without Forgetting (LwF) [19]; and attention-based approached like Hard Attention to the Task (HAT) [37]. For each we propose modiﬁcations speciﬁc to their application to recurrent LSTM networks, in general, and more speciﬁcally to image captioning in the presence of transient task vocabularies.
The contributions of this work are threefold: (1) we propose a new framework and splitting method-ologies for modeling continual learning of sequential generation problems like image captioning; (2) we propose an approach to continual learning in recurrent networks based on transient attention masks that reﬂect the transient nature of the vocabularies underlying continual image captioning; and (3) we support our conclusions with extensive experimental evaluation on our new continual image captioning benchmarks and compare our proposed approach to continual learning baselines based on weight regularization and knowledge distillation. To the best of our knowledge we are the ﬁrst to consider continual learning of sequential models in the presence of transient tasks vocabularies whose classes may appear in some learning sessions, then disappear, only to reappear in later ones. 2