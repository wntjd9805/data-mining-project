Abstract
We consider the problem of learning the best-ﬁtting single neuron as measured by the expected square loss E(x,y)∼D[(σ(w(cid:62)x) − y)2] over some unknown joint distribution D by using gradient descent to minimize the empirical risk induced by a set of i.i.d. samples S ∼ Dn. The activation function σ is an arbitrary Lipschitz and non-decreasing function, making the optimization problem nonconvex and nonsmooth in general, and covers typical neural network activation functions and inverse link functions in the generalized linear model setting. In the agnostic PAC learning setting, where no assumption on the relationship between the labels y and the input x is made, if the optimal population risk is OPT, we show that gradient descent achieves population risk O(OPT) + ε in polynomial time and sample complexity when σ is strictly increasing. For the ReLU activation, our population risk guarantee is O(OPT1/2) + ε. When labels take the form y = σ(v(cid:62)x) + ξ for zero-mean sub-Gaussian noise ξ, we show that the population risk guarantees for gradient descent improve to OPT + ε. Our sample complexity and runtime guarantees are (almost) dimension independent, and when σ is strictly increasing, require no distributional assumptions beyond boundedness. For ReLU, we show the same results under a nondegeneracy assumption for the marginal distribution of the input. 1

Introduction
We study learning the best possible single neuron that captures the relationship between the input x ∈ Rd and the output label y ∈ R as measured by the expected square loss over some unknown but
ﬁxed distribution (x, y) ∼ D. In particular, for a given activation function σ : R → R, we deﬁne the population risk F (w) associated with a set of weights w as
F (w) := (1/2)E(x,y)∼D (cid:104)(cid:0)σ(w(cid:62)x) − y(cid:1)2(cid:105)
. (1.1)
The activation function is assumed to be non-decreasing and Lipschitz, and includes nearly all activation functions used in neural networks such as the rectiﬁed linear unit (ReLU), sigmoid, tanh, and so on. In the agnostic PAC learning setting [23], no structural assumption is made regarding the relationship of the input and the label, and so the best-ﬁtting neuron could, in the worst case, have nontrivial population risk. Concretely, if we denote v := argmin(cid:107)w(cid:107)2≤1F (w), OPT := F (v), (1.2) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
then the goal of a learning algorithm is to (efﬁciently) return weights w such that the population risk
F (w) is close to the best possible risk OPT. The agnostic learning framework stands in contrast to the realizable PAC learning setting, where one assumes OPT = 0, so that there exists some v such that the labels are given by y = σ(v(cid:62)x).
The learning algorithm we consider in this paper is empirical risk minimization using vanilla gradient i=1 ∼ Dn, and we run descent. We assume we have access to a set of i.i.d. samples {(xi, yi)}n gradient descent with a ﬁxed step size on the empirical risk (cid:98)F (w) = (1/2n) (cid:80)n i=1(σ(w(cid:62)xi) − yi)2.
A number of early neural network studies pointed out that the landscape of the empirical risk of a single neuron has unfavorable properties, such as a large number of spurious local minima [6, 3], and led researchers to instead study gradient descent on a convex surrogate loss [16, 17]. Despite this, we are able to show that gradient descent on the empirical risk itself ﬁnds weights that not only have small empirical risk but small population risk as well.
Surprisingly little is known about neural networks trained by minimizing the empirical risk with gradient descent in the agnostic PAC learning setting. We are aware of two works [2, 1] in the improper agnostic learning setting, where the goal is to return a hypothesis h ∈ H that achieves population risk close to (cid:91)OPT, where (cid:91)OPT is the smallest possible population risk achieved by a different set of hypotheses (cid:98)H. Another work considered the random features setting where only the ﬁnal layer of the network is trained and the marginal distribution over x is uniform on the unit sphere [34]. But none of these address the simplest possible neural network: that of a single neuron x (cid:55)→ σ(w(cid:62)x). We believe a full characterization of what we can (or cannot) guarantee for gradient descent in the single neuron setting will help us understand what is possible in the more complicated deep neural network setting. Indeed, two of the most common hurdles in the analysis of deep neural networks trained by gradient descent—nonconvexity and nonsmoothness—are also present in the case of the single neuron. We hope that our analysis in this relatively simple setup will be suggestive of what is possible in more complicated neural network models.
Our main contributions can be summarized as follows. 1) Agnostic setting (Theorem 3.3). Without any assumptions on the relationship between y and x, and assuming only boundedness of the marginal distributions of x and y, we show that for any ε > 0, gradient descent ﬁnds a point wt with population risk O(OPT) + ε with sample complexity O(ε−2) and runtime O(ε−1) when σ(·) is strictly increasing and Lipschitz. When σ is
ReLU, we obtain a population risk guarantee of O(OPT1/2) + ε with sample complexity O(ε−4) and runtime O(ε−2) when the marginal distribution of x satisﬁes a nondegeneracy condition (Assumption 3.2). The sample and runtime complexities are independent of the input dimension for both strictly increasing activations and ReLU. 2) Noisy teacher network setting (Theorem 4.1). When y = σ(v(cid:62)x) + ξ, where ξ|x is zero-mean and sub-Gaussian (and possibly dependent on x), we demonstrate that gradient descent ﬁnds wt satisfying F (wt) ≤ OPT + ε for activation functions that are strictly increasing and Lipschitz assuming only boundedness of the marginal distribution over x. The same result holds for ReLU under a marginal spread assumption (Assumption 3.2). The runtime and sample complexities are of order ˜O(ε−2), with a logarithmic dependence on the input dimension. When the noise is bounded, our guarantees are dimension independent. If we further know ξ ≡ 0, i.e. the learning problem is in the realizable rather than agnostic setting, we can improve the runtime and sample complexity guarantees from O(ε−2) to O(ε−1) by using online stochastic gradient descent (Theorem E.1). 2