Abstract
We study the low rank regression problem y = M x + (cid:15), where x and y are d1 and d2 dimensional vectors respectively. We consider the extreme high-dimensional setting where the number of observations n is less than d1 +d2. Existing algorithms are designed for settings where n is typically as large as rank(M )(d1 + d2). This work provides an efﬁcient algorithm which only involves two SVD, and establishes statistical guarantees on its performance. The algorithm decouples the problem by
ﬁrst estimating the precision matrix of the features, and then solving the matrix denoising problem. To complement the upper bound, we introduce new techniques for establishing lower bounds on the performance of any algorithm for this problem.
Our preliminary experiments conﬁrm that our algorithm often out-performs existing baselines, and is always at least competitive. 1

Introduction
We consider the regression problem y = M x + (cid:15) in the high dimensional setting, where x ∈ Rd1 is the vector of features, y ∈ Rd2 is a vector of responses, M ∈ Rd2×d1 are the learnable parameters, and (cid:15) ∼ N (0, σ2 (cid:15) Id2×d2 ) is a noise term. High-dimensional setting refers to the case where the number of observations n is insufﬁcient for recovery and hence regularization for estimation is necessary [26, 30, 12]. This high-dimensional model is widely used in practice, such as identifying biomarkers [48], understanding risks associated with various diseases [18, 7], image recognition [34, 17], forecasting equity returns in ﬁnancial markets [33, 39, 28, 8], and analyzing social networks [46, 35].
We consider the “large feature size” setting, in which the number of features d1 is excessively large and can be even larger than the number of observations n. This setting frequently arises in practice because it is often straightforward to perform feature-engineering and produce a large number of potentially useful features in many machine learning problems. For example, in a typical equity forecasting model, n is around 3,000 (i.e., using 10 years of market data), whereas the number of potentially relevant features can be in the order of thousands [33, 22, 25, 13]. In predicting the popularity of a user in an online social network, n is in the order of hundreds (each day is an observation and a typical dataset contains less than three years of data) whereas the feature size can easily be more than 10k [36, 6, 38].
Existing low-rank regularization techniques (e.g., [3, 23, 26, 30, 27] ) are not optimized for the large feature size setting. These results assume that either the features possess the so-called restricted isometry property [10], or their covariance matrix can be accurately estimated [30]. Therefore, their sample complexity n depends on either d1 or the smallest eigenvalue value λmin of x’s covariance matrix. For example, a mean-squared error (MSE) result that appeared in [30] is of the form
∗ Correspondence to: Qiong Wu <qwu05@email.wm.edu>.
† Currently at Google. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(cid:16) r(d1+d2) nλ2 (cid:17) min
. When n ≤ d1/λ2 min, this result becomes trivial because the forecast ˆy = 0 produces
O a comparable MSE. We design an efﬁcient algorithm for the large feature size setting. Our algorithm is a simple two-stage algorithm. Let X ∈ Rn×d1 be a matrix that stacks together all features and
Y ∈ Rn×d2 be the one that stacks the responses. In the ﬁrst stage, we run a principal component analysis (PCA) on X to obtain a set of uncorrelated features ˆZ. In the second stage, we run another
PCA to obtain a low rank approximation of ˆZTY and use it to construct an output.
While the algorithm is operationally simple, we show a powerful and generic result on using PCA to process features, a widely used practice for “dimensionality reduction” [11, 21, 19]. PCA is known to be effective to orthogonalize features by keeping only the subspace explaining large variations.
But its performance can only be analyzed under the so-called factor model [40, 39]. We show the efﬁcacy of PCA without the factor model assumption. Instead, PCA should be interpreted as a robust estimator of x’s covariance matrix. The empirical estimator C = 1 n XXT in the high-dimensional setting cannot be directly used because n (cid:28) d1 × d2, but it exhibits an interesting regularity: the leading eigenvectors of C are closer to ground truth than the remaining ones. In addition, the number of reliable eigenvectors grows as the sample size grows, so our PCA procedure projects the features along reliable eigenvectors and dynamically adjusts ˆZ’s rank to maximally utilize the raw features.
Under mild conditions on the ground-truth covariance matrix C ∗ of x, we show that it is always possible to decompose x into a set of near-independent features and a set of (discarded) features that have an inconsequential impact on a model’s MSE.
When features x are transformed into uncorrelated ones z, our original problem becomes y = N z + (cid:15), which can be reduced to a matrix denoising problem [16] and be solved by the second stage. Our algorithm guarantees that we can recover all singular vectors of N whose associated singular values are larger than a certain threshold τ . The performance guarantee can be translated into MSE bounds parametrized by commonly used variables (though, these translations usually lead to looser bounds).
For example, when N ’s rank is r, our result reduces the MSE from O( r(d1+d2) n + n−c) for a suitably small constant c. The improvement is most pronounced when n (cid:28) d1.
) to O( rd2 nλ2 min
We also provide a new matching lower bound. Our lower bound asserts that no algorithm can recover a fraction of singular vectors of N whose associated singular values are smaller than ρτ , where
ρ is a “gap parameter”. Our lower bound contribution is twofold. First, we introduce a notion of
“local minimax”, which enables us to deﬁne a lower bound parametrized by the singular values of
N . This is a stronger lower bound than those delivered by the standard minimax framework, which are often parametrized by the rank r of N [26]. Second, we develop a new probabilistic technique for establishing lower bounds under the new local minimax framework. Roughly speaking, our techniques assemble a large collection of matrices that share the same singular values of N but are far from each other, so no algorithm can successfully distinguish these matrices with identical spectra. 2 Preliminaries
T r V A r r ΣA
Notation. Let X ∈ Rn×d1 and Y ∈ Rn×d2 be data matrices with their i-th rows representing the i-th observation. For matrix A, we denote its singular value decomposition as A = U AΣA(V A)T and Pr(A) (cid:44) U A is the rank r approximation obtained by keeping the top r singular values and the corresponding singular vectors. When the context is clear, we drop the superscript A and use
U, Σ, and V (Ur, Σr, and Vr) instead. Both σi(A) and σA i are used to refer to i-th singular value of
A. We use MATLAB notation when we refer to a speciﬁc row or column, e.g., V1,: is the ﬁrst row of
V and V:,1 is the ﬁrst column. (cid:107)A(cid:107)F , (cid:107)A(cid:107)2, and (cid:107)A(cid:107)∗ are Frobenius, spectral, and nuclear norms of A. In general, we use boldface upper case (e.g., X) to denote data matrices and boldface lower case (e.g., x) to denote one sample. Regular fonts denote other matrices. Let C ∗ = IE[xxT] and
C = 1 n XTX be the empirical estimate of C ∗. Let C ∗ = V ∗Λ∗(V ∗)T be the eigen-decomposition of the matrix C ∗, and λ∗
≥ 0 be the diagonal entries of Λ∗. Let {u1, u2, . . . u(cid:96)} be an arbitrary set of column vectors, and Span({u1, u2, . . . , u(cid:96)}) be the subspace spanned by it. An event happens with high probability means that it happens with probability ≥ 1 − n−5, where 5 is an arbitrarily chosen large constant and is not optimized. 2, . . . , ≥ λ∗ d1 1 ≥ λ∗
Our model. We consider the model y = M x + (cid:15), where x ∈ Rd1 is a multivariate Gaussian, y ∈ Rd2, M ∈ Rd2×d1, and (cid:15) ∼ N (0, σ2 (cid:15) Id2×d2 ). We can relax the Gaussian assumptions on x and 2
[U, Σ, V ] = svd(X) n (Σ2); λi = Λi,i.
STEP-1-PCA-X(X) 1 2 Λ = 1 3 (cid:3) Gap thresholding. 4 (cid:3) δ = n−O(1) is a tunable parameter. 5 k1 = max{k1 : λk1 − λk1+1 ≥ δ}, 6 Λk1: diagonal matrix comprised of {λi}i≤k1. 7 Uk1, Vk1: k1 leading columns of U and V . 8 9 10 return { ˆZ+, ˆΠ}. 2 V T k1 nUk1(= X ˆΠT).
ˆΠ = (Λk1)− 1
√
ˆZ+ =
ˆZT
ˆN T
+ ← 1 n
STEP-2-PCA-DENOISE( ˆZ+, Y)
+Y. 1 2 (cid:3) Absolute value thresholding. 3 (cid:3) θ is a suitable constant; σ(cid:15) is std. of the noise. 4 k2 = max 5 (cid:110) k2 : σk2( ˆN+) ≥ θσ(cid:15) return Pk2( ˆN+) (cid:113) d2 n (cid:111)
.
ADAPTIVE-RRR(X, Y)
[ ˆZ+, ˆΠ] = STEP-1-PCA-A(X). return ˆM = Pk2 ( ˆN+) ˆΠ 2]. We are speciﬁcally interested in the setting in which d2 ≈ n ≤ d1. 1 2 Pk2( ˆN+) = STEP-2-PCA-DENOISE( ˆZ+, Y). 3
Figure 1: Our algorithm (ADAPTIVE-RRR) for solving the regression y = M x + (cid:15). (cid:15) for most results we develop. We assume a PAC learning framework, i.e., we observe a sequence
{(xi, yi)}i≤n of independent samples and our goal is to ﬁnd an ˆM that minimizes the test error
IEx,y[(cid:107) ˆM x − M x(cid:107)2
The key assumption we make to circumvent the d1 ≥ n issue is that the features are correlated. This assumption can be justiﬁed for the following reasons: (i) In practice, it is difﬁcult, if not impossible, to construct completely uncorrelated features. (ii) When n (cid:28) d1, it is not even possible to test whether the features are uncorrelated [5]. (iii) When we indeed know that the features are independent, there are signiﬁcantly simpler methods to design models. For example, we can build multiple models such that each model regresses on an individual feature of x, and then use a boosting/bagging method [19, 37] to consolidate the predictions.
The correlatedness assumption implies that the eigenvalues of C ∗ decays. The only (full rank) positive semideﬁnite matrices that have non-decaying (uniform) eigenvalues are the identity matrix (up to some scaling). In other words, when C ∗ has uniform eigenvalues, x has to be uncorrelated.
We aim to design an algorithm that works even when the decay is slow, such as when λi(C ∗) has a heavy tail. Speciﬁcally, our algorithm assumes λi’s are bounded by a heavy-tail power law series:
Assumption 2.1. The λi(C ∗) series satisﬁes λi(C ∗) ≤ c · i−ω for a constant c and ω ≥ 2.
We do not make functional form assumptions on λi’s. This assumption also covers many benign cases, such as when C ∗ has low rank or its eigenvalues decay exponentially. Many empirical studies report power law distributions of data covariance matrices [2, 31, 44, 14]. Next, we make standard normalization assumptions. IE(cid:107)x(cid:107)2 2 = 1, (cid:107)M (cid:107)2 ≤ Υ = O(1), and σ(cid:15) ≥ 1. Remark that we assume only the spectral norm of M is bounded, while its Frobenius norm can be unbounded. Also, we assume the noise σ(cid:15) ≥ 1 is sufﬁciently large, which is more important in practice. The case when
σ(cid:15) is small can be tackled in a similar fashion. Finally, our studies avoid examining excessively unrealistic cases, so we assume d1 ≤ d3 2. We examine the setting where existing algorithms fail to deliver non-trivial MSE, so we assume that n ≤ rd1 ≤ d4 2. 3 Upper bound
√
Our algorithm (see Fig. 1) consists of two steps. Step 1. Producing uncorrelated features. We run a PCA to obtain a total number of k1 orthogonalized features. See STEP-1-PCA-X in Fig. 1. Let the SVD of X be X = U Σ(V )T. Let k1 be a suitable rank chosen by inspecting the gaps of X’s singular values (Line 5 in STEP-1-PCA-X). ˆZ+ = nUk1 is the set of transformed features output by this step. The subscript + in ˆZ+ reﬂects that a dimension reduction happens so the number of columns in ˆZ+ is smaller than that in X. Compared to standard PCA dimension reduction, there are two differences: (i) We use the left leading singular vectors of X (with a re-scaling factor n) as the output, whereas the PCA reduction outputs Pk1(X). (ii) We design a specialized rule to choose k1 whereas PCA usually uses a hard thresholding or other ad-hoc rules. Step 2. Matrix denoising. We
ˆZT run a second PCA on the matrix ( ˆN+)T (cid:44) 1
+Y. The rank k2 is chosen by a hard thresholding rule n (Line 4 in STEP-2-PCA-DENOISE). Our ﬁnal estimator is Pk2 ( ˆN+) ˆΠ, where ˆΠ = (Λk1)− 1 is computed in STEP-1-PCA-X(X). 2 V T k1
√ 3
3.1
Intuition of the design
While the algorithm is operationally simple, its design is motivated by carefully unfolding the statistical structure of the problem. We shall realize that applying PCA on the features should not be viewed as removing noise from a factor model, or ﬁnding subspaces that maximize variations explained by the subspaces as suggested in the standard literature [19, 40, 41]. Instead, it implicitly implements a robust estimator for x’s precision matrix, and the design of the estimator needs to be coupled with our objective of forecasting y, thus resulting in a new way of choosing the rank.
Design motivation: warm up. We ﬁrst examine a simpliﬁed problem y = N z + (cid:15), where variables in z are assumed to be uncorrelated. Assume d = d1 = d2 in this simpliﬁed setting. Observe that 1 n
ZTY = 1 n
ZT(ZN T + E) = ( 1 n
ZTZ)N T + 1 n
ZTE ≈ Id1×d1 N T + 1 n
ZTE = N T + E, (1) where E is the noise term and E can be approximated by a matrix with independent zero-mean noises.
Solving the matrix denoising problem. Eq. 1 implies that when we compute ZTY, the problem reduces to an extensively studied matrix denoising problem [16, 20]. We include the intuition for solving this problem for completeness. The signal N T is overlaid with a noise matrix E. E will elevate (cid:112)d/n. We run a PCA to extract reliable signals: when all the singular values of N T by an order of σ(cid:15) (cid:112)d/n, the subspace contains signiﬁcantly more signal than the singular value of a subspace is (cid:29) σ(cid:15) (cid:112)d/n noise and thus we keep the subspace. Similarly, a subspace associated a singular value (cid:46) σ(cid:15) mostly contains noise. This leads to a hard thresholding algorithm that sets ˆN T = Pr(N T + E), where r is the maximum index such that σr(N T + E) ≥ c(cid:112)d/n for some constant c. In the general setting y = M x + (cid:15), x may not be uncorrelated. But when we set z = (Λ∗)− 1 2 (V ∗)Tx, we see that
IE[zzT] = I. This means knowing C ∗ sufﬁces to reduce the original problem to a simpliﬁed one.
Therefore, our algorithm uses Step 1 to estimate C ∗ and Z, and uses Step 2 to reduce the problem to a matrix denoising one and solve it by standard thresholding techniques.
Relationship between PCA and precision matrix estimation. In step 1, while we plan to estimate
C ∗, our algorithm runs a PCA on X. We observe that empirical covariance matrix C = 1 n XTX = 1 n V (Σ)2(V )T, i.e., C’s eigenvectors coincide with X’s right singular vectors. When we use the n(Σ)−1(V )Tx. When we apply this map to empirical estimator to construct ˆz, we obtain ˆz =
√ every training point and assemble the new feature matrix, we exactly get ˆZ = nU .
It means that using C to construct ˆz is the same as running a PCA in STEP-1-PCA-X with k1 = d1. nXV (Σ)−1 =
√
√
When k1 < d1, PCA uses a low rank approxima-tion of C as an estimator for C ∗. We now explain why this is effective. First, note that C is very far from C ∗ when n (cid:28) d1, therefore it is dangerous to directly plug in C to ﬁnd ˆz. Second, an interesting regularity of C exists and can be best explained by a picture. In Fig. 2, we plot the pairwise angles between eigenvectors of C and those of C ∗ from a synthetic dataset. Columns are sorted by the C ∗’s eigenvalues in decreasing order. When C ∗ and C coincide, this plot would look like an identity ma-trix. When C and C ∗ are unrelated, then the plot behaves like a block of white Gaussian noise. We observe a pronounced pattern: the angle matrix can be roughly divided into two sub-blocks (see the red lines in Fig. 2). The upper left sub-block behaves like an identity matrix, suggesting that the leading eigenvectors of C are close to those of C ∗. The lower right block behaves like a white noise matrix, suggesting that the “small” eigenvectors of C are far from those of C ∗. When n grows, one can observe the upper left block becomes larger and this the eigenvectors of C will sequentially get stabilized. Leading eigenvectors are ﬁrst stabilized, followed by smaller ones. Our algorithm leverages this regularity by keeping only a suitable number of reliable eigenvectors from C while ensuring not much information is lost when we throw away those “small” eigenvectors.
Figure 2: The angle matrix between C and C ∗.
Implementing the rank selection. We rely on three interacting building blocks: 4
i>k1 2 (V ∗)T. 2 by standard matrix perturbation results [24]. 2 , so we need additional manipulation to argue our estimate is close to (Λ∗)− 1 1. Dimension-free matrix concentration. First, we need to ﬁnd a concentration behavior of C for n ≤ d1 to decouple d1 from the MSE bound. We utilize a dimension-free matrix concentration inequality [32]. Roughly speaking, the concentration behaves as (cid:107)C − C ∗(cid:107)2 ≈ n− 1 2 . This guarantees that |λi(C) − λi(C ∗)| ≤ n− 1 2. Davis-Kahan perturbation result. However, the pairwise closeness of the λi’s does not imply the eigenvectors are also close. When λi(C ∗) and λi+1(C ∗) are close, the corresponding eigenvectors in C can be “jammed” together. Thus, we need to identify an index i, at which λi(C ∗) − λi+1(C ∗) exhibits signiﬁcant gap, and use a Davis-Kahan result to show that Pi(C) is close to Pi(C ∗). On the other hand, the map Π∗((cid:44) (Λ∗)− 1 2 (V ∗)T) we aim to ﬁnd depends on the square root of inverse (Λ∗)− 1 3. The connection between gap and tail.
Finally, the performance of our procedure is also characterized by the total volume of signals that are discarded, i.e., (cid:80)
λi(C ∗), where k1 is the location that exhibits the gap. The question becomes whether it is possible to identify a k1 that simultaneously exhibits a large gap and ensures the tail after it is well-controlled, e.g., the sum of the tail is O(n−c) for a constant c. We develop a combinatorial analysis to show that it is always possible to ﬁnd such a gap under the assumption that λi(C ∗) is bounded by a power law distribution with exponent ω ≥ 2. Combining all these three building blocks, we have:
Proposition 1. Let ξ and δ be two tunable parameters such that ξ = ω(log3 n/ n) and δ3 = ω(ξ).
Assume that λ∗ i ≤ c·i−ω. Consider running STEP-1-PCA-X in Fig. 1, with high probability, we have (i) Leading eigenvectors/values are close: there exists a unitary matrix W and a constant c1 such that (cid:107)Vk1 (Λk1)− 1
ω+1 for a constant c2.
Prop. 1 implies that our estimate ˆz+ = ˆΠ(x) is sufﬁciently close to z = Π∗(x), up to a unitary transform. We then execute STEP-2-PCA-DENOISE to reduce the problem to a matrix denoising one and solve it by hard-thresholding. Let us refer to y = N z + (cid:15), where z is a standard multivariate
Gaussian and N = M V ∗(Λ∗) 1 2 as the orthogonalized form of the problem. While we do not directly observe z, our performance is characterized by spectra structure of N .
Theorem 1. Consider running ADAPTIVE-RRR in Fig. 1 on n independent samples (x, y) from the model y = M x + (cid:15), where x ∈ Rd1 and y ∈ Rd2 . Let C ∗ = IE[xxT]. Assume that (i) (cid:107)M (cid:107)2 ≤ Υ = O(1), and (ii) x is a multivariate Gaussian with (cid:107)x(cid:107)2 = 1. In addition, λ1(C ∗) < 1 and for all i, λi(C ∗) ≤ c/iω for a constant c, and (iii) (cid:15) ∼ N (0, σ2
Let ξ = ω(log3 n/ orthogonalized form of the problem. Let (cid:96)∗ be the largest index such that σN our testing forecast. With high probability over the training data: n), δ3 = ω(ξ), and θ be a suitably large constant. Let y = N z + (cid:15) be the n . Let ˆy be (cid:15) Id1), where σ(cid:15) ≥ min{Υ, 1}.
δ3 . (ii) Small tail: (cid:80) 2 W (cid:107) ≤ c1ξ
λ∗ i ≤ c2δ 2 − V ∗ k1 (cid:96)∗ > θσ(cid:15) (Λ∗ k1 (cid:113) d2
)− 1 i≥k1
√
√
ω−1
IE[(cid:107)ˆy − y(cid:107)2 2] ≤ (σN i )2 + O (cid:88) i>(cid:96)∗ (cid:19) (cid:18) (cid:96)∗d2θ2σ2 n (cid:15)
+ O (cid:32)(cid:114) (cid:33)
ξ
δ3 (cid:16)
δ
ω−1 4(ω+1) (cid:17)
+ O (2)
The expectation is over the randomness of the test data.
Theorem 1 also implies that there exists a way to parametrize ξ and δ such that IE[(cid:107)ˆy − y(cid:107)2 (cid:80) 2] ≤
+ O(n−c0) for some constant c0. We next interpret each term in (2). i>(cid:96)∗ (σN i )2 + O (cid:16) (cid:96)∗d2θ2σ2 n (cid:17) (cid:15) (cid:16) (cid:96)∗d2θ2σ2 n (cid:15) (cid:17) i>(cid:96)∗ (σN i )2 + O
Terms (cid:80)
N T + E): we can extract signals associated with (cid:96)∗ leading singular vectors of N , so (cid:80) starts at i > (cid:96)∗. For each direction we extract, we need to pay a noise term of order θ2σ2 (cid:15) (cid:19)
+ + E(≈ i>(cid:96)∗ (σN i )2 d2 n , leading come from the estimations error of ˆz+ are typical for solving a matrix denoising problem ˆN T to the term O
. Terms O (cid:18)(cid:113) ξ
ω−1 4(ω+1)
+ O (cid:17) (cid:16) (cid:17)
δ (cid:16) (cid:96)∗d2θ2σ2 n (cid:15)
δ3
ω−1 produced from Prop. 1, consisting of both estimation errors of C ∗’s leading eigenvectors and the error of cutting out a tail. We pay an exponent of 1
ω+1 in Prop. 1 becomes
δ 4(ω+1) ) because we used Cauchy-Schwarz (CS) twice. One is used in running matrix denoising algorithm with inaccurate z+; the other one is used to bound the impact of cutting a tail. It remains open whether two CS is can be circumvented. 4 on both terms (e.g., δ
ω−1 5
Sec. 4 explains how Thm 1 and the lower bound imply the algorithm is near-optimal. Sec. 5 compares our result with existing ones under other parametrizations, e.g. rank(M ). 4 Lower bound
Figure 3: (a) Major result: signals in N are partitioned into four blocks. All signals in block 1 can be estimated (Thm 1). All signals in block 3 cannot be estimated (Prop 2). Our lower bound techniques does not handle a small tail in Block 4. A gap in block 2 exists between upper and lower bounds. (b)-(d) Constructing N: Step 1 and 2 belong to the ﬁrst stage; step 3 belongs to the second stage. (b) Step 1. Generate a random subset D(i) for each row i, representing its non-zero positions. (c) Step 2. Randomly sample from D, where D is the Cartesian product of D(i). (d) Step 3. Fill in non-zero entries sequentially from left to right. (cid:113) d2
Our algorithm accurately estimates the singular vectors of N that correspond to singular values above n . However, it may well happen that most of the spectral ‘mass’ of N lies the threshold τ = θσ(cid:15) only slightly below this threshold τ . In this section, we establish that no algorithm can do better than us, in a bi-criteria sense, i.e. we show that any algorithm that has a slightly smaller sample than ours can only minimally outperform ours in terms of MSE.
We establish ‘instance dependent’ lower bounds: When there is more ‘spectral mass’ below the threshold, the performance of our algorithm will be worse, and we will need to establish that no algorithm can do much better. This departs from the standard minimax framework, in which one examines the entire parameter space of N , e.g. all rank r matrices, and produces a large set of statistically indistinguishable ‘bad’ instances [43]. These lower bounds are not sensitive to instance-speciﬁc quantities such as the spectrum of N , and in particular, if prior knowledge suggests that the unknown parameter N is far from these bad instances, the minimax lower bound cannot be applied.
We introduce the notion of local minimax. We partition the space into parts so that similar matrices are together. Similar matrices are those N that have the same singular values and right singular vectors; we establish strong lower bounds even against algorithms that know the singular values and right singular vectors of N . An equivalent view is to assume that the algorithm has oracle access to
C ∗, M ’s singular values, and M ’s right singular vectors. This algorithm can solve the orthogonalized form as N ’s singular values and right singular vectors can easily be deduced. Thus, the only reason why the algorithm needs data is to learn the left singular vectors of N . The lower bound we establish is the minimax bound for this ‘unfair’ comparison, where the competing algorithm is given more information. In fact, this can be reduced further, i.e., even if the algorithm ‘knows’ that the left singular vectors of N are sparse, identifying the locations of the non-zero entries is the key difﬁculty that leads to the lower bound.
Deﬁnition 1 (Local minimax bound). Consider a model y = M x + (cid:15), where x is a random vector, so C ∗(x) = IE[xxT] represents the co-variance matrix of the data distribution, and M =
U M ΣM (V M )T. The relation (M, x) ∼ (M (cid:48), x(cid:48)) ⇔ (ΣM = ΣM (cid:48)
∧C ∗(x) = C ∗(x(cid:48))) is an equivalence relation and let the equivalence class of (M, x) be R(M, x) = {(M (cid:48), x(cid:48)) : ΣM (cid:48)
=
ΣM , V M (cid:48)
= V M , and C ∗(x(cid:48)) = C ∗(x)}. The local minimax bound for y = M x + (cid:15) with n independent samples and (cid:15) ∼ N (0, σ2
∧V M = V M (cid:48) (cid:15) Id2×d2 ) is (cid:104)
IEx(cid:48)[(cid:107) ˆM (X, Y)x(cid:48) − M (cid:48)x(cid:48)(cid:107)2 (cid:105) 2 | X, Y]
. (3) r(x, M, n, σ(cid:15)) = min
ˆM max (M (cid:48),x(cid:48))∈R(M,x)
E X, Y from y∼M (cid:48)x(cid:48)+(cid:15) 6
It is worth interpreting (3) in some detail. For any two (M, x), (M (cid:48), x(cid:48)) in R(M, x), the algorithm has the same ‘prior knowledge’, so it can only distinguish between the two instances by using the observed data, in particular ˆM is a function only of X and Y, and we denote it as ˆM (X, Y) to emphasize this. Thus, we can evaluate the performance of ˆM by looking at the worst possible (M (cid:48), x(cid:48)) and considering the MSE IE(cid:107) ˆM (X, Y)x(cid:48) − M (cid:48)x(cid:48)(cid:107)2.
Proposition 2. Consider the problem y = M x + (cid:15) with normalized form y = N z + (cid:15). Let ξ be a sufﬁcient small constant. There exists a sufﬁciently small constant ρ0 (that depends on ξ) and a constant c such that for any ρ ≤ ρ0, r(x, M, n, σ(cid:15)) ≥ (1 − cρ 1
, where 2 −ξ) (cid:80) (cid:16) ρ i≥t(σN i )2 − O (cid:17) 1
−ξ 2 dω−1 2 t is the smallest index such that σN t ≤ ρσ(cid:15) (cid:113) d2 n .
Proposition 2 gives the lower bound on the MSE in expectation; it can be turned into a high probability result with suitable modiﬁcations. The proof of the lower bound uses a similar ‘trick’ to the one used (cid:17) in the analysis of the upper bound analysis to cut the tail. This results in an additional term O which is generally smaller than the n−c0 tail term in Theorem 1 and does not dominate the gap. 1
−ξ 2 dω−1 2 (cid:16) ρ (cid:113) d2
Gap requirement and bi-criteria approximation algorithms. Let τ = σ(cid:15) n . Theorem 1 asserts that any signal above the threshold θτ can be detected, i.e., the MSE is at most (cid:80) i (N ) (plus inevitable noise), whereas Proposition 2 asserts that any signal below the threshold ρτ cannot be detected, i.e., the MSE is approximately at least (cid:80) i (N ). There is a ‘gap’ between θτ and ρτ , as θ > 1 and ρ < 1. See Fig. 3(a). This kind of gap is inevitable because both bounds are ‘high probability’ statements. This gap phenomenon appears naturally when the sample size is small as can be illustrated by this simple example. Consider the problem of estimating µ when we see one sample from N (µ, σ2). Roughly speaking, when µ (cid:29) σ, the estimation is feasible, and whereas µ (cid:28) σ, the estimation is impossible. For the region µ ≈ σ, algorithms fail with constant probability and we cannot prove a high probability lower bound either. i ≥ρτ (1 − poly(ρ))σ2
σN i >θτ σ2
σN
While many of the signals can ‘hide’ in the gap, the inability to detect signals in the gap is a transient phenomenon. When the number of samples n is modestly increased, our detection threshold n shrinks, and this hidden signal can be fully recovered. This observation naturally leads
τ = θσ(cid:15) to a notion of bi-criteria optimization that frequently arises in approximation algorithms. (cid:113) d2
Deﬁnition 2. An algorithm for solving the y = M x + (cid:15) problem is (α, β)-optimal if, when given an i.i.d. sample of size αn as input, it outputs an estimator whose MSE is at most β worse than the local minimax bound, i.e., IE[(cid:107)ˆy − y(cid:107)2 2] ≤ r(x, M, n, σ(cid:15)) + β. 5 2 2 −ξ)(cid:107)M x(cid:107)2 2 + O(n−c0) and β = O(ρ 1
Corollary 1. Let ξ and c0 be small constants and ρ be a tunable parameter. Our algorithm is (α, β)-optimal for α = θ2
ρ
The error term β consists of ρ 1 2 that is directly characterized by the signal strength and an additive term O(n−c0 ) = o(1). Assuming that (cid:107)M x(cid:107) = Ω(1), i.e., the signal is not too weak, the term β becomes a single multiplicative bound O(ρ 1 2. This gives an easily interpretable result. For example, when our data size is n log n, the performance gap between our algorithm and any algorithm that uses n samples is at most o((cid:107)M x(cid:107)2 2). The improvement is signiﬁcant when other baselines deliver MSE in the additive form that could be larger than (cid:107)M x(cid:107)2 2 in the regime n ≤ d1. 2 −ξ + n−c0 )(cid:107)M x(cid:107)2 2 −(cid:15)(cid:107)M x(cid:107)2
Preview of techniques. Let N = U N ΣN (V N )T be the instance (in orthogonalized form). Our goal is to construct a collection N = {N1, . . . , NK} of K matrices so that (i) For any Ni ∈ N , (ii) For any two Ni, Nj ∈ N , (cid:107)N − N (cid:48)(cid:107)F is large, and (iii)
ΣNi = ΣN and V Ni = V N .
K = exp(Ω(poly(ρ)d2)) (cf. [43, Chap. 2])
Condition (i) ensures that it sufﬁces to construct unitary matrices U Ni’s for N , and that the resulting instances will be in the same equivalence class. Conditions (ii) and (iii) resemble standard construction of codes in information theory: we need a large ‘code rate’, corresponding to requiring a large K as well as large distances between codewords, corresponding to requiring that (cid:107)Ui − Uj(cid:107)F be large.
Standard approaches for constructing such collections run into difﬁculties. Getting a sufﬁciently tight concentration bound on the distance between two random unitary matrices is difﬁcult as the matrix 7
Table 1: Summary of results for equity return forecasts (left) and average results for Twitter (right) from 10 random samples. R2 are measured by basis points (bps). 1bps = 10−4. Bold font denotes the best out-of-sample results and smallest gap. out − in denotes MSEout−in
.
Equity return
Twitter dataset
R2
Model out
ADAPTIVE-RRR 18.576 1.124 0.212 1.082 4.580 2.210 5.233
Lasso
Ridge
Reduced ridge
RRR
Nuclear norm
PCR
Sharp 1.623 0.595 0.574 1.548
-0.477
-0.370 1.280 t-stat MSEout 1.005 15.413 1.063 0.018 1.029 0.067 1.972 0.062 1.087 0.640 1.109
-0.899 1.026 0.699 out − in 0.1006 0.534 0.355 1.235 0.474 0.955 0.493 corrout 0.67 ± 0.13 0.47 ± 0.15 0.47 ± 0.17 0.49 ± 0.18 0.38 ± 0.22 0.48 ± 0.16 0.48 ± 0.15
MSEout 9.42 ± 2.31 14.82 ± 4.81 13.62 ± 4.39 12.23 ± 2.70 13.07 ± 2.63 13.05 ± 4.38 13.08 ± 4.19 out − in 4.417 12.452 12.2 27 7.708 8.731 8.668 8.889 entries, by necessity, are correlated. On the other hand, starting with a large collection of random unit vectors and using its Cartesian product to build matrices does not necessarily yield unitary matrices.
We design a two-stage approach to decouple condition (iii) from (i) and (ii) by only generating sparse matrices U Ni. See Fig. 3(b)-(d). In the ﬁrst stage (Steps 1 & 2 in Fig. 3(b)-(c)), we only specify the non-zero positions (sparsity pattern) in each U Ni. It sufﬁces to guarantee that the sparsity patterns of the matrices U Ni and U Nj have little overlap. The existence of such objects can easily be proved using the probabilistic method. Thus, in the ﬁrst stage, we can build up a large number of sparsity patterns. In the second stage (Step 3 in Fig. 3(d)), we carefully ﬁll in values in the non-zero positions for each U Ni. When the number of non-zero entries is not too small, satisfying the unitary constraint is feasible. As the overlap of sparsity patterns of any two matrices is small, we can argue the distance between them is large. By carefully trading off the number of non-zero positions and the portion of overlap, we can simultaneously satisfy all three conditions. 5