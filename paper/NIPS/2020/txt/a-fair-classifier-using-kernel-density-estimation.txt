Abstract
As machine learning becomes prevalent in a widening array of sensitive applica-tions such as job hiring and criminal justice, one critical aspect in the design of machine learning classiﬁers is to ensure fairness: Guaranteeing the irrelevancy of a prediction to sensitive attributes such as gender and race. This work develops a kernel density estimation (KDE) methodology to faithfully respect the fairness constraint while yielding a tractable optimization problem that comes with high accuracy-fairness tradeoff. One key feature of this approach is that the fairness measure quantiﬁed based on KDE can be expressed as a differentiable function w.r.t. model parameters, thereby enabling the use of prominent gradient descent to readily solve an interested optimization problem. This work focuses on classiﬁ-cation tasks and two well-known measures of group fairness: demographic parity and equalized odds. We empirically show that our algorithm achieves greater or comparable performances against prior fair classifers in accuracy-fairness tradeoff as well as in training stability on both synthetic and benchmark real datasets. 1

Introduction
During the last decade, we have witnessed an unprecedented explosion of academic and popular interests in machine learning. Machine learning is no longer just an engine behind image classiﬁers and spam ﬁlters. It is now employed to make critical decisions that affect our lives, cultures, and rights, e.g., screening job applicants, and informing bail & parole decisions. With a surge of such applications, one major criterion in the design of machine learning algorithms is to ensure fairness.
A fair classiﬁer aims at achieving the irrelevancy of a prediction to sensitive attributes such as race, sex, age, and religion. Prior works in the fairness literature have developed several metrics that capture various notions of discrimination. Three major fairness measures have been taken into consideration: (i) group fairness [8, 3, 12, 42, 41] that intends to ensure similar statistics across distinct demographics; (ii) individual fairness [7, 9, 33, 40] that targets nondiscriminatory predictions across nearby examples; (iii) causality-based fairness counterparts [20, 24, 30, 37, 45, 46, 18]. This work focuses on group fairness that has been widely explored in a variety of applications. Prominent group fairness measures include demographic parity [8, 42], equal opportunity [12], and equalized odds [12]. All of these intend to quantify how prediction outputs vary depending on sensitive attributes.
There has been a proliferation of fair classiﬁers [12, 7, 5, 1, 28, 43, 17, 23]. One challenge that arises in the prior algorithms is that they suffer from obtaining an explicit and possibly differentiable fairness measure w.r.t. model parameters and the non-differentiability often prevents the use of popular algorithms such as gradient descent. This naturally leads to a common approach: Incorporating an expressible fairness proxy as a regularization term in an interested optimization. One pioneering work along this direction [42] employs as a fairness proxy a covariance function between a sensitive attribute and a prediction. However, such a proxy-based approach may not well respect fairness constraints when it serves as a weak constraint as in [42]. A small covariance does not ensure statistical independence although the reverse is always the case. Hence, any theoretical performance 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is not guaranteed for a wide range of real datasets in which the low correlation may not necessarily ensure independence.
Contribution: To address the issue, we take a distinct approach that allows us to directly quantify fairness measures without relying on such proxy. Our methodology is based on kernel density estimation (KDE) [4] that serves to estimate a probability distribution. We emphasize three notable aspects of our KDE-based framework. The ﬁrst is that it enables a direct computation of an interested fairness measure without introducing any proxy. Second, it yields high accuracy on the distribution estimate. In the binary classiﬁer of our consideration, a moderate sample size ensures a reasonably precise estimate, in stark contrast to high-dimensional settings [34, 32, 14]; see Remark 1 for details.
Lastly, the fairness measure computed based on KDE can be expressed as a differentiable function w.r.t. model parameters, thereby enabling the use of standard gradient descent to easily solve a constrained optimization problem taking the fairness measure as a regularization term. Our extensive experiments conducted both on synthetic and benchmark real datasets (Law School Admissions [36],
Adult Census [6], Credit Card Default [6, 39], and COMPAS [2]) demonstrate that our algorithm achieves higher accuracy-fairness tradeoff relative to the states of the arts [42, 41, 44, 1, 25, 12], both w.r.t. demographic parity and equalized odds. It also exhibits an enhanced performance in training stability, compared to adversarial learning based frameworks [44, 11].