Abstract
Despite recent progress in Graph Neural Networks (GNNs), explaining predictions made by GNNs remains a challenging open problem. The leading method indepen-dently addresses the local explanations (i.e., important subgraph structure and node features) to interpret why a GNN model makes the prediction for a single instance, e.g. a node or a graph. As a result, the explanation generated is painstakingly customized for each instance. The unique explanation interpreting each instance independently is not sufﬁcient to provide a global understanding of the learned
GNN model, leading to the lack of generalizability and hindering it from being used in the inductive setting. Besides, as it is designed for explaining a single instance, it is challenging to explain a set of instances naturally (e.g., graphs of a given class). In this study, we address these key challenges and propose PGExplainer, a parameterized explainer for GNNs. PGExplainer adopts a deep neural network to parameterize the generation process of explanations, which enables PGExplainer a natural approach to explaining multiple instances collectively. Compared to the existing work, PGExplainer has better generalization ability and can be utilized in an inductive setting easily. Experiments on both synthetic and real-life datasets show highly competitive performance with up to 24.7% relative improvement in
AUC on explaining graph classiﬁcation over the leading baseline. 1

Introduction
Graph Neural Networks (GNNs) are powerful tools for representation learning of graph-structured data, such as social networks [46], document citation graphs [33], and microbiological graphs [44].
GNNs broadly adopt a message passing scheme to learn node representations by aggregating rep-resentation vectors of its neighbors [49, 16]. This scheme enables GNN to capture both node features and graph topology. GNN-based methods have achieved state-of-the-art performance in node classiﬁcation, graph classiﬁcation, and link prediction, etc [21, 45, 56].
Despite their remarkable effectiveness, the rationales of predictions made by GNNs are not easy for humans to understand. Since GNNs aggregate both node features and graph topology to make predictions, to understand predictions made by GNNs, important subgraphs and/or a set of features, which are also known as explanations, need to be uncovered. In the literature, although a variety of efforts have been undertaken to interpret general deep neural networks, existing approaches [6, 26, 12, 31, 43, 19, 20] in this line fall short in their ability to explain graph structures, which is essential for GNNs. Explaining predictions made by GNNs remains a challenging open problem, on which few methods have been proposed. The combinatorial nature of explaining graph structures makes it difﬁcult to design models that are both robust and efﬁcient. Recently, the ﬁrst general model-agnostic
∗Equal Contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: PGExplainer provides human-understandable explanations for predictions made by GNNs.
The left part shows the process of applying GNNs for graph classiﬁcation on the MUTAG dataset. A
GNN based model is trained to predict their mutagenic effects. As a post-hoc method, PGExplainer takes the trained GNN model as input and provides consistent explanations for predictions made by the GNN model. For the mutagen molecule graphs in the example, the explanation is the N O2 group. approach for GNNs, GNNExplainer [53], was proposed to address the problem. It takes a trained
GNN and its predictions as inputs to provide interpretable explanations for a given instance, e.g. a node or a graph. The explanation includes a compact subgraph structure and a small subset of node features that are crucial in GNN’s prediction for the target instance. Nevertheless, there are several limitations in the existing approach. First, GNNExplainer largely focuses on providing the local interpretability by generating a painstakingly customized explanation for a single instance individually and independently. The explanation provided by GNNExplainer is limited to the single instance, making GNNExplainer difﬁcult to be applied in the inductive setting because the explanations are hard to generalize to other unexplained nodes. As pointed out in previous studies, models interpreting each instance independently are not sufﬁcient to provide a global understanding of the trained model [19].
Furthermore, GNNExplainer has to be retrained for every single explanation. As a result, in real-life scenarios where plenty of nodes need to be interpreted, GNNExplainer would be time-consuming and impractical. Moreover, as GNNExplainer was developed for interpreting individual instances, the explanatory motifs are not learned end-to-end with a global view of the whole GNN model. Thus, it may suffer from suboptimal generalization performance. How to explain predictions of GNNs on a set of instances collectively and easily generalize the learned explainer model to other instances in the inductive setting remains largely unexplored in the literature.
To provide a global understanding of predictions made by GNNs, in this study, we emphasize the collective and inductive nature of this problem and present our method PGExplainer (Figure 1).
PGExplainer is a general explainer that applies to any GNN based models in both transductive and inductive settings. Speciﬁcally, a generative probabilistic model for graph data is utilized in
PGExplainer. Generative models have shown the power to learn succinct underlying structures from the observed graph data [23]. PGExplainer uncovers these underlying structures as the explanations, which is believed to make the most contribution to GNNs’ predictions [35]. We model the underlying structure as edge distributions, where the explanatory graph is sampled. To collectively explain predictions of multiple instances, the generation process in PGExplainer is parameterized with a deep neural network. Since the neural network parameters are shared across the population of explained instances, PGExplainer is naturally applicable to provide model-level explanations for each instance with a global view of the GNN model. Furthermore, PGExplainer has better generalization power because a trained PGExplainer model can be utilized in an inductive setting to infer explanations of unexplained nodes without retraining the explanation model. This also makes PGExplainer much faster than the existing approaches.
Experimental results on both synthetic and real-life datasets demonstrate that PGExplainer can achieve consistent and accurate explanations, bringing up to 24.7% improvement in AUC over the
SOTA method on explaining graph classiﬁcation with signiﬁcant speed-up. 2