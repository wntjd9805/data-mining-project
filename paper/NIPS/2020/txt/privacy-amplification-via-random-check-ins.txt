Abstract
Differentially Private Stochastic Gradient Descent (DP-SGD) forms a fundamen-tal building block in many applications for learning over sensitive data. Two stan-dard approaches, privacy ampliﬁcation by subsampling, and privacy ampliﬁcation by shufﬂing, permit adding lower noise in DP-SGD than via na¨ıve schemes. A key assumption in both these approaches is that the elements in the data set can be uniformly sampled, or be uniformly permuted — constraints that may become prohibitive when the data is processed in a decentralized or distributed fashion.
In this paper, we focus on conducting iterative methods like DP-SGD in the set-ting of federated learning (FL) wherein the data is distributed among many de-vices (clients). Our main contribution is the random check-in distributed protocol, which crucially relies only on randomized participation decisions made locally
It has privacy/accuracy trade-offs similar to and independently by each client. privacy ampliﬁcation by subsampling/shufﬂing. However, our method does not require server-initiated communication, or even knowledge of the population size.
To our knowledge, this is the ﬁrst privacy ampliﬁcation tailored for a distributed learning framework, and it may have broader applicability beyond FL. Along the way, we improve the privacy guarantees of ampliﬁcation by shufﬂing and show that, in practical regimes, this improvement allows for similar privacy and utility using data from an order of magnitude fewer users. 1

Introduction
Modern mobile devices and web services beneﬁt signiﬁcantly from large-scale machine learning, often involving training on user (client) data. When such data is sensitive, steps must be taken to ensure privacy, and a formal guarantee of differential privacy (DP) [16, 15] is the gold standard.
For this reason, DP has been adopted by companies including Google [20, 10, 18], Apple [2], Mi-crosoft [13], and LinkedIn [31], as well as the US Census Bureau [26].
Other privacy-enhancing techniques can be combined with DP to obtain additional beneﬁts. In par-ticular, cross-device federated learning (FL) [27] allows model training while keeping client data decentralized (each participating device keeps its own local dataset, and only sends model updates or gradients to the coordinating server). However, existing approaches to combining FL and DP make a number of assumptions that are unrealistic in real-world FL deployments such as [11]. To highlight these challenges, we must ﬁrst review the state-of-the-art in centralized DP training, where differentially private stochastic gradient descent (DP-SGD) [33, 9, 1] is ubiquitous. It achieves opti-mal error for convex problems [9], and can also be applied to non-convex problems, including deep learning, where the privacy ampliﬁcation offered by randomly subsampling data to form batches is critical for obtaining meaningful DP guarantees under computational constraints [25, 9, 1, 5, 35].
∗DeepMind. borja.balle@gmail.com
†Google. {kairouz, mcmahan, omthkkr, athakurta}@google.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Attempts to combine FL and the above lines of DP research have been made previously; notably,
[28, 3] extended the approach of [1] to FL and user-level DP. However, these works and others in the area sidestep a critical issue: the DP guarantees require very speciﬁc sampling or shufﬂing schemes assuming, for example, that each client participates in each iteration with a ﬁxed probability. While possible in theory, such schemes are incompatible with the practical constraints and design goals of cross-device FL protocols [11]; to quote [23], a comprehensive recent FL survey, “such a sampling procedure is nearly impossible in practice.”3 The fundamental challenge is that clients decide when they will be available for training and when they will check in to the server, and by design the server cannot index speciﬁc clients. In fact, it may not even know the size of the participating population.
Our work targets these challenges. Our primary goal is to provide strong central DP guarantees for the ﬁnal model released by FL-like protocols, under the assumption of a trusted4 orchestrating server.
This is accomplished by building upon recent work on ampliﬁcation by shufﬂing [19, 12, 18, 22, 6] and combining it with new analysis techniques targeting FL-speciﬁc challenges (e.g., client-initiated communications, non-addressable global population, and constrained client availability).
We propose the ﬁrst privacy ampliﬁcation analysis speciﬁcally tailored for distributed learning frameworks. At the heart of our result is a novel technique, called random check-in, that relies only on randomness independently generated by each individual client participating in the training procedure. We show that distributed learning protocols based on random check-ins can attain pri-vacy gains similar to privacy ampliﬁcation by subsampling/shufﬂing (see Table 1 for a comparison), while requiring minimal coordination from the server. While we restrict our exposition to distributed
DP-SGD within the FL framework for clarity and concreteness (see Figure 1 for a schematic of one of our protocols), we note that the techniques used in our analyses are broadly applicable to any distributed iterative method and might be of interest in other applications5. (a) (b)
Figure 1: A schematic of the Random Check-ins protocol with Fixed Windows (Section 3.1) for
Distributed DP-SGD (Algorithm 1). For the central DP guarantee, all solid arrows represent com-munication over privileged channels not accessible to any external adversary. (a) n clients perform-ing random check-ins with a ﬁxed window of m time steps. ‘X’ denotes that the client randomly chose to abstain from participating. (b) A time step at the server, where for training time i ∈ [m], the server selects a client j from those who checked-in for time i, requests an update for model θi, and then updates the model to θi+1 (or gradient accumulator if using minibatches).
Contributions The main contributions of this paper can be summarized as follows: 1. We propose random check-ins, the ﬁrst privacy ampliﬁcation technique for distributed systems with minimal server-side overhead. We also instantiate three distributed learning protocols that use random check-ins, each addressing different natural constraints that arise in applications. 2. We provide formal privacy guarantees for our protocols, and show that random check-ins attain similar rates of privacy ampliﬁcation as subsampling and shufﬂing while reducing the need for 3In cross-silo FL applications [23], an enumerated set of addressable institutions or data-silos participate in
FL, and so explicit server-mediated subsampling or shufﬂing using existing techniques may be feasible. 4Notably, our guarantees are obtained by amplifying the privacy provided by local DP randomizers; we treat this use of local DP as an implementation detail in accomplishing the primary goal of central DP. As a byproduct, our approach offers (weaker) local DP guarantees even in the presence of an untrusted server. 5In particular, the Federated Averaging [27] algorithm, which computes an update based on multiple local
SGD steps rather than a single gradient, can immediately be plugged into our framework. 2
Figure 2: Values of ε (for δ = 10−6) after ampliﬁcation by shufﬂing of ε0-DP local randomizers obtained from: The-orem 5.1 (solid lines) and [19, Theorem 7] (dotted lines).
The grey line represents the threshold of no ampliﬁcation (ε = ε0); after crossing the line ampliﬁcation bounds be-come vacuous. Observe that our bounds with n = 103 and n = 104 are similar to the bounds from [19] with n = 104 and n = 105, respectively. server-side orchestration. We also provide utility guarantees for one of our protocols in the convex case that match the optimal privacy/accuracy trade-offs for DP-SGD in the central setting [8]. 3. As a byproduct of our analysis, we improve privacy ampliﬁcation by shufﬂing [19] on two fronts.
For ε0-DP local randomizers, we improve the dependency of the ﬁnal central DP ε by O(e0.5ε0 ).
Figure 2 provides a numerical comparison of the bound from [19] with our bound; for typical parameter values this improvement allows us to provide similar privacy guarantees while reducing the number of required users by one order of magnitude. We also extend the analysis to the case of (ε0, δ0)-DP local randomizers, including Gaussian randomizers that are widely used in practice.