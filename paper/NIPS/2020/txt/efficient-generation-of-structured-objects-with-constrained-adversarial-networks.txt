Abstract
Generative Adversarial Networks (GANs) struggle to generate structured objects like molecules and game maps. The issue is that structured objects must satisfy hard requirements (e.g., molecules must be chemically valid) that are difﬁcult to acquire from examples alone. As a remedy, we propose Constrained Adversarial Networks (CANs), an extension of GANs in which the constraints are embedded into the model during training. This is achieved by penalizing the generator proportionally to the mass it allocates to invalid structures. In contrast to other generative models,
CANs support efﬁcient inference of valid structures (with high probability) and allows to turn on and off the learned constraints at inference time. CANs handle arbitrary logical constraints and leverage knowledge compilation techniques to efﬁciently evaluate the disagreement between the model and the constraints. Our setup is further extended to hybrid logical-neural constraints for capturing very complex constraints, like graph reachability. An extensive empirical analysis shows that CANs efﬁciently generate valid structures that are both high-quality and novel. 1

Introduction
Many key applications require to generate objects that satisfy hard structural constraints, like drug molecules, which must be chemically valid, and game levels, which must be playable. Despite their impressive success [1, 2, 3], Generative Adversarial Networks (GANs) [4] struggle in these applications. The reason is that data alone are often insufﬁcient to capture the structural constraints (especially if noisy) and convey them to the model.
As a remedy, we derive Constrained Adversarial Networks (CANs), which extend GANs to generating valid structures with high probabilty. Given a set of arbitrary discrete constraints, CANs achieve this by penalizing the generator for allocating mass to invalid objects during training. The penalty term is implemented using the semantic loss (SL) [5], which turns the discrete constraints into a differentiable loss function implemented as an arithmetic circuit (i.e., a polynomial). The SL is probabilistically sound, can be evaluated exactly, and supports end-to-end training. Importantly, the polynomial – which can be quite large, depending on the complexity of the constraints – can be thrown away after training. In addition, CANs handle complex constraints, like reachability on graphs, by ﬁrst embedding the candidate conﬁgurations in a space in which the constraints can be encoded compactly, and then applying the SL to the embeddings.
∗Equal contributions.
†This work was partially carried out when PM was working at the University of Trento. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Since the constraints are embedded directly into the generator, high-quality structures can be sampled efﬁciently (in time practically independent of the complexity of the constraints) with a simple forward pass on the generator, as in regular GANs.3 No costly sampling or optimization steps are needed. We additionally show how to equip CANs with the ability to switch constraints on and off dynamically during inference, at no run-time cost.
Contributions. Summarizing, we contribute: 1) CANs, an extension of GANs in which the generator is encouraged at training time to generate valid structures and support efﬁcient sampling, 2) native support for intractably complex constraints, 3) conditional CANs, an effective solution for dynami-cally turning on and off the constraints at inference time, 4) a thorough empirical study on real-world data showing that CANs generate structures that are likely valid and coherent with the training data. 2