Abstract
We consider the problem of factorizing a structured 3-way tensor into its constituent Canonical Polyadic (CP) factors. This decomposition, which can be viewed as a generalization of singular value decomposition (SVD) for tensors, reveals how the tensor dimensions (features) interact with each other. However, since the factors are a priori unknown, the corresponding optimization problems are inherently non-convex. The existing guaranteed algorithms which handle this non-convexity incur an irreducible error (bias), and only apply to cases where all factors have the same structure. To this end, we develop a provable algorithm for online structured tensor factor-ization, wherein one of the factors obeys some incoherence conditions, and the others are sparse. Speciﬁcally we show that, under some relatively mild conditions on initialization, rank, and sparsity, our algorithm recovers the factors exactly (up to scaling and permutation) at a linear rate. Comple-mentary to our theoretical results, our synthetic and real-world data eval-uations showcase superior performance compared to related techniques. 1

Introduction
Canonical Polyadic (CP) /PARAFAC decomposition aims to express a tensor as a sum of rank-1 tensors, each of which is formed by the outer-product (denoted by “◦”) of constituent factors columns. In this work, we consider the online factorization of a structured tensor 3-way tensor Z(t) ∈ Rn×J×K arriving at time t, as
Z(t) = Pm
◦ C∗(t) i = [[A∗, B∗(t), C∗(t)]], i ◦ B∗(t) i=1 A∗ (1) i and C∗(t) i , ◦B∗(t) i i are columns of factors A∗, B∗(t), and C∗(t), respectively, and are where A∗ a priori unknown. A popular choice for the batch setting (not online) is via the alternating least squares (ALS) algorithm, where appropriate regularization terms (such as ‘1 loss for sparsity) are added to the least-square objective to steer towards speciﬁc solutions [1–4].
However, these approaches suﬀer from three major issues – a) the non-convexity of associated formulations makes it challenging to establish recovery and convergence guarantees, b) one may need to solve an implicit model selection problem (e.g., choose the tensor rank m), and c) regularization may be computationally expensive, and may not scale well in practice.
Recent works for guaranteed tensor factorization – based on tensor power method [5], convex relaxations [6], sum-of-squares formulations [7–9], and variants of ALS algorithm [10] – have focused on recovery of tensor factors wherein all factors have a common structure; based on some notion of incoherence of individual factor matrices such as sparsity, incoherence, or both [11]. Furthermore, these algorithms a) incur bias in estimation, b) are computationally expensive in practice, and c) are not amenable for online (streaming) tensor factorization;
See Table 1. Consequently, there is a need to develop fast, scalable provable algorithms for exact (unbiased) factorization of structured tensors arriving (or processed) in a streaming fashion (online), generated by heterogeneously structured factors. To this end, we develop 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
C∗(t) i n
K
=
Z(t)
B∗(t) i m
P i=1 a provable algorithm to recover the unknown fac-tors of tensor(s) Z(t) in Fig.1 (arriving, or made available for sequential processing, at an instance t), assumed to be generated as (1), wherein the factor A∗ is incoherent and ﬁxed (deterministic), and the factors B∗(t) and C∗(t) are sparse and vary with t (obey some randomness assumptions).
Model Justiﬁcation: The tensor factorization task of interest arises in streaming applica-tions where users interact only with a few items at each time t, i.e. the user-item interactions are sparse. Here, the ﬁxed incoherent factor A∗ columns model the underlying ﬁxed inter-actions patterns (signatures). At time t, a fresh observation tensor Z(t) arrives, and the task is to estimate sparse factors (users and items), and the incoherent factor (patterns). This estimation procedure reveals users B∗ i and items C∗ i , i.e. the underlying clustering, and ﬁnds applications in scrolling pattern analysis in web analytics
[12], sports analytics (section 5.2), patient response to probes [13, 14], electro-dermal re-sponse to audio-visual stimuli [15, 16], and organizational behavior via email activity [1, 17].
Figure 1:Tensor Z(t) ∈ Rn×J×K of interest, a few mode-1 ﬁbers are dense. i sharing the same pattern A∗
A∗ i
J 1.1 Overview of the results
We take a matrix factorization view of the tensor factorization task to develop an online provable tensor factorization algorithm for exact recovery of the constituent factors. Lever-aging the structure, we envision the non-zero ﬁbers as being generated by a dictionary learning model, where the data samples y(j) ∈ Rn are assumed to be generated as follows from an a priori unknown dictionary A∗ ∈ Rn×m and sparse coeﬃcients x∗ (j), kx∗ (j)k0 ≤ s for all j = 1, 2, . . . y(j) = A∗x∗ (j) ∈ Rm. (2)
This modeling procedure includes a matricization or ﬂattening of the tensor, which leads to a
Kronecker (Khatri-Rao) dependence structure among the elements of the resulting coeﬃcient matrix; see section 4. As a result, the main challenges in developing recovery guarantees are to: a) analyze the Khatri Rao product (KRP) structure to identify and quantify data samples (non-zero ﬁbers) available for learning, b) establish guarantees on the resulting sparsity structure, and c) develop a SVD-based guaranteed algorithm to successfully untangle the sparse factors using corresponding coeﬃcient matrix estimate and the underlying KRP structure. Also, our matricization-based analysis can be of independent interest. 1.2 Contributions
We develop an algorithm to recover the CP factors of tensor(s) Z(t) ∈Rn×J×K, arriving (or made available) at time t, generated as per (1) from constituent factors A∗ ∈ Rn×m,
B∗(t) ∈ RJ×m, and C∗(t) ∈ RK×m, where the unit-norm columns of A∗ obey some incoher-ence assumptions, and B∗(t) and C∗(t) are sparse. Our speciﬁc contributions are:
• Exact recovery and linear convergence: Our algorithm TensorNOODL, to the best of our knowledge, is the ﬁrst to accomplish recovery of the true CP factors of this structured tensor(s) Z(t) exactly (up to scaling and permutations) at a linear rate. Speciﬁcally, starting with an appropriate initialization A(0) of A∗ , we have A(t) i →πBiB∗(t)
, and bC(t)
, as iterations t→∞, for constants πBi and πCi.
• Provable algorithm for heterogeneously-structured tensor factorization: We consider the exact tensor factorization (an inherently non-convex task) when the factors do not obey same structural assumptions. That is, our algorithmic procedure overcomes the non-convexity bottleneck suﬀered by related optimization-based ALS formulations.
• Online, fast, and scalable: The online nature of our algorithm, separability of updates due to bio-inspired neural plausibility, and relatively easy to tune parameters, make it suitable for large-scale distributed implementations. Furthermore, our numerical sim-ulations (both synthetic and real-world) demonstrate superior performance in terms of accuracy, number of iterations, demonstrating its applicability to real-world tasks. i →πCiC∗(t) i , bB(t) i →A∗ i i
Furthermore, although estimating the rank of a given tensor is NP hard, the incoherence assumption on A∗, and distributional assumptions on B∗(t) and C∗(t), ensure that our matrix factorization view is rank revealing [18].
In other words, our assumptions ensure that the dictionary initialization algorithms (such as [19]) can recover the rank of the tensor.
Following this, TensorNOODL recovers the true factors (up to scaling and permutation) whp. 2
Table 1: Comparing provable algorithms for tensor factorization and dictionary learning. As shown here, the existing provable techniques do not apply where A: incoherent, (B, C): sparse.
Method
Model
Considered
Conditions
Rank
TensorNOODL (this work) A: incoherent, (B, C): m = O(n)
Initialization
Constraints (cid:17)
O∗ (cid:16) 1 log(n)
Recovery Guarantees
Estimation Bias
Convergence
No Bias
Linear
Sun et al. [11]‡ sparse (A, B, C): all incoherent and sparse m = o(n1.5) o(1) kAij − bAijk∞ = O( 1 n0.25 )† Not established
Sharan and Valiant [10]‡ (A, B, C): all incoherent m = o(n0.25)
Anandkumar et al. [5]‡ (A, B, C): all incoherent
Arora et al. [19]
Dictionary Learning (2) m = O(n) m = o(n1.5) m = O(n) m = O(n)
Random (cid:17)¶
O∗ (cid:16) 1√ n
O(1)
O∗ (cid:16) 1
O∗ (cid:16) 1 log(n) log(n) (cid:17) (cid:17) kAi − bAik2 = O(p m n )† kAi − bAik2 = eO( 1√ n )†
√ m n )† kAi − bAik2 = eO(
O(ps/n)
Negligible bias §
Quadratic
Linear§
Linear
Linear
Linear
Mairal et al. [20]
Dictionary Learning (2)
Convergence to stationary point; similar guarantees by Huang et al. [21].
‡ This procedure is not online. † Result applies for each i ∈ [1, m]. ¶ Polynomial number of initializations mβ2 are required, for β ≥ m/n.
§ The procedure has an almost Quadratic rate initially. 1.3