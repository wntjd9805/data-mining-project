Abstract
We uncover an ever-overlooked deﬁciency in the prevailing Few-Shot Learning (FSL) methods: the pre-trained knowledge is indeed a confounder that limits the performance. This ﬁnding is rooted from our causal assumption: a Structural Causal
Model (SCM) for the causalities among the pre-trained knowledge, sample features, and labels. Thanks to it, we propose a novel FSL paradigm: Interventional Few-Shot Learning (IFSL). Speciﬁcally, we develop three effective IFSL algorithmic implementations based on the backdoor adjustment, which is essentially a causal intervention towards the SCM of many-shot learning: the upper-bound of FSL in a causal view. It is worth noting that the contribution of IFSL is orthogonal to existing ﬁne-tuning and meta-learning based FSL methods, hence IFSL can improve all of them, achieving a new 1-/5-shot state-of-the-art on miniImageNet, tieredImageNet, and cross-domain CUB. Code is released at https://github. com/yue-zhongqi/ifsl. 1

Introduction
Few-Shot Learning (FSL) — the task of training a model using very few samples — is nothing short of a panacea for any scenario that requires fast model adaptation to new tasks [64], such as minimizing the need for expensive trials in reinforcement learning [29] and saving computation resource for light-weight neural networks [26, 24]. Although we knew that, more than a decade ago, the crux of FSL is to imitate the human ability of transferring prior knowledge to new tasks [17], not until the recent advances in pre-training techniques, had we yet reached a consensus on “what & how to transfer”: a powerful neural network Ω pre-trained on a large dataset D. In fact, the prior knowledge learned from pre-training prospers today’s deep learning era, e.g., D = ImageNet, Ω = ResNet in visual recognition [23, 22]; D = Wikipedia, Ω = BERT in natural language processing [61, 15].
Ω
Ωφ
Meta-Learning
{Fine-tune(Si, Qi)}
In the context of pre-trained knowledge, we de-note the original FSL training set as support set S and the test set as query set Q, where the classes in (S, Q) are unseen (or new) in
D. Then, we can use Ω as a backbone (ﬁxed or partially trainable) for extracting sample rep-resentations x, and thus FSL can be achieved simply by ﬁne-tuning the target model on S and test it on Q [11, 16]. However, the ﬁne-tuning only exploits the D’s knowledge on “what to transfer”, but neglects “how to transfer”. Fortu-nately, the latter can be addressed by applying a post-pre-training and pre-ﬁne-tuning strategy: meta-learning [52]. Different from ﬁne-tuning whose goal is the “model” trained on S and tested on Q, meta-learning aims to learn the “meta-model”
Figure 1: The relationships among different FSL paradigms (color green and orange). Our goal is to remove the deﬁciency introduced by Pre-Training.
Fine-Tuning (S, Q)
Fine-Tuning (S, Q)
Pre-Training
D or
Ω 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Accuracy
Support Set
Query Set
“Lion”
Classiﬁed as “Dog” (due to “yellow grass”)
S ∼ Q
Average
S (cid:54)∼ Q (a)
“African Hunting Dog”
Classiﬁed as “Lion” (due to “green grass”) (b)
Figure 2: Quantitative and qualitative evidences of pre-trained knowledge misleading the ﬁne-tune FSL paradigm. (a) miniImageNet ﬁne-tuning accuracy on 1-/5-/10-shot FSL using weak and strong backbones: ResNet-10 and
WRN-28-10. S ∼ Q (or S (cid:54)∼ Q) denotes the pre-trained classiﬁer scores of the query is similar (or dissimilar) to that of the support set. “Average” is the mean of both. The dissimilarity is measured using query hardness deﬁned in Section 5.1. (b) An example of 5-shot S (cid:54)∼ Q.
— a learning behavior — trained on many learning episodes {(Si, Qi)} sampled from D and tested on the target task (S, Q). In particular, the behavior can be parametrized by φ using model parameter generator [46, 19] or initialization [18]. After meta-learning, we denote Ωφ as the new model starting point for the subsequent ﬁne-tuning on target task (S, Q). Figure 1 illustrates the relationships among the above discussed FSL paradigms.
It is arguably a common sense that the stronger the pre-trained Ω is, the better the downstream model will be. However, we surprisingly ﬁnd that this may not be always the case in FSL. As shown in Figure 2(a), we can see a paradox: though stronger Ω improves the performance on average, it indeed degrades that of samples in Q dissimilar to S. To illustrate the “dissimilar”, we show a 5-shot learning example in Figure 2(b), where the prior knowledge on “green grass” and “yellow grass” is misleading. For example, the “Lion” samples in Q have “yellow grass”, hence they are misclassiﬁed as “Dog” whose S has major “yellow grass”. If we use stronger Ω, the seen old knowledge (“grass”
& “color”) will be more robust than the unseen new knowledge (“Lion” & “Dog”), and thus the old becomes even more misleading. We believe that such a paradox reveals an unknown systematic deﬁciency in FSL, which has been however hidden for years by our gold-standard “fair” accuracy, averaged over all the random (S, Q) test trials, regardless of the similarity between S and Q (cf.
Figure 2(a)). Though Figure 2 only illustrates the ﬁne-tune FSL paradigm, the deﬁciency is expected in the meta-learning paradigm, as ﬁne-tune is also used in each meta-train episode (Figure 1). We will analyze them thoroughly in Section 5.
In this paper, we ﬁrst point out that the cause of the deﬁciency: pre-training can do evil in FSL, and then propose a novel FSL paradigm: Interventional Few-Shot Learning (IFSL), to counter the evil.
Our theory is based on the assumption of the causalities among the pre-trained knowledge, few-shot samples, and class labels. Speciﬁcally, our contributions are summarized as follows.
• We begin with a Structural Causal Model (SCM) assumption in Section 2.2, which shows that the pre-trained knowledge is essentially a confounder that causes spurious correlations between the sample features and class labels in support set. As an intuitive example in Figure 2(b), even though the “grass” feature is not the cause of the “Lion” label, the prior knowledge on “grass” still confounds the classiﬁer to learn a correlation between them.
• In Section 2.3, we illustrate a causal justiﬁcation of why the proposed IFSL fundamentally works better: it is essentially a causal approximation to many-shot learning. This motivates us to develop three effective implementations of IFSL using the backdoor adjustment [44] in Section 3.
• Thanks to the causal intervention, IFSL is naturally orthogonal to the downstream ﬁne-tuning and meta-learning based FSL methods [18, 62, 27]. In Section 5.2, IFSL improves all base-lines by a considerable margin, achieving new 1-/5-shot state-of-the-arts: 73.51%/83.21% on miniImageNet [62], 83.07%/88.69% on tieredImageNet [49], and 50.71%/64.43% on cross-domain CUB [65].
• We further diagnose the detailed performances of FSL methods across different similarities between S and Q. We ﬁnd that IFSL outperforms all baselines in every inch. 2
2 Problem Formulations 2.1 Few-Shot Learning
We are interested in a prototypical FSL: train a K-way classiﬁer on an N -shot support set S, where N is a small number of training samples per class (e.g., N =1 or 5); then test the classiﬁer on a query set
Q. As illustrated in Figure 1, we have the following two paradigms to train the classiﬁer P (y|x; θ), predicting the class y ∈ {1, ..., K} of a sample x:
Fine-Tuning. We consider the prior knowledge as the sample feature representation x, encoded by the pre-trained network Ω on dataset D. In particular, we refer x to the output of the frozen sub-part of Ω and the rest trainable sub-part of Ω (if any) can be absorbed into θ. We train the classiﬁer
P (y|x; θ) on the support set S, and then evaluate it on the query set Q in a standard supervised way.
Meta-Learning. Yet, Ω only carries prior knowledge in a way of “representation”. If the dataset D can be re-organized as the training episodes {(Si, Qi)}, each of which can be treated as a “sandbox” that has the same N -shot-K-way setting as the target (S, Q). Then, we can model the “learning behavior” from D parameterized as φ, which can be learned by the above ﬁne-tuning paradigm for each (Si, Qi). Formally, we denote Pφ(y|x; θ) as the enhanced classiﬁer equipped with the learned behavior. For example, φ can be the classiﬁer weight generator [19], distance kernel function in k-NN [62], or even θ’s initialization [18]. Considering Lφ(Si, Qi; θ) as the loss function of Pφ(y|x; θ) trained on Si and tested on Qi, we can have φ ← arg min(φ,θ) Ei [Lφ(Si, Qi; θ)], and then we ﬁx the optimized φ and ﬁne-tune for θ on S and test on Q. Please refer to Appendix 5 for the details of various ﬁne-tuning and meta-learning settings. 2.2 Structural Causal Model
From the above discussion, we can see that (φ, θ) in meta-learning and θ in ﬁne-tuning are both dependent on the pre-training. Such “dependency” can be formalized with a Structural Causal Model (SCM) [44] proposed in Figure 3(a), where the nodes denote the abstract data variables and the directed edges denote the (functional) causality, e.g., X → Y denotes that X is the cause and Y is the effect. Now we introduce the graph and the rationale behind its construction at a high-level.
Please see Section 3 for the detailed functional implementations.
D → X. We denote X as the fea-ture representation and D as the pre-trained knowledge, e.g., the dataset D and its induced model Ω. This link as-sumes that the feature X is extracted by using Ω. (c) (a) (b)
D → C ← X. We denote C as the transformed representation of X in the low-dimensional manifold, whose base is inherited from D. This as-sumption can be rationalized as fol-lows. 1) D → C: a set of data points are usually embedded in a low-dimensional manifold. This ﬁnding can be dated back to the long history of dimensionality reduction [59, 50].
Nowadays, there are theoretical [3, 8] and empirical [77, 71] evidences showing that disentangled semantic manifolds emerge during training deep networks. 2) X → C: features can be represented using (or projected onto) the manifold base linearly [60, 9] or non-linearly [6]. In particular, as later discussed in Section 3, we explicitly implement the base as feature dimensions (Figure 3(b)) and class-speciﬁc mean features (Figure 3(c)).
Figure 3: (a) Causal Graph for FSL; (b) Feature-wise illustration of
D → C: Feature channels of pre-trained network(e.g.1 . . . 512 for
ResNet-10). X → C: Per-channel response to an image (“school bus”) visualized by CAM[77]; (c) Class-wise illustration for D →
C: features are clustered according to the pre-training semantic classes (colored t-SNE plot[37]). X → C: An image (“school bus”) can be represented in terms of the similarities among the base classes (“ashcan”, “unicycle”, “sign”).
X → Y ← C. We denote Y as the classiﬁcation effect (e.g., logits), which is determined by X via two ways: 1) the direct X → Y and 2) the mediation X → C → Y . In particular, the ﬁrst way can be removed if X can be fully represented by C (e.g., feature-wise adjustment in Section 3). The second way is inevitable even if the classiﬁer does not take C as an explicit input, because any X can be 3
inherently represented by C. To illustrate, suppose that X is a linear combination of two base vectors plus a noise residual: x = c1b1 + c2b2 + e, any classiﬁer f (x) = f (c1b1 + c2b2 + e) will implicitly exploit the C representation in terms of b1 and b2. In fact, this assumption also fundamentally validates unsupervised representation learning [5]. To see this, if C (cid:54)→ Y in Figure 3(a), uncovering the latent knowledge representation from P (Y |X) would be impossible, because the only path left that transfers knowledge from D to Y : D → X → Y , is cut off by conditioning on X: D (cid:54)→ X → Y .
An ideal FSL model should capture the true causality between X and Y to generalize to unseen samples. For example, as illustrated in Figure 2(b), we expect that the “Lion” prediction is caused by the “lion” feature per se, but not the background “grass”. However, from the SCM in Figure 3(a), the conventional correlation P (Y |X) fails to do so, because the increased likelihood of Y given X is not only due to “X causes Y” via X → Y and X → C → Y , but also the spurious correlation via 1)
D → X, e.g., the “grass” knowledge generates the “grass” feature, and 2) D → C → Y , e.g., the
“grass” knowledge generates the “grass” semantic, which provides useful context for “Lion” label.
Therefore, to pursue the true causality between X and Y , we need to use the causal intervention
P (Y |do(X)) [45] instead of the likelihood P (Y |X) for the FSL objective. 2.3 Causal Intervention via Backdoor Adjustment
By now, an astute reader may notice that the causal graph in Figure 3(a) is also valid for Many-Shot
Learning (MSL), i.e., conventional learning based on pre-training. Compared to FSL, the P (Y |X) estimation of MSL is much more robust. For example, on miniImageNet, a 5-way-550-shot ﬁne-tuned classiﬁer can achieve 95% accuracy, while a 5-way-5-shot one only obtains 79%. We used to blame
FSL for insufﬁcient data by the law of large numbers in point estimation [14]. However, it does not answer why MSL converges to the true causal effects as the number of samples increases inﬁnitely.
In other words, why P (Y |do(X)) ≈ P (Y |X) in MSL while P (Y |do(X)) (cid:54)≈ P (Y |X) in FSL?
To answer the question, we need to incorporate the endogenous feature sampling x ∼ P (X|I) into the estimation of P (Y |X), where I denotes the sample ID. We have P (Y |X = xi) :=
Ex∼P (X|I)P (Y |X = x, I = i) = P (Y |I), i.e., we can use P (Y |I) to estimate P (Y |X). In Fig-ure 4(a), the causal relation between I and X is purely I → X, i.e., X → I does not exist, because tracing the X’s ID out of many-shot samples is like to ﬁnd a needle in a haystack, given the nature that a DNN feature is an abstract and diversity-reduced representation of many samples [21]. However, as shown in Figure 4(b), X → I persists in FSL, because it is much easier for a model to “guess” the correspondence, e.g., the 1-shot extreme case that has a trivial 1-to-1 mapping for X ↔ I. Therefore, as we formally show in Appendix 1, the key causal difference between MSL and FSL is: MSL essentially makes I an instrumental variable [1] that achieves P (Y |X) := P (Y |I) ≈ P (Y |do(X)).
Intuitively, we can see that all the causalities between I and D in MSL are all blocked by col-liders1, making I and D independent. So, the feature X is essentially “intervened” by I, no longer dictated by D, e.g., neither “yellow grass” nor “green grass” dominates “Lion” in Fig-ure 2(b), mimicking the casual intervention by controlling the use of pre-trained knowledge.
In this paper, we propose to use the backdoor adjustment [44] to achieve
P (Y |do(X)) without the need for many-shot, which certainly under-mines the deﬁnition of FSL. The back-door adjustment assumes that we can observe and stratify the confounder, i.e., D = {d}, where each d is a strat-iﬁcation of the pre-trained knowledge.
Formally, as shown in Appendix 2, the backdoor adjustment for the graph in
Figure 3(a) is:
P (Y |do(X = x)) = (cid:88) d (a) (b) (c)
Figure 4: Causal graphs with sampling process. (a) Many-Shot
Learning, where P (Y |X) ≈ P (Y |do(X)); (b) Few-Shot Learn-ing where P (Y |X) (cid:54)≈ P (Y |do(X); (c) Interventional Few-Shot
Learning where we directly model P (Y |do(X)).
P (Y |X = x, D = d, C = g(x, d)) P (D = d), (1) 1In causal graph, the junction A → B ← C is called a “collider”, making A and C independent even though
A and C are linked via B [44]. For example, A = “Quality”, C = “Luck”, and B = “Paper Acceptance”. 4
where g is a function deﬁned later. However, it is not trivial to instantiate d, especially when D is a 3rd-party delivered pre-trained network where the dataset is unobserved [20]. Next, we will offer three practical implementations of Eq. (1) for Interventional FSL. 3
Interventional Few-Shot Learning
Our implementation idea is inspired from the two inherent properties of any pre-trained DNN. First, each feature dimension carries a semantic meaning, e.g., every channel in convolutional neural network is well-known to encode visual concepts [77, 71]. So, each feature dimension represents a piece of knowledge. Second, most prevailing pre-trained models use a classiﬁcation task as the objective, such as the 1,000-way classiﬁer of ResNet [23] and the token predictor of BERT [15].
Therefore, the classiﬁer can be considered as the distilled knowledge, which has been already widely adopted in literature [24]. Next, we will detail the proposed Interventional FSL (IFSL) by providing three different implementations2 for g(x, d), P (Y |X, D, C), and P (D) in Eq. (1). In particular, the exact forms of P (Y |·) across different classiﬁers are given in Appendix 5.
Feature-wise Adjustment. Suppose that F is the index set of the feature dimensions of x, e.g., from the last-layer of the pre-trained network Ω. We divide F into n equal-size disjoint subsets, e.g., the output feature dimension of ResNet-10 is 512, if n = 8, the i-th set will be a feature dimension index set of size 512/8 = 64, i.e., Fi = {64(i − 1) + 1, ..., 64i}. The stratum set of pre-trained knowledge is deﬁned as D := {d1, . . . , dn}, where each di = Fi. (i) g(x, di) := {k|k ∈ Fi ∩ It}, where It is an index set whose corresponding absolute values in x are larger than the threshold t. The reason is simple: if a feature dimension is inactive in x, its corresponding adjustment can be omitted. We set t=1e-3 in this paper. (ii) P (Y |X, D, C) = P (Y |[x]c), where c = g(x, di) is implemented as the index set deﬁned above,
[x]c = {xk}k∈c is a feature selector which selects the dimensions of x according to the index set c.
The classiﬁer takes the adjusted feature [x]c as input. Note that d is already absorbed in c, so [x]c is essentially a function of (X, D, C). (iii) P (di) = 1/n, where we assume a uniform prior for the adjusted features. (iv) The overall feature-wise adjustment is:
P (Y |do(X = x)) = 1 n n (cid:88) i=1
P (Y |[x]c), where c = {k|k ∈ Fi ∩ It}. (2)
It is worth noting that the feature-wise adjustment is always applicable, as we can always have the feature representation x from the pre-trained network. Interestingly, our feature-wise adjustment sheds some light on the theoretical justiﬁcations for the multi-head trick in transformers [61]. We will explore this in future work.
Class-wise Adjustment. Suppose that there are m pre-training classes, denoted as A = {a1, . . . am}.
In class-wise adjustment, each stratum of pre-trained knowledge is deﬁned as a pre-training class, i.e.,
D := {d1, . . . , dm} and each di = ai. (i) g(x, di) := P (ai|x)¯xi, where P (ai|x) is the pre-trained classiﬁer’s probability output that x belongs to class ai, and ¯xi is the mean feature of pre-training samples from class ai. Note that unlike feature-wise adjustment where c is an index set, here c = g(x, di) is implemented as a real vector. (ii) P (Y |X, D, C) = P (Y |x ⊕ g(x, di)), where ⊕ denotes vector concatenation. (iii) P (di) = 1/m, where we assume a uniform prior of each class. (iv) The overall class-wise adjustment is:
P (Y |do(X = x)) = 1 m m (cid:88) i=1
P (Y |x ⊕ P (ai|x)¯xi) ≈ P (Y |x ⊕ 1 m m (cid:88) i=1
P (ai|x)¯xi) , (3) 2We assume that the combinations of the feature dimensions or classes are linear, otherwise the adjustment requires prohibitive O(2n) sampling. We will relax this assumption in future work. 5
where we adopt the Normalized Weighted Geometric Mean (NWGM) [66, 67] approximation to move the outer sum (cid:80) P into the inner P ((cid:80)). This greatly reduces the network forward-pass consumption as m is usually large in pre-training dataset. Please refer to Appendix 3 for the detailed derivation.
Combined Adjustment. We can combine feature-wise and class-wise adjustment to make the stratiﬁcation in backdoor adjustment much more ﬁne-grained. Our combination is simple: applying feature-wise adjustment after class-wise adjustment. Thus, we have:
P (Y |do(X = x)) ≈ 1 n n (cid:88) i=1
P (Y |[x]c ⊕ 1 m m (cid:88)
[P (aj|x)¯xj]c), where c = {k|k ∈ Fi ∩ It}. (4) j=1 4