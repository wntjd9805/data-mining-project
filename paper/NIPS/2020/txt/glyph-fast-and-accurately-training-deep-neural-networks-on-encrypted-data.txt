Abstract
Because of the lack of expertise, to gain beneﬁts from their data, average users have to upload their private data to cloud servers they may not trust. Due to legal or privacy constraints, most users are willing to contribute only their encrypted data, and lack interests or resources to join deep neural network (DNN) training in cloud.
To train a DNN on encrypted data in a completely non-interactive way, a recent work proposes a fully homomorphic encryption (FHE)-based technique implementing all activations by Brakerski-Gentry-Vaikuntanathan (BGV)-based lookup tables.
However, such inefﬁcient lookup-table-based activations signiﬁcantly prolong private training latency of DNNs.
In this paper, we propose, Glyph, a FHE-based technique to fast and accurately train DNNs on encrypted data by switching between TFHE (Fast Fully Homo-morphic Encryption over the Torus) and BGV cryptosystems. Glyph uses logic-operation-friendly TFHE to implement nonlinear activations, while adopts vectorial-arithmetic-friendly BGV to perform multiply-accumulations (MACs). Glyph fur-ther applies transfer learning on DNN training to improve test accuracy and reduce the number of MACs between ciphertext and ciphertext in convolutional layers.
Our experimental results show Glyph obtains state-of-the-art accuracy, and re-duces training latency by 69% ∼ 99% over prior FHE-based privacy-preserving techniques on encrypted datasets. 1

Introduction
Deep learning is one of the most dominant approaches to solving a wide variety of problems such as computer vision and natural language processing [1], because of its state-of-the-art accuracy. By only sufﬁcient data, DNN weights can be trained to achieve high enough accuracy. Average users typically lack knowledge and expertise to build their own DNN models to harvest beneﬁts from their own data, so they have to depend on big data companies such as Google, Amazon and Microsoft. However, due to legal or privacy constraints, there are many scenarios where the data required by DNN training is extremely sensitive. It is risky to provide personal information, e.g., ﬁnancial or healthcare records, to untrusted companies to train DNNs. Federal privacy regulations also restrict the availability and sharing of sensitive data.
Recent works [2, 3, 4] propose cryptographic schemes to enable privacy-preserving training of DNNs.
Private federated learning [4] (FL) is created to decentralize DNN training and enable users to train with their own data locally. QUOTIENT [3] takes advantage of multi-party computation (MPC) to interactively train DNNs on both servers and clients. Both FL and MPC require users to stay online and heavily involve in DNN training. However, in some cases, average users may not have strong interest, powerful hardware, or fast network connections for interactive DNN training [5]. To enable
DNN training on encrypted data in a completely non-interactive way, a recent study presents the ﬁrst fully homomorphic encryption (FHE)-based stochastic gradient descent technique [2], FHESGD. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
During FHESGD, a user encrypts and uploads private data to an untrusted server that performs both forward and backward propagations on the encrypted data without decryption. After uploading encrypted data, users can simply go ofﬂine. Privacy is preserved during DNN training, since input and output data, activations, losses and gradients are all encrypted.
However, FHESGD [2] is seriously limited by its long training latency, because of its BGV-lookup-table-based sigmoid activations. Speciﬁcally, FHESGD builds a Multi-Layer Perceptron (MLP) with 3 layers to achieve < 98% test accuracy on an encrypted MNIST after 50 epochs. A mini-batch including 60 samples takes ∼ 2 hours on a 16-core CPU. FHESGD uses the BGV cryptosystem [6] to implement stochastic gradient descent, because BGV is good at performing large vectorial arithmetic operations frequently used in a MLP. However, FHESGD replaces all activations of a MLP by sigmoid functions, and uses BGV table lookups [7] to implement a sigmoid function. A BGV table lookup in the setting of FHESGD is so slow that BGV-lookup-table-based sigmoid activations consume ∼ 98% of the training time.
In this paper, we propose a FHE-based technique, Glyph, to enable fast and accurate training over encrypted data. Glyph adopts the logic-operation-friendly TFHE cryptosystem [8] to implement activations such as ReLU and softmax in DNN training. TFHE-based activations have shorter latency.
We present a cryptosystem switching technique to enable Glyph to perform activations by TFHE and switch to the vectorial-arithmetic-friendly BGV when processing fully-connected and convolutional layers. By switching between TFHE and BGV, Glyph substantially improves the speed of privacy-preserving DNN training on encrypted data. At last, we apply transfer learning on Glyph to not only accelerate private DNN training but also improve its test accuracy. Glyph achieves state-of-the-art accuracy, and reduces training latency by 69% ∼ 99% over prior FHE-based privacy-preserving techniques on encrypted datasets. 2