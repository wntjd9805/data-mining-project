Abstract
The task of maximizing a monotone submodular function under a cardinality constraint is at the core of many machine learning and data mining applications, including data summarization, sparse regression and coverage problems. We study this classic problem in the fully dynamic setting, where elements can be both inserted and removed. Our main result is a randomized algorithm that maintains an efﬁcient data structure with a poly-logarithmic amortized update time and yields a (1/2 − (cid:15))-approximate solution. We complement our theoretical analysis with an empirical study of the performance of our algorithm. 1

Introduction
Thanks to the ubiquitous nature of “diminishing returns” functions, submodular optimization has established itself as a central topic in machine learning, with a myriad of applications ranging from active learning [GK11] to sparse reconstruction [Bac10, DDK12, DK11], video analysis [ZJCP14] and data summarization [BIRB15]. In this ﬁeld, the problem of maximizing a monotone submodular function under a cardinality constraint is perhaps the most central. Despite its generality, the problem can be (approximately) solved using a simple and efﬁcient greedy algorithm [NWF78].
However, this classic algorithm is inefﬁcient when applied on modern large datasets. To overcome this limitation, in recent years there has been much interest in designing efﬁcient streaming [BMKK14,
CK14, BFS15, FKK18, NTM+18] and distributed algorithms [MZ15, MKBK15, BENW16, ENV19] for submodular maximization.
Although those algorithms have found numerous applications, they are not well-suited for the common applications where data is highly dynamic. In fact, real-world systems often need to handle evolving datasets, where elements are added and deleted continuously. For example, in a recent study [DJR12],
Dey et al. crawled two snapshots of 1.4 million New York City Facebook users several months apart and reported that 52% of them had changed their proﬁle privacy settings signiﬁcantly during this period. Similarly, Snapchat processes several million picture uploads and deletions daily; Twitter processes several million tweet uploads and deletions daily. As one must still be able to run basic machine learning tasks, such as sparse recovery or data summarization, in such highly dynamic settings, we need fully dynamic algorithms: ones able to efﬁciently handle a stream containing not only insertions, but also an arbitrary number of deletions, with small processing time per update.
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The general dynamic setting is classic and a staple of algorithm design, with many applications in machine learning systems. However, for many problems it is notoriously difﬁcult to obtain efﬁcient algorithms in this model. In the case of submodular maximization, algorithms have been proposed for the specialized settings of sliding windows [CNZ16, ELVZ17] and robustness [MBN+17, KZK18].
However, as we discuss below, these solutions cannot handle the full generality of the described real-world scenarios.
Our contribution. maximization under a cardinality constraint. Our algorithm:
In this paper we design an efﬁcient fully dynamic algorithm for submodular
• takes as input a sequence of arbitrarily interleaved insertions and deletions,
• after each such update, it continuously maintains a solution whose value is in expectation at least (1/2 − (cid:15)) times the optimum of the underlying dataset at the current time,
• has amortized time per update that is poly-logarithmic in the length of the stream.
This result settles the status of submodular maximization as tractable in the dynamic setting. We also empirically validate the efﬁciency of our algorithm in several applications.