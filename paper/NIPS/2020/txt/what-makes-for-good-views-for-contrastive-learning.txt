Abstract
Contrastive learning between multiple views of the data has recently achieved state of the art performance in the ﬁeld of self-supervised representation learning.
Despite its success, the inﬂuence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the impor-tance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmenta-tion as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classiﬁcation accuracy. As a by-product, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classiﬁcation (73% top-1 linear readout with a ResNet-50)1. 1

Introduction
It is commonsense that how you look at an object does not change its identity. Nonetheless, Jorge
Luis Borges imagined the alternative. In his short story on Funes the Memorious, the titular character becomes bothered that a “dog at three fourteen (seen from the side) should have the same name as the dog at three ﬁfteen (seen from the front)" [6]. The curse of Funes is that he has a perfect memory, and every new way he looks at the world reveals a percept minutely distinct from anything he has seen before. He cannot collate the disparate experiences.
Most of us, fortunately, do not suffer from this curse. We build mental representations of identity that discard nuisances like time of day and viewing angle. The ability to build up view-invariant representations is central to a rich body of research on multiview learning. These methods seek representations of the world that are invariant to a family of viewing conditions. Currently, a popular paradigm is contrastive multiview learning, where two views of the same scene are brought together in representation space, and two views of different scenes are pushed apart.
This is a natural and powerful idea but it leaves open an important question: “which viewing conditions should we be invariant to?" It’s possible to go too far: if our task is to classify the time of day then we certainly should not use a representation that is invariant to time. Or, like Funes, we could go not far enough: representing each speciﬁc viewing angle independently would cripple our ability to track a dog as it moves about a scene.
We therefore seek representations with enough invariance to be robust to inconsequential variations but not so much as to discard information required by downstream tasks. In contrastive learning, 1Project page: http://hobbitlong.github.io/InfoMin 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
the choice of “views" is what controls the information the representation captures, as the framework results in representations that focus on the shared information between views [42]. Views are commonly different sensory signals, like photos and sounds [3], or different image channels [53] or slices in time [55], but may also be different “augmented" versions of the same data tensor [7]. If the shared information is small, then the learned representation can discard more information about the input and achieve a greater degree of invariance against nuisance variables. How can we ﬁnd the right balance of views that share just the information we need, no more and no less?
We investigate this question in two ways: 1) we demonstrate that the optimal choice of views depends critically on the downstream task. If you know the task, it is often possible to design effective views. 2) We empirically demonstrate that for many common ways of generating views, there is a sweet spot in terms of downstream performance where the mutual information (MI) between views is neither too high nor too low.
Our analysis suggests an “InfoMin principle". A good set of views are those that share the minimal information necessary to perform well at the downstream task. This idea is related to the idea of minimal sufﬁcient statistics [48] and the Information Bottleneck theory [54, 2], which have been previously articulated in the representation learning literature. This principle also complements the already popular “InfoMax principle" [34] , which states that a goal in representation learning is to capture as much information as possible about the stimulus. We argue that maximizing information is only useful in so far as that information is task-relevant. Beyond that point, learning representations that throw out information about nuisance variables is preferable as it can improve generalization and decrease sample complexity on downstream tasks [48].
Based on our ﬁndings, we also introduce a semi-supervised method to learn views that are effective for learning good representations when the downstream task is known. We additionally demonstrate that the InfoMin principle can be practically applied by simply seeking stronger data augmentation to further reduce mutual information toward the sweet spot. This effort results in state of the art accuracy on a standard benchmark.
Our contributions include:
• Demonstrating that optimal views for contrastive representation learning are task-dependent.
• Empirically ﬁnding a U-shaped relationship between an estimate of mutual information and representation quality in a variety of settings.
• A new semi-supervised method to learn effective views for a given task.
• Applying our understanding to achieve state of the art accuracy of 73.0% on the ImageNet linear readout benchmark with a ResNet-50. 2