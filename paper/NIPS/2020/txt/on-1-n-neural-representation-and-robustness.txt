Abstract
Understanding the nature of representation in neural networks is a goal shared by neuroscience and machine learning. It is therefore exciting that both ﬁelds converge not only on shared questions but also on similar approaches. A pressing question in these areas is understanding how the structure of the representation used by neural networks affects both their generalization, and robustness to perturbations.
In this work, we investigate the latter by juxtaposing experimental results regarding the covariance spectrum of neural representations in the mouse V1 (Stringer et al) with artiﬁcial neural networks. We use adversarial robustness to probe Stringer et al’s theory regarding the causal role of a 1/n covariance spectrum. We empiri-cally investigate the beneﬁts such a neural code confers in neural networks, and illuminate its role in multi-layer architectures. Our results show that imposing the experimentally observed structure on artiﬁcial neural networks makes them more robust to adversarial attacks. Moreover, our ﬁndings complement the existing theory relating wide neural networks to kernel methods, by showing the role of intermediate representations. 1

Introduction
Artiﬁcial neural networks and theoretical neuroscience have a shared ancestry of models they use and develop, this includes the McCulloch-Pitts model [33], Boltzmann machines [1] and convolutional neural networks [16, 31]. The relation between the disciplines, however, goes beyond the use of cognate mathematical models and includes a diverse set of shared interests – importantly, the overlap in interests increased as more theoretical questions came to the fore in deep learning. As such, the two disciplines have settled on similar questions about the nature of ‘representations’ or neural codes: how they develop during learning; how they enable generalization to new data and new tasks; their dimensionality and embedding structure; what role attention plays in their modulation; how their properties guard against illusions and adversarial examples.
⇤equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Central to all these questions, is the exact nature of representations emergent in both artiﬁcial and biological neural networks. Even though relatively little is known about either, the known differences between both offer a point of comparison, that can potentially give us deeper insight into the properties of different neural codes, and their mechanistic role in giving rise to some of the observed properties. Perhaps the most prominent example of the difference between artiﬁcial and biological neural networks is the existence of adversarial examples [21, 23, 25, 44]– arguably they are of interest primarily not because of their genericity [14, 25], but because they expose the stark difference between computer vision algorithms and human perception.
Figure 1: Studying the beneﬁts of the spectrum of neural code for adversarial robustness. The neural code of the biological brain shows 1/n power-law spectrum and also robust. Meaningful manifolds in the input space can gain nonlinear features or lose their structure depending on the power-law exponent ↵. Using artiﬁcial neural networks and statistical whitening, we investigate how the “dimensionality”, controlled by ↵, of the neural code impacts its robustness.
In this work, we use adversarial robustness to probe ideas regarding the ‘dimensionality’ of neural representations. The neuroscience community has advanced several normative theories, which include optimal coding [4], sparse coding [17, 35], as well a host of experimental data and statistical models [8, 18, 20, 22, 42] – resulting in, often conﬂicting, arguments in support of the prevalence of both low-dimensional and high-dimensional neural codes. By comparison, the machine learning community inspected the properties of hidden unit representations through the lens of statistical learning theory [15, 34, 47], information theory [40, 46], and mean-ﬁeld and kernel methods [12, 27, 36]. The last two, by considering the limiting behavior as the number of neurons per layer goes to inﬁnity, have been particularly successful, allowing analytical treatment of optimization, and generalization [2, 6, 7, 27].
Paralleling this development, a recent study recorded from a large number of mouse early visual area (V1) neurons [43]. The statistical analysis of the data leveraged kernel methods, much like the mean-ﬁeld methods mentioned above, and has revealed that the covariance between neurons (marginalized over input) had a spectrum that decayed as a 1/n power-law regardless of the input image statistics. To provide a potential rationale for the representation to be poised between low and high dimensionality, the authors developed a corresponding theory that relates the spectrum of the neural repertoire to the continuity and mean-square differentiability of a manifold in the input space [43]. Even with this proposed theory, the mechanistic role of the 1/n neural code is not known, but Stringer et al. [43] conjectured that it strikes a balance between expressivity and robustness.
Moreover, the proposed theory only investigated the relationship between the input and output of the neural network; as many neural networks involve multiple layers, it is not clear how the neural code used by the intermediate layers affects the network as a whole. Similarly existing literature that relates the spectra of kernels to their out-of-sample generalization implicitly treats multi-layer 2
architectures as shallow ones [5, 6, 10]. It is therefore desirable that a comprehensive theory ought to explain the role that the covariance spectrum plays at each layer of a neural network.
In this work, we empirically investigate the advantages of an 1/n neural code, by enforcing the spectral properties of the biological visual system in artiﬁcial neural networks. To this end, we propose a spectral regularizer to enforce a 1/n eigenspectrum.
With these spectrally-regularized models in hand, we aim to answer the following questions:
•
•
Does having an 1/n neural code make the network more robust to adversarial attacks?
For multi-layer networks, how does the neural code employed by the intermediate layers affect the robustness of the network?
The paper is organized as follows: we ﬁrst provide a brief review of the empirical and theoretical results of Stringer et al. [43], followed by background information on deep neural networks. We then propose a spectral regularization technique and employ it in a number of empirical experiments to investigate the role of the 1/n neural code. 2