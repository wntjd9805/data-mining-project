Abstract
Imitation learning trains a policy by mimicking expert demonstrations. Various imitation methods were proposed and empirically evaluated, meanwhile, their theoretical understanding needs further studies. In this paper, we ﬁrstly analyze the value gap between the expert policy and imitated policies by two imitation methods, behavioral cloning and generative adversarial imitation. The results support that generative adversarial imitation can reduce the compounding errors compared to behavioral cloning, and thus has a better sample complexity. Noticed that by considering the environment transition model as a dual agent, imitation learning can also be used to learn the environment model. Therefore, based on the bounds of imitating policies, we further analyze the performance of imitating environments. The results show that environment models can be more effectively imitated by generative adversarial imitation than behavioral cloning, suggesting a novel application of adversarial imitation for model-based reinforcement learning.
We hope these results could inspire future advances in imitation learning and model-based reinforcement learning. 1

Introduction
Sequential decision-making under uncertainty is challenging due to the stochastic dynamics and delayed feedback [27, 8]. Compared to reinforcement learning (RL) [46, 38] that learns from delayed feedback, imitation learning (IL) [37, 34, 23] learns from expert demonstrations that provide immediate feedback and thus is efﬁcient in obtaining a good policy, which has been demonstrated in playing games [45], robotic control [19], autonomous driving [14], etc.
Imitation learning methods have been designed from various perspectives. For instance, behavioral cloning (BC) [37, 50] learns a policy by directly minimizing the action probability discrepancy with supervised learning; apprenticeship learning (AL) [1, 47] infers a reward function from expert demon-strations by inverse reinforcement learning [34], and subsequently learns a policy by reinforcement learning using the recovered reward function. Recently, Ho and Ermon [23] revealed that AL can be viewed as a state-action occupancy measure matching problem. From this connection, they proposed the algorithm generative adversarial imitation learning (GAIL). In GAIL, a discriminator scores the agent’s behaviors based on the similarity to the expert demonstrations, and the agent learns to maximize the scores, resulting in expert-like behaviors.
Many empirical studies of imitation learning have been conducted. It has been observed that, for example, GAIL often achieves better performance than BC [23, 28, 29]. However, the theoretical
∗This work is supported by National Key R&D Program of China (2018AAA0101100), NSFC (61876077), and Collaborative Innovation Center of Novel Software Technology and Industrialization. Yang Yu is the corresponding author. This work was done when Ziniu Li was an intern in Polixir Technologies. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
explanations behind this observation have not been fully understood. Only until recently, there emerged studies towards understanding the generalization and computation properties of GAIL
[13, 55]. In particular, Chen et al. [13] studied the generalization ability of the so-called
-distance given the complete expert trajectories, while Zhang et al. [55] focused on the global convergence properties of GAIL under sophisticated neural network approximation assumptions.
R
In this paper, we present error bounds on the value gap between the expert policy and imitated policies from BC and GAIL, as well as the sample complexity of the methods. The error bounds indicate that
γ)2, and cannot be improved the policy value gap is quadratic w.r.t. the horizon for BC, i.e., 1/(1 in the worst case, which implies large compounding errors [39, 40]. Meanwhile, the policy value
γ). Similar to [13], the sample complexity gap is only linear w.r.t. the horizon for GAIL, i.e., 1/(1 also hints that controlling the complexity of the discriminator set in GAIL could be beneﬁcial to the generalization. But our analysis strikes that a richer discriminator set is still required to reduce the policy value gap. Besides, our results provide theoretical support for the experimental observation that GAIL can generalize well even provided with incomplete trajectories [28].
−
−
Moreover, noticed that by regarding the environment transition model as a dual agent, imitation learning can also be applied to learn the transition model [51, 44, 43]. Therefore, based on the analysis of imitating policies, we further analyze the error bounds of imitating environments. The results indicate that the environment model learning through adversarial approaches enjoys a linear policy evaluation error w.r.t. the model-bias, which improves the previous quadratic results [31, 25] and suggests a promising application of GAIL for model-based reinforcement learning. 2