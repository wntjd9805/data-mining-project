Abstract
A fundamental question in neuroscience is how the brain creates an internal model of the world to guide actions using sequences of ambiguous sensory information.
This is naturally formulated as a reinforcement learning problem under partial observations, where an agent must estimate relevant latent variables in the world from its evidence, anticipate possible future states, and choose actions that optimize total expected reward. This problem can be solved by control theory, which allows us to ﬁnd the optimal actions for a given system dynamics and objective function.
However, animals often appear to behave suboptimally. Why? We hypothesize that animals have their own ﬂawed internal model of the world, and choose actions with the highest expected subjective reward according to that ﬂawed model. We describe this behavior as rational but not optimal. The problem of Inverse Rational Control (IRC) aims to identify which internal model would best explain an agent’s actions.
Our contribution here generalizes past work on Inverse Rational Control which solved this problem for discrete control in partially observable Markov decision processes. Here we accommodate continuous nonlinear dynamics and continuous actions, and impute sensory observations corrupted by unknown noise that is private to the animal. We ﬁrst build an optimal Bayesian agent that learns an optimal policy generalized over the entire model space of dynamics and subjective rewards using deep reinforcement learning. Crucially, this allows us to compute a likelihood over models for experimentally observable action trajectories acquired from a suboptimal agent. We then ﬁnd the model parameters that maximize the likelihood using gradient ascent. Our method successfully recovers the true model of rational agents. This approach provides a foundation for interpreting the behavioral and neural dynamics of animal brains during complex tasks. 1

Introduction
Brains evolved to understand, interpret, and act upon the physical world. To thrive and reproduce in a harsh and dynamic natural environment, brains, therefore, evolved ﬂexible, robust controllers. To be the controller, the fundamental function of the brain is to organize sensory data into an internal model of the outside world. The animals are never able to get complete information about the world. Instead, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
they only get partial and noisy observations of it. Thus, the brain should build its own internal model which necessarily includes uncertainties of the outside world, and base its decision upon that model
[1]. However, we hypothesize that this internal model is not always correct, but the animals still behave rationally — meaning that animals act optimally according to their own internal model of the world, which may differ from the true world.
The goal of this paper is to identify the internal model of the agent by observing its actions. Unlike
Inverse Reinforcement Learning (IRL) [2, 3, 4] which aims to learn only the reward function of target agent, or Inverse Optimal Control (IOC) [5, 6] to infer only unknown dynamics model, we use
Inverse Rational Control (IRC) [7] to infer both. Since we consider neuroscience tasks which include naturalistic controls and complex physics of the world, we substantially extend past work [7] to include continuous spaces of state, action, and parameter with nonlinear dynamics. We parameterize nonlinear task dynamics and reward functions based on a physics model such that the family of tasks shares an overall structure but has different model parameters. In our framework, an experimentalist can observe state information of the environment and actions taken by the agent. On the other hand, the experimentalist cannot observe information about the agent’s internal model, such as its observations and beliefs. IRC infers the latent internal information of the agent using the data observable by the experimentalist.
The task is formulated as a Partially Observable Markov Decision Process (POMDP) [8, 9], a powerful framework for modeling agent behavior under uncertainty. In order to model an animal’s cognitive process whereby the decision-making is based on its own beliefs about the world, we reformulate the
POMDP as a belief Markov Decision Process (belief MDP) [10, 11]. The agent builds its belief (i.e., its posterior distribution over world states) based on partial, noisy observations and its internal model, and the decision-making is based on its belief.
We construct a Bayesian agent to learn optimal policies and value functions over an entire parameter-ized family of models, which can be viewed as an optimized ensemble of agents each dedicated to one task. This then allows us to maximize the likelihood of the state-action trajectories generated by a target agent, by ﬁnding which parameters from the ensemble best explain the target agent’s data.
The main contributions of this paper are the following. First, our work is able to ﬁnd both the reward function and internal dynamics model simultaneously in continuous nonlinear tasks. Note that continuous nonlinear dynamical systems are the most general form of tasks, so it is trivial to solve discrete and/or linear systems using the proposed approach. Second, we propose a novel approach to implement the Bayesian optimal control ensembles, including an idea of belief representation and belief updating method using estimators with constrained representational capacity (e.g., an extended
Kalman ﬁlter). This allows us to build an algorithm that imitates the bounded rational cognitive process of the brain [12] and to perform belief-based decision-making. Lastly, we propose a novel approach to IRC combining Maximum Likelihood Estimation (MLE) and Monte Carlo Expectation-Maximization (MCEM). This method successfully infers the reward function and internal model parameters of the target agent by maximizing the likelihood of state-action trajectories under the assumption of rationality, while marginalizing over latent sensory observations. Importantly, this is possible because we trained ensembles of agents over entire parameter spaces using ﬂexible function approximators. To the best of our knowledge, our work is the ﬁrst study to infer both the reward and internal model of an unknown agent with partially observable continuous nonlinear dynamics. 2