Abstract
To better conform to data geometry, recent deep generative modelling techniques adapt Euclidean constructions to non-Euclidean spaces. In this paper, we study nor-malizing ﬂows on manifolds. Previous work has developed ﬂow models for speciﬁc cases; however, these advancements hand craft layers on a manifold-by-manifold basis, restricting generality and inducing cumbersome design constraints. We overcome these issues by introducing Neural Manifold Ordinary Differential Equa-tions, a manifold generalization of Neural ODEs, which enables the construction of Manifold Continuous Normalizing Flows (MCNFs). MCNFs require only local geometry (therefore generalizing to arbitrary manifolds) and compute probabilities with continuous change of variables (allowing for a simple and expressive ﬂow construction). We ﬁnd that leveraging continuous manifold dynamics produces a marked improvement for both density estimation and downstream tasks. 1

Introduction
Deep generative models are a powerful class of neural networks which ﬁt a probability distribution to produce new, unique samples. While latent variable models such as Generative Ad-versarial Networks (GANs) [18] and Variational Autoencoders (VAEs) [27] are capable of producing reasonable samples, com-puting the exact modeled data posterior is fundamentally in-tractable. By comparison, normalizing ﬂows [38] are capable of learning rich and tractable posteriors by transforming a sim-ple probability distribution through a sequence of invertible mappings. Formally, in a normalizing ﬂow, a complex distri-bution p(x) is transformed to a simple distribution π(z) via a diffeomorphism f (i.e. a differentiable bijective map with a differentiable inverse) with probability values given by the change of variables: log p(x) = log π(z) − log det (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
∂f −1
∂z (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
, z = f (x).
Figure 1: A manifold ODE solu-tion for a given vector ﬁeld on the sphere.
To compute this update efﬁciently, f must be constrained to allow for fast evaluation of the determinant, which in the absence of additional constraints takes
* indicates equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
O(D3) time (where D is the dimension of z). Furthermore, to efﬁciently generate samples, f must have a computationally cheap inverse. Existing literature increases the expressiveness of such models under these computational constraints and oftentimes parameterizes f with deep neural networks
[17, 8, 26, 11]. An important recent advancement, dubbed the Continuous Normalizing Flow (CNF), constructs f using a Neural Ordinary Differential Equation (ODE) with dynamics g and invokes a continuous change of variables which requires only the trace of the Jacobian of g [4, 19].
Since f is a diffeomorphism, the topologies of the distributions p and π must be equivalent. Fur-thermore, this topology must conform with the underlying latent space, which previous work mostly assumes to be Euclidean. However, topologically nontrivial data often arise in real world examples such as in quantum ﬁeld theory [45], motion estimation [14], and protein structure prediction [22].
In order to go beyond topologically trivial Euclidean space, one can model the latent space with a smooth manifold. An n-dimensional manifold M can be thought of as an n-dimensional analogue of a surface. Concretely, this is formalized with charts, which are smooth bijections ϕx : Ux → Vx, where Ux ⊆ Rn, x ∈ Vx ⊆ M, that also satisfy a smoothness condition when passing between charts. For charts ϕx1, ϕx2 with corresponding Ux1, Vx1 , Ux2 , Vx2, the composed map ϕ−1
◦ ϕx1 : x2 x1 (Vx1 ∩ Vx2 ) → ϕ−1
ϕ−1
Preexisting manifold normalizing ﬂow works (which we present a complete history of in Section 2) do not generalize to arbitrary manifolds. Furthermore, many examples present constructions extrinsic to the manifold. In this work, we solve these issues by introducing Manifold Continuous Normalizing
Flows (MCNFs), a manifold analogue of Continuous Normalizing Flows. Concretely, we: x2 (Vx1 ∩ Vx2 ) is a diffeomorphism. (i) introduce Neural Manifold ODEs as a generalization of Neural ODEs (seen in Figure 1). We leverage existing literature to provide methods for forward mode integration, and we derive a manifold analogue of the adjoint method [37] for backward mode gradient computation. (ii) develop a dynamic chart method to realize Neural Manifold ODEs in practice. This approach integrates local dynamics in Euclidean space and passes between domains using smooth chart transitions. Because of this, we perform computations efﬁciently and can accurately compute gradients. Additionally, this allows us to access advanced ODE solvers (without manifold equivalents) and augment the Neural Manifold ODE with existing Neural ODE improvements [10, 19]. (iii) construct Manifold Continuous Normalizing Flows. These ﬂows are constructed by inte-grating local dynamics to construct diffeomorphisms, meaning that they are theoretically complete over general manifolds. Empirically, we ﬁnd that our method outperforms existing manifold normalizing ﬂows on their speciﬁc domain. 2