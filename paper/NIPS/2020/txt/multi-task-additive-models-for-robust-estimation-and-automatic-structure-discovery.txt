Abstract
Additive models have attracted much attention for high-dimensional regression estimation and variable selection. However, the existing models are usually lim-ited to the single-task learning framework under the mean squared error (MSE) criterion, where the utilization of variable structure depends heavily on a priori knowledge among variables. For high-dimensional observations in real environ-ment, e.g., Coronal Mass Ejections (CMEs) data, the learning performance of previous methods may be degraded seriously due to the complex non-Gaussian noise and the insufﬁciency of a prior knowledge on variable structure. To tackle this problem, we propose a new class of additive models, called Multi-task Addi-tive Models (MAM), by integrating the mode-induced metric, the structure-based regularizer, and additive hypothesis spaces into a bilevel optimization framework.
Our approach does not require any priori knowledge of variable structure and suits for high-dimensional data with complex noise, e.g., skewed noise, heavy-tailed noise, and outliers. A smooth iterative optimization algorithm with convergence guarantees is provided to implement MAM efﬁciently. Experiments on simulations and the CMEs analysis demonstrate the competitive performance of our approach for robust estimation and automatic structure discovery. 1

Introduction
Additive models [14], as nonparametric extension of linear models, have been extensively investigated in machine learning literatures [1, 5, 34, 44]. The attractive properties of additive models include the ﬂexibility on function representation, the interpretability on prediction result, and the ability to circumvent the curse of dimensionality. Typical additive models are usually formulated under
Tikhonov regularization schemes and fall into two categories: one focuses on recognizing dominant variables without considering the interaction among the variables [21, 28, 29, 46] and the other aims to screen informative variables at the group level, e.g., groupwise additive models [4, 42].
Although these existing models have shown promising performance, most of them are limited to the single-task learning framework under the mean squared error (MSE) criterion. Particularly, the groupwise additive models depend heavily on a priori knowledge of variable structure. In this paper, we consider a problem commonly encountered in multi-task learning, in which all tasks share an underlying variable structure and involve data with complex non-Gaussian noises, e.g., skewed
∗Corresponding author. email: chenh@mail.hzau.edu.cn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: A schematic comparison between previous groupwise models and MAM. (a) Data gener-ating process. (b) Mean-based groupwise models with a priori knowledge of group structure, e.g., group lasso [33, 43] and group additive models [42]. (c) Mode-based MAM for robust estimation and automatic structure discovery. noise, heavy-tailed noise, and outliers. The main motivation of this paper is described in Figure 1.
As shown in Figure 1(a), the intrinsic variable structure for generating data is encoded by several variable groups {G1, G2, ..., GL}, where some groups also contain inactive variables. For each task t ∈ {1, ..., T }, the output is related to different dominant groups, e.g., G1, G2 for the ﬁrst task. With a prior knowledge of group structure, single-task groupwise models shown in Figure 1(b) aim to estimate the conditional mean independently, e.g., group lasso [13, 22, 33, 43] and group additive models [4, 16, 42]. All above models are formulated based on a prior knowledge of group structure and Gaussian noise assumption. However, these requirements are difﬁcult to be satisﬁed in real applications, e.g., Coronal Mass Ejections (CMEs) analysis [20].
To relax the dependence on a prior structure and Gaussian noise, this paper proposes a class of
Multi-task Additive Models (MAM) by integrating additive hypothesis space, mode-induced metric
[6, 41, 10], and structure-based regularizer [12] into a bilevel learning framework. The bilevel learning framework is a special kind of mathematical program related closely with optimization schemes in [7, 12]. A brief overview of MAM is shown in Figure 1(c). The proposed MAM can achieve robust estimation under complex noise and realize data-driven variable structure discovery.
The main contributions of this paper are summarized as below:
• Model: A new class of multi-task additive models is formulated by bringing four distinct concepts (e.g., multi-task learning [2, 9], sparse additive models [3, 4, 18, 42], mode-induced metric [10, 38], and bilevel learning framework [12, 32]) together in a coherent way to realize robust and interpretable learning. As far as we know, these issues have not been uniﬁed in a similar fashion before.
• Optimization: An optimization algorithm is presented for the non-convex and non-smooth
MAM by integrating Half Quadratic (HQ) optimization [24] and dual Forward-Backward algorithm with Bregman distance (DFBB) [37] into proxSAGA [30]. In theory, we provide the convergence analysis of the proposed optimization algorithm.
• Effectiveness: Empirical effectiveness of the proposed MAM is supported by experimental evaluations on simulated data and CMEs data. Experimental results demonstrate that MAM can identify variable structure automatically and estimate the intrinsic function efﬁciently even if the datasets are contaminated by non-Gaussian noise. 2
Table 1: Algorithmic properties ((cid:88)-has the given information, ×-hasn’t the given information)
Hypothesis Space
Learning Task
Evaluation Criterion
Objective Function
Robust Estimation
Sparsity on Grouped Features
Sparsity on Individual Features
Variable Structure Discovery
RMR [38]
Linear
Single-task
Mode-induced
Nonconvex (cid:88)
× (cid:88)
×
GroupSpAM [42]
Additive
Single-task
Mean-induced
Convex
× (cid:88)
×
×
CGSI[26]
Additive
Single-task
Mean-induced
Convex
× (cid:88)
× (cid:88)
BIGL[12]
Linear
Multi-task
Mean-induced
Convex
× (cid:88)
× (cid:88)
MAM (ours)
Additive
Multi-task
Mode-induced
Nonconvex (cid:88) (cid:88) (cid:88) (cid:88)