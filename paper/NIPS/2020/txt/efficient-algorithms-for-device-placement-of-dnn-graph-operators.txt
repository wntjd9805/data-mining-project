Abstract
Modern machine learning workloads use large models, with complex structures, that are very expensive to execute. The devices that execute complex models are becoming increasingly heterogeneous as we see a ﬂourishing of domain-speciﬁc accelerators being offered as hardware accelerators in addition to CPUs. These trends necessitate distributing the workload across multiple devices. Recent work has shown that signiﬁcant gains can be obtained with model parallelism, i.e, partitioning a neural network’s computational graph onto multiple devices. In particular, this form of parallelism assumes a pipeline of devices, which is fed a stream of samples and yields high throughput for training and inference of DNNs.
However, for such settings (large models and multiple heterogeneous devices), we require automated algorithms and toolchains that can partition the ML workload across devices. In this paper, we identify and isolate the structured optimization problem at the core of device placement of DNN operators, for both inference and training, especially in modern pipelined settings. We then provide algorithms that solve this problem to optimality. We demonstrate the applicability and efﬁciency of our approaches using several contemporary DNN computation graphs. 1

Introduction
Deep Neural Networks (DNNs) have been effective across a range of applications, including image classiﬁcation [KSH12, SZ14, HZRS15a], translation [WSC+16], language modeling [MKS17], and video captioning [VRD+15]. The proliferation of heterogeneous hardware accelerators [JYP+17,
SPM+16] coupled with the dramatic growth in the size and the structural complexity of DNNs has bolstered the importance of model parallelism, where for both inference and training, the model is distributed across devices.
DNN inference in the “single-stream” setting [mlp], where only one inference request is issued at a time, is latency-sensitive. To achieve low latency, model parallel executions split the model across many accelerators [CKES16, FOP+18, CFO+18]. Model-parallel inference is beneﬁcial due to three primary reasons. First, such splits are mandated by the memory-capacity (size) limitations of accelerators that cannot ﬁt a single DNN model. Current DNN models have billions of parameters and require multiple GBs of space to store the weights and intermediate activations. Second, wide branching in recent DNN structures, as well as in the operator-granularity graphs for established
DNNs, opens up the potential of executing data-independent sections of the computation in parallel to reduce latency. Third, the model needs to be split across multiple types of devices when a subset of operators in the graph are better suited or only supported to execute on certain accelerators.
∗Work done while at Microsoft Research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
DNN training, on the other hand, is throughput-bound, as is DNN inference for the “ofﬂine” setting where many inputs can be serviced together [mlp]. Model parallelism has been proposed for training for the very same motivational reasons listed for inference above [KSH12, Kri14]. Early inﬂuential systems such as DistBelief [DCM+12] and Project Adam [CSAK14] split models to operate on commodity CPU clusters and out of CPU caches. In such a setting, operators in a DNN model are partitioned across the available devices, with each device evaluating and performing updates only on a subset of the model’s parameters for all inputs. While traditional model parallel training suffers from problems of low hardware utilization, as only a single accelerator is active at any given time, pipelined model parallelism overcomes this deﬁciency. The amount of data communicated in pipelined training is the size of intermediate outputs (and corresponding gradients), which need to be sent across accelerators, and is much lower than the size of data communicated in data-parallel training.
In particular, for a range of existing models that ﬁt on a single GPU, PipeDream [HNP+18, NHP+19] uses pipelined model-parallelism to achieve much faster training time to advertised accuracy than data-parallelism. Similarly, GPipe [HCC+18, HCB+19] uses pipelined model-parallel training for very large models whose total training memory footprint exceeds the memory capacity of a single accelerator.
Given the importance of model-parallel inference and training, in this paper we present efﬁcient algorithms to answer the following general question: For a DNN model and a deployment scenario (a set of accelerators and their memory and interconnect constraints), how can we effectively partition the model to optimize the metric of interest, such as latency or throughput, relevant to the inference or training task at hand?
We provide novel algorithmic approaches to tackle the problem of partitioning the model in both model-parallel inference and training scenarios, optimizing for their corresponding metrics of interest:
• Inference – (i) Model-Parallel Inference, optimized for “single-stream” latency (Figure 2a), (ii)
Pipelined Inference, optimized for “ofﬂine” throughput (Figure 3a).
• Training, optimized for throughput – (i) Model-Parallel Training (Figure 2b), (ii) Pipeline-Parallel
Training with PipeDream and GPipe schedules (Figure 4).
In particular, for both non-pipelined and pipelined settings, we identify the combinatorial optimization problem at the core of the device placement question, whose solution will yield the optimal partition.
We then show how to solve this problem to optimality via Integer Programming (IP) and Dynamic
Programming (DP) based algorithms. Our methods are general as they can be applied either to coarse-granularity layer graphs or to more complex ﬁne-granularity operator graphs. We support graph partitions where accelerators can hold a non-contiguous fragment of the graph. We evaluate our partitioning algorithms for different scenarios described above for a variety of modern DNN workloads (7 DNNs, 16 layer and operator graphs). We ﬁnd that the placements are efﬁcient and result in non-trivial optimal splits; non-contiguous splits outperform all the techniques, with an improvement of up to 2× over expert (average 1.46×), 2.08× over local search (average 1.29×) [MKA07], 1.21× over PipeDream (average 1.10×) [NHP+19], 7.69× over Scotch (average 1.50×) [Pel09]. 2