Abstract
Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional obser-vation spaces present a number of challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efﬁcient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model’s learned la-tent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of ﬁnal performance and sample efﬁciency, on a range of difﬁcult image-based control tasks. Our code and videos of our results are available at our website.1 1

Introduction
Deep reinforcement learning (RL) algorithms can learn to solve tasks directly from raw, low-level observations such as images. However, such high-dimensional observation spaces present a number of challenges in practice: On one hand, it is difﬁcult to directly learn from these high-dimensional inputs, but on the other hand, it is also difﬁcult to tease out a compact representation of the underlying task-relevant information from which to learn instead. Standard model-free deep RL aims to unify these challenges of representation learning and task learning into a single end-to-end training procedure.
However, solving both problems together is difﬁcult, since an effective policy requires an effective representation, and an effective representation requires meaningful gradient information to come from the policy or value function, while using only the model-free supervision signal (i.e., the reward function). As a result, learning directly from images with standard end-to-end RL algorithms can in practice be slow, sensitive to hyperparameters, and inefﬁcient.
Instead, we propose to separate representation learning and task learning, by relying on predictive model learning to explicitly acquire a latent representation, and training the RL agent in that learned latent space. This alleviates the representation learning challenge because predictive learning beneﬁts from a rich and informative supervision signal even before the agent has made any progress on the task, and thus results in improved sample efﬁciency of the overall learning process. In this work, our predictive model serves to accelerate task learning by separately addressing representation learning, in contrast to existing model-based RL approaches, which use predictive models either for generating cheap synthetic experience [51, 22, 32] or for planning into the future [11, 13, 46, 9, 55, 26]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. 1https://alexlee-gk.github.io/slac/
Our proposed stochastic sequential model (Figure 1) models the high-dimensional observations as the consequence of a latent process, with a Gaussian prior and latent dynamics. This model represents a partially observed Markov decision process (POMDP), where the stochastic latent state enables the model to represent uncertainty about any of the state variables, given the past observations. Solving such a POMDP exactly would be computationally intractable, since it amounts to solving the decision problem in the space of beliefs [5, 33]. Recent works approximate the belief as encodings of latent samples from forward rollouts or particle ﬁltering [8, 30], or as learned belief representations in a belief-state forward model [21]. We instead propose a simple approximation, which we derive from the control as inference framework, that trains a Markovian critic on latent state samples and trains an actor on a history of observations and actions, resulting in our stochastic latent actor-critic (SLAC) algorithm. Although this approximation loses some of the beneﬁts of full POMDP solvers (e.g. reducing uncertainty), it is easy and stable to train in practice, achieving competitive results on a range of challenging problems.
The main contribution of this work is a novel and principled approach that integrates learning stochastic sequential models and RL into a single method, performing RL in the model’s learned latent space. By formalizing the problem as a control as inference problem within a POMDP, we show that variational inference leads to the objective of our SLAC algorithm. We empirically show that
SLAC beneﬁts from the good asymptotic performance of model-free RL while also leveraging the improved latent space representation for sample efﬁciency, by demonstrating that SLAC substantially outperforms both prior model-free and model-based RL algorithms on a range of image-based continuous control benchmark tasks. 2