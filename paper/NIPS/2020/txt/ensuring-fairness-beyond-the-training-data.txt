Abstract
We initiate the study of fair classiﬁers that are robust to perturbations in the training distribution. Despite recent progress, the literature on fairness has largely ignored the design of fair and robust classiﬁers. In this work, we develop classiﬁers that are fair not only with respect to the training distribution, but also for a class of distributions that are weighted perturbations of the training samples. We formulate a min-max objective function whose goal is to minimize a distributionally robust training loss, and at the same time, ﬁnd a classiﬁer that is fair with respect to a class of distributions. We ﬁrst reduce this problem to ﬁnding a fair classiﬁer that is robust with respect to the class of distributions. Based on online learning algorithm, we develop an iterative algorithm that provably converges to such a fair and robust solution. Experiments on standard machine learning fairness datasets suggest that, compared to the state-of-the-art fair classiﬁers, our classiﬁer retains fairness guarantees and test accuracy for a large class of perturbations on the test set. Furthermore, our experiments show that there is an inherent trade-off between fairness robustness and accuracy of such classiﬁers. 1

Introduction
Machine learning (ML) systems are often used for high-stakes decision-making, including bail decision and credit approval. Often these applications use algorithms trained on past biased data, and such bias is reﬂected in the eventual decisions made by the algorithms. For example, Bolukbasi et al. [9] show that popular word embeddings implicitly encode societal biases, such as gender norms.
Similarly, Buolamwini and Gebru [10] ﬁnd that several facial recognition softwares perform better on lighter-skinned subjects than on darker-skinned subjects. To mitigate such biases, there have been several approaches in the ML fairness community to design fair classiﬁers [4, 19, 35].
However, the literature has largely ignored the robustness of such fair classiﬁers. The “fairness” of such classiﬁers are often evaluated on the sampled datasets, and are often unreliable because of various reasons including biased samples, missing and/or noisy attributes. Moreover, compared to the traditional machine learning setting, these problems are more prevalent in the fairness domain, as the data itself is biased to begin with. As an example, we consider how the optimized pre-processing algorithm [11] performs on ProPublica’s COMPAS dataset [1] in terms of demographic parity (DP), which measures the difference in accuracy between the protected groups. Figure 1 shows two situations – (1) unweighted training distribution (in blue), and (2) weighted training distributions (in red). The optimized pre-processing algorithm [11] yields a classiﬁer that is almost fair on the unweighted training set (DP ≤ 0.02). However, it has DP of at least 0.2 on the weighted set, despite the fact that the marginal distributions of the features look almost the same for the two scenarios. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Unweighted vs Reweighted COMPAS dataset. The marginals of the two distributions are almost the same, but standard fair classiﬁers show demographic parity of at least 0.2 on the reweighted dataset.
This example motivates us to design a fair classiﬁer that is robust to such perturbations. We also show how to construct such weighted examples using a few linear programs.
Contributions: In this work, we initiate the study of fair classiﬁers that are robust to perturbations in the training distribution. The set of perturbed distributions are given by any arbitrary weighted combinations of the training dataset, say W. Our main contributions are the following:
• We develop classiﬁers that are fair not only with respect to the training distribution, but also for the class of distributions characterized by W. We formulate a min-max objective whose goal is to minimize a distributionally robust training loss, and simultaneously, ﬁnd a classiﬁer that is fair with respect to the entire class.
• We ﬁrst reduce this problem to ﬁnding a fair classiﬁer that is robust with respect to the class of distributions. Based on online learning algorithm, we develop an iterative algorithm that provably converges to such a fair and robust solution.
• Experiments on standard machine learning fairness datasets suggest that, compared to the state-of-the-art fair classiﬁers, our classiﬁer retains fairness guarantees and test accuracy for a large class of perturbations on the test set. Furthermore, our experiments show that there is an inherent trade-off between fairness robustness and accuracy of such classiﬁers.