Abstract
Machine learning algorithms typically require abundant data under a stationary environment. However, environments are nonstationary in many real-world applica-tions. Critical issues lie in how to effectively adapt models under an ever-changing environment. We propose a method for transferring knowledge from a source domain to a target domain via (cid:96)1 regularization in high dimension. We incorporate (cid:96)1 regularization of differences between source parameters and target parameters, in addition to an ordinary (cid:96)1 regularization. Hence, our method yields sparsity for both the estimates themselves and changes of the estimates. The proposed method has a tight estimation error bound under a stationary environment, and the estimate remains unchanged from the source estimate under small residuals. Moreover, the estimate is consistent with the underlying function, even when the source estimate is mistaken due to nonstationarity. Empirical results demonstrate that the proposed method effectively balances stability and plasticity. 1

Introduction
Machine learning algorithms typically require abundant data under a stationary environment. However, real-world environments are often nonstationary due to, for example, changes in the users’ preferences, hardware or software faults affecting a cyber-physical system, or aging effects in sensors [39].
Concept drift, which means the underlying functions change over time, is recognized as a root cause of decreased effectiveness in data-driven information systems [25].
Under an ever-changing environment, critical issues lie in how to effectively adapt models to a new environment. Traditional approaches tried to detect concept drift based on hypothesis test [11, 26, 19, 5], but they are hard to capture continuously ever-changing environments. Continuously updating approaches, in contrast, are effective for complex concept drift by avoiding misdetection. These include tree-based methods [7, 16, 24] and ensemble-based methods [31, 20, 10]. Additionally, parameter-based transfer learning for transferring knowledge from past (source domains) to present (target domains) has been studied empirically and theoretically [27, 35, 21, 22]. They employed an empirical risk minimization with (cid:96)2 regularization, and the regularization was extended to strongly convex functions. However, these methods do not yield sparsity of parameter changes, so that even slight changes of data incur update of all parameters.
Speciﬁcally, we consider the problem of sparse regression [32, 14]. Sparse models are widely used in decision making since they have few active features and thus easy to obtain some insight. However, existing sparse regression methods are not necessarily effective for routine decision making, because they can signiﬁcantly change parameters even when the data changes only slightly.
In this paper, we provide a method for transferring knowledge in high dimension via (cid:96)1 regularization that allows sparse estimates and changes. We incorporate the (cid:96)1 regularization of the difference between source parameters and target parameters into the ordinary Lasso regularization. The ordinary
Lasso regularization has a role of restricting the model complexity in high dimension. The additional 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(cid:96)1 regularization of difference plays a key role of sparse update, that is, only a small number of parameters are changed. Because of these two kinds of sparsity, it is easy to interpret and manage models and their changes. The proposed method has a single additional hyper-parameter compared to the ordinary Lasso. It controls the regularization strengths of estimates themselves and their changes, thereby balances stability and plasticity to mitigate so-called stability-plasticity dilemma [13, 6].
Therefore, our method transfers knowledge from past to present when the environment is stationary; while it discards the outdated knowledge when concept drift occurs.
Our main contribution is to give a method with clear theoretical justiﬁcations. We demonstrate three favorable characteristics for our method. First, our method presents a smaller estimation error than Lasso when the underlying functions do not change and the source estimate is the same as a target parameter. This indicates that our method effectively transfers knowledge under a stationary environment. Second, our method gives a consistent estimate even when the source estimate is mistaken, albeit with a weak convergence rate due to the phenomenon of so-called negative transfer [42]. This implies that our method can effectively discard the outdated knowledge and obtain new knowledge under nonstationary environment. Third, our method does not update estimates when the residuals of the predictions are small and the regularization is large. Hence, our method has an implicit stationarity detection mechanism.
The remainder of this paper is organized as follows. We begin with the description of the proposed method in Section 2. We also give some reviews on related work, including concept drift, transfer learning, and online learning. We next show some theoretical properties in Section 3. We ﬁnally illustrate empirical results in Section 4 and conclude in Section 5. All the proofs, as well as additional theoretical properties and empirical results, are given in the supplementary material. 2 Methods 2.1 Transfer Lasso
Let Xi ∈ X and Yi ∈ R be the feature and response, respectively, for i = 1, . . . , n. Consider a linear function p (cid:88) fβ(·) =
βjψj(·), j=1 where β = (βj) ∈ Rp and ψj(·) is a dictionary function from X to R. Let the target function and noise be denoted by f ∗(·) = fβ∗ (·) := p (cid:88) j=1 j ψj(·) and εi := Yi − f ∗(Xi),
β∗ j ) ∈ Rp, and y = (Yi) ∈ Rn. and in matrix notion, f ∗ := Xβ∗ and ε := y − f ∗, where f ∗ = (f ∗(Xi)) ∈ Rn, X = (ψj(Xi)) ∈
Rn×p, β∗ = (β∗
In high-dimensional settings, a reasonable approach to estimating β∗ is to assume sparsity of β∗, in which the cardinality s = |S| of its support S := {j ∈ {1, . . . , p} : β∗ j (cid:54)= 0} satisﬁes s (cid:28) p, and to solve the Lasso problem [32], given by (cid:40) 1 2n n (cid:88) i=1 min
β∈Rp (Yi − fβ(Xi))2 + λ(cid:107)β(cid:107)1
. (cid:41)
Lasso shrinks the estimate to zero and yields a sparse solution. We focus on the squared loss function, but it is applicable to other loss function, as seen in Section 4.3.
Suppose that we have an initial estimate of β∗ as ˜β ∈ Rp and that the initial estimate is associated with the present estimate. Then, a natural assumption is that the difference between initial and present estimates is sparse. Thus, we employ the (cid:96)1 regularization of the estimate difference and incorporate it into the ordinary Lasso regularization as
ˆβ = argmin
β∈Rp (cid:40) 1 2n n (cid:88) i=1 (Yi − fβ(Xi))2 + λ (cid:16) 2
α(cid:107)β(cid:107)1 + (1 − α)(cid:107)β − ˜β(cid:107)1 (cid:41) (cid:17)
=: L(β; ˜β), (1)
Figure 1: Contours of our regularizer for α = 3/4, 1/2, 1/4, 0 with ˜β = (1, 1/2)(cid:62). where λ = λn > 0 and 0 ≤ α ≤ 1 are regularization parameters. We call this method “Transfer
Lasso”. There are two anchor points, zero and the initial estimate. The ﬁrst regularization term in (1) shrinks the estimate to zero and induces the sparsity of the estimate. The second regularization term in (1) shrinks the estimate to the initial estimate and induces the sparsity of changes from the initial estimates. The parameter α controls the balance between transferring and discarding knowledge.
It is preferable to transfer knowledge of the initial estimate when the underlying functions remain unchanged, while not preferable to transfer when a concept drift occurred. As a particular case, if
α = 1, Transfer Lasso reduces to ordinary Lasso and discards knowledge of the initial estimate.
On the other hand, if α = 0, Transfer Lasso reduces to Lasso predicting the residuals of the initial estimate, y − X ˜β, and the initial estimate is utilized as a base learner. The regularization parameters,
λ and α, are typically determined by cross validation.
Figure 1 shows the contours of our regularizer for p = 2. Contours are polygons pointed at βj = 0 and βj = ˜βj so that our estimate can shrink to zero and the initial estimate. The regularization parameter α controls the shrinkage strengths to zero and the initial estimate. We also see that Transfer
Lasso mitigates feature selection instability in the presence of highly correlated features. This is because the loss function tends to be parallel to β1 + β2 = c for highly correlated features but the contours are not necessarily parallel to β1 + β2 = c for a quadrant of ˜β. For α = 1/2, the sum of the two regularization terms equals λ ˜β in the rectangle of βj ∈ [min{ ˜βj, 0}, max{ ˜βj, 0}]. If the least square estimate lies in this region, it becomes the solution of Transfer Lasso, that is, it does not have any estimation bias. 2.2 Algorithm and Soft-Threshold Function
We provide a coordinate descent algorithm for Transfer Lasso. It is guaranteed to converge to a global optimal solution [36], because the problem is convex and the penalty is separable. Let β be the current value. Consider a new value βj as a minimizer of L(β; ˜β) when other elements of β except for βj are ﬁxed. We have
∂βj L(β; ˜β) = − 1 n
X(cid:62) j (y − X−jβ−j) + βj + λα sgn(βj) + λ(1 − α) sgn(βj − ˜βj) = 0, where Xj and X−j denote the j-th column of X and X without j-th column, respectively, and sgn(·) denotes the sign function, hence we obtain the update rule as
βj ← T (cid:18) 1 n
X(cid:62) j (y − X−jβ−j) , λ, λ(2α − 1), ˜βj (cid:19)
, where
T (z, γ1, γ2, b) :=


 0 b z − γ2 sgn(b) z − γ1 sgn(z) b ≥ 0
−γ1 ≤ z ≤ γ2 for for γ2 + b ≤ z ≤ γ1 + b for for
γ2 ≤ z ≤ γ2 + b otherwise b ≤ 0
−γ2 ≤ z ≤ γ1
|
| −γ1 + b ≤ z ≤ −γ2 + b
|
|
−γ2 + b ≤ z ≤ −γ2 otherwise.
The computational complexity of Transfer Lasso is the same as ordinary Lasso. 3
Figure 2: The soft-thresholding function for b ≥ 0 (left) and b ≤ 0 (right) with γ1 > 0 and |γ2| ≤ γ1.
Figure 2 shows the soft-threshold function T (z, γ1, γ2, b) with |γ2| = |λ(2α − 1)| ≤ λ = γ1. There are two steps at 0 and b = ˜β. This implies that each parameter ˆβj is likely to be zero or the initial estimate ˜βj. As α approaches 1, the step of the initial estimate disappears and reduces to the standard soft-thresholding function. As α instead approaches 0, the step of zero disappears and the parameter only shrinks to the initial estimate. 2.3