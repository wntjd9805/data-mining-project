Abstract
Learning from label proportions (LLP) is a weakly supervised setting for classiﬁ-cation in which unlabeled training instances are grouped into bags, and each bag is annotated with the proportion of each class occurring in that bag. Prior work on LLP has yet to establish a consistent learning procedure, nor does there exist a theoretically justiﬁed, general purpose training criterion. In this work we address these two issues by posing LLP in terms of mutual contamination models (MCMs), which have recently been applied successfully to study various other weak supervi-sion settings. In the process, we establish several novel technical results for MCMs, including unbiased losses and generalization error bounds under non-iid sampling plans. We also point out the limitations of a common experimental setting for LLP, and propose a new one based on our MCM framework. 1

Introduction
Learning from label proportions (LLP) is a weak supervision setting for classiﬁcation in which training data come in the form of bags. Each bag contains unlabeled instances and is annotated with the proportion of instances arising from each class. Various methods for LLP have been developed, including those based on support vector machines and related models [28, 39, 38, 26, 8, 18, 32],
Bayesian and graphical models [17, 14, 35, 25, 15], deep learning [19, 1, 12, 20, 36], clustering
[6, 34], and random forests [33]. In addition, LLP has found various applications including image and video analysis [7, 18], high energy physics [9], vote prediction [35], remote sensing [19, 10], medical image analysis [4], activity recognition [25], and reproductive medicine [15].
Despite the emergence of LLP as a prominent weak learning paradigm, the theoretical underpinnings of LLP have been slow to develop. In particular, prior work has not established an algorithm for LLP that is consistent with respect to a classiﬁcation performance measure. Furthermore, there does not even exist a general-purpose, theoretically grounded empirical objective for training LLP classiﬁers.
We propose a statistical framework for LLP based on mutual contamination models (MCMs), which have been used previously as models for classiﬁcation with noisy labels and other weak supervision problems [30, 2, 21, 3, 16]. We use this framework to motivate a principled empirical objective for
LLP, prove generalization error bounds associated to two bag generation models, and establish univer-sal consistency with respect to the balanced error rate (BER). The MCM framework further motivates a novel experimental setting that overcomes a limitation of earlier experimental comparisons.