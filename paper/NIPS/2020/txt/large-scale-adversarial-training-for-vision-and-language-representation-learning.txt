Abstract
We present VILLA, the ﬁrst known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-speciﬁc adversarial ﬁnetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the “free” adver-sarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning,
Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.1 1

Introduction
Inspired by the success of BERT [11] on natural language understanding, there has been a surging re-search interest in developing multimodal pre-training methods for vision-and-language representation learning (e.g., ViLBERT [35], LXMERT [58], and UNITER [10]). When ﬁnetuned on downstream tasks, these pre-trained models have achieved state-of-the-art performance across diverse V+L tasks, such as Visual Question Answering (VQA) [4, 15], Visual Commonsense Reasoning (VCR) [72], and Referring Expression Comprehension [69]. However, due to the immense capacity of large-scale pre-trained models yet limited amount of labeled data in downstream tasks, aggressive ﬁnetuning often falls into the overﬁtting trap [22]. Adversarial training, a method to combat adversarial attacks in order to create robust neural networks [57, 14], has recently shown great potential in improving the generalization ability of pre-trained language models [76, 22] and image classiﬁers [64]. A natural question that came to our mind: can we apply similar adversarial training techniques to V+L problems to improve model performance?
We propose VILLA (Vision-and-Language Large-scale Adversarial training), which advocates the use of adversarial training for V+L representation learning. As illustrated in Figure 1, VILLA consists of two training stages: (i) task-agnostic adversarial pre-training (APT); followed by (ii) task-speciﬁc adversarial ﬁne-tuning (AFT). Intuitively, if well-designed, multimodal pre-training tasks such as image-conditioned masked language modeling and image-text matching can resonate well with many downstream tasks that require visual grounding and reasoning abilities. This leads to our hypothesis that the improved generalization ability of pre-trained models learned during APT stage can be readily transferred to the AFT stage for diverse tasks. In other words, APT is able to uniformly lift model performance for all downstream tasks in a task-agnostic way, while AFT can further enhance the
ﬁnetuned models by leveraging task-speciﬁc supervision signals. 1Code is available at https://github.com/zhegan27/VILLA. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Overview of the proposed VILLA framework for vision-and-language representation learning.
To bring in more ﬂexibility in generating adversarial examples for robust training, we propose to perform adversarial training on the embedding level for multi-modalities, instead of operating on image pixel and sub-word token level in conventional practice. For text modality, we add adversarial perturbations to word embeddings [41, 76, 22]. For image modality, most previous work observes that robustness is at odds with generalization, i.e., trained models are able to resist adversarial attacks on clean images at the expense of performance [39, 65, 74]. Distinctive from these studies, we directly add adversarial perturbations to extracted image-region features [2], as our end goal is the
ﬁnal V+L model performance rather than crafting adversarial image examples. Experiments show that this strategy leads to large performance gain on clean inputs.
Adversarial training procedure is time-consuming and computationally expensive. To power efﬁcient large-scale training, we adopt the recently proposed “free” adversarial training strategy [50, 73, 76], which obtains the gradients of parameters with almost no extra cost when computing the gradients of inputs. In addition to requiring adversarial perturbations to be label-preserving, we also introduce
KL-divergence-based regularization to enforce the conﬁdence level of the prediction to be close, characterized by the “dark” knowledge hidden in the probability vectors. This promotes higher smoothness of the training objective and has empirically proven as important regularization effective for further performance boost.
For evaluation, we mostly focus on UNITER [10], the current best-performing V+L model with state-of-the-art performance across many popular V+L benchmarks, and enhance UNITER with
VILLA through comprehensive experiments on six V+L tasks: VQA [15], VCR [72], NLVR2 [54],
Visual Entailment [66], Referring Expression Comprehension [69], and Image-Text Retrieval [27].
VILLA is a generic framework that can be applied to any multimodal pre-training method. To demonstrate its versatility, we further apply it to LXMERT on VQA, GQA [21], and NLVR2 tasks for generalizability test.
The main contributions are summarized as follows. (i) We present VILLA, the ﬁrst known effort on adversarial pre-training and adversarial ﬁnetuning for V+L representation learning. (ii) Instead of operating on pixel and word token level, we propose to add adversarial perturbations in the embedding space of multi-modalities, and introduce a smoothness-inducing adversarial regularization term on top of the “free” adversarial training strategy. (iii) VILLA achieves new state of the art across six popular V+L tasks. In particular, by relying on standard bottom-up image features only [2], VILLA improves the single-model performance of UNITER-large from 74.02 to 74.87 on VQA, and from 62.8 to 65.7 on VCR. With ensemble, VQA performance is further boosted to 75.85. 2