Abstract
Uniform stability is a notion of algorithmic stability that bounds the worst case change in the model output by the algorithm when a single data point in the dataset is replaced. An inﬂuential work of Hardt et al. [20] provides strong upper bounds on the uniform stability of the stochastic gradient descent (SGD) algorithm on sufﬁciently smooth convex losses. These results led to important progress in understanding of the generalization properties of SGD and several applications to differentially private convex optimization for smooth losses.
Our work is the ﬁrst to address uniform stability of SGD on nonsmooth convex losses. Speciﬁcally, we provide sharp upper and lower bounds for several forms of SGD and full-batch GD on arbitrary Lipschitz nonsmooth convex losses. Our lower bounds show that, in the nonsmooth case, (S)GD can be inherently less stable than in the smooth case. On the other hand, our upper bounds show that (S)GD is sufﬁciently stable for deriving new and useful bounds on generalization error.
Most notably, we obtain the ﬁrst dimension-independent generalization bounds for multi-pass SGD in the nonsmooth case. In addition, our bounds allow us to derive a new algorithm for differentially private nonsmooth stochastic convex optimization with optimal excess population risk. Our algorithm is simpler and more efﬁcient than the best known algorithm for the nonsmooth case [16]. 1

Introduction
Successful applications of a machine learning algorithm require the algorithm to generalize well to unseen data. Thus understanding and bounding the generalization error of machine learning algorithms is an area of intense theoretical interest and practical importance. The single most popular approach to modern machine learning relies on the use of continuous optimization techniques to optimize the appropriate loss function, most notably the stochastic (sub)gradient descent (SGD) method. Yet the generalization properties of SGD are still not well understood.
∗Part of this research was done while the author was at Google Research. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Consider the setting of stochastic convex optimization (SCO). In this problem, we are interested in the minimization of the population risk FD(x) := Ez∼D[f (x, z)], where D is an arbitrary and unknown distribution, for which we have access to an i.i.d. sample of size n, S = (z1, . . . , zn); and f (·, z) is convex and Lipschitz for all z. The performance of an algorithm A is quantiﬁed by its expected excess population risk,
E[εrisk(A)] := E[FD(A(S))] − min x∈X
FD(x), where the expectation is taken with respect to the randomness of the sample S and internal randomness of A. A standard way to bound the excess risk is given by its decomposition into optimization error (a.k.a. training error) and generalization error (see eqn. (2) in Sec. 2). The optimization error can be easily measured empirically but assessing the generalization error requires access to fresh samples from the same distribution. Thus bounds on the generalization error lead directly to provable guarantees on the excess population risk.
Classical analysis of SGD allows obtaining bounds on the excess population risk of one pass SGD.
In particular, with an appropriately chosen step size, SGD gives a solution with expected excess population risk of O(1/ n) and this rate is optimal [30]. However, this analysis does not apply to multi-pass SGD that is ubiquitous in practice.
√
In an inﬂuential work, Hardt et al. [20] gave the ﬁrst bounds on the generalization error of general forms of SGD (such as those that make multiple passes over the data). Their analysis relies on algorithmic stability, a classical tool for proving bounds on the generalization error. Speciﬁcally, they gave strong bound on the uniform stability of several variants of SGD on convex and smooth losses (with 2/η-smoothness sufﬁcing when all the step sizes are at most η). Uniform stability bounds the worst case change in loss of the model output by the algorithm on the worst case point when a single data point in the dataset is replaced [4]. Formally, for a randomized algorithm A, loss functions f (·, z) and S (cid:39) S(cid:48) and z ∈ Z, let γA(S, S(cid:48), z) := f (A(S), z) − f (A(S(cid:48)), z), where S (cid:39) S(cid:48) denotes that the two datasets differ only in a single data point. We say A is γ-uniformly stable if sup
S(cid:39)S(cid:48),z
E[γA(S, S(cid:48), z)] ≤ γ, where the expectation is over the internal randomness of A. Stronger notions of stability can also be considered, e.g., bounding the probability – over the internal randomness of A – that γA(S, S(cid:48), z) > γ.
Using stability, [20] showed that several variants of SGD simultaneously achieve the optimal tradeoff n). Several works have used between the excess empirical risk and stability with both being O(1/ this approach to derive new generalization properties of SGD [28, 9, 19].
√
The key insight of Hardt et al. [20] is that a gradient step on a sufﬁciently smooth convex function is a nonexpansive operator (that is, it does not increase the (cid:96)2 distance between points). Unfortunately, this property does not hold for nonsmooth losses such as the hinge loss. As a result, no non-trivial bounds on the uniform stability of SGD have been previously known in this case.
Uniform stability is also closely related to the notion of differential privacy (DP). DP upper bounds the worst case change in the output distribution of an algorithm when a single data point in the dataset is replaced [14]. This connection has been exploited in the design of several DP algorithms for SCO.
In particular, bounds on the uniform stability of SGD from [20] have been crucial in the design and analysis of new DP-SCO algorithms [36, 12, 17, 2, 16]. 1.1 Our Results
We establish tight bounds on the uniform stability of the (stochastic) subgradient descent method on nonsmooth convex losses. These results demonstrate that in the nonsmooth case SGD can be substantially less stable. At the same time we show that SGD has strong stability properties even in the regime when its iterations can be expansive.
For convenience, we describe our results in terms of uniform argument stability (UAS), which bounds the output sensitivity in (cid:96)2-norm w.r.t. an arbitrary change in a single data point. Formally, a (randomized) algorithm has δ-UAS if sup
S(cid:39)S(cid:48)
E (cid:107)A(S) − A(S(cid:48))(cid:107)2 ≤ δ. (1)
This notion is implicit in existing analyses of uniform stability [4, 32, 20] and was explicitly deﬁned by Liu et al. [27]. In this work, we prove stronger – high probability – upper bounds on the random 2
variable δA(S, S(cid:48)) := (cid:107)A(S) − A(S(cid:48))(cid:107),2 and we provide matching lower bounds for the weaker – in expectation – notion of UAS (1). A summary of our bounds is in Table 1. For simplicity, they are provided for constant step size; general step sizes (for upper bounds) are provided in Section 3.
Algorithm
GD (full batch)
H.p. upper bound 4(cid:0)η
T + ηT n
√ (cid:1)
T + ηT n (cid:1) min{1, T n }3η
T + 4 ηT n
Expectation upper bound
Expectation Lower bound
√ 4(cid:0)η
T + ηT n
√
√ (cid:1)
Ω(cid:0)η
√ (cid:1)
T + ηT n
√ (cid:16)
Ω (cid:16) min{1, T n }η
T + ηT n (cid:17) (cid:17)
√
T + ηT n 2η n }η
Table 1: UAS for GD and SGD. Here T = # iterations; η is the step size.
Ω min{1, T n }2η min{1, T
T + 4 ηT n
T + 4 ηT n
SGD (w/replacement)
SGD (ﬁxed permutation)
√ 4(cid:0)η
√
Compared to the smooth case [20], the main difference is the presence of the additional η
T term.
This term has important implications for the generalization bounds derived from UAS. The ﬁrst one is that the standard step size η = Θ(1/ n) used in single pass SGD leads to a vacuous stability bound. Unfortunately, as shown by our lower bounds, this is unavoidable (at least in high dimension).
However, by decreasing the step size and increasing the number of steps, one obtains a variant of
SGD with nearly optimal balance between the UAS and the excess empirical risk.
√
√
We highlight two major consequences of our bounds:
√
• Generalization bounds for multi-pass nonsmooth SGD. We prove that the generalization error
Kn + K)η). This result can be easily of multi-pass SGD with K passes is bounded by O(( combined with training error guarantees to provide excess risk bounds for this algorithm. Since training error can be measured directly, our generalization bounds would immediately yield strong guarantees on the excess risk in practical scenarios where we can certify small training error.
• Differentially private stochastic convex optimization for non-smooth losses. We show that a variant of standard noisy SGD [3] with constant step size and n2 iterations yields the optimal (cid:1) for convex nonsmooth losses under (α, β)-differential excess population risk O(cid:0) 1√ privacy. The best previous algorithm for this problem is substantially more involved: it relies on a multi-phase regularized SGD with decreasing step sizes and variable noise rates and uses
O(n2(cid:112)log(1/β)) gradient computations [16]. d log(1/β) n +
√
αn 1.2 Overview of Techniques
• Upper bounds. When gradient steps are nonexpansive, upper-bounding UAS requires simply summing the differences between the gradients on the neighboring datasets when the replaced data point is used [20]. This gives the bound of ηT /n in the smooth case.
By contrast, in the nonsmooth case, UAS may increase even when the gradient step is performed on the same function. As a result it may increase in every single iteration. However, we use the fact that the difference in the subgradients has negative inner product with the difference between the iterates themselves (by monotonicity of the subgradient). Thus the increase in distance satisﬁes a recurrence with a quadratic and a linear term. Solving this recurrence leads to our upper bounds.
• Lower bounds. The lower bounds are based on a function with a highly nonsmooth behavior around the origin. More precisely, it is the maximum of linear functions plus a small linear drift that is controlled by a single data point. We show that, when starting the algorithm from the origin, the presence of the linear drift pushes the iterate into a trajectory in which each subgradient step is orthogonal to the current iterate. Thus, if d ≥ min{T, 1/η2}, we get the
T η increase in UAS.
Our lower bounds are also robust to averaging of the iterates. The detailed constructions and analyses can be found in the full version of the paper.
√ 1.3 Other