Abstract
In recent years, we have witnessed tremendous progress in deep reinforcement learning (RL) for tasks such as Go, Chess, video games, and robot control. Never-theless, other combinatorial domains, such as AI planning, still pose considerable challenges for RL approaches. The key difﬁculty in those domains is that a positive reward signal becomes exponentially rare as the minimal solution length increases.
So, an RL approach loses its training signal. There has been promising recent progress by using a curriculum-driven learning approach that is designed to solve a single hard instance. We present a novel automated curriculum approach that dynamically selects from a pool of unlabeled training instances of varying task complexity guided by our difﬁculty quantum momentum strategy. We show how the smoothness of the task hardness impacts the ﬁnal learning results. In particular, as the size of the instance pool increases, the “hardness gap” decreases, which facilitates a smoother automated curriculum based learning process. Our automated curriculum approach dramatically improves upon the previous approaches. We show our results on Sokoban, which is a traditional PSPACE-complete planning problem and presents a great challenge even for specialized solvers. Our RL agent can solve hard instances that are far out of reach for any previous state-of-the-art
Sokoban solver. In particular, our approach can uncover plans that require hundreds of steps, while the best previous search methods would take many years of comput-ing time to solve such instances. In addition, we show that we can further boost the
RL performance with an intricate coupling of our automated curriculum approach with a curiosity-driven search strategy and a graph neural net representation. 1

Introduction
Planning is an area in core artiﬁcial intelligence (AI), which emerged in the early days of AI as part of research on robotics. An AI planning problem consists of a speciﬁcation of an initial state, a goal state, and a set of operators that speciﬁes how one can move from one state to the next. In robotics, a planner can be used to synthesize a plan, i.e., a sequence of robot actions from an initial state to a desired goal state. The generality of the planning formalism captures a surprisingly wide range of tasks, including task scheduling, program synthesis, and general theorem proving (actions are 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
inference steps). The computational complexity of general AI planning is at least PSPACE-complete
[4, 6]. There are now dozens of AI planners, many of these compete in the regular ICAPS Planning competition [19]. In this paper, we consider how deep reinforcement learning (RL) can boost the performance of plan search by automatically uncovering domain structure to guide the search process.
A core difﬁculty for RL for AI planning is the extreme sparsity of the reward function. For instances whose shortest plans consist of hundreds of steps, the learning agent either gets a positive reward by correctly ﬁnding the whole chain of steps, or no reward otherwise. A random strategy cannot
“accidentally” encounter a valid chain. In [11], we proposed an approach to addressing this issue by introducing a curriculum-based training strategy [2] for AI planning. The curriculum starts with training on a set of quite basic planning sub-tasks and proceeds with training on increasingly complex set of sub-tasks until enough domain knowledge is acquired by the RL framework to solve the original planning task. We showed how this strategy can solve several surprisingly challenging instances from the Sokoban planning domain.
Herein we introduce a novel automated dynamic curriculum strategy, which is more general and signiﬁcantly extends the curriculum approach of [11] and allows for solving a broad set of previously unsolved planning problem instances. Our approach starts with a broad pool of sub-tasks of the target problem to be solved. In contrast to [11], our approach does require the manual identiﬁcation of groups of increasingly hard sub-tasks. All sub-tasks are “unlabeled,” i.e., no solution plans are provided and many may even be unsolvable. We introduce a novel multi-armed bandit strategy that automatically selects batches of sub-tasks to feed to our planning system or RL agent, which we refer to as difﬁculty quantum momentum strategy, using the current deep net policy function and a Monte
Carlo Tree Search based search strategy. The system selects tasks that can be solved and uses those instances to subsequently update the policy network. By repeating these steps, the policy network becomes increasingly effective and starts solving increasingly hard sub-tasks until enough domain knowledge is captured and the original planning task can be solved. As we will demonstrate, the difﬁculty quantum momentum multi-armed bandit strategy allows the system to focus on sub-tasks that lie on the boundary of being solvable by the planning agent. In effect, the system dynamically uncovers the most useful sub-tasks to train on. Intuitively, these instances ﬁll the “complexity gap” between the current knowledge in the policy network and the next level of problem difﬁculty. The whole framework proceeds in an unsupervised manner — little domain knowledge of the background task is needed and the bandit is being guided by a simple but surprisingly effective rule.
As in [11], we use Sokoban as our background domain due to its extreme difﬁculty for AI planners
[16]. See Figure 1 for an example Sokoban problem. We have a 2-D grid setup, in which, given equal number of boxes and goal squares, the player needs to push all boxes to goal squares without crossing walls or pushing boxes into walls. The player can only move upward, downward, leftward and rightward. Sokoban is a challenging combinatorial problem for AI planning despite its simple conceptual rules. The problem was proven to be PSPACE-complete [7] and a regular size board (around 15 × 15) can require hundreds of pushes. Another reason for its difﬁculty is that many pushes are irreversible. That is, with a few wrong pushes, the board can become a dead-end state, from which no valid plan leading to a goal state exits. Modern specialized Sokoban solvers are all based on combinatorial search augmented with highly sophisticated handcrafted pruning rules and various dead-end detection criteria to avoid spending time in search space with no solution.
Preview of Main Results We evaluate our approach using a large Sokoban repository [23]. This repository contains 3362 Sokoban problems, including 225 instances that have not been solved with any state-of-the-art search based methods. [11] focused on solving single hard instances and showed how several such instances could be solved using a handcrafted portfolio strategy. Our focus here is on the full subset of unsolved instances and our automated dynamic curriculum strategy.
Table 3 provides a summary of our overall results. Our baseline strategy (BL) uses a convolutional network to capture the policy and samples uniformly from the sub-tasks. Our baseline can solve 30 of the 225 unsolved instances (13%), using 12 hours per instance, including training time. Adding curiosity rewards (CR) to the search component and using a graph neural net (GN), we can solve 72 instances (32%). Then, adding the multi-armed bandit dynamic portfolio strategy, enables us to solve 115 cases (51%), and, training on a pool of all open problem instances and their (unsolved) sub-cases together, lets us solve 146 instances (65%). Finally, we also added the remaining 3137 instances from the repository to our training pool. These are solvable by existing search-based solvers but we do not use those solutions. With these extra “practice problems,” our automated curriculum deep 2
Figure 1: Sokoban. Left panel: An example of a Sokoban instance [11]. The blue circle represents the player, red circles represent boxes and dark cells are goal squares. Walls are represented by light colored cells. The player has to push all boxes to goal squares without going through walls. The player can only move upward, downward, leftward and rightward and push a box into a neighbor empty square and when placed in an empty square next to a box. Right panel: a subcase with 2 boxes and goal squares. To build a subcase, we ﬁrst randomly pick the number of box/goal cells of the subcases, and then randomly select box/goal locations as a subset of initial box/goal locations.
RL planning approach can solve 179 out of 225 unsolved problems (80%). Many of the solutions require plans with several hundreds of steps. Moreover, these instances are now solved with a single deep policy network augmented with Monte Carlo tree search (MCTS). This suggests that the deep network successfully captures a signiﬁcant amount of domain knowledge about the planning domain.
This knowledge, augmented with a limited amount of combinatorial search (UCT mechanism), can solve a rich set of very hard Sokoban planning problems. 2