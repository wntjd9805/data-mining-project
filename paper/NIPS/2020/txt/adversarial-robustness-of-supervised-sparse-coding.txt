Abstract
Several recent results provide theoretical insights into the phenomena of adversarial examples. Existing results, however, are often limited due to a gap between the simplicity of the models studied and the complexity of those deployed in practice.
In this work, we strike a better balance by considering a model that involves learning a representation while at the same time giving a precise generalization bound and a robustness certiﬁcate. We focus on the hypothesis class obtained by combining a sparsity-promoting encoder coupled with a linear classiﬁer, and show an interesting interplay between the expressivity and stability of the (supervised) representation map and a notion of margin in the feature space. We bound the robust risk (to (cid:96)2-bounded perturbations) of hypotheses parameterized by dictionaries that achieve a mild encoder gap on training data. Furthermore, we provide a robustness certiﬁcate for end-to-end classiﬁcation. We demonstrate the applicability of our analysis by computing certiﬁed accuracy on real data, and compare with other alternatives for certiﬁed robustness. 1

Introduction
With machine learning applications becoming ubiquitous in modern-day life, there exists an increasing concern about the robustness of the deployed models. Since ﬁrst reported in [Szegedy et al., 2013,
Goodfellow et al., 2014, Biggio et al., 2013], these adversarial attacks are small perturbations of the input, imperceptible to the human eye, which can nonetheless completely ﬂuster otherwise well-performing systems. Because of clear security implications [DARPA, 2019], this phenomenon has sparked an increasing amount of work dedicated to devising defense strategies [Metzen et al., 2017, Gu and Rigazio, 2014, Madry et al., 2017] and correspondingly more sophisticated attacks
[Carlini and Wagner, 2017, Athalye et al., 2018, Tramer et al., 2020], with each group trying to triumph over the other in an arms-race of sorts.
A different line of research attempts to understand adversarial examples from a theoretical standpoint.
Some works have focused on giving robustness certiﬁcates, thus providing a guarantee to withstand the attack of an adversary under certain assumptions [Cohen et al., 2019, Raghunathan et al., 2018,
Wong and Kolter, 2017]. Other works address questions of learnabiltiy [Shafahi et al., 2018, Cullina et al., 2018, Bubeck et al., 2018, Tsipras et al., 2018] or sample complexity [Schmidt et al., 2018, Yin et al., 2018, Tu et al., 2019], in the hope of better characterizing the increased difﬁculty of learning hypotheses that are robust to adversarial attacks. While many of these results are promising, the analysis is often limited to simple models. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Here, we strike a better balance by considering a model that involves learning a representation while at the same time giving a precise generalization bound and a robustness certiﬁcate. In particular, we focus our attention on the adversarial robustness of the supervised sparse coding model [Mairal et al., 2011], or task-driven dictionary learning, consisting of a linear classiﬁer acting on the representation computed via a supervised sparse encoder. We show an interesting interplay between the expressivity and stability of a (supervised) representation map and a notion of margin in the feature space. The idea of employing sparse representations as data-driven features for supervised learning goes back to the early days of deep learning [Coates and Ng, 2011, Kavukcuoglu et al., 2010, Zeiler et al., 2010, Ranzato et al., 2007], and has had a signiﬁcant impact on applications in computer vision and machine learning [Wright et al., 2010, Henaff et al., 2011, Mairal et al., 2008, 2007, Gu et al., 2014].
More recently, new connections between deep networks and sparse representations were formalized by Papyan et al. [2018], which further helped deriving stability guarantees [Papyan et al., 2017b], providing architecture search strategies and analysis [Tolooshams et al., 2019, Murdock and Lucey, 2020, Sulam et al., 2019], and other theoretical insights [Xin et al., 2016, Aberdam et al., 2019,
Aghasi et al., 2020, Aberdam et al., 2020, Moreau and Bruna, 2016]. While some recent work has leveraged the stability properties of these latent representations to provide robustness guarantees against adversarial attacks [Romano et al., 2019], these rely on rather stringent generative model assumptions that are difﬁcult to be satisﬁed and veriﬁed in practice. In contrast, our assumptions rely on the existence of a positive gap in the encoded features, as proposed originally by Mehta and
Gray [2013]. This distributional assumption is signiﬁcantly milder – it is directly satisﬁed by making traditional sparse generative model assumptions – and can be directly quantiﬁed from data.
This work makes two main contributions: The ﬁrst is a bound on the robust risk of hypotheses that achieve a mild encoder gap assumption, where the adversarial corruptions are bounded in (cid:96)2-norm.
Our proof technique follows a standard argument based on a minimal (cid:15)-cover of the parameter space, dating back to Vapnik and Chervonenkis [1971] and adapted for matrix factorization and dictionary learning problems in Gribonval et al. [2015]. However, the analysis of the Lipschitz continuity of the adversarial loss with respect to the model parameters is considerably more involved. The increase in the sample complexity is mild with adversarial corruptions of size ν manifesting as an additional term of order O (cid:0)(1 + ν)2/m(cid:1) in the bound, where m is the number of samples, and a minimal encoder gap of O(ν) is necessary. Much of our results extend directly to other supervised learning problems (e.g. regression). Our second contribution is a robustness certiﬁcate that holds for every hypothesis in the function class for (cid:96)2 perturbations for multiclass classiﬁcation. In a nutshell, this result guarantees that the label produced by the hypothesis will not change if the encoder gap is large enough relative to the energy of the adversary, the classiﬁer margin, and properties of the model (e.g. dictionary incoherence). 2 Preliminaries and