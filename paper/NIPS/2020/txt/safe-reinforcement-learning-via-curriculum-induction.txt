Abstract
In safety-critical applications, autonomous agents may need to learn in an envi-ronment where mistakes can be very costly. In such settings, the agent needs to behave safely not only after but also while learning. To achieve this, existing safe reinforcement learning methods make an agent rely on priors that let it avoid dangerous situations during exploration with high probability, but both the prob-abilistic guarantees and the smoothness assumptions inherent in the priors are not viable in many scenarios of interest such as autonomous driving. This paper presents an alternative approach inspired by human teaching, where an agent learns under the supervision of an automatic instructor that saves the agent from violating constraints during learning. In this new model, the instructor needs to know neither how to do well at the task the agent is learning, nor how the environment works.
Instead, it has a library of reset controllers that it activates when the agent starts behaving dangerously, preventing it from doing damage. Crucially, the choices of which reset controller to apply in which situation affect the speed of agent learning.
Based on observing agents’ progress, the teacher itself learns a policy for choosing the reset controllers, a curriculum, to optimize the agent’s ﬁnal policy reward.
Our experiments use this framework in two challenging environments to induce curricula for safe and efﬁcient learning. 1

Introduction
Safety is a major concern that prevents application of reinforcement learning (RL) [45] to many practical problems [16]. Among the RL safety notions studied in the literature [23], ensuring that the agent does not violate constraints is perhaps the most important. Consider, for exmple, training a policy for a self-driving car’s autopilot. Although simulations are helpful, much of the training needs to be done via a process akin to RL on a physical car [26]. At that stage, it is critical to avoid damage to property, people, and the car itself. Safe RL techniques aim to achieve this primarily by imparting the agent with priors about the environment and equipping it with sound ways of updating this information with observations [17, 9, 12, 13]. Some do this heuristically [2], while others provide safety guarantees through demonstrations [47], or by assuming access to fairly accurate dynamical models [4], or at the cost of smoothness assumptions [9, 28, 48, 49]. These assumptions hold, e.g., in certain drone control scenarios [8] but are violated in settings such as autonomous driving, where a small delta in control inputs can make a difference between safe passage and collision.
⇤The author did part of this work while at Microsoft Research, Redmond. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we propose Curriculum Induction for Safe Reinforcement learning (CISR, “Caesar”), a safe RL approach that lifts several prohibitive assumptions of existing ones. CISR is motivated by the fact that, as humans, we successfully overcome challenges similar to those in the autopilot training scenario when we help our children learn safely. Children possess inaccurate notions of danger, have difﬁculty imitating us at tasks requiring coordination, and often ignore or misunderstand requests to be careful. Instead, e.g., when they learn how to ride a bicycle, we help them do it safely by ﬁrst equipping the bicycle with training wheels, then following the child while staying prepared to catch them if they fall, ﬁnally letting them ride freely but with elbow and knee guards for some time, and only then allowing them to ride like grown-ups. Importantly, each “graduation" to the next stage happens based on the learner’s observed performance under the current safeguard mechanism.
Key ideas. In CISR, an artiﬁcial teacher helps an agent (student) learn potentially dangerous skills by inducing a sequence of safety-ensuring training stages called curriculum. A student is an RL agent trying to learn a policy for a constrained MDP (CMDP) [6]. A teacher has a decision rule – a curriculum policy – for constructing a curriculum for a student given observations of the student’s behavior. Each curriculum stage lasts for some number of RL steps of the student and is characterized by an intervention (e.g., the use of training wheels) that the teacher commits to use throughout that stage. Whenever the student runs the risk of violating a constraint (falling off the bike), that stage’s intervention automatically puts the agent into a safe state (e.g., the way training wheels keep the bike upright), in effect by temporarily overriding the dynamics of the student’s CMDP. The teacher’s curriculum policy chooses interventions from a pre-speciﬁed set such as {use of training wheels, catching the child if they fall, wearing elbow and knee guards} with the crucial property that any single intervention from this set keeps the agent safe as described above. A curriculum policy that commits to any one of these interventions for the entire learning process is sufﬁcient for safety, but note that in the biking scenario we don’t keep the training wheels on the bike forever: at some point they start hampering the child’s progress. Thus, the teacher’s natural goal is to optimize the curriculum policy with respect to the student’s policy performance at the end of the learning process, assuming the process is long enough that the student’s rate of attempted constraint violations becomes very small. In CISR, the teacher does this via a round-based process, by playing a curriculum policy in every round, observing a student learn under the induced curriculum, evaluating its performance, and trying an improved curriculum policy on a new student in the next round.