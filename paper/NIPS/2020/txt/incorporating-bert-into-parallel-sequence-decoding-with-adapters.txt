Abstract
While large scale pre-trained language models such as BERT [5] have achieved great success on various natural language understanding tasks, how to efﬁciently and effectively incorporate them into sequence-to-sequence models and the cor-responding text generation tasks remains a non-trivial problem. In this paper, we propose to address this problem by taking two different BERT models as the encoder and decoder respectively, and ﬁne-tuning them by introducing simple and lightweight adapter modules, which are inserted between BERT layers and tuned on the task-speciﬁc dataset. In this way, we obtain a ﬂexible and efﬁcient model which is able to jointly leverage the information contained in the source-side and target-side BERT models, while bypassing the catastrophic forgetting problem. Each com-ponent in the framework can be considered as a plug-in unit, making the framework
ﬂexible and task agnostic. Our framework is based on a parallel sequence decoding algorithm named Mask-Predict [8] considering the bi-directional and conditional independent nature of BERT, and can be adapted to traditional autoregressive decoding easily. We conduct extensive experiments on neural machine translation tasks where the proposed method consistently outperforms autoregressive baselines while reducing the inference latency by half, and achieves 36.49/33.57 BLEU scores on IWSLT14 German-English/WMT14 German-English translation. When adapted to autoregressive decoding, the proposed method achieves 30.60/43.56
BLEU scores on WMT14 English-German/English-French translation, on par with the state-of-the-art baseline models. 1

Introduction
Pre-trained language models [26, 27, 5, 39] have received extensive attention in natural language processing communities in recent years. Generally, training of these models consists of two stages.
Firstly, the model is trained on a large scale monolingual corpus in a self-supervised manner, and then ﬁne-tuned end-to-end on downstream tasks with task-speciﬁc loss functions and datasets. In this way, pre-trained language models have achieved great success on various natural language understanding tasks such as reading comprehension and text classiﬁcation, and BERT [5] is one of the most successful models among them.
While ﬁne-tuning BERT for common language understanding tasks is straightforward, for natural language generation which is one of the core problems in NLP [2, 34, 24], how to incorporate BERT
∗Corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
remains substantially challenging. We conclude the main challenges as three-fold considering that the sequence-to-sequence framework [33] is the backbone model of generation tasks. On the encoder side, as studied in [41], simply initializing the encoder with a pre-trained BERT will actually hurt the performance. One possible explanation could be that training on a complex task with rich resources (e.g., machine translation) leads to the catastrophic forgetting problem [23] of the pre-trained model.
On the decoder side, which can be treated as a conditional language model, it is naturally non-trivial to marry unconditional pre-training with conditional ﬁne-tuning. And the bidirectional nature of
BERT also prevents it from being directly applied to common autoregressive text generation. In addition, ﬁne-tuning the full model is parameter inefﬁcient considering the enormous scale of recent pre-trained language models [28] while being unstable and fragile on small datasets [20].
To tackle these challenges, in this paper, we propose a new paradigm of incorporating BERT into text generation tasks under the sequence-to-sequence framework. Speciﬁcally, we construct our framework based on the following steps. We ﬁrst choose two pre-trained BERT models from the source/target side respectively, and consider them as the encoder/decoder. For example, on the
WMT14 English-German machine translation task, we take bert-base-cased as the encoder and bert-base-german-cased as the decoder. Then, we introduce lightweight neural network components named adapter layers and insert them into each BERT layer to achieve the adaptation to new tasks. While ﬁne-tuning on task speciﬁc datasets, we freeze the parameters of BERT layers and only tune the adapter layers. We design different architectures for adapters. Speciﬁcally, we stack two feed-forward networks as the encoder adapter, mainly inspired from [3]; and an encoder-decoder attention module is considered as the decoder adapter. Considering that BERT utilizes bi-directional context information and ignores conditional dependency between tokens, we build our framework on a parallel sequence decoding algorithm named Mask-Predict [8] to make the most of BERT and keep the consistency between training and inference.
In this way, the proposed framework achieves the following beneﬁts. 1) By introducing the adapter modules, we decouple the parameters of the pre-trained language model and task-speciﬁc adapters, therefore bypassing the catastrophic forgetting problem. And the conditional information can be learned through the cross-attention based adapter on the decoder side; 2) Our model is parameter efﬁcient and robust while tuning as a beneﬁt from the lightweight nature of adapter modules. In addition, thanks to parallel decoding, the proposed framework achieves better performance than autoregressive baselines while doubling the decoding speed; 3) Each component in the framework can be considered as a plug-in unit, making the framework very ﬂexible and task agnostic. For example, our framework can be adapted to autoregressive decoding straightforwardly by only incorporating the source-side BERT encoder and adapters while keeping the original Transformer decoder.
We evaluate our framework on various neural machine translation tasks, and the proposed framework achieves 36.49/33.57 BLEU scores on the IWSLT14 German-English/WMT14 German-English translation tasks, achieving 3.5/0.88 improvements over traditional autoregressive baselines with half of the inference latency. When adapting to autoregressive decoding, we achieve 30.60/43.56
BLEU scores on the WMT14 English-German/English-French translation tasks, on par with the state-of-the-art baseline models. 2