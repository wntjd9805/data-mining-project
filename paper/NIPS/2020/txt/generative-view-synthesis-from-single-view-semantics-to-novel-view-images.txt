Abstract
Content creation, central to applications such as virtual reality, can be tedious and time-consuming. Recent image synthesis methods simplify this task by offering tools to generate new views from as little as a single input image, or by converting a semantic map into a photorealistic image. We propose to push the envelope fur-ther, and introduce Generative View Synthesis (GVS) that can synthesize multiple photorealistic views of a scene given a single semantic map. We show that the sequential application of existing techniques, e.g., semantics-to-image translation followed by monocular view synthesis, fail at capturing the scene’s structure. In contrast, we solve the semantics-to-image translation in concert with the estimation of the 3D layout of the scene, thus producing geometrically consistent novel views that preserve semantic structures. We ﬁrst lift the input 2D semantic map onto a 3D layered representation of the scene in feature space, thereby preserving the seman-tic labels of 3D geometric structures. We then project the layered features onto the target views to generate the ﬁnal novel-view images. We verify the strengths of our method and compare it with several advanced baselines on three different datasets.
Our approach also allows for style manipulation and image editing operations, such as the addition or removal of objects, with simple manipulations of the input style images and semantic maps respectively. For code and additional results, visit the project page at https://gvsnet.github.io 1

Introduction
The rising demand for digital content, together with the widespread availability of high-quality digital cameras, has fueled the need for tools and algorithms to democratize content creation. A prominent example of one such technology is novel view synthesis (NVS), which allows the artist to render a scene from new viewpoints using as few as two images [11, 36], or even just one [34]. Photorealistic images can also be generated by editing a simpliﬁed representation of the scene, such as a semantic map, followed by image-to-image translation [25], but the viewpoint cannot be manipulated.
In this work, we propose Generative View Synthesis (GVS), which combines the advantages of both approaches. Given a single semantic map, which is easy to edit and requires no image capture, GVS can generate RGB images of the same layout, but from new, arbitrary viewpoints. Not surprisingly,
GVS also inherits the challenges of both: generating RGB values from a bare semantic map is an ill-posed problem that is further complicated by the need for the different output views to be photometrically and geometrically consistent. One could tackle this problem with the sequential application of existing techniques. That is, we can ﬁrst convert the single-view semantic map into an RGB image using image-to-image translation techniques [20], and generate novel RGB views using monocular novel view synthesis techniques [34]. However, we observe that this may fail at preserving the scene’s structure accurately, as shown in the animation in Figure 1.
Our key insight is that semantic maps are particularly informative about the structure of a scene, despite offering no information about its photometric properties. Semantic segments, in fact, carry more unambiguous information about occlusion boundaries. This is in contrast with RGB images, where edges can also result from texture. We leverage this observation to preserve geometric 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Generative View Synthesis is a method to generate photorealistic images from novel viewpoints, given just a semantic map and a style image. Here we show lateral (l0−7) and forward (f0−7) camera motion. Because no methods exist to solve this problem, we propose to use SPADE [25] followed by single-image MPI rendering [36] as a baseline. Our method better preserves thin structures and produces geometrically consistent views. Animated ﬁgure. Please view in Adobe
Reader and click on the image to see the animation. Other PDF viewers may have issues, in which case please refer to the supplementary materials. consistency between multiple output views. Speciﬁcally, instead of converting the semantic map to an RGB image, we propose to ﬁrst uplift the 2D semantics into layered 3D semantics with a structure similar to multi-plane images (MPI) for RGB images [30, 36]. We call this structure lifted semantics.
Unlike MPIs, to relax the memory requirements and for translation efﬁciency, our lifted semantics use a hybrid representation with a small set of semantic layers and a larger set of transparency layers.
We convert the lifted semantics to layered features, which we refer to as layered appearance, and combine them with the transparency layers. Finally, we project the resulting appearance features onto the target views and convert them to RGB images with a small network. The late fusion of the lifted semantics is key to the quality of our results. In summary, our work beneﬁts from, and leverages three main observations: 1. Semantic maps naturally disambiguate between texture edges and geometric discontinuities, thus offering a representation well-suited to generate novel views. 2. As a result, preserving semantic information for as long as possible in the pipeline, rather than converting to RGB early, encourages geometric consistency between the output views. 3. However, projecting the semantic information directly in the output views and converting to RGB causes photometric inconsistencies, which can be addressed by converting the semantics to an
MPI-based representation of appearance ﬁrst.
We perform extensive experimental analysis on three different multi-view datasets: CARLA [14],
Cityscapes [12], and Virtual-KITTI-2 [3]. We show both qualitatively and quantitatively that our approach, which compares favorably with strong baseline techniques, produces novel-view images that are geometrically and semantically consistent. In addition, we also demonstrate that we can estimate high-quality depth information from single-view semantics. 2