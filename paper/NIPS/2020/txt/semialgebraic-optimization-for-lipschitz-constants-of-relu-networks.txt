Abstract
The Lipschitz constant of a network plays an important role in many applications of deep learning, such as robustness certiﬁcation and Wasserstein Generative
Adversarial Network. We introduce a semideﬁnite programming hierarchy to estimate the global and local Lipschitz constant of a multiple layer deep neural network. The novelty is to combine a polynomial lifting for ReLU functions derivatives with a weak generalization of Putinar’s positivity certiﬁcate. This idea could also apply to other, nearly sparse, polynomial optimization problems in machine learning. We empirically demonstrate that our method provides a trade-off with respect to state of the art linear programming approach, and in some cases we obtain better bounds in less time. 1

Introduction
We focus on the multiple layer networks with ReLU activations. We propose a computationally efﬁcient method to give a valid upper bound on the Lipschitz constant of such networks. Recall that a function f , deﬁned on a convex set X ⊆ Rn, is L-Lipschitz with respect to the norm || · || if for all x, y ∈ X , we have |f (x) − f (y)| ≤ L||x − y||. The Lipschitz constant of f with respect to norm
|| · ||, denoted by L||·||
, is the inﬁmum of all those valid Ls: f
L||·|| f
:= inf{L : ∀x, y ∈ X , |f (x) − f (y)| ≤ L||x − y||} . (1)
For deep networks, they play an important role in many applications related to robustness certiﬁcation which has emerged as an active topic. See recent works [9, 31] based on semideﬁnite programming (SDP), [8, 40] based on linear programming (LP), [34] based on mixed integer programming (MIP), and [6, 38, 39, 41] based on outer polytope approximation, [7] based on averaged activation operators.
We follow a different route and compute upper bounds on the Lipschitz constant of neural networks
[35].
Another important application is the Wasserstein Generative Adversarial Network (WGAN) [3].
Wasserstein distance is estimated by using the space of functions encoded by 1-Lipschitz neural networks. This requires a precise estimation of the Lipschitz constants , see recent contributions
[2, 11, 25]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Recently there has been a growing interest in polynomial optimization for such problems. In [31], robustness certiﬁcation is modeled as a quadratically constrained quadratic problem (QCQP) for
ReLU networks. Similarly, in [21], an upper bound on the Lipschitz constant of an ELU network is obtained from a polynomial optimization problem (POP). In contrast to optimization problems with more general functions, powerful (global) positivity certiﬁcates are available for POP. Such certiﬁcates are needed to approximate global optima as closely as desired [20].
Such positivity certiﬁcates have been already applied with success in various areas of science and engineering. The ﬁrst attempt to compute lower bounds of a QCQP by solving an SDP can be traced back to Shor [32], recently applied to certify robustness of neural networks in [31]. Converging LP based hierarchies are based on Krivine-Stengle’s certiﬁcates [15, 19, 33]. In [21], sparse versions are used to bound the Lipschitz constant of neural networks. On the other hand, Putinar’s certiﬁcate
[17, 29] is implemented via an SDP-based hierarchy (a.k.a., “Lasserre’s hierarchy”) and provides converging approximate solutions of a POP. Various applications are described in [16], see also its application to large scale optimal power ﬂow problems in [26, 27] and for roundoff error certiﬁcation in [24]. The LP hierarchy is cheaper than the SDP hierarchy, but less efﬁcient for combinatorial optimization [22], and cannot converge in ﬁnitely many steps for continuous POPs. Finally, weaker positivity certiﬁcates can be used, for example DSOS/SDSOS [1] based on second-order cone programming, or hybrid BSOS hierarchy [19]. 1.1