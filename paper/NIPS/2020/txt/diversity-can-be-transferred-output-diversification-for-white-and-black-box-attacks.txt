Abstract
Adversarial attacks often involve random perturbations of the inputs drawn from uniform or Gaussian distributions, e.g., to initialize optimization-based white-box attacks or generate update directions in black-box attacks. These simple perturbations, however, could be sub-optimal as they are agnostic to the model being attacked. To improve the efﬁciency of these attacks, we propose Output
Diversiﬁed Sampling (ODS), a novel sampling strategy that attempts to maximize diversity in the target model’s outputs among the generated samples. While ODS is a gradient-based strategy, the diversity offered by ODS is transferable and can be helpful for both white-box and black-box attacks via surrogate models. Empirically, we demonstrate that ODS signiﬁcantly improves the performance of existing white-box and black-box attacks. In particular, ODS reduces the number of queries needed for state-of-the-art black-box attacks on ImageNet by a factor of two. 1

Introduction
Deep neural networks have achieved great success in image classiﬁcation. However, it is known that they are vulnerable to adversarial examples [1] — small perturbations imperceptible to humans that cause classiﬁers to output wrong predictions. Several studies have focused on improving model robustness against these malicious perturbations. Examples include adversarial training [2, 3], input puriﬁcation using generative models [4, 5], regularization of the training loss [6, 7, 8, 9], and certiﬁed defenses [10, 11, 12].
Strong attacking methods are crucial for evaluating the robustness of classiﬁers and defense mecha-nisms. Many existing adversarial attacks rely on random sampling, i.e., adding small random noise to the input. In white-box settings, random sampling is widely used for random restarts [13, 14, 15, 16] to ﬁnd a diverse set of starting points for the attacks. Some black-box attack methods also use random sampling to explore update directions [17, 18] or to estimate gradients of the target mod-els [19, 20, 21]. In these attacks, random perturbations are typically sampled from a naïve uniform or
Gaussian distribution in the input pixel space.
Random sampling in the input space, however, may not sufﬁciently explore the output (logits) space of a neural network — diversity in the input space does not directly translate to diversity in the output space of a deep nonlinear model. We illustrate this phenomenon in the left panel of Figure 1. When we add random perturbations to an image in the input space (see dashed blue arrows in the ﬁrst plot of Figure 1), the corresponding output logits could be very similar to the output for the original image (as illustrated by the second plot of Figure 1). Empirically, we observe that this phenomenon can negatively impact the performance of attack methods.
To overcome this issue, we propose a sampling strategy designed to obtain samples that are diverse in the output space. Our idea is to perturb an input away from the original one as measured directly by 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Input space
Output space
Output space (surrogate model)
Output space (target model)
Figure 1: Illustration of the differences between random sampling (blue dashed arrows) and ODS (red solid arrows). In each ﬁgure, the black ‘o’ corresponds to an original image, and white ‘o’s represent sampled perturbations. (Left): white-box setting. Perturbations by ODS in the input space are crafted by maximizing the distance in the output space. (Right): black-box setting. Perturbations crafted on the surrogate model transfer well to perturbations on the target model. distances in the output space (see solid red arrows in the second plot in Figure 1). First, we randomly specify a direction in the output space. Next, we perform gradient-based optimization to generate a perturbation in the input space that yields a large change in the speciﬁed direction. We call this new sampling technique Output Diversiﬁed Sampling (ODS).
ODS can improve adversarial attacks under both white-box and black-box settings. For white-box attacks, we exploit ODS to initialize the optimization procedure of ﬁnding adversarial examples (called ODI). ODI typically provides much more diverse (and effective) starting points for adversarial attacks. Moreover, this initialization strategy is agnostic to the underlying attack method, and can be incorporated into most optimization-based white-box attack methods. Empirically, we demonstrate that ODI improves the performance of (cid:96)∞ and (cid:96)2 attacks compared to naïve initialization methods.
In particular, the PGD attack with ODI outperforms the state-of-the-art MultiTargeted attack [16] against pre-trained defense models, with 50 times smaller computational complexity on CIFAR-10.
In black-box settings, we cannot directly apply ODS because we do not have access to gradients of the target model. As an alternative, we apply ODS to surrogate models and observe that the resulting samples are diverse with respect to the target model: diversity in the output space transfers (see the rightmost two plots in Figure 1). Empirically, we demonstrate that ODS can reduce the number of queries needed for a score-based attack (SimBA [18]) by a factor of two on ImageNet. ODS also shows better query-efﬁciency than P-RGF [22], which is another method exploiting surrogate models to improve a black-box attack. These attacks with ODS achieve better query-efﬁciency than the state-of-the-art Square Attack [23]. In addition, ODS with a decision-based attack (Boundary
Attack [17]) reduces the median perturbation distances of adversarial examples by a factor of three compared to the state-of-the-art HopSkipJump [24] and Sign-OPT [25] attacks. 2 Preliminaries
We denote an image classiﬁer as f : x ∈ [0, 1]D (cid:55)→ z ∈ RC, where x is an input image, z represents the logits, and C is the number of classes. We use h(x) = arg maxc=1,...,C fc(x) to denote the model prediction, where fc(x) is the c-th element of f (x).
Adversarial attacks can be classiﬁed into targeted and untargeted attacks. Given an image x, a label y and a classiﬁer f , the purpose of untargeted attacks is to ﬁnd an adversarial example xadv that is similar to x but causes misclassiﬁcation h(xadv) (cid:54)= y. In targeted settings, attackers aim to change the model prediction h(xadv) to a particular target label t (cid:54)= y. The typical goal of adversarial attacks is to ﬁnd an adversarial example xadv within B(cid:15)(x) = {x + δ : (cid:107)δ(cid:107)p ≤ (cid:15)}, i.e., the (cid:15)-radius ball around an original image x. Another common setting is to ﬁnd a valid adversarial example with the smallest (cid:96)p distance from the original image.
White-box attacks
In white-box settings, attackers can access full information of the target model.
One strong and popular example is the Projected Gradient Descent (PGD) attack [2], which iteratively applies the following update rule: xadv k+1 = ProjB(cid:15)(x) (cid:16) xadv k + η sign (cid:16)
∇xadv k 2
L(f (xadv k ), y) (cid:17)(cid:17) (1)
where ProjB(cid:15)(x)(xadv) (cid:44) arg minx(cid:48)∈B(cid:15)(x) (cid:107)xadv − x(cid:48)(cid:107)p, η is the step size, and L(f (x), y) is a loss function, e.g. the margin loss deﬁned as maxi(cid:54)=y fi(x) − fy(x). To increase the odds of success, the procedure is restarted multiple times with uniformly sampled initial inputs from B(cid:15)(x).
Black-box attacks
In black-box settings, the attacker only has access to outputs of the target model without knowing its architecture and weights. Black-box attacks can be largely classiﬁed into transfer-based, score-based, and decision-based methods respectively. Transfer-based attacks craft white-box adversarial examples with respect to surrogate models, and transfer them to the target model. The surrogate models are typically trained with the same dataset as the target model so that they are close to each other. In score-based settings, attackers can know the output scores (logits) of the classiﬁer; while for decision-based settings, attackers can only access the output labels of the classiﬁer. For these two approaches, attacks are typically evaluated in terms of query efﬁciency, i.e. the number of queries needed to generate an adversarial example and its perturbation size.
Recently, several studies [22, 26, 27] employed surrogate models to estimate the gradients of the loss function of the target model. Some attack methods used random sampling in the input space, such as the decision-based Boundary Attack [17] and the score-based Simple Black-Box Attack [18]. 3 Output Diversiﬁed Sampling
As intuitively presented in Figure 1, random sampling in the input space does not necessarily produce samples with high diversity as measured in the output space. To address this problem, we propose Output Diversiﬁed Sampling (ODS). Given an image x, a classiﬁer f and the direction of diversiﬁcation wd ∈ RC, we deﬁne the normalized perturbation vector of ODS as follows: vODS(x, f , wd) = (cid:124) d f (x))
∇x(w (cid:124) d f (x))(cid:107)2 (cid:107)∇x(w
, (2) where wd is sampled from the uniform distribution over [−1, 1]C. Below we show how to enhance white- and black-box attacks with ODS. 3.1
Initialization with ODS for white-box attacks
In white-box settings, we utilize ODS for initialization (ODI) to generate output-diversiﬁed starting points. Given an original input xorg and the direction for ODS wd, we try to ﬁnd a restart point x that (cid:124) d (f (x) − f (xorg)) via the following iterative is as far away from xorg as possible by maximizing w update: xk+1 = ProjB(xorg) (xk + ηODI sign(vODS(xk, f , wd))) (3) where B(xorg) is the set of allowed perturbations, which is typically an (cid:15)-ball in (cid:96)p norm, and ηODI is a step size. When applying ODI to (cid:96)2 attacks, we omit the sign function. After some steps of ODI, we start an attack from the restart point obtained by ODI. We sample a new direction wd for each restart in order to obtain diversiﬁed starting points for the attacks. We provide the pseudo-code for
ODI in Algorithm A of the Appendix.
One sampling step of ODI costs roughly the same time as one iteration of most gradient-based attacks (e.g., PGD). Empirically, we observe that the number of ODI steps NODI = 2 is already sufﬁcient to obtain diversiﬁed starting points (details of the sensitivity analysis are in Appendix C.2), and ﬁx
NODI = 2 in all our experiments unless otherwise speciﬁed. We emphasize that ODS is not limited to
PGD, and can be applied to a wide family of optimization-based adversarial attacks.
Experimental veriﬁcation of increased diversity: We quantitatively evaluate the diversity of start-ing points in terms of pairwise distances of output values f (x), conﬁrming the intuition presented in the left two plots of Figure 1. We take a robust model on CIFAR-10 as an example of target models, and generate starting points with both ODI and standard uniform initialization to calculate the mean pairwise distance. The pairwise distance (i.e. diversity) obtained by ODI is 6.41, which is about 15 times larger than that from uniform initialization (0.38). In addition, PGD with the same steps as
ODI does not generate diverse samples (pairwise distance is 0.43). Details are in Appendix C.1. 3
Algorithm 1 Simple Black-box Attack [18] with ODS 1: Input: A targeted image x, loss function L, a target classiﬁer f , a set of surrogate models G 2: Output: attack result xadv 3: Set the starting point xadv = x 4: while xadv is not adversary do 5: 6: 7: 8: 9:
Choose a surrogate model g from G, and sample wd ∼ U (−1, 1)C
Set q = vODS(xadv, g, wd) for α ∈ {(cid:15), −(cid:15)} do if L(xadv + α · q) > L(xadv) then
Set xadv = xadv + α · q and break 3.2 Sampling update directions with ODS for black-box attacks
In black-box settings, we employ ODS to sample update directions instead of random sampling.
Given a target classiﬁer f , we cannot directly calculate the ODS perturbation vODS(x, f , wd) because gradients of the target model f are unknown. Instead, we introduce a surrogate model g and calculate the ODS vector vODS(x, g, wd).
ODS can be applied to attack methods that rely on random sampling in the input space. Since many black-box attacks use random sampling to explore update directions [17, 18] or estimate gradients of the target models [19, 21, 24], ODS has broad applications. In this paper, we apply ODS to two popular black-box attacks that use random sampling: decision-based Boundary Attack [17] and score-based Simple Black-Box Attack (SimBA [18]). In addition, we compare ODS with P-RGF [22], which is an another attack method using surrogate models.
To illustrate how we apply ODS to existing black-box attack methods, we provide the pseudo-code of SimBA [18] with ODS in Algorithm 1. The original SimBA algorithm picks an update direction q randomly from a group of candidates Q that are orthonormal to each other. We replace it with
ODS, as shown in the line 5 and 6 of Algorithm 1. For other attacks, we replace random sampling with ODS in a similar way. Note that in Algorithm 1, we make use of multiple surrogate models and uniformly sample one each time, since we empirically found that using multiple surrogate models can make attacks stronger.
Experimental veriﬁcation of increased diversity: We quantitatively evaluate that ODS can lead to high diversity in the output space of the target model, as shown in the right two plots of Figure 1.
We use pre-trained Resnet50 [28] and VGG19 [29] models on ImageNet as the target and surrogate models respectively. We calculate and compare the mean pairwise distances of samples with ODS and random Gaussian sampling. The pairwise distance (i.e. diversity) for ODS is 0.79, which is 10 times larger than Gaussian sampling (0.07). Details are in Appendix D.1. We additionally observe that ODS does not produce diversiﬁed samples when we use random networks as surrogate models.
This indicates that good surrogate models are crucial for transferring diversity. 4 Experiments in white-box settings
In this section, we show that the diversity offered by ODI can improve white-box attacks for both (cid:96)∞ and (cid:96)2 distances. Moreover, we demonstrate that a simple combination of PGD and ODI achieves new state-of-the-art attack success rates. All experiments are for untargeted attacks. 4.1 Efﬁcacy of ODI for white-box attacks
We combine ODI with two popular attacks: PGD attack [2] with the (cid:96)∞ norm and C&W attack [30] with the (cid:96)2 norm. We run these attacks on MNIST, CIFAR-10 and ImageNet.
Setup We perform attacks against three adversarially trained models from MadryLab1 [2] for
MNIST and CIFAR-10 and the Feature Denoising ResNet152 network2 [31] for ImageNet. For PGD 1https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/ cifar10_challenge. We use their secret model. 2https://github.com/facebookresearch/ImageNet-Adversarial-Training. 4
attacks, we evaluate the model accuracy with 20 restarts, where starting points are uniformly sampled over an (cid:15)-ball for the naïve resampling. For C&W attacks, we calculate the minimum (cid:96)2 perturbation that yields a valid adversarial example among 10 restarts for each image, and measure the average of the minimum perturbations. Note that the original paper of C&W [30] attacks did not apply random restarts. Here for the naïve initialization of C&W attacks we sample starting points from a Gaussian distribution and clip it into an (cid:15)-ball (details in Appendix B.1).
For fair comparison, we test different attack methods with the same amount of computation. Speciﬁ-cally, we compare k-step PGD with naïve initialization (denoted as PGD-k) against (k-2)-step PGD with 2-step ODI (denoted as ODI-PGD-(k-2)). We do not adjust the number of steps for C&W attacks because the computation time of 2-step ODI are negligible for C&W attacks.
Table 1: Comparing different white-box attacks. We report model accuracy (lower is better) for PGD and average of the minimum (cid:96)2 perturbations (lower is better) for C&W. All results are the average of three trials. model
PGD naïve (PGD-k) ODI (ODI-PGD-(k-2))
C&W naïve
ODI
MNIST
CIFAR-10
ImageNet 90.31 ± 0.02% 46.06 ± 0.02% 43.5 ± 0.0% 90.21 ± 0.05% 44.45 ± 0.02% 42.3 ± 0.0% 2.27 ± 0.00 0.71 ± 0.00 1.58 ± 0.00 2.25 ± 0.01 0.67 ± 0.00 1.32 ± 0.01
Results We summarize all quantitative results in Table 1. Attack performances with ODI are better than naïve initialization for all models and attacks. The improvement by ODI on the CIFAR-10 and
ImageNet models is more signiﬁcant than on the MNIST model. We hypothesize that this is due to the difference in model non-linearity. When a target model includes more non-linear transformations, the difference in diversity between the input and output space could be larger, in which case ODI will be more effective in providing a diverse set of restarts. 4.2 Comparison between PGD attack with ODI and state-of-the-art attacks
To further demonstrate the power of ODI, we perform ODI-PGD against MadryLab’s robust mod-els [2] on MNIST and CIFAR-10 and compare ODI-PGD with state-of-the-art attacks.
Setup One state-of-the-art attack we compare with is the well-tuned PGD attack [16], which achieved 88.21% accuracy for the robust MNIST model. The other attack we focus on is the
MultiTargeted attack [16], which obtained 44.03% accuracy against the robust CIFAR-10 model. We use all test images on each dataset and perform ODI-PGD under two different settings. One is the same as Section 4.1. The other is ODI-PGD with tuned hyperparameters, e.g. increasing the number of steps and restarts. Please see Appendix B.2 for more details of tuning.
Table 2: Comparison of ODI-PGD with state-of-the-art attacks against pre-trained defense models.
The complexity rows display products of the number of steps and restarts. Results for ODI-PGD are the average of three trials. For ODI-PGD, the number of steps is the sum of ODS and PGD steps. model
MNIST
CIFAR-10
ODI-PGD (in Sec. 4.1) tuned
ODI-PGD tuned PGD
[16]
MultiTargeted
[16] accuracy complexity accuracy complexity 90.21 ± 0.05% 88.13 ± 0.01% 1000 × 1000 44.45 ± 0.02% 44.00 ± 0.01% 40 × 20 20 × 20 150 × 20 88.21% 1000 × 1800 44.51% 1000 × 180 88.36% 1000 × 1800 44.03% 1000 × 180
Results We summarize the comparison between ODI-PGD and state-of-the-art attacks in Table 2.
Our tuned ODI-PGD reduces the accuracy to 88.13% for the MNIST model, and to 44.00% for the
CIFAR-10 model. These results outperform existing state-of-the-art attacks. 5
To compare their running time, we report the total number of steps (the number of steps multiplied by the number of restarts) as a metric of complexity, because the total number of steps is equal to the number of gradient computations (the computation time per gradient evaluation is comparable for all gradient-based attacks). In Table 2, the computational cost of tuned ODI-PGD is smaller than that of state-of-the-art attacks, and especially 50 times smaller on CIFAR-10. Surprisingly, even without tuning ODI-PGD (in the ﬁrst column) can still outperform tuned PGD [16] while also being drastically more efﬁcient computationally. 5 Experiments in black-box settings
In this section, we demonstrate that black-box attacks combined with ODS signiﬁcantly reduce the number of queries needed to generate adversarial examples. In experiments below, we randomly sample 300 correctly classiﬁed images from the ImageNet validation set. We evaluate both untargeted and targeted attacks. For targeted attacks, we uniformly sample target labels. 5.1 Query-efﬁciency of score-based attacks with ODS 5.1.1 Applying ODS to score-based attacks
To show the efﬁciency of ODS, we combine ODS with the score-based Simple Black-Box Attack (SimBA) [18]. SimBA randomly samples a vector and either adds or subtracts the vector to the target image to explore update directions. The vector is sampled from a pre-deﬁned set of orthonormal vectors in the input space. These are the discrete cosine transform (DCT) basis vectors in the original paper [18]. We replace the DCT basis vectors with ODS sampling (called SimBA-ODS),
Setup We use pre-trained ResNet50 model as the target model and select four pre-trained models (VGG19, ResNet34, DenseNet121 [32], MobileNetV2 [33]) as surrogate models. We set the same hyperparameters for SimBA as [18]: the step size is 0.2 and the number of iterations (max queries) is 10000 (20000) for untargeted attacks and 30000 (60000) for targeted attacks. As the loss function in SimBA, we employ the margin loss for untargeted attacks and the cross-entropy loss for targeted attacks.
Results First, we compare SimBA-DCT [18] and SimBA-ODS. Table 3 reports the number of queries and the median (cid:96)2 perturbations. Remarkably, SimBA-ODS reduces the average number of queries by a factor between 2 and 3 compared to SimBA-DCT for both untargeted and targeted settings. This conﬁrms that ODS not only helps white-box attacks, but also leads to signiﬁcant improvements of query-efﬁciency in black-box settings. In addition, SimBA-ODS decreases the average perturbation sizes by around a factor of two, which means that ODS helps ﬁnd better adversarial examples that are closer to the original image.
Table 3: Number of queries and size of (cid:96)2 perturbations for score-based attacks. attack num. of surrogates success rate untargeted average median (cid:96)2 queries perturbation targeted success average median (cid:96)2 queries perturbation rate
SimBA-DCT [18]
SimBA-ODS 0 4 100.0% 909 100.0% 242 2.95 1.40 97.0% 7114 98.3% 3503 7.00 3.55 5.1.2 Comparing ODS with other methods using surrogate models
We consider another black-box attack that relies on surrogate models: P-RGF [22], which improves over the original RGF (random gradient-free) method for gradient estimation. P-RGF exploits prior knowledge from surrogate models to estimate the gradient more efﬁciently than RGF. Since RGF uses random sampling to estimate the gradient, we propose to apply ODS to RGF (new attack named
ODS-RGF) and compare it with P-RGF under (cid:96)2 and (cid:96)∞ norms.
For fair comparison, we use a single surrogate model as in [22]. We choose pre-trained ResNet50 model as the target model and ResNet34 model as the surrogate model. We give query-efﬁciency 6
Table 4: Comparison of ODS-RGF and P-RGF on ImageNet. Hyperparameters for RGF are same as [22] :max queries are 10000, sample size is 10, step size is 0.5 ((cid:96)2) and 0.005 ((cid:96)∞), and epsilon is
√ 0.001 · 2242 · 3 ((cid:96)2) and 0.05 ((cid:96)∞). norm attack (cid:96)2 (cid:96)∞
RGF
P-RGF [25]
ODS-RGF
RGF
P-RGF [25]
ODS-RGF num. of surrogates untargeted success average median (cid:96)2 queries perturbation rate targeted success average median (cid:96)2 queries perturbation rate 0 1 1 0 1 1 100.0% 633 100.0% 211 100.0% 133 520 97.0% 99.7% 88 100.0% 74 3.07 2.08 1.50
---99.3% 3141 97.0% 2296 99.3% 1043 25.0% 2971 65.3% 2123 92.0% 985 8.23 7.03 4.47
---results of both methods in Table 4. The average number of queries required by ODS-RGF is less than that of P-RGF in all settings. This suggests ODS-RGF can estimate the gradient more precisely than
P-RGF by exploiting diversity obtained via ODS and surrogate models. The differences between ODS-RGF and P-RGF are signiﬁcant in targeted settings, since ODS-RGF achieves smaller perturbations than P-RGF (see median perturbation column). To verify the robustness of our results, we also ran experiments using VGG19 as a surrogate model and obtained similar results.
We additionally consider TREMBA [34], a black-box attack (restricted to the (cid:96)∞-norm) that is state-of-the-art among those using surrogate models. In TREMBA, a low-dimensional embedding is learned via surrogate models so as to obtain initial adversarial examples which are then updated using a score-based attack. Our results show that ODS-RGF combined with SI-NI-DIM [35], which is a state-of-the-art transfer-based attack, is comparable to TREMBA even though ODS-RGF is not restricted to the (cid:96)∞-norm. Results and more details are provided in Appendix D.3. 5.1.3 Comparison of ODS with state-of-the-art score-based attacks
To show the advantage of ODS and surrogate models, we compare SimBA-ODS and ODS-RGF with the Square Attack [23], which is a state-of-the-art attack for both (cid:96)∞ and (cid:96)2 norms when surrogate models are not allowed. For comparison, we regard SimBA as (cid:96)2 bounded attacks: the attack is successful when adversarial (cid:96)2 perturbation is less than a given bound (cid:15). We set (cid:15) = 5 ((cid:96)2) and 0.05 ((cid:96)∞) as well as other hyperparameters according to the original paper [23], except that we set the max number of queries to 20000 for untargeted attacks and 60000 for targeted attacks. For
ODS-RGF, we use four surrogate models as discussed in Section 5.1.1 for SimBA-ODS.
Table 5: Number of queries for attacks with ODS versus the Square Attack. norm attack (cid:96)2 (cid:96)∞
Square [23]
SimBA-ODS
ODS-RGF
Square [23]
ODS-RGF untargeted targeted num. of surrogates success rate average queries success rate average queries 0 4 4 0 4 99.7% 99.7% 100.0% 100.0 % 100.0 % 647 237 144 60 78 96.7% 11647 2843 90.3% 99.0% 1285 100.0% 2317 1242 97.7%
As shown in Table 5, the number of queries required for ODS-RGF and SimBA-ODS are lower than that of the Square Attack under the (cid:96)2 norm. The improvement is especially large for ODS-RGF.
The difference between ODS-RGF and SimBA-ODS mainly comes from different base attacks (i.e.,
RGF and SimBA). For the (cid:96)∞ norm setting, ODS-RGF is comparable to the Square Attack. We hypothesize that the beneﬁt of estimated gradients by RGF decreases under the (cid:96)∞ norm due to the sign function. However, because ODS can be freely combined with many base attacks, a stronger base attack is likely to further improve query-efﬁciency. 7
5.2 Query-efﬁciency of decision-based attacks with ODS
We demonstrate that ODS also improves query-efﬁciency for decision-based attacks. We combine
ODS with the decision-based Boundary Attack [17]. The Boundary Attack starts from an image which is adversarial, and iteratively updates the image to ﬁnd smaller perturbations. To generate the update direction, the authors of [17] sampled a random noise vector from a Gaussian distribution
N (0, I) each step. We replace this random sampling procedure with sampling by ODS (we call the new method Boundary-ODS). We give the pseudo-code of Boundary-ODS in Algorithm B (in the
Appendix).
Setup We use the same settings as the previous section for score-based attacks: 300 validation images on ImageNet, pre-trained ResNet50 target model, and four pre-trained surrogate models. We test on both untargeted and targeted attacks. In targeted settings, we give randomly sampled images with target labels as initial images. We use the implementation in Foolbox [36] for Boundary Attack with default parameters, which is more efﬁcient than the original implementation.
We also compare Boundary-ODS with two state-of-the-art decision-based attacks: the HopSkipJump attack [24] and the Sign-OPT attack [25]. We use the implementation in ART [37] for HopSkipJump and the author’s implementation for Sign-OPT. We set default hyperparameters for both attacks.
Results Table 6 summarizes the median sizes of (cid:96)2 adversarial perturbations obtained with a ﬁxed number of queries. Clearly, Boundary-ODS signiﬁcantly improves query-efﬁciency compared to the original Boundary Attack. In fact, Boundary-ODS outperforms state-of-the-art attacks: it decreases the median (cid:96)2 perturbation at 10000 queries to less than one-third of previous best untargeted attacks and less than one-fourth of previous best targeted attacks. We additionally describe the relationship between median (cid:96)2 perturbations and the number of queries in Figure 2. Note that Boundary-ODS outperforms other attacks, especially in targeted settings. Moreover, Boundary-ODS only needs fewer than 3500 queries to achieve the adversarial perturbation obtained by other attacks with 10000 queries.
Table 6: Median (cid:96)2 perturbations for Boundary-ODS and decision-based state-of-the-art attacks. attack
Boundary [17]
Boundary-ODS
HopSkipJump [24]
Sign-OPT [25] num. of surrogates 0 4 0 0 number of queries untargeted 5000 10000 1000 targeted 5000 11.46 0.98 3.50 3.98 4.30 0.57 1.79 2.01 73.94 27.24 65.88 68.75 41.88 6.84 33.98 36.93 1000 45.07 7.57 14.86 21.73 10000 27.05 3.76 18.25 22.43
Untargeted
Targeted
Figure 2: Relationship between median (cid:96)2 perturbations and the number of queries for decision-based attacks. Error bars show 25th and 75th percentile of (cid:96)2 perturbations. 5.3 Effectiveness of ODS with out-of-distribution images
Although several studies use prior knowledge from surrogate models to improve performance of black-box attacks, there is a drawback—those approaches require a dataset to train surrogate models. 8
In reality, it is typically impossible to obtain the same dataset used for training the target model. We show that ODS is applicable even when we only have a limited dataset that is out-of-distribution (OOD) and may contain only images with irrelevant labels.
We select 100 ImageNet classes which do not overlap with the classes used in the experiments of
Section 5.2. We train surrogate models using an OOD training dataset with these 100 classes. We train ﬁve surrogate models with the same ResNet18 architecture because multiple surrogate models provide diversiﬁed directions. Then, we run Boundary-ODS with the trained surrogate models under the same setting as Section 5.2. As shown in Table 7, although Boundary-ODS with the OOD training dataset underperforms Boundary-ODS with the full dataset, it is still signiﬁcantly better than the original Boundary Attack with random sampling. This demonstrates that the improved diversity achieved by ODS improves black-box attacks even if we only have OOD images to train a surrogate.
Table 7: Median (cid:96)2 perturbations for Boundary-ODS with surrogate models trained on OOD images. attack
Boundary [17]
Boundary-ODS (OOD dataset)
Boundary-ODS (full dataset in Sec. 5.2) number of queries untargeted 5000 10000 1000 targeted 5000 11.46 1.63 0.98 4.30 0.98 0.57 73.94 41.67 27.24 41.88 13.72 6.84 1000 45.07 11.27 7.57 10000 27.05 8.39 3.76 6