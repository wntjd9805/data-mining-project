Abstract
We present a series of new PAC-Bayes learning guarantees for randomized algo-rithms with sample-dependent priors. Our most general bounds make no assump-tion on the priors and are given in terms of certain covering numbers under the inﬁnite-Rényi divergence and the (cid:96)1 distance. We show how to use these general bounds to derive learning bounds in the setting where the sample-dependent priors obey an inﬁnite-Rényi divergence or (cid:96)1-distance sensitivity condition. We also provide a ﬂexible framework for computing PAC-Bayes bounds, under certain stability assumptions on the sample-dependent priors, and show how to use this framework to give more reﬁned bounds when the priors satisfy an inﬁnite-Rényi divergence sensitivity condition. 1

Introduction
The PAC-Bayesian framework provides generalization bounds for the performance of randomized learning algorithms [McAllester, 1999b,a, Shawe-Taylor and Williamson, 1997]. Rather than out-putting a single hypothesis, such algorithms output a probability distribution Q (the posterior) over a hypothesis set H. In the PAC-Bayes framework, the generalization guarantees associated with Q are typically expressed in terms of the relative entropy, D(Q ∥ P ), where P is a ﬁxed prior distribution over the hypothesis set. In the traditional framework, the prior P must be selected before receiving a training sample [Langford and Caruana, 2002, Langford and Seeger, 2001, Seeger, 2002].
In recent years, there have been efforts to establish more reﬁned PAC-Bayes bounds in which the prior can depend on the distribution generating the data or a separate sample drawn from the same distribution [Catoni, 2007, Ambroladze et al., 2007, Parrado-Hernández et al., 2012, Lever et al., 2013]. However, in practice, information about the underlying data distribution is available only via the training sample and discarding a fraction of that data to compute a generalization bound can be wasteful, motivating the study of PAC-Bayes bounds for sample-dependent priors.1
In the context of overparameterized deep neural networks, where deriving non-vacuous generalization bounds is notoriously hard, it has been argued that sample-dependent priors can lead to ﬁner gen-eralization bounds [Nagarajan and Kolter, 2019, Dziugaite and Roy, 2017, Neyshabur et al., 2018]. 1The same notion has also been called data-dependent priors in the literature [Dziugaite and Roy, 2018a,
Negrea et al., 2019, Dziugaite et al., 2020, Haghifam et al., 2020]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Sample-dependent priors can also lead to new learning methods. For instance, when training deep neu-ral networks via stochastic gradient descent, a standard choice for the prior P is a Gaussian centered around the parameters at random initialization. In this case, D(Q ∥ P ) is related to the distance from initialization of the ﬁnal iterate’s parameters. This, however, can be large in most realistic settings and it is therefore more appealing to choose as prior a Gaussian centered around parameters obtained by running some amount of pre-training using the training data, and subsequently use that prior as a guide for ﬁne-tuning the parameters with additional training. This combination of pre-training followed by ﬁne-tuning is common practice and clearly such a choice would be sample-dependent.
Sample-dependent priors are also relevant in emerging scenarios such as adversarial training. A common practice here is to smooth a given classiﬁer by injecting Gaussian noise into the inputs.
This results in a classiﬁer with a more favorable Lipschitz property, thereby improving robustness
[Lecuyer et al., 2019, Cohen et al., 2019]. While the choice of the noise magnitude depends on the input, typically, these methods choose a priori a uniform noise magnitude across all inputs. It is much more appealing instead to choose a posterior over the noise magnitudes and inform this choice by carefully selecting a prior P based on the sample, over the noise magnitudes, and using the prior P as a regularizer to guide the search for the posterior.
From a theoretical perspective, there has been little work on generalization bounds for sample-dependent priors. The recent work of [Dziugaite and Roy, 2018a,b] took an important step in this direction by showing that for sample-dependent priors chosen via a differentially private mechanism
PAC-Bayesian generalization bounds can be derived. They also showed that weaker conditions where the sample-dependent prior need only be “close” to a differentially private prior sufﬁce for the bounds.
We also recently became aware of [Rivasplata et al., 2020], which will appear at NeurIPS 2020 as well; this work also discusses general sample-dependent priors, although it is not yet apparent how the results compare. The following are our main contributions: 1. General bounds via covering numbers. We give general PAC-Bayes bounds, with no assumption on the sample-dependent priors, in terms of certain covering numbers of the priors. We provide two such bounds using covering numbers computed with the inﬁnite-Rényi divergence and the (cid:96)1 distance. 2. Bounds for stable priors. We say that sample-dependent priors satisfy prior stability, if for any two samples S and S′ that differ in exactly one input, the corresponding sample-dependent priors PS and PS′ are close. Closeness here is measured either in terms of the inﬁnite-Rényi divergence or the (cid:96)1 distance. For both cases, we show that our general covering-number-based bounds already give non-trivial generalization bounds. 3. Framework for PAC-Bayes bounds under prior stability. Building on the work of Foster et al.
[2019] on hypothesis set stability, we provide a general method for deriving PAC-Bayes bounds assuming prior stability. We show how this method leads to reﬁnements of the PAC-Bayes bound mentioned above for inﬁnite-Rényi divergence prior stability.