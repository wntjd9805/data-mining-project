Abstract
In the last decade, there has been increasing interest in topological data analysis, a new methodology for using geometric structures in data for inference and learning.
A central theme in the area is the idea of persistence, which in its most basic form studies how measures of shape change as a scale parameter varies. There are now a number of frameworks that support statistics and machine learning in this context.
However, in many applications there are several different parameters one might wish to vary: for example, scale and density. In contrast to the one-parameter setting, techniques for applying statistics and machine learning in the setting of multiparameter persistence are not well understood due to the lack of a concise representation of the results.
We introduce a new descriptor for multiparameter persistence, which we call the Multiparameter Persistence Image, that is suitable for machine learning and statistical frameworks, is robust to perturbations in the data, has ﬁner resolution than existing descriptors based on slicing, and can be efﬁciently computed on data sets of realistic size. Moreover, we demonstrate its efﬁcacy by comparing its performance to other multiparameter descriptors on several classiﬁcation tasks. 1

Introduction
Topological data analysis (TDA) is a new and rapidly evolving branch of computer science and statistics that provides tools to analyze geometric structures in data using ideas from algebraic topology. The success of clustering methods and nonlinear dimensionality reduction techniques make it clear that even crude approaches to leveraging the geometry of data can be very effective. TDA provides more reﬁned geometric information, and indeed, there have been a variety of successful applications of TDA, including, among others: graph analysis [CCI+20, ZW19], computational biology [ACC+20, CR20, GPCI15, HMMB19, KDS+18, RB19], ﬁnance [dCBSB17, GGK+18,
GK18] and computer graphics [COO15, LOC14, PSO18].
The standard setup for the use of TDA is a data set given as a ﬁnite metric space X (i.e., points and a distance function) and a continuous function f : X → R. This function can be understood as giving a parameter that ﬁlters X and encodes how the topology changes as the parameter varies. A classical setting is when X = Rn with the standard Euclidean distance and f is given as the distance to a point cloud P ⊂ Rn, that is, f (x) = min {(cid:107)x − p(cid:107) : p ∈ P }. See Figure 1. Another example is when X is a graph G and f is a function deﬁned on the nodes of G (see Supplementary Material, Section 4).
The fundamental geometric summary of TDA, the persistence diagram, characterizes the changing topology of the family of sublevel sets of f , that is, the family {Fα}α∈R = {x ∈ X : f (x) ≤ α}α∈R.
For instance, in the point cloud setting, the sublevel set Fα is the union of balls of radius α centered on the points of P . See Figure 1. Persistence diagrams are built by increasing α from −∞ to +∞, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
which creates a growing sequence of sublevel sets, called a ﬁltration, and recording the various topological changes that occur in this process. These changes are eventually summarized into a set of points in the plane R2, where each point represents a speciﬁc topological feature of the data set (e.g., a connected component, a cycle, a cavity...), and its coordinates are the parameter values α1, α2 for which the structure appeared and disappeared in the ﬁltration. Often one views points away from the diagonal, i.e., which represent topological features that existed at many scales, as encoding robust geometric information and near-diagonal points as noise.
Figure 1: The ﬁltration of the distance to the point cloud function is a sequence of union of balls with growing radii. As the radius increases, topological structures appear (e.g., the loop highlighted in red in the third union) and are eventually ﬁlled in. The “birth” and “death” values are the coordinates for a point in the persistence diagram, colored to indicate their dimension (0 is red, 1 is blue).
A problem with the summaries provided by persistence diagrams is that the space of persistence diagrams is not a convenient place to do statistics. For example, it is not a vector space, and centroids are hard to compute and not necessarily unique [TMMH14]. To handle this, a lot of work has gone into developing frameworks for supporting statistics, and machine learning for persistence diagrams has been developed in the past few years [AEK+17, BGMP14, Bub15, CCO17, CGLM15, FLR+14,
KHF16, LY18, RHBK15]. However, the use of single-parameter ﬁltrations often misses relevant information. For instance, in the point cloud setting, the scale ﬁltration does not account for density, and so can be susceptible to outliers; for example, see Figure 4.
There are various approaches to handling the issue of variation in density in the context of the statistical frameworks for standard persistence. A more ﬂexible general framework was introduced by
Carlsson and Zomorodian: multiparameter (sometimes called multidimensional) persistence [CZ09].
This formalism encodes multiple ﬁltration directions; the simplest example is the case of biﬁltrations, that is, by ﬁltering X with two parameters, or functions, instead of just one. For example, point cloud outliers can be detected by simultaneously ﬁltering by scale and density.
However, working with multiparameter persistence is substantially more difﬁcult. Analogues of persistence diagrams can be deﬁned in restricted settings for some speciﬁc bivariate and multivariate functions [BCB18, BLO20, CKMW20, CO19] and metrics between them have been deﬁned, as well as algorithms for their computation [KN19, KLO19]. But in general there is no simple summary of a multiparameter persistence diagram and the integration with statistics and machine learning is still unsettled. Existing approaches for statistics in the context of bivariate functions [CFK+19, Vip20] are deﬁned by slicing, that is, by considering persistence diagrams associated to linear combinations of the coordinates of the bivariate function, which is known to be limited since it is equivalent to an incomplete summary of multiparameter persistence called the rank invariant [CZ09].
In this article, we provide a more reﬁned numerical invariant of multiparameter persistence:
• We introduce the Multiparameter Persistence Image, a compact descriptor which inte-grates information across slices by tracking changes in adjacent slices. We also investigate empirically the stability of this invariant in the face of perturbations of the slices.
• We show how to efﬁciently compute this descriptor using any black box matching algorithm, and provide more details for the vineyards algorithm [CSEM06] that we use in experi-ments. We also provide open-source implementation for our descriptor and for the other approaches [CFK+19, Vip20], in a public Python package [Car20].
• We demonstrate in several experiments classiﬁcation experiments that the multiparameter persistence image has performance comparable to or superior to existing summaries. 2
2