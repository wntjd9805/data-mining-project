Abstract
Disentangled representation learning has seen a surge in interest over recent times, generally focusing on new models which optimise one of many disparate dis-entanglement metrics. Symmetry Based Disentangled Representation learning introduced a robust mathematical framework that deﬁned precisely what is meant by a “linear disentangled representation”. This framework determined that such representations would depend on a particular decomposition of the symmetry group acting on the data, showing that actions would manifest through irreducible group representations acting on independent representational subspaces. Caselles-Dupré et al. [2019] subsequently proposed the ﬁrst model to induce and demonstrate a linear disentangled representation in a VAE model. In this work we empirically show that linear disentangled representations are not generally present in standard
VAE models and that they instead require altering the loss landscape to induce them. We proceed to show that such representations are a desirable property with regard to classical disentanglement metrics. Finally we propose a method to induce irreducible representations which forgoes the need for labelled action sequences, as was required by prior work. We explore a number of properties of this method, including the ability to learn from action sequences without knowledge of inter-mediate states and robustness under visual noise. We also demonstrate that it can successfully learn 4 independent symmetries directly from pixels. 1

Introduction
Many learning machines make use of an internal representation [Bengio et al., 2013] to help inform their decisions. It is often desirable for these representations to be interpretable in the sense that we can easily understand how individual parts contribute to solving the task at hand. Interpretable representations have slowly become the major goal in the sub-ﬁeld of deep learning concerned with
Variational Auto-Encoders (VAEs) [Kingma and Welling, 2014], seceding from the usual goal of generative models, accurate/realistic sample generation. In this area, representations generally take the form of a multi-dimensional vector space (latent space), and the particular form of interpretability is known as Disentanglement, where each latent dimension (or group of such) is seen to represent an individual (and independent) generative/explanatory factor of the data. Standard examples of such factors include the x position, the y position, an object’s rotation, etc. Effectively separating them into separate subspaces, “disentangling them” is the generally accepted aim of disentanglement research.
Assessing disentangled representations (in the speciﬁc case of VAEs) has primarily been based on the application of numerous “disentanglement metrics”. These metrics operate on disparate understand-ings of what constitutes ideal disentanglement, and required large scale work [Locatello et al., 2018] to present correlations between them and solidify the fact that they should be considered jointly and not as individuals. Symmetry based disentangled representation learning (SBDRL) [Higgins et al., 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2018] offers a mathematical framework through which a rigorous deﬁnition of a linear disentangled representation can be formed. In essence, if data has a given symmetry structure (expressed through
Group Theory), the linear disentangled representations are exactly those which permit irreducible representations of group elements. Caselles-Dupré et al. [2019] propose modelling and inducing such representations through observation of action transitions under the framework of reinforcement learning. Their model successfully induces linear disentangled representations with respect to the chosen symmetry structure, demonstrating that they are achievable through simple methods. Whilst
Caselles-Dupré et al. [2019] assume symmetry structure comprising of purely cyclic groups, Quessard et al. [2020] expand this to the more expressive class of SO(3) matrices.
Current work shows that irreducible representations can be induced in latent spaces, but has yet to determine if they can be found without explicitly structuring the loss landscape. Furthermore, they are not related back to previous disentanglement metrics to demonstrate the utility of such structures being present. Finally, by sticking to the reinforcement framework, Caselles-Dupré et al. [2019] and
Quessard et al. [2020] allow the model direct knowledge of which actions they are observing. This restricts the applicable domain to data where action transition pairs are explicitly labelled.
In this work, we make the following contributions:
•
•
•
•
•
We conﬁrm empirically that irreducible representations are not generally found in standard
VAE models without biasing the loss landscape towards them.
We determine that inducing such representations in VAE latent spaces garners improved performance on a number of standard disentanglement metrics.
We introduce a novel disentanglement metric to explicitly measure linear disentangled representations and we modify the mutual information gap metric to be more appropriate for this setting.
We propose a method to induce irreducible representations without the need for labelled action-transition pairs.
We demonstrate a number of properties of such a model, show that it can disentangle 4 separate symmetries directly from pixels and show it continues to lead to strong scores on classical disentanglement metrics. 2 Symmetry Based Disentangled Representation Learning and Prior Works
This section provides a high level overview of the SBDRL framework without the mathematical grounding in Group and Representation theory on which it is based. The work and appendices of Higgins et al. [2018] offer a concise overview of the necessary deﬁnitions and theorems. We encourage the reader to ﬁrst study their work since they provide intuition and examples which we cannot present here given space constraints.
×
Rnx ny (generally images) to a vector space forming the latent space
SBDRL VAE representation learning is concerned with the mapping from an observation space
Rl. SBDRL
O ⊂
Rd containing the possible states of the includes the additional construct of a world space and a inference world which observations represent. There exists a generative process b : process h :
, the latter being accessible and parametrised by the VAE encoder. SBDRL assumes for convenience that both h and b are injective. All problems in this work have the property
= N , i.e. there are a ﬁnite number of world states and there is no occlusion in
O → Z
=
|W| observations. We should note however that neither of these are required for our work.
W → O
W ⊂
Z ⊂
|O|
|Z|
=
SBDRL proposes to disentangle symmetries of the world space, transformations that preserve some (mathematical or physical) feature, often object identity. Speciﬁc transformations may be translation of an object or its rotation, both independent of any other motions (note the similarity to generative data factors). Such symmetries are described by symmetry groups and the symmetry structure of the world space is represented by a group with decomposition G = G1 × · · · × through action the particular decomposition need not be unique.
W
. The component groups Gi reﬂect the individual symmetries and
Gs acting on
× W → W
: G
·W
SBDRL calls disentangled with respect to decomposition G = G1 × · · · ×
Z
Gs if: 1. There is a group action
: G
·Z
× Z → Z 2
2. The composition f = h
. i.e. g b : f (w) = f (g and
◦
Z
·Z 3. There is a decomposition
Gj, j
= i and affected only by Gi
W → Z w)
, g
·W
∀
Z1 × · · · × Zs such that
=
∈ W
∈
Z is equivariant with respect to the group actions on w
G.
W
Zi is ﬁxed by the action of all
Z is a (real) vector space, SBDRL reﬁnes this through group representations,
Since we assume
GL(V ) as constructs which preserve the linear structure. Deﬁne a group representation ρ : G disentangled with respect to G = G1 × · · · ×
Vs
ρs, i.e. ρ(g1, . . . , gs)(v1, . . . , vs) = and representations ρi : Gi → (ρ1(g1)v(1), . . . , ρs(gs)vs). A consequence of this is that ρ is disentangled if each factor of ρ is w = ρ(g)(w). irreducible. Note that we can associate an action with a group representation through g
Gs if there exists a decomposition V = V1 ⊕ · · · ⊕
GL(Vi) such that ρ = ρ1 ⊕ · · · ⊕
·
Given this, a linear disentangled representation is deﬁned in SBDRL to be any f : that admits a disentangled group representation with respect to the decomposition G = G1 × · · · ×
Gs. As where actions by Gi are equivalent to irreducible representations. such we can look for mappings to
It will be useful in later sections to know that the (real) irreducible representations of the cyclic group
CN are the rotation matrices with angle 2π
N .
W → Z
→
Z
ForwardVAE Higgins et al. [2018] provide the framework for linear disentanglement however intentionally restrained from empirical ﬁndings. Caselles-Dupré et al. [2019] presented the ﬁrst results inducing such representations in VAE latent spaces. Through observing transitions induced
Cy structure, their model successfully learns the rotation by actions in a grid world with G = Cx × matrix representations corresponding to the known irreducible representations of CN .
In implementation, the model stores a learnable parameter matrix (a representation) for each possible action, which is applied when that action is observed. Under the reinforcement learning setting of environment-state-action sequences, they have labelled actions for each observation pair, allowing the action selection at each step. This is suitable for reinforcement problems, however cannot be applied to problems which lack action labelling.
Finally, they offer two theoretical proofs centred around linear disentangled representations. We brieﬂy outline the theorems here: 1) Without interaction with the environment (observing action transitions), you are not guaranteed to learn linear disentangled representations with respect to any of given symmetry structure. 2) It is impossible to learn linear disentangled representation spaces
Z order 2 for the Flatland problem, i.e. learn 1D representations for each component cyclic group.
Further symmetry structures ForwardVAE only explored cyclic symmetry structures G = CN ×
CN , albeit with continuous representations that are expressive enough for SO(2). Subsequent work by
Quessard et al. [2020] explored (outside of the VAE framework) linear disentangled representations of
SO(2) and non-abelian (non-commutative) SO(3). Similar to ForwardVAE, they required knowledge of the action at each step.
Learning Observed Actions Section 5 revolves around predicting observed actions, a concept which has some prior work. Rybkin et al. [2018] learn a (composable) mapping from pre to post-action latents, conditioned on observing the action. Edwards et al. [2019] learn both a forward dynamics model to predict post action states (given state and action) and a distribution over actions given the initial state. Contrary to our work, both methods allow arbitrarily non-linear actions (parametrised by neural networks) which makes them unsuitable for our task. Furthermore they differ signiﬁcantly in implementation. Thomas et al. [2017] utilise an autoencoder with a ‘disentanglement’ objective to encourage actions to correspond to changes in independent latent dimensions. They use a similar reward signal to that we use in Section 5, however have access to labelled actions at each step and encourage no latent structure other than single latents varying with single actions. Choi et al.
[2018] learn to predict actions between two frames in an (action) supervised manner with a focus on localising the agents in the image. 3 Which Spaces Admit Linear Disentangled Representations
This section empirically explores admission of irreducible representations in latent spaces. particular we look at standard VAE baselines and search for known cyclic symmetry structure.
In 3 (cid:54)
Figure 1: Sequential observations from the Flatland toy problem. If the agent would contact the boundary, it is instead warped to the opposing side (e.g. between the third and fourth observations).
W
Problem Setting We shall use the Flatland problem [Caselles-Dupré et al., 2018] for consistency
Cy, manifesting as translation with ForwardVAE, a grid world with symmetry structure G = Cx × 64px. Under the (in 5px steps) of a circle/agent (radius 15px) around a canvas of size 64px
×
SBDRL framework, we have world space
, the set of all possible locations of (xi, yi)
=
}
{ the agent. The agent is warped to the opposite boundary if within a distance less than r. The observation space
, renderings of this space as binary images (see Figure 1), is generated with the
PyGame framework [Shinners] which represents the generative process b. The inference process h is parametrised by candidate VAE models, speciﬁcally the encoder parameters θ. The order of the cyclic groups is given by (64 7 which leads to the approximate phase angle of 0.924. All candidate representation spaces shall be restricted to 4 dimensional for simplicity and
α consistency. All experiments in this and later sections report errors as one standard deviation over 3 runs, using randomly selected validation splits of 10%. We shall evaluate the following baselines,
VAE [Kingma and Welling, 2014], β-VAE [Higgins et al., 2017], CC-VAE [Burgess et al., 2018],
FactorVAE [Kim and Mnih, 2018] and DIP-VAE-I/II [Kumar et al., 2017a]. 15)/5
O
≈
≈
−
∗ 2 1
Evaluation Method Once we have a candidate representation (latent) space
, we need to locate potential irreducible representations ρ(g) of action a by group element g. For the deﬁned symmetry structure G, we know that the irreducible representations take the form of 2D rotation matrices. We further restrict to observations of actions by either of the cyclic generators gx, gy or their inverses 1 y . Consequently, we store 4 learnable matrices to represent each of these possible actions. x , g− g−
Since there is no requirement for representations to be admissible on axis aligned planes, we also learn a change of basis matrix for each representation. To locate group representations, we encode the pre-action observation and apply the corresponding matrix, optimising to reconstruct the latent representation of the post-action observation. As we iterate through the data and optimise for best latent reconstruction, the matrices should converge towards irreducible representations if admissible.
Z za||
ˆza −
||
If cyclic representations are admissible, then our matrices should learn to estimate the post action latent with low error. However since not all models will encode to the same space, we also compare the relative improvement rel, the latent error divided by the expected distance between latent codes. We also introduce a metric to explicitly measure the extent to which actions operate on independent subspaces as required by the deﬁnition of a linear disentangled representation. We call this metric the independence score and deﬁne it as a function of the latent code z and the latent code after applying action a, denoted za, for actions of G = G0 × · · · × za
− za||2 ·
−
Independence Score = 1 max
Gi,b
∈ z z
|| z z
|| 1 s!
Gs, (1)
− (cid:19)
∈ a
.
=i (cid:88)i, j
Gj (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) zb
− zb||2 (cid:12)
− (cid:12) (cid:12) (cid:12)
Results For comparison, we ﬁrst discuss results with ForwardVAE, where cyclic representations are known to reside in axis aligned planes. We provide in supplementary section B explicit errors for post action estimation whilst restricted to axis aligned planes. Comparing ForwardVAE with baseline
VAEs, Table 1 reports mean reconstruction errors for the post action observations xa, post action latent codes za, the estimated phase angle for cyclic representations (against known true value α) and the independence score. We can see that none of the standard baselines achieve reconstruction errors close to that of ForwardVAE. Neither do we see good approximations of the true cyclic representation angles (α) in any other model. Whilst the mean α learnt by the VAE and β-VAE are close to the ground truth, the deviation is very large. Dip-II achieves lower error however it is still an order of magnitude worse than ForwardVAE. This is further reﬂected in the independence scores of each model, with ForwardVAE performing strongly and consistently whilst the baselines perform worse and with high variance. These results suggests that the baseline models do not effectively learn linear disentangled representations. 4 (cid:54)
Table 1: Mean reconstruction values through learnt estimation of a cyclic representation.
Forward
VAE
β-VAE
CC-VAE
FactorVAE DIP-I
DIP-II xa||1
ˆxa −
|| za||1
ˆza −
|| za||
ˆza −
||
α
ˆα
||1
−
||
Indep rel 0.011 0.034 0.054 0.009 0.926
±
±
±
±
±
.004 0.096
.023 0.328
.006 0.202
.013 0.054
.063 0.791
.004 0.108
.029 0.255
.078 0.537
.366 0.070
.109 0.581
±
±
±
±
±
.020 0.093
.037 0.151
.242 0.337
.316 0.100
.475 0.289
±
±
±
±
±
±
±
±
±
±
.015 0.060
.028 1.286
.043 0.161
.297 0.260
.465 0.547
.027 0.054
.748 0.431
.032 0.431
.190 0.139
.362 0.655
.011 0.045
.140 0.270
.088 0.614
.147 0.051
.216 0.814
±
±
±
±
±
±
±
±
±
±
.013
.106
.147
.062
.119
±
±
±
±
±
Table 2: Disentanglement metrics for baseline models on Flatland and the spearman correlation of independence score with each (italics indicate low conﬁdence).
Beta
Mod
SAP
MIG
DCI
FL
Indep
Forward
VAE
β-VAE cc-VAE
Factor
DIP-I
DIP-II 1.000 0.876 0.954 0.883 0.995 0.983 0.795
±
±
.001 0.977
±
.006 0.387
.079 0.698
.198 0.530
.005 0.767
.028 0.643
.146 0.292 0.851
±
±
.002 0.301
±
.055 0.296
±
.239 0.620
.356 0.475
.099 0.404
.109 0.558
.234 0.252
± 0 .293
.080 0.021
.161 0.044
±
.165 0.087
±
.387 0.056
.056 0.084
.203 0.038
.075 0.057
± 0 .180
.013 0.960
.010 0.010
.110 0.221
±
.062 0.113
.093 0.090
.050 0.076
.066 0.044 0.864
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
.013 0.320
±
.011 0.697
.165 0.539
.159 0.624
.024 0.506
.062 0.597
.071 0.762
.003 0.989
±
.024 0.625
.239 0.808
.211 0.542
.130 0.766
.109 0.894
.203 0.760
±
±
±
±
.004
±
.167
±
±
±
±
±
±
.250
.186
.079
.024
.132
± 0.855
− 1.0
Corr 0.743 4 Are Linear Disentangled Representations Advantageous for Classical
Disentanglement
This section compares independence and linear disentanglement to a number of classical metrics from the literature. By Locatello et al. [2018], it is known that not all such metrics correlate, and as such it is useful to contrast linear disentanglement with previous understandings.
Problem Setting Flatland has two generative factors, the x and y coordinates of the agent. With these we can evaluate standard disentanglement metrics in an effort to discern commonalities between previous understandings. In this work we adapt the open source code of Locatello et al. [2018] to PyTorch and evaluate using the following metrics: Higgins metric (Beta) [Higgins et al., 2017],
Mutual Information Gap (MIG) [Chen et al., 2018], DCI Disentanglement metric [Eastwood and
Williams, 2018], Modularity (Mod) metric [Ridgeway and Mozer, 2018], SAP metric [Kumar et al., 2017b]. In addition, we evaluate two further metrics that expand on previous works: Factor Leakage (FL) and the previously introduced Independence score.
Factor Leakage Our Factor Leakage (FL) metric is descended from the MIG metric which mea-sures, for each generative factor, the difference in information content between the ﬁrst and second most informative latent dimensions. Considering linear disentangled group representations are fre-quently of dimension 2 or higher, this would result in low MIG scores, implying entanglement despite being linearly disentangled. We extend it by measuring the information of all latent dimensions for each generative factor (i.e. each group action) and reporting the expected area under this curve for each action/factor. Intuitively, entangled representations encode actions over all dimensions; the fewer dimensions the action is encoded in, the more disentangled it is considered with respect to this metric.
Results Table 2 reports classical disentanglement metrics alongside our FL and independence metrics with comparisons of ForwardVAE to baselines. ForwardVAE clearly results in stronger scores with very low deviations under each metric except the SAP and MIG score, both of which measure the extent factors are encoded to single latent dimensions. Naturally, this is not suited to the 2 dimensional representations we expect in Flatland. Indeed, by Caselles-Dupré et al. [2019], no action can be learnt in a single dimension on this problem. We also report correlation of the independence scores against all other metrics. We see strong correlations with all metrics other than 5
Figure 2: Schematic diagram for RGroupVAE and components.
Dashes denote possible paths dependent on selected action.
• denotes matrix vector multiplication.
- Learnable module.
- Loss.
- Operation without parameters
SAP and MIG. From the performance of ForwardVAE and the correlation of independence and other metrics, we can see linear disentangled representations are beneﬁcial for classical disentanglement metrics, and appears to result in very low variance metric scores. 5 Unsupervised Action Estimation
ForwardVAE explicitly requires the action at each step in order to induce linear disentangled structures in latent spaces. We now show that policy gradients allow jointly learning to estimate the observed action alongside the latent representation mapping. We will then examine properties of the model such as learning over longer term action sequences and temporal consistency. All experimental details are reported in supplementary material section A and code is available at https://github.com/
MattPainter01/UnsupervisedActionEstimation. Before introducing the model, we brieﬂy outline the policy gradient method that it will utilise.
Policy Gradients Policy gradient methods will allow us to optimise through a Categorical sampling of ψ-parametrised distribution p(A and conditioned on
ψ, s) over possible choices
| state s. The policy gradient loss in the REINFORCE [Williams, 1992] setting where action Ai receives reward R(Ai, s) is given by,
A1, . . . , AN }
{
−
−
R(Ai, s) log(p(Ai| log(1
− if R(Ai, s) > 0
Lpolicy = if R(Ai, s) < 0 maxj R(Aj, s) instead of maximising reward provides
We ﬁnd that minimising the regret R(Ai, s) more stable learning for FlatLand, however the increased runtime was unacceptable for the more varied dSprites data, which uses standard reward maximisation. To encourage exploration we use an
ψ, s)), a common technique used by (cid:15)-greedy policy or subtract the weighted entropy 0.01H(p(Ai| methods such as Soft Actor-Critic [Haarnoja et al., 2018].
ψ, s)) p(Ai|
−
R(Ai)
|
·
ψ, s)) (2)
· | (cid:26)
.
Reinforced GroupVAE Reinforced GroupVAE (RGrVAE) is our proposed method for learning linear disentangled representation in a VAE without the constraint of action supervision. A schematic diagram is given in Figure 2. Alongside a standard VAE we use a small CNN (parameters ψ) which infers from each image pair a distribution over a set of possible (learnable) internal representation matrices Ai. These matrices can be restricted to purely cyclic representations by learning solely a cyclic angle, or they can be learnt as generic matrices. We also introduce a decay loss on each representation towards the identity representation, since we prefer to use the minimum number of representations possible. The policy selection distribution is sampled categorically and the chosen representation matrix is applied to the latent code of the pre-action image with the aim of reconstructing that of the post-action image. The policy selection network is optimised through policy gradients with rewards provided as a function of the pre-action latent code z, the post action latent code za and the predicted post action code ˆza, z
|| (3) za||
We then train a standard VAE with the additional policy gradient loss and a weighted prediction loss given by
R(a, x, xa) = 2 2 − ||
ˆza − za||
− 2 2
.
Lpred =
ˆza −
|| za|| 2 2
, total =
L
VAE +
L policy + γ
L
. pred
L (4) 6
Table 3: Reconstruction errors for post action observation (x), latent (z) and representation α. (a) FlatLand
ForwardVAE RGrVAE (b) dSprites
RGrVAE xa||1
ˆxa −
|| za||1
ˆza −
|| za||
ˆza −
||
α
ˆα
||1
||
−
Independence rel 0.011 0.034 0.054 0.009
±
±
± 0.989 0.004 0.023 0.005 0.013
± 0.004
± 0.016 0.100 0.183 0.012 0.960 0.004 0.029 0.034 0.040
±
±
±
± 0.015
± xa||1
ˆxa −
|| za||1
ˆza −
|| za||
ˆza −
||
α
ˆα
||1
||
−
Independence rel 0.008 0.103 0.407 0.205
±
±
±
± 0.005 0.040 0.100 0.095 0.985 0.014
±
VAE 0.010 0.294 0.925 0.312 0.879 0.000 0.106 0.290 0.159 0.050
±
±
±
±
±
Table 4: Disentanglement Metrics for RGrVAE. We see similar scores on dSprites and FlatLand, suggesting linear disentanglement in both cases.
SAP
MIG
Mod
Beta
DCI
FL
FlatLand dSprites 1.000 0.998 0.000 0.001
±
± 0.956 0.901 0.010 0.010
±
± 0.464 0.420
±
± 0.059 0.040 0.051 0.033 0.020 0.014
±
± 0.809 0.689 0.059 0.057
±
± 0.355 0.165
±
± 0.016 0.030
Flatland To demonstrate the effectiveness of the policy gradient network, we evaluate RGrVAE on Flatland. We allow 4 latent dimensions and initialise 2 cyclic representations per latent pair with random angles but alternating signs to speed up convergence. Examples of the learnt actions are given in supplementary section C. Table 3a reports reconstruction errors showing RGrVAE achieves similar error rates to ForwardVAE. Whilst latent reconstructions are similar to some baselines, it is much more independent in these (and all) cases. This proves the basic premise that RGrVAE can induce linear disentangled representations without action supervision.
C8 ×
C10 × dSprites Table 3b reports reconstruction errors on dSprites, with symmetries in translation, scale
C8. We see that the symmetry and rotation leading to symmetry structure G = C3 × reconstruction is an order of magnitude worse than on FlatLand which is expected since it is a harder problem. Whilst we see similar latent/relative reconstruction as on FlatLand, the observation reconstruction appears much closer to the baseline. This is due to the object taking up a signiﬁcantly smaller fraction of the image in dSprites than in FlatLand, so the expected distance between pre and post action observations is much lower. We do note that RGrVAE continues to have highly independent representations compared to the baseline. This combined with action traversals in Fig-ure 3 (see supplementary material C for full traversals) and disentanglement metrics (next paragraph) demonstrate that RGrVAE has extracted 4 symmetries, the most currently demonstrated directly from pixels. Note from the traversals, however that the rotation symmetry learnt was C4, due to spatial symmetries of the square and oval resulting in degenerate actions and their structures being C4/C5, not C10. It is possible with additional training and learning rate annealing this could be overcome, but likely with signiﬁcantly increased runtime. We believe a comparably trained ForwardVAE would perform similarly or better thus our comparison to a baseline in this experiment.
Classical Disentanglement Metrics Table 4 reports disentanglement metrics for RGrVAE on
FlatLand and dSprites. On FlatLand, we can see that despite removing the requirement of action supervision, RGrVAE performs similarly to ForwardVAE (Table 2) on the Beta, Modularity, FL and Independence metrics. The SAP score is more comparable to baseline methods, implying that despite the high independence and low cyclic reconstruction errors, RGrVAE may rely more on single dimensions than ForwardVAE. The DCI disentanglement is also lower and the MIG larger, which are both consistent with this. Despite this, the DCI disentanglement is comparatively much larger than baselines, which shows, with the Beta, FL and independence scores that RGrVAE does not encode factors in solely single dimensions, instead capturing a linear disentangled representation. We also include metrics evaluated on dSprites where we see similar scores to FlatLand and a similar level of consistency in the scores which was not as evident in baselines. This provides further evidence that
RGrVAE learns linear disentangled representations effectively. 7
Tx
Scale
Rotation
Ty
Figure 3: Action traversals for main actions of RGrVAE trained on dSprites. We sampled the true actions from the dataset based on the following symmetry groups: Scale: C3, Rotation: C10,
Translation: C8. (a) (b) (c)
Figure 4: Properties of RGrVAE. a) α reconstruction MSE vs sequence length. b) Estimated active number of representations in total (solid) and per action (dashed) over training for different total available representations. c) Latent reconstruction MSE for applying actions over a number of steps.
Longer Action Sequences By applying a chosen action and decoding the result, we have a new image pair (with the target) from which we can infer another action. Repeating this allows us to explore initial observation pairs which differ by more than the application of a single action. In the limit this could remove the requirement of having sequential data, however by [Caselles-Dupré et al., 2019], we know that this no longer guarantees linear disentanglement with respect to any particular symmetry structure. Figure 4a reports the CN α reconstruction error, where we see that larger steps results in gradually degraded estimation, however it is relatively consistent for a small number of steps.
Note that this does not preclude linear disentanglement, simply linear disentanglement with respect
Cy is extremely unlikely. Since our measure is speciﬁc to G, linear disentanglement to G = Cx × with respect to other symmetry structures would not be evident from this ﬁgure.
Over Representation Thus far we have not been concerned with the choice of how many internal representations to allow RGrVAE. In many cases we will not know the exact symmetry structure so a 1 to 1 mapping of internal representations to expected actions on the data will not be possible. We now explore over-representation, the case where the number of internal representations is greater than the number of expected true actions. In Figure 4b we plot an estimate of the number of total/per-action eh where h is the mean or per action entropy). Over training we see the active representations (N number of active representations decrease, towards 1 for each action but between 4 and 6 for the total.
This total is higher than the expected 4. Since for each action there is close to 1 active representation we believe it is increased by policies for different actions choosing the same minority representation some small fraction of the time. These ﬁndings show that as long as we choose a number of internal representations greater than the number of true actions then we can still learn.
≈
Temporal Consistency A desirable property of representing actions is that they remain accurate over time - the representation isn’t degraded as we apply new actions. Figure 4c reports the latent reconstruction error za||1 as additional actions are applied. We can see that the quality of representations degrades gradually as we add steps and indeed is only marginally worse than the error achieved by ForwardVAE. Note that for this test, only the initial observation is provided to the model,
ˆza −
|| 8
Table 5: Performance of RGrVAE under visual noises. τ is the number of epochs to 0.95 estimated independence, representing policy network convergence.
None
Gaussian
Salt