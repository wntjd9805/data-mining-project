Abstract
This paper proposes two efﬁcient algorithms for computing approximate second-order stationary points (SOSPs) of problems with generic smooth non-convex objective functions and generic linear constraints. While ﬁnding (approximate)
SOSPs for the class of smooth non-convex linearly constrained problems is com-putationally intractable, we show that generic problem instances in this class can be solved efﬁciently. Speciﬁcally, for a generic problem instance, we show that certain strict complementarity (SC) condition holds for all Karush-Kuhn-Tucker (KKT) solutions. Based on this condition, we design an algorithm named Successive Negative-curvature grAdient Projection (SNAP), which per-forms either conventional gradient projection or some negative curvature based projection steps to ﬁnd SOSPs. SNAP is a second-order algorithm that requires
}) iterations to compute an (ϵG, ϵH )-SOSP, where eO hides eO(max{1/ϵ2 the iteration complexity for eigenvalue-decomposition. Building on SNAP, we propose a ﬁrst-order algorithm, named SNAP+, that requires O(1/ϵ2.5) iterations
ϵ)-SOSP. The per-iteration computational complexities of our al-to compute (ϵ, gorithms are polynomial in the number of constraints and problem dimension. To the best of our knowledge, this is the ﬁrst time that ﬁrst-order algorithms with polynomial per-iteration complexity and global sublinear rate are designed to ﬁnd
SOSPs of the important class of non-convex problems with linear constraints (al-most surely).
G, 1/ϵ3
H
√ 1

Introduction
We consider the following class of non-convex linearly constrained optimization problems minimize x f (x), subject to x ∈ X , {x | Ax ≤ b} (1) where f : Rd → R is twice differentiable (possibly non-convex); A ∈ Rm×d and b ∈ Rm are some given matrix and vector. Such a class of problems ﬁnds many applications in machine learning and data science. For example, in the nonnegative matrix factorization (NMF) problem, a given data
∗contributed to this work when he was working as a Ph.D. student at the University of Minnesota. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
matrix M ∈ Rn×p is to be factorized into two nonnegative matrices W ∈ Rn×k and H ∈ Rp×k such that ∥WHT − M∥2
F is minimized [1]. Further, for non-convex problems with ℓ1 regularizer (such as sparse PCA [2]),we need to solve min g(x) + ∥x∥1, which can be equivalently written as (2) subject to − y ≤ x ≤ y. min g(x) + 1T y,
Applications having these linear inequality constraints include neural networks training with the nonnegative constraint [3], nonnegative tensor factorization [4], statistical learning with simplex constraints [5], and portfolio optimization under budget constraints [6], to name just a few.
Recently, algorithms that can escape strict saddle points (stationary points at which there exist di-rections of negative curvature) for unconstrained non-convex problems have attracted considerable research attention, and they ﬁnd applications in tensor decomposition [7], phase retrieval [8], low-rank matrix factorization [9], etc. However, it is not straightforward to extend these “saddle-point escaping” algorithms to problems with simple constraints or non-smooth regularizers. As will be seen shortly, even checking that a solution is a second-order stationary point (SOSP) for linearly constrained problems can be daunting. The main task of this paper is to identify situations in which
ﬁnding a SOSP for problem (1) is easy and to design efﬁcient algorithms for this task.