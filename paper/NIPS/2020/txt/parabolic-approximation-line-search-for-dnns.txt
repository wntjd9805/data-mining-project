Abstract
A major challenge in current optimization research for deep learning is to automat-ically ﬁnd optimal step sizes for each update step. The optimal step size is closely related to the shape of the loss in the update step direction. However, this shape has not yet been examined in detail. This work shows empirically that the batch loss over lines in negative gradient direction is mostly convex locally and well suited for one-dimensional parabolic approximations. By exploiting this parabolic property we introduce a simple and robust line search approach, which performs loss-shape dependent update steps. Our approach combines well-known methods such as parabolic approximation, line search and conjugate gradient, to perform efﬁciently. It surpasses other step size estimating methods and competes with common optimization methods on a large variety of experiments without the need of hand-designed step size schedules. Thus, it is of interest for objectives where step-size schedules are unknown or do not perform well. Our extensive evaluation includes multiple comprehensive hyperparameter grid searches on several datasets and architectures. Finally, we provide a general investigation of exact line searches in the context of batch losses and exact losses, including their relation to our line search approach. 1

Introduction
Automatic determination of optimal step sizes for each update step of stochastic gradient descent is a major challenge in current optimization research for deep learning [3, 5, 12, 29, 38, 43, 46, 50, 58]. One default approach to tackle this challenge is to apply line search methods. Several of these have been introduced for Deep Learning [12, 29, 38, 43, 58]. However, these approaches have not analyzed the shape of the loss functions in update step direction in detail, which is important, since the optimal step size stands in strong relation to this shape. To shed light on this, our work empirically analyses the shape of the loss function in update step direction for deep learning scenarios often considered in optimization. We further elaborate the properties found to deﬁne a simple, competitive, empirically justiﬁed optimizer.
Our contributions are as follows: 1: Empirical analysis suggests that the loss function in negative gradient direction mostly shows locally convex shapes. Furthermore, we show that parabolic approx-imations are well suited to estimate the minima in these directions (Section 3). 2: Exploiting the parabolic property, we build a simple line search optimizer which constructs its own loss function de-pendent learning rate schedule. The performance of our optimization method is extensively analyzed, including a comprehensive comparison to other optimization methods (Sections 4,5). 3: We provide a convergence analysis which backs our empirical results, under strong assumptions (Section 4.4). 4:
We provide a general investigation of exact line searches on batch losses and their relation to line searches on the exact loss as well as their relation to our line search approach (Section 6) and, ﬁnally, analyze the relation of our approach to interpolation (Section 7). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
The empirical loss L is deﬁned as the average over realizations of a batch-wise loss function L:
L(θ) : Rm → R, θ (cid:55)→ n−1 (cid:80)n i=1 L(xi; θ) with n being the amount of batches, xi denotes a batch of a dataset and θ ∈ Rm denotes the parameters to be optimized. Note, that we consider a sample as one batch of multiple inputs. We denote L(xt; θt) the batch loss of a batch x at optimization step t.
In this work, we consider L(xt; θt) in negative gradient direction: lt(s) : R → R, s (cid:55)→ L(xt; θt + s ·
−gt
||gt||
) (1) where gt is ∇θt L(xt; θt). For simpliﬁcation, we denote lt(s) a line function or vertical cross section and s a step on this line. The motivation of our work builds upon the following assumption:
−gt
Assumption 1. (Informal) The position θmin = θt + smin
||gt|| of a minimum of lt is a well enough estimator for the position of the minimum of the empirical loss L on the same line to perform a successful optimization process.
We empirically analyze Assumption 1 further in section 6. 2