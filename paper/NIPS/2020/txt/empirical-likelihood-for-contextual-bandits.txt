Abstract
We propose an estimator and conﬁdence interval for computing the value of a policy from off-policy data in the contextual bandit setting. To this end we apply empirical likelihood techniques to formulate our estimator and conﬁdence interval as simple convex optimization problems. Using the lower bound of our conﬁdence interval, we then propose an off-policy policy optimization algorithm that searches for policies with large reward lower bound. We empirically ﬁnd that both our estimator and conﬁdence interval improve over previous proposals in ﬁnite sample regimes. Finally, the policy optimization algorithm we propose outperforms a strong baseline system for learning from off-policy data. 1

Introduction
Contextual Bandits [3, 17] are now in widespread practical use ([19, 7, 25]). Key to their success is the ability to do off-policy or counterfactual estimation [12] of the value of any policy enabling sound train/test regimes similar to supervised learning. However, off-policy evaluation requires more data than supervised learning to produce estimates of the same accuracy. This is because off-policy data needs to be importance-weighted and accurate estimation for importance-weighted data is still an active research area. How can we ﬁnd a tight conﬁdence interval (CI) on counterfactual estimates?
And since tight CIs are deeply dependent on the form of their estimate, how can we ﬁnd a tight estimate? And given what we discover, how can we leverage this for improved learning algorithms?
We discover good answers to these questions through the application of empirical likelihood [24], a nonparametric maximum likelihood approach that treats the sample as a realization from a multino-mial distribution with an inﬁnite number of categories. Like a likelihood method, empirical likelihood (EL) adapts to the difﬁculty of the problem in an automatic way and results in efﬁcient estimators.
Unlike parametric likelihood methods, we do not need to make any parametric assumptions about the data generating process. We do assume that the expected importance weight is 1, a nonpara-metric moment condition that is supposed to hold for correctly collected off-policy data. Finally,
EL-based estimators and conﬁdence intervals can be computed by efﬁcient algorithms that solve low dimensional convex optimization problems. Figure 1 shows a preview of our results.
In section 4.2 we introduce our estimator. The estimator is computationally tractable, requiring a bisection search over a single scalar, has provably low bias (see Theorem 1) and in section 5.1 we experimentally demonstrate performance exceeding that of popular alternatives.
The estimator leads to an asymptotically exact conﬁdence interval for off-policy estimation which we describe in section 4.3. Other CIs are either narrow but fail to guarantee prescribed coverage, or guarantee prescribed coverage but are too wide to be useful. Our interval is narrow and (despite having only an asymptotic guarantee) empirically approaches nominal coverage from above as in
Figure 1 and Table 3. Finally, in section 4.5, we use our CI to construct a robust counterfactual learning objective. We experiment with this in section 5.3 and empirically outperform a strong baseline.
We now highlight several innovations in our approach: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: A comparison of conﬁdence intervals on contextual bandit data. The EL conﬁdence interval is dramatically tighter than an approach based on a binomial conﬁdence interval while avoiding chronic undercoverage as per the asymptotic Gaussian conﬁdence interval. In some regimes, the asymptotic Gaussian CI both undercovers and has greater average width. This is possible as the
EL CI has a different functional form than a multiplier on the Gaussian CI. On the left, shaded area represents 90% of the empirical distribution indicating the EL CI width varies less over realizations.
On the right, shaded area represents 4 times the standard error of the mean indicating coverage differences are everywhere statistically signiﬁcant.
• We use a nonparametric likelihood approach. This maintains [15] some of the asymptotic optimality results known for likelihood in the multinomial (hence well-speciﬁed) case[11].
• We prove a ﬁnite sample result on the bias of our estimator. This also implies our estimator is asymptotically consistent.
• Our CI considers a large set of plausible worlds (alternative hypotheses) from which the observed off-policy data could have come from. One implication (cf. section 4.4) is that for binary rewards the CI lower bound will be < 1 (and > 0) even if all observed rewards are 1.
• We show how to compute the conﬁdence interval directly, saving a factor of log(1/(cid:15)) in time complexity compared to standard implementations of EL for general settings.
• We propose a learning objective that searches for a policy with the best lower bound on its reward and draw connections with distributionally robust optimization. 2