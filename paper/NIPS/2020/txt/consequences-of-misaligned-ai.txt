Abstract
AI systems often rely on two key components: a speciﬁed goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial speciﬁcation of the principal’s goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the L attributes of the state correspond to different sources of utility for the principal.
We assume that the reward function given to the agent only has support on J < L attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal—agent problem from artiﬁcial intelligence; 2) we provide necessary and sufﬁcient conditions under which indeﬁnitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identiﬁes a theoretical scenario where some degree of interactivity is desirable. 1

Introduction
In the story of King Midas, an ancient Greek king makes a wish that everything he touch turn to gold.
He subsequently starves to death as his newfound powers transform his food into (inedible) gold. His wish was an incomplete representation of his actual desires and he suffered as a result. This story, which teaches us to be careful about what we ask for, lays out a fundamental challenge for designers of modern autonomous systems.
Almost any autonomous agent relies on two key components: a speciﬁed goal or reward function for the system and an optimization algorithm to compute the optimal behavior for that goal. This procedure is intended to produce value for a principal: the user, system designer, or company on whose behalf the agent acts. Research in AI typically seeks to identify more effective optimization techniques under the, often unstated, assumption that better optimization will produce more value for the principal. If the speciﬁed objective is a complete representation of the principal’s goals, then this assumption is surely justiﬁed.
Instead, the designers of AI systems often ﬁnd themselves in the same position as Midas. The mis-alignment between what we can specify and what we want has already caused signiﬁcant harms (32).
Perhaps the clearest demonstration is in content recommendation systems that rank videos, articles, or posts for users. These rankings often optimize engagement metrics computed from user behavior.
The misalignment between these proxies and complex values like time-well-spent, truthfulness, and cohesion contributes to the prevalence of clickbait, misinformation, addiction, and polarization online (31). Researchers in AI safety have argued that improvements in our ability to optimize 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Our model of the principal—agent problem in AI. Starting from an initial state s(0), the robot eventually outputs a mapping from time t ∈ Z+ to states. Left: The human gives the robot a single proxy utility function to optimize for all time. We prove that this paradigm reliably leads to the human actor losing utility, compared to the initial state. Right: An interactive solution, where the human changes the proxy utility function at regular time intervals depending on the current allocation of resources. We show that, at least in theory, this approach does produce value for the human, even under adversarial assumptions. behavior for speciﬁed objectives, without corresponding advancements in our ability to avoid or correct speciﬁcation errors, will amplify the harms of AI systems (8; 19). We have ample evidence from both theory and application, that it is impractical, if not impossible, to provide a complete speciﬁcation of preferences to an autonomous agent.
The gap between speciﬁed proxy rewards and the true objective creates a principal—agent problem between the designers of an AI system and the system itself: the objective of the principal (the designer) is different from, and thus potentially in conﬂict with, the objective of the autonomous agent.
In human principal—agent problems, seemingly inconsequential changes to an agent’s incentives often lead to surprising, counter-intuitive, and counter-productive behavior (21). Consequently, we must ask when this misalignment is costly: when is it counter-productive to optimize for an incomplete proxy?
In this paper, we answer this question in a novel theoretical model of the principal—agent value alignment problem in artiﬁcial intelligence. Our model (Fig. 1, left), considers a resource-constrained world where the L attributes of the state correspond to different sources of utility for the (human) principal. We model incomplete speciﬁcation by limiting the (artiﬁcial) agent’s reward function to have support on J < L attributes of the world. Our main result identiﬁes conditions such that any misalignment is costly: starting from any initial state, optimizing any ﬁxed incomplete proxy eventually leads the principal to be arbitrarily worse off. We show relaxing the assumptions of this theorem allows the principal to gain utility from the autonomous agent. Our results provide theoretical justiﬁcation for impact avoidance (23) and interactive reward learning (19) as solutions to alignment problems.
The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal— agent problem from artiﬁcial intelligence; 2) we provide necessary and sufﬁcient conditions within this framework under which, any incompleteness in objective speciﬁcation is arbitrarily costly; and 3) we show how relaxing these assumptions can lead to models which have good average case and worst case solutions. Our results suggest that managing the gap between complex qualitative goals and their representation in autonomous systems is a central problem in the ﬁeld of artiﬁcial intelligence. 1.1