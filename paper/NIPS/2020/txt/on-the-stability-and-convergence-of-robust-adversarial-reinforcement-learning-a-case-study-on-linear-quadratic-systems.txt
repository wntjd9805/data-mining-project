Abstract
Reinforcement learning (RL) algorithms can fail to generalize due to the gap between the simulation and the real world. One standard remedy is to use robust adversarial RL (RARL) that accounts for this gap during the policy training, by modeling the gap as an adversary against the training agent. In this work, we reexamine the effectiveness of RARL under a fundamental robust control setting: the linear quadratic (LQ) case. We ﬁrst observe that the popular RARL scheme that greedily alternates agents’ updates can easily destabilize the system. Motivated by this, we propose several other policy-based RARL algorithms whose convergence behaviors are then studied both empirically and theoretically. We ﬁnd: i) the conventional RARL framework (Pinto et al., 2017) can learn a destabilizing policy if the initial policy does not enjoy the robust stability property against the adversary; and ii) with robustly stabilizing initializations, our proposed double-loop RARL algorithm provably converges to the global optimal cost while maintaining robust stability on-the-ﬂy. We also examine the stability and convergence issues of other variants of policy-based RARL algorithms, and then discuss several ways to learn robustly stabilizing initializations. From a robust control perspective, we aim to provide some new and critical angles about RARL, by identifying and addressing the stability issues in this fundamental LQ setting in continuous control. Our results make an initial attempt toward better theoretical understandings of policy-based
RARL, the core approach in Pinto et al., 2017. 1

Introduction
Reinforcement learning (RL) can fail to generalize due to the gap between the simulation and the real world. One common remedy for this is to use robust adversarial RL (RARL) that accounts for this gap during the policy training, by modeling the gap as an adversary against the training agent [1, 2].
To achieve the goal of learning a policy that is robust against a family of possible model uncertainties,
RARL jointly trains a protagonist and an adversary, where the protagonist learns to robustly perform the control tasks under the possible disturbances generated by its adversary. Despite the recent development of various robust RL algorithms, especially for continuous control tasks [2, 3, 4, 5, 6], there is no clean baseline delineating the robustness of the policies learned by such a framework. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Motivated by the deep connection between RARL and robust control theory, this paper reexamines the effectiveness of RARL under a fundamental robust control setting: the linear quadratic (LQ) case.
Speciﬁcally, we consider a RARL setting where the state transition follows linear dynamics, and the reward/cost of the protagonist and the adversary is quadratic in the state and the joint control actions.
Such a linear quadratic setting is one of the most fundamental models for robust control [7, 8], and can be viewed as the robust version of the classic linear quadratic regulator (LQR) model, one of the most fundamental models in continuous control and RL [9, 10, 11, 12]. Such a model also ﬁts within the general RARL proposed in the pioneering work [2], with continuous state-action spaces.
The popular RARL scheme with a policy optimization framework [2], though enjoying great empirical successes, has not yet been put on a solid theoretical footing. Some pressing issues are whether/where the policy-based RARL scheme converges, what robust performance can be guaranteed, and whether it preserves certain robustness during learning. None of these issues have been rigorously resolved in RARL. Nonetheless, these issues are of paramount importance when applying RARL to control systems, especially safety-critical ones. A non-convergent algorithm and/or a failure to preserve robustness or even stability during learning, i.e., robust/stable on-the-ﬂy, can cause detrimental consequences to the system. A destabilized system cannot be used for learning anymore, as the objective is then not even well-deﬁned. See more discussions in §3. In this work, we make an attempt toward addressing these questions, by reexamining RARL in the LQ setup. Inspired by the recent results on policy-based RL for LQR [10, 13, 14, 15, 16, 17, 18] and related variants [19, 20, 21], we develop new theoretical results on the stability and convergence of LQ RARL.
In this paper, we ﬁrst observe some negative results by applying the popular policy-based RARL scheme from [2] onto the LQ setup: the alternating update as in [2] can easily destabilize the system.
We identify that guaranteeing stability during learning is non-trivial, which critically relies on both the update rule and the controller1 initialization. Some seemingly reasonable initializations, e.g., simply a stabilizing controller, can still fail. Motivated by this, we develop an update-initialization pair that provably guarantees both robust stability and convergence. It seems that both the stability issues (negative results) and the signiﬁcance of robust stability (positive results) have been overlooked in the RARL literature, if they were to be applied in continuous control tasks. We highlight our contributions as follows:
• We identify several stability issues of the popular RARL scheme in the LQ setup, showing that guaranteeing robust stability during learning requires a non-trivial intertwinement of update rules and controller initializations.
• We propose a double-loop natural policy gradient (PG) algorithm which updates the protago-nist’s policy incrementally. We prove that this algorithm, with some robust-control meaningful initialization, is guaranteed to maintain robust stability on-the-ﬂy and leads to convergence to the optimal cost. We also explore the potential stability and convergence issues of several other algorithm variants.
• We develop new robustiﬁcation techniques, from an H∞-robust control perspective, to learn such robustly stabilizing initializations, with empirical validations.
We expect that both our theoretical and experimental ﬁndings will shed new lights on RARL, from a rigorous robust control perspective. We also note that although our system dynamics are linear, our model can handle general nonlinear disturbances, which can be viewed as the approximation error of linearizing nonlinear dynamics.