Abstract
We consider the task of generating diverse and novel videos from a single video sample. Recently, new hierarchical patch-GAN based approaches were proposed for generating diverse images, given only a single sample at training time. Moving to videos, these approaches fail to generate diverse samples, and often collapse into generating samples similar to the training video. We introduce a novel patch-based variational autoencoder (VAE) which allows for a much greater diversity in generation. Using this tool, a new hierarchical video generation scheme is constructed: at coarse scales, our patch-VAE is employed, ensuring samples are of high diversity. Subsequently, at ﬁner scales, a patch-GAN renders the ﬁne details, resulting in high quality videos. Our experiments show that the proposed method produces diverse samples in both the image domain, and the more challenging video domain. Our code and supplementary material (SM) with additional samples are available at https://shirgur.github.io/hp-vae-gan. 1

Introduction
Video is often considered an extension of images, and in many cases, methods that are applied to images, such as CNNs, are also applied to video sequences, perhaps with 3D convolutions instead of 2D ones. However, when considering synthesis, video generation introduces challenges that do not exist for images. First, one needs to generate, not just one, but many images. Second, video generation requires accurate continuity in time, especially since the human visual system easily spots violations of continuity. Third, the composition of the scene needs to make sense both with regards to object placement, and with regards to their motion and interactions.
A fourth challenge, that has become apparent during the development process, is that the problem of mode collapse is much more severe in videos than in images. Recent GAN-based approaches for synthesis from a single-sample have shown that variability is readily created by hierarchical patch modeling. However, in video, each patch is constrained along three different axes, and the temporal axis does not necessarily comply with the fractal structure of images [1, 2, 3]. Therefore, a direct generalization of such models to video results in a generation that collapses to the original video.
To overcome this, we ﬁrst offer a novel patch-VAE formulation that explicitly models the patch distribution of a video. This approach enables the faithful reconstruction of each patch in the video, as well as the generation of novel samples, thus avoiding the problem of mode collapse and memorization of the input video sample. We subsequently use our patch-VAE in a novel hierarchical formulation.
The newly formulated hierarchical patch-VAE employs a multi-scale decoder but only one encoder.
The VAE’s KL constraint is only applied to the activation map that this single encoder outputs. The different scales of the decoder are trained sequentially, such that the encoder and the top part of the decoder are trained at each step with a reconstruction term.
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
s e k a
F s l a e
R s e k a
F s l a e
R
Figure 1: Random video generations. Showing every other frame for both reals and fakes. Row 3 and 4 - Highlighted on ﬁrst frame are the skydivers. Note the difference in composition and amount.
Full videos are available in the supplementary material (SM) .
The sequence of VAE decoder modules at multiple scales stops before the top resolution, at which point, multiple scales of conditional GANs are used to upsample the video. The two generator types (VAE and GAN) play different roles, and determining the number of scales for each of the two types has a dramatic effect on the results. The role of the patch-VAE is to generate diverse samples at coarse scales. At these scales, the global structure of the video, including the background and the various objects and their placement, is determined. At ﬁner scales, the role of the patch-GAN is to reﬁne those samples by adding ﬁne textural details, resulting in high-quality samples. The alternative of using patch-GAN for all levels of the hierarchy results in low diversity, and samples resembling the input video. On the other extreme, using patch-VAE for all levels results in low quality samples.
This is shown experimentally in section Sec. 4. A sample result of our framework is shown in Fig. 1.
Our experiments show that the novel method outperforms previous work, where we compare to (i) current video generation models, which can only be trained on multiple video samples, and thus beneﬁt from an unfair advantage, (ii) recent image generation methods trained on a single image sample, and (iii) the extension of these methods to video, which, does not replicate their success in image generation. Finally, we consider the effect of different components of our method, such as the number of patch-VAE levels, the receptive ﬁeld, and updating networks only at speciﬁc layers.
To summarize, our work provides the following novelties: (1) a new patch-VAE formulation for a single sample generation, which encourages large diversity and avoids mode collapse, (2) a novel patch VAE-GAN method, which generates diverse and high quality samples, and (3) the ﬁrst method capable of unconditional video generation from a single video sample. 2