Abstract
Deep metric learning plays a key role in various machine learning tasks. Most of the previous works have been conﬁned to sampling from a mini-batch, which cannot precisely characterize the global geometry of the embedding space. Al-though researchers have developed proxy- and classiﬁcation-based methods to tackle the sampling issue, those methods inevitably incur a redundant computa-tional cost. In this paper, we propose a novel Proxy-based deep Graph Metric
Learning (ProxyGML) approach from the perspective of graph classiﬁcation, which uses fewer proxies yet achieves better comprehensive performance. Speciﬁcally, multiple global proxies are leveraged to collectively approximate the original data points for each class. To efﬁciently capture local neighbor relationships, a small number of such proxies are adaptively selected to construct similarity subgraphs between these proxies and each data point. Further, we design a novel reverse label propagation algorithm, by which the neighbor relationships are adjusted accord-ing to ground-truth labels, so that a discriminative metric space can be learned during the process of subgraph classiﬁcation. Extensive experiments carried out on widely-used CUB-200-2011, Cars196, and Stanford Online Products datasets demonstrate the superiority of the proposed ProxyGML over the state-of-the-art methods in terms of both effectiveness and efﬁciency. The source code is publicly available at https://github.com/YuehuaZhu/ProxyGML. 1

Introduction
Deep metric learning (DML) has been extensively studied in the past decade due to its broad applica-tions, e.g., zero-shot classiﬁcation [37, 41, 36], image retrieval [35, 3], person re-identiﬁcation [7, 48], and face recognition [38]. The core idea of DML is to learn an embedding space, where the embedded vectors of similar samples are close to each other while those of dissimilar ones are far apart from each other.
An embedding space with such a desired property is typically learned by metric losses, such as contrastive loss [19, 38] and triplet loss [8]. However, these losses rely on pairs or triplets constructed from samples in a mini-batch, empirically suffering from the sampling issue [7, 23] and leading to a polynomial growth with respect to the number of training examples. It thus turns out that the previous metric losses are highly redundant and less informative. In light of this, many efforts have been devoted to developing efﬁcient hard/semi-hard negative sample mining strategies [7, 39] for handling the sampling issue. Essentially, these strategies still select hard samples from a subset (mini-batch) of the whole training data set, which fail to characterize the global geometry of the embedding space precisely.
∗The corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Another type of methods circumvent such a sampling issue with a global consideration. For instance,
ProxyNCA [23] assigns a trainable reference point to each class, namely proxy, and enforces each raw data point to be close to its relevant positive proxy and far away from the other negative proxies.
During training, all the proxies are kept in the memory, therefore avoiding the sampling issue over dif-ferent mini-batches. However, one proxy for each class is insufﬁcient to represent complex intra-class variations (e.g., poses and shapes of images). In view of this, MaPML [27] proposes to learn latent examples with different distortions to address various uncertainties in real world. On the other hand, training with classiﬁcation-based losses [21, 33, 34, 26] can also avoid the sampling issue by directly ﬁtting the class distribution with fully-connected classiﬁcation layers. However, the aforementioned methods equally treat each raw data point by calculating with either all reference points or class-speciﬁc parameters in classiﬁca-tion layers, hence failing to capture the most dis-criminative relationships among raw data points.
In addition, what follows is expensive compu-tational consumption when many classes are in-volved [7].
In this paper, we propose a novel Proxy-based deep Graph Metric Learning approach, dubbed
ProxyGML, which uses fewer proxies to achieve better comprehensive performance (see Fig. 1) from a graph classiﬁcation perspective. First, in contrast to ProxyNCA [23], we represent each class with multiple trainable proxies to better characterize the intra-class variations. Second, a directed similarity graph is constructed to model the global relationships between all the proxies and raw data samples in a mini-batch. Third, in order to capture informative ﬁne-grained neighborhood structures for each raw data point, the directed similarity graph is decomposed into a series of k-nearest neighbor subgraphs by adaptively selecting a small number of informative proxies. Fourth, these subgraphs are classiﬁed according to their corresponding sample labels. In particular, inspired by the idea of label propagation (LP), we design a novel reverse LP algorithm to adjust the neighbor relationships in each subgraph with the help of known labels. Above all, the proxies and subgraphs collaborate to capture both global and local similarity relationships among raw data samples, so that a discriminative metric space can be learned in a both effective and efﬁcient manner.
Figure 1: ProxyGML converges faster with higher
Recall@1 values on the Cars196 test set (embed-ding dimension is 512 for all compared methods).
The major contributions of this paper are three-fold:
• We propose a novel reverse label propagation algorithm, offering a new insight into DML.
To the best of our knowledge, this work ﬁrstly introduces graph classiﬁcation into supervised
DML.
• The proposed ProxyGML is an efﬁcient drop-in replacement for existing DML losses, which can be readily applied to various tasks, such as image retrieval and clustering [9].
• Extensive experiments demonstrate the superiority of the proposed ProxyGML over the state-of-the-art methods in terms of both effectiveness and efﬁciency. 2