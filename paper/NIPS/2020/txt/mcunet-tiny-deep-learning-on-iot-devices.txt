Abstract
Machine learning on tiny IoT devices based on microcontroller units (MCU) is appealing but challenging: the memory of microcontrollers is 2-3 orders of magni-tude smaller even than mobile phones. We propose MCUNet, a framework that jointly designs the efﬁcient neural architecture (TinyNAS) and the lightweight inference engine (TinyEngine), enabling ImageNet-scale inference on microcon-trollers. TinyNAS adopts a two-stage neural architecture search approach that
ﬁrst optimizes the search space to ﬁt the resource constraints, then specializes the network architecture in the optimized search space. TinyNAS can automatically handle diverse constraints (i.e. device, latency, energy, memory) under low search costs. TinyNAS is co-designed with TinyEngine, a memory-efﬁcient inference library to expand the search space and ﬁt a larger model. TinyEngine adapts the memory scheduling according to the overall network topology rather than layer-wise optimization, reducing the memory usage by 3.4×, and accelerating the inference by 1.7-3.3× compared to TF-Lite Micro [3] and CMSIS-NN [28].
MCUNet is the ﬁrst to achieves >70% ImageNet top1 accuracy on an off-the-shelf commercial microcontroller, using 3.5× less SRAM and 5.7× less Flash compared to quantized MobileNetV2 and ResNet-18. On visual&audio wake words tasks,
MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4× faster than Mo-bileNetV2 and ProxylessNAS-based solutions with 3.7-4.1× smaller peak SRAM.
Our study suggests that the era of always-on tiny machine learning on IoT devices has arrived. 1

Introduction
The number of IoT devices based on always-on microcontrollers is increasing rapidly at a historical rate, reaching 250B [2], enabling numerous applications including smart manufacturing, personalized healthcare, precision agriculture, automated retail, etc. These low-cost, low-energy microcontrollers give rise to a brand new opportunity of tiny machine learning (TinyML). By running deep learning models on these tiny devices, we can directly perform data analytics near the sensor, thus dramatically expand the scope of AI applications.
However, microcontrollers have a very limited resource budget, especially memory (SRAM) and storage (Flash). The on-chip memory is 3 orders of magnitude smaller than mobile devices, and 5-6 orders of magnitude smaller than cloud GPUs, making deep learning deployment extremely difﬁcult.
As shown in Table 1, a state-of-the-art ARM Cortex-M7 MCU only has 320kB SRAM and 1MB
Flash storage, which is impossible to run off-the-shelf deep learning models: ResNet-50 [21] exceeds the storage limit by 100×, MobileNetV2 [43] exceeds the peak memory limit by 22×. Even the int8 quantized version of MobileNetV2 still exceeds the memory limit by 5.3×*, showing a big gap between the desired and available hardware capacity.
Different from the cloud and mobile devices, microcontrollers are bare-metal devices that do not have an operating system. Therefore, we need to jointly design the deep learning model and the inference
*Not including the runtime buffer overhead (e.g., Im2Col buffer); the actual memory consumption is larger. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1. Left: Microcontrollers have 3 orders of magnitude less memory and storage compared to mobile phones, and 5-6 orders of magnitude less than cloud GPUs. The extremely limited memory makes deep learning deployment difﬁcult. Right: The peak memory and storage usage of widely used deep learning models. ResNet-50 exceeds the resource limit on microcontrollers by 100×, MobileNet-V2 exceeds by 20×. Even the int8 quantized MobileNetV2 requires 5.3× larger memory and can’t ﬁt a microcontroller. library to efﬁciently manage the tiny resources and ﬁt the tight memory&storage budget. Existing efﬁcient network design [25, 43, 48] and neural architecture search methods [44, 6, 47, 5] focus on
GPU or smartphones, where both memory and storage are abundant. Therefore, they only optimize to reduce FLOPs or latency, and the resulting models cannot ﬁt microcontrollers. There is limited literature [16, 31, 42, 29] that studies machine learning on microcontrollers. However, due to the lack of system-algorithm co-design, they either study tiny-scale datasets (e.g., CIFAR or sub-CIFAR level), which are far from real-life use case, or use weak neural networks that cannot achieve decent performance.
In this paper, we propose MCUNet, a system-model co-design framework that enables ImageNet-scale deep learning on off-the-shelf microcontrollers. To handle the scarce on-chip memory on microcontrollers, we jointly optimize the deep learning model design (TinyNAS) and the inference library (TinyEngine) to reduce the memory usage. TinyNAS is a two-stage neural architecture search (NAS) method that can handle the tiny and diverse memory constraints on various microcontrollers.
The performance of NAS highly depends on the search space [38], yet there is little literature on the search space design heuristics at the tiny scale. TinyNAS addresses the problem by ﬁrst optimizing the search space automatically to ﬁt the tiny resource constraints, then performing neural architecture search in the optimized space. Speciﬁcally, TinyNAS generates different search spaces by scaling the input resolution and the model width, then collects the computation FLOPs distribution of satisfying networks within the search space to evaluate its priority. TinyNAS relies on the insight that a search space that can accommodate higher FLOPs under memory constraint can produce better model.
Experiments show that the optimized space leads to better accuracy of the NAS searched model.
To handle the extremely tight resource constraints on microcontrollers, we also need a memory-efﬁcient inference library to eliminate the unnecessary memory overhead, so that we can expand the search space to ﬁt larger model capacity with higher accuracy. TinyNAS is co-designed with
TinyEngine to lift the ceiling for hosting deep learning models. TinyEngine improves over the existing inference library with code generator-based compilation method to eliminate memory overhead . It also supports model-adaptive memory scheduling: instead of layer-wise optimization, TinyEngine optimizes the memory scheduling according to the overall network topology to get a better strategy.
Finally, it performs specialized computation kernel optimization (e.g., loop tiling, loop unrolling, op fusion, etc.) for different layers, which further accelerates the inference.
MCUNet dramatically pushes the limit of deep network performance on microcontrollers. TinyEngine reduces the peak memory usage by 3.4× and accelerates the inference by 1.7-3.3× compared to TF-Lite and CMSIS-NN, allowing us to run a larger model. With system-algorithm co-design, MCUNet (TinyNAS+TinyEngine) achieves a record ImageNet top-1 accuracy of 70.7% on an off-the-shelf commercial microcontroller. On visual&audio wake words tasks, MCUNet achieves state-of-the-art accuracy and runs 2.4-3.4× faster than existing solutions at 3.7-4.1× smaller peak SRAM. For interactive applications, our solution achieves 10 FPS with 91% top-1 accuracy on Speech Commands dataset. Our study suggests that the era of tiny machine learning on IoT devices has arrived. 2