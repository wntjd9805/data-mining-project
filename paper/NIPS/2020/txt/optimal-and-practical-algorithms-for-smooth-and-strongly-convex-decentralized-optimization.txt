Abstract
We consider the task of decentralized minimization of the sum of smooth strongly convex functions stored across the nodes of a network. For this problem, lower bounds on the number of gradient computations and the number of communication rounds required to achieve ε accuracy have recently been proven. We propose two new algorithms for this decentralized optimization problem and equip them with complexity guarantees. We show that our ﬁrst method is optimal both in terms of the number of communication rounds and in terms of the number of gradient computations. Unlike existing optimal algorithms, our algorithm does not rely on the expensive evaluation of dual gradients. Our second algorithm is optimal in terms of the number of communication rounds, without a logarithmic factor. Our approach relies on viewing the two proposed algorithms as accelerated variants of the Forward Backward algorithm to solve monotone inclusions associated with the decentralized optimization problem. We also verify the efﬁcacy of our methods against state-of-the-art algorithms through numerical experiments. 1

Introduction
In this paper we are concerned with the design and analysis of new efﬁcient algorithms for solving optimization problems in a decentralized storage and computation regime. In this regime, a network of agents/devices/workers, such as mobile devices, hospitals, wireless sensors, or smart home appliances, collaborates to solve a single optimization problem whose description is stored across the nodes of the network. Each node can perform computations using its local state and data, and is only allowed to communicate with its neighbors.
Problems of this form have been traditionally studied in the signal processing community (Xu et al., 2020), but are attracting increasing interest from the machine learning and optimization community as well (Scaman et al., 2017). Indeed, the training of supervised machine learning models via empirical risk minimization from training data stored across a network is most naturally cast as a decentralized optimization problem. Finally, while current federated learning (Koneˇcný et al., 2016; McMahan et al., 2017) systems rely on a star network topology, with a trusted server performing aggregation and coordination placed at the center of the network, advances in decentralized optimization could be useful in new generation federated learning formulations that would rely on fully decentralized computation (Li et al., 2019). In summary, decentralized optimization is of direct relevance to machine learning, present and future. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
1.1 Formalism
Formally, given an undirected connected network G = (V, E) with nodes/vertices V = {1, . . . , n} and edges E ⊂ V × V, we consider optimization problems of the form (cid:88) fi(x), (1) min x∈Rd i∈V where the data describing functions fi : Rd → R is stored on node i and not directly available to any other node. Decentralized algorithms for solving this problem need to respect the network structure of the problem, which is to say that computation can only be made on the nodes i ∈ V from data and information available on the nodes, and communication is constrained to only happen along the edges e ∈ E. 1.2 Computation and communication
Several decentralized gradient-type algorithms have been proposed to solve (1) in the smooth and strongly convex regime. Two key efﬁciency measures used to compare such methods are: i) the number of gradient evaluations (where one gradient evaluation refers to computing ∇fi(xi) for all i ∈ V for some input vectors xi), and ii) the number of communication rounds, where one round allows each node to send O(1) vectors of size d to their neighbors. If computation is costly, the ﬁrst comparison metric is more important, and if communication is costly, the second is more important.
Note that problem (1) poses certain intrinsic difﬁculties each method designed for it needs to address.
Clearly, more information can be communicated in each communication round if the network G is
“more highly” connected. By χ we denote the condition number associated with (the connectivity of) the graph G; a formal deﬁnition is given later. Likewise, more computation will be needed if the functions fi are “more complicated”. We will entirely focus on problems where all functions fi are L-smooth and µ-strongly convex, which naturally leads to the quantity κ := L/µ as a condition number associated with computation.
Much of decentralized optimization research is focused on designing decentralized algorithms with computation and communication guarantees which have as good as possible dependence on the intrinsic properties of the problem, i.e., on the condition numbers κ and χ. 2