Abstract
We examine the problem of transferring a policy learned in a source environment to a target environment with different dynamics, particularly in the case where it is crit-ical to reduce the amount of interaction with the target environment during learning.
This problem is particularly important in sim-to-real transfer because simulators inevitably model real-world dynamics imperfectly. In this paper, we show that one existing solution to this transfer problem—grounded action transformation— is closely related to the problem of imitation from observation (IfO): learning behaviors that mimic the observations of behavior demonstrations. After estab-lishing this relationship, we hypothesize that recent state-of-the-art approaches from the IfO literature can be effectively repurposed for grounded transfer learning.
To validate our hypothesis we derive a new algorithm—generative adversarial reinforced action transformation (GARAT)—based on adversarial imitation from observation techniques. We run experiments in several domains with mismatched dynamics, and ﬁnd that agents trained with GARAT achieve higher returns in the target environment compared to existing black-box transfer methods. 1

Introduction
Transfer learning with dynamics mismatch refers to using experience in a source environment to more efﬁciently learn control policies that perform well in a target environment, where the two environments differ only in their transition dynamics. For example, if the friction coefﬁcient in the source and target environments is sufﬁciently different it might cause the action of placing a foot on the ground to work well in one environment, but cause the foot to slip in the other. One possible application of such transfer is where the source environment is a simulator and the target environment
∗to be joining the Computer Sciences department at the University of Wisconsin – Madison
§Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is a robot in the real world, called sim-to-real. In sim-to-real scenarios, source environment (simulator) experience is readily available, but target environment (real world) experience is expensive. Sim-to-real transfer has been used effectively to learn a fast humanoid walk [15], dexterous manipulation
[29, 22, 38, 26, 6, 24, 23], and agile locomotion skills [32]. In this work, we focus on the paradigm of simulator grounding [10, 15, 8], which modiﬁes the source environment’s dynamics to more closely match the target environment dynamics using a relatively small amount of target environment data.
Policies then learned in such a grounded source environment transfer better to the target environment.
Separately, the machine learning community has also devoted attention to imitation learning [5], i.e. the problem of learning a policy to mimic demonstrations provided by another agent. In particular, recent work has considered the speciﬁc problem of imitation from observation (IfO) [25], in which an imitator mimics the expert’s behavior without knowing which actions the expert took, only the outcomes of those actions (i.e. state-only demonstrations). While the lack of action information presents an additional challenge, recently-proposed approaches have suggested that this challenge may be addressable [48, 50].
In this paper, we show that a particular grounded transfer technique that has been shown to successfully accomplish sim-to-real transfer, called grounded action transformation (GAT) [15], can be seen as a form of IfO. We therefore hypothesize that recent, state-of-the-art approaches for addressing the
IfO problem might also be effective for grounding the source environment, leading to improved transfer. Speciﬁcally, we derive a distribution-matching objective similar to ones used in adversarial approaches for generative modeling [14], imitation learning [18], and IfO [49] with considerable empirical success. Based on this objective, we propose a novel algorithm, Generative Adversarial
Reinforced Action Transformation (GARAT), to ground the source environment by reducing the distribution mismatch between the source and target environments.
Our experiments conﬁrm our hypothesis by showing that GARAT reduces the difference in the dynamics between two environments more effectively than GAT. Moreover, our experiments show that, in several domains, this improved grounding translates to better transfer of policies from one environment to the other.
The contributions of this paper are as follows: (1) we show that learning the grounded action transformation can be seen as an IfO problem, (2) we derive a novel adversarial imitation learning algorithm, GARAT, to learn an action transformation policy for transfer learning with dynamics mismatch, and (3) we experimentally evaluate the efﬁcacy of GARAT for transfer with dynamics mismatch. 2