Abstract
We study minimax optimal reinforcement learning in episodic factored Markov decision processes (FMDPs), which are MDPs with conditionally independent transition components. Assuming the factorization is known, we propose two model-based algorithms. The ﬁrst one achieves minimax optimal regret guarantees for a rich class of factored structures, while the second one enjoys better com-putational complexity with a slightly worse regret. A key new ingredient of our algorithms is the design of a bonus term to guide exploration. We complement our algorithms by presenting several structure-dependent lower bounds on regret for
FMDPs that reveal the difﬁculty hiding in the intricacy of the structures. 1

Introduction
In reinforcement learning (RL) an agent interacts with an unknown environment seeking to maximize its cumulative reward. The dynamics of the environment and the agent’s interaction with it are typically modeled as a Markov decision process (MDP). We consider the speciﬁc setting of episodic
MDPs with a ﬁxed interaction horizon. Here, at each step the agent observes the current state of the environment, takes an action, and receives a reward. Given the agent’s action, the environment then transitions to the next state. The interaction horizon is the number of steps in an episode. Both the transitions and rewards may be unknown to the agent.
√
The agent’s performance can be quantiﬁed using regret: the gap between the expected cumulative rewards the agent receives and those obtainable by following an optimal policy. An optimal RL algorithm is then one that incurs the minimum regret. For episodic MDPs, the minimax regret bound
HSAT ) [1] (the notation ˜O(·) hides log factors), where H, S, A, T denote is known to be ˜O( the horizon, the size of the state space, the size of the action space, and the total number of steps, respectively.