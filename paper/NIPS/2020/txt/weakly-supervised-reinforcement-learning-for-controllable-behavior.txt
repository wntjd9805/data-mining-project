Abstract
Reinforcement learning (RL) is a powerful framework for learning to take actions to solve tasks. However, in many settings, an agent must winnow down the inconceivably large space of all possible tasks to the single task that it is currently being asked to solve. Can we instead constrain the space of tasks to those that are semantically meaningful? In this work, we introduce a framework for using weak supervision to automatically disentangle this semantically meaningful subspace of tasks from the enormous space of nonsensical “chaff” tasks. We show that this learned subspace enables efﬁcient exploration and provides a representation that captures distance between states. On a variety of challenging, vision-based continuous control problems, our approach leads to substantial performance gains, particularly as the complexity of the environment grows. 1

Introduction
A general purpose agent must be able to efﬁciently learn a diverse array of tasks through interacting with the real world. The typical approach is to manually deﬁne a set of reward functions and only learn the tasks induced by these reward functions [18, 37]. However, deﬁning and tuning the reward functions is labor intensive and places a signiﬁcant burden on the user to specify reward functions for all tasks that they care about. Designing reward functions that provide enough learning signal yet still induce the correct behavior at convergence is challenging [34]. An alternative approach is to parametrize a family of tasks, such as goal-reaching tasks, and learn a policy for each task in this family [28, 38, 49, 61, 62, 71]. However, learning a single goal-conditioned policy for reaching all goals is a challenging optimization problem and is prone to underﬁtting, especially in high-dimensional tasks with limited data [12]. In this work, we aim to accelerate the acquisition of goal-conditioned policies by narrowing the goal space through weak supervision. Answering this question would allow an RL agent to prioritize exploring and learning meaningful tasks, resulting in faster acquisition of behaviors for solving human-speciﬁed tasks.
How might we constrain the space of tasks to those that are semantically meaningful? Reward functions and demonstrations are the predominant approaches to training RL agents, but they are expensive to acquire [34]. Generally, demonstrations require expert humans to be present [15, 20, 47], and it remains a challenge to acquire high-quality demonstration data from crowdsourcing [53]. In contrast, human preferences and ranking schemes provide an interface for sources of supervision that are easy and intuitive for humans to specify [11], and can scale with the collection of ofﬂine data via crowd-sourcing. However, if we are interested in learning many tasks rather than just one, these approaches do not effectively facilitate scalable learning of many different tasks or goals.
In this work, we demonstrate how weak supervision provides useful information to agents with minimal burden, and how agents can leverage that supervision when learning in an environment. We study one approach to using weak supervision in the goal-conditioned RL setting [2, 43, 57, 60, 66]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In which image...
...is the door opened wider?
...is the lighting brighter?
...is the robot closer to the door?
Figure 1: We propose weak supervision as a means to scalably introduce structure into goal-conditioned RL. The weak supervision is provided by answering binary ques-tions based on the two images (left).
Instead of exploring and learning to reach ev-ery goal state, our weakly-supervised agent need only learn to reach states along meaning-ful axes of variation, ignoring state dimensions that are irrelevant to solving human-speciﬁed tasks. Critically, we propose to place such constraints through weak forms of supervi-sion, instead of enumerating goals or tasks and their corresponding rewards. This weak supervision is obtained by pairwise queries (see Figure 1), and our approach uses this supervision to learn a structured representation space of observations and goals, which can in turn be used to guide exploration, goal generation, and learning. Our approach enables the user to specify the factors of variation that matter for the efﬁcient development of general-purpose agents.
The main contribution of this work is weakly-supervised control (WSC), a simple framework for introducing weak supervision into RL. Our approach learns a semantically meaningful representation space with which the agent can generate its own goals, acquire distance functions, and perform directed exploration. WSC consists of two stages: we ﬁrst learn a disentangled representation of states from weakly-labeled ofﬂine data, then we use the disentangled representation to constrain the exploration space for RL agents. We empirically show that learning disentangled representations can speed up reinforcement learning on various manipulation tasks, and improve the generalization abilities of the learned RL agents. We also demonstrate that WSC produces an interpretable latent policy, where latent goals directly align with controllable features of the environment. 2 Preliminaries
In this section, we overview notation and prior methods that we build upon in this work.
Goal-conditioned RL: We deﬁne a ﬁnite-horizon goal-conditioned Markov decision process by a tuple (S, A, P, H, G) where S is the observation space, A is the action space, P (s(cid:48) | s, a) is an unknown dynamics function, H is the maximum horizon, and G ⊆ S is the goal space. In goal-conditioned RL, we train a policy πθ(at | st, g) to reach goals from the goal space g ∼ G by s∈τ Rg(s)(cid:3), where Rg(s) is a reward optimizing the expected cumulative reward Eg∼G,τ ∼(π,P ) function deﬁned by some distance metric between goals g ∈ G and observations s ∈ S.
In low-dimensional tasks, the reward can simply be deﬁned as the negative (cid:96)2-distance in the state space [2]. However, deﬁning distance metrics is more challenging in high-dimensional spaces such as images [81]. Prior work on visual goal-conditioned RL [57, 61] train an additional state representation model, such as a VAE encoder, and train a policy over encoded states and goals using (cid:96)2-distance in latent space as the reward. In this work, we accelerate the training of goal-conditioned RL by using a (learned) weakly-supervised disentangled representation to guide exploration and goal generation. (cid:2)(cid:80)
Weakly-supervised disentangled representations: Our approach leverages weakly-supervised dis-entangled representation learning in the context of reinforcement learning. Disentangled representa-tion learning aims to learn interpretable representations of data, where each latent dimension measures a distinct factor of variation, conditioned on which the data was generated (see Fig. 2 for examples of factors). More formally, consider data-generating processes where (f1, . . . , fK) ∈ F are the factors of variation, and observations s ∈ S are generated from a function g∗ : F → S. We would like to learn a disentangled latent representation e : S → Z such that, for any factor subindices I ⊆ [K], the subset of latent values eI(s) = zI are only inﬂuenced by the true factors fI, and conversely, e\I(s) = z\I are only inﬂuenced by f\I.
We consider a form of weak supervision called rank pairing, where data samples consist of pairs of observations {s1, s2} and weak binary labels y ∈ {0, 1}K, where yk = 1(fk(s1) < fk(s2)) indicates whether the kth factor value of observation s1 is smaller than the corresponding factor value of s2. Using this data, the weakly-supervised method proposed by Shu et al. [68] trains a discriminator D, a generator G : Z → S, and an encoder e : S → Z to approximately invert G: min
D max
G
E(s1,s2,y)∼D [D(s1, s2, y)] + Ez1,z2∼N (0,I)
Ez1,z2∼N (0,I) (cid:104)
D(G(z1), G(z2), yfake) (cid:105)
, (cid:16) 1 − D(G(z1), G(z2), yfake) (cid:17) max e
Ez∼N (0,I) [e(z | G(z))] (1) where yfake = 0 are fake labels. This approach is guaranteed to recover the true disentangled representation under mild assumptions [68]. We build upon their work in two respects. First, while 2
Figure 2: Our method uses weak supervision to direct exploration and accelerate learning on visual manipulation tasks. We extend the environments from [57] to make the tasks considerably harder by adding distractor objects (Push n > 1), randomized lighting (e.g., PickupLights), and randomized colors (e.g. PickupColors). Each data sample consists of a pair of image observations {s1, s2} and factor labels yk = 1(fk(s1) < fk(s2)) that indicate whether the kth factor of image s1 has smaller value than that of image s2. Example factors include the gripper position, object positions, brightness, and door angle. While these factors of variation could be directly measured with additional sensors in a controlled environment, weak supervision will be more useful in complex domains where such instrumentation is challenging. We only need to collect labels for the axes of variation that may be relevant for future downstream tasks (see Appendix A.2). Bolded factors correspond to the user-speciﬁed factor indices I indicating which of the factors are relevant for solving the class of tasks (see Sec. 3)
Shu et al. [68] used balanced and clean datasets, our method works with unbalanced classes, object occlusion, and varying lighting conditions. Second, we show how the learned representations can be used to accelerate RL. 3 The Weakly-Supervised RL Problem
Unlike standard RL, which requires hand-designed reward functions that are often expensive to obtain in complex environments, we aim to design the weakly-supervised RL problem in a way that provides a convenient form of supervision that scales with the collection of ofﬂine data. We aim to avoid labels in the loop of RL, precise segmentations, and numerical coordinates of objects.
Consider an environment with high complexity and large observation space such that it is intractable for an agent to explore the entire state space. Suppose that we have access to an ofﬂine dataset of weakly-labeled observations, where the labels capture semantically meaningful properties about the environment that are helpful to solving downstream tasks. How can a general-purpose RL agent leverage this dataset to learn new tasks faster? In this section, we formalize this problem statement.
Problem Statement. Assume we are given a weakly-labelled dataset D := {(s1, s2, y)}, which con-sists of pairs of observations {s1, s2} and weak binary labels y ∈ {0, 1}K, where yk = 1(fk(s1) < fk(s2)) indicates whether the k-th factor value for observation s1 is smaller than the corresponding factor value for s2. Beyond these labels, the user also speciﬁes a set of K factors, and a subset of indices I ⊆ [K] specifying which of the factors are relevant for solving a class of tasks. During training, the agent may interact with the environment, but receives no supervision (e.g. no rewards) beyond the weak labels in D. At test time, an unknown goal factor f ∗
I ∈ FI is sampled, and the agent receives a goal observation, e.g. a goal image, whose factors are equal to f ∗
I . The agent’s objective is to learn a latent-conditioned RL policy that minimizes the goal distance: minπ Eπ d(fI(s), f ∗
I ).
Weakly-supervised RL may be useful in many real-world scenarios, as weak-supervision is often cheaper to collect than expert demonstrations. For a visual manipulation task (Fig. 2), labels might indicate the relative position of the robot gripper arm between two image observations; the corresponding goal factor space FI might be the XY-position of an object. Note that we only need to collect labels for the axes of variation that may be relevant for future downstream tasks (see
Appendix A.2). At test time, the agent receives a goal image observation, and is evaluated on how closely it can move the object to the goal location.
The next section develops a framework for solving the weakly-supervised RL problem. Then Sec. 5 empirically investigates whether weak supervision accelerates RL in an economical way. 3
Figure 3: Weakly-Supervised Control framework. Left: In Phase 1, we use the weakly-labelled dataset to learn a disentangled representation by optimizing the losses in Eq. 1. Right: In Phase 2, we use the learned disentangled representation to guide goal generation and deﬁne distances. At the start of each episode, the agent samples a latent goal zg either by encoding a goal image g sampled from the replay buffer, or by sampling directly from the latent goal distribution (Eq. 2). The agent samples actions using the goal-conditioned policy, and deﬁnes rewards as the negative (cid:96)2 distance between goals and states in the disentangled latent space (Eq. 3). 4 Weakly-Supervised Control
In this section, we describe a simple training framework for the weakly-supervised RL problem. Our weakly-supervised control (WSC) framework consists of two stages: we ﬁrst learn a disentangled representation from weakly-labelled RL observations, and then use this disentangled space to guide the exploration of goal-conditioned RL along semantically meaningful directions. 4.1 Learning disentangled representations from observations
We build upon the work of Shu et al. [68] for learning disentangled representations, though other methods could be used. Their method trains an encoder, generator, and discriminator by optimizing the losses in Eq. 1. After training the disentanglement model, we discard the discriminator and the generator, and use the encoder to deﬁne the goal space and compute distances between states.
While Shu et al. [68] assumes that all combinations of factors are present in the dataset and that data classes are perfectly balanced (i.e., exactly one image for every possible combination of factors), these assumptions usually do not hold for signiﬁcantly less clean data coming from an agent’s observations in a physical world. For example, not all factor combinations are physically possible to achieve: an object cannot be ﬂoating in mid-air without a robot gripper holding it, and two solid objects cannot occupy the same space at the same time. This affects the data distribution: for example, when the robot is holding the object in Pickup, there is high correlation between the gripper and object positions. Another issue is partial observability: the agent may lack sensors to observe some aspects of its environment, such as being unable to see through occlusions.
To generate the Sawyer datasets shown in Fig. 2, we corrected the sampled factor combinations to be physically feasible before generating the corresponding image observations in the Mujoco simulator. Furthermore, to reﬂect the difﬁculty of collecting a large amount of samples in complex
RL environments, we only sampled 256 or 512 images in the training dataset, which is much smaller than the combinatorial size of toy datasets such as dSprites [54] (737,280 images).
Empirically, we found that it is more challenging to learn a disentangled representation on the Sawyer observations (see Table 3), yet we show in Sec. 5 that imperfect disentanglement models can still drastically accelerate training of goal-conditioned RL. In the next section, we describe how we use the learned disentangled space to generate goals, deﬁne reward functions, and do directed exploration. 4.2 Structured Goal Generation & Distance Function
Next, we describe how our method uses the learned disentangled model e : S → Z and the user-speciﬁed factor indices I ⊆ [K] to train a goal-conditioned policy. The agent will propose its own goals to practice, attempt the proposed goals, and use the experience to update its policy.
Our method deﬁnes the goal space to be the learned disentangled latent space ZI, restricted to the indices in I. The goal sampling distribution is deﬁned as p(ZI) := Uniform(Z min
I , Z max
I ), (2) where Z min over the dataset, and Uniform(·, ·) denotes the uniform distribution.
I = mins∈D eI(s) and Z max
I = maxs∈D eI(s) denote the elementwise min and max 4
In each iteration, our method samples latent goals zg ∈ ZI by either sampling from p(ZI), or sampling an image observation from the re-play buffer and encoding it with the disentan-gled model, zg = eI(sg). Then, our method at-tempts this goal by executing the policy to get a trajectory (s1, a1, ..., sT ). When sampling tran-sitions (st, at, st+1, zg) from the replay buffer for RL training, we use hindsight relabeling [2] with corrected goals to provide additional train-ing signal. In other words, we sometimes re-label the transition (st, at, st+1, z(cid:48) g) with a cor-rected goal z(cid:48) g, which is sampled from either the goal distribution p(ZI) in Eq. 2, or from a future state in the current trajectory. Our method deﬁnes the reward function as the nega-tive (cid:96)2-distance in the disentangled latent space: rt := Rzg (st+1) := −(cid:107)eI(st+1) − zg(cid:107)2 2. (3)
Algorithm 1 Weakly-Supervised Control
Input:Weakly-labeled data D, factor indices I ⊆ [K]
Train disentangled representation e(s) using D.
Compute Z min
I = mins∈D eI(s).
Compute Z max
I = maxs∈D eI(s).
Deﬁne p(ZI) := Uniform(Z min
Initialize replay buffer R ← ∅. for iteration= 0, 1, . . . , do
I , Z max
I ).
Sample a goal zg ∈ Z and an initial state s0. for t = 0, 1, . . . , H − 1 do
Get action at ∼ π(st, zg).
Execute action and observe st+1 ∼ p(· | st, at).
Store (st, at, st+1, zg) into replay buffer R. for t = 0, 1, . . . , H − 1 do for j = 0, 1, . . . , J do
With probability p, sample z(cid:48) g ∼ p(ZI). Oth-erwise, sample a future state s(cid:48) ∈ τ>t in the current trajectory and compute z(cid:48) g = eI(s(cid:48)).
Store (st, at, st+1, z(cid:48) for k = 0, 1, . . . , N − 1 do
Sample (s, a, s(cid:48), zg) ∼ R.
Compute r = Rzg (s(cid:48)) = −(cid:107)eI(s(cid:48)) − zg(cid:107)2 2.
Update actor and critic using (s, a, s(cid:48), zg, r).
We summarize our weakly-supervised control (WSC) framework in Fig. 3 and Alg. 1. We start by learning the disentanglement module using the weakly-labelled data. Next, we train the policy with off-policy RL, sampling transitions with hindsight relabeling. At termination, our method outputs a goal-conditioned policy which is trained to go to a state that is close to the goal in the disentangled latent space. return π(a | s, z) g) into R. 5 Experiments
We aim to ﬁrst and foremost answer our core hypothesis: (1) Does weakly-supervised control help guide exploration and learning, for increased performance over prior approaches? Further we also investigate: (2) What is the relative importance of disentanglement for goal generation versus for distances?, (3) Is the policy’s behavior interpretable?, (4) Is weak supervision necessary for learning a disentangled state representation?, (5) How much weak supervision is needed to learn a sufﬁciently disentangled state representation, and (6) How much error can be tolerated in the labeling? Questions 1∼3 are investigated in this section, while questions 4∼6 are studied in Appendix A.
To answer these questions, we consider several vision-based, goal-conditioned manipulation tasks shown in Fig. 2. We extend the environments from [57] to make the tasks considerably harder, by adding distractor objects, randomized lighting, and randomized colors. In environments with
‘light’ as a factor (e.g., PushLights), the lighting conditions change randomly at the start of each episode. In environments with ‘color’ as a factor (e.g., PickupColors), both the object color and table color randomly change at the start of each episode (5 table colors, 3 object colors). In the Push and
Pickup environments, the agent’s task is to move a speciﬁc object to a goal location. In the Door environments, the agent’s task is to open the door to match a goal angle. Both the state and goal observations are 48 × 48 RGB images. p(Z)
Method
N (0, I) pskew(R)
RIG
SkewFit
WSC
Comparisons: We compare our method to prior state-of-the-art goal-conditioned RL methods, which are summarized in Table 1. While the original hindsight experience replay (HER) algo-rithm [2] requires the state space to be disentan-gled, this assumption does not hold in our prob-lem setting, where the observations are high-dimensional images. Thus, in our experiments, we modiﬁed HER [2] to sample relabeled goals from the VAE prior g ∼ N (0, I) and use the negative (cid:96)2-distance between goals and VAE-encoded states as the reward function. RIG [57] and SkewFit [61] are extensions of HER that use a modiﬁed goal sampling distribution that places higher weight on rarer states. RIG uses MLE to train the VAE, while SkewFit uses data samples from
Table 1: Conceptual comparison between our method weakly-supervised control (WSC), and prior visual goal-conditioned RL methods, with their respective latent goal distributions p(Z) and goal-conditioned reward functions Rzg (s(cid:48)). Our method can be seen as an ex-tension of prior work to the weakly-supervised setting.
Rzg (s(cid:48))
−(cid:107)eVAE(s(cid:48)) − zg(cid:107)2 2
−(cid:107)eVAE(s(cid:48)) − zg(cid:107)2 2
−(cid:107)eI(s(cid:48)) − zg(cid:107)2 2
I , Z max
I )
Unif(Z min 5
Figure 4: Performance vs. training steps on visual goal-conditioned tasks. Weakly-supervised control (WSC) learns more quickly than prior state-of-the-art goal-conditioned RL methods (HER, RIG, SkewFit), particularly as the complexity of the environment grows. Thus, we see that doing directed exploration and goal sampling in a (learned) semantically-disentangled latent space can be more effective than doing purely unsupervised exploration in the VAE latent space. pskew(R) to train the VAE. For direct comparison, we use the weakly-labeled dataset D in HER, RIG, and SkewFit to pre-train the VAE, from which goals are sampled. Note that all algorithms used in our experiments have the same time complexity. See Appendix B for further implementation details.
Additionally, to investigate whether our disentanglement approach for utilizing weak suppervision is better than alternative methods, we compare to a variant of SkewFit that optimizes an auxiliary prediction loss on the factor labels, which we refer to as Skewﬁt+pred.
Dataset generation: Both the training and test datasets were generated from the same distribution, and each set consists of 256 or 512 images (see Table 6). To generate the Sawyer datasets shown in Fig. 2, we ﬁrst sampled each factor value uniformly within their respective range, then corrected the factors to be physically feasible before generating the corresponding image observations in the
Mujoco simulator. In Push environments with n > 1 objects, the object positions were corrected to avoid collision. In Pickup environments, we sampled the object position on the ground (obj z=0) with 0.8 probability, and otherwise placed the object in the robot gripper (obj z≥ 0). In Door environments, the gripper position was corrected to avoid collision with the door.
Eval metric: At test-time, all RL methods only have access to the test goal image, and is evaluated on the true goal distance. In Push and Pickup, the true goal distance is deﬁned as the (cid:96)2-distance between the current object position and the goal position. In Push environments with n > 1 objects, we only consider the goal distance for the blue object, and ignore the red and green objects (which are distractor objects to make the task more difﬁcult). In Door environments, the true goal distance is deﬁned as the distance between the current door angle and the goal angle value. 5.1 Does weakly-supervised control help guide exploration and learning?
Do the disentangled representations acquired by our method guide goal-conditioned policies to explore in more semantically meaningful ways? In Fig. 4, we compare our method to prior state-of-the-art goal-conditioned RL methods on visual goal-conditioned tasks in the Sawyer environments (see Fig. 2). We see that doing directed exploration and goal sampling in a (learned) disentangled latent space is substantially more effective than doing purely unsupervised exploration in VAE latent state space, particularly for environments with increased variety in lighting and appearance.
Then, a natural question remains: is our disentanglement approach for utilizing weak supervision better than alternative methods? One simple approach is to add an auxiliary loss to predict the factor identity from the representation. We train a variant of SkewFit where the ﬁnal hidden layer of the
VAE is also trained to optimize an auxiliary prediction loss on the factor identity, which we refer to as ‘Skewﬁt+pred’. In Fig. 4, we ﬁnd that Skewﬁt+pred performs worse than WSC even though it uses stronger supervision (exact labels).This comparison suggests that disentangling meaningful and irrelevant factors of the environment is important for effectively leveraging weak supervision. 6
Figure 5: SkewFit+DR is a variant that samples goals in VAE latent space, but uses reward distances in disentangled latent space. We see that the disentangled distance metric can help slightly in harder environments (e.g., Push n = 3), but the goal generation mechanism of WSC is crucial for efﬁcient exploration.
Push n = 1
Push n = 2
Push n = 3
C
S
W t i
F w e k
S
Figure 6: We roll out policies on the Push tasks after training.
The disentangled distance optimized by WSC (top) is more indicative of the true goal distance than the latent distance optimized by SkewFit (bottom), especially for more complex tasks (n > 1).
Figure 7: Interpretable control: We roll out a trained WSC policy conditioned on varying dis-I , Z max entangled latent goals (z1, z2) ∈ [Z min
I ], and plot the trajectory of the object (white line).
The latent goals directly align with the direction in which the policy moves the blue object. 5.2 Ablation: What is the role of distances vs. goals?
Our method uses the representation in two places: for goal-generation (Eq. 2) and for the distance metric (Eq. 3). Our next experiment studies the relative importance of using a disentangled repre-sentation in both places. First, we investigate whether the distance metric in the disentangled space alone is enough to learn goal-conditioned tasks quickly. To do so, we trained a variant of SkewFit that samples latent goals in VAE latent space, but uses distances in disentangled latent space as the reward function. In Fig. 5, we see that the disentangled distance metric can help slightly in harder environments, but underperforms compared to the full method (WSC) with goal generation in disentangled latent space. Thus, we conclude that both the goal generation mechanism and distance metric of our method are crucial components for enabling efﬁcient exploration.
Next, we tested whether the distance metric deﬁned over the learned disentangled representation provides a more accurate signal for the true goal distance. In Fig. 6, we evaluate trained policies on the Push tasks, and compare the latent goal distance vs. the true goal distance at every timestep.
The disentangled distance optimized by WSC is more indicative of the true (cid:96)2 goal distance than the latent distance optimized by SkewFit, especially for more complex tasks (n > 1), suggesting that the disentangled representation provide a more accurate reward signal for the training agent. 5.3
Is the policy’s latent space interpretable?
Since our method uses an interpretable latent goal space to generate self-proposed goals and compute rewards for training the policy, we checked whether the learned policy is also semantically meaningful.
Table 2 in the Appendix measures the correlation between latent goals and the ﬁnal states of the policy rollout. For various latent goals zg ∈ Z, we rolled out the trained policy π(a | s, zg) and compared the ﬁnal state with the latent goal zg that the policy was conditioned on (see Section 5.3).
For our method, we did a grid sweep over the latent goal values in [Z min
I ]. For SkewFit, we took the latent dimensions that have the highest correlations with the true object XY positions, then did a similar grid sweep over the latent goal space. Our method achieves higher Pearson correlation between latent goals and ﬁnal states, meaning that it learns a more interpretable goal-conditioned policy where the latent goals align directly with the ﬁnal state of the trajectory rollout.
I , Z max 7
In Fig. 7, we visualize the trajectories generated by our method’s policy when conditioned on different latent goals, obtained by doing a grid sweep over the latent space [Z min
I ]. We ﬁxed the initial object and gripper positions for each trajectory. We see that the latent goal values zg directly align with the ﬁnal object position after rolling out the policy π(a | s, zg). In other words, varying each latent goal dimension corresponds to directly changing the object position in the X- or Y-coordinate.
I , Z max 6