Abstract
We investigate whether Differentially Private SGD offers better privacy in practice than what is guaranteed by its state-of-the-art analysis. We do so via novel data poisoning attacks, which we show correspond to realistic privacy attacks. While previous work (Ma et al., arXiv 2019) proposed this connection between differential privacy and data poisoning as a defense against data poisoning, our use as a tool for understanding the privacy of a speciﬁc mechanism is new. More generally, our work takes a quantitative, empirical approach to understanding the privacy afforded by speciﬁc implementations of differentially private algorithms that we believe has the potential to complement and inﬂuence analytical work on differential privacy. 1

Introduction
Differential privacy [DMNS06] has become the de facto standard for guaranteeing privacy in ma-chine learning and statistical analysis, and is now being deployed by many organizations including
Apple [TVV+17], Google [EPK14, BEM+17, PSM+18], and the US Census Bureau [HMA+17].
Now that differential privacy has moved from theory to practice, there has been considerable attention on optimizing and evaluating differentially private machine learning algorithms, notably differentially private stochastic gradient descent (henceforth, DP-SGD) [SCS13, BST14, ACG+16], which is now widely available in TensorFlow Privacy [Goo]. DP-SGD is the building block for training many widely used private classiﬁcation models, including feed-forward and convolutional neural networks.
Differential privacy gives a strong worst-case guarantee of individual privacy: a differentially private algorithm ensures that, for any set of training examples, no attacker, no matter how powerful, can learn much more information about a single training example than they could have learned had that example been excluded from the training data. The amount of information is quantiﬁed by a privacy parameter ε.1 Intuitively, a smaller ε means stronger privacy protections, but leads to lower accuracy.
As such there is often pressure to set this parameter as large as one feels still gives a reasonable privacy guarantee, and relatively large parameters such as ε = 2 are not uncommon. However, this guarantee is not entirely satisfying, as such an algorithm might allow an attacker to guess a random bit of information about each training example with approximately 86% accuracy. As such there is often a gap between the strong formal protections promised by differential privacy and the speciﬁc quantitative implications of the choice of ε in practice.
This state-of-affairs is often justiﬁed by the fact that our analysis of the algorithm is often pessimistic.
First of all, ε is a parameter that has to be determined by careful analysis, and often existing theoretical analysis is not tight. Indeed a big part of making differentially private machine learning practical has been the signiﬁcant body of work giving progressively more reﬁned privacy analyses speciﬁcally 1There are several common variants of differential privacy [DKM+06, DR16, BS16, Mir17, BDRS18,
DRS19] that quantify the inﬂuence of a single example in slightly different ways, sometimes using more than one parameter. For this high-level discussion, we focus on the single, primary parameter ε. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for DP-SGD [ACG+16, DRS19, MTZ19, YLP+19], and for all we know these bounds on ε will continue to shrink. Indeed, it is provably intractable to determine the tightest bound on ε for a given algorithm [GM18]. Second, differential privacy is a worst-case notion, as the mechanism might have stronger privacy guarantees on realistic datasets and realistic attackers. Although it is plausible that differentially private algorithms with large values of ε provide strong privacy in practice, it is far from certain, which makes it difﬁcult to understand the appropriate value of ε for practical deployments. 1.1 Our Contributions
Auditing DP-SGD. In this paper we investigate the extent to which DP-SGD,2 does or does not give better privacy in practice than what its current theoretical analysis suggests. We do so using novel data poisoning attacks. Speciﬁcally, our method starts with a dataset D of interest (e.g. Fashion-MNIST) and some algorithm A (e.g. DP-SGD with a speciﬁc setting of hyperparameters), and produces a small poisoning set S of k points and a binary classiﬁer T such that T distinguishes the distribution
A(D) from A(D ∪ S) with signiﬁcant advantage over random guessing. If A were ε-DP, then T could have accuracy at most exp(εk)/(1 + exp(εk)), so if we can estimate the accuracy of T we can infer a lower bound on ε. While previous work [MZH19] proposed to use this connection between differential privacy and data poisoning as a defense against data poisoning, our use in this context of auditing the privacy of DP-SGD is new.
Speciﬁcally, for certain natural choices of hyperparameters in DP-SGD, and standard benchmark datasets (see Figure 2), our attacks give lower bounds on ε that are approximately 10x better than what we could obtain from previous methods, and are within approximately 10x of the worst-case, analytically derived upper bound. For context, previous theoretical improvements to the analysis have improved the worst-case upper bounds by factors of more than 1000x over the na¨ıve analysis, and thus our results show that we cannot hope for similarly dramatic gains in the future.
Novel Data Poisoning Attacks. We ﬁnd that existing data poisoning attacks, as well as membership inference attacks proposed by prior work, have poor or trivial performance not only against DP-SGD, but even against SGD with gradient clipping (i.e. rescaling gradients to have norm no larger than some C). Gradient clipping is an important part of DP-SGD, but does not provide any formal privacy guarantees on its own. Thus, we develop a novel data poisoning attack that is more robust to gradient clipping, and also performs much better against DP-SGD.
Intuitively, data poisoning attacks introduce new points whose gradients will change the model in a certain direction, and the attack impact increases when adding poisoning points of larger gradients.
Existing attacks modify the model in a random direction, and have to push far enough that the original distribution on model parameters and the new distribution become distinguishable. To be effective, these attacks use points which induce large gradients, making the attack sensitive to gradient clipping.
On the other hand, our attack improves by ﬁnding the direction where the model parameters have the lowest variance, and select poisoning points that modify the model in that direction. Therefore, we achieve the same effect of model poisoning with poisoning points of smaller gradients, thereby making the attack more robust to clipping.
The Role of Auditing in DP. More generally, our work takes a quantitative, empirical approach to auditing the privacy afforded by speciﬁc implementations of differentially private algorithms. Our auditing algorithm described in Section 2.2 is generic, and can be used with an appropriate poisoning algorithm, like the one we describe for DP-SGD. We do not advocate trying to deﬁnitively measure privacy of an algorithm empirically, since it’s hopeless to try to anticipate all future attacks. Rather, we believe this empirical approach has the potential to complement and inﬂuence analytical work on differential privacy, somewhat analogous to the way cryptanalysis informs the design and deployment of cryptography.
Speciﬁcally, we believe this approach can complement the theory in several ways: (1) Most directly, by advancing the state-of-art in privacy attacks, we can either demonstrate that a given algorithm with a given choice of parameters is not sufﬁciently private, or give some conﬁdence that it might be sufﬁciently private. (2) Establishing strong lower bounds on ε gives a sense of how much more one could hope to get out of tightening the existing privacy analysis. (3) Observing how the performance 2Although our methods are general, in this work we exclusively study the implementation and privacy analysis of DP-SGD in TensorFlow Privacy [Goo]. 2
of the attack depends on different datasets, hyperparameters, and variants of the algorithm can identify promising new phenomena to explore theoretically. (4) Producing concrete privacy violations can help non-experts interpret the concrete implications of speciﬁc choices of the privacy parameter. 1.2