Abstract
We present a ﬂexible framework for learning predictive models that approximately satisfy the equalized odds notion of fairness. This is achieved by introducing a general discrepancy functional that rigorously quantiﬁes violations of this criterion.
This differentiable functional is used as a penalty driving the model parameters towards equalized odds. To rigorously evaluate ﬁtted models, we develop a formal hypothesis test to detect whether a prediction rule violates this property, the ﬁrst such test in the literature. Both the model ﬁtting and hypothesis testing leverage a resampled version of the sensitive attribute obeying equalized odds, by construction.
We demonstrate the applicability and validity of the proposed framework both in regression and multi-class classiﬁcation problems, reporting improved performance over state-of-the-art methods. Lastly, we show how to incorporate techniques for equitable uncertainty quantiﬁcation—unbiased for each group under study—to communicate the results of the data analysis in exact terms. 1

Introduction
Machine learning algorithms are now frequently used to inform high-stakes decisions—and even to make them outright. As such, society has become increasingly critical of the ethical implications of automated decision making, and researchers in algorithmic fairness are responding with new tools. While fairness is context dependent and may mean different things to different people, a suite of recent work has given rise to a useful vocabulary for discussing fairness in automated systems
[1, 2, 3, 4, 5, 6, 7]. Fairness constraints can often be articulated as conditional independence relations, and in this work we will focus on the equalized odds criterion [8], deﬁned as
ˆY ⊥⊥ A | Y, (1) where the relationship above applies to test points; here, Y is the response variable, A is a sensitive attribute (e.g. gender), X is a vector of features that may also contain A, and ˆY = ˆf (X) is the prediction obtained with a ﬁxed prediction rule ˆf (·). While the idea that a prediction rule obeying the equalized odds property is desirable has gained traction, actually ﬁnding such a rule for a real-valued or multi-class response is a relatively open problem. Indeed, there are only a few recent works attempting this task [9, 10]. Moreover, there are no existing methods to rigorously check whether a learned model achieves this property.
We address these two questions by introducing a novel training scheme to ﬁt models that approx-imately satisfy the equalized odds criterion and a hypothesis test to detect when a prediction rule violates this same criterion. Both solutions build off of one key idea: we create a synthetic version 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Baseline, majority. (b) Baseline, minority. (c) Proposed, majority. (d) Proposed, minority.
Figure 1: The effect of our learning framework on simulated data: (a,b) predictions from the baseline linear model; (c,d) predictions from the linear model ﬁtted with the proposed equalized odds penalty.
˜A of the sensitive attribute such that the triple ( ˆY , ˜A, Y ) obeys (1) with ˜A in lieu of A. To achieve equitable model ﬁts, we regularize our models toward the distribution of the synthetic data. Similarly, to test whether equalized odds holds, we compare the observed data to a collection of artiﬁcial data sets. The synthetic data is straightforward to sample, making our framework both simple to implement and modular in that it works together with any loss function, architecture, training algorithm, and so on. Based on real data experiments on both regression and multi-class classiﬁcation tasks, we ﬁnd improved performance compared to state-of-the-art methods. 1.1 A synthetic example
To set the stage for our methodology, we ﬁrst present an experiment demonstrating the challenges of making equitable predictions as well as a preview of our method’s results. We simulate a regression data set with a binary sensitive attribute and two features: (X1, X2) | (A = 0) d= (Z1, 3Z2) (X1, X2) | (A = 1) d= (3Z1, Z2), and where Z1, Z2 ∼ N (0, 1) is a pair of independent standard normal variables, and the symbol d= denotes equality in distribution. We create a population where 90% of the observations are from the group A = 0 in order to investigate a setting with a large majority group. After conditioning on A, the model for Y | X is linear: Y = X (cid:62)βA + (cid:15), with noise (cid:15) ∼ N (0, 1) and coefﬁcients
β0 = (0, 3) and β1 = (3, 0). We designed the model in this way so that the distribution of Y given
X is the same for the two groups, up to a permutation of the coordinates. (In some settings, we might say that both groups are therefore equally deserving.) Consequently, the best model has equal performance in both groups. We therefore ﬁnd it reasonable to search for a ﬁtted model that achieves equalized odds in this setting.
To serve as an initial point of comparison, we ﬁrst ﬁt a classic linear regression model with coefﬁcients
ˆβ ∈ R2 on the training data, minimizing the mean squared error. Figures (1a) and (1b) show the performance of the ﬁtted model for each group on a separate test set. The ﬁtted model performs signiﬁcantly better on the samples from the majority group A = 0 than those from the minority group
A = 1. This is not surprising since the model seeks to minimize the overall prediction error. Here, the overall root mean squared error (RMSE) evaluated on test points is equal to 2.40, with an average value of 1.79 for group A = 0 and of 5.48 for group A = 1. It is visually clear that for any vertical slice of the graph at a ﬁxed value of Y , the distribution of ˆY is different in the two classes, i.e. the equalized odds property in (1) is violated. This fact can be checked formally with our hypothesis test for (1) described later in Section 3. The resulting p-value on the test set is 0.001 providing rigorous evidence that equalized odds is violated in this case.
Next, we apply our proposed ﬁtting method (see Section 2) on this data set. Rather than a naive least squares ﬁt, we instead ﬁt a linear regression model that approximately satisﬁes the equalized odds criterion. The new predictions are displayed in Figures (1c) and (1d). In contrast to the naive ﬁt, the new predictive model achieves a more balanced performance across the two groups: the blue points are dispersed similarly in these two panels. This observation is consistent with the results of our hypothesis test; the p-value on the test set is equal to 0.476, which provides no indication that the equalized odds property is violated. Turning to the statistical efﬁciency, the equitable model has improved performance for observations in the minority group A = 1 with an RMSE equal to 3.31, at 2
the price of reduced performance in the majority group A = 0, where the RMSE rises to 3.41. The overall RMSE is 3.40, larger than that of the baseline model. 1.2