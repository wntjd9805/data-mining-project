Abstract
Recent studies have shown that introducing communication between agents can sig-niﬁcantly improve overall performance in cooperative Multi-agent reinforcement learning (MARL). However, existing communication schemes often require agents to exchange an excessive number of messages at run-time under a reliable commu-nication channel, which hinders its practicality in many real-world situations. In this paper, we present Temporal Message Control (TMC), a simple yet effective approach for achieving succinct and robust communication in MARL. TMC applies a temporal smoothing technique to drastically reduce the amount of information exchanged between agents. Experiments show that TMC can signiﬁcantly reduce inter-agent communication overhead without impacting accuracy. Furthermore,
TMC demonstrates much better robustness against transmission loss than existing approaches in lossy networking environments. 1

Introduction
Multi-agent reinforcement learning (MARL) has achieved remarkable success in a variety of chal-lenging problems, including intelligent trafﬁc signal control [36], swarm robotics [15], autonomous driving [28]. At run time, a group of agents interact with each other in a shared environment. Each agent takes an action based on its local observation as well as both direct and indirect interactions with the other agents. This complex interaction model often introduces instability in the training process, which can seriously undermine the overall effectiveness of the model.
In recent years, numerous methods have been proposed to enhance the stability of cooperative MARL.
Among these methods, the centralized training and decentralized execution paradigm [21] has gained much attention due to its superior performance. A promising approach to exploit this paradigm is the value function decomposition method [33, 23, 31]. In this method, each agent is given an individual
Q-function. A joint Q-function, which is the aggregation of individual Q-functions, is learned during the training phase to capture the cooperative behavior of the agents. During the execution phase, each agent acts independently by selecting its action based on its own Q-function. However, even though value function decomposition has demonstrated outstanding performance in solving simple tasks [33], it does not allow explicit information exchange between agents during execution phrase, which hinders its performance in more complex scenarios. In certain cases, some agents may overﬁt their strategies to the behaviours of the other agents, causing serious performance degradation [17, 18].
Motivated by the drawbacks due to lack of communication, recent studies [13, 5, 11] have introduced inter-agent communication during the execution phase, which enables agents to better coordinate and react to the environment with their joint experience. However, while an extensive amount of work has concentrated on leveraging communication for better overall performance, little attention has been paid to the reliability of transmission channel and efﬁciency during the message exchange.
Moreover, recent work has shown that the message exchange between agents tends to be excessive and redundant [39]. Furthermore, for many real applications such as autonomous driving and drone
∗Equal contribution, names are ranked alphabetically 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (a) Markov model for wireless loss modeling. (b-d) Impact of small difference on message. control [30], even though the observations received by the agents change frequently, the useful information presented in the consecutive observations is often quite similar. This further leads to highly time-correlated and redundant information in the output messages. For example, when an autonomous car drives over a spacious mountainous area, its front camera may capture the trees on both sides of the road most of the time. Although the observations are changing continually, it hardly impacts the navigation decision. In fact, these changing observations may cause superﬂuous and noisy messages to be exchanged, which can reduce the overall system performance. We want the messages to be transmitted only when useful information is captured by the camera (e.g.obstacle is detected). Given the limited bandwidth and potentially unreliable communication channels for some of these applications, it is desirable to leverage temporal locality to reduce communication overhead and improve overall system efﬁciency.
Motivated by this observation, in this paper we present Temporal Message Control (TMC), a MARL framework that leverages temporal locality to achieve succinct and robust inter-agent message exchange. Speciﬁcally, we introduce regularizers that encourage agents to reduce the number of temporally correlated messages. On the sender side, each agent sends out a new message only when the current message contains relatively new information compared to the previously transmitted message. On the receiver side, each agent stores the most recent messages from the other agents in a buffer, and uses the buffered messages to make action decisions. This simple buffering mechanism also naturally enhances the robustness against transmission loss. We have evaluated TMC on multiple environments including StarCraft Multi-Agent Challenge (SMAC) [25], Predator-Prey [19] and Cooperative Navigation [19]. The results indicate that TMC achieves a 23% higher average winning rates and up to 80% reduction in communication overhead compared to existing schemes.
Furthermore, our experiments show that TMC can still maintain a good winning rate under extremely lossy communication channel, where the winning rates of other approaches almost degrade to 0%.
A video demo is available at [3] to show TMC performance, and the code for this work is provided in [2]. To the best of our knowledge, this is the ﬁrst study on MARL system that can operate in bandwidth-limited and lossy networking environments. 2