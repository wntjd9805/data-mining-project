Abstract
Current methods for training robust networks lead to a drop in test accuracy, which has led prior works to posit that a robustness-accuracy tradeoff may be inevitable in deep learning. We take a closer look at this phenomenon and ﬁrst show that real image datasets are actually separated. With this property in mind, we then prove that robustness and accuracy should both be achievable for benchmark datasets through locally Lipschitz functions, and hence, there should be no inherent tradeoff between robustness and accuracy. Through extensive experiments with robustness methods, we argue that the gap between theory and practice arises from two limitations of current methods: either they fail to impose local Lipschitzness or they are insufﬁciently generalized. We explore combining dropout with robust training methods and obtain better generalization. We conclude that achieving robustness and accuracy in practice may require using methods that impose local
Lipschitzness and augmenting them with deep learning generalization techniques.1 1

Introduction
A growing body of research shows that neural networks are vulnerable to adversarial examples, test inputs that have been modiﬁed slightly yet strategically to cause misclassiﬁcation [56, 17]. While a number of defenses have been proposed [10, 32, 46, 65], they are known to hurt test accuracy on many datasets [41, 32, 68]. This observation has led prior works to claim that a tradeoff between robustness and accuracy may be inevitable for many classiﬁcation tasks [57, 65].
We take a closer look at the tradeoff between robustness and accuracy, aiming to identify properties of data and training methods that enable neural networks to achieve both. A plausible reason why robustness may lead to lower accuracy is that different classes are very close together or they may even overlap (which underlies the argument for an inevitable tradeoff [57]). We begin by testing if this is the case in real data through an empirical study of four image datasets. Perhaps surprisingly, we ﬁnd that these datasets actually satisfy a natural separation property that we call r-separation: examples from different classes are at least distance 2r apart in pixel space. This r-separation holds for values of r that are higher than the perturbation radii used in adversarial example experiments.
We next consider separation as a guiding principle for better understanding the robustness-accuracy tradeoff. Neural network classiﬁers are typically obtained by rounding an underlying continuous
∗Equal contribution 1Code available at https://github.com/yangarbiter/robust-local-lipschitz. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
X →
RC with C classes. We take inspiration from prior work, which shows that function f :
Lipschitzness of f is closely related to its robustness [10, 21, 46, 60, 64]. However, one drawback of the existing arguments is that they do not provide a compelling and realistic assumption on the data that guarantees robustness and accuracy. We show theoretically that any r-separated data distribution has a classiﬁer that is both robust up to perturbations of size r, and accurate, and it can be obtained by rounding a function that is locally Lipschitz around the data. This suggests that there should exist a robust and highly accurate classiﬁer for real image data. Unfortunately, the current state of robust classiﬁcation falls short of this prediction, and the discrepancy remains poorly understood.
To better understand the theory-practice gap, we empirically investigate several existing methods on a few image datasets with a special focus on their local Lipschitzness and generalization gaps. We
ﬁnd that of the methods investigated, adversarial training (AT) [32], robust self-training (RST) [42] and TRADES [64] impose the highest degree of local smoothness, and are the most robust. We also
ﬁnd that the three robust methods have large gaps between training and test accuracies as well as adversarial training and test accuracies. This suggests that the disparity between theory and practice may be due to the limitations of existing training procedures, particularly in regards to generalization.
We then experiment with dropout, a standard generalization technique, on top of robust training methods on two image datasets where there is a signiﬁcant generalization gap. We see that dropout in particular narrows the generalization gaps of TRADES and RST, and improves test accuracy, test adversarial accuracy as well as test Lipschitzness. In summary, our contributions are as follows.
•
•
•
Through empirical measurements, we show that several image datasets are separated.
We prove that this separation implies the existence of a robust and perfectly accurate classiﬁer that can be obtained by rounding a locally Lipschitz function. In contrast to prior conjectures [12, 16, 57], robustness and accuracy can be achieved together in principle.
We investigate smoothness and generalization properties of classiﬁers produced by current training methods. We observe that the training methods AT, TRADES, and RST, which produce robust classiﬁers, also suffer from large generalization gaps. We combine these robust training methods with dropout [54], and show that this narrows the generalization gaps and sometimes makes the classiﬁers smoother.
What do our results imply about the robustness-accuracy tradeoff in deep learning? They suggest that this tradeoff is not inherent. Rather, it is a consequence of current robustness methods. The past few years of research in robust machine learning has led to a number of new loss functions, yet the rest of the training process – network topologies, optimization methods, generalization tools – remain highly tailored to promoting accuracy. We believe that in order to achieve both robustness and accuracy, future work may need to redesign other aspects of the training process such as better network architectures using neural architecture search [13, 19, 47, 69]. Combining this with improved optimization methods and robust losses may be able to reduce the generalization gap in practice. 2 Preliminaries
X ⊆
Rd be an instance space equipped with a metric dist :
Let which robustness is measured. Let [C] =
RC, let f (x)i denote the value of the ith coordinate.
For a function f :
Robustness and Astuteness. Let B(x, ε) denote a ball of radius ε > 0 around x in a metric space.
We use B∞ to denote the (cid:96)∞ ball. A classiﬁer g is robust at x with radius ε > 0 if for all x(cid:48)
B(x, ε),
B(x, ε). The astuteness we have g(x(cid:48)) = g(x). Also, g is astute at (x, y) if g(x(cid:48)) = y for all x(cid:48) of g at radius ε > 0 under a distribution µ is
R+; this is the metric in 2. denote the set of possible labels with C 1, 2, . . . , C
X × X →
X →
≥
∈
∈
}
{
Pr (x,y)∼µ
[g(x(cid:48)) = y for all x(cid:48)
B(x, ε)].
∈
The goal of robust classiﬁcation is to ﬁnd a g with the highest astuteness [59]. We sometimes use clean accuracy to refer to standard test accuracy (no adversarial perturbation), in order to differentiate it from robust accuracy a.k.a. astuteness (with adversarial perturbation).
Local Lipschitzness. Here we deﬁne local Lipschitzness theoretically; Section 5 later provides an empirical way to estimate this quantity. 2
Deﬁnition 1. Let ( radius r if for each i
, dist) be a metric space. A function f :
[C], we have f (x(cid:48))i f (x)i
L dist(x, x(cid:48)) for all x(cid:48) with dist(x, x(cid:48))
RC is L-locally Lipschitz at r.
X →
|
−
| ≤
·
≤
X
∈ (1), . . . ,
Separation. We formally deﬁne separated data distributions as follows. Let
[C]. classes
Deﬁnition 2 (r-separation). We say that a data distribution over (cid:83) dist( (C), where all points in (i) have label i for i
= j, where dist( 2r for all i (j)) (i), (i),
X
X
X
∈ i∈[C] X (j)) = minx∈X (i),x(cid:48)∈X (j) dist(x, x(cid:48)). contain C disjoint
X (i) is r-separated if
X
X
≥
X
X
In other words, the distance between any two examples from different classes is at least 2r. One of our motivating observations is that many real classiﬁcation tasks comprise of separated classes; for example, if dist is the (cid:96)∞ norm, then images with different categories (e.g., dog, cat, panda, etc) will be r-separated for a value r > 0 depending on the image space (see Figure 1). In the next section, we empirically verify that this property actually holds for a number of standard image datasets.
Figure 1: Intuitive example of r-separated images from Restricted ImageNet [57]. Even with a 2r-perturbation, classes do not overlap. Moving 2r from turtle to ﬁsh still looks more like a turtle. 3 Real Image Datasets are r-Separated
We begin by addressing the question: Are image datasets r-separated for ε r and attack radii ε in standard robustness experiments? While we cannot determine the underlying data distribution, we can empirically measure whether current training and test sets are r-separated. These measurements can potentially throw light on what can be achieved in terms of test robustness in real data. (cid:28)
We consider four datasets: MNIST, CIFAR-10, SVHN and Restricted ImageNet (ResImageNet), where ResImageNet contains images from a subset of ImageNet classes [32, 45, 57]. We present two statistics in Table 1 The Train-Train Separation is the (cid:96)∞ distance between each training example and its closest neighbor with a different class label in the training set, while the Test-Train Separation is the (cid:96)∞ distance between each test example and its closest example with a different class in the training set. See Figure 3 for histograms. We use exact nearest neighbor search to calculate distances. Table 1 also shows the typical adversarial attack radius ε for the datasets; more details are in Appendix D. adversarial perturbation
ε minimum
Train-Train separation minimum
Test-Train separation
MNIST
CIFAR-10
SVHN
ResImageNet 0.1 0.031 0.031 0.005 0.737 0.212 0.094 0.180 0.812 0.220 0.110 0.224
Table 1: Separation of real data is 3 perturbation radii.
× to 7
× typical
Figure 2: Robust classifers exist if the perturbation is less than the separation. (a) MNIST (b) SVHN (c) CIFAR-10 (d) Restricted ImageNet
Figure 3: Train-Test separation histograms: MNIST, SVHN, CIFAR-10 and Restricted ImageNet. 3 (cid:54)
Both the Train-Train and Test-Train separations are higher than 2ε for all four datasets. We note that SVHN contains a single duplicate example with multiple labels, and one highly noisy example; removing these three examples out of 73, 257 gives us a minimum Train-Train Separation of 0.094, which is more than enough for attack radius ε = 0.031 8/255. Restricted ImageNet is similar with three pairs of duplicate examples, and two other highly noisy training examples (see Figures 7 and 8 in Appendix D). Barring a handful of highly noisy examples, real image datasets are indeed r-separated when r is equal to the attack radii commonly used in adversarial robustness experiments.
≈
These results imply that in real image data, the test images are far apart from training images from a different class. There perhaps are images of dogs which look like cats, but standard image datasets are quite clean, and such images mostly do not occur in either their test nor the training sets. In the next section, we explore consequences of this separation. 4 Robustness and Accuracy for r-Separated Data
We have just shown that four image datasets are indeed r-separated, for ε r where ε is the typical adversarial perturbation used in experiments. We now show theoretically that if a data distribution is r-separated, then there exists a robust and accurate classiﬁer that can be obtained by rounding a locally Lipschitz function. Additionally, we supplement these results in Appendix C by a constructive
“existence proof” that demonstrates proof-of-concept neural networks with both high accuracy and robustness on some of these datasets; this illustrates that at least on these image datasets, these classiﬁers can potentially be achieved by neural networks. (cid:28) 4.1 r-Separation implies Robustness and Accuracy through Local Lipschitzness
We show that it is theoretically possible to achieve both robustness and accuracy for r-separated data.
In particular, we exhibit a classiﬁer based on a locally Lipschitz function, which has astuteness 1 with radius r. Working directly in the multiclass case, our proof uses classiﬁers of the following
RC so that f (x) is a form. If there are C classes, we start with a vector-valued function f : (i)) = minz∈X (i) dist(x, z). We analyze the following
C-dimensional real vector. We let dist(x, function
X →
X
In other words, we set f (x)i = 1 r · dist(x,
X f (x) = dist(x, (1)), . . . , dist(x, (cid:17) (C))
. (cid:16) 1 r ·
X (i)). Then, we deﬁne a classiﬁer g :
X
[C] as
X → (1) (2) g(x) = argmin f (x)i. i∈[C]
We show that accuracy and local Lipschitzness together imply astuteness (proofs in Appendix A).
Lemma 4.1. Let f : with true label y
[C]. If
RC be a function, and consider x r -Locally Lipschitz in a radius r around x, and
X →
∈ X
∈ f is 1 f (x)j
•
• f (x)y 2 for all j
= y,
−
≥ then g(x) = argmini f (x)i is astute at x with radius r.
Finally, we show that there exists an astute classiﬁer when the distribution is r-separated.
Theorem 4.2. Suppose the data distribution
X
RC such that
There exists a function f : is r-separated, denoting C classes (1), . . . , (C).
X
X
X → (a) f is 1 r -locally-Lipschitz in a ball of radius r around each x (b) the classiﬁer g(x) = argmini f (x)i has astuteness 1 with radius r.
∈ i∈[C] X (cid:83) (i), and
While the classiﬁer g used in the proof of Theorem 4.2 resembles the 1-nearest-neighbor classiﬁer, it is actually different on any ﬁnite sample, and the classiﬁers only coincide in the inﬁnite sample limit or when the class supports are known.
Binary case. We also state results for the special case of binary classiﬁcation. Let be the instance space with disjoint class supports
− =
+
X
. Then, we deﬁne f :
∅
+)
X
.
−
+
∪ X
R as
=
X
X → f (x) = dist(x,
X
X
−)
∩ X dist(x,
− 2r 4 (cid:54)
It is immediate that if classiﬁer g = sign(f ) achieves the guarantees in the following theorem using the next lemma. is r-separated, then f is locally Lipschitz with constant 1/r, and also the
X
Lemma 4.3. Let f : of radius r > 0 around x, (b)
| ≥ g = sign(f ) is astute at x with radius r.
R, and let x f (x)
|
X → have label y. If (a) f is 1 r -Locally Lipschitz in a ball
∈ X 1, and (c) g(x) has the same sign as y, then the classiﬁer
Theorem 4.4. Suppose the data distribution
X function f such that (a) f is 1 classiﬁer g = sign(f ) has astuteness 1 with radius r.
+ r -locally Lipschitz in a ball of radius r around all x
− is r-separated. Then, there exists a and (b) the
∈ X
∪ X
=
X
A visualization of the function (and resulting classiﬁer) from Theorem 4.4 for a binary classiﬁcation dataset appears in Figure 4. Dark colors indicate high conﬁdence (far from decision boundary) and lighter colors indicate the gradual change from one label to the next. The classiﬁer g = sign(f ) guaranteed by this theorem will predict the label based on which decision region (positive or negative) is closer to the input example. Figure 5 shows a pictorial example of why using a locally Lipschitz function can be just as expressive while also being robust.
Figure 4: Plot of f (x) from Theorem 4.4 for the spiral dataset. The classiﬁer g = sign(f ) has high accuracy and astuteness because it gradually changes near the decision boundary. 4.2
Implications
Figure 5: The classiﬁer corresponding to the orange boundary has small local Lipschitz-ness because it does not change in the (cid:96)∞ balls around data points. The black curve, however, is vulnerable to adversarial exam-ples even though it has high clean accuracy.
We consider the consequences of Table 1 and Theorem 4.2 taken together. Then, in Section 5, we empirically explore the limitations of current robustness techniques.
Signiﬁcance for real data. Theorem 4.2 refers to supports of distributions, while our measurements in Table 1 are on actual data. Hence, the results do not imply perfect distributional accuracy and robustness. However, our test set measurements suggest that even if the distributional supports may be close in the inﬁnite sample limit, the close images are rare enough that we do not see them in the test sets. Thus, we still expect high accuracy and robustness on these test sets. Additionally, if we are willing to assume that the data supports are representative of the support of the distribution, then we can conclude the existence of a distributionally robust and accurate classiﬁer. Combined with proof-of-concept results in Appendix C, we deduce that these classiﬁers can be implemented by neural networks. The remaining question is how such networks can be trained with existing methods.
Optimally astute classiﬁer and non-parametrics (comparison to Yang et al. [62]). Prior work proposes adversarial pruning, a method that removes training examples until different classes are r-separated. They exhibit connections to maximally astute classiﬁer, which they call the r-Optimal classiﬁer for size r perturbations [62]. Follow-up work proved that training various non-parametric classiﬁers after pruning leads them to converge to maximally astute classiﬁers under certain condi-tions [5]. Our result in Theorem 4.2 complements these efforts, showing that the r-Optimal classiﬁer can be obtained by the classiﬁer in Theorem 4.2. Moreover, we provide additional justiﬁcation for adversarial pruning by presenting a new perspective on the role of data separation in robustness.
Lower bounds on test error. Our results also corroborate some recent works that use optimal transport to estimate a lower bound on the robust test accuracy of any classiﬁer on standard datasets.
They ﬁnd that it is actually zero for typical perturbation sizes [4, 38]. In other words, we have further evidence that well-curated benchmark datasets are insufﬁcient to demonstrate a tradeoff between robustness and accuracy, in contrast to predictions of an inevitable tradeoff [11, 12, 16, 57]. 5
Robustness is not inherently at odds with accuracy (comparison to Tsipras et al. [57]). Prior work provides a theoretical example of a data distribution where any classiﬁer with high test accuracy must also have small adversarial accuracy under (cid:96)∞ perturbations. Their theorem led the authors to posit that (i) accuracy and robustness may be unachievable together due their inherently opposing goals, and (ii) the training algorithm may not be at fault [57]. We provide an alternative view.
− 1)-dimensional Gaussian distributions either with mean r or
Their distribution is deﬁned using the following sampling process: the ﬁrst feature is the binary class label (ﬂipped with a small probability), and the other d 1 features are sampled from one of two (d r depending on the true example label. While the means are separated with distance 2r, their distribution is not r-separated due to the noise in the ﬁrst feature combined with the inﬁnite support of the Gaussians. Their lower bound is tight and only holds for (cid:96)∞ perturbations ε satisfying ε 2r. Our experiments in Section 3 have already shown that r-separation is a realistic assumption, and typical perturbations ε satisfy
ε r. Taken together with Theorem 4.2, we conclude that the robustness-accuracy tradeoff in neural networks and image classiﬁcation tasks is not intrinsic. (cid:28)
−
−
≥ 5 A Closer Look at Existing Training Methods
So far we have shown that robustness and accuracy should both be achievable in principle, but practical networks continue to trade robustness off for accuracy. We next empirically investigate why this tradeoff might arise. One plausible reason might be that existing training methods do not impose local Lipschitzness properly; another may be that they do not generalize enough. We next explore these hypotheses in more detail, considering the following questions:
How locally Lipschitz are the classiﬁers produced by existing training methods?
How well do classiﬁers produced by existing training methods generalize?
•
•
These questions are considered in the context of one synthetic and four real datasets, as well as several plausible training methods for improving adversarial robustness. We do not aim to achieve best performance for any method, but rather to understand smoothness and generalization. 5.1 Experimental Methodology
We evaluate train/test accuracy, adversarial accuracy and local lipschitzness of neural networks trained using different methods. We also measure generalization gaps: the difference between train and test clean accuracy (or between train and test adversarial accuracy).
Training Methods. We consider neural networks trained via Natural training (Natural), Gradient
Regularization (GR) [14], Locally Linear Regularization (LLR) [40], Adversarial Training (AT) [32], and TRADES [65]. Additionally, we use Robust Self Training (RST) [42], a recently introduced method that minimizes a linear combination of clean and robust accuracy in an attempt to improve robustness-accuracy tradeoffs. For fair comparison between methods, we use a version of RST that only uses labeled data. Both RST and TRADES have a parameter; for RST higher λ means higher weight is given to the robust accuracy, while for TRADES higher β means higher weight given to enforcing local Lipschitzness. Details are provided in Appendix B.
Adversarial Attacks. We evaluate robustness with two attacks. In this section, we use Projected gradient descent (PGD) [25] for adversarial accuracy with step size ε/5 and a total of 10 steps. The
Multi-Targeted Attack (MT) [18] leads to similar conclusions; results in Appendix E.1.
Measuring Local Lipschitzness. For each classiﬁer, we empirically measure the local Lipschitzness of the underlying function by the empirical Lipschitz constant deﬁned as the following quantity 1 n n (cid:88) i=1 max i∈B∞(xi,ε) x(cid:48) (cid:107) f (xi) xi (cid:107) f (x(cid:48) i) (cid:107) x(cid:48)
∞ i(cid:107)
−
− 1
. (3)
A lower value of the empirical Lipschitz constant implies a smoother classiﬁer. We estimate this through a PGD-like procedure, where we iteratively take a step towards the gradient direction
) where ε is the perturbation radius. We use step size ε/5 and a total of 10 steps. ( i)(cid:107)1 (cid:107)f (xi)−f (x(cid:48) (cid:107)xi−x(cid:48) i(cid:107)∞
∇x(cid:48) i 6
architecture
CNN1
CNN2 train acc. 100.00 99.99 100.00 99.98 100.00 100.00 100.00 99.81 99.21 97.50 test acc. adv test acc. test lipschitz 99.20 99.29 99.43 99.31 99.34 99.31 99.31 99.26 98.96 97.54 59.83 91.03 92.14 97.21 96.53 96.96 97.09 96.60 96.66 93.68 67.25 26.05 30.44 8.84 11.09 11.31 12.39 9.69 7.83 2.87 adv gap 0.45 3.49 4.42 2.67 3.16 2.95 2.87 2.10 1.33 0.37 gap 0.80 0.70 0.57 0.67 0.66 0.69 0.69 0.55 0.25
-0.04 train acc. 100.00 99.99 100.00 99.98 100.00 100.00 100.00 99.96 99.80 99.61 test acc. adv test acc. test lipschitz 99.51 99.55 99.57 99.48 99.53 99.55 99.56 99.58 99.57 99.59 86.01 93.71 95.13 98.03 97.72 98.27 98.48 98.10 98.54 98.73 23.06 20.26 9.75 6.09 8.27 6.26 4.55 4.74 2.14 1.36 adv gap
-0.28 2.55 2.28 1.92 2.27 1.73 1.52 1.70 1.18 0.80 gap 0.49 0.44 0.43 0.50 0.47 0.45 0.44 0.38 0.23 0.02
Natural
GR
LLR
AT
RST(λ=.5)
RST(λ=1)
RST(λ=2)
TRADES(β=1)
TRADES(β=3)
TRADES(β=6)
Table 2: MNIST (perturbation 0.1). We compare two networks: CNN1 (smaller) and CNN2 (larger).
We evaluate adversarial accuracy with the PGD-10 attack and compute Lipschitzness with Eq. (3).
We also report the standard and adversarial generalization gaps.
.
CIFAR-10
Restricted ImageNet train acc. 100.00 94.90 100.00 99.90 99.86 99.73 99.84 99.76 99.78 98.93 test acc. adv test acc. test lipschitz 93.81 80.74 91.44 85.11 84.61 83.87 83.51 84.96 85.55 84.46 0.00 21.32 22.05 39.58 40.89 41.75 43.51 43.66 46.63 48.58 425.71 28.53 94.68 20.67 23.15 23.80 26.23 28.01 22.42 13.05 gap 6.19 14.16 8.56 14.79 15.25 15.86 16.33 14.80 14.23 14.47 adv gap 0.00 3.94 4.50 36.26 41.31 43.54 49.94 44.60 47.67 42.65 train acc. test acc. adv test acc. test lipschitz gap 97.72 91.12 98.76 96.08 95.66 96.02 96.22 97.39 95.74 93.34 93.47 88.51 93.44 92.02 92.06 91.14 90.33 92.27 90.75 88.92 7.89 62.14 52.62 79.24 79.69 81.41 82.25 79.90 82.28 82.13 32228.51 886.75 4795.66 451.57 355.43 394.40 287.97 2144.66 396.67 200.90 4.25 2.61 5.32 4.06 3.61 4.87 5.90 5.13 5.00 4.42 adv gap
-0.46 0.19 0.22 4.57 4.67 6.19 8.23 6.66 6.41 5.31
Natural
GR
LLR
RST(λ=.5)
RST(λ=1)
RST(λ=2)
AT
TRADES(β=1)
TRADES(β=3)
TRADES(β=6)
Table 3: CIFAR-10 (perturbation 0.031) and Restricted ImageNet (perturbation 0.005). We evaluate adversarial accuracy with the PGD-10 attack and compute Lipschitzness with Eq. (3).
Datasets. We evaluate the various algorithms on one synthetic dataset: Staircase [41] and four real datasets: MNIST [26], SVHN [33], CIFAR-10 [24] and Restricted ImageNet [57]. We consider adversarial (cid:96)∞ perturbations for all datasets. More details are in Appendix B. 5.2 Observations
Our experimental results, presented in Tables 2 and 3, provide a number of insights into the smooth-ness and generalization properties of classiﬁers trained by existing methods.
How well do existing methods impose local Lipschitzness? There is a large gap in the degree of local Lipschitzness in classiﬁers trained by AT, RST and TRADES and those trained by natural training, GR and LLR. Classiﬁers in the former group are considerably smoother than the latter.
Classiﬁers produced by TRADES are the most locally Lipschitz overall, with smoothness improving with increasing β. AT and RST also produce classiﬁers of comparable smoothness – but less smooth than TRADES. Overall, local Lipschitzness appears mostly correlated with adversarial accuracy; the more robust methods are also the ones that impose the highest degree of local Lipschitzness. But there are diminishing returns in the correlation between robustness and accuracy and local Lipschitzness; for example, the local smoothness of TRADES improves with higher β; but increasing β sometimes leads to drops in test accuracy even though the Lipschitz constant continues to decrease.
How well do existing methods generalize? We observe that for the methods that produce locally
Lipschitz classiﬁers – namely, AT, TRADES and RST – also have large generalization gaps while natural training, GR and LLR generalize much better. In particular, there is a large gap between training and test accuracies of AT, RST and TRADES, and an even larger one between training and test adversarial accuracies. Although RST has better test accuracy than AT, it continues to have a large generalization gap with only labeled data. An interesting fact is that this generalization behaviour is quite unlike linear classiﬁcation, where imposing local Lipschitzness leads to higher margin and better generalization [61] – imposing local Lipschitzness in neural networks, at least through these methods, appears to hurt generalization instead of helping. This suggests that these robust training methods may not be generalizing properly. 7
SVHN
CIFAR-10 dropout test acc. adv test acc. test lipschitz
Natural
Natural
AT
AT
RST(λ=2)
RST(λ=2)
TRADES(β=3)
TRADES(β=3)
TRADES(β=6)
TRADES(β=6)
False
True
False
True
False
True
False
True
False
True 95.85 96.66 91.68 93.05 92.39 95.19 91.85 94.00 91.83 93.46 2.66 1.52 54.17 57.90 51.39 55.22 54.37 62.41 58.12 63.24 149.82 152.38 16.51 11.68 23.17 17.59 10.15 4.99 5.20 3.30 gap 4.15 3.34 5.11
-0.14 6.86 1.90 7.48 0.48 5.35 0.45 adv gap 0.87 1.22 25.74 6.48 36.02 11.30 33.33 7.91 23.88 5.97 test acc. adv test acc. test lipschitz 93.81 93.87 83.51 85.20 83.87 85.49 85.55 86.43 84.46 84.69 0.00 0.00 43.51 43.07 41.75 40.24 46.63 49.01 48.58 52.32 425.71 384.48 26.23 31.59 23.80 34.45 22.42 14.69 13.05 8.13 gap 6.19 6.13 16.33 14.51 15.86 14.00 14.23 12.59 14.47 11.91 adv gap 0.00 0.00 49.94 44.05 43.54 33.07 47.67 35.03 42.65 26.49
Table 4: Dropout and generalization. SVHN (perturbation 0.031, dropout rate 0.5) and CIFAR-10 (perturbation 0.031, dropout rate 0.2). We evaluate adversarial accuracy with the PGD-10 attack and compute Lipschitzness with Eq. (3). 5.3 A Closer Look at Generalization
A natural follow-up question is whether the generalization gap of existing methods can be reduced by existing generalization-promoting methods in deep learning. In particular, we ask: Can we improve the generalization gap of AT, RST and TRADES through generalization tools?
To better understand this question, we consider two medium-sized datasets, SVHN and CIFAR-10, which nevertheless have a reasonably high gap between the test accuracy of the model produced by natural training and the best robust model. We then experiment with dropout [54], a standard and highly effective generalization method. For SVHN, we use a dropout rate of 0.5 and for CIFAR-10 a rate of 0.2. More experimental details are provided in the Appendix B.
Table 4 shows the results, contrasted with standard training. We observe that dropout narrows the generalization gap between training and test accuracy, as well as adversarial training and test accuracy signiﬁcantly for all methods. For SVHN, after incorporating dropout, the best test accuracy is achieved by RST (95.19%) along with an adversarial test accuracy of 55.22%; the best adversarial test accuracy (62.41%) is with TRADES (β = 3) along with a test accuracy of (94.10%). Both accuracies are much closer to the accuracy of natural training (96.66%), and the test adversarial accuracies are also signiﬁcantly higher. A similar narrowing of the generalization gap is visible for
CIFAR-10 as well. Dropout also appears to make the networks smoother as test Lipschitzness also appears to improve for all algorithms for SVHN, and for TRADES for CIFAR-10.
Dropout Improvements. Our results show that the generalization gap of AT, RST and TRADES can be reduced by adding dropout; this reduction is particularly effective for RST and TRADES. Dropout additionally decreases the test local Lipschitzness of all methods – and hence promotes generalization all round – in accuracy, adversarial accuracy, and also local Lipschitzness. This suggests that combining dropout with the robust methods may be a good strategy for overall generalization. 5.4
Implications
Our experimental results lead to three major observations. We see that the training methods that produce the smoothest and most robust classiﬁers are AT, RST and TRADES. However, these robust methods also do not generalize well, and the generalization gap narrows when we add dropout.
Comparison with Rice et al. [43]. An important implication of our results is that generalization is a particular challenge for existing robust methods. The fact that AT may sometimes overﬁt has been previously observed by [41, 43, 55]; in particular, Rice et al. [43] also experiments with a few generalization methods (but not dropout) and observes that only early stopping helps overcome overﬁtting to a certain degree. We expand the scope of these results to show that RST and TRADES also suffer from large generalization gaps, and that dropout can help narrow the gap in these two methods. Furthermore, we demonstrate that dropout often decreases the local Lipschitz constant. 8
6