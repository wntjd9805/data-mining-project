Abstract
We establish a new class of minimax prediction error bounds for generalized linear models. Our bounds signiﬁcantly improve previous results when the design matrix is poorly structured, including natural cases where the matrix is wide or does not have full column rank. Apart from the typical L2 risks, we study a class of entropic risks which recovers the usual L2 prediction and estimation risks, and demonstrate that a tight analysis of Fisher information can uncover underlying structural dependency in terms of the spectrum of the design matrix. The minimax approach we take differs from the traditional metric entropy approach, and can be applied to many other settings. 1

Introduction
Throughout, we consider a parametric framework where observations X ∈ Rn are generated according to X ∼ Pθ, where Pθ denotes a probability measure on a measurable space (X ⊆ Rn, F) indexed by an underlying parameter θ ∈ Θ ⊂ Rd. For each Pθ, we associate a density f (·; θ) with respect to an underlying measure λ on (X , F) according to dPθ(x) = f (x; θ)dλ(x).
This setup contains a vast array of fundamental applications in machine learning, engineering, neuroscience, ﬁnance, statistics and information theory [1–10]. As examples, mean estimation [1], covariance and precision matrix estimation [2], phase retrieval [3,4], group or membership testing [5], pairwise ranking [10], can all be modeled in terms of parametric statistics. The central question to address in all of these problems is essentially the same: how accurately can we infer the parameter θ given the observation X?
One of the most popular parameteric families is the exponential family, which captures a rich variety of parametric models such as binomial, Gaussian, Poisson, etc. Given a parameter η ∈ R, a density f (·; η) is said to belong to the exponential family if it can be written as f (x; η) = g(x) exp (cid:18) ηx − Φ(η) s(σ) (cid:19)
. (1)
Here, the parameter η is the natural parameter, g : X ⊆ R → [0, ∞) is the base measure, Φ : R → R is the cumulant function, and s(σ) > 0 is a variance parameter. The density f (·; η) is understood to be on a probability space (X ⊆ R, F) with respect to a dominating σ-ﬁnite measure λ.
In this work, we are interested in the following generalized linear model (GLM), where observation
X ∈ Rn is generated according to an exponential family with natural parameter equal to a linear transformation of the underlying parameter θ. In other words, f (x; θ) = n (cid:89) (cid:26) i=1 g(xi) exp (cid:18) xi(cid:104)mi, θ(cid:105) − Φ((cid:104)mi, θ(cid:105)) s(σ) (cid:19)(cid:27)
, (2) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
i=1 ⊂ Rd. for a real parameter θ := (θ1, θ2, . . . , θd) ∈ Rd and a ﬁxed design matrix M ∈ Rn×d, with rows given by the vectors {mi}n
The above model assumes each Xi is drawn from its own exponential family, with respective natural parameters (cid:104)mi, θ(cid:105), i = 1, 2, . . . , n. Evidently, this captures the classical (Gaussian) linear model X = M θ + Z, where f (·; θ) is taken to be the usual Gaussian density, and also captures a much broader class of problems including phase retrieval, matrix recovery and logistic regression.
See [11–13] for history and theory of the generalized linear model.
In order to evaluate the performance of an estimator ˆθ (i.e., a measurable function of X), it is common to deﬁne a loss function L(·, ·) : Rd × Rd (cid:55)−→ R and analyze the loss L(θ, ˆθ). A typical ﬁgure of merit is the constrained minimax risk R(M, Θ), deﬁned as
R(M, Θ) := inf
ˆθ sup
θ∈Θ
L(θ, ˆθ).
In words, the minimax risk characterizes the worst-case risk under the speciﬁed loss L(·, ·) achieved by the best estimator, with a constraint that θ belongs to a speciﬁed parameter space Θ.
Two choices of the loss function L(·, ·) give rise to the usual variants of L2 loss: 1. Estimation loss, where the loss function L(·, ·) is deﬁned as
L1(θ, ˆθ) = E(cid:107)θ − ˆθ(cid:107)2 for all θ, ˆθ ∈ Rd. 2. Prediction loss, where the loss function L(·, ·) is deﬁned as
L2(θ, ˆθ) = 1 n
E(cid:107)M θ − M ˆθ(cid:107)2 for all θ, ˆθ ∈ Rd. (3) (4)
In this work, we shall approach things from an information theoretic viewpoint. In particular, we will bound minimax risk under entropic loss (closely connected to logarithmic loss in the statistical learning and information literature, see, e.g., [14–16]), from which L2 estimates will follow. To start, let us review some of the key deﬁnitions in information theory. Suppose the parameter θ ∈ Rd follows a prior π, a probability measure on Rd having density ψ with respect to Lebesgue measure.
The differential entropy h(θ) corresponding to random variable θ is deﬁned as h(θ) := − (cid:90)
Rd
ψ(u) log ψ(u)du.
Here and throughout, we will take logarithms with respect to the natural base, and assume all entropies exist (i.e., their deﬁning integrals exist in the Lebesgue sense). The mutual information I(θ; X) between parameter θ ∼ π and observation X ∼ Pθ is deﬁned as (cid:90) (cid:90)
I(θ; X) := f (x; θ) log dλ(x)dπ(θ). f (x; θ)
Rd f (x; θ(cid:48))dπ(θ(cid:48)) (cid:82)
Rd
X
The conditional entropy is deﬁned as h(θ|X) := h(θ) − I(θ; X). The entropy power of a random variable U is deﬁned as exp(2h(U )), and for any two random variables U and V with well-deﬁned conditional entropy, the conditional entropy power is deﬁned similarly as exp(2h(U |V )).
Lower bounds on conditional entropy power can be translated into lower bounds of other losses, via tools in rate distortion theory [17]. To illustrate this, let’s consider the following two Bayes risks, with suprema taken over all priors π on the parameter space Θ ⊆ Rd, and inﬁma taken over all valid estimators ˆθ (i.e., measurable functions of X). 1. Entropic estimation loss, where the Bayes risk is deﬁned as
Re(M, Θ) := inf
ˆθ sup
π (cid:16) 2h(θi|ˆθi) (cid:17)
. exp n (cid:88) i=1 2. Entropic prediction loss, where the Bayes risk is deﬁned as
Rp(M, Θ) := inf
ˆθ sup
π 1 n n (cid:88) i=1 (cid:16) exp 2h(m(cid:62) i θ|m(cid:62) i (cid:17)
.
ˆθ) (5) (6) 2
The following simple observation shows that any lower bound derived for the entropic Bayes risks implies a lower bound on the minimax L2 risks.
Lemma 1. We have inf ˆθ supθ∈Θ L1(θ, ˆθ) (cid:38) Re(M, Θ) and inf ˆθ supθ∈Θ L2(θ, ˆθ) (cid:38) Rp(M, Θ).
Proof. This follows since Gaussians maximize entropy subject to second moment constraints and conditioning reduces entropy: E(θi− ˆθi)2 ≥ Var(θi− ˆθi) (cid:38) exp(2h(θi− ˆθi)) (cid:38) exp(2h(θi| ˆθi)).
Here and onwards, we use “(cid:38)” (also “(cid:46)” and “(cid:16)”) to refer to “≥” (and “≤”, “=”, respectively) up to constants that do not depend on parameters.
Although we focus on L2 loss in the present work, we remark that minimax bounds on entropic loss directly yield corresponding estimates on Lp loss using standard arguments involving covering and packing numbers of Lp spaces. See, for example, the work by Raskutti et al. [18]. Despite its universal nature, there is relatively limited work on deriving minimax bounds for the entropic loss. This is the focus of the present work, and as a consequence, we obtain bounds on L2 loss that signiﬁcantly improve on prior results when the matrix M is poorly structured. 1.1 Contributions
In this paper, we make three main contributions. 1. First, we establish L2 minimax risk and entropic Bayes risk bounds for the generalized linear model (2). The generality of the GLM allows us to extend our results to speciﬁc instances of the GLM such as the Gaussian linear model, phase retrieval and matrix recovery. 2. Second, we establish L2 minimax risk and entropic Bayes risk bounds for the Gaussian linear model. In particular, our bounds are nontrivial for many instances where previous results fail (for example when M ∈ Rn×d does not have full column rank, including cases with d > n), and can be naturally applied to the sparse problem where (cid:107)θ(cid:107)0 ≤ k. Further, we show that both our minimax risk and entropic Bayes risk bounds are tight up to constants and log factors when M is sampled from a Gaussian ensemble. 3. Third, we investigate the L2 minimax risk via the lens of the entropic Bayes risk, and provide evidence that information theoretic minimax methods can naturally extract dependencies on the structure of design matrix M via analysis of Fisher information. The techniques we develop are general and can be used to establish minimax results for other problems. 2 Main Results and Discussion
The following notation is used throughout: upper-case letters (e.g., X, Y ) denote random variables or matrices, and lower-case letters (e.g., x, y) denote realizations of random variables or vectors. We use subscript notation vi to denote the i-th component of a vector v = (v1, v2, . . . , vd). We let [k] denote the set {1, 2, . . . , k}.
We will be making the following assumption.
Assumption: The second derivative of the cumulant function Φ is bounded uniformly by a constant
L > 0: Φ(cid:48)(cid:48)(·) ≤ L.
The following lemma characterizes the mean and variance of densities in the exponential family.
Lemma 2 (Page 29, [11]). Any observation X generated according to the exponential family (1) has mean Φ(cid:48)(η) and variance s(σ) · Φ(cid:48)(cid:48)(η).
In other words, our assumption is equivalent to saying that the variance of each observation
X1, . . . , Xn is bounded. This is a common assumption made in the literature; See, for exam-ple, [19–22].
Our ﬁrst main result establishes a minimax prediction lower bound corresponding to the generalized linear model (2). Let us ﬁrst make a few deﬁnitions. For an n × k matrix A, we deﬁne the vector
ΛA := (λ1, . . . , λk) ∈ Rk, where the λi’s denote the eigenvalues of the k × k symmetric matrix 3
A(cid:62)A in descending order. (cid:107)ΛA(cid:107)p denotes the usual Lp norm of the vector ΛA for p ≥ 1. Finally, we deﬁne
Γ(A) := max (cid:18) (cid:107)ΛA(cid:107)2 1 (cid:107)ΛA(cid:107)2 2
, λmin(A(cid:62)A) (cid:107)Λ−1
A (cid:107)1 (cid:19)
, (7)
:= (λ−1 1 , . . . , λ−1 k ), with the convention that λmin(A(cid:62)A)(cid:107)Λ−1 where Λ−1
A
λmin(A(cid:62)A) = 0.
Theorem 3. For observations X ∈ Rn generated via the generalized linear model (2) with a ﬁxed design matrix M ∈ Rn×d, the minimax L2 prediction risk and the entropic Bayes prediction risk are lower bounded by
A (cid:107)1 = 0 when 1 n 1 n inf
ˆθ sup
θ∈Rd
E(cid:107)M ˆθ − M θ(cid:107)2 (cid:38) 1 n s(σ)
L
Γ(M ). inf
ˆθ sup
π n (cid:88) i=1 (cid:16) exp 2h(m(cid:62) i θ | m(cid:62) i (cid:17)
ˆθ) (cid:38) 1 n s(σ)
L (cid:107)ΛM (cid:107)2 1 (cid:107)ΛM (cid:107)2 2
.
Bounds on minimax risk under an additional sparsity constraint (cid:107)θ(cid:107)0 ≤ k (i.e., the true parameter θ has at most k non-zero entries) can be derived as a corollary.
Corollary 4 (Sparse Version of Theorem 3). For observations X ∈ Rn generated via the generalized linear model (2), with the additional constraint that (cid:107)θ(cid:107)0 ≤ k (i.e., Θ := {θ ∈ Rd : (cid:107)θ(cid:107)0 ≤ k}), the minimax prediction error is lower bounded by 1 n 1 n inf
ˆθ sup
θ∈Θ
E(cid:107)M ˆθ − M θ(cid:107)2 (cid:38) 1 n s(σ)
L inf
ˆθ sup
π n (cid:88) i=1 (cid:16) exp 2h(m(cid:62) i θ | m(cid:62) i (cid:17)
ˆθ) max
Q∈Mk (cid:38) 1 n
Γ(Q). s(σ)
L max
Q∈Mk (cid:107)ΛQ(cid:107)2 1 (cid:107)ΛQ(cid:107)2 2
.
Here, the maximum is taken over Mk, the set of all n × k(cid:48) submatrices of M , with k(cid:48) ≤ k.
We now note an important specialization of Corollary 4. In particular, consider the Gaussian linear model with observations X ∈ Rn generated according to
X = M θ + Z, (8) with Z ∼ N (0, σ2 In) the standard Gaussian vector. This corresponds to the GLM of (2) when the functions are taken to be h(x) = e−x2/(2σ2), s(σ) = σ2, and Φ(t) = t2/2 (hence, L = 1). This is a particularly important instance worth highlighting because of the ubiquity of the Gaussian linear model in applications.
Theorem 5. For observations X ∈ Rn generated via the Gaussian linear model (8), with the sparsity constraint (cid:107)θ(cid:107)0 ≤ k (i.e., Θ := {θ ∈ Rd : (cid:107)θ(cid:107)0 ≤ k}), the minimax prediction error is lower bounded by 1 n 1 n inf
ˆθ sup
θ∈Θ
E(cid:107)M ˆθ − M θ(cid:107)2 (cid:38) σ2 n max
Q∈Mk
Γ(Q). inf
ˆθ sup
π n (cid:88) i=1 (cid:16) exp 2h(m(cid:62) i θ | m(cid:62) i (cid:17)
ˆθ) (cid:38) σ2 n max
Q∈Mk (cid:107)ΛQ(cid:107)2 1 (cid:107)ΛQ(cid:107)2 2
.
Here, the maximum is taken over Mk, the set of all n × k(cid:48) submatrices of M , with k(cid:48) ≤ k.
Remark 6. In the above results, the function Γ(·) can in fact be replaced with
˜Γ(M ) := max (cid:32) n (cid:88) i=1 (cid:107)mi(cid:107)4 2 (cid:107)M mi(cid:107)2 , λmin(M (cid:62)M )(cid:107)Λ−1
M (cid:107)1 (cid:33)
, which is stronger than the original statements. However, the chosen statements above highlight the simple dependence on the spectrum of ΛM . 4
2.1