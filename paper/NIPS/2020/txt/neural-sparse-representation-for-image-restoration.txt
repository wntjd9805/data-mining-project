Abstract
Inspired by the robustness and efﬁciency of sparse representation in sparse coding based image restoration models, we investigate the sparsity of neurons in deep net-works. Our method structurally enforces sparsity constraints upon hidden neurons.
The sparsity constraints are favorable for gradient-based learning algorithms and attachable to convolution layers in various networks. Sparsity in neurons enables computation saving by only operating on non-zero components without hurting accuracy. Meanwhile, our method can magnify representation dimensionality and model capacity with negligible additional computation cost. Experiments show that sparse representation is crucial in deep neural networks for multiple image restora-tion tasks, including image super-resolution, image denoising, and image compres-sion artifacts removal. Code is available at https://github.com/ychfan/nsr. 1

Introduction
Sparse representation plays a critical role in image restoration problems, such as image super-resolution [1, 2, 3], denoising [4], compression artifacts removal [5], and many others [6, 7]. These tasks are inherently ill-posed, where the input signal usually has insufﬁcient information while the output has inﬁnitely many solutions w.r.t. the same input. Thus, it is commonly believed that sparse representation is more robust to handle the considerable diversity of solutions.
Sparse representation in sparse coding is typically high-dimensional but with limited non-zero components. Input signals are represented as sparse linear combinations of tokens from a dictionary.
High dimensionality implies larger dictionary size and generally leads to better restoration accuracy, since a more massive dictionary is capable of more thoroughly sampling the underlying signal space, and thus more precisely representing any query signal. Besides, sparsity limits numbers of non-zero elements work as an essential image prior, which has been extensively investigated and exploited to make restoration robust. Sparsity also brings computational efﬁciency by ignoring zero parts.
Deep convolutional neural networks for image restoration [8, 9, 10, 11, 12] extend the sparse coding based methods with repeatedly cascaded structures. The deep network based approach was ﬁrstly introduced to improve the performance in [13] and conceptually connected with previous sparse coding based methods. A simple network, with two convolutional layers bridged by a non-linear activation layer, can be interpreted as: activation denotes sparse representation; non-linearity enforces sparsity and convolutional kernels consist of the dictionary. SRResNet [14] extended the basic structure with skip connection to form a residual block and cascaded a large number of blocks to construct very deep residual networks.
Sparsity of hidden representation in deep neural networks cannot be efﬁciently solved by iterative optimization as sparse coding, since deep networks are feed-forward during inference. Sparsity of neurons is commonly achieved by ReLU activation in [15] by thresholding negative values to zero independently in each neuron. Still, its 50% sparsity on random vectors is far from the sparsity deﬁnition on the overall number of non-zero components. Oppositely, sparsity constraints are more 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
actively used in model parameters to achieve network pruning [16]. However, hidden representation dimensionality is reduced in pruned networks, and accuracy may hurt.
In this paper, we propose a method that can structurally enforce sparsity constraints upon hidden neurons in deep networks but also keep representation in high dimensionality. Given high-dimensional neurons, we divide them into groups along channels and allow only one group of neurons can be non-zero each time. The adaptive selection of the non-sparse group is modeled by tiny side networks upon context features. And computation is also saved when only performed on the non-zero group.
However, the selecting operation is not differentiable, so it is difﬁcult to embed the side networks for joint training. We relax the sparse constraints to soft and approximately reduce as a sparse linear combination of multiple convolution kernels instead of hard selection. We further introduce additional cardinal dimensions to decompose sparsity prediction into sub-problems by splitting each sparse group and concatenating after cardinal-independent combination of parameters.
To demonstrate the signiﬁcance of neural sparse representation, we conduct extensive experiments on image restoration tasks, including image super-resolution, denoising, and compression artifacts removal. Our experiments conclude that: (1) dedicated constraints are essential to achieve neural spar-sity representation and beneﬁt deep networks; (2) our method can signiﬁcantly reduce computation cost and improve accuracy, given the same size of model footprint; (3) our method can dramatically enlarge the model capacity and boost accuracy with negligible additional computation cost. 2