Abstract
Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing ﬂows are ﬂexible deep generative models that often surpris-ingly fail to distinguish between in- and out-of-distribution data: a ﬂow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing ﬂows perform poorly for OOD detection. We demonstrate that
ﬂows learn local pixel correlations and generic image-to-latent-space transforma-tions which are not speciﬁc to the target image datasets, focusing on ﬂows based on coupling layers. We show that by modifying the architecture of ﬂow coupling layers we can bias the ﬂow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable ﬂows to generate high-ﬁdelity images can have a detrimental effect on OOD detection. 1

Introduction
Normalizing ﬂows [42, 9, 10] seem to be ideal candidates for out-of-distribution detection, since they are simple generative models that provide an exact likelihood. However, Nalisnick et al. [29] revealed the puzzling result that ﬂows often assign higher likelihood to out-of-distribution data than the data used for maximum likelihood training. In Figure 1(a), we show the log-likelihood histogram for a
RealNVP ﬂow model [10] trained on the ImageNet dataset [37] subsampled to 64 64 resolution.
The ﬂow assigns higher likelihood to both the CelebA dataset of celebrity photos, and the SVHN dataset of images of house numbers, compared to the target ImageNet dataset.
⇥
While there has been empirical progress in improving OOD detection with ﬂows [29, 7, 30, 39, 40, 48], the fundamental reasons for why ﬂows fail at OOD detection in the ﬁrst place are not fully understood.
In this paper, we show how the inductive biases [28, 47] of ﬂow models — implicit assumptions in the architectures and training procedures — can hinder OOD detection.
In particular, our contributions are the following:
• We show that ﬂows learn latent representations for images largely based on local pixel correlations, rather than semantic content, making it difﬁcult to detect data with anomalous semantics.
• We identify mechanisms through which normalizing ﬂows can simultaneously increase likelihood for all structured images. For example, in Figure 1(b, c), we show that the coupling layers of RealNVP transform the in-distribution ImageNet in the same way as the
OOD CelebA.
• We show that by changing the architectural details of the coupling layers, we can encourage
ﬂows to learn transformations speciﬁc to the target data, improving OOD detection.
⇤Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Log-likelihoods (b) ImageNet input, in-distribution (c) CelebA input, OOD
Figure 1: RealNVP ﬂow on in- and out-of-distribution images. (a): A histogram of log-likelihoods that a RealNVP ﬂow trained on ImageNet assigns to ImageNet, SVHN and CelebA. The
ﬂow assigns higher likelihood to out-of-distribution data. (b, c): A visualization of the intermediate layers of a RealNVP model on an (b) in-distribution image and (c) OOD image. The ﬁrst row shows the coupling layer activations, the second and third rows show the scale s and shift t parameters pre-dicted by a neural network applied to the corresponding coupling layer input. Both on in-distribution and out-of-distribution images, s and t accurately approximate the structure of the input, even though the model has not observed inputs (images) similar to the OOD image during training. Flows learn generic image-to-latent-space transformations that leverage local pixel correlations and graphical details rather than the semantic content needed for OOD detection.
• We show that OOD detection is improved when ﬂows are trained on high-level features which contain semantic information extracted from image datasets.
We also provide code at https://github.com/PolinaKirichenko/flows_ood. 2