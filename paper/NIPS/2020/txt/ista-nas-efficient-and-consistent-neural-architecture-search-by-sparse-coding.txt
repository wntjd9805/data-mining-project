Abstract
Neural architecture search (NAS) aims to produce the optimal sparse solution from a high-dimensional space spanned by all candidate connections. Current gradient-based NAS methods commonly ignore the constraint of sparsity in the search phase, but project the optimized solution onto a sparse one by post-processing. As a result, the dense super-net for search is inefﬁcient to train and has a gap with the projected architecture for evaluation. In this paper, we formulate neural architecture search as a sparse coding problem. We perform the differentiable search on a compressed lower-dimensional space that has the same validation loss as the original sparse solution space, and recover an architecture by solving the sparse coding problem.
The differentiable search and architecture recovery are optimized in an alternate manner. By doing so, our network for search at each update satisﬁes the sparsity constraint and is efﬁcient to train. In order to also eliminate the depth and width gap between the network in search and the target-net in evaluation, we further propose a method to search and evaluate in one stage under the target-net settings.
When training ﬁnishes, architecture variables are absorbed into network weights.
Thus we get the searched architecture and optimized parameters in a single run.
In experiments, our two-stage method on CIFAR-10 requires only 0.05 GPU-day for search. Our one-stage method produces state-of-the-art performances on both
CIFAR-10 and ImageNet at the cost of only evaluation time1. 1

Introduction
Current NAS studies can be mainly categorized into reinforcement learning-based [1, 60, 58, 61, 4], evolution-based [41, 40, 29, 35, 14], Bayesian optimization-based [25, 59], and gradient-based methods [30, 48, 5, 53]. Most reinforcement learning and evolution-based algorithms suffer from huge computational cost, while gradient-based methods are simple to implement.
In gradient-based methods, an over-parameterized super-net is constructed to cover all candidate connections. Different architectures are sub-graphs of the super-net by weight sharing [39]. Thus the aim of search is to determine a sparse solution from the high-dimensional architecture space. Liu et al. propose a differentiable search framework, DARTS [30], by introducing a set of architecture variables jointly optimized with the network weights. After search, the architecture variables are projected to be sparse by only keeping the top-2 strongest connections for each intermediate node as the target-net for evaluation. This method is the basis of many follow-up studies [48, 5, 9, 49].
However, we note that the architecture variables do not satisfy the sparsity constraint during search.
There is a gap between the dense super-net in search and the sparse target-net in evaluation (the term
∗Corresponding author. 1Code address: https://github.com/iboing/ISTA-NAS. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
“evaluation” in this paper denotes re-training on the full train set and inference on the test set). From the perspective of optimization, solutions should be constrained in the feasible region at each iteration, as adopted in the Projected Gradient Descent (PGD) [36] and proximal algorithms [37]. Otherwise, the converged solution may be far from the optimal one in the feasible region. It explains the fact that there is a poor correlation between the performances of super-net for search and target-net for evaluation in DARTS [48, 7, 50]. A high-performance super-net does not necessarily produce a good architecture after projection onto the sparsity constraint. Besides, the dense super-net covering all candidate connections is inefﬁcient to train due to its huge computational and memory cost.
In some follow-up studies [48, 13, 47, 7], the Gumbel Softmax strategy [24, 33] is adopted to enforce sparsiﬁcation. Nevertheless, the super-net still contains the whole graph which is different from the derived architecture in target-net and does not reduce the computational and memory consumption.
In ProxylessNAS [5] and NASP [53], only sampled or projected connections are active for each edge to reduce the memory consumption and search cost. But the active paths of super-net still deviate from the target-net as there is no connection for some edges in the target-net. For single-path architecture search, DSNAS [21] proposes a one-stage method to eliminate the inconsistency between super-net and target-net brought by two-stage methods, while there is yet no single-stage solution to
DARTS-based architecture where the dimension of candidate connections is relatively higher.
Inspired by the observation that architecture variables in DARTS should have a sparse structure that can be well-represented by a compact space, in this paper, we propose to formulate NAS as a sparse coding problem, named ISTA-NAS. We construct an equivalent compressed search space where each point corresponds to a sparse solution in the original space. We perform gradient-based search in the compressed space with the sparsity constraint inherently satisﬁed, and then recover a new architecture by the sparse coding problem, which can be efﬁciently solved by well-developed methods, such as the iterative shrinkage thresholding algorithm (ISTA) [12]. The differentiable search and architecture recovery are conducted in an alternate way, so at each update, the network for search is sparse and efﬁcient to train. After convergence, there is no need of projection onto sparsity constraint by post-processing and the searched architecture is directly available for evaluation.
In previous studies [30, 48, 49], the super-net in search is dense, and thus has a smaller depth and width than the target-net setting due to limited memory. However, the number of cells and channels has a signiﬁcant effect on the search result [50, 9]. In order to also eliminate these gaps between super-net in search and target-net in evaluation, we develop a one-stage framework where search and evaluation share the same super-net under the target-net settings, such as depth, width and batchsize.
One-stage ISTA-NAS begins to search with all batch normalization (BN) [23] layers frozen. When a termination condition is satisﬁed, architecture variables cease to change and one sparse architecture is searched and ﬁxed. BN layers are now trainable and other network weights continue to be optimized.
After training, architecture variables are absorbed into the parameters of BN layers, and we get the searched architecture and all optimized parameters in a single run with only evaluation cost.
We list the contributions of this paper as follows:
• We formulate neural architecture search as a sparse coding problem and propose ISTA-NAS, which performs the differentiable search on an equivalent compressed space and recovers its corresponding sparse architecture in an alternate manner. The sparsity constraint is inherently satisﬁed at each update so the search is more efﬁcient and consistent with evaluation.
• We further develop a one-stage ISTA-NAS that incorporates search and evaluation in a single framework under the target-net settings. The network for search has no gap in terms of depth, width and even training batchsize with the derived architecture for evaluation.
• In experiments, the two-stage ISTA-NAS ﬁnishes search on CIFAR-10 in only 0.05 GPU-day.
And the one-stage version produces state-of-the-art performances in a single run on CIFAR-10 or directly on ImageNet at the cost of only evaluation time. 2