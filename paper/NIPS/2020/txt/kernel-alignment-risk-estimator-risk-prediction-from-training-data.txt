Abstract
We study the risk (i.e. generalization error) of Kernel Ridge Regression (KRR) for a kernel K with ridge λ > 0 and i.i.d. observations. For this, we introduce two objects: the Signal Capture Threshold (SCT) and the Kernel Alignment Risk
Estimator (KARE). The SCT ϑK,λ is a function of the data distribution: it can be used to identify the components of the data that the KRR predictor captures, and to approximate the (expected) KRR risk. This then leads to a KRR risk approximation by the KARE ρK,λ, an explicit function of the training data, agnostic of the true data distribution. We phrase the regression problem in a functional setting. The key results then follow from a ﬁnite-size analysis of the Stieltjes transform of general Wishart random matrices. Under a natural universality assumption (that the KRR moments depend asymptotically on the ﬁrst two moments of the obser-vations) we capture the mean and variance of the KRR predictor. We numerically investigate our ﬁndings on the Higgs and MNIST datasets for various classical kernels: the KARE gives an excellent approximation of the risk, thus supporting our universality assumption. Using the KARE, one can compare choices of Kernels and hyperparameters directly from the training set. The KARE thus provides a promising data-dependent procedure to select Kernels that generalize well. 1

Introduction
Kernel Ridge Regression (KRR) is a widely used statistical method to learn a function from its values on a training set [27, 29]. It is a non-parametric generalization of linear regression to inﬁnite-dimensional feature spaces. Given a positive-deﬁnite kernel function K and (noisy) observations y(cid:15) of a true function f ∗ at a list of points X = x1, . . . , xN , the λ-KRR estimator ˆf (cid:15)
λ of f ∗ is deﬁned by
ˆf (cid:15)
λ(x) = 1
N
K(x, X) (cid:18) 1
N
K(X, X) + λIN (cid:19)−1 y(cid:15), where K(x, X) = (K(x, xi))i=1,..,N ∈ RN and K(X, X) = (K(xi, xj))i,j=1,..,N ∈ RN ×N .
Despite decades of intense mathematical progress, the rigorous analysis of the generalization of kernel methods remains a very active and challenging area of research. In recent years, many new 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
kernels have been introduced for both regression and classiﬁcation tasks; notably, a large number of kernels have been discovered in the context of deep learning, in particular through the so-called
Scattering Transform [22], and in close connection with deep neural networks [7, 17], yielding ever-improving performance for various practical tasks [1, 10, 18, 28]. Currently, theoretical tools to select the relevant kernel for a given task, i.e. to minimize the generalization error, are however lacking.
While a number of bounds for the risk of Linear Ridge Regression (LRR) or KRR [6, 15, 31, 23] exist, most focus on the rate of convergence of the risk: these estimates typically involve constant factors which are difﬁcult to control in practice. Recently, a number of more precise estimates have been given [21, 9, 24, 20, 5]; however, these estimates typically require a priori knowledge of the data distribution. It remains a challenge to have estimates based on the training data alone, enabling one to make informed decisions on the choices of the ridge and of the kernel. 1.1 Contributions
We consider a generalization of the KRR predictor ˆf (cid:15) one tries to reconstruct a true
λ: function f ∗ in a space of continuous functions C from noisy observations y(cid:15) of the form (o1(f ∗) + (cid:15)e1, . . . , oN (f ∗) + (cid:15)eN ), where the observations oi are i.i.d. linear forms C → R sampled from a distribution π, (cid:15) is the level of noise, and the e1, . . . , eN are centered of unit variance. We work under the universality assumption that, for large N , only the ﬁrst two moments of π determine the behavior of the ﬁrst two moments of ˆf (cid:15)
λ. We obtain the following results: 1. We introduce the Signal Capture Threshold (SCT) ϑ(λ, N, K, π), which is determined by the ridge λ, the size of the training set N , the kernel K, and the observations distribution π (more precisely, the dependence on π is only through its ﬁrst two moments). We give approximations for the expectation and variance of the KRR predictor in terms of the SCT. 2. Decomposing f ∗ along the kernel principal components of the data distribution, we observe that in expectation, the predictor ˆf (cid:15)
λ captures only the signal along the principal components with eigenvalues larger than the SCT. If N increases or λ decreases, the SCT ϑ shrinks, allowing the predictor to capture more signal. At the same time, the variance of ˆf (cid:15)
λ scales with the derivative
∂λϑ, which grows as λ → 0, supporting the classical bias-variance tradeoff picture [14]. 3. We give an explicit approximation for the expected MSE risk R(cid:15)( ˆf (cid:15)
λ) and empirical MSE risk
λ) for an arbitrary continuous true function f ∗. We ﬁnd that, surprisingly, the expected risk
ˆR(cid:15)( ˆf (cid:15) and expected empirical risk are approximately related by
E[R(cid:15)( ˆf (cid:15)
λ)] ≈
ϑ(λ)2
λ2
E[ ˆR(cid:15)( ˆf (cid:15)
λ)]. 4. We introduce the Kernel Alignment Risk Estimator (KARE) as the ratio ρ deﬁned by
ρ(λ, N, y(cid:15), G) = 1
N (y(cid:15))T (cid:0) 1 (cid:104)(cid:0) 1 (cid:16) 1
N Tr
N G + λIN
N G + λIN (cid:1)−2 y(cid:15) (cid:1)−1(cid:105)(cid:17)2 , where G is the Gram matrix of K on the observations. We show that the KARE approximates the expected risk; unlike the SCT, it is agnostic of the true data distribution. This result follows
N G − zIN )−1(cid:3) is the Stieltjes from the fact that ϑ(λ) ≈ 1/mG(−λ), where mG(z) = Tr (cid:2)( 1
Transform of the Gram matrix. 5. Empirically, we ﬁnd that the KARE predicts the risk on the Higgs and MNIST datasets. We see empirically that our results extend extremely well beyond the Gaussian observation setting, thus supporting our universality assumption (see Figure 1).
Our proofs (see the Appendix) rely on a generalized and reﬁned version of the ﬁnite-size analysis of [16] of generalized Wishart matrices, obtaining sharper bounds and generalizing the results to operators. Our analysis relies in particular on the complex Stieltjes transform mG(z), evaluated at z = −λ, and on ﬁxed-point arguments. 2
KARE
Risk
Train err. 1 0.8 0.6 0.4 0.2 0
E
S
M
!
! y e h 1 0.8 0.6 0.4 0.2 0 1 0.8 0.6 0.4 0.2 0 1 0.8 0.6 0.4 0.2 0 10−810−710−610−510−410−310−210−1 100 ridge λ 10−2 10−1 1/d·lengthscale (cid:96) 100 10−8 10−7 10−6 10−5 10−4 10−3 10−2 10−1 100 ridge λ 10−2 10−1 100 1/d·lengthscale (cid:96) 101 (a) MNIST, (cid:96) = d (b) MNIST, λ = 10−5 (c) Higgs, (cid:96) = d (d) Higgs, λ = 10−4
Figure 1: Comparison between the KRR risk and the KARE for various choices of normalized lengthscale (cid:96)/d and ridge λ on the MNIST dataset (restricted to the digits 7 and 9, labeled by 1 and
−1 respectively, N = 2000) and on the Higgs dataset (classes ‘b’ and ‘s’, labeled by −1 and 1,
N = 1000) with the RBF Kernel K(x, x(cid:48)) = exp(−(cid:107)x−x(cid:48)(cid:107)2 2/(cid:96)) (see the Appendix for experiments with the Laplacian and (cid:96)1-norm kernels). KRR predictor risks, and KARE curves (shown as dashed lines, 5 samples) concentrate around their respective averages (solid lines). 1.2