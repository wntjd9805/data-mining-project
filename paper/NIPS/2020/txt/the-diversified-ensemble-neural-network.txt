Abstract
Ensemble is a general way of improving the accuracy and stability of learning models, especially for the generalization ability on small datasets. Compared with tree-based methods, relatively less works have been devoted to an in-depth study on effective ensemble design for neural networks. In this paper, we propose a principled ensemble technique by constructing the so-called diversiﬁed ensemble layer to combine multiple networks as individual modules. Through comprehensive theoretical analysis, we show that each individual model in our ensemble layer corresponds to weights in the ensemble layer optimized in different directions.
Meanwhile, the devised ensemble layer can be readily integrated into popular neural architectures, including CNNs, RNNs, and GCNs. Extensive experiments are conducted on public tabular datasets, images, and texts. By adopting weight sharing approach, the results show our method can notably improve the accuracy and stability of the original neural networks with ignorable extra time and space overhead. 1

Introduction
Deep neural networks (DNNs) have shown expressive representation power based on the cascading structure. However, their high model capacity also leads to the overﬁtting issue and making DNNs a less popular choice on small datasets, especially compared with decision tree-based methods.
In particular, ensemble has been a de facto engineering protocol for more stable prediction, by combining the outputs of multiple modules. In ensemble learning, it is desirable that the modules can be complementary to each other, and module diversity has been a direct pursuit for this purpose. In tree-based methods such as LightGBM [1] and XGBoost [2], diversity can be effectively achieved by different sampling and boosting techniques. However, such strategies are not so popular for neural networks, and the reasons may include: i) neural networks (and their ensemble) are less efﬁcient; ii) the down-sampling strategy may not work well on neural networks as each of them can be more prone to overﬁtting (e.g., by using only part of the training dataset), which affects the overall performance.
In contrast, decision tree models are known more robust to overﬁtting, and also more efﬁcient.
We are aimed to devise a neural network based ensemble model to be computationally efﬁcient and stable. In particular, the individual models is trained for maximizing their diversity such that the ensemble can be less prone to overﬁtting. To this end, we propose the so-called diversiﬁed ensemble layer, which can be used as a plug in with different popular network architectures, including CNNs [3],
RNNs [4], and GCNs [5]. Meanwhile, due to its partial weight sharing strategy, it incurs relatively small extra time overhead in both training and inference. The main contributions are as follows: 1) Instead of adopting existing popular down-sampling and feature selection strategies, we propose another principled technique, whereby each individual model can use full features and samples for
∗Junchi Yan is the correspondence author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The ensemble network with the proposed diversiﬁed ensemble layer (in red). The outputs of the front-end network, as can be embodied by architectures like CNN, RNN, GCN is fed into the
FC layer to extract features. Until this step, all the weights are shared across different modules in the ensemble layer. The modules are trained together with the other parts of the whole network. end-to-end learning. Thus, the individual models can be optimized in different directions for diversity, to enhance the generalization ability. We further provide theoretical analysis to show its effectiveness. 2) We propose a novel and adaptive learning procedure, which balances model diversity and training accuracy, so as to improve its generalization ability on testing data. Its efﬁciency is fulﬁlled by partial weight sharing across individual modules, which also plays a role in extracting common features for further extraction by the individual modules. 3) Extensive experimental results show that our ensemble layer can signiﬁcantly improve the accuracy by taking relatively low extra time and space, as shown in our extensive experimental results. Our ensemble layer can also be easily applied to CNNs, RNNs, GCNs, etc. 2