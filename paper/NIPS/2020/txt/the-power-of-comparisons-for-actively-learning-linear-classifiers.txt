Abstract
In the world of big data, large but costly to label datasets dominate many ﬁelds.
Active learning, a semi-supervised alternative to the standard PAC-learning model, was introduced to explore whether adaptive labeling could learn concepts with exponentially fewer labeled samples. While previous results show that active learn-ing performs no better than its supervised alternative for important concept classes such as linear separators, we show that by adding weak distributional assumptions and allowing comparison queries, active learning requires exponentially fewer samples. Further, we show that these results hold as well for a stronger model of learning called Reliable and Probably Useful (RPU) learning. In this model, our learner is not allowed to make mistakes, but may instead answer “I don’t know.”
While previous negative results showed this model to have intractably large sample complexity for label queries, we show that comparison queries make RPU-learning at worst logarithmically more expensive in both the passive and active regimes. 1

Introduction
In recent years, the availability of big data and the high cost of labeling has lead to a surge of interest in active learning, an adaptive, semi-supervised learning paradigm. In traditional active learning, given an instance space X, a distribution D on X, and a class of concepts c : X → {0, 1}, the learner receives unlabeled samples x from D with the ability to query an oracle for the labeling c(x). Classically our goal would be to minimize the number of samples the learner draws before approximately learning the concept class with high probability (PAC-learning). Instead, active learning assumes unlabeled samples are inexpensive, and rather aims to minimize expensive queries to the oracle. While active learning requires exponentially fewer labeled samples than PAC-learning for simple classes such as thresholds in one dimension, it fails to provide asymptotic improvement for classes essential to machine learning such as linear separators [1].
However, recent results point to the fact that with slight relaxations or additions to the paradigm, such concept classes can be learned with exponentially fewer queries. In 2013, Balcan and Long
[2] proved that this was the case for homogeneous (through the origin) linear separators, as long as the distribution over the instance space X = Rd was (nearly isotropic) log-concave–a wide range of distributions generalizing common cases such as gaussians or uniform distributions over convex sets. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Later, Balcan and Zhang [3] extended this to isotropic s-concave distributions, a diverse generalization of log-concavity including fat-tailed distributions. Similarly, El-Yaniv and Wiener [4] proved that non-homogeneous linear separators can be learned with exponentially fewer queries over gaussian distributions with respect to the accuracy parameter, but require exponentially more queries in the dimension of the instance space X, making their algorithm intractable in high dimensions.
Kane, Lovett, Moran, and Zhang (KLMZ) [5] broke the non-homogeneity barrier for general dis-tributions in two dimensions by empowering the oracle to compare points rather than just label them. Queries of this type are called comparisons, and are notable not only for their increase in computational power, but for their real world applications such as in recommender systems [6] or for increasing accuracy over purely label-based techniques [7]. Our work adopts a mixture of the approaches of Balcan, Long, and Zhang, and KLMZ. By leveraging comparison queries, we provide a computationally efﬁcient algorithm for active learning non-homogeneous linear separa-tors in exponentially fewer samples as long as the distribution satisﬁes weak concentration and anti-concentration bounds, conditions realized by, for instance, (not necessarily isotropic) s-concave distributions. Further, by introducing a new average case complexity measure, average inference dimension, that extends KLMZ’s techniques to the distribution dependent setting, we prove that comparisons provide signiﬁcantly stronger guarantees than the PAC-learning paradigm.
In the late 80’s, Rivest and Sloan [8] proposed a competing model to PAC-learning called Reliable and Probably Useful (RPU) learning. This model, a learning theoretic formalization of Chow’s [9] selective classiﬁcation, does not allow the learner to make mistakes, but instead allows the answer “I don’t know” written as “⊥”. Here, error is measured not by the amount of misclassiﬁed examples, but by the measure of examples on which our learner returns ⊥. RPU-learning was for the most part abandoned by the early 90’s in favor of PAC-learning as Kivinen [10–12] proved the sample complexity of RPU-learning simple concept classes such as rectangles required an exponential number of samples even under the uniform distribution. However, the model was recently re-introduced by
El-Yaniv and Wiener [4], who termed it perfect selective classiﬁcation. El-Yaniv and Wiener prove a connection between Active and RPU-learning similar to the strategy employed by KLMZ [5] (who refer to RPU-learners as “conﬁdent” learners). We extend the lower bound of El-Yaniv and Wiener to prove that actively RPU-learning linear separators with only labels is exponentially difﬁcult in dimension even for nice distributions. On the other hand, through a structural analysis of average inference dimension, we prove that comparison queries allow RPU-learning with nearly matching sample and query complexity to PAC-learning, as long as the underlying distribution is sufﬁciently nice. This last result has already found further use by Hopkins, Kane, Lovett, and Mahajan [13], who use our analysis of average inference dimension to extend their comparison-based algorithms for robustly learning non-homogeneous hyperplanes to higher dimensions. 2