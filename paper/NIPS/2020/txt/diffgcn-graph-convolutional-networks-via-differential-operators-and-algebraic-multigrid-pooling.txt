Abstract
Graph Convolutional Networks (GCNs) have shown to be effective in handling unordered data like point clouds and meshes. In this work we propose novel approaches for graph convolution, pooling and unpooling, inspired from ﬁnite differences and algebraic multigrid frameworks. We form a parameterized convolu-tion kernel based on discretized differential operators, leveraging the graph mass, gradient and Laplacian. This way, the parameterization does not depend on the graph structure, only on the meaning of the network convolutions as differential operators. To allow hierarchical representations of the input, we propose pooling and unpooling operations that are based on algebraic multigrid methods, which are mainly used to solve partial differential equations on unstructured grids. To motivate and explain our method, we compare it to standard convolutional neural networks, and show their similarities and relations in the case of a regular grid. Our proposed method is demonstrated in various experiments like classiﬁcation and part-segmentation, achieving on par or better than state of the art results. We also analyze the computational cost of our method compared to other GCNs. 1

Introduction
The emergence of deep learning and Convolutional Neural Networks (CNNs) [1, 2, 3] in recent years has had great impact on the community of computer vision and graphics [4, 5, 6, 7]. Over the past years, multiple works used standard CNNs to perform 3D related tasks on unordered data (e.g., point clouds and meshes), one of which is PointNet [8, 9], that operates directly on point clouds. Along with these works, another massively growing ﬁeld is Graph Convolutional Networks (GCNs) [10], or Geometric Deep Learning, which suggests using graph convolutions for tasks related to three dimensional inputs, arising from either spectral theory [11, 12, 13] or spatial convolution
[14, 15, 16, 17]. This makes the processing of unstructured data like point clouds, graphs and meshes more natural by operating directly in the underlying structure of the data.
In this work we aim to bridge the gap between ordered and unordered deep learning architectures, and to build on the foundation of standard CNNs in unordered data. To this end, we leverage the similarity between standard CNNs and partial differential equations (PDEs) [18], and propose a new approach to deﬁne convolution operators on graphs that are based on discretization of differential operators on unstructured grids. Speciﬁcally, we deﬁne a 3D convolution kernel which is based on discretized differential operators. We consider the mass (self-feature), gradient and Laplacian of the graph, and discretize them using a simple version of ﬁnite differences, similarly to the way that standard graph
Laplacians are deﬁned. Such differential operators form a subspace which spans standard convolution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
kernels on structured grids. Leveraging such operators for unstructured grids leads to an abstract parameterization of the convolution operation, which is independent of the speciﬁc graph geometry.
Our second contribution involves unstructured pooling and unpooling operators, which together with the convolution, are among the main building blocks of CNNs. To this end, and further motivated by the PDE interpretation of CNNs, we utilize multigrid methods which are among the most efﬁcient numerical solvers for PDEs. Such methods use a hierarchy of smaller and smaller grids to represent the PDE on various scales. Speciﬁcally, algebraic multigrid (AMG) approaches [19, 20] are mostly used to solve PDEs on unstructured grids by forming the same hierarchy of problems using coarsening and upsampling operators. Using these building blocks of AMG, we propose novel pooling and unpooling operations for GCNs. Our operators are based on the Galerkin coarsening operator of aggregation-based AMG [21, 22], performing pure aggregation for pooling and smoothed aggregation as the unpooling operator. The advantage of having pooling capability, as seen both in traditional
CNNs and GCNs [4, 23, 5, 6] are the enlargement of the receptive ﬁeld of the neurons, and reduced computational cost (in terms of ﬂoating operations), allowing for wider and deeper networks.
In what follows, we elaborate on existing unordered data methods in Section 2, and present our method in Section 3. We discuss the similarity between traditional CNNs and our proposed GCN, and motivate the use of differential operators as a parameterization to a convolution kernel in Section 3.3.
Furthermore, we compare the computational cost of our method compared to other message-passing, spatially based GCNs in Section 3.5 . To validate our model, we perform experiments on point cloud classiﬁcation and segmentation tasks on various datasets in Section 4. Finally, we study the importance and contribution of the different terms in the parameterization to the performance of our method in Section 4.3. 2