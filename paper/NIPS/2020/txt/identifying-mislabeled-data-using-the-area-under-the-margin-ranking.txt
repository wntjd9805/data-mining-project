Abstract
Not all data in a typical training set help with generalization; some samples can be overly ambiguous or outrightly mislabeled. This paper introduces a new method to identify such samples and mitigate their impact when training neural networks. At the heart of our algorithm is the Area Under the Margin (AUM) statistic, which exploits differences in the training dynamics of clean and mislabeled samples. A simple procedure—adding an extra class populated with purposefully mislabeled threshold samples—learns a AUM upper bound that isolates mislabeled data.
This approach consistently improves upon prior work on synthetic and real-world datasets. On the WebVision50 classiﬁcation task our method removes 17% of training data, yielding a 1.6% (absolute) improvement in test error. On CIFAR100 removing 13% of the data leads to a 1.2% drop in error. 1

Introduction
As deep networks become increasingly powerful, the potential improvement of novel architectures in many applications is inherently limited by data quality. In many real-world settings, datasets may contain samples that are “weakly-labeled” through proxy variables or web scraping [e.g. 28, 35, 60].
Human annotators, especially on crowdsourced platforms, can also be prone to making labeling mistakes. Even the most celebrated and highly-curated datasets, like MNIST [31] and ImageNet [13], famously contain harmful examples. See Fig. 1 for suspicious examples detected by our proposed method—some are clearly mislabeled, others inherently ambiguous. Mislabeled training data are problematic for overparameterized deep networks, which can achieve zero training error even on randomly-assigned labels [65]. If a BIRD is mislabeled as a DOG, a model will learn overly speciﬁc
ﬁlters—only applicable for this one image—which will result in overﬁtting and worse performance.
Our goal is to automatically identify and subsequently remove mislabeled samples from training datasets. Discarding these harmful data will reduce memorization and improve generalization.
Perhaps more importantly, identifying mislabeled data allows practitioners to easily audit and curate their datasets. For example, a company might like to know about common labeling mistakes in order to reduce systematic error in their annotation pipeline. Large datasets may be too costly to manually inspect; therefore, an automated method should isolate mislabeled data with high precision and recall.
Prior works have investigated multi-stage pipelines [e.g. 12, 20] or robust loss functions [e.g. 61, 67] for mislabeled sample identiﬁcation. We instead wish to create a method that is fully “plug-and-play” with existing training methods for maximum compatibility and minimal implementation overhead.
†Work done while at ASAPP. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Images from MNIST (left) and ImageNet (right) with lowest Area Under the Margin
Figure 1: (AUM) ranking (most likely to be mislabeled). AUMs are computed with LeNet/ResNet-50 models.
To this end, we propose a novel method that identiﬁes mislabeled data simply by observing a network’s training dynamics. Our method builds upon recent theoretical and empirical works [2, 5, 6, 33, 44] that suggest that dynamics of SGD contain salient signals about noisy data and generalization.
Consider an image of a BIRD accidentally mislabeled as a DOG. Its memorization is the outcome of a delicate tension. During training, the gradient updates from the image itself encourage the network to (wrongly) predict the DOG label, whereas gradient updates from other training images encourage predicting BIRD through generalization. The opposing updates between the (incorrect) assigned label and the (hidden) true class membership are ultimately reﬂected in the logits during training.
To capture this phenomenon, we introduce the Area Under the Margin (AUM) statistic, which measures the average difference between the logit values for a sample’s assigned class and its highest non-assigned class. Correctly-labeled data, which generalize from similarly-labeled examples, do not exhibit this tension and thus have a larger AUM than mislabeled data. To separate mislabeled samples from difﬁcult but beneﬁcial samples, we make a second contribution. We introduce an extra (artiﬁcial) class and purposefully assign a small percentage of threshold training data to this new class. All samples assigned to this new class are by deﬁnition mislabeled; therefore, we can use the
AUM statistics of these points as a threshold to separate correctly-labeled data from mislabeled data.
The AUM statistic and threshold samples are trivially compatible with any classiﬁcation network.2
Implementing this method simply requires logging the model’s logits during training. Training data whose AUM falls below the threshold can be conﬁdently removed from the training set. On standard benchmark tasks, we improve upon the performance of existing methods simply by removing identi-ﬁed mislabeled samples. We are also able to clean many real-world datasets—including WebVision and Tiny ImageNet—for improved classiﬁcation performance. Most surprisingly, removing 13% of the CIFAR100 dataset results in a 1.2% reduction in test-error for a ResNet-32 model. 2