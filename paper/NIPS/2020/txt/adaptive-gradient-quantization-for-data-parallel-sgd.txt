Abstract
Many communication-efﬁcient variants of SGD use gradient quantization schemes.
These schemes are often heuristic and ﬁxed over the course of training. We empirically observe that the statistics of gradients of deep models change during the training. Motivated by this observation, we introduce two adaptive quantization schemes, ALQ and AMQ. In both schemes, processors update their compression schemes in parallel by efﬁciently computing sufﬁcient statistics of a parametric distribution. We improve the validation accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost communication setups. Our adaptive methods are also signiﬁcantly more robust to the choice of hyperparameters. 1

Introduction
Stochastic gradient descent (SGD) and its variants are cur-rently the method of choice for training deep models. Yet, large datasets cannot always be trained on a single com-putational node due to memory and scalability limitations.
Data-parallel SGD is a remarkably scalable variant, in par-ticular on multi-GPU systems [1–10]. However, despite its many advantages, distribution introduces new challenges for optimization algorithms. In particular, data-parallel
SGD has large communication cost due to the need to transmit potentially huge gradient vectors. Ideally, we want distributed optimization methods that match the per-formance of SGD on a single hypothetical super machine, while paying a negligible communication cost.
A common approach to reducing the communication cost in data-parallel SGD is gradient compression and quan-tization [4, 11–16]. In full-precision data-parallel SGD, each processor broadcasts its locally computed stochastic gradient vector at every iteration, whereas in quantized data-parallel SGD, each processor compresses its stochas-tic gradient before broadcasting. Current quantization methods are either designed heuristically or ﬁxed prior to training. Convergence rates in a stochastic optimization problem are controlled by the trace of the gradient covariance matrix, which is referred
Figure 1: Changes in the average variance of normalized gradient coordinates in a ResNet-32 model trained on CIFAR-10. Colors dis-tinguish different runs with different seeds.
Learning rate is decayed by a factor of 10 twice at 40K and 60K iterations. The vari-ance changes rapidly during the ﬁrst epoch.
The next noticeable change happens after the
ﬁrst learning rate drop and another one ap-pears after the second drop.
⇤Equal contributions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
as the gradient variance in this paper [17]. As Fig. 1 shows, no ﬁxed method can be optimal through-out the entire training because the distribution of gradients changes. A quantization method that is optimal at the ﬁrst iteration will not be optimal after only a single epoch.
In this paper, we propose two adaptive methods for quantizing the gradients in data-parallel SGD.
We study methods that are deﬁned by a norm and a set of quantization levels. In Adaptive Level
Quantization (ALQ), we minimize the excess variance of quantization given an estimate of the distribution of the gradients. In Adaptive Multiplier Quantization (AMQ), we minimize the same objective as ALQ by modelling quantization levels as exponentially spaced levels. AMQ solves for the optimal value of a single multiplier parametrizing the exponentially spaced levels. 1.1 Summary of contributions
• We propose two adaptive gradient quantization methods, ALQ and AMQ, in which processors update their compression methods in parallel.
• We establish an upper bound on the excess variance for any arbitrary sequence of quantization levels under general normalization that is tight in dimension, an upper bound on the expected number of communication bits per iteration, and strong convergence guarantees on a number of problems under standard assumptions. Our bounds hold for any adaptive method, including ALQ and AMQ.
• We improve the validation accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost communication setups. Our adaptive methods are signiﬁcantly more robust to the choice of hyperparameters.2 1.2