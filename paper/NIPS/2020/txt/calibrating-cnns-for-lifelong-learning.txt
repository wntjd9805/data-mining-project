Abstract
We present an approach for lifelong/continual learning of convolutional neural networks (CNN) that does not suffer from the problem of catastrophic forget-ting when moving from one task to the other. We show that the activation maps generated by the CNN trained on the old task can be calibrated using very few calibration parameters, to become relevant to the new task. Based on this, we calibrate the activation maps produced by each network layer using spatial and channel-wise calibration modules and train only these calibration parameters for each new task in order to perform lifelong learning. Our calibration modules intro-duce signiﬁcantly less computation and parameters as compared to the approaches that dynamically expand the network. Our approach is immune to catastrophic forgetting since we store the task-adaptive calibration parameters, which contain all the task-speciﬁc knowledge and is exclusive to each task. Further, our approach does not require storing data samples from the old tasks, which is done by many replay based methods. We perform extensive experiments on multiple benchmark datasets (SVHN, CIFAR, ImageNet, and MS-Celeb), all of which show substan-tial improvements over state-of-the-art methods (e.g., a 29% absolute increase in accuracy on CIFAR-100 with 10 classes at a time). On large-scale datasets, our approach yields 23.8% and 9.7% absolute increase in accuracy on ImageNet-100 and MS-Celeb-10K datasets, respectively, by employing very few (0.51% and 0.35% of model parameters) task-adaptive calibration parameters. 1

Introduction
Humans are adept at continual/lifelong learning of multiple tasks in a sequence, while not forgetting the knowledge acquired from earlier tasks when subjected to new learning tasks. Unfortunately, deep neural networks do not have such an inherent property. It has been well-recognized that deep neural networks suffer from the problem of catastrophic forgetting [1], e.g., in a categorization task, the network performance on the previously trained categories tends to fall drastically as if the network has “forgotten” those categories of data.
Continual learning [2, 3] involves training the network in such a way that it can continually learn from tasks that arrive sequentially and add to its existing knowledge base instead of replacing knowledge about the older tasks. A trivial solution to this is to simply store all the data from all the tasks as and when they arrive so that whenever a new task arrives, we can train the network on all the previous task data and the current task data. However, deep learning is used in many diverse applications, where storing such a large amount of data is not feasible. Therefore, an incrementally trained model must be able to learn from new tasks that arrive sequentially by only training on that task and still retain the knowledge gained from the previous task without having to re-train on all the previously
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
seen data. Finally, we should obtain a model that performs well for all the sequentially added tasks.
Solving this problem will make deep learning models much more human-like.
In addition to preserving the knowledge of the old tasks, lifelong learning models must also leverage this knowledge to help in learning new tasks. This is referred to as forward transfer. When training on a new task, if the network drastically forgets knowledge from the older tasks, then it is said to be a plastic network. On the other hand, if a network gives so much importance to the older tasks that it is unable to properly learn the new task, then it is said to be a stable network. Too much stability or plasticity will be harmful to this problem. Therefore, for lifelong learning (which we will occasionally interchangeably refer to as continual/incremental learning in the rest of the exposition) to be successful, a stability-plasticity balance must be maintained.
The following objectives are important for any lifelong learning algorithm: 1) the network should exhibit zero or near-zero catastrophic forgetting; 2) in the attempt to achieve this, the method performance should not fall signiﬁcantly on the continual learning task; 3) the number of parameters and computations in the network should not increase signiﬁcantly; and 4) the network should be able to perform forward knowledge transfer from old tasks to the new ones. Even when one achieves zero catastrophic forgetting, this does not trivially lead to good performance. For example, some existing methods such as Piggyback [4] and others [5, 6] do not suffer from catastrophic forgetting but suffer from performance degeneration and hence fail as per the second objective. On the other hand, some methods [7, 5, 8] increase the network size signiﬁcantly and hence fail the third objective. To the best of our knowledge, none of the existing methods satisfy all of the above objectives.
Transfer learning is a technique for transferring knowledge from one dataset/domain/network to another [9]. It enables utilizing the knowledge gained from training the network on the source dataset to help in training the network better on the target dataset and also help converge faster. The intuition behind transfer learning is that the initial layers of the network typically learn to extract basic features common to both source and target datasets, such as edges and corners of an object (in image-based data). However, based on this idea, if we directly train the network on a new task, it will override the older parameter weights that had been learned for the older task, resulting in catastrophic forgetting.
In order to prevent this, we can freeze the entire network that has been trained on the older task and only train extra network layers for each new task. This will eliminate catastrophic forgetting while promoting knowledge transfer from the initial task to the later task using the frozen network. However, adding extra network layers for each new task is not a scalable solution when the number of tasks is large. A more desirable approach would be to learn a small number of (re)calibration parameters to modify the intermediate activations produced by the frozen network to make them relevant to the new task. This will have no catastrophic forgetting and also only cause a very insigniﬁcant increase in network size per task.
We propose a novel method to address the lifelong learning problem in convolutional neural networks (CNNs) based on the above idea, aimed at accomplishing all of the aforementioned objectives. We focus on maximum re-use of features generated by the CNN trained on the ﬁrst task to obtain features for images in the subsequent tasks. We achieve this by performing spatial and channel-wise calibration of the intermediate activation-maps according to the task that is currently being learned.
Speciﬁcally, our method involves training a CNN-based base module for the ﬁrst task and training a set of calibration parameters for every intermediate activation map generated by the base module for all subsequent tasks. Except for the ﬁrst task, task-adaptive calibration parameters are the only trainable parameters. Moreover, the calibration parameters of the previous task also serve as good initial weights for learning the calibration parameters of the new task. Since our approach re-uses the
CNN trained on the old task for the new tasks, forward knowledge transfer is achieved. During testing, when the task is changed, we simply change the calibration parameters. This ensures no catastrophic forgetting for the previous tasks and an insigniﬁcant increase of parameters and computation per task compared to other dynamic network-based continual learning methods [5, 10, 11, 12, 13, 14]. Our proposed method is described in detail in Sec 2.2. We perform extensive experiments on several benchmark datasets and perform various ablation experiments to validate our approach.
To summarize, our major contributions are as follows:
• We propose a novel method of lifelong learning for convolutional neural networks, which involves (re)calibrating the activation maps generated by the network trained on older tasks to produce features relevant to the newer tasks. 2
Figure 1: Calibration module (CM) containing the spatial calibration module (SCM) and channel-wise calibration module (CCM) that are applied sequentially to the activation maps. Here ⊕ and ⊗ represent element-wise addition and channel-wise multiplication operation respectively.
• We empirically show that our method introduces a very small number of parameters com-pared to other dynamic network-based lifelong learning methods.
• We experimentally show that our method performs signiﬁcantly better than existing, state-of-the-art lifelong learning methods. 2 Proposed Method 2.1 Problem Setting
We consider the task incremental classiﬁcation setting [7], where new tasks with new sets of classes are sequentially provided to the network. Let the total number of tasks be K, each having U new classes. The objective is to train a network in this setting such that the ﬁnal network performs well on the new tasks as well as on the old tasks without any performance loss. 2.2 Method Overview
As mentioned earlier, we propose a lifelong learning approach for convolutional neural networks called Calibrating CNNs for Lifelong Learning (CCLL). It is designed to effectively re-use the features learned by the network, trained on the initial task, and efﬁciently (re)calibrate them, using a very small number of calibration parameters, in order to make them relevant to the new tasks.
Our method requires task-labels during test time in order to identify which task-adaptive calibration parameters to use for re-calibrating the convolutional layer outputs.
Our network consists of three types of modules: base module E, task-adaptive calibration modules
CM t i , and task-speciﬁc classiﬁcation modules C t. The base module is a convolutional neural network with N layers (L1 to LN ) and parameters θE. Each layer i ∈ [1, N ] produces an output activation map M t i , where t refers to the task t. We add a calibration module (CM) after each layer of the i is added after the ith layer of the base module for task t. base module. The calibration module CM t
Each calibration module consists of a spatial calibration module (SCM) followed by a channel-wise calibration module (CCM), as shown in Fig. 1. The spatial calibration module learns weights to calibrate each point in the activation maps while the channel-wise calibration module learns weights to calibrate each channel of the activation maps. The output of the ith layer of the base module is fed to the ith calibration module, which feeds its output to the (i + 1)th layer of the base module.
Assume M t i to be of size H × W × C, where H, W , and C denotes height, width, and number of channels, respectively. Let Φt i be the SCM operator added after the ith layer of the base module for task t. The spatial calibration module uses group convolution with 3 × 3 kernel size, the number of groups equal to C
α with each group having α channels. The output of Φt i will also be of size
H × W × C, representing the spatial calibration weights. Therefore, the SCM operator can be described as the function Φt i : RH×W ×C −→ RH×W ×C
Φt i(M t i ) = GCONVα(M t i ) (1) where GCONVα represents group convolution with the number of groups equal to C
α . 3
Figure 2: Our proposed architecture for lifelong learning. The top architecture is for the ﬁrst task t = 1 and bottom architecture is for all the subsequent tasks t > 1. L1 − LN represent the layers of i to produce M t∗∗ the base module. The calibration module CM t that is given as an input to the i + 1th layer. After the ﬁrst task, for all the subsequent tasks, the base module is frozen and its layers are not trainable and are marked in gray color with hatched pattern. i calibrates the ith layer output M t i
The calibration weights will get added element-wise to M t maps M t∗
. The spatially calibrated activation maps M t∗ i calibration module. M t∗ i is obtained as i to give the spatially calibrated activation i are given as input to the channel-wise
M t∗ i = Φt i(M t i ) ⊕ M t i (2) where ⊕ represents the element-wise addition operation.
Let ξt i be the CCM operator added after the SCM operator for the ith layer of the base module for task t as shown in Fig. 1. The channel-wise calibration module ﬁrst performs global average pooling (GAP) on M t∗
. This produces an output of size 1 × 1 × C. The channel-wise calibration module i performs group convolution with kernel size 1 × 1, the number of groups equal to C
β with each group having β channels, on the output of the global average pooling operation. This is followed by a sigmoid activation function that again produces an output size of 1 × 1 × C which represents the channel-wise calibration weights. Therefore, the CCM operator can be described as the function i : RH×W ×C −→ [0, 1]1×1×C
ξt i (M t∗
ξt i ) = σ(BN (GCONVβ(GAP (M t∗ i )))) (3) where GCONVβ represents group convolution with the number of groups equal to C batch normalization, and σ represents the sigmoid activation function.
Each of the calibration weights gets multiplied to the corresponding channel of M t∗ i
ﬁnal calibrated activation maps M t∗∗ for the ith layer. M t∗∗ can be obtained as i i
β , BN represents to produce the (4) i ) ⊗ M t∗ i where ⊗ represents the channel-wise multiplication operation. i = ξt i (M t∗
M t∗∗
M t∗∗ i (M t i ) = ξt where CM t i = CM t
Therefore, we can describe the overall calibration process as a combination of Eqs. 2 and 4 (Fig. 1) i(M t i is the task t calibration module added after the ith layer of the base module.
For the ﬁrst task, we train the base module, the calibration modules, and the classiﬁcation module.
For the subsequent tasks t > 1, we keep the base module weights θE as frozen and only train the task-adaptive calibration modules CM t i for all i ∈ [1, N ], and the task-speciﬁc classiﬁcation module
C t. In this way, we adapt features relevant to the new task from the base module using the calibration i ) ⊕ M t i ) i ) ⊗ (Φt i ) ⊕ M t i(M t i (Φt (5) 4
modules. We train the network only on the classiﬁcation loss using cross-entropy loss, and we do not use the distillation loss or task exemplar replay/rehearsal. The full architecture is shown in Fig. 2.
Through an ablation experiment, we show that if we do not train the base module at all and only train the calibration parameters for each task, the network performance is hurt drastically. Therefore, in our method, transfer of knowledge occurs from the ﬁrst task to the later tasks using the base module (that is only trained on the ﬁrst task). Through another ablation experiment, we show that if we use the calibration parameter weights of the previous task as the initial weights for training the calibration parameters for the next task, we get better results than training the task adaptive calibration parameters from scratch for each task. This also shows that forward transfer of knowledge is happening from previous tasks to the next task.
The task-adaptive calibration parameters are stored. During testing, depending on the task-label, the corresponding task-adaptive calibration parameters are used, and classiﬁcation is performed. Since our calibration module is light-weight, the number of extra parameters introduced per task and the increase in the total number of computations are not signiﬁcant. Therefore, our proposed method is a very efﬁcient lifelong learning method with no catastrophic forgetting. 3