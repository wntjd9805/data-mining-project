Abstract
Model-based reinforcement learning (RL), which ﬁnds an optimal policy using an empirical model, has long been recognized as one of the cornerstones of RL.
It is especially suitable for multi-agent RL (MARL), as it naturally decouples the learning and the planning phases, and avoids the non-stationarity problem when all agents are improving their policies simultaneously using samples. Though intuitive and widely-used, the sample complexity of model-based MARL algorithms has been investigated relatively much less often. In this paper, we aim to address the fundamental open question about the sample complexity of model-based MARL.
We study arguably the most basic MARL setting: two-player discounted zero-sum
Markov games, given only access to a generative model of state transition. We show that model-based MARL achieves a sample complexity of (cid:101)O(|S||A||B|(1 −
γ)−3(cid:15)−2) for ﬁnding the Nash equilibrium (NE) value up to some (cid:15) error, and the (cid:15)-NE policies, where γ is the discount factor, and S, A, B denote the state space, and the action spaces for the two agents. We also show that this method is near-minimax optimal with a tight dependence on 1 − γ and |S| by providing a lower bound of Ω(|S|(|A| + |B|)(1 − γ)−3(cid:15)−2). Our results justify the efﬁciency of this simple model-based approach in the multi-agent RL setting. 1

Introduction
Recent years have witnessed phenomenal successes of reinforcement learning (RL) in many appli-cations, e.g., playing strategy games [1, 2], playing the game of Go [3, 4], autonomous driving [5], and security [6, 7]. Most of these successful while practical applications involve more than one decision-maker, giving birth to the surging interests and efforts in studying multi-agent RL (MARL) recently, especially on the theoretical side [8, 9, 10, 11, 12, 13]. See also comprehensive surveys on
MARL in [14, 15, 16].
In general MARL, all agents affect both the state transition and the rewards of each other, while each agent may possess different, sometimes even totally conﬂicting objectives. Without knowledge of the model, the agents have to resort to data to either estimate the model, improve their own policy, and/or infer other agents’ policies. One fundamental challenge in MARL is the emergence of non-stationarity during the learning process [14, 15]: when multiple agents improve their policies concurrently and directly using samples, the environment becomes non-stationary from each agent’s 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
perspective. This has posed great challenge to development of effective MARL algorithms based on single-agent ones, especially model-free ones, as the condition for guaranteeing convergence in the latter fails to hold in MARL, and additional non-trivial efforts are required to address it [17, 18, 19].
One tempting remedy is the simple while intuitive method – model-based MARL: one ﬁrst estimates an empirical model using data, and then ﬁnds the optimal, more speciﬁcally, equilibrium policies in this empirical model, via planning. Model-based MARL naturally decouples the learning and planning phases, and can be incorporated with any black-box planning algorithm that is efﬁcient, e.g., value iteration [20] and (generalized) policy iteration [21, 22].
Though intuitive and widely-used, rigorous theoretical justiﬁcations for these model-based MARL methods are relatively rare. In this work, we aim to answer the following standing question: how good is the performance of this simple “plug-in” method in terms of non-asymptotic sample complexity?
To this end, we focus on arguably the most basic setting of MARL, well recognized ever since [17]: two-player discounted zero-sum Markov games (MGs) with simultaneous-move agents, given only access to a generative model of state transition. This generative model allows agents to sample the MG, and query the next state from the transition process, given any state-action pair as input.
The generative model setting has been a benchmark in RL when studying the sample efﬁciency of algorithms [23, 24, 25, 26, 27]. Indeed, this model allows the study of sample-based multi-agent planning over a long horizon, and helps develop better understanding of the statistical properties of the algorithms, decoupled from the exploration complexity.
Motivated by recent minimax optimal complexity results for single-agent model-based RL [23], we address the question above with a positive answer: model-based MARL approach can achieve near-minimax optimal sample complexity in ﬁnding both the Nash equilibrium (NE) value and policies. See a detailed description as follows. Our results have justiﬁed the efﬁciency of this simple model-based approach to MARL.
Contribution. We establish the sample complexities of model-based MARL in zero-sum Markov games, when a generative model is available. We show that the approximate solution to the empirical model can achieve the NE value of the true model up to some (cid:15)-error with (cid:101)O(|S||A||B|(1 − γ)−3(cid:15)−2) samples, where (cid:101)O suppresses the logarithmic factors, γ is the discount factor, and S, A, B denote the state space, and the action spaces for the two agents, respectively. Establishing a lower bound of
Ω(|S|(|A| + |B|)(1 − γ)−3(cid:15)−2), we show that this simple method is indeed near-minimax optimal, with a tight dependence on 1 − γ and |S|, and a sublinear dependence on model-size. This result then induces a (cid:101)O(|S||A||B|(1 − γ)−5(cid:15)−2) sample complexity for achieving (cid:15)-NE policies. Moreover, we provide a planning oracle that is smooth in producing the approximate NE policies, and show that obtaining (cid:15)-NE policies can also achieve the near-optimal complexity of (cid:101)O(|S||A||B|(1 − γ)−3(cid:15)−2).
These near-optimal results are ﬁrst-of-their-kind in model-based MARL, to the best of our knowledge.