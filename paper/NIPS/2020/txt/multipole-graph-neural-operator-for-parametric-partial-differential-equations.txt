Abstract
One of the main challenges in using deep learning-based methods for simulating physical systems and solving partial differential equations (PDEs) is formulating physics-based data in the desired structure for neural networks. Graph neural networks (GNNs) have gained popularity in this area since graphs offer a natural way of modeling particle interactions and provide a clear way of discretizing the continuum models. However, the graphs constructed for approximating such tasks usually ignore long-range interactions due to unfavorable scaling of the compu-tational complexity with respect to the number of nodes. The errors due to these approximations scale with the discretization of the system, thereby not allowing for generalization under mesh-reﬁnement. Inspired by the classical multipole methods, we propose a novel multi-level graph neural network framework that captures interaction at all ranges with only linear complexity. Our multi-level formulation is equivalent to recursively adding inducing points to the kernel matrix, unifying
GNNs with multi-resolution matrix factorization of the kernel. Experiments con-ﬁrm our multi-graph network learns discretization-invariant solution operators to
PDEs and can be evaluated in linear time. 1

Introduction
A wide class of important scientiﬁc applications involve numerical approximation of parametric PDEs.
There has been immense research efforts in formulating and solving the governing PDEs for a variety of physical and biological phenomena ranging from the quantum to the cosmic scale. While this endeavor has been successful in producing solutions to real-life problems, major challenges remain.
Solving complex PDE systems such as those arising in climate modeling, turbulent ﬂow of ﬂuids, and plastic deformation of solid materials requires considerable time, computational resources, and domain expertise. Producing accurate, efﬁcient, and automated data-driven approximation schemes has the potential to signiﬁcantly accelerate the rate of innovation in these ﬁelds. Machine learning based methods enable this since they are much faster to evaluate and require only observational data to train, in stark contrast to traditional Galerkin methods [50] and classical reduced order models[40]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
While deep learning approaches such as convolutional neural networks can be fast and powerful, they are usually restricted to a speciﬁc format or discretization. On the other hand, many problems can be naturally formulated on graphs. An emerging class of neural network architectures designed to operate on graph-structured data, Graph neural networks (GNNs), have gained popularity in this area.
GNNs have seen numerous applications on tasks in imaging, natural language modeling, and the simulation of physical systems [47, 34, 31]. In the latter case, graphs are typically used to model particles systems (the nodes) and the their interactions (the edges). Recently, GNNs have been directly used to learn solutions to PDEs by constructing graphs on the physical domain [4], and it is was further shown that GNNs can learn mesh-invariant solution operators [32]. Since GNNs offer great
ﬂexibility in accurately representing solutions on any unstructured mesh, ﬁnding efﬁcient algorithms is an important open problem.
The computational complexity of GNNs depends on the sparsity structure of the underlying graph, scaling with the number of edges which may grow quadratically with the number of nodes in fully connected regions [47]. Therefore, to make computations feasible, GNNs make approximations using nearest neighbor connection graphs which ignore long-range correlations. Such approximations are not suitable in the context of approximating solution operators of parametric PDEs since they will not generalize under reﬁnement of the discretization, as we demonstrate in Section 4. However, using fully connected graphs quickly becomes computationally infeasible. Indeed evaluation of the kernel matrices outlined in Section 2.1 is only possible for coarse discretizations due to both memory and computational constraints. Throughout this work, we aim to develop approximation techniques that help alleviate this issue.
To efﬁciently capture long-range interaction, multi-scale methods such as the classical fast multipole methods (FMM) have been developed [22]. Based on the insight that long-range interaction are smooth, FMM decomposes the kernel matrix into different ranges and hierarchically imposes low-rank structures to the long-range components (hierarchical matrices)[11]. This decomposition can be viewed as a speciﬁc form of the multi-resolution matrix factorization of the kernel [29, 11]. However, the classical FMM requires nested grids as well as the explicit form of the PDEs. We generalize this idea to arbitrary graphs in the data-driven setting, so that the corresponding graph neural networks can learn discretization-invariant solution operators.
Main contributions.
Inspired by the fast multipole method (FMM), we propose a novel hierarchi-cal, and multi-scale graph structure which, when deployed with GNNs, captures global properties of the PDE solution operator with a linear time-complexity [22, 48]. As shown in Figure 1, starting with a nearest neighbor graph, instead of directly adding edges to connect every pair of nodes, we add inducing points which help facilitate long-range communication. The inducing points may be thought of as forming a new subgraph which models long-range correlations. By adding a small amount of inducing nodes to the original graph, we make computation more efﬁcient. Repeating this process yields a hierarchy of new subgraphs, modeling correlations at different length scales.
We show that message passing through the inducing points is equivalent to imposing a low-rank structure on the corresponding kernel matrix, and recursively adding inducing points leads to multi-resolution matrix factorization of the kernel [29, 11]. We propose the graph V-cycle algorithm (ﬁgure 1) inspired by FMM, so that message passing through the V-cycle directly computes the multi-resolution matrix factorization. We show that the computational complexity of our construction is linear in the number of nodes, achieving the desired efﬁciency, and we demonstrate the linear complexity and competitive performance through experiments on Darcy ﬂow [44], a linear second-order elliptic equation, and Burgers’ equation[43], which considered a stepping stone to Naiver-Stokes, is nonlinear, long-range correlated and more challenging. Our primary contributions are listed below.
• We develop the multipole graph kernel neural network (MGKN) that can capture long-range correlations in graph-based data with a linear time complexity in the nodes of the graph.
• We unify GNNs with multi-resolution matrix factorization through the V-cycle algorithm.
• We verify, analytically and numerically, the linear time complexity of the proposed method-ology.
• We demonstrate numerically our method’s ability to capture global information by learning mesh-invariant solution operators to the Darcy ﬂow and Burgers’ equations. 2
Left: the multi-level graph. Right: one V-cycle iteration for the multipole graph kernel network.
Figure 1: V-cycle 2 Operator Learning
We consider the problem of learning a mapping between two inﬁnite-dimensional function spaces.
Let A and U be separable Banach spaces and F † : A → U be the target mapping. Suppose we have observation of N pairs of functions {aj, uj}N j=1 where aj ∼ µ is i.i.d. sampled from a measure µ supported on A and uj = F †(aj), potentially with noise. The goal is to ﬁnd an ﬁnite dimension approximation Fθ : A → U, parametrized by θ ∈ Θ, such that
Fθ ≈ F †, µ − a.e.
We formulate this as an optimization problem where the objective J(θ) is the expectation of a loss functional (cid:96) : U × U → R. We consider the squared-error loss in an appropriate norm (cid:107) · (cid:107)U on U and discretize the expectation using the training data:
J(θ) = Ea∼µ[(cid:96)(Fθ(a), F †(a))] ≈ 1
N
N (cid:88) j=1 (cid:107)Fθ(aj) − F †(aj)(cid:107)2
U .
Parametric PDEs. An important example of the preceding problem is learning the solution operator of a parametric PDE. Consider La a differential operator depending on a parameter a ∈ A and the general PDE x ∈ D (Lau)(x) = f (x), (1) for some bounded, open set D ⊂ Rd, a ﬁxed function f living in an appropriate function space, and a boundary condition on ∂D. Assuming the PDE is well-posed, we may deﬁne the mapping of interest F † : A → U as mapping a to u the solution of (1). For example, consider the second order elliptic operator La· = −div(a∇·). For a ﬁxed f ∈ L2(D; R) and a zero Dirichlet boundary 0 (D; R) for any parameter condition on u, equation (1) has a unique weak solution u ∈ U = H 1 a ∈ A = L∞(D; R+) ∩ L2(D; R+). This example is prototypical of many scientiﬁc applications including hydrology [8] and elasticity [6].
Numerically, we assume access only to point-wise evaluations of the training pairs (aj, uj). In particular, let Dj = {x1, . . . , xn} ⊂ D be a n-point discretization of the domain D and assume we have observations aj|Dj , uj|Dj ∈ Rn. Although the data given can be discretized in an arbitrary manner, our approximation of the operator F † can evaluate the solution at any point x ∈ D.
Learning the Operator. Learning the operator F † is typically much more challenging than ﬁnding the solution u ∈ U of a PDE for a single instance with the parameter a ∈ A. Most existing methods, ranging from classical ﬁnite elements and ﬁnite differences to modern deep learning approaches such as physics-informed neural networks (PINNs) [38] aim at the latter task. And therefore they are computationally expensive when multiple evaluation is needed. This makes them impractical for applications such as inverse problem where we need to ﬁnd the solutions for many different instances of the parameter. On the other hand, operator-based approaches directly approximate the operator and are therefore much cheaper and faster, offering tremendous computational savings when compared to traditional solvers. 3
• PDE solvers: solve one instance of the PDE at a time; require the explicit form of the PDE; have a speed-accuracy trade-off based on resolution: slow on ﬁne grids but less accurate on coarse grids.
• Neural operator: learn a family of equations from data; don’t need the explicit knowledge of the equation; much faster to evaluate than any classical method; no training needed for new equations; error rate is consistent with resolution. 2.1 Graph Kernel Network (GKN)
Suppose that La in (1) is uniformly elliptic then the Green’s representation formula implies u(x) = (cid:90)
D
Ga(x, y)[f (y) + (Γau)(y)] dy. (2) where Ga is a Newtonian potential and Γa is an operator deﬁned by appropriate sums and composi-tions of the modiﬁed trace and co-normal derivative operators [41, Thm. 3.1.6]. We have turned the
PDE (1) into the integral equation (2) which lends itself to an iterative approximation architecture.
Kernel operator. Since Ga is continuous for all points x (cid:54)= y, it is sensible to model the action of the integral operator in (2) by a neural network κφ with parameters φ. To that end, deﬁne the operator
Ka : U → U as the action of the kernel κφ on u: (Kau)(x) = (cid:90)
D
κφ(a(x), a(y), x, y)u(y) dy (3) where the kernel neural network κφ takes as inputs spatial locations x, y as well as the values of the parameter a(x), a(y). Since Γa is itself an operator, its action cannot be fully accounted for by the kernel κφ, we therefore add local parameters W = wI to Ka and apply a non-linear activation σ, deﬁning the iterative architecture u(t) = σ((W + Ka)u(t−1)) for t = 1, . . . T with u(0) = a. Since
Γa is local w.r.t u, we only need local parameters to capture its effect, while we expect its non-locality w.r.t. a to manifest via the initial condition [41]. To increase expressiveness, we lift u(x) ∈ R to a higher dimensional representation v(x) ∈ Rdv by a point-wise linear transformation v0 = P u0, and update the representation v(t) = σ(cid:0)(W + Ka)v(t−1)(cid:1), t = 1, . . . , T (4) projecting it back u(T ) = Qv(T ) at the last step. Hence the kernel is a mapping κφ : R2(d+1) →
Rdv×dv and W ∈ Rdv×dv . Note that, since our goal is to approximate the mapping a (cid:55)→ u with f in (1) ﬁxed, we do not need explicit dependence on f in our architecture as it will remain constant for any new a ∈ A.
For a speciﬁc discretization Dj, aj|Dj , uj|Dj ∈ Rn are n-dimensional vectors, and the evalu-ation of the kernel network can be viewed as a n × n matrix K, with its x, y entry (K)xy =
κφ(a(x), a(y), x, y). Then the action of Ka becomes the matrix-vector multiplication Ku. For the lifted representation (4), for each x ∈ Dj, v(t)(x) ∈ Rdv and the output of the kernel (K)xy = κφ(a(x), a(y), x, y) is a dv × dv matrix. Therefore K becomes a fourth order tensor with shape n × n × dv × dv.
Kernel convolution on graphs. Since we assume a non-uniform discretization of D that can differ for each data pair, computing with (4) cannot be implemented in a standard way. Graph neural networks offer a natural solution since message passing on graphs can be viewed as the integration (3). Since the message passing is computed locally, it avoids storing the full kernel matrix K.
Given a discretization Dj, we can adaptively construct graphs on the domain D. The structure of the graph’s adjacency matrix transfers to the kernel matrix K. We deﬁne the edge attributes e(x, y) = (a(x), a(y), x, y) ∈ R2(d+1) and update the graph nodes following (4) which mimics the message passing neural network [21]: v(t+1)(x) = (σ(W + Ka)v(t))(x) ≈ σ (cid:16)
W v(t) + (cid:88) (cid:17) (cid:0)e(x, y)(cid:1)v(t)(y)
κφ (5) 1
|N (x)| y∈N (x) where N (x) is the neighborhood of x, in this case, the entire discritized domain Dj. 4
Domain of Integration. Construction of fully connected graphs is memory intensive and can become computationally infeasible for ﬁne discretizations i.e. when |Dj| is large. To partially alleviate this, we can ignore the longest range kernel interactions as they have decayed the most and change the integration domain in (3) from D to B(x, r) for some ﬁxed radius r > 0. This is equivalent to imposing a sparse structure on the kernel matrix K so that only entries around the diagonal are non-zero and results in the complexity O(n2rd).
Nyström approximation. To further relieve computational complexity, we use Nyström approx-imation or the inducing points method by uniformly sampling m < n nodes from the n nodes discretization, which is to approximate the kernel matrix by a low-rank decomposition
Knn ≈ KnmKmmKmn (6) where Knn = K is the original n × n kernel matrix and Kmm is the m × m kernel matrix corresponding to the m inducing points. Knm and Kmn are transition matrices which could include restriction, prolongation, and interpolation. Nyström approximation further reduces the complexity to O(m2rd). 3 Multipole Graph Kernel Network (MGKN)
The fast multipole method (FMM) is a systematic approach of combining the aforementioned sparse and low-rank approximations while achieving linear complexity. The kernel matrix is decomposed into different ranges and a hierarchy of low-rank structures is imposed on the long-range components.
We employ this idea to construct hierarchical, multi-scale graphs, without being constraint to particular forms of the kernel [48]. We elucidate the workings of the FMM through matrix factorization. 3.1 Decomposition of the kernel
The key to the fast multipole method’s linear complexity lies in the subdivision of the kernel matrix according to the range of interaction, as shown in Figure 2:
K = K1 + K2 + . . . + KL (7) where K1 corresponds to the shortest-range interaction, and KL corresponds to the longest-range interaction. While the uniform grids depicted in Figure 2 produce an orthogonal decomposition of the kernel, the decomposition may be generalized to arbitrary graphs by allowing overlap.
Figure 2: Hierarchical matrix decomposition
The kernel matrix K is decomposed respect to ranges. K1 corresponds to short-range interaction; it is sparse but high-rank. K3 corresponds to long-range interaction; it is dense but low-rank. 3.2 Multi-scale graphs.
We construct L graph levels, where the ﬁnest graph corresponds to the shortest-range interaction
K1, and the coarsest graph corresponds to the longest-range interaction KL. In what follows, we will drop the time dependence from (4) and use the subscript vl to denote the representation at each level of the graph. Assuming the underlying graph is a uniform grid with resolution s such that sd = n, the L multi-level graphs will be grids with resolution sl = s/2l−1, and consequentially nl = sd l = (s/2l−1)d for l = 1, . . . , L. In general, the underlying discretization can be arbitrary and we produce a hierarchy of L graphs with a decreasing number of nodes n1, . . . , nL. 5
The coarse graph representation can be understood as recursively applying an inducing points approximation: starting from a graph with n1 = n nodes, we impose inducing points of size n2, n3, . . . which all admit a low-rank kernel matrix decomposition of the form (6). The original n × n kernel matrix Kl is represented by a much smaller nl × nl kernel matrix, denoted by Kl,l. As shown in Figure (2), K1 is full-rank but very sparse while KL is dense but low-rank. Such structure can be achieved by applying equation (6) recursively to equation (7), leading to the multi-resolution matrix factorization [29]:
K ≈ K1,1 + K1,2K2,2K2,1 + K1,2K2,3K3,3K3,2K2,1 + · · · (8) where K1,1 = K1 represents the shortest range, K1,2K2,2K2,1 ≈ K2, represents the second shortest range, etc. The center matrix Kl,l is a nl × nl kernel matrix corresponding to the l-level of the graph described above. The long matrices Kl+1,l, Kl,l+1 are nl+1×nl and nl+1×nl transition matrices. We deﬁne them as moving the representation vl between different levels of the graph via an integral kernel that we learn. In general, v(t)(x) ∈ Rdv and the output of the kernel (Kl,l(cid:48))xy = κφ(a(x), a(y), x, y) is itself a dv × dv matrix, so all these matrices are again fourth-order tensors. (cid:90)
Kl,l : vl (cid:55)→ vl =
B(x,rl,l) (cid:90)
Kl+1,l : vl (cid:55)→ vl+1 =
Kl,l+1 : vl+1 (cid:55)→ vl =
B(x,rl+1,l) (cid:90)
B(x,rl,l+1)
κφl,l (a(x), a(y), x, y)vl(y) dy
κφl+1,l (a(x), a(y), x, y)vl(y) dy
κφl,l+1(a(x), a(y), x, y)vl+1(y) dy (9) (10) (11)
Linear complexity. The complexity of the algorithm is measured in terms of the sparsity of K as this is what affects all computations. The sparsity represents the complexity of the convolution, and it is equivalent to the number of evaluations of the kernel network κ. Each matrix in the decomposition (7) is represented by the kernel matrix Kl,l corresponding to the appropriate sub-graph. Since the number of non-zero entries of each row in these matrices is constant, we obtain that the computational complexity is (cid:80) l O(nl). By designing the sub-graphs so that nl decays fast enough, we can obtain linear complexity. For example, choose nl = O(n/2l) then (cid:80) l n/2l = O(n).
Combined with a Nyström approximation, we obtain O(m) complexity. l O(nl) = (cid:80) 3.3 V-cycle Algorithm
We present a V-cycle algorithm (not to confused with multigrid methods), see Figure 1, for efﬁciently computing (8). It consists of two steps: the downward pass and the upward pass. Denote the representation in downward pass and upward pass by ˇv and ˆv respectively. In the downward step, the algorithm starts from the ﬁne graph representation ˇv1 and updates it by applying a downward transition ˇvl+1 = Kl+1,lˇvl. In the upward step, the algorithm starts from the coarse presentation ˆvL and updates it by applying an upward transition and the center kernel matrix ˆvl = Kl,l−1ˆvl−1 +Kl,lˇvl.
Notice that the one level downward and upward exactly computes K1,1 + K1,2K2,2K2,1, and a full
L-level v-cycle leads to the multi-resolution decomposition (8).
Employing (9)-(11), we use L neural networks κφ1,1, . . . , κφL,L to approximate the kernel Kl,l, and 2(L − 1) neural networks κφ1,2, κφ2,1, . . . to approximate the transitions Kl+1,l, Kl,l+1. Following the iterative architecture (4), we also introduce the linear operator W , denoting it by Wl for each corresponding resolution. Since it acts on a ﬁxed resolution, we employ it only along with the kernel
Kl,l and not the transitions. At each time step t = 0, . . . , T − 1, we perform a full V-cycle:
Downward Pass:
For l = 1, . . . , L :
Upward Pass:
For l = L, . . . , 1 : l+1 = σ(ˆv(t)
ˇv(t+1) l+1 + Kl+1,lˇv(t+1) l
)
ˆv(t+1) l
= σ((Wl + Kl,l)ˇv(t+1) l
+ Kl,l−1ˆv(t+1) l−1 ). (12) (13) 1 = P u(0) = P a and output u(T ) = Qv(T ) = Qˆv(T )
We initialize as v(0)
. The algorithm uniﬁes multi-resolution matrix decomposition with iterative graph kernel networks. Combined with a Nyström approximation it leads to O(m) computational complexity that can be implemented with message passing neural networks. Notice GKN is a speciﬁc case of V-cycle when L = 1. 1 6
Figure 3: Properties of multipole graph kernel network (MGKN) on Darcy ﬂow
Left: compared to GKN whose complexity scales quadratically with the number of nodes, MGKN has a linear complexity; Mid: Adding more levels reduces test error; Right: MGKN can be trained on a coarse resolution and perform well when tested on a ﬁne resolution, showing invariance to discretization. 4 Experiments
We demonstrate the linear complexity and competitive performance of multipole graph kernel network through experiments on Darcy equation and Burgers equation. 4.1 Properties of the multipole graph kernel network
In this section, we show that MGKN has linear complexity and learns discretization invariant solutions by solving the steady-state of Darcy ﬂow. In particular, we consider the 2-d PDE
−∇ · (a(x)∇u(x)) = f (x) u(x) = 0 x ∈ (0, 1)2 x ∈ ∂(0, 1)2 (14) and approximate the mapping a (cid:55)→ u which is non-linear despite the fact that (14) is a linear PDE.
We model the coefﬁcients a as random piece-wise constant functions and generate data by solving (14) using a second-order ﬁnite difference scheme on a ﬁne grid. Data of coarser resolutions are sub-sampled. See the supplements for further details. The code depends on Pytorch Geometric[19], also included in the supplements.
We use Nyström approximation by sampling m1, . . . , mL nodes for each level. When changing the number of levels, we ﬁx coarsest level mL = 25, rL,L = 2−1, and let ml = 25 · 4L−l, rl,l = 2−(L−l), and rl,l+1 = rl+1,l = 2−(L−l)+1/2. This set-up is one example that can obtain linear complexity.
In general, any choice satisfying (cid:80) l,l = O(m1) also works. We set width dv = 64, iteration
T = 5 and kernel network κφl,l as a three-layer neural network with width 256/2(l−1), so that coarser grids will have smaller kernel networks. l m2 l r2 1. Linear complexity: The left most plot in ﬁgure 3 shows that MGKN (blue line) achieves linear time complexity (the time to evaluate one equation) w.r.t. the number of nodes, while GKN (red line) has quadratic complexity (the solid line is interpolated; the dash line is extrapolated). Since the GPU memory used for backpropagation also scales with the number of edges, GKN is limited to m ≤ 800 on a single 11G-GPU while MGKN can scale to much higher resolutions . In other words, MGKN can be applicable for larger settings where GKN cannot. 2. Comparing with single-graph: As shown in ﬁgure 3 (mid), adding multi-leveled graphs helps decrease the error. The MGKN depicted in blue bars starts from a ﬁne sampling L = 1; m = [1600], and adding subgraphs, L = 2; m = [400, 1600], L = 3; m = [100, 400, 1600], up to, L = 4; m =
[25, 100, 400, 1600]. When L = 1, MGKN and GKN are equivalent. This experiment shows using multi-level graphs helps improve accuracy without increasing much of time-complexity. 3. Generalization to resolution: The MGKN is discretization invariant, and therefore capable of super-resolution.We train with nodes sampled from a s × s resolution mesh and test on nodes sampled 7
Figure 4: Comparsion with benchmarks on Burgers equation, with examples of inputs and outputs. 1-d Burgers equation with viscosity ν = 0.1. Left: performance of different methods. MGKN has competitive performance. Mid: input functions (u0) of two examples. Right: corresponding outputs from MGKN of the two examples, and their ground truth (dash line). The error is minimal on both examples.. from a s(cid:48) × s(cid:48) resolution mesh. As shown in on the right of ﬁgure 3, MGKN achieves similar testing error on s(cid:48) = 61, 121, 241, independently of the training discretization.
Notice traditional PDE solvers such as FEM and FDM approximate a single function and therefore their error to the continuum decreases as resolution is increased. On the other hand, operator approximation is independent of the ways its data is discretized as long as all relevant information is resolved. Therefore, if we truly approximate an operator mapping, the error will be constant at any resolution, which many CNN-based methods fail to preserve. 4.2 Comparison with benchmarks
We compare the accuracy of our methodology with other deep learning methods as well as reduced order modeling techniques that are commonly used in practice. As a test bed, we use 2-d Darcy ﬂow (14) and the 1-d viscous Burger’s equations:
∂tu(x, t) + ∂x(u2(x, t)/2) = ν∂xxu(x, t), u(x, 0) = u0(x), x ∈ (0, 2π), t ∈ (0, 1] x ∈ (0, 2π) (15) with periodic boundary conditions. We consider mapping the initial condition to the solution at time one u0 (cid:55)→ u(·, 1). Burger’s equation re-arranges low to mid range energies resulting in steep discontinuities that are dampened proportionately to the size of the viscosity ν. It acts as a simpliﬁed model for the Navier-Stokes equation. We sample initial conditions as Gaussian random ﬁelds and solve (15) via a split-step method on a ﬁne mesh, sub-sampling other data as needed; two examples of u0 and u(·, 1) are shown in the middle and right of ﬁgure 4.
Figure 4 shows the relative test errors for a variety of methods on Burger’s (15) (left) as a function of the grid resolution. First notice that MGKN achieves a constant steady state test error, demonstrating that it has learned the true inﬁnite-dimensional solution operator irrespective of the discretization.
This is in contrast to the state-of-the-art fully convolution network (FCN) proposed in [49] which has the property that what it learns is tied to a speciﬁc discretization. Indeed, we see that, in both cases, the error increases with the resolution since standard convolution layer are parametrized locally and therefore cannot capture the long-range correlations of the solution operator. Using linear spaces, the (PCA+NN) method proposed in [10] utilizes deep learning to produce a fast, fully data-driven reduced order model. The graph convolution network (GCN) method follows [4]’s architecture, with naive nearest neighbor connection. It shows simple nearest-neighbor graph structures are insufﬁcient.
The graph kernel network (GKN) employs an architecture similar to (4) but, without the multi-level graph extension, it can be slow due the quadratic time complexity. For Burger’s equation, when linear spaces are no longer near-optimal, MGKN is the best performing method. This is a very encouraging result since many challenging applied problem are not well approximated by linear spaces and can therefore greatly beneﬁt from non-linear approximation methods such as MGKN. For details on the other methods, see the supplements.
The benchmark of the 2-d Darcy equation is given in Table 4, where MGKN again achieves a competitive error rate. Notice in the 1-d Burgers’ equation (Table 5), we restrict the transition matrices 8
Kl,l+1, Kl+1,l to be restrictions and prolongations and thereby force the kernels K1, . . . , KL to be orthogonal. In the 2-d Darcy equation (Table 4), we use general convolutions (10, 11) as the transition, allowing overlap of these kernels. We observe the orthogonal decomposition of kernel tends to have better performance. 5