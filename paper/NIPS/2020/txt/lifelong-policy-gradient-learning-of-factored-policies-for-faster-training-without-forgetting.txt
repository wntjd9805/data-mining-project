Abstract
Policy gradient methods have shown success in learning control policies for high-dimensional dynamical systems. Their biggest downside is the amount of explo-ration they require before yielding high-performing policies. In a lifelong learning setting, in which an agent is faced with multiple consecutive tasks over its lifetime, reusing information from previously seen tasks can substantially accelerate the learning of new tasks. We provide a novel method for lifelong policy gradient learning that trains lifelong function approximators directly via policy gradients, allowing the agent to beneﬁt from accumulated knowledge throughout the en-tire training process. We show empirically that our algorithm learns faster and converges to better policies than single-task and lifelong learning baselines, and completely avoids catastrophic forgetting on a variety of challenging domains. 1

Introduction
Policy gradient (PG) methods have been successful in learning control policies on high-dimensional, continuous systems [33, 23, 34]. However, like most methods for reinforcement learning (RL), they require the agent to interact with the world extensively before outputting a functional policy. In some settings, this experience is prohibitively expensive, such as when training an actual physical system.
If an agent is expected to learn multiple consecutive tasks over its lifetime, then we would want it to leverage knowledge from previous tasks to accelerate the learning of new tasks. This is the premise of lifelong RL methods. Most previous work in this ﬁeld has considered the existence of a central policy that can be used to solve all tasks the agent will encounter [21, 35]. If the tasks are sufﬁciently related, this model serves as a good starting point for learning new tasks, and the main problem becomes how to avoid forgetting the knowledge required to solve tasks encountered early in the agent’s lifetime.
However, in many cases, the tasks the agent will encounter are less closely related, and so a single policy is insufﬁcient for solving all tasks. A typical approach for handling this (more realistic) setting is to train a separate policy for each new task, and then use information obtained during training to ﬁnd commonalities to previously seen tasks and use these relations to improve the learned policy [6, 5, 19]. Note that this only enables the agent to improve policy performance after an initial policy has been trained. Such methods have been successful in outperforming the original policies trained independently for each task, but unfortunately do not allow the agent to reuse knowledge from previous tasks to more efﬁciently search for policies, and so the learning itself is not accelerated.
We propose a novel framework for lifelong RL via PG learning that automatically leverages prior experience during the training process of each task. In order to enable learning highly diverse tasks, we follow prior work in lifelong RL by searching over factored representations of the policy-34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
parameter space to learn both a shared repository of knowledge and a series of task-speciﬁc mappings to constitute individual task policies from the shared knowledge.
Our algorithm, lifelong PG: faster training without forgetting (LPG-FTW) yields high-performing policies on a variety of benchmark problems with less experience than independently learning each task, and avoids the problem of catastrophic forgetting [24]. Moreover, we show theoretically that
LPG-FTW is guaranteed to converge to a particular approximation to the multi-task objective. 2