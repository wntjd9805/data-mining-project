Abstract
Developing learning methods which do not discriminate subgroups in the popu-lation is a central goal of algorithmic fairness. One way to reach this goal is by modifying the data representation in order to meet certain fairness constraints. In this work we measure fairness according to demographic parity. This requires the probability of the possible model decisions to be independent of the sensitive information. We argue that the goal of imposing demographic parity can be sub-stantially facilitated within a multitask learning setting. We present a method for learning a shared fair representation across multiple tasks, by means of different new constraints based on MMD and Sinkhorn Divergences. We derive learning bounds establishing that the learned representation transfers well to novel tasks.
We present experiments on three real world datasets, showing that the proposed method outperforms state-of-the-art approaches by a signiﬁcant margin. 1

Introduction
During the last decade, the widespread distribution of automatic systems for decision making is raising concerns about their potential for unfair behaviour [3, 7, 37]. As a consequence, machine learning models have often to meet fairness requirements, ensuring the correction and limitation of – for example – racist or sexist decisions. In literature, it is possible to ﬁnd a plethora of different methods to generate fair models with respect to one or more sensitive attributes (e.g. gender, ethnic group, age). These methods can be mainly divided into three families: (i) methods in the ﬁrst family change a pre-trained model in order to make it more fair (while trying to maintain the classiﬁcation performance, i.e., post-processing of the model) [14, 19, 34]; (ii) in the second family, we can ﬁnd methods that enforce fairness directly during the training phase, e.g. [1, 12, 41, 42]; (iii) the third family of methods implements fairness by modifying the data representation (i.e., pre-processing of the data), and then employs standard machine learning methods [9, 43].
All methods in the previous three families have in common the goal of creating a fair and accurate model from scratch on the speciﬁc task at hand. This solution may work well in speciﬁc cases, but in a large number of real world applications, using the same model (or at least part of it) over different tasks is helpful if not mandatory. For example, it is common to perform a ﬁne tuning over pre-trained models [11], keeping ﬁxed the internal representation. Indeed, most modern machine learning frameworks (especially the deep learning ones) offer a set of pre-trained models that are distributed in 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
so-called model zoos1. Unfortunately, ﬁne tuning pre-trained models on novel previously unseen tasks could lead to an unexpected unfairness behaviour, even starting from an apparently fair model for previous tasks (e.g. discriminatory transfer [24] or negative legacy [22]), due to missing generalization guarantees concerning the fairness property of the model over new tasks.
In order to overcome the above problem, in this paper we follow the framework of multitask learning.
We aim to leverage task similarities to learn a fair representation that provably generalizes well to unseen tasks. By this we mean that when the representation is used to learn novel tasks, it is guaranteed to learn a model that has both a small error and meets the fairness requirements. We measure fairness according to demographic parity [8] (for an extended analysis of different fairness deﬁnitions see [39, 42]). It requires the probability of possible model decisions to be independent of the sensitive information. We argue that multitask methods are well suited to learn a shared fair representation according to demographic parity. The fairness of the representation is enforced by imposing the distributions of the different subgroups to be close with respect to three different distances, respectively between their: (i) average value (AVG) [32], (ii) Maximum Mean Discrepancy (MMD) [35, 38], and (iii) Sinkhorn Divergences (SNK) [10].
Contributions. We propose a method for learning a shared fair representation across the multiple tasks, by incorporating novel constraints based on MMD and Sinkhorn Divergences on the represen-tation. We show empirically and theoretically that the representation learned by the method transfers well to novel tasks in the sense that whenever the empirical unfairness is small on the training tasks then the unfairness on a future task will likely be small as well. An important implication of our results is that the learned representation can be used as is to learn models for new unseen tasks, that are provably fair, without the need of imposing any further fairness constraint on the model.
Organization. The paper is organized as follows. In Section 2, we discuss previous related work aimed at learning fair representations. In Section 3, we introduce the proposed method. In Section 4, we study the generalization properties of our method. In Section 5, we experimentally compare the proposed method against different baselines and state-of-the-art approaches on three real world datasets. Finally, in Section 6 we discuss directions of future research. 2