Abstract
Flow models have recently made great progress at modeling ordinal discrete data such as images and audio. Due to the continuous nature of ﬂow models, dequantization is typically applied when using them for such discrete data, resulting in lower bound estimates of the likelihood. In this paper, we introduce subset
ﬂows, a class of ﬂows that can tractably transform ﬁnite volumes and thus allow exact computation of likelihoods for discrete data. Based on subset ﬂows, we identify ordinal discrete autoregressive models, including WaveNets, PixelCNNs and Transformers, as single-layer ﬂows. We use the ﬂow formulation to compare models trained and evaluated with either the exact likelihood or its dequantization lower bound. Finally, we study multilayer ﬂows composed of PixelCNNs and non-autoregressive coupling layers and demonstrate state-of-the-art results on
CIFAR-10 for ﬂow models trained with dequantization. 1

Introduction
Y
Z
Flow f
Learning generative models of high-dimensional data poses a signiﬁcant challenge. The model will have to capture not only the marginal distributions of each of the variables, but also the potentially combi-natorial number of interactions between them. Deep generative models provide tools for learning richly-structured, high-dimensional distributions, utilizing the vast amounts of unlabeled data available. Gen-erative adversarial networks (GANs) (Goodfellow et al., 2014) are one class of deep generative mod-els that have demonstrated an impressive ability to generate plausible-looking images. However, GANs typically lack support over the full data distribution and provide no quantitative measure of performance.
Likelihood-based deep generative models, on the other hand, do provide this and can be classiﬁed as:
Figure 1: Subset ﬂows f : Y → Z allow not only to transform points z = f (y), but also subsets Zi = f (Yi), in one pass. As a result, these ﬂows can be trained on ordinal discrete data without the need for dequantization.
Y1 Y2
Z1
Z4
Z3
Z2
Y3
Y4 1. Latent variable models such as Deep Belief Networks (Hinton et al., 2006; Hinton, 2007),
Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009), Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014). 2. Autoregressive models such as Recurrent Neural Networks (RNNs), MADE (Germain et al., 2015), WaveNet (van den Oord et al., 2016a), PixelCNN (van den Oord et al., 2016c),
PixelCNN++ (Salimans et al., 2017), Sparse Transformers (Child et al., 2019). 3. Flow models such as RealNVP (Dinh et al., 2017), Glow (Kingma and Dhariwal, 2018),
MAF (Papamakarios et al., 2017), FFJORD (Grathwohl et al., 2019). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Data recorded from sensors are quantized before storage, resulting in ordinal data, i.e. discrete data with a natural ordering. Autoregressive models excel at modelling such data since they can directly model discrete distributions. Apart from discrete ﬂows (Tran et al., 2019; Hoogeboom et al., 2019) – which are severely restricted in expressiveness – the vast majority of ﬂow models are continuous and therefore require dequantization to be applied to discrete data. However, dequantization comes at the cost of lower bound estimates of the discrete likelihood (Theis et al., 2016; Ho et al., 2019).
In this paper: 1) We introduce subset ﬂows, a class of ﬂows that allow tractable transformation of ﬁnite volumes and consequently may be trained directly on discrete data such as images, audio and video without the need for dequantization. 2) Based on subset ﬂows, we formulate existing autoregressive models for ordinal discrete data, such as PixelCNN (van den Oord et al., 2016c) and PixelCNN++ (Salimans et al., 2017), as single-layer autoregressive ﬂows. 3) Using the ﬂow formulation of PixelCNNs, we quantify how dequantization used in training and evalutation impacts performance. 4) We construct multilayer ﬂows using compositions of PixelCNNs and coupling layers (Dinh et al., 2017). For CIFAR-10, we demonstrate state-of-the-art results for ﬂow models trained with dequantization. The code used for experiments is publicly available at https://github.com/ didriknielsen/pixelcnn_flow. 2