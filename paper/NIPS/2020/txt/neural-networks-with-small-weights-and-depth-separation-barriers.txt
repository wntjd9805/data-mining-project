Abstract
In studying the expressiveness of neural networks, an important question is whether there are functions which can only be approximated by sufﬁciently deep networks, assuming their size is bounded. However, for constant depths, existing results are limited to depths 2 and 3, and achieving results for higher depths has been an important open question. In this paper, we focus on feedforward ReLU networks, and prove fundamental barriers to proving such results beyond depth 4, by reduction to open problems and natural-proof barriers in circuit complexity. To show this, we study a seemingly unrelated problem of independent interest: Namely, whether there are polynomially-bounded functions which require super-polynomial weights in order to approximate with constant-depth neural networks. We provide a negative and constructive answer to that question, by showing that if a function can be approximated by a polynomially-sized, constant depth k network with arbitrarily large weights, it can also be approximated by a polynomially-sized, depth 3k + 3 network, whose weights are polynomially bounded. 1

Introduction
The expressive power of feedforward neural networks has been extensively studied in recent years. It is well-known that sufﬁciently large depth-2 neural networks, using reasonable activation functions, can approximate any continuous function on a bounded domain ([5, 9, 15, 2]). However, the required size of such networks can be exponential in the input dimension, which renders them impractical.
From a learning perspective, both theoretically and in practice, the main interest is in neural networks whose size is at most polynomial in the input dimension.
When considering the expressive power of neural networks of bounded size, a key question is what are the tradeoffs between the width and the depth. Overwhelming empirical evidence indicates that deeper networks tend to perform better than shallow ones, a phenomenon supported by the intuition that depth, providing compositional expressibility, is necessary for efﬁciently representing some functions. From the theoretical viewpoint, quite a few works in the past few years have explored the beneﬁcial effect of depth on increasing the expressiveness of neural networks. A main focus is on depth separation, namely, showing that there is a function f : Rd → R that can be approximated by a poly(d)-sized network of a given depth, with respect to some input distribution, but cannot be approximated by poly(d)-sized networks of a smaller depth. Depth separation between depth 2 and 3 was shown by [7] and [6]. However, despite much effort, no such separation result is known for any constant greater than 2. Thus, it is an open problem whether there is separation between depth 3 and some constant depth greater than 3. Separation between networks of a constant depth and networks with poly(d) depth was shown by [30] (see related work section below for more details).
In fact, a similar question has been extensively studied by the theoretical computer science community over the past decades, in the context of Boolean and threshold circuits of bounded size. Showing limitations for the expressiveness of such circuits (i.e. circuit lower bounds) can contribute to our 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
understanding of the P (cid:54)= N P question, and can have other signiﬁcant theoretical implications
[1]. Despite many attempts, the results on circuit lower bounds were limited. In a seminal work,
[23] described a main technical limitation of current approaches for proving circuit lower bounds:
They deﬁned a notion of “natural proofs" for a circuit lower bound (which include current proof techniques), and showed that obtaining lower bounds with such proof techniques would violate a widely accepted conjecture, namely, that pseudorandom functions exist. This natural-proof barrier explains the lack of progress on circuit lower bounds. More formally, they show that if a class C of circuits contains a family of pseudorandom functions, then showing for some function f that f (cid:54)∈ C cannot be done with a natural proof. As a result, if we consider the class C of poly(d)-sized circuits of some bounded depth k, where k is large enough so that C contains a pseudorandom function family, then it will be difﬁcult to show that some functions are not in C, and hence that these functions require depth larger than k to express.
An object closer to actual neural networks are threshold circuits. These are essentially neural networks with a threshold activation function in all neurons (including the output neuron), and where the inputs are in {0, 1}d. The problem of depth separation in threshold circuits was widely studied [22]. This problem requires, for some integer k, a function that cannot be computed by a threshold circuit of width poly(d) and depth k, but can be computed1 by a threshold circuit of width poly(d) and depth k(cid:48) > k. [20] and [16] showed a candidate pseudorandom function family computable by threshold circuits of depth 4, width poly(d), and poly(d)-bounded weights. By [23], it implies that for every k(cid:48) > k ≥ 4, there is a natural-proof barrier for showing depth separation between threshold circuits of depth k and depth k(cid:48). As for smaller depths, a separation between threshold circuits of depth 3 and some k > 3 is a longstanding open problem (although there is no known natural-proof barrier in this case), and separation between threshold circuits of depth 2 and 3 is known under the assumption that the weight magnitudes are poly(d) bounded [14].
Since a threshold circuit is a special case of a neural network with threshold activation and where the inputs and output are Boolean, it is natural to ask whether the barriers to depth separation in threshold circuits have implications on the problem of depth separation in neural networks. Such implications are not obvious, since neural networks have real-valued inputs and outputs (not necessarily just
Boolean ones), and a continuous activation function. Thus, it might be possible to come up with a depth-separation result, which crucially utilizes some function and inputs in Euclidean space. In fact, this can already be seen in existing results: For example, separation between threshold circuits of constant depth (TC0) and threshold circuits of poly(d) depth (which equals the complexity class
P/poly) is not known, but [30] showed such a result for neural networks. His construction is based on the observation that for one dimensional data, a network of depth k is able to express a sawtooth function on the interval [0, 1] which oscillates O(2k) times. Clearly, this utilizes the continuous structure of the domain, in a way that is not possible with Boolean inputs. Also, the depth-2 vs. 3 separation results of [7] and [6] rely on harmonic analysis of real functions. Finally, the result of [7] does not make any assumption on the weight magnitudes, whereas relaxing this assumption for the parallel result on threshold circuits is a longstanding open problem [22].
Main Result 1: Barriers to Depth Separation
In this work, we focus on real-valued neural networks with the ReLU activation function, and show (under some mild assumptions on the input distribution and on the function) that any depth-separation result between neural networks of depth k ≥ 4 and some constant k(cid:48) > k, would imply depth separation between threshold circuits of depth k − 2 and some constant greater than k − 2. Hence, showing depth separation with k = 5 would solve the longstanding open problem of separating between threshold circuits of depth 3 and some larger constant. Showing depth separation with k ≥ 6 would solve the open problem of separating between threshold circuits of depth k − 2 and some larger constant, which is especially challenging due to the natural-proof barrier for threshold circuits of depth at least 4. Finally, showing depth separation with k = 4 would solve the longstanding open problem of separating between threshold circuits of depth 2 (with arbitrarily large weights) and some larger constant (we note that separation between threshold circuits of depth 2 and 3 is known only under the assumption that the weight magnitudes are poly(d) bounded). The result applies to both 1Note that in this literature it is customary to require exact representation of the function, rather than merely approximating it. 2
continuous and discrete input distributions. Thus, we show a barrier to depth separation, that explains the lack of progress on depth separation for constant-depth neural networks of depth at least 4.
While this is a strong barrier to depth separation in neural networks, it should not discourage researchers from continuing to investigate it. First, our results focus on plain feedforward ReLU networks, and do not necessarily apply to other architectures. Second, we do make assumptions on the input distribution and the function, which are mild but perhaps can be circumvented (or alternatively, relaxed). Third, our barrier does not apply to separation between depth 3 and some larger constant. That being said, we do show that in order to achieve separation between depth k ≥ 3 and some constant k(cid:48) > k, some different approach than these used in current results would be required. As far as we know, in all existing depth-separation results for continuous input distributions (e.g., [7, 6, 30, 24, 17, 32, 25]) the functions are either of the form f (x) = g((cid:107)x(cid:107)) or f (x) = g(x1) for g : R → R. Namely, f is either radial or depends on one component2. We show that for such functions, networks of a constant depth greater than 3 do not have more power than depth-3 networks.
Main Result 2: Effect of Weight Magnitude on Expressiveness
To establish our depth-separation results, we actually go through a seemingly unrelated problem of independent interest: Namely, what is the impact on expressiveness if we force the network weights to have reasonably bounded weights (say, poly(d)). This is a natural restriction: Exponentially-large weights are unwieldy, and moreover, most neural networks used in practice have small weights, due to several reasons related to the training process, such as regularization, standard initialization of the weights to small values, normalization heuristics, and techniques to avoid the exploding gradient problem [13]. Therefore, it is natural to ask how bounding the size of the weights affects the expressive power of neural networks. As far as we know, there are surprisingly few works on this, and current works on the expressiveness of neural networks often assume that the weights may be arbitrarily large, although this is not the case in practice.
If we allow arbitrary functions, there are trivial cases where limiting the weight magnitudes hurts expressiveness. For example, let f : [0, 1]d → R, where for every x = (x1, . . . , xd) we have f (x) = x1 · 2d. Clearly, f can be expressed by a network of depth 1 with exponential (in d) weights.
This function cannot be approximated w.r.t. the uniform distribution on [0, 1]d by a constant-depth network with poly(d) width and poly(d)-bounded weights, since such networks cannot compute exponentially-large values. However, functions of practical interest only have constant or poly(d)-sized values (or at least can be well-approximated by such functions). Thus, a more interesting question is whether for approximating such functions, we may need weights larger than poly(d).
In our paper, we provide a negative answer to this question, in the following sense: Under some mild assumptions on the input distribution, if the function can be approximated by a network with ReLU activation, width poly(d), constant depth k and arbitrarily large weights, then we show how it can be approximated by a network with ReLU activation, width poly(d), depth 3k + 3, and weights bounded by poly(d) or by a constant. The result applies to both continuous and discrete input distributions.
The two problems that we consider, namely depth-separation and the power of small weights, may seem unrelated. Indeed, each problem considers a different aspect of expressiveness in neural networks. However, perhaps surprisingly, the proofs for our results on barriers to depth separation follow from our construction of networks with small weights. In a nutshell, the idea is that our deeper small-weight network is such that most layers implement a threshold circuit. Thus, if we came up with a “hard” function f that provably requires much depth to express with a neural network, then the threshold circuit used in expressing it (via our small-weight construction) also provably requires much depth – since otherwise, we could make our small-weight network shallower, violating the assumption on f . This would lead to threshold-circuit lower bounds.