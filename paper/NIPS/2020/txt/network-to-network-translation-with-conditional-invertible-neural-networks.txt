Abstract
Given the ever-increasing computational costs of modern machine learning mod-els, we need to ﬁnd new ways to reuse such expert models and thus tap into the resources that have been invested in their creation. Recent work suggests that the power of these massive models is captured by the representations they learn. There-fore, we seek a model that can relate between different existing representations and propose to solve this task with a conditionally invertible network. This network demonstrates its capability by (i) providing generic transfer between diverse do-mains, (ii) enabling controlled content synthesis by allowing modiﬁcation in other domains, and (iii) facilitating diagnosis of existing representations by translating them into interpretable domains such as images. Our domain transfer network can translate between ﬁxed representations without having to learn or ﬁnetune them. This allows users to utilize various existing domain-speciﬁc expert models from the literature that had been trained with extensive computational resources.
Experiments on diverse conditional image synthesis tasks, competitive image mod-iﬁcation results and experiments on image-to-image and text-to-image generation demonstrate the generic applicability of our approach. For example, we translate between BERT and BigGAN, state-of-the-art text and image models to provide text-to-image generation, which neither of both experts can perform on their own. 1

Introduction
One of the key features of intelligence is the ability to combine and transfer information between diverse domains and modalities [12, 73, 65, 68, 61]. In contrast, artiﬁcial intelligence research has made great progress in learning powerful representations for individual domains [28, 71, 19, 69, 15, 5] that can even achieve superhuman performance on conﬁned tasks such as trafﬁc sign recognition
[10, 11], image classiﬁcation [29] or question answering [15]. However, learning representations for different domains that also allow a domain-to-domain transfer of information between them is signiﬁcantly more challenging [2]: There is a trade-off between the expressiveness of individual domain representations and their compatibility to another to support transfer. While for limited training data multimodal learning has successfully trained representations for different domains together [66, 74], the overall most powerful domain-speciﬁc representations typically result from training huge models speciﬁcally for individual challenging domains using massive amounts of training data and computational resources, e.g. [19, 69, 5]. With the dawn of even more massive models like the recently introduced GPT-3 [5], where training on only a single domain already demands most of the available resources, we must ﬁnd new, creative ways to make use of these powerful models, which none but the largest institutions can afford to train and experiment with, and thereby utilize the huge amount of resources and knowledge which are distilled into the model’s representations—in other words, we have to ﬁnd ways to cope with "The Bitter Lesson" [67].
∗Both authors contributed equally to this work.
Code available at https://github.com/CompVis/net2net. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
source domain target domain
A blue bird sitting on top of a ﬁeld
A yellow bird is perched on a branch
A couple of zebras are standing in a ﬁeld
A school bus parked in a parking lot
A park bench sitting in the middle of a park
A pizza sitting on top of a white plate
Figure 1: BERT [15] to BigGAN [4] transfer: Our approach enables translation between ﬁxed off-the-shelve expert models such as BERT and BigGAN without having to modify or ﬁnetune them.
Consequently, we seek a model for generic domain-to-domain transfer between arbitrary ﬁxed representations that come from highly complex, off-the-shelf, state-of-the-art models and we learn a domain translation that does not alter or retrain the individual representations but retains the full capabilities of original expert models. This stands in contrast to current inﬂuential domain transfer approaches [41, 51, 83] that require learning or ﬁnetuning existing domain representations to facilitate transfer between them.
Since different domains are typically not isomorphic to another, i.e. translations between them are not uniquely determined, the domain translation between ﬁxed domain representations requires learning the corresponding ambiguities. For example, there are many images which correspond to the same textual description and vice versa. To faithfully translate between domains, we employ a conditional invertible neural network (cINN) that also explicitly captures these transfer uncertainties.
The INN conditionally learns a unique translation of one domain representation together with its complementary residual onto another. This generic network-to-network translation between arbitrary models can efﬁciently transfer between diverse state-of-the-art models such as transformer-based natural language model BERT [15] and a BigGAN [4] for image synthesis to achieve competitive text-to-image translation, see Fig. 1.
To summarize, our contributions are as follows: We (i) provide a generic approach that allows to translate between ﬁxed off-the-shelf model representations, (ii) learns the inherent ambiguity of the domain translation, which facilitates content creation and model diagnostics, and (iii) enables compelling performance on various different domain transfer problems. We (iv) make transfer between domains and datasets computationally affordable, since our method does not require any gradient computations on the expert models but can directly utilize existing representations. 2