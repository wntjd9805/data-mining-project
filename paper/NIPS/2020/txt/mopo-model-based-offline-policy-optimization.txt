Abstract
Ofﬂine reinforcement learning (RL) refers to the problem of learning policies entirely from a large batch of previously collected data. This problem setting offers the promise of utilizing such datasets to acquire policies without any costly or dangerous active exploration. However, it is also challenging, due to the distribu-tional shift between the ofﬂine training data and those states visited by the learned policy. Despite signiﬁcant recent progress, the most successful prior methods are model-free and constrain the policy to the support of data, precluding generaliza-tion to unseen states. In this paper, we ﬁrst observe that an existing model-based
RL algorithm already produces signiﬁcant gains in the ofﬂine setting compared to model-free approaches. However, standard model-based RL methods, designed for the online setting, do not provide an explicit mechanism to avoid the ofﬂine setting’s distributional shift issue. Instead, we propose to modify the existing model-based RL methods by applying them with rewards artiﬁcially penalized by the uncertainty of the dynamics. We theoretically show that the algorithm maxi-mizes a lower bound of the policy’s return under the true MDP. We also characterize the trade-off between the gain and risk of leaving the support of the batch data.
Our algorithm, Model-based Ofﬂine Policy Optimization (MOPO), outperforms standard model-based RL algorithms and prior state-of-the-art model-free ofﬂine
RL algorithms on existing ofﬂine RL benchmarks and two challenging continuous control tasks that require generalizing from data collected for a different task. 1

Introduction
Recent advances in machine learning using deep neural networks have shown signiﬁcant successes in scaling to large realistic datasets, such as ImageNet [13] in computer vision, SQuAD [55] in NLP, and
RoboNet [10] in robot learning. Reinforcement learning (RL) methods, in contrast, struggle to scale to many real-world applications, e.g., autonomous driving [74] and healthcare [22], because they rely on costly online trial-and-error. However, pre-recorded datasets in domains like these can be large and diverse. Hence, designing RL algorithms that can learn from those diverse, static datasets would both enable more practical RL training in the real world and lead to more effective generalization.
While off-policy RL algorithms [43, 27, 20] can in principle utilize previously collected datasets, they perform poorly without online data collection. These failures are generally caused by large extrapolation error when the Q-function is evaluated on out-of-distribution actions [19, 36], which can lead to unstable learning and divergence. Ofﬂine RL methods propose to mitigate bootstrapped error by constraining the learned policy to the behavior policy induced by the dataset [19, 36, 72, 30, 49, 52, 58]. While these methods achieve reasonable performances in some settings, their learning is limited to behaviors within the data manifold. Speciﬁcally, these methods estimate error with respect to out-of-distribution actions, but only consider states that lie within the ofﬂine dataset and do not
∗equal contribution. † equal advising. Orders randomized. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
consider those that are out-of-distribution. We argue that it is important for an ofﬂine RL algorithm to be equipped with the ability to leave the data support to learn a better policy for two reasons: (1) the provided batch dataset is usually sub-optimal in terms of both the states and actions covered by the dataset, and (2) the target task can be different from the tasks performed in the batch data for various reasons, e.g., because data is not available or hard to collect for the target task. Hence, the central question that this work is trying to answer is: can we develop an ofﬂine RL algorithm that generalizes beyond the state and action support of the ofﬂine data?
To approach this question, we ﬁrst hypothesize that model-based RL methods [64, 12, 42, 38, 29, 44] make a natural choice for enabling gener-alization, for a number of reasons. First, model-based RL algorithms effectively receive more supervision, since the model is trained on every transition, even in sparse-reward settings. Sec-ond, they are trained with supervised learning, which provides more stable and less noisy gra-dients than bootstrapping. Lastly, uncertainty estimation techniques, such as bootstrap ensem-bles, are well developed for supervised learning methods [40, 35, 60] and are known to perform poorly for value-based RL methods [72]. All of these attributes have the potential to improve or control generalization. As a proof-of-concept experiment, we evaluate two state-of-the-art off-policy model-based and model-free algorithms,
MBPO [29] and SAC [27], in Figure 1. Although neither method is designed for the batch setting, we ﬁnd that the model-based method and its variant without ensembles show surprisingly large gains. This ﬁnding corroborates our hypothesis, suggesting that model-based methods are particularly well-suited for the batch setting, motivating their use in this paper.
Figure 1: Comparison between vanilla model-based
RL (MBPO [29]) with or without model ensembles and vanilla model-free RL (SAC [27]) on two ofﬂine RL tasks: one from the D4RL benchmark [18] and one that demands out-of-distribution generalization. We ﬁnd that
MBPO substantially outperforms SAC, providing some evidence that model-based approaches are well-suited for batch RL. For experiment details, see Section 5.
Despite these promising preliminary results, we expect signiﬁcant headroom for improvement.
In particular, because ofﬂine model-based algorithms cannot improve the dynamics model using additional experience, we expect that such algorithms require careful use of the model in regions outside of the data support. Quantifying the risk imposed by imperfect dynamics and appropriately trading off that risk with the return is a key ingredient towards building a strong ofﬂine model-based
RL algorithm. To do so, we modify MBPO to incorporate a reward penalty based on an estimate of the model error. Crucially, this estimate is model-dependent, and does not necessarily penalize all out-of-distribution states and actions equally, but rather prescribes penalties based on the estimated magnitude of model error. Further, this estimation is done both on states and actions, allowing generalization to both, in contrast to model-free approaches that only reason about uncertainty with respect to actions.
The primary contribution of this work is an ofﬂine model-based RL algorithm that optimizes a policy in an uncertainty-penalized MDP, where the reward function is penalized by an estimate of the model’s error. Under this new MDP, we theoretically show that we maximize a lower bound of the return in the true MDP, and ﬁnd the optimal trade-off between the return and the risk. Based on our analysis, we develop a practical method that estimates model error using the predicted variance of a learned model, uses this uncertainty estimate as a reward penalty, and trains a policy using
MBPO in this uncertainty-penalized MDP. We empirically compare this approach, model-based ofﬂine policy optimization (MOPO), to both MBPO and existing state-of-the-art model-free ofﬂine
RL algorithms. Our results suggest that MOPO substantially outperforms these prior methods on the ofﬂine RL benchmark D4RL [18] as well as on ofﬂine RL problems where the agent must generalize to out-of-distribution states in order to succeed. 2