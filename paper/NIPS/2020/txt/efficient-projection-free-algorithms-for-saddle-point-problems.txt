Abstract
The Frank-Wolfe algorithm is a classic method for constrained optimization prob-lems. It has recently been popular in many machine learning applications because its projection-free property leads to more efﬁcient iterations. In this paper, we study projection-free algorithms for convex-strongly-concave saddle point problems with complicated constraints. Our method combines Conditional Gradient Sliding with
Mirror-Prox and shows that it only requires ˜O(1/ (cid:15)) gradient evaluations and
˜O(1/(cid:15)2) linear optimizations in the batch setting. We also extend our method to the stochastic setting and propose ﬁrst stochastic projection-free algorithms for saddle point problems. Experimental results demonstrate the effectiveness of our algorithms and verify our theoretical guarantees.
√ 1

Introduction
In this paper, we study the following saddle point problems: min x∈X max y∈Y f (x, y) where the objective function f (x, y) is convex-concave and L-smooth; X and Y are convex and compact sets. Besides this general form, we also consider the stochastic minimax problem: min x∈X max y∈Y f (x, y) (cid:44) Eξ[F (x, y; ξ)], (1) where ξ ∈ Ξ is a random variable. One popular speciﬁc setting of (1) is the ﬁnite-sum case where ξ is i=1. Denoting Fi(x, y) (cid:44) F (x, y; ξi), we can write the objective sampled from a ﬁnite set Ξ = {ξi}n function as f (x, y) (cid:44) 1 n n (cid:88) i=1
Fi(x, y). (2)
We are interested in the cases where the feasible set is complicated such that projecting onto X × Y is rather expensive or even intractable. One example of such case is the nuclear norm ball constraint, which is widely used in machine learning applications such as multiclass classiﬁcation [5], matrix completion [2, 14, 17], factorization machine [20], polynomial neural nets [21] and two-player games whose strategy space contains a large number of constraints [1].
The Frank-Wolfe (FW) algorithm [6] (a.k.a. conditional gradient method) is initially proposed for constrained convex optimization. It has recently become popular in the machine learning community
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
because of its projection-free property [13]. The Frank-Wolfe algorithm calls a linear optimization (LO) oracle at each iteration, which is usually much faster than projection for complicated feasible sets. Recently, FW-style algorithms for convex and nonconvex minimization problems has been widely studied [9, 10, 11, 17, 19, 27, 28, 30, 32, 33, 34]. However, the only known projection-free algorithms for minimax optimization are for very special cases (e.g. the saddle point belongs to the interior of the feasible set [7]).
√
In this paper, we propose a projection-free algorithm, which we refer to as Mirror-Prox Conditional
Gradient Sliding (MPCGS), for convex-strongly-concave saddle point problems. Our method lever-ages the idea from some projection-type methods [23, 31], which is based on proximal point iterations.
By combining the idea of Mirror-Prox [31] with the conditional gradient sliding (CGS) [19], MPCGS only requires at most ˜O(1/ (cid:15)) exact gradient evaluations and ˜O(1/(cid:15)2) linear optimizations to guar-antee O((cid:15)) suboptimality error in expectation. We also extend our framework to the stochastic setting and propose Mirror-Prox Stochastic Conditional Gradient Sliding (MPSCGS), which requires to compute at most ˜O(1/(cid:15)2) stochastic gradients and call the LO oracle for at most ˜O(1/(cid:15)2) times.
To the best of our knowledge, MPSCGS is the ﬁrst stochastic projection-free algorithm for convex-strongly-concave saddle point problems. We also conduct experiments on several real-world data sets for robust optimization problem to validate our theoretical analysis. The empirical results show that the proposed methods outperform previous projection-free and projection-based methods when the feasible set is complicated.