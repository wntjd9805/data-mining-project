Abstract
Reinforcement learning (RL) algorithms update an agent’s parameters according to one of several possible rules, discovered manually through years of research.
Automating the discovery of update rules from data could lead to more efﬁcient algorithms, or algorithms that are better adapted to speciﬁc environments. Although there have been prior attempts at addressing this signiﬁcant scientiﬁc challenge, it remains an open question whether it is feasible to discover alternatives to fundamen-tal concepts of RL such as value functions and temporal-difference learning. This paper introduces a new meta-learning approach that discovers an entire update rule which includes both ‘what to predict’ (e.g. value functions) and ‘how to learn from it’ (e.g. bootstrapping) by interacting with a set of environments. The output of this method is an RL algorithm that we call Learned Policy Gradient (LPG). Empirical results show that our method discovers its own alternative to the concept of value functions. Furthermore it discovers a bootstrapping mechanism to maintain and use its predictions. Surprisingly, when trained solely on toy environments, LPG gen-eralises effectively to complex Atari games and achieves non-trivial performance.
This shows the potential to discover general RL algorithms from data. 1

Introduction
Reinforcement learning (RL) has a clear objective: to maximise expected cumulative rewards (or average rewards), which is simple, yet general enough to capture many aspects of intelligence. Even though the objective of RL is simple, developing efﬁcient algorithms to optimise such objective typically involves a tremendous research effort, from building theories to empirical investigations.
An appealing alternative approach is to automatically discover RL algorithms from data generated by interaction with a set of environments, which can be formulated as a meta-learning problem. Recent work has shown that it is possible to meta-learn a policy update rule when given a value function, and that the resulting update rule can generalise to similar or unseen tasks (see Table 1).
However, it remains an open question whether it is feasible to discover fundamental concepts of RL entirely from scratch. In particular, a deﬁning aspect of RL algorithms is their ability to learn and utilise value functions. Discovering concepts such as value functions requires an understanding of both ‘what to predict’ and ‘how to make use of the prediction’. This is particularly challenging to discover from data because predictions only have an indirect effect on the policy over the course of multiple updates. We hypothesise that a method capable of discovering value functions for itself may also discover other useful concepts, potentially opening up entirely new approaches to RL.
Motivated by the aforementioned open questions, this paper takes a step towards discovering general-purpose RL algorithms. We introduce a meta-learning framework that jointly discovers both ‘what the agent should predict’ and ‘how to use predictions for policy improvement’ from data generated by interacting with a distribution of environments. Our architecture, Learned Policy Gradient (LPG), does not enforce any semantics on the agent’s vector-valued outputs but instead allows the update
∗Corresponding author: junhyuk@google.com 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Algorithm
RL2 [9, 38]
EPG [18]
ML3 [6]
MetaGenRL [19]
Table 1: Methods for discovering RL algorithms.
Train
Discovery Method Generality
Test
N/A
ˆπ
ˆπ
ˆπ
∇
ES
∇∇
∇∇
Domain-speciﬁc 3D maze
Domain-speciﬁc MuJoCo
Domain-speciﬁc MuJoCo
General
Similar 3D maze
Similar MuJoCo
Similar MuJoCo
MuJoCo Unseen MuJoCo
LPG
ˆπ: policy update rule, ˆy: prediction update rule (i.e., semantics of agent’s prediction).
∇: gradient descent, ∇∇: meta-gradient descent, ES: evolutionary strategy.
General
Toy
∇∇
ˆπ, ˆy
Unseen Atari rule (i.e., the meta-learner) to decide what this vector should be predicting. We then propose a meta-learning framework to discover such update rule from multiple learning agents, each of which interacts with a different environment.
Experimental results show that our algorithm can discover useful functions, and use those functions effectively to update the agents policy. Furthermore, empirical analysis shows that the discovered functions converge towards an encoding of a notion of value function, and furthermore maintain this value function via a form of bootstrapping. We also evaluated the ability of the discovered
RL algorithm to generalise to new environments. Surprisingly, even though the update rule was discovered solely from interactions with a very small set of toy environments, it was able to generalise to a number of complex Atari games [3], as shown in Figure 9. To our knowledge, this is the ﬁrst to show that it is possible to discover an entire update rule, and that the update rule discovered from toy domains can be competitive with human-designed algorithms on a challenging benchmark. 2