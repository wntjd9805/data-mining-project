Abstract
For many initialization schemes, parameters of two randomly initialized deep neural networks (DNNs) can be quite different, but feature distributions of the hidden nodes are similar at each layer. With the help of a new technique called neural network grafting, we demonstrate that even during the entire training process, feature distributions of differently initialized networks remain similar at each layer.
In this paper, we present an explanation of this phenomenon. Speciﬁcally, we consider the loss landscape of an overparameterized convolutional neural network (CNN) in the continuous limit, where the numbers of channels/hidden nodes in the hidden layers go to inﬁnity. Although the landscape of the overparameterized
CNN is still non-convex with respect to the trainable parameters, we show that very surprisingly, it can be reformulated as a convex function with respect to the feature distributions in the hidden layers. Therefore by reparameterizing neural networks in terms of feature distributions, we obtain a much simpler characterization of the landscape of overparameterized CNNs. We further argue that training with respect to network parameters leads to a ﬁxed trajectory in the feature distributions. 1

Introduction
A good characterization of the landscape is very important for understanding remarkable successes of
DNNs in various domains including computer vision, nature language processing and speech. There have been a lot of research [29, 21, 24, 20, 26, 8] analyzing the landscape of deep neural network in parameter space. On one hand, even one-hidden-layer neural network has complex landscape with exponential number of local minima and saddle points. On the other hand, some interesting empirical phenomena like mode connectivity, that low-cost solutions of DNN could be connected by simple path in parameter space, have been found, indicating that the loss landscape is not as complex as we expected. These two conclusions of the landscape in parameter space seem to be contradictory.
We believe that the loss landscape should be investigated in the feature distribution space directly instead of the parameter space as existing studies have done due to the excellent capability of
DNN to learn effective feature representations. But our understanding on the feature distribution learned by DNNs is still limited. For example, even to the basic question of to what extent two feature representations learned by different DNNs are essentially the same, various studies such as [23, 32, 19] reached different conclusions due to the redundancy in the feature representations comes from the special properties of DNNs, e.g., the permutation & scale invariance of the trainable parameters, the truncation operation in ReLU, etc. Two feature representations that look quite different at current layer could be essentially the same. As the example illustrated in Fig. 1(b), the difference may have little effect on the features in subsequent layers and could be alleviated, or even eliminated completely after passing the linear transformations and the activation functions ReLU
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (a) The ﬂowchart of neural network grafting. (b) An example of redundancy. The yellow and green curves left represent the features in layer (cid:96) + 1 learned in two nets. They look different but are essential similar since they become the same after passing two more layers. (a) (b) in the subsequent layers. Although several remedies are proposed in recent studies, the redundancy cannot be completely eliminated and different amounts of redundancy may lead to inconsistent conclusions. This discourages the researchers from investigating DNNs in feature distribution space.
In this paper, we start from the problem of inconsistent conclusions above and propose an effective technique named neural network grafting (NNG) for comparing feature distributions (see Section 4).
We consider the most direct way to identify whether the feature representations learned in two hidden layers of two networks are essentially the same. The key idea is to check whether the representation learned by one network can be used in the other one without signiﬁcant sacriﬁce in accuracy, i.e., whether one of these two networks can be grafted onto the other at that hidden layer (Fig. 1(a)).
Using NNG, we ﬁnd that feature distributions learned by two networks with the same architecture but different initilizations are almost the same during the whole training process. That means their solution paths, if we view them in the aspect of feature distributions, are the same, while they seem chaotic and quite different under the view of trainable parameters (see Section 6.1).
Our surprising ﬁnding on the uniqueness of the solution path implies that NNs become much easier to understand if we view them in the aspect of feature distributions instead of trainable parameters.
This motivates us to reparameterize NNs with respect to the feature function distributions to obtain a simpler loss landscape. Speciﬁcally, we ﬁrst propose a new method to reformulate a CNN as an approximation to a continuous CNN, which is obtained by letting the number of channels/hidden nodes in each hidden layer go to inﬁnity. We then extend the emerging technique for reformulating fully connected NNs [4, 5] to continuous CNN to reparameterize it with respect to feature distributions (See Section 5). We show that although the loss of continuous CNN is still non-convex with respect to the trainable parameters, it becomes convex with respect to the feature distributions. Therefore, we obtain a much simper characterization for the loss landscape.
Although our convexity is reformulated in the feature distribution space instead of the trainable parameter space, it has signiﬁcant implications on NN optimization. In fact, it can be shown that under suitable conditions, the training algorithms (e.g., SGD) of DNNs converge to a solution that is a stationary point of the convex reformulation, as shown in Section B of the Appendix. This excludes bad local minimum solutions for DNN optimization. Moreover, the convexity and unique solution path imply that CNNs are much simpler if we view them in the feature distribution space. All these demonstrate the value of studying DNNs in feature distribution space.
Notations. For a positive integer n, we let [n] to be the set {1, 2, . . . , n}. We denote (cid:107)x(cid:107)1 and (cid:107)x(cid:107) to be the (cid:96)1 and (cid:96)2 norm of a vector x in Rd. For any k ∈ [d], we let xk be the value of the k-th dimension of x and denote vec(w) to be the operator for reshaping a matrix or tensor w into a vector. 2