Abstract
Cross-domain disentanglement is the problem of learning representations parti-tioned into domain-invariant and domain-speciﬁc representations, which is a key to successful domain transfer or measuring semantic distance between two do-mains. Grounded in information theory, we cast the simultaneous learning of domain-invariant and domain-speciﬁc representations as a joint objective of multi-ple information constraints, which does not require adversarial training or gradient reversal layers. We derive a tractable bound of the objective and propose a gener-ative model named Interaction Information Auto-Encoder (IIAE). Our approach reveals insights on the desirable representation for cross-domain disentanglement and its connection to Variational Auto-Encoder (VAE). We demonstrate the validity of our model in the image-to-image translation and the cross-domain retrieval tasks.
We further show that our model achieves the state-of-the-art performance in the zero-shot sketch based image retrieval task, even without external knowledge.

Introduction 1
There have been great interests in learning disentangled representation for various purposes, such as identifying sources of variation [3, 4, 15, 18, 20] for interpretability, obtaining representation invariant to nuisance factors [1, 8, 30, 34, 39, 40], and domain transfer [12, 26, 28, 35, 44, 47]. In particular, the cross-domain disentanglement problem [12] assumes the dataset composed of paired samples (x ∈ X, y ∈ Y ) where every sample has some shared information. The problem requires a model to learn a representation explicitly separated into three parts: domain-invariant representation shared across two data domains and domain-speciﬁc representations exclusive to each domain. This task is challenging since those representations must be (1) disentangled so that they are independent to one another, while (2) informative in such a way that every factor of variation is captured in the right part of the representation.
In recent studies, many models have been proposed to tackle important tasks related to cross-domain disentanglement, such as image-to-image translation [12, 26, 28, 35, 44] and Zero-Shot Sketch Based
Image Retrieval (ZS-SBIR) [6, 9, 22, 23, 27, 38]. Although those models perform reasonably well with realistic datasets, most of them take a heuristic combination of techniques that regularize the latent space, such as cycle consistency loss [46], cross-reconstruction loss [27], adversarial training
[14, 43], and Gradient Reversal Layer (GRL) [10]. Consequently, it is not obvious to interpret each module or identify key factors that contribute to disentanglement in their models.
In this paper, we address the cross-domain disentanglement problem with a novel principle based on information theory. Speciﬁcally, we train a generative model named Interaction Information Auto-Encoder (IIAE) whose representations are enforced to be informative but disentangled by information regularization terms that we will describe shortly. Leveraging representations learned by IIAE, we show that image manipulation tasks such as image translation and synthesis can be done in ﬁne details. Furthermore, we demonstrate that IIAE outperforms Generative Adversarial Network (GAN)
[13] based models in the cross-domain retrieval task. Lastly, we empirically show that our model 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
outperforms the state-of-the-art models for ZS-SBIR which strongly depend on external knowledge such as word embedding of class labels. Our contributions are three-fold: 1. We propose a novel information-theoretic framework to learn and disentangle shared and exclusive representations and derive a tractable lower bound of the optimization objective. 2. By bridging the lower bound of the objective and the Evidence Lower Bound (ELBO), we introduce IIAE, a simple and interpretable generative model trained by maximizing the lower bound. 3. The performance of IIAE are demonstrated on an extensive set of tasks, such as cross-domain image translation, cross-domain retrieval, and ZS-SBIR. 2 Method
Consider a set of paired data sampled from an unknown joint distribution (x, y) ∼ pD(x, y), where each element of a pair x ∈ X and y ∈ Y is extracted from different domains X and Y , respectively.
We assume that two domains exhibit domain-speciﬁc factors of variations while sharing some common factors of variations. For instance, x and y can be images in different styles (e.g., sketch and photo) sharing the same semantic content, or images of different content (e.g., different types of car) sharing the same factors of variation (e.g., rotation and scale).
Given this data, the goal of cross-domain disentanglement is to ﬁnd the structured representation that can be factorized into three parts: domain-speciﬁc representations Z X and Z Y that capture the distinctive and exclusive characteristics of each domain X and Y , respectively, and the shared representation Z S that captures common factors shared across the domains. Figure 1a describes our graphical model encoding this structure.
A typical way to learn a latent variable model is maximizing the marginal likelihood [21]. In our problem, we maximize the marginal likelihood of the joint distribution of X and Y : (cid:90) pθ(x, y) = dzxdzsdzypθX (x|zx, zs)pθY (y|zy, zs)p(zx)p(zs)p(zy), (1) where θ = {θX , θY } denotes the parameter modeling the conditional distributions. Our objective is then training the generative model pθ(x, y) that not only maximizes the joint distribution pD(x, y) by optimizing θ, but also disentangles the exclusive representations Z X and Z Y from the shared representation Z S. Below, we describe our approach to optimize the Eq. (1) while enforcing the disentanglement constraints on the latent representations. 2.1 Generative model for the joint distribution pD(x, y)
Since the direct optimization of Eq. (1) is intractable, we employ variational inference based on
Variational Auto-Encoder (VAE) [21]. Speciﬁcally, we approximate the true posterior distribu-tion pθ(zx, zs, zy|x, y) using the approximated posterior qφ(zx, zs, zy|x, y), which is factorized according to the graphical model in Figure 1b as follows: qφ(zx, zs, zy|x, y) = qφX (zx|x)qφS (zs|x, y)qφY (zy|y), (2) where qφX and qφY are encoders for domain-speciﬁc latent variable Z X and Z Y , respectively, qφS is the encoder for the shared latent variable Z S, and φ = {φX , φS, φY } is the encoder parameter. In the following, we omit subscripts θ and φ for brevity. Using the Eq. (2), we can derive the ELBO of
Eq. (1) as follow (see A.1 in the supplementary material for the derivation): (cid:20) (cid:21) log p(x, y) ≥ Eq(zx,zs,zy|x,y) log p(x, y, zx, zs, zy) q(zx, zs, zy|x, y)
= Eq(zx|x)q(zs|x,y) [log p(x|zx, zs)] + Eq(zy|y)q(zs|x,y) [log p(y|zy, zs)]
− DKL [q(zx|x)(cid:107)p(zx)] − DKL [q(zy|y)(cid:107)p(zy)]
− DKL [q(zs|x, y)(cid:107)p(zs)] . (3) (4)
Unfortunately, maximizing the ELBO does not necessarily encourage the structured representations.
This is mainly because we have no control over the assignment of the generative factors to representa-tions learned by three different encoders q(zx|x), q(zs|x, y), and q(zy|y). Speciﬁcally, the following desiderata of the cross-domain disentanglement should be reﬂected in the objective: 2
(a) Generative model pθ (b) Approximate inference model qφ
Figure 1: Graphical models for cross-domain disentanglement. 1. Disentanglement of Z X , Z Y and Z S: the generative factors learned by Z X , Z Y and Z S should be mutually exclusive to each other to avoid encoding redundant information. 2. Decomposition of domain-speciﬁc and shared representations: the generative factors exclusively presented in each domain should be captured by Z X and Z Y , while the rest of factors shared across the domains should be encoded in Z S.
To guide the model to learn desirable latent representations that satisfy the above properties, we propose to introduce regularizations on q motivated by information theory, which are described below. 2.2
Information regularization on q for cross-domain disentanglement
Enforcing disentanglement Desirable shared and exclusive representations must be disentangled so that none of factors of variation is shared across any representations. Thus, we introduce reg-ularizations that minimize the mutual information I(Z X ; Z S) and I(Z Y ; Z S) so that exclusive representations are statistically independent to shared representation, and vice versa. Here we only present our formulation for domain X, as the one for domain Y is analogous.
To gain better insights on how minimizing the mutual information impacts the disentanglement, we rewrite I(Z X ; Z S) as follows (see A.2 in the supplementary for details):
I(Z X ; Z S) = −I(X; Z X , Z S) + I(X; Z X ) + I(X; Z S). (5)
Surprisingly, Eq. (5) implies that minimizing the mutual information of Z X and Z S encourages them to be jointly informative to domain X (the ﬁrst term in RHS). Since the last two terms will penalize the total amount of information in Z X and Z S, minimizing Eq. (5) will naturally encourage Z S and
Z X to encode the mutually exclusive information of domain X.
However, we also notice that minimization of Eq. (5) does not enforce any constraints on separation of domain-speciﬁc and domain-invariant representation to Z X and Z S; any arbitrary mutually exclusive factorization will be equally preferred, even those with no information captured in Z S. It motivates us to introduce additional regularization to enforce a proper disentanglement on domain-speciﬁc and shared information.
Enforcing decomposition To encourage decomposition of domain-speciﬁc and shared represen-tation, we introduce a regularization on the shared latent variable Z S. Speciﬁcally, we encourage
Z S to capture the shared information across domains, which is enforced based on interaction information [31] (also known as co-information [2]).
Interaction information is a generalization of mutual information among three or more random variables, and quantiﬁes the amount of shared information among them. Speciﬁcally, we deﬁne the interaction information among two domains X, Y and the shared representation Z S as follows:
I(X; Y ; Z S) = I (cid:0)X; Z S(cid:1) − I (cid:0)X; Z S|Y (cid:1)
= I (cid:0)Y ; Z S(cid:1) − I (cid:0)Y ; Z S|X(cid:1) , (6) (7) where the equality in Eq. (7) holds due to symmetry. The above equations show how maximizing interaction information encourages Z S to encode the shared information. For instance, in Eq. (6), the
ﬁrst term in RHS is maximized when Z S becomes informative to X, while the second term will be minimized if such information in Z S can be also inferred from Y ; the combination of both terms will naturally make Z S to encode information shared between X and Y .
Joint regularization Our ﬁnal regularization on cross-domain disentanglement is obtained by combining regularizations on disentanglement and decomposition. To make analysis easier, we ﬁrst present the objective with respect to domain X and show the complete one on both domains later. 3
Combining Eq. (5) and (6), our preference for q on domain X (the negative of regularization) becomes max q
I(X; Y ; Z S) − I(Z X ; Z S)
= (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
I(X; Z S) − I(X; Z S|Y ) (cid:124) (cid:123)(cid:122) (cid:125)
I(X;Y ;ZS )
+ I(X; Z X , Z S) − I(X; Z X ) − (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)
I(X; Z S) (cid:123)(cid:122) (cid:125)
−I(ZX ;ZS ) (cid:124)
= I(X; Z X , Z S) − I(X; Z X ) − I(X; Z S|Y ). (8)
Optimization Direct optimization of Eq. (8) is intractable since each term involves several in-tractable integrals. The details are in A.3 in the supplementary material. q(zx,zs|x)pD(x)
The ﬁrst term I(X; Z X , Z S) in Eq. (8) is intractable since q(x|zx, zs) = (cid:82) pD(x,y) q(zx,zs|x,y) dxdy involves intractable integral (unknown pD(x, y) and pD(x)). Thus, we derive its lower bound with the generative distribution p(x|zx, zs) as follows:
I(X; Z X , Z S) = Eq(zx,zs|x)pD(x) (cid:20) log (cid:21) q(x|zx, zs) pD(x)
= H(X) + Eq(zx,zs|x)pD(x) [log p(x|zx, zs)] + Eq(zx,zs) [DKL [q(x|zx, zs)(cid:107)p(x|zx, zs)]]
≥ H(X) + Eq(zx,zs|x)pD(x) [log p(x|zx, zs)]
= H(X) + EpD(x,y) q(zx|x) q(zs|x,y) [log p(x|zx, zs)] . (9)
Note that maximization of Eq. (9) not only maximizes I(X; Z X , Z S) but also ﬁts p(x|zx, zs) to q(x|zx, zs) so that we can utilize it as a decoder.
The second term −I(X; Z X ) is intractable since q(zx) = (cid:82) pD(x)q(zs|x) dx is intractable (un-known distribution pD(x)). We use −EpD(x) [DKL [q(zx|x)(cid:107)p(zx)]] as its lower bound with the generative distribution p(zx) deﬁned as the standard Gaussian, which is also known as the Variational
Information Bottleneck (VIB) [1].
The last term is also intractable because q(zs|y) = (cid:82) pD(x|y)q(zs|x, y)dx is intractable (unknown pD(x|y)). Similar to VIB, we use variational distribution ry(zs|y) to maximize its lower bound:
−I(X; Z S|Y ) = −EpD(x,y)q(zs|x,y)
= −EpD(x,y)q(zs|x,y) (cid:20) log (cid:20) log (cid:21) q(zs|x, y) q(zs|y) q(zs|x, y)ry(zs|y) ry(zs|y)q(zs|y) (cid:21)
= −EpD(x,y) [DKL [q(zs|x, y)(cid:107)ry(zs|y)]] + EpD(y) [DKL [q(zs|y)(cid:107)ry(zs|y)]]
≥ −EpD(x,y) [DKL [q(zs|x, y)(cid:107)ry(zs|y)]] . (10)
Thus, the maximization of Eq. (10) not only minimizes I(X; Z S|Y ) but also ﬁts ry(zs|y) to q(zs|y).
Putting together, we are ready to derive the lower bound of the preference for q on domain X and Y : (I(X; Y ; Z S) − I(Z X ; Z S)) + (I(X; Y ; Z S) − I(Z Y ; Z S))
= 2 · I(X; Y ; Z S) − I(Z X ; Z S) − I(Z Y ; Z S)
= I(X; Z X , Z S) + I(Y ; Z Y , Z S) − I(X; Z X ) − I(Y ; Z Y ) − I(X; Z S|Y ) − I(Y ; Z S|X) (cid:2) Eq(zs|x,y)q(zx|x) [log p(x|zx, zs)] + Eq(zs|x,y)q(zy|y) [log p(y|zy, zs)] (cid:3)
≥ EpD(x,y)
− EpD(x,y) [ DKL [q(zx|x)(cid:107)p(zx)] + DKL [q(zy|y)(cid:107)p(zy)] ]
− EpD(x,y) [ DKL [q(zs|x, y)(cid:107)ry(zs|y)] + DKL [q(zs|x, y)(cid:107)rx(zs|x)] ]
+ H(X) + H(Y ). (11)
Surprisingly, many of the terms are also present in the ELBO. Thus, when we add the above lower bound to the ELBO objective to perform joint optimization, many of the terms above are obtained with very little additional cost by sharing parameters and computations, which we describe below. 2.3
Interaction Information Auto-Encoder
Our goal is to learn a latent variable model with maximum likelihood objective (ELBO in Eq. (3)) under the the information regularization for cross-domain disentanglement (Eq. (11)). Due to the 4
Figure 2: The architecture of Interaction Information Auto-Encoder. difﬁculties in the constrained optimization, we relax this problem as a joint maximization problem similar to [34], which we name Interaction Information Auto-Encoder (IIAE) shown in ﬁgure 2, as follows (see A.3 in the supplementary material for details): (cid:20)
Eq(zx,zs,zy,x,y) p(x, y, zx, zs, zy) q(zx, zs, zy|x, y) (1 + λ) · EpD(x,y) [ ELBO(p, q) ] log max p,q
≥ max p,q,r (cid:21)
+ λ (cid:0)2 · I(X; Y ; Z S) − I(Z X ; Z S) − I(Z Y ; Z S)(cid:1)
+ λ · EpD(x,y) [ DKL [q(zs|x, y)(cid:107)p(zs)] ]
− λ · EpD(x,y) [ DKL [q(zs|x, y)(cid:107)ry(zs|y)] + DKL [q(zs|x, y)(cid:107)rx(zs|x)] ] . (12) (13)
This objective is essentially augmenting the ELBO with Eq. (12) and Eq. (13), which trades off the overall amount of information captured by the shared representation with that from the domain-speciﬁc information, by factor λ. This augmented term encourages the shared representation to exclude domain-speciﬁc factors of variation. Finally, note that Eq. (13) yields variational encoders rx(zs|x) and ry(zs|y) as byproducts of optimization, which is useful for many tasks such as image translation and retrieval where we need to extract the shared representation zs only from x or y. 3