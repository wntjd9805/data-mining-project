Abstract
The recent advances in image generation have been achieved by style-based image generators. Such approaches learn to disentangle latent factors in different image scales and encode latent factors as “style” to control image synthesis. However, existing approaches cannot further disentangle ﬁne-grained semantics from each other, which are often conveyed from feature channels. In this paper, we propose a novel image synthesis approach by learning Semantic-aware relative importance for feature channels in Generative Adversarial Networks (SariGAN). Such a model disentangles latent factors according to the semantic of feature channels by channel-/group- wise fusion of latent codes and feature channels. Particularly, we learn to cluster feature channels by semantics and propose an adaptive group-wise
Normalization (AdaGN) to independently control the styles of different channel groups. For example, we can adjust the statistics of channel groups for a human face to control the open and close of the mouth, while keeping other facial features unchanged. We propose to use adversarial training, a channel grouping loss, and a mutual information loss for joint optimization, which not only enables high-ﬁdelity image synthesis but leads to superior interpretable properties. Extensive experiments show that our approach outperforms the SOTA style-based approaches in both unconditional image generation and conditional image inpainting tasks. 1

Introduction
Image generation has achieved signiﬁcant progress in recent years as generative adversarial networks (GAN) [1] attract a lot of attention and develop rapidly [2, 3, 4, 5, 6, 7, 8, 9, 10]. Notably, the recent success of GANs takes advantage of progressive growing for the generator. Such architectures make it possible to learn disentangled image styles from different spatial resolutions in the generator
[2, 4, 11, 12], e.g., styles for global structures in low-resolution layers and styles for local details in high-resolution layers. Particularly, StyleGAN embeds latent codes into different spatial resolutions to control the scale-aware image styles during synthesizing [2, 12]. For example, StyleGAN is able to change the color schema of a generated human face by adjusting the style codes in high-resolution layers, while keeping the style of the global structure synthesized in low-resolution layers unchanged.
Through such a scale-aware disentangled representation learning, StyleGAN yields state-of-the-art results for high-resolution image generation.
∗This work was performed when Heliang Zheng and Yanhong Zeng were visiting Microsoft Research as research interns.
†corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: An example to show that semantic disentangling helps to generate more realistic images.
For the image generated by (a) StyleGAN2, artifacts can be observed around the eyes, and the background is also unnatural. (b) Our model can remove such artifacts by learning to disentangle semantics (e.g., eyes and glasses).
However, the disentanglement in StyleGAN is limited, which makes it hard to further disentangle
ﬁne-grained semantics. Speciﬁcally, StyleGAN tends to change the image styles controlled by layers with different spatial resolutions, while it is difﬁcult to change a speciﬁc style in the same layer.
Inspired by the recent study on interpretable learning that different feature channels are closely related to different semantics [13, 14] and feature channels with the same semantics can be grouped together by their similarity [15, 16], we propose to further disentangle ﬁne-grained semantics from each other by leveraging the semantics of feature channels. Speciﬁcally, after grouping feature channels into different groups by semantics, we propose to control the styles of different semantics independently.
To enable such a ﬁne-grained semantic disentanglement, we propose to learn semantic-aware relative importance for channel groups in GAN (SariGAN) from deep image features. First, we design a similarity-based grouping module (SGM) to cluster channels with the same semantics together according to their similarity. The SGM separates different semantics from each other, which enables independent control for different semantic groups in subsequent style embedding operations. Second, we embed the input latent code into semantic-aware intermediate latent space (i.e., intra-group and inter-group) by learning a mapping network. Speciﬁcally, the intra-group codes modify the relative importance of features within each group for the subsequent convolution operation via an AdaIN operation. Meanwhile, the inter-group codes are used to control the relative importance of different groups by the proposed adaptive group-wise Normalization (AdaGN). Such channel-/group- wise fusions integrate the semantic group information into latent space and enables semantic disentangling for latent factors. Finally, the full model is jointly optimized by a non-saturating logistic GAN loss, a channel grouping loss and a mutual information loss [17]. Through such a design, the proposed
SariGAN is able to disentangle ﬁne-grained semantics and control the styles of speciﬁc semantics during generation.
In summary, our main contribution is to propose learning semantic-aware relative importance for
ﬁne-grained semantic disentanglement in GAN. We conduct both quantitative comparisons and quali-tative analysis on several unconditional image generation benchmarks. Experimental results show that SariGAN can not only realize high-ﬁdelity image synthesis but also has superior interpretabil-ity. Moreover, extensive experiments on conditional image inpainting also show improvements in generalization for SariGAN. 2