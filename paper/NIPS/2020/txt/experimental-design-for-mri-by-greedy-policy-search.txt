Abstract
In today’s clinical practice, magnetic resonance imaging (MRI) is routinely ac-celerated through subsampling of the associated Fourier domain. Currently, the construction of these subsampling strategies - known as experimental design - relies primarily on heuristics. We propose to learn experimental design strategies for accelerated MRI with policy gradient methods. Unexpectedly, our experiments show that a simple greedy approximation of the objective leads to solutions nearly on-par with the more general non-greedy approach. We offer a partial explanation for this phenomenon rooted in greater variance in the non-greedy objective’s gra-dient estimates, and experimentally verify that this variance hampers non-greedy models in adapting their policies to individual MR images. We empirically show that this adaptivity is key to improving subsampling designs. 1

Introduction
Magnetic resonance imaging (MRI) is a non-invasive medical imaging technique with a wide range of diagnostic applications. However, long acquisition times during the imaging process limit patient comfort, throughput, and imaging quality (for instance due to patient movement) [47]. Reducing imaging times has been an active ﬁeld of research for the past ﬁfty years. A potential avenue for tackling this problem is to acquire less measurement data during a scan, linearly reducing acquisition times: this is often referred to as accelerated MRI [47].
Measurements in MR imaging are performed in the frequency domain, also known as k-space. These measurements are transformed (reconstructed) into the familiar MR images through the inverse
Fourier transform. Accelerating MRI - reducing data acquisition - amounts to subsampling the k-space, which due to the Nyquist-Shannon sampling theorem will introduce aliasing artefacts in naive reconstructions. The presence of such artefacts renders the resulting images unusable for diagnostic purposes [47]: in order to improve image quality, additional information must be included in the reconstruction process. In clinical settings today, this is typically done using compressed sensing (CS) techniques [26, 6, 8]. With the rise of deep learning (DL), some successes have also been seen using deep reconstruction networks to obtain diagnostic quality images from more aggressively subsampled k-space [18, 25, 16, 34, 14, 43, 30, 29, 40]. The additional information utilised is implicitly learned from training data. Such neural networks are trained by applying predetermined subsampling masks to k-space: from this masked frequency domain the model then learns to reconstruct target images obtained from the fully sampled k-space [47]. In these CS and DL settings, subsampling masks are typically determined beforehand: either carefully crafted by experts, or based on heuristics [19].
A natural next step is a move away from handcrafted subsampling masks towards learned acquisition strategies. The process of choosing an optimal set of measurements is known as experimental design
[37]. While such design methods can be employed to learn a ﬁxed subsampling mask for a data set
[15, 2, 45, 11, 12, 32], an ostensively more salient approach is to learn an adaptive strategy that has the ability to propose different masks for different MR images (e.g. various patients or locations).
Intuitively, such methods should outperform their non-adaptive counterparts on reconstruction quality given the same measurement budget. In the DL literature two trends are noticeable. The ﬁrst learns 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Rec(cid:82)(cid:81)(cid:86)(cid:87)(cid:85)(cid:88)c(cid:87)ed high-(cid:85)e(cid:86)(cid:82)(cid:79)(cid:88)(cid:87)i(cid:82)(cid:81) MR i(cid:80)age
Recon(cid:86)(cid:87)(cid:85)(cid:88)c(cid:87)ion  me(cid:87)hod
Polic(cid:92) ne(cid:87)(cid:90)o(cid:85)k
L(cid:82)(cid:90)-(cid:85)e(cid:86)(cid:82)(cid:79)(cid:88)(cid:87)i(cid:82)(cid:81)
MR i(cid:80)age
SSIM (cid:85)e(cid:90)a(cid:85)d
G(cid:85)adie(cid:81)(cid:87)
In(cid:89)e(cid:85)(cid:86)e F(cid:82)(cid:88)(cid:85)ie(cid:85) (cid:87)(cid:85)an(cid:86)f(cid:82)(cid:85)m
S(cid:88)b(cid:86)a(cid:80)(cid:83)(cid:79)ed (cid:78)-(cid:86)(cid:83)ace
G(cid:85)(cid:82)(cid:88)(cid:81)d (cid:87)(cid:85)(cid:88)(cid:87)h
Sam(cid:83)ling (cid:83)olic(cid:92)
Ac(cid:84)(cid:88)i(cid:85)e a k-(cid:86)(cid:83)ace mea(cid:86)(cid:88)(cid:85)emen(cid:87) f(cid:85)(cid:82)m g(cid:85)(cid:82)(cid:88)nd (cid:87)(cid:85)(cid:88)(cid:87)h
Figure 1: The iterative acquisition procedure. An initial subsampling of k-space is obtained from the ground truth image. The subsampled frequency domain is fed into a reconstruction method, which for neural network-based reconstructions typically starts with an inverse Fourier transform, such that the input and output domain match. This intermediate step results in a low-resolution, so-called zero-ﬁlled reconstruction [18]. The high-resolution reconstructed MR image is input to the policy network, which outputs a discrete probability distribution that represents the suggested sampling policy. An action is sampled from this policy, corresponding to a measurement of k-space. This measurement is simulated from the ground truth MR image, and the procedure is repeated until the acquisition budget is exhausted. The reward of an acquisition step is given by the improvement in
SSIM of the ground truth and reconstruction resulting from that acquisition. to sequentially acquire measurements in k-space until some budget is exhausted. These adaptive acquisition methods are trained separately [28] or jointly [19] with the reconstruction model, based on some heuristic [33], or a combination of these [48]. The second type of approach involves jointly learning an optimal subsampling and reconstruction by parameterising the mask itself [45, 2, 15].
These directly learned masks are typically not adaptive.
Here, we frame ﬁnding an optimal subsampling mask as a reinforcement learning (RL) problem.
Our approach - like [19] and (concurrently) [28] - employs a separate reconstruction and policy model. The reconstruction model performs reconstructions from k-space measurements suggested by the policy model. The policy model suggests which measurement to make, based on the current reconstruction. The reward of each of these measure-reconstruct cycles is given as the improvement in reconstruction quality provided by some appropriate metric. The reinforcement learning perspective additionally allows for an analysis of a second split in the literature: greedy and non-greedy models.
When optimising an MRI subsampling mask - given some budget - a natural question to ask is whether direct optimisation of the long-term reconstruction quality enjoys strong advantages over a greedy optimisation that acquires the measurement that leads to the greatest immediate improvement.
In the literature, proposed models are often compared to compressed sensing baselines, but rarely to other DL methods due to an historical lack of standardised datasets and evaluation metrics.
This limits MRI experimental design research, as it is unclear how different families of methods (e.g. adaptive/non-adaptive, greedy/non-greedy) compare, and thus which directions are the most promising future research targets. We attempt to bridge some of this gap here, in the hope to guide future research into experimental design for MRI. In this paper, we provide a proof of concept showing that direct policy gradient methods can be used to learn experimental design for MRI. We construct both greedy and a non-greedy models using the above-mentioned RL framework. The greedy model is quicker to train and outperforms some of its non-greedy counterparts in our experiments. We hypothesise that the underperformance of non-greedy models is partially due to weaker adaptivity to individual MR slices as a consequence of greater variance in the gradient estimates, and we provide experimental evidence for this hypothesis.1 1Code is available at: https://github.com/Timsey/pg_mri. 2
2