Abstract
With the success of language pretraining, it is highly desirable to develop more efﬁcient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efﬁciency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intu-ition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives,
Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer
FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classiﬁcation, language understanding, and reading comprehension.1 1

Introduction
With the recent success of unsupervised language pretraining [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], the power of neural self-attention models (a.k.a. Transformer) [13] has been pushed to a new level, leading to dramatic advancements in machine learning and natural language processing (NLP). More importantly, it has been observed that with more FLOPs invested in longer pretraining and/or larger models, the performance of pretrained Transformer models consistently improve. However, it is extremely expensive to pretrain or even just to ﬁnetune the state-of-the-art self-attention models, as they require much more FLOPs and memory resources compared to traditional models in NLP. This largely limits their applications and success in more ﬁelds.
Given this challenge, there has been an increasing amount of efforts to reduce the costs of pretraining and ﬁnetuning self-attention models. From the perspective of post-pretraining processing, typical approaches include distillation, pruning and quantization of various kinds, which try to derive a lighter model from an well-pretrained model by taking advantage of the richer signals in the larger model or learning to remove less important operations. Another line of research aims at designing an architecture that not only has a lower resource-to-performance ratio (more efﬁcient) but also scales as well as the Transformer, at least in certain domains. Most of such methods build upon the Transformer backbone and focus on redesigning its building blocks. Representative solutions include searching for better micro operation or macro module designs [14, 15], replacing the full pairwise attention with local operations such as convolution [16] and dynamic convolution [17], and optimizing the hidden size combinations for existing blocks [18].
∗Equal contribution. 1The code and pretrained checkpoints are available at github.com/laiguokun/Funnel-Transformer. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Across the wide variety of ideas mentioned above, a common strategy is to identify redundant operations or representations and replace them with more efﬁcient ones. Inspired by this line of thinking, in this work, we will be focusing on the potential redundancy induced by always maintaining a full-length sequence of hidden representations across all layers in Transformer. Intuitively, for many sequence-level NLP tasks such as text classiﬁcation and ranking, the most common use case is to extract a single vector from the entire sequence, which does not necessarily preserve all information down to the token-level granularity. Hence, for such tasks, the full-length sequence of hidden states may contain signiﬁcant redundancy. This is analogous to the case of image recognition, where the convolution neural network gradually reduces the spatial resolution/size of feature maps as the neural network goes deeper. In addition, linguistic prior also encourages gradually merging nearby tokens (words) into larger semantic units (phrases), which naturally leads to a shorter sequence of representations.
Concretely, we propose to gradually reduce the sequential resolution (i.e. length) of the hidden representation in self-attention models. Immediately, the reduction in sequence length can lead to signiﬁcant savings in both FLOPs and memory. More importantly, the saved computational resource can be directly re-invested in constructing a deeper (or wider) model to boost the model capacity without additional computational burden. In addition, to address the challenge that common pretraining objectives such as masked language modeling (MLM) [2] require separate representations for each token, we design a simple strategy to decode a full-length sequence of deep representations from the hidden state of reduced length. As a result, the proposed model can be directly trained without modifying the pretraining objectives, as well as adopted for downstream tasks that require token-level representations.
Empirically, with comparable or even fewer FLOPs, by trading sequential resolution for depth, our proposed model achieves an improved performance over the standard Transformer on a wide variety of sequence-level prediction tasks, including text classiﬁcation, language understanding, and reading comprehension. 2 Method 2.1