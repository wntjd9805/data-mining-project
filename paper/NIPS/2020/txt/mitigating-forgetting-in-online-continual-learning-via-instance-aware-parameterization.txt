Abstract
Online continual learning is a challenging scenario where a model needs to learn from a continuous stream of data without revisiting any previously encountered data instances. The phenomenon of catastrophic forgetting is worsened since the model should not only address the forgetting at the task-level but also at the data instance-level within the same task. To mitigate this, we leverage the concept of
"instance awareness" in the neural network, where each data instance is classiﬁed by a path in the network searched by the controller from a meta-graph. To preserve the knowledge we learn from previous instances, we proposed a method to protect the path by restricting the gradient updates of one instance from overriding past updates calculated from previous instances if these instances are not similar. On the other hand, it also encourages ﬁne-tuning the path if the incoming instance shares the similarity with previous instances. The mechanism of selecting paths according to instances similarity is naturally determined by the controller, which is compact and online updated. Experimental results show that the proposed method outperforms state-of-the-arts in online continual learning. Furthermore, the proposed method is evaluated against a realistic setting where the boundaries between tasks are blurred. Experimental results conﬁrm that the proposed method outperforms the state-of-the-arts on CIFAR-10, CIFAR-100, and Tiny-ImageNet. 1

Introduction
Continual learning aims at ﬁnding the method that will prevent a learning model from forgetting previously learned tasks when learning the new task, i.e. to overcome the catastrophic forgetting.
From the human perspective, we expect that one may have a better understanding if the task is related to previously learned knowledge. For deep learning models, however, knowledge learned from a previous task is often “washed out” by ﬁne-tuning on a different one, making neural networks to exhibit the behavior of catastrophic forgetting. Mitigating forgetting and transferring knowledge between tasks are the key components of continual learning. Although there are plenty of experiment settings proposed by previous works, recent studies such as [1–4] start to solve the more challenging scenario: online continual learning. Instead of using the entire training set repeatedly within a task, which is found in the conventional continual learning setting, online continual learning further restricts that each sample can only be seen once. This setting may emulate scenarios when the learner cannot get a sufﬁcient amount of data for tasks at the beginning of the training phase or it is not possible to store the data for the entire task due to privacy issues or memory limitations.
Currently, there are three types of continual learning methods based on how they store the informa-tion of previous task: regularization-based, replay-based, and architectural-based. Regularization 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
approaches such as [5] regularize the network parameter during training to prevent parameters move away from the old tasks. Replay-based approaches replay the previous data stored in a ﬁxed-sized buffer when training on new tasks. Architectural, expansion, parameter-isolation approaches can accommodate the knowledge to prevent new data interference with previous knowledge. We are interested in architectural approaches in our work that can truly satisfy the online learning assumption, i.e., each sample can only be seen once, which is not true for replay-based methods.
One of the reasons for catastrophic forgetting has to do with the fact that updates on a latter task usually wash out the knowledge learned in a previous task. This is a natural consequence of using gradient descent based methods that optimizes directly on an objective deﬁned by a dataset. Models from previous tasks are seen as initialization and will be "washed out" by new updates. This will be more severe for online continual learning, since previous instances in the same task may also be "washed out". We include a baseline, Independent, in the Section 4 to elaborate the idea. One direct way of solving catastrophic forgetting in online continual learning is to learn each instance individually. Nevertheless, it is often difﬁcult to achieve that, due to the lack of amount of data and the unrestricted growing size of the neural network. Also, it is believed that to solve the continual learning problem, the network should have the ability to both reduce the interference and maximal the potential knowledge transfer by weight sharing [6].
We propose a new direction for solving online continual learning with the concept of instance-awareness. For each sample in the neural networks, the controller will assign the most suitable path based on its visual feature. Paths assigned by the controller will receive the images with similar visual features, which is beneﬁcial to help reduce the probability to overwrite the knowledge and improve the performance. Instance-aware has been used in image classiﬁcation [7–9] for solving the diverse and various samples in the dataset. It is believed that for each given sample, there exists the best model to classify it correctly and efﬁciently. Inspired by InstaNAS [7], the high-level illustration of the model with instance-awareness is to ﬁrst prepare a meta-graph, then tailor a child model from the meta-graph for each input sample. In our work, we leverage the instance-aware concept to alleviate catastrophic forgetting for each instance. When training the neural network, the controller will ﬁnd the best path in the meta-graph for the given sample, then only train the given path and mitigate the forgetting by minimizing the probability to overwrite the existing knowledge from other unrelated paths. By only updating the path with similar visual patterns, our model can both reduce the interference and transfer the existing knowledge to the new instance. Note that the mechanism of selecting paths according to instances similarity is naturally determined by the controller, which is compact and online updated.
The overall concept is illustrated as Figure 1. Here we see two samples in the Figure with two images of butterﬂies that are similar to each other and an image of a dog that is very different from the other two. We would want the parameters to be updated on the ﬁrst two images to be similar to each other but not the third one. To achieve that, we store a large meta-graph that consists of L × K model components, representing a possible universe of architectural spaces that has L layers and K blocks for each layer. We then employ a controller network that dynamically chose the neural network architecture in the meta-graph using reinforcement learning. As each sample would have its child architecture, doing this will essentially prevent model updates to override past knowledge. This can be illustrated in Figure 1 that the dog sample having a very different probability of building networks in the meta-graph than that of the butterﬂies, highlighted using a scale of the color panel. As a result, updates that belong to the dog instance will avoid “washing out” the model weights that are very different from itself and mitigated catastrophic forgetting from happening. 2