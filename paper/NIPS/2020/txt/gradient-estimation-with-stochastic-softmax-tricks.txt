Abstract
The Gumbel-Max trick is the basis of many relaxed gradient estimators. These estimators are easy to implement and low variance, but the goal of scaling them comprehensively to large combinatorial distributions is still outstanding. Work-ing within the perturbation model framework, we introduce stochastic softmax tricks, which generalize the Gumbel-Softmax trick to combinatorial spaces. Our framework is a uniﬁed perspective on existing relaxed estimators for perturbation models, and it contains many novel relaxations. We design structured relaxations for subset selection, spanning trees, arborescences, and others. When compared to less structured baselines, we ﬁnd that stochastic softmax tricks can be used to train latent variable models that perform better and discover more latent structure. 1

Introduction
Gradient computation is the methodological backbone of deep learning, but computing gradients is not always easy. Gradients with respect to parameters of the density of an integral are generally intractable, and one must resort to gradient estimators [8, 61]. Typical examples of objectives over densities are returns in reinforcement learning [76] or variational objectives for latent variable models
[e.g., 37, 68]. In this paper, we address gradient estimation for discrete distributions with an emphasis on latent variable models. We introduce a relaxed gradient estimation framework for combinatorial discrete distributions that generalizes the Gumbel-Softmax and related estimators [53, 35].
Relaxed gradient estimators incorporate bias in order to reduce variance. Most relaxed estimators are based on the Gumbel-Max trick [52, 54], which reparameterizes distributions over one-hot binary vectors. The Gumbel-Softmax estimator is the simplest; it continuously approximates the Gumbel-Max trick to admit a reparameterization gradient [37, 68, 72]. This is used to optimize the “soft” approximation of the loss as a surrogate for the “hard” discrete objective.
Adding structured latent variables to deep learning models is a promising direction for addressing a number of challenges: improving interpretability (e.g., via latent variables for subset selection [17] or parse trees [19]), incorporating problem-speciﬁc constraints (e.g., via enforcing alignments [58]), and improving generalization (e.g., by modeling known algorithmic structure [30]). Unfortunately, the vanilla Gumbel-Softmax cannot scale to distributions over large state spaces, and the development of structured relaxations has been piecemeal.
We introduce stochastic softmax tricks (SSTs), which are a uniﬁed framework for designing structured relaxations of combinatorial distributions. They include relaxations for the above applications, as well
∗Equal Contribution. Correspondence to max.paulus@inf.ethz.ch, choidami@cs.toronto.edu.
†Work done partly at the Institute for Advanced Study, Princeton, NJ. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Finite set
Random utility
Stoch. Argmax Trick
Stoch. Softmax Trick
Figure 1: Stochastic softmax tricks relax discrete distributions that can be reparameterized as random and a random linear programs. X is the solution of a random linear program deﬁned by a ﬁnite set
Rm. To design relaxed gradient estimators with respect to θ, Xt is the utility U with parameters θ solution of a random convex program that continuously approximates X from within the convex hull of
. The Gumbel-Softmax [53, 35] is an example of a stochastic softmax trick.
X
∈
X as many novel ones. To use an SST, a modeler chooses from a class of models that we call stochastic argmax tricks (SMT). These are instances of perturbation models [e.g., 64, 33, 78, 27], and they induce a distribution over a ﬁnite set by optimizing a linear objective (deﬁned by random utility
. An SST relaxes this SMT by combining a strongly convex regularizer with the
U random linear objective. The regularizer makes the solution a continuous, a.e. differentiable function of U and appropriate for estimating gradients with respect to U ’s parameters. The Gumbel-Softmax is a special case. Fig. 1 provides a summary.
Rn) over
X
X
∈
We test our relaxations in the Neural Relational Inference (NRI) [38] and L2X [17] frameworks. Both
NRI and L2X use variational losses over latent combinatorial distributions. When the latent structure in the model matches the true latent structure, we ﬁnd that our relaxations encourage the unsupervised discovery of this combinatorial structure. This leads to models that are more interpretable and achieve stronger performance than less structured baselines. All proofs are in the Appendix. 2 Problem Statement
Y
X ⊆
Rn of
, deﬁne the embeddings
Y
Rn.3 For example, if be a non-empty, ﬁnite set of combinatorial objects, e.g. the spanning trees of a graph. To
Let
Y of some represent embedding function rep : is the set of spanning trees of a graph with
Y and let rep(y) be the one-hot binary vector of edges E, then we could enumerate y1, . . . , y|Y| in
. length
|Y|
Alternatively, in this case we could use a more efﬁcient, structured representation: rep(y) could be a
, with rep(y)e = 1 iff edge e is in the tree y. See Fig. 2 binary indicator vector of length for visualizations and additional examples of structured binary representations. We assume that is convex independent.4
Y
, with rep(y)i = 1 iff y = yi. This requires a very large ambient dimension n = to be the image rep(y)
{
| (cid:28) |Y|
∈ Y}
Y →
E
|
|Y|
X y
|
Given a probability mass function pθ : (0, 1] that is differentiable in θ
: Rn
R, and X pθ, our ultimate goal is gradient-based optimization of E[
X →
∈
L are concerned in this paper with the problem of estimating the derivatives of the expected loss,
→
∼
L
Rm, a loss function (X)]. Thus, we d dθ
E[
L (X)] = d dθ (cid:16)(cid:88) x∈X L (cid:17) (x)pθ(x)
. (1) 3