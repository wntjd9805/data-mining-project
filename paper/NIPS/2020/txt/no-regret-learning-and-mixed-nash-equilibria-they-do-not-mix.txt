Abstract
Understanding the behavior of no-regret dynamics in general ùëÅ-player games is a fundamental question in online learning and game theory. A folk result in the
Ô¨Åeld states that, in Ô¨Ånite games, the empirical frequency of play under no-regret learning converges to the game‚Äôs set of coarse correlated equilibria. By contrast, our understanding of how the day-to-day behavior of the dynamics correlates to the game‚Äôs Nash equilibria is much more limited, and only partial results are known for certain classes of games (such as zero-sum or congestion games). In this paper, we study the dynamics of follow the regularized leader (FTRL), arguably the most well-studied class of no-regret dynamics, and we establish a sweeping negative result showing that the notion of mixed Nash equilibrium is antithetical to no-regret learning. SpeciÔ¨Åcally, we show that any Nash equilibrium which is not strict (in that every player has a unique best response) cannot be stable and attracting under the dynamics of FTRL. This result has signiÔ¨Åcant implications for predicting the outcome of a learning process as it shows unequivocally that only strict (and hence, pure) Nash equilibria can emerge as stable limit points thereof. 1

Introduction
Regret minimization is one of the most fundamental requirements for online learning and decision-making in the presence of uncertainty and unpredictability [11]. DeÔ¨Åned as the difference between the cumulative performance of an adaptive policy and that of the best Ô¨Åxed action in hindsight, the regret of an agent provides a concise and meaningful benchmark for quantifying the ability of an online algorithm to adapt to an otherwise unknown and unpredictable environment.
‚àóEqual contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Arguably, the most widely studied class of no-regret algorithms is the general algorithmic scheme known as follow the regularized leader (FTRL) [56, 57]. This umbrella learning framework includes as special cases the multiplicative weights update (MWU) [2, 3, 32, 62] and online gradient descent (OGD) algorithms [64], both of which achieve a min-max optimal O(ùëá 1/2) regret guarantee. For obvious reasons, the ability of FTRL to adapt optimally to an unpredictable environment makes them ideal for applying them in multi-agent environments ‚Äì i.e., games. In this case, if all agents adhere to a no-regret learning process based on FTRL (or one of its variants), as the sequence of play becomes more predictable, stronger regret guarantees are achievable, possibly down to constant regret, see e.g.,
[5, 6, 20, 30, 37, 38, 50, 58] and references therein. As such, several crucial questions arise:
What are the game-theoretic implications of the no-regret guarantees of FTRL?
Do the dynamics of FTRL converge to an equilibrium of the underlying game?
A folk answer to this question is that ‚Äúno-regret learning converges to equilibrium in all games‚Äù
[43], suggesting in this way that no-regret dynamics inherently gravitate towards game-theoretically meaningful states. However, at this level of abstraction, both the type of convergence as well as the speciÔ¨Åc notion of equilibrium that go in this statement are not as strong as one would have hoped for.
Formally, the only precise conclusion that can be drawn is as follows: under a no-regret learning procedure, the empirical frequency of play converges to the game‚Äôs set of coarse correlated equilibria
[23, 24].
This leads to an important disconnect with standard game-theoretic solution concepts on several grounds. First, even in 2-player games, coarse correlated equilibria may be exclusively supported on strictly dominated strategies [60], so they fail even the most basic requirements of rationalizability
[19, 22]. Second, the archetypal game-theoretic solution concept is that of Nash equilibrium (NE), and convergence to a Nash equilibrium is a much more tenuous affair: since no-regret dynamics are, by construction, uncoupled (in the sense that a player‚Äôs update rule does not explicitly depend on the payoffs of other players), the impossibility result of Hart & Mas-Colell [25] precludes the convergence of no-regret learning to Nash equilibrium in all games. This is consistent with the numerous negative complexity results for Ô¨Ånding a Nash equilibrium [18, 54]: an incremental method like FTRL simply cannot have enough power to overcome PPAD completeness and converge to Nash equilibrium given adversarially chosen initial conditions.
In view of the above, a natural test of whether the dynamics of FTRL favor convergence to a Nash equilibrium is to see whether they eventually stabilize and converge to it when initialized nearby. In more precise language, are Nash equilibria asymptotically stable in the dynamics of FTRL? And, perhaps more importantly, are all Nash equilibria created equal in this regard?
Our contributions. We establish a stark and robust dichotomy between how the dynamics of
FTRL treat Nash equilibria in mixed (i.e., randomized) vs. pure strategies. For the case of mixed
Nash equilibria we establish a sweeping negative result to the effect that the notion of mixed Nash equilibrium is antithetical to no-regret learning. More precisely, we show that any Nash equilibrium which is not strict (in the sense that every player has a unique best response) cannot be stable and attracting under the dynamics of FTRL. Schematically:
Informal Theorem: Asymptotically stable point for FTRL =‚áí Pure Nash equilibrium
Equivalently: Mixed Nash equilibrium =‚áí Not asymptotically stable under FTRL
The linchpin of our analysis is the following striking property of the FTRL dynamics: when viewed in the space of ‚Äúpayoffs‚Äù (their natural state space), they preserve volume irrespective of the underlying game. More precisely, the Lebesgue measure of any open set of initial conditions in the space of payoffs remains invariant as it is carried along the Ô¨Çow of the FTRL dynamics (cf. Fig. 2).
Importantly, this result is not true in the problem‚Äôs ‚Äúprimal‚Äù space, i.e., the space of the player‚Äôs mixed strategies: here, sets of initial conditions can expand or contract indeÔ¨Ånitely under the standard
Euclidean volume form.
This duality between payoffs and strategies is the leitmotif of our approach and has a number of important consequences. First, exploiting the volume-preservation property of FTRL, we show that no interior Nash equilibrium (and, furthermore, no closed set in the interior of the strategy space) can be asymptotically stable under the dynamics of FTRL, as this effectively would necessitate volume contraction in the interior of the space (Theorem 4.2). 2
To move beyond this result and disqualify all non-strict Nash equilibria (not just interior ones) more intricate arguments are required. In this case, a fundamental distinction arises between classes of dynamics that may attain the boundary of the players‚Äô strategy space in Ô¨Ånite time versus those that do not. The Ô¨Årst case concerns FTRL dynamics with an everywhere-differentiable regularizer, like the Euclidean regularizer that gives rise to OGD and the associated projection dynamics. The second concerns dynamics where the regularizer becomes steep at the boundary of the strategy simplex, e.g., like the Shannon-Gibbs entropy that gives rise to the multiplicative weights update (MWU) algorithm and the replicator dynamics. While the interior of the strategy simplex is invariant for the second class of dynamics, this is not the case for the former: in Euclidean-like cases, the support of the mixed strategy of an agent may change over time. This leads to an essential dichotomy in the boundary behavior of different classes of FTRL dynamics. Nonetheless, despite the qualitatively distinct long-run behavior of the dynamics, a uniÔ¨Åed message emerges: under the dynamics of FTRL, only strict Nash equilibria survive (Theorem 4.3).
Finally, for the case of steep, entropy-like regularizers we prove that not only their asymptotically stable points but much more generally any asypmptotically stable set must contain at least one pure strategy proÔ¨Åle (Theorem 4.5).