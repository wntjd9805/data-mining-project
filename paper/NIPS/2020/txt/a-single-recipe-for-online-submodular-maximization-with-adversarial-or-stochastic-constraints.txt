Abstract (cid:80)T
In this paper, we consider an online optimization problem in which the reward functions are DR-submodular, and in addition to maximizing the total reward, the sequence of decisions must satisfy some convex constraints on average. Speciﬁcally, at each round t ∈ {1, . . . , T }, upon committing to an action xt, a DR-submodular utility function ft(·) and a convex constraint function gt(·) are revealed, and the goal is to maximize the overall utility while ensuring the average of the constraint functions 1 t=1 gt(xt) is non-positive. Such cumulative constraints arise natu-T rally in applications where the average resource consumption is required to remain below a prespeciﬁed threshold. We study this problem under an adversarial model and a stochastic model for the convex constraints, where the functions gt can vary arbitrarily or according to an i.i.d. process over time slots t ∈ {1, . . . , T }, respec-tively. We propose a single algorithm which achieves sub-linear (with respect to T ) regret as well as sub-linear constraint violation bounds in both settings, without prior knowledge of the regime. Prior works have studied this problem in the special case of linear constraint functions. Our results not only improve upon the existing bounds under linear cumulative constraints, but also give the ﬁrst sub-linear bounds for general convex long-term constraints. 1

Introduction
Online optimization covers a large number of problems in which information is revealed incrementally (i.e., online) and irrevocable decisions should be made at each step in face of uncertainty about the future arriving information [1–5]. Such problems could be formulated as a repeated game between the decision maker (i.e., the learner) and the adversary (i.e., the nature or environment). At each iteration of this game, the learner chooses an action from a ﬁxed domain set and then, it receives feedback in the form of utility or reward for her selected action. In the non-stochastic feedback model, no assumptions are made on the sequence of arriving rewards except their boundedness. As time goes by, the learner aims to observe the past and make better decisions to maximize the overall reward.
The performance of online algorithms are usually measured through the regret or the competitive ratio of the algorithm. In the regret analysis framework, at each round, the learner has to commit to an action before observing the corresponding reward function and the goal is to design algorithms whose total accumulated reward differs sub-linearly (in the time horizon T ) from the reward of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
best ﬁxed benchmark action (or sequence) with hindsight information [4, 6–8]. On the other hand, in the competitive analysis setting, the decision maker is allowed to ﬁrst observe the reward function at each step and then, choose her action accordingly (i.e., the 1-lookahead setting). In this setting, the goal is to obtain bounds for the ratio of the total reward of the algorithm and the ofﬂine optimum (i.e., the competitive ratio) [9, 10]. In this work, we focus on the regret analysis setting.
In most of the prior work on online learning, there are no constraints on the sequence of decisions made by the learner and maximizing the overall reward is the sole objective [4, 5]. However, in many applications, there indeed exist some constraints on the decisions of the algorithm which need to be satisﬁed on average [11–14]. For instance, in an online task assignment problem in crowdsourcing markets, the requester needs to balance her total payment to workers against a prespeciﬁed allotted budget [15]. The advertiser in an online ad placement problem has a limited budget to invest on buying ads on different websites [16–18]. Note that in both of these problems, the resource (budget) consumption at each round is not known ahead of time. In crowdsourcing, the consumed resource depends on the workers’ overall cost for performing the task, and even if a worker’s hourly rate is known, the length of time required may not be known beforehand; in the online ad allocation problem, resource use depends on the number of clicks on the ads. 1.1