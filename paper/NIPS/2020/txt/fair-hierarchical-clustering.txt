Abstract
As machine learning has become more prevalent, researchers have begun to recog-nize the necessity of ensuring machine learning systems are fair. Recently, there has been an interest in deﬁning a notion of fairness that mitigates over-representation in traditional clustering.
In this paper we extend this notion to hierarchical clustering, where the goal is to recursively partition the data to optimize a speciﬁc objective. For various natural objectives, we obtain simple, efﬁcient algorithms to ﬁnd a provably good fair hierarchical clustering. Empirically, we show that our algorithms can ﬁnd a fair hierarchical clustering, with only a negligible loss in the objective. 1

Introduction
Algorithms and machine learned models are increasingly used to assist in decision making on a wide range of issues, from mortgage approval to court sentencing recommendations [28]. It is clearly undesirable, and in many cases illegal, for models to be biased to groups, for instance to discriminate on the basis of race or religion. Ensuring that there is no bias is not as easy as removing these protected categories from the data. Even without them being explicitly listed, the correlation between sensitive features and the rest of the training data may still cause the algorithm to be biased. This has led to an emergent literature on computing provably fair outcomes (see the book [7]).
The prominence of clustering in data analysis, combined with its use for data segmentation, feature engineering, and visualization makes it critical that efﬁcient fair clustering methods are developed.
There has been a ﬂurry of recent results in the ML research community, proposing algorithms for fair
ﬂat clustering, i.e., partitioning a dataset into a set of disjoint clusters, as captured by K-CENTER, K-MEDIAN, K-MEANS, correlation clustering objectives [3, 4, 6, 8, 9, 14, 18, 24, 25, 30, 31]. However, the same issues affect hierarchical clustering, which is the problem we study.
The input to the hierarchical clustering problem is a set of data points, with pairwise similarity or dissimilarity scores. A hierarchical clustering is a tree, whose leaves correspond to the individual datapoints. Each internal node represents a cluster containing all the points in the leaves of its subtree. Naturally, the cluster at an internal node is the union of the clusters given by its children.
Hierarchical clustering is widely used in data analysis [21], social networks [32, 34], and image/text organization [26]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Hierarchical clustering is frequently used for ﬂat clustering when the number of clusters is a priori unknown. A hierarchical clustering yields a set of clusterings at different granularities that are consistent with each other. Therefore, in all clustering problems where fairness is desired but the number of clusters is unknown, fair hierarchical clustering is useful. As concrete examples, consider a set of news articles organized by a topic hierarchy, where we wish to ensure that no single source or view point is over-represented in a cluster; or a hierarchical division of a geographic area, where the sensitive attribute is gender or race, and we wish to ensure balance in every level of the hierarchy.
There are many such problems that beneﬁt from fair hierarchical clustering, motivating its study.
Our contributions. We initiate an algorithmic study of fair hierarchical clustering. We build on
Dasgupta’s seminal formal treatment of hierarchical clustering [20] and prove our results for the revenue [33], value [19], and cost [20] objectives in his framework.
To achieve fairness, we show how to extend the fairlets machinery, introduced by [16] and extended by [3], to this problem. We then investigate the complexity of ﬁnding a good fairlet decomposition, giving both strong computational lower bounds and polynomial time approximation algorithms.
Finally, we conclude with an empirical evaluation of our approach. We show that ignoring protected attributes when performing hierarchical clustering can lead to unfair clusters. On the other hand, adopting the fairlet framework in conjunction with the approximation algorithms we propose yields fair clusters with a negligible objective degradation.