Abstract
In this paper, we study communication efﬁcient distributed algorithms for distribu-tionally robust federated learning via periodic averaging with adaptive sampling.
In contrast to standard empirical risk minimization, due to the minimax structure of the underlying optimization problem, a key difﬁculty arises from the fact that the global parameter that controls the mixture of local losses can only be updated infre-quently on the global stage. To compensate for this, we propose a Distributionally
Robust Federated Averaging (DRFA) algorithm that employs a novel snapshot-ting scheme to approximate the accumulation of history gradients of the mixing parameter. We analyze the convergence rate of DRFA in both convex-linear and nonconvex-linear settings. We also generalize the proposed idea to objectives with regularization on the mixture parameter and propose a proximal variant, dubbed as DRFA-Prox, with provable convergence rates. We also analyze an alternative optimization method for regularized case in strongly-convex-strongly-concave and non-convex (under PL condition)-strongly-concave settings. To the best of our knowledge, this paper is the ﬁrst to solve distributionally robust federated learn-ing with reduced communication, and to analyze the efﬁciency of local descent methods on distributed minimax problems. We give corroborating experimental evidence for our theoretical results in federated learning settings. 1

Introduction
Federated learning (FL) has been a key learning paradigm to train a centralized model from an extremely large number of devices/users without accessing their local data [21]. A commonly used approach is to aggregate the individual loss functions usually weighted proportionally to their sample sizes and solve the following optimization problem in a distributed manner: min w∈W
F (w) :=
N (cid:88) i=1 ni n
{fi(w) := Eξ∼Pi[(cid:96)(w; ξ)]} , (1) where N is number of clients each with ni training samples drawn from some unknown distribution
Pi (possibly different from other clients), fi(w) is the local objective at device i for a given loss function (cid:96), W is a closed convex set, and n is total number of samples.
In a federated setting, in contrast to classical distributed optimization, in solving the optimization problem in Eq. 1, three key challenges need to be tackled including i) communication efﬁciency, ii) the low participation of devices, and iii) heterogeneity of local data shards. To circumvent the communication bottleneck, an elegant idea is to periodically average locally evolving models as employed in FedAvg algorithm [34]. Speciﬁcally, each local device optimizes its own model for τ local iterations using SGD, and then a subset of devices is selected by the server to communicate their models for averaging. This approach, which can be considered as a variant of local SGD [44, 13, 14] but with partial participation of devices, can signiﬁcantly reduce the number of communication rounds, as demonstrated both empirically and theoretically in various studies [26, 20, 12, 15, 46]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
While being compelling from the communication standpoint, FedAvg does not necessarily tackle the data heterogeneity concern in FL. In fact, it has been shown that the generalization capability of the central model learned by FedAvg, or any model obtained by solving Eq. 1 in general, is inevitably plagued by increasing the diversity among local data distributions [24, 18, 12]. This is mainly due to the fact the objective in Eq. 1 assumes that all local data are sampled from the same distribution, but in a federated setting, local data distributions can signiﬁcantly vary from the average distribution.
Hence, while the global model enjoys a good average performance, its performance often degrades signiﬁcantly on local data when the distributions drift dramatically.
To mitigate the data heterogeneity issue, one solution is to personalize the global model to local distributions. A few notable studies [8, 32] pursued this idea and proposed to learn a mixture of the global and local models. While it is empirically observed that the per-device mixture model can reduce the generalization error on local distributions compared to the global model, however, the learned global model still suffers from the same issues as in FedAvg, which limits its adaptation to newly joined devices. An alternative solution is to learn a model that has uniformly good performance over almost all devices by minimizing the agnostic (distributionally robust) empirical loss: min w∈W max
λ∈Λ
F (w, λ) :=
N (cid:88) i=1
λifi(w), (2) where λ ∈ Λ
.
= {λ ∈ RN
+ : (cid:80)N i=1 λi = 1} is the global weight for each local loss function.
The main premise is that by minimizing the robust empirical loss, the learned model is guaranteed to perform well over the worst-case combination of empirical local distributions, i.e., limiting the reliance to only a ﬁxed combination of local objectives1. Mohri et al. [35] was among the ﬁrst to introduce the agnostic loss into federated learning, and provided convergence rates for convex-linear and strongly-convex-strongly-concave functions. However, in their setting, the server has to communicate with local user(s) at each iteration to update the global mixing parameter λ, which hinders its scalability due to communication cost.
The aforementioned issues, naturally leads to the following question: Can we propose a provably communication efﬁcient algorithm that is also distributionally robust? The purpose of this paper is to give an afﬁrmative answer to this question by proposing a Distributionally Robust Federated
Averaging (DRFA) algorithm that is distributionally robust, while being communication-efﬁcient via periodic averaging, and partial node participation, as we show both theoretically and empirically.
From a high-level algorithmic perspective, we develop an approach to analyze minimax optimization methods where model parameter w is trained distributedly at local devices, and mixing parameter
λ is only updated at server periodically. Speciﬁcally, each device optimizes its model locally, and a subset of them are adaptively sampled based on λ to perform model averaging. We note that since λ is updated only at synchronization rounds, it will inevitably hurt the convergence rate. Our key technical contribution is the introduction and analysis of a randomized snapshotting schema to approximate the accumulation of history of local gradients to update λ as to entail good convergence.
Contributions. We summarize the main contributions of our work as follows:
• To the best of our knowledge, the proposed DRFA algorithm is the ﬁrst to solve distributionally robust optimization in a communicationally efﬁcient manner for federated learning, and to give theoretical analysis on heterogeneous (non-IID) data distributions. The proposed idea of decoupling the updating of w from λ can be integrated as a building block into other federated optimization methods, e.g. [18, 23] to yield a distributionally robust solution.
• We derive the convergence rate of our algorithm when loss function is convex in w and linear in
λ, and establish an O(1/T 3/8) convergence rate with only O (cid:0)T 3/4(cid:1) communication rounds. For nonconvex loss, we establish convergence rate of O(1/T 1/8) with only O (cid:0)T 3/4(cid:1) communication rounds. Compared to [35], we signiﬁcantly reduce the communication rounds.
• For the regularized objectives, we propose a variant algorithm, dubbed as DRFA-Prox, and prove that it enjoys the same convergence rate as DRFA. We also analyze an alternative method for optimizing regularized objective and derive the convergence rate in strongly-convex-strongly-concave and non-convex (under PL condition)-strongly-concave settings.
• We demonstrate the practical efﬁcacy of the proposed algorithm over competitive baselines through experiments on federated datasets. 1Beyond robustness, agnostic loss yields a notion of fairness [35], which is not the focus of present work. 2
2