Abstract
The loss landscapes of deep neural networks are not well understood due to their high nonconvexity. Empirically, the local minima of these loss functions can be connected by a learned curve in model space, along which the loss remains nearly constant; a feature known as mode connectivity. Yet, current curve ﬁnding algo-rithms do not consider the inﬂuence of symmetry in the loss surface created by model weight permutations. We propose a more general framework to investigate the effect of symmetry on landscape connectivity by accounting for the weight permutations of the networks being connected. To approximate the optimal per-mutation, we introduce an inexpensive heuristic referred to as neuron alignment.
Neuron alignment promotes similarity between the distribution of intermediate activations of models along the curve. We provide theoretical analysis establishing the beneﬁt of alignment to mode connectivity based on this simple heuristic. We empirically verify that the permutation given by alignment is locally optimal via a proximal alternating minimization scheme. Empirically, optimizing the weight per-mutation is critical for efﬁciently learning a simple, planar, low-loss curve between networks that successfully generalizes. Our alignment method can signiﬁcantly alleviate the recently identiﬁed robust loss barrier on the path connecting two adversarial robust models and ﬁnd more robust and accurate models on the path.
Code is available at https://github.com/IBM/NeuronAlignment. 1

Introduction
Loss surfaces of neural networks have been of recent interest in the deep learning community both from a numerical (Dauphin et al., 2014; Sagun et al., 2014) and a theoretical (Choromanska et al., 2014; Safran & Shamir, 2015) perspective. Their optimization yields interesting examples of a high-dimensional non-convex problem, where counter-intuitively gradient descent methods successfully converge to non-spurious minima. Practically, recent advancements in several applications have used insights on loss surfaces to justify their approaches. For instance, Moosavi-Dezfooli et al. (2019) investigates regularizing the curvature of the loss surface to increase the robustness of trained models.
One interesting question about these non-convex loss surfaces is to what extent trained models, which correspond to local minima, are connected. Here, connection denotes the existence of a path between the models, parameterized by their weights, along which loss is nearly constant. There has been 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
conjecture that such models are connected asymptotically, with respect to the width of hidden layers.
Recently, Freeman & Bruna (2016) proved this for rectiﬁed networks with one hidden layer.
When considering the connection between two neural networks, it is important for us to consider what properties of the neural networks are intrinsic. Intuitively, there is a permutation ambiguity in the indexing of units in a given hidden layer of a neural network, and as a result, this ambiguity extends to the network weights themselves. Thus, there are numerous equivalent points in model space that correspond to a given neural network, creating weight symmetry in the loss landscape. It is possible that the minimal loss paths between a network and all networks equivalent to a second network could be quite different. If we do not consider the best path among this set, we could fail to see to what extent models are intrinsically connected. Therefore, in this work we propose to develop a technique for more consistent model interpolation / optimal connection ﬁnding by investigating the effect of weight symmetry in the loss landscape.The analyses and results will give us insight into the geometry of deep neural network loss surfaces that is often hard to study theoretically.