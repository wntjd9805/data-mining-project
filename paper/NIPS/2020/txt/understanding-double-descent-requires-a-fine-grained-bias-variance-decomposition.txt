Abstract
Classical learning theory suggests that the optimal generalization performance of a machine learning model should occur at an intermediate model complexity, with simpler models exhibiting high bias and more complex models exhibiting high variance of the predictive function. However, such a simple trade-off does not adequately describe deep learning models that simultaneously attain low bias and variance in the heavily overparameterized regime. A primary obstacle in explaining this behavior is that deep learning algorithms typically involve multiple sources of randomness whose individual contributions are not visible in the total variance. To enable ﬁne-grained analysis, we describe an interpretable, symmetric decomposition of the variance into terms associated with the randomness from sam-pling, initialization, and the labels. Moreover, we compute the high-dimensional asymptotic behavior of this decomposition for random feature kernel regression, and analyze the strikingly rich phenomenology that arises. We ﬁnd that the bias decreases monotonically with the network width, but the variance terms exhibit non-monotonic behavior and can diverge at the interpolation boundary, even in the absence of label noise. The divergence is caused by the interaction between sampling and initialization and can therefore be eliminated by marginalizing over samples (i.e. bagging) or over the initial parameters (i.e. ensemble learning). 1

Introduction
It is undeniable that modern neural networks (NNs) are becoming larger and more complex, with many state-of-the-art models now employing billions of trainable parameters [1–3]. While parameter count may be a crude way of quantifying complexity, there is little doubt that these models have enormous capacity, often far more than is needed to perfectly ﬁt the training data, even if the labels are pure noise [4]. Surprisingly, these same high-capacity models generalize well when trained on real data.
These observations conﬂict with classical generalization theory, which contends that models of intermediate complexity should generalize best, striking a balance between the bias and the variance of their predictive functions. A paradigm for understanding the observed generalization behavior of modern methods is known as double descent [5], in which the test error behaves as predicted by classical theory and follows the standard U-shaped curve until the point where the training set can be
ﬁt exactly, but after this point it begins to descend again, eventually ﬁnding its global minimum in the overparameterized regime.
While double descent has been the focus of signiﬁcant research, a concrete and interpretable theoreti-cal explanation for the phenomenon has thus far been lacking. One of the challenges in developing
⇤Both authors contributed equally to this work. †Work done as a member of the Google AI Residency program (https://g.co/airesidency). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
such an explanation is that the full phenomenology of double descent is not evident in linear models that are easy to analyze. Indeed, for linear models the number of parameters is tied to the number of features and there is no natural way to adjust the capacity of the model without simultaneously adjusting the data distribution. In this work, we overcome this challenge by providing a precise asymptotic analysis of random feature kernel regression, which is a model rich enough to exhibit all the interesting features of double descent.
Another challenge in understanding double descent is that the classical bias-variance decomposition is itself insufﬁciently nuanced to reveal all the underlying explanatory factors. Indeed, modern learning algorithms typically involve multiple sources of randomness and isolating the variation caused by each of these sources of randomness is key to building an effective interpretation. As we will see, it is not possible to fully understand the spike in test error near the interpolation threshold without performing a truly multivariate variance decomposition.
While decomposing the variance has been proposed before, prior work has naively relied on the law of total variance, which requires specifying an ordering of conditioning that leads to some arbitrariness. Instead, we present a principled symmetric decomposition which leads to unambiguous interpretations and clear credit assignment. Decomposing the variance of a random variable in this way is related to ANOVA [6], which has been used previously in a machine learning context to ﬁnd the best approximating functions (in terms of mean squared error) to a random variable with limited dependence on the inputs [7, 8] and to study quasi Monte Carlo methods for integration [9].
Finally, we remark that an improved understanding of the bias and variance of machine learning models might naturally suggest ways to improve their performance. Speciﬁcally, any prior knowledge about what sources of variance may be dominant could help inform decisions about which types of ensemble or bagging techniques to utilize. 1.1