Abstract
Graph neural networks (GNNs) have been successful in learning representations from graphs. Many popular GNNs follow the pattern of aggregate-transform: they aggregate the neighbors’ attributes and then transform the results of aggre-gation with a learnable function. Analyses of these GNNs explain which pairs of non-identical graphs have different representations. However, we still lack an un-derstanding of how similar these representations will be. We adopt kernel distance and propose transform-sum-cat as an alternative to aggregate-transform to reﬂect the continuous similarity between the node neighborhoods in the neighborhood ag-gregation. The idea leads to a simple and efﬁcient graph similarity, which we name
Weisfeiler–Leman similarity (WLS). In contrast to existing graph kernels, WLS is easy to implement with common deep learning frameworks. In graph classiﬁca-tion experiments, transform-sum-cat signiﬁcantly outperforms other neighborhood aggregation methods from popular GNN models. We also develop a simple and fast GNN model based on transform-sum-cat, which obtains, in comparison with widely used GNN models, (1) a higher accuracy in node classiﬁcation, (2) a lower absolute error in graph regression, and (3) greater stability in adversarial training of graph generation. 1

Introduction
Graphs are the most popular mathematical abstractions for relational data structures. One of the core problems of graph theory is to identify which graphs are identical (i.e. isomorphic). Since its introduction, the Weisfeiler–Leman (WL) algorithm (Weisfeiler & Leman, 1968) has been extensively studied as a test of isomorphism between graphs. Although it is easy to ﬁnd a pair of non-isomorphic graphs that the WL-algorithm cannot distinguish, many graph similarity measures and graph neural networks (GNNs) have adopted the WL-algorithm at the core, due to its algorithmic simplicity.
The WL-algorithm boils down to the neighborhood aggregation. One of the most famous GNNs,
GCN (Kipf & Welling, 2017), uses degree-normalized averaging as its aggregation. GraphSAGE (Hamilton et al. , 2017) applies simple averaging. GIN (Xu et al. , 2019) uses the sum instead of the average. Other GNN models such as GAT (Veliˇckovi´c et al. , 2018), GatedGCN (Bresson & Laurent, 2017), and MoNet (Monti et al. , 2017) assign different weights to the neighbors depending on their attributes before aggregation.
All the methods mentioned above follow the pattern of aggregate-transform. Xu et al. (2019) note that many GNNs based on graph convolution employ the same strategy: aggregate ﬁrst and then transform. In this paper, we identify a problematic aspect of aggregate-transform when applied to graphs with continuous node attributes. Instead, we propose transform-sum-cat, where cat indicates concatenation with the information from the central node. We justify our proposal by applying the well-established theory of kernel distance to the WL algorithm. It naturally leads to a simple and fast graph similarity, which we name Weisfeiler–Leman similarity (WLS). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of WL-iterations. (a) We set f (v) = 1 for all v ∈ V (G) initially, if not given in the data. (b) Each node attribute is updated with the pair of itself and the (multi)set of neighbor attributes. (c) The attributes are re-labeled for the convenience of further iterations. (d) Steps (b) and (c) are repeated for a ﬁxed number of iterations. See Section 2.2.
We test the applicability of our proposal in several experiments. First, we compare different aggrega-tion methods from GNN literature for graph classiﬁcation where transform-sum-cat outperforms the rest. Then, we build a simple GNN based on the same idea, which obtains (1) a higher accuracy than other popular GNNs on node classiﬁcation, (2) a lower absolute error on graph regression and when used as a discriminator, (3) enhanced stability of the adversarial training of graph generation. We summarize our contributions as follows.
• We propose a transform-sum-cat scheme for graph neural networks, as opposed to the pre-dominantly adopted aggregate-transform operation. We present examples where transform-sum-cat is better than aggregate-transform for continuous node attributes.
• We deﬁne a simple and efﬁcient graph similarity based on transform-sum-cat, which is easy to implement with deep learning frameworks. The similarity extends the Weisfeiler–Leman graph isomorphism test.
• We build a simple graph neural network based on transform-sum-cat, which outperforms widely used graph neural networks in node classiﬁcation and graph regression. We also show a promising application of our proposal in one-shot generation of molecular graphs.
The code is available at https://github.com/se-ok/WLsimilarity. 2 Preliminaries 2.1 Notations
Let G be a graph with a set of nodes V (G) or simply V . We assume each node v ∈ V is assigned an attribute f (v), which is either a categorical variable from a ﬁnite set or a vector in Rd. If we update the attribute on v, the original attribute is written as f 0(v) and the successively updated ones as f 1(v), f 2(v), etc. The set of nodes adjacent to v is denoted as N (v). The edge that connects u and v is denoted as uv. We denote the concatenation operator as ⊕. Abusing the common notation, we shall write a multiset simply as a set. 2.2 Weisfeiler–Leman isomorphism test
The Weisfeiler–Leman (WL) test (Weisfeiler & Leman, 1968) is an algorithmic test of isomorphism (identity) between graphs, possibly with categorical node attributes. Although the original test is parameterized by dimension k, we only explain the 1-dimensional version, which is used in most machine learning applications.
Let G be the path of length 3 depicted in Figure 1 (a). If G has no node attributes we set f (v) = 1 for all v ∈ V (G). Then, we update the node attributes in "stages," once for all nodes at each stage.
The updated attribute is the pair of itself and the set of attributes of its neighbors. In Figure 1 (b), the middle vertices have two 1’s in the (multi)set notation { }. For further iterations, the new attributes may be re-labeled via an injective mapping, for example, (1, {1}) → 2 and (1, {1, 1}) → 3, as in
Figure 1 (c). The next iteration is done by analogy, as in Figure 1 (d).
After a ﬁxed number of iterations, we compare the set of resulting attributes to that from another graph. If two sets differ, then the two graphs are non-isomorphic and are distinguishable by the 2
(a) Two graphs on four nodes, one at the origin con-nected to three other nodes on the unit circle. (b) orientation-invariance (c) monotonicity
Figure 2: Illustration of (a): a problem of existing neighborhood aggregation methods, and (b), (c): desirable properties of neighborhood distance for continuous attributes. The two graphs in (a) have 2D coordinates as the node attributes. The neighborhood aggregation at the central nodes are indis-tinguishable by common aggregation methods. We claim that a good neighborhood representation should have an associated distance that is (b) orientation-invariant, and (c) strictly increasing up to the degree of small rigid transformation. See Section 2.4 for further discussion.
WL-test. Many indistinguishable non-isomorphic pairs of graphs exist. However, asymptotically, the
WL-test uniquely identiﬁes almost all graphs; c.f. Babai et al. (1980); Arvind et al. (2017). 2.3 Vector representation of a set (cid:1). Function K is a positive deﬁnite kernel (pd kernel) if, for any constants {ci}n
In this section, we brieﬂy introduce the kernel distance between the point sets, focusing only on what is required in this paper. To summarize, we represent a set of vectors by the sum of the vectors after applying a transformation called a feature map. For an excellent introduction to the kernel method, see Phillips & Venkatasubramanian (2011) or Hein & Bousquet (2004).
Let K : Rd × Rd → R be a function. For example, we may think of the Gaussian kernel exp (cid:0) − (cid:107)x−y(cid:107)2 i=1 and points 2
{xi}n j cicjK(xi, xj) ≥ 0. A pd kernel K has an associated reproducing kernel Hilbert space H with feature map φ : Rd → H such that K(x, y) = (cid:104)φ(x), φ(y)(cid:105) for all x, y ∈ Rd, where (cid:104), (cid:105) denotes the inner product on H.
A pd kernel K with feature map φ induces a (pseudo-)distance dK on Rd, which is deﬁned by
K(x, y) = (cid:107)φ(x) − φ(y)(cid:107)2 d2
H = (cid:104)φ(x) − φ(y), φ(x) − φ(y)(cid:105) = K(x, x) − 2K(x, y) + K(y, y). For sets of points X = {xi}m j=1, the induced distance DK is similarly deﬁned as i=1 in Rd, we have (cid:80) (cid:80) i
D2
K(X, Y ) = i=1 and Y = {yj}n (cid:88) (cid:88)
K(x, x(cid:48)) − 2 x∈X x(cid:48)∈X (cid:88) (cid:88) x∈X y∈Y
K(x, y) + (cid:88) (cid:88) y∈Y y(cid:48)∈Y
K(y, y(cid:48)) (cid:88)
= (cid:13) (cid:13) x∈X
φ(x) −
φ(y)(cid:13) 2 (cid:13)
. (cid:88) y∈Y i viφ(xi) and φ(Y ) = (cid:80)
Hence, φ(X) = (cid:80) i φ(xi) represents set X independently of Y , and the set distance DK can be computed using the distance between the representation vectors. If points xi and yj are associated with weights vi and wj in R, then we replace K(xi, yj) with viwjK(xi, yj) and obtain φ(X) = (cid:80) j wjφ(yj) in the same manner. For many known kernels, the explicit feature maps are unclear or inﬁnite-dimensional. However, we remark that when walking backward, for an arbitrary map φ : Rd → RD, there is an associated set similarity using the representation map
φ(X) = (cid:80) x∈X φ(x). Its usefulness depends on speciﬁc problems at hand. For instance, in Section 3.2, we deﬁne a neural network based on WLS, where φ learns a suitable feature map in a supervised task. 2.4 Problems of aggregation-ﬁrst scheme
Many modern GNNs can be analyzed with the Weisfeiler–Leman framework (Xu et al. , 2019). It is common practice to aggregate the attributes of neighbors ﬁrst and then transform with a learnable function. However, we do not yet have a reliable theory that explains this order of operations. Instead, we present two examples where aggregation-ﬁrst might be dangerous for continuous node attributes.
Let us consider a graph on four nodes drawn on the Euclidean plane: one node at (0, 0) connected to three other nodes on the unit circle. See Figure 2 (a) for two different examples. We assign the 3
2D coordinates as the node attributes. If we apply the common neighborhood aggregation at the central nodes (such as average, sum, or various weighted sums), the results of the two graphs are indistinguishable. However, if we apply the set representation (cid:80) φ(x) from Section 2.3 with φ from a polynomial kernel, not only can we distinguish the two graphs but we can also deduce the exact locations of three neighbors from the set representation. See Appendix B.2 for a detailed explanation.
We target two properties, illustrated in Figure 2, for a good neighborhood representation. We consider the (dis-)similarity between a set of vectors and another set obtained by applying a rigid transformation to all elements of the former set. First, orientation-invariance indicates the following: (1) the similarity from a rotation should be the same as the similarity from the inverse of the rotation. (2) the similarity from a translation to a direction should be the same as the similarity from the same amount of translation to another direction. Second, monotonicity indicates that, when we apply a rotation or a translation, the dissimilarity must be strictly increasing up to a small positive degree of transformation. Note that the set representation from Section 2.3 with the Gaussian kernel satisfy both orientation-invariance and monotonicity. However, fast kernels and popular GNN aggregations have difﬁculties. For formal deﬁnition and discussion, see Appendix B.3. 3 Weisfeiler–Leman similarity
In this section we deﬁne Weisfeiler–Leman Similarity (WLS), which describes a diverse range of graph similarities rather than a single reference implementation. After introducing the general concept, we provide an implementation of WLS as a graph kernel in Section 3.1. We also present a similarly designed neural network in Section 3.2.
Recall from Section 2.2 that the core idea of the WL-algorithm is to iteratively update the node attributes using the neighbors’ information. Our focus is to reﬂect the similarity between the sets of neighbors’ attributes into the node-wise updated attributes via the set-representation vector from
Section 2.3. A formal statement is in Algorithm 1.
Algorithm 1: Updating node attributes in Weisfeiler–Leman similarity
Data: Graph G, nodes V , initial attributes f 0(v) for v ∈ V , iteration number k, and feature maps φi for i = 1, 2, . . . , k.
Result: Updated attributes f k(v) for v ∈ V . for i ← 1 to k do for v ∈ V do gi(v) ← φi(f i−1(v));
ˆf i(v) ← (cid:80) f i(v) ← COMBINEi u∈N (v) gi(u); (cid:16) (cid:17) f i−1(v), ˆf i(v)
; end end
As noted in Section 2.3, feature maps φi can be ones from well-known kernels or problem-speciﬁc functions. If we use the concatenation as COMBINEi in Algorithm 1, because (cid:107)f i(v) − f i(v(cid:48))(cid:107)2 = (cid:107)f i−1(v) − f i−1(v(cid:48))(cid:107)2 + (cid:107) ˆf i(v) − ˆf i(v(cid:48))(cid:107)2, both the similarities between f i−1 and between the sets of neighbors’ attributes are incorporated into f i(v). The steps in a single iteration correspond to transform-sum-cat in this case.
After the node-wise update, we keep the set of updated attributes and discard the adjacency. To compare two graphs G and G(cid:48), we measure the distance between {f k(v) : v ∈ V (G)} and {f k(v(cid:48)) : v(cid:48) ∈ V (G(cid:48))} using another kernel of choice. An example is to use the Gaussian kernel between the sum of node attributes.
If the adjacency is not represented as a binary value but instead a number wuv is
Extensions. assigned to the edge from u to v, we may reﬂect the weights by replacing the set representation (cid:80) u∈N (v) wuv · gi(u); see Section 2.3. If an edge uv has an attribute e(u, v) (cid:0)e(u, v), f i(u)(cid:1) : u ∈ N (v)} then instead of set {f i(u) : u ∈ N (v)} we consider set {COMBINEe i u∈N (v) gi(u) with (cid:80) 4
with an edge-combination function COMBINEe i . That is, we are interested in similarity not merely between the sets of node attributes, but between the sets of pairs of edge attributes and node attributes.
In practice, the graph size and the node degrees may be too large to handle. We may choose to sample some of the neighbor attributes, to apply transform-mean then to multiply the node degree, which simulates the transform-sum operation. 3.1 WLS kernel
To test our idea, we implemented a WLS-based graph kernel. We report its classiﬁcation accuracy on the TU datasets (Kersting et al. , 2016) in Section 5.1. Here, we describe which components are built into our kernel.
Algorithm 1 requires the iteration number k and feature maps φi for all iterations. We set k = 5. For the feature map φi, we chose the second-order Taylor approximation of the Gaussian kernel K(x, y) = exp (cid:0) − (cid:107)x−y(cid:107)2 2 all multiplied by exp(−(cid:107)x(cid:107)2/2). See
Appendix B.1 for the derivation. For functions COMBINEi we tested two options: (1) the sum
φ(f i−1(v)) + ˆf i(v) and (2) the concatenation f i−1(v) ⊕ ˆf i(v). (cid:1) so that φ(x) has entries 1, xi, and xixj/
√ 2
Once the node-wise update is done, as noted above, we discard the adjacency and consider the set of updated node attributes. In Section 5.1, we used the Gaussian kernel between the sum of the ﬁnal node attributes to compute the graph similarity.
Dimensionality reduction. As we apply the feature maps iteratively, the dimension quickly sur-passes the memory limit. For example, PROTEINS_full from the TU datasets (Kersting et al. , 2016) contains node attributes in R29. As a result, after three iterations with the above φi, it yields more than ﬁve billion numbers for a single node. We must reduce the dimension. x∈X φ(x) − (cid:80)
What properties must our dimensionality reduction have? Recall from Section 2.3 that the set distance between X and Y is computed as (cid:107)(cid:80) y∈Y φ(y)(cid:107), the Euclidean norm of the difference between set representations. Therefore, we would like to preserve the Euclidean norm through the reduction as much as possible. Motivated by the Johnson–Lindenstrauss lemma of high dimension D (Johnson & Lindenstrauss, 1984), we multiply COMBINEi with a random d × D matrix, say Mi. The entries of Mi are i.i.d. random samples from the normal distribution, and the column norms of Mi are normalized to 1. We apply this Mi to all nodes at the i-th iteration. For a detailed explanation and empirical tests of stability, see Appendix C.2. We set d = 200; hence, after each WL-iteration, all nodes have a vector of dimension 200. f i−1(v), ˆf i(v) (cid:17) (cid:16) 3.2 WLS neural network
Now, we propose a graph neural network based on Weisfeiler–Leman similarity. The node attributes are updated using Algorithm 1. We set k = 4 to compare the performance with other GNN models from Dwivedi et al. (2020). Each transformation φi is a three-layer multi-layer perceptron (MLP), where each layer consists of a linear transformation, 1D batch normalization, ReLU, and dropout in sequence. If the option residual is turned on, we add the input to the output of MLP after linearly transforming the input to match the dimension. All hyperparameters are listed in Appendix E.
After applying φi by going through the corresponding MLP, we use another 1D batch normalization.
Then, for each node, we sum up the outputs of neighbors to obtain representation ˆf i(v) of the neighborhood. Further, we use COMBINEi
= φi(f i−1(v)) ⊕ ˆf i(v). f i−1(v), ˆf i(v) (cid:17) (cid:16)
Again, following the experimental setup of Dwivedi et al. (2020), we place an MLP classiﬁer with two hidden layers on top of the output vectors of Algorithm 1 for node classiﬁcation. For graph classiﬁcation, the averages of all node representations are put into the same classiﬁer. We do not use the edge attributes, even if given, for these two tasks.
In the graph regression experiment, we additionally test an extended model that uses the edge attributes. The graphs in ZINC dataset has categorical edge attributes from {0, 1, 2}. Thus we assign each WL-iteration with its own learnable edge embeddings for the attributes. Let us denote tuv the attribute of edge uv and ei(tuv) the corresponding embedding for the i-th iteration. Instead of 5
Table 1: Graph classiﬁcation results on the TU datasets via WLS kernels with different aggregations.
The numbers are mean test accuracies over ten splits. Bold-faced numbers are the top scores for the corresponding datasets. The proposed aggregation (WLS) shows strong performance compared with other aggregations from the literature. See Section 5.1.
Aggregation
BZR
COX2
DHFR
ENZYMES PROTEINS
Synthie
GAT
GCN 83.21±4.52 79.26±3.54 67.74±4.00 58.83±6.85 74.02±5.72 46.00±3.69 80.49±3.22 78.60±1.52 67.74±4.91 60.67±7.98 73.04±4.70 45.72±3.72
GraphSAGE 77.53±3.73 79.01±2.42 67.61±3.48 58.83±6.85 74.47±5.59 46.00±3.69 79.02±2.04 78.79±1.27 67.49±6.05 58.83±6.85 73.04±4.70 46.00±3.69
WWL
WLSLin
WLS 79.99±3.04 77.93±2.42 68.26±2.59 58.83±6.85 74.39±3.14 55.00±7.22 83.45±6.49 77.95±3.48 77.92±4.78 68.00±3.99 75.38±4.26 86.79±5.82 (cid:80) u∈N (v) φi(f i−1(u)) as the neighborhood representation for node v, we use (cid:80) u∈N (v) φi(ei(tuv)⊕ f i−1(u)). The rest are the same as in the WLS classiﬁcation network. 4