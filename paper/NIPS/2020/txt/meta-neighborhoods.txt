Abstract
Making an adaptive prediction based on one’s input is an important ability for general artiﬁcial intelligence. In this work, we step forward in this direction and pro-pose a semi-parametric method, Meta-Neighborhoods, where predictions are made adaptively to the neighborhood of the input. We show that Meta-Neighborhoods is a generalization of k-nearest-neighbors. Due to the simpler manifold struc-ture around a local neighborhood, Meta-Neighborhoods represent the predictive distribution p(y | x) more accurately. To reduce memory and computation over-head, we propose induced neighborhoods that summarize the training data into a much smaller dictionary. A meta-learning based training mechanism is then exploited to jointly learn the induced neighborhoods and the model. Extensive studies demonstrate the superiority of our method.1 1

Introduction
Discriminative machine learning models typically learn the predic-tive distribution p(y | x). There are two paradigms to build a model, parametric methods and non-parametric methods [12]. Parametric methods assume that a set of ﬁxed parameters θ dominates the pre-dictive distribution, i.e., p(y | x; θ). The training process estimates θ and then discard the training data completely, as the learned param-eters θ are responsible for the following prediction. This paradigm has proven effective, however, it leaves the entire burden on learning a complex predictive distribution over potentially large support. Non-parametric models differ in that the number of parameters scales with data. They typically reuse the training data during the testing phase to make predictions. For instance, the well-known k-nearest neighbor (KNN) estimator often achieves surprisingly good results by leveraging neighbors from the training data, which reduces the problem to a much simpler local-manifold. Despite its ﬂexibility, non-parametric methods are required to store the training data and traverse them during testing, which may impose signiﬁcant memory and computation overhead for large training sets.
Figure 1: Top: traditional para-metric models. Bottom: our per-instance adapted model. 1The code is available at https://github.com/lupalab/Meta-Neighborhoods 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this work, we combine the merits of both paradigms and propose a semi-parametric method called
Meta-Neighborhoods. The main body of Meta-Neighborhoods is a parametric neural network, but we adapt its parameters to a local neighborhood in a non-parametric scheme. The prediction is made on the local manifold by the adapted model. Fig. 1 illustrates the difference between traditional parametric models and the proposed model. Inspired by the success of inducing point methods from sparse Gaussian process literature [34, 38] to alleviate the storage burden and reduce time complexity, we learn induced neighborhoods, which summarize the training data into a much smaller dictionary.
The induced neighborhoods and the neural network parameters are learned jointly.
Our model is also closely related to locally linear embeddings [30], which reconstructs the non-linear manifold with locally linear approximation around each neighborhood. In our method, we adapt an initial model (not necessarily linear) to local neighborhoods. Since the local manifold is much simpler, we expect the adapted model can better capture the predictive distribution. Overall, it learns a better discriminative model on the entire support.
Our method imposes challenges of adapting the initial model since the local neighborhoods usually do not contain enough training instances to independently adapt the model, and the induced neigh-borhoods contain even fewer instances. Inspired by the few-shot and meta-learning literature [6], we propose a meta-learning based training mechanism, where we learn an initial model so that it adapts to the local neighborhood after only several ﬁnetuning steps over a few instances inside the neighborhood.
The prediction process of our model remains ﬂexible by following a non-parametric scheme. An input x is ﬁrst paired with its neighbors by querying the induced dictionary. The initial model is adapted to its neighborhood by ﬁnetuning several steps on the neighbors. We then predict the target y using the adapted model.
Our contributions are as follows:
• We combine parametric and non-parametric methods in a meta-learning framework.
• We propose Meta-Neighborhoods to jointly learn the induced neighborhoods and an adaptive initial model, which can adapt its parameters to a local neighborhood according to the input through both ﬁnetuning and a proposed instance-wise modulation scheme, iFiLM.
• Extensive empirical studies demonstrate the superiority of Meta-Neighborhoods for both regres-sion and classiﬁcation tasks.
• We empirically ﬁnd the induced neighbors are semantically meaningful: they represent informative boundary cases on both realistic and toy datasets; they also capture sub-category concepts even though such information is not given during training. 2 Method
Problem Formulation Given a training set D = {(xi, yi)}N i=1 with N input-target pairs, we learn a discriminative model fφ(x) and a dictionary M = {(kj, vj)}S j=1 jointly from D. The learned dictionary stores the neighbors induced from the training set, where S is the number of induced neighbors. Just like the real training set D, the dictionary stores input-target pairs as key-value pairs (kj,vj), where both the keys and the values are learned end-to-end with the model parameters
φ in a meta-learning framework. For classiﬁcation tasks, vj is a vector representing the class probabilities while for regression tasks vj is the regression target. In the following text, we will use the terms “induced neighbors" and “learnable neighbors" interchangeably. We defer the exact training mechanism to Section 2.2. 2.1 Predict with Induced Neighborhoods
In this section, we assume access to the learned neighborhoods in M and the learned model fφ.
Different from the conventional parametric setting, where the learned model is employed directly to predict the target, we adapt the model according to the neighborhoods retrieved from M and the adapted model is the one responsible for making predictions. Speciﬁcally, for a test data xi, relevant entries in M are retrieved in a soft-attention manner by comparing xi to kj via a attention function ω. 2
Algorithm 1 META-NEIGHBORHOODS: TRAINING PHASE
Require: ω: similarity metric, η: outer loop learning rate 1: Initialize θ, φ, α, M = {(kj, vj)}S 2: while not done do 3: 4: 5: 6:
Sample a batch of training data {(xi, yi)}B for all (xi, yi) in current batch do
Compute the feature vector zi = µθ(xi) (φ) = (cid:80)S
Compute Linner
Finetune φ: φi = φ − α∇φLinner j=1 ω(zi, kj)L(fφ(kj), vj) (φ) j=1 i=1 i i end for
Compute Lmeta(φ, θ, M, α) = 1
B
Update model parameters Θ = {θ, φ, M, α} using gradient descent as Θ ← Θ − η∇ΘLmeta (cid:80)B i=1 L(fφi(µθ(xi)), yi) 7: 8: 9: 10:
Figure 2: Model overview. 11: end while
The retrieved entries are then utilized to ﬁnetune fφ following
φi ← φ − α∇φ
S (cid:88) j=1
ω(xi, kj)L(fφ(kj), vj), (1) where α is the ﬁnetuning step size. Note we weight the loss terms of all dictionary entries by their similarities to the input test data xi. The intuition is that nearby neighbors play a more important role in placing xi into the correct local manifold. Since the model fφ is specially trained in a meta-learning scheme, it adapts to the local neighborhoods after only a few ﬁnetuning steps.
To better understand our method, we draw connections to other well-known techniques. The above prediction process is similar to a one-step EM algorithm. Speciﬁcally, the dictionary querying step is an analogy to the Expectation step, which determines the latent variables (in our case, the neighborhood assignment). And the ﬁnetuning step is similar to the Maximization step, which maximizes the expected log-likelihood. We can also view this process from a Bayesian perspective, where the initial parameter φ is an empirical prior estimated from data; posteriors are derived from neighbors following the ﬁnetuning steps. The predictive distribution with posterior is used for the
ﬁnal predictions. 2.2
Joint Meta Learning
Above, we assume access to given M and fφ, in this section, we describe our meta-learning mecha-nism to train them jointly. The training strategy of Meta-Neighborhoods resembles MAML [6] in that both adopt a nested optimization procedure, which involves an inner loop update and an outer loop update in each iteration. Note that in contrast to MAML, we are solving a general discriminative task rather than a few-shot task. Given a batch of training data {xi, yi}B i=1 with a batch size B, in the inner loop we ﬁnetune the initial parameter φ to φi in a similar fashion to (1). With φ individually
ﬁnetuned for each training data xi using its corresponding neighborhoods, we then jointly train the model parameter φ, the dictionary M as well as the inner loop learning rate α in the outer loop using the following meta-objective function
Lmeta(φ, M, α) = 1
B
B (cid:88) i=1
L(fφi(xi), yi) = 1
B
B (cid:88) i=1
L(fφ−α∇φLinner i (xi), yi), (2) (φ) = (cid:80)S i where Linner j=1 ω(xi, kj)L(fφ(kj), vj) according to (1). We set α to be a learnable scalar or diagonal matrix. Lmeta encourages learning shared φ, M , and α that are widely applicable for data with the same distribution as the training data. An overview of our model is shown in Fig. 2.
Parameter φ serves as initial weights that can be quickly adapted to a speciﬁed neighborhood. This meta training scheme effectively tackles the overﬁtting problem caused by the limited number of
ﬁnetuning instances, as it explicitly optimizes the generalization performance after ﬁnetuning.
For high-dimensional inputs such as images, learning kj in the input space could be prohibitive.
Therefore, we employ a feature extractor µθ to extract the feature embedding zi = µθ(xi) for each 3
xi and learn kj in the embedding space. We accordingly modify (1) to
φi ← φ − α∇φ
S (cid:88) j=1
ω(µθ(xi), kj)L(fφ(kj), vj), (3) where the attention function ω is employed in embedding space. The meta-objective is accordingly modiﬁed as Lmeta(φ, θ, M, α) = 1 i=1 L(fφi(µθ(xi)), yi). We train θ and other learnable param-B eters jointly. Note that the model without a feature extractor can be viewed as a special case where
µθ is an identity mapping. The pseudocode of our training algorithm is given in Algorithm 1. (cid:80)B
It is also desirable to adjust µθ per-instance. However, when µθ is a deep convolution neural network, tuning the entire feature extractor µθ is computationally expensive. Inspired by FiLM [28], we propose instance-wise FiLM (iFiLM) that adjusts the batch normalization layers individually for each instance. Suppose al ∈ RB×Cl×W l×H l is the output of the lth Batch Normalization layer BNl, where B is batch size, C l is the number of channels, W l and H l are the feature map width and height.
Along with each BNl, we deﬁne a learnable dictionary M l = {kl j, βl j are the j ∈ RCl keys used for querying. γl represent the scale and shift parameters used for adaptation respectively. When querying M l, the outputs al are ﬁrst aggregated across their spatial dimensions using global pooling, i.e. gl = GlobalAvgPool(al) ∈ RB×Cl
. Then, the instance-wise adaptation parameters (cid:98)γl j=1 of size Sl. kl j}Sl j, βl j, γl i are computed as i and (cid:98)βl (cid:98)γl i =
Sl (cid:88) j=1
ω(gl i, kl j)γl j ∈ RCl (cid:98)βl i =
Sl (cid:88) j=1
ω(gl i, kl j)βl j ∈ RCl
, (4) where ω is deﬁned as in (1) and i ∈ {1, 2, . . . , B}. Following FiLM [28], each individual activation al i is then adapted with an afﬁne transformation (cid:98)γl i. Note the transformation is agnostic to spatial position, which helps to reduce the number of learnable parameters in the dictionary M . i + (cid:98)βl i · al 2.3 Other Details and Considerations
In this section, we discuss further implementation details. We also motivate our method from the perspective of k-nearest neighbor (KNN).
Similarity Metrics To implement the attention function ω in (1)(3)(4), we need a similarity metric to compare a input xi with each key kj. We try two types of metrics, cosine similarity and negative
Euclidean distance. The similarities of xi to all keys are normalized using a softmax function with a temperature parameter T [14], i.e.,
ω(xi, kj) = exp(sim(xi, kj)/T ) s=1 exp(sim(xi, ks)/T ) (cid:80)S
, (5) where sim(·) represents the similarity metric.
Initialization of the Dictionary Since we use similarity-based attention function ω in (3), we would like to initialize the key kj to have a similar distribution to zi = µθ(xi), otherwise, kj cannot receive useful training signal at early training steps. To simplify the initialization, we follow [8] to remove the non-linear function (e.g. ReLU) at the end of µθ so that features extracted by µθ are approximately Gaussian distributed. With this modiﬁcation, we can simply initialize kj with
Gaussian distribution.
Cosine-similarity Based Classiﬁcation Since the model fφ is ﬁnetuned using the learned dictio-nary in the inner loop, the quality of the dictionary has a signiﬁcant impact on ﬁnal performance.
Typical neural network classiﬁers employ dot product to compute the logits, where the magnitude and direction could both affect the results. Therefore, the model needs to learn both the magnitude and the direction of kj. To alleviate the training difﬁculty, we eliminate the inﬂuence of magnitude by using a cosine similarity classiﬁer, where only the direction of kj can affect the computation of logits. Cosine similarity classiﬁers have been adopted to replace dot product classiﬁers for few-shot learning [8, 2] and robust classiﬁcation [40]. 4
Relationship to KNN Below, we show that Meta-Neighborhoods can be derived as a direct gener-alization of KNN under a multi-task learning framework. Considering a regression task where the regression target is a scalar, the standard view of KNN is as follows. First, aggregate the k-nearest neighbors of a query ˜xi from the training set D as N( ˜xi) = {(xj, yj)}k j=1 ⊂ D. Then, predict an average of the responses in the neighborhood: ˆy = 1 k j=1 yj. (cid:80)k (cid:80)k
Instead of simply performing an average of responses in a neighborhood, we frame KNN as a solution to a multi-task learning problem with tasks corresponding to individual neighborhoods as follows. Here, we take each query (test) data ˜xi as a single task, Ti. To ﬁnd the optimal estimator on the neighborhood N( ˜xi) = {(xj, yj)}k j=1, we optimize the following loss LTi(fi) = 1 j=1 L(fi(xj), yj) where L is a supervised loss, and fi is the estimator to be optimized. For k example, for MSE-based regression the loss for each task is LTi(fi) = 1 j=1(fi(xj) − yj)2. If one k takes fi to be a constant function fi(ηj) = Ci, then the loss is simply LTi(fi) = 1 j=1(Ci − ζj)2, k which leads to an optimal fi( ˜xi) = Ci = 1 j=1 ζj, the same solution as traditional KNN. Similar k observations hold for classiﬁcation. Thus, given neighborhood assignments, one can view KNN as solving for individual tasks in the special case of a constant estimator fi(xj) = Ci. (cid:80)k (cid:80)k (cid:80)k
With the multi-task formulation of KNN, we can generalize KNN to derive our Meta-Neighborhoods method by considering a non-constant estimator as fi. For instance, one may take fi as a parametric output function fφi (e.g. a linear model or neural networks), and ﬁnetune the parameter φ to φi for a data xi according to the loss on neighborhood N(xi). Instead of ﬁtting a single label on the neighborhood, a parametric approach attempts to ﬁt a richer (e.g. linear) dependency between input features and labels for the neighborhood. In addition, the multi-task formulation gives rise to a way of constructing meta-learning episodes. Also, we learn both neighborhoods and the function fφ jointly in our Meta-Neighborhoods framework. 3