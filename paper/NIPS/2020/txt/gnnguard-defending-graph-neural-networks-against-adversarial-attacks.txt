Abstract
Deep learning methods for graphs achieve remarkable performance across a variety of domains. However, recent ﬁndings indicate that small, unnoticeable pertur-bations of graph structure can catastrophically reduce performance of even the strongest and most popular Graph Neural Networks (GNNs). Here, we develop
GNNGUARD, a general algorithm to defend against a variety of training-time attacks that perturb the discrete graph structure. GNNGUARD can be straight-forwardly incorporated into any GNN. Its core principle is to detect and quantify the relationship between the graph structure and node features, if one exists, and then exploit that relationship to mitigate negative effects of the attack. GNN-GUARD learns how to best assign higher weights to edges connecting similar nodes while pruning edges between unrelated nodes. The revised edges allow for robust propagation of neural messages in the underlying GNN. GNNGUARD introduces two novel components, the neighbor importance estimation, and the layer-wise graph memory, and we show empirically that both components are necessary for a successful defense. Across ﬁve GNNs, three defense methods, and four datasets, including a challenging human disease graph, experiments show that GNNGUARD outperforms existing defense approaches by 15.3% on average. Remarkably, GN-NGUARD can effectively restore state-of-the-art performance of GNNs in the face of various adversarial attacks, including targeted and non-targeted attacks, and can defend against attacks on heterophily graphs. 1

Introduction
Deep learning on graphs and Graph Neural Networks (GNNs), in particular, have achieved remarkable success in a variety of application areas [1, 2, 3, 4, 5]. The key to the success of GNNs is the neural message passing scheme [6] in which neural messages are propagated along edges of the graph and typically optimized for performance on a downstream task. In doing so, the GNN is trained to aggregate information from neighbors for every node in each layer, which allows the model to eventually generate representations that capture useful node feature as well as topological structure information [7]. While the aggregation of neighbor nodes’ information is a powerful principle of representation learning, the way that GNNs exchange that information between nodes makes them vulnerable to adversarial attacks [8].
Adversarial attacks on graphs, which carefully rewire the graph topology by selecting a small number of edges or inject carefully designed perturbations to node features, can contaminate local node neighborhoods, degrade learned representations, confuse the GNN to misclassify nodes in the graph, and can catastrophically reduce the performance of even the strongest and most popular GNNs [9, 10].
The lack of GNN robustness is a critical issue in many application areas, including those where adversarial perturbations can undermine public trust [11], interfere with human decision making [12], and affect human health and livelihoods [13]. For this reason, it is vital to develop GNNs that are 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
robust against adversarial attacks. While the vulnerability of machine learning methods to adversarial attacks has raised many concerns and has led to theoretical insights into robustness [14] and the development of effective defense techniques [9, 12, 15], adversarial attacks and defense on graphs remain poorly understood.
Present work. Here, we introduce GNNGUARD1, an ap-proach that can defend any GNN model against a variety of training-time attacks that perturb graph structure (Fig-ure 1). GNNGUARD takes as input an existing GNN model.
It mitigates adverse effects by modifying the
GNN’s neural message passing operators. In particular, it revises the message passing architecture such that the revised model is robust to adversarial perturbations while at the same time the model keeps it representation learn-ing capacity. To this end, GNNGUARD develops two key components that estimate neighbor importance for every node and coarsen the graph through an efﬁcient memory layer. The former component dynamically adjusts the rel-evance of nodes’ local network neighborhoods, prunes likely fake edges, and assigns less weight to suspicious edges based on network theory of homophily [16]. The latter components stabilizes the evolution of graph struc-ture by preserving, in part the memory from a previous layer in the GNN.
Figure 1: A. Small, adversarial perturbations of the graph structure and node features lead
GNN to misclassify target u. B. The GNN, when integrated with GNNGUARD, correctly predicts u’s label.
We compare GNNGUARD to three state-of-the-art GNN defenders across four datasets and under a variety of attacks, including direct targeted, inﬂuence targeted, and non-targeted attacks. Experiments show that GNNGUARD improves state-of-the-art methods by up to 15.3% in defense performance.
Importantly, unlike existing GNN defenders [17, 18, 19, 20], GNNGUARD is a general approach and can be effortlessly combined with any GNN architecture. To that end, we integrate GNNGUARD into
ﬁve GNN models. Remarkably, results show that GNNGUARD can effectively restore state-of-the-art performance of even the strongest and most popular GNNs [3, 21, 7, 22, 23], thereby demonstrating broad applicability and relevance of GNNGUARD for graph machine-learning. Finally, GNNGUARD is the ﬁrst technique that shows a successful defense on heterophily graphs [24]. In contrast, previous defenders, e.g., [17, 18, 19, 20], focused on homophily graphs [16]. Results show that GNNGUARD can be easily generalized to graphs with abundant structural equivalences where connected nodes can have different node features yet similar structural roles within their local topology [25]. 2