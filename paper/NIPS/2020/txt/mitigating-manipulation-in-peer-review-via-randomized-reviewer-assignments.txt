Abstract
We consider three important challenges in conference peer review: (i) reviewers maliciously attempting to get assigned to certain papers to provide positive reviews, possibly as part of quid-pro-quo arrangements with the authors; (ii) “torpedo reviewing,” where reviewers deliberately attempt to get assigned to certain papers that they dislike in order to reject them; (iii) reviewer de-anonymization on release of the similarities and the reviewer-assignment code. On the conceptual front, we identify connections between these three problems and present a framework that brings all these challenges under a common umbrella. We then present a (randomized) algorithm for reviewer assignment that can optimally solve the reviewer-assignment problem under any given constraints on the probability of assignment for any reviewer-paper pair. We further consider the problem of restricting the joint probability that certain suspect pairs of reviewers are assigned to certain papers, and show that this problem is NP-hard for arbitrary constraints on these joint probabilities but efﬁciently solvable for a practical special case. Finally, we experimentally evaluate our algorithms on datasets from past conferences, where we observe that they can limit the chance that any malicious reviewer gets assigned to their desired paper to 50% while producing assignments with over 90% of the total optimal similarity. 1

Introduction
Peer review, the evaluation of work by others working in the same ﬁeld as the producer of the work or with similar competencies, is a critical component of scientiﬁc research. It is regarded favorably by a signiﬁcant majority of researchers and is seen as being essential to both improving the quality of published research and validating the legitimacy of research publications [1–3]. Due to the wide adoption of peer review in the publication process in academia, the peer-review process can be very high-stakes for authors, and the integrity of the process can signiﬁcantly inﬂuence the careers of the authors (especially due to the prominence of a “rich get richer” effect in academia [4]).
However, there are several challenges that arise in peer review relating to the integrity of the review process. In this work, we address three such challenges for peer review in academic conferences where a number of papers need to be assigned to reviewers at the same time. (1) Untruthful favorable reviews. In order to achieve a good reviewer assignment, peer review systems must solicit some information about their reviewers’ knowledge and interests (e.g., through bidding). This can be manipulated, and in fact this is known to have happened in at least one ACM 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
conference [5]: “Another SIG community has had a collusion problem where the investigators found that a group of PC members and authors colluded to bid and push for each other’s papers violating the usual conﬂict-of-interest rules.” The problem of manipulation is not limited to the bidding system, as practically anything used to determine paper assignments (e.g., self-reported area of expertise, list of papers the reviewer has published) can potentially be manipulated [6, 7]. In some cases, unethical authors may enter into deals with potential reviewers for their paper, where the reviewer agrees to attempt to get assigned to the author’s paper and give it a favorable review in exchange for some outside reward (e.g., as part of a quid-pro-quo arrangement for the reviewer’s own paper in another publication venue). To preserve the integrity of the reviewing process and maintain community trust, the paper assignment algorithm should guarantee the mitigation of these kinds of arrangements. (2) Torpedo reviewing. In “torpedo reviewing,” unethical reviewers attempt to get assigned to papers they dislike with the intent of giving them an overly negative review and blocking the paper from publication. This can have wide-reaching consequences [8]: “Repeated indeﬁnitely, this gives the power to kill off new lines of research to the 2 or 3 most close-minded members of a community, potentially substantially retarding progress for the community as a whole.” One special case of torpedo reviewing has been called “rational cheating,” referring to reviewers negatively reviewing papers that compete with their own authored work [9, 10]. A paper assignment algorithm should guarantee to authors that their papers are unlikely to have been torpedo-reviewed. (3) Reviewer de-anonymization in releasing assignment data. For transparency and research purposes, conferences may wish to release the paper-reviewer similarities and the paper assignment algorithm used after the conference. However, if the assignment algorithm is deterministic, this would allow for authors to fully determine who reviewed their paper, breaking the anonymity of the reviewing process. Even when reviewer and paper names are removed, identities can still be discovered (as in the case of the Netﬂix Prize dataset [11]). Consequently, a rigorous guarantee of anonymity is needed in order to release the data.
Although these challenges may seem disparate, we address all of them under a common umbrella framework. Our contributions are as follows. Conceptually, we formulate problems concerning the three aforementioned issues, and propose a framework to address them through the use of randomized paper assignments (Section 3). Theoretically, we design computationally efﬁcient, randomized assignment algorithms that optimally assign reviewers to papers subject to given restrictions on the probability of assigning any particular reviewer-paper pair (Section 4). We further consider the more complex case of preventing suspicious pairs of reviewers from being assigned to the same paper (Section 5). We show that ﬁnding the optimal assignment subject to arbitrary constraints on the probabilities of reviewer-reviewer-paper assignments is NP-hard. In the practical special case where the program chairs want to prevent pairs of reviewers within the same subset of some partition of the reviewer set (e.g., by academic institution) from being assigned to the same paper, we present an algorithm that ﬁnds the optimal randomized assignment with this guarantee. Empirically, we test our algorithms on datasets from past conferences and show their practical effectiveness (Section 6). As a representative example, on data reconstructed from ICLR 2018, our algorithms can limit the chance of any reviewer-paper assignment to 50% while achieving 90.8% of the optimal total similarity.
Our algorithms can continue to achieve this similarity while also preventing reviewers with close associations from being assigned to the same paper. We further demonstrate, using the ICLR 2018 dataset, that our algorithm successfully prevents manipulation of the assignment by a simulated malicious reviewer.
All of the code for our algorithms and our empirical results is freely available online.1 2 Related Literature
Many paper assignment algorithms for conference peer review have been proposed in past work.
The widely-used Toronto Paper Matching System (TPMS) [12] computes a similarity score for each reviewer-paper pair based on analysis of the reviewers’ past work and bids, and then aims to maximize the total similarity of the resulting assignment. The framework of “compute similarities and maximize total similarity” (and similar variants) encompasses many paper assignment algorithms, where similarities can be computed in various ways from automated and manual analysis and 1https://github.com/theryanl/mitigating_manipulation_via_randomized_reviewer_ass ignment/ 2
reviewer bids [13–19]. We treat the bidding process and computation of similarities as given, and focus primarily on adjusting the optimization problem to address the three aforementioned challenges.
There are also a number of recent works [20–34] which deal with other aspects of peer review.
Much prior work has studied the issue of preventing or mitigating strategic behavior in peer review.
This work usually focuses on the incentives reviewers have to give poor reviews to other papers in the hopes of increasing their own paper’s chances of acceptance [35–39]. Unlike the issues we deal with in this paper, these works consider only reviewers’ incentives to get their own paper accepted and not other possible incentives. We instead consider arbitrary incentives for a reviewer to give an untruthful review, such as a personal dislike for a research area or misincentives brought about by author-reviewer collusion. Instead of aiming to remove reviewer incentives to be untruthful, our work focuses on mitigating the effectiveness of manipulating the reviewer assignment process.
Randomized assignments have been used to address the problem of fair division of indivisible goods such as jobs or courses [40, 41], as well as in the context of Stackelberg security games [42]. The paper [22] uses randomization to address the issue of miscalibration in ratings, such as those given to papers in peer review. To the best of our knowledge, the use of randomized reviewer-paper assignments to address the issues of malicious reviewers or reviewer de-anonymization in peer review has not been studied previously.
Additional coverage of related literature is presented in Appendix A. 3