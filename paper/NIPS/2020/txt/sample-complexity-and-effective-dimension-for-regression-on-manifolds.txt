Abstract
We consider the theory of regression on a manifold using reproducing kernel
Hilbert space methods. Manifold models arise in a wide variety of modern machine learning problems, and our goal is to help understand the effectiveness of vari-ous implicit and explicit dimensionality-reduction methods that exploit manifold structure. Our ﬁrst key contribution is to establish a novel nonasymptotic version of the Weyl law from differential geometry. From this we are able to show that certain spaces of smooth functions on a manifold are effectively ﬁnite-dimensional, with a complexity that scales according to the manifold dimension rather than any ambient data dimension. Finally, we show that given (potentially noisy) function values taken uniformly at random over a manifold, a kernel regression estimator (derived from the spectral decomposition of the manifold) yields minimax-optimal error bounds that are controlled by the effective dimension. 1

Introduction
High-dimensional data is ubiquitous in modern machine learning. Examples include images (2-D and 3-D), document texts, DNA, and neural recordings. In many cases, the number of dimensions in the data is much larger than the number of actual data samples. Traditional statistical methods cannot handle such cases, so researchers have turned to a variety of explicit dimensionality-reduction techniques—which make inference more tractable—and to tools such as neural networks that of-ten implicitly transform the data into a much lower-dimensional feature space. These techniques inherently assume that the data have an intrinsic dimension that is much lower than that of the data’s original representation. Our goal in this paper is to show that the difﬁculty of a supervised learning problem depends only on this intrinsic dimension and not on the (potentially much larger) ambient dimension. In particular, we consider the common assumption that the data lie on a low-dimensional manifold embedded in Euclidean space (see [1–4] for some of the many example applications).
As an illustration of the kind of results we hope to obtain, we ﬁrst consider a simple example: a function on the circle S1 (or, equivalently, a periodic function on the real line). Speciﬁcally, suppose that we want to estimate a function f ∗ on the circle from random samples. In general, it is intractable to estimate an arbitrary function from ﬁnitely many samples, but it becomes possible if we assume f ∗ is structured. For example, f ∗ may exhibit a degree of smoothness, which can be readily characterized via the Fourier series for f ∗. Speciﬁcally, recall that we can write f ∗ as the Fourier series sum f ∗(x) = a0 + (cid:80) (cid:96)≥1(a(cid:96) cos(2π(cid:96)x) + b(cid:96) sin(2π(cid:96)x)). One common notion of smoothness in signal processing is that f ∗ is bandlimited, meaning that this sum can be truncated at some largest frequency
Ω. In this case, f ∗ lies in a subspace of dimension at most p(Ω) = 2(cid:98)Ω/2π(cid:99) + 1. We know (see, e.g., [5, Chapter 12] or [6]) that we can recover such a function exactly, with high probability, from n (cid:38) p(Ω) log p(Ω) samples placed uniformly at random. If there is measurement noise, the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
squared L2 error due to noise scales like p(Ω) n σ2. In higher dimensions (say, on the torus T m), an
Ω-bandlimited function lies in a space of dimension p(Ω) = O(Ωm), and the number of random samples required scales accordingly.
Another model for smoothness is that f ∗, rather than being bandlimited, has exponentially-decaying frequency components. For example, suppose the Fourier coefﬁcients satisfy (cid:80) (cid:96) ) < ∞ for some t > 0 (this is roughly equivalent to f ∗ being the convolution of a Gaussian function with an arbitrary function in L2). The space of such functions is inﬁnite-dimensional, but any function in it can be approximated as Ω-bandlimited to within an error of size O(e−cΩ2t), which should enable us to recover a close approximation to f ∗ from O(p(Ω) log p(Ω)) samples. (cid:96) + b2 (cid:96) et(cid:96)2 (a2
In this paper, we provide precise analogs of these sample complexity results in the general case of a function on an arbitrary manifold M with dimension m. As on the circle or torus, an L2 function f (x) on a Riemannian manifold has a spectral decomposition into modes u(cid:96)(x) corresponding to vibrational frequencies ω(cid:96) for all non-negative integers (cid:96); these modes are the eigenfunctions of the Laplace-Beltrami operator on M. Our ﬁrst key contribution (described in Theorem 2) is a nonasymptotic version of the Weyl law from differential geometry: this states that, for large enough Ω, the set Hbl
Ω of Ω-bandlimited functions on M (functions composed of modes with frequencies below
Ω) has dimension dim(Hbl
Ω ) ≤ Cm vol(M) Ωm =: p(Ω). Thus the number of degrees of freedom scales according to the manifold dimension m rather than a larger ambient dimension.
Our second key contribution is an error bound for recovering functions on M from randomly-placed samples using kernel regression. We show in Theorem 3 that if we take n (cid:38) p(Ω) log p(Ω) samples of f ∗, we can recover any Ω-bandlimited function with error (cid:107) ˆf − f ∗(cid:107)2
L2 vol(M) (cid:46) p(Ω) n
σ2, which is precisely the error rate for parametric regression in a D(Ω)-dimensional space. Our results extend further to approximately-bandlimited functions: for example, if f ∗ satisﬁes (cid:80) (cid:96) < ∞, where f ∗ = (cid:80) (cid:96) a2 (cid:96) a(cid:96)u(cid:96), then, again with n (cid:38) p(Ω) log p(Ω) samples, we get (Theorem 4) (cid:96) etω2 (cid:107) ˆf − f ∗(cid:107)2
L2 vol(M) (cid:46) p(Ω) n
σ2 + O(e−cΩ2t).
Both bounds are minimax optimal in the presence of noise.
These results follow from our Theorem 1, which is a more general result on regression in a reproducing kernel Hilbert space. Theorems 3 and 4 adapt this result to a speciﬁc choice of kernel.
The paper is organized as follows. Sections 2 and 3 describe our framework, survey the relevant literature, and compare it to our results. Section 4 contains our main theoretical results. The proofs are in the appendices in the supplementary material. The key technical results are Theorem 1, which is proved via empirical risk minimization and operator concentration inequalities, and Lemma 1 (used to prove Theorem 2), which is proved via heat kernel comparison results on manifolds of bounded curvature. 2 Framework and notation 2.1 Kernel regression and interpolation
Kernels provide a convenient and popular framework for nonparametric function estimation. They allow us to treat the evaluation of a nonlinear function as a linear operator on a Hilbert space, and they give us a computationally feasible way to estimate such a function (which is often in an inﬁnite-dimensional space) from a ﬁnite set of samples. Here, we review some of the key ideas that we will need in analyzing kernel methods.
Let S be an arbitrary set, and suppose k : S × S → R is a positive deﬁnite kernel. Let H be its associated reproducing kernel Hilbert space (RKHS), characterized by the identity f (x) = (cid:104)f, k(·, x)(cid:105)H for all f ∈ H and x ∈ S.
Now, suppose we have X1, . . . , Xn ∈ S, f ∗ ∈ H is an unknown function, and we observe Yi = f ∗(Xi) + ξi for i = 1, . . . , n, where the ξi’s represent noise. A common estimator for f ∗ is the 2
regularized empirical risk minimizer
ˆf = arg min f ∈H 1 n n (cid:88) (Yi − f (Xi))2 + α(cid:107)f (cid:107)2
H, i=1 where α ≥ 0 is a regularization parameter. The solution to the optimization problem (1) is
ˆf (x) = n (cid:88) i=1 aik(x, Xi), where a = (a1, . . . , an) ∈ Rn is given by a = (nαIn + K)−1Y , (1) (2) where Y = (Y1, . . . , Yn) ∈ Rn, K is the kernel matrix on X1, . . . , Xn deﬁned by Kij = k(Xi, Xj), and In is the n × n identity matrix.
In general, ˆf corresponds to a ridge regression estimate of f ∗. The limiting case α = 0 can be recast as the problem
ˆf = arg min f ∈H (cid:107)f (cid:107)H s.t. Yi = f (Xi), i = 1, . . . , n.
In this case, if the Xi’s are distinct, then ˆf interpolates the measured values of f ∗. 2.2 Kernel integral operator and eigenvalue decomposition
A common tool for analyzing kernel interpolation and regression, which will play a central role in our analysis in Section 4, is the eigenvalue decomposition of a kernel’s associated integral operator.
The integral operator T is deﬁned for functions f on S by (T (f ))(x) = (cid:90)
S k(x, y)f (y) dµ(y), where µ is a measure on S. Under certain assumptions1 on S, µ, and k, T is a well-deﬁned operator on L2(S), is compact and positive deﬁnite with respect to the L2 inner product, and has eigenvalue decomposition
T (f ) =
∞ (cid:88) (cid:96)=1 t(cid:96)(cid:104)f, v(cid:96)(cid:105)L2 v(cid:96), f ∈ L2(S), where the eigenvalues {t(cid:96)} are arranged in decreasing order and converge to 0, and the eigenfunctions
{v(cid:96)} are an orthonormal basis for L2(S). We also have k(x, y) = (cid:80)∞ (cid:96)=1 t(cid:96)v(cid:96)(x)v(cid:96)(y), where the convergence is uniform and in L2.
This eigendecomposition plays an important role in characterizing the RKHS H associated with the kernel k. Combining this expression for k with the identity (cid:104)f, k(·, x)(cid:105)H = f (x), we can derive the fact that, for all f, g ∈ H, (cid:104)f, g(cid:105)H =
∞ (cid:88) (cid:96)=1 (cid:104)f, v(cid:96)(cid:105)L2 (cid:104)g, v(cid:96)(cid:105)L2 t(cid:96)
.
This implies that (cid:104)f, g(cid:105)L2 = (cid:104)T 1/2(f ), T 1/2(g)(cid:105)H for all f, g ∈ L2(S). Thus T 1/2 is an isometry from L2(S) to H, and so for any f ∈ H, we can write f = T 1/2(f0), where (cid:107)f0(cid:107)L2 = (cid:107)f (cid:107)H. This implies that, for any p ≥ 1, the projection of f onto (span{v1, . . . , vp})⊥ has L2 norm at most
√ tp+1(cid:107)f (cid:107)H. Hence the decay of the eigenvalues {t(cid:96)} of T characterizes the “effective dimension” of H in L2, which will be a fundamental building block for our analysis. 1E.g., S is a compact metric space; µ is strictly positive, ﬁnite, and Borel; and k is continuous [7]. 3
2.3 Spectral decomposition of a manifold and related kernels
We now turn to our speciﬁc problem of regression on a manifold, considering how an RKHS framework can help us. The book [8] is an excellent reference for the material in this section.
A smooth, compact Riemannian manifold M (without boundary) can be analyzed via the spectral decomposition of its Laplace-Beltrami operator ∆M (we will often call it the Laplacian for short).
This operator is deﬁned as ∆Mf := − div(∇f ). In Rm, it is simply the operator − (cid:80)m
. The
Laplacian can be diagonalized as
∂2
∂x2 i i=1
∆Mf =
∞ (cid:88) (cid:96)=0
λ(cid:96)(cid:104)f, u(cid:96)(cid:105)L2u(cid:96), where 0 = λ0 < λ1 ≤ λ2 ≤ · · · , the sequence λ(cid:96) → ∞ as (cid:96) → ∞, and {u(cid:96)} is an orthonormal basis for L2(M) (all integrals are with respect to the standard volume measure on M).
The eigenvalues {λ(cid:96)} are the squared resonant frequencies of M, and the eigenfunctions {u(cid:96)} are the vibrating modes, since solutions to the wave equation ftt + ∆Mf = 0 on M have the form f (t, x) =
∞ (cid:88) (cid:96)=0 (a(cid:96) sin (cid:112)
λ(cid:96)t + b(cid:96) cos (cid:112)
λ(cid:96)t)u(cid:96)(x).
The classical Weyl law (e.g., [8, p. 9]) says that, if M has dimension m, then, asymptotically,
|{(cid:96) : λ(cid:96) ≤ λ}| ∼ cm vol(M)λm/2 as λ → ∞, where cm = (2π)−m Vm, with Vm denoting the volume of the unit ball in Rm.
Using the spectral decomposition of the Laplacian, any number of kernels can be deﬁned by k(x, y) =
∞ (cid:88) (cid:96)=0 g(λ(cid:96))u(cid:96)(x)u(cid:96)(y) for some function g. With this construction, the integral operator of k has eigenvalue decomposition
T (f ) = (cid:80) (cid:96)≥0 g(λ(cid:96))(cid:104)f, u(cid:96)(cid:105)L2u(cid:96), hence, per Section 2.2, (cid:107)f (cid:107)2 (cid:96)≥0(cid:104)f, u(cid:96)(cid:105)2/g(λ(cid:96)).
H = (cid:80)
Our results could, in principle, apply to many kernels with the above form, but we will primarily consider bandlimited kernels and the heat kernel. The bandlimited kernel with bandlimit Ω > 0 is (cid:88) kbl
Ω (x, y) = u(cid:96)(x)u(cid:96)(y), which is the reproducing kernel of the space of bandlimited functions on M:
Hbl
Ω = (cid:8)f ∈ L2(M) : f ∈ span{u(cid:96) : λ(cid:96) ≤ Ω2}(cid:9)
λ(cid:96)≤Ω2 with (cid:107)f (cid:107)Hbl
Ω . The heat kernel is a natural counterpart to the common Gaussian radial basis function on Rm. Detailed treatments can be found in [8, 9]. We will deﬁne it for t > 0 as
= (cid:107)f (cid:107)L2 for f ∈ Hbl
Ω kh t (x, y) =
∞ (cid:88) (cid:96)=0 e−λ(cid:96)t/2u(cid:96)(x)u(cid:96)(y).
Its corresponding RKHS is (cid:40)
Hh t = f ∈ L2(M) : (cid:107)f (cid:107)2
Hh t eλ(cid:96)t/2(cid:104)f, u(cid:96)(cid:105)2
L2
< ∞
. (cid:41)
=
∞ (cid:88) (cid:96)=0
The heat kernel kh ft + 1 2 ∆Mf = 0 on M. The heat kernel on Rm is kh t gets its name from the fact that it is the fundamental solution to the heat equation t (x, y) = (2πt)m/2 e−(cid:107)x−y(cid:107)2/2t. 1 4
3