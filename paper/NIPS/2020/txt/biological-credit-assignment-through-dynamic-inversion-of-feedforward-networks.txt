Abstract
Learning depends on changes in synaptic connections deep inside the brain. In multilayer networks, these changes are triggered by error signals fed back from the output, generally through a stepwise inversion of the feedforward processing steps.
The gold standard for this process — backpropagation — works well in artiﬁcial neural networks, but is biologically implausible. Several recent proposals have emerged to address this problem, but many of these biologically-plausible schemes are based on learning an independent set of feedback connections. This complicates the assignment of errors to each synapse by making it dependent upon a second learning problem, and by ﬁtting inversions rather than guaranteeing them. Here, we show that feedforward network transformations can be effectively inverted through dynamics. We derive this dynamic inversion from the perspective of feedback control, where the forward transformation is reused and dynamically interacts with ﬁxed or random feedback to propagate error signals during the backward pass. Importantly, this scheme does not rely upon a second learning problem for feedback because accurate inversion is guaranteed through the network dynamics.
We map these dynamics onto generic feedforward networks, and show that the resulting algorithm performs well on several supervised and unsupervised datasets.
Finally, we discuss potential links between dynamic inversion and second-order optimization. Overall, our work introduces an alternative perspective on credit assignment in the brain, and proposes a special role for temporal dynamics and feedback control during learning. 1

Introduction
Synaptic credit assignment refers to the difﬁcult task of relating a motor or behavioral output of the brain to the many neurons and synapses that produced it (Roelfsema and Holtmaat, 2018) — a problem which must be solved in order for effective learning to occur. While credit is assigned in artiﬁcial neural networks through the backpropagation of error gradients (Rumelhart et al., 1986;
LeCun et al., 2015), a direct mapping of this algorithm to biology leads to several characteristics that are either in conﬂict with what is currently known about neural circuits, or that violate harder physical constraints, such as the local nature of synaptic plasticity (Grossberg, 1987; Crick, 1989).
Many biologically-plausible modiﬁcations to backpropagation have been proposed over the years (Whittington and Bogacz, 2019), with several recent studies focusing on one issue in particular, the fact that error is fed back using an exact copy of the forward weights (the “weight transport” or “weight symmetry” problem, Lillicrap et al. (2020)). Recently, it was discovered that random feedback weights are sufﬁcient to train deep networks on modest supervised learning problems (Lillicrap et al., 2016). However, this method appears to have shortcomings in scaled-up tasks, as
∗Correspondence: {william.podlaski, christian.machens}@research.fchampalimaud.org 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
well as in convolutional and bottleneck architectures (Bartunov et al., 2018; Moskovitz et al., 2018).
Several studies have therefore aimed to identify the necessary precision of feedback (Nøkland, 2016;
Xiao et al., 2018), and others have proposed to learn separate feedback connections (Kolen and
Pollack, 1994; Bengio, 2014; Lee et al., 2015; Akrout et al., 2019; Lansdell et al., 2019). While it is indeed plausible that feedback weights are updated alongside forward ones, these schemes complicate credit assignment by making error backpropagation dependent upon an additional learning problem (with uncertain accuracy), and by potentially introducing more learning phases.
One important characteristic of biological neural circuits is their dynamic nature, which has been harnessed in many previous learning models (Hinton et al., 1995; O’Reilly, 1996; Rao and Ballard, 1999). Here, we take inspiration from this dynamical perspective, and propose a model of error backpropagation as a feedback control problem — during the backward pass, feedback connections are used in concert with forward connections to dynamically invert the forward transformation, thereby enabling errors to ﬂow backward. Importantly, this inversion works with arbitrary ﬁxed
In the feedback weights, and avoids introducing a second learning problem for the feedback. following, we derive this dynamic inversion, map it onto deep feedforward networks, and demonstrate its performance on several supervised tasks, as well as an autoencoder task. Then, we discuss the biological implications of this perspective, possible links to second-order learning, and its relation to previous dynamic algorithms for credit assignment. 2 Deep learning in feedforward networks 2.1 Notation and forward transformation
We consider nonlinear feedforward networks with L layers. The forward pass (forward transformation;
Fig. 1a) from one layer to the next is hl = g(al) = g(Wlhl−1), (1) where hl ∈ Rdl is the activity of layer l, g(·) is an arbitrary element-wise nonlinearity, al ∈ Rdl is the “pre-activation” activity of layer l, and Wl ∈ Rdl×dl−1 denotes the weight matrix from layers l − 1 to l (including bias). The input data, network output, and supervised target are denoted x = h0, y = hL, and t, respectively. The error is denoted e = y − t, and the loss function is L(x, t). 2.2 Error backpropagation and inversion of the forward transformation
For such networks, each layer’s weights are commonly optimized using gradient descent:
∆Wl ∝ −
∂L
∂Wl
= −
∂L
∂al
∂al
∂Wl
= −δlhT l−1, with the backpropagated error δl = ∂L/∂al ∈ Rdl . We write δl in a generalized recursive form
δl−1 =
∂al
∂al−1
δl = Mlδl ◦ g(cid:48)(al−1) = Dl−1Mlδl, (2) (3) where ◦ is the Hadamard (element-wise) product, Dl = diag(g(cid:48)(al)), and Ml = WT the feedback weight matrix (the source of the weight transport problem). l ∈ Rdl−1×dl is
As mentioned above, learning can sometimes be achieved with a ﬁxed random feedback matrix, a strategy termed feedback alignment (FA), in part due to the observed alignment between the forward weights and the pseudoinverse of the feedback weights during training (Lillicrap et al., 2016).
The authors of this study also describe a biologically-implausible idealization of this algorithm, pseudobackprop (PBP), which propagates errors through the pseudoinverse of the current feedforward weights. These results, as well as other studies proposing to learn feedback as an inverted forward transformation (e.g., target prop, Bengio (2014); Lee et al. (2015)), motivate the perspective that the goal of credit assignment is to invert the feedforward transformation of the network.
We summarize these variants of backpropagation as different choices for Ml in Eq. (3):
Ml =



WT l
Bl
W+ l for backpropagation (BP) for feedback alignment (FA) for pseudobackprop (PBP) (4) where Bl is a ﬁxed random matrix and W+ l is the Moore-Penrose pseudoinverse of Wl. 2
Figure 1: Schematic of forward and backward passes. a: Standard forward pass from Eq. (1). b: error propagation formulated as a feedback control problem — the difference between the forward transformation (˜δl) and the target error value (δl) is integrated and fed back to produce a new target error δl−1. c: Dynamic inversion during the backward pass implements this control problem. 3 Dynamic inversion as feedback control
We now introduce a simple recurrent architecture (Fig. 1b,c) which dynamically and implicitly performs inversions similar to those explicitly performed by pseudobackprop and target prop as outlined above. Considering the backward pass of a linear feedforward network (Eq. (1), with g(x) = x), the error from the l-th layer, δl, should be transformed into an error for the (l − 1)-th layer, δl−1. From a linear feedback control perspective, we can let the l-th layer feed a control signal, u(t) ∈ Rdl , into the (l − 1)-th layer, such that the state of this layer, δl−1, when propagated through the feedforward transformation of the network, reproduces, as close as possible, the target error vector, δl, of layer l. We deﬁne this as a linear control problem of the following form:
˙δl−1(t) = −δl−1(t) + Blul(t) (5)
˜δl(t) = Wlδl−1(t), (6) where δl−1(t) ∈ Rdl−1 is the system state of layer l − 1, ˜δl(t) ∈ Rdl is the readout or forward transformation of this system, ul(t) ∈ Rdl is the control signal fed back from layer l, and Bl ∈
Rdl−1×dl is a matrix of arbitrary feedback weights. We deﬁne a ﬁxed, target error value for the readout, δl, and a separate controller error, el(t) = ˜δl(t) − δl. 3.1 Leaky integral control
A standard approach in designing a controller is to use a proportional-integral-derivative (PID) formulation (Åström and Murray, 2010) that acts on the controller error el(t), with dynamics
˙ul(t) = Kp ˙el(t) + Kiel(t) + Kd¨el(t) + Kuul(t), where Kp, Ki, and Kd are coefﬁcient matrices for the proportional, integral, and derivative compo-nents, respectively, along with an additional leak component with coefﬁcients Ku. For mathematical simplicity and biological plausibility, we only consider the integral and leak components (see Discus-sion for interpretation of other terms), setting their coefﬁcients to Ki = Il, and Ku = −αIl, where
Il is the identity matrix of size dl. These components have a direct interpretation in rate networks (Dayan and Abbott, 2001), and have been used in other neuroscience and biological contexts (Miller and Wang, 2006; Somvanshi et al., 2015). We thus obtain the leaky integral-only controller (7)
˙ul(t) = −αul(t) + el(t) = −αul(t) + Wlδl−1(t) − δl, (8) which acts on Eq. (5). For a ﬁxed target δl, this controller has the steady-state equality
Wlδl−1 = δl + αul, which suggests that the steady state of δl−1 approximates the target δl through the forward transfor-mation (for small α). For α > 0, we use Eq. (5) in the steady-state to write δl−1 as (9)
δl−1 = Bl(WlBl − αIl)−1δl = (BlWl − αIl−1)−1Blδl.
When α = 0, only one of these equalities will hold, depending on the dimensionalities dl and dl−1.
For expository purposes, we also write the solution as a function of the control signal ul:
δl−1 = MDI (δl + αul), (10) (11) l 3
where
MDI l =



Bl(WlBl)−1
W+ l
W−1 l for dl < dl−1 for dl > dl−1, α > 0 for dl = dl−1, (12) and W+ is the Moore-Penrose pseudoinverse of the forward matrix Wl. We thus see that this l system dynamically inverts the forward transformation of the network (for small α; see Suppl. Fig. 1 for plot of accuracy as a function of α), implicitly solving the linear system Wlδl−1 = δl. For dl ≥ dl−1 (expanding layer), Wl has a well-deﬁned left pseudoinverse (or inverse, for dl = dl−1), and so the inversion follows directly from Eq. (9). In contrast, for dl < dl−1 (contracting layer), the system may have inﬁnite solutions. The dynamics instead solves the fully-determined system (WlBl − αI)ul = δl, which is then projected through Bl to obtain δl−1 (i.e., one solution to the desired linear system, constrained by Bl). 3.2 Linear stability and and initialization
Dynamic inversion will only be useful if it is stable and fast. Integral-only control may exhibit substantial transient oscillations, which can be mitigated if the system dynamics are fast compared to the controller. Assuming this separation of timescales, we can study the controller dynamics from Eq. (8) when the system is at its steady state (δl−1 = Blul from Eq. (5)):
˙ul(t) = (WlBl − αIl)ul(t) − δl. (13)
Linear stability thus depends on the eigenvalues of (WlBl−αI). Generally, the stability of interacting neural populations (and the eigenvalues of arbitrary matrix products), is an open question and we do not aim to solve it here. We instead propose that clever initialization of Bl will provide stability throughout training (in addition to a non-zero leak, α). One easy way to ensure this is to initialize
Bl = −WT l (0), which makes the matrix product negative semi-deﬁnite (zero index indicates the start of training). From Eq. (12), this also means that for dl < dl−1, dynamic inversion will use the
Moore-Penrose pseudoinverse at the start of training. Note that this initialization does not imply a correspondence between the forward and backward weights throughout training, as they may become unaligned when the forward weights are updated. In the case where dl > dl−1, the matrix product is singular and requires α > 0 (but see Supplementary Materials for an alternative architecture). 3.3 Nonlinearities
We now return to the general nonlinear case. Both nonlinear control and nonlinear inverse problems are active areas of research with solutions tailored to particular applications (Slotine et al., 1991;
Mueller and Siltanen, 2012), and several approaches may be suitable here. We discuss two possibili-ties. First, nonlinearities may be directly incorporated into the control problem through the readout
˜δ(t) in Eq. (6), leading to a nonlinear controller with dynamics
˙ul(t) = −αul(t) + Wlg(δl−1(t)) − δl, (14) where g(·) is an arbitrary nonlinearity. We keep the feedback in Eq. (5) linear for simplicity. Compared to Eq. (1), the order of the matrix product and nonlinearity in (14) is reversed to obtain an error with respect to the pre-activation variables as in backpropagation. The steady-state for the controller is
Wlg(δl−1) = δl + αul. (15)
Deriving an explicit relationship between δl−1 and δl is tricky here, especially with common transfer functions like tanh and ReLU, which do not have well-deﬁned inverses (at least numerically). Again for expository purposes, we use somewhat sloppy notation and write an implicit, non-unique inverse g−1(·), for which g−1(g(δl−1)) ≈ δl−1 and g(g−1(δl)) ≈ δl. We can then write δl−1 recursively as (16)
δl−1 = g−1(MDI (δl + αul)), l with MDI analysis still provides a decent indication of stability in the general case. from Eq. (12). Stability is no longer guaranteed, but in practice we ﬁnd that linear stability l
An alternative approach for handling nonlinearities is to keep dynamic inversion linear, and then apply an element-wise multiplication by g(cid:48)(al−1) either during or after convergence (following BP,
Eq. (3)). From Eqs. (10) and (11), this makes the full dynamic inversion error per layer
δl−1 = Bl(WlBl − αIl)−1δl ◦ g(cid:48)(al−1) = (BlWl − αIl−1)−1Blδl ◦ g(cid:48)(al−1), (17) 4
Figure 2: Schematic of forward (left) and backward (right) passes for chained dynamic inversion. a:
Sequential dynamic inversion (right), in which the error is inverted through one layer at a time, with each layer ﬁrst receiving control signals from the layer above, and then acting as the controller for the layer below. B: Repeat layer dynamic inversion, enabling each layer to both give and receive control, so that the full backward pass converges at once. C: Single loop dynamic inversion (SLDI) features feedback from the output layer to the ﬁrst hidden layer, which effectively inverts each hidden layer. or equivalently
δl−1 = MDI l (δl + αul) ◦ g(cid:48)(al−1). (18)
In practice we found this second option to have better performance, so we primarily use this method for the experiments presented below (except for SLDI, see next section). 4 Dynamic inversion of deep feedforward networks
Backpropagation in feedforward networks is a recursive, layer-wise process. However, when chaining together multiple dynamic inversions, each hidden layer must simultaneously serve as the recipient of control from the layer above, as well as the controller for the layer below. We propose three architectures which solve this problem in different ways, illustrated in Fig. 2. 4.1 Architectures for chained dynamic inversion
The most direct way of mapping multiple dynamic inversions onto a feedforward network is to prescribe that each inversion happens sequentially — from the output to the ﬁrst hidden layer — with only one pair of layers dynamically interacting at a time (sequential dynamic inversion, Fig. 2a). Such a scheme begins by feeding the output error, δL, into the output layer, which provides control to the last hidden layer until convergence to the target δL−1. Next, this target is held ﬁxed and is re-passed as input back into layer L − 1, which now acts as a controller for layer L − 2, to obtain the target
δL−2. This is repeated until the ﬁrst hidden layer converges to its target, δ1. This scheme requires a backward pass with multiple steps for networks with more than one hidden layer (L − 1 steps).
The fact that each hidden layer functions as both a recipient of control, and a controller itself, motivates the second architecture, in which the hidden layers have two separate populations, each serving one of these roles (repeat layer dynamic inversion, Fig. 2b). For the forward pass to remain unchanged, these populations (hA l ) have an identity transformation between them. During the backward pass, each controller receives the target value δl as it settles, speeding up convergence.
The steady state errors will be equivalent to the sequential case, but only a single backward pass is needed. Due to the equivalence of this scheme to the ﬁrst, we do not explicitly simulate it here. l and hB
An alternative approach to chaining multiple dynamic inversion control problems together is to turn them into a single problem (single loop dynamic inversion, SLDI, Fig. 2c). In this scheme, the output layer acts as the controller for the activity of the ﬁrst hidden layer, and the forward transformation 5
encompasses all layers in between (see Supplementary Materials for a detailed description). While in some special cases this scheme is equivalent to the ones above, in general the dynamics will converge to a different solution. We test the single loop backward pass on one experiment below (nonlinear regression), but mainly focus on the sequential method. 4.2 Update rules
We deﬁne the backpropagated error signal δl for dynamic inversion (DI) as the steady state of the linearized feedback control dynamics from Eq. (17), and weight updates as in Eq. (2). Biases are not included in the dynamics of the backward pass, but are updated with the layer-wise error signals similar to standard backprop. As a point of comparison, we also implement a non-dynamic inversion (NDI), in which the exact steady state from Eq. (17) is used in lieu of simulating temporal dynamics.
Therefore, correspondence between DI and NDI updates is indicative of successful convergence of the dynamics. In contrast, single-loop dynamic inversion (SLDI) utilizes a nonlinear controller as in
Eq. (14), then weight updates as in Eq. (2) (Supplementary Materials). 4.3 Relation to second-order learning
The inversion of the forward weights in DI suggests a resemblance to second-order learning (Martens, 2014; Lillicrap et al., 2016). Though a full theoretical study is out of the scope of this paper, in the Supplementary Materials, we postulate a link to layer-wise Gauss-Newton optimization (Botev et al., 2017), and describe a simple example. Interestingly, recent work linking target propagation to Gauss-Newton optimization shows that the dynamic inversion of targets rather than errors may produce a more coherent connection to second-order learning (Meulemans et al., 2020; Bengio, 2020).
Speciﬁcally, Meulemans et al. (2020) propose that a direct feedback approach similar to a single-loop architecture applied to each hidden layer (Fig. 2c) may be most effective (see Discussion). 5 Experiments
We tested dynamic inversion (DI) and non-dynamic inversion (NDI) against backpropagation (BP), feedback alignment (FA), and pseudobackprop (PBP) on four modest supervised and unsupervised learning tasks — linear regression, nonlinear regression, MNIST classiﬁcation, and MNIST au-toencoding. We varied the leak values (α) for DI and NDI, as well as the feedback initializations (“Tr-Init”, Bl = −WT l ; “R-Init”, random stable Bl) for DI, NDI, and FA. To impose stability for random initialization, we optimized the feedback matrix Bl using smoothed spectral abscissa (SSA) optimization (Vanbiervliet et al., 2009; Hennequin et al., 2014) at the start of training (Supplementary
Materials). We note that DI remained stable throughout training for all experiments, suggesting that initialization is sufﬁcient to ensure stability. DI was simulated numerically using 1000 Euler steps with dt = 0.5. 5.1 Linear and nonlinear function approximation
Following Lillicrap et al. (2016), we tested the algorithms on a simple linear regression task with a two-layer network (dim. 30-20-10). Training was done with a ﬁxed learning rate (Fig. 3a), or with
ﬁxed norm weight updates (Fig. 3b) in order to probe the update directions that each algorithm ﬁnds.
All algorithms were able to solve this simple task with ease, with DI, NDI, and PBP converging faster than BP and FA in the ﬁxed norm case, suggesting they ﬁnd better update directions. Dynamic inversion remained stable throughout training for all examples shown, with updates well-aligned to the non-dynamic version (Fig. 3c,d). Furthermore, the alignment between the feedback and the negative transpose of the forward weights settled to around 45 degrees for all DI and NDI models, which also produced alignment with the PBP updates (Fig. 3e,f).
Next, we tested performance on a small nonlinear regression task with a three-layer network (dim. 30-20-10-10, tanh nonlinearities) also following Lillicrap et al. (2016). All inversion algorithms
ﬁnished with equal or better performance compared to BP and FA (Fig. 3g), but often with slower convergence, which was unexpected considering the potential link to second-order optimization (see Discussion). DI dynamics were stable throughout training, remaining nearly constant (Fig. 3h).
Furthermore, DI again closely followed NDI updates (Fig. 3i), though error curves between the two drifted apart during learning (Fig. 3g), indicating that a small amount of variability in convergence can 6
Figure 3: Results for linear (30-20-10) and nonlinear (30-20-10-10) regression tasks for BP, FA, PBP, and three realizations of DI, NDI, and SLDI (only for nonlinear) with different weight initialization and leak values. Legend below panel g. Learning rate =10−2 for all algorithms. a: Normalized mean squared error (NMSE) for linear regression. b: NMSE for linear regression with ﬁxed-norm weight updates. c: Stability of DI as measured by the maximum real eigenvalues of the system dynamics. d:
Angle between the backpropagated error vectors to the hidden layer (δ1) for NDI vs DI. e: Angle between feedback weights and negative transpose of forward weights (shown for DI and NDI). f:
Angle between backpropagated error vectors for DI and PBP. g-j: Same as a,c,d, e but for nonlinear regression. f: Angle between the backpropagated error vectors to the hidden layer for NDI vs SLDI.
Dashed lines refer to output layer, and solid linear refer to middle layer in h-k. lead to different learning trajectories. Alignment of feedback B varied with the layer and algorithm (Fig. 3j) — some layers settled to ~45 degrees, but others remained close to zero — this is intriguing, but may be due to the simplicity of the problem. Finally, we also simulated single-loop dynamic inversion (SLDI). The alignment between SLDI and NDI was small (~45 degrees or less), but larger than DI, supporting the claim that it converges to similar, but not necessarily equivalent steady states (Fig. 3k). 5.2 MNIST classiﬁcation and autoencoder
We next tested dynamic inversion on the MNIST handwritten digit dataset, where we use the standard training and test datasets (LeCun et al., 1998), with a two-layer architecture (dim. 784-1000-10 as in Lillicrap et al. (2016)). All algorithms showed decent performance after 10 epochs (Fig. 4a), but with DI ﬂattening out at a higher test error (BP, 2.2%; FA, 1.8%; PBP, all NDI, DI ~2.8%), again suggesting slower convergence (Discussion). We speculate that a more thorough exploration of hyperparameters, such as changing the architecture or using mini-batches may help here.
Finally, we trained a bottleneck autoencoder network on the MNIST dataset (dim. 784-500-250-30-250-500-784; nonlinearities tanh-tanh-linear-tanh-tanh-linear, a reduced version of Hinton and
Salakhutdinov (2006)) with mini-batch training (100 examples per batch). Notably, ﬁrst-order optimization algorithms have trouble dealing with the “pathological curvature” of such problems and often have very slow learning (Martens, 2010) (especially FA, Lansdell et al. (2019)). We trained the algorithms with random uniform weight initializations, similar to the previous experiments, as well as with random orthogonal initializations, which has been shown to speed up learning (Saxe et al., 2013). We found that BP only learns successfully with orthogonal weight initialization, whereas PBP,
NDI, and DI perform decently in either case, further suggesting they use second-order information.
Notably, PBP, NDI, and DI performance is slower with orthogonal initialization, where second-order information is not useful (but this might be mitigated by having non-orthogonal feedback weights).
Furthermore, FA performed poorly in both cases, and regardless of the type of feedback initialization.
This provides evidence that dynamic inversion can be superior to random feedback in some tasks, and 7
Figure 4: Results for MNIST classiﬁcation (784-1000-10 architecture) and autoencoding (784-500-250-30 architecture) trained with BP, FA, PBP, and two realizations of NDI and DI. a: Test error of
MNIST classiﬁcation over 10 epochs of online training (learning rate=10−3 for all algorithms). b:
Training error of MNIST autoencoding over 25 epochs of mini-batch training (learning rate = 10−6 for all algorithms). c,d: Examples of input and output digits and a 2D t-SNE representation of the 30-dim. latent space for BP (orthogonal init.) and DI (uniform init.). that weight initialization alone is not responsible for this beneﬁt. BP with orthogonal initialization and DI with random initialization result in similar performance (Fig. 4c,d). 6 Discussion 6.1