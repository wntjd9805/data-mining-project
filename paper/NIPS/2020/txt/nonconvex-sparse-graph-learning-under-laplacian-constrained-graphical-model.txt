Abstract
In this paper, we consider the problem of learning a sparse graph from the Laplacian constrained Gaussian graphical model. This problem can be formulated as a penalized maximum likelihood estimation of the precision matrix under Laplacian structural constraints. Like in the classical graphical lasso problem, recent works made use of the (cid:96)1-norm with the goal of promoting sparsity in the Laplacian constrained precision matrix estimation. However, through empirical evidence, we observe that the (cid:96)1-norm is not effective in imposing a sparse solution in this problem. From a theoretical perspective, we prove that a large regularization parameter will surprisingly lead to a solution representing a complete graph, i.e., every pair of vertices is connected by an edge. To address this issue, we propose a nonconvex penalized maximum likelihood estimation method, and establish the order of the statistical error. Numerical experiments involving synthetic and real-world data sets demonstrate the effectiveness of the proposed method. An open source R package is available at https://github.com/mirca/sparseGraph. 1

Introduction
Gaussian graphical models (GGM) have been widely used in a number of ﬁelds such as ﬁnance, bioinformatics, and image analysis [1, 31, 38]. Graph learning under GGM can be formulated to estimate the precision matrix that captures the conditional dependency relations between random variables [10, 29]. In this paper, the goal is to learn a sparse graph under the Laplacian constrained
GGM, where the precision matrix obeys Laplacian structural constraints.
The general GGM has received broad interest in statistical machine learning, where the problem can be formulated as a sparse precision matrix estimation. The papers [1, 9, 58] proposed the (cid:96)1-norm penalized maximum likelihood estimation, also known as graphical lasso, to encourage sparsity in its entries. Various extensions of graphical lasso and their theoretical properties are also studied
[11, 18, 21, 35, 41, 42, 47, 55, 56]. To reduce the estimation bias, nonconvex penalties like the smooth clipped absolute deviation (SCAD) [16], minimax concave penalty (MCP) [60], and capped (cid:96)1-penalty [61] have been introduced in estimating a sparse precision matrix [2, 7, 27, 33, 46, 54, 60].
However, those methods mentioned above focus on general graphical models, and cannot be directly extended to Laplacian constrained GGM because of the multiple constraints on the precision matrices.
Moreover, unlike the above GGM cases, this paper will show that the (cid:96)1-norm is not effective in 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
promoting sparsity in the penalized maximum likelihood estimation of the Laplacian constrained precision matrices.
In recent years, Laplacian constrained GGM has received increasing attention in signal processing and machine learning over graphs [6, 13, 23, 26, 30, 37, 48]. Under the Laplacian constrained GGM, graph learning can be formulated as Laplacian constrained precision matrix estimation. Unlike the general GGM, the precision matrix in Laplacian constrained GGM enjoys the spectral property that its eigenvalues and eigenvectors can be interpreted as spectral frequencies and Fourier basis [48], which is useful in computing graph Fourier transform in graph signal processing [37, 48], and graph convolutional networks [3, 36, 45]. The authors in [12, 14, 19, 59] formulated the graph signals as random variables under the Laplacian constrained GGM. The learned graph under Laplacian constrained GGM favours smooth graph signal representations [12], since the graph Laplacian quadratic term quantiﬁes the smoothness of graph signals [22, 24]. However, sparse graph learning under the Laplacian constrained GGM remains to be further explored. For example, how to effectively and efﬁciently learn a sparse graph and how to characterize the estimation error under the Laplacian constrained GGM are to be investigated.
This paper focuses on the problem of learning a sparse graph under the Laplacian constrained GGM.
The contributions of this paper are summarized as follows. First, we ﬁnd an unexpected behavior of the (cid:96)1-norm in Laplacian constrained GGM. Through empirical evidence, we observe that the widely used (cid:96)1-norm is not effective in imposing a sparse solution under the Laplacian constrained
GGM. From a theoretical perspective, we prove that a large regularization parameter of the (cid:96)1-norm will lead to a solution representing a complete graph, i.e., every pair of vertices is connected by an edge, instead of a sparse graph. Second, we propose a nonconvex penalized maximum likelihood estimation method by solving a sequence of weighted (cid:96)1-norm penalized sub-problems, and establish the order of the statistical error. To the best of our knowledge, this is the ﬁrst work to analyze the non-asymptotic optimization performance guarantees on both optimization error and statistical error under the Laplacian constrained GGM. Finally, numerical experiments on both synthetic and real-world data sets demonstrate the effectiveness of the proposed method.
The remainder of the paper is organized as follows. Problem formulation and related work are provided in Section 2. We present the proposed method and the theoretical results in Section 3.
Experimental results are provided in Section 4. We draw the conclusions in Section 5.
Notation Lower case bold letters denote vectors and upper case bold letters denote matrices. Both
Xij and [X]ij denote the (i, j)-th entry of the matrix X. X (cid:62) denotes transpose of matrix X. [p] denotes the set {1, . . . , p}. The all-zero and all-one vectors or matrices are denoted by 0 and 1, respectively. (cid:107)x(cid:107), (cid:107)X(cid:107)F and (cid:107)X(cid:107)2 denote Euclidean norm, Frobenius norm, and operator norm, respectively. Let supp+(x) = {i ∈ [p] |xi > 0} for x ∈ Rp. (cid:100)x(cid:101) denotes the least integer greater than or equal to x. Let (cid:107)x(cid:107)max = maxi |xi| and (cid:104)x, y(cid:105) = (cid:80) i xiyi. For functions f (n) and g(n), we use f (n) (cid:46) g(n) if f (n) ≤ Cg(n) for some constant C ∈ (0, +∞). S p
++ denote the sets of positive semi-deﬁnite and positive deﬁnite matrices with the size p × p, respectively.
+ and S p 2 Problem Formulation and