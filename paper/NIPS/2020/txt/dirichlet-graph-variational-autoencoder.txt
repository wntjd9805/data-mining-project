Abstract
Graph Neural Networks (GNNs) and Variational Autoencoders (VAEs) have been widely used in modeling and generating graphs with latent factors. However, there is no clear explanation of what these latent factors are and why they perform well.
In this work, we present Dirichlet Graph Variational Autoencoder (DGVAE) with graph cluster memberships as latent factors. Our study connects VAEs based graph generation and balanced graph cut, and provides a new way to understand and improve the internal mechanism of VAEs based graph generation. Speciﬁcally, we ﬁrst interpret the reconstruction term of DGVAE as balanced graph cut in a principled way. Furthermore, motivated by the low pass characteristics in bal-anced graph cut, we propose a new variant of GNN named Heatts to encode the input graph into cluster memberships. Heatts utilizes the Taylor series for fast computation of heat kernels and has better low pass characteristics than Graph
Convolutional Networks (GCN). Through experiments on graph generation and graph clustering, we demonstrate the effectiveness of our proposed framework. 1

Introduction
Since the introduction of Graph Neural Networks (GNNs) [18, 4, 6] and Variational Autoencoders (VAEs) [16], many studies [19, 21, 8] have used GNNs and VAEs (GVAEs) to generate realistic graphs with latent factors. The latent factors are usually formulated as Gaussian variables and comply with some predeﬁned prior distributions, e.g., isotropic Gaussian with diagonal covariance. Although some encouraging progress has been achieved, there is still little insight into the internal operation and behaviour of these complex models, or what intrinsic information the latent factors have captured with respect to the input graph. Inspired by the recent development of variational autoencoder topic model [27, 3] in text generation, in this work, we propose to formulate the latent factors in GVAEs as graph cluster memberships, analogous to topics in text generation.
Indeed, it is not a new idea to generate graphs via exploiting certain structures (i.e., clusters or subgraphs). JT-VAE [14] proposes to generate molecular graphs in two phases, in which it ﬁrst decomposes the whole molecule into subgraphs or clusters of atoms, e.g., rings or bonds. It then utilizes these clusters to assemble a coherent molecular graph. However, JT-VAE relies on tree decomposition algorithms tailored for molecules, which is hard to generalize to non-molecular graph generation. In this paper, we propose a novel Dirichlet Graph Variational Audoencoder (DGVAE) to automatically encode the cluster decomposition in latent factors by replacing node-wise Gaussian variables with Dirichlet distributions, where the latent factors can be taken as cluster memberships.
Under this framework, we provide a transparent interpretation of the de facto reconstruction term of Evidence Lower Bound (ELBO) in VAEs, which is in effect equivalent to adopting the spectral 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
relaxed balanced graph cut [28]. In this vein, it further sheds light on the notable successful graph reconstruction performance.
Inspired by the connection between DGVAE and the spectral relaxed balanced graph cut, we refer to the literature of balanced graph cut to understand and improve DGVAE. Due to the hardness of balanced graph cut, previous solutions rely on spectral relaxation [25, 9] to solve the cut problem.
Among them, the most popular approach is spectral clustering, which requires the computation of the
ﬁrst K eigenvectors of the Laplacian matrix L, or low pass in spectral domain [25, 28]. Motivated by this low pass characteristics of spectral clustering, we introduce a novel variant of GNN to encode the input graph into latent cluster memberships. The introduced GNN, which is referred to as Heatts, uses Taylor series approximation towards the fast computation of heat kernels [5] and shows better low pass characteristics than Graph Convolutional Networks (GCN).
The contributions of this work are summarized as follows:
• We introduce DGVAE, which uses the Dirichlet distributions as priors on the latent variables and the latent variables are graph cluster memberships.
• We show the reconstruction term of ELBO in DGVAE can be interpreted as the spectral relaxed balanced graph cut.
• We identify low pass characteristics in the encoding stage of DGVAE and propose a novel variant of GNN based on Taylor series approximation towards heat kernels.
This paper is organized as follows. Section 2 gives preliminaries about balanced graph cut and
GVAEs. Section 3 describes the design of DGVAE. Section 4 interprets the reconstruction term in
DGVAE as balanced graph cut. Section 5 presents Heatts. We report the experimental results in
Section 6 and discuss related work in Section 7. Finally, Section 8 concludes the paper. 2 Preliminaries
Formally, we are given an undirected, unweighted graph G = (V, A, X). V is the node set and
N = |V | denotes the number of nodes. The adjacency matrix A ∈ RN ×N represents the graph structure. The feature matrix X ∈ RN ×d represents the node attributes. Our goal is to learn an encoder and a decoder to map between the space of graph G and their latent factors Z ∈ RN ×K. 2.1 Balanced graph cut
Balanced graph cut, i.e., ratio cut [30] and normalized cut [25], are widely used criteria for graph clustering. Formally, graph cut is deﬁned as, 1
K (cid:2) k cut(Vk, Vk), (1) where Vk is the node set assigned to cluster k, Vk = V \ Vk, cut(Vk, Vk) = i∈Vk,j∈Vk Aij and it calculates the number of edges with one end point inside cluster k and the other in the rest of the graph. Suppose we are given a cluster indicator C ∈ {0, 1}N ×K, Cik = 1 represents node i belongs to cluster k and 0 otherwise. Similar to [28], we can re-write the graph cut as, (cid:3) 1
K (cid:2) k (C (cid:3)
:,kDC:,k − C (cid:3)
:,kAC:,k) = 1
K
Tr(C (cid:3)LC), (2) in which C:,k is the k-th column vector, D and L are the degree and Laplacian matrices for matrix
A, respectively. C (cid:3)
:,kDC:,k stands for the number of edges with at least one end point in Vk and
C (cid:3)
:,kAC:,k counts the number of edges within cluster k. Unfortunately, graph cut favors imbalanced clustering [30, 28]. Thus, we utilize ratio cut [30], which is deﬁned as, 1
K (cid:2) k
C (cid:3)
:,kLC:,k
|Vk|
, 2 (3)
where |Vk| = C (cid:3)
:,kC:,k counts the number of nodes within cluster k. In particular, the minimum of (cid:3)K k=1(1/|Vk|) is achieved if all |Vk| (k = 1, . . . , K) are the same [28]. Unfortunately, the function introducing this balancing condition |Vk| makes ratio cut NP-hard (a detailed discussion can be found in [29]). In the literature, existing approaches rely on spectral relaxation [25, 9] or greedy algorithms to solve the cut problem. The most famous approach is spectral clustering [28], which removes the discrete constraint of cluster indicators. Due to its generality and rich theoretical foundation, spectral clustering has become one of the major clustering methods [2]. The algorithm of spectral clustering requires the computation of the ﬁrst K eigenvectors of the Laplacian matrix L, which corresponds to the smallest K eigenvalues, or low pass for short. Denoting {λi}N i=1 as the eigenvalues of the (normalized) Laplacian matrix and λK as the threshold, the ideal low pass ﬁlter gid(λi) required by spectral clustering can be deﬁned as, (cid:4) 1 0
λi ≤ λK
λi > λK. (4) gid(λi) = 2.2 GNNs and VAEs
In typical GNNs and VAEs (GVAEs) such as VGAE [19] and Graphite [8], the encoder is deﬁned as a variational posterior qφ(Z|G) parameterized by GNNs, and the decoder is deﬁned by a generative distribution pθ(A|Z), where φ and θ are the corresponding parameters. Usually there is a prior distribution p(Z) acting as a regularization for qφ(Z|G). The whole framework is trained by maximizing Evidence Lower Bound (ELBO),
LELBO(φ, θ; G) = −KL(qφ(Z|G)||p(Z)) + Eqφ(Z/G) log pθ(A|Z), (cid:6) (cid:3) (5) where the Kullback-Leibler divergence is deﬁned as KL(P ||Q) =
. The second part is the reconstruction term, which is used to guarantee the similarity between the generated structure and the input structure. j Pj log (cid:5) Pj
Qj
In the encoding stage of GVAEs, many studies [19, 21, 8] use GNNs to encode the input graph into node-wise latent factors. Most of them focus on deriving latent isotropic Gaussian distributions.
Recently some studies [31, 23] have shown that GCN [18], one of the popular GNNs, is in essence a linear approximation to low pass ﬁlters in the spectral domain. Speciﬁcally, Wu et al. [31] show that
GCN is a linear spectral ﬁlter with gc(λi) = 1 − λi, which deviates low pass characteristics deﬁned in Eq. 4 as gc(λi) becomes negative when λi > 1. 3 Dirichlet graph variational autoencoder
In this section, we present Dirichlet Graph Variational Autoencoder (DGVAE). Our primary idea is to replace Gaussian variables by the Dirichlet distributions in latent modeling of VAEs, such that the latent factors can be adopted to describe graph cluster memberships. It makes the graph generation process analogous to text generation (see LDA [1]), i.e., a graph (document) ﬁrst samples clusters (topics), and then draws nodes (words) conditioned on the chosen clusters (topics).
Encoder We follow VGAE [19] and Graphite [8] by using the mean ﬁeld approximation to deﬁne the variational family,
N(cid:7) qφ(Z|A, X) = qφi (zi|A, X). (6) i=1
As discussed, our variational marginals qφi (zi|A, X) are assumed to follow the Dirichlet distributions to make the latent factors interpretable. However, directly approaching the Dirichlet distributions would make the reparameterization trick hard to apply. To this end, we resolve this issue by using Laplace approximation [12, 27], in which the main idea is to approximate the Dirichlet distributions with the logistic normal distribution. Formally, the parameters for the variational marginals qφi (zi|A, X) are speciﬁed by a multi-layer GNN,
μ0, σ0 = GNNφ(A, X), (7) where μ0 ∈ RK and σ0 ∈ RK are the vector of means and variances for the normal distributions.
Following [12, 27], we can approximate the Dirichlet distributions qφ(zi|G) thereafter by sampling 3
(cid:6) ∼ N (0, I) and compute zi = softmax(μ0 + (Σ0)1/2(cid:6)), (8) where Σ0 = diag(σ0) returns a square diagonal matrix with the elements of vector σ0 on the main diagonal, and Z = {zi}N i=1. Note that Z coincides with graph cluster memberships C, i.e., spectral relaxed cluster indicators in Section 2.1.
KL divergence We follow the idea in Laplace approximation [12] by rewriting p(zi) = Dir(α) as the logistic normal distribution with mean μ1 and covariance matrix Σ1, k = log αk − 1
μ1
K (cid:2) i log αi,
Σ1 kk = 1
αk (1 − 2
K
) + 1
K 2 (cid:2) i 1
αi
. (9) (10)
Now the KL divergence can be computed between two logistic normal distributions as,
KL(qφ(zi|G)||p(zi)) = 1 2
{Tr((Σ1)−1Σ0) + (μ1 − μ0)(cid:3)(Σ1)−1(μ1 − μ0) − K + log
|Σ1|
|Σ0|
}. (11)
Decoder Similar to VGAE [19], we approximate p(A|Z) by, p(A|Z) = ξ (cid:7)
Aij =1 exp f (Ci, Cj) (cid:7)
Aij =0 exp{1 − f (Ci, Cj)}, (12) where f (·, ·) denotes a distance metric, e.g., f (Ci, Cj) = C (cid:3) and MSE(·, ·) represents mean squared error. ξ is a re-scaling term to ensure p(A|Z) ∈ [0, 1]. i Cj or f (Ci, Cj) = 1 − MSE(Ci, Cj)
Component collapsing Previous work [17] has observed that VAEs training often suffers from a particular local optimum known as component collapsing, in which the model reaches close to the prior belief and ignores the latent factors. This phenomenon becomes even more severe for modeling Dirichlet priors, as a good optimization of Dirichlet KL-divergence often results in a bad reconstruction term [27, 3].
We resolve this issue by aggressively optimizing the reconstruction term with more updates [11].
Speciﬁcally, we consider the optimization of Dirichlet KL-divergence and reconstruction term is imbalanced. In each iteration, we train the whole framework by separating these two terms and specializing an inner loop for updating the reconstruction term. Different from methods that modify the training objective [13, 36], our training dynamics can hold the standard ELBO tightly. 4 Reconstruction term as balanced graph cut
Claim 4.1 Maximizing the reconstruction term of DGVAE is equivalent to minimizing the spectral relaxed graph cut and a regularization that encourages balanced cluster size.
To prove Claim 4.1, we ignore the constant and re-write the reconstruction term, log p(A|Z) ≈ 2 f (Ci, Cj) − f (Ci, Cj), (13) (cid:2) (cid:2)
Consider f (Ci, Cj) = 1 − MSE(Ci, Cj), then the ﬁrst component can be re-written as,
Aij =1
Aij (cid:2)
Aij =1 f (Ci, Cj) = m −
N(cid:2) i,j
AijMSE(Ci, Cj) (14)
= m − 2
K
Tr(C (cid:3)LC), 4
Figure 1: Spectral coefﬁcients for low order Taylor Series approximations w.r.t. heat kernel with s = 1, depth denotes the number of layers. where m is the number of edges in the graph. Eq.14 shows that maximizing the ﬁrst component of the reconstruction term is equivalent to minimizing spectral relaxed graph cut. The second component of the reconstruction term is, (cid:2) f (Ci, Cj) =
Aij
Aij (cid:2)
{1 − 1
K (cid:2) k (Cik − Cjk)2}. (15) (cid:3) (cid:3)K
Aij k=1(Cik − Cjk)2, we aim at exploring its regularization interpretation in the
Considering graph cut problem. Here, the soft membership matrix satisﬁes C ∈ RN ×K, Ci ∈ ΔK−1 where Ci is the i-th row of matrix C and ΔK−1 is the (K − 1)-simplex. From the statistical viewpoint, we can regard row Ci in the matrix as an independent sample drawn from the posterior Dirichlet distributions, which exhibits the following characteristics,
Ci ∼ Dir(β), β ∈ RK
+ .
From this perspective, the following lemma is derived to conclude the proof of Claim 4.1.
Lemma 4.1 The regularization term in Eq. 15 is equivalent to maximizing the sample variance of k βk → 0.
Dir(β) when N goes to inﬁnity, whose optimum is achieved when all βk are equal and (cid:3)
Please refer to Appendix A for the proof. Lemma 4.1 shows the regularization encourages the uniform distribution of cluster memberships over K clusters. Thus, it is reasonable to conclude that this regularization will help us to enforce a balanced graph cluster size. 5 Taylor series approximation for heat kernels
Motivated by the low pass characteristics in balanced graph cut, we propose a new variant of GNN to encode the input graph into cluster memberships.
A straightforward requirement for the new variant is that it should retain the low eigenvectors and drop the high eigenvectors of the Laplace matrix L, or low pass in Eq. 4. Another constraint is that it should admit fast algorithms and does not involve explicit eigendecomposition of L, as eigendecomposition is prohibitively expensive for large graphs [18]. In this regard, GCN uses spectral graph theory to learn such a low pass graph ﬁlter. However, the problem of GCN is that it quickly shrinks high eigenvectors at the expense of reducing useful low eigenvectors [31]. In the next, we introduce a new variant of GNN named Heatts, which utilizes the Taylor series for the fast computation of heat kernels and has better low pass characteristics than GCN.
Consider the heat kernel gs(λ) = e−sλ [5], where s > 0 is a scaling hyperparameter. The spectral graph convolutions on a signal x ∈ RN is deﬁned as, gs ∗ x = U diag(gs(λ1), . . . , gs(λN ))U (cid:3)x = U gs(Λ)U (cid:3)x, (16) where U is the eigenvector matrix of the (normalized) graph Laplacian L = U ΛU (cid:3). Then, we apply
Taylor series approximation on gs(Λ) and get a fast algorithm as below, (cid:3) (cid:3) gs ∗ x = U (−1)n n! snΛnU (cid:3)x = n (−1)n n! snLnx. n (17) 5
Table 1: Test graph generation comparison of different methods
Erdos-Renyi
Ego
Regular
Geometric
Power Law
Barabasi-Albert
NLL
RMSE NLL
RMSE NLL
RMSE NLL
RMSE NLL
RMSE NLL
RMSE
GAE 0.647
VGAE 1.010 0.678
Graphite-AE
Graphite-VAE 1.087
Abl-AE
Abl-VAE
DGAE
DGVAE 0.646 0.760 0.239 0.286 0.535 0.609 0.529 0.602 0.530 0.512 0.186 0.249 0.356 0.917 0.370 0.896 0.349 0.541 0.250 0.436 0.346 0.492 0.333 0.496 0.400 0.445 0.231 0.274 0.523 0.914 0.526 0.983 0.497 0.601 0.305 0.516 0.455 0.452 0.395 0.474 0.429 0.454 0.282 0.340 0.583 0.813 0.851 0.846 0.472 0.682 0.406 0.537 0.370 0.524 0.385 0.536 0.350 0.475 0.182 0.233 0.580 0.901 0.541 0.938 0.536 0.638 0.383 0.519 0.401 0.485 0.399 0.466 0.419 0.395 0.415 0.255 0.553 0.894 0.557 0.925 0.536 0.678 0.308 0.346 0.428 0.501 0.390 0.482 0.383 0.430 0.214 0.194
Usually truncated low order approximation is used in practice with n ≤ 3 [4]. Here the ﬁrst order approximation is exactly GCN [18] with gc(λi) = 1 − λi when s = 1. Compared with the ﬁrst order approximation, the third order approximation can retain more useful low eigenvectors while yielding less negative spectral coefﬁcients when λi > 1. Moreover, applying multiple layers of the third order approximation can eliminate the effect of negative coefﬁcients, which neglects trivial solutions of renormalization trick used in GCN [18]. In Figure 1, we plot this low order approximation with s = 1. The eigenvalue range for normalized Laplacian is λi ∈ [0, 2] [31]. As Figure 1 shows, the third order approximation (in red) acts quite similar to heat kernel (in blue) when the model depth = 2. More importantly, it retains more useful low eigenvectors and drops more high eigenvectors than
GCN (in green). We shall further discuss this point in Section 5.1.
Meanwhile, a feature transformation is applied to the feature matrix X. From another viewpoint,
Heatts can be understood as a special case of a simple differentiable message-passing framework [6], message function : M l = 3(cid:2) n=0 (−1)n n! snLnH l, vertex update function : H l+1 = ReLU (M lW ), (18) (19) where H 0 = X. W is the parameter set to be learned. 5.1 Heatts and related GNNs
We ﬁrst show Heatts has better low pass characteristics than GCN [18].
Given the eigenvalue range for normalized Laplacian is [0, 2], the distance between an arbitrary spectral ﬁlter g(λ) and the ideal low pass ﬁlter gid(λ) in Eq. 4 is deﬁned as,
Distance(g, gid) = (cid:8) λK 0 (1 − g(λ))2dλ + (cid:8) 2
λK (0 − g(λ))2dλ. (20)
Intuitively, this deﬁnition computes the squared Euclidean distance between g(·) and gid(·).
Proposition 5.1 The distance between Heatts gs(λ) and the ideal low pass ﬁlter gid(λ) is always smaller than that between GCN gc(λ) and gid(λ).
Please refer to Appendix B for the proof. We then contrast the difference between Heatts and other related GNNs including Cheby-GCN [4], GraphHeat [33].
Heatts vs. Cheby-GCN Cheby-GCN [4] uses parameterized Kth-order coefﬁcients to approximate
Chebyshev polynomials, which suffers from the problem of overﬁtting on local neighborhood structures for graphs [18]. Heatts can be considered as Chebyshev polynomials with a speciﬁc set of coefﬁcients determined by Taylor series approximation to heat kernel.
Heatts vs. GraphHeat GraphHeat uses parameterized Chebyshev polynomials to approximate heat kernel, and the learned parameters can resemble arbitrary ﬁlters. On the contrary, Heatts is an explicit Taylor series approximation towards heat kernel. 6
Figure 2: Left one in blue: the input graphs. Right ﬁve in colors: graph samples generated by
DGVAE, where colors indicate latent cluster memberships with K = 3. 6 Experiments 6.1 Graph generation
Data and baselines We follow Graphite [8] and create data sets from six graph families with ﬁxed, known generative processes, to evaluate the performance of DGVAE on graph generation. For more detailed settings, please refer to [8]. We compare with GAE/VGAE [19] and Graphite-AE/VAE [8].
We denote our framework using GCN [18] in the encoders as Abl-AE/VAE.
Setup For DGVAE/DGAE, we use the same network architecture through all the experiments. We train the model using minibatch based Adam optimizer. We train for 200 iterations with a learning rate of 0.01. The output dimension of the ﬁrst hidden layer is 32 and that of the second-layer (K) is 16. The Dirichlet prior is set to be 0.01 for all dimensions if not speciﬁed otherwise. For Heatts, we let s = 1 for all experiments.
Results The negative log-likelihood (NLL) and root mean square error (RMSE) on a test set of instances are shown in Table 1. Both DGVAE and DGAE outperform their competitors signiﬁcantly on all data sets, indicating the effectiveness of DGVAE and DGVE. As an ablation study, when replacing Heatts with GCN [18], the performance is just comparable to baselines, and worse than
DGVAE, which shows the superiority of Heatts.
Component collapsing We evaluate how the training dynamics affects the model performance. As shown in Figure 3, this training dynamics can signiﬁcantly relieve the posterior collapse problem.
Visualization We plot four graphs and their samples generated by DGVAE in Figure 2 with latent cluster dimension K = 3. We let the number of edges for graph samples equal with the number for the input graphs. As we have analyzed, most cluster members are connected and uniformly distributed over the three clusters, indicating our model encourages a balanced graph cluster size. 6.2 Graph clustering
Data and baselines We use three benchmark data sets, i.e., Pubmed, Citeseer [24] and Wiki [34].
Statistics of the data sets can be found in Table 3 in Appendix. As baselines, we compare against (1) Spectral Clustering (SC) [28], which only takes the node adjacency matrix as afﬁnity matrix; (2)
Node2vec [7] + Kmeans (N2v&K), which ﬁrst uses Node2vec to derive node embeddings and then utilizes K-means to generate cluster results; (3) GAE [19] + Kmeans (GAE&K); and (4) VGAE [19]
+ Kmeans (VGAE&K). We denote our framework using GCN [18] in the encoders as Abl-AE/VAE.
Setup For DGVAE/DGAE, we adopt the same settings as in the experiment of graph generation except that the output dimension of the second-layer equals to the number of clusters. 7
Figure 3: The training behaviour on Erdos Renyi graphs. Left: without training dynamics; right: with training dynamics.
Table 2: Cluster performance comparison of different methods
ACC (%) 58.3 ±0.5
SC 67.7 ±1.2
N2v&K 64.2 ±1.9
GAE&K
VGAE&K 62.0 ±3.0 66.3 ±1.4
Abl-AE 62.6 ±2.1
Abl-VAE 68.4 ±1.9
DGAE 64.9 ±2.0
DGVAE
Pubmed
NMI (%) 19.0 ±0.6 29.5 ±1.3 24.0 ±1.5 20.4 ±1.7 25.7 ±1.9 24.2 ±2.7 28.8 ±2.1 25.8 ±2.5
F1 (%) 43.2 ±0.4 66.3 ±1.1 64.4 ±1.1 62.5 ±2.8 66.2 ±1.7 61.4 ±2.5 67.3 ±2.3 66.5 ±2.2
ACC (%) 23.9 ±1.4 41.3 ±1.1 41.2 ±0.9 43.4 ±3.3 46.1 ±1.9 40.2 ±2.7 51.3 ±2.1 44.9 ±2.8
Citeseer
NMI (%) 5.9 ±3.5 16.7 ±0.8 20.8 ±1.2 22.7 ±0.9 24.3 ±2.2 16.1 ±2.2 27.2 ±1.5 19.4 ±2.7
F1 (%) 29.5 ±2.6 39.5 ±1.3 40.1 ±1.2 41.8 ±3.0 40.1 ±2.2 38.5 ±2.4 49.4 ±1.8 41.9 ±3.1
ACC (%) 23.6 ±3.7 34.9 ±1.8 25.1 ±1.9 32.2 ±2.1 38.1 ±2.3 36.2 ±2.3 38.9 ±1.8 37.5 ±3.3
Wiki
NMI (%) 19.3 ±3.2 31.1 ±2.2 26.7 ±1.7 30.2 ±2.8 33.8 ±1.6 31.4 ±1.4 36.9 ±1.5 31.7 ±2.6
F1 (%) 17.3 ±2.5 30.3 ±1.3 19.8 ±1.8 29.3 ±2.2 24.7 ±2.1 25.9 ±2.7 27.7 ±2.3 28.7 ±1.9
Results The clustering accuracy (ACC), normalized mutual information (NMI) and macro F1 score (F1) are shown in Table 2. Both DGVAE and DGAE outperform their competitors on most data sets.
As DGVAE/DGAE does not rely on K-means to derive cluster memberships, this cluster performance indicates the effectiveness of our framework on graph clustering tasks. As an ablation study, when replacing Heatts with GCN [18], the performance is just comparable to baselines, and worse than
DGVAE, which again proves the superiority of Heatts over GCN. 7