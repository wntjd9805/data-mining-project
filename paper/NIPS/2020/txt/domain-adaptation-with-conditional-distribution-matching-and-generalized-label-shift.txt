Abstract
Adversarial learning has demonstrated good performance in the unsupervised domain adaptation setting, by learning domain-invariant representations. However, recent work has shown limitations of this approach when label distributions differ between the source and target domains. In this paper, we propose a new assumption, generalized label shift (GLS), to improve robustness against mismatched label distributions. GLS states that, conditioned on the label, there exists a representation of the input that is invariant between the source and target domains. Under GLS, we provide theoretical guarantees on the transfer performance of any classiﬁer.
We also devise necessary and sufﬁcient conditions for GLS to hold, by using an estimation of the relative class weights between domains and an appropriate reweighting of samples. Our weight estimation method could be straightforwardly and generically applied in existing domain adaptation (DA) algorithms that learn domain-invariant representations, with small computational overhead. In particular, we modify three DA algorithms, JAN, DANN and CDAN, and evaluate their performance on standard and artiﬁcial DA tasks. Our algorithms outperform the base versions, with vast improvements for large label distribution mismatches. Our code is available at https://tinyurl.com/y585xt6j. 1

Introduction
In spite of impressive successes, most deep learning models [22] rely on huge amounts of labelled data and their features have proven brittle to distribution shifts [39, 55]. Building more robust models, that learn from fewer samples and/or generalize better out-of-distribution is the focus of many recent works [2, 5, 53]. The research direction of interest to this paper is that of domain adaptation, which aims at learning features that transfer well between domains. We focus in particular on unsupervised domain adaptation (UDA), where the algorithm has access to labelled samples from a source domain and unlabelled data from a target domain. Its objective is to train a model that generalizes well to the target domain. Building on advances in adversarial learning [23], adversarial domain adaptation (ADA) leverages the use of a discriminator to learn an intermediate representation that is invariant between the source and target domains. Simultaneously, the representation is paired with a classiﬁer, trained to perform well on the source domain [20, 32, 49, 60]. ADA is rather successful on a variety
∗The ﬁrst two authors contributed equally to this work. Work done while HZ was at Carnegie Mellon
University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
of tasks, however, recent work has proven an upper bound on the performance of existing algorithms when source and target domains have mismatched label distributions [62]. Label shift is a property of two domains for which the marginal label distributions differ, but the conditional distributions of input given label stay the same across domains [48, 58].
In this paper, we study domain adaptation under mismatched label distributions and design methods that are robust in that setting. Our contributions are the following. First, we extend the upper bound by Zhao et al. [62] to k-class classiﬁcation and to conditional domain adversarial networks, a recently introduced domain adaptation algorithm [37]. Second, we introduce generalized label shift (GLS), a broader version of the standard label shift where conditional invariance between source and target domains is placed in representation rather than input space. Third, we derive performance guarantees for algorithms that seek to enforce GLS via learnt feature transformations, in the form of upper bounds on the error gap and the joint error of the classiﬁer on the source and target domains. Those guarantees suggest principled modiﬁcations to ADA to improve its robustness to mismatched label distributions. The modiﬁcations rely on estimating the class ratios between source and target domains and use those as importance weights in the adversarial and classiﬁcation objectives. The importance weights estimation is performed using a method of moment by solving a quadratic program, inspired from Lipton et al. [31]. Following these theoretical insights, we devise three new algorithms based on learning importance-weighted representations, DANNs [20], JANs [36] and CDANs [37]. We apply our variants to artiﬁcial UDA tasks with large divergences between label distributions, and demonstrate signiﬁcant performance gains compared to the algorithms’ base versions. Finally, we evaluate them on standard domain adaptation tasks and also show improved performance. 2 Preliminaries
Notation We focus on the general k-class classiﬁcation problem. X and Y denote the input and output space, respectively. Z stands for the representation space induced from X by a feature transformation g : X (cid:55)→ Z. Accordingly, we use X, Y, Z to denote random variables which take values in X , Y, Z. Domain corresponds to a joint distribution on the input space X and output space
Y, and we use DS (resp. DT) to denote the source (resp. target) domain. Noticeably, this corresponds to a stochastic setting, which is stronger than the deterministic one previously studied [6, 7, 62]. A hypothesis is a function h : X → [k]. The error of a hypothesis h under distribution DS is deﬁned as: (h(X) (cid:54)= Y), i.e., the probability that h disagrees with Y under DS.
εS(h) := PrDS
Domain Adaptation via Invariant Representations For source (DS) and target (DT) domains, we use DX
S and DY
T to denote the marginal data and label distributions. In UDA, the algorithm has access to n labeled points {(xi, yi)}n j=1 ∈ X m sampled i.i.d. from the source and target domains. Inspired by Ben-David et al. [7], a common approach is to learn representations invariant to the domain shift. With g : X (cid:55)→ Z a feature transformation and h : Z (cid:55)→ Y a hypothesis on the feature space, a domain invariant representa-tion [20, 49, 59] is a function g that induces similar distributions on DS and DT. g is also required to preserve rich information about the target task so that εS(h ◦ g) is small. The above process results in the following Markov chain (assumed to hold throughout the paper): i=1 ∈ (X × Y )n and m unlabeled points {xj}m
S , DX
T , DY h−→ (cid:98)Y,
T denote the pushforwards (induced distributions)
T by g and h ◦ g. Invariance in feature space is deﬁned as minimizing a distance or g
−→ Z
X
S and D (cid:98)Y
T , D (cid:98)Y
S , DZ (1) where (cid:98)Y = h(g(X)). We let DZ of DX divergence between DZ
S and DX
S and DZ
T .
Invariance is often attained by training a discriminator d : Z (cid:55)→
Adversarial Domain Adaptation
[0, 1] to predict if z is from the source or target. g is trained both to maximize the discriminator loss and minimize the classiﬁcation loss of h ◦ g on the source domain (h is also trained with the latter objective). This leads to domain-adversarial neural networks [20, DANN], where g, h and d are parameterized with neural networks: gθ, hφ and dψ (see Algo. 1 and App. ??). Building on
DANN, conditional domain adversarial networks [37, CDAN] use the same adversarial paradigm.
However, the discriminator now takes as input the outer product, for a given x, between the predictions of the network h(g(x)) and its representation g(x). In other words, d acts on the outer product: h ⊗ g(x) := (h1(g(x)) · g(x), . . . , hk(g(x)) · g(x)) rather than on g(x). hi denotes the i-th element of vector h. We now highlight a limitation of DANNs and CDANs. 2
Table 1: Common assumptions in the domain adaptation literature.
Covariate Shift
S (cid:54)= DX
T
∀x ∈ X , DS(Y | X = x) = DT(Y | X = x)
DX
Label Shift
S (cid:54)= DY
T
∀y ∈ Y, DS(X | Y = y) = DT(X | Y = y)
DY
An Information-Theoretic Lower Bound We let DJS denote the Jensen-Shanon divergence be-tween two distributions (App. ??), and (cid:101)Z correspond to Z (for DANN) or to (cid:98)Y ⊗ Z (for CDAN). The following theorem lower bounds the joint error of the classiﬁer on the source and target domains:
T ) ≥ DJS(D (cid:101)Z
Theorem 2.1. Assuming that DJS(DY
S (cid:107) D (cid:101)Z
εS(h ◦ g) + ε T(h ◦ g) ≥
DJS(DY
S (cid:107) DY
T ) −
DJS(D (cid:101)Z
S (cid:107) D (cid:101)Z
T )
S (cid:107) DY (cid:18)(cid:113) 1 2
T ), then: (cid:113) (cid:19)2
.
Remark The lower bound is algorithm-independent. It is also a population-level result and holds asymptotically with increasing data. Zhao et al. [62] prove the theorem for k = 2 and (cid:101)Z = Z.
We extend it to CDAN and arbitrary k (it actually holds for any (cid:101)Z s.t. (cid:98)Y = (cid:101)h( (cid:101)Z) for some (cid:101)h, see
App. ??). Assuming that label distributions differ between source and target domains, the lower bound shows that: the better the alignment of feature distributions, the worse the joint error. For an invariant representation (DJS(D ˜Z
T ) = 0) with no source error, the target error will be larger than
DJS(DY
T )/2. Hence algorithms learning invariant representations and minimizing the source error are fundamentally ﬂawed when label distributions differ between source and target domains.
S , D ˜Z
S , DY
Common Assumptions to Tackle Domain Adaptation Two common assumptions about the data made in DA are covariate shift and label shift. They correspond to different ways of decomposing the joint distribution over X × Y, as detailed in Table 1. From a representation learning perspective, covariate shift is not robust to feature transformation and can lead to an effect called negative transfer [62]. At the same time, label shift clearly fails in most practical applications, e.g. transferring knowledge from synthetic to real images [51]. In that case, the input distributions are actually disjoint. 3 Main Results
In light of the limitations of existing assumptions, (e.g. covariate shift and label shift), we propose generalized label shift (GLS), a relaxation of label shift that substantially improves its applicability.
We ﬁrst discuss some of its properties and explain why the assumption is favorable in domain adaptation based on representation learning. Motivated by GLS, we then present a novel error decomposition theorem that directly suggests a bound minimization framework for domain adaptation.
The framework is naturally compatible with F -integral probability metrics [40, F -IPM] and generates a family of domain adaptation algorithms by choosing various function classes F . In a nutshell, the proposed framework applies a method of moments [31] to estimate the importance weight w of the marginal label distributions by solving a quadratic program (QP), and then uses w to align the weighted source feature distribution with the target feature distribution. 3.1 Generalized Label Shift
Deﬁnition 3.1 (Generalized Label Shift, GLS). A representation Z = g(X) satisﬁes GLS if
DS(Z | Y = y) = DT(Z | Y = y), ∀y ∈ Y. (2)
First, when g is the identity map, the above deﬁnition of GLS reduces to the original label shift assumption. Next, GLS is always achievable for any distribution pair (DS, DT): any constant function g ≡ c ∈ R satisﬁes the above deﬁnition. The most important property is arguably that, unlike label shift, GLS is compatible with a perfect classiﬁer (in the noiseless case). Precisely, if there exists a ground-truth labeling function h∗ such that Y = h∗(X), then h∗ satisﬁes GLS. As a comparison, without conditioning on Y = y, h∗ does not satisfy DS(h∗(X)) = DT(h∗(X)) if the marginal label distributions are different across domains. This observation is consistent with the lower bound in Theorem 2.1, which holds for arbitrary marginal label distributions. 3
GLS imposes label shift in the feature space Z instead of the original input space X . Conceptually, although samples from the same classes in the source and target domain can be dramatically different, the hope is to ﬁnd an intermediate representation for both domains in which samples from a given class look similar to one another. Taking digit classiﬁcation as an example and assuming the feature variable Z corresponds to the contour of a digit, it is possible that by using different contour extractors for e.g. MNIST and USPS, those contours look roughly the same in both domains. Technically, GLS can be facilitated by having separate representation extractors gS and gT for source and target [9, 49]. 3.2 An Error Decomposition Theorem based on GLS
We now provide performance guarantees for models that satisfy GLS, in the form of upper bounds on the error gap and on the joint error between source and target domains. It requires two concepts:
Deﬁnition 3.2 (Balanced Error Rate). The balanced error rate (BER) of predictor (cid:98)Y on DS is:
BERDS ( (cid:98)Y (cid:107) Y) := max j∈[k]
DS( (cid:98)Y (cid:54)= Y | Y = j). (3)
Deﬁnition 3.3 (Conditional Error Gap). Given a joint distribution D, the conditional error gap of a classiﬁer (cid:98)Y is ∆
CE( (cid:98)Y) := maxy(cid:54)=y(cid:48)∈Y 2 |DS( (cid:98)Y = y(cid:48) | Y = y) − DT( (cid:98)Y = y(cid:48) | Y = y)|.
When GLS holds, ∆
CE( (cid:98)Y) is equal to 0. We now give an upper bound on the error gap between source and target, which can also be used to obtain a generalization upper bound on the target risk.
Theorem 3.1. (Error Decomposition Theorem) For any classiﬁer (cid:98)Y = (h ◦ g)(X), ( (cid:98)Y (cid:107) Y) + 2(k − 1)∆
|εS(h ◦ g) − ε T(h ◦ g)| ≤ (cid:107)DY
T (cid:107)1 · BERDS
S − DY
CE( (cid:98)Y), where (cid:107)DY
S − DY
T (cid:107)1 := ∑k i=1 |DS(Y = i) − DT(Y = i)| is the L1 distance between DY
S and DY
T .
S − DY
Remark The upper bound in Theorem 3.1 provides a way to decompose the error gap between source and target domains. It also immediately gives a generalization bound on the target risk
ε T(h ◦ g). The bound contains two terms. The ﬁrst contains (cid:107)DY
T (cid:107)1, which measures the distance between the marginal label distributions across domains and is a constant that only depends on the adaptation problem itself, and BER, a reweighted classiﬁcation performance on the source domain. The second is ∆
CE( (cid:98)Y) measures the distance between the family of conditional distributions (cid:98)Y | Y. In other words, the bound is oblivious to the optimal labeling functions in feature space.
This is in sharp contrast with upper bounds from previous work [7, Theorem 2], [62, Theorem 4.1], which essentially decompose the error gap in terms of the distance between the marginal feature distributions (DZ
T ). Because the optimal labeling function in feature space depends on Z and is unknown in practice, such decomposition is not very informative. As a comparison, Theorem 3.1 provides a decomposition orthogonal to previous results and does not require knowledge about unknown optimal labeling functions in feature space.
T ) and the optimal labeling functions ( f Z
S , DZ
S , f Z ( (cid:98)Y (cid:107) Y), only depends on samples from the source domain
Notably, the balanced error rate, BERDS and can be minimized. Furthermore, using a data-processing argument, the conditional error gap
∆
CE( (cid:98)Y) can be minimized by aligning the conditional feature distributions across domains. Putting everything together, the result suggests that, to minimize the error gap, it sufﬁces to align the conditional distributions Z | Y = y while simultaneously minimizing the balanced error rate. In fact, under the assumption that the conditional distributions are perfectly aligned (i.e., under GLS), we can prove a stronger result, guaranteeing that the joint error is small:
Theorem 3.2. If Z = g(X) satisﬁes GLS, then for any h : Z → Y and letting (cid:98)Y = h(Z) be the predictor, we have εS( (cid:98)Y) + ε T( (cid:98)Y) ≤ 2BERDS ( (cid:98)Y (cid:107) Y). 3.3 Conditions for Generalized Label Shift
The main difﬁculty in applying a bound minimization algorithm inspired by Theorem 3.1 is that we do not have labels from the target domain in UDA, so we cannot directly align the conditional label distributions. By using relative class weights between domains, we can provide a necessary condition for GLS that bypasses an explicit alignment of the conditional feature distributions. 4
Deﬁnition 3.4. Assuming DS(Y = y) > 0, ∀y ∈ Y, we let w ∈ Rk denote the importance weights of the target and source label distributions: wy :=
DT(Y = y)
DS(Y = y)
,
∀y ∈ Y. (4)
Lemma 3.1. (Necessary condition for GLS) If Z = g(X) satisﬁes GLS, then DT( (cid:101)Z) = ∑y∈Y wy ·
DS( (cid:101)Z, Y = y) =: Dw
S ( (cid:101)Z) where (cid:101)Z veriﬁes either (cid:101)Z = Z or (cid:101)Z = (cid:98)Y ⊗ Z.
Compared to previous work that attempts to align DT(Z) with DS(Z) (using adversarial discrim-inators [20] or maximum mean discrepancy (MMD) [34]) or DT( ˆY ⊗ Z) with DS( ˆY ⊗ Z) [37],
Lemma 3.1 suggests to instead align DT( (cid:101)Z) with the reweighted marginal distribution Dw
S ( (cid:101)Z).
Reciprocally, the following two theorems give sufﬁcient conditions to know when perfectly aligned target feature distribution and reweighted source feature distribution imply GLS:
Theorem 3.3. (Clustering structure implies sufﬁciency) Let Z = g(X) such that DT(Z) = Dw
S (Z).
Assume DT(Y = y) > 0, ∀y ∈ Y. If there exists a partition of Z = ∪y∈Y Zy such that ∀y ∈ Y,
DS(Z ∈ Zy | Y = y) = DT(Z ∈ Zy | Y = y) = 1, then Z = g(X) satisﬁes GLS.
Remark Theorem 3.3 shows that if there exists a partition of the feature space such that instances with the same label are within the same component, then aligning the target feature distribution with the reweighted source feature distribution implies GLS. While this clustering assumption may seem strong, it is consistent with the goal of reducing classiﬁcation error: if such a clustering exists, then there also exists a perfect predictor based on the feature Z = g(X), i.e., the cluster index.
Theorem 3.4. Let (cid:98)Y = h(Z), γ := miny∈Y DT(Y = y) and wM := maxy∈Y wy. For (cid:101)Z = Z or (cid:101)Z = ˆY ⊗ Z, we have: max y∈Y dTV(DS(Z | Y = y), DT(Z | Y = y)) ≤ wMεS( (cid:98)Y) + ε T( (cid:98)Y) + 8DJS(Dw
S ( (cid:101)Z)(cid:107)DT( (cid:101)Z))
. (cid:113)
γ
Theorem 3.4 conﬁrms that matching DT( (cid:101)Z) with Dw
S ( (cid:101)Z) is the proper objective in the context of mismatched label distributions. It shows that, for matched feature distributions and a source error equal to zero, successful domain adaptation (i.e. a target error equal to zero) implies that GLS holds.
Combined with Theorem 3.2, we even get equivalence between the two.
Remark Thm. 3.4 extends Thm. 3.3 by incorporating the clustering assumption in the joint error achievable by a classiﬁer (cid:98)Y based on a ﬁxed Z. In particular, if the clustering structure holds, the joint error is 0 for an appropriate h, and aligning the reweighted feature distributions implies GLS. 3.4 Estimating the Importance Weights w
Inspired by the moment matching technique to estimate w under label shift from Lipton et al. [31], we propose a method to get w under GLS by solving a quadratic program (QP).
Deﬁnition 3.5. We let C ∈ R|Y |×|Y | denote the confusion matrix of the classiﬁer on the source domain and µ ∈ R|Y | the distribution of predictions on the target one, ∀y, y(cid:48) ∈ Y:
Cy,y(cid:48) := DS( (cid:98)Y = y, Y = y(cid:48)),
µy := DT( (cid:98)Y = y).
Lemma 3.2. If GLS is veriﬁed, and if the confusion matrix C is invertible, then w = C−1µ.
The key insight from Lemma 3.2 is that, to estimate the importance vector w under GLS, we do not need access to labels from the target domain. However, matrix inversion is notoriously numerically unstable, especially with ﬁnite sample estimates ˆC and ˆµ of C and µ. We propose to solve instead the following QP (written as QP( ˆC, ˆµ)), whose solution will be consistent if ˆC → C and ˆµ → µ: minimize w 1 2 (cid:107) ˆµ − ˆCw(cid:107)2 2, subject to w ≥ 0, wTDS(Y) = 1. (5)
The above program (5) can be efﬁciently solved in time O(|Y |3), with |Y | small and constant; and by construction, its solution is element-wise non-negative, even with limited amounts of data to estimate
C and µ. 5
Algorithm 1 Importance-Weighted Domain Adaptation 1: Input: source and target data (xS, yS), xT; gθ, hφ and dψ; epochs E, batches per epoch B 2: Initialize w1 = 1 3: for t = 1 to E do 4: 5: 6: 7: 8: 9:
S) and (xi
DA w.r.t. θ, minimize Lwt
Sample batches (xi
Maximize Lwt for i = 1 to s do
Initialize ˆC = 0, ˆµ = 0 for b = 1 to B do
DA w.r.t. ψ and minimize Lwt
C w.r.t. θ and φ
T) of size s
S, yi
ˆC·yi
← ˆC·yi
+ hφ(gθ(xi
ˆC ← ˆC/sB and ˆµ ← ˆµ/sB;
S
S
S)) (yi
S-th column) and then wt+1 = λ · QP( ˆC, ˆµ) + (1 − λ)wt
ˆµ ← ˆµ + hφ(gθ(xi
T)) 10:
Lemma 3.3. If the source error εS(h ◦ g) is zero and the source and target marginals verify
DJS(D ˜w
S (Z), DT(Z)) = 0, then the estimated weight vector w is equal to ˜w.
Lemma 3.3 shows that the weight estimation is stable once the DA losses have converged, but it does not imply convergence to the true weights (see Sec. 4.2 and App. ?? for more details). 3.5 F -IPM for Distributional Alignment
To align the target feature distribution and the reweighted source feature distribution as suggested by
Lemma 3.1, we now provide a general framework using the integral probability metric [40, IPM].
Deﬁnition 3.6. With F a set of real-valued functions, the F -IPM between distributions D and D(cid:48) is dF (D, D(cid:48)) := sup f ∈F
|E
X∼D[ f (X)] − E
X∼D(cid:48) [ f (X)]|. (6)
By approximating any function class F using parametrized models, e.g., neural networks, we obtain a general framework for domain adaptation by aligning reweighted source feature distribution and target feature distribution, i.e. by minimizing dF (DT( (cid:101)Z), Dw
S ( (cid:101)Z)). Below, by instantiating F to be the set of bounded norm functions in a RKHS H [25], we obtain maximum mean discrepancy methods, leading to IWJAN (cf. Section 4.1), a variant of JAN [36] for UDA. 4 Practical Implementation 4.1 Algorithms
The sections above suggest simple algorithms based on representation learning: (i) estimate w on the ﬂy during training, (ii) align the feature distributions (cid:101)Z of the target domain with the reweighted feature distribution of the source domain and, (iii) minimize the balanced error rate. Overall, we present the pseudocode of our algorithm in Alg. 1.
To compute w, we build estimators ˆC and ˆµ of C and µ by averaging during each epoch the predictions of the classiﬁer on the source (per true class) and target (overall). This corresponds to the inner-most loop of Algorithm 1 (lines 8-9). At epoch end, w is updated (line 10), and the estimators reset to 0. We have found empirically that using an exponential moving average of w performs better. Our results all use a factor λ = 0.5. We also note that Alg. 1 implies a minimal computational overhead (see App. ?? for details): in practice our algorithms run as fast as their base versions.
Using w, we can deﬁne our ﬁrst algorithm, Importance-Weighted Domain Adversarial Network (IWDAN), that aligns Dw
S (Z) and DT(Z)) using a discriminator. To that end, we modify the DANN losses LDA and LC as follows. For batches (xi
S, yi
S) and (xi log(dψ(gθ(xi
T) of size s, the weighted DA loss is:
S))) + log(1 − dψ(gθ(xi
T))). (7)
Lw
DA(xi
S, yi
S, xi
T; θ, ψ) = − 1 s s
∑ i=1 wyi
S
We verify in App. ??, that the standard ADA framework applied to Lw
DJS(Dw
DA indeed minimizes
S (Z)(cid:107)DT(Z)). Our second algorithm, Importance-Weighted Joint Adaptation Networks 6
Figure 1: Gains of our algorithms vs their base versions (the horizontal grey line) for 100 tasks. The x-axis is DJS(DY
T ). The mean improvements for IWDAN and IWDAN-O (resp. IWCDAN and
IWCDAN-O) are 6.55% and 8.14% (resp. 2.25% and 2.81%).
S , DY (IWJAN) is based on JAN [36] and follows the reweighting principle described in Section 3.5 with
F a learnt RKHS (the exact JAN and IWJAN losses are speciﬁed in App. ??). Finally, our third algorithm is Importance-Weighted Conditional Domain Adversarial Network (IWCDAN). It matches
S ( ˆY ⊗ Z) with DT( ˆY ⊗ Z) by replacing the standard adversarial loss in CDAN with Eq. 7, where
Dw dψ takes as input (hφ ◦ gθ) ⊗ gθ instead of gθ. The classiﬁer loss for our three variants is: s
∑ i=1 1 k · DS(Y = y) log(hφ(gθ(xi
S; θ, φ) = −
C (xi
S))yi
S, yi
Lw 1 s (8)
).
S
This reweighting is suggested by our theoretical analysis from Section 3, where we seek to minimize ( (cid:98)Y (cid:107) Y). We also deﬁne oracle versions, IWDAN-O, IWJAN-O the balanced error rate BERDS and IWCDAN-O, where the weights w are the true weights. It gives an idealistic version of the reweighting method, and allows to assess the soundness of GLS. IWDAN, IWJAN and IWCDAN are Alg. 1 with their respective loss functions, the oracle versions use the true weights instead of wt. 4.2 Experiments
We apply our three base algorithms, their importance weighted versions, and the oracles to 4 standard
DA datasets generating 21 tasks: Digits (MNIST ↔ USPS [18, 29]), Visda [51], Ofﬁce-31 [45] and
Ofﬁce-Home [50]. All values are averages over 5 runs of the best test accuracy throughout training (evaluated at the end of each epoch). We used that value for fairness with respect to the baselines (as shown in the left panel of Figure 2, the performance of DANN decreases as training progresses, due to the inappropriate matching of representations showcased in Theorem 2.1). For full details, see
App. ?? and ??.
Performance vs DJS We artiﬁcially generate 100 tasks from MNIST and USPS by considering various random subsets of the classes in either the source or target domain (see Appendix ?? for details). These 100 DA tasks have a DJS(DY
T ) varying between 0 and 0.1. Applying IWDAN and IWCDAN results in Fig. 1. We see a clear correlation between the improvements provided by our algorithms and DJS(DY
T ), which is well aligned with Theorem 2.1. Moreover, IWDAN outperfoms DANN on the 100 tasks and IWCDAN bests CDAN on 94. Even on small divergences, our algorithms do not suffer compared to their base versions.
S , DY
S , DY
Original Datasets The average results on each dataset are shown in Table 2 (see App.?? for the per-task breakdown). IWDAN outperforms the basic algorithm DANN by 1.75%, 1.64%, 1.16% and 2.65% on the Digits, Visda, Ofﬁce-31 and Ofﬁce-Home tasks respectively. Gains for IWCDAN are more limited, but still present: 0.18%, 0.89%, 0.07% and 1.07% respectively. This might be explained by the fact that CDAN enforces a weak form of GLS (App. ??). Gains for JAN are 0.58%, 0.19% and 0.19%. We also show the fraction of times (over all seeds and tasks) our variants outperform the original algorithms. Even for small gains, the variants provide consistent improvements. Additionally, the oracle versions show larger improvements, which strongly supports enforcing GLS. 7
Table 2: Average results on the various domains (Digits has 2 tasks, Visda 1, Ofﬁce-31 6 and Ofﬁce-Home 12). The preﬁx s denotes the experiment where the source domain is subsampled to increase
DJS(DY
T ). Each number is a mean over 5 seeds, the subscript denotes the fraction of times (out of 5 seeds × #tasks) our algorithms outperform their base versions. JAN is not available on Digits.
S , DY
METHOD
DIGITS sDIGITS
VISDA sVISDA
NO DA 77.17 75.67 48.39 49.02
O-31 77.81 sO-31 75.72
O-H 56.39 sO-H 51.34 93.15 95.72 83.24 82.74 52.85 61.88
DANN 94.90100% 92.54100% 63.52100% 60.18100% 83.9087% 82.60100% 62.2797% 57.61100%
IWDAN
IWDAN-O 95.27100% 94.46100% 64.19100% 62.10100% 85.3397% 84.41100% 64.68100% 60.87100%
CDAN 60.19 95.9080% 93.22100% 66.4960% 65.83100% 87.3073% 83.88100% 65.6670% 61.24100%
IWCDAN
IWCDAN-O 95.8590% 94.81100% 68.15100% 66.85100% 88.1490% 85.47100% 67.6498% 63.73100% 50.64
JAN
IWJAN
IWJAN-O 57.56100% 57.12100% 85.3260% 82.6197% 59.7863% 55.89100% 61.48100% 61.30100% 87.14100% 86.24100% 60.7392% 57.36100%
N/A
N/A
N/A
N/A
N/A
N/A 65.60 76.17 56.25 56.98 81.62 85.13 64.59 59.62 59.59 53.94 87.23 51.83 78.21 88.23
Figure 2: Left Accuracy on sDigits. Right Euclidian distance between estimated and true weights.
S (cid:107) DY
Subsampled datasets The original datasets have fairly balanced classes, making the JSD between source and target label distributions DJS(DY
T ) rather small (Tables ??, ?? and ?? in App. ??).
To evaluate our algorithms on larger divergences, we arbitrarily modify the source domains above by considering only 30% of the samples coming from the ﬁrst half of their classes. This results in larger divergences (Tables ??, ?? and ??). Performance is shown in Table 2 (datasets preﬁxed by s). For IWDAN, we see gains of 9.3%, 7.33%, 6.43% and 5.58% on the digits, Visda, Ofﬁce-31 and Ofﬁce-Home datasets respectively. For IWCDAN, improvements are 4.99%, 5.64%, 2.26% and 4.99%, and IWJAN shows gains of 6.48%, 4.40% and 1.95%. Moreover, on all seeds and tasks but one, our variants outperform their base versions.
Importance weights While our method demonstrates gains empirically, Lemma 3.2 does not guaran-tee convergence of w to the true weights. In Fig. 2, we show the test accuracy and distance between estimated and true weights during training on sDigits. We see that DANN’s performance gets worse after a few epoch, as predicted by Theorem 2.1. The representation matching objective collapses classes that are over-represented in the target domain on the under-represented ones (see App. ??).
This phenomenon does not occur for IWDAN and IWDAN-O. Both monotonously improve in accu-racy and estimation (see Lemma 3.3 and App. ?? for more details). We also observe that IWDAN’s weights do not converge perfectly. This suggests that ﬁne-tuning λ (we used λ = 0.5 in all our experiments for simplicity) or updating w more or less often could lead to better performance.
Ablation Study Our algorithms have two components, a weighted adversarial loss Lw
DA and a weighted classiﬁcation loss Lw
C . In Table 3, we augment DANN and CDAN using those losses separately (with the true weights). We observe that DANN beneﬁts essentially from the reweighting of its adversarial loss Lw
DA, the classiﬁcation loss has little effect. For CDAN, gains are essentially seen on the subsampled datasets. Both losses help, with a +2% extra gain for Lw
DA. 8
Method
Table 3: Ablation study on the Digits tasks. sDigits Method
Digits
Digits
DANN
DANN + Lw
C
DANN + Lw
DA
IWDAN-O 93.15 93.27 95.31 95.27 83.24 84.52 94.41 94.46
CDAN
CDAN + Lw
C
CDAN + Lw
DA
IWCDAN-O 95.72 95.65 95.42 95.85 sDigits 88.23 91.01 93.18 94.81 5