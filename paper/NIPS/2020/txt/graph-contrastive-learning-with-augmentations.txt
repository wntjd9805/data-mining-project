Abstract
Generalizable, transferrable, and robust representation learning on graph-structured data remains a challenge for current graph neural networks (GNNs). Unlike what has been developed for convolutional neural networks (CNNs) for image data, self-supervised learning and pre-training are less explored for GNNs.
In this paper, we propose a graph contrastive learning (GraphCL) framework for learning unsupervised representations of graph data. We ﬁrst design four types of graph augmentations to incorporate various priors. We then systematically study the impact of various combinations of graph augmentations on multiple datasets, in four different settings: semi-supervised, unsupervised, and transfer learning as well as adversarial attacks. The results show that, even without tuning augmentation extents nor using sophisticated GNN architectures, our GraphCL framework can produce graph representations of similar or better generalizability, transferrability, and robustness compared to state-of-the-art methods. We also investigate the impact of parameterized graph augmentation extents and patterns, and observe further performance gains in preliminary experiments. Our codes are available at: https://github.com/Shen-Lab/GraphCL. 1

Introduction
Graph neural networks (GNNs) [1, 2, 3], following a neighborhood aggregation scheme, are increas-ingly popular for graph-structured data. Numerous variants of GNNs have been proposed to achieve state-of-the-art performances in graph-based tasks, such as node or link classiﬁcation [1, 2, 4, 5, 6], link prediction [7] and graph classiﬁcation [8, 3]. Intriguingly, in most scenarios of graph-level tasks, GNNs are trained end-to-end under supervision. For GNNs, there is little exploration (except
[9]) of (self-supervised) pre-training, a technique commonly used as a regularizer in training deep architectures that suffer from gradient vanishing/explosion [10, 11]. The reasons behind the intriguing phenomena could be that most studied graph datasets, as shown in [12], are often limited in size and
GNNs often have shallow architectures to avoid over-smoothing [13] or “information loss” [14].
We however argue for the necessity of exploring GNN pre-training schemes. Task-speciﬁc labels can be extremely scarce for graph datasets (e.g. in biology and chemistry labeling through wet-lab experiments is often resource- and time-intensive) [15, 9], and pre-training can be a promising technique to mitigate the issue, as it does in convolutional neural networks (CNNs) [16, 17, 18]. As to the conjectured reasons for the lack of GNN pre-training: ﬁrst, real-world graph data can be huge and even benchmark datasets are recently getting larger [12, 19]; second, even for shallow models, pre-training could initialize parameters in a “better" attraction basin around a local minimum associated with better generalization [11]. Therefore, we emphasize the signiﬁcance of GNN pre-training.
Compared to CNNs for images, there are unique challenges of designing GNN pre-training schemes for graph-structured data. Unlike geometric information in images, rich structured information of
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
various contexts exist in graph data [20, 21] as graphs are abstracted representations of raw data with diverse nature (e.g. molecules made of chemically-bonded atoms and networks of socially-interacting people). It is thus difﬁcult to design a GNN pre-training scheme generically beneﬁcial to down-stream tasks. A naïve GNN pre-training scheme for graph-level tasks is to reconstruct the vertex adjacency information (e.g. GAE [22] and GraphSAGE [23] in network embedding). This scheme can be very limited (as seen in [20] and our Sec. 5) because it over-emphasizes proximity that is not always beneﬁcial [20], and could hurt structural information [24]. Therefore, a well designed pre-training framework is needed to capture highly heterogeneous information in graph-structured data.
Recently, in visual representation learning, contrastive learning has renewed a surge of interest
[25, 26, 27, 18, 28]. Self-supervision with handcrafted pretext tasks [29, 30, 31, 32] relies on heuristics to design, and thus could limit the generality of the learned representations. In comparison, contrastive learning aims to learn representations by maximizing feature consistency under differently augmented views, that exploit data- or task-speciﬁc augmentations [33], to inject the desired feature invariance.
If extended to pre-training GCNs, this framework can potentially overcome the aforementioned limitations of proximity-based pre-training methods [22, 23, 34, 35, 36, 37, 38, 39]. However, it is not straightforward to be directly applied outside visual representation learning and demands signiﬁcant extensions to graph representation learning, leading to our innovations below.
Our Contributions. In this paper, we have developed contrastive learning with augmentations (i) Since data for GNN pre-training to address the challenge of data heterogeneity in graphs. augmentations are the prerequisite for constrastive learning but are under-explored in graph-data
[40], we ﬁrst design four types of graph data augmentations, each of which imposes certain prior over graph data and parameterized for the extent and pattern. (ii) Utilizing them to obtain correlated views, we propose a novel graph contrastive learning framework (GraphCL) for GNN pre-training, so that representations invariant to specialized perturbations can be learned for diverse graph-structured data. Moreover, we show that GraphCL actually performs mutual information maximization, and the connection is drawn between GraphCL and recently proposed contrastive learning methods that we demonstrate that GraphCL can be rewritten as a general framework unifying a broad family of contrastive learning methods on graph-structured data. (iii) Systematic study is performed to assess the performance of contrasting different augmentations on various types of datasets, revealing the rationale of the performances and providing the guidance to adopt the framework for speciﬁc datasets. (iv) Experiments show that GraphCL achieves state-of-the-art performance in the settings of semi-supervised learning, unsupervised representation learning and transfer learning. It additionally boosts robustness against common adversarial attacks. 2