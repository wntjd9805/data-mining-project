Abstract
To learn good joint policies for multi-agent collaboration with imperfect infor-mation remains a fundamental challenge. While for two-player zero-sum games, coordinate-ascent approaches (optimizing one agent’s policy at a time, e.g., self-play [35, 20]) work with guarantees, in multi-agent cooperative setting they often converge to sub-optimal Nash equilibrium. On the other hand, directly modeling joint policy changes in imperfect information game is nontrivial due to complicated interplay of policies (e.g., upstream updates affect downstream state reachability).
In this paper, we show global changes of game values can be decomposed to policy changes localized at each information set, with a novel term named policy-change density. Based on this, we propose Joint Policy Search (JPS) that iteratively improves joint policies of collaborative agents in imperfect information games, without re-evaluating the entire game. On multi-agent collaborative tabular games,
JPS is proven to never worsen performance and can improve solutions provided by unilateral approaches (e.g, CFR [44]), outperforming algorithms designed for col-laborative policy learning (e.g. BAD [16]). Furthermore, for real-world game with exponential states, JPS has an online form that naturally links with gradient updates.
We test it to Contract Bridge, a 4-player imperfect-information game where a team of 2 collaborates to compete against the other. In its bidding phase, players bid in turn to ﬁnd a good contract through a limited information channel. Based on a strong baseline agent that bids competitive Bridge purely through domain-agnostic self-play, JPS improves collaboration of team players and outperforms WBridge5, a championship-winning software, by +0.63 IMPs (International Matching Points) per board over 1000 games, substantially better than previous SoTA (+0.41 IMPs/b against WBridge5) under Double-Dummy evaluation. Note that +0.1 IMPs/b is regarded as a nontrivial improvement in Computer Bridge. Part of the code is released in https://github.com/facebookresearch/jps.

Introduction 1
Deep reinforcement learning has demonstrated strong or even super-human performance in many complex games (e.g., Atari [28], Dota 2 [30], Starcraft [42], Poker [5, 29], Find and Seek [1], Chess,
Go and Shogi [34, 36, 39]). While massive computational resources are used, the underlying approach is quite simple: to iteratively improve the policy of the current agent, assuming stationary environment and ﬁxed policies of all other agents. Although for two-player zero-sum games this is effective, for multi-agent collaborative with imperfect information, it often leads to sub-optimal Nash equilibria where none of the agents is willing to change their policies unilaterally. For example, if speaking one speciﬁc language becomes a convention, then unilaterally switching to a different one is not a good choice, even if the other agent actually knows that language better.
In this case, it is necessary to learn to jointly change policies of multiple agents to achieve better equilibria. One brute-force approach is to change policies of multiple agents simultaneously, and re-evaluate them one by one on the entire game to seek for performance improvement, which is 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
computationally expensive. Alternatively, one might hope that a change of a sparse subset of policies might lead to “local” changes of game values and evaluating these local changes can be faster. While this is intuitively reasonable, in imperfect information game (IG), a local policy change could affects the value of both downstream and upstream decision points, leading to non-local interplay.
In this paper, we realize this locality idea by proposing policy-change density, a quantity deﬁned at each perfect information history state with two key properties: (1) when summing over all states, it gives overall game value changes upon policy update, and (2) when the local policy remains the same, the density vanishes regardless of any policy changes at other parts of the game tree. Based on this density, the value changes of any policy update on a sparse set of decision points can be decomposed into a summation on each decision point (or information set), which is easy and efﬁcient to compute.
Based on that, we propose a novel approach, called Joint Policy Search (JPS). For tabular IG, JPS is proven to never worsen the current policy, and is computationally more efﬁcient than brute-force approaches. For simple collaborative games with enumerable states, we show that JPS improves policies returned by Counterfactual Regret Minimization baseline [44] by a fairly good margin, outperforming methods with explicit belief-modeling [16] and Advantageous Actor-Critic (A2C) [27] with self-play, in particular in more complicated games.
Furthermore, we show JPS has a sample-based formulation and can be readily combined with gradient methods and neural networks. This enables us to apply JPS to Contract Bridge bidding, in which enumerating the information sets are computationally prohibitive1. Improved by JPS upon a strong
A2C baseline, the resulting agent outperforms Wbridge5, a world computer bridge program that won multiple championships, by a large margin of +0.63 IMPs per board (IMPs/b) over a tournament of 1000 games, better than previous state-of-the-art [18] that beats WBridge5 by +0.41 IMPs/b.
All of them use Double-Dummy evaluation [19]. Note that +0.1 IMPs/b is regarded as nontrivial improvement in computer bridge [32]. 2