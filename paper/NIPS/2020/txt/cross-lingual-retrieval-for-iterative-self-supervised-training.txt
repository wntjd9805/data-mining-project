Abstract
Recent studies have demonstrated the cross-lingual alignment ability of multilingual pretrained language models. In this work, we found that the cross-lingual alignment can be further improved by training seq2seq models on sentence pairs mined using their own encoder outputs. We utilized these ﬁndings to develop a new approach — cross-lingual retrieval for iterative self-supervised training (CRISS), where mining and training processes are applied iteratively, improving cross-lingual alignment and translation ability at the same time. Using this method, we achieved state-of-the-art unsupervised machine translation results on 9 language directions with an average improvement of 2.4 BLEU, and on the Tatoeba sentence retrieval task in the XTREME benchmark on 16 languages with an average improvement of 21.5% in absolute accuracy. Furthermore, CRISS also brings an additional 1.8 BLEU improvement on average compared to mBART, when ﬁnetuned on supervised machine translation downstream tasks. Our code and pretrained models are publicly available. 1 1

Introduction
Pretraining has demonstrated success in various natural language processing (NLP) tasks. In particular, self-supervised pretraining can learn useful representations by training with pretext task, such as cloze and masked language modeling, denoising autoencoder, etc. on large amounts of unlabelled data
[11, 25, 28, 32, 35, 51]. Such learned “universal representations" can be ﬁnetuned on task-speciﬁc training data to achieve good performance on downstream tasks.
More recently, new pretraining techniques have been developed in the multilingual settings, pushing the state-of-the-art on cross-lingual understandin, and machine translation. Since the access to labeled parallel data is very limited, especially for low resource languages, better pretraining techniques that utilizes unlabeled data is the key to unlock better machine translation performances [9, 27, 42].
In this work, we propose a novel self-supervised pretraining method for multilingual sequence gener-ation: Cross-lingual Retrieval for Iterative Self-Supervised training (CRISS). CRISS is developed based on the ﬁnding that the encoder outputs of multilingual denoising autoencoder can be used as language agnostic representation to retrieve parallel sentence pairs, and training the model on these retrieved sentence pairs can further improve its sentence retrieval and translation capabilities in an iterative manner. Using only unlabeled data from many different languages, CRISS iteratively mines 1https://github.com/pytorch/fairseq/blob/master/examples/criss 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for parallel sentences across languages, trains a new better multilingual model using these mined sentence pairs, mines again for better parallel sentences, and repeats.
In summary, we present the following contributions in this paper:
• We present empirical results that show the encoder outputs of multilingual denoising autoen-coder (mBART) represent language-agnostic semantic meaning.
• We present empirical results that show ﬁnetuning mBART on only one pair of parallel bi-text will improve cross-lingual alignment for all language directions.
• We introduce a new iterative self-supervised learning method that combines mining and multilingual training to improve both tasks after each iteration.
• We signiﬁcantly outperform the previous state of the art on unsupervised machine translation and sentence retrieval.
• We show that our pretraining method can further improve the performance of supervised machine translation task compared to mBART.
This paper is organized as follows. Section 2 is devoted to related work. Section 3 introduces improvable language agnostic representation emerging from pretraining. Section 4 describes the details of cross-lingual retrieval for iterative self-supervised training (CRISS). Section 5 evaluates
CRISS with unsupervised and supervised machine translation tasks as well as sentence retrieval tasks.
Section 6 iterates over ablation studies to understand the right conﬁgurations of the approach. Then we conclude by Section 7. 2