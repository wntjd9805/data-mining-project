Abstract
Self-imitation learning motivated by lower-bound Q-learning is a novel and effec-tive approach for off-policy learning. In this work, we propose a n-step lower bound which generalizes the original return-based lower-bound Q-learning, and introduce a new family of self-imitation learning algorithms. To provide a formal motivation for the potential performance gains provided by self-imitation learning, we show that n-step lower bound Q-learning achieves a trade-off between ﬁxed point bias and contraction rate, drawing close connections to the popular uncorrected n-step
Q-learning. We ﬁnally show that n-step lower bound Q-learning is a more robust alternative to return-based self-imitation learning and uncorrected n-step, over a wide range of continuous control benchmark tasks. The implementation is available at https://github.com/robintyh1/nstep-sil. 1

Introduction
Learning with off-policy data is of central importance to scalable reinforcement learning (RL). The traditional framework of off-policy learning is based on importance sampling (IS): for example, in policy evaluation, given trajectories (xt, at, rt)∞ t=0 generated under behavior policy µ, the objective is to evaluate Q-function Qπ(x0, a0) of a target policy π. Naive IS estimator involves products of the form π(at | xt)/µ(at | xt) and is infeasible in practice due to high variance. To control the variance, a line of prior work has focused on operator-based estimation to avoid full IS products, which reduces the estimation procedure into repeated iterations of off-policy evaluation operators
[1–3]. Each iteration of the operator requires only local IS ratios, which greatly stabilizes the update.
More formally, such operators T are designed such that their ﬁxed points are the target Q-function
T Qπ = Qπ. As such, these operators are unbiased and conducive to theoretical analysis. However, a large number of prior work has observed that certain biased operators tend to have signiﬁcant empirical advantages [4–6]. One notable example is the uncorrected n-step operator, which directly bootstraps from n-step target trajectories without IS corrections [4]. The removal of all IS ratios biases the estimate, but allows the learning signal to be propagated over a longer horizon (in Section 2, we will characterize such effects as contraction rates). Indeed, when behavior trajectories are unlikely under the current policy, small IS ratios π(at | xt)/µ(at | xt) quickly cut off the learning signal. In general, there is a trade-off between the ﬁxed point bias and contraction rates. Empirical ﬁndings suggest that it might be desirable to introduce bias in exchange for faster contractions in practice [7].
Recently, self-imitation learning (SIL) has been developed as a family of novel off-policy algorithms which facilitate efﬁcient learning from highly off-policy data [8–10]. In its original form, SIL is motivated as lower bound Q-learning [11]. In particular, let QL(x, a) ≤ Qπ∗ (x, a) denote a lower bound of the optimal Q-function Qπ∗
. Optimizing auxiliary losses which encourage Qθ(x, a) ≥
QL(x, a) could signiﬁcantly speed up learning with the trained Q-function Qθ(x, a). Such auxiliary losses could be extended to actor-critic algorithms with stochastic policies [8]: SIL suggests optimizing a policy πθ(a | x) by maximizing an objective similar to [Qµ(a | x) − V πθ (x)]+ log πθ(a | x), 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
where V πθ (x) is the value-function for policy πθ, with [x]+ := max(0, x). The update is intuitively reasonable: if a certain actions a is high-performing under behavior policy µ, such that Qµ(x, a) >
V πθ (x), the policy πθ(a | x) should imitate such actions.
On a high-level, SIL is similar to the uncorrected n-step update in several aspects. With no explicit
IS ratios, both methods entail that off-policy learning signals propagate over long horizons without being cut-off. As a result, both methods are biased due to the absence of proper corrections, and could be seen as trading-off ﬁxed point bias for fast contractions.
Main idea.
In this paper, we make several theoretical and empirical contributions.
• Generalized SIL. In Section 3, we propose generalized SIL which strictly extends the original SIL formulation [8]. Generalized SIL provides additional ﬂexibility and advantages over the original
SIL: it learns from partial trajectories and bootstraps with learned Q-function; it applies to both stochastic and deterministic actor-critic algorithms.
• Trade-off. In Section 4, we formalize the trade-offs of SIL. We show that generalized SIL trades-off contraction rates with ﬁxed point bias in a similar way to uncorrected n-step [7]. Unlike uncorrected n-step, for which ﬁxed point bias could be either positive or negative, the operator for
SIL induces positive bias, which ﬁts the motivation of SIL to move towards optimal Q-functions.
• Empirical. In Section 5, we show generalized SIL outperforms alternative baseline algorithms. 2