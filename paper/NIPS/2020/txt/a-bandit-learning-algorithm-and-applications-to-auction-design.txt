Abstract
We consider online bandit learning in which at every time step, an algorithm has to make a decision and then observe only its reward. The goal is to design efficient (polynomial-time) algorithms that achieve a total reward approximately close to that of the best fixed decision in hindsight. In this paper, we introduce a new notion of (λ, µ)-concave functions and present a bandit learning algorithm that achieves a performance guarantee which is characterized as a function of the concavity parameters λ and µ. The algorithm is based on the mirror descent algorithm in which the update directions follow the gradient of the multilinear extensions of the reward functions. The regret bound induced by our algorithm is (cid:101)O(
T ) which is nearly optimal.
We apply our algorithm to auction design, specifically to welfare maximization, revenue maximization, and no-envy learning in auctions. In welfare maximization, we show that a version of fictitious play in smooth auctions guarantees a com-petitive regret bound which is determined by the smooth parameters. In revenue maximization, we consider the simultaneous second-price auctions with reserve prices in multi-parameter environments. We give a bandit algorithm which achieves the total revenue at least 1/2 times that of the best fixed reserve prices in hind-sight. In no-envy learning, we study the bandit item selection problem where the player valuation is submodular and provide an efficient 1/2-approximation no-envy algorithm.
√ 1

Introduction
In online learning, the goal is to design algorithms which are robust in dynamically evolving environ-ments by applying optimization methods that learn from experience and observations. Characterizing conditions, or in general discovering regularity properties, under which efficient online learning algorithms with performance guarantee exist is a major research agenda in online learning. In this paper, we consider this line of research and present a new regularity condition for the design of efficient online learning algorithms. Subsequently, we establish the applicability of our approach in auction design.
General online problem. At each time step t = 1, 2, . . ., an algorithm chooses xt ∈ [0, 1]n. After the algorithm has committed to its choice, an adversary selects a function f t : [0, 1]n → [0, 1] that subsequently induces the reward of f t(xt) for the algorithm. In the problem, we are interested in the bandit setting that at every time t, the algorithm observes only its reward f t(xt). The goal is to efficiently achieve the total gain approximately close to that obtained by the best decision in hindsight.
We consider the following notion of regret which measures the performance of algorithms. An algorithm is (r, R(T ))-regret if for arbitrary total number of time steps T and for any sequence of reward functions f 1, . . . , f T ∈ F, (cid:80)T t=1 f t(x) − R(T ). We also t=1 f t(xt) ≥ r · maxx∈[0,1]n (cid:80)T 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
say that the algorithm achieves a r-regret bound of R(T ). In general, one seeks algorithms with (r, R(T ))-regret such that r > 0 is as large as possible (ideally, close to 1) and R(T ) is sublinear as a function of T , i.e., R(T ) = o(T ). We also call r as the approximation ratio of the algorithm.
We introduce a regularity notion that generalizes the notion of concavity. The new notion, while simple, is crucial in our framework in order to design efficient online learning algorithms with performance guarantee.
Definition 1 A function F is (λ, µ)-concave if for every vectors x and x∗, (cid:104)∇F (x), x∗ − x(cid:105) ≥ λF (x∗) − µF (x) (1)
Note that a concave function is (1, 1)-concave. A non-trivial example, shown in the paper, is the (1, 2)-concavity of the multilinear extension of a monotone submodular function. 1.1 The Main Algorithm
We aim to design a bandit algorithm for the general online problem with emphasis on auctions.
Bandit algorithms have been widely studied in online convex optimization [17] but in the context of auction design, standard approaches have various limits. The main issues are: (1) the non-concavity of the (reward) functions, and (2) the intrinsic nature of the bandit setting (only the value f t(xt) is observed). We overcome these issues by the approach which consists of lifting the search space (of the solutions) and the reward functions to a higher dimension space and considering the multilinear extensions of the reward functions in that space. Concretely, we consider a sufficiently dense lattice
L in [0, 1]n such that every point in [0, 1]n can be approximated by a point in L. Then, we lift all lattice points in L to vertices of a hypercube in a high dimension space. Subsequently, we consider the multilinear extensions of reward functions f t in that space. This procedure enables several useful properties, in particular the (·, ·)-concavity, that hold for the multilinear extensions but not necessarily for the original reward functions. (For example, the multilinear extension of a monotone submodular function is always (1, 2)-concave but the submodular function is not.) The introduction of (·, ·)-concavity and the use of multilinear extensions constitute the novel points in our approach compared to the previous ones. This allows us to bound the regret of our algorithm which is based on the classic mirror descent with respect to the gradients of the multilinear extensions.
Informal Theorem 1 Let f t : [0, 1]n → [0, 1] be the reward function at time 1 ≤ t ≤ T and let F t be the multilinear extension of the discretization of f t on a lattice L. Assume that f t’s are L-Lipschitz and F t’s are (λ, µ)-concave. Then, there exists a bandit algorithm achieving
T (cid:88) t=1
E(cid:2)f t(xt)(cid:3) ≥
λ
µ
· max x∈[0,1]n
T (cid:88) t=1 f t(x) − O(cid:0)max{λ/µ, 1}Ln3/2(log T )3/2(log log T )
√
T (cid:1).
The formal statement corresponding to the above informal theorem is Theorem 2. By this theorem, determining the performance guarantee is reduced to computing the concave parameters. Moreover,
T ) is nearly optimal that has been proved in the context of online convex the regret bound of (cid:101)O( optimization (for concave functions, i.e., (1, 1)-concave functions). The approach is convenient to derive bandit learning algorithms in the context of auction design as shown in the applications.
√ 1.2 Applications to Auction Design
In a general auction design setting, each player i has a valuation (or type) vi and a set of actions Ai for 1 ≤ i ≤ n. Given an action profile a = (a1, . . . , an) consisting of actions chosen by players, the auctioneer decides an allocation o(a) and a payment pi(o(a)) for each player i. Note that for a fixed auction o, the outcome o(a) of the game is completely determined by the action profile a.
Then, the utility of player i with valuation vi, following the quasi-linear utility model, is defined as ui(o(a); vi) = vi(o(a)) − pi(o(a)). The social welfare of an auction is defined as the total utility of all participants (the players and the auctioneer): SW(o(a); v) = (cid:80)n i=1 ui(o(a); vi) + (cid:80)n i=1 pi(a). The total revenue of the auction is REV(o(a), v) = (cid:80)n i=1 pi(o(a)). When o is clear in the context, we simply write vi(a), ui(a; vi), pi(a), SW(a; v), REV(a, v) instead of vi(o(a)), ui(o(a); vi), pi(o(a)), SW(o(a); v), REV(o(a), v), respectively. In the paper, we con-sider two standard objectives: welfare maximization and revenue maximization. Note that in revenue maximization, we call players as bidders. 2
1.2.1 Fictitious Play in Smooth Auctions
We consider adaptive dynamics in auctions. In the setting, there is an underlying auction o and there are n players, each player i has a set of actions Ai and a valuation function vi taking values in
[0, 1] (by normalization). In every time step 1 ≤ t ≤ T , each player i selects a strategy which is a distribution in ∆(Ai) according to some given adaptive dynamic. After all players have committed their strategies, which result in a strategy profile σt ∈ ∆(A), the auction induces a social welfare (cid:2)SW(o(a); v)(cid:3). In this setting, we study the total welfare achieved by the
SW(o, σt) := Ea∼σt given adaptive dynamic comparing to the optimal welfare. This problem can be cast in the online optimization framework in which at time step t, the player strategy profile corresponds to the decision of the algorithm and subsequently, the gain of the algorithm is the social welfare induced by the auction w.r.t the strategy profile.
Smooth auctions is an important class of auctions in welfare maximization. The smoothness notion has been introduced [32, 28] in order to characterize the efficiency of (Bayes-Nash) equilibria of auctions. It has been shown that several auctions in widely studied settings are smooth; and many proof techniques analyzing equilibrium efficiency can be reduced to the smooth argument.
Definition 2 ([32, 28]) For parameters λ, µ ≥ 0, an auction is (λ, µ)-smooth if for every valuation profile v = (v1, . . . , vn), there exist action distributions D1(v), . . . , Dn(v) over A1, . . . , An such (cid:2)ui(ai, a−i; vi)(cid:3) ≥ λ · SW(a; v) − µ · SW(a; v) that, for every action profile a, (cid:80)n where a−i is the action profile similar to a without player i. ai∼Di(v) i=1
E
It has been proved that if an auction is (λ, µ)-smooth then every Bayes-Nash equilibrium of the auction has expected welfare at least λ/µ fraction of the optimal auction [28, 32]. Moreover, the smoothness framework does extend to individually-vanishing-regret dynamics. A sequence of actions profiles a1, a2, . . . , is an individually-vanishing-regret sequence if for every player i and action a(cid:48) i, limT →∞
−i; vi) − ui(at; vi)(cid:3) ≤ 0. (cid:2)ui(a(cid:48) i, at (cid:80)T t=1 1
T
However, several interesting dynamics are not guaranteed to have the individually-vanishing-regret property. In a recent survey, Roughgarden et al. [30] have raised a question whether adaptive dynamics without the individually-vanishing-regret condition can achieve approximate optimal welfare. Among others, fictitious play [5] is an interesting, widely-studied dynamic which attracts a significant attention in the community.
In the paper, we consider a version of fictitious play in smooth auctions, namely Perturbed Discrete
Time Fictitious Play (PDTFP). In general, it is not known whether this dynamic has individually-vanishing-regret. Despite that fact, using our framework, we prove that given an offline (λ, µ)-smooth auction, PDTFP dynamic achieves a λ/(1 + µ) fraction of the optimal welfare.
Informal Theorem 2 If the underlying auction o is a (λ, µ)-smooth then the PDTFP dynamic achieves
-regret where R(T ) = O(cid:0) √ (cid:16) λ (cid:1). (cid:17) 1+µ , R(T )
T 1+µ 1.2.2 Revenue maximization in Multi-Dimensional Environments i = (rt i1, . . . , rt i where bt
We consider online simultaneous second-price auctions with reserve prices in multi-dimensional environments. In the setting, there are n bidders and m items to be sold to these bidders. At every time step t = 1, 2, . . . , T , the auctioneer selects reserve prices rt im) for each bidder i where rt ij is the reserve price of item j for bidder i. Each bidder i for 1 ≤ i ≤ n has a (private) i : 2[m] → R+ over subsets of items. After the reserve prices have been chosen, every valuation vt bidder i picks a bid vector bt ij is the bid of bidder i on item j for 1 ≤ j ≤ m. Then the auction for each item 1 ≤ j ≤ m works as follows: (1) remove all bidders i with bt ij; (2) run the second price auction on the remaining bidders to determine the winner of item j — the bidder with highest non-removed bid on item j; and (3) charge the winner of item j the price which is the maximum of rt ij and the second highest bid among non-removed bids bt ij. The objective of the auctioneer is to achieve the total revenue approximately close to that achieved by the best fixed reserve-price auction. Note that in the bandit setting, the auction is given as a blackbox and at every time step, the auctioneer observes only the total revenue (total price) without knowing neither the bids of bidders nor the winner/price of each item. The setting enhances, among others, the privacy of bidders. ij < rt 3
The second-price auctions with reserve prices in single-parameter environments have been considered by Roughgarden and Wang [29] in full-information online learning. Using the Follow-the-Perturbed-Leader strategy, they gave a polynomial-time online algorithm that achieves half the revenue of the best fixed reserve-price auction minus a term O(
T log T ))-regret in our terminology). The problem we consider cannot be reduced to applying their algorithm on m separated items since (1) bids on different items might be highly correlated (due to bidders’ valuations); and (2) in the bandit setting for multiple items, the auctioneer know only the total revenue (not the revenue from each item). Using our framework, we prove the following result.
T log T ) (so their algorithm is (1/2, O(
√
√
Informal Theorem 3 There exist a bandit algorithm that achieves (cid:0)1/2, O(m for revenue maximization in multi-parameter environments.
√ nmT log T )(cid:1)-regret 1.2.3 Bandit No-Envy Learning in Auctions
The concept of no-envy learning in auctions has been introduced by Daskalakis and Syrgkanis [10] in order to maintain approximate welfare optimality while guaranteeing computational tractability.
The concept is inspired by the notion of Walrasian equilibrium. Intuitively, an allocation of items to buyers together with a price on each item forms a Walrasian equilibrium if no buyer envies other allocation given the current prices. In the paper, we consider no-envy bandit learning algorithms for the following online item selection problem.
In the problem, there are m items and a player with monotone valuation v : 2[m] → R+. At every time step 1 ≤ t ≤ T , the player chooses a subset of items St ⊂ [m] and the adversary picks adaptively (probably depending on the history up to time t−1 but not on the current set St) a threshold vector pt.
The player observed the total price (cid:80) j. A learning algorithm for the online item selection problem is a r-approximate no-envy [10] if for any adaptively chosen sequence of threshold vectors pt for 1 ≤ t ≤ T , the sets St for 1 ≤ t ≤ T chosen by the algorithm satisfy E(cid:2)(cid:80)T (cid:1) − R(T ) where the regret R(T ) = o(T ). j and gets the reward of v(St) − (cid:80) (cid:1)(cid:3) ≥ maxS⊆[m] (cid:0)r · v(S) − (cid:80) (cid:0)v(St) − (cid:80) j∈St pt j j∈St pt j∈St pt j∈S pt j (cid:80)T t=1 t=1
Daskalakis and Syrgkanis [10] considered the problem in the full-information setting (i.e., at every time step t, the player observes the whole vector pt) where the valuation v is a coverage function1
T ). The algorithm is and gave an (1 − 1/e)-approximate no-envy algorithm with regret bound O( designed via the convex rounding scheme [12], a technique which has been used in approximation algorithms and in truthful mechanism design. In this paper, we consider submodular valuations, a more general and widely-studied class of valuations. A valuation v : 2[m] → R+ is submodular if for any sets S ⊂ T ⊂ [m], and for every item j, it holds that v(S ∪ j) − v(S) ≥ v(T ∪ j) − v(T ).
Using our framework, we derive the following result.
√
Informal Theorem 4 There exist an (cid:0)1/2, O(m3/2 rithm for the bandit item selection problem where the player valuation is submodular.
T log(mT ))(cid:1)-regret no-envy learning algo-√ 1.3