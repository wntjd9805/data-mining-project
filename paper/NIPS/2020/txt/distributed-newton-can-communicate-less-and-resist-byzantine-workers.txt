Abstract
We develop a distributed second order optimization algorithm that is communication-efﬁcient as well as robust against Byzantine failures of the worker machines. We propose COMRADE (COMunication-efﬁcient and Robust Approxi-mate Distributed nEwton), an iterative second order algorithm, where the worker machines communicate only once per iteration with the center machine. This is in sharp contrast with the state-of-the-art distributed second order algorithms like
GIANT [31] and DINGO[6], where the worker machines send (functions of) local gradient and Hessian sequentially; thus ending up communicating twice with the center machine per iteration. Moreover, we show that the worker machines can further compress the local information before sending it to the center. In addition, we employ a simple norm based thresholding rule to ﬁlter-out the Byzantine worker machines. We establish the linear-quadratic rate of convergence of COMRADE and establish that the communication savings and Byzantine resilience result in only a small statistical error rate for arbitrary convex loss functions. To the best of our knowledge, this is the ﬁrst work that addresses the issue of Byzantine resilience in second order distributed optimization. Furthermore, we validate our theoreti-cal results with extensive experiments on synthetic and benchmark LIBSVM [4] data-sets and demonstrate convergence guarantees. 1

Introduction
In modern data-intensive applications like image recognition, conversational AI and recommendation systems, the size of training datasets has grown in such proportions that distributed computing have become an integral part of machine learning. To this end, a fairly common distributed learning framework, namely data parallelism, distributes the (huge) data-sets over multiple worker machines to exploit the power of parallel computing. In many applications, such as Federated Learning
[17], data is stored in users’ personal devices and judicious exploitation of the on-device machine intelligence can speed up computation. Usually, in a distributed learning framework, computation (such as processing, training) happens in the worker machines and the local results are communicated to a center machine (ex., a parameter server). The center machine updates the model parameters by properly aggregating the local results.
Such distributed frameworks face the following two fundamental challenges: First, the parallelism gains are often bottle-necked by the heavy communication overheads between worker and the center machines. This issue is further exacerbated where large clusters of worker machines are used for 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
modern deep learning applications using models with millions of parameters (NLP models, such as
BERT [9], may have well over 100 million parameters). Furthermore, in Federated Learning, this uplink cost is tied to the user’s upload bandwidth. Second, the worker machines might be susceptible to errors owing to data crashes, software or hardware bugs, stalled computation or even malicious and co-ordinated attacks. This inherent unpredictable (and potentially adversarial) nature of worker machines is typically modeled as Byzantine failures. As shown in [18], Byzantine behavior a single worker machine can be fatal to the learning algorithm.
Both these challenges, communication efﬁciency and Byzantine-robustness, have been addressed in a signiﬁcant number of recent works, albeit mostly separately. For communication efﬁciency, several recent works [29, 27, 2, 13, 1, 32, 16] use quantization or sparsiﬁcation schemes to compress the message sent by the worker machines to the center machine. An alternative, and perhaps more natural way to reduce the communication cost (via reducing the number of iterations) is to use second order optimization algorithms; which are known to converge much faster than their ﬁrst order counterparts.
Indeed, a handful of algorithms has been developed using this philosophy, such as DANE [24],
DISCO [35], GIANT [31] , DINGO [6], Newton-MR [23], INEXACT DANE and AIDE [22]. On the other hand, the problem of developing Byzantine-robust distributed algorithms has also been considered recently (see [26, 12, 5, 33, 34, 14, 3] ). However, all of these papers analyze different variations of the gradient descent, the standard ﬁrst order optimization algorithm.
In this work, we propose COMRADE, a distributed approximate Newton-type algorithm that com-municates less and is resilient to Byzantine workers. Speciﬁcally, we consider a distributed setup with m worker machines and one center machine. The goal is to minimize a regularized convex loss f : Rd → R, which is additive over the available data points. Furthermore, we assume that α fraction of the worker machines are Byzantine, where α ∈ [0, 1/2). We assume that Byzantine workers can send any arbitrary values to the center machine. In addition, they may completely know the learning algorithm and are allowed to collude with each other. To the best of our knowledge, this is the ﬁrst paper that addresses the problem of Byzantine resilience in second order optimization.
In our proposed algorithm, the worker machines communicate only once per iteration with the center machine. This is in sharp contrast with the state-of-the-art distributed second order algorithms (like
GIANT [31], DINGO [6], Determinantal Averaging [8]), which sequentially estimates functions of local gradients and Hessians and communicate them with the center machine. In this way, they end up communicating twice per iteration with the center machine. We show that this sequential estimation is redundant. Instead, in COMRADE, the worker machines only send a d dimensional vector, the product of the inverse of local Hessian and the local gradient. Via sketching arguments, we show that the empirical mean of the product of local Hessian inverse and local gradient is close to the global Hessian inverse and gradient product, and thus just sending the above-mentioned product is sufﬁcient to ensure convergence. Hence, in this way, we save O(d) bits of communication per iteration. Furthermore, in Section 5, we argue that, in order to cut down further communication, the worker machines can even compress the local Hessian inverse and gradient product. Speciﬁcally, we use a (generic) ρ-approximate compressor ([16]) for this, that encompasses sign-based compressors like QSGD [1] and topk sparsiﬁcation [25].
For Byzantine resilience, COMRADE employs a simple thresholding policy on the norms of the local
Hessian inverse and local gradient product. Note that norm-based thresholding is computationally much simpler in comparison to existing co-ordinate wise median or trimmed mean ([33]) algorithms.
Since the norm of the Hessian-inverse and gradient product determines the amount of movement for
Newton-type algorithms, this norm corresponds to a natural metric for identifying and ﬁltering out
Byzantine workers.
Our Contributions: We propose a communication efﬁcient Newton-type algorithm that is robust to Byzantine worker machines. Our proposed algorithm, COMRADE takes as input the local Hessian inverse and gradient product (or a compressed version of it) from the worker machines, and performs a simple thresholding operation on the norm of the said vector to discard β > α fraction of workers having largest norm values. We prove the linear-quadratic rate of convergence of our proposed algorithm for strongly convex loss functions. In particular, suppose there are m worker machines, each containing s data points; and let ∆t = wt − w∗, where wt is the t-th iterate of COMRADE, and w∗ is the optimal model we want to estimate. In Theorem 2, we show that (cid:114) 1 s (cid:107)∆t+1(cid:107) ≤ max{Ψ(1) t (cid:107)∆t(cid:107)2} + (Ψ(3) t (cid:107)∆t(cid:107), Ψ(2) t + α)
, 2
t }3 where {Ψ(i) i=1 are quantities dependent on several problem parameters. Notice that the above implies a quadratic rate of convergence when (cid:107)∆t(cid:107) ≥ Ψ(1)
. Subsequently, when (cid:107)∆t(cid:107) becomes sufﬁciently small, the above condition is violated and the convergence slows down to a linear rate. The error-ﬂoor, which is O(1/ s) comes from the Byzantine resilience subroutine in conjunction with the simultaneous estimation of Hessian and gradient. Furthermore, in Section 5, we consider worker machines compressing the local Hessian inverse and gradient product via a ρ-approximate compressor [16], and show that the (order-wise) rate of convergence remain unchanged, and the compression factor, ρ affects the constants only. t /Ψ(2)
√ t
We experimentally validate our proposed algorithm, COMRADE, with several benchmark data-sets.
We consider several types of Byzantine attacks and observe that COMRADE is robust against
Byzantine worker machines, yielding better classiﬁcation accuracy compared to the existing state-of-the-art second order algorithms.
A major technical challenge of this paper is to approximate local gradient and Hessian simultaneously in the presence of Byzantine workers. We use sketching, similar to [31], along with the norm based
Byzantine resilience technique. Using incoherence (deﬁned shortly) of the local Hessian along with concentration results originating from uniform sampling, we obtain the simultaneous gradient and
Hessian approximation. Furthermore, ensuring at least one non-Byzantine machine gets trimmed at every iteration of COMRADE, we control the inﬂuence of Byzantine workers.