Abstract
We consider the problem of lossy image compression with deep latent variable models. State-of-the-art methods [Ballé et al., 2018, Minnen et al., 2018, Lee et al., 2019] build on hierarchical variational autoencoders (VAEs) and learn inference networks to predict a compressible latent representation of each data point. Drawing on the variational inference perspective on compression [Alemi et al., 2018], we identify three approximation gaps which limit performance in the conventional approach: an amortization gap, a discretization gap, and a marginalization gap.
We propose remedies for each of these three limitations based on ideas related to iterative inference, stochastic annealing for discrete optimization, and bits-back coding, resulting in the ﬁrst application of bits-back coding to lossy compression.
In our experiments, which include extensive baseline comparisons and ablation studies, we achieve new state-of-the-art performance on lossy image compression using an established VAE architecture, by changing only the inference method. 1

Introduction
Deep learning methods are reshaping the ﬁeld of data compression, and recently started to outperform state-of-the-art classical codecs on image compression [Minnen et al., 2018]. Besides useful on its own, image compression is a stepping stone towards better video codecs [Lombardo et al., 2019,
Habibian et al., 2019, Yang et al., 2020a], which can reduce a sizable amount of global internet trafﬁc.
State-of-the-art neural methods for lossy image compression [Ballé et al., 2018, Minnen et al., 2018, Lee et al., 2019] learn a mapping between images and latent variables with a variational autoencoder (VAE). An inference network maps a given image to a compressible latent representation, which a generative network can then map back to a reconstructed image. In fact, compression can be more broadly seen as a form of inference: to compress—or “encode”—data, one has to perform inference over a well-speciﬁed decompression—or “decoding”—algorithm.
In classical compression codecs, the decoder has to follow a well-speciﬁed procedure to ensure interoperability between different implementations of the same codec. By contrast, the encoding process is typically not uniquely deﬁned: different encoder implementations of the same codec often compress the same input data to different bitstrings. For example, pngcrush [Randers-Pehrson, 1997] typically produces smaller PNG ﬁles than ImageMagick. Yet, both programs are standard-compliant
PNG encoder implementations as both produce a compressed ﬁle that decodes to the same image. In other words, both encoder implementations perform correct inference over the standardized decoder speciﬁcation, but use different inference algorithms with different performance characteristics.
The insight that better inference leads to better compression performance even within the same codec motivates us to reconsider how inference is typically done in neural data compression with VAEs. In this paper, we show that the conventional amortized inference [Kingma and Welling, 2013, Rezende et al., 2014] in VAEs leaves substantial room for improvement when used for data compression.
We propose an improved inference method for data compression tasks with three main innovations: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Graphical model and control ﬂow charts. a) generative model with hyperlatents z, latents y, and image x [Minnen et al., 2018]; b) conventional method for compression (dashed blue, see Eq. 1) and decompression (solid black); c) common training objective due to [Ballé et al., 2017] (see
Eq. 3); d) proposed hybrid amortized (dashed blue) / iterative (dotted red) inference (Section 3.1); e)-f) inference in the proposed lossy bitsback method (Section 3.3); the encoder ﬁrst executes e) and then keeps ˆy ﬁxed while executing f); the decoder reconstructs ˆy and then executes f) to get bits back. 1. Improved amortization: The amortized inference strategy in VAEs speeds up training, but is restrictive at compression time. We draw a connection between a recently proposed iterative procedure for compression [Campos et al., 2019] to the broader literature of VI that closes the amortization gap, which provides the basis of the following two novel inference methods. 2. Improved discretization: Compression requires discretizing the latent representation from
VAEs, because only discrete values can be entropy coded. As inference over discrete variables is difﬁcult, existing methods typically relax the discretization constraint in some way during inference and then discretize afterwards. We instead propose a novel method based on a stochastic annealing scheme that performs inference directly over discrete points. 3. Improved entropy coding: In lossless compression with latent-variable models, bits-back coding [Wallace, 1990, Hinton and Van Camp, 1993] allows approximately coding the latents with the marginal prior. It is so far believed that bits-back coding is incompatible with lossy compression [Habibian et al., 2019] because it requires inference on the decoder side, which does not have access to the exact (undistorted) input data. We propose a remedy to this limitation, resulting in the ﬁrst application of bits-back coding to lossy compression.
We evaluate the above three innovations on an otherwise unchanged architecture of an established model and compare against a wide range of baselines and ablations. Our proposals signiﬁcantly improve compression performance and results in a new state of the art in lossy image compression.
The rest of the paper is structured as follows: Section 2 summarizes lossy compression with VAEs, for which Section 3 proposes the above three improvements. Section 4 reports experimental results.
We conclude in Section 5. We review related work in each relevant subsection. 2