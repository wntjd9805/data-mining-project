Abstract
A key requirement for the success of supervised deep learning is a large labeled dataset - a condition that is difﬁcult to meet in medical image analysis. Self-supervised learning (SSL) can help in this regard by providing a strategy to pre-train a neural network with unlabeled data, followed by ﬁne-tuning for a downstream task with limited annotations. Contrastive learning, a particular variant of SSL, is a powerful technique for learning image-level representations. In this work, we propose strategies for extending the contrastive learning framework for segmen-tation of volumetric medical images in the semi-supervised setting with limited annotations, by leveraging domain-speciﬁc and problem-speciﬁc cues. Speciﬁcally, we propose (1) novel contrasting strategies that leverage structural similarity across volumetric medical images (domain-speciﬁc cue) and (2) a local version of the contrastive loss to learn distinctive representations of local regions that are use-ful for per-pixel segmentation (problem-speciﬁc cue). We carry out an extensive evaluation on three Magnetic Resonance Imaging (MRI) datasets. In the limited annotation setting, the proposed method yields substantial improvements compared to other self-supervision and semi-supervised learning techniques. When combined with a simple data augmentation technique, the proposed method reaches within 8% of benchmark performance using only two labeled MRI volumes for training, cor-responding to only 4% (for ACDC) of the training data used to train the benchmark.
The code is made public at https://github.com/krishnabits001/domain_speciﬁc_cl. 1

Introduction
Supervised deep learning provides state-of-the-art medical image segmentation [47, 39, 31, 32], when large labeled datasets are available. However, assembling such large annotated datasets is challenging, thus methods that can alleviate this requirement are highly desirable. Self-supervised learning (SSL) is a promising direction to this end: it provides a pre-training strategy that relies only on unlabeled data to obtain a suitable initialization for training downstream tasks with limited annotations.
In recent years, SSL methods [16, 44, 42, 21] have been highly successful for downstream analysis of not only natural images [48, 35, 20], but also medical images [71, 5, 60, 30, 52, 11].
In this work, we focus on contrastive learning [12, 40, 4, 27, 57, 43], a successful variant of SSL.
The intuition of this approach is that different transformations of an image should have similar representations and that these representations should be dissimilar from those of a different image.
In practice, a suitable contrastive loss [24, 12] is formulated to express this intuition and a neural network (NN) is trained with unlabeled data to minimize this loss. The resulting NN extracts image representations that are useful for downstream tasks, such as classiﬁcation or object detection, and 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
constitutes a good initialization that can be ﬁne-tuned into an accurate model, even with limited labeled examples.
Despite its success, we believe that two important aspects have been largely unexplored in the existing contrastive learning literature that can improve the current state-of-the-art with respect to medical image segmentation. Firstly, most works focus on extracting global representations and do not explicitly learn distinctive local representations, which we believe will be useful for per-pixel prediction tasks such as image segmentation. Secondly, the contrasting strategy is often devised based on transformations used in data augmentation, and do not necessarily utilize a notion of similarity that may be present across different images in a dataset. We believe that a domain-speciﬁc contrasting strategy that leverages such inherent structure in the data may lead to additional gains by providing the network with more complex similarity cues than what augmentation can offer.
In this work, we aim to ﬁll these gaps in the contrastive learning literature in the context of seg-mentation of volumetric medical images and make the following contributions. 1) We propose new domain-speciﬁc contrasting strategies for volumetric medical images, such as Magnetic Resonance
Imaging (MRI) and Computed Tomography (CT). 2) We propose a local version of contrastive loss, which encourages representations of local regions in an image to be similar under different transformations, and dissimilar to those of other local regions in the same image. 3) We evaluate the proposed strategies on three MRI datasets and ﬁnd that combining the proposed global and local strategies consistently leads to substantial performance improvements compared to no pre-training, pre-training with pretext tasks, pre-training with global contrastive loss, as demonstrated to yield state-of-the-art accuracy in [12], as well as semi-supervised learning methods. 4) We investigate if pre-training with the proposed strategies has complementary beneﬁts to other methods for learning with limited annotations, such as data augmentation and semi-supervised training. 2