Abstract
Most state-of-the-art deep geometric learning single-view reconstruction ap-proaches rely on encoder-decoder architectures that output either shape parametriza-tions [7, 8, 23] or implicit representations [14, 26, 4]. However, these representa-tions rarely preserve the Euclidean structure of the 3D space objects exist in. In this paper, we show that building a geometry preserving 3-dimensional latent space helps the network concurrently learn global shape regularities and local reasoning in the object coordinate space and, as a result, boosts performance.
We demonstrate both on ShapeNet synthetic images, which are often used for benchmarking purposes, and on real-world images that our approach outperforms state-of-the-art ones. Furthermore, the single-view pipeline naturally extends to multi-view reconstruction, which we also show. 1

Introduction
Most state-of-the-art deep geometric learning Single-View Reconstruction approaches (SVR) rely on encoder-decoder architectures that output either explicit shape parametrizations [7, 8, 23] or implicit representations [14, 26, 4]. However, the representations they learn rarely preserve the Euclidean structure of the 3D space objects exist in, and rather rely on a global vector embedding of the input image at a semantic level. In this paper, we show that building a geometry preserving 3-dimensional representation helps the network concurrently learn global shape regularities and local reasoning in the object coordinate space and, as a result, boosts performance. This corroborates the observation that choosing the right coordinate frame for the output of a deep network matters a great deal [21].
In our work, we use camera projection matrices to explicitly link camera- and object-centric coordinate frames. This allows us to reason about geometry and learn object priors in a common 3D coordinate system. More speciﬁcally, we use regressed camera pose information to back-project 2D feature maps to 3D feature grids at several scales. This is achieved within our novel architecture that comprises a 2D image encoder and a 3D shape decoder. They feature symmetrical downsampling and upsampling parts and communicate through multi-scale skip connections, as in the U-Net architecture [16].
However, unlike in other approaches, the bottleneck is made of 3D feature grids and we use back-projection layers [12, 11, 17] to lift 2D feature maps to 3D grids. As a result, feature localization from the input view is preserved. In other words, our feature embedding has a Euclidean structure and is aligned with object coordinate frame. Fig. 1 depicts this process. In reference to its characteristics, we dub our architecture UCLID-Net.
Earlier attempts at passing 2D features to a shape decoder via local feature extraction [24, 26] enabled spatial information to ﬂow to the decoder in a non semantic manner, often with limited impact on the ﬁnal result. In these approaches, the same local feature is attributed to all points lying along a camera ray. By contrast, UCLID-Net uses 3D convolutions to volumetrically process local features before passing them to the local shape decoders. This allows them to make different contributions at 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: UCLID-Net. Given input image I, a CNN encoder estimates 2D feature maps Fs for scales s from 1 to S while pre-trained CNNs regress a depth map D and a camera pose P . P is used to backproject the feature maps Fs to object aligned 3D feature grids GFs for 1 ≤ s ≤ S without using depth information. In parallel, S corresponding voxelized depth grids GD s are built from D and P without using feature information. A 3D CNN then aggregates feature and depth grids from the lowest to the highest resolution into outputs HS, . . . , H0 of increasing resolutions. From H0, fully connected layers regress a coarse voxel shape, which is then reﬁned into a point cloud using local patch foldings. Supervision comes in the form of binary cross-entropy on the coarse output and Chamfer distance on the ﬁnal 3D point cloud. different places along camera rays. To further promote geometrical reasoning, it never computes a global vector encoding of the input image. Instead, it relies on localized feature grids, either 2D in the image plane or 3D in object space. Finally, the geometric nature of the 3D feature grids enables us to exploit estimated depth maps and further boost reconstruction performance.
We demonstrate both on ShapeNet synthetic images, which are often used for benchmarking purposes, and on real-world images that our approach outperforms state-of-the-art ones. Our contribution is therefore a demonstration that creating a Euclidean preserving latent space provides a clear beneﬁt for single-image reconstruction and a practical approach to taking advantage of it. Finally, the single-view pipeline naturally extends to multi-view reconstruction, which we also provide an example for. 2