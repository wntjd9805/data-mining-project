Abstract
Reinforcement learning algorithms are highly sensitive to the choice of hyperpa-rameters, typically requiring signiﬁcant manual effort to identify hyperparameters that perform well on a new domain. In this paper, we take a step towards addressing this issue by using metagradients to automatically adapt hyperparameters online by meta-gradient descent (Xu et al., 2018). We apply our algorithm, Self-Tuning Actor-Critic (STAC), to self-tune all the differentiable hyperparameters of an actor-critic loss function, to discover auxiliary tasks, and to improve off-policy learning using a novel leaky V-trace operator. STAC is simple to use, sample efﬁcient and does not require a signiﬁcant increase in compute. Ablative studies show that the overall performance of STAC improved as we adapt more hyperparameters. When applied to the Arcade Learning Environment (Bellemare et al. 2012), STAC improved the median human normalized score in 200M steps from 243% to 364%. When applied to the DM Control suite (Tassa et al., 2018), STAC improved the mean score in 30M steps from 217 to 389 when learning with features, from 108 to 202 when learning from pixels, and from 195 to 295 in the Real-World Reinforcement
Learning Challenge (Dulac-Arnold et al., 2020). 1

Introduction
Deep Reinforcement Learning (RL) algorithms often have many modules and loss functions with many hyperparameters. When applied to a new domain, these hyperparameters are searched via cross-validation, random search (Bergstra & Bengio, 2012), or population-based training (Jaderberg et al., 2017), which requires extensive computing resources. Meta-learning approaches in RL (e.g.,
MAML, Finn et al. (2017)) focus on learning good initialization via multi-task learning and transfer.
However, many of the hyperparameters must be adapted during the agent’s lifetime to achieve good performance (learning rate scheduling, exploration annealing, etc.). This motivates a signiﬁcant body of work on speciﬁc solutions to tune speciﬁc hyperparameters, within a single agent lifetime (Schaul et al., 2019; Mann et al., 2016; White & White, 2016; Rowland et al., 2019; Sutton, 1992).
Metagradients, on the other hand, provide a general and compute-efﬁcient approach for self-tuning in a single lifetime. The general concept is to represent the training loss as a function of both the agent parameters and the hyperparameters. The agent optimizes the parameters to minimize this loss function, w.r.t the current hyperparameters. The hyperparameters are then self-tuned via backpropagation to minimize a ﬁxed loss function. This approach has been used to learn the discount factor or the λ coefﬁcient (Xu et al., 2018), to discover intrinsic rewards (Zheng et al., 2018) and auxiliary tasks (Veeriah et al., 2019). Finally, we note that there also exist derivative-free approaches for self-tuning hyper parameters (Paul et al., 2019; Tang & Choromanski, 2020).
This paper makes the following contributions. First, we introduce two novel ideas that extend
IMPALA (Espeholt et al., 2018) with additional components. (1) The ﬁrst agent, referred to as a
Self-Tuning Actor-Critic (STAC), self-tunes all the differentiable hyperparameters in the IMPALA loss function. In addition, STAC introduces a leaky V-trace operator that mixes importance sampling 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(IS) weights with truncated IS weights. The mixing coefﬁcient in leaky V-trace is differentiable (unlike the original V-trace) but similarly balances the variance-contraction trade-off in off-policy learning. (2) The second agent, STACX (STAC with auXiliary tasks), adds auxiliary parametric actor-critic loss functions to the loss function and self-tunes their metaparameters. STACX self-tunes the discount factors of these auxiliary losses to different values than those of the main task, helping it to reason about multiple horizons.
Second, we demonstrate empirically that self-tuning consistently improves performance. When applied to the Arcade Learning Environment (Bellemare et al., 2013, ALE), STAC improved the median human normalized score in 200M steps from 243% to 364%. When applied to the DM
Control suite (Tassa et al., 2018), STAC improved the mean score in 30M steps from 217 to 389 when learning with features, from 108 to 202 when learning from pixels, and from 195 to 295 in the
Real-World Reinforcement Learning Challenge (Dulac-Arnold et al., 2020).
We conduct extensive ablation studies, showing that the performance of STACX consistently improves as it self-tunes more hyperparameters; and that STACX improves the baseline when self-tuning differ-ent subsets of the metaparameters. STACX performs considerably better than previous metagradient algorithms (Xu et al., 2018; Veeriah et al., 2019) and across a broader range of environments.
Finally, we investigate the properties of STACX via a set of experiments. (1) We show that STACX is more robust to its hyperparameters than the IMPALA baseline. (2) We visualize the self-tuned metaparameters through training and identify trends. (3) We demonstrate a tenfold scale up in the number of self-tuned hyperparameters – 21 compared to two in (Xu et al., 2018). This is the most signiﬁcant number of hyperparameters tuned by meta-learning at scale and does not require a signiﬁcant increase in compute (see Table 4 in the supplementary and the discussion that follows it). 2