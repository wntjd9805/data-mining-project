Abstract
Recent works on One-Shot Neural Architecture Search (NAS) mostly adopt a bilevel optimization scheme to alternatively optimize the supernet weights and architecture parameters after relaxing the discrete search space into a differentiable space. However, the non-negligible incongruence in their relaxation methods is hard to guarantee the differentiable optimization in the continuous space is equivalent to the optimization in the discrete space. Differently, this paper utilizes a variational graph autoencoder to injectively transform the discrete architecture space into an equivalently continuous latent space, to resolve the incongruence. A probabilistic exploration enhancement method is accordingly devised to encourage intelligent exploration during the architecture search in the latent space, to avoid local optimal in architecture search. As the catastrophic forgetting in differentiable One-Shot
NAS deteriorates supernet predictive ability and makes the bilevel optimization inefﬁcient, this paper further proposes an architecture complementation method to relieve this deﬁciency. We analyze the proposed method’s effectiveness, and a series of experiments have been conducted to compare the proposed method with state-of-the-art One-Shot NAS methods. 1

Introduction
While Neural Architecture Search (NAS) [9, 18, 29] has achieved impressive results in many automat-ing neural network designing tasks, it has also imposed huge demand of computation power for most machine learning practitioners. To mitigate this problem, many recent studies have been devoted to reducing the search cost through the weight-sharing paradigm (which is also called One-Shot NAS)
[4]. These methods deﬁne a supernet to subsume all possible architectures in the search space, and evaluate architectures through inheriting weights from the supernet. Early One-Shot NAS approaches
ﬁrst adopt a controller to sample architectures for the supernet training, and then use heuristic search methods to ﬁnd the promising architecture over a discrete search space based on the trained supernet
[14, 19, 26]. Later researches [6, 11, 15, 22, 23, 32] further employ the continuous relaxation to make the architecture differentiable, so that gradient descent can be used to optimize the architecture with respect to validation accuracy, and this paradigm is also referred to as differentiable NAS [22].
One shortcoming for the discrete-continuous conversion in differentiable NAS is that there is no theoretical foundation showing that the optimization in the continuous latent space is equivalent to discrete space. The lack of injective constraints in the simple continuous relaxation hardly guarantees that performing optimization in the continuous latent space is equivalent to doing so in the discrete space. Several concurrent works [7, 36] further reveal that this incongruence, which is correlated
∗Corresponding Author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
with the Hessian norm of architecture parameters, constantly increases during the architecture search of differentiable NAS. In addition, current differentiable NAS methods only rely on the performance reward to update the architecture parameters. This method entails the rich-get-richer problem [1, 40], since architectures with better performance in the early stage would be trained more frequently, and the updated weights further make these architectures having a higher probability of being sampled, which easily leads to a local optimal.
Another limitation of differentiable NAS is the catastrophic forgetting problem arisen in the training process. Differentiable methods assume that the inner supernet weights learning in each step improves the validation performance of all architectures with inheriting the supernet weights. However, this assumption may not hold. In practice, each step of supernet training in One-Shot NAS usually deteriorates other architectures’ validation performance containing partially shared weights with currently learned architecture [5]. This forgetting problem is less studied in differentiable NAS.
Motivated by the aforementioned observations, this paper develops an Exploration Enhancing Neural
Architecture Search with Architecture Complementation (E2NAS) to address the limitations faced by existing differentiable NAS approaches. For the incongruence in the relaxation transformation of differentiable NAS, we utilize a variational graph autoencoder with an asynchronous message passing scheme to transform the discrete architectures into an equivalent continuous space injectively.
Because of the injectiveness, we could equivalently perform optimization in the continuous latent space with a solid theoretical foundation [33, 38]. For the rich-get-richer problem entailed by the reward-based gradient methods, we devised a probabilistic exploration enhancement method to encourage intelligent exploration during the architecture search in the latent space. As to the common catastrophic forgetting in differentiable NAS, an architecture complementation based continual learning method is further proposed for the supernet training, to force the supernet to keep the memory of previously visited architectures. We compared the proposed approach with different One-Shot NAS baselines on the NAS benchmark dataset NAS-Bench-201 [13], and extensive experimental results illustrate the effectiveness of our method, which outperforms all baselines on this dataset. 2