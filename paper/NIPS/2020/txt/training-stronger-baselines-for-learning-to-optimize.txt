Abstract
Learning to optimize (L2O) is gaining increased attention because classical op-timizers require laborious, problem-speciﬁc design and hyperparameter tuning.
However, there are signiﬁcant performance and practicality gaps between manually designed optimizers and existing L2O models. Speciﬁcally, learned optimizers are applicable to only a limited class of problems, often exhibit instability, and gener-alize poorly. As research efforts focus on increasingly sophisticated L2O models, we argue for an orthogonal, under-explored theme: improved training techniques for L2O models. We ﬁrst present a progressive, curriculum-based training scheme, which gradually increases the optimizer unroll length to mitigate the well-known
L2O dilemma of truncation bias (shorter unrolling) versus gradient explosion (longer unrolling). Secondly, we present an off-policy imitation learning based approach to guide the L2O learning, by learning from the behavior of analytical optimizers. We evaluate our improved training techniques with a variety of state-of-the-art L2O models and immediately boost their performance, without making any change to their model structures. We demonstrate that, using our improved training techniques, one of the earliest and simplest L2O models [1] can be trained to outperform even the latest and most complex L2O models on a number of tasks.
Our results demonstrate a greater potential of L2O yet to be unleashed, and prompt a reconsideration of recent L2O model progress. Our codes are publicly available at: https://github.com/VITA-Group/L2O-Training-Techniques. 1

Introduction
Learning to optimize (L2O) [1–10], a rising subﬁeld of meta learning, aims to replace manually designed analytical opti-mizers with learned optimizers, i.e., update rules as functions that can be ﬁt from data. An L2O method typically assumes a model to parameterize the target update rule. A trained
L2O model will act as an algorithm (optimizer) itself, that can be applied to training other machine learning models, called optimizees, sampled from a speciﬁc class of similar problem instances. The training of the L2O model is usually done in a meta-fashion, by enforcing it to decrease the loss values over sampled optimizees from the same class, via certain training techniques.
Figure 1: Learning to optimize.
Earlier L2O methods refer to black-box hyper-parameter tuning approaches [11–16] but these methods often scale up poorly [17] when the optimizer’s parameter amounts grow large. The recent mainstream works in this vein [1, 6–9] leverage a recurrent network as the L2O model, typically long short-term memory (LSTM). That LSTM is unrolled to mimic the behavior of an iterative optimizer and trained
∗Equal Contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to ﬁt the optimization trajectory. At each step, the LSTM takes as input some optimizee’s current measurement (such as zero- and ﬁrst-order information), and returns an update for the optimizee. A high-level overview of L2O workﬂow is presented in Figure 1.
Although learned optimizers outperform hand-designed optimizers on certain problems, they are still far from being mature or practically popular. Training LSTM-based L2O models is notoriously difﬁcult and unstable; meanwhile, the learned optimizers often suffer from poor generalization. Both problems arise from decisions made on the unroll length of the LSTM during L2O training [17–20].
For training modern models, an optimizer (either hand-crafted or learned) can take thousands of iterations, or more. Since naively unrolling the LSTM to this full length is impractical, most LSTM-based L2O methods [1, 6–9, 17, 21, 22] take advantage of truncating the unrolled optimization. As a result, the entire optimization trajectory is divided into consecutive shorter pieces, where each piece is optimized by applying a truncated LSTM. However, choosing the unrolling (or division) length faces a well-known dilemma [7]: on one hand, a short-truncated LSTM can result in premature termination of the iterative solution. The resulting “truncation bias" causes learned optimizers to exhibit instability and yield poor-quality solutions when applied to training optimizees. On the other hand, although a longer truncation is favored for optimizer performance, if unrolled too long, common training pitfalls for LSTM, such as gradient explosion, will result [19].
To tackle these challenges, a number of solutions [7–9, 17] have been proposed, mainly devoted to designing more sophisticated architectures of L2O models (LSTM variants), as well as enriching the input features for L2O models [7–9]. These approaches introduce complexity, despite continuing to fall short on performance, generalization, and stability issues. In contrast, this paper seeks to improve
L2O from an orthogonal perspective: can we train a given L2O model better?
Our Contributions We offer a toolkit of novel training techniques, and demonstrate that, solely by introducing our techniques into the training of existing, state-of-the-art L2O models, our methods substantially reduce training instability, and improve performance and generalization of trained models. More speciﬁcally:
• We propose a progressive training scheme to gradually unroll the learned optimizer, based on an exploration-exploitation view of L2O. We ﬁnd it to effectively mitigate the dilemma between truncation bias (shorter unrolling) v.s. gradient explosion (longer unrolling).
• We introduce off-policy imitation learning to further stabilize the L2O training, by learning from the behavior of hand-crafted optimizers. The L2O performance gain by imitation learning endorses the implicit injection of useful design knowledge of good optimizers.
• We report extensive experiments using a variety of L2O models, optimizees, and datasets.
Incorporating our improved training techniques immediately boosts all state-of-the-art L2O models, without any other change. We note especially that the earliest and simplest LSTM-based L2O baseline [1] can be trained to outperform latest and much more sophisticated
L2O models.
Our results offer an important reminder when tackling new domains, such as L2O, with machine learning techniques: besides developing more complicated models, training existing simple baselines better is equally important. Only by fully unleashing “simple" models’ potential, can we lay a solid and fair ground for evaluating the L2O progress. 2