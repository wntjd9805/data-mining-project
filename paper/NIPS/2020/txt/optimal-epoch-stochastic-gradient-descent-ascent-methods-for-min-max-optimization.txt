Abstract
Epoch gradient descent method (a.k.a. Epoch-GD) proposed by [16] was deemed a breakthrough for stochastic strongly convex minimization, which achieves the optimal convergence rate of O(1/T ) with T iterative updates for the objective gap. However, its extension to solving stochastic min-max problems with strong convexity and strong concavity still remains open, and it is still unclear whether a fast rate of O(1/T ) for the duality gap is achievable for stochastic min-max optimization under strong convexity and strong concavity. Although some re-cent studies have proposed stochastic algorithms with fast convergence rates for min-max problems, they require additional assumptions about the problem, e.g., smoothness, bi-linear structure, etc. In this paper, we bridge this gap by providing a sharp analysis of epoch-wise stochastic gradient descent ascent method (referred to as Epoch-GDA) for solving strongly convex strongly concave (SCSC) min-max problems, without imposing any additional assumption about smoothness or the function’s structure. To the best of our knowledge, our result is the ﬁrst one that shows Epoch-GDA can achieve the optimal rate of O(1/T ) for the duality gap of general SCSC min-max problems. We emphasize that such generalization of
Epoch-GD for strongly convex minimization problems to Epoch-GDA for SCSC min-max problems is non-trivial and requires novel technical analysis. Moreover, we notice that the key lemma can also be used for proving the convergence of
Epoch-GDA for weakly-convex strongly-concave min-max problems, leading to a nearly optimal complexity without resorting to smoothness or other structural conditions. 1

Introduction
In this paper, we consider stochastic algorithms for solving the following min-max saddle-point problem with a general objective function f without smoothness or any other special structure: min x∈X max y∈Y f (x, y), (1) where X ⊆ Rd and Y ⊆ Rn are closed convex sets and f : X × Y → R is continuous. It is of great interest to ﬁnd a saddle-point solution to the above problem, which is deﬁned as (x∗, y∗) such thatf (x∗, y) ≤ f (x∗, y∗) ≤ f (x, y∗), ∀x ∈ X, y ∈ Y. Problem (1) covers a number of applications 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
in machine learning, including distributionally robust optimization (DRO) [31, 30], learning with non-decomposable loss functions [27, 11, 43, 26], and generative adversarial networks [13, 3].
In this work, we focus on two classes of the min-max problems: (i) strongly-convex strongly-concave (SCSC) problem where f is strongly convex in terms of x for any y ∈ Y and is strongly concave in terms of y for any x ∈ X; (ii) weakly-convex strongly-concave (WCSC) problem, where there exists
ρ > 0 such that f (x, y) + ρ 2 (cid:107)x(cid:107)2 is strongly convex in terms of x for any y ∈ Y and is strongly concave in terms of y for any x ∈ X. Both classes have applications in machine learning [41, 36].
Although stochastic algorithms for convex-concave min-max problems have been studied exten-sively in the literature, their research is still far behind its counterpart for stochastic convex minimization problems. Below, we highlight some of these gaps to motivate the present work.
For the sake of presentation, we ﬁrst introduce some terminologies. The duality gap at (x, y) is deﬁned as Gap(x, y) := f (x, ˆy(x)) − f (ˆx(y), y), where ˆx(y) := arg minx(cid:48)∈X f (x(cid:48), y) and
ˆy(x) := arg maxy(cid:48)∈Y f (x, y(cid:48)). If we denote by P (x) := maxy(cid:48)∈Y f (x, y(cid:48)), then P (x) − P (x∗) is the primal objective gap, where x∗ = arg minx∈X P (x).
When f is convex in x and concave in y, many studies have designed and analyzed stochastic primal-dual algorithms for solving the min-max problems under different conditions of the problem (see references in next section). A standard result is provided by [32], which proves that primal-T ) for the duality gap without imposing any dual SGD suffers from a convergence rate of O(1/ additional assumptions about the objective function. This is analogous to that for stochastic convex minimization [32]. However, the research of stochastic algorithms for SCSC problems lacks behind that for strongly convex minimization problems. A well-known result for stochastic strongly convex minimization is given by [16], which presents the ﬁrst fast convergence rate O(1/T ) for stochastic strongly convex minimization by the Epoch-GD algorithm, which runs standard SGD in an epoch-wise manner by decreasing the step size geometrically. However, a fast rate of O(1/T ) for the duality gap of a stochastic algorithm is still unknown for general SCSC problems. We notice that there are extensive studies about stochastic algorithms with faster convergence rates than O(1/
T ) for solving convex-concave min-max problems [46, 38, 37, 10, 6, 5, 35, 21, 41, 18, 47]. However, these works usually require additional assumptions about the objective function (e.g., smoothness, bilinear structure) or only prove the convergence in weaker measures (e.g., the primal objective gap, the distance of a solution to the saddle point).
√
√
We aim to bridge this gap by presenting the ﬁrst optimal rate O(1/T ) of the duality gap for solving general SCSC problems. In particular, we propose an epoch-wise stochastic gradient descent ascent (Epoch-GDA) algorithm - a primal-dual variant of Epoch-GD that runs stochastic gradient descent update for the primal variable and stochastic gradient ascent update for the dual variable for solving (1).
Although the algorithmic generalization is straightforward, the proof of convergence in terms of the duality gap for Epoch-GDA is not straightforward at all. We note that the key difference in the analysis of Epoch-GDA is that to upper bound the duality gap of a solution (¯x, ¯y) we need to deal with the distance of an initial solution (x0, y0) to the reference solutions (ˆx(¯y), ˆy(¯x)), where
ˆx(¯y) = arg minx(cid:48)∈X f (x(cid:48), ¯y) and ˆy(¯x) = arg maxy(cid:48)∈Y f (¯x, y(cid:48)) depend on ¯y and ¯x, respectively. In contrast, in the analysis of the objective gap for Epoch-GD, one only needs to deal with the distance from an initial solution x0 to the optimal solution x∗, i.e., (cid:107)x0 − x∗(cid:107)2 2, which by strong convexity can easily connects to the objective gap P (x0) − P (x∗), leading to the telescoping sum on the objective gap. Towards addressing the challenge caused by dealing with the duality gap, we present a key lemma that connects the distance measure (cid:107)x0 − ˆx(¯y)(cid:107)2 2 to the duality gap of (x0, y0) and (¯x, ¯y). In addition, since we use the same technique as Epoch-GD for handling the variance of stochastic gradient by projecting onto a bounded ball with shrinking radius, we have to carefully prove that such restriction does not affect the duality gap for the original problem, which also needs to deal with bounding (cid:107)x0 − ˆx(¯y)(cid:107)2
Moreover, we notice that the aforementioned key lemma and the telescoping technique based on the duality gap can also be used for proving the convergence of Epoch-GDA for ﬁnding an approximate stationary solution of general WCSC problems. The algorithmic framework is similar to that proposed by [36], i.e., by solving SCSC problems successively, but with a subtle difference in handling the dual variable. In particular, we do not need additional condition on the structure of the objective function and extra care for dealing with the dual variable for restart as done in [36]. This key difference is caused by our sharper analysis, i.e., we use the telescoping sum based on the duality gap instead of the primal objective gap as in [36]. As a result, our algorithm and analysis lead to a 2 and (cid:107)y0 − ˆy(¯x)(cid:107)2 2. 2 + (cid:107)y0 − ˆy(¯x)(cid:107)2 2
Table 1: Summary of complexity results of this work and previous works for ﬁnding an (cid:15)-duality-gap solution for SCSC or an (cid:15)-stationary solution for WCSC min-max problems. We focus on comparison of existing results without assuming smoothness of the objective function. Restriction means whether an additional condition about the objective function’s structure is imposed.
Setting Works
Restriction Convergence
SCSC
[32]
[41]
This paper
No
Yes
No
[36]
WCSC [36]
No
Yes
This paper No
Duality Gap
Primal Gap
Duality Gap
Nearly Stationary
Nearly Stationary
Nearly Stationary
Complexity
O (cid:0)1/(cid:15)2(cid:1)
O (1/(cid:15) + n log(1/(cid:15)))
O (1/(cid:15)) (cid:101)O (cid:0)1/(cid:15)6(cid:1) (cid:101)O (cid:0)1/(cid:15)4 + n/(cid:15)2(cid:1) (cid:101)O (cid:0)1/(cid:15)4(cid:1) nearly optimal complexity for solving WCSC problems without the smoothness assumption on the objective [2] 1. Finally, we summarize our results and the comparison with existing results in Table 1. 2