Abstract
Long-term temporal credit assignment is an important challenge in deep rein-forcement learning (RL). It refers to the ability of the agent to attribute actions to consequences that may occur after a long time interval. Existing policy-gradient and Q-learning algorithms typically rely on dense environmental rewards that provide rich short-term supervision and help with credit assignment. However, they struggle to solve tasks with delays between an action and the corresponding rewarding feedback. To make credit assignment easier, recent works have proposed algorithms to learn dense guidance rewards that could be used in place of the sparse or delayed environmental rewards. This paper is in the same vein – starting with a surrogate RL objective that involves smoothing in the trajectory-space, we arrive at a new algorithm for learning guidance rewards. We show that the guidance rewards have an intuitive interpretation, and can be obtained without training any additional neural networks. Due to the ease of integration, we use the guidance rewards in a few popular algorithms (Q-learning, Actor-Critic, Distributional-RL) and present results in single-agent and multi-agent tasks that elucidate the beneﬁt of our approach when the environmental rewards are sparse or delayed 1. 1

Introduction
Deep Reinforcement Learning (RL) agents are tasked with maximization of long-term environmental rewards. Prevalent algorithms for deep RL typically need to estimate the expected future rewards after taking an action in a particular state – Actor-critic and Q-learning involve computing the Q-value, while policy-gradient methods tend to be more stable when using the advantage function. The value estimation is performed using temporal difference (TD) or Monte-Carlo (MC) learning. Although deep RL algorithms can achieve remarkable results on a wide variety of tasks, their performance crucially depends on the meticulously designed reward function, which provides a dense per-timestep learning signal and facilitates value estimation. In real-world sequential decision-making problems, however, the rewards are often sparse or delayed. Examples include, to name a few, industrial process control [9], molecular design [17], and resource allocation [19]. Delayed rewards introduce high bias in TD-learning and high variance in MC-learning [1], leading to poor value estimates. This impedes long-term temporal credit assignment [15, 32], which refers to the ability of the agent to attribute actions to consequences that may occur after a long time interval. As a motivating example in a simulated domain, Figure 1 shows the performance with Soft-Actor-Critic (SAC) [7], a popular off-policy RL method, on two MuJoCo locomotion tasks from the Gym suite. For delay=k, the agent receives no reward for (k − 1) timesteps and is then provided the accumulated rewards at the kth timestep. Increasing the delay leads to progressively worse performance.
Another class of policy search algorithms, which are particularly handy when rewards are delayed, 1Code for this paper is available at https://github.com/tgangwani/GuidanceRewards 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
is black-box stochastic-optimization; examples include Evolu-tion Strategies [22] and Deep-Neuroevolution [30]. They are invariant to delayed rewards since the trajectories are not decom-posed into individual timesteps for learning; rather zeroth-order
ﬁnite-difference or gradient-free methods are used to learn poli-cies based only on the trajectory returns (or aggregated rewards).
However, one downside is that discarding the temporal struc-ture of the RL problem leads to inferior sample-efﬁciency when compared with the standard RL algorithms. Our goal in this paper is to design an approach that easily integrates into the existing RL algorithms, thus enjoying the sample-efﬁciency beneﬁts, while being invariant to delayed rewards. To achieve this, we introduce a surrogate RL objective that involves smoothing in the trajectory-space and arrive at a new algorithm for learning guidance rewards. The guidance rewards are curated using only the trajectory returns and are easily inferred for each state-action tuple. The dense supervision from the guidance rewards makes value estimation and credit assignment easier, substantially accelerating learning when the original environmental rewards are sparse or delayed.
Figure 1: Effect of delayed rewards
We provide an intuitive understanding for the guidance rewards in terms of uniform credit assignment – they characterize a uniform redistribution of the trajectory return to each constituent state-action pair.
A favorable property of our approach is that no additional neural networks need to be trained to obtain the guidance rewards, unlike recent works that also examine RL with delayed rewards (c.f. Section 5).
For quantitative evaluation, we combine the guidance rewards with a variety of RL algorithms and environments. These include single-agent tasks: Q-learning [33] in a discrete grid-world and SAC on continuous control locomotion tasks; and multi-agent tasks: TD3 [4] and Distributional-RL [2] in multi-particle cooperative environments. 2