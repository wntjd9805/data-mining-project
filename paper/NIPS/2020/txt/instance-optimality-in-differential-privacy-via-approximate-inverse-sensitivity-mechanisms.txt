Abstract
We study and provide instance-optimal algorithms in differential privacy by ex-tending and approximating the inverse sensitivity mechanism. We provide two approximation frameworks, one which only requires knowledge of local sensitiv-ities, and a gradient-based approximation for optimization problems, which are efﬁciently computable for a broad class of functions. We complement our analysis with instance-speciﬁc lower bounds for vector-valued functions, which demonstrate that our mechanisms are (nearly) instance-optimal under certain assumptions and that minimax lower bounds may not provide an accurate estimate of the hardness of a problem in general: our algorithms can signiﬁcantly outperform minimax bounds for well behaved instances. Finally, we use our approximation framework to develop private mechanisms for unbounded-range mean estimation, principal component analysis, and linear regression. For PCA, our mechanisms give an efﬁcient (pure) differentially private algorithm with near-optimal rates. 1

Introduction
We study the estimation of a function (statistic) of interest under differential privacy, where strong privacy protections usually decrease utility relative to non-private data analysis. In an effort to improve the utility of private algorithms, it is of utmost importance to design mechanisms that adapt to the hardness of the underlying data. Such mechanisms are of growing prevalence in the privacy literature, with prominent examples including the smooth sensitivity [25] and propose-test-release [12] frameworks.
To further investigate adaptivity to underlying instance, Asi and Duchi [4] recently study instance-optimal mechanisms—which, in a sense, achieve optimal utility for every possible data instance—in differentially private release of 1-dimensional quantities, moving beyond the more standard (worst case) minimax optimality. Inspired by classical statistical theory, Asi and Duchi develop local-minimax optimality and optimality against unbiased mechanisms, both of which aim to capture the hardness of the underlying data. By developing instance-speciﬁc lower bounds, they show that classical frameworks such as smooth sensitivity and propose-test-release may not be instance-optimal in general. To overcome this challenge, they investigate what they term the inverse sensitivity mechanism, showing it is instance-optimal for a wide range of functions.
Yet instance-optimality in private statistical estimation remains widely unexplored. First, the imple-mentation of the inverse sensitivity mechanism requires a calculation of a particular sample distance (see Section 1.1.1), which may be intractable. Moreover, the current instance-optimality guarantees are not sharp for vector-valued functions. This is in part because the paper [4] tailors its instance-optimality notions for 1-dimensional functions by leveraging Stein’s “hardest one-dimensional alternative” approach to lower bounds [31, 9], which gives tight lower bounds for 1-dimensional functions but fails to yield correct bounds in higher dimensions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To address these challenges, in this work we develop extensions and approximations to the inverse sensitivity mechanism with efﬁcient implementations for a broad class of functions, which allows us to (for example) develop efﬁcient algorithms for private PCA with near-optimal sample complexity.
We also establish complementary instance-optimality results for vector-valued functions by proposing two approaches for instance-speciﬁc lower bounds: (i) a local minimax approach that measures the risk of an instance through the loss that an algorithm must incur on instances in a small neighborhood around it, and (ii) lower bounds against families of appropriately unbiased mechanisms, which includes many standard mechanisms. These instance-speciﬁc bounds suggest the limitations of more prevalent minimax (worst-case) bounds in privacy [19, 11]: they do not always give the correct limits on the performance of algorithms, and algorithms exist that achieve lower error on many instances. 1.1 Preliminaries
Given a function f : X n → T and instance x ∈ X n, we wish to design differentially private mechanisms that accurately estimates the value f (x). We usually take X , T ⊂ Rd for a dimension d.
We begin by recalling the standard deﬁnition of differential privacy [16, 15]. We say that two instances x, x(cid:48) ∈ X n are neighboring if they differ in at most one example, that is, dham(x, x(cid:48)) ≤ 1.
Deﬁnition 1.1. A randomized algorithm M : X n → T is (ε, δ)-differentially private if for all neighboring datasets x, x(cid:48) ∈ X n and all measurable S ⊆ T ,
P (M (x) ∈ S) ≤ eεP (M (x(cid:48)) ∈ S) + δ.
If δ = 0, then M is ε-differentially private.
Given a loss function L : T × T → R+, we quantify the utility of a mechanism M on instance x through its expected loss E[L(M (x), f (x))]. A mechanism is instance-optimal if it achieves the best utility for every instance. We formalize this through instance-speciﬁc lower bounds in Section 3.
For a function f : X n → R, the standard method to preserve privacy is the Laplace mechanism [16].
Deﬁning the global sensitivity of f to be GSf := supx,x(cid:48):dham(x,x(cid:48))≤1 |f (x) − f (x(cid:48))|, it adds Laplace noise, MLap(x) := f (x)+ GSf
ε Lap(1). This can be conservative, therefore Nissim et al. [25] consider the local sensitivity at instance x at hand LSf (x) := supx(cid:48):dham(x,x(cid:48))≤1 |f (x) − f (x(cid:48))|. Directly using the local sensitivities may compromise privacy, hence the smooth sensitivity framework adds noise that is proportional to a smooth upper bound Sβ(x) on the local sensitivity, that is,
Msm(x) := f (x) + 2Sβ (x)
ε Z, where Z is sampled from an admissible noise distribution and Sβ(x) is the smooth sensitivity satisfying LS(x) ≤ Sβ(x) and Sβ(x) ≤ eβSβ(x(cid:48)) for neighboring instances x, x(cid:48), and β is chosen appropriately to guarantee the desired privacy level. 1.1.1 The inverse sensitivity mechanism
Our work builds on the inverse sensitivity mechanism [4], which we review. Key to the mechanism is the path-length (inverse sensitivity), which, for a target t, measures how many users we must change in x to reach x(cid:48) with a target value t: lenf (x; t) := inf x(cid:48)
{dham(x, x(cid:48)) | f (x(cid:48)) = t} . (1)
The basic inverse sensitivity mechanism then instantiates the exponential mechanism [24] with the path-length function (1), yielding the density
πMinv(x)(t) = e−lenf (x;t)ε/2 (cid:82)
T e−lenf (x;s)ε/2ds
. (M.1)
A smoother variant of mechanism (M.1) is sometimes necessary to achieve instance-optimality, where one instead uses lenρ f (x; t) = inf s∈T :(cid:107)s−t(cid:107)≤ρ lenf (x; s), with a smoothing parameter ρ > 0 [4]. Different variations of these mechanisms are instance-optimal for a range of real-valued functions. Yet while examples exist, it is often unclear how to compute the length (1). 2
Instance-speciﬁc bounds depend on the modulus of continuity, which (focusing in this work on a vector space T with norm (cid:107)·(cid:107)p) measures the sensitivity of a function when changing k users:
ωp f (x; k) = sup x(cid:48)∈X n (cid:110) (cid:107)f (x) − f (x(cid:48))(cid:107)p : dham(x, x(cid:48)) ≤ k (cid:111)
. (2)
Instance-speciﬁc lower bounds show that the risk we expect for an ε-differentially private algorithm on instance x is in general roughly ωf (x; 1/ε) for 1-dimensional functions (with p = 1) and loss
L(s, t) = |s − t| [4]. Unfortunately, this is not tight for d-dimensional functions.
Notation We denote samples using bold symbols x ∈ X n and individual examples using non-bold symbol x ∈ X . We let dham(x, x(cid:48)) denote the Hamming distance of instance x, x(cid:48) ∈ X n. The local sensitivity of f : X n → T at instance x is LSp f (x; 1), and the global sensitivity of f is
GSp f (x; 1). To facilitate notation, we sometimes remove the superscript p if p = 2.
We let diamp(T ) = sups,t∈T (cid:107)s − t(cid:107)p, and Bd−1 p = {x ∈ Rd : (cid:107)x(cid:107)p ≤ 1} denote the (cid:96)p-ball. f = supx∈X n ωp f (x) = ωp 1.2 Contributions
Approximate inverse sensitivity mechanisms We develop two approximation methods for the inverse sensitivity mechanism: (i) using local sensitivities in Section 2.1 and (ii) a gradient-based method for minimization problems in Section 2.2. These methods have efﬁcient implementations for a wide range of problems and can outperform smooth sensitivity mechanisms for pure differential privacy. In contrast to Cauchy and Student’s T distributions used in such instantiations [7]—which have inﬁnite ﬁrst and third moment respectively—our mechanisms add noise with bounded p’th moments for all ﬁnite p, resulting in improved high-probability bounds for utility analysis which is especially important for high dimensional functions as our examples demonstrate.
Instance-optimality and lower bounds We propose two notions of instance optimality for vector-valued functions and prove tight lower bounds for both notions in Section 3. Similarly to the 1-dimensional setting, our results give a characterization of the risk through the modulus of continuity.
Combined with our instance-speciﬁc upper bounds, these bounds establish that approximate and exact inverse mechanisms are (nearly) instance-optimal for vector-valued functions under some assumptions.
Applications We study three problems that illustrate the methodological possibilities of the inverse sensitivity framework and its approximations in Section 4: mean estimation, PCA and linear regres-sion. The utility improvements in these examples demonstrate the advantages of our mechanisms over standard frameworks and the importance of these notions of instance-optimality. Here we highlight the PCA example where smooth sensitivity algorithms require sample complexity (for dimension d and ignoring other parameters) O(d3/2) [18], whereas our mechanisms require O(d) samples, which is the optimal dependence on the dimension d according to PCA lower bounds [10, 21]. 1.3