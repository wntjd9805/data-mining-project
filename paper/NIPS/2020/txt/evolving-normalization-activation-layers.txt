Abstract
Normalization layers and activation functions are fundamental components in deep networks and typically co-locate with each other. Here we propose to design them using an automated approach. Instead of designing them separately, we unify them into a single tensor-to-tensor computation graph, and evolve its structure starting from basic mathematical functions. Examples of such mathematical functions are addition, multiplication and statistical moments. The use of low-level mathematical functions, in contrast to the use of high-level modules in mainstream NAS, leads to a highly sparse and large search space which can be challenging for search methods. To address the challenge, we develop efﬁcient rejection protocols to quickly ﬁlter out candidate layers that do not work well. We also use multi-objective evolution to optimize each layer’s performance across many architectures to prevent overﬁtting. Our method leads to the discovery of EvoNorms, a set of new normalization-activation layers with novel, and sometimes surprising structures that go beyond existing design patterns. For example, some EvoNorms do not assume that normalization and activation functions must be applied sequentially, nor need to center the feature maps, nor require explicit activation functions.
Our experiments show that EvoNorms work well on image classiﬁcation models including ResNets, MobileNets and EfﬁcientNets but also transfer well to Mask
R-CNN with FPN/SpineNet for instance segmentation and to BigGAN for image synthesis, outperforming BatchNorm and GroupNorm based layers in many cases.1 1

Introduction
Normalization layers and activation functions are fundamental building blocks in deep networks for stable optimization and improved generalization. Although they frequently co-locate, they are designed separately in previous works. There are several heuristics widely adopted during the design process of these building blocks. For example, a common heuristic for normalization layers is to use mean subtraction and variance division [1–4], while a common heuristic for activation functions is to use scalar-to-scalar transformations [5–11]. These heuristics may not be optimal as they treat normalization layers and activation functions as separate. Can automated machine learning discover a novel building block to replace these layers and go beyond the existing heuristics?
Here we revisit the design of normalization layers and activation functions using an automated approach. Instead of designing them separately, we unify them into a normalization-activation layer.
With this uniﬁcation, we can formulate the layer as a tensor-to-tensor computation graph consisting of basic mathematical functions such as addition, multiplication and cross-dimensional statistical moments. These low-level mathematical functions form a highly sparse and large search space, in contrast to mainstream NAS which uses high-level modules (e.g., Conv-BN-ReLU). To address the challenge of the size and sparsity of the search space, we develop novel rejection protocols to efﬁciently ﬁlter out candidate layers that do not work well. To promote strong generalization across different architectures, we use multi-objective evolution to explicitly optimize each layer’s 1Code for EvoNorms on ResNets: https://github.com/tensorﬂow/tpu/tree/master/models/ofﬁcial/resnet 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
BN-ReLU max (cid:18)
EvoNorm-B0
√ max( x−µb,w,h(x)
√ s2 b,w,h(x) x s2 b,w,h(x),v1x+ (cid:19)
γ + β, 0
√
γ + β w,h(x)) s2
Table 1: A searched layer named EvoNorm-B0 which consistently outperforms BN-ReLU. µb,w,h, s2 b,w,h, s2 w,h, v1 refer to batch mean, batch vari-ance, instance variance and a learnable variable.
Figure 1: Computation graph of EvoNorm-B0. performance over multiple architectures. Our method leads to the discovery of a set of novel layers, dubbed EvoNorms, with surprising structures that go beyond expert designs (an example layer is shown in Table 1 and Figure 1). For example, our most performant layers allow normalization and activation functions to interleave, in contrast to BatchNorm-ReLU or ReLU-BatchNorm [1,12] where normalization and activation function are applied sequentially. Some EvoNorms do not attempt to center the feature maps, and others require no explicit activation functions.
EvoNorms consist of two series: B series and S series. The B series are batch-dependent and were discovered by our method without any constraint. The S series work on individual samples, and were discovered by rejecting any batch-dependent operations. We verify their performance on a number of image classiﬁcation architectures, including ResNets [13], MobileNetV2 [14] and EfﬁcientNets [15].
We also study their interactions with a range of data augmentations, learning rate schedules and batch sizes, from 32 to 1024 on ResNets. Our experiments show that EvoNorms can substantially outperform popular layers such as BatchNorm-ReLU and GroupNorm-ReLU. On Mask R-CNN [16] with FPN [17] and with SpineNet [18], EvoNorms achieve consistent gains on COCO instance segmentation with negligible computation overhead. To further verify their generalization, we pair
EvoNorms with a BigGAN model [19] and achieve promising results on image synthesis.
Our contributions can be summarized as follows:
• We are the ﬁrst to search for the combination of normalization-activation layers. Our proposal to unify them into a single graph and the search space are novel. Our work tackles a missing link in AutoML by providing evidence that it is possible to use AutoML to discover a new building block from low-level mathematical operations (see Sec. 2 for a comparison with related works). A combination of our work with traditional NAS may realize the full potential of AutoML in automatically designing machine learning models from scratch.
• We propose novel rejection protocols to ﬁlter out candidates that do not work well or fail to work, based on both their performance and stability. We are also the ﬁrst to address strong generalization of layers by pairing each candidate with multiple architectures and to explicitly optimize their cross-architecture performance. Our techniques can be used by other
AutoML methods that have large, sparse search spaces and require strong generalization.
• Our discovered layers, EvoNorms, by themselves are novel contributions because they are different from previous works. They work well on a diverse set of architectures, including
ResNets [12, 13], MobileNetV2 [14], MnasNet [20], EfﬁcientNets [15], and transfer well to
Mask R-CNN [16], SpineNet [21] and BigGAN-deep [19]. E.g., on Mask-RCNN our gains are +1.9AP over BN-ReLU and +1.3AP over GN-ReLU. EvoNorms have a high potential impact because normalization and activation functions are central to deep learning.
• EvoNorms shed light on the design of normalization and activation layers. E.g., their struc-tures suggest the potential beneﬁts of non-centered normalization schemes, mixed variances and tensor-to-tensor rather than scalar-to-scalar activation functions (these properties can be seen in Table 1). Some of these insights can be used by experts to design better layers. 2