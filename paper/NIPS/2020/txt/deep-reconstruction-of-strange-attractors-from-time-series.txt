Abstract
Experimental measurements of physical systems often have a limited number of independent channels, causing essential dynamical variables to remain unobserved.
However, many popular methods for unsupervised inference of latent dynamics from experimental data implicitly assume that the measurements have higher intrin-sic dimensionality than the underlying system—making coordinate identiﬁcation a dimensionality reduction problem. Here, we study the opposite limit, in which hidden governing coordinates must be inferred from only a low-dimensional time series of measurements. Inspired by classical analysis techniques for partial ob-servations of chaotic attractors, we introduce a general embedding technique for univariate and multivariate time series, consisting of an autoencoder trained with a novel latent-space loss function. We show that our technique reconstructs the strange attractors of synthetic and real-world systems better than existing tech-niques, and that it creates consistent, predictive representations of even stochastic systems. We conclude by using our technique to discover dynamical attractors in diverse systems such as patient electrocardiograms, household electricity usage, neural spiking, and eruptions of the Old Faithful geyser—demonstrating diverse applications of our technique for exploratory data analysis. 1

Introduction
Faced with an unfamiliar experimental system, it is often impossible to know a priori which quan-tities to measure in order to gain insight into the system’s dynamics. Instead, one typically must rely on whichever measurements are readily observable or technically feasible, resulting in partial measurements that fail to fully describe a system’s important properties. These hidden variables seemingly preclude model building, yet history provides many compelling counterexamples of mech-anistic insight emerging from simple measurements—from Shaw’s inference of the strange attractor driving an irregularly-dripping faucet, to Winfree’s discovery of toroidal geometry in the Drosophila developmental clock [1, 2].
Here, we consider this problem in the context of recent advances in unsupervised learning, which have been applied to the broad problem of discovering dynamical models directly from experimental data. Given high-dimensional observations of an experimental system, various algorithms can be used to extract latent coordinates that are either time-evolved through empirical operators or ﬁt directly to differential equations [3, 4, 5, 6, 7, 8, 9, 10]. This process represents an empirical analogue of the traditional model-building approach of physics, in which approximate mean-ﬁeld or coarse-grained dynamical variables are inferred from ﬁrst principles, and then used as independent coordinates in a reduced-order model [11, 12]. However, many such techniques implicitly assume that the
∗Code available at: https://github.com/williamgilpin/fnn 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Overview of problem and approach. A univariate time series y1(t) is observed from a multivariate attractor Y = [y1(t) y2(t) y3(t)]. This signal is converted into a time-lagged Hankel matrix X, which is used to train an autoencoder with the false-nearest-neighbor loss LFNN. The latent variables reconstruct the original coordinates. degrees of freedom in the raw data span the system’s full dynamics, making dynamical inference a dimensionality reduction problem.
Here, we study the inverse problem: given a single, time-resolved measurement of a complex dynamical system, is it possible to reconstruct the higher-dimensional process driving the dynamics?
This process, known as state space reconstruction, is the focus of many classical results in nonlinear dynamics theory, which demonstrate various heuristics for reconstructing effective coordinates given the time history of a system [13, 14]. Such techniques have broad application throughout the natural sciences, particularly in areas in which simultaneous multidimensional measurements are difﬁcult to obtain—such as ecology, physiology, and climate science [15, 16, 17, 18]. However, these embedding techniques are strongly sensitive to hyperparameter choice, system dimensionality, non-stationarity, continuous spectra, and experimental measurement error—therefore requiring extensive tuning and in-sample cross-validation before they can be applied to a new dataset [19, 20, 15]. Additionally, current methods cannot consistently infer the underlying dimensionality as the original system, making them prone to redundancy and overﬁtting [21]. Several of these shortcomings may be addressable by revisiting classical techniques with contemporary methods, thus motivating our study.
Here, we introduce a general method for reconstructing the d-dimensional attractor of an unknown dynamical system, given only a univariate measurement time series. We introduce a custom loss function and regularizer, the false-nearest-neighbor loss, that allows recurrent autoencoder networks to successfully reconstruct unseen dynamical variables from time series. We embed a variety of dynamical systems, and we formalize several existing and novel metrics for comparing an inferred attractor to a system’s original attractor—and we demonstrate that our method outperforms baseline state space reconstruction methods. We test the consistency of our technique on stochastic dynamical systems, and ﬁnd that it generates robust embeddings that can effectively forecast the dynamics at long time horizons, in contrast with previous methods. We conclude by performing exploratory analysis of datasets that have previously been hypothesized to occupy strange attractors, and discover underlying attractors in systems spanning earth science, neuroscience, and physiology. 2