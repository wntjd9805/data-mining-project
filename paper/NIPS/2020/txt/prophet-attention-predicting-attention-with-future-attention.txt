Abstract
Recently, attention based models have been used extensively in many sequence-to-sequence learning systems. Especially for image captioning, the attention based models are expected to ground correct image regions with proper generated words.
However, for each time step in the decoding process, the attention based models usually use the hidden state of the current input to attend to the image regions.
Under this setting, these attention models have a “deviated focus” problem that they calculate the attention weights based on previous words instead of the one to be generated, impairing the performance of both grounding and captioning. In this paper, we propose the Prophet Attention, similar to the form of self-supervision. In the training stage, this module utilizes the future information to calculate the “ideal” attention weights towards image regions. These calculated “ideal” weights are further used to regularize the “deviated” attention. In this manner, image regions are grounded with the correct words. The proposed Prophet Attention can be easily incorporated into existing image captioning models to improve their performance of both grounding and captioning. The experiments on the Flickr30k Entities and the MSCOCO datasets show that the proposed Prophet Attention consistently outperforms baselines in both automatic metrics and human evaluations. It is worth noticing that we set new state-of-the-arts on the two benchmark datasets and achieve the 1st place on the leaderboard of the online MSCOCO benchmark in terms of the default ranking score, i.e., CIDEr-c40. 1

Introduction
The task of image captioning [7] aims to generate a textual description for an input image and has received extensive research interests. Recently, the attention-enhanced encoder-decoder framework
[2, 17, 20, 29, 38, 54] have achieved great success in advancing the state-of-the-arts. Speciﬁcally, they use a Faster-RCNN [2, 45] to acquire region-based visual representations and an RNN [14, 18] to generate the coherent captions, where the attention model [3, 32, 49, 53] guides the decoding process by attending the hidden state to the image regions at each time step. Many sequence-to-sequence learning systems, including machine translation [3, 49] and text summarization [58], have proven the importance of the attention mechanism in generating meaningful sentences. Especially for image captioning, the attention model can ground the salient image regions to generate the next word in the sentence [2, 26, 32, 53].
Current attention model attends to image regions based on current hidden state [49, 53], which contains the information of past generated words. As a result, the attention model has to predict 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of the sequence of the attended image regions from a state-of-the-art system
[20] in generating each word for a complete image description. At each time step, only the top-1 attended image region is shown [59]. As we can see, the attended image regions are grounded more on the input words than the output words, such as the timesteps that input yellow and umbrella, demonstrating poor grounding accuracies of the current attention model. attention weights without knowing the word it should ground. Figure 1 illustrates a generated caption and the attended image regions from a state-of-the-art captioning system [20]. As we can see, the attended image regions are more grounded on current input word than the output one. For example, at the time step to generate the 5th word yellow, the attended image region is the woman instead of the umbrella. As a result, the incorrect adjective yellow is generated rather than the correct adjective red. This is mainly due to the “focus” of the attention is “deviated” several steps backwards and the conditioned words are woman and holding; Another example is at the time step to generate the 7th word wearing, the attended image region should be the woman instead of the umbrella. Although the generated word is correct, the unfavorable attended image region impairs the grounding performance
[59] and ruins the model interpretability, because the attended image region often serves as a visual interpretation to qualitative measurement of the captioning model [9, 11, 33, 48, 59].
In this paper, to address the “deviated focus” issue of current attention models, we propose the novel
Prophet Attention to ground the image regions with proper generated words in a manner similar to self-supervision. As shown in Figure 2, in the training stage, for each time step in the decoding process, we ﬁrst employ the words that will be generated in the future, to calculate the “ideal” attention weights towards image regions. And then the calculated “ideal” attention weights are used to guide the attention calculation based on the input words that have already been generated (without future words to be generated). It indicates that the conventional attention model will be regularized by the calculated attention weights based on future words. We evaluate the proposed Prophet Attention on two benchmark image captioning datasets. According to both automatic metrics and human evaluations, the captioning models equipped with Prophet Attentions outperform baselines.
Overall, the contributions of this work are as follows:
• We propose Prophet Attention to enable attention models to correctly ground words that are to be generated to proper image regions. The Prophet Attention can be easily incorporated into existing models to improve their performance of both grounding and captioning.
• We evaluate Prophet Attention for image captioning on the Flickr30k Entities and the
MSCOCO datasets. The captioning models equipped with the Prophet Attention signiﬁ-cantly outperform the ones without it. Besides automatic metrics, we also conduct human evaluations to evaluate Prophet Attention from the user experience perspective. At the time of submission (2 June 2020), we achieve the 1st place on the leaderboard of the MSCOCO online server benchmark in terms of the default ranking score (CIDEr-c40).
• In addition to image captioning task, we also attempt to adapt Prophet Attention to other language generation tasks. We obtain positive experimental results on paraphrase generation and video captioning tasks.
The rest of the paper is organized as follows. Section 2 introduces the proposed Prophet Attention.
Section 3 and Section 4 present the experimental results. Section 5 and Section 6 review the related work and conclude the paper, respectively. 2
Figure 2: Illustration of the conventional attention model (left) and our Prophet Attention (right) approach. As we can see, our approach calculates “ideal” attention weights ˆαt based on future generated words yi:j (j ≥ t) as a target for the attention model based on previous generated words. 2 Approach
We ﬁrst brieﬂy review the conventional attention-enhanced encoder-decoder framework in image captioning and then describe the proposed Prophet Attention in detail. 2.1