Abstract
Deep classiﬁers have achieved great success in visual recognition. However, real-world data is long-tailed by nature, leading to the mismatch between training and testing distributions. In this paper, we show that the Softmax function, though used in most classiﬁcation tasks, gives a biased gradient estimation under the long-tailed setup. This paper presents Balanced Softmax, an elegant unbiased extension of
Softmax, to accommodate the label distribution shift between training and testing.
Theoretically, we derive the generalization bound for multiclass Softmax regression and show our loss minimizes the bound. In addition, we introduce Balanced Meta-Softmax, applying a complementary Meta Sampler to estimate the optimal class sample rate and further improve long-tailed learning. In our experiments, we demonstrate that Balanced Meta-Softmax outperforms state-of-the-art long-tailed classiﬁcation solutions on both visual recognition and instance segmentation tasks.† 1

Introduction
Most real-world data comes with a long-tailed nature: a few high-frequency classes (or head classes) contributes to most of the observations, while a large number of low-frequency classes (or tail classes) are under-represented in data. Taking an instance segmentation dataset, LVIS [7], for example, the number of instances in banana class can be thousands of times more than that of a bait class. In practice, the number of samples per class generally decreases from head to tail classes exponentially.
Under the power law, the tails can be undesirably heavy. A model that minimizes empirical risk on long-tailed training datasets often underperforms on a class-balanced test dataset. As datasets are scaling up nowadays, the long-tailed nature poses critical difﬁculties to many vision tasks, e.g., visual recognition and instance segmentation.
An intuitive solution to long-tailed task is to re-balance the data distribution. Most state-of-the-art (SOTA) methods use the class-balanced sampling or loss re-weighting to “simulate" a balanced training set [3, 31]. However, they may under-represent the head class or have gradient issues during optimization. Cao et al. [4] introduced Label-Distribution-Aware Margin Loss (LDAM), from the perspective of the generalization error bound. Given fewer training samples, a tail class should have a higher generalization error bound during optimization. Nevertheless, LDAM is derived from the hinge loss, under a binary classiﬁcation setup and is not suitable for multi-class classiﬁcation.
*Corresponding author
†Code available at https://github.com/jiawei-ren/BalancedMetaSoftmax 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We propose Balanced Meta-Softmax (BALMS) for long-tailed visual recognition. We ﬁrst show that the Softmax function is intrinsically biased under the long-tailed scenario. We derive a Balanced
Softmax function from the probabilistic perspective that explicitly models the test-time label distribu-tion shift. Theoretically, we found that optimizing for the Balanced Softmax cross-entropy loss is equivalent to minimizing the generalization error bound. Balanced Softmax generally improves long-tailed classiﬁcation performance on datasets with moderate imbalance ratios, e.g., CIFAR-10-LT [18] with a maximum imbalance factor of 200. However, for datasets with an extremely large imbalance factor, e.g., LVIS [7] with an imbalance factor of 26,148, the optimization process becomes difﬁcult.
Complementary to the loss function, we introduce the Meta Sampler, which learns to re-sample for achieving high validation accuracy by meta-learning. The combination of Balanced Softmax and
Meta Sampler could efﬁciently address long-tailed classiﬁcation tasks with high imbalance factors.
We evaluate BALMS on both long-tailed image classiﬁcation and instance segmentation on ﬁve commonly used datasets: CIFAR-10-LT [18], CIFAR-100-LT [18], ImageNet-LT [23], Places-LT [34] and LVIS [7]. On all datasets, BALMS outperforms state-of-the-art methods. In particular, BALMS outperforms all SOTA methods on LVIS, with an extremely high imbalanced factor, by a large margin.
We summarize our contributions as follows: 1) we theoretically analyze the incapability of Softmax function in long-tailed tasks; 2) we introduce Balanced Softmax function that explicitly considers the label distribution shift during optimization; 3) we present Meta Sampler, a meta-learning based re-sampling strategy for long-tailed learning. 2