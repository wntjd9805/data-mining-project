Abstract
A key tool for building differentially private systems is adding Gaussian noise to the output of a function evaluated on a sensitive dataset. Unfortunately, using a continuous distribution presents several practical challenges. First and foremost,
ﬁnite computers cannot exactly represent samples from continuous distributions, and previous work has demonstrated that seemingly innocuous numerical errors can entirely destroy privacy. Moreover, when the underlying data is itself dis-crete (e.g., population counts), adding continuous noise makes the result less interpretable.
With these shortcomings in mind, we introduce and analyze the discrete Gaussian in the context of differential privacy. Speciﬁcally, we theoretically and experimentally show that adding discrete Gaussian noise provides essentially the same privacy and accuracy guarantees as the addition of continuous Gaussian noise. We also present an simple and efﬁcient algorithm for exact sampling from this distribution. This demonstrates its applicability for privately answering counting queries, or more generally, low-sensitivity integer-valued queries. 1

Introduction
Differential Privacy [DMNS06] provides a rigorous standard for ensuring that the output of an algorithm does not leak the private details of individuals contained in its input. A standard technique for ensuring differential privacy is to evaluate a function on the input and then add a small amount of random noise to the result before releasing it. Speciﬁcally, it is common to add noise drawn from a
Laplace or Gaussian distribution, which is scaled according to the sensitivity of the function – i.e., how much one person’s data can change the function value. These are two of the most fundamental algorithms in differential privacy, which are used as subroutines in almost all differentially private systems. For example, differentially private algorithms for convex empirical risk minimization and deep learning are based on adding noise to gradients [BST14; ACGMMTZ16].
However, the Laplace and Gaussian distributions are both continuous over the real numbers. As such, it is not possible to even represent a sample from them on a ﬁnite computer, much less produce such a sample. One might suppose that such issues are purely of theoretical interest, and that they can be resolved in practice by simply using standard ﬂoating-point arithmetic and representations.
Unfortunately, this is not the case: Mironov [Mir12] demonstrated that the naïve use of ﬁnite-precision approximations can result in catastrophic failures of privacy. In particular, by examining the low-order bits of the noisy output, the noiseless value can often be determined. Mironov showed that this information can allow the entire input dataset to be rapidly reconstructed, while only a negligible privacy loss is recorded by the system. Despite this demonstration, the ﬂawed methods continue to appear in open source implementations of differentially private mechanisms [Cen18; Dif19; Goo20;
Whi20]. This demonstrates a real need for us to provide safe and practical solutions to enable the deployment of differentially private systems in real-world privacy-critical settings. In this work, we 0Alphabetical authorship. Full version available at https://arxiv.org/abs/2004.00010 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
carefully consider how to securely implement these basic differentially private methods on ﬁnite computers that cannot faithfully represent real numbers.
One solution is to instead sample from a discrete distribution that can be sampled on a ﬁnite computer.
For many natural queries, the output of the function to be computed is naturally discrete – e.g., counting how many records in a dataset satisfy some predicate – and hence there is no loss in accuracy when adding discrete noise to it. Otherwise, the function value must be rounded before adding noise. 2 · e−ε|x| at x ∈ R we have a probability mass of eε−1
The discrete Laplace distribution (a.k.a. two-sided geometric distribution) [GRS12] is the natural discrete analogue of the continuous Laplace distribution. That is, instead of a probability density eε+1 · e−ε|x| at x ∈ Z. Like its continuous of ε counterpart, the discrete Laplace distribution provides pure (ε, 0)-differential privacy and has many other desirable properties. Notably, the discrete Laplace distribution is used in the TopDown algorithm being developed to protect the data collected in the 2020 US Census [KCKHM18; Abo18; Cen18].
The (continuous) Gaussian distribution has many advantages over the (continuous) Laplace distribu-tion (and also some disadvantages), making it better suited for many applications. For example, the
Gaussian distribution has lighter tails than the Laplace distribution. In settings with a high degree of composition – i.e., answering many queries with independent noise, rather than a single query – the scale (e.g., variance) of Gaussian noise is also lower than the scale of Laplace noise required for a comparable privacy guarantee. The privacy analysis under composition of Gaussian noise addition is typically simpler and sharper; in particular, these privacy guarantees can be cleanly expressed in terms of concentrated differential privacy (CDP) [DR16; BS16] and related variants of differential privacy [Mir17; BDRS18; DRS19]. (See Section 3.1 for further discussion.)
Thus, it is natural to wonder whether a discretization of the Gaussian distribution retains the privacy and utility properties of the continuous Gaussian distribution, as is the case for the Laplace distribution.
In this paper, we show that this is indeed the case.
Deﬁnition 1 (Discrete Gaussian). Let µ, σ ∈ R with σ > 0. The discrete Gaussian distribution (cid:0)µ, σ2(cid:1). It is a probability distribution supported on the with location µ and scale σ is denoted NZ
[X = x] = e−(x−µ)2 /2σ2 integers and deﬁned by ∀x ∈ Z,
P
X←NZ(µ,σ2) (cid:80) y∈Z e−(y−µ)2 /2σ2 .
Note that we exclusively consider µ ∈ Z; in this case, the distribution is symmetric and centered at 2πσ2) ·
µ. This is the natural discrete analogue of the continuous Gaussian (which has density (1/ e−(x−µ)2/2σ2 at x ∈ R), and it arises in lattice-based cryptography (in a multivariate form, which is believed to be hard to sample from) [GPV08; Reg09; Pei10; Ste17, etc.].
√ 1.1 Overview of Our Results
Our investigations focus on three aspects of the discrete Gaussian: privacy, utility, and sampling. In summary, we demonstrate that the discrete Gaussian provides the same level of privacy and utility as the continuous Gaussian. We also show that it can be efﬁciently sampled on a ﬁnite computer, thus addressing the shortcomings of continuous distributions discussed earlier. (cid:0)0, 1/ε2(cid:1) to an integer-valued sensitivity-1 query (e.g., a counting query) provides 1
§2 Privacy. The discrete Gaussian enjoys privacy guarantees which are almost identical to those of the continuous Gaussian. More precisely, in Theorem 4, we show that adding noise drawn 2 ε2-from NZ concentrated differential privacy. This is exactly the same guarantee attained by adding a draw from N (0, 1/ε2). Furthermore, in Theorem 6, we provide tight bounds on the discrete Gaussian’s approximate differential privacy guarantees. For large scales σ, the discrete and continuous Gaussian have virtually the same privacy guarantee. Along the way, we develop new tools for converting concentrated differential privacy guarantees into approximate differential privacy guarantees, which are not speciﬁc to the discrete Gaussian and are of independent interest.
§3 Utility. The accuracy attained by the discrete Gaussian is the same as (or slightly better than) the (cid:0)0, σ2(cid:1) is analogous continuous Gaussian. Speciﬁcally, Corollary 9 shows that the variance of NZ at most σ2, and that it also satisﬁes sub-Gaussian tail bounds comparable to N (0, σ2). We show numerically that the discrete Gaussian is better than rounding the continuous Gaussian to an integral value. We also provide a thorough comparison between the discrete Gaussian and the discrete Laplace distribution in Section 3.1, with a particular focus on performance under composition. 2
§4 Sampling. We present a practical, simple, and efﬁcient procedure for exact sampling from (cid:0)0, σ2(cid:1) that only requires access to uniformly random bits and does not involve any real-arithmetic
NZ operations or non-trivial function evaluations (Algorithm 1). As there are prior efﬁcient meth-ods [Kar16], we do not consider this to be one of our primary contributions. Nonetheless, we include these results for completeness and because they are arguably simpler than prior work.
On a technical note, while the takeaway of many of our conclusions is that the discrete and continuous
Gaussian are qualitatively similar, we comment that such statements are non-trivial to prove, relying upon methods such as the Poisson summation formula and Fourier analysis. For instance, even basic statements on the stability property of Gaussians under linear combinations do not hold for the discrete counterpart, with approximate versions being highly involved to establish (see, e.g., [AR16]). 1.2