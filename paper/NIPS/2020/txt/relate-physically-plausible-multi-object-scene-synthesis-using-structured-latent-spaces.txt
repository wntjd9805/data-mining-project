Abstract
We present RELATE, a model that learns to generate physically plausible scenes and videos of multiple interacting objects. Similar to other generative approaches,
RELATE is trained end-to-end on raw, unlabeled data. RELATE combines an object-centric GAN formulation with a model that explicitly accounts for correla-tions between individual objects. This allows the model to generate realistic scenes and videos from a physically-interpretable parameterization. Furthermore, we show that modeling the object correlation is necessary to learn to disentangle object positions and identity. We ﬁnd that RELATE is also amenable to physically realistic scene editing and that it signiﬁcantly outperforms prior art in object-centric scene generation in both synthetic (CLEVR, ShapeStacks) and real-world data (cars).
In addition, in contrast to state-of-the-art methods in object-centric generative modeling, RELATE also extends naturally to dynamic scenes and generates videos of high visual ﬁdelity. Source code, datasets and more results are available at http://geometry.cs.ucl.ac.uk/projects/2020/relate/. 1

Introduction
We consider the problem of learning to generate plausible images of scenes starting from parameters that are physically interpretable. Furthermore, we wish to learn such a capability from raw images
Image generation is often approached via alone, without any manual or external supervision.
Generative Adversarial Networks (GAN) [10]. These models learn to map noise vectors, used as a source of randomness, to image samples. While the resulting images are realistic, the random vectors that parameterize them are not interpretable. To address this issue, authors have recently proposed to structure the latent space of deep generative models, giving it a partial physical interpretability [28, 29, 36]. For example, HoloGAN [28] samples volumes and cameras to generate 2D images of 3D objects, and BlockGAN [29] creates scenes by composing multiple objects. The resulting GANs have been shown to learn concepts such as viewpoint and object disentangling from raw images.
BlockGAN is of particular interest because, via its relatively strong architectural biases, it provides interpretable parameters for the scene, incorporating concepts such as position and orientation.
However, BlockGAN comes with a signiﬁcant limitation in that it assumes that objects are mutually independent. This approximation is acceptable only when objects interact weakly, but it is badly violated for medium to densely packed scenes, or for scenes such as stacking wooden blocks or cars following a path, where the (object) correlation is strong.
∗indicates equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Image generation using RELATE. Individual scene components, such as background and foreground objects are represented by appearance z0 and pairs of appearance and pose vectors (zi, θi), i ∈ {1, . . . , K}, respectively. The key spatial relationship module Γ adjusts the initial independent pose samples ˆθi to be physically plausible (e.g., non-intersecting) to produce θi. The structured scene tensor W is ﬁnally transformed by the the generator network G to produce an image ˆI. RELATE is trained end-to-end in a GAN setup (D denotes the discriminator) on real unlabelled images.
Recent work in object-centric generative modeling has attempted to speciﬁcally address this by capturing correlations in latent space (e.g., [7, 36]). However as object state information remains signiﬁcantly entangled in these models they have, to date, been unable to operate on real-world data.
In this paper, we introduce RELATE, a model which explicitly leverages the strong architectural biases of BlockGAN to effectively model correlations between latent object state variables. This leads to a powerful model class, which is able to capture complex physical interactions, while still being able to learn from raw visual inputs alone. Empirically, we show that only when we model such interactions our GAN model correctly disentangles different objects when they exhibit even a moderate amount of correlation (ﬁgures 2 and 3). Without this component, the model may still generate high ﬁdelity images, but it generally fails to establish a physically-plausible association between the parameters and the generated images. Our results also demonstrate that GANs are surprisingly sensitive to the correlation of objects in natural scenes, and can thus be used to directly learn these without resorting to techniques such as variational auto-encoding (VAE [19]).
We demonstrate the efﬁcacy of RELATE in several scenarios, including balls rolling in bowls of variable shape [6], cluttered tabletops (CLEVR [16]), block stacking (ShapeStacks [12]), and videos of trafﬁc at busy intersection. By ablating the interaction module, we show that modeling the spatial correlation between the objects is key. Furthermore, we compare RELATE to several recent GAN-and VAE-based baselines, including BlockGAN [29], GENESIS [7] and OCF [1], in terms of Fréchet
Inception Distance (FID) [13], and outperform even the best state-of-the-art model by up to 29 points.
Qualitatively, we show that modeling spatial relationships strongly affects scene decomposition and the enforcement of spatial constraints in the generated images. We also show that the physically interpretable latent space learned by RELATE can be used to edit scenes as well as to generate scenes outside the distribution of the training data (e.g., containing more or fewer objects). Finally, we show that the parameterization can be used to generate long plausible video sequences (as measured according to FVD score [34]) by simulating their dynamics while preserving their spatial consistency. 2