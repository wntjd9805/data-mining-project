Abstract
Learning from indirect supervision signals is important in real-world AI applica-tions when, often, gold labels are missing or too costly. In this paper, we develop a uniﬁed theoretical framework for multi-class classiﬁcation when the supervision is provided by a variable that contains nonzero mutual information with the gold label. The nature of this problem is determined by (i) the transition probability from the gold labels to the indirect supervision variables and (ii) the learner’s prior knowledge about the transition. Our framework relaxes assumptions made in the literature, and supports learning with unknown, non-invertible and instance-dependent transitions. Our theory introduces a novel concept called separation, which characterizes the learnability and generalization bounds. We also demon-strate the application of our framework via concrete novel results in a variety of learning scenarios such as learning with superset annotations and joint supervision signals. 1

Introduction
We are interested in the problem of multiclass classiﬁcation where direct and gold annotations for the unlabeled instance are expensive or inaccessible, and instead the observation of a dependent variable of the true label is used as supervision signal. Examples include learning from noisy annotations
[1, 21, 26], partial annotations [16, 22, 14] or feedback from an external world [15, 8].
To extract the information contained in a dependent variable, the learner should have certain prior knowledge about the relation between the true label and the supervision signal, which can be expressed in various forms. For example, in the noisy label problem, the noisy rate is assumed to be bounded by a constant (such as the Massart noise [23, 17]). In the superset problem, the true label is commonly assumed to be contained in (or consistent with) the superset annotation [16, 22].
As in [13, 29, 36], we model the aforementioned relation using a transition probability, which is the distribution of the observable variable conditioned on the label and instance. The transition enables the learner to induce a prediction of the observable via the prediction of the label, and construct loss functions based on the induced prediction and the observable.
In this paper, instead of assuming that the learner fully knows the transition, we formalize the concept of transition class, a set that contains all the candidate transitions, to describe more general forms of prior information. Also, we deﬁne the concept of separation to quantify whether the information is enough to distinguish different labels. With these concepts, we are able to study a variety of learning scenarios with unknown, non-invertible and instance-dependent transitions in a uniﬁed way. We show this under the realizability assumption (also called separable in linear classiﬁcation), a commonly made assumption (such as [2, 19, 22]) that assumes that the true classiﬁer is in the hypothesis space.
Our goal is to develop a uniﬁed theoretical framework that can (i) provide learnability conditions for general indirect supervision problems, (ii) describe what prior knowledge is needed about the transition, and (iii) characterize the difﬁculty of learning with indirect supervision.
∗Work done while at the Allen Institute for AI and at the University of Illinois at Urbana-Champaign. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Speciﬁcally, in this paper, our main contribution includes: 1. We decompose the learnability condition of a general indirect supervision problem into three aspects: complexity, consistency and identiﬁability and provide a uniﬁed learning bound for the problem (Theorem 4.2). 2. We propose a simple yet powerful concept called separation, which encodes the prior knowledge about the transition using statistical distance between distributions over the annotation space and uses it to characterize consistency and identiﬁability (Theorem 5.2). 3. We formalize two ways to achieve separation: total variation and joint supervision, and use them to derive concrete novel results of practical learning problems of interest (Section 5.2 and 5.3).
All proofs of the theoretical results are presented in the supplementary material. 2