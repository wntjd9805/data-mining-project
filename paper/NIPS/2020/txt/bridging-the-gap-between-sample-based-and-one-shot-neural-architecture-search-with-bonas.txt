Abstract
Neural Architecture Search (NAS) has shown great potentials in ﬁnding better neural network designs. Sample-based NAS is the most reliable approach which aims at exploring the search space and evaluating the most promising architectures.
However, it is computationally very costly. As a remedy, the one-shot approach has emerged as a popular technique for accelerating NAS using weight-sharing.
However, due to the weight-sharing of vastly different networks, the one-shot approach is less reliable than the sample-based approach. In this work, we propose
BONAS (Bayesian Optimized Neural Architecture Search), a sample-based NAS framework which is accelerated using weight-sharing to evaluate multiple related architectures simultaneously. Speciﬁcally, we apply a Graph Convolutional Net-work predictor as surrogate model for Bayesian Optimization to select multiple related candidate models in each iteration. We then apply weight-sharing to train multiple candidate models simultaneously. This approach not only accelerates the traditional sample-based approach signiﬁcantly, but also keeps its reliability.
This is because weight-sharing among related architectures is more reliable than that in the one-shot approach. Extensive experiments are conducted to verify the effectiveness of our method over competing algorithms.1 1

Introduction
Designing an appropriate deep network architecture for each task and data set is tedious and time-consuming. Neural architecture search (NAS) [42], which attempts to ﬁnd this architecture automati-cally, has aroused signiﬁcant interest recently. Results competitive with hand-crafted architectures have been obtained in many application areas, such as natural language processing [20, 30] and computer vision [25, 9, 3, 16, 4].
Optimization in NAS is difﬁcult because the search space can contain billions of network architectures.
Moreover, the performance (e.g., accuracy) of a particular architecture is computationally expensive to evaluate. Hence, a central component in NAS is the strategy to search such a huge space of architectures. These strategies can be broadly categorized into two groups. Sample-based algorithms
[41, 17, 25, 19], which perform NAS in two phases: (i) search for candidate architectures with potentially good performance; and (ii) query their actual performance by full training. The second category contains one-shot NAS algorithms, which combine architectures in the whole search space together using weight sharing [24] or continuous relaxation [18, 19] for faster evaluation. Despite
∗Equal contribution. 1The code is available at https://github.com/pipilurj/BONAS. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Overview of the proposed BONAS. In the search phase, we use GCN embedding extractor and Bayesian Sigmoid Regression as the surrogate model for Bayesian Optimization and multiple candidate architectures are selected. In the query phase, we merge them as a super-network. Based on the trained super-network, we can query each sub-network using corresponding paths. their appealing speed, one-shot algorithms suffer from the following: (i) The obtained result can be sensitive to initialization, which hinders reproducibility; (ii) Constraints need to be imposed on the search space so as to constrain the super-network size (otherwise, it may be too large to ﬁt in the memory). As opposed to one-shot algorithms, sample-based approaches are more ﬂexible with respect to the search space, and can usually ﬁnd promising architectures regardless of initialization.
However, the heavy computation required by sample-based methods inevitably becomes the major obstacle. In this paper, we aim to develop a more efﬁcient sample-based NAS algorithm while taking advantage of the weight-sharing paradigm.
Due to the large sizes of most search spaces, searching for competitive architectures can be very difﬁ-cult. To alleviate this issue, Bayesian optimization (BO) [23], which explicitly considers exploitation and exploration, comes in handy as an efﬁcient model for search and optimization problems. In BO, a commonly used surrogate model is the Gaussian process (GP) [28]. However, its time complexity increases cubically with the number of samples [29]. Hence, it is costly for use in NAS due to the huge search space. Another drawback of GP in NAS is that it requires a manually designed kernel on architectures. While two heuristically-designed kernels are provided in [10] and [11], they can neither be easily adapted to different architecture families nor be further optimized based on data. It is still an open issue on how to deﬁne a good neural architecture kernel. On the other hand, in the query phase, the traditional approach of fully training the neural architectures is costly. Although early stopping can be adopted [33, 35], it cannot reduce the training time substantially while inevitably compromising the ﬁdelity of the obtained results. In one-shot methods, weight-sharing is performed on the whole space of sub-networks [24]. These sub-networks can be very different and so sharing their weights may not be a good idea.
To alleviate these problems, we present BONAS (Bayesian Optimized Neural Architecture Search), which is a sample-based NAS algorithm combined with weight-sharing (Figure 1). In the search phase, we ﬁrst use a graph convolutional network (GCN) [13] to produce embeddings for the neural architectures. This naturally handles the graph structures of neural architectures, and avoids deﬁning GP’s kernel function. Together with a novel Bayesian sigmoid regressor, it replaces the
GP as BO’s surrogate model. In the query phase, we construct a super-network from a batch of promising candidate architectures, and train them by uniform sampling. These candidates are then queried simultaneously based on the learned weight of the super-network. As weight-sharing is now performed only on a small subset of similarly-performing sub-networks with high BO scores, this is more reasonable than sharing the weights of all sub-networks in the search space as in one-shot NAS methods [24].
Empirically, the proposed BONAS outperforms state-of-the-art methods. We observe consistent gains on multiple search spaces for vision and NLP tasks. These include the standard benchmark data sets of NAS-Bench-101 [38] and NAS-Bench-201 [8] on convolutional architectures, and a new NAS benchmark data set LSTM-12K we recently collected for LSTMs. The proposed algorithm also ﬁnds competitive models efﬁciently in open-domain search with the NASNet search space [42]. 2
The contributions of this paper are as follows. (i) We improve the efﬁciency of sample-based NAS using Bayesian optimization in combination with a novel GCN embedding extractor and Bayesian
Sigmoid Regression to select candidate architectures. (ii) We accelerate the evaluation of sample-based NAS by training multiple related architectures simultaneously using weight-sharing. (iii)
Extensive experiments on both closed and open domains demonstrate the efﬁciency of the proposed method. BONAS achieves consistent gains on different benchmarks compared with competing baselines. It bridges the gap between training speeds of sample-based and one-shot NAS methods. 2