Abstract
Conditional stochastic optimization covers a variety of applications ranging from invariant learning and causal inference to meta-learning. However, constructing unbiased gradient estimators for such problems is challenging due to the composi-tion structure. As an alternative, we propose a biased stochastic gradient descent (BSGD) algorithm and study the bias-variance tradeoff under different structural assumptions. We establish the sample complexities of BSGD for strongly convex, convex, and weakly convex objectives under smooth and non-smooth conditions.
Our lower bound analysis shows that the sample complexities of BSGD cannot be improved for general convex objectives and nonconvex objectives except for smooth nonconvex objectives with Lipschitz continuous gradient estimator. For this special setting, we propose an accelerated algorithm called biased SpiderBoost (BSpiderBoost) that matches the lower bound complexity. We further conduct numerical experiments on invariant logistic regression and model-agnostic meta-learning to illustrate the performance of BSGD and BSpiderBoost. 1

Introduction
We study a class of optimization problems, called conditional stochastic optimization (CSO):
F (x) := Eξfξ(Eη|ξgη(x, ξ)), min x∈X (1) where X ⊆ Rd, gη(·, ξ) : Rd → Rk is a vector-valued function dependent on both random vectors
ξ and η, fξ(·) : Rk → R depends on the random vector ξ, and the inner expectation is taken with respect to the conditional distribution of η|ξ. Throughout, we assume access to samples from the distribution P (ξ) and the conditional distribution P (η|ξ).
CSO includes the classical stochastic optimization as a special case when gη(x, ξ) = x but is much more general. It has been recently utilized to solve a variety of applications in machine learning, ranging from the policy evaluation and control in reinforcement learning [12, 13, 35], the optimal control in linearly-solvable Markov decision process [12], to instrumental variable regression in causal inference [41, 34].
One common challenge with these applications is that in the extreme case, a few or only one sample is available from the conditional distribution of η|ξ for each given ξ. To deal with this limitation, a
∗The ﬁrst two authors have equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
primal-dual stochastic approximation algorithm was proposed to solve a min-max reformulation of
CSO using the kernel embedding techniques [12]. However, this approach requires convexity of f and linearity of g, which are not satisﬁed by general applications when neural networks are involved.
On the other hand, for many other applications, e.g., those arising from invariant learning and meta-learning, we do have access to multiple samples from the conditional distribution. Take the model-agnostic meta-learning (MAML) [19] as an example. MAML learns a meta-initialization parameter using metadata from similar learning tasks such that taking one or multiple gradient steps on a small training data would generalize well on a new task. It can be framed into the following
CSO problem: min w
Ei∼p,a∼Di query li (cid:16)
E b∼Di support (cid:17) (cid:0)w − α∇li(w, b)(cid:1), a
, (2) where p represents the distribution of different tasks, Di query correspond to support (training) data and query (testing) data of the task i, li(·, Di) is the loss function on data Di from task i, and α is a ﬁxed meta step size. Setting ξ = (i, a) and η = b, (2) is clearly a special case of
CSO for which multiple samples can be drawn from the conditional distribution of P (η|ξ). Since the loss function generally involves neural networks, the resulting CSO is often nonconvex. Thus, the previous primal-dual algorithm and kernel embedding techniques developed in [12] no longer apply. support and Di
In this paper, we focus on the general CSO problem where multiple samples from the conditional distribution P (η|ξ) are available, and the objective is not necessarily in the compositional form of a convex loss fξ(·) and a linear mapping gη(·, ξ). Recently, Hu et al. [30] studied the generalization error bound and sample complexity of empirical risk minimization (ERM), a.k.a., sample average approximation (SAA) for general CSO: min x∈X 1 n n (cid:88) fξi (cid:16) 1 m m (cid:88) (cid:17) gηij (x, ξi)
, j=1 i=1 are i.i.d. samples from P(ξ) and {ηij}m i=1 j=1 are i.i.d. samples from P(η|ξi). They as-where {ξi}n sumed that the global optimal solution to ERM can be computed without specifying how. Differently, here we aim at developing efﬁcient stochastic gradient-based methods that directly solve the CSO problem (1) and ﬁnd either a global optimal solution in the convex setting or a stationary point in the nonconvex setting, respectively.
Due to the composition structure of the CSO objective in (1), constructing unbiased gradient es-timators is not possible in general. Instead, we leverage a mini-batch of conditional samples to construct the gradient estimator with controllable bias and propose a family of biased ﬁrst-order methods, including (1) the biased stochastic gradient descent (BSGD) algorithm for general convex and nonconvex CSO objectives and (2) the biased SpiderBoost (BSpiderBoost) algorithm, designed for nonconvex smooth CSO objectives. Note that BSpiderBoost is inspired by the variance reduced method for nonconvex smooth stochastic optimization in [18, 44]. 1.1 Our contributions
Our main results are summarized in Table 1. Our contributions are three-fold:
• We establish the ﬁrst sample complexity results of BSGD and BSpiderBoost in the context of CSO. Since the bias of BSGD comes from estimating the conditional expectation rather than from a given stochastic oracle, the sample complexity closely depends on the smoothness conditions of the outer function, which is distinct from traditional SGD results. For convex problem, to achieve an (cid:15)-optimal solution, the sample complexity of BSGD improves from
O((cid:15)−4) to ˜O((cid:15)−3) when either fξ is smooth or F is strongly convex and further improves to
˜O((cid:15)−2) when both conditions hold, where ˜O(·) represents the bound with hidden logarithmic factors. For weakly convex CSO problems, BSGD requires a total sample complexity of
O((cid:15)−8) to achieve an (cid:15)-stationary point, and of O((cid:15)−6) when fξ is smooth. If we further assume that both fξ and gη are Lipschitz continuous and Lipschitz smooth, the biased gradient estimator is Lipschitz continuous, and then the sample complexity can be improved to O((cid:15)−5) by BSpiderBoost.
• We analyze the lower bounds on the minimax error of ﬁrst-order algorithms using speciﬁc biased oracles for CSO objectives. With the upper bounds results, BSGD is optimal for strongly 2
Table 1: Sample Complexity for CSO
Algorithm
ˆF fξ
SAA [30]*
BSGD
BSpiderBoost
Lower Bound
SC
SC
Lipschitz
Smooth
O((cid:15)−2) O((cid:15)−3)
˜O((cid:15)−3)
˜O((cid:15)−2)
--O((cid:15)−2) O((cid:15)−3)
WC
Smooth
-Assumptions
Convex
Lipschitz
˜O(d(cid:15)−4)
O((cid:15)−4) O((cid:15)−6) O((cid:15)−8) O((cid:15)−6)
O((cid:15)−5)
O((cid:15)−4) O((cid:15)−6) O((cid:15)−8) O((cid:15)−5)
WC
Lipschitz
-Smooth
Smooth
-Convex
Smooth
˜O(d(cid:15)−3)
O((cid:15)−3)
-O((cid:15)−3)
---Goal: ﬁnd (cid:15)-optimal solution for convex F and (cid:15)-stationary point for weakly convex F .
ˆF is deﬁned in (4). SC: strongly convex; WC: weakly convex; Lipschitz = Lipschitz continuous.
* SAA requires further solving the empirical risk minimization. convex, convex, and weakly convex CSO objectives, and BSpiderBoost is optimal for the nonconvex smooth CSO problems under the additional oracle assumption that the gradient estimator returned by the oracle is Lipschitz continuous.
• When applied to MAML, BSGD converges to a stationary point under simple deterministic stepsize rules and appropriate inner mini-batch sizes. In contrast, the commonly used ﬁrst-order
MAML algorithm [19] ignores the Hessian information and is not guaranteed to converge even when a large inner mini-batch size is used. For smooth MAML, compared with the algorithm recently introduced in [17], BSGD without requiring stochastic stepsizes and mini-batches of outer samples at each iteration, thus is more practical. Leveraging the variance reduction technique, BSpiderBoost attains the best-known sample complexity for MAML, to our best knowledge. Numerically, we further demonstrate that BSGD and BSpiderBoost achieve superior performance for MAML. 1.2