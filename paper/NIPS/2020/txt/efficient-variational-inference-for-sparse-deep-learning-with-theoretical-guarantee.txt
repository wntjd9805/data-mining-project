Abstract
Sparse deep learning aims to address the challenge of huge storage consumption by deep neural networks, and to recover the sparse structure of target functions.
Although tremendous empirical successes have been achieved, most sparse deep learning algorithms are lacking of theoretical support. On the other hand, another line of works have proposed theoretical frameworks that are computationally in-feasible. In this paper, we train sparse deep neural networks with a fully Bayesian treatment under spike-and-slab priors, and develop a set of computationally efﬁ-cient variational inferences via continuous relaxation of Bernoulli distribution. The variational posterior contraction rate is provided, which justiﬁes the consistency of the proposed variational Bayes method. Notably, our empirical results demon-strate that this variational procedure provides uncertainty quantiﬁcation in terms of Bayesian predictive distribution and is also capable to accomplish consistent variable selection by training a sparse multi-layer neural network. 1

Introduction
Dense neural network (DNN) may face various problems despite its huge successes in AI ﬁelds.
Larger training sets and more complicated network structures improve accuracy in deep learning, but always incur huge storage and computation burdens. For example, small portable devices may have limited resources such as several megabyte memory, while a dense neural networks like ResNet-50 with 50 convolutional layers would need more than 95 megabytes of memory for storage and numerous ﬂoating number computation (Cheng et al., 2018). It is therefore necessary to compress deep learning models before deploying them on these hardware limited devices.
In addition, sparse neural networks may recover the potential sparsity structure of the target function, e.g., sparse teacher network in the teacher-student framework (Goldt et al., 2019; Tian, 2018). Another example is from nonparametric regression with sparse target functions, i.e., only a portion of input variables are relevant to the response variable. A sparse network may serve the goal of variable selection (Feng and Simon, 2017; Liang et al., 2018; Ye and Sun, 2018), and is also known to be robust to adversarial samples against l8 and l2 attacks (Guo et al., 2018). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Bayesian neural network (BNN), which dates back to MacKay (1992); Neal (1992), comparing with frequentist DNN, possesses the advantages of robust prediction via model averaging and automatic uncertainty quantiﬁcation (Blundell et al., 2015). Conceptually, BNN can easily induce sparse network selection by assigning discrete prior over all possible network structures. However, challenges remain for sparse BNN including inefﬁcient Bayesian computing issue and the lack of theoretical justiﬁcation.
This work aims to resolve these two important bottlenecks simultaneously, by utilizing variational inference approach (Jordan et al., 1999; Blei et al., 2017). On the computational side, it can reduce the ultra-high dimensional sampling problem of Bayesian computing, to an optimization task which can still be solved by a back-propagation algorithm. On the theoretical side, we provide a proper prior speciﬁcation, under which the variational posterior distribution converges towards the truth. To the best of our knowledge, this work is the ﬁrst one that provides a complete package of both theory and computation for sparse Bayesian DNN.