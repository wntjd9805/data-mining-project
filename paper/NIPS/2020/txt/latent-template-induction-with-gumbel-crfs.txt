Abstract
Learning to control the structure of sentences is a challenging problem in text gener-ation. Existing work either relies on simple deterministic approaches or RL-based hard structures. We explore the use of structured variational autoencoders to infer latent templates for sentence generation using a soft, continuous relaxation in order to utilize reparameterization for training. Speciﬁcally, we propose a Gumbel-CRF, a continuous relaxation of the CRF sampling algorithm using a relaxed Forward-Filtering Backward-Sampling (FFBS) approach. As a reparameterized gradient estimator, the Gumbel-CRF gives more stable gradients than score-function based estimators. As a structured inference network, we show that it learns interpretable templates during training, which allows us to control the decoder during testing.
We demonstrate the effectiveness of our methods with experiments on data-to-text generation and unsupervised paraphrase generation. 1

Introduction
Recent work in NLP has focused on model interpretability and controllability [63, 34, 24, 55, 16], aiming to add transparency to black-box neural networks and control model outputs with task-speciﬁc constraints. For tasks such as data-to-text generation [50, 63] or paraphrasing [37, 16], interpretability and controllability are especially important as users are interested in what linguistic properties – e.g., syntax [4], phrases [63], main entities [49] and lexical choices [16] – are controlled by the model and which part of the model controls the corresponding outputs.
Most existing work in this area relies on non-probabilistic approaches or on complex Reinforce-ment Learning (RL)-based hard structures. Non-probabilistic approaches include using attention weights as sources of interpretability [26, 61], or building specialized network architectures like entity modeling [49] or copy mechanism [22]. These approaches take advantages of differentiability and end-to-end training, but does not incorporate the expressiveness and ﬂexibility of probabilis-tic approaches [45, 6, 30]. On the other hand, approaches using probabilistic graphical models usually involve non-differentiable sampling [29, 65, 34]. Although these structures exhibit better interpretability and controllability [34], it is challenging to train them in an end-to-end fashion.
In this work, we aim to combine the advantages of relaxed training and graphical models, focusing on conditional random ﬁeld (CRF) models. Previous work in this area primarily utilizes the score function estimator (aka. REINFORCE) [62, 52, 29, 32] to obtain Monte Carlo (MC) gradient estimation for simplistic categorical models [44, 43]. However, given the combinatorial search space, these approaches suffer from high variance [20] and are notoriously difﬁcult to train [29].
Furthermore, in a linear-chain CRF setting, score function estimators can only provide gradients for the whole sequence, while it would be ideal if we can derive ﬁne-grained pathwise gradients [44] for each step of the sequence. In light of this, naturally one would turn to reparameterized estimators with pathwise gradients which are known to be more stable with lower variance [30, 44].
∗Work done during an internship at Alibaba DAMO Academy, in collaboration with PKU and Cornell. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our simple approach for reparameterizing CRF inference is to directly relax the sampling process itself. Gumbel-Softmax [27, 38] has become a popular method for relaxing categorical sampling.
We propose to utilize this method to relax each step of CRF sampling utilizing the forward-ﬁltering backward-sampling algorithm [45]. Just as with Gumbel-Softmax, this approach becomes exact as temperature goes to zero, and provides a soft relaxation in other cases. We call this approach
Gumbel-CRF. As is discussed by previous work that a structured latent variable may have a better inductive bias for capturing the discrete nature of sentences [28, 29, 16], we apply Gumbel-CRF as the inference model in a structured variational autoencoder for learning latent templates that control the sentence structures. Templates are deﬁned as a sequence of states where each state controls the content (e.g., properties of the entities being discussed) of the word to be generated.
Experiments explore the properties and applications of the Gumbel-CRF approach. As a reparameter-ized gradient estimator, compared with score function based estimators, Gumbel-CRF not only gives lower-variance and ﬁne-grained gradients for each sampling step, which leads to a better text model-ing performance, but also introduce practical advantages with signiﬁcantly fewer parameters to tune and faster convergence (§ 6.1). As a structured inference network, like other hard models trained with
REINFORCE, Gumbel-CRF also induces interpretable and controllable templates for generation. We demonstrate the interpretability and controllability on unsupervised paraphrase generation and data-to-text generation (§ 6.2). Our code is available at https://github.com/FranxYao/Gumbel-CRF. 2