Abstract
Research in adversarial learning follows a cat and mouse game between attackers and defenders where attacks are proposed, they are mitigated by new defenses, and subsequently new attacks are proposed that break earlier defenses, and so on.
However, it has remained unclear as to whether there are conditions under which no better attacks or defenses can be proposed. In this paper, we propose a game-theoretic framework for studying attacks and defenses which exist in equilibrium.
Under a locally linear decision boundary model for the underlying binary classiﬁer, we prove that the Fast Gradient Method attack and a Randomized Smoothing defense form a Nash Equilibrium. We then show how this equilibrium defense can be approximated given ﬁnitely many samples from a data-generating distribution, and derive a generalization bound for the performance of our approximation. 1

Introduction
Neural network classiﬁers have been shown to be vulnerable to additive perturbations to the input, which can cause an anomalous change in the classiﬁcation output. There are several attack methods to compute such perturbations for any input instance which assume access to the model gradient information, e.g., Fast Gradient Sign Method [12] and Projected Gradient Method [24]. In response to such additive attacks, researchers have proposed many additive defense methods with varying levels of success, e.g., Randomized Smoothing [7]. However, it has been later discovered that a lot of these defenses are in turn susceptible to further additive attacks handcrafted for the particular defenses. This back and forth where attacks are proposed breaking previous defenses and then further defenses are proposed mitigating earlier attacks has been going on for some time in the community and there are several open questions to be answered. Can all defenses be broken, or do there exist defenses for which we can get provable guarantees? Similarly, does there always exist a defense against any attack, or are there attacks with provable performance degradation guarantees? Do there exist scenarios under which attackers always win, and similarly scenarios where defenders always win? Are there conditions under which an equilibrium between attacks and defenses exist?
In this work, we answer some of these questions in the afﬁrmative in a binary classiﬁcation setting with locally linear decision boundaries. Speciﬁcally, we ﬁnd a pair of attack A and defense D such that if the attacker uses A, then there is no defense which can perform better than D, and vice versa.
Our approach can be seen as a novel way to obtain both provable attacks and provable defenses, complementing recent advances on certiﬁable defenses in the literature ([37, 4, 15, 9, 35, 14, 19, 22]).
To summarize, our contributions are as follows: 1. We introduce a game-theoretic framework for studying equilibria of attacks and defenses on an underlying binary classiﬁer. In order to do so, we ﬁrst specify the capabilities, i.e., the changes the attacker and defender are allowed to make to the input, the amount of knowledge the attacker and the defender have about each other, and formalize the strategies that they can follow. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2. We show that the Fast Gradient Method attack and a Randomized Smoothing defense 1 form a
Nash Equilibrium under the assumption of a zero-sum game with locally linear decision boundary for the underlying binary classiﬁer and full knowledge of the data-generating distribution. 3. We propose an optimization-based method to approximate the optimal defense given access to a ﬁnite training set of n independent samples and we derive generalization bounds on the performance of this ﬁnite-sample approximation. Our bounds show that the approximation approaches the optimal defense at a fast rate of O((cid:112)log n/n) with the number of samples n.
The rest of the paper is organized as follows: In Section 2 we describe the proposed game theoretic setup. In Section 3 we state our main result showing the existence of an optimal attack and defense.
This is followed by Section 4 where we propose an optimization method to approximate the optimal defense, and provide some experiments validating our methods and models. Section 4 presents a generalization analysis showing that the proposed approximation approaches the optimal defense at a fast rate. Finally we conclude in Section 6 by putting our work into perspective with related work. 2 A Game Theoretic Setup for Additive Adversarial Attacks and Defenses
We will denote a random variable with an upper-case letter, e.g., X, and a realization of a random variable with a lower-case letter, e.g., x. We will consider a binary classiﬁcation task with data distribution pX deﬁned over the input space X ⊂ Rm. The true decision boundary corresponding to the discrete labels {−1, +1} will be deﬁned by the zero-contour of a classiﬁer f : X → R, i.e.,
{x : f (x) = 0}, and the label of each data point x ∈ X will be given by sgn(f (x)).
We will deﬁne a two-player, single-shot, simultaneous, zero-sum game between an attacker A and a defender D. In this setting the attacker and defender are allowed to simultaneously make additive perturbations a(x) and d(x), respectively, for a given a data point x, i.e., both A and D submit their perturbation at the same time and the perturbed point is x+a(x)+d(x). The size of each perturbation is limited to be at most (cid:15) in (cid:96)2 norm, i.e., for all x ∈ X , a(x), d(x) ∈ V , where V := {v : (cid:107)v(cid:107)2 ≤ (cid:15)}.
The score uA assigned to A is determined by whether the label of the data point x has changed under a locally linear approximation of f around x after both the perturbations a(x) and d(x) are applied: uA(x, a(x), d(x)) = (cid:26)+1
−1 if sgn(fL(x)) (cid:54)= sgn(fL(x + a(x) + d(x))) otherwise. (1)
In the above, fL(x(cid:48)) = f (x)+∇f (x)(cid:62)(x(cid:48)−x) is a linear approximation of f in the 2(cid:15) neighbourhood of x. Similarly, the score uD assigned to D is deﬁned as the negative of the utility assigned to A, i.e., uD(x, a, d) = −uA(x, a, d) for all (x, a, d), thus making the game zero-sum.
Note that the locally linear model as-sumption holds whenever the data dis-tribution places no mass in regions of space that are less than 2(cid:15) distance away from curved parts of the deci-sion boundary. Concretely, for a neu-ral network with ReLU activations, the assumption holds whenever none of the data points lie very close to the intersection of 2 or more hyper-planes which make up the classiﬁca-tion boundaries. This is reasonable as such regions form a set of measure zero in the input space. We refer the reader to Sec. E of the Appendix for a more detailed discussion about the validity of such locally-ﬂat boundary assumptions for deep neural networks.
A deterministic strategy for the at-tacker consists of choosing a function
Figure 1: The decision boundary is given by the dashed line. In order for our locally linear modeling assumption to hold, data-points should lie anywhere except in the red regions, i.e., within 2(cid:15) distance to a half-plane intersection. The green shaded regions show the geometry of the robust sets R(x), which follows from
Lemma 1. Observe that R(x) becomes larger as x moves farther away from the decision boundary. 1Note that our randomized smoothing defense does not sample from an isotropic gaussian distribution. 2
a : X → V which dictates the perturbation a(x) that is made by A for the point x ∈ X . Hence, the action space, i.e., the set of all deterministic strategies that can be followed by the attacker, is the function set AA = {a|a : X → V }.
A deterministic strategy for the defender consists of the set of all constant functions, i.e., functions which take the same perturbation direction for each point x ∈ X . Hence the action space AD for the defender consists of the function set AD = {dv|v ∈ V, dv : X → V s.t. ∀x ∈ X , dv(x) = v}.
Since each deterministic strategy for the defender can be uniquely with a point v ∈ V , the reader can think of AD as V for ease of understanding. The reasons for constraining the set of strategies for the defender are twofold: ﬁrst, we want to model existing literature in adversarial attacks where defenders typically follow a single strategy (e.g., smooth input, quantize input) agnostic of the test data point.
Second, this restriction captures the fact that defenders are typically given the adversarially perturbed input x + a(x) and are expected to ﬁx it without knowing the original label or original data point x (and hence have to use a strategy that is agnostic to the relation between the data point and its label).
In reality, attackers and defenders can choose even randomized strategies, where the function a or d is sampled according to some probability density on the set of allowed deterministic strategies. Such a randomized strategy sA for the attacker is speciﬁed by a density pA ∈ P(AA), where we deﬁne
P(AA) to be the set of all probability densities over AA. Similarly, a randomized strategy sD for the defender is speciﬁed by a density pD ∈ P(AD). Given a pair of strategies (sA, sD), we deﬁne the utility function of the attacker, ¯uA : P(AA) × P(AD) → R as follows:
¯uA(sA, sD) =
E x∼pX ,a∼pA,d∼pD uA(x, a(x), d(x)) (2)
Note that since the game is zero-sum, the corresponding utility function ¯uD for the defender is just the negative of that for the attacker. To complete the setup, we specify that both the attacker and defender have perfect knowledge about the (possibly randomized) strategy that the other follows, and have access to the linear approximation fL around any data point x. In summary, we have a white-box evasion attack scenario and aim to analyze preprocessing attacks and defenses (i.e. they preprocess the input before they are fed to the classiﬁer) that make additive perturbations to the input.
In the following section, we will further assume both A and D have access to the full data-generating distribution pX in order to derive the optimal attack and defense strategy. We will show later that this assumption is not needed in practice by constructing an approximation given ﬁnitely many samples from pX . We will then prove that this approximation approaches the true optimum at a fast rate. 3 A Characterization of Optimal Attack and Defense Strategies
In this section, we will show in Theorem 1 that the FGM attack and Randomized Smoothing defense exist in a Nash Equilibrium, i.e., if A follows the strategy deﬁned by the FGM attack then D cannot do better than following a randomized smoothing strategy, and vice-versa. In order to show this, we will ﬁrst establish in Lemma 2 that the FGM attack is the best response to any defense in our setting, and then show in Lemma 3 that randomized smoothing is a best response to FGM. Together, these lemmas would lead to our main result. Before stating our results, we begin by deﬁning the robust set.
Deﬁnition 1 (Robust set). The robust set R(x) = {v : v ∈ V s.t. ∀v(cid:48) ∈ V uA(x, v(cid:48), v) = −1} of a point x ∈ X is the subset of allowed perturbations v ∈ V such that if the defender plays v at x, then the attacker always gets an utility of −1, i.e., the least possible utility, no matter what she plays.
Lemma 1 shows that R(x) is the intersection of a half-plane with V , as illustrated by the green shaded regions in Fig. 1. Observe that when x is far from the decision boundary, the robust set is equal to V .
Lemma 1 (Geometry of the robust set). For any x ∈ X , the robust set R(x) is given by
R(x) = {v : sgn(f (x))(f (x) + ∇f (x)(cid:62)v) − (cid:15)(cid:107)∇f (x)(cid:107) ≥ 0} ∩ {v : (cid:107)v(cid:107)2 ≤ (cid:15)}. (3)
Interestingly, the proof of the lemma shows that for a ﬁxed x, in order for v to achieve uA(x, v(cid:48), v) =
−1 for all v(cid:48) ∈ V , it is sufﬁcient to ensure that uA(x, v(cid:48), v) = −1 when v(cid:48) is chosen according to the Fast Gradient Method (FGM). The FGM attack was proposed in [12] and makes the additive perturbation aFGM(x) = −(cid:15) sgn(f (x))
∇f (x). Note that the original attack was called Fast Gradient (cid:107)∇f (x)(cid:107)2
Sign Method, as it was derived for (cid:96)∞ bounded perturbations. The same attack for (cid:96)2 bounded perturbations is called the FGM attack. Since this attack does not involve any randomness, the strategy sFGM followed by the attacker in our framework places probability 1 on the function aFGM. 3
Lemma 2 (FGM is a best-response to any defense). For any strategy sD ∈ P(AD) played by the defender D, the strategy sFGM ∈ P(AA) played by the attacker A achieves the largest possible utility against sD, i.e., ¯uA(sFGM, sD) ≥ ¯uA(sA, sD) for all sA ∈ P(AA).
Having established the optimality of the FGM attack, we now turn our attention to ﬁnding an optimal defense strategy. To that end, we deﬁne an instance of the Randomized Smoothing defense proposed in [7] that will be shown to be a best-response to FGM. For any perturbation v ∈ V , we deﬁne φ(v) to be the measure of the set of points in X whose robust sets cointain v:
φ(v) = (cid:90)
X 1[R(x) (cid:51) v]pX (x)dx (4)
To deﬁne sSMOOTH we need to specify the probability distribution pSMOOTH ∈ P(AD). The idea is to sample uniformly from the set of maximizers of φ, i.e., V ∗ = {v∗ : φ(v∗) ≥ φ(v), ∀v ∈ V }.
Accordingly, our defense strategy sSMOOTH samples from a uniform distribution over the set of functions F ∗ = {dv∗ : v∗ ∈ V ∗} ⊆ AD, where recall dv : X → V is the constant function deﬁned as ∀x ∈ X dv(x) = v.
Lemma 3 (Randomized Smoothing is a best-response to FGM). The strategy sSMOOTH achieves the largest possible utility for the defender against the attack sFGM played by the attacker, i.e., for any defense sD ∈ P(AD) we have ¯uD(sFGM, sSMOOTH) ≥ ¯uD(sFGM, sD).
Note that Lemma 2 is a stronger result than we need for our further analysis, and we will only be using the following implication of Lemma 2:
Corollary 2.1 (FGM is a best-response to Randomized Smoothing). The strategy sFGM ∈ i.e.,
P(AA) played by the attacker A achieves the largest possible utility against sSMOOTH,
¯uA(sFGM, sSMOOTH) ≥ ¯uA(sA, sSMOOTH) for all sA ∈ P(AA).
As a consequence of Corollary 2.1 and Lemma 3 we have established our main result, as follows:
Theorem 1 ((FGM, Randomized Smoothing) form a Nash Equilibrium). Neither player gains utility by unilaterally deviating when A plays sFGM and D plays sSMOOTH, i.e., ∀sA ∈
P(AA), sD ∈ P(AD), we have ¯uA(sFGM, sSMOOTH) ≥ ¯uA(sA, sSMOOTH) and ¯uD(sFGM, sSMOOTH) ≥
¯uD(sFGM, sD).
Implications. At this point we will pause to note some implications of Theorem 1. 1. Theoretical insight: First, Theorem 1 gives us a new theoretical insight into randomized-smoothing.
Speciﬁcally, one should select the smoothing distribution according to the classiﬁer f to obtain an optimal defense, instead of sampling from the rotationally symmetric Gaussian distribution, which completely ignores the effect of the classiﬁer. 2. Provable attacks: Second, Theorem 1 shows that in some settings some attacks are optimal in the sense that they will perform better than any alternative regardless of the defense that is employed.
This motivates the study of provable attacks, something that has been largely ignored by the community which focusses a lot on provable defenses. 3. Winner takes all: Third, we see from the proofs of Lemmas 2 and 3 that the equilibrium utility obtained by the equilibrium attacker is 1 − 2φ(v∗), which shows that whenever the classiﬁcation boundaries are such that φ(v∗) = 0, the attacker will always win, i.e., obtain a utility of 1 over the entire dataset. Similarly, the utility obtained by the equilibrium defender is 2φ(v∗) − 1, meaning that the defender wins completely whenever the classiﬁcation boundaries are such that φ(v∗) = 1.
Relating this to the widely used metric robust accuracy, the above is a characterization of cases where the robust accuracy obtained by the best possible defense is 0% and 100% respectively.
As we saw in this section, we need access to the full data-generating distribution pX in order to compute ssmooth, which is an unreasonable assumption in practice. Hence, in the following section we will demonstrate how one can approximate ssmooth given access to ﬁnitely many samples from pX . 4 Approximation Properties: How to compute the optimal defense?
As we saw in Section 3, the optimal defense relies on the knowledge of the subset of perturbation directions that maximize φ, V ∗, which in turns depends on the distribution pX of the input data. 4
Given n i.i.d. samples X1, X2, . . . , Xn ∼ pX , we deﬁne the ﬁnite-sample approximation of φ as:
φn(v) = 1 n n (cid:88) i=1 1[v ∈ R(Xi)]. (5)
A straightforward modiﬁcation of the proof of Lemma 3 shows that the same result can be obtained even if psmooth places all its mass on a single element of V ∗. That is, an optimal defense can also be achieved by a deterministic strategy. Hence, our goal will now be to solve the following problem: v∗ n = max v
φn(v) subject to (cid:107)v(cid:107)2 ≤ (cid:15). (6)
Recall from Lemma 1 that the robust set at xi can be written as R(xi) = {v ∈ V : c(cid:62) i v + bi ≥ 0}, where ci = sgn(f (xi))∇f (xi) and bi = |f (xi)| − (cid:15)(cid:107)∇f (xi)(cid:107). Therefore, we obtain the following equivalent version of Eq. (6): max v 1 n n (cid:88) i=1 1[c(cid:62) i v + bi ≥ 0] subject to (cid:107)v(cid:107)2 ≤ (cid:15). (7)
Observe that the objective in (7) takes values in {0, 1 n , . . . , 1} and it is equal to 1 iff there is a v with (cid:107)v(cid:107) ≤ (cid:15) such that c(cid:62) i v + bi ≥ 0 for all i = 1, . . . , n. Trivially, this happens if bi ≥ 0 for all i = 1, . . . , n, in which case we can choose v = 0, meaning that no defense is needed. By inspection, we see that bi ≥ 0 when |f (xi)| is large, meaning that the classiﬁer is conﬁdent about its prediction, and (cid:107)∇f (xi)(cid:107) is small, meaning that the response of the classiﬁer is not very sensitive to input perturbations. This is very consistent with our intuition that defenses are needed when the classiﬁer is not very conﬁdent (small |f (xi)|) or its response is sensitive to input perturbations (large (cid:107)∇f (xi)(cid:107)). n , 2
To solve the optimization problem in (7), consider the case where there is a single sample, i.e., n = 1.
In this case, if the half-space H = {v : c(cid:62) 1 v +b1 ≥ 0} does not intersect the hypersphere B(0, (cid:15)), then any v∗ ∈ V is a solution to (7) with φ(v∗) = 0. Else, if H intersects the hypersphere, a solution is given by the projection of the origin onto H, which gives φ(v∗) = 1 (see left panel of Fig. 2 for illustration). When n = 2, there are up to two hyperplanes, which divide the space into at most 4 regions. When no half-space intersects the hypersphere, any v∗ ∈ V is an optimal solution and
φ(v∗) = 0. When only one half-space intersects the hypersphere, as before an optimal solution is given by the projection of the origin onto the half-space, which gives φ(v∗) = 1/2. When both half-spaces intersect the hypersphere, but the half-spaces intersect each other outside the hypersphere, v∗ can be the projection of the origin onto either half-space. It is only when both half-spaces intersect inside the hypersphere we have φ(v∗) = 1 and a maximizer is given by the projection of the origin onto the intersection of both half-spaces (see right panel of Fig. 2 for illustration). However, as n increases, the number of regions grows exponentially in n, rendering such a direct region-enumeration intractable. We thus follow an optimization-based approach to ﬁnd an approximate maximizer of (7).
More speciﬁcally, we use projected gradient descent on v with the constraint set (cid:107)v(cid:107)2 ≤ (cid:15) to solve the optimization problem in (7) and obtain (cid:98)v∗ n. Additionally, as the gradients of the indicator function are not very useful, we use the relaxation 1[α ≥ 0] ≥ min(max(0, α), 1) and optimize the RHS.
Our experiments are conducted on the MNIST and FMNIST datasets restricted to two classes.
We train a 4-layer convolutional neural network with ReLU activation functions for this binary classiﬁcation task. The classiﬁcation results are shown in Table 1, from which we can draw two main conclusions: (1) If the defender uses the equilibrium defense, then the attacker gets the most reduction in approximate accuracy 2 when using the equilibrium attack, as using any other attack improves the performance of the defended classiﬁer. A similar statement holds from the
Table 1: Mean (Variance) of the approximate accuracy computed over binary classiﬁcations tasks on MNIST and FMNIST corresponding to (cid:0)10 (cid:1) pairs of classes. 2
Attack Defense MNIST (%) FMNIST (%)
--99.9 (0.0)
-FGM 53.3 (10.0)
FGM SMOOTH 71.2 (14.2)
PGD 71.9 (12.0)
PGD SMOOTH 94.0 (4.0)
-99.9 (0.1) 47.4 (5.1) 67.4 (9.0) 74.7 (7.3) 90.3 (8.5) 2Approximate Accuracy is deﬁned as the accuracy of the model obtained by linearizing the decision boundary in a 2(cid:15)-ball around data-points, thus satisfying our modelling assumption. A detailed description, as well as more experimental details can be found in Sec. D of the Appendix. 5
other side. (2) The equilibrium defense SMOOTH leads to signiﬁcant gains in approximate accuracy against both FGM and PGD, in agreement with our result that SMOOTH is optimal when the decision boundaries satisfy our model. 5 Generalization Properties of the Approximation to the Optimal Defense
In the previous section, we saw how to practically approximate the optimal defense as v∗ n given access to a ﬁnite number of samples drawn i.i.d. from the data distribution pX . But how good is this approximation? In this section, we derive generalization bounds showing that φ(v∗ n) approaches φ(v∗) at a fast rate w.r.t. n. Before proceeding, we review some necessary results from learning theory.
Learning theory review. A function h : Rm ×. . .×Rm → R is said to satisfy the bounded difference assumption if for all 1 ≤ i ≤ n there exists ﬁnite ci ∈ R such that: sup x1,x2,...,xi,...xn,x(cid:48) i∈Rm
|h(x1, x2, . . . , xi, . . . , xn) − h(x1, x2, . . . , x(cid:48) i, . . . , xn)| ≤ ci. (8)
In other words, the bounded difference assumption states that h changes by at most a ﬁnite amount if any of the individual inputs are changed, while keeping all others constant. Now, let h be a function satisfying the bounded difference assumption, and X1, . . . , Xn ∼ pX be i.i.d. random variables.
Then, h satisﬁes the following useful property called the McDiarmid’s inequality, which shows that that the function values of h are tightly concentrated around the mean: (cid:104)
|h(X1, . . . , Xn) −
Pr
E
X1,...,Xn∼pX (cid:105) h(X1, . . . , Xn)| > (cid:15)
≤ exp (cid:110) −2(cid:15)2 (cid:80) i c2 i (cid:111)
. (9)
Next we need some results from Vapnik-Chervonenkis Theory. Let X1, . . . , Xn ∼ pX be i.i.d. random variables each taking values in Rm. Let B be a family of subsets of Rm. Let B ∈ B be any subset in the family. Deﬁne µ as µ(B) = Pr[X1 ∈ B]. Further, given a particular realization x1, . . . , xn of X1, . . . Xn, deﬁne the ﬁnite-sample approximation µn as µn(B) = 1 i=1 1[Xi ∈ n
B]. In other words, µ(B) is the probability that a sample from pX lies in B, and µn(B) estimates this probability using n samples from D. Taking h(x1, . . . , xn) = supB∈B |µn(B) − µ(B)| in
McDiarmid’s inequality, we observe that ci = 1 (cid:80)n n and we get the following: (cid:12) (cid:105) (cid:12) (cid:12) > (cid:15)
|µn(B) − µ(B)|
≤ exp (cid:110)
− 2n(cid:15)2(cid:111)
. (10)
Pr (cid:104)(cid:12) (cid:12) (cid:12) sup
B∈B
|µn(B) − µ(B)| −
E
X1,...,Xn∼pX sup
B∈B
In other words, we see that the maximum inaccuracy incurred in estimating Pr[X1 ∈ B] from ﬁnite samples is tightly concentrated around the mean. The ﬁnal piece we need from VC theory is an upper bound on this mean inaccuracy:
E
X1,...,Xn∼pX sup
B∈B (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ 2 (cid:12)µn(B) − µ(B) (cid:114) 2 log SB(n) n
, where SB(n) is the shatter coefﬁcient for the family B, which is deﬁned as follows:
SB(n) = sup x1,x2,...,xn∈Rm (cid:12) (cid:12) (cid:12) (cid:110)
{x1, x2, . . . , xn} ∩ B : B ∈ B (cid:111)(cid:12) (cid:12) (cid:12). (11) (12)
In the above, each of the terms being considered in the supremum counts the number of distinct intersections with members of B. For illustration, say we are working in R2, and let B be the family of subsets generated by taking each rectangle r in the plane and considering Br to be the points contained in r. Now given any 3 points {x1, x2, x3} in the plane, we can ﬁnd rectangles r such that
{x1, x2, x3} ∩ Br equals each of the 8 possible subsets {}, {x1}, {x2}, {x1, x2}, . . . , {x1, x2, x3}.
This shows that SB(3) ≥ 8 (which implies SB(3) = 8 as SB(n) ≤ 2n).
In other words, the shatter coefﬁcient at n equals the largest p such that n points can be broken into p subsets by members of B. Hence, SB(n) ≥ p implies there exists at least one such example of x1, . . . , xn that can be broken into p subsets. On the other hand, SB(n) < p + 1 implies that for every choice of n points, they cannot be broken into p + 1 or more distinct sets by members in B.
Generalization bound. We will now apply the above literature to our setup. Going forward, we will think of n as being the size of the entire training data that is available to us. Recall that in our 6
setting, we are given a ﬁxed base classiﬁer f . Given f , and a sample x1, . . . , xn, we know the robust sets R(x1), R(x2), . . . , R(xn). For each direction that can be taken by the defender, i.e., v ∈ V , we deﬁne Bv to be the subset of X for which v is a robust direction as:
Bv = {x ∈ X : v ∈ R(x)}. (13)
Now we can deﬁne the family of subsets B = {Bv : v ∈ V }. With this deﬁnition, we can see that φ corresponds to µ, and φn corresponds to µn as:
φ(v) := µ(Bv) = Pr[X1 ∈ Bv] = Pr[v ∈ R(X1)]
φn(v) := n (cid:88) i=1 1[v ∈ R(xi)] = µn(Bv). (14) (15)
Given the base classiﬁer f and the training data, we used an optimization algorithm to obtain the maximizer of φn(v) in Section 4. We will assume for the purposes of this section that there is no optimization error, i.e., our optimizer ﬁnds the best direction for the given training data, i.e., n = arg maxv φn(v). We are interested in the difference between φ(v∗ v∗ n) and φ(v∗) , i.e.:
φ(v∗) − φ(v∗ n) =
φ(v∗) − φn(v∗ n) (cid:17) (cid:16)
+
φn(v∗ (cid:17) n) − φ(v∗ n) n)|
≤ |φ(v∗) − φn(v∗)| + |φn(v∗
|φ(v) − φn(v)| + sup
≤ sup n) − φ(v∗
|φn(v) − φ(v)| = 2 sup v v (cid:16) v (16) (17) (18)
|φ(v) − φn(v)|.
In other words, we can get an upper bound on the quantity of interest by analysing supv |φ(v)−φn(v)|, which is the largest inaccuracy we get due to estimating φ(v) from a ﬁnite number of samples. (10) shows that this quantity is sharply concentrated at its mean, which can be upper bounded by (11) as:
E
X1,...,Xn∼pX sup v∈V (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ 2 (cid:12)φn(v) − φ(v) (cid:114) 2 log SB(n) n
. (19)
Hence, the problem boils down to getting an upper bound on SB(n). For families where we can obtain a bound that is sub-exponential in n, we can see that the RHS converges to 0 as n becomes large. Hence, we will now upper-bound the shatter coefﬁcient SB(n) for our setting.
Recall that we approximate the base classiﬁer f around each data point x using a linear approximation fL(x(cid:48)) = f (x) + ∇f (x)(cid:62)(x(cid:48) − x). We have seen that the robust set is the region enclosed between a half-plane and the boundary of the set V (see Fig. 1 and Lemma 1).
Figure 2: We can compute an upper bound to SB by looking at the different regions formed by the robust sets
R(xi). The rightmost panel shows the superimposition of R(x1), R(x2), R(x3), showing the different subsets formed.
We now want to upper-bound the maximum number of different partitions of {x1, x2, . . . , xn} that can be formed by taking subsets speciﬁed by Bv for v ∈ V . Observe that overlaying all the robust sets in V gives us a collection of regions, with the property that Bv ∪ {x1, x2, . . . , xn} is constant when v is varied inside any region, as shown in Fig. 2.
This implies that an upper bound on SB(n) is equal to the number of distinct regions formed. When we are in the 2-dimensional case, this is same as the number of regions formed by n lines in a plane, which is known to be (n2 + n + 2)/2 = O(n2). For higher dimensions m, the maximum possible number of regions grows as O(nm). Thus the upper bound given by (19) reduces to:
E
X1,...,Xn∼pX sup v∈V (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ≤ 4 (cid:12)φn(v) − φ(v) (cid:114) m log n n
. (20) 7
The above shows that as n gets larger, i.e., we take more and more samples (m is a constant), we approach the best defense at a fast rate of O((cid:112)log n/n). This bound indicates that efﬁcient learning from ﬁnite samples is possible. This is corroborated by our experiments, which show even faster rates. We thus believe that our generalization analysis can be sharpened by using recent advances in
PAC-Bayesian learning theory, as well as modern extensions to VC-Theory. 6 Conclusion,