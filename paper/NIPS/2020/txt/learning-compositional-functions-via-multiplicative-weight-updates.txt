Abstract
Compositionality is a basic structural feature of both biological and artiﬁcial neural networks. Learning compositional functions via gradient descent incurs well known problems like vanishing and exploding gradients, making careful learning rate tuning essential for real-world applications. This paper proves that multiplicative weight updates satisfy a descent lemma tailored to compositional functions. Based on this lemma, we derive Madam—a multiplicative version of the Adam optimiser—and show that it can train state of the art neural network architectures without learning rate tuning. We further show that Madam is easily adapted to train natively compressed neural networks by representing their weights in a logarithmic number system. We conclude by drawing connections between multiplicative weight updates and recent ﬁndings about synapses in biology. 1

Introduction
Neural computation in living systems emerges from the collective behaviour of large numbers of low precision and potentially faulty processing elements. This is a far cry from the precision and reliability of digital electronics. Looking at the numbers, a synapse on a computer is often represented using 32 bits taking more than 4 billion distinct values. In contrast, a biological synapse is estimated to take 26 distinguishable strengths requiring only 5 bits to store [1]. This is a discrepancy between nature and engineering that spans many orders of magnitude. So why does the brain learn stably whereas deep learning is notoriously ﬁnicky and sensitive to myriad hyperparameters?
Meanwhile, an industrial effort is underway to scale artiﬁcial networks up to run on supercomputers and down to run on resource-limited edge devices. While learning algorithms designed to run natively on low precision hardware could lead to smaller and more power efﬁcient chips [2, 3], progress is hampered by our poor understanding of how precision impacts learning. As such, existing numerical representations have developed somewhat independently from learning algorithms. The next generation of neural hardware could beneﬁt from more principled algorithmic co-design [4].
Our contributons: 1. Building on recent results in the perturbation analysis of compositional functions [5], we show that a multiplicative learning rule satisﬁes a descent lemma tailored to neural networks. 2. We propose and benchmark Madam—a multiplicative version of the Adam optimiser.
Empirically, Madam seems to not require learning rate tuning. Further, it may be used to train neural networks with low bit width synapses stored in a logarithmic number system. 3. We point out that multiplicative weight updates respect certain aspects of neuroanatomy.
First, synapses are exclusively excitatory or inhibitory since their sign is preserved under the update. Second, multiplicative weight updates are most naturally implemented in a logarithmic number system, in line with anatomical ﬁndings about biological synapses [1]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
2