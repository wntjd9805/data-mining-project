Abstract
In this paper we study the problem of escaping from saddle points and achieving second-order optimality in a decentralized setting where a group of agents collabo-rate to minimize their aggregate objective function. We provide a non-asymptotic (ﬁnite-time) analysis and show that by following the idea of perturbed gradient descent, it is possible to converge to a second-order stationary point in a number of iterations which depends linearly on dimension and polynomially on the accuracy of second-order stationary point. Doing this in a communication-efﬁcient manner requires overcoming several challenges, from identifying (ﬁrst order) stationary points in a distributed manner, to adapting the perturbed gradient framework with-out prohibitive communication complexity. Our proposed Perturbed Decentralized
Gradient Tracking (PDGT) method consists of two major stages: (i) a gradient-based step to ﬁnd a ﬁrst-order stationary point and (ii) a perturbed gradient descent step to escape from a ﬁrst-order stationary point, if it is a saddle point with sufﬁ-cient curvature. As a side beneﬁt of our result, in the case that all saddle points are non-degenerate (strict), the proposed PDGT method ﬁnds a local minimum of the considered decentralized optimization problem in a ﬁnite number of iterations. 1

Introduction
Recently, we have witnessed an unprecedented increase in the amount of data that is gathered in a distributed fashion and stored over multiple agents (machines). Moreover, the advances in data-driven systems such as Internet of Things, health-care, and multi-agent robotics demand for developing machine learning frameworks that can be implemented in a distributed manner. Simultaneously, convex formulations for training machine learning tasks have been replaced by nonconvex repre-sentations such as neural networks. These rapid changes call for the development of a class of communication-efﬁcient algorithms to solve nonconvex decentralized learning problems.
In this paper, we focus on a nonconvex decentralized optimization problem where a group of m agents collaborate to minimize their aggregate loss function, while they are allowed to exchange information only with their neighbors. To be more precise, the agents (nodes) aim to solve min x∈Rd f (x) = 1 m m (cid:88) i=1 fi(x), (1) where fi : Rd → R is the objective function of node i which is possibly nonconvex. Finding the global minimizer of this problem, even in the centralized setting where all the functions are available at a single machine, is hard. Given this hardness result, we often settle for ﬁnding a stationary point of Problem (1). There have been several lines of work on ﬁnding an approximate ﬁrst-order 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
stationary point of this distributed problem, i.e., ﬁnding a set of local solutions ˜x1, . . . , ˜xm where their average ˜xavg has a small gradient norm (cid:107)∇f (˜xavg)(cid:107) and a small consensus error (cid:80)m i=1 (cid:107)˜xi − ˜xavg(cid:107).
Achieving ﬁrst-order optimality, however, in nonconvex settings may not lead to a satisfactory solution as it could be a poor saddle point. Therefore, ﬁnding a second-order stationary point could improve the quality of the solution. In fact, when all saddle points are non-degenerate ﬁnding a second-order stationary point implies convergence to a local-minimum, and in several problems including matrix completion [1], phase retrieval [2], and dictionary learning [3] local minima are global minima.
While convergence to a second-order stationary point for the centralized setting has been extensively studied in the recent literature, the non-asymptotic complexity analysis of ﬁnding such a point for decentralized problems (under standard smoothness assumptions) has thus far evaded solution, in part because of signiﬁcant additional challenges presented by communication limitations. A major difference between the centralized and the decentralized framework lies in the exchange of information between the nodes. Exchanging Hessian information is, of course, prohibitively expensive.
Furthermore, turning to approximating schemes has the potential to create catastrophic problems for the algorithm, as small errors in approximation across the nodes could lead to inconsistent updates that could reverse progress made by prior steps. Moreover, escaping from ﬁrst-order stationary points requires identifying that the algorithm has reached such a point, and accomplishing even this basic step in a communication-efﬁcient manner presents challenges.
Contributions. In this paper we develop a novel gradient-based method for escaping from saddle points in a decentralized setting and characterize its overall communication cost for achieving a second-order stationary point. The proposed Perturbed Decentralized Gradient Tracking (PDGT) algorithm consists of two major steps: (i) A local decentralized gradient tracking scheme to ﬁnd a ﬁrst-order stationary point, while maintaining consensus by averaging over neighboring iterates; (ii) A perturbed gradient tracking scheme to escape from saddle points that are non-degenerate. We show that to achieve an ((cid:15), γ, ρ)-second-order stationary point (see Deﬁnition 2) the proposed PDGT f (x0)−f ∗ algorithm requires at most ˜Θ (1−σ)2 min{(cid:15)2,ρ2}γ3 , d rounds of communication, where
γ6 d is dimension, f (x0) is the initial objective function value, f ∗ is the optimal function value, and
σ is the second largest eigenvalue of mixing matrix in terms of absolute norm which depends on the connectivity of the underlying graph. To the best of our knowledge, this result provides the ﬁrst non-asymptotic guarantee for achieving second-order optimality in decentralized optimization under standard smoothness assumptions. max (cid:111)(cid:17) (cid:110) (cid:16) 1.1