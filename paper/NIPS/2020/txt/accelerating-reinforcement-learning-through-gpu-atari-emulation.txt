Abstract
We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari
Learning Environment (ALE) which is used for the development of deep rein-forcement algorithms. CuLE overcomes many limitations of existing CPU-based emulators and scales naturally to multiple GPUs. It leverages GPU parallelization to run thousands of games simultaneously and it renders frames directly on the
GPU, to avoid the bottleneck arising from the limited CPU-GPU communication bandwidth. CuLE generates up to 155M frames per hour on a single GPU, a ﬁnding previously achieved only through a cluster of CPUs. Beyond highlighting the differ-ences between CPU and GPU emulators in the context of reinforcement learning, we show how to leverage the high throughput of CuLE by effective batching of the training data, and show accelerated convergence for A2C+V-trace. CuLE is available at https://github.com/NVlabs/cule. 1

Introduction
Initially triggered by the success of DQN [13], research in Deep Reinforcement Learning (DRL) has grown in popularity in the last years [10, 12, 13], leading to intelligent agents that solve non-trivial tasks in complex environments. But DRL also soon proved to be a challenging computational problem, especially if one wants to achieve peak performance on modern architectures.
Traditional DRL training focuses on CPU environments that execute a set of actions {at−1} at time t − 1, and produce observable states {st} and rewards {rt}. Environment data is then processed by a Deep Neural Network (DNN) on the GPU to select the next action, {at}, which is copied back to the CPU. This sequence of operations deﬁnes the inference path, whose main aim is to generate training data. A training buffer on the GPU stores the states generated on the inference path; this is periodically used to update the DNN’s weights θ, according to the training rule of the
DRL algorithm (training path). A computationally efﬁcient DRL system should balance the data generation and training processes, while minimizing the communication overhead along the inference path and consuming, along the training path, as many data per second as possible [1, 2]. The solution to this problem is however non-trivial and many DRL implementations do not leverage the full computational potential of modern systems [19].
We focus our attention on the inference path and move from the traditional CPU implementation of the Atari Learning Environment (ALE), a set of Atari 2600 games that emerged as an excellent
DRL benchmark [3, 11]. We show that signiﬁcant performance bottlenecks stem from CPU-based environment emulation because the CPU cannot run a large set of environments simultaneously and the CPU-GPU communication bandwidth is limited. To both investigate and mitigate these limitations we introduce CuLE (CUDA Learning Environment), a DRL library containing a CUDA enabled
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Average training times, raw frames to reach convergence, FPS, and computational resources of existing accelerated DRL schemes, compared to CuLE. Data from [8]; FPS are taken from the corresponding papers, if available, and measured on the entire Atari suite for CuLE.
Algorithm
Ape-X DQN [8]
Rainbow [7]
Distributional (C51) [4]
A3C [12]
GA3C [1, 2]
Prioritized Dueling [22]
DQN [13]
Gorila DQN [14]
Unreal [9]
Stooke (A2C / DQN) [19]
IMPALA (A2C + V-Trace) [6]
CuLE (emulation only)
CuLE (inference only, A2C, single batch)
CuLE (training, A2C + V-trace, multiple batches)
CuLE (training, A2C + V-trace, multiple batches)*
N/A
N/A 1 hour mins
*FPS measured on Asterix, Assault, MsPacman, and Pong.
Time
Frames 5 days 10 days 10 days 4 days 1 day 9.5 days 9.5 days 4 days
— hours mins/hours
FPS 50K
—
— 2K 8K
—
—
—
—
Resources 376 cores, 1 GPU 1 GPU 1 GPU 16 cores 16 cores, 1 GPU 1 GPU 1 GPU
> 100 cores 16 cores 35K 250K 40 CPUs, 8 GPUs (DGX-1) 100-200 cores, 1 GPU 41K-155K 39K-125K 26K-68K 142-187K
System I (1 GPU)
System I (1 GPU)
System I (1 GPU)
System III (4 GPUs) 22,800M 200M 200M
—
— 200M 200M
— 250M 200M 200M
N/A
N/A 200M 200M
Table 2: Systems used for experiments.
System
Intel CPU
NVIDIA GPU
I
II
III 12-core Core i7-5930K @3.50GHz 6-core Core i7-8086K @5GHz 20-core Core E5-2698 v4 @2.20GHz × 2
Titan V
Tesla V100
Tesla V100 × 8, NVLink
Atari 2600 emulator that renders frames directly in GPU memory, avoids off-chip communication and achieves high GPU utilization by processing thousands of environments in parallel—something so far achievable only through large and costly distributed systems. Compared to the traditional
CPU-based approach, GPU emulation improves the utilization of the computational resources: CuLE on a single GPU generates more Frames Per Second2 (FPS) on the inference path (between 39K and 125K, depending on the game, see Table 1) compared to its CPU counterpart (between 12.5K and 19.8K). CuLE’s throughput is of the same order of magnitude of much larger distributed systems, like IMPALA [6] or a DGX-1 [20], which eventually leads to a signiﬁcant reduction in the wall clock training time and therefore to an immediate practical advantage [18, 21, 5] for the researchers working in this ﬁeld. Beyond offering CuLE (https://github.com/NVlabs/cule) as a tool for research in the DRL ﬁeld, our contribution can be summarized as follow: (1) We identify common computational bottlenecks in several DRL implementations that prevent effective utilization of high throughput compute units and effective scaling to distributed systems. (2) We introduce an effective batching strategy for large environment sets, that allows leveraging the high throughput generated by CuLE to quickly reach convergence with A2C+V-trace [6], and show effective scaling on multiple GPUs. This leads to the consumption of 26-68K FPS along the training path on a single GPU, and up to 187K FPS using four GPUs, comparable (Table 1) to those achieved by large clusters [20, 6]. (3) We analyze advantages and limitations of GPU emulation with CuLE in DRL, including the effect of thread divergence and of the lower (compared to CPU) number of instructions per second per thread, and hope that our insights may be of value for the development of efﬁcient DRL systems. 2