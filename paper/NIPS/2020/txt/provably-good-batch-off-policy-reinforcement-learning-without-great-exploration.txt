Abstract
Batch reinforcement learning (RL) is important to apply RL algorithms to many high stakes tasks. Doing batch RL in a way that yields a reliable new policy in large domains is challenging: a new decision policy may visit states and actions outside the support of the batch data, and function approximation and optimization with limited samples can further increase the potential of learning policies with overly optimistic estimates of their future performance. Some recent approaches to address these concerns have shown promise, but can still be overly optimistic in their expected outcomes. Theoretical work that provides strong guarantees on the performance of the output policy relies on a strong concentrability assumption, which makes it unsuitable for cases where the ratio between state-action distribu-tions of behavior policy and some candidate policies is large. This is because, in the traditional analysis, the error bound scales up with this ratio. We show that using pessimistic value estimates in the low-data regions in Bellman optimality and evaluation back-up can yield more adaptive and stronger guarantees when the concentrability assumption does not hold. In certain settings, they can ﬁnd the approximately best policy within the state-action space explored by the batch data, without requiring a priori assumptions of concentrability. We highlight the necessity of our pessimistic update and the limitations of previous algorithms and analyses by illustrative MDP examples and demonstrate an empirical comparison of our algorithm and other state-of-the-art batch RL baselines in standard benchmarks. 1

Introduction
A key question in Reinforcement Learning is about learning good policies from off policy batch data in large or inﬁnite state spaces. This problem is not only relevant to the batch setting; many online
RL algorithms use a growing batch of data such as a replay buffer [24, 28]. Thus understanding and advancing batch RL can help unlock the potential of large datasets and may improve online RL algorithms. In this paper, we focus on the algorithm families based on Approximate Policy Iteration (API) and Approximate Value Iteration (AVI), which form the prototype of many model-free online and ofﬂine RL algorithms. In large state spaces, function approximation is also critical to handle state generalization. However, the deadly triad [35] of off-policy learning, function approximation and bootstrapping poses a challenge for model-free batch RL. One particular issue is that the max in the
Bellman operator may pick actions in (s, a) pairs with limited but rewarding samples, which can lead to overly optimistic value function estimates and under-performing policies [26]. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
This issue has been studied from an algorithmic and empirical perspective in many ways. Different heuristic approaches [12, 21, 17] have been proposed and shown to be effective to relieve this weakness empirically. However the theoretical analysis of these methods is limited to tabular problem settings, and the practical algorithms also differ substantially from their theoretical prototypes.
Other literature focuses primarily on approaches that have strong theoretical guarantees. Some work considers safe batch policy improvement: only deploying new policies if with high conﬁdence they improve over prior policies. However, such work either assumes that the policy class can be enumerated [38], which is infeasible in a number of important cases; or uses regularization with a behavior policy [37] as a heuristic, which can disallow signiﬁcantly different but better policies. On the other hand, there are a number of formal analyses of API and AVI algorithms in batch settings with large or inﬁnite state and policy spaces [29, 30, 3, 31, 8, 5, 40]. These results make strong assumptions about the distribution of the batch data, known as the concentrability condition. Concentrability ensures that the ratio between the induced state-action distribution of any non-stationary policy and the state-action distribution in the batch data is upper bounded by a constant, called the concentrability coefﬁcient. This is a strong assumption and hard to verify in practice since the space of policies and their induced state-action distributions is huge. For example, in healthcare datasets about past physician choices and patient outcomes, decisions with a poor prognosis may be very rare or absent for a number of patient conditions. This results in a large concentration coefﬁcient, and hence existing performance bounds [29, 30, 5, 3] end up being prohibitively large. This issue occurs even if good policies (such as another physician’s decision-making policy) are well supported by the dataset.
In the on-policy setting, various relaxations of the concentrability assumption have been studied. For example, some methods [15, 16, 1] obtain guarantees scaling in the largest density ratio between the optimal policy and an initial state distribution, which is a signiﬁcantly milder assumption than concentrability [34, 1]. Unfortunately, leveraging a similar assumption is not straightforward in the fully ofﬂine batch RL setting, where an erroneous estimate of a policy’s quality in a part of the state space not supported by data would never be identiﬁed through subsequent online data collection.
Our contributions. Given these considerations, an appealing goal is to ensure that we can output the best possible policy which is well supported by our dataset in terms of its state-action distribution.
We achieve this goal by leveraging the idea of pessimism in face of uncertainty. Rather than assuming concentrability on the entire policy space, we algorithmically choose to focus on policies that satisfy a bounded density ratio assumption akin to the on-policy policy gradient methods, and successfully compete with such policies. Our methods are guaranteed to converge to the approximately best decision policy in this set, and our error bound scales with a parameter that deﬁnes this policy set and controls the amount of pessimism. If the behavior data provides sufﬁcient coverage of states and actions visited under an optimal policy, then our algorithms output a near-optimal policy. In the physician example and many real-world scenarios, good policies are often well-supported by the behavior distribution even when the concentrability assumption fails.
Many recent state-of-the-art batch RL algorithms [12, 21, 17] do not provide such guarantees and can struggle, as we show shortly with an illustrative example. Our methods use pessimistic value estimates for state-action pairs with insufﬁcient data in the Bellman operators. The key insight is that prior works also add pessimism but only based on the conditional action distribution; we instead select among actions which have good coverage according to the marginalized support of states and actions under the behavior dataset, and enforce pessimistic value estimates otherwise. Our policy iteration algorithm provides the desired “doing the best with what we’ve got” guarantee, and we provide a slightly weaker result for a value iteration method.
We then validate a practical implementation of our algorithm in a discrete task and some continuous control benchmarks. It achieves better and more robust performance with how exploratory the data distribution is, compared with baseline algorithms. This work makes a concrete step forward on providing guarantees on the quality of batch RL with function approximation. 2 Problem Setting
Let M =< S, A, P, R, γ, ρ > be a Markov Decision Process (MDP), where S, A, P , R, γ, ρ are the state space, action space, dynamics model, reward model, discount factor and distribution over initial states, respectively. A policy π : S → ∆(A) is a conditional distribution over actions given state. To simplify the exposition, our derivations will assume that A is discrete – the algorithm can 2
(a) MDP with a rare transition (b) Combination lock
Figure 1: Challenging MDPs for prior approaches. In both MDPs, the episode starts from s0 and ends after a ﬁxed horizon, 2 or 10 respectively. The optimal policy is to reach the green state. Transition probabilities are labeled as p on edges or are deterministic. µ is the conditional probability of action given state when generating the data. The reward distribution is labeled on the edges unless it is 0. be generalized to continuous A straightforwardly and demonstrated in Section 6.2. A policy π and
MDP M together induce a joint distribution over trajectories: s0, a0, r0, s1, a1, . . . , where s0 ∼ ρ(·), ah ∼ π(sh), rh ∼ R(sh, ah), sh+1 ∼ P (sh, ah). The expected discounted sum of rewards of a (cid:3), and Qπ(s, a) further conditions on policy π given an initial state s is V π(s) = E (cid:2)(cid:80)∞ the ﬁrst action being a. We assume for all possible π, Qπ(s, a) ∈ [0, Vmax]. We deﬁne vπ to be the expectation of V π(s) under initial state distribution. We are given a dataset D with n samples drawn i.i.d. from a behavior distribution µ over S × A, and overload notation to denote the marginal distribution over states by µ(s) = (cid:80) a∈A µ(s, a). Approximate value and policy iteration (AVI/API) style algorithms ﬁt a Q-function over state, action space: f : S × A → [0, Vmax]. Deﬁne the Bellman optimality/evaluation operators T and T π as: (cid:105) h=0 γhrh (cid:104) f (s(cid:48), a(cid:48)) and (T πf )(s, a) := r(s, a) + γ Es(cid:48) Ea(cid:48)∼πf (s(cid:48), a(cid:48)). (T f )(s, a) := r(s, a) + γ Es(cid:48) max a(cid:48)
Deﬁne (cid:98)T and (cid:98)T π by replacing expectations with sample averages. Then AVI iterates fk+1 ← (cid:98)T fk.
API performs policy evaluation by iterating fi+1 ← (cid:98)T πk fi until convergence to get fk+1 followed by a greedy update to a deterministic policy: πk+1(s) = arg maxa fk+1(s, a). 3 Challenges for Existing Algorithms
Value-function based batch RL typically uses AVI- or API-style algorithms described above. A standard assumption used in theoretical analysis for API and AVI is the concentrability assump-tion [29, 30, 5] that posits: for any distribution ν that is reachable for some non-stationary policy, (cid:107)ν(s, a)/µ(s, a)(cid:107)∞ ≤ C. Note that even if C might be small for many high value and optimal policies in a domain, there may exist some policies which force C to be exponentially large (in the number of actions and/or effective horizon). Consider a two-arm bandit where the good arm has a large probability under behavior policy. Intuitively it should be easy to ﬁnd a good policy supported by the behavior data but there exist some policies which are neither good nor supported, and the theoretical results requires setting an upper bound C for admitting policies choosing the bad arm. 1
Nevertheless, the standard concentrability assumption has been employed in many prior works on
API [22, 29] and AVI [30, 31, 10, 5]. We also observe that without algorithmic changes, the poorly supported, low-value policies have high variance estimates in the example scenario above, and can indeed be returned by AVI/API erroneously, suggesting that the assumption is not superﬂuous, but rather correctly captures the behavior of these methods. As further evidence, these algorithms often diverge [33, 14], potentially due to the uncontrolled extrapolations caused by both the failure of this assumption and function approximation. Such empirical observations have helped motivate several recent AVI (e.g. [12, 17]) and API (e.g. [21]) works that improve stability and performance.
One attempt in this direction follows an intuition of constraining the backups of Q-values from certain state action pairs, and yields signiﬁcant empirical improvement in several batch RL tasks. BCQL algorithm [12] only bootstraps value estimates from actions with conditional probabilities above a threshold under the behavior policy2. BEAR algorithm [17] uses distribution-constrained backups as its prototype for theoretical analysis which, in the tabular setting, is essentially same as BCQL using non-zero threshold. However, we ﬁnd that these algorithms have failure modes even in simple MDPs shown in Figure 1, due to the fact that the constraint in their algorithm is on µ(a|s) which cannot 1For detailed discussions on the unreasonableness of concentrability, please see [5, 34, 1]. 2In their tabular algorithm BCQL the threshold is zero, but we extend it to non-zero which is also more consistent with their deep RL variant BCQ. 3
(a) MDP with a rare transition (b) 2-arm combination lock
Figure 2: Frequency of converging to π(cid:63) (Success Rate) for different variants of approximated value iteration: AVI, API, BCQL [12] (with different thresholds), SPIBB [21], our algorithms (PPI,
PQI). Tabular algorithm of BEAR [17] is same as BCQL with non-zero threshold. Frequencies are computed from 100 runs, and the change of frequencies over sample size is shown across X-axis.
Error bars are standard deviation. We use a MLE estimate as (cid:98)µ and b = 10/sample size. model the whole uncertainty in the state-action space and backups. Speciﬁcally, BCQL and BEAR perform bootstrapping based on just the action probability, even if the state in question itself is less explored.3 Figure 1b shows an example where a sequence of large action probabilities can result in an exponentially small marginalized state visitation. This causes BCQL and BEAR to bootstrap values from infrequent states in the data, leading to bad performance as shown in Figure 2b.
SPIBB [21] follows the behavior policy in less explored state-action pairs while attempting improve-ment everywhere else, assuming they know the behavior policy. Thus it is robust to this failure mode when the behavior policy is known. In this paper, we do not make this assumption as it is unknown in many settings (e.g., physicians’ decision making policy in a medical dataset). Consequently, following the estimated behavior policy from rare transitions is dangerous as our estimates can also be unreliable there. This can yield poor performance as shown in Figure 2a. In that plot, rare events start to appear as sample size increases and baseline algorithms begin to bootstrap unsafe values. Hence we see poor success rate as we get more behavior data. As samples size get very large all algorithms eventually see enough data to correct this effect. Next we describe the design and underlying theory for our new algorithms that successfully perform in these challenging MDPs (see Figure 2). 4 Pessimistic Policy Iteration and Q Iteration Algorithms
Our aim is to create algorithms that are guaranteed to ﬁnd an approximately optimal policy over all policies that only visit states and actions with sufﬁcient visitation under µ. In order to present the algorithms, we introduce some useful notation and present the classical ﬁtted Q iteration (FQI) [36] and ﬁtted policy iteration (FPI) [3] algorithms which are the function approximation counterparts of AVI and API respectively. For the FPI algorithm, let Π ⊂ (S → ∆(A)) be a policy class. Let
F ⊆ (S × A → [0, Vmax]) be a Q function class in both FQI and FPI. We assume Π and F are both
ﬁnite but can be very large (error bounds will depend on log(|F||Π|)). For any function g we deﬁne shorthand (cid:107)g(cid:107)p,µ to denote (E(s,a)∼µ g(s, a)p)1/p. FQI updates fk+1 = arg minf ∈F (cid:107)f − (cid:98)T fk(cid:107)2 2,µ and returns the greedy policy with respect to the ﬁnal fk upon termination. FPI instead iterates fi+1 = arg minf ∈F (cid:107)f − (cid:98)T πk fi(cid:107)2 2,µ until convergence to get fk+1 followed by the greedy policy update to get πk+1. When a restricted policy set Π is speciﬁed so that the greedy policy improvement is not possible, weighted classiﬁcation is used to update the policy in some prior works [9]. Since both AVI and API suffer from bootstrapping errors in less visited regions of the MDP even in the tabular setting, FQI and FPI approaches have the same drawback.
We now show how to design more robust algorithms by constraining the Bellman update to be only over state action pairs that are sufﬁciently covered by µ. Implementing such a constraint requires access to the density function µ, over an extremely large or inﬁnite space. Given we have samples of this distribution, several density estimation techniques [25, 27] can be used to estimate µ in practice, and here we assume we have a density function (cid:98)µ which is an approximate estimate of µ. In the analysis section we will specify how our error bounds scale with the accuracy of (cid:98)µ. Given (cid:98)µ and a 3This corresponds to a small f ((cid:15)) in Theorem 4.2 of Kumar et al. [17], hence in line with their theory. 4
Algorithm 1 Pessimistic Policy Iteration (PPI) 1: Input: D, F, Π, (cid:98)µ, b 2: Output: (cid:98)πT 3: for t = 0 to T − 1 do for k = 0 to K do 4: 5: 6: 7: 8: end for ft,k+1 ← arg minf ∈F LD(f, ft,k; (cid:98)πt)
ED[Eπ [ζ ◦ ft,K+1]] end for (cid:98)πt+1 ← arg maxπ∈Π
Algorithm 2 Pessimistic Q Iteration (PQI) 1: Input: D, F, (cid:98)µ, b 2: Output: (cid:98)πT 3: for t = 0 to T − 1 do 4: 5: 6: end for ft+1 ← arg minf ∈F LD(f ; ft) (cid:98)πt+1(s)←arg maxa∈A ζ ◦ ft+1(s, a) threshold b (as algorithm input) we deﬁne a ﬁlter function over the state-action space S × A:
ζ(s, a; (cid:98)µ, b) = 1 ((cid:98)µ(s, a) ≥ b) . (1)
For simplicity we write ζ(s, a; (cid:98)µ, b) as ζ(s, a) and deﬁne ζ ◦ f (s, a) := ζ(s, a)f (s, a). We now introduce our Pessimistic Policy Iteration (PPI) algorithm (Algorithm 1), a minor modiﬁcation to vanilla FPI that constrains the Bellman backups and policy improvement step to have sufﬁcient support on the provided data, and enforces pessimistic value estimates elsewhere. The key is to change the policy evaluation operator and only evaluate next step policy value over supported (s, a) pairs, and constrain policy improvement to only optimize over supported (s, a) pairs deﬁned by ζ.
We deﬁne the ζ-constrained Bellman evaluation operator T π
ζ as, for any f : S × A → R, (T π
ζ f )(s, a) = r(s, a) + γEs(cid:48) (cid:80) a(cid:48)∈A [π(a(cid:48)|s(cid:48))ζ ◦ f (s(cid:48), a(cid:48))] . (2)
This reduces updates that may be over-optimistic estimates in (s(cid:48), a(cid:48))’s that are not adequately covered by µ by using the most pessimistic estimate of 0 from such pairs. There is an important difference with SPIBB [21], which still backs up f values (but by estimated behavior probabilities) from such rarely visited state-action pairs: given limited data, this can lead to erroneous decisions (c.f. Fig. 2).
Given a batch dataset, we follow the common batch RL choice of least-squares residual minimiza-tion [29] and deﬁne empirical loss of f given f (cid:48) (from last iteration) and policy π: a(cid:48)∈A π(a(cid:48)|s(cid:48))ζ ◦ f (cid:48)(s(cid:48), a(cid:48))(cid:1)2 (cid:0)f (s, a) − r − γ (cid:80)
LD(f ; f (cid:48), π) := ED
.
In the policy improvement step (line 7 of Algorithm 1), to ensure that the resulting policy has sufﬁcient support, our algorithm applies the ﬁlter to the computed state-action values before performing maximization, like classiﬁcation-based API [9]. We maximize using the dataset D (ED is a sample average) and within the policy class Π, which may not include all deterministic policies.
Analogous to PPI, we introduce Pessimistic Q Iteration (PQI) (Algorithm 2) which similarly applies pessimistic estimates in the the Bellman backups where the support from data is insufﬁcient. Deﬁne
Tζ to be a ζ-constrained Bellman optimality operator: for any f : S × A → R, (Tζf )(s, a) := r(s, a) + γEs(cid:48) [maxa(cid:48) ζ ◦ f (s(cid:48), a(cid:48))] . (3)
Similarly, we deﬁne the empirical loss of f given another function f (cid:48) as:
LD(f ; f (cid:48)) := ED (f (s, a) − r − γ maxa(cid:48)∈A ζ ◦ f (cid:48)(s(cid:48), a(cid:48)))2 .
We also alter the ﬁnal policy output step to only select among actions which lie in the support set (line 5 of Algorithm 2). In Figure 2 we show that our PQI and PPI can both successfully return the optimal policy when the data distribution covers the optimal policy in two illustrative examples where prior approaches struggle. The threshold b is the only hyper-parameter needed for our algorithms, which trades off the risk of extrapolation with the potential beneﬁts of Bellman backups from more state-action pairs seen in the batch of data. We discuss how practitioners can set b in Section 6. 5 Analysis
We now provide performance bounds on the policy output by PPI: the PQI result is similar. Complete proofs for both are in the appendix. We start with some deﬁnitions and assumptions. Given π, let ηπ h (s) be the marginal distribution of sh under π, that is, ηπ h (s)π(a|s), and ηπ(s, a) = (1 − γ) (cid:80)∞
Assumption 1 (Bounded densities). For any non-stationary policy π and h ≥ 0, ηπ h (s, a). We make the following assumptions: h (s) := Pr[sh = s|π], ηπ h (s, a) = ηπ h=0 γhηπ h (s, a) ≤ U . 5
Assumption 2 (Density estimation error). With probability at least 1 − δ, (cid:107)(cid:98)µ − µ(cid:107)TV ≤ (cid:15)µ.
Assumption 3 (Completeness under T π 2,µ ≤ (cid:15)F .
Assumption 4 (Π Completeness). ∀f ∈ F, minπ∈Π (cid:107)Eπ [ζ ◦ f (s, a)] − maxa ζ ◦ f (s, a)(cid:107)1,µ ≤ (cid:15)Π.
ζ ). ∀π ∈ Π, maxf ∈F ming∈F (cid:107)g − T π
ζ f (cid:107)2
Assumptions 3 and 4 are common but adapted to our ζ−ﬁltered operators, implying that the function class chosen is approximately complete with respect to our operator and that policy class can approximately recover the greedy policy. Assumptions 1 and 2 are novel. Assumption 2 bounds the accuracy of estimating the state–action behavior density function from limited data: 1/ n errors are standard using maximum likelihood estimation for instance [42], and the size of this error appears in the bounds. Finally Assumption 1 that the probability/density of any marginal state distribution is bounded is not very restrictive.4 For example, this assumption holds when the density function of transitions p(·|s, a) and the initial state distribution are both bounded. This is always true for discrete spaces, and also holds for many distributions in continuous spaces including Gaussian distributions.
√
The dataset may not have sufﬁcient samples of state–action pairs likely under the optimal policy in order to reliably and conﬁdently return the optimal policy. Instead we hope and will show that our methods will return a policy that is close to the best policy which has sufﬁcient support in the provided dataset. More formally, given a ζ ﬁlter over state–action pairs, we deﬁne a set of policies:
Deﬁnition 1 (ζ-constrained policy set ). Let Πall
C be the set of policies S → ∆(A) such that
Pr(ζ(s, a) = 0|π) ≤ (cid:15)ζ. That is, Es,a∼ηπ [1 (ζ(s, a) = 0)] ≤ (cid:15)ζ. (cid:15)ζ bounds the probability under a policy of escaping to state-actions with insufﬁcient data during an episode. Πall
C adapts its size w.r.t. the ﬁlter ζ which is a function of the hyper-parameter b, and Πall
C does not need to be contained in Π. We now lower bound the value of the policy returned by PPI by the value of any (cid:101)π ∈ Πall
C up to a small error, which implies a small error w.r.t. the policy with the highest value in Πall
C . For ease of notation, we will denote C = U/b in our results and analysis, being clear that C is not assumed (as in concentrability) and is simply a function of the hyper-parameter b.
Theorem 1 (Comparison with best covered policy). When Assumptions 1 and 2 hold, given an MDP
M , a dataset D = {(s, a, r, s(cid:48))} with n samples drawn i.i.d. from µ × R × P , and a Q-function class
F and a policy class Π satisfying Assumptions 3 and 4, (cid:98)πt from Algorithm 1 satisﬁes that w. p. at least 1 − 3δ, (cid:32)
M − v(cid:98)πt v(cid:101)π
M ≤ O
C(cid:112)V 2 max ln(|F||Π|/δ)
√ (1 − γ)3 n
√ 8C (cid:33)
+ (cid:15)F + 6CVmax(cid:15)µ (1 − γ)3
+ 2C(cid:15)Π + 3γK−1Vmax (1 − γ)2
+
Vmax(cid:15)ζ 1 − γ
, for any policy (cid:101)π ∈ Πall (inner loop) and t is the number of policy improvement steps.
C and any t ≥ K. C = U/b. K is the number of policy evaluation iterations
Proof sketch. The key is to characterize the ﬁltration effect of the conservative Bellman operators in terms of the resulting value function estimates. As an analysis tool, we construct an auxiliary MDP
M (cid:48) by adding one additional action aabs in each state, leading to a zero reward absorbing state sabs.
ζ is Qπ in M (cid:48). For
For a subset of policies in M (cid:48), the ﬁxed point of our conservative operator T π that subset, we also have a bounded density ratio, thus we can provide the error bound in M (cid:48). For any ζ-constrained policy, we show that it can be mapped to that subset of policies in M (cid:48) without a substantial loss of value, and then we ﬁnish the proof to yield a bound in M .
We make a few remarks about the comparison between this result and prior related results below:
Remark 1 (Comparison with prior API/AVI bounds). Our results match the fast rate error bound of API/AVI [7, 32, 5, 22] in their dependence on n. A better coefﬁcient on (cid:15)F can be achieved by reﬁning the analysis and we show this version for ease of presentation. The dependency on horizon also matches the standard API analysis and is O(1/(1 − γ)3). The dependency on U/b is same as the dependency on concentrability coefﬁcient for vanilla API analysis, but our guarantee adapts given a hyper-parameter choice instead of imposing a worst case bound over all policies.
Remark 2 (Comparison with the theory in Kumar et al. [17]). Unlike Theorem 4.2 of Kumar et al. [17], there is no uncontrolled f ((cid:15))-like term. We avoid that term by being more pessimistic in our updates and by constraining the comparator class Πall
C . The term f ((cid:15)) is not controlled by the 4We need this assumption only because we ﬁlter based on ˆµ instead of ηπ/µ during policy improvement. 6
algorithm, because a bounded overlap in the conditional action distribution given state can potentially result in an exponentially (in horizon) small overlap in state-action joint distribution. This follows almost the same reasoning as the fact that concentrability coefﬁcient can be exponential in horizon
[5]. Hence, their algorithm and analysis combined with our comparator class Πall
C does not recover our guarantees (see also discussion of BEAR in Section 3). This is also veriﬁed conceptually by our illustrative example (b) in Figure 1b.
When the policy set Πall
C contains at least one high-value policy, Theorem 1 provides a strong guarantee. While this does not always hold, we now provide two illustrative corollaries. One when the optimal policy lies in Πall
C and another on safe policy improvement when µ itself lies in the set.
More generally, in many situations where a concentrability-based analysis might offer nearly vacuous guarantees, we expect our theory to degrade more gracefully with the quality of the collected data.
Corollary 1 (µ covers an optimal policy). If there exists an optimal policy π(cid:63) in M such that
Pr(µ(s, a) ≤ 2b|π(cid:63)) ≤ (cid:15). Then under the conditions in Theorem 1, (cid:98)πt returned by Algorithm 1 satisﬁes that with probability at least 1 − 3δ, v(cid:98)πt
M − ∆, where ∆ is the right hand side of
Theorem 1 and (cid:15)ζ in ∆ is (cid:15) + C(cid:15)µ.
M ≥ v(cid:63)
When the completeness assumptions holds without error and γK−1 ≤ (cid:15), the error bound reduces to
C(cid:112)ln(|F||Π|/δ)/n + C(cid:15)µ + (cid:15)
Vmax/(1 − γ)3 (cid:16) (cid:17) v(cid:98)πt
M ≥ v(cid:63)
M − O
This is similarly tight as prior analysis assuming concentrability [19, 4]. When comparing to on-policy policy optimization [15, 34, 2, 13], the constant C is akin to the density ratio with respect to an optimal policy in those works, though we pay an additional price on the size of densities. The terms regarding value function completeness can be avoided by Monte-carlo estimation in on-policy settings and there is no need for density estimation to regularize value bootstrapping either.
For PQI, the error bound takes a similar form. However the general PQI bound of Corollary 4 has an additional Bellman residual term related to Tζ and π(cid:63). This term arises since in value iteration, the
ﬁxed point of Tζ may no longer be the value function of the optimal policy under support, and it may not be any policy’s value function.
Note that if we can ﬁnd a b and (cid:15)ζ such that µ ∈ Πall
C given sufﬁcient data, then Theorem 1 immediately yields a policy improvement guarantee too, analogous to the tabular guarantees known for BCQL, SPIBB and other safe policy improvement guarantees. Here, we provide the safe policy improvement guarantees in tabular settings. The details of proof are in Appendix C.5.
Corollary 2 (Safe policy improvement – discrete state space). For ﬁnite state action spaces and b ≤ µmin, under the same assumptions as Theorem 1, there exist function sets F and Π (speciﬁed in the proof) such that (cid:98)πt from Algorithm 1 satisﬁes that with probability at least 1 − 3δ,
M ≥ vµ v ˆπt
M − (cid:101)O (cid:32)
Vmax b(1 − γ)3 (cid:32)
|S||A| n
+ (cid:114)
|S||A| n (cid:33)
+
γKVmax (1 − γ)2 (cid:33)
This corollary is comparable with the safe policy improvement result in [21] in its S, A dependence.
Note that their hyper-parameter N∧ is analogous to bn in our result. Our dependency on 1 − γ is worse, as our algorithm is not designed for the tabular setting, and matches prior results in the function approximation setting as remarked after Theorem 1.
To summarize, our analysis makes two main contributions. First, compared to prior work which provides guarantees of the performance relative to the optimal policy under concentrability, we provide similar bounds by only requiring that an optimal policy, instead of every policy, is well supported by µ. Second, when an optimal policy is not well supported, our algorithms are guaranteed to output a policy whose performance is near optimal in the supported policy class. 6 Experimental Results
The key innovation in our algorithm uses an estimated ζ to ﬁlter backups from unsound state-action pairs. In Section 3 we showed how prior approaches without this can fail in some illustrative examples. We now experiment in two standard domains – Cartpole and Mujoco, that utilize very different ζ-estimation procedures. We show several experiments where the data collected ranges 7
from being inadequate for batch RL to complete coverage (where even unsafe batch RL algorithms can succeed). Our algorithms return policies better than any baseline batch RL algorithm across the entire spectrum of datasets. Our algorithms need the hyperparameter b to trade off conservatism of a large b (where the algorithm stays at its initialization in the limit) and unfounded optimism of b = 0 (classical FQI/FPI). In discrete spaces, we can set b = n0/n where n0 is our prior for the number of samples we need for reliable distribution estimates and n is the total sample size. In continuous spaces, we can set the threshold to be a percentile of (cid:98)µ, so as to ﬁlter out updates from rare outliers in the dataset. We can also run post-hoc diagnostics on the choice of b by computing the average of ζ(s, π(s)) for the resulting policy π over the batch dataset. If this quantity is too small, we can conclude that ζ ﬁlters out too many Bellman backups and hence rerun the procedure with a lower b. 6.1 PQI in discrete spaces
We ﬁrst compare PQI with AVI, BCQL [12], SPIBB [21] and behavior cloning (BC) in CartPole-v0, with a discrete state space by binning each state dimension resulting in 104 states. AVI uses a vanilla
Bellman operator to update the Q function in each iteration. For BCQL, we ﬁt a modiﬁed operator in their Eq (10), changing the constraint (cid:98)µ(a|s) > 0 to allow a threshold b ≥ 0. For SPIBB, we learn the Q function by ﬁtting the operator in their Eq (9) (SPIBB-DQN objective). Both SPIBB and PQI require (cid:98)µ(s, a) to construct the modiﬁed operator, but use different notions for being conservative.
For all algorithms, (cid:98)µ(s, a) and (cid:98)µ(a|s) are constructed using empirical counts.
Figure 3: CartPole-v0. Left: convergent policy value across different ((cid:15)-greedy) behavior policies.
Middle and Right: learning curves when (cid:15) = 0.3, 0.6. We allow non-zero threshold for BCQL to subsume the tabular algorithm of BEAR [17]. Shaded regions show standard deviations over 10 runs.
We collect n = 104 transitions from an epsilon-greedy policy w.r.t a near-optimal Q function. We report the ﬁnal policy value from different algorithms for epsilon from 0.1 to 0.9 in Figure 3 (left), and learning curves for (cid:15) = 0.3, 0.6 in Figure 3 (middle, right). The results are averaged over 10 random seeds. Notice that BCQL, SPIBB, and our algorithm need a threshold of µ(s, a) or µ(a|s) as hyper-parameter. We show the results with the best threshold (in a set) of µ(s, a) for PQI and SPIBB and best threshold of µ(a|s) for BCQL. We searched over a larger set of threshold for baselines than our algorithm: {5e-4, 1e-3, 5e-3 } for PQI, {1e-4, 5e-4, 1e-3, 5e-3, 1e-2} for SPIBB and {0, 0.05, 0.1, 0.2} for BCQL. Our algorithm picks larger b for smaller (cid:15) which matches the intuition of more conservatism when the batch dataset is not sufﬁciently diverse. The results show that our algorithm achieves good performance among all different (cid:15) values, and is always better than or close to both behavior cloning and vanilla FQI unlike other baselines. 6.2 PQL in continuous spaces
The core argument for PQI is that ζ-ﬁltration should focus on state-action (cid:98)µ(s, a) distributions rather than (cid:98)µ(a|s). To test this argument in a more complex domain, we introduce Pessimistic Q Learning (PQL) for continuous action spaces which incorporates our ζ-ﬁltration on top of the BCQ architecture
[12]. PQL (like BCQ) employs an actor in continuous action space and a variational auto-encoder (VAE) to approximate µ(a|s). We use an additional VAE to ﬁt the marginal state distribution in the dataset. Since the BCQ architecture already prevents backup from (s, a) with small µ(a|s), we construct an additional ﬁlter function ζ(s) on state space by ζ(s) = 1(ELBO(s) > P2) where
ELBO(s) is the evidence lower bound from VAE, and P2 is the 2nd percentile of ELBO values of s in the whole batch. Thus the major difference between PQL and BCQ is that PQL applies additional
ﬁltration ζ(s(cid:48)) to the update from s(cid:48) in Eq (13) in [12]. Additionally, PQL is a Q learning algorithm instead of actor-critic algorithm. That means in each backup step, we sample a batch of a(cid:48) from the
VAE approximating µ(a|s) then compute max of next Q values. 8
SAC
Task Name halfcheetah-medium -4.3 0.8 hopper-medium 0.9 walker2d-medium
BC BCQ BEAR 5 38.6 36.1 47.6 29.0 33.2 6.6 40.7 54.5 53.1
PQL 38.4 75.2 68.1
Table 1: The ﬁnal policy after 500K training steps in 3 D4RL tasks. The values are normalized with respect to the random policy (0) and expert policy (100). The results of our algorithm is averaged over 5 random seeds and the results of other algorithm are from D4RL evaluations.
We compare PQL with several state-of-the-art batch RL algorithms as well as several baselines, in a subset of tasks in the D4RL batch RL benchmark [11] (halfcheetah-medium, hopper-medium, and walker2d-medium). The data is collected by rolling out a partially trained policy using SAC for 1M steps in the corresponding MuJoCo environment. Results are given in Table 1. It shows that our algorithm is close to prior methods in the half-cheetah domain and better in the other two. As a proof-of-concept experiment, we highlight here the importance of conservative constraints on the state distribution, not only in theory but also for more practical deep RL algorithms. 7