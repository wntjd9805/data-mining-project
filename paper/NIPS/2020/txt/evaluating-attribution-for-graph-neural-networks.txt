Abstract
Interpretability of machine learning models is critical to scientiﬁc understanding,
AI safety, and debugging. Attribution is one approach to interpretability, which highlights input dimensions that are inﬂuential to a neural network’s prediction.
Evaluation of these methods is largely qualitative for image and text models, because acquiring ground truth attributions requires expensive and unreliable human judgment. Attribution has been comparatively understudied for graph neural networks (GNNs), a model class of growing importance that makes predictions on arbitrarily-sized graphs. Graph-valued data offer an opportunity to quantitatively benchmark attribution methods, because challenging synthetic graph problems have computable ground-truth attributions. In this work we adapt commonly-used attribution methods for GNNs and quantitatively evaluate them using the axes of attribution accuracy, stability, faithfulness and consistency. We make concrete recommendations for which attribution methods to use, and provide the data and code for our benchmarking suite. Rigorous and open source benchmarking of attribution methods in graphs could enable new methods development and broader use of attribution in real-world ML tasks. 1

Introduction
With the increasing use of automated decision making aided by machine learning models, the credibility of the models we produce takes on a heightened importance, particularly for ﬁelds such as drug discovery. Credibility describes the extent to which a user trusts a model’s output, and this concept can be difﬁcult to formalize [17]. In the absence of a concrete deﬁnition of credibility, we may provide a window into the model’s “decision making process”, or provide interpretability. In fact, a new wide-reaching European regulatory framework for the application of ML (GDPR; [1]), explicitly requires interpretability of some kind for deployed models. There are many ways to provide interpretations of a model (which we review below), but in this work we focus on perhaps the simplest
— attribution.
An attribution is a credit assignment on each individual input feature xi of a data input x (e.g. for images, each pixel; for text, each character or word) that measures how important the feature is to the model’s prediction of a target property y, often presented to users as a heatmap overlaid on the original data. The attribution heatmap visually indicates what aspects of a particular data example has the greatest inﬂuence on the model’s prediction of property y.
Attributions can expose the statistical regularities that the model leverages to make a prediction [47].
If these patterns match our intuition, they can bolster our conﬁdence in the model’s predictions. If attributions instead reveal that the model is exploiting spurious correlations, or plainly violates a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
practitioner’s common sense, we may use attributions as a debugging tool — we can highlight, and then subsequently correct, spurious correlations in a dataset [29], or apply regularization to encourage desired behavior in the model [33, 38].
Attribution methods have been most studied in the domains of image modeling [16] and text [6], areas where humans have strong intuition. A “ground truth” credit assignment in these domains ultimately rests with subjective human judgment. Unfortunately, obtaining ground truth for realistic image and text tasks is subjective, expensive, and time-consuming.
The introduction and reﬁnement of graph-based neural network models [11, 39] has opened up new and powerful capabilities for modeling structured data. For instance, social networks [52], protein-protein interaction networks [54], and molecules [18, 20] are naturally represented as graphs.
Graph-valued data offer an opportunity to inexpensively and quantitatively benchmark attribution methods, due to the fact that challenging synthetic graph problems have computable ground-truth attributions. This allows us to quantitatively measure the performance of popular attribution methods on several GNN model types, built for a variety of tasks.
Figure 1: Schematic of attribution task setup and attribution metrics. A. We create classiﬁcation and regression tasks for which we have a computable ground-truth. We train GNN models on these labels, and calculate attributions using the graph inputs and attribution methods we adapt to graphs. B. We quantify attribution performance with four metrics. Accuracy measures how well an attribution matches ground-truth.
Consistency measures how accuracy varies across different hyperparameters of a model. Faithfulness measures how well the performance of an attribution method matches model performance. Stability measures how attributions change when the input is perturbed.
Measuring Performance of Attribution Methods We use tasks with graph-valued data and com-putable ground truths (Figure 1, left) to examine qualities of an attribution method that are necessary for credibility: accuracy, faithfulness, consistency and stability [37] (Figure 1, right). We consider an attribution method to have high attribution performance if it scores well on all four properties.
We focus on these qualities from [37] because they are quantitative, do not require soliciting human judgment, and are speciﬁc to attribution, as opposed to interpretability more broadly.
Accuracy. We assess attribution accuracy by quantifying how well attributions match ground-truth credit assignments. If the model is “right for the right reasons” [17], we expect the attribution method to highlight the correct, ground truth nodes in the input graph (Figure 1B, upper left).
Consistency. The accuracy of an attribution technique should be consistent across high-performing model architectures. To test attribution consistency, we quantify the variability in attribution accuracy using the top 10% of models through a hyperparameter scan over model architectures (Figure 1B, lower left).
Faithfulness. The performance of a faithful attribution method should reﬂect the performance of the model. To quantify faithfulness, we run two experiments where we intentionally damage the training dataset to degrade a model’s predictive performance, and systematically measure how each attribution 2
method responds (Figure 1B, upper right). What we term faithfulness in this work is the concept of
ﬁdelity from Robnik-Šikonja and Bohanec [37].
Stability. An attribution method should be invariant to small changes in input features that do not affect an example’s class label or the model’s prediction. To assess stability, we make small graph perturbations on test set examples that leave the ground-truth attribution and predicted class label unchanged, and assess the degree of change in attribution accuracy (Figure 1B, lower right).
We perform these experiments across four popular GNN architectures (GCN, MPNN, GAT, Graph
Nets), and six common attribution methods (GradInput, SmoothGrad, two variants of GradCAM,
Integrated Gradients, CAM and Attention weights) which are further explained in section 3.2.
Contributions
We offer three main contributions:
• We build a comprehensive and open-source benchmarking suite for attribution methods on GNNs and graph-valued data1.
• We evaluate the performance of commonly-used attribution methods in GNNs using the axes of accuracy, faithfulness, stability and consistency using targeted experiments.
• We ﬁnd that, when applicable, CAM applied to GCNs is the best performing attribution method for
GNNs. CAM is generally the best attribution method across all architectures, and all attribution methods tend to perform best when applied to GCNs. If CAM cannot be used, IG is a good substitute. For graph datasets containing only adjacency information, GradCAM is the best performance attribution method. 2