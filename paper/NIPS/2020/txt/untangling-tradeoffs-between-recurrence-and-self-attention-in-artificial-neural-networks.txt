Abstract
Attention and self-attention mechanisms, are now central to state-of-the-art deep learning on sequential tasks. However, most recent progress hinges on heuristic approaches with limited understanding of attention’s role in model optimization and computation, and rely on considerable memory and computational resources that scale poorly. In this work, we present a formal analysis of how self-attention affects gradient propagation in recurrent networks, and prove that it mitigates the problem of vanishing gradients when trying to capture long-term dependencies by establishing concrete bounds for gradient norms. Building on these results, we propose a relevancy screening mechanism, inspired by the cognitive process of memory consolidation, that allows for a scalable use of sparse self-attention with recurrence. While providing guarantees to avoid vanishing gradients, we use simple numerical experiments to demonstrate the tradeoffs in performance and computational resources by efﬁciently balancing attention and recurrence. Based on our results, we propose a concrete direction of research to improve scalability of attentive networks. 1

Introduction
We live in a world where most of the information takes a sequential form, largely because it is delivered over time. Performing computations on streams of sequential inputs requires extracting relevant temporal dependencies and learning to recognize patterns across several timescales. Humans can effortlessly make associations relating events stored in memory which are far from each other in time and thus, capture long-term dependencies.
Historically, recurrent neural networks (RNNs) have been the deep network architecture of choice for this type of task since, just like neural circuits in the brain, they enable dynamics that can be shaped to interact with input streams. However, RNNs (including gated RNNs [35, 10]) still struggle with large timescales as their iterative nature leads to unstable information propagation [5, 30, 35, 18].This is because most standard RNNs rely on their current state ht, a vector of ﬁxed dimension, to represent a summary of relevant past information. Indeed, Bengio et al. [5] showed that without making additional assumptions, storing information in a ﬁxed-size state vector in a stable way necessarily leads to vanishing gradients when back-propagating through time (see also [18]).
∗Indicates ﬁrst authors. Ordering determined by coin ﬂip. 1: Mila - Quebec AI Institute, Canada 2: Université de Montréal, Département d’Informatique et Recherche Opérationelle, Montreal, Canada 3: Université de Montréal, CIRRELT, Montreal, Canada 4: CIFAR senior fellow 5: Université de Montréal, Département de Mathématiques et Statistiques, Montreal, Canada
Correspondence to: <giancarlo.kerg@gmail.com> 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Several attempts have been made to augment RNN dynamics with external memory to mitigate these issues [37, 14, 34, 15], but it is only recently that access to externally stored information has become effective with the introduction of attention, and more particularly soft attention mechanisms [4].
Attention provides a way by which a system can dynamically access past states and inputs across several timescales, bypassing the need of sequential propagation and ignoring irrelevant information (or distractor information). There is substantial empirical evidence that attention, especially self-attention (Vaswani et al. [38], Ke et al. [22]), is very helpful to improve learning and computations over long-term dependencies. However, to the best of our knowledge, there is currently limited understanding of gradient scaling properties in the presence of attention. Moreover, attending over long sequences requires to hold inputs and/or past states in memory, a process that typically scales quadratically with sequence length.
Much like work from the ’90s established formal results for gradient exploding/vanishing in deep/recurrent networks [5], we believe it is crucial to establish similar theoretical tools for attention mechanisms, as these methods are under intense development where scalability and complexity are important issues. In this paper, we contribute to this direction with a formal analysis of gradient propagation in self-attentive systems which precisely quantify trade-offs between recurrence and attention, offering valuable guarantees for attention mechanism development. Concretely exploiting these theorems, we propose a simple family of screening mechanisms to maximally reduce computa-tional complexity and memory usage, while simultaneously maintaining good gradient propagation over large time scales. Using simple tasks for their ease of interpretation, and their variety of computational demands, we illustrate the efﬁcacy of this approach in numerical experiments.
The remainder of this paper is as follows. In Section 2, we give a brief outline of related cognitive processes and neural network mechanisms. In Section 3, we present our central results: asymptotic guarantees for gradient propagation in self-attentive recurrent networks. To illustrate how to exploit these guarantees, in Section 4, we showcase a simple relevancy screening mechanism that aims to efﬁciently consolidate relevant memory, reducing the size of the computational graph from quadratic to linear in sequence length. Finally, in Section 5, we compare various recurrent and attention models with our proposed relevancy screening mechanism on a series of simple numerical experiments, while, in Section 6, we analyze their gradient propagation properties together with their GPU usage. 2