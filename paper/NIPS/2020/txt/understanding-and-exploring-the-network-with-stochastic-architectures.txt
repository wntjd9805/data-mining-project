Abstract
There is an emerging trend to train a network with stochastic architectures to enable various architectures to be plugged and played during inference. However, the existing investigation is highly entangled with neural architecture search (NAS), limiting its widespread use across scenarios. In this work, we decouple the training of a network with stochastic architectures (NSA) from NAS and provide a ﬁrst systematical investigation on it as a stand-alone problem. We ﬁrst uncover the char-acteristics of NSA in various aspects ranging from training stability, convergence, predictive behaviour, to generalization capacity to unseen architectures. We iden-tify various issues of the vanilla NSA, such as training/test disparity and function mode collapse, and further propose the solutions to these issues with theoretical and empirical insights. We believe that these results could also serve as good heuristics for NAS. Given these understandings, we further apply the NSA with our improvements into diverse scenarios to fully exploit its promise of inference-time architecture stochasticity, including model ensemble, uncertainty estimation and semi-supervised learning. Remarkable performance (e.g., 2.75% error rate and 0.0032 expected calibration error on CIFAR-10) validate the effectiveness of such a model, providing new perspectives of exploring the potential of the network with stochastic architectures, beyond NAS. 1

Introduction
Deep neural networks (DNNs) are the de facto methods to model complex data in a wide spectrum of practical scenarios [12, 36, 38, 40]. The design of neural architectures has always been an active research topic in DNNs, aiming to discover effective connectivity patterns for building networks, in manually designed [33, 14, 43, 13, 51, 32] or automatic [52, 31, 53, 22] manners. Recent research even permits us to train a network without a ﬁxed architecture [3, 45, 42, 1, 11], i.e., at every training iteration, an architecture sample is randomly drawn from an architecture distribution and used to guide the training of network weights (see Fig. 1 for more insights), which is also known as the weight sharing technique in neural architecture search (NAS)1.
Though the weight-sharing network with stochastic architectures is promising, its usage is closely encoupled with NAS to relieve the burden of training thousands of networks. Indeed, the stochasticity over the architectures inside the model itself is also of interest, in consideration of multiple aspects: (i) In the spirit of stochastic regularization, the introduced architecture variability helps to regularize deep models from co-adaptation and over-ﬁtting, in a more structured and global style than standard stochastic regularizations applied on local feature maps or weights [35, 39, 19, 8]. (ii) The trained
∗Corresponding author. 1In NAS, the distribution where the stochastic architectures are sampled may also be updated w.r.t. validation data simultaneously, with the purpose of discovering outperforming architectures. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: (Best viewed in color.) We plot four convolutions due to space limitation. (a)-(c): Diverse architectures lying in the wiring-based space. Note that (c) actually equals to the residual connections, and refer to [48] for the details of such equivalence. (d): The network with stochastic architecture is suitable for various architectures, and randomly activates an architecture at each training/test step. See Appendix A for practically used architectures. weight-sharing network can adopt diverse architectures, seen or even unseen (as shown in Sec. 4.2) during training, to perform inference, enabling us to leverage the expressivity of various architectures with training only one set of weights. The predictions provided by different architectures can be further assembled or used to calculate uncertainty estimates, making the prediction model more accurate, robust, and calibrated.
In this work, we disentangle the Network with Stochastic Architectures (referred to as NSA) from the task of NAS, and provide a ﬁrst systematical investigation on NSA as a stand-alone problem beyond NAS. At ﬁrst, we investigate the un-identiﬁed characteristics and limitations of NSA. Through thorough empirical analyses, we have uncovered the training/test disparity and mode collapse issues of the vanilla NSA, which are non-trivial and neglected by existing works. We then develop several improvements to address these problems with theoretical and empirical insights. Furthermore, we also observe some remarkable features of NSA and our improved versions, such as good generalization capacity to unseen architectures, which could be intentionally leveraged to build NSA with enhanced predictive performance and hopefully beneﬁt existing NAS methods. Finally, to fully exploit its potentials from inference-time stochastic architectures, we apply the NSA with our improvements into several challenging scenarios. Experimental results on multiple tasks testify the effectiveness of
NSA. In summary, our contributions are as follows: 1. We provide a systematical investigation on the network with stochastic architectures (NSA). 2. We uncover a wide range of characteristics of NSA, identify two issues of it, i.e., training/test disparity and mode collapse, and propose two techniques to address the issues. 3. We extend NSA into scenarios like model ensemble, uncertainty estimation, and semi-supervised learning, to enjoy the beneﬁts from the stochasticity over the architectures.
Extensive experiments prove the effectiveness of NSA in these scenarios. 2 NSA: Network with Stochastic Architectures
Before delving into the details of understanding and exploring NSA, we describe its basic deﬁnition and motivation, as well as its training and test principles. Then we brieﬂy present its building details.
What is NSA? Basically, NSA is deﬁned as a network with a ﬁxed set of weights, but stochastically sampled architectures in both training and inference, distinct from the regular DNNs. To meet such a deﬁnition, the space, where we sample architectures, is usually required to be well structured [30, 22, 44], so that the shared weights can be architecture compatible. There are two popular structured architecture spaces: (i) in a sub-graph view – different architectures are different sub-graphs of a super graph with redundant computational branches [30, 22]; (ii) in a wiring view – different architectures activate different skip-connections among a ﬁxed number of computational operations [44, 48] (see
Fig. 1 for the details). In this work, we consider the latter as: (i) the operation redundancy in the former may probably limit the convergence of network weights; and (ii) the latter has a higher alignment with the classic ResNets [12] and DenseNets [14]. We parameterize the architecture as the discrete adjacency matrix of the directed graph on the ﬁxed set of operation nodes. 2
Why do we need a NSA? At ﬁrst, the architecture stochasticity is likely to regularize the training properly, in light of the stochastic regularization scheme. A more promising aspect is that given a trained NSA, we can evaluate the incoming data with diverse architectures thanks to the plug-and-play nature of the model for neural architectures. This enables us not only to evaluate a broad range of architectures with only the training efforts of once, but also to exploit the diverse predictive behaviors of different architectures, which are thought to carry specialized inductive bias. Moreover, the predictions from different architectures can further be assembled or integrated to calculate uncertainty estimates, giving rise to a more accurate, robust, and calibrated prediction model.
Training principles. To train a NSA to predict well under various architectures, we minimize the expected empirical risk w.r.t. the variable architecture for weight updating, as suggested in [1, 45, 3, 11]. Speciﬁcally, we assume that the architecture α follows a distribution p(α), and we have access i=1 of size n, where xi ∈ Rd and yi ∈ Y denote the data and label, to a training set D = {(xi, yi)}n respectively. The loss function for training is formulated as:
L(w) = E
α∼p(α) (cid:104) 1 n (cid:88) (xi,yi)∈D
− log p(yi|xi; w, α) (cid:105)
≈ 1
|B| (cid:88) (xi,yi)∈B
− log p(yi|xi; w, α), α ∼ p(α), (1) where w denotes the weights, B represents a stochastic batch of data, and p(y|xi; w, α) is the predictive distribution. Note that the sampled architecture is typically used for the whole batch [45, 3].
Given this, we can iteratively perform stochastic gradient descent (SGD) for weight training.2
Test principles. Based on a trained NSA, we can predict for the validation data with diverse architectures seen or even unseen (we will justify this in Sec. 4.2) during training, due to its high compatibility with various architectures. The accuracy on the validation data Dval of a speciﬁc (cid:1).
I(cid:0) arg maxy p(y|xi; w, α0) = yi architecture α0 takes the form of A(α0) = 1
|Dval|
Unlike regular DNNs, we can also ensemble the predictions from T architectures {αt}T t=1 for (cid:17) performance estimation: Aens = 1
= yi
.
Note that NAS always takes A(α0) as a proxy of α0’s stand-alone performance (i.e., the performance of the architecture with individual weights trained from scratch) to guide architecture search. t=1 p(y|xi; w, αt) arg maxy (xi,yi)∈Dval (xi,yi)∈Dval (cid:16) 1
T (cid:80)T
|Dval| (cid:80) (cid:80) (cid:17) (cid:16)
I
A reﬁned training space of architecture. To avoid meaningless architecture samples, we adopt a knowledge guided sampler, i.e., the Erd˝os-Rényi (ER) [6] model with 0.3 probability to activate any one of the possible skip-connections, suggested by [44], to sample from the whole space. We also demand there is an overall chain-like connection. Before training, we reﬁne the broad architecture space by randomly sampling a subset of it in the size of S with the sampler. The S architectures span the training space, and we uniformly choose one of them at each iteration during training (corresponding to p(α) in Eq. (1)). We apply such a reﬁned training space of architectures for two reasons: (i) though the original architecture space is huge (e.g., > 1010), the architectures that could be sampled and used in training are limited owing to the limited training steps (e.g., < 105); (ii) by
ﬁxing the training space at a certain size, we can further examine if training with more architectures harms the convergence of NSA and can the NSA trained under a limited number of architectures generalize to unseen architectures, as revealed in Sec. 3 and Sec. 4.2, respectively.
Network details. By convention, on the CIFAR-10 [16] task, the deployed network is divided into 3 stages in different spatial sizes, each containing 8 convolution modules, and we randomly sample an individual architecture for each stage. We use wide convolutions with the widening factor of 10 for feature extraction, inspired by their success in Wide Residual Networks (WRNs) [49]. A uniform sum precedes each convolution module, i.e., a ReLU-Conv-BN triplet, to aggregate incoming feature maps. We use explicit down-sampling modules between stages to make the feature maps in every stage have the same size and hence can be freely connected. As a note, the residual connections also lie in the space (shown in Fig. 1(c)), allowing us to implement a comparable WRN as a baseline (denoted as WRN-28-10†). The training cost of NSA is almost identical to that of WRN-28-10†, taking about 0.6 GPU day on a GTX 2080Ti for 300 training epochs. 3 Training/test Disparity of NSA
Though NSA has been widely deployed in NAS, the focus was mainly placed on evaluating the architecture candidates given a trained NSA, while leaving several important characteristics of NSA 2We view p(α) as ﬁxed for simplicity despite updating it w.r.t. validation results at the same time is feasible. 3
(a) S = 500 (b) S = 5000 (c) S = 500 (d) S = 5000
Figure 2: (a)-(b): The training curves of NSA trained under architecture spaces of size 500 and 5000. (c)-(d):
The histograms for the validation accuracy of 100 random architectures (seen during training) with the training mode of BN turned on/off. unexplored, such as the training stability, convergence, and sensitivity to the training architecture space size. In this section, we examine these previously ignored aspects of NSA and present the key observation of sharp training/test disparity. We further draw theoretical insights from the mechanism of Batch Normalization (BN) [15] to explain this phenomenon and propose solutions accordingly.
We start by examining NSA on the typical CIFAR-10 image classiﬁcation task, with the number of used architectures S during training varying from 500, to 5000 and 50000. We calculate the accuracy and loss of every batch of data given a batch-speciﬁc random architecture, according to
Eq. (1), and take the average as the whole dataset accuracy and loss. We draw the training curves in Fig. 2(a)-(b) and Appendix B, respectively. Surprisingly, the training of NSA is stable and well converged in all cases, as well exhibiting tolerance to the variability of the training architecture space.
This is somewhat counter-intuitive as there seems to be higher architecture variability in a wider space, rendering the data-ﬁtting harder. We speculate that the well structured architecture space yields inherently consistent architecture samples, resulting in such results.
A much more attractive part is the test loss and test accuracy curves, owing to its severe instability.
Typically, the training and test disparity of a DNN model is caused by the inconsistency inside BN – during training, batch speciﬁc statistics are used to normalize features while in test, their exponential moving average (ema) takes over to make the inference stable and behavior independent. To conﬁrm this, we calculate the validation accuracy of 100 random architectures with the training mode of BN turned on or off, and plot their histograms in Fig. 2(c)-(d). The visualization echoes our speculation: the model behaviour becomes signiﬁcantly unstable when replacing the training mode with the test one. This phenomenon is also found by some recent works [47, 46]. It may also explain why methods in NAS tend to evaluate the architectures with the training mode on [22, 45], though the validation results given by training-mode BN are not pretty reliable.
To ﬁgure out the underlying reasons of such a problem, we ﬁrst of all draw some insights from the formulation of BN. Given a mini-batch of |B| instances, we consider a single channel of the batch features {h1,α, h2,α, ..., h|B|,α}, with the assumption that the spatial dimension is 1 for simplicity.
α in the subscript refers to the used architecture for the batch. BN works by applying the following transformation on the features (the afﬁne transformation is omitted):
µ = 1
|B|
|B| (cid:88) i=1 hi,α, σ2 = 1
|B|
|B| (cid:88) (hi,α − µ)2, ˆhtrain i,α = i=1 hi,α − µ
√
σ2 + (cid:15)
, ˆhtest i,α = hi,α − µema (cid:112) ema + (cid:15)
σ2
. (2)
Then we look at the variance of µ, since that the gap between µ and the constant µema is the major discrepancy between training and test, and obtain: var(µ) = 1 i=1 var(hi,α) + (cid:80) i(cid:54)=j cov(hi,α, hj,α)). Intuitively, the features generated with the same architecture α are highly correlated, because the architecture commonly shifts the features toward a certain direction. This makes the second term of the decomposition of var(µ) undesirably large, given that it is a summation over |B| × (|B| − 1) terms. So, NSA uses batch statistics varying across architectures during training, but uses architecture agnostic ones during test, bringing inconsistency and hence unstable prediction.
|B|2 ((cid:80)|B|
With the root of the problem diagnosed, we gain the opportunities to solve it. To reduce the correlation between the features in a batch, a straight-forward solution is to reduce the correlation between the architectures which generate these features. Continuing from this, we replace the batch speciﬁc 4
(a) S = 500 (b) S = 5000 (c) S = 500 (d) S = 5000
Figure 3: (a)-(b): The training curves of NSA-i trained under architecture spaces of size 500 and 5000. (c)-(d):
The histograms for the validation accuracy of 100 random architectures tested upon the trained NSA-i, with the training mode of BN turned on/off. architectures in Eq. (1) with instance speciﬁc ones3:
L∗(w) = 1
|B| (cid:88) (xi,yi)∈B
− log p(yi|xi; w, αi), αi ∼ p(α), i = 1, ..., |B|. (3) the batch mean is re-calculated as µ∗ = 1
|B| i=1 var(hi,αi) + (cid:80) i=1 hi,αi, with variance var(µ∗) =
Then,
|B|2 ((cid:80)|B| 1 i(cid:54)=j cov(hi,αi, hj,αj )). cov(hi,αi, hj,αj ) should be small given that the architecture for each data is i.i.d., thus ideally we can reduce the variance of the batch mean by one order of magnitude. We refer to network trained with Eq. (3) as improved NSA (NSA-i), and take
NSA-i as the default model in the following evaluation for its advantages. (cid:80)|B|
We provide the training curves and validation results of NSA-i in Fig. 3. As expected, in Fig. 3(a)-(b), the test results are much more stable and consistent with the training ones. Surprisingly, in
Fig. 3(c)-(d), test-mode BN induces notably better validation accuracy, implying the weaknesses of training-mode BN: the training statistics commonly cannot approximate the whole dataset ones well.
A direct comparison on var(µ) between NSA and NSA-i is deferred to Appendix C.
At last, we have done another interesting study – ranking 100 random architectures w.r.t. their validation accuracy with the training mode of BN turned on or off, and calculating the Spearman rank correlation [27] between the two modes (the higher, the more correlated). The results of NSA are 0.33, 0.37, and 0.258 with the training space containing 500, 5000, and 50000 architectures, respectively.
As a comparison, NSA-i offers 0.588, 0.395, and 0.615. This testiﬁes that the architecture assessment provided by NSA, trained with batch speciﬁc architectures, is indeed less stable than that from NSA-i.
This also highlights the necessity of solving the BN problem in NAS (either with the investigated architecture space or with the popular DARTS space [22]), and challenges the effectiveness of using training-mode BN for architecture evaluation, as in almost all efﬁcient NAS methods. 4
Inference-time Properties of NSA
In this section, we use the trained NSA for inference, and aim to analyze some properties of its inference-time behaviour. We concern (i) Do diverse architectures behave diversely given shared weights? (ii) Can NSA trained under a limited architecture space generalize to unseen architectures in the broad, raw architecture space? The two aspects are of central importance for both architecture evaluation and ensemble with various architectures. We answer the two questions in the following. 4.1 Mode Collapse of Diverse Architectures
As stated, the network architecture is capable to carry speciﬁc inductive bias, thus different architec-tures may deliver diverse predictions for the same data. Such predictive diversity is comprehensively helpful to the model, ranging from enhancing performance [20] and robustness [29], to yielding more calibrated uncertainty estimates [41]. But can the predictive diversity still be held given only a set of shared weights in NSA? Intuitively, the answer is not positive, because the weights in NSA are architecture agnostic to permit the trained weights generalize across the whole architecture space. The 3Using instance speciﬁc architectures is feasible to implement when using the architecture space of [44], as done in this work, but is not directly implementable in the sub-graph based space, left as a future work. 5
expectation w.r.t. architecture in the training loss forces the weights to be robust against architecture variability, and the model to predict consistently under diverse architectures. Thus, the network would seemingly yield architecture agnostic prediction, referred to as the function mode collapse, and lose the advantages of exploring diverse modes of prediction behaviour from multiple architectures.
Based on these speculations, we launch a set of experiments to identify whether mode collapse indeed exists or not. A realistic barrier is that the prediction behaviour of a network model can hardly be numerically measured, owing to its black-box nature. Drawing inspiration from the fact that model ensemble frequently beneﬁts from diverse base predictors [20], we opt to use the ensemble performance gain as a metric, to estimate the behaviour diversity of different architectures.
Speciﬁcally, we test on the NSA-i model, given its supremacy over naive NSA, trained with S = 500 architectures. We let the ensemble number of architectures T range from 1 to 500, and draw the change of ensemble accuracy w.r.t. T in Fig. 4.
As shown, the ensemble performance gain is limited (almost 0.003) and stops increasing quickly. Such results substantiate that there are moderate levels of function mode collapse among various architectures when using a shared set of weights.
We know the source of this issue is the shared weights are archi-tecture agnostic, then as a solution, we can augment the shared weights with an extra set of architecture dependent weights, to enjoy the beneﬁts from more diverse function modes of different architectures4. The extra weights of every architecture should be low-dimensional, because at per training step, only the extra weights of several architectures (no more than batch size consid-ering Eq. (3)) would be updated. If not, they will not be trained thoroughly. Under this consideration, we employ architecture dependent aggregation and BN in NSA-id, following the style of the class-conditional BN widely used in conditional generative modeling [26]. Namely, we build an individual set of trainable aggregation coefﬁcients and BN afﬁne parameters for each architecture, and select the corresponding set to the architecture for calculation at per step. We refer to NSA-i with architecture dependent weights as NSA-id.
Figure 4: The change of the ensem-ble performance w.r.t. the number of architecture used to ensemble.
Then, we assess the mode collapse level of NSA-id with the aforementioned ensemble based evalua-tion. To compare with NSA-i fairly, we use only the architecture conditional aggregations, which introduces negligible extra weights, in this experiment. We exhibit the results in Fig. 4. As expected, the ensemble gain is more obvious compared to NSA-i5. However, identical to NSA-i, NSA-id cannot enjoy further ensemble gain after seeing almost 20 architectures. We think it is reasonable: as discussed, the main weights of the network are architecture agnostic, rendering it hard to exhaustively diversify the predictions of various architectures with only few additional weights. To summarize, mode collapse indeed occurs and employing architecture conditional weights mitigates it. 4.2 Generalization Capacity to Unseen Architectures
As we stated, the inference-time architecture stochasticity of NSA is desirable, making us capable of exploiting the predictive power of various architectures. But does the trained NSA only accommodate the architectures seen during training? Can the trained NSA generalize to unseen architectures for broader exploration? Here, we offer answers for them with both qualitative and quantitative evidence.
First, we calculate the test accuracy of 200 randomly sampled architectures based on the NSA-i models trained under various spaces (as shown in Appendix D, the naive NSA models with training-mode BN would provide similar results). A half of the 200 architectures are seen during training while the other half not. We depict the test accuracy histograms of the two types of architectures in
Fig. 5. An intuitive conclusion could be drawn is that with the training architecture space large enough (i.e., S ≥ 500), the trained NSA-i can present matched performance on the unseen architectures with 4Of course, introducing architecture dependent weights will hinder the trained weights from generalizing to unseen architectures as we can only deploy extra weights for architectures seen during training. 5The performance drop in Fig. 4 may stem from the facts that the 500 used architectures are randomly sampled and we perform only uniform ensemble instead of weighted ensemble. Thus assembling more base learners may not give rise to rigidly better predictions. 6
(a) S = 5 (b) S = 50 (c) S = 500 (d) S = 5000
Figure 5: The histograms for the validation accuracy of 100 architectures seen during training vs. those for 100 unseen architectures, tested on the trained NSA-i models with different training space sizes. the ones used for training. When the training space is too narrow (e.g., S ≤ 50), the network behaves distinctly over the two classes of architectures, ruling out the generalization across architectures.
An alternative to quantitatively analyze the generalization capacity of NSA-i is to check whether we can distinguish the seen architectures from the unseen ones w.r.t. their validation accuracy.
A golden metric to estimate the goodness of a classiﬁer on such a binary classiﬁcation task is the Area under the ROC Curve (AUC). Thus, we report the AUCs of the trained NSA-i models in Table 1. We also report the average accuracy of the seen architectures and the unseen ones for reference. Consistent with the his-tograms, with the training space increases, it is harder to differentiate these two classes of architectures w.r.t. validation accuracy; when S ≥ 500, almost any binary classiﬁer randomly guesses, given the near 0.5 AUCs.
Meanwhile, we also notice that the average validation accuracy of seen architectures decreases slightly.
Table 1: The change of the AUC, which mea-sures the differentiability between the seen ar-chitectures and unseen ones given the validation accuracy, w.r.t. the training space size S. We also report the average accuracy of the seen and unseen architecture for reference.
Avg acc. (unseen) 91.81% 96.23% 96.12% 96.01%
Avg acc. (seen) 96.57% 96.47% 96.16% 96.01%
AUC 1.00 0.77 0.57 0.52
S 5 50 500 5000
These results validate the generalization capacity of NSA, perhaps because the shared weights learn common structures of the architectures. As shown, we can train a NSA with a suitable number of architectures (e.g., [500, 5000]) to conjoin architecture generalization and accuracy. We hope that this may also serve as an insightful heuristic for training weight-sharing proxy networks in NAS. 5 Applications of NSA
Given the potential of NSA to unleash the predictive capacity of diverse architectures during inference, in this section, we apply NSA to a variety of tasks ranging from ensemble learning, uncertainty esti-mation, to semi-supervised learning, which is unexplored in previously works. As discussed, we take
WRN-28-10† as a main baseline, for its identical settings with NSA and the strong competitiveness of residual connections [12, 49]. As a note, NSA leverages stochastic architectures for inference, consistent with the empirical Bayes methods, e.g., Monte Carlo (MC) dropout [7]. Thus, we take MC dropout built upon WRN-28-10† as another baseline.
Hyper-parameter setting. We enable the architecture conditional BNs in the following experiments.
We empirically found that using more than 10 architectures in training frequently results in worse validation results, possibly due to the incomplete training of the redundant weights in BNs, as explained in Sec. 4.1. Therefore, we use S = 5 randomly sampled architectures for training and inference. To further facilitate the convergence of the shared weights, we deploy an auxiliary classiﬁer following [22] with 0.1 loss coefﬁcient (also deployed in the baselines). We apply standard data processing and CutOut augmentation [5]. The optimization settings follow WRN-28-10 [49]. 5.1 Model Ensemble with Stochastic Architectures on CIFAR-10 and CIFAR-100
As shown in Sec. 4.1, there is evidence to suggest that ensemble the predictions from different architectures does boost the validation performance, consistent with the common knowledge [18], so we continue evaluating this technique on the more expressive NSA-id models with conditional
BNs used. For the results of NSA-id, as stated, we use S = 5 architectures for training and ensemble 7
Table 2: Comparison of NSA-id, using ensemble of 5 different architectures for prediction, and a range of competing baseline, in terms of test error and ECE. ENAS and DARTS adpot the parameter-efﬁcient separable convolutions and apply re-training to get the results.
Method
# params
WRN-28-10 [49]
DenseNet-BC [14]
ENAS + CutOut [30]
DARTS + CutOut [22]
WRN-28-10†
WRN-28-10†, MC dropout
Average of individuals
NSA-id 36.5M 25.6M 4.6M 3.4M 39.5M 39.5M 39.5M 39.6M
CIFAR-10
CIFAR-100
Test error (%) ↓ 4.00 3.46 2.89 2.83 2.93 3.23 2.97 2.75
ECE ↓
----0.0140 0.0107 0.0153 0.0032
Test error (%) ↓ 19.25 17.18
--16.75 17.16 17.02 16.44
ECE ↓
----0.0672 0.0454 0.0446 0.0212 all of them for test (i.e., T = 5). For WRN-28-10† with MC dropout, we predict one data for 100 times with randomly sampled dropout masks and assemble them. We implement a further baseline:
Average of individuals, in which we individually trains 5 networks with the 5 architectures used by
NSA-id, and report their average results, to present the average performance of the used architectures, rather than their ensemble as comparing that to NSA-id is unfair given the need of 5× training costs.
We report the results in Table 2. In both tasks, NSA-id surpasses the strong baselines with clear margins. The 2.75% error rate on CIFAR-10 is rather promising considering the wide convolution based backbone. Notably, the comparison between NSA-id and Average of individuals conﬁrms that ensembling multiple architectures leads to improved performance over a single architecture, despite using shared weights6. These results also prove the ER-0.3 model provides a good architecture space.
We also detail the model calibration, which is another major concern of classiﬁcation model, in
Table 2. Following [10], we take expected calibration error (ECE) as a measure of calibration.
Surprisingly, NSA-id shows lower ECE than the strong, principled baseline MC dropout with huge margins. We think this results from the fact that different architectures offer relatively diverse predictions in NSA-id, alleviating the over-conﬁdence, while MC dropout is known to suffer from mode collapse [18], and hence cannot beneﬁt too much from prediction ensemble. 5.2 Uncertainty Estimation (cid:80)T (cid:80)T i=1 p(y|x; w, αi)] − 1
T
In NSA-id, the architecture stochasticity results in the predictive uncertainty, permitting us to regard
NSA-id as an empirical Bayes method. In this section, we assess the uncertain estimates provided by NSA-id. Suggested by [34], we adopt the mutual information (MI) between the prediction of incoming data and the model parameters as the uncertainty measure, namely, I(w, α, y|D, x) ≈
H[ 1 i=1 H[p(y|x; w, αi)], where H is the entropy, and T = 5 as stated.
T
We test the uncertainty estimates on two challenging kinds of samples: out-of-distribution (OOD) ones and adversarial ones. The models to evaluate are trained on CIFAR-10, and OOD samples refer to the test data of SVHN. For the adversarial samples, we use the frequently adopted, performant
Projected Gradient Descent (PGD) [25] to craft. In practice, we ﬁrst calculate the MI of normal test samples and OOD (or adversarial) ones, then we compute and report the AUC of the binary classiﬁcation of directly distinguishing the normal ones (class 0) from OOD (or adversarial) ones (class 1) based on the MI. The underlying notion is that OOD (or adversarial) samples commonly deviate from the manifold of normal ones, thus have high uncertainty. As shown in Table 3, NSA-id consistently displays improved uncertainty estimates than the golden baseline WRN-28-10† with MC dropout. Also of note that, NSA-id shows stronger adversarial robustness against PGD attack. 5.3 Semi-supervised Learning
At last, we use NSA-id to perform semi-supervised classiﬁcation on CIFAR-10, using only 4000 labeled data. The notion of applying NSA-id into such a scenario is that with the uncertainty estimates provided by NSA-id, we can minimize the predictive uncertainty (i.e., the aforementioned mutual in-formation) of unlabeled data, to assist the learning with the labeled data. The uncertainty minimization 6As a note, the ensemble of the 5 aforementioned individuals yields striking 2.36% error rate on CIFAR-10, conﬁrming that weight sharing is a main cause of mode collapse. 8
Table 3: Comparison between NSA-id and MC dropout in terms of the quality of uncertainty estimates. PGDa-b-c represents the PGD adversary with perturbation budget a/255, number of steps b, and step size c/255.
PGD3-4-1
PGD1-2-1
PGD2-3-1
Method
WRN-28-10†, MC dropout
NSA-id
OOD
AUC ↑ Acc. ↑ AUC ↑ Acc. ↑ AUC ↑ Acc. ↑ AUC ↑ 0.564 0.345 0.935 0.618 0.401 0.970 0.622 0.630 0.183 0.263 0.735 0.737 0.694 0.705 i i ||p(y|xun is achieved by optimizing a consistency loss: Lunlabeled = (cid:80) i)||2
; w, αi) − p(y|xun 2, i denotes the ith unlabeled data, and αi and α(cid:48) where xun i are two randomly sampled architectures.
In practice, we use the distance between output logits instead of that of probabilities. We set the coefﬁcient of consistency loss to be 20, following an anneal schedule [17]. After training, we as-semble the predictions of different architectures for ﬁnal prediction. We implement two baselines: (i) WRN-28-10† with Π model [17], which works the same as NSA-id expect for using dropout to provide twice predictions for one data; (ii) WRN-28-10† trained with only labeled data. The validation accuracy of NSA-id and the two baselines are 86.96%, 85.22%, and 83.87%, respectively.
NSA-id outperforms Π model, perhaps because the architecture stochasticity can explore more diverse predictions across decision boundaries, thus penalizing the inconsistency between the predictions of unlabeled data would drive the decision boundaries to be more robust.
; w, α(cid:48) i 6