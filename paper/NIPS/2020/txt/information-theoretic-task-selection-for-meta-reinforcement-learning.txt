Abstract
In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of tasks to prepare for and learn faster in new, unseen, but related tasks. The training tasks are usually hand-crafted to be representative of the expected distribution of test tasks and hence all used in training. We show that given a set of training tasks, learning can be both faster and more effective (leading to better performance in the test tasks), if the training tasks are appropriately selected. We propose a task selection algorithm, Information-Theoretic Task Selection (ITTS), based on information theory, which optimizes the set of tasks used for training in meta-RL, irrespectively of how they are generated. The algorithm establishes which training tasks are both sufﬁciently relevant for the test tasks, and different enough from one another. We reproduce different meta-RL experiments from the literature and show that ITTS improves the ﬁnal performance in all of them. 1

Introduction
One of the main challenges presented in Reinforcement Learning (RL) is to be able to perform optimally across a range of tasks, especially in the real world, where experience is expensive, and a poor initial behaviour may result in signiﬁcant costs. The trained agent would ideally be able to adapt to slight variations in the dynamics of the environment without retraining, and show close to optimal performance from the beginning of the new task.
Meta-learning in RL has been gaining popularity as a methodology to face such challenges, and train agents requiring minimal online adjustments when a variation of a previously seen task is presented [7; 30; 8; 22]. In meta-learning, an agent is trained on a wide set of tasks so that it can learn their common features, and transfer knowledge to unseen tasks that share some properties with the training set. Meta-RL has also had promising success in real-world scenarios, when adapting to new tasks [20; 2; 23].
Training tasks are designed with the intention of being representative of a family of test tasks, or a skill the agent is expected to learn. A common framework consists in modeling the range of tasks the agent may encounter as a distribution over all possible tasks. Existing meta-RL methods use dense coverage of task distributions, generating a very large set of training tasks used to meta-learn a policy that can quickly adapt to new, unseen, tasks. However, dense sampling of tasks is highly computationally expensive, and in some cases infeasible. Standard meta-RL methods have not considered these scenarios where a limited number of training tasks is available. Furthermore, they have so far not considered that the training tasks may not be equally informative, beneﬁcial, or promoting generalization. In this paper, we show that when a limited set of training tasks is available for training, not all tasks are necessarily beneﬁcial, and selecting a subset of training tasks may lead to a better performance in the test tasks. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We introduce an Information-Theoretic Task Selection (ITTS) algorithm, that ﬁlters the set of training tasks identifying a subset of tasks that are both different enough from one another, and relevant to tasks sampled from the target distribution. The method is independent of the meta-learning algorithm used. The outcome is a smaller training set, which can be learned more quickly and results in better performance than the original set.
Task selection is performed before meta-learning and in conjunction with an existing meta-learning algorithm. We identiﬁed ﬁve domains in the literature that have been used to assess existing meta-RL algorithms, and evaluated ITTS in the same settings. The results show that task-selection improves the performance of two meta-RL algorithms (RL2 [7] and MAML[8]) in all domains. We also introduce a sixth domain as an example of a real-world application on device control for micro grids, and use it to validate our approach in a realistic setting. 2