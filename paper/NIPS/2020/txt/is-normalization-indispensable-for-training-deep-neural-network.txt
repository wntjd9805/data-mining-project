Abstract
Normalization operations are widely used to train deep neural networks, and they can improve both convergence and generalization in most tasks. The theories for normalization’s effectiveness and new forms of normalization have always been hot topics in research. To better understand normalization, one question can be whether normalization is indispensable for training deep neural networks? In this paper, we analyze what would happen when normalization layers are removed from the networks, and show how to train deep neural networks without normalization layers and without performance degradation. Our proposed method can achieve the same or even slightly better performance in a variety of tasks: image classiﬁcation in ImageNet, object detection and segmentation in MS-COCO, video classiﬁcation in Kinetics, and machine translation in WMT English-German, etc. Our study may help better understand the role of normalization layers and can be a competitive alternative to normalization layers. Codes are available at https://github.com/ hukkai/rescaling. 1

Introduction
Deep neural networks have greatly advanced the benchmarks in many artiﬁcial intelligence appli-cations, such as image recognition [19], speech recognition [1], and natural language processing
[32], etc. However, training effective deep neural networks is often non-trivial, and beset by many problems, one of most annoying of which might be that of vanishing or exploding gradients, which directly relates to the problem of vanishing or exploding variance of a signal (i.e. an input) as it passes through the network [13]. Batch Normalization (BN) [17] greatly mitigates this problem. Since the introduction of BN, several variants have been proposed that apply the underlying principle to a wider range of tasks: Layer Normalization for recurrent neural networks [2], Instance Normalization (IN)
[33] for stylization, Group Normalization (GN) [36] for small-batch training, etc. Normalization operations are, by now, default components of the state of the art in many tasks.
Despite the popularity and success of normalization, the theory behind the effectiveness of such operations in neural networks is not yet fully understood. The original motivation provided in [17] is that BN can reduce internal covariate shift. Other studies [29, 22, 4, 38] also show several other advantages provided by normalization in terms of loss landscape, regularization, etc. However, most theoretical analyses require assumptions about the data/feature distribution, and the developed theories are only evaluated on small datasets. Analyzing the optimization and generalization of normalization in deep networks is still an open problem.
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Since the effectiveness of normalization is still almost a black box, a natural question arises: is it indispensable for training deep neural networks? There are, actually, two parts to this question: 1) Can training of non-normalized models be stable? 2) Can we train non-normalized models and achieve the same performance as the corresponding normalized models? The answer to the ﬁrst question is clear.
The simplest solution is to train the network with very a small learning rate (although this may lead to bad local minimum). A better solution is Fix-up initialization [40]. With this careful initialization, residual networks can be trained with large learning rate and limited performance degradation.
This paper focuses on the second question. By analyzing what would happen when normalization lay-ers are removed from the networks, we show how to train deep neural networks without normalization layers and without performance degradation.
We focus on the residual network (ResNet) [14], since residual learning tends to have better per-formance when the network is very deep. Our method starts with solving the vanishing/exploding variance problem (which is the main problem solved by normalization). Note that good initialization
[13] can mitigate this problem in a conventional non-residual network. However, the problem of explosion returns with residual connections of the form yi = xi + F(xi). We propose RescaleNet to handle this problem, with the new formulation yi = αixi + βiF(xi) where αi and βi are carefully selected constants. Concomitantly, we propose a novel network bias setting to compensate for the common problem of “dead” neurons that arise in un-normalized networks.
We validate our method on a wide range of tasks. On ImageNet, our un-normalized RescaleNet models can achieve the same or slightly better performance than the corresponding normalized models (ResNet, ResNext) with the same training settings. Our un-normalized RescaleNet variant on ResNet50 has 0.3% lower error than its BN/GN ResNet50 counterpart. Our method can also apply to conventional non-residual networks. Our 19 layer VGG [30] model without normalization achieves a top-1 validation error rate of 25.0%, which is 2.6% lower than PyTorch’s pre-trained model
[26]. Our method also shows consistent improvement on Mask R-CNN for COCO object detection and segmentation [20], 3D convolutional networks for Kinetics video classiﬁcation [18], and deep transformers for WMT English-German machine translation [34]. In cases where normalization operations may cause problems, our method can be a competitive alternative. 2