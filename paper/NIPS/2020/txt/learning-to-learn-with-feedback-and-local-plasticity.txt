Abstract
Interest in biologically inspired alternatives to backpropagation is driven by the desire to both advance connections between deep learning and neuroscience and address backpropagation’s shortcomings on tasks such as online, continual learn-ing. However, local synaptic learning rules like those employed by the brain have so far failed to match the performance of backpropagation in deep networks.
In this study, we employ meta-learning to discover networks that learn using feedback connections and local, biologically inspired learning rules. Importantly, the feedback connections are not tied to the feedforward weights, avoiding bio-logically implausible weight transport. Our experiments show that meta-trained networks effectively use feedback connections to perform online credit assignment in multi-layer architectures. Surprisingly, this approach matches or exceeds a state-of-the-art gradient-based online meta-learning algorithm on regression and classiﬁcation tasks, excelling in particular at continual learning. Analysis of the weight updates employed by these models reveals that they differ qualitatively from gradient descent in a way that reduces interference between updates. Our results support the view that biologically plausible learning mechanisms may not only match gradient descent-based learning, but also overcome its limitations.1 1

Introduction
Deep learning has achieved impressive success in solving complex tasks, and in some cases its learned representations have been shown to match those in the brain [14, 21, 23, 30, 36]. However, there is much debate over how well the backpropagation algorithm commonly used in deep learning resembles biological learning algorithms. Several key features of backpropagation do not obviously map onto biological implementations. One such feature is the requirement in backpropagation that feedback weights are exactly tied to feedforward weights, even as weights are updated with learning.
Another is that backpropagation applies the derivatives of the forward-pass nonlinearities during the feedback pass, which would require that feedback pathways have knowledge of the state of feedforward pathways, likely at some time offset. The question of how credit assignment – the communication of appropriate learning signals to neurons upstream of behavioral outputs – can be implemented by biological circuits remains open. It also remains unclear whether feedback pathways in neural circuits are best thought of as implementing an approximation to backpropagation, or some other qualitatively different learning algorithm.
We propose a learning paradigm that aims to solve the credit assignment problem in more biologically plausible fashion. Our approach is as follows: (1) apply local plasticity rules in a neural network to update feedforward synaptic weights, (2) endow the network with feedback connections that propagate information about target outputs to upstream neurons in order to guide this plasticity, and (3) employ meta-learning to optimize feedback weights, feedforward weight initializations, and rates of synaptic plasticity. The purpose of the meta-learned feedback is to modulate upstream activity in 1Source code for our experiments is available at github.com/jlindsey15/FeedbackAndLocalPlasticity 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
such a way that, when the local plasticity rule is applied, useful weight updates are performed. On a set of online regression and classiﬁcation learning tasks, we ﬁnd that meta-learned deep networks can successfully perform useful weight updates in non-readout layers. In fact, we ﬁnd that feedback with local learning rules can match and sometimes outperform gradient descent as a within-lifetime learning algorithm. 2