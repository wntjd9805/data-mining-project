Abstract
Existing neural network based one-class learning methods mainly use various forms of auto-encoders or GAN style adversarial training to learn a latent representation of the given one class of data. This paper proposes an entirely different approach based on a novel regularization, called holistic regularization (or H-regularization), which enables the system to consider the data holistically, not to produce a model that biases towards some features. Combined with a proposed 2-norm instance-level data normalization, we obtain an effective one-class learning method, called
HRN. To our knowledge, the proposed regularization and the normalization method have not been reported before. Experimental evaluation using both benchmark image classiﬁcation and traditional anomaly detection datasets show that HRN markedly outperforms the state-of-the-art existing deep/non-deep learning models.
The code of HRN can be found here3. 1

Introduction
One-class learning or classiﬁcation has many applications. For example, in information retrieval, one has a set of documents of interest and wants to identify more such documents [55]. Perhaps, the biggest application is in anomaly or novelty detection, e.g., intrusion detection, fraud detection, medical anomaly detection, anomaly detection in social networks and Internet of things, etc [8, 9].
Recently, image and video based applications have also become popular [13, 49, 70]. More details about these applications and others can be found in the recent survey [7, 61].
One-class learning: Let X be the space of all possible data. Let X ⊆ X be the set of all instances of a particular class. Given a training dataset T ⊆ X of the class, we want to learn a one-class classiﬁer f (x) : X → {0, 1}, where f (x) = 1 if x ∈ X (i.e., x is an instance of the class) and f (x) = 0 otherwise (i.e., x is not an instance of the class, e.g., an anomaly). In most applications, deciding whether a data instance belongs to the given training class or is an anomaly can be subjective and a threshold is often used based on the application. Like most existing papers [68, 64, 8, 82], this work is interested in a score function instead, and ignores the above binary decision problem. In this case, the commonly used evaluation metric is AUC (Area Under the ROC curve).
Early works on one-class classiﬁcation or learning include one-class SVM (OCSVM) [75], and
Support Vector Data Description (SVDD) [78]. More recently, deep learning models have been proposed for the same purpose [68, 8], which mainly learn a good latent representation of the given
∗Equal contribution
†Corresponding author. The work was done when B. Liu was at Peking University on leave of absence from
University of Illinois at Chicago, liub@uic.edu. 3https://github.com/morning-dews/HRN 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
class of data using various auto-encoders [1, 14, 65, 71, 83, 69] or GAN [27] style adversarial training [74, 72, 15, 64]. Recent surveys of one-class classiﬁcation can be found in [8, 37].
In this paper, we propose an entirely new one-class learning approach, which directly learns from a single class of data without using any auto-encoder or adversarial training technique. The key novelty of the proposed method is a new loss function (called one-class loss), which consists of negative log likelihood (NLL) for one class and a novel regularization method called holistic regularization (or
H-regularization). This new regularization constrains the model training so that it considers the one class of data holistically, not arbitrarily biases any features. We argue that one of the key issues of one-class learning is how to avoid biasing some features in model building as we have no idea where anomalies or negative data may be or what their distribution may be. Any bias can be detrimental.
This issue has not been explicitly addressed by existing approaches. Combined with a 2-norm instance-level normalization for each data instance (different from that in [79], see Sec. 3.2), we obtain an effective one-class learning method, called HRN (H-Regularization with 2-Norm instance-level normalization). To our knowledge, both H-regularization and the normalization method have not been reported in the literature. Empirical evaluation using three image classiﬁcation datasets widely used in evaluating one-class learners and three traditional benchmark anomaly detection datasets demonstrates the effectiveness of HRN. It outperforms eleven state-of-the-art baselines considerably.
On broader impact, we believe that our holistic one-class learning can help positive and unlabeled (PU) learning [52], open-world learning (or out-of-distribution detection) [23], and continual learning [11, 63] as all these learning paradigms need to face unseen/novel situations. We will brieﬂy discuss a continual learning method based on the proposed one-class loss, which achieves very good results. 2