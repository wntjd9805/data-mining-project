Abstract
Convolutional Neural Networks (CNNs) have proved exceptional at learning repre-sentations for visual object categorization. However, CNNs do not explicitly encode objects, parts, and their physical properties, which has limited CNNs’ success on tasks that require structured understanding of visual scenes. To overcome these lim-itations, we introduce the idea of “Physical Scene Graphs” (PSGs), which represent scenes as hierarchical graphs, with nodes in the hierarchy corresponding intuitively to object parts at different scales, and edges to physical connections between parts.
Bound to each node is a vector of latent attributes that intuitively represent ob-ject properties such as surface shape and texture. We also describe PSGNet, a network architecture that learns to extract PSGs by reconstructing scenes through a PSG-structured bottleneck. PSGNet augments standard CNNs by including: recurrent feedback connections to combine low and high-level image information; graph pooling and vectorization operations that convert spatially-uniform feature maps into object-centric graph structures; and perceptual grouping principles to encourage the identiﬁcation of meaningful scene elements. We show that PSGNet outperforms alternative self-supervised scene representation algorithms at scene segmentation tasks, especially on complex real-world images, and generalizes well to unseen object types and scene arrangements. PSGNet is also able learn from physical motion, enhancing scene estimates even for static images. We present a series of ablation studies illustrating the importance of each component of the PS-GNet architecture, analyses showing that learned latent attributes capture intuitive scene properties, and illustrate the use of PSGs for compositional scene inference. 1

Introduction
To make sense of their visual environment, intelligent agents must construct an internal representation of the complex visual scenes in which they operate. Recently, one class of computer vision algorithms –
Convolutional Neural Networks (CNNs) – has shown an impressive ability to extract useful categorical information from visual scenes. However, human perception (and the aim of computer vision) is not only about image classiﬁcation. Humans also group scenes into object-centric representations in which information about objects, their constituent parts, positions, poses, 3D geometric and physical material properties, and their relationships to other objects, are explicitly available. Such object-centric, geometrically-rich representations natively build in key cognitive concepts such as object permanence, and naturally support high-level visual planning and inference tasks. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Overview of the PSG representation and the PSGNet architecture. Brown boxes indicate the three stages of PSGNet: (1) Feature Extraction from visual input with a ConvRNN, (2) Graph Construction from
ConvRNN features, and (3) Graph Rendering for end-to-end training. Graph Construction consists of a pair of learnable modules, Graph Pooling and Graph Vectorization, that together produce a new, higher PSG level from an existing one. The former partitions existing PSG nodes into new cluster nodes, while the latter produces an attribute vector for each new node by summarizing the lower-level nodes. Three levels of an example PSG are shown (center, bottom) along with its quadratic texture (QTR) and shape (QSR) rendering (right.)
Recent work, such as MONet [4] and IODINE [17], has made initial progress in object-centric scene understanding. Learning from self-supervision, these models achieve some success at decomposing simple synthetic scenes into objects. However, these approaches impose little physical structure on their inferred scene representations and learn only from static images. As a result, they do not perform well on complex, real-world data. Recent approaches from 3D computer vision, such as 3D-RelNet [31], have attacked problems of physical understanding by combining key geometric structures (such as meshes) with more standard convolutional features. Such works achieve promising results, but require detailed ground truth supervision of scene structure.
In this work, we propose a new representation, which we call a Physical Scene Graph (PSG). The
PSG concept generalizes ideas from both the MONet/IODINE and the 3D-RelNet lines of work, seeking to simultaneously handle complex object shapes and textures, to explicitly decompose scenes into their physical parts, to support the top-down inferential reasoning capacities of generative models, and to learn from real-world or realistic visual data through self-supervision, so as to not require ground truth labeling of scene components. PSGs represent scenes as hierarchical graphs, with nodes toward the top of the PSG hierarchy intuitively corresponding to larger groupings (e.g., whole objects), those nearer the bottom corresponding more closely to the subparts of the object, and edges representing within-object “bonds” that hold the parts together. PSGs are spatially registered, with each node tied to a set of locations in image from which it is derived. Node attributes represent physically-meaningful properties of the object parts relevant to each level of the hierarchy, such as object position, surface normals, shape, and visual appearance.
Our key contribution is a family of self-supervised neural network architectures, PSGNets, that learn to estimate PSGs from visual inputs. PSGNets augment standard CNN architectures in several ways.
To efﬁciently combine high- and low-level visual information during initial feature extraction, we add local recurrent and long-range feedback connections on top of a CNN backbone, producing a convolutional recurrent neural network (ConvRNN). We introduce a learnable Graph Pooling operation that transforms spatially-uniform ConvRNN feature map input into object-centric graph outputs. We also introduce a novel way to summarize the properties of spatially-extended objects using a Graph Vectorization operation. A series of alternating Pooling and Vectorization modules forms a hierarchical graph constructor. To encourage the graph constructor to produce physically meaningful scene representations, we encode key cognitive principles of perceptual grouping into the model [45], including both static and motion-based grouping primitives. To ensure that learned node attributes contain disentangled geometric and visual properties of the latent object entities, we employ novel decoders that render PSGs, top-down, into scene reconstructions or predictions.
On several datasets, PSGNets substantially outperform alternative unsupervised approaches to scene description at segmentation tasks – especially for real-world images. We also ﬁnd that a PSGNet can 2
usefully exploit object motion both when it is explicitly available and in learning to understand static images, leading to large improvements in scene segmentation compared to an alternative motion-aware self-supervised method. The representations learned by PSGNets transfer well to objects and scenes never seen during training, suggesting that the strong constraints in the PSG structure force the system to learn general scene properties efﬁciently, from far less data than atlernative methods.
Finally, we show how the latent PSG structure identiﬁes key geometric and object-centric properties, making it possible to compositionally "edit" the objects and attributes of inferred scenes.