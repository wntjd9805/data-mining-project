Abstract
Adversarial Imitation Learning alternates between learning a discriminator – which tells apart expert’s demonstrations from generated ones – and a generator’s policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefﬁcient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Speciﬁcally, our discriminator is explicitly conditioned on two poli-cies: the one from the previous generator’s iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator’s policy. Con-sequently, our discriminator’s update solves the generator’s optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods. 1

Introduction
Imitation Learning (IL) treats the task of learning a policy from a set of expert demonstrations. IL is effective on control problems that are challenging for traditional Reinforcement Learning (RL) methods, either due to reward function design challenges or the inherent difﬁculty of the task itself
[1, 24].
Most IL work can be divided into two branches: Behavioral Cloning and Inverse Reinforcement
Learning. Behavioral Cloning casts IL as a supervised learning objective and seeks to imitate the expert’s actions using the provided demonstrations as a ﬁxed dataset [19]. Thus, Behavioral Cloning usually requires a lot of expert data and results in agents that struggle to generalize. As an agent deviates from the demonstrated behaviors – straying outside the state distribution on which it was trained – the risks of making additional errors increase, a problem known as compounding error [24].
∗Equal contribution.
†Work conducted while interning at Ubisoft Montreal’s La Forge R&D laboratory.
‡Canada CIFAR AI Chair. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Inverse Reinforcement Learning aims to reduce compounding error by learning a reward function under which the expert policy is optimal [1]. Once learned, an agent can be trained (with any RL algorithm) to learn how to act at any given state of the environment. Early methods were prohibitively expensive on large environments because they required training the RL agent to convergence at each learning step of the reward function [30, 1]. Recent approaches instead apply an adversarial formulation (Adversarial Imitation Learning, AIL) in which a discriminator learns to distinguish between expert and agent behaviors to learn the reward optimized by the expert. AIL methods allow for the use of function approximators and can in practice be used with only a few policy improvement steps for each discriminator update [13, 6, 4].
While these advances have allowed Imitation Learning to tackle bigger and more complex environ-ments [16, 3], they have also signiﬁcantly complexiﬁed the implementation and learning dynamics of
Imitation Learning algorithms. It is worth asking how much of this complexity is actually mandated.
For example, in recent work, Reddy et al. [20] have shown that competitive performance can be obtained by hard-coding a very simple reward function to incentivize expert-like behaviors and manage to imitate it through off-policy direct RL. Reddy et al. [20] therefore remove the reward learning component of AIL and focus on the RL loop, yielding a regularized version of Behavioral
Cloning. Motivated by these results, we also seek to simplify the AIL framework but following the opposite direction: keeping the reward learning module and removing the policy improvement loop.
We propose a simpler yet competitive AIL framework. Motivated by Finn et al. [4] who use the optimal discriminator form, we propose a structured discriminator that estimates the probability of demonstrated and generated behavior using a single parameterized maximum entropy policy.
Discriminator learning and policy learning therefore occur simultaneously, rendering seamless generator updates: once the discriminator has been trained for a few epochs, we simply use its policy model to generate new rollouts. We call this approach Adversarial Soft Advantage Fitting (ASAF).
We make the following contributions:
• Algorithmic: we present a novel algorithm (ASAF) designed to imitate expert demonstrations without any Reinforcement Learning step.
• Theoretical: we show that our method retrieves the expert policy when trained to optimality.
• Empirical: we show that ASAF outperforms prevalent IL algorithms on a variety of discrete and continuous control tasks. We also show that, in practice, ASAF can be easily modiﬁed to account for different trajectory lengths (from full length to transition-wise). 2