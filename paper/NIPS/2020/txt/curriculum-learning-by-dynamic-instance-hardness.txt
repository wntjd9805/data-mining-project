Abstract
A good teacher can adjust a curriculum based on students’ learning history. By analogy, in this paper, we study the dynamics of a deep neural network’s (DNN) performance on individual samples during its learning process. The observed properties allow us to develop an adaptive curriculum that leads to faster learning of more accurate models. We introduce dynamic instance hardness (DIH), the exponential moving average of a sample’s instantaneous hardness (e.g., a loss, or a change in output) over the training history. A low DIH indicates that a model retains knowledge about a sample over time. For DNNs, we ﬁnd that a sample’s
DIH early in training predicts its DIH in later stages. Hence, we can train a model using samples mostly with higher DIH and safely deprioritize those with lower
DIH. This motivates a DIH guided curriculum learning (DIHCL) procedure.
Compared to existing CL methods: (1) DIH is more stable over time than using only instantaneous hardness, which is noisy due to stochastic training and DNN’s non-smoothness; (2) DIHCL is computationally inexpensive since it uses only a byproduct of back-propagation and thus does not require extra inference. On 11 datasets, DIHCL signiﬁcantly outperforms random mini-batch SGD and recent
CL methods in terms of efﬁciency and ﬁnal performance. The code of DIHCL is available at https://github.com/tianyizhou/DIHCL. 1

Introduction
A curriculum plays an important role in human learning. Given different curricula of the same training materials, students’ learning efﬁciency and performance can vary drastically. A good teacher is able to choose the contents of the next stage of learning according to a student’s past performance.
Analogously, in machine learning, instead of training the model with a random sequence of data, recent work in curriculum learning (CL) [4, 25, 17, 52, 13] shows that manipulating the sequence of training data can improve both training efﬁciency and model accuracy. In each epoch, CL selects a subset of training samples based on the difﬁculty and/or the informativeness of each sample — this is usually measured using instantaneous feedback from the model (e.g., the loss). CL then uses only these samples to update the model. Inspired by human learning curricula, a schedule of training samples is constructed (e.g., usually from easy to hard), sometimes combining with other criteria (e.g., diversity). As exhibited in previous work, CL can help to avoid local minima, improve the training efﬁciency, and can lead to better generalization performance.
Instantaneous hardness, however, does not take the training history of each sample into account.
When applied to deep neural network (DNNs) training, and due to the non-smooth/non-convex nature of the loss and the randomness of stochastic gradient descent (SGD), the instantaneous hardness of each sample can change dramatically between consecutive epochs, so it is not reﬂective of the utility of each sample in the future. This results in a large difference between training sets selected over successive epochs, leading to an inconsistency of optimization objectives and gradients, and making
⇤Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
training less stable. Furthermore, keeping instantaneous hardness up to date requires extra inference steps of a model over all the samples, which can be expensive for DNNs [7, 15]. Though some recent work ﬁnds that data selection within each mini-batch [18] or based on the latest evaluated (but outdated) loss [28] may still perform well, this selection can be sub-optimal and unstable.
In this paper, we study the training dynamics of DNNs on individual samples from which a more accurate hardness measure can be computed that does not require extra inference and that can signiﬁcantly improve performance. We study the difﬁculty a model has over time (i.e., training epochs) in learning each training sample. We introduce “dynamic instance hardness (DIH)” as the exponential moving average of an instantaneous hardness measure of a sample over time. We use three types of instantaneous hardness to compute DIH (fully deﬁned in Section 2): the loss; the loss change; and the prediction ﬂip (the 0-1 indicator of whether the prediction correctness changes) between two consecutive time steps. The ﬁrst has been commonly used in CL, while the latter two capture a form of momentum of the loss/prediction.
We exploit several DIH properties that enable more effective CL approaches. Firstly, DIH can vary dramatically between different samples. Samples with smaller DIH seem to be more memorable (i.e., are retained more easily), while samples with larger DIH are harder to learn and retain. While the model is more likely to stay at a minimum of the easy samples’ loss, its prediction on the hard samples is less stable under changes in optimization parameters (e.g., the learning rate). Secondly, unlike instantaneous hardness, the DIH status of a sample becomes consistent only after a few epochs.
That is, a sample’s DIH value converges quickly to its ﬁnal relative position amongst all of the samples. For example, if a sample’s DIH quickly becomes small, it stays small relative to the other samples; if it becomes large, it stays there. We can therefore accurately identify categories of hard and easy samples relatively early in the course of training. Thirdly, the DIH of each sample tends to monotonically decrease during training. This implies that the learning process strives for better local minimum for all samples, i.e., while easy samples stay easy throughout training, the hard samples also become easier the more we train on them.
These properties motivate a natural curriculum learning strategy “DIH guided curriculum learning (DIHCL)” that keeps training the model on those samples that have historically been hard since the model does not perform well on them. By contrast, it is safe to revisit easy samples (those with small DIH values) less frequently because the model is more likely to stay at those samples’ minima.
Hence, DIHCL helps a model focus on that which it ﬁnds difﬁcult. This is similar to strategies that improve human learning, such as the Leitner system for spaced repetition [26]. This is also analogous to boosting [37] — in boosting, however, we average the instantaneous sample performance of multiple weak learners at the current time, while in DIHCL we average the instantaneous sample performance of one strong learner over the training history.
At each training step, DIHCL selects a subset of samples according to their DIH values, where the hard samples have higher probabilities of being selected relative to the easy samples. The model is updated by (stochastic) gradients computed on the selected samples. We then update the DIH of the selected samples by using their instantaneous hardness, a byproduct of back-propagation (since it needs to perform inference at ﬁrst, e.g., a forward-propagation of a DNN). This signiﬁcantly improves the efﬁciency of previous CL methods, which rely on extra inference steps to evaluate the instantaneous hardness of all the samples. Here, it is safe to update only the DIH of the selected samples since the unselected ones have smaller and decreasing DIH values (due to the observed properties of DIH) and thus keeping a stale DIH for them will not reduce their chance of being selected in the future steps. As mentioned earlier, in the training of DNNs, the hardness ranking of each sample by DIH will quickly converge after a few training steps and remains consistent for future steps. Those early steps provide the opportunity for the necessary exploration to ensure that hardness ranking is via DIH is accurate. To improve the exploration efﬁciency, DIHCL sweeps through the entire training set for the ﬁrst few epochs and then starts to select training samples by
DIH-weighted subset (random) sampling, and we gradually decrease the subset size during training.
We provide several options for weighted sampling, using different distributions, and we integrate subset diversity into the selection criteria as well when feasible. Empirically, we evaluate several variants of DIHCL and compare them against random mini-batch SGD as well as recent curriculum learning algorithms on 11 datasets. DIHCL shows an advantage over other baselines in terms both of time/sample efﬁciency and test set accuracy. 2
1.1