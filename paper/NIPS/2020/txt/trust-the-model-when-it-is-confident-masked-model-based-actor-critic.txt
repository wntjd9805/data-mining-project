Abstract
It is a popular belief that model-based Reinforcement Learning (RL) is more sample efﬁcient than model-free RL, but in practice, it is not always true due to overweighed model errors. In complex and noisy settings, model-based RL tends to have trouble using the model if it does not know when to trust the model.
In this work, we ﬁnd that better model usage can make a huge difference. We show theoretically that if the use of model-generated data is restricted to state-action pairs where the model error is small, the performance gap between model and real rollouts can be reduced. It motivates us to use model rollouts only when the model is conﬁdent about its predictions. We propose Masked Model-based Actor-Critic (M2AC), a novel policy optimization algorithm that maximizes a model-based lower-bound of the true value function. M2AC implements a masking mechanism based on the model’s uncertainty to decide whether its prediction should be used or not. Consequently, the new algorithm tends to give robust policy improvements.
Experiments on continuous control benchmarks demonstrate that M2AC has strong performance even when using long model rollouts in very noisy environments, and it signiﬁcantly outperforms previous state-of-the-art methods. 1

Introduction
Deep RL has achieved great successes in complex decision-making problems [17, 21, 9]. Most of the advances are due to deep model-free RL, which can take high-dimensional raw inputs to make decisions without understanding the dynamics of the environment. Although having good asymptotic performance, current model-free RL methods usually require a tremendous number of interactions with the environment to learn a good policy. On the other hand, model-based reinforcement learning (MBRL) enhances sample efﬁciency by searching the policy under a ﬁtted model that approximates the true dynamics, so it is favored in problems where only a limited number of interactions are available. For example, in continuous control, recently model-based policy optimization (MBPO,
[11]) yields comparable results to state-of-the-art model-free methods with much fewer samples, which is appealing for real applications.
However, there is a fundamental concern of MBRL [7] that learning a good policy requires an accurate model, which in turn requires a large number of interactions with the true environment.
For this issue, theoretical results of MBPO [11] suggest to use the model when the model error is
“sufﬁciently small”, which contradicts the intuition that MBRL should be used in low-data scenario.
In other words, “when to trust the model?” remains an open problem, especially in settings with nonlinear and noisy dynamics and one wants to use a deep neural network as the model.
⇤Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Due to the model bias, existing deep MBRL methods tend to perform poorly when 1) the model overﬁts in low-data regime, 2) the algorithm runs long-horizon model rollouts, and 3) the true dynamic is complex and noisy. As a result, most state-of-the-art MBRL algorithms use very short model rollouts (usually less than four steps [7, 3, 16, 11]), and the applicability is often limited in settings with deterministic dynamics. We show two motivating examples in Figure 1, which demonstrates that MBPO [11], a state-of-the-art MBRL method, indeed suffers from the mentioned limitations. These issues make them impractical to use in real-world problems.
In this work, we introduce Masked Model-based Actor-Critic (M2AC), which alle-viates the mentioned issues by reducing large inﬂuences of model errors through a masking mechanism. We derive theoreti-cal results that if the model is only used when the model error is small, the gap between the real return and the masked model rollout value can be bounded. This result motivates us to design a practical al-gorithm M2AC by interpolating masked model-based rollouts with model-free ex-perience replay. We empirically ﬁnd the new algorithm sample efﬁcient as well as robust across various tasks.
Our contributions are outlined as follows.
Figure 1: Two motivating examples. Left: returns in
HalfCheetah-noisy2 environments at 25k steps. Previ-ous method is sensitive to model rollout length because the model error accumulates as the length increases, while our method (M2AC) can beneﬁt from long roll-outs. Right: returns at 25k steps in HalfCheetah and three noisy derivatives with different levels of random-ness. The scores of MBPO drop rapidly when the envi-ronment becomes noisy, while M2AC is more robust.
•
•
•
We put forward masked model rollouts and derive a general value bounds for Masked
Model-based Policy Iteration. We prove that the gap between the model-based and the true
Q-values is bounded and can be reduced via better model usage.
We propose a simple but powerful algorithm named Masked Model-based Actor-Critic (M2AC). It reduces the inﬂuences of model error with a masking mechanism that “trusts the model when it is conﬁdent” and eliminates unreliable model-generated samples.
Extensive experiments on continuous control show that M2AC has high sample efﬁciency that it reaches comparable returns to state-of-the-art model-free methods with much fewer interactions, and is robust even when using long model rollouts in very noisy environments. 2