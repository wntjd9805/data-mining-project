Abstract
Important insights towards the explainability of neural networks reside in the char-acteristics of their decision boundaries. In this work, we borrow tools from the ﬁeld of adversarial robustness, and propose a new perspective that relates dataset features to the distance of samples to the decision boundary. This enables us to carefully tweak the position of the training samples and measure the induced changes on the boundaries of CNNs trained on large-scale vision datasets. We use this framework to reveal some intriguing properties of CNNs. Speciﬁcally, we rigorously conﬁrm that neural networks exhibit a high invariance to non-discriminative features, and show that the decision boundaries of a DNN can only exist as long as the classiﬁer is trained with some features that hold them together. Finally, we show that the construction of the decision boundary is extremely sensitive to small perturbations of the training samples, and that changes in certain directions can lead to sudden invariances in the orthogonal ones. This is precisely the mechanism that adversarial training uses to achieve robustness. 1

Introduction
The set of points that partitions the input space onto labeled regions is known as the decision boundary of a classiﬁer. Describing how a classiﬁer creates such boundaries is crucial for its explainability.
Interestingly, even when deep networks succeed on a task, their high vulnerability to imperceptible perturbations [1, 2] implies that their boundaries lie alarmingly close to any input sample. This unintuitive behaviour contradicts the common belief that a successful classiﬁer should be invariant to non-discriminative information of its input data. However, it seems that such perturbations are not irrelevant signals, but rather discriminative features of the training set [3, 4].
In that sense, explaining the mechanisms that construct the decision boundary of deep neural networks is key to understand the dynamics of adversarial training [5]. This training scheme only differs from standard training in that it slightly perturbs the training samples during optimization. However, these small changes can utterly change the geometry of these classiﬁers [6].
An example of such change can be seen in Fig. 1, which shows the minimal perturbations – constrained to lie on a low and a high frequency subspace – required to ﬂip the decision of a network. The norm of the perturbations measures the distance (margin) to the decision boundary in these subspaces. Clearly, reaching the boundary using high frequency perturbations requires much more energy than using
∗Equal contribution. Correspondence to {guillermo.ortizjimenez, apostolos.modas}@epfl.ch.
The code to reproduce our experiments can be found at https://github.com/LTS4/hold-me-tight. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Minimal adversarial perturbations constrained to lie in different DCT frequency bands (8 × 8 subspaces taken from the top left and bottom right of the 224 × 224 DCT matrix) for a
ResNet-50 trained (left), and adversarially trained (right) on ImageNet. low frequency ones [7]. But surprisingly, when the network is adversarially trained [5], the largest increase in margin happens in the high frequency subspace. Note that, on the standard network, this distance is already much greater than the size of the training perturbations. Based on this observation, we pose the following questions: 1. How is the margin in different directions related to the features in the training data? 2. How can very small perturbations signiﬁcantly change the geometry of deep networks?
In this work, we propose a novel approach to answer these questions. In particular, we develop a new methodology to construct a local summary of the decision boundary of a neural network from margin observations along a sequence of orthogonal directions. This framework permits to carefully tweak the properties of the training samples and measure the induced changes on the boundaries of convolutional neural networks (CNNs) trained on synthetic and large-scale vision datasets (e.g., ImageNet). The main contributions of our work are the following: 1. We provide a new perspective on the relationship between the distance of a set of samples to the boundary, and the discriminative features used by a network. We empirically support our ﬁndings by extensive evaluations on both synthetic and real datasets. 2. Via a series of carefully designed experiments, we rigorously conﬁrm the “common belief” that CNNs tend to behave as ideal classiﬁers and are approximately invariant to non-discriminative features of a dataset. 3. We further show that the construction of the decision boundary is extremely sensitive to the position of the training samples, such that very small perturbations in certain directions can utterly change the decision boundaries in some orthogonal directions. 4. Finally, we demonstrate that adversarial training exploits this training sensitivity and invari-ance bias to build robust classiﬁers.
We believe that the perspective proposed in this paper can have implications in future research on explainability and robustness, as it gives a new way to measure and understand decision boundaries.
This new framework can be used to shed light onto the dynamics and inductive bias of deep learning.