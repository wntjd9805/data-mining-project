Abstract
On-device learning enables edge devices to continually adapt the AI models to new data, which requires a small memory footprint to ﬁt the tight memory constraint of edge devices. Existing work solves this problem by reducing the number of trainable parameters. However, this doesn’t directly translate to memory saving since the major bottleneck is the activations, not parameters. In this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efﬁcient on-device learning.
TinyTL freezes the weights while only learns the bias modules, thus no need to store the intermediate activations. To maintain the adaptation capacity, we introduce a new memory-efﬁcient bias module, the lite residual module, to reﬁne the feature extractor by learning small residual feature maps adding only 3.8% memory overhead. Extensive experiments show that TinyTL signiﬁcantly saves the memory (up to 6.5×) with little accuracy loss compared to ﬁne-tuning the full network. Compared to ﬁne-tuning the last layer, TinyTL provides signiﬁcant accuracy improvements (up to 34.1%) with little memory overhead. Furthermore, combined with feature extractor adaptation, TinyTL provides 7.3-12.9× memory saving without sacriﬁcing accuracy compared to ﬁne-tuning the full Inception-V3. 1

Introduction
Intelligent edge devices with rich sensors (e.g., billions of mobile phones and IoT devices)1 have been ubiquitous in our daily lives. These devices keep collecting new and sensitive data through the sensor every day while being expected to provide high-quality and customized services without sacriﬁcing privacy2. These pose new challenges to efﬁcient AI systems that could not only run inference but also continually ﬁne-tune the pre-trained models on newly collected data (i.e., on-device learning).
Though on-device learning can enable many appealing applications, it is an extremely challenging problem. First, edge devices are memory-constrained. For example, a Raspberry Pi 1 Model A only has 256MB of memory, which is sufﬁcient for inference, but by far insufﬁcient for training (Figure 1 left), even using a lightweight neural network architecture (MobileNetV2 [1]). Furthermore, the memory is shared by various on-device applications (e.g., other deep learning models) and the operating system. A single application may only be allocated a small fraction of the total memory, which makes this challenge more critical. Second, edge devices are energy-constrained. DRAM access consumes two orders of magnitude more energy than on-chip SRAM access. The large memory footprint of activations cannot ﬁt into the limited on-chip SRAM, thus has to access DRAM.
For instance, the training memory of MobileNetV2, under batch size 16, is close to 1GB, which is by far larger than the SRAM size of an AMD EPYC CPU3 (Figure 1 left), not to mention lower-end 1https://www.statista.com/statistics/330695/number-of-smartphone-users-worldwide/ 2https://ec.europa.eu/info/law/law-topic/data-protection_en 3https://www.amd.com/en/products/cpu/amd-epyc-7302 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Left: The memory footprint required by training is much larger than inference. Right:
Memory cost comparison between ResNet-50 and MobileNetV2-1.4 under batch size 16. Recent advances in efﬁcient model design only reduce the size of parameters, but the activation size, which is the main bottleneck for training, does not improve much. edge platforms. If the training memory can ﬁt on-chip SRAM, it will drastically improve the speed and energy efﬁciency.
There is plenty of efﬁcient inference techniques that reduce the number of trainable parameters and the computation FLOPs [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], however, parameter-efﬁcient or FLOPs-efﬁcient techniques do not directly save the training memory. It is the activation that bottlenecks the training memory, not the parameters. For example, Figure 1 (right) compares ResNet-50 and MobileNetV2-1.4. In terms of parameter size, MobileNetV2-1.4 is 4.3× smaller than ResNet-50. However, for training activation size, MobileNetV2-1.4 is almost the same as ResNet-50 (only 1.1× smaller), leading to little memory reduction. It is essential to reduce the size of intermediate activations required by back-propagation, which is the key memory bottleneck for efﬁcient on-device training.
In this paper, we propose Tiny-Transfer-Learning (TinyTL) to address these challenges. By analyzing the memory footprint during the backward pass, we notice that the intermediate activations (the main bottleneck) are only needed when updating the weights, not the biases (Eq. 2). Inspired by this ﬁnding, we propose to freeze the weights of the pre-trained feature extractor and only update the biases to reduce the memory footprint (Figure 2b). To compensate for the capacity loss, we introduce a memory-efﬁcient bias module, called lite residual module, which improves the model capacity by reﬁning the intermediate feature maps of the feature extractor (Figure 2c). Meanwhile, we aggressively shrink the resolution and width of the lite residual module to have a small memory overhead (only 3.8%). Extensive experiments on 9 image classiﬁcation datasets with the same pre-trained model (ProxylessNAS-Mobile [11]) demonstrate the effectiveness of TinyTL compared to previous transfer learning methods. Further, combined with a pre-trained once-for-all network [10],
TinyTL can select a specialized sub-network as the feature extractor for each transfer dataset (i.e., feature extractor adaptation): given a more difﬁcult dataset, a larger sub-network is selected, and vice versa. TinyTL achieves the same level of (or even higher) accuracy compared to ﬁne-tuning the full
Inception-V3 while reducing the training memory footprint by up to 12.9×. Our contributions can be summarized as follows:
• We propose TinyTL, a novel transfer learning method to reduce the training memory footprint by an order of magnitude for efﬁcient on-device learning. We systematically analyze the memory of training and ﬁnd the bottleneck comes from updating the weights, not biases (assume ReLU activation).
• We also introduce the lite residual module, a memory-efﬁcient bias module to improve the model capacity with little memory overhead.
• Extensive experiments on transfer learning tasks show that our method is highly memory-efﬁcient and effective. It reduces the training memory footprint by up to 12.9× without sacriﬁcing accuracy. 2