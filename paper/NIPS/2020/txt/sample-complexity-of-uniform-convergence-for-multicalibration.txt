Abstract
There is a growing interest in societal concerns in machine learning systems, espe-cially in fairness. Multicalibration gives a comprehensive methodology to address group fairness. In this work, we address the multicalibration error and decouple it from the prediction error. The importance of decoupling the fairness metric (multicalibration) and the accuracy (prediction error) is due to the inherent trade-off between the two, and the societal decision regarding the “right tradeoff” (as imposed many times by regulators). Our work gives sample complexity bounds for uniform convergence guarantees of multicalibration error, which implies that regardless of the accuracy, we can guarantee that the empirical and (true) multi-calibration errors are close. We emphasize that our results: (1) are more general than previous bounds, as they apply to both agnostic and realizable settings, and do not rely on a speciﬁc type of algorithm (such as differentially private), (2) improve over previous multicalibration sample complexity bounds and (3) implies uniform convergence guarantees for the classical calibration error. 1

Introduction
Data driven algorithms inﬂuence our everyday lives. While they introduce signiﬁcant achievements in face recognition, to recommender systems and machine translation, they come at a price. When deployed for predicting outcomes that concern individuals, such as repaying a loan, surviving surgery, or skipping bail, predictive systems are prone to accuracy disparities between different social groups that often induce discriminatory results. These signiﬁcant societal issues arise due to a variety of reasons: problematic analysis, unrepresentative data and even inherited biases against certain social groups due to historical prejudices. At a high level, there are two separate notions of fairness: individual fairness and group fairness. Individual fairness is aimed to guarantee fair prediction to each given individual, while group fairness aggregates statistics of certain subpopulations, and compares them. There is a variety of fairness notions for group fairness, such as demographic parity, equalized odds, equalized opportunity, and more (see Barocas et al. (2019)). Our main focus would be on multicalibration criteria for group fairness Hebert-Johnson et al. (2018). Multicalibration of a predictor is deﬁned as follows. There is a prespeciﬁed set of subpopulations of interest. The predictor returns a value for each individual (which can be interpreted as a probability). The multicalibration requires that for any “large” subpopulation, and for any value which is predicted “frequently” on that subpopulation, the predicted value and average realized values would be close on this subpopulation.
Note that calibration addresses the relationship between the predicted and average realized values, and is generally unrelated to the prediction quality. For example, if a population is half positive and half negative, a predictor that predicts for every individual a value of 0.5 is perfectly calibrated but has poor accuracy. The work of Hebert-Johnson et al. (2018) proposes a speciﬁc algorithm to ﬁnd a multicalibrated predictor and derived its sample complexity. The work of Liu et al. (2018) related the calibration error to the prediction loss, speciﬁcally, it bounds the calibration error as a function of the
∗First two authors have equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
difference between the predictor loss and the Bayes optimal prediction loss. Their bound implies that in a realizable setting, where the Bayes optimal hypothesis is in the class, using ERMyields a vanishing calibration error, but in an agnostic setting this does not hold. With the motivation of fairness in mind, it is important to differentiate between the prediction loss and the calibration error.
In many situations, the society (through regulators) might sacriﬁce prediction loss to improve fairness, and the right trade-off between them may be task dependent. On the other hand, calibration imposes self-consistency, namely, that predicted values and the average realized values should be similar for any protected group. In particular, there is no reason to prefer un-calibrated predictors over calibrated ones, assuming they have the same prediction loss. An important concept in this regard is uniform convergence. We would like to guarantee that the multicalibration error on the sample and the true multicalibration error are similar. This will allow society to rule-out un-calibrated predictors when optimizing over accuracy and other objectives that might depend on the context and the regulator.
Our main results in this work are sample bounds that guarantee uniform convergence of a given class of predictors. We start by deriving a sample bound for the case of a ﬁnite hypothesis class, and derive a sample complexity bound which is logarithmic in the size of the hypothesis class. Later, for an inﬁnite hypothesis class, we derive a sample bound that depends on the graph dimension of the class (which is an extension of the VC dimension for multiclass predictions). Finally, we derive a lower bound on the sample size required.
Technically, an important challenge in deriving the uniform convergence bounds is that the multicali-bration error depends, not only on the correct labeling but also on the predictions by the hypothesis, similar in spirit to the internal regret notion in online learning. We remark that these techniques are suitable to reproduce generalization bounds for other complex measures such as F-score.
We stress that in contrast to previous works that either attained speciﬁc efﬁcient algorithms for ﬁnding calibrated predictors Hebert-Johnson et al. (2018) or provided tight connections between calibration error and prediction loss (mainly in the realizable case) Liu et al. (2019), we take a different approach.
We concentrate on the statistical aspects of generalization bounds rather than algorithmic ones, and similar to much of the generalization literature in machine learning derive generalization bounds over calibration error for any predictor class with a ﬁnite size or a ﬁnite graph dimension.
Nevertheless, our work does have algorithmic implications. For example, similarly to running ERM, running empirical multicalibration risk minimization over a hypothesis class with bounded complexity
H and “large enough” training set, would output a nearly-multicalibrated predictor, assuming one exists. We guarantee that the empirical and true errors of this predictor would be similar, and derive the required sample size either as a function of the logarithm of the size of the predictor class or of its
ﬁnite graph dimension. Our bounds improve over previous sample complexity bounds and also apply in more general settings (e.g., agnostic learning). So while multicalibration uniform convergence is not formally necessary for learning multicalibrated predictors, the advantage of our approach is that the learner remains with the freedom to choose any optimization objectives or algorithms, and would still get a good estimation of the calibration error. To the best of our knowledge, this also introduces the ﬁrst uniform convergence results w.r.t. calibration as a general notion (i.e., even not as a fairness notion).