Abstract
We present a novel texture synthesis framework, enabling the generation of inﬁnite, high-quality 3D textures given a 2D exemplar image. Inspired by recent advances in natural texture synthesis, we train deep neural models to generate textures by non-linearly combining learned noise frequencies. To achieve a highly realistic output conditioned on an exemplar patch, we propose a novel loss function that combines ideas from both style transfer and generative adversarial networks. In particular, we train the synthesis network to match the Gram matrices of deep features from a discriminator network. In addition, we propose two architectural concepts and an extrapolation strategy that signiﬁcantly improve generalization performance. In particular, we inject both model input and condition into hidden network layers by learning to scale and bias hidden activations. Quantitative and qualitative evaluations on a diverse set of exemplars motivate our design decisions and show that our system performs superior to previous state of the art. Finally, we conduct a user study that conﬁrms the beneﬁts of our framework. 1

Introduction
Texture synthesis is an important ﬁeld of research with a multitude of applications in both computer vision and graphics. While vision applications mainly focus on texture analysis and understanding, in computer graphics textures are most important in the ﬁeld of image manipulation and photo-realistic rendering. In this context, applications require synthesis techniques that can generate highly realistic textures. Due to the problem being highly complex, today’s applications largely rely on techniques that generate textures based on real photos, which is a time-consuming process that requires expert knowledge. In this work, we present a novel texture synthesis framework that can generate high-quality 3D textures given an exemplar image as input (See Figure 1).
Techniques for texture synthesis can be classiﬁed into photogrammetric and procedural techniques.
Photogrammetry relies on methods that capture the color appearance of physical objects to obtain (typically several) photos from the target texture. Next, texture mapping algorithms are employed to supply a digital 3D model, usually a polygon mesh, with texture coordinates that deﬁne a mapping from 3D surface points to 2D texture pixels. Such photogrammetric approaches yield high-quality textures but rely on time-consuming procedures that require expert knowledge. Moreover, the required projection from 3D to 2D introduces seam and distortion artifacts, for example when texturing spherical objects. Nevertheless, this is still a widely used technique, despite its shortcomings.
An entirely different class of techniques is referred to as procedural texturing, where textures are generated by an algorithm instead of relying on acquired data. Ideally the algorithm takes a 3D position as input and computes a texture color value as output, omitting the error-prone projection step entirely. Pioneering work in procedural texturing is the Perlin noise texture model [1], which 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Results produced by our framework. Given an exemplar texture image (left), our model can faithfully resynthesize it inﬁnitively (middle), suitable to texture 3D models (right). dates back to 1985. The core idea is to linearly combine noise frequencies, or octaves, to generate different textures. An artist manually adjusts octave weights to obtain a speciﬁc look. While this early approach conveniently avoids both data acquisition and texture mapping, its applicability to natural textures is rather limited mainly due to the predeﬁned, ﬁxed noise frequencies. Moreover, a user has to manually tweak the parameters in order to obtain a desired look, which is a tedious process.
Recently, several techniques that leverage deep learning have been proposed to train generative models for procedural texture synthesis. Most of these leverage convolutional neural networks (CNNs) to generate texture pixel values as a function of spatially neighboring data. Recent examples in this direction either use generative adversarial networks (GANs) [2] or a style loss [3] to optimize their models. While these CNN-based approaches yield high-quality textures, in practice their application is limited to either 2D textures or very low resolution 3D textures due to the high memory footprint of 3D CNNs. Moreover, an unsolved problem with these techniques is the lack of diversity or rather stochasticity, i.e. such models struggle producing diverse results that represent the same texture.
Despite attempts to mitigate this problem by explicitly minimizing a diversity term [4], true diversity has not been demonstrated by this approach.
Our work is inspired by to the model proposed by Henzler et al. [5], which features both memory efﬁciency and diversity. The former is achieved by formulating the texture synthesis problem as a point operation parametrized using a multilayer perceptron (MLP), while the latter is achieved by deﬁning the model to be a function of noise frequencies, similarly to Perlin. However, this ﬂexibility comes at the cost of lower image quality and a lack of similarity to the exemplar texture.
We present a point operation model that learns to generate textures as a function of noise frequencies, but we propose a novel loss function and architectural framework, which together improve image quality signiﬁcantly. Moreover, our proposed network architecture inherently supports an extrapo-lation technique that improves generalization to unseen textures, that can be substantially different from the training set exemplars. In particular, we make the following contributions: (1) a deep MLP architecture that effectively combines learned noise frequencies; (2) a novel GAN loss inspired by style transfer to achieve both realistic image quality and high similarity to the input exemplar; and (3) a novel extrapolation strategy that signiﬁcantly improves the results on unseen exemplars. 2