Abstract
The objective of this paper is visual-only self-supervised video representation learn-ing. We make the following contributions: (i) we investigate the beneﬁt of adding semantic-class positives to instance-based Info Noise Contrastive Estimation (In-foNCE) training, showing that this form of supervised contrastive learning leads to a clear improvement in performance; (ii) we propose a novel self-supervised co-training scheme to improve the popular infoNCE loss, exploiting the com-plementary information from different views, RGB streams and optical ﬂow, of the same data source by using one view to obtain positive class samples for the other; (iii) we thoroughly evaluate the quality of the learnt representation on two different downstream tasks: action recognition and video retrieval. In both cases, the proposed approach demonstrates state-of-the-art or comparable performance with other self-supervised approaches, whilst being signiﬁcantly more efﬁcient to train, i.e. requiring far less training data to achieve similar performance. 1

Introduction
The recent progress in self-supervised representation learning for images and videos has demonstrated the beneﬁts of using a discriminative contrastive loss on data samples [12, 13, 27, 28, 45, 59], such as NCE [24, 34]. Given a data sample, the objective is to discriminate its transformed version against other samples in the dataset. The transformations can be artiﬁcial, such as those used in data augmentation [12], or natural, such as those arising in videos from temporal segments within the same clip. In essence, these pretext tasks focus on instance discrimination: each data sample is treated as a
‘class’, and the objective is to discriminate its own augmented version from a large number of other data samples or their augmented versions. Representations learned by instance discrimination in this manner have demonstrated extremely high performance on downstream tasks, often comparable to that achieved by supervised training [12, 27].
In this paper, we target self-supervised video representation learning, and ask the question: is instance discrimination making the best use of data? We show that the answer is no, in two respects:
First, we show that hard positives are being neglected in the self-supervised training, and that if these hard positives are included then the quality of learnt representation improves signiﬁcantly.
To investigate this, we conduct an oracle experiment where positive samples are incorporated into the instance-based training process based on the semantic class label. A clear performance gap is observed between the pure instance-based learning (termed InfoNCE [59]) and the oracle version (termed UberNCE). Note that the oracle is a form of supervised contrastive learning, as it encourages linear separability of the feature representation according to the class labels. In our experiments, training with UberNCE actually outperforms the supervised model trained with cross-entropy, a phenomenon that is also observed in a concurrent work [36] for image classiﬁcation. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Two video clips of a golf-swing action and their corresponding optical ﬂows. In this example, the
ﬂow patterns are very similar across different video instances despite signiﬁcant variations in RGB space. This observation motivates the idea of co-training, which aims to gradually enhance the representation power of both networks, f1(·) and f2(·), by mining hard positives from one another.
Second, we propose a self-supervised co-training method, called CoCLR, standing for ‘Co-training
Contrastive Learning of visual Representation’, with the goal of mining positive samples by using other complementary views of the data, i.e. replacing the role of the oracle. We pick RGB video frames and optical ﬂow as the two views from hereon. As illustrated in Figure 1, positives obtained from ﬂow can be used to ‘bridge the gap’ between the RGB video clips instances. In turn, positives obtained from RGB video clips can link optical ﬂow clips of the same action. The outcome of training with the CoCLR algorithm is a representation that signiﬁcantly surpasses the performance obtained by the instance-based training with InfoNCE, and approaches the performance of the oracle training with UberNCE.
To be clear, we are not proposing a new loss function or pretext task here, but instead we target the training regime by improving the sampling process in the contrastive learning of visual representation, i.e. constructing positive pairs beyond instances. There are two beneﬁts: ﬁrst, (hard) positive examples of the same class (e.g. the golf-swing action shown in Figure 1) are used in training; second, these positive samples are removed from the instance level negatives – where they would have been treated as false negatives for the action class. Our primary interest in this paper is to improve the representation of both the RGB and Flow networks, using the complementary information provided by the other view. For inference, we may choose to use only the RGB network or the Flow network, or both, depending on the applications and efﬁciency requirements.
To summarize, we investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical ﬂow, or from both, and make the following contributions: (i) we show that an oracle with access to semantic class labels improves the performance of instance-based contrastive learning; (ii) we propose a novel self-supervised co-training scheme, CoCLR, to improve the training regime of the popular InfoNCE, exploiting the complementary information from different views of the same data source; and (iii) we thoroughly evaluate the quality of the learnt representation on two downstream tasks, video action recognition and retrieval, on UCF101 and HMDB51. In all cases, we demonstrate state-of-the-art or comparable performance over other self-supervised approaches, while being signiﬁcantly more efﬁcient, i.e. less data is required for self-supervised pre-training.
Our observations of using a second complementary view to bridge the RGB gap between positive instances from the same class is applied in this paper to optical ﬂow. However, the idea is generally applicable for other complementary views: for videos, audio or text narrations can play a similar role to optical ﬂow; whilst for still images, the multiple views can be formed by passing images through different ﬁlters. We return to this point in Section 5. 2