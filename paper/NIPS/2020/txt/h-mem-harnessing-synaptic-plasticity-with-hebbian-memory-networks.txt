Abstract
The ability to base current computations on memories from the past is critical for many cognitive tasks such as story understanding. Hebbian-type synaptic plasticity is believed to underlie the retention of memories over medium and long time scales in the brain. However, it is unclear how such plasticity processes are integrated with computations in cortical networks. Here, we propose Hebbian
Memory Networks (H-Mems), a simple neural network model that is built around a core hetero-associative network subject to Hebbian plasticity. We show that the network can be optimized to utilize the Hebbian plasticity processes for its computations. H-Mems can one-shot memorize associations between stimulus pairs and use these associations for decisions later on. Furthermore, they can solve demanding question-answering tasks on synthetic stories. Our study shows that neural network models are able to enrich their computations with memories through simple Hebbian plasticity processes. 1

Introduction
Virtually any task faced by humans has a temporal component and therefore demands some form of memory. Consequently, a variety of memory systems and mechanisms have been shown to exist in the brain of humans and other animals [1]. These memory systems operate on a multitude of time scales, from seconds to years. Yet, it is still not well understood how memory is implemented in the brain and how cortical neuronal networks utilize these systems for computation. Most computational models of memory focus on working memory. However, many everyday tasks necessitate a more associative type of memory that acts on a longer time scale. Imagine for example a person reading a book. The person encounters names of people and has to associate these names with speciﬁc characteristics and events. As the person continues to read through the book, she has to remember many such associations to build an internal model of the story. This is a veritable computational problem and yet humans are able to solve it seemingly without effort.
Due to its very limited capacity and its short-term nature, working memory cannot satisfy the needs for such tasks. It is widely believed that longer-term storage capabilities are based on Hebbian synaptic plasticity [2]. While it has been shown that Hebbian plasticity can implement auto-associative and hetero-associative memory [3]–[5], it was rarely demonstrated that it can be utilized by neural networks for demanding tasks [6], [7]. There exists abundant evidence that memory is not an automatic process. Rather, cortical networks — in particular networks in prefrontal cortex — are believed to control the storage and retrieval of associations in memory [8]. In this article, we propose a simple network architecture inspired by this idea: the Hebbian Memory Network (H-Mem). The core of H-Mem is a simple hetero-associative network where synapses are subject to Hebbian plasticity.
The content to be stored there, and retrieved from there, is deﬁned and prepared by a number of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
small sub-networks around that memory. We then train these networks to make optimal use of the
Hebbian plasticity in the associative network. We show that this simple architecture is sufﬁcient to solve rather complex tasks that require substantial amounts of memory. We ﬁrst show that H-Mem can memorize in a single shot associations between stimulus pairs and later use these associations for decisions. Second, we demonstrate that this biologically plausible architecture can solve all of the bAbI tasks [9]. This suite of question answering tasks on synthetic stories was introduced to probe the story-understanding capabilities of deep neural networks. We ﬁnd that our model outperforms long short-term memory (LSTM, [10]) networks on such tasks and performs comparable to or better than memory-augmented neural networks recently proposed in the machine learning literature [11]–[13]. 2