Abstract
Temporal point process (TPP) models combined with recurrent neural networks provide a powerful framework for modeling continuous-time event data. While such models are ﬂexible, they are inherently sequential and therefore cannot beneﬁt from the parallelism of modern hardware. By exploiting the recent developments in the ﬁeld of normalizing ﬂows, we design TriTPP— a new class of non-recurrent
TPP models, where both sampling and likelihood computation can be done in parallel. TriTPP matches the ﬂexibility of RNN-based methods but permits orders of magnitude faster sampling. This enables us to use the new model for variational inference in continuous-time discrete-state systems. We demonstrate the advantages of the proposed framework on synthetic and real-world datasets. 1

Introduction
Temporal data lies at the heart of many high-impact machine learning applications. Electronic health records, ﬁnancial transaction ledgers and server logs contain valuable information. A common challenge encountered in all these settings is that both the number of events and their times are variable. The framework of temporal point processes (TPP) allows us to naturally handle data that consists of variable-number events in continuous time. Du et al. [1] have shown that the ﬂexibility of
TPPs can be improved by combining them with recurrent neural networks (RNN). While such models are expressive and can achieve good results in various prediction tasks, they are poorly suited for sampling: sequential dependencies preclude parallelization. We show that it’s possible to overcome the above limitation and design ﬂexible TPP models without relying on RNNs. For this, we use the framework of triangular maps [2] and recent developments in the ﬁeld of normalizing ﬂows [3].
Our main contributions are: (1) We propose a new parametrization for several classic TPPs. This enables efﬁcient parallel likelihood computation and sampling, which was impossible with existing parametrizations. (2) We propose TriTPP— a new class of non-recurrent TPPs. TriTPP matches the ﬂexibility of RNN-based methods, while allowing orders of magnitude faster sampling. (3) We derive a differentiable relaxation for non-differentiable sampling-based TPP losses. This allows us to design a new variational inference scheme for Markov jump processes. 2