Abstract
Progress in Reinforcement Learning (RL) algorithms goes hand-in-hand with the development of challenging environments that test the limits of current meth-ods. While existing RL environments are either sufﬁciently complex or based on fast simulation, they are rarely both. Here, we present the NetHack Learning
Environment (NLE), a scalable, procedurally generated, stochastic, rich, and chal-lenging environment for RL research based on the popular single-player terminal-based roguelike game, NetHack. We argue that NetHack is sufﬁciently complex to drive long-term research on problems such as exploration, planning, skill acquisi-tion, and language-conditioned RL, while dramatically reducing the computational resources required to gather a large amount of experience. We compare NLE and its task suite to existing alternatives, and discuss why it is an ideal medium for testing the robustness and systematic generalization of RL agents. We demonstrate empirical success for early stages of the game using a distributed Deep RL baseline and Random Network Distillation exploration, alongside qualitative analysis of various agents trained in the environment. NLE is open source and available at https://github.com/facebookresearch/nle. 1

Introduction
Recent advances in (Deep) Reinforcement Learning (RL) have been driven by the development of novel simulation environments, such as the Arcade Learning Environment (ALE) [9], StarCraft [64, 69], BabyAI [16], Obstacle Tower [38], Minecraft [37, 29, 35], and Procgen Benchmark [18]. These environments introduced new challenges for state-of-the-art methods and demonstrated failure modes of existing RL approaches. For example, Montezuma’s Revenge highlighted that methods performing well on other ALE tasks were not able to successfully learn in this sparse-reward environment. This sparked a long line of research on novel methods for exploration [e.g., 8, 66, 53] and learning from demonstrations [e.g., 31, 62, 6]. However, this progress has limits: the current best approach on this environment, Go-Explore [22, 23], overﬁts to speciﬁc properties of ALE and Montezuma’s Revenge.
While Go-Explore is an impressive solution for Montezuma’s Revenge, it exploits the determinism of environment transitions, allowing it to memorize sequences of actions that lead to previously visited states from which the agent can continue to explore.
We are interested in surpassing the limits of deterministic or repetitive settings and seek a simulation environment that is complex and modular enough to test various open research challenges such as exploration, planning, skill acquisition, memory, and transfer. However, since state-of-the-art RL approaches still require millions or even billions of samples, simulation environments need to be fast to allow RL agents to perform many interactions per second. Among attempts to surpass the limits of deterministic or repetitive settings, procedurally generated environments are a promising path 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
towards testing systematic generalization of RL methods [e.g., 39, 38, 60, 18]. Here, the game state is generated programmatically in every episode, making it extremely unlikely for an agent to visit the exact state more than once during its lifetime. Existing procedurally generated RL environments are either costly to run [e.g., 69, 37, 38] or are, as we argue, of limited complexity [e.g., 17, 19, 7].
To address these issues, we present the NetHack Learning Environment (NLE), a procedurally generated environment that strikes a balance between complexity and speed. It is a fully-featured
Gym environment [11] around the popular open-source terminal-based single-player turn-based
“dungeon-crawler” game, NetHack [43]. Aside from procedurally generated content, NetHack is an attractive research platform as it contains hundreds of enemy and object types, it has complex and stochastic environment dynamics, and there is a clearly deﬁned goal (descend the dungeon, retrieve an amulet, and ascend). Furthermore, NetHack is difﬁcult to master for human players, who often rely on external knowledge to learn about strategies and NetHack’s complex dynamics and secrets.1 Thus, in addition to a guide book [58, 59] released with NetHack itself, many extensive community-created documents exist, outlining various strategies for the game [e.g., 50, 25].
In summary, we make the following core contributions: (i) we present NLE, a fast but complex and feature-rich Gym environment for RL research built around the popular terminal-based game,
NetHack, (ii) we release an initial suite of tasks in the environment and demonstrate that novel tasks can be added easily, (iii) we introduce baseline models trained using IMPALA [24] and Random
Network Distillation (RND) [13], a popular exploration bonus, resulting in agents that learn diverse policies for early stages of NetHack, and (iv) we demonstrate the beneﬁt of NetHack’s symbolic observation space by presenting in-depth qualitative analyses of trained agents. 2 NetHack: a Frontier for Reinforcement Learning Research
In traditional so-called roguelike games (e.g., Rogue, Hack, NetHack, and Dungeon Crawl Stone
Soup) the player acts turn-by-turn in a procedurally generated grid-world environment, with game dynamics strongly focused on exploration, resource management, and continuous discovery of entities and game mechanics [IRDC, 2008]. These games are designed to provide a steep learning curve and a constant level of challenge and surprise to the player. They are generally extremely difﬁcult to win even once, let alone to master, i.e., win regularly and multiple times in a row.
As advocated by [39, 38, 18], procedurally generated environments are a promising direction for testing systematic generalization of RL agents. We argue that such environments need to be both sufﬁciently complex and fast to run to serve as a challenging long-term research testbed. In Section 2.1, we illustrate that NetHack contains many desirable properties, making it an excellent candidate for driving long-term research in RL. We introduce NLE in Section 2.2, an initial suite of tasks in
Section 2.3, an evaluation protocol for measuring progress towards solving NetHack in Section 2.4, as well as baseline models in Section 2.5. 2.1 NetHack
NetHack is one of the oldest and most popular roguelikes, originally released in 1987 as a successor to Hack, an open-source implementation of the original Rogue game. At the beginning of the game, the player takes the role of a hero who is placed into a dungeon and tasked with ﬁnding the Amulet of Yendor to offer it to an in-game deity. To do so, the player has to descend to the bottom of over 50 procedurally generated levels to retrieve the amulet and then subsequently escape the dungeon, unlocking ﬁve extremely challenging ﬁnal levels (the four Elemental Planes and the Astral Plane).
Many aspects of the game are procedurally generated and follow stochastic dynamics. For example, the overall structure of the dungeon is somewhat linear, but the exact location of places of interest (e.g., the Oracle) and the structure of branching sub-dungeons (e.g., the Gnomish Mines) are determined randomly. The procedurally generated content of each level makes it highly unlikely that a player will ever experience the exact same situation more than once. This provides a fundamental challenge to learning systems and a degree of complexity that enables us to more effectively evaluate an agent’s ability to generalize. It also disqualiﬁes current state-of-the-art exploration methods such as
Go-Explore [22, 23] that are based on a goal-conditioned policy to navigate to previously visited 1“NetHack is largely based on discovering secrets and tricks during gameplay. It can take years for one to become well-versed in them, and even experienced players routinely discover new ones.” [26] 2
Figure 1: Annotated example of an agent at two different stages in NetHack (Left: a procedurally generated ﬁrst level of the Dungeons of Doom, right: Gnomish Mines). A larger version of this ﬁgure is displayed in Figure 11 in the appendix. states. Moreover, states in NetHack are composed of hundreds of possible symbols, resulting in an enormous combinatorial observation space.2 It is an open question how to best project this symbolic space to a low-dimensional representation appropriate for methods like Go-Explore. For example,
Ecoffet et al.’s heuristic of downsampling images of states to measure their similarity to be used as an exploration bonus will likely not work for large symbolic and procedurally generated environments.
NetHack provides further variation by different hero roles (e.g., monk, valkyrie, wizard, tourist), races (human, elf, dwarf, gnome, orc) and random starting inventories (see Appendix A for details).
Consequently, NetHack poses unique challenges to the research community and requires novel ways to determine state similarity and, likely, entirely new exploration frameworks.
To provide a glimpse into the complexity of
NetHack’s environment dynamics, we closely follow the educational example given by “Mr
Wendal” on YouTube.3 At a speciﬁc point in the game, the hero has to get past Medusa’s Is-land (see Figure 2 for an example). Medusa’s
Island is surrounded by water } that the agent has to cross. Water can rust and corrode the hero’s metallic weapons ) and armor [. Ap-plying a can of grease ( prevents rusting and corrosion. Furthermore, going into water will make a hero’s inventory wet, erasing scrolls ? and spellbooks + that they carry. Applying a can of grease to a bag or sack ( will make it a water-proof container for items. But the sea can also contain a kraken ; that can grab and drown the hero, leading to instant death. Applying a can of grease to a hero’s armor prevents the kraken from grabbing the hero. However, a cursed can of grease will grease the hero’s hands instead and they will drop their weapon and rings. One can use a towel ( to wipe off grease. To reach Medusa @, the hero can alternatively use magic to freeze the water and turn it into walkable ice .. Wearing snow boots [ will help the hero not to slip. When
Medusa is in the hero’s line of sight, her gaze will petrify and instantly kill—the hero should use a towel to cover their eyes to ﬁght Medusa, or even apply a mirror ( to petrify her with her own gaze.
There are many other entities a hero must learn to face, many of which appear rarely even across multiple games, especially the most powerful monsters. These entities are often compositional, for example a monster might be a wolf d, which shares some characteristics with other in-game canines such as coyotes d or hell hounds d. To help a player learn, NetHack provides in-game messages
Figure 2: The hero (@) has to cross water (}) to get past Medusa (@, out of the hero’s line of sight) down the staircase (>) to the next level. 2Information about the over 450 items and 580 monster types, as well as environment dynamics involving these entities can be found in the NetHack Wiki [50] and to some extent in the NetHack Guidebook [59]. 3youtube.com/watch?v=SjuTyJlgLJ8 3
describing many of the hero’s interactions (see the top of Figure 1).4 Learning to capture these interesting and somewhat realistic albeit abstract dynamics poses challenges for multi-modal and language-conditioned RL [46].
NetHack is an extremely long game. Successful expert episodes usually last tens of thousands of turns, while average successful runs can easily last hundreds of thousands of turns, spawning multiple days of play-time. Compared to testbeds with long episode horizons such as StarCraft and Dota 2,
NetHack’s “episodes” are one or two orders of magnitude longer, and they wildly vary depending on the policy. Moreover, several ofﬁcial conducts exist in NetHack that make the game even more challenging, e.g., by not wearing any armor throughout the game (see Appendix A for more).
Finally, in comparison to other classic roguelike games, NetHack’s popularity has attracted a larger number of contributors to its community. Consequently, there exists a comprehensive game wiki [50] and many so-called spoilers [25] that provide advice to players. Due to the randomized nature of
NetHack, this advice is general in nature (e.g., explaining the behavior of various entities) and not a step-by-step guide. These texts could be used for language-assisted RL along the lines of [72].
Lastly, there is also a large public repository of human replay data (over ﬁve million games) hosted on the NetHack Alt.org (NAO) servers, with hundreds of ﬁnished games per day on average [47].
This extensive dataset could spur research advances in imitation learning, inverse RL, and learning from demonstrations [1, 3]. 2.2 The NetHack Learning Environment
The NetHack Learning Environment (NLE) is built on NetHack 3.6.6, the 36th public release of
NetHack, which was released on March 8th, 2020 and is the latest available version of the game at the time of publication of this paper. NLE is designed to provide a common, turn-based (i.e., synchronous) RL interface around the standard terminal interface of NetHack. We use the game as-is as the backend for our NLE environment, leaving the game dynamics unchanged. We added to the source code more control over the random number generator for seeding the environment, as well as various modiﬁcations to expose the game’s internal state to our Python frontend.
By default, the observation space consists of the elements glyphs, chars, colors, specials, blstats, message, inv_glyphs, inv_strs, inv_letters, as well as inv_oclasses. The elements glyphs, chars, colors, and specials are tensors representing the (batched) 2D symbolic observation of the dungeon; blstats is a vector of agent coordinates and other character attributes (“bottom-line stats”, e.g., health points, strength, dexterity, hunger level; normally displayed in the bottom area of the GUI), message is a tensor representing the current message shown to the player (normally displayed in the top area of the GUI), and the inv_* elements are padded tensors representing the hero’s inventory items. More details about the default observation space and possible extensions can be found in Appendix B.
The environment has 93 available actions, corresponding to all the actions a human player can take in
NetHack. More precisely, the action space is composed of 77 command actions and 16 movement actions. The movement actions are split into eight “one-step” compass directions (i.e., the agent moves a single step in a given direction) and eight “move far” compass directions (i.e., the agent moves in the speciﬁed direction until it runs into some entity). The 77 command actions include eating, opening, kicking, reading, praying as well as many others. We refer the reader to Appendix C as well as to the NetHack Guidebook [59] for the full table of actions and NetHack commands.
NLE comes with a Gym interface [11] and includes multiple pre-deﬁned tasks with different reward functions and action spaces (see next section and Appendix E for details). We designed the interface to be lightweight, achieving competitive speeds with Gym-based ALE (see Appendix D for a rough comparison). Finally, NLE also includes a dashboard to analyze NetHack runs recorded as terminal tty recordings. This allows NLE users to analyze replays of the agent’s behavior at an arbitrary speed and provides an interface to visualize action distributions and game events (see Appendix H for details).
NLE is available under an open source license at https://github.com/facebookresearch/nle. 4An example interaction after applying a ﬁgurine of an Archon: “You set the ﬁgurine on the ground and it transforms. You get a bad feeling about this. The Archon hits! You are blinded by the Archon’s radiance!
You stagger. . . It hits! You die. . . But wait. . . Your medallion feels warm! You feel much better! The medallion crumbles to dust! You survived that attempt on your life.” 4
Figure 3: Overview of the core architecture of the baseline models released with NLE. A larger version of this ﬁgure is displayed in Figure 12 in the appendix.
Figure 4: Training and test performance when training on restricted sets of seeds. 2.3 Tasks
NLE aims to make it easy for researchers to probe the behavior of their agents by deﬁning new tasks with only a few lines of code, enabled by NetHack’s symbolic observation space as well as its rich entities and environment dynamics. To demonstrate that NetHack is a suitable testbed for advancing
RL, we release a set of initial tasks for tractable subgoals in the game: navigating to a staircase down to the next level, navigating to a staircase while being accompanied by a pet, locating and eating edibles, collecting gold, maximizing in-game score, scouting to discover unseen parts of the dungeon, and ﬁnding the oracle. These tasks are described in detail in Appendix E, and, as we demonstrate in our experiments, lead to unique challenges and diverse behaviors of trained agents. 2.4 Evaluation Protocol
We lay out a protocol and provide guidance for evaluating future work on NLE in a reproducible manner. The overall goal of NLE is to train agents that can solve NetHack. An episode in the full game of NetHack is considered solved if the agent retrieves the Amulet of Yendor and offers it to its co-aligned deity in the Astral Plane, thereby ascending to demigodhood. We declare NLE to be solved once agents can be trained to consecutively ascend (ten episodes without retry) to demigodhood on unseen seeds given a random role, race, alignment, and gender combination. Since the environment is procedurally generated and stochastic, evaluating on held-out unseen seeds ensures we test systematic generalization of agents. As of October 2020, NAO reports the longest streak of human ascensions on
NetHack 3.6.x to be 61; the role, race, etc. are not necessarily randomized for these ascension streaks.
Since we believe that this goal is out of reach for machine learning approaches in the foreseeable future, we recommend comparing models on the score task in the meantime. Using NetHack’s in-game score as the measure for progress has caveats. For example, expert human players can solve
NetHack while minimizing the score [see 50, “Score” entry, for details]. NAO reports ascension scores for NetHack 3.6.x ranging from the low hundreds of thousands to tens of millions. Although we believe training agents to maximize the in-game score is likely insufﬁcient for solving the game, the in-game score is still a sensible proxy for incremental progress on NLE as it is a function of, among other things, the dungeon depth that the agent reached, the number of enemies it killed, the amount of gold it collected, as well as the knowledge it gathered about potions, scrolls, and wands.
When reporting results on NLE, we require future work to state the full character speciﬁca-tion (e.g., mon-hum-neu-mal), all NetHack options that were used (e.g., whether or not autopickup was used), which actions were allowed (see Table 1), which actions or action-sequences were hard-coded (e.g., engraving [see 50, “Elbereth” as an example]) and how many different seeds were used during training. We ask to report the average score obtained on 1000 episodes of randomly sampled and previously unseen seeds. We do not impose any restrictions during training, but at test time any save scumming (i.e., saving and loading previous checkpoints of the episode) or manipulation of the random number generator [e.g., 2] is forbidden. 2.5 Baseline Models
For our baseline models, we encode the multi-modal observation ot as follows. Let the observation ot at time step t be a tuple (gt, zt) consisting of the 21 79 matrix of glyph identiﬁers and a 21-dimensional vector containing agent stats such as its (x, y)-coordinate, health points, experience level, and so on. We produce three dense representations based on the observation (see Figure 3). For
× 5
every of the 5991 possible glyphs in NetHack (monsters, items, dungeon features, etc.), we learn a k-dimensional vector embedding. We apply a ConvNet (red) to all visible glyph embeddings as well as another ConvNet (blue) to the 9 9 crop of glyphs around the agent to create a dedicated egocentric representation for improved generalization [32, 71]. We found this egocentric representation to be an important component during preliminary experiments. Furthermore, we use an MLP to encode the hero’s stats (green). These vectors are concatenated and processed by another MLP to produce a low-dimensional latent representation ot of the observation. Finally, we employ a recurrent policy parameterized by an LSTM [33] to obtain the action distribution. For baseline results on the tasks above, we use a reduced action space that includes the movement, search, kick, and eat actions.
×
For the main experiments, we train the agent’s policy for 1B steps in the environment using IM-PALA [24] as implemented in TorchBeast [44]. Throughout training, we change NetHack’s seed for procedurally generating the environment after every episode. To demonstrate NetHack’s variability based on the character conﬁguration, we train with four different agent characters: a neutral human male monk (mon-hum-neu-mal), a lawful dwarf female valkyrie (val-dwa-law-fem), a chaotic elf male wizard (wiz-elf-cha-mal), and a neutral human female tourist (tou-hum-neu-fem). More implementation details can be found in Appendix F.
In addition, we present results using Random Network Distillation (RND) [13], a popular exploration technique for Deep RL. As previously discussed, exploration techniques which require returning to previously visited states such as Go-Explore are not suitable for use in NLE, but RND does not have this restriction. RND encourages agents to visit unfamiliar states by using the prediction error of a ﬁxed random network as an intrinsic exploration reward, which has proven effective for hard exploration games such as Montezuma’s Revenge [12]. The intrinsic reward obtained from RND can create “reward bridges” between states which provide sparse extrinsic environmental rewards, thereby enabling the agent to discover new sources of extrinsic reward that it otherwise would not have reached. We replace the baseline network’s pixel-based feature extractor with the symbolic feature extractor described above for the baseline model, and use the best conﬁguration of other RND hyperparameters documented by the authors (see Appendix G for full details). 3 Experiments and Results
We present quantitative results on the suite of tasks included in NLE using a standard distributed
Deep RL baseline and a popular exploration method, before additionally analyzing agent behavior qualitatively. For each model and character combination, we present results of the mean episode return over the last 100 episodes averaged for ﬁve runs in Figure 5. We discuss results for individual tasks below (see Table 5 in the appendix for full details).
Staircase: Our agents learning to navigate the dungeon to the staircase > with a success rate of 77.26% for the monk, 50.42% for the tourist, 74.62% for the valkyrie, and 80.42% for the wizard.
What surprised us is that agents learn to reliably kick in locked doors. This is a costly action to explore as the agent loses health points and might even die when accidentally kicking against walls.
Similarly, the agent has to learn to reliably search for hidden passages and secret doors. Often, this involves using the search action many times in a row, sometimes even at many locations on the map (e.g., around all walls inside a room). Since NLE is procedurally generated, during training agents might encounter easier environment instances and use the acquired skills to accelerate learning on the harder ones [60, 18]. With a small probability, the staircase down might be generated near the agent’s starting position. Using RND exploration, we observe substantial gains in the success rate for the monk (+13.58pp), tourist (+6.52pp) and valkyrie (+16.34pp) roles, while lower results for wizard roles ( 12.96pp).
−
Pet: Finding the staircase while taking care of the hero’s pet (e.g., the starting kitten f or little dog d) is a harder task as the pet might get killed or fall into a trap door, making it impossible for the agent to successfully complete the episode. Compared to the staircase task, the agent success rates are generally lower (62.02% for monk, 25.66% for tourist, 63.30% for valkyrie, and wizard 66.80%).
Again, RND exploration provides consistent and substantial gains.
Eat: This tasks highlights the importance of testing with different character classes in NetHack.
The monk and tourist start with a number edible items (e.g., food rations %, apples % and oranges %). A 6
Figure 5: Mean return of the last 100 episodes averaged over ﬁve runs. sub-optimal strategy is to consume all of these comestibles right at the start of the episode, potentially risking choking to death. In contrast, the other roles have to hunt for food, which our agents learn to do slowly over time for the valkyrie and wizard roles. By having more pressure to quickly learn a sustainable food strategy, the valkyrie learns to outlast other roles and survives the longest in the game (on average 1713 time steps). Interestingly, RND exploration leads to consistently worse results for this task.
Gold: Locating gold $ in NetHack provides a relatively sparse reward signal. Still, our agents learn to collect decent amounts during training and learn to descend to deeper dungeon levels in search for more. For example, monk agents reach dungeon level 4.2 on average for the CNN baseline and even 5.0 using RND exploration.
Score: As discussed in Section 2.4, we believe this task is the best candidate for comparing future methods regarding progress on NetHack. However, it is questionable whether a reward function based on NetHack’s in-game score is sufﬁcient for training agents to solve the game. Our agents average at a score of 748 for monk, 11 for tourist, 573 for valkyrie, and 314 for wizard, with RND exploration again providing substantial gains (e.g. increasing the average score to 780 for monk). The resulting agents explore much of the early stages of the game, reaching dungeon level 5.4 on average for the monk with the deepest descent to level 11 achieving a high score of 4260 while leveling up to experience level 7 (see Table 6 in the appendix).
Scout: The scout task shows a trend that is similar to the score task. Interestingly, we observe a lower experience level and in-game score, but agents descend, on average, similarly deep into the dungeon (e.g. level 5.5 for monk). This is sensible, since a policy that avoids to ﬁght monsters, thereby lowering the chances of premature death, will not increase the in-game score as fast or level up the character as quickly, thus keeping the difﬁculty of spawned monsters low. We note that delaying to level up in order to avoid encountering stronger enemies early in the game is a known strategy human players adopt in NetHack [e.g. 50, “Why do I keep dying?” entry, January 2019 version].
Oracle: None of our agents ﬁnd the Oracle @ (except for one lucky valkyrie episode). Locating the Oracle is a difﬁcult exploration task. Even if the agent learns to make its way down the dungeon levels, it needs to search many, potentially branching, levels of the dungeon. Thus, we believe this task serves as a challenging benchmark for exploration methods in procedurally generated environments in the short term. Long term, many tasks harder than this (e.g., reaching Minetown, Mines’ End,
Medusa’s Island, The Castle, Vlad’s Tower, Moloch’s Sanctum etc.) can be easily deﬁned in NLE with very few lines of code. 3.1 Generalization Analysis
Akin to [18], we evaluate agents trained on a limited set of seeds while still testing on 100 held-out seeds. We ﬁnd that test performance increases monotonically with the size of the set of seeds that the agent is trained on. Figure 4 shows this effect for the score and staircase tasks. Training only on a limited number of seeds leads to high training performance, but poor generalization. The gap between training and test performance becomes narrow when training with at least 1000 seeds, indicating 7
that at that point agents are exposed to sufﬁcient variation during training to make memorization infeasible. We also investigate how model capacity affects performance by comparing agents with
ﬁve different hidden sizes for the ﬁnal layer (of the architecture described in Section 2.5). Figure 7 in the appendix shows that increasing the model capacity improves results on the score but not on the staircase task, indicating that it is an important hyperparameter to consider, as also noted by [18]. 3.2 Qualitative Analysis
We analyse the cause for death of our agents during training and present results in Figure 9 in the appendix. We notice that starvation and traps become a less prominent cause of death over time, most likely because our agents, when starting to learn to descend dungeon levels and ﬁght monsters, are more likely to die in combat before they starve or get killed by a trap. In the score and scout tasks, our agents quickly learn to avoid eating rotten corpses, but food poisoning becomes again prominent towards the end of training.
We can see that gnome lords G, gnome kings G, chameleons :, and even mind ﬂayers h become a more prominent cause of death over time, which can be explained with our agents leveling up and descending deeper into the dungeon. Chameleons are a particularly interesting entity in NetHack as they regularly change their form to a random animal or monster, thereby adversarially confusing our agent with rarely seen symbols for which it has not yet learned a meaningful representation (similar to unknown words in natural language processing). We release a set of high-score recordings of our agents (see Appendix J on how to view them via a browser or terminal). 4