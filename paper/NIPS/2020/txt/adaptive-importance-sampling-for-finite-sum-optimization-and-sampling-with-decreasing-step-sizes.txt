Abstract
Reducing the variance of the gradient estimator is known to improve the conver-gence rate of stochastic gradient-based optimization and sampling algorithms. One way of achieving variance reduction is to design importance sampling strategies.
Recently, the problem of designing such schemes was formulated as an online learning problem with bandit feedback, and algorithms with sub-linear static regret were designed. In this work, we build on this framework and propose Avare, a simple and efﬁcient algorithm for adaptive importance sampling for ﬁnite-sum optimization and sampling with decreasing step-sizes. Under standard technical conditions, we show that Avare achieves O(T 2/3) and O(T 5/6) dynamic regret for SGD and SGLD respectively when run with O(1/t) step sizes. We achieve this dynamic regret bound by leveraging our knowledge of the dynamics deﬁned by the algorithm, and combining ideas from online learning and variance-reduced stochastic optimization. We validate empirically the performance of our algorithm and identify settings in which it leads to signiﬁcant improvements. 1

Introduction
Functions f : Rd → R of the form: f (x) =
N (cid:88) i=1 fi(x) (1) are prevalent in modern machine learning and statistics. Important examples include the empirical risk in the empirical risk minimization framework, 1 or the log-posterior of an exchangeable Bayesian model. When N is large, the preferred methods for solving the resulting optimization or sampling problem usually rely on stochastic estimates of the gradient of f , using variants of stochastic gradient descent (SGD) [1] or stochastic gradient Langevin dynamics (SGLD) [2]: xSGD t+1 = xSGD t+1 = xSGLD xSGLD
− αtN ∇fIt(xSGD
)
− αtN ∇fIt(xSGLD t t t t
) + ξt
ξt ∼ N (0, 2αt) (2) (3) where {αt}T t=1 is a sequence of step-sizes, and the index It is sampled uniformly from [N ], making
N ∇fIt(x) an unbiased estimator of the gradient of f . We use {xt}T t=1 to refer to either sequence when we do not wish to distinguish between them. It is well known that the quality of the answers given by these algorithms depends on the (trace of the) variance of the gradient estimator, and considerable efforts have been made to design methods that reduce this variance. 1Up to a normalizing factor of 1
N which does not affect the optimization 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we focus on the importance sampling approach to achieving variance reduction. At each iteration, the algorithm samples It according to a speciﬁed distribution pt, and estimates the gradient using:
ˆgt := 1 pt
It
It is immediate to verify that ˆgt is an unbiased estimator of gt := ∇f (xt). By cleverly choosing the distributions pt, one can achieve signiﬁcant variance reduction (up to a factor of N) compared to the estimator based on uniform sampling. Unfortunately, computing the variance-minimizing distributions at each iteration requires the knowledge of the Euclidean norm of all the individual gradients gt i := ∇fi(xt) at each iteration, making it unpractical [3, 4].
∇fIt(xt) (4)
Many methods have been proposed that attempt to construct sequences of distributions {pt}T t=1 that result in efﬁcient estimators [3, 4, 5, 6, 7, 8, 9]. Of particular interest to us, the task of designing such sequences was recently cast as an online learning problem with bandit feedback [10, 11, 12, 13]. In this formulation, one attempts to design algorithms with sub-linear expected static regret, which is deﬁned as:
RegretS(T ) := ct(pt) − min p∈∆ ct(p)
T (cid:88) t=1
T (cid:88) t=1 where ∆ denotes the probability simplex in RN , and ct(p) is the trace of the covariance matrix of the gradient estimator (4), which is easily shown to be: ct(p) :=
N (cid:88) i=1 1 pi (cid:13) (cid:13)gt i 2 − (cid:13) (cid:13) 2 (cid:13) (cid:13)gt(cid:13) 2 (cid:13) 2 (5)
Note that the second term cancels in the deﬁnition of regret, and we omit it in the rest of our discussion.
In this formulation, and to keep the computational load manageable, one has only access to partial feedback in the form of the norm of the I th t gradient, and not to the complete cost function (5).
Under the assumption of uniformly bounded gradients, the best result in this category can be found in
[12] where an algorithm with ˜O(T 2/3) static regret is proposed. A more difﬁcult but more natural performance measure that makes the attempt to approximate the optimal distributions explicit is the dynamic regret, deﬁned by:
RegretD(T ) :=
T (cid:88) t=1 ct(pt) −
T (cid:88) t=1 min p∈∆ ct(p) (6)
Guarantees with respect to the expected dynamic regret are more difﬁcult to obtain, and require that the cost functions ct(p) do not change too rapidly with respect to some reasonable measure of variation. See [14, 15, 16, 17, 18] for examples of such measures and the corresponding regret bounds for general convex cost functions.
In this work, we propose Avare, an algorithm that achieves sub-linear dynamic regret for both SGD and SGLD when the sequence of step-sizes {αt}T t=1 is decreasing. The name Avare is derived from adaptive variance minimization. Speciﬁcally, our contributions are as follows:
• We show that Avare achieves O(T 2/3) and O(T 5/6) dynamic regret for SGD and SGLD respectively when αt is O(1/t).
• We propose a new mini-batch estimator that combines the beneﬁts of sampling without replacement and importance sampling while preserving unbiasedness.
• We validate empirically the performance of our algorithm and identify settings in which it leads to signiﬁcant improvements.
We would like to point out that while the decreasing step size requirement might seem restrictive, we argue that for SGD and SGLD, it is the right setting to consider for variance reduction. Indeed, it is well known that under suitable technical conditions, both algorithms converge to their respective solutions exponentially fast in the early stages. Variance reduction is primarily useful at later stages when the noise from the stochastic gradient dominates. In the absence of control variates, one is forced to use decreasing step-sizes to achieve convergence. This is precisely the regime we consider. 2
2