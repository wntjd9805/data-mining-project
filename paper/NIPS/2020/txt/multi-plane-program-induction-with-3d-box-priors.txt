Abstract
We consider two important aspects in understanding and editing images: modeling regular, program-like texture or patterns in 2D planes, and 3D posing of these planes in the scene. Unlike prior work on image-based program synthesis, which assumes the image contains a single visible 2D plane, we present Box Program Induction (BPI), which infers a program-like scene representation that simultaneously models repeated structure on multiple 2D planes, the 3D position and orientation of the planes, and camera parameters, all from a single image. Our model assumes a box prior, i.e., that the image captures either an inner view or an outer view of a box in 3D. It uses neural networks to infer visual cues such as vanishing points or wireframe lines to guide a search-based algorithm to ﬁnd the program that best explains the image. Such a holistic, structured scene representation enables 3D-aware interactive image editing operations such as inpainting missing pixels, changing camera parameters, and extrapolate the image contents. 1

Introduction
We aim to build autonomous algorithms that can infer two important structures for compositional scene understanding and editing from a single image: the regular, program-like texture or patterns in 2D planes and the 3D posing of these planes in the scene. As a motivating example, when observing a single image of a corridor like the one in Fig. 1, we humans can effortlessly infer the camera pose, partition the image into ﬁve planes—including left and right walls, ﬂoor, ceiling, and a far plane—and recognize the repeated pattern on each of these planes. Such a holistic and structural representation allows us to ﬂexibly edit the image, for instance by inpainting missing regions, moving the camera, and extrapolating the corridor to make it inﬁnite.
A range of computer vision algorithms have utilized such a holistic scene representation to guide image manipulation tasks. Several recent ones ﬁt into a program-guided image manipulation framework [11, 24, 25]. These methods infer a program-like image representation that captures camera parameters and scene structures, enabling image editing operations guided by such programs so that the scene structure is preserved during editing. However, due to the combinatorial complexity of possible compositions of elementary components based on the program grammar, these methods usually only work for images in highly speciﬁc domains with a ﬁxed set of primitives such as hand-drawn ﬁgures of simple 2D geometric shapes [11] and synthesized tabletop scenes [24], or natural images with of a single visible plane, such as ground tiles and patterned cloth [25, 21].
To address these issues and scale up program-guided image manipulation, we present a new frame-work, namely, Box Program Induction (BPI, for short), that jointly segments the image into multiple planes and infers the repeated structure on each plane. Our model assumes a box prior, leveraging the observation that box-like structures widely exist in images. Many indoor and outdoor scenes fall into this category: walking in a corridor or room corresponds to observing a box from the inside, and taking a picture of a building corresponds to seeing a box from the outside.
*: indicates equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: We present Box Program Induction (BPI), which infers a program-like scene representation that simultaneously models repeated structure on multiple 2D planes, 3D positions and orientations of the planes, relative to the camera, all from a single image. The inferred program can be used to guide perspective- and regularity-aware image manipulation tasks, including image inpainting, extrapolation, and view synthesis.
To enable efﬁcient inference of box programs, we also propose to utilize mid-level cues, such as vanishing points or wireframe lines, as well as high-level visual cues such as subject segmentations, to implicitly constrain the search space of candidate programs. Given the input image, BPI ﬁrst infers these visual cues with pre-trained data-driven models. Next, it enumerates all candidate programs that satisfy the implicit constraints imposed by these inferred visual features. Finally, it ranks all candidate programs using low-level visual features such as pixel reconstruction.
In summary, we present BPI, a framework for inducing box programs from images by exploiting learned visual cues. Our experiments show that BPI can efﬁciently and accurately infer the structure and camera parameters for both indoor and outdoor scenes. The inference procedure is robust to errors and noise inherent to visual cue prediction: BPI automatically selects the best candidate wireframe lines and reﬁnes the vanishing points if they are not accurate. BPI also enables users to make 3D-aware interactive editing to images, such as inpainting missing pixels, extrapolating image content in speciﬁc directions, and changing camera parameters. 2