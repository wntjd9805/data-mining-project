Abstract
Modern large scale datasets are often plagued with missing entries. For tabular data with missing values, a ﬂurry of imputation algorithms solve for a complete matrix which minimizes some penalized reconstruction error. However, almost none of them can estimate the uncertainty of its imputations. This paper pro-poses a probabilistic and scalable framework for missing value imputation with quantiﬁed uncertainty. Our model, the Low Rank Gaussian Copula, augments a standard probabilistic model, Probabilistic Principal Component Analysis, with marginal transformations for each column that allow the model to better match the distribution of the data. It naturally handles Boolean, ordinal, and real-valued observations and quantiﬁes the uncertainty in each imputation. The time required to ﬁt the model scales linearly with the number of rows and the number of columns in the dataset. Empirical results show the method yields state-of-the-art imputation accuracy across a wide range of data types, including those with high rank. Our uncertainty measure predicts imputation error well: entries with lower uncertainty do have lower imputation error (on average). Moreover, for real-valued data, the resulting conﬁdence intervals are well-calibrated. 1

Introduction
Missing data imputation forms the ﬁrst critical step of many data analysis pipelines; indeed, in the context of recommender systems, imputation itself is the task. The remarkable progress in low rank matrix completion (LRMC) [6, 25, 36] has led to wide use in collaborative ﬁltering [37], transductive learning [17], automated machine learning [43], and beyond. Nevertheless, reliable decision making requires one more step: assessing the uncertainty of the imputed entries. While multiple imputation
[39, 28] is a classical tool to quantify uncertainty, its computation is often expensive and limits the use on large datasets. For single imputation methods such as LRMC, very little work has sought to quantify imputation uncertainty. The major difﬁculty in quantifying uncertainty lies in characterizing how the imputations depend on the observations through the solution to a nonsmooth optimization problem. Chen et al. [9] avoids this difﬁculty, providing conﬁdence intervals for imputed real valued matrices, by assuming isotropic Gaussian noise and a large signal-to-noise ratio (SNR). However, these assumptions are hardly satisﬁed for most noisy real data.
The probabilistic principal component analysis (PPCA) model [41] provides a different approach to quantify uncertainty. The PPCA model posits that the data in each row is sampled iid from a Gaussian factor model. In this framework, each missing entry has a closed form distribution conditional on the observations. The conditional mean, which is simply a linear transformation of the observations, is used for imputation [44, 22]. However, the Gaussian assumption is unrealistic for most real datasets.
The Gaussian copula model presents a compelling alternative that enjoys the analytical beneﬁts of Gaussians and yet ﬁts real datasets well. The Gaussian copula (or equivalently nonparanormal distribution) [29, 12, 15, 21] can model real-valued, ordinal and Boolean data by transforming a latent 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Gaussian vector to match given marginal distributions. Recently, Zhao and Udell [45] proposed an imputation framework based on the Gaussian copula model and empirically demonstrated state-of-the-art performance of Gaussian copula imputations on long skinny datasets. However, their algorithm scales cubically in the number of columns, which is too expensive for applications to large-scale datasets such as collaborative ﬁltering and medical informatics.
Our contribution We propose a low rank Gaussian copula model for imputation with quantiﬁed uncertainty. The proposed model combines the advantages of PPCA and Gaussian copula: the probabilistic description of missing entries allows for uncertainty quantiﬁcation; the low rank structure allows for efﬁcient estimation from large-scale data; and the copula framework provides the generality to accurately ﬁt real-world data. The imputation proceeds in two steps: ﬁrst we ﬁt the LRGC model, and then we compute the distribution of the missing values separately for each row, conditional on the observed values in that row. We impute the missing values with the conditional mean and quantify their uncertainty with the conditional variance. Our contributions are as follows. 1. We propose a probabilistic imputation method based on the low rank Gaussian copula model to impute real-valued, ordinal and Boolean data. The rank of the model is the only tuning parameter. 2. We propose an algorithm to ﬁt the proposed model that scales linearly in the number of rows and the number of columns. Empirical results show our imputations provide state-of-the-art accuracy across a wide range of data types, including those with high rank. 3. We characterize how the mean squared error (MSE) of our imputations depends on the SNR. In particular, we show the MSE converges exponentially to the noise level in the limit of high SNR. 4. We quantify the uncertainty of our estimates. Concretely, we construct conﬁdence intervals for imputed real values and provide lower bounds on the probability of correct prediction for imputed ordinal values. Empirical results show our conﬁdence intervals are well-calibrated and our uncertainty measure predicts imputation error well: entries with lower estimated uncertainty do have lower imputation error (on average).