Abstract
We analyze the extent to which existing methods rely on accurate training data for a speciﬁc class of reinforcement learning (RL) algorithms, known as Safe and
Seldonian RL. We introduce a new measure of security to quantify the susceptibility to perturbations in training data by creating an attacker model that represents a worst-case analysis, and show that a couple of Seldonian RL methods are extremely sensitive to even a few data corruptions. We then introduce a new algorithm that is more robust against data corruptions, and demonstrate its usage in practice on some RL problems, including a grid-world and a diabetes treatment simulation. 1

Introduction
Reinforcement learning (RL) algorithms have been proposed for many high-risk applications, such as improving type 1 diabetes and sepsis treatments [44; 18]. One type of safe RL algorithm [41; 43], subsequently referred to as Safe and/or Seldonian RL [44], enables these high-risk applications by providing high-conﬁdence guarantees that the application will not cause undesirable behavior like increasing the frequency of dangerous patient outcomes.
However, existing safe RL algorithms rely on the assumption that training data is free from anomalies such as errors, missing entries, and malicious attacks. In real applications, anomalies are common when training data comes from a pipeline that includes human interactions, natural language process-ing, device malfunctions, etc. For example, the recent application of RL to sepsis treatment in the intensive care unit (ICU) used training data generated from hand-written doctors’ notes [18]. In a high-stress ICU environment, missing records and poorly written notes are difﬁcult to automatically parse [1]. Furthermore, Petit et al. [35] demonstrated the importance of using reliable training data for self-driving cars, a potential area for the real application of RL. They executed a series of attacks on the camera and sensors of self-driving cars in a lab environment to demonstrate how the safety of passengers can be compromised.
In this paper, we analyze how robust Seldonian RL algorithms are to perturbations in data. Speciﬁcally, we analyze the robustness of a speciﬁc component, called the safety test. This component makes current Seldonian algorithms safe: the safety test checks whether necessary safety constraints are satisﬁed with high probability. Using data collected from a baseline policy, it outputs new policies that are highly likely to perform at least as well as the baseline. The safety test ﬁrst computes estimates of the expected performance of a new policy from training data, using importance sampling (IS). It then uses concentration inequalities (CI) to bound the expectation of the IS estimates.
First, we propose a new measure, which we call α-security, for quantifying how robust the safety test of a Seldonian RL algorithm is to data anomalies. To create this measure, we deﬁne an attacker that adds adversarially corrupt data points to training data. Although anomalies in data are often not due to an adversarial attacker, if we create algorithms that are robust to adversarial attacks, they 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
will also be robust to non-adversarial anomalies in data. Second, we analyze the security of existing safety test mechanisms using α-security, and ﬁnd that even if only one data point is corrupted, the high-conﬁdence safety guarantees provided by several Seldonian RL algorithms can be egregiously violated. Then we propose a new algorithm that is more robust to anomalies in training data, ensuring safety with high probability when an upper bound on the number of adversarially corrupt data points is known. Finally, we present experiments that support our theoretical analysis.
Our work is directly applicable to any scenario that requires computing conﬁdence intervals around
IS estimates. More broadly, the community is also interested in our deﬁnition of safety [8] and its limitations [11], and IS [4; 16; 26; 29]. Lastly, our α-security formalization also pertains to high-conﬁdence methods that do not use IS [41; 21; 22], and can be used as a general framework to study their robustness to data corruption attacks. 2