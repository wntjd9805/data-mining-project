Abstract
We introduce a deep learning model for speech denoising, a long-standing challenge in audio analysis arising in numerous applications. Our approach is based on a key observation about human speech: there is often a short pause between each sentence or word. In a recorded speech signal, those pauses introduce a series of time periods during which only noise is present. We leverage these incidental silent intervals to learn a model for automatic speech denoising given only mono-channel audio. Detected silent intervals over time expose not just pure noise but its time-varying features, allowing the model to learn noise dynamics and suppress it from the speech signal. Experiments on multiple datasets conﬁrm the pivotal role of silent interval detection for speech denoising, and our method outperforms several state-of-the-art denoising methods, including those that accept only audio input (like ours) and those that denoise based on audiovisual input (and hence require more information). We also show that our method enjoys excellent generalization properties, such as denoising spoken languages not seen during training. 1

Introduction
Noise is everywhere. When we listen to someone speak, the audio signals we receive are never pure and clean, always contaminated by all kinds of noises—cars passing by, spinning fans in an air conditioner, barking dogs, music from a loudspeaker, and so forth. To a large extent, people in a conversation can effortlessly ﬁlter out these noises [42]. In the same vein, numerous applica-tions, ranging from cellular communications to human-robot interaction, rely on speech denoising algorithms as a fundamental building block.
Despite its vital importance, algorithmic speech denoising remains a grand challenge. Provided an input audio signal, speech denoising aims to separate the foreground (speech) signal from its additive background noise. This separation problem is inherently ill-posed. Classic approaches such as spectral subtraction [7, 98, 6, 72, 79] and Wiener ﬁltering [80, 40] conduct audio denoising in the spectral domain, and they are typically restricted to stationary or quasi-stationary noise. In recent years, the advance of deep neural networks has also inspired their use in audio denoising.
While outperforming the classic denoising approaches, existing neural-network-based approaches use network structures developed for general audio processing tasks [56, 90, 100] or borrowed from other areas such as computer vision [31, 26, 3, 36, 32] and generative adversarial networks [70, 71].
Nevertheless, beyond reusing well-developed network models as a black box, a fundamental question remains: What natural structures of speech can we leverage to mold network architectures for better performance on speech denoising? 1.1 Key insight: time distribution of silent intervals
Motivated by this question, we revisit one of the most widely used audio denoising methods in practice, namely the spectral subtraction method [7, 98, 6, 72, 79]. Implemented in many commercial software such as Adobe Audition [39], this classical method requires the user to specify a time interval during which the foreground signal is absent. We call such an interval a silent interval. A silent interval is a time window that exposes pure noise. The algorithm then learns from the silent 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Silent intervals over time. (top) A speech signal has many natural pauses. Without any noise, these pauses are exhibited as silent intervals (highlighted in red). (bottom) However, most speech signals are contaminated by noise. Even with mild noise, silent intervals become overwhelmed and hard to detect. If robustly detected, silent intervals can help to reveal the noise proﬁle over time. interval the noise characteristics, which are in turn used to suppress the additive noise of the entire input signal (through subtraction in the spectral domain).
Yet, the spectral subtraction method suffers from two major shortcomings: i) it requires user spec-iﬁcation of a silent interval, that is, not fully automatic; and ii) the single silent interval, although undemanding for the user, is insufﬁcient in presence of nonstationary noise—for example, a back-ground music. Ubiquitous in daily life, nonstationary noise has time-varying spectral features. The single silent interval reveals the noise spectral features only in that particular time span, thus inade-quate for denoising the entire input signal. The success of spectral subtraction pivots on the concept of silent interval; so do its shortcomings.
In this paper, we introduce a deep network for speech denoising that tightly integrates silent intervals, and thereby overcomes many of the limitations of classical approaches. Our goal is not just to identify a single silent interval, but to ﬁnd as many as possible silent intervals over time. Indeed, silent intervals in speech appear in abundance: psycholinguistic studies have shown that there is almost always a pause after each sentence and even each word in speech [78, 21]. Each pause, however short, provides a silent interval revealing noise characteristics local in time. All together, these silent intervals assemble a time-varying picture of background noise, allowing the neural network to better denoise speech signals, even in presence of nonstationary noise (see Fig. 1).
In short, to interleave neural networks with established denoising pipelines, we propose a network structure consisting of three major components (see Fig. 2): i) one dedicated to silent interval detection, ii) another that aims to estimate the full noise from those revealed in silent intervals, akin to an inpainting process in computer vision [38], and iii) yet another for cleaning up the input signal.
Summary of results. Our neural-network-based denoising model accepts a single channel of audio signal and outputs the cleaned-up signal. Unlike some of the recent denoising methods that take as input audiovisual signals (i.e., both audio and video footage), our method can be applied in a wider range of scenarios (e.g., in cellular communication). We conducted extensive experiments, including ablation studies to show the efﬁcacy of our network components and comparisons to several state-of-the-art denoising methods. We also evaluate our method under various signal-to-noise ratios—even under strong noise levels that are not tested against in previous methods. We show that, under a variety of denoising metrics, our method consistently outperforms those methods, including those that accept only audio input (like ours) and those that denoise based on audiovisual input.
The pivotal role of silent intervals for speech denoising is further conﬁrmed by a few key results. Even without supervising on silent interval detection, the ability to detect silent intervals naturally emerges in our network. Moreover, while our model is trained on English speech only, with no additional training it can be readily used to denoise speech in other languages (such as Chinese, Japanese, and
Korean). Please refer to the supplementary materials for listening to our denoising results. 2