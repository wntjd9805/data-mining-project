Abstract
We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP has impractically slow convergence to high frequency signal components. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-speciﬁc Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities. 1

Introduction
A recent line of research in computer vision and graphics replaces traditional discrete representations of objects, scene geometry, and appearance (e.g. meshes and voxel grids) with continuous functions parameterized by deep fully-connected networks (also called multilayer perceptrons or MLPs). These
MLPs, which we will call “coordinate-based” MLPs, take low-dimensional coordinates as inputs (typically points in R3) and are trained to output a representation of shape, density, and/or color at each input location (see Figure 1). This strategy is compelling since coordinate-based MLPs are amenable to gradient-based optimization and machine learning, and can be orders of magnitude more compact than grid-sampled representations. Coordinate-based MLPs have been used to represent images [31, 42] (referred to as “compositional pattern producing networks”), volume density [30], occupancy [27], and signed distance [35], and have achieved state-of-the-art results across a variety of tasks such as shape representation [9, 11, 13, 14, 19, 29, 35], texture synthesis [17, 34], shape inference from images [24, 25], and novel view synthesis [30, 32, 38, 41].
We leverage recent progress in modeling the behavior of deep networks using kernel regression with a neural tangent kernel (NTK) [18] to theoretically and experimentally show that standard MLPs are poorly suited for these low-dimensional coordinate-based vision and graphics tasks. In particular,
MLPs have difﬁculty learning high frequency functions, a phenomenon referred to in the literature as
“spectral bias” [3, 36]. NTK theory suggests that this is because standard coordinate-based MLPs correspond to kernels with a rapid frequency falloff, which effectively prevents them from being able to represent the high-frequency content present in natural images and scenes.
∗ Authors contributed equally to this work. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
s e r u t a e f r e i r u o
F o
N s e r u t a e f r e i r u o
F h t i
W v
=
) v (
γ
) v (
F
F
=
) v (
γ (a) Coordinate-based MLP (b) Image regression (c) 3D shape regression (d) MRI reconstruction (e) Inverse rendering (x,y) → RGB (x,y,z) → occupancy (x,y,z) → density (x,y,z) → RGB, density
Figure 1: Fourier features improve the results of coordinate-based MLPs for a variety of high-frequency low-dimensional regression tasks, both with direct (b, c) and indirect (d, e) supervision.
We visualize an example MLP (a) for an image regression task (b), where the input to the network is a pixel coordinate and the output is that pixel’s color. Passing coordinates directly into the network (top) produces blurry images, whereas preprocessing the input with a Fourier feature mapping (bottom) enables the MLP to represent higher frequency details. 1 v), a1 sin(2πbT
A few recent works [30, 48] have experimentally found that a heuristic sinusoidal mapping of input coordinates (called a “positional encoding”) allows MLPs to represent higher frequency content.
We observe that this is a special case of Fourier features [37]: mapping input coordinates v to
γ(v) = (cid:2)a1 cos(2πbT 1 v), . . . , am cos(2πbT before passing them into an MLP. We show that this mapping transforms the NTK into a stationary (shift-invariant) kernel and enables tuning the NTK’s spectrum by modifying the frequency vectors bj, thereby controlling the range of frequencies that can be learned by the corresponding MLP. We show that the simple strategy of setting aj = 1 and randomly sampling bj from an isotropic distribution achieves good performance, and that the scale (standard deviation) of this distribution matters much more than its speciﬁc shape. We train MLPs with this Fourier feature input mapping across a range of tasks relevant to the computer vision and graphics communities. As highlighted in Figure 1, our proposed mapping dramatically improves the performance of coordinate-based MLPs. In summary, we make the following contributions: mv), am sin(2πbT mv)(cid:3)T
• We leverage NTK theory and simple experiments to show that a Fourier feature mapping can be used to overcome the spectral bias of coordinate-based MLPs towards low frequencies by allowing them to learn much higher frequencies (Section 4).
• We demonstrate that a random Fourier feature mapping with an appropriately chosen scale can dramatically improve the performance of coordinate-based MLPs across many low-dimensional tasks in computer vision and graphics (Section 5). 2