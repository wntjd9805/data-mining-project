Abstract
A challenge that machine learning practitioners in the industry face is the task of selecting the best model to deploy in production. As a model is often an intermedi-ate component of a production system, online controlled experiments such as A/B tests yield the most reliable estimation of the effectiveness of the whole system, but can only compare two or a few models due to budget constraints. We propose an automated online experimentation mechanism that can efﬁciently perform model se-lection from a large pool of models with a small number of online experiments. We derive the probability distribution of the metric of interest that contains the model uncertainty from our Bayesian surrogate model trained using historical logs. Our method efﬁciently identiﬁes the best model by sequentially selecting and deploying a list of models from the candidate set that balance exploration-exploitation. Using simulations based on real data, we demonstrate the effectiveness of our method on two different tasks. 1

Introduction
Evaluating the effect of individual changes to machine learning (ML) systems such as choice of algorithms, features, etc., is the key to growth in many internet services and industrial applications.
Practitioners are faced with the decision of choosing one model from several candidates to deploy in production. This can be viewed as a model selection problem. Classical model selection paradigms such as cross-validation consider ML models in isolation and focus on selecting the model with the best predictive power on unseen data. This approach does not work well for modern industrial
ML systems, as such a system usually consists of many individual components and a ML model is only one of them. The metric of interest often depends on uncontrollable factors such as users’ responses. Only optimizing the predictive power of the ML model would not lead to a better metric of the overall system. Instead, randomized experiments (also known as "A/B tests") are considered as the gold-standard for evaluating system changes [1] as they provide a more direct measure of the metric. However, only a few variants of the ML model can be tested using randomized experiments as they are time-consuming to conduct and have resource constraints (such as the number of active users, etc.). Furthermore, deploying bad systems can lead to catastrophic consequences.
An alternative approach is to exploit log data collected under the production system to estimate the metric of interest if a different ML model is deployed. Typical methods include developing ofﬂine measures or simulators that model users’ behavior [2, 3], and replaying the recorded decisions with a probability ratio correction between the new and previous model, which is referred to as off-policy evaluation (OPE) [4, 5, 6, 7, 8, 9]. A big challenge faced by these methods is the selection bias of the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
log data. As a consequence, these methods work well when the considered model behaves similar to the logging model, but the effectiveness deteriorates quickly when the considered model behaves increasingly differently from the logging model.
To overcome the selection bias, we suggest to include the data collection process into the model selection approach. We propose a new framework of model selection for production system, where the best model is selected via deploying a sequence of models online. This allows deploying the model that can provide maximum information, iteratively reﬁning the belief about the candidate models and efﬁciently identifying the model that leads to the best metric for the overall system. Concretely, we target at a speciﬁc but widely existing scenario, in which the metric of interest can be decomposed into an average of immediate feedback, e.g., the click-through rate in recommender systems. We develop a Bayesian surrogate model that can efﬁciently digest the collected data from online experiments and derive the probability distribution of the metric of interest based on the surrogate model. The model to deploy is selected by balancing the exploration-exploitation trade-off. Comparing with A/B testing, our approach can perform model selection from a large pool of candidates by using not only the recorded metric but also the log data about individual user interactions. Comparing with OPE, our approach provides more accurate estimation of model performance by avoiding the selection bias through controlling the data collection process. Overall, our approach correctly identiﬁes the best model even if it behaves very differently from the one in production. 2 Model selection with automated online experiments
We deﬁne the problem of model selection for production system (MSPS) as follows: given a set of candidate models Mi ∈ M and an online budget, the goal is to identify model M∗ with maximum utility of the overall system:
M∗ = arg max
Mi∈M v(Mi). (1)
In this work, we focus on the scenario where a model takes an input representation x, returns a decision a while observing an immediate feedback for each individual decision. The utility associated with a given model Mi is inﬂuenced by immediate feedback, which could be an indirect and complex relationship such as the relation between proﬁt margin and user clicks. Here, we restrict our focus to the cases where the utility has an additive relation with immediate feedback and refer to it as accumulative metric. The above setting is common in the industry; for example, in recommender systems, the inputs x are users or context (user representation, time of the request, etc.), the decisions are the choice of recommendation, and the accumulative metric could be a metric such as total consumption, which is the sum of consumption associated with individual recommendations.
A model can be represented as a probability distribution of the decision conditioned on the input p(a|x), where a deterministic system simply results in a delta distribution. The distribution of the inputs to the model represented as p(x) is typically unknown. The accumulative metric for a given model Mi can be deﬁned as, v(Mi) = (cid:90) (cid:90)
X
A m p(m|a, x)p(a|x, Mi)p(x) da dx, (2) where the integration is over the space of input x ∈ X and the space of decision a ∈ A. The accumulative metric is deﬁned as an expectation of immediate feedback with respect to the distribution of input and decisions conditioned on individual inputs. Unfortunately, the accumulative metric is not tractable, because both the distribution of input p(x) and the distribution of immediate feedback p(m|a, x) are unknown. (cid:80)
With a production system, the information about the accumulative metric can be collected by deploying the model of interest in production and let it react to real trafﬁc and record the corresponding accumulative metric. The collected data from such a deploy consist of a recorded accumulative metric, in our case ˆv = 1 i=1. In this work, we
N deﬁne MSPS as a sequential optimal decision problem. A MSPS method iteratively chooses a model from the candidate set to deploy online for data collection with the aim of identifying the model with the best accumulative metric in the fewest number of online experiments. A model deployment is a expensive process, as each deployment takes a long time, and only a small number of models can be deployed in parallel due to the limited number of users and the affordable degradation in service quality. Global optimization methods like Bayesian optimization (BO) [10] do not work well in this i mi, and a set of interactions D = {(mi, ai, xi)}N 2
setting, because BO requires the search space to be represented in a relatively low dimensional space but embedding the model candidate sets (especially models of different types) into a low-dimensional space is non-trivial. Unlike BO methods that only take into account the accumulative metric from online experiments, our approach takes advantage of the full log data by training a Bayesian surrogate model. The uncertainty of the surrogate model is then used to balance between exploring an uncertain but potentially good choice and exploiting a known one. 3 Bayesian surrogate for accumulative metric
Instead of using the recorded accumulative metric from online experiments, we propose to estimate it from its deﬁnition in (2). In this formulation, p(a|x, Mi) is known and p(x) could be replaced with an empirical distribution, therefore, the key is to capture the distribution of the immediate feedback p(m|a, x). The data collected from online experiments contains lots of data points about this distribution. This allows us to build a Bayesian surrogate model for the immediate feedback. 3.1 Gaussian process surrogate model
We propose to use a Gaussian process (GP) as the surrogate model for the distribution of the immediate feedback. There is often stochasticity in the immediate feedback data including the intrinsic stochasticity in human interactions, e.g., some random reactions from a user, as well as the information that is not available to the production system. To accommodate this stochasticity, we divide the Bayesian surrogate model into two parts: (i) a GP model that captures the “noise-free" component of the immediate feedback, denoted as p(f |a, x); (ii) a noise distribution used to absorb all the stochasticity that cannot be explained by x and a, denoted as p(m|f ). When the immediate feedback is a continuous value, we use a Gaussian noise distribution. The resulting surrogate model can be written as, m = f (a, x) + (cid:15), f ∼ GP(0, k(·, ·)), (cid:15) ∼ N (0, σ2), (3) where the GP has zero mean and a covariance function k(·, ·). Stationary covariance functions are the most common choices, such as the radial basis function (RBF) and the Matérn covariance functions.
Note that the distribution of the immediate feedback p(m|a, x) is independent of the choice of candidate models. This allows us to train a single surrogate model and use it to score all the candidate models.
In some use cases, the inputs x and/or the decisions a may be categorical values, e.g., in recommender systems, the input may be a user ID and the decision may be an item ID, both of which are categorical values. The standard one-hot encoding is not a good representation for GP. Instead, we embed each unique ID as a latent variable in a low dimensional space, e.g., ak ∈ RQ, ak ∼ N (0, I). This approach is closely related to variational multi-output GPs [11]. Deep GPs [12, 13] can be considered if the distribution of immediate feedback is heavily non-stationary.
With a surrogate model for the immediate feedback, we have all the pieces to estimate the accumulative metric from (2). The integral is generally intractable but could be approximated by the methods like
Monte Carlo sampling. Note that the resulting quantity v(Mi) is deterministic as all the involved probability distributions are integrated out. It can serve as an estimator for the accumulative metric but is unable to be used for exploration-exploitation tradeoff. In order to construct an efﬁcient MSPS method, we need to represent the accumulative metric as a random variable, of which the uncertainty reﬂects the current belief of its value according to the surrogate model, which is often referred to as model uncertainty. 3.2 Estimation of the accumulative metric
To derive the accumulative metric as a random variable that reﬂects model uncertainty, we ﬁrst need to remove the uncertainty from the noise distribution, which corresponds to aleatoric uncertainty.
This is particularly crucial for the case of a binary immediate feedback, which will be explained in the next section. Firstly, we derive the expected immediate feedback from the noise distribution, i.e., ¯m = (cid:104)m(cid:105)p(m|f ). In the case of a normal noise distribution, the expected immediate feedback is the mean of the noise distribution, ¯m = (cid:82) mN (m; f, σ2) dm = f . Then, we derive the predictive expected immediate feedback from a inferred GP surrogate model by a change of random variable, 3
p( ¯m|A, X, D) = p(f |f = ¯m|A, X, D), where p(f |A, X, D) is the noise-free predictive distribution from GP conditioned on the collected data via online experiments.
Consider a list of inputs X = (x1, . . . , xT ) and the decision space A being discrete, denoted as
A = (a1, . . . , aK). Given a model Mi, the distribution of the model can be represented as a matrix
P ∈ [0, 1]K×T , where each entry pij = p(ai|xj). The accumulative metric is deﬁned as the sum of immediate feedback weighted by the inputs and decision probabilities. This allows us to derive the accumulative metric as a random variable ˆv|Mi, D,
ˆv|Mi, D = 1
T
P(cid:62)
: ¯m,
¯m ∼ p( ¯m|A, X, D), (4) where the subscript : denotes the vectorization of a matrix and ¯m is the vector of expected immediate feedback corresponding to the combinatorial of X and A, denoted as W = ((a1, x1), . . . , (aK, x1), . . . , (aK, xT )). As the change of random variable in (4) is a linear op-eration, the resulting random variable ˆv is jointly GP distributed with ¯m. It turns out that the resulting distribution p(ˆv|Mi, D) can be derived in closed-form, p(ˆv|Mi, D) = N (cid:18) 1
T
P(cid:62)
: K∗K−1m, 1
T
P(cid:62)
: (K∗∗ − K∗K−1K(cid:62)
∗ )P: (cid:19)
, (5) where m is the recorded immediate feedback in D, K is the covariance matrix among the observed data D, K∗ is the cross-covariance matrix between W and D and K∗∗ is the covariance among W.
Note that the expectation of the random variable ˆv recovers the accumulative metric estimator in (2), i.e., v(Mi) = (cid:82) ˆvp(ˆv|Mi, D) dˆv. As the probability distributions of inputs and decisions are represented in the matrix P and the uncertainty from the noise distribution is removed, the uncertainty in ˆv is a result of the model uncertainty of the GP surrogate model, which is crucial for the exploration-exploitation tradeoff.
For a real world problem, D often contains lots of data points, for which the cubic complexity of exact
GP inference is too expensive. For scalability, we use the variational sparse GP approximation [14]. It augments the original data with a set of pseudo data u at the corresponding locations Z. Such an aug-mentation does not change the original model distribution p(f |A, X) = (cid:82) p(f |u, A, X, Z)p(u|Z) du.
With an efﬁcient variational lower bound, the computational complexity reduces from O(N 3) to
O(N C 2), where C is the number of pseudo data. The inference result of sparse GP is often repre-sented by the variational posterior of the pseudo data, denoted as q(u) = N (mu, Su). With sparse
GP approximation, the distribution p(ˆv|Mi, D) becomes p(ˆv|Mi, D) = N (cid:18) 1
T
P(cid:62)
: K∗uK−1 uumu, 1
T
P(cid:62)
: (K∗∗ − K∗u(K−1 uu − K−1 uuSuK−1 uu)K(cid:62)
∗u)P: (cid:19)
, (6) where Kuu is the covariance matrix among the pseudo data and K∗u is the cross-covariance matrix between W and the pseudo data.
For a large problem, the variance calculation in (6) can also be very expensive as K∗∗ is a KT -by-KT matrix. For efﬁcient computation, we use a FITC approximation [15] at prediction time, (cid:1) and diag (·) i.e., pFITC(f |u, A, X, Z) = N (KfuK−1 makes a matrix into a diagonal matrix by letting off-diagonal entries be zero. Note that, although the conditional distribution p(f |u) is independent among the entries of f , the resulting distri-bution p( ¯m|A, X, D) is still correlated due to the correlation from the pseudo data. With the
FITC approximation, the mean p(ˆv|Mi, D) remains to be the same, while the variance becomes 1
T P(cid:62)
∗u)P:, in which only the diagonal entries of K∗∗ needs to be com-puted. uuu, Λ), where Λ = diag (cid:0)Kﬀ − KfuK−1
: (Λ + K∗uK−1 uuSuK−1 uuK(cid:62) fu uuK(cid:62) 3.3 Binary immediate feedback
In industrial use cases, binary immediate feedback is widely used because it is easy to calculate and easy to interpret by human, e.g., whether a user has responded to a shown item, whether a customer has purchased an item or whether a user has played a music or a movie. To apply our method to binary immediate feedback, we need to modify the GP surrogate model.
Firstly, we need to change the noise distribution to a Bernoulli distribution, p(m|f ) = σ(f )m(1 −
σ(f ))1−m, where σ(·) is a link function that squashes the value of f to be in (0, 1). The most 4
common link function is the logistic function. This makes the GP surrogate model become a GP binary classiﬁcation model, of which the marginal likelihood is no longer closed-form. For both tractability and scalability, we use stochastic variational sparse GP approximation [16], of which the intractable 1D integral in the variational lower bound is approximated by Gauss–Hermite quadrature.
Then, we derive the expected immediate feedback from the Bernoulli distribution, which is the probability of the immediate feedback being one, i.e., ¯m = (cid:80) m∈{0,1} m p(m|f ) = σ(f ). The predictive expected immediate feedback from a inferred GP surrogate model can be derived by a change of random variable, p( ¯m|A, X, D) = p(f |f =σ−1( ¯m)|A, X, D) (cid:12) (cid:12) (cid:12) dσ−1( ¯m) d ¯m (cid:12) (cid:12) (cid:12) . (7) where σ−1(·) is the inverse of the link function. Both σ(·) and σ−1(·) are scalar functions. We use
σ−1( ¯m) to denote applying σ−1(·) to the individual entries of ¯m. For binary immediate feedback, the random variable of the accumulative metric ˆv deﬁned in (4) no longer has a closed form probability density function. Fortunately, we can efﬁciently sample from p(ˆv|Mi, D), by ﬁrst drawing a sample from the “noise-free" GP surrogate model and compute the sample of ˆv according to (4), i.e.,
ˆvi = 1
T
P(cid:62)
: σ(fi), fi ∼ p(f |A, X, D). (8)
Note that for binary immediate feedback, it is crucial to derive the random variable ˆv from the expected immediate feedback ¯m instead of the original immediate feedback m. Imagine that we derive ˆv from m by replacing ¯m with m. The consequence is that the variance of ˆv will be at maximum if the expected immediate feedback m equals to 0.5, no matter how small the model uncertainty is. In this case, the uncertain in ˆv does not reﬂect the amount of unknowns in the surrogate model. Instead, deriving ˆv from ¯m can avoid this problem because the uncertainty from the noise distribution is excluded. 4 Choosing the next online experiment
After deriving the probability distribution of the accumulative metric, we use an acquisition function to guide the choice of the next online experiment. We consider the acquisition functions α(·) widely used in BO e.g. expected improvement (EI), probability of improvement (PI) and upper conﬁdence bound (UCB). A major difference to BO is that the space of choices is no longer the same as the input space of the surrogate model. In our case, the space of choices are the set of candidate models, while the input to the surrogate model is the input to the ML model and the decision from the ML model. As a result, the acquisition functions designed by considering an extra hypothetical evaluation such as entropy search [17] cannot be used within our method. For binary feedback, the acquisition functions are not closed form. We use Monte Carlo sampling to compute the acquisition function by drawing samples from the distribution of the accumulative metric.
We name the resulting algorithm as automated online experimentation (AOE), of which the overall procedure is shown in Algorithm 1. We start with an initial dataset D0, which may be collected by deploying the model online. The model can be randomly chosen or chosen according to some domain knowledge or ofﬂine accuracy measure. Note that often the training data for the candidate models may be used to train the surrogate model as well. At each iteration, we ﬁrst update surrogate model by inferring the variational posterior distribution as mentioned in Section 3. Then, we score all the candidate models with the acquisition function, which takes as inputs the distribution of the accumulative metric and select the model with the highest score. The selected model is deployed online for data collection. The collected data are augmented into the dataset for updating the surrogate model. We repeat this process until the online experiment budget is over. Then, the best model can be estimated from the latest surrogate model. 5