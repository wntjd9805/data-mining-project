Abstract
Linear relaxation based perturbation analysis (LiRPA) for neural networks, which computes provable linear bounds of output neurons given a certain amount of input perturbation, has become a core component in robustness veriﬁcation and certiﬁed defense. The majority of LiRPA-based methods focus on simple feed-forward networks and need particular manual derivations and implementations when extended to other architectures. In this paper, we develop an automatic framework to enable perturbation analysis on any neural network structures, by generalizing existing LiRPA algorithms such as CROWN to operate on general computational graphs. The ﬂexibility, differentiability and ease of use of our framework allow us to obtain state-of-the-art results on LiRPA based certiﬁed defense for fairly complicated networks like DenseNet, ResNeXt and Transformer that are not supported by prior works. Our framework also enables loss fusion, a technique that signiﬁcantly reduces the computational complexity of LiRPA for certiﬁed defense. For the ﬁrst time, we demonstrate LiRPA based certiﬁed defense on Tiny ImageNet and Downscaled ImageNet where previous approaches cannot scale to due to the relatively large number of classes. Our work also yields an open-source library for the community to apply LiRPA to areas beyond adversarial robustness without much LiRPA expertise, e.g., we create a neural network with a provably ﬂat optimization landscape by applying LiRPA to network parameters and considering perturbations on model weights. Our open source library is available at https://github.com/KaidiXu/auto_LiRPA. 1

Introduction
Bounding the range of a neural network outputs given a certain amount of input perturbation has become an important theme for neural network veriﬁcation and certiﬁed adversarial defense [48, 31, 45, 57]. However, computing the exact bounds for output neurons is usually intractable [21]. Recent research studies have developed perturbation analysis bounds that are sound, computationally feasible, and relatively tight [48, 54, 40, 47, 38, 46]. For a neural network function f (x) ∈ R, to study its behaviour at x0 with bounded perturbation δ such that x = x0 + δ ∈ S (e.g., S is a (cid:96)p norm ball around x0), these works provide two linear functions f (x) := a(cid:62)x+b and f (x) := a(cid:62)x+b that are guaranteed lower and upper bounds respectively for output neurons w.r.t. the input under perturbation: f (x) ≤ f (x) ≤ f (x) (∀x ∈ S). We refer to the technique used in these works as Linear Relaxation 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
based Perturbation Analysis (LiRPA). CROWN [54] and DeepPoly [40] are two representative
LiRPA algorithms. Beyond its usage in neural network veriﬁcation and certiﬁed defense, LiRPA is capable to serve as a general toolbox to understand the behavior of deep neural networks (DNNs) within a predeﬁned input region, and has been used for interpretation and explanation [24, 37].
To compute LiRPA bounds, the ﬁrst step is to obtain linear relaxations of any non-linear units [54, 36] (e.g., activation functions) in a network. Then, these relaxations need to be “glued” together according to the network structure to obtain the ﬁnal bounds. Early developments of LiRPA focused on feed-forward networks, and it has been extended to a few more complicated network structures for real-world applications. For example, Wong et al. [50] implemented LiRPA for convolutional ResNet on computer vision tasks; Zügner & Günnemann [59] extended [48] to graph convolutional networks;
Ko et al. [24] and Shi et al. [37] extended CROWN [54] to recurrent neural networks and Transformers respectively. Unfortunately, each of these works extends LiRPA with an ad-hoc implementation that only works for speciﬁc network architecture. This is similar to the “pre-automatic differentiation” era where researchers have to implement gradient computation by themselves for their designed network structure. Since LiRPA is signiﬁcantly more complicated than backpropagation, non-experts in neural network veriﬁcation can ﬁnd it challenging to understand and use LiRPA for their purpose.
Our paper takes a big leap towards making LiRPA a useful tool for general machine learning audience, by generalizing existing LiRPA algorithms to general computational graphs. Our framework is a superset of many existing works [49, 54, 47, 24, 37], and our automatic perturbation analysis algorithm is analogous to automatic differentiation. Our algorithm can compute LiRPA automatically for a given PyTorch model without manual derivation or implementation for the speciﬁc network architecture. Importantly, our LiRPA bounds are differentiable which allows efﬁcient training of these bounds. In addition, our proposed framework enables the following contributions:
• The ﬂexibility and ease-of-use of our framework allow us to easily obtain state-of-the-art certiﬁed defense and robustness veriﬁcation results for fairly complicated networks, such as DenseNet,
ResNeXt and Transfomer that are hardly supported in existing works due to tremendous efforts required for manual LiRPA implementation.
• We propose loss fusion, a technique that signiﬁcantly reduces the computational complexity of
LiPRA for certiﬁed defense. We demonstrate the ﬁrst LiPRA-based certiﬁed defense training on Tiny
ImageNet and Downscaled ImageNet [5], with a two-magnitude improvement on training efﬁciency.
• Our framework allows ﬂexible perturbation speciﬁcations beyond (cid:96)p-balls. For example, we demonstrate a dynamic programming approach to concretize linear bounds under discrete perturbation of synonym-based word substitution in a sentiment analysis task.
• We showcase that LiRPA can be a valuable tool beyond adversarial robustness, by demonstrating how to create a neural network with a provably ﬂat optimization landscape and revisit a popular hypothesis on generalization and the ﬂatness of optimization landscape. This is enabled by our uniﬁed treatment and automatic derivation of LiRPA bounds for parameter space variables (model weights). 2