Abstract
Graph pooling is a central component of a myriad of graph neural network (GNN) architectures. As an inheritance from traditional CNNs, most approaches formulate graph pooling as a cluster assignment problem, extending the idea of local patches in regular grids to graphs. Despite the wide adherence to this design choice, no work has rigorously evaluated its inﬂuence on the success of GNNs. In this paper, we build upon representative GNNs and introduce variants that challenge the need for locality-preserving representations, either using randomization or clustering on the complement graph. Strikingly, our experiments demonstrate that using these variants does not result in any decrease in performance. To understand this phenomenon, we study the interplay between convolutional layers and the subsequent pooling ones. We show that the convolutions play a leading role in the learned representations. In contrast to the common belief, local pooling is not responsible for the success of GNNs on relevant and widely-used benchmarks. 1

Introduction
The success of graph neural networks (GNNs) [3, 36] in many domains [9, 15, 25, 41, 46, 50] is due to their ability to extract meaningful representations from graph-structured data. Similarly to convolutional neural networks (CNNs), a typical GNN sequentially combines local ﬁltering, non-linearity, and (possibly) pooling operations to obtain reﬁned graph representations at each layer.
Whereas the convolutional ﬁlters capture local regularities in the input graph, the interleaved pooling operations reduce the graph representation while ideally preserving important structural information.
Although strategies for graph pooling come in many ﬂavors [26, 30, 47, 49], most GNNs follow a hierarchical scheme in which the pooling regions correspond to graph clusters that, in turn, are combined to produce a coarser graph [4, 7, 13, 21, 47, 48]. Intuitively, these clusters generalize the notion of local neighborhood exploited in traditional CNNs and allow for pooling graphs of varying sizes. The cluster assignments can be obtained via deterministic clustering algorithms [4, 7] or be learned in an end-to-end fashion [21, 47]. Also, one can leverage node embeddings [21], graph topology [8], or both [47, 48], to pool graphs. We refer to these approaches as local pooling.
Together with attention-based mechanisms [24, 26], the notion that clustering is a must-have property of graph pooling has been tremendously inﬂuential, resulting in an ever-increasing number of pooling schemes [14, 18, 21, 27, 48]. Implicit in any pooling approach is the belief that the quality of the cluster assignments is crucial for GNNs performance. Nonetheless, to the best of our knowledge, this belief has not been rigorously evaluated.
Misconceptions not only hinder new advances but also may lead to unnecessary complexity and obfuscate interpretability. This is particularly critical in graph representation learning, as we have seen a clear trend towards simpliﬁed GNNs [5, 6, 11, 31, 43].
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In this paper, we study the extent to which local pooling plays a role in GNNs. In particular, we choose representative models that are popular or claim to achieve state-of-the-art performances and simplify their pooling operators by eliminating any clustering-enforcing component. We either apply randomized cluster assignments or operate on complementary graphs. Surprisingly, the empirical results show that the non-local GNN variants exhibit comparable, if not superior, performance to the original methods in all experiments.
To understand our ﬁndings, we design new experiments to evaluate the interplay between convolu-tional layers and pooling; and analyze the learned embeddings. We show that graph coarsening in both the original methods and our simpliﬁcations lead to homogeneous embeddings. This is because successful GNNs usually learn low-pass ﬁlters at early convolutional stages. Consequently, the speciﬁc way in which we combine nodes for pooling becomes less relevant.
In a nutshell, the contributions of this paper are: i) we show that popular and modern representative
GNNs do not perform better than simple baselines built upon randomization and non-local pooling; ii) we explain why the simpliﬁed GNNs work and analyze the conditions for this to happen; and iii) we discuss the overall impact of pooling in the design of efﬁcient GNNs. Aware of common misleading evaluation protocols [10, 11], we use benchmarks on which GNNs have proven to beat structure-agnostic baselines. We believe this work presents a sanity-check for local pooling, suggesting that novel pooling schemes should count on more ablation studies to validate their effectiveness.
, with n > 0 nodes, as an ordered pair (A, X) comprising a
Notation. We represent a graph
Rn×d. The matrix A symmetric adjacency matrix A deﬁnes the graph structure: two nodes i, j are connected if and only if Aij = 1. We denote by D the
= ( ¯A, X), diagonal degree matrix of where ¯A has zeros in its diagonal and ¯Aij = 1 j Aij. We denote the complement of
= j. n×n and a matrix of node features X
, i.e., Dii = (cid:80)
G 0, 1
} by ¯
G
Aij for i
∈ {
∈
G
G
− 2 Exposing local pooling 2.1 Experimental setup
Models. To investigate the relevance of local pooling, we study three representative models. We
ﬁrst consider GRACLUS [8], an efﬁcient graph clustering algorithm that has been adopted as a pooling layer in modern GNNs [7, 34]. We combine GRACLUS with a sum-based convolutional operator [28].
Our second choice is the popular differential pooling model (DIFFPOOL) [47]. DIFFPOOL is the pioneering approach to learned pooling schemes and has served as inspiration to many methods [14].
Last, we look into the graph memory network (GMN) [21], a recently proposed model that reports state-of-the-art results on graph-level prediction tasks. Here, we focus on local pooling mechanisms and expect the results to be relevant for a large class of models whose principle is rooted in CNNs.
Tasks and datasets. We use four graph-level prediction tasks as running examples: predicting the constrained solubility of molecules (ZINC, [20]), classifying chemical compounds regarding their activity against lung cancer (NCI1, [40]); categorizing ego-networks of actors w.r.t. the genre of the movies in which they collaborated (IMDB-B, [45]); and classifying handwritten digits (Superpixels
MNIST, [1, 10]). The datasets cover graphs with none, discrete, and continuous node features. For completeness, we also report results on ﬁve other broadly used datasets in Section 3.1. Statistics of the datasets are available in the supplementary material (Section A.1).
Evaluation. We split each dataset into train (80%), validation (10%) and test (10%) data. For the regression task, we use the mean absolute error (MAE) as performance metric. We report statistics of the performance metrics over 20 runs with different seeds. Similarly to the evaluation protocol in
[10], we train all models with Adam [22] and apply learning rate decay, ranging from initial 10−3 down to 10−5, with decay ratio of 0.5 and patience of 10 epochs. Also, we use early stopping based on the validation accuracy. For further details, we refer to Appendix A in the supplementary material.
Notably, we do not aim to benchmark the performance of the GNN models. Rather, we want to isolate local pooling effects. Therefore, for model selection, we follow guidelines provided by the original authors or in benchmarking papers and simply modify the pooling mechanism, keeping the remaining model structure untouched. All methods were implemented in PyTorch [12, 33] and our code is available at https://github.com/AaltoPML/Rethinking-pooling-in-GNNs. 2 (cid:54)
Figure 1: Comparison between GRACLUS and COMPLEMENT. The plots show the distribution of the performances over 20 runs. In all tasks, GRACLUS and COMPLEMENT achieve similar performance.
The results indicate that pooling nearby nodes is not relevant to obtain a successful pooling scheme.
Case 1: Pooling with off-the-shelf graph clustering
We ﬁrst consider a network design that resembles standard CNNs. Following architectures used in [7, 12, 13], we alternate graph convolutions [28] and pooling layers based on graph clustering [8].
At each layer, a neighborhood aggregation step combines each node feature vector with the features of its neighbors in the graph. The features are linearly transformed before running through a component-wise non-linear function (e.g., ReLU). In matrix form, the convolution is
Z(l) = ReLU (cid:16)
X (l−1)W (l) 1 + A(l−1)X (l−1)W (l) 2 (cid:17) with (A(0), X (0)) = (A, X), (1) 1 ∈ 2 , W (l) where W (l)
Rdl−1×dl are model parameters, and dl is the embedding dimension at layer l.
The next step consists of applying the GRACLUS algorithm [8] to obtain a cluster assignment matrix
S(l) nl−1×nl mapping each node to its cluster index in
, with nl < nl−1 clusters. 0, 1
}
}
We then coarsen the features by max-pooling the nodes in the same cluster: 1, . . . , nl
{
∈ {
X (l) kj = max i:S(l) ik =1
Z (l) ij k = 1, . . . , nl and coarsen the adjacency matrix such that A(l) ij = 1 iff clusters i and j have neighbors in
A(l) = S(l) (cid:124)
A(l−1)S(l) with A(l) kk = 0 k = 1, . . . , nl. (2) (l−1):
G (3)
Clustering the complement graph. The clustering step holds the idea that good pooling regions, equivalently to their CNN counterparts, should group nearby nodes. To challenge this intuition, we follow an opposite idea and set pooling regions by grouping nodes that are not connected in the graph.
In particular, we compute the assignments S(l) by applying GRACLUS to the complement graph
¯ (l−1). Note that we only employ the complement graph to compute cluster assignments (l−1) of
G
S(l). With the assignments in hand, we apply the pooling operation (Equations 2 and 3) using the original graph structure. Henceforth, we refer to this approach as COMPLEMENT.
G
Results. Figure 1 shows the distribution of the performance of the standard approach (GRACLUS) and its variant that operates on the complement graph (COMPLEMENT). In all tasks, both models perform almost on par and the distributions have a similar shape.
Table 1: GRACLUS and COMPLEMENT performances are similar to those from the best performing isotropic GNNs (GIN and
GraphSAGE) in [11] or [10]. For ZINC, lower is better.
Despite their simplicity, GRACLUS and
COMPLEMENT are strong baselines (see
Table 1). For instance, Errica et al. [11] report GIN [44] as the best performing model for the NCI1 dataset, achieving 80.0 1.4 accuracy. This is indistinguish-± able from COMPLEMENT’s performance (80.1 1.6). We observe the same trend on IMDB-B, GraphSAGE [16] obtains 4.6 and COMPLEMENT scores 69.9 70.6 5.1 — a less than 0.6% difference. Using the same data splits as [10], COMPLEMENT and GIN perform within a margin of 1% in accuracy (SMNIST) and 0.01 in absolute error (ZINC).
GraphSAGE
GIN
GRACLUS
COMPLEMENT
SMNIST 97.1 94.0 94.3 94.5
IMDB-B 69.9 66.8 69.9 70.9 (cid:3) 0.01 0.01 0.02 0.02 (cid:2) 0.02 1.30 0.34 0.33 0.41 0.41 0.42 0.43 76.0 80.0 80.1 80.2 (cid:2) 4.6 3.9 3.5 3.9 (cid:2) 1.8 1.4 1.6 1.4
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
Models
±
±
ZINC
NCI1
± 3
(0, 1),
Figure 2: Boxplots for DIFFPOOL (DP) and its random variants: Uniform (0.3). For all datasets, at least one of the random models achieves higher average and Bernoulli accuracy than DIFFPOOL. Also, random pooling does not consistently lead to higher variance. The results show that the learned pooling assignments are not relevant for the performance of DIFFPOOL. (0, 1), Normal
N
B
U
Case 2: Differential pooling
DIFFPOOL [47] uses a GNN to learn cluster assignments for graph pooling. At each layer l, the soft cluster assignment matrix S(l)
Rnl−1×nl is
S(l) = softmax (cid:16) 1 (A(l−1), X (l−1)) (cid:17) with (A(0), X (0)) = (A, X). (4)
∈
GNN(l)
The next step applies S(l) and a second GNN to compute the graph representation at layer l:
X (l) = S(l) (cid:124)
GNN(l) 2 (A(l−1), X (l−1)) and
A(l) = S(l) (cid:124)
A(l−1)S(l). (5)
During training, DIFFPOOL employs a sum of three loss functions: i) a supervised loss; ii) the
Frobenius norm between A(l) and the Gramian of the cluster assignments, at each layer, i.e., (cid:80)
; iii) the entropy of the cluster assignments at each layer. The second (cid:107) loss is referred to as the link prediction loss and enforces nearby nodes to be pooled together. The third loss penalizes the entropy, encouraging sharp cluster assignments.
S(l)S(l)
A(l) l (cid:107)
− (cid:124)
Random assignments. To confront the inﬂuence of the learned cluster assignments, we replace
S(l) in Equation 4 with a normalized random matrix softmax( ˜S(l)). We consider three distributions: (Uniform) ˜S(l) ij ∼ U (a, b) (Normal) ˜S(l) ij ∼ N (µ, σ2) (Bernoulli) ˜S(l) ij ∼ B (α) (6)
We sample the assignment matrix before training starts and do not propagate gradients through it.
Results. Figure 2 compares DIFFPOOL against the randomized variants.
In all tasks, the highest average accuracy is due to a randomized approach. Nonetheless, there is no clear winner among all meth-ods. Notably, the variances obtained with the random pooling schemes are not signif-icantly higher than those from DIFFPOOL.
Remark 1. Permutation invariance is an important property of most GNNs that as-sures consistency across different graph representations. However, the random-ized variants break the invariance of DIFF-POOL. A simple ﬁx consists of taking ˜S(l) = X (l−1) ˜S(cid:48), where ˜S(cid:48)
Rdl−1×nl is a random matrix.
Figure 3 compares the randomized variants with and without this ﬁx w.r.t. the validation error on artiﬁcially permuted graphs during training on the ZINC dataset. Results suggest that the variants are approximately invariant.
Figure 3: MAE obtained over random permutations of each graph in the validation set. Both Invariant and fully Random produce similar predictions throughout the training.
∈ 4
Case 3: Graph memory networks
Graph memory networks (GMNs) [21] consist of a sequence of memory layers stacked on top of a GNN, also known as the initial query network. We denote the output of the initial query network by Q(0). The ﬁrst step in a memory layer computes kernel matrices between input queries
Q(l−1) = [q(l−1) nl−1 ](cid:124) and multi-head keys K(l)
, . . . , q(l−1) 1h, . . . , k(l) h = [k(l) nlh](cid:124): 1 h : S(l)
S(l) ijh ∝ (cid:16) 1 + q(l−1) i (cid:107) k(l) jh (cid:107) 2/τ
− (cid:17)− τ +1 2 h = 1 . . . H,
∀ (7) where H is the number of heads and τ is the degrees of freedom of the Student’s t-kernel.
We then aggregate the multi-head assignments S(l) 1 convolution followed by row-wise softmax normalization. Finally, we pool the node embeddings Q(l−1) according to their soft assignments S(l) and apply a single-layer neural net:
Q(l−1)W (l)(cid:17) h into a single matrix S(l) using a 1
Q(l) = ReLU
S(l)
× (cid:16) (cid:124)
. (8)
In this notation, queries Q(l) correspond to node embeddings X (l) for l > 0. Also, note that the memory layer does not leverage graph structure information as it is fully condensed into Q(0).
Following [21], we use a GNN as query network. In particular, we employ a two layer network with the same convolutional operator as in Equation 1.
The loss function employed to learn GMNs consists of a convex combination of: i) a supervised loss; and ii) the Kullback-Leibler divergence between the learned assignments and their self-normalized squares. The latter aim to enforce sharp soft-assignments, similarly to the entropy loss in DIFFPOOL.
Remark 2. Curiously, the intuition behind the loss that allegedly improves cluster purity might be misleading. For instance, uniform cluster assignments, the ones the loss was engineered to avoid, are a perfect minimizer for it. We provide more details in Appendix D.
Simplifying GMN. The principle behind GMNs consists of grouping nodes based on their sim-ilarities to learned keys (centroids). To scrutinize this principle, we propose two variants. In the
ﬁrst, we replace the kernel in Equation 7 by the euclidean distance taken from ﬁxed keys drawn from a uniform distribution. Opposite to vanilla GMNs, the resulting assignments group nodes that are farthest from a key. The second variant substitutes multi-head assignment matrices for a ﬁxed matrix whose entries are independently sampled from a uniform distribution. We refer to these approaches, respectively, as DISTANCE and RANDOM.
Results. Figure 4 compares GMN with its simpliﬁed variants. For all datasets, DIS-TANCE and RANDOM perform on par with
GMN, with slightly better MAE for the
ZINC dataset. Also, the variants present no noticeable increase in variance.
It is worth mentioning that the simpliﬁed
GMNs are naturally faster to train as they have signiﬁcantly less learned parameters.
In the case of DISTANCE, keys are taken as constants once sampled. Additionally,
RANDOM bypasses the computation of the pairwise distances in Equation 7, which dominates the time of the forward pass in
GMNs. On the largest dataset (SMNIST),
DISTANCE takes up to half the time of
GMN (per epoch), whereas RANDOM is up to ten times faster than GMN.
Figure 4: GMN versus its randomized variants DISTANCE and
RANDOM. The plots show that pooling based on dissimilarity measure (distance) instead of similarity (kernel) does not have a negative impact in performance. The same holds true when we employ random assignments. Both variants achieve lower
MAE for the regression task (ZINC). 5
Figure 5: Non-local pooling: activations for three arbitrary graphs, before and after the ﬁrst pooling layer. The convolutional layers learn to output similar embeddings within a same graph. As a result, methods become approximately invariant to how embeddings are pooled. Darker tones denote lower activation values. To better assess homogeneity, we normalize the color range of each embedding matrix individually. The number of nodes for each embedding matrix is given inside brackets. 3 Analysis
The results in the previous section are counter-intuitive. We now analyze the factors that led to these results. We show that the convolutions preceding the pooling layers learn representations which are approximately invariant to how nodes are grouped. In particular, GNNs learn smooth node representations at the early layers. To experimentally show this, we remove the initial convolutions that perform this early smoothing. As a result, all networks experience a signiﬁcant drop in performance.
Finally, we show that local pooling offers no beneﬁt in terms of accuracy to the evaluated models.
Pooling learns approximately homogeneous node representations. The results with the random pooling methods suggest that any convex combination of the node features enables us to extract good graph representation. Intuitively, this is possible if the nodes display similar activation patterns before pooling. If we interpret convolutions as ﬁlters deﬁned over node features/signals, this phenomenon can be explained if the initial convolutional layers extract low-frequency information across the graph input channels/embedding components. To evaluate this intuition, we compute the activations before the ﬁrst pooling layer and after each of the subsequent pooling layers. Figure 5 shows activations for the random pooling variants of DIFFPOOL and GMN, and the COMPLEMENT approach on ZINC.
The plots in Figure 5 validate that the ﬁrst convolutional layers produce node features which are relatively homogeneous within the same graph, specially for the randomized variants. The networks learn features that resemble vertical bars. As expected, the pooling layers accentuate this phenomenon, extracting even more similar representations. We report embeddings for other datasets in Appendix E.
Even methods based on local pooling tend to learn homogeneous representations. As one can notice from Figure 6, DIFFPOOL and GMN show smoothed patterns in the outputs of their initial pooling layer. This phenomenon explains why the performance of the randomized approaches matches those of their original counterparts. The results suggest that the loss terms designed to enforce local clustering are either not beneﬁcial to the learning task or are obfuscated by the supervised loss. This observation does not apply to GRACLUS, as it employs a deterministic clustering algorithm, separating the possibly competing goals of learning hierarchical structures and minimizing a supervised loss.
Figure 6: Local pooling: activations for the same graphs from Figure 5, before and after the ﬁrst pooling layer. Similar to the simpliﬁed variants, their standard counterparts learn homogenous embeddings, specially for DIFFPOOL and GMN. The low variance across node embedding columns illustrates this homogeneity. Darker tones denote lower values and colors are not comparable across embedding matrices. Brackets (top) show the number of nodes after each layer for each graph. 6
To further gauge the impact of the unsupervised loss on the perfor-mance of these GNNs, we compare two DIFFPOOL models trained with the link prediction loss multiplied by the weighting factors λ = 100 and λ = 103. Figure 7 shows the validation curves of the supervised loss for ZINC and SMNIST. We ob-serve that the supervised losses for models with both of the λ values converge to a similar point, at a sim-ilar rate. This validates that the un-supervised loss (link prediction) has little to no effect on the predictive performance of DIFFPOOL.
We have observed a similar behavior for GMNs, which we report in the supplementary material.
Figure 7: Supervised loss on validation set for ZINC and SMNIST datasets. Scaling up the unsupervised loss has no positive impact on the performance of DIFFPOOL.
In Figure 6, we observe homogeneous node representations even before
Discouraging smoothness. the ﬁrst pooling layer. This naturally poses a challenge for the upcoming pooling layers to learn meaningful local structures. These homogeneous embeddings correspond to low frequency signals deﬁned on the graph nodes. In the general case, achieving such patterns is only possible with more than a single convolution. This can be explained from a spectral perspective. Since each convolutional layer corresponds to ﬁlters that act linearly in the spectral domain, a single convolution cannot ﬁlter out speciﬁc frequency bands. These ideas have already been exploited to develop simpliﬁed GNNs [31, 43] that compute ﬁxed polynomial ﬁlters in a normalized spectrum.
= 1
ZINC
IMDB-B 69.7 69.8 76.6 74.2 80.1 80.2
DIFFPOOL
GRACLUS
COMPLEMENT
SMNIST
# Convolutions = 1 > 1 = 1 > 1 = 1 > 1 94.3 94.5
Table 2: All networks experience a decrease in performance when implemented with a single convolution before pooling (numbers in red).
For ZINC, lower is better.
NCI1
Remarkably, using multiple con-volutions to obtain initial embed-dings is common practice [18, 26].
To evaluate its impact, we apply a single convolution before the
ﬁrst pooling layer in all networks.
We then compare these networks against the original implementa-tions. Table 2 displays the re-sults. All models report a signif-icant drop in performance with a single convolutional layer. On
NCI1, the methods obtain accura-cies about 4% lower, on average. Likewise, GMN and GRACLUS report a 4% performance drop on SMNIST. With a single initial GraphSAGE convolution, the performances of Diffpool and its uniform variant become as lower as 66.4% on SMNIST. This dramatic drop is not observed only for
IMDB-B, which counts on constant node features and therefore may not beneﬁt from the additional convolutions. Note that under reduced expressiveness, pooling far away nodes seems to impose a negative inductive bias as COMPLEMENT consistently fails to rival the performance of GRACLUS.
Intuitively, the number of convolutions needed to achieve smooth representations depend on the richness of the initial node features. Many popular datasets rely on one-hot features and might require a small number of initial convolutions. Extricating these effects is outside the scope of this work.
U
GMN
Rand. GMN 0.459 0.457 0.560 0.544 0.409 0.400 0.429 0.431 0.458 0.489 0.469 0.465
DIFFPOOL 68.5 68.7 66.6 66.4 76.9 76.8 89.6 89.4 71.3 72.8 69.9 66.8 87.4 90.0 69.9 70.9 80.2 80.2 92.4 94.0 89.1 81.0 68.5 69.0 77.1 76.4 68.9 66.6
> 1
Is pooling overrated? Our experiments suggest that local pooling does not play an important role on the performance of GNNs. A natural next question is whether local pooling presents any advantage over global pooling strategies. We run a GNN with global mean pooling on top of three 0.18 convolutional layers, the results are: 79.65 (SMNIST), and 0.443 0.03 (ZINC). These performances are on par with our previous results
± obtained using GRACLUS, DIFFPOOL and GMN. 4.58 (IMDB-B), 95.20 2.07 (NCI), 71.05
±
±
±
Regardless of how surprising this may sound, our ﬁndings are consistent with results reported in the literature. For instance, GraphSAGE networks outperform DIFFPOOL in a rigorous evaluation protocol [10, 11], although GraphSAGE is the convolutional component of DIFFPOOL. Likewise, but in the context of attention, Knyazev et al. [24] argue that, except under certain circumstances, general attention mechanisms are negligible or even harmful. Similar ﬁndings also hold for CNNs [35, 39]. 7
Figure 8: Performance gap between GRACLUS and COMPLEMENT for different hyperparameter settings. Numbers denote the performance of GRACLUS — negative MAE for ZINC, AUROC for
MOLHIV, and accuracy for the rest. Red tones denote cases on which COMPLEMENT obtains better performance. For all datasets, the performance gap is not larger than 0.05 (or 5% in accuracy). The predominant light tones demonstrate that both models obtain overall similar results, with no clear trend toward speciﬁc hyperparameter choices. is a relevant property for GNNs as it guarantees that the network’s
Permutation invariance predictions do not vary with the graph representation. For completeness, we state in Appendix C the permutation invariance of the simpliﬁed GNNs discussed in Section 2. 3.1 Additional results
More datasets. We consider four additional datasets commonly used to assess GNNs, three of which are part of the TU datasets [29]: PROTEINS, NCI109, and DD; and one from the recently proposed Open Graph Benchamark (OGB) framework [17]: MOLHIV. Since Ivanov et al. [19] showed that the IMDB-B dataset is affected by a serious isomorphism bias, we also report results on the “cleaned” version of the IMDB-B dataset, hereafter refereed to as IMDB-C.
Table 3: Results for the additional datasets. Again, we observe that non-local pooling yields results as good as local pooling.
Models
Table 3 shows the results for the additional datasets. Similarly to what we have observed in the pre-vious experiments, non-local pool-ing performs on par with local pooling. The largest performance gap occurs for the PROTEINS and MOHIV datasets, on which
COMPLEMENT achieves accuracy around 3% higher than GRACLUS, on average. The performance of all methods on IMDB-C does not differ signiﬁcantly from their performance on IMDB-B. However, removing the isomorphisms in IMDB-B reduces the dataset size, which clearly increases variance.
N 68.4
GMN
Random-GMN 70.2
PROTEINS 4.1 72.6 3.1 75.3
MOLHIV 1.8 74.6 1.9 76.4
NCI109 2.6 77.7 2.4 77.0
IMDB-C 8.3 70.5 7.0 70.1
GRACLUS
COMPLEMENT
-DIFFPOOL
DIFFPOOL 74.8 74.8 75.2 74.6 73.7 73.5 75.1 74.8 77.0 76.9 72.2 71.8 76.9 78.3 73.9 74.3 72.1 72.1 70.9 70.3 1.5 2.1 4.0 3.8 4.6 3.6 1.8 2.6 3.3 3.9 1.3 1.3 1.7 2.0 5.2 3.6 4.4 4.1 6.7 7.5 7.1 8.0
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
DD
Another pooling method. We also provide results for MINCUTPOOL [2], a recently proposed pooling scheme based on spectral clustering. This scheme integrates an unsupervised loss which stems from a relaxation of a MINCUT objective and learns to assign clusters in a spectrum-free way.
We compare MINCUTPOOL with a random variant for all datasets employed in this work. Again, we
ﬁnd that a random pooling mechanism achieves comparable results to its local counterpart. Details are given in Appendix B.
Sensitivity to hyperparameters. As mentioned in Section 2.1, this paper does not intend to bench-mark the predictive performance of models equipped with different pooling strategies. Consequently, we did not exhaustively optimize model hyperparameters. One may wonder whether our results hold for a broader set of hyperparameter choices. Figure 8 depicts the performance gap between
GRACLUS and COMPLEMENT for a varying number of pooling layers and embedding dimensionality over a single run. We observe the greatest performance gap in favor of COMPLEMENT (e.g., 5 layers and 32 dimensions on ZINC), which does not amount to an improvement greater than 0.05 in MAE.
Overall, we ﬁnd that the choice of hyperparameters does not signiﬁcantly increase the performance gap between GRACLUS and COMPLEMENT. 8
4