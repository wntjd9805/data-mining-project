Abstract
Entropy regularization, smoothing of Q-values and neural network function ap-proximator are key components of the state-of-the-art reinforcement learning (RL) algorithms, such as Soft Actor-Critic [1]. Despite the widespread use, the impact of these core techniques on the convergence of RL algorithms is not yet fully understood. In this work, we analyse these techniques from error propagation perspective using the approximate dynamic programming framework. In particular, our analysis shows that (1) value smoothing results in increased stability of the algorithm in exchange for slower convergence, (2) entropy regularization reduces overestimation errors at the cost of modifying the original problem, (3) we study a combination of these techniques that describes the Soft Actor-Critic algorithm. 1

Introduction
In practical settings, the reinforcement learning (RL) algorithms are faced with a challenge of maximizing the cumulative reward given a Ô¨Ånite sample of environment transitions and inexact representation of policy and value function. This gives rise to errors that propagate across learning iterations and, combined, can result in divergence. Recently, state-of-the-art RL algorithms have been successful in solving complex environments and, hence, overcoming inaccuracies and their accumulation.
A number of techniques is commonly used in the large-scale RL setting, namely, entropy regulariza-tion, smoothing of Q-values and neural network function approximation. The entropy regularized methods [2, 3, 1] have shown robust behaviour on a variety of simulated and real-world tasks. In addition to regularization, smoothing of Q-value function, known as "soft" or delayed target update, is reported to increase stability of the algorithm and lower the risk of divergence [4, 5, 1]. Besides entropy regularization and value smoothing, notable progress on complex environments has been achieved with the use of neural network function approximators [6].
Despite practically valuable results, the theory behind these components is incomplete. Recently, several theoretical frameworks have been proposed to capture a multitude of entropy-regularized methods [7, 8, 9], still the practical effectiveness of entropy regularization is not yet fully understood.
Recent works contributed to the theoretical grounding of the state-of-the-art entropy regularized policy-gradient algorithms [10, 11] and understanding the geometry of their objective function [12].
Contributions.
In this paper, we show the implications of the above techniques on error propagation and convergence of actor-critic and value-based RL algorithms. We carry the error propagation analysis of abstract algorithms implementing entropy regularization and value smoothing using approximate dynamic-programming framework [13].
In detail, (1) We formalize the value smoothing technique using a new smooth Bellman operator.
The analysis of the resulting dynamic programming scheme shows that value smoothing results in increased stability in exchange for slower convergence. (2) Based on the formalism of regularized
Bellman operators [8], we show that the entropy regularization reduces overestimation errors [14] at the cost of modifying the original problem. The amount of this reduction depends on the size of state-action advantages scaled by the regularization parameter. (3) Motivated by the popular 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Soft Actor-Critic algorithm [1], we provide error bounds for an abstract algorithm that combines smoothing with regularization and utilises neural network function approximation.