Abstract
Current state-of-the-art object recognition models are largely based on convo-lutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we ﬁrst observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adver-sarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a ﬁxed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientiﬁc model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor ﬁlter bank, simple and com-plex cell nonlinearities, and a V1 neuronal stochasticity generator. After training,
VOneNets retain high ImageNet performance, but each is substantially more ro-bust, outperforming the base CNNs and state-of-the-art methods by 18% and 3%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications. 1

Introduction
For the past eight years, convolutional neural networks (CNNs) of various kinds have dominated object recognition [1, 2, 3, 4], even surpassing human performance in some benchmarks [5]. However, scratching beneath the surface reveals a different picture. These CNNs are easily fooled by imper-ceptibly small perturbations explicitly crafted to induce mistakes, usually referred to as adversarial 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
attacks [6, 7, 8, 9, 10]. Further, they exhibit a surprising failure to recognize objects in images corrupted with different noise patterns that humans have no trouble with [11, 12, 13]. This remarkable fragility to image perturbations has received much attention in the machine learning community, often from the perspective of safety in real-world deployment of computer vision systems [14, 15, 16, 17, 18, 19, 20, 21, 22]. As these perturbations generally have no perceptual alignment with the object class [23], the failures suggest that current CNNs obtained through task-optimization end up relying on visual features that are not all the same as those used by humans [24, 25]. Despite these limitations, some CNNs have achieved unparalleled success in partially explaining neural responses at multiple stages of the primate ventral stream, the set of cortical regions underlying primate visual object recognition [26, 27, 28, 29, 30, 31, 32].
How can we develop CNNs that robustly generalize like human vision? Incorporating biological constraints into CNNs to make them behave more in line with primate vision is an active ﬁeld of research [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 32]. Still, no neurobiological prior has been shown to considerably improve CNN robustness to both adversarial attacks and image corruptions in challenging real-world tasks such as ImageNet [44]. Here, we build on this line of work, starting with the observation that the ability of each CNN to explain neural response patterns in primate primary visual cortex (V1) is strongly correlated with its robustness to imperceptibly small adversarial attacks.
That is, the more biological a CNN’s "V1" is, the more adversarially robust it is.
Inspired by this, we developed VOneNets, a new class of hybrid CNNs, containing a biologically-constrained neural network that simulates primate V1 as the front-end, followed by an off-the-shelf
CNN back-end trained using standard methods. The V1 front-end, VOneBlock, is based on the classical neuroscientiﬁc linear-nonlinear-Poisson (LNP) model, consisting of a ﬁxed-weight Gabor
ﬁlter bank (GFB), simple and complex cell nonlinearities, and neuronal stochasticity. The VOneBlock outperforms all standard ImageNet trained CNNs we tested at explaining V1 responses to naturalistic textures and noise samples. After training, VOneNets retain high ImageNet performance, but are substantially more robust than their corresponding base models, and compete with state-of-the-art defense methods on a conglomerate benchmark covering a variety of adversarial images and common
Importantly, these beneﬁts transfer across different architectures including image corruptions.
ResNet50 [4], AlexNet [1], and CORnet-S [32]. We dissect the VOneBlock, showing that all properties work in synergy to improve robustness and that speciﬁc aspects of VOneBlock circuitry offer robustness to different perturbation types. Notably, we ﬁnd that neuronal stochasticity plays a large role in the white box adversarial robustness, but that stochasticity alone is insufﬁcient to explain our results—neuronal stochasticity interacts supralinearly with the VOneBlock features to drive adversarial robustness. Finally, as a large percentage of this robustness remains even when we remove stochasticity only during the adversarial attack, we conclude that training with stochasticity at the VOneBlock level leads the downstream layers to learn more robust representations.
Model weights and code are available at https://github.com/dicarlolab/vonenet. 1.1