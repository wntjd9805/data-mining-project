Abstract
Vulnerability to adversarial attacks is one of the principal hurdles to the adoption of deep learning in safety-critical applications. Despite signiﬁcant efforts, both practical and theoretical, the problem remains open. In this paper, we analyse the geometry of adversarial attacks in the large-data, overparametrized limit for
Bayesian Neural Networks (BNNs). We show that, in the limit, vulnerability to gradient-based attacks arises as a result of degeneracy in the data distribution, i.e., when the data lies on a lower-dimensional submanifold of the ambient space.
As a direct consequence, we demonstrate that in the limit BNN posteriors are robust to gradient-based adversarial attacks. Experimental results on the MNIST and Fashion MNIST datasets, representing the ﬁnite data regime, with BNNs trained with Hamiltonian Monte Carlo and Variational Inference support this line of argument, showing that BNNs can display both high accuracy and robustness to gradient based adversarial attacks. 1

Introduction
Adversarial attacks are small, potentially imperceptible pertubations of test inputs that can lead to catastrophic misclassiﬁcations in high-dimensional classiﬁers such as deep Neural Networks (NN).
Since the seminal work of Szegedy et al. [2013], adversarial attacks have been intensively studied, and even state-of-the-art deep learning models, trained on very large data sets, have been shown to be susceptible to such attacks [Goodfellow et al., 2014]. In the absence of effective defenses, the widespread existence of adversarial examples has raised serious concerns about the security and robustness of models learned from data [Biggio and Roli, 2018]. As a consequence, the development of machine learnig models that are robust to adversarial perturbations is an essential pre-condition for 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
their application in safety-critical scenarios, where model failures have already led to fatal accidents
[Yadron and Tynan, 2016].
Many attack strategies are based on identifying directions of high variability in the loss function by evaluating gradients w.r.t. input points (see, e.g., Goodfellow et al. [2014], Madry et al. [2017]). Since such variability can be intuitively linked to uncertainty in the prediction, Bayesian Neural Networks (BNNs) [Neal, 2012] have been recently suggested as a more robust deep learning paradigm, a claim that has found some empirical support [Feinman et al., 2017, Gal and Smith, 2018, Bekasov and
Murray, 2018, Liu et al., 2018]. However, neither the source of this robustness, nor its general applicability are well understood mathematically.
In this paper we show a remarkable property of BNNs: in a suitably deﬁned large data limit, we prove that the gradients of the expected loss function of a BNN w.r.t. the input points vanish. Our analysis shows that adversarial attacks for deterministic NNs in the large data limit arise necessarily from the low dimensional support of the data generating distribution. By averaging over nuisance dimensions,
BNNs achieve zero expected gradient of the loss and are thus provably immune to gradient-based adversarial attacks.
We experimentally support our theoretical ﬁndings on various BNN architectures trained with
Hamiltonian Monte Carlo (HMC) and with Variational Inference (VI) on both MNIST and Fashion
MNIST data sets, empirically showing that the magnitude of the gradients decreases as more samples are taken from the BNN posterior. We also test this decreasing effect when approaching towards the overparametrized case on the Half Moons dataset. We experimentally show that two popular gradient-based attack strategies for attacking NNs are unsuccessful on BNNs. Finally, we conduct a large-scale experiment on thousands of different networks, showing that for BNNs high accuracy correlates with high robustness to gradient-based adversarial attacks, contrary to what observed for deterministic NNs trained via standard Stochastic Gradient Descent (SGD) [Su et al., 2018].
In summary, this paper makes the following contributions:
•
•
•
A theoretical framework to analyse adversarial robustness of BNNs in the large data limit.
A proof that, in this limit, the posterior average of the gradients of the loss function vanishes, providing robustness against gradient-based attacks.
Large-scale experiments, showing empirically that BNNs are robust to gradient-based attacks and can resist the well known accuracy-robustness trade-off.1