Abstract
We revisit the problem of learning from untrusted batches introduced by Qiao and
Valiant [QV17]. Recently, Jain and Orlitsky [JO19] gave a simple semideﬁnite programming approach based on the cut-norm that achieves essentially information-theoretically optimal error in polynomial time. Concurrently, Chen et al. [CLM19] considered a variant of the problem where µ is assumed to be structured, e.g. log-concave, monotone hazard rate, t-modal, etc. In this case, it is possible to achieve the same error with sample complexity sublinear in n, and they exhibited a quasi-polynomial time algorithm for doing so using Haar wavelets.
In this paper, we ﬁnd an appealing way to synthesize [JO19] and [CLM19] to give the best of both worlds: an algorithm which runs in polynomial time and can exploit structure in the underlying distribution to achieve sublinear sample complexity. Along the way, we simplify the approach of [JO19] by avoiding the need for SDP rounding and giving a more direct interpretation of it via soft ﬁltering, a powerful recent technique in high-dimensional robust estimation. We validate the usefulness of our algorithms in preliminary experimental evaluations. 1

Introduction
In this paper, we consider the problem of learning structured distributions from untrusted batches.
This is a variant on the problem of learning from untrusted batches, as introduced in [QV17]. Here, there is an unknown distribution µ over {1, . . . , n}, and we are given N batches of samples, each of size k. A (1 − (cid:15))-fraction of batches are “good,” and consist of k i.i.d. samples from some distribution
µi at distance at most ω from µ in total variation distance, but an (cid:15)-fraction of batches are “bad,” and can be adversarially corrupted. The goal is to estimate µ in total variation, equivalently L1.
This problem models a situation where we get batches of data from different users, e.g. in a crowdsourcing application. Each honest user provides a small batch of data, which is by itself insufﬁcient to learn a good model, and moreover, can come from slightly different distributions depending on the user, due to heterogeneity. At the same time, a non-trivial fraction of data can come from malicious users who wish to game our algorithm to their own ends. The high level question is whether we can exploit the batch structure of our data to improve the robustness of our estimator.
There are three separate, but equally important, metrics under which we can evaluate our estimator:
Robustness How accurately can we estimate µ in total variation distance?
Runtime Are there algorithms that run in polynomial time in all the relevant parameters?
Sample complexity How few batches do we need in order to estimate µ?
In the original paper, Qiao and Valiant [QV17] focus primarily on robustness. They give an algorithm for learning general µ from untrusted batches that uses a polynomial number of batches, and estimates
µ to within O in total variation distance, and they proved that this is the best possible
ω + (cid:15)/
√ (cid:17) (cid:16) k 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
up to constant factors. However, their estimator runs in time 2n. Qiao and Valiant [QV17] also gave an algorithm based on low-rank tensor approximation that needs nk time and batches. (cid:16)
ω + (cid:15)√ k
A natural question is whether or not this robustness can be achieved efﬁciently. Chen et al. [CLM19] gave an nlog2 1/(cid:15) time sum-of-squares algorithm that uses nlog 1/(cid:15) batches and estimates µ to within in L1. Concurrently and independently, Jain and Orlitsky [JO19] gave a
O polynomial time algorithm based on a much simpler SDP that achieves the same error. Their approach was based on an elegant way to combine approximation algorithms for the cut-norm [AN04] with the
ﬁltering approach for robust estimation [DKK+17, SCV18, DKK+18, DKK+19, DHL19]. (cid:17) (cid:112)log 1/(cid:15)
To some extent, the results of [CLM19, JO19] also address the third consideration, sample complexity.
[JO19] uses N = (cid:101)O(n/(cid:15)2) batches to achieve the above error rate. Even without corruptions, for general µ any algorithm needs at least Ω(n/(cid:15)2) batches of size k to learn to L1 error O(ω + (cid:15)/ k).
Thus, their sample complexity is nearly-optimal unless one makes additional assumptions.
√
Unfortunately, in many cases, domain size n can be very large, and sample complexity growing strongly with n can render the estimator impractical. However in most applications, we have prior knowledge about the shape of µ that could in principle be used to drastically reduce the sample complexity. For example, if µ is log-concave, monotone, or multimodal with bounded number of modes, µ can be approximated by a piecewise polynomial function, and sans corruptions, this can be used to reduce the sample complexity to logarithmic in n [CDSS14b]. An appealing aspect of the relaxation in [CLM19] was that it could incorporate shape-constraints, via Haar wavelets, allowing them to achieve sample complexity quasipolynomial in d and s, respectively the degree and number of parts in the piecewise polynomial approximation, and quasipolylogarithmic in n. Unfortunately, while [JO19] achieves better runtime and sample complexity in the unstructured setting, a priori their techniques do not extend to obtain a similar sample complexity under structural assumptions.
This raises a natural question: can we build on [JO19] and [CLM19], to incorporate shape constraints into a simple SDP approach that can achieve nearly-optimal robustness, polynomial runtime, and sample complexity sublinear in n? In this paper, we answer this question in the afﬁrmative: (cid:16)
η + ω + (cid:15)√ k
Theorem 1.1. Let µ be a distribution over [n] which is η-approximated by an s-part piecewise polynomial with degree at most d. There is an algorithm which runs in time polynomial in all parameters and estimates µ to within O in total variation after drawing N (cid:15)-corrupted batches, each of size k, where N = (cid:101)O (cid:0)(s2d2/(cid:15)2) · log3(n)(cid:1) is the number of batches.
√
Any algorithm for this problem must take at least Ω(sd/(cid:15)2) batches to achieve error O(η +ω +(cid:15)/ k), and an interesting open question is whether there is a polynomial time algorithm that achieves these bounds. For robust mean estimation for Gaussians, there is evidence for a Ω((cid:112)log 1/(cid:15)) gap between the best possible estimation error and what can be achieved by polynomial time algorithms [DKS17].
It seems plausible that the Ω((cid:112)log 1/(cid:15)) gap incurred by our result is unavoidable as well. (cid:17) (cid:112)log 1/(cid:15) 1.1 High-Level Argument
In this work we show how to unite the ﬁltering of [JO19] with the Haar wavelet technology of
[CLM19] to obtain a polynomial-time, sample-efﬁcient algorithm for learning structured distributions from untrusted batches. In this section, we will specialize to the case of ω = 0 for the sake of clarity.
Learning via Filtering Given a batch of samples Yi = (Y 1 i ) from a distribution µ over (cid:80)k
[n], the frequency vector { 1 i = a]}a∈[n] is distributed according to the normalized k multinomial distribution Mulk(µ) given by k draws from µ (see Section 1.3 for notation). Note that
µ is precisely the mean of Mulk(µ), so the problem of estimating µ from an (cid:15)-corrupted set of N frequency vectors is equivalent to that of robustly estimating the mean of a multinomial in L1. i , ..., Y k 1[Y j j=1
As such, it is natural to try to adapt existing algorithms for robust mean estimation of other distri-butions (in Euclidean norm); the fastest ones are based on the following simple ﬁltering approach.
Maintain weights for each point, initialized to uniform. At every step, measure the maximum “skew” of the weighted dataset in any direction, and if this skew is too high, update the weights by 1) ﬁnding the direction v in which the corruptions skew the dataset the most, 2) giving a “score” to each point 2
based on how badly it skews the dataset in direction v, 3) downweighting or removing points with high scores. Otherwise, if the skew is low, output the empirical mean of the weighted dataset.
To prove correctness, one must show three things for the particular skewness measure and score function chosen: A) (Regularity) for any sufﬁciently large collection of (cid:15)-corrupted batches, a particular deterministic regularity condition holds (Deﬁnition 3.2 and Lemma 3.3), B) (Soundness) under the regularity condition, if the skew of the weighted dataset is small, then the empirical mean of the weighted dataset is sufﬁciently close to the true mean (Lemma 3.5), C) (Progress) under the regularity condition, if the skew of the weighted dataset is large, then one iteration of the above update scheme will remove more weight from bad batches than from good (Lemma 3.6).
For isotropic Gaussians, skewness is just given by the maximum variance of the weighted dataset in any direction, i.e. maxv∈Sn−1 (cid:104)vv(cid:62), ˜Σ(cid:105) where ˜Σ is the empirical covariance of the weighted dataset.
Given maximizing v, the “score” of a point X is then simply its contribution to the skewness.
To learn in L1 distance, the right set of test vectors v to use is the Hamming cube {0, 1}n, so a natural attempt at adapting the above skewness measure to robust mean estimation of multinomials is to consider the quantity maxv∈{0,1}n (cid:104)vv(cid:62), ˜Σ(cid:105). But one of the key challenges in passing from isotropic
Gaussians to multinomial distributions is that this quantity is not very informative because we do not have a good handle on the covariance of Mulk(µ). In particular, it could be that for a direction v, (cid:104)vv(cid:62), ˜Σ(cid:105) is high simply because the good points have high variance to begin with.
An SDP for Skewness The clever workaround of [JO19] was to observe that we know exactly what the projection of a multinomial distribution Mulk(µ) in any {0, 1}n direction v is, namely
Bin(k, (cid:104)v, µ(cid:105)). And so to discern whether the corrupted points skew our estimate in a given direction v, one should measure not the variance in the direction v, but rather the following corrected quantity: the variance in the direction v, minus what the variance would be if the distribution of the projections in the v direction were actually given by Bin(k, (cid:104)v, ˜µ(cid:105)), where ˜µ is the empirical mean of the weighted dataset. We call this latter quantity a variance proxy. The new skewness measure can be written as max v∈{0,1}n (cid:26) (cid:104)vv(cid:62), ˜Σ(cid:105) − 1 k ((cid:104)v, ˜µ(cid:105) − (cid:104)v, ˜µ(cid:105)2) (cid:27)
. (1)
Finding the direction v ∈ {0, 1}n which maximizes this corrected quantity is some Boolean quadratic programming problem which can be solved approximately by solving the natural SDP relaxation and rounding to a Boolean vector v using the machinery of [AN04]. Using this approach, [JO19] obtained a polynomial-time algorithm for learning general discrete distributions from untrusted batches.
Structured Distributions: Beyond Boolean Test Vectors Learning structured distributions in the classical sense is well-understood: if a distribution µ is close in total variation distance to being s-piecewise degree-d, then to estimate µ in total variation distance it is enough to approximate µ in a much weaker norm which we will denote by (cid:107) · (cid:107)AK , where K is a parameter that depends on s and d. We review the details for this in Section 1.3.
The key challenge that [CLM19] had to address to port these techniques to the untrusted batches setting was that unlike the Hamming cube or Sn−1, it is unclear how to optimize over the set of test vectors dual to the AK norm. Combinatorially, this set is easy to characterize: (cid:107)µ − ˆµ(cid:107)AK is small if and only if (cid:104)µ − ˆµ, v(cid:105) is small for all v ∈ V n 2K is the set of all v ∈ {±1}n 2K ⊂ {±1}n, where V n with at most 2K sign changes when read as a vector from left to right.
A key observation in [CLM19] was that vectors with few sign changes admit sparse representations in the Haar wavelet basis, so instead of working with V n 2K, one can simply work with a convex relaxation of this Haar-sparsity constraint. As such, if we let K ⊆ Rn×n denote the relaxation of the set of {vv(cid:62)|v ∈ V n 2K} to all matrices Σ whose Haar transforms are “analytically sparse” in some appropriate, convex sense (see Section 2 for a formal deﬁnition), then as this set of test matrices contains the set of test matrices vv(cid:62) for v ∈ V n 2K, it is enough to learn µ in the norm associated to K, which is strictly stronger than the AK norm.
Our goal then is to produce ˆµ for which (cid:107)ˆµ − µ(cid:107)K (cid:44) supΣ∈K(cid:104)Σ, (ˆµ − µ)⊗2(cid:105)1/2 is small. Even though (cid:107) · (cid:107)K is a stronger norm, it turns out K’s metric entropy is still quite small. As we elaborate on in the supplementary material, the analysis of this in [CLM19] left much room for tightening. A reﬁned analysis of K in the present work allows us to get nearly tight sample complexity bounds. 3
Putting Everything Together All the pieces are in place to instantiate the ﬁltering framework: in lieu of the quantity in (1), which can be phrased as the maximization of some quadratic (cid:104)vv(cid:62), M (w)(cid:105) over {±1}n, where M (w) ∈ Rn×n depends on the dataset and the weights w on its points,1 we can deﬁne our skewness measure as maxΣ∈K(cid:104)Σ, M (w)(cid:105) = (cid:107)M (w)(cid:107)K, and we can deﬁne the score for each point in the dataset to be its contribution to the skewness measure (see (3)).
The reader may be wondering why, unlike [JO19] or applications of ﬁltering in other contexts, we never round Σ to an actual vector v ∈ V n 2K before computing skewness and scores. We emphasize that once restricted to bounded sign change vectors v ∈ V n 2K, the optimization problem in (1) becomes signiﬁcantly harder. Indeed, optimizing general quadratic forms v(cid:62)Av over V n 2K can essentially capture the problem of densest 2K-subgraph: for any A(cid:48), take A (cid:44) T (cid:62)A(cid:48)T where T is the matrix which maps v to (0, v2 − v1, v3 − v2, ..., vn − vn−1). Then optimizing v(cid:62)Av over v ∈ V n 2K is equivalent to optimizing w(cid:62)A(cid:48)w over (cid:96)-sparse vectors w ∈ {0, ±1}n whose nonzero entries are alternating in sign. Modulo this alternating sign condition, this is at least as hard as densest (cid:96)-subgraph, for which it is impossible, under the Exponential Time Hypothesis, to efﬁciently achieve any sub-poly(n) factor approximation [Man17]. In contrast, for the original optimization problem (1) considered in [JO20], one can reduce to the question of computing cut-norm, which can be done to within a constant factor via Krivine rounding.[AN04]
As our subsequent analysis will show, it turns out that rounding is unnecessary, both in our setting and in the unstructured distribution setting of [JO19]. This should be quite surprising as the afore-mentioned hardness of approximation [Man17] suggests that the integrality gap of our relaxation should be terrible. In particular, the norm (cid:107) · (cid:107)K induced by our relaxation might be very distorted relative to the A(cid:96) norm we actually care about. However, if one examines the proof ingredients enumerated above, it becomes evident that the ﬁltering framework does not actually require ﬁnding a concrete direction in Rn in which to ﬁlter, merely a skewness measure and score functions under which regularity, soundness, and progress can be proven. That said, it becomes more challenging to prove these ingredients when Σ is not rounded to an actual direction, though nevertheless possible.
We hope that this observation will prove useful in future applications of ﬁltering. 1.2