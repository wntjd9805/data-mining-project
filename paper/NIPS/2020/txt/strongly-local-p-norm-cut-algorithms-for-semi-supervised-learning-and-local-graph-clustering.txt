Abstract
Graph based semi-supervised learning is the problem of learning a labeling function for the graph nodes given a few example nodes, often called seeds, usually under the assumption that the graph’s edges indicate similarity of labels. This is closely related to the local graph clustering or community detection problem of ﬁnding a cluster or community of nodes around a given seed. For this problem, we propose a novel generalization of random walk, diffusion, or smooth function methods in the literature to a convex p-norm cut function. The need for our p-norm methods is that, in our study of existing methods, we ﬁnd those principled methods based on eigenvector, spectral, random walk, or linear system often have difﬁculty capturing the correct boundary of a target label or target cluster. In contrast, 1-norm or maxﬂow-mincut based methods capture the boundary, but cannot grow from small seed set; hybrid procedures that use both have many hard to set parameters. In this paper, we propose a generalization of the objective function behind these methods involving p-norms. To solve the p-norm cut problem we give a strongly local algorithm – one whose runtime depends on the size of the output rather than the size of the graph. Our method can be thought as a nonlinear generalization of the Anderson-Chung-Lang push procedure to approximate a personalized PageRank vector efﬁciently. Our procedure is general and can solve other types of nonlinear objective functions, such as p-norm variants of Huber losses. We provide a theoretical analysis of ﬁnding planted target clusters with our method and show that the p-norm cut functions improve on the standard Cheeger inequalities for random walk and spectral methods. Finally, we demonstrate the speed and accuracy of our new method in synthetic and real world datasets. 1

Introduction
Many datasets important to machine learning either start as a graph or have a simple translation into graph data. For instance, relational network data naturally starts as a graph. Arbitrary data vectors become graphs via nearest-neighbor constructions, among other choices. Consequently, understanding graph-based learning algorithms – those that learn from graphs – is a recurring problem. This ﬁeld has a rich history with methods based on linear systems [61, 62], eigenvectors [27, 24], graph cuts [8], and network ﬂows [35, 4, 54], although recent work in graph-based learning has often focused on embeddings [50, 22] and graph neural networks [58, 29, 39]. Our research seeks to understand the possibilities enabled by a certain p-norm generalization of the standard techniques.
Perhaps the two prototypical graph-based learning problems are semi-supervised learning and local clustering. Other graph-based learning problems include role discovery and alignments. Semi-supervised learning involves learning a labeling function for the nodes of a graph based on a few examples, often called seeds. The most interesting scenarios are when most of the graph has unknown 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Seed node and the target. (b) 2-norm problem. (c) 1.1-norm problem.
Figure 1: A simple illustration of the beneﬁts of our p-norm methods. In this problem, we generate a graph from an image with weighted neighbors as described in [51]. We intentionally make this graph consider large regions, so each pixel is connected to all neighbors within 40 pixels away. (Full details in the supplement.) The target in this problem is the cluster deﬁned by the interior of the window and we select a single pixel inside the window as the seed. The three colors (yellow, orange, red) show how the non-zero elements of the solution ﬁll-in as we decrease a sparsity penalty in our formulation (yellow is sparsest, red is densest). The 2-norm result exhibits a typical phenomenon of over-expansion, whereas the 1.1-norm accurately captures the true boundary. We tried running various 1-norm methods, but they were unable to grow a single seed node, as has been observed in many past experiments and also theoretically justiﬁed in [15, Lemma 7.2]. labels and there are only a few examples per label. This could be a constant number of examples per label, such as 10 or 50, or a small fraction of the total label size, such as 1%. Local clustering is the problem of ﬁnding a cluster or community of nodes around a given set of seeds. This is closely related to semi-supervised learning because that cluster is a natural suggestion for nodes that ought to share the same label, if there is a homophily property for edges in the network. If this homophily is not present, then there are transformations of the graph that can make these methods work better [49].
For both problems, a standard set of techniques is based on random walk diffusions and mincut constructions [61, 62, 27, 21, 48]. These reduce the problem to a linear system, eigenvector, random walk, or mincut-maxﬂow problem, which can often be further approximated. As a simple example, consider solving a seeded PageRank problem that is seeded on the nodes known to be labeled with a single label. The resulting PageRank vector indicates other nodes likely to share that same label.
This propensity of PageRank to propogate labels has been used in a many applications and it has many interpretations [32, 20, 45, 48, 41, 18], including guilt-by-association [33]. A related class of mincut-maxﬂow constructions uses similar reasoning [8, 54, 55].
The link between these PageRank methods and the mincut-maxﬂow computations is that they correspond to 1-norm and 2-norm variations on a general objective function (see [19] and Equation 1).
In this paper, we replace the norm with a general p-norm. (For various reasons, we refer to it as a q-norm in the subsequent technical sections. We use p-norm here as this usage is more common.) The literature on 1 and 2-norms is well established and largely suggests that 1-norm (mincut) objectives are best used for reﬁning large results from other methods – especially because they tend to sharpen boundaries – whereas 2-norm methods are best used for expanding small seed sets [54]. There is a technical reason for why mincut-maxﬂow formulations cannot expand small seed sets, unless they have uncommon properties, discussed in [15, Lemma 7.2]. The downside to 2-norm methods is that they tend to “expand” or “bleed out” over natural boundaries in the data. This is illustrated in Figure 1(b). The hypothesis motivating this work is that techniques that use a p-norm where 1 < p < 2 should provide a useful alternative – if they can be solved as efﬁciently as the other cases.
This is indeed what we ﬁnd and a small example of what our methods are capable of is shown in
Figure 1(c), where we use a 1.1-norm to avoid the over-expansion from the 2-norm method.
We are hardly the ﬁrst to notice these effects or propose p-norms as a solution. For instance, the p-Laplacian [3] and related ideas [2] has been widely studied as a way to improve results in spectral clustering [10] and semi-supervised learning [9]. This has recently been used to show the power of simple nonlinearities in diffusions for semi-supervised learning as well [25]. The major rationale for our paper is that our algorithmic techniques are closely related to those used for 2-norm optimization.
It remains the case that spectral (2-norm) approaches are far more widely used in practice, partly because they are simpler to implement and use, whereas the other approaches involve more delicate computations. Our new formulations are amenable to similar computation techniques as used for 2-norm problems, which we hope will enable them to be widely used. 2
The remainder of this paper consists of a demonstration of the potential of this idea. We ﬁrst formally state the problem and review technical preliminaries in Section 2. As an optimization problem the p-norm problem is strongly convex with a unique solution. Next, we provide a strongly local algorithm to approximate the solution (Section 3). A strongly local algorithm is one where the runtime depends on the size of the output rather than the size of the input graph. This enables the methods to run efﬁciently even on large graphs, because, simply put, we are able to bound the maximum output size and runtime independently of the graph size. A hallmark of the existing literature on these methods is a recovery guarantee called a Cheeger inequality. Roughly, this inequality shows that, if the methods are seeded nearby a good cluster, then the methods will return something that is not too far away from that good cluster. This is often quantiﬁed in terms of the conductance of the good cluster and the conductance of the returned cluster. There are a variety of tradeoffs possible here [5, 63, 57]. We prove such a relationship for our methods where the quality of the guarantee depends on the exponent 1/p, which reproduces the square root Cheeger guarantees [13] for p = 2 but gives better results when p < 2. Finally, we empirically demonstrate a number of aspects of our methods in comparison with a number of other techniques in Section 5. The goal is to highlight places where our p-norm objectives differ.
At the end, we have a number of concluding discussions (Section 6), which highlight dimensions where our methods could be improved, as well as related literature. For instance, there are many ways to use personalized PageRank methods with graph convolutional networks and embedding techniques [29] – we conjecture that our p-norm methods will simply improve on these relationships.
Also, and importantly, as we were completing this paper, we became aware of [17] which discusses p-norms for ﬂow-based diffusions. Our two papers have many similar ﬁndings on the beneﬁt of p-norms, although there are some meaningful differences in the approaches, which we discuss in
Section 6. In particular, our algorithm is distinct and follows a simple generalization of the widely used and deployed push method for PageRank. Our hope is that both papers can highlight the beneﬁts of this idea to improve the practice of graph-based learning. 2 Generalized local graph cuts
We consider graphs that are undirected, connected, and weighted with positive edge weights lower-bounded by 1. Let G = (V, E, w) be such a graph, where n = |V | and m = |E|. The adjacency matrix A has non-zero entries w(i, j) for each edge (i, j), and all other entries are zero. This is symmetric because the graph is undirected. The degree vector d is deﬁned as the row sum of A and
D is a diagonal matrix deﬁned as diag(d). The incidence matrix B ∈ {0, −1, 1}m×n measures the differences of adjacent nodes. The kth row of B represents the kth edge and each row has exactly two nonzero elements, i.e. 1 for start node of kth edge and −1 for end node of kth edge. For undirected graphs, either node can be the start node or end node and the order does not matter. We use vol(S) for the sum of weighted degrees of the nodes in S and φ(S) = min(vol(S),vol( ¯S)) for conductance. cut(S)
For simplicity, we begin with PageRank, which has been used for all of these tasks in various guises [61, 21, 5]. A PageRank vector [20] is the solution of the linear system (I − αAD−1)x = (1 − α)v where α is a probability between 0 and 1 and v is a stochastic vector that gives the seed distribution. This can be easily reworked into the equivalent linear system (γD + L)y = γv where x = Dy and L is the graph Laplacian L = D − A. The starting point for our methods is a result shown in [19], where we can further translate this into a 2-norm “cut” computation on a graph called the localized cut graph that is closely related to common constructions in maxﬂow-mincut computations for cluster improvement [4, 15].
The localized cut graph is created from the original graph, a set S, and a value γ. The construction adds an extra source node s and an extra sink node t, and edges from s to the original graph that localize a solution, or bias, a solution within the graph near the set S. Formally, given a graph
G = (V, E) with adjacency matrix A, a seed set S ⊂ V and a non-negative constant γ, the adjacency matrix of the localized cut graph is:
AS = (cid:34) 0 γdT
S (cid:35) 0
γdS A γd ¯S 0 γdT
¯S 0 and a small illustration is
Here ¯S is the complement set of S, dS = DeS, d ¯S = De ¯S, and eS is an indicator vector for S. 3
(a) PageRank (α = 0.85) (b) q=2, γ= κ =10−3 (c) q=5, γ=10−5, κ=10−4 (d) q=1.25, γ= κ =10−3 (e) heat kernel [12, 30] t = 10, ε = 0.003 (f) CRD [57]
U = 60, h = 60, w = 5 (g) p = 1.5-diffusion [25], h=0.002, k = 35000 (h) 1.5-Laplacian [25], h= 0.0001, n = 7500
Figure 2: A comparison of seeded cut-like and clustering objectives on a regular grid-graph with 4 axis-aligned neighbors. The graph is 50-by-50, the seed is in the center. The diffusions localize before the boundary so we only show the relevant region and the quantile contours of the values. We selected the parameters to give similar-sized outputs. (Top row) At left (a), we have seeded PageRank; (b)-(d) show our q-norm objectives; (b) is a 2-norm which closely resembles PageRank; (c) is a 5-norm that has diamond-contours; and (d) is a 1.25-norm that has square contours. (Bottom row)
Existing work with the (e) heat kernel diffusion [12, 30], (f) CRD [57], (g) nonlinear diffusions [25] (with a simple (g) p-norm nonlinearity in the diffusion or a (h) p-Laplacian) show that similar results are possible with existing methods, although they lack the simplicity of our optimization setup and often lack the strongly local algorithms.
Let B, w be the incidence matrix and weight vector for the localized cut-graph. Then PageRank is equivalent to the following 2-norm-cut problem (see full details in [19]) wT (Bx)2 = (cid:80) minimize x subject to xs = 1, xt = 0 i,j wi,j(xi − xj)2 = xT BT diag(w)Bx (1)
We call this a cut problem because if we replace the squared term with an absolute value (i.e., (cid:80) wi,j|xi − xj|), then we have the standard s, t-mincut problem. Our paper proceeds from changing this power of 2 into a more general loss-function (cid:96) and also adding a sparsity penalty, which is often needed to produce strongly local solutions [19]. We deﬁne this formally now.
Deﬁnition 1 (Generalized local graph cut). Fix a set S of seeds and a value of γ. Let B, w be the incidence matrix and weight vector of the localized cut graph. Then the generalized local graph cut problem is: wT (cid:96)(Bx) + κγdT x = (cid:80) minimize x subject to xs = 1, xt = 0, x ≥ 0. ij wi,j(cid:96)(xi − xj) + κγ (cid:80) i xidi (2)
Here (cid:96)(x) is an element-wise function and κ ≥ 0 is a sparsity-promoting term.
We compare using power functions (cid:96)(x) = 1 q |x|q to a variety of other techniques for semi-supervised learning and local clustering in Figure 2. If (cid:96) is convex, then the problem is convex and can be solved via general-purpose solvers such as CVX. An additional convex solver is SnapVX [23], which studied a general combination of convex functions on nodes and edges of a graph, although neither of these approaches scale to the large graphs we study in subsequent portions of this paper (65 million edges).
To produce a specialized, strongly local solver, we found it necessary to restrict the class of functions (cid:96) to have similar properties to the power function (cid:96)(x) = 1 q |x|q and its derivative (cid:96)(cid:48)(x). 4
Deﬁnition 2. In the [−1, 1] domain, the loss function (cid:96)(x) should satisfy (1) (cid:96)(x) is convex; (2) (cid:96)(cid:48)(x) is an increasing and anti-symmetric function; (3) For ∆x > 0, (cid:96)(cid:48)(x) should satisfy either of the following condition with constants k > 0 and c > 0 (3a) (cid:96)(cid:48)(x+∆x) ≤ (cid:96)(cid:48)(x)+k(cid:96)(cid:48)(∆x) and (cid:96)(cid:48)(cid:48)(x) > c or (3b) (cid:96)(cid:48)(x) is strictly increasing, c-Lipschitz continuous and (cid:96)(cid:48)(x + ∆x) ≥ (cid:96)(cid:48)(x) + k(cid:96)(cid:48)(∆x) when x ≥ 0.
Remark. If (cid:96)(cid:48)(x) is Lipschitz continuous with Lipschitz constant to be L and (cid:96)(cid:48)(cid:48)(x) > c, then constraint 3(a) can be satisﬁed with k = L/c. However, (cid:96)(cid:48)(x) can still satisfy 3(a) even if it is not
Lipschitz continuous. A simple example is (cid:96)(x) = |x|1.5, −1 ≤ x ≤ 1. In this case, k = 1 but it is not Lipschitz continuous at x = 0. On the other hand, when (cid:96)(cid:48)(x) is Lipschitz continuous, it can satisfy constraint 3(b) even if (cid:96)(cid:48)(cid:48)(x) = 0. An example is (cid:96)(x) = |x|3.5, −1 < x < 1. In this case (cid:96)(cid:48)(cid:48)(x) = 0 when x = 0 but (cid:96)(cid:48)(x + ∆x) ≥ (cid:96)(cid:48)(x) + (cid:96)(cid:48)(∆x) when x ≥ 0.
Lemma 2.1. The power function (cid:96)(x) = 1 q |x|q, −1 < x < 1 satisﬁes deﬁnition 2 for any q > 1.
More speciﬁcally, when 1 < q < 2, (cid:96)(x) satisﬁes 3(a) with c = q − 1 and k = 22−q, when q ≥ 2, (cid:96)(x) satisﬁes 3(b) with c = q − 1 and k = 1.
All proofs and additional lemmas are in the supplementary material for Sections 2, 3, 4.
Note that the (cid:96)(x) = |x| does not satisfy either choice for property (3). Consequently, our theory will not apply to mincut problems. In order to justify the generalized term, we note that q-norm generalizations of the Huber and Berhu loss functions [47] do satisfy these deﬁnitions.
Deﬁnition 3. Given 1 < q < 2 and 0 < δ < 1, the “q-Huber” and “Berq” function are q-Huber (cid:96)(x) =
= (cid:26) 1 1 2 δq−2x2 q |x|q + ( q−2 2q )δq if |x| ≤ δ otherwise
Berq (cid:96)(x) =
= (cid:40) 1 q δ2−q|x|q 2 x2 + ( 2−q 1 2q )δ2 if |x| ≤ δ otherwise.
Lemma 2.2. When −1 ≤ x ≤ 1, both “q-Huber” and “Berq” satisfy Deﬁnition 2. The value of k for both is 22−q, the c for q-Huber is q − 1 while the c for “Berq” is 1.
We now state uniqueness.
Theorem 2.1. Fix a set S, γ > 0, κ > 0. For any loss function satisfying Deﬁnition 2, then the
γ BT diag((cid:96)(cid:48)(Bx))w. solution x of (2) is unique. Moreover, deﬁne a residual function r(x) = − 1
A necessary and sufﬁcient condition to satisfy the KKT conditions is to ﬁnd x∗ where x∗ ≥ 0, r(x∗) = [rs, gT , rt]T with g ≤ κd (where d reﬂects the original graph), k∗ = [0, κd − g, 0]T and xT (κd − g) = 0. 3 Strongly Local Algorithms
In this section, we will provide a strongly local algorithm to approximately optimize equation (2) with (cid:96)(x) satisfying deﬁnition 2. The simplest way to understand this algorithms is as a nonlinear generalization of the Andersen-Chung-Lang push procedure for PageRank [5], which we call ACL. (The ACL procedure has strong relationships with Gauss-Seidel, coordinate solvers, and various other standard algorithms.) The overall algorithm is simple: ﬁnd a vertex i where the KKT conditions from Theorem 2.1 are violated and increase xi on that node until we approximately satisfy the KKT conditions. Update the residual, look for another violation, and repeat. The ACL algorithm targets q = 2 case, which has a closed form update. We simply need to replace this with a binary search.
For ρ < 1, we only approximately satisfy the KKT conditions, as discussed further in the supplement.
We have the following strongly local runtime guarantee when 3(a) or 3(b) in deﬁnition 2 is satisﬁed. (This ignores binary search, but that only scales the runtime by log(1/ε) because the values are in
[0, 1].) 5
Algorithm nonlin-cut(γ, κ, ρ, ε) for set S and graph G where 0<ρ<1 and 0<ε determine accuracy 1: Let x(i) = 0 except for xs = 1 and set r = − 1 2: While there is any vertex i where ri > κdi, or stop if none exists (ﬁnd a KKT violation) 3: 4: Return x
Apply nonlin-push at vertex i, updating x and r
γ BT diag[(cid:96)(cid:48)(Bx)]w
Algorithm nonlin-push(i, γ, κ, x, r, ρ, ε) 1: Use binary search to ﬁnd ∆xi such that the ith coordinate of the residual after adding ∆xi to xi, r(cid:48) i = ρκdi, the binary search stops when the range of ∆x is smaller than ε (satisfy KKT at i). 2: Change the following entries in x and r to update the solution and residual 3: (a) xi ← xi + ∆xi 4: (b) For each neighbor j in the original graph G, rj ← rj+1
γ wi,j(cid:96)(cid:48)(xj−xi)−1
γ wi,j(cid:96)(cid:48)(xj−xi−∆xi)
Theorem 3.1. Let γ > 0, κ > 0 be ﬁxed and let k and c be the parameters from 3(a) of Deﬁnition 2 for (cid:96)(x). For 0 < ρ < 1, suppose nonlin-cut stops after K iterations, and di is the degree of node updated at the i-th iteration, then K must satisfy: (cid:80)K i=1 di ≤ vol(S)/c(cid:96)(cid:48)−1 (γ(1 − ρ)κ/k(1 + γ)) =
O(vol(S)). (cid:48)−1 refers to the inverse functions of (cid:96)(cid:48)(x), This function must be invertible under the
The notation (cid:96) the deﬁnition of 3(a). The runtime bound when 3(b) holds is slightly different, see below. Note that this sum of degrees bounds the total work because a push step at node i is O(di) work (ignoring the binary search).
The key to prove this runtime bound is that after each nonlin-push procedure, the sum of residuals will decrease by a value that is independent of the size of the entire graph. And the initial sum of residuals is vol(S). Also note that if κ = 0, γ = 0, or ρ = 1, then this bound goes to ∞ and we lose our guarantee. However, if these are not the case, then the bound shows that the algorithm will terminate in time that is independent of the size of the graph. This is the type of guarantee provided by strongly local graph algorithms and has been extremely useful to scalable network analysis methods [37, 26, 60, 54, 30]. We also show that a similar runtime guarantee holds when (cid:96)(x) satisﬁes 3(b) of Deﬁnition 2.
Theorem 3.2. Let γ > 0, κ > 0 be ﬁxed and let k and c be the parameters from 3(b) of Deﬁnition 2 for (cid:96)(x). For 0 < ρ < 1, suppose nonlin-cut stops after T iterations, and di is the degree of node updated at the i-th iteration, then T must satisfy: (cid:80)T i=1 di ≤ vol(S)/k(cid:96)(cid:48) (γ(1 − ρ)κ/c(1 + γ)) =
O(vol(S)). 4 Main Theoretical Results – Cut Quality Analysis
A common use for the results of these localized cut solutions is as localized Fiedler vectors of a graph to induce a cluster [5, 37, 42, 63, 46]. This was the original motivation of the ACL procedure [5], for which the goal was a small conductance cluster. One of the most common (and theoretically justiﬁed!) ways to convert a real-valued “clustering hint” vector x into clusters is to use a sweep cut process. This involves sorting x in decreasing order and evaluating the conductance of each preﬁx set Sj = {x1, x2, ..., xj} for each j ∈ [n]. The set with the smallest conductance will be returned. This computation is a key piece of Cheeger inequalities [13, 43]. In the following, we seek a slightly different type of guarantee. We posit the existence of a target cluster T and show that if T has useful clustering properties (small conductance, no good internal clusters), then a sweep cut over a q-norm or q-Huber localized cut vector seeded inside of T will accurately recover T . The key piece is understanding how the computation plays out with respect to T inside the graph and T as a graph by itself. We use volT (S), φT (S) to be the volume or conductance of set S in the subgraph induced by T and ∂T ⊂ T to be the boundary set of T , i.e. nodes in ∂T has at least one edge connecting to
¯T . Quantities with tildes, e.g., ˜d, reﬂect quantities in the subgraph induced by T . We assume κ = 0,
ρ = 1 and:
Assumption 1. The seed set S satisﬁes S ⊆ T , S ∩ ∂T = ∅ and (cid:80) 2φ(T )vol(S). i∈∂T (di − ˜di)xq−1
≤ i 6
We call this the leaking assumption, which roughly states that the solution with the set S stays mostly within the set T . As some quick justiﬁcation for this assumption, we note that when when q = 2, [63] shows by a Markov bound that there exists Tg where vol(Tg) ≥ 1 2 vol(T ) such that any node i ∈ Tg satisﬁes (cid:80) i∈∂T (di − ˜di)xi ≤ 2φ(T )di. So in that case, any seed sets S ⊆ Tg meets our assumption.
For 1 < q < 2, it is straightforward to see any set S with vol(S) ≥ 1 2 vol(T ) satisﬁes this assumption since the left hand side is always smaller than cut(T ). However, such a strong assumption is not necessary for our approach. The above guarantee allows for a small vol(S) and we simply require
Assumption 1 holds. We currently lack a detailed analysis of how many such seed sets there will be.
Assumption 2. A relatively small γ should be chosen such that the solution of localized q-norm cut problem in the subgraph induced by target cluster T can satisfy min(˜xT ) ≥ (0.5volT (S))1/(q−1) (volT (T ))1/(q−1) = M .
Deﬁne γ2 to be the largest γ such that assumption 2 is satisﬁed at q = 2 and assume γ2 < 1. Then
[63] shows that γ2 = Θ(φ(T ) · Gap). Here Gap is deﬁned as the ratio of internal connectivity and external connectivity and often assumed to be Ω(1). Formally:
Deﬁnition 4. Given a target cluster T with vol(T ) ≤ 1 the Gap is deﬁned as: 2 vol(V ), φ(T ) ≤ Ψ and minA⊂T φT (A) ≥ Φ,
Gap =
Φ2/log vol(T )
Ψ
We refer to [63] for a detailed explanation of this. In the case of q = 2, by using the inﬁnity-norm mixing time of a Markov chain, any γ ≤ O(φ(T ) · Gap) satisﬁes this assumption as shown in lemma 3.2 of [63]. For 1 < q < 2, it will be more difﬁcult to derive a closed form solution on how small γ needs to be. However, in the supplement, we can show that this assumption still holds for subgraphs with small diameters, i.e. O(log(|T |)) (This is reasonable because we expect good clusters and good communities to have small diameters.). Combining these results gives us the following theorem.
Theorem 4.1. Assume the subgraph induced by target cluster T has diameter O(log(|T |)), when we uniformly randomly sample points from T as seed sets, the expected largest distance of any volT (T ) ≤ 2(cid:0)( γ2 node in ¯S to S is O where l ≤ (1 + γ)max( ˜di), then we can set γ = γq−1 to satisfy assumption 2 for 1 < q < 2. Then a sweep cut over x will ﬁnd a cluster R where φ(R) = O(cid:0)φ(T )
|S| log(1+l1/(q−1))(cid:1)q−1
. Assume volT (S) (cid:16) log(|T |)
|S| q /Gap 2 (cid:1).
)/|T | 1+γ2 q−1 (cid:17) 2 1 1
Our proof is largely a generalization of Lemma 4.1 in [63]. We will deﬁne a Lovasz-Simonovits curve over dixq−1
. Since we choose γ = (γ2)q−1 and
, then we can show φ(R) = O
γ2 = Θ(φ(T ) · Gap), we have φ(R) = O(cid:0) φ(T )(3−q)/2
Gap(q−1)/2 (cid:1) ≤ O(cid:0) φ(T )1/q
Gap(q−1)/2 (cid:16) φ(T )
√
γ (cid:1). (cid:17) i 5 Experiments
We perform three experiments that are designed to compare our method to others designed for similar problems. We call ours SLQ (strongly local q-norm) for (cid:96)(x) = (1/q)|x|q with parameters γ for localization and κ for the sparsity. We call it SLQδ with the q-Huber loss. Full details are in the supplemental material. Existing solvers are (i) ACL [5], that computes a personalized PageRank vector approximately adapted with the same parameters [19]; (ii) CRD [57], which is hybrid of ﬂow and spectral ideas; (iii) FS is FlowSeed [55], a 1-norm based method; (iv) HK is the push-based heat kernel [30]; (v) NLD is a recent nonlinear diffusion [25]; (vi) GCN is a graph convolutional network [28]. Parameters are chosen based on defaults or with slight variations designed to enhance the performance within a reasonable running time. All experiments in this section are performed on a server with Intel Xeon Platinum 8168 CPU and 5.9T RAM. (Nothing remotely used the full capacity of the system and these were run concurrently with other processes.) We provide a full
Julia implementation of SLQ in the supplement. We evaluate the routines in terms of their recovery performance for planted sets and clusters. The bands reﬂect randomizing seeds choices in the target cluster.
The ﬁrst experiment uses the LFR benchmark [34]. We vary the mixing parameter µ (where larger µ is more difﬁcult) and provide 1% of a cluster as a seed, then we check how much of the cluster we recover after a conductance-based sweep cut over the solutions from various methods. Here, we use 7
Figure 3: The left ﬁgure shows the median running time for the methods as we scale the graph size keeping the cluster sizes roughly the same. As we vary cluster mixing µ for a graph with 10, 000 nodes, the middle ﬁgure shows the median F1 score (higher is better) along with the 20-80% quantiles; the right ﬁgure shows the conductance values (lower is better). These results show SLQ is better than
ACL and competitive with CRD while running much faster.
Table 1: Cluster recovery results from a set of 7 Facebook networks [53]. Students with a speciﬁc graduation class year are used as target cluster. We use a random set of 1% of the nodes identiﬁed with that class year as seeds. The class year 2009 is the set of incoming students, which form better conductance groups because the students had not yet mixed with the other classes. Class year 2008 is already mixed and so the methods do not do as well there. The values are median F 1 and the violin plots show the distribution over choices of the seeds.
Year Alg
UCLA
F1 & Med. F1 & Med. F1 & Med. F1 & Med. F1 & Med. F1 & Med. F1 & Med.
Stanford
Cornell
UPenn
Duke
MIT
Yale 0.9 0.9 0.9 0.9 0.9 0.3 0.4 0.2 0.3 0.7 0.6 0.6 0.5 0.5 0.5 0.5 0.3 0.3 0.9 0.8 0.7 0.9 0.8 0.4 0.5 0.2 0.2 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.3 0.3 2009 SLQ
SLQδ
CRD-3
CRD-5
ACL
FS
HK
NLD
GCN 2008 SLQ
SLQδ
CRD-3
CRD-5
ACL
FS
HK
NLD
GCN
Method 1.0 1.0 0.7 1.0 0.9 0.9 0.9 1.0 0.9 0.6 1.0 0.9 0.9 0.9 1.0 0.9 0.7 1.0 0.9 0.5 0.9 0.9 0.9 0.5 0.9 0.9 0.5 0.9 0.3 0.3 0.3 0.3 0.3 0.2 0.3 0.3 0.8 0.7 0.7 0.5 0.7 0.7 0.0 0.3 0.3 0.8 0.7 0.7 0.5 0.7 0.6 0.5 0.3 0.3 0.8 0.7 0.7 0.7 0.7 0.7 0.5 0.3 0.3 0.8 0.7 0.6 0.6 0.7 0.6 0.5 0.3 0.3 0.2 0.3 0.9 0.9 0.5 0.9 0.9 0.9 0.4 0.3 0.2 0.8 0.7 0.6 0.5 0.7 0.7 0.5
Table 2: Total running time of methods in this experiment.
HK
CRD-3 CRD-5 ACL
SLQδ
FS
SLQ
NLD
GCN
Time (seconds) 123 80 3049 9378 12 1593 106 10375 16534 the F 1 score (harmonic mean of precision and recall) and conductance value (cut to volume ratio) of the sets to evaluate the methods. The results are in Figure 3.
The second experiment uses the class-year metadata on Facebook [53], which is known to have good conductance structure for at least class year 2009 [56] that should be identiﬁable with many methods.
Other class years are harder to detect with conductance. Here, we use F 1 values alone. We use 1% of the true set as seed. (For GCN, we also use the same number of negative nodes.) In the supplementary material, we show what happens when varying the number of seeds. The results are in Table 1,2 and show SLQ is as good, or better than, CRD and much faster.
The ﬁnal experiment evaluates a ﬁnding from [31] on the recall of seed-based community detection methods. For a group of communities with roughly the same size, we evaluate the recall of the largest k entries in a diffusion vector. Minimizing conductance is not an objective in this experiment. 8
They found PageRank (ACL) outperformed many different methods. Also, ACL – with the standard degree normalization for conductance based sweepcuts performed worse than ACL without degree normalization in this particular setting, which is different from what conductance theory suggests.
Here, with the ﬂexibility of q, we see the same general result with respect to degree normalization and found that SLQ with q > 2 gives the best performance even though the conductance theory suggests 1 < q < 2 for the best conductance bounds. (a) DBLP (b) LiveJournal
Figure 4: A replication of an experiment from [31] with SLQ on DBLP [6, 59] (with 1M edges) and edges LiveJournal [44] (with 65M edges). The plot shows median recall over 600 groups of roughly the same size as we look at the top k entries in the solution vector (x axis). The envelope represents 2 standard error. This shows SLQ with q > 2 gives better performance than ACL (PageRank), and all improve on the degree-normalized (DN) versions used for conductance-minimizing sweep cuts. 6