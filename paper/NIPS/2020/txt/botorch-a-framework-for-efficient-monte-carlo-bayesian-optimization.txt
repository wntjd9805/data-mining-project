Abstract
Bayesian optimization provides sample-efﬁcient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce BOTORCH, a modern programming frame-work for Bayesian optimization that combines Monte-Carlo (MC) acquisition functions, a novel sample average approximation optimization approach, auto-differentiation, and variance reduction techniques. BOTORCH’s modular design facilitates ﬂexible speciﬁcation and optimization of probabilistic models written in PyTorch, simplifying implementation of new acquisition functions. Our ap-proach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel
“one-shot” formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efﬁciency of BOTORCH relative to other popular libraries. 1

Introduction
Computational modeling and machine learning (ML) have led to an acceleration of scientiﬁc in-novation in diverse areas, ranging from drug design to robotics to material science. These tasks often involve solving time- and resource-intensive global optimization problems to achieve optimal performance. Bayesian optimization (BO) [75, 46, 76], an established methodology for sample-efﬁcient sequential optimization, has been proposed as an effective solution to such problems, and has been applied successfully to tasks ranging from hyperparameter optimization [24, 92, 110], robotic control [15, 5], chemical design [36, 60, 111], and tuning and policy search for internet-scale software systems [4, 58, 57, 23]. Meanwhile, ML research has been undergoing a revolution driven largely by new programming frameworks and hardware that reduce the time from ideation to execution
[43, 16, 1, 81]. While BO has become rich with new methodologies, today there is no coherent framework that leverages these computational advances to simplify and accelerate BO research in the same way that modern frameworks have for deep learning. In this paper, we address this gap by introducing BOTORCH, a modular and scalable Monte Carlo (MC) framework for BO that is built around modern paradigms of computation, and theoretically grounded in novel convergence results.
Our contributions include:
• A novel approach to optimizing MC acquisition functions that effectively combines with determin-istic higher-order optimization algorithms and variance reduction techniques. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
• The ﬁrst convergence results for sample average approximation (SAA) of MC acquisition functions, including novel general convergence results for SAA via randomized quasi-MC.
• A new, SAA-based “one-shot” formulation of the Knowledge Gradient, a look-ahead acquisition function, with improved performance over the state-of-the-art.
• Composable model-agnostic abstractions for MC BO that leverage modern computational tech-nologies, including auto-differentiation and scalable parallel computation on CPUs and GPUs.
We discuss related work in Section 2 and then present the methodology underlying BOTORCH in
Sections 3 and 4. Details of the BOTORCH framework, including its modular abstractions and implementation examples, are given in Section 5. Numerical results are provided in Section 6. 2