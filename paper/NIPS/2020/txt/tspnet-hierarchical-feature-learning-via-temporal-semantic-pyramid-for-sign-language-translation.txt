Abstract
Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of sign videos to learn more discriminative features. To this end, we ﬁrst present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called
TSPNet. Speciﬁcally, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with signiﬁcant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96) on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet. 1

Introduction
Sign language translation (SLT), as an essential sign language interpretation task, aims to provide text-based natural language translation for continuously signing videos. Since sign languages are distinct linguistic systems [1] which differ from natural languages, signed sentence and their translation into natural languages do not syntactically align. For instance, sign languages have different word ordering rules from their natural language counterparts. Because of such discrepancies between a sign language and its natural language translation, SLT methods are often required to jointly learn embedding space of sign sentence videos and natural languages as well as mappings between them, leading to a difﬁcult sequential learning problem.
Existing SLT approaches can be categorized into two-staged and bootstrapping approaches depending on whether they require additional annotations for video and text alignments or not. Two-staged models require extra annotations, namely gloss, to describe sign videos with word labels in their occurring order. These models ﬁrst learn to recognize gestures using gloss annotations and then rearrange the recognition results into natural language sentences. Gloss annotations signiﬁcantly
∗Authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
ease the syntactic alignment in these approaches. However, gloss annotations are not easy to acquire since they require expertise in sign languages [2]. In contrast, bootstrapping models directly learn to translate from video inputs to natural language sentences without gloss annotations. These models extend easily to a wider range of sign language resources, and have recently attracted great research interests. This paper also investigates bootstrapping methods and aims to minimize the translation accuracy gap between these two approaches by learning more expressive sign features.
Sign gestures are the minimal units that preserve semantics in sign language videos. However, because of motion blurs, ﬁne-grained gestural details, and the transitions between different sign gestures, inferring boundaries between sign gestures is difﬁcult. Thus, current approaches [2, 3] extract sign features in a frame-wise fashion. By doing so, only spatial appearance features are captured while neglecting the temporal dependencies between sign gestures. However, temporal information is helpful in distinguishing different signs when similar body poses appear, and therefore we expect that this information to be useful in SLT models.
In this paper, we propose a Temporal Semantic Pyramid Network (TSPNet) to learn features from video segments instead of single frames. Particularly, we aim to learn sign video representations that encode both spatial appearance and temporal dynamics. However, obtaining accurate gesture segments from a continuous sign video is difﬁcult, while noisy segments bring substantial ambiguity for feature learning. Here, we observe two important factors impacting the semantics of sign segments.
First, sign video semantics are coherent, implying that segments that are temporally close share consistent semantics locally. Second, the semantics of sign gestures are context-dependent. Namely, non-local information is helpful to disambiguate the semantics of local gestures. Motivated by these, we divide each video into segments of different granularities. Our proposed TSPNet then exploits the semantic consistency among them to further enhance sign representations. To be speciﬁc, after organizing multiple video segments of different granularities in a hierarchy, our TSPNet enforces local semantic consistency by aggregating features of segments in each semantic neighborhood using an inter-scale attention. When tackling local ambiguity caused by imprecise segmentation, we develop an intra-scale attention to re-weight the local gesture features along the whole video sequence. By learning features from sign segments in a hierarchical approach, our TSPNet captures temporal information in sign gestures thus producing more discriminative sign video features. As a result of stronger feature semantics, we ease the difﬁculty in constructing mappings between sign videos and natural language sentences, thus improving the translation results.
Our model signiﬁcantly improves the translation quality on the largest public sign language translation dataset RWTH-PHOENIX-WEATHER-2014T, increasing the BLEU score from 9.58 [2] to 13.41 and
ROUGE score from 31.80 [2] to 34.96, greatly relaxing the constraint on expensive gloss annotations for sign language translation models. 2