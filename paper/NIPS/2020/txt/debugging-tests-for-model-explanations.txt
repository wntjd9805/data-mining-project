Abstract
We investigate whether post-hoc model explanations are effective for diagnos-ing model errors–model debugging. In response to the challenge of explaining a model’s prediction, a vast array of explanation methods have been proposed.
Despite increasing use, it is unclear if they are effective. To start, we catego-rize bugs, based on their source, into: data, model, and test-time contamination bugs. For several explanation methods, we assess their ability to: detect spurious correlation artifacts (data contamination), diagnose mislabeled training examples (data contamination), differentiate between a (partially) re-initialized model and a trained one (model contamination), and detect out-of-distribution inputs (test-time contamination). We ﬁnd that the methods tested are able to diagnose a spurious background bug, but not conclusively identify mislabeled training examples. In ad-dition, a class of methods, that modify the back-propagation algorithm are invariant to the higher layer parameters of a deep network; hence, ineffective for diagnosing model contamination. We complement our analysis with a human subject study, and ﬁnd that subjects fail to identify defective models using attributions, but instead rely, primarily, on model predictions. Taken together, our results provide guid-ance for practitioners and researchers turning to explanations as tools for model debugging.1 1

Introduction
Diagnosing and ﬁxing model errors–model debugging–remains a longstanding machine learning challenge [12, 14–17, 55, 73]. Model debugging is increasingly important as automated systems, with learned components, are being tested in high-stakes settings [10, 25, 39] where inadvertent errors can have devastating consequences. Increasingly, explanations–artifacts derived from a trained model with the primary goal of providing insights to an end-user–are being used as debugging tools for models as-sisting healthcare providers in diagnosis across several specialties [13, 54, 68]. Despite a vast array of explanation methods and increased use for debugging, little guidance exists on method effectiveness.
For example, should an explanation work equally well for diagnosing mislabeled training samples and detecting spurious correlation artifacts? Should an explanation that is sensitive to model parameters also be effective for detecting domain shift? Consequently, we ask and address the following question: which explanation methods are effective for which classes of model bugs?
To address this question, we make the following contributions: 1. Bug Categorization. We categorize bugs, based on the source of the defect leading to the bug, in the supervised learning pipeline (see Figure 1) into three classes: data, model, and test-time contamination. These contamination classes capture defects in the training data, model speciﬁcation and parameters, and with the input at test-time. 1We encourage readers to consult the more complete manuscript on the arXiv. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Debugging framework for the standard supervised learning pipeline. Schematic of the standard supervised learning pipeline along with examples of bugs that can occur at each stage of the pipeline. The categorization captures defects that can occur with the training data, model, and at test-time. We term these: data, model, and test-time contamination tests. 2. Empirical Assessment. We conduct comprehensive control experiments to assess several feature attribution methods against 4 bugs: ‘spurious correlation artifact’, mislabelled training examples, re-initialized weights, and out-of-distribution (OOD) shift. 3. Insights. We ﬁnd that the feature attribution methods tested can identify a spurious background bug but not conclusively distinguish between normal and mislabeled training examples. In addi-tion, attribution methods that derive relevance by modifying the back-propagation computation via
‘positive aggregation’ (see Section 4) are invariant to the higher layer parameters of a deep neural network (DNN) model. Finally, we ﬁnd that in speciﬁc settings, attributions for out-of-distribution examples are visually similar to attributions of these examples but with an ‘in-domain’ model, suggesting that debugging solely based on visual inspection might be misleading. 4. Human Subject Study. We conduct a 54-person IRB-approved study to assess whether end-users can identify defective models with attributions. We ﬁnd that users rely, primarily, on the model predictions to ascertain that a model is defective, even in the presence of attributions.