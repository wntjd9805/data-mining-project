Abstract
Neural network architecture design mostly focuses on the new convolutional op-erator or special topological structure of network block, little attention is drawn to the conﬁguration of stacking each block, called Block Stacking Style (BSS).
Recent studies show that BSS may also have an unneglectable impact on networks, thus we design an efﬁcient algorithm to search it automatically. The proposed method, AutoBSS, is a novel AutoML algorithm based on Bayesian optimization by iteratively reﬁning and clustering Block Stacking Style Coding (BSSC), which can ﬁnd optimal BSS in a few trials without biased evaluation. On ImageNet classiﬁcation task, ResNet50/MobileNetV2/EfﬁcientNet-B0 with our searched
BSS achieve 79.29%/74.5%/77.79%, which outperform the original baselines by a large margin. More importantly, experimental results on model compression, object detection and instance segmentation show the strong generalizability of the proposed AutoBSS, and further verify the unneglectable impact of BSS on neural networks. 1

Introduction
Recent progress in computer vision is mostly driven by the advance of Convolutional Neural Networks (CNNs). With the evolution of network architectures from AlexNet [1], VGG [2], Inception [3] to
ResNet [4], the performance has been steadily improved. Early works [1, 2, 5] designed layer-based architectures, while most of the modern architectures [3, 4, 6, 7, 8, 9] are block-based. For those block-based networks, the design procedure consists of two steps: (1) designing the block structure. (2) stacking the blocks to construct a complete network architecture. The manner for stacking blocks is named as Block Stacking Style (BSS) inspired by BCS from [10]. Compared with the block structure, BSS draws little attention from the community.
The modern block-based networks are commonly constructed by stacking blocks sequentially. The backbone can be divided into several stages, thus BSS can be simply described by the number of blocks in each stage and the number of channels for each block. The general rule to set channels for each block is to double the channels when downsampling the feature maps. This rule is adopted by a lot of famous networks, such as VGG [2], ResNet [4] and ShufﬂeNet [11, 8]. As for the number of blocks in each stage, there is merely a rough rule that more blocks should be allocated in the middle stages [4, 7, 8]. Such human design paradigm arouses our questions: Is this the best BSS conﬁguration for all networks? However, recent works show that BSS may have an unneglectable impact on the performance of a network [12, 10]. [12] ﬁnd a kind of pyramidal BSS style which is better than the original ResNet. Even further, [10] tries to use reinforcement learning to ﬁnd optimal 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: The overall framework of our proposed AutoBSS.
Block Connection Style (similar to BSS) for searched network block. These studies imply that the design of BSS has not been fully understood.
In this paper, we aim to break the BSS designing principles deﬁned by human, and propose an efﬁcient
AutoML based method called AutoBSS. The overall framework is shown in Figure 1, where each
BSS conﬁguration is represented by Block Stacking Style Coding (BSSC). Our goal is to search an optimal BSSC with the best accuracy under some target constraints (e.g. FLOPs or latency). Current
AutoML algorithms usually use a biased evaluation protocol to accelerate search [13, 14, 15, 16, 17], such as early stop or or parameter sharing. However, BSS search space has its unique beneﬁts, where
BSSC has a strong physical meaning. Each BSSC affects the computation allocation of a network, thus we have an intuition that similar BSSC may have similar accuracy. Based on this intuition, we propose a Bayesian Optimization (BO) based approach. However, BO based approach does not perform well in a large discrete search space. Beneﬁt from the strong prior, we present several methods to improve the effectiveness and sample efﬁciency of BO on BSS search. BSS Clustering aggregates BSSC into clusters, each BSSC in the same cluster have similar accuracy, thus we only need to search over cluster centers. BSSC reﬁning enhances the coding representation by increasing the correlation between BSSC and corresponding accuracy. To improve BSS Clustering, we propose a candidate set construction method to select a subset from search space efﬁciently. Based on these improvements, AutoBSS is extremely sample efﬁcient and only needs to train tens of BSSC, thus we use an unbiased evaluation scheme and avoid the strong inﬂuence caused by widely used tricks in neural architecture search (NAS) methods, such as early stopping or parameter sharing.
Experiment results on various tasks demonstrate the superiority of our proposed method. The BSS searched within tens of samplings can largely boost the performance of well-known models. On
ImageNet classiﬁcation task, ResNet50/MobileNetV2/EfﬁcientNet-B0 with searched BSS achieve 79.29%/74.5%/77.79%, which outperform the original baselines by a large margin. Perhaps more surprisingly, results on model compression(+1.6%), object detection(+0.91%) and instance segmen-tation(+0.63%) show the strong generalizability of the proposed AutoBSS, and further verify the unneglectable impact of BSS on neural networks.
The contributions of this paper can be summarized as follows:
• We demonstrate that BSS has a strong impact on the performance of neural networks, and the BSS of current state-of-the-art networks is not the optimal solution.
• We propose a novel algorithm called AutoBSS that can ﬁnd a better BSS conﬁguration for a given network within only tens of trials. Due to the sample efﬁciency, AutoBSS can search with unbiased evaluation under limited computing cost, which overcomes the errors caused by the biased search scheme of current AutoML methods.
• The proposed AutoBSS improves the performance of widely used networks on classiﬁcation, model compression, object detection and instance segmentation tasks, which demonstrate the strong generalizability of the proposed method. 2
2