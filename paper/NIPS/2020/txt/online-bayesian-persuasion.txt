Abstract
In Bayesian persuasion, an informed sender has to design a signaling scheme that discloses the right amount of information so as to inﬂuence the behavior of a self-interested receiver. This kind of strategic interaction is ubiquitous in real-world economic scenarios. However, the seminal model by Kamenica and Gentzkow makes some stringent assumptions that limit its applicability in practice. One of the most limiting assumptions is, arguably, that the sender is required to know the receiver’s utility function to compute an optimal signaling scheme. We relax this assumption through an online learning framework in which the sender repeatedly faces a receiver whose type is unknown and chosen adversarially at each round from a ﬁnite set of possible types. We are interested in no-regret algorithms prescribing a signaling scheme at each round of the repeated interaction with performances close to that of a best-in-hindsight signaling scheme. First, we prove a hardness result on the per-round running time required to achieve no-α-regret for any α < 1. Then, we provide algorithms for the full and partial feedback models with regret bounds sublinear in the number of rounds and polynomial in the size of the instance. 1

Introduction
Bayesian persuasion was ﬁrst introduced by Kamenica and Gentzkow [23] as the problem faced by an informed sender trying to inﬂuence the behavior of a self-interested receiver via the strategic provision of payoff-relevant information. In Bayesian persuasion, the agents’ beliefs are inﬂuenced only by controlling ‘who gets to know what’. This ‘sweet talk’ is ubiquitous among all sorts of economic activities, and it was famously attributed to a quarter of the GDP in the United States by McCloskey and Klamer [28]. 2 The computational study of Bayesian persuasion has been largely driven by its application in domains such as auctions and online advertisement [7, 19, 11], voting [1, 14, 16], trafﬁc routing [9, 32], recommendation systems [26], security [30, 34], and product marketing [6, 13].
In the model by Kamenica and Gentzkow [23], the sender’s and receiver’s payoffs are determined by the receiver’s action and a set of parameters collectively termed the state of nature. Unlike the receiver, the sender observes the realized state of nature drawn from a shared prior distribution. The sender uses this private information to determine a signal for the receiver according to a publicly known signaling scheme, i.e., a mapping from states of nature to probability distributions over signals.
In this paper, we focus on arguably one of the most severe limitations of the basic model: the sender must know exactly the receiver’s utility function to compute an optimal signaling scheme.
∗The work was conducted while the author was a postdoc at Politecnico di Milano. 2A more recent estimate by Antioch and others [2] places this ﬁgure at 30%. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Our model and results We deal with uncertainty about the receiver’s type by framing the Bayesian persuasion problem in an online learning framework. We study a repeated Bayesian persuasion problem where, at each round, the receiver’s type is adversarially chosen from a ﬁnite set of types.
Our goal is the design of an online algorithm that recommends a signaling scheme at each round, guaranteeing an expected utility for the sender close to that of the best-in-hindsight signaling scheme.
We study this problem under two models of feedback: in the full information model, the sender selects a signaling scheme and later observes the type of the best-responding receiver; in the partial information model, the sender only observes the actions taken by the receiver.
First, in Section 4, we provide a negative result that rules out, even in the full information setting, the possibility of designing a no-regret algorithm with polynomial per-round running time. Furthermore, the same hardness result holds when adopting the notion of no-α-regret (in the additive sense) for any α < 1. Then, we focus on the problem of designing no-regret algorithms by relaxing the running time constraint. We show that it is possible to achieve a regret polynomial in the size of the problem instance and sublinear in the number of rounds T under both full (with O(T −1/2)) and partial feedback (with O(T −1/5)). We present these results in Sections 5 and 6, respectively.