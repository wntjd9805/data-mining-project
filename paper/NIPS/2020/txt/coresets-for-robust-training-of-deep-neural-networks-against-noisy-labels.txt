Abstract
Modern neural networks have the capacity to overﬁt noisy labels frequently found in real-world datasets. Although great progress has been made, existing techniques are limited in providing theoretical guarantees for the performance of the neural networks trained with noisy labels. Here we propose a novel approach with strong theoretical guarantees for robust training of deep networks trained with noisy labels.
The key idea behind our method is to select weighted subsets (coresets) of clean data points that provide an approximately low-rank Jacobian matrix. We then prove that gradient descent applied to the subsets do not overﬁt the noisy labels. Our extensive experiments corroborate our theory and demonstrate that deep networks trained on our subsets achieve a signiﬁcantly superior performance compared to state-of-the art, e.g., 6% increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in accuracy on mini Webvision1. 1

Introduction
The success of deep neural networks relies heavily on the quality of training data, and in particular accurate labels of the training examples. However, maintaining label quality becomes very expensive for large datasets, and hence mislabeled data points are ubiquitous in large real-world datasets [21].
As deep neural networks have the capacity to essentially memorize any (even random) labeling of the data [49], noisy labels have a drastic effect on the generalization performance of deep neural networks. Therefore, it becomes crucial to develop methods with strong theoretical guarantees for robust training of neural networks against noisy labels. Such guarantees become of the utmost importance in safety-critical systems, such as aircraft, autonomous cars, and medical devices.
There has been a great empirical progress in robust training of neural networks against noisy labels.
Existing directions mainly focus on: estimating the noise transition matrix [13, 34], designing robust loss functions [12, 42, 44, 52], correcting noisy labels [26, 36, 41], using explicit regularization techniques [7, 50, 51], and selecting or reweighting training examples [9, 14, 17, 27, 37, 44]. In general, estimating the noise transition matrix is challenging, correcting noisy labels is vulnerable to overﬁtting, and designing robust loss functions or using explicit regularization cannot achieve 1Code available at https://github.com/snap-stanford/crust. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
state-of-the-art performance [16, 23, 51]. Therefore, the most promising methods rely on selecting or reweighting training examples by knowledge distillation from auxiliary models [14, 17, 27], or exploiting an extra clean labelled dataset containing no noisy labels [17, 25, 37, 43, 50]. In practice, training reliable auxiliary models could be challenging, and relying on an extra dataset is restrictive as it requires the training and extra dataset to follow the same distribution. Nevertheless, the major limitation of the state-of-the-art methods is their inability to provide theoretical guarantees for the performance of neural networks trained with noisy labels.
There has been a few recent efforts to theoretically explain the effectiveness of regularization and early stopping in generalization of over-parameterized neural networks trained on noisy labels
[16, 23]. Speciﬁcally, Hu et al. [16] proved that when width of the hidden layers is sufﬁciently large (polynomial in the size of the training data), gradient descent with regularization by distance to initialization corresponds to kernel ridge regression using the Neural Tangent Kernel (NTK). Kernel ridge regression performs comparably to early-stopped gradient descent [35, 45], and leads to a generalization guarantee in presence of noisy labels. In another work, Li et al. [23] proved that under a rich (clusterable) dataset model, a one-hidden layer neural network trained with gradient descent ﬁrst ﬁts the correct labels, and then starts to overﬁt the noisy labels. This is consistent with the previous empirical ﬁndings showing that deep networks tend to learn simple examples ﬁrst, then gradually memorize harder instances [5]. In practice, however, regularization and early-stopping provide robustness only under relatively low levels of noise (up to 20% of noisy labels) [16, 23].
Here we develop a principled technique, CRUST, with strong theoretical guarantees for robust training of neural networks against noisy labels. The key idea of our method is to carefully select subsets of clean data points that allow the neural network to effectively learn from the training data, but prevent it to overﬁt noisy labels. To ﬁnd such subsets, we rely on recent results that characterize the training dynamics based on properties of neural network Jacobian matrix containing all its ﬁrst-order partial derivatives. In particular, (1) learning along prominent singular vectors of the Jacobian is fast and generalizes well, while learning along small singular vectors is slow and leads to overﬁtting; and (2) label noise falls on the space of small singular values and impedes generalization [32]. To effectively and robustly learn from the training data, CRUST efﬁciently ﬁnds subsets of clean and diverse data points for which the neural network has an approximately low-rank Jacobian matrix.
We show that the set of medoids of data points in the gradient space that minimizes the average gradient dissimilarity to all the other data points satisﬁes the above properties. To avoid overﬁtting noisy labels, CRUST iteratively extracts and trains on the set of updated medoids. We prove that for large enough coresets and a constant fraction of noisy labels, deep networks trained with gradient descent on the medoids found by CRUST do not overﬁt the noisy labels. We then explain how mixing up [51] the centers with a few other data points reduces the error of gradient descent updates on the coresets. Effectively, clean coresets found by CRUST improve the generalization performance by reducing the ratio of noisy labels and their alignment with the space of small singular values.
We conduct experiments on noisy versions of CIFAR-10 and CIFAR-100 [22] with noisy labels generated by random ﬂipping the original ones, and the mini Webvision datasets [24] which is a benchmark consisting of images crawled from websites, containing real-world noisy labels. Empirical results demonstrate that the robustness of deep models trained by CRUST is superior to state-of-the-art baselines, e.g. 6% increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in accuracy on mini Webvision. We note that CRUST achieves state-of-the-art performance without the need for training any auxiliary model or utilizing an extra clean dataset. 2 Additional