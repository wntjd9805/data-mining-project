Abstract
In this paper, we consider the problem of computing the barycenter of a set of probability distributions under the Sinkhorn divergence. This problem has recently found applications across various domains, including graphics, learning, and vision, as it provides a meaningful mechanism to aggregate knowledge. Unlike previous approaches which directly operate in the space of probability measures, we recast the Sinkhorn barycenter problem as an instance of unconstrained functional opti-mization and develop a novel functional gradient descent method named Sinkhorn
Descent (SD). We prove that SD converges to a stationary point at a sublinear rate, and under reasonable assumptions, we further show that it asymptotically ﬁnds a global minimizer of the Sinkhorn barycenter problem. Moreover, by providing a mean-ﬁeld analysis, we show that SD preserves the weak convergence of empiri-cal measures. Importantly, the computational complexity of SD scales linearly in the dimension d and we demonstrate its scalability by solving a 100-dimensional
Sinkhorn barycenter problem. 1

Introduction
Computing a nonlinear interpolation between a set of probability measures is a foundational task across many disciplines. This problem is typically referred as the barycenter problem and, as it provides a meaningful metric to aggregate knowledge, it has found numerous applications. Examples include distribution clustering [Ye et al., 2017], Bayesian inference [Srivastava et al., 2015], texture mixing [Rabin et al., 2011], and graphics [Solomon et al., 2015], etc. The barycenter problem can be naturally cast as minimization of the average distance between the target measure (barycenter) and the source measures; and the choice of the distance metric can signiﬁcantly impact the quality of the barycenter [Feydy et al., 2019]. In this regard, the Optimal Transport (OT) distance (a.k.a. the
Wasserstein distance) and its entropy regularized variant (a.k.a. the Sinkhorn divergence) are the most suitable geometrically-faithful metrics, while the latter is more computational friendly. In this paper, we provide efﬁcient and provable methods for the Sinkhorn barycenter problem.
The prior work in this domain has mainly focused on ﬁnding the barycenter by optimizing directly in the space of (discrete) probability measures. We can divide these previous methods into three broad classes depending on how the support of the barycenter is determined: (i) The ﬁrst class assumes a ﬁxed and prespeciﬁed support set for the barycenter and only optimizes the corresponding weights [Staib et al., 2017, Dvurechenskii et al., 2018, Kroshnin et al., 2019].
Accordingly, the problem reduces to minimizing a convex objective subject to a simplex constraint.
However, ﬁxing the support without any prior knowledge creates undesired bias and affects the quality of the ﬁnal solution. While increasing the support size (possibly exponentially in the dimension d) can help to mitigate the bias, it renders the procedure computationally prohibitive as d grows. (ii) To reduce the bias, the second class considers optimizing the support and the weights through an alternating procedure [Cuturi and Doucet, 2014, Claici et al., 2018]. Since the barycenter objective is 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
not jointly convex with respect to the support and the weights, these methods in general only converge to a stationary point, which can be far from the true minimizers. (iii) Unlike the aforementioned classes, Luise et al. [2019] recently proposed a conditional gradient method with a growing support set. This method enjoys sublinear convergence to the global optimum under the premise that a d-dimensional nonconvex subproblem can be globally minimized per-iteration. However, nonconvex optimization is generally intractable in high dimensional problems (large d) and only stationary points can be efﬁciently reached. Hence, the guarantee of [Luise et al., 2019] has limited applicability as the dimension grows.
In this paper, we provide a new perspective on the Sinkhorn barycenter problem: Instead of operating in the space of probability measures, we view the barycenter as the push-forward measure of a given initial measure under an unknown mapping. We thus recast the barycenter problem as an unconstrained functional optimization over the space of mappings. Equipped with this perspective, we make the following contributions:
• We develop a novel functional gradient descent method, called Sinkhorn Descent (SD), which operates by ﬁnding the push-forward mapping in a Reproducing Kernel Hilbert Space that allows the fastest descent, and consequently solves the Sinkhorn barycenter problem iteratively. We then deﬁne the Kernelized Sinkhorn Barycenter Discrepancy (KSBD) to characterize the non-asymptotic convergence of SD. In particular, we prove that KSBD vanishes under the SD iterates at the rate of O( 1 t ), where t is the iteration number.
• We prove that SD preserves the weak convergence of empirical measures. Concretely, use
SDt(·) to denote the output of SD after t iterations and let αN be an empirical measure of α with N samples. We have limN→∞ SDt(αN ) = SDt(α). Such asymptotic analysis allows us to jointly study the behavior of SD under either discrete or continuous initialization.
• Under a mild assumption, we prove that KSBD is a valid discrepancy to characterize the optimality of the solution, i.e. the vanishing of KSBD implies the output measure of SD converges to the global optimal solution set of the Sinkhorn barycenter problem.
Further, we show the efﬁciency and efﬁcacy of SD by comparing it with prior art on several problems.
We note that the computation complexity of SD depends linearly on the dimension d. We hence validate the scalability of SD by solving a 100-dimensional barycenter problem, which cannot be handled by previous methods due to their exponential dependence on the problem dimension.
Notations. Let X ⊆ Rd be a compact ground set, endowed with a symmetric ground metric c : X × X → R+. Without loss of generality, we assume c(x, y) = ∞ if x /∈ X or y /∈ X . We use
∇1c(·, ·) : X 2 → X to denote its gradient w.r.t. its ﬁrst argument. Let M+ 1 (X ) and C(X ) be the space of probability measures and continuous functions on X . We denote the support for a probability measure α ∈ M+ 1 (X ) by supp(α) and we use α − a.e. to denote "almost everywhere w.r.t. α".
For a vector a ∈ Rd, we denote its (cid:96)2 norm by (cid:107)a(cid:107). For a function f : X → R, we denote its L∞ norm by (cid:107)f (cid:107)∞:= maxx∈X |f (x)| and denote its gradient by ∇f (·) : X → Rd. For a vector function f : X → Rd, we denote its (2, ∞) norm by (cid:107)f (cid:107)2,∞:= maxx∈X (cid:107)f (x)(cid:107). For an integer n, denote
[n]:={1, · · · , n}.
Given an Reproducing Kernel Hilbert Space (RKHS) H with a kernel function k : X × X → R+, we say a vector function ψ = [[ψ]1, · · · , [ψ]d] ∈ Hd if each component [ψ]i is in H. The space
H has a natural inner product structure and an induced norm, and so does Hd, i.e. (cid:104)f, g(cid:105)Hd = (cid:80)d
Hd = (cid:104)f, f (cid:105)Hd . The reproducing property of the
RKHS H reads that given f ∈ Hd, one has [f ]i(x) = (cid:104)[f ]i, kx(cid:105)H with kx(y) = k(x, y), which by
Cauchy-Schwarz inequality implies that there exists some constant MH > 0 such that i=1(cid:104)[f ]i, [g]i(cid:105)H, ∀f, g ∈ Hd and the norm (cid:107)f (cid:107)2 (cid:107)f (cid:107)2,∞ ≤ MH(cid:107)f (cid:107)Hd , ∀f ∈ Hd. (1)
Additionally, for a functional F : Hd → R, the Fréchet derivative of F is deﬁned as follows.
Deﬁnition 1.1 (Fréchet derivative in RKHS). For a functional F : Hd → R, its Fréchet derivative
DF [ψ] at ψ ∈ Hd is a function in Hd satisfying the following: For any ξ ∈ Hd with (cid:107)ξ(cid:107)Hd < ∞, lim (cid:15)→0
F [ψ + (cid:15)ξ] − F [ψ] (cid:15)
= (cid:104)DF [ψ], ξ(cid:105)Hd .
Note that the Fréchet derivative at ψ, i.e. DF [ψ], is a bounded linear operator from Hd to R. It can be written in the form DF [ψ](ξ) = (cid:104)DF [ψ], ξ(cid:105)Hd due to the Riesz–Fréchet representation theorem. 2
1.1