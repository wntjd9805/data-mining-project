Abstract
The challenge of developing powerful and general Reinforcement Learning (RL) agents has received increasing attention in recent years. Much of this effort has focused on the single-agent setting, in which an agent maximizes a predeﬁned extrinsic reward function. However, a long-term question inevitably arises: how will such independent agents cooperate when they are continually learning and acting in a shared multi-agent environment? Observing that humans often provide incentives to inﬂuence others’ behavior, we propose to equip each RL agent in a multi-agent environment with the ability to give rewards directly to other agents, using a learned incentive function. Each agent learns its own incentive function by explicitly accounting for its impact on the learning of recipients and, through them, the impact on its own extrinsic objective. We demonstrate in experiments that such agents signiﬁcantly outperform standard RL and opponent-shaping agents in challenging general-sum Markov games, often by ﬁnding a near-optimal division of labor. Our work points toward more opportunities and challenges along the path to ensure the common good in a multi-agent future. 1

Introduction
Reinforcement Learning (RL) [37] agents are achieving increasing success on an expanding set of tasks [28, 20, 32, 41, 6]. While much effort is devoted to single-agent environments and fully-cooperative games, there is a possible future in which large numbers of RL agents with imperfectly-aligned objectives must interact and continually learn in a shared multi-agent environment. The option of centralized training with a global reward [13, 35, 31] is excluded as it does not scale easily to large populations and may not be adopted by self-interested parties. On the other hand, the paradigm of decentralized training—in which no agent is designed with an objective to maximize collective performance and each agent optimizes its own set of policy parameters—poses difﬁculties for agents to attain high individual and collective return [29]. In particular, agents in many real world situations with mixed motives, such as settings with nonexcludable and subtractive common-pool resources, may face a social dilemma wherein mutual selﬁsh behavior leads to low individual and total utility, due to fear of being exploited or greed to exploit others [30, 23, 24]. Whether, and how, independent learning and acting agents can cooperate while optimizing their own objectives is an open question.
The conundrum of attaining multi-agent cooperation with decentralized training of agents, who may have misaligned individual objectives, requires us to go beyond the restrictive mindset that the collection of predeﬁned individual rewards cannot be changed by the agents themselves. We draw inspiration from the observation that this fundamental multi-agent problem arises at multiple
∗Work done during internship at DeepMind
†On leave from College of Computing, Georgia Institute of Technology 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
lever
-1
-1
-1 or 10 start
-1
-1 or 10
-1 door
Figure 1: The N -player Escape Room game ER(N, M ). For M < N , if fewer than M agents pull the lever, which incurs a cost of −1, then all agents receive −1 for changing positions. Otherwise, the agent(s) who is not pulling the lever can get +10 at the door and end the episode. scales of human activity and, crucially, that it can be successfully resolved when agents give the right incentives to alter the objective of other agents, in such a way that the recipients’ behavior changes for everyone’s advantage. Indeed, a signiﬁcant amount of individual, group, and international effort is expended on creating effective incentives or sanctions to shape the behavior of other individuals, social groups, and nations [39, 8, 9]. The rich body of work on game-theoretic side payments
[19, 16, 14] further attests to the importance of inter-agent incentivization in society.
Translated to the framework of Markov games for multi-agent reinforcement learning (MARL) [26], the key insight is to remove the constraints of an immutable reward function. Instead, we allow agents to learn an incentive function that gives rewards to other learning agents and thereby shape their behavior. The new learning problem for an agent becomes two-fold: learn a policy that optimizes the total extrinsic rewards and incentives it receives, and learn an incentive function that alters other agents’ behavior so as to optimize its own extrinsic objective. While the emergence of incentives in nature may have an evolutionary explanation [15], human societies contain ubiquitous examples of learned incentivization and we focus on the learning viewpoint in this work.
The Escape Room game. We may illustrate the beneﬁts and necessity of incentivization with a simple example. The Escape Room game ER(N, M ) is a discrete N -player Markov game with individual extrinsic rewards and parameter M < N , as shown in Figure 1. An agent gets +10 extrinsic reward for exiting a door and ending the game, but the door can only be opened when M other agents cooperate to pull the lever. However, an extrinsic penalty of −1 for any movement discourages all agents from taking the cooperative action. If agents optimize their own rewards with standard independent RL, no agent can attain positive reward, as we show in Section 5.
This game may be solved by equipping agents with the ability to incentivize other agents to pull the lever. However, we hypothesize—and conﬁrm in experiments—that merely augmenting an agent’s action space with a “give-reward” action and applying standard RL faces signiﬁcant learning difﬁculties. Consider the case of ER(2, 1): suppose we allow agent A1 an additional action that sends
+2 reward to agent A2, and let it observe A2’s chosen action prior to taking its own action. Assuming that A2 conducts sufﬁcient exploration, an intelligent reward-giver should learn to use the give-reward action to incentivize A2 to pull the lever. However, RL optimizes the expected cumulative reward within one episode, but the effect of a give-reward action manifests in the recipient’s behavior only after many learning updates that generally span multiple episodes. Hence, a reward-giver may not receive any feedback within an episode, much less an immediate feedback, on whether the give-reward action beneﬁted its own extrinsic objective. Instead, we need an agent that explicitly accounts for the impact of incentives on the recipient’s learning and, thereby, on its own future performance.
As a ﬁrst step toward addressing these new challenges, we make the following conceptual, algorithmic, and experimental contributions. (1) We create an agent that learns an incentive function to reward other learning agents, by explicitly accounting for the impact of incentives on its own performance, through the learning of recipients. (2) Working with agents who conduct policy optimization, we derive the gradient of an agent’s extrinsic objective with respect to the parameters of its incentive function. We propose an effective training procedure based on online cross-validation to update the incentive function and policy on the same time scale. (3) We show convergence to mutual cooperation in a matrix game, and experiment on a new deceptively simple Escape Room game, which poses signiﬁcant difﬁculties for standard RL and action-based opponent-shaping agents, but on which our agent consistently attains the global optimum. (4) Finally, our agents discover near-optimal division of labor in the challenging and high-dimensional social dilemma problem of Cleanup [18]. Taken together, we believe this is a promising step toward a cooperative multi-agent future. 2
2