Abstract
This article suggests that deterministic Gradient Descent, which does not use any stochastic gradient approximation, can still exhibit stochastic behaviors. In particu-lar, it shows that if the objective function exhibit multiscale behaviors, then in a large learning rate regime which only resolves the macroscopic but not the micro-scopic details of the objective, the deterministic GD dynamics can become chaotic and convergent not to a local minimizer but to a statistical distribution. In this sense, deterministic GD resembles stochastic GD even though no stochasticity is injected. A sufﬁcient condition is also established for approximating this long-time statistical limit by a rescaled Gibbs distribution, which for example allows escapes from local minima to be quantiﬁed. Both theoretical and numerical demonstrations are provided, and the theoretical part relies on the construction of a stochastic map that uses bounded noise (as opposed to Gaussian noise). 1

Introduction
Among ﬁrst-order optimization methods which are a central ingredient of machine learning, arguably the most used is gradient descent method (GD), or rather one of its variants, stochastic gradient descent method (SGD). Designed for objective functions that sum a large amount of terms, which for instance can originate from big data, SGD introduces a randomization mechanism of gradient subsampling to improve the scalability of GD (e.g., Zhang [2004], Moulines and Bach [2011], Roux et al. [2012]). Consequently, the iteration of SGD, unlike GD, is not deterministic even when it is started at a ﬁxed initial condition. In fact, if one ﬁxes the learning rate (LR) in SGD, the iteration does not converge to a local minimizer like in the case of GD; instead, it converges to a statistical distribution with variance controlled by the LR (e.g., Borkar and Mitter [1999], Mandt et al. [2017],
Li et al. [2017]). Diminishing LR was thus proposed to ensure that SGD remains as an optimization algorithm (e.g., Robbins and Monro [1951]). On the other hand, more recent perspectives include that the noise in SGD may actually facilitate escapes from bad local minima and improve generalization (see Sec.1.2 and references therein). In addition, non-diminishing LRs often correspond to faster computations, and therefore are of practical relevance1. Meanwhile, GD does not need the LR to be small in order to reduce the stochasticity, although in practices the LR is often chosen small enough to fully resolve the landscape of the objective, corresponding to a stability upper bound of 1/L under the common L-smooth assumption of the objective function.
We consider deterministic GD2 with ﬁxed large LR, based on the conventional belief that it optimizes more efﬁciently than small LR. The goal is to understand if large LR works, and if yes, in what sense. 1Optimizing LR is an important subarea but out of our scope; see e.g., Smith [2017] and references therein. 2Despite of the importance of SGD, there are still contexts in which deterministic GD is worth studying; e.g., for training with scarce data, for low-rank approximation (e.g., Tu et al. [2015]) and robust PCA (e.g., Yi et al.
[2016]), and for theoretical understandings of large neural networks (e.g, Du et al. [2018, 2019b]). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We will show that in a speciﬁc and yet not too restrictive setup, if LR becomes large enough (but not arbitrarily large), GD no longer converges to a local minimum but instead a statistical distribution.
This behavior bears signiﬁcant similarities to SGD, including (under reasonable assumptions):
• starting with an arbitrary initial condition, the empirical distribution of GD iterates (collected along discrete time) converges to a speciﬁc statistical distribution, which is not Dirac but almost a rescaled Gibbs distribution, just like SGD;
• starting an ensemble of arbitrary initial conditions and evolving each one according to GD, the ensemble, collected at the same number of iterations, again converges to the same almost
Gibbs distribution as the number of iteration increases, also like SGD.
Their difference, albeit obvious, should also be emphasized:
• GD is deterministic, and the same constant initial condition will always lead to the same iterates. No ﬁltration is involved, and unlike SGD the iteration is not a stochastic process.
In this sense, GD with large LR works in a statistical sense. One can obtain stochasticity without any algorithmic randomization! Whether this has implications on generalization is beyond the scope of this article, but large LR does provide a mechanism for escapes from local minima. We’ll see that microscopic local minima can always be escaped, and sometimes macroscopic local minima too. 1.1 Main Results
How is stochasticity generated out of determinism? Here it is due to chaotic dynamics. To further explain, consider an objective function f : Rd → R that admits a macro-micro decomposition f (x) := f0(x) + f1,(cid:15)(x) (1) where 0 < (cid:15) (cid:28) 1, f0, f1,(cid:15) ∈ C2(Rd), and the microscopic f1,(cid:15) satisﬁes the following conditions.
Condition 1. There exists a bounded nonconstant random variable (r.v.) ζ, with range in Rd and
Eζ = 0, such that: ∀(cid:15) > 0 and ∀x ∈ Rd, there exists a positive measured set Γx,(cid:15) ⊂ B(0, δ((cid:15))) with lim(cid:15)↓0 δ((cid:15)) = 0, such that the r.v. uniformly distributed on Γx,(cid:15), denoted by Yx,(cid:15), satisﬁes
∇f1,(cid:15)(x + Yx,(cid:15)) w−→ −ζ uniformly with respect to x as (cid:15) → 0. Assume without loss of generality that Eζ = 0 (nonzero mean can be absorbed into f0).
Notation: Throughout this paper ‘w’ means weak convergence: a sequence of random variables
{Xn}∞ n=1 has a random variable X as its weak limit, if and only if for any compactly supported test function g ∈ C∞(Rd), Eg(Xn) − Eg(X) → 0 as n → ∞.
Condition 2. (cid:15)∇2f1,(cid:15) is uniformly bounded as (cid:15) → 0, and ∃m ∈ R, s.t. for any bounded rectangle (cid:3) → m, where UΓ is a uniform r.v. on Γ.
Γ ⊂ Rd whose area |Γ| > 0, E (cid:2)ln (cid:107)(cid:15)∇2f1,(cid:15)(UΓ)(cid:107)2
Example 1 (periodic micro-scale). For intuition, consider a special case where f1,(cid:15) := (cid:15)f1 a periodic f1 ∈ C2(R). It is easy to check that both conditions are satisﬁed.
Example 2 (aperiodic micro-scale). Given a C2 function F (x1, x2, · · · , xN ) : Rd × Rd × · · · ×
Rd → R, that is periodic in each xi ∈ Rd, i.e., there exists constant vector T ∈ Rd such that
F (x1, · · · , xi, · · · , xN ) = F (x1, · · · , xi + T, · · · , xN ) for all x1, · · · , xN and i = 1, · · · , N . Then for any ω1, · · · , ωN ∈ R, f1,(cid:15)(x) := (cid:15)F ( ω1x (cid:15) , ω2x (cid:15) ) satisﬁes Cond.1 and 2. If the ω’s are nonresonant, meaning that the only solution to z1ω1 + z2ω2 + · · · + zN ωN = 0 for zi ∈ Z is z1 = z2 = · · · = zN = 0, then f1,(cid:15) is not periodic. An example is f1,(cid:15) = (cid:15)(g1(x/(cid:15)) + g2( 2x/(cid:15))) for any 1-periodic g1 and g2.
Remark 1. Cond.1 and 2 generalize and relax the periodic micro-scale requirement. Still required is, intuitively speaking, that every part of the small scale f1,(cid:15) appears similar in a weak sense.
In the special case of periodic micro-scale, it is easy to see f1,(cid:15) = O((cid:15)), ∇f1,(cid:15) = O(1) and
∇2f1,(cid:15) = O((cid:15)−1). However, after the relaxation of periodicity requirement, it may only be implied that ∇f1,(cid:15) = O(1) (Cond.1) and ∇f1,(cid:15) = O((cid:15)−1) (Cond.2). Later on, Cond.1 will help connect deterministic and stochastic maps, and Cond.2 will help estimate the Lyapunov exponent so that the onset of chaos can be quantiﬁed. (cid:15) , · · · , ωN x (cid:1) for (cid:0) x (cid:15)
√ 2
Fig.1 provides an example of f . This class of f models objective landscapes that assume certain macroscopic shapes (described by f0), but when zoomed-in exhibit additional small-in-x and f ﬂuctuations (produced by f1,(cid:15)). Taking the loss function of a neural network as an example, our intuition is that if the training data is drawn from a distribution, the distribution itself produces the dominant macroscopic part of the landscape (i.e., f0), and noises in the training data could lead to f1,(cid:15) which corresponds to small and localized perturbations to the loss (see Appendix C and also e.g., Mei et al.
[2018], Jin et al. [2018]).
Note although the length and height scales of f1,(cid:15) can be both much smaller than those of f0, ∇f0 and ∇f1,(cid:15) are nevertheless both O(1), creating nonconvexity and a large number of local minima even if f0 is (strongly) convex.
A multi-Figure 1: f (x) = scale function, (x2 − 1)2/4 + x/8 + (cid:15) (cid:0)sin (x/(cid:15)) + sin (cid:0)√ 2x/(cid:15)(cid:1)(cid:1), (cid:15) = 0.01.
What happens when gradient decent is applied to f (x), following repeated applications of the map
ϕ(x) := x − η∇f (x) = x − η∇f0(x) − η∇f1,(cid:15)(x)? (η will be called, interchangeably, learning rate (LR) or time step.)
When η (cid:28) (cid:15), GD converges to a local minimum (or a saddle, or in general a stationary point where
∇f = 0). This is due to the well known convergence of GD when η = o(1/L) for L-smooth f , and
L = O((cid:15)−1) for our multiscale f ’s (Rmk.1).
For η (cid:29) 1, or more precisely when it exceeds 1/L0 for L0-smooth f0, the iteration generally blows up and does not converge. However, there is a regime in-between corresponding to (cid:15) (cid:46) η (cid:28) 1, and this is what we call large LR, because here η is too large to resolve the micro-scale (i.e., f1,(cid:15), whose gradient has an O((cid:15)−1) Lipschitz constant).
Figure 2: What happens as learning rate increases?
Fig.2 previews what happens over the spectrum of η values. The difference between ‘local chaos’ and ‘global chaos’ will be detailed in Sec. 2.3.1 and B.3.2.
In fact, for the multiscale function f , one may prefer to ﬁnd a ‘macroscopic’ local minimum created by f0, instead of being trapped at one of the numerous local minima created by f1,(cid:15), which could just be artifacts due to imperfection of training data. A small LR will not be able to do so, but we’ll see below that large LR in some sense is better at this: it will lead GD to converge to a distribution peaked at f0’s minimizer(s), despite that the iteration is based on the ∇f (x) = ∇f0(x) + ∇f1,(cid:15)(x).
Our approach for demonstrating the ‘stochasticity’ of ϕ consists of three key ingredients: (i) construct another map ˆϕ, which is a truly stochastic counterpart of ϕ, so that they share the same invariant distribution; (ii) ﬁnd an approximation of the invariant distribution of ˆϕ, namely rescaled Gibbs; (iii) establish conditions for ϕ iterations to generate deterministic chaotic dynamics, which provides a route of convergence to a statistical distribution.
More speciﬁcally, we deﬁne the stochastic map ˆϕ as
ˆϕ : x (cid:55)→ x − η∇f0(x) + ηζ, where ζ is deﬁned in Cond.1. Then we have (note many of these results persist in numerical experiments under relaxed conditions; see Sec.3).
Theorem 1 (informal version of Thm.4). Fix η and let (cid:15) → 0. If ϕ has a family of nondegenerate3 invariant distributions for {(cid:15)i}∞ i=1 → 0, which converges in the weak sense, then the weak limit is an invariant distribution of ˆϕ. 3By ‘nondegenerate’, we require the distribution to be absolutely continuous w.r.t. Lebesgue measure.
Invariant distribution of ϕ always exists; an example is a Dirac distribution concentrated at any stationary point of f . See Rmk.3. 3
Theorem 2 (informal version of Lem.5, Thm.13 & Thm.7). Suppose f0 ∈ C2 is strongly convex and
L-smooth, and f1,(cid:15) ∈ C1 satisﬁes condition 1. Then for η ≤ C with some C > 0 independent of (cid:15), ˆϕ has an unique invariant distribution, and its iteration converges exponentially fast to this distribution.
Moreover, if the covariance matrix of ζ is isotropic, i.e., σ2Id, then the rescaled Gibbs distribution 1
Z exp
Theorem 3 (informal version of Thm.8). Suppose f0, f1,(cid:15) ∈ C1(R), f0 is L-smooth, grows unbound-edly at inﬁnity, and f1,(cid:15) satisﬁes Cond.1. If f0 has a stationary point, then ∃ηJ > 0 such that for any
ﬁxed 0 < η < ηJ , ∃(cid:15)0 > 0, s.t. when (cid:15) < (cid:15)0, the ϕ dynamics is chaotic. dx is an O(η2) approximation of it.
− 2f0(x)
ησ2 (cid:17) (cid:16)
In addition, we will show the onset of local chaos as η increases is via the common route of period doubling [Alligood et al., 1997]. We will also establish and estimate the positive Lyapunov exponent of ϕ in the large LR regime, which is strongly correlated with chaotic dynamics [Lyapunov, 1992].
The reason that we investigate chaos is the following. Although general theories are not uniﬁed yet, it is widely accepted that chaotic systems are often ergodic (on ergodic foliations), meaning the temporal average of an observable along any orbit (starting from the same foliation) converges, as the time horizon goes to inﬁnity, to the spatial average of that observable over an invariant distribution (e.g., Eckmann and Ruelle [1985], Young [1998], Ott [2002]). Moreover, many chaotic systems are also mixing (see e.g., Ott [2002]), which implies that if one starts with an ensemble of initial conditions and evolves each one of them by the deterministic map, then the whole ensemble converges to the (ergodic) invariant distribution.
Therefore, our last step in establishing stochasticity of GD is to show the deterministic ϕ map becomes chaotic for large η. This way, in most situations it is also ergodic and the assumption of Theorem 1 is satisﬁed, allowing us to demonstrate and quantify the stochastic behavior of deterministic GD.
Note that we also know that if f0 has multiple minima and associated potential wells, then GD can have stochastic behaviors with non-unique statistics (see Remark 12, 24 and Section D.5).
Therefore, mixing is not provable unless additional conditions are imposed, and this paper only presents numerical evidence (see section 3.1 and D.2). Meanwhile, note (i) since mixing implies ergodicity and Li-Yorke chaos [Akin and Kolyada, 2003, Iwanik, 1991], our necessary conditions are also necessary for mixing, and (ii) proving mixing of deterministic dynamics is difﬁcult, and only several examples have been well understood; see e.g., Sinai [1970], Ornstein and Weiss [1973].
Remark 2. For these reasons, we clarify that the theory in this paper does not quantify the speed of convergence of deterministic GD (ϕ) to its long time statistical limit. It is only shown that the stochastic map ˆϕ converges to its statistical limit exponentially fast for strongly-convex f0, and the deterministic map ϕ shares the same statistical limit with ˆϕ.
Relevance to machine learning practices: see Sec.3.3 (empirical) & C (theoretical) for examples. 1.2