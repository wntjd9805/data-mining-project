Abstract
We create a framework for bootstrapping visual representation learning from a primitive visual grouping capability. We operationalize grouping via a contour detector that partitions an image into regions, followed by merging of those regions into a tree hierarchy. A small supervised dataset sufﬁces for training this grouping primitive. Across a large unlabeled dataset, we apply this learned primitive to automatically predict hierarchical region structure. These predictions serve as guidance for self-supervised contrastive feature learning: we task a deep network with producing per-pixel embeddings whose pairwise distances respect the region hierarchy. Experiments demonstrate that our approach can serve as state-of-the-art generic pre-training, beneﬁting downstream tasks. We additionally explore applications to semantic region search and video-based object instance tracking. 1

Introduction
The ability to learn from large unlabeled datasets appears crucial to deploying machine learning techniques across many application domains for which annotating data is too costly or simply infeasible due to scale. For visual data, quantity and collection rate often far outpace ability to annotate, making self-supervised approaches particularly crucial to future advancement of the ﬁeld.
Recent efforts on self-supervised visual learning fall into several broad camps. Among them,
Kingma et al. [24] and Donahue et al. [12] design general architectures to learn latent feature representations, driven by modeling image distributions. Another group of approaches [16, 10, 28, 35] leverage, as supervision, pseudo-labels automatically generated from hand-designed proxy tasks.
Here, the general strategy is to split data examples into two parts and predict one from the other.
Alternatively, Wu et al. [45] and Zhuang et al. [52] learn visual features by asking a deep network to embed the entire training dataset, mapping each image to a location different from others, and relying on this constraint to drive the emergence of a topology that reﬂects semantics. Across many of these camps, technical improvements to the scale and efﬁciency of learning further boost results [19, 7, 36, 13]. Section 2 provides a more complete background.
We approach self-supervised learning using a strategy somewhat different from those approaches outlined above. While incorporating aspects of proxy tasks and embedding objectives, a key distinc-tion is that our system’s proxy task is itself generated by a (simpler) trained vision system. We thus seek to bootstrap visual representation learning in a manner loosely inspired by, though certainly not accurately mirroring, a progression of simple to complex biological vision systems. This is an under-explored, though not unrecognized, strategy in computer vision. Serving as a noteworthy example is Li et al.’s [30] approach of using motion as a readily available, automatically-derived, supervisory signal for learning to detect contours in static images. We focus on the next logical stage in such a bootstrapping sequence: using a pre-existing contour detector to automatically deﬁne a task objective for learning semantic visual representations. Figure 1 illustrates how this primitive 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Bootstrapping semantic representation learning via primitive hierarchical grouping.
Top: Self-Supervised Target Sampling. From a hierarchical segmentation of an image (i.e., a region tree T ), rendered here as a boundary strength map (OWT-UCM [2]), we deﬁne distance between regions dT (R1, R2) according to the level in the hierarchy at which they merge. Treating this distance as a similarity measure between pixels, we sample positive and negative pairs of pixels, according to their grouping likelihood in the hierarchy. Bottom: Contrastive Feature Learning. A contour detector [2], trained on a small dataset [34], produces hierarchical segmentations across a larger unlabeled image set. Automatically extracted positive and negative pixel pairs serve to drive a from-scratch initialized CNN to learn to predict pixel-wise embeddings which respect the region hierarchy. Unlike SegSort [21], our pipeline does not merely ﬁne-tune ImageNet [9] pre-trained models for semantic segmentation, but instead addresses representation learning entirely from scratch.
Figure 2: Visualization of feature embeddings. We apply PCA to the embeddings produced by a
CNN trained using the self-supervised bootstrapping approach of Figure 1. On validation examples, we display the ﬁrst three PCA components as an RGB image. The output feature representations capture aspects of semantic category (e.g., top left) and object instance identity (e.g., top right). visual system, combined with a modern contrastive feature learning framework, trains a convolutional neural network (CNN) to produce semantic embeddings (Figure 2). We defer full details to Section 3.
Our system not only leverages contours to learn visual semantics, but also leverages a small amount of annotated data to learn from a vastly larger pool of unlabeled data. Our visual primitive of contour detection is trained in a supervised manner from only 500 annotated images [34]. This primitive then drives self-supervised learning across datasets ranging from tens of thousands to millions of images; in this latter phase, our system does not utilize any annotations and trains from randomly initialized parameters. This is a crucial distinction from SegSort [21], whose pipeline bears coarse resemblance to our Figure 1. SegSort’s “unsupervised” learning setting still relies on starting from ImageNet 2
pre-trained CNNs; its “unsupervised” aspect is only with respect to forgoing use of segmentation ground-truth. In contrast, we address the problem of representation learning entirely from scratch, save for the 500 annotated images of the Berkeley Segmentation Dataset [34].
ImageNet, even without labels, is curated: most ImageNet images contain a single category. This provides some implicit supervision, which may bias the self-supervised work that experimentally targets learning from ImageNet, including MoCo [19], InstFeat [48] and others [45, 7]. Many use cases for self-supervision will lack such curation. As our bootstrapping strategy utilizes a visual primitive geared toward partitioning complex scenes into meaningful components, it is a better ﬁt to learning on unlabeled examples from datasets containing scenes (e.g., PASCAL [14], COCO [31]).
Using a similar siamese network, we outperform InstFeat [48] by a large margin on the task of learning transferable representations from PASCAL and COCO images alone (disregarding labels).
In this setting, our results are competitive with those of the state-of-the-art MoCo system [19], while our method remains simpler. We do not rely on a momentum encoder or memory bank. Even with this simpler training architecture, our segmentation-aware approach enhances the efﬁciency of learning from complex scenes. Here, our pre-training converges in under half the epochs needed by MoCo to learn representations with comparable transfer performance on downstream tasks.
In addition to evaluating learned representation quality on standard classiﬁcation tasks, Sections 4 and 5 explore applications to semantic region search and instance tracking in video. Using similarity in our learned feature space to conduct matching across images and frames, we outperform competing methods in both applications. Our results point to a promising new pathway of crafting self-supervised learning strategies around bootstrapping the training of one visual module from another. 2