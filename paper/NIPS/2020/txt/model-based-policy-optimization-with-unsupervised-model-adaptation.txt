Abstract
Model-based reinforcement learning methods learn a dynamics model with real data sampled from the environment and leverage it to generate simulated data to derive an agent. However, due to the potential distribution mismatch between simulated data and real data, this could lead to degraded performance. Despite much effort being devoted to reducing this distribution mismatch, existing methods fail to solve it explicitly. In this paper, we investigate how to bridge the gap between real and simulated data due to inaccurate model estimation for better policy optimization.
To begin with, we ﬁrst derive a lower bound of the expected return, which naturally inspires a bound maximization algorithm by aligning the simulated and real data distributions. To this end, we propose a novel model-based reinforcement learning framework AMPO, which introduces unsupervised model adaptation to minimize the integral probability metric (IPM) between feature distributions from real and simulated data. Instantiating our framework with Wasserstein-1 distance gives a practical model-based approach. Empirically, our approach achieves state-of-the-art performance in terms of sample efﬁciency on a range of continuous control benchmark tasks. 1

Introduction
In recent years, model-free reinforcement learning (MFRL) has achieved tremendous success on a wide range of simulated domains, e.g., video games [Mnih et al., 2015], complex robotic tasks [Haarnoja et al., 2018], just to name a few. However, model-free methods are notoriously data inefﬁcient and often require a massive number of samples from the environment. In many high-stakes real-world applications, e.g., autonomous driving, online education, etc., it is often expensive, or even infeasible, to collect such large-scale datasets. On the other hand, model-based reinforcement learning (MBRL), in contrast, is considered to be an appealing alternative that is able to substantially reduce sample complexity [Sun et al., 2018, Langlois et al., 2019].
At a colloquial level, model-based approaches build a predictive model of the environment dynam-ics and generate simulated rollouts from it to derive a policy [Janner et al., 2019, Luo et al., 2018,
Kaiser et al., 2019] or a planner [Chua et al., 2018, Hafner et al., 2019]. However, the asymptotic performance of MBRL methods often lags behind their model-free counterparts, mainly due to the fact that the model learned from ﬁnite data can still be far away from the underlying dynamics of the environment. To be precise, even equipped with a high-capacity model, such model error still exists due to the potential distribution mismatch between the training and generating phases, i.e., the state-action input distribution used to train the model is different from the one generated by the model [Talvitie, 2014]. Because of this gap, the learned model may give inaccurate predictions
∗Work done while at Carnegie Mellon University. †Weinan Zhang is the corresponding author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
on simulated data and the errors can compound for multi-step rollouts [Asadi et al., 2018], which will be exploited by the follow-up policy optimization or planning procedure, leading to degraded performance.
In the literature, there is a fruitful line of works focusing on reducing the distribution mismatch prob-lem by improving the approximation accuracy of model learning, or by designing careful strategies when using the model for simulation. For model learning, different architectures [Asadi et al., 2018,
Asadi et al., 2019, Chua et al., 2018] and loss functions [Farahmand, 2018, Wu et al., 2019] have been proposed to mitigate overﬁtting or improve multi-step predictions so that the simulated data generated by the model are more like real. For model usage, delicate rollout schemes
[Janner et al., 2019, Buckman et al., 2018, Nguyen et al., 2018, Xiao et al., 2019] have been adopted to exploit the model before the simulated data departure from the real distribution. Although these existing methods help alleviate the distribution mismatch, this problem still exists.
In this paper, we take a step further towards the goal of explicit mitigation of the distribution mismatch problem for better policy optimization in Dyna-style MBRL [Sutton, 1990]. To begin with, we derive a lower bound of the expected return in the real environment, which naturally inspires a bound maximization algorithm according to the theory of unsupervised domain adaptation.
To this end, we propose a novel model-based framework, namely AMPO (Adaptation augmented
Model-based Policy Optimization), by introducing a model adaptation procedure upon the existing
MBPO [Janner et al., 2019] method. To be speciﬁc, model adaptation encourages the model to learn invariant feature representations by minimizing integral probability metric (IPM) between the feature distributions of real data and simulated data. By instantiating our framework with Wasserstein-1 distance [Villani, 2008], we obtain a practical method. We evaluate our method on challenging continuous control benchmark tasks, and the experimental results demonstrate that the proposed
AMPO achieves better performance against state-of-the-art MBRL and MFRL methods in terms of sample efﬁciency. 2 Preliminaries
We ﬁrst introduce the notation used throughout the paper and brieﬂy discuss the problem setup of reinforcement learning and concepts related to integral probability metric.
Reinforcement Learning A Markov decision process (MDP) is deﬁned by the tuple (S, A, T, r, γ), where S and A are the state and action spaces, respectively. Throughout the paper, we assume that the state space is continuous and compact. γ ∈ (0, 1) is the discount factor. T (s(cid:48) | s, a) is the transition density of state s(cid:48) given action a made under state s, and the reward function is denoted as r(s, a).
The goal of reinforcement learning (RL) is to ﬁnd the optimal policy π∗ that maximizes the expected return (sum of discounted rewards), denoted by η:
π∗ := arg max
η[π] = arg max
π
π (cid:34) ∞ (cid:88) (cid:35)
γtr(st, at)
,
Eπ t=0 (1) where st+1 ∼ T (s | st, at) and at ∼ π(a | st).
In practice, the groundtruth transition T is unknown and MBRL methods aim to construct a model ˆT of the transition dynamics, using data collected from interaction with the MDP. Furthermore, different from several previous MBRL works
[Chua et al., 2018, Luo et al., 2018], the reward function r(s, a) is also unknown throughout the paper, and an agent needs to learn the reward function simultaneously.
For a policy π, we deﬁne the normalized occupancy measure, ρπ (s, a) [Ho and Ermon, 2016], as the
ˆT discounted distribution of the states and actions visited by the policy π on the dynamics model ˆT : (s), where P π t=0 γtP π
ρπ (s) denotes the density of state s visited
ˆT ,t
ˆT ,t
ˆT by π under ˆT at time step t. Similarly, ρπ
T (s, a) represents the discounted occupancy measure visited by π under the real dynamics T . Using this deﬁnition, we can equivalently express the objective function as follows: η[π] = Eρπ
T (s, a)r(s, a) ds da. To simplify the notation, we also deﬁne the normalized state visit distribution as νπ (s, a) = (1 − γ) · π(a | s) (cid:80)∞
T (s,a)[r(s, a)] = (cid:82) ρπ
T (s) := (1 − γ) (cid:80)∞ t=0 γtP π
T,t(s).
Integral Probability Metric
Integral probability metric (IPM) is a family of discrepancy mea-sures between two distributions over the same space [Müller, 1997, Sriperumbudur et al., 2009]. 2
Speciﬁcally, given two probability distributions P and Q over X , the F-IPM is deﬁned as dF (P, Q) := sup f ∈F
Ex∼P[f (x)] − Ex∼Q[f (x)], (2) where F is a class of witness functions f : X → R. By choosing different function class F, IPM reduces to many well-known distance metrics between probability distributions. In particular, the
Wasserstein-1 distance [Villani, 2008] is deﬁned using the 1-Lipschitz functions {f : (cid:107)f (cid:107)L ≤ 1}, where the Lipschitz semi-norm (cid:107)·(cid:107)L is deﬁned as (cid:107)f (cid:107)L = supx(cid:54)=y |f (x)−f (y)|/|x−y|. Furthermore, total variation is also a kind of IPM and we use dTV(·, ·) to denote it. 3 A Lower Bound for Expected Return
In this section, we derive a lower bound for the expected return function in the context of deep MBRL with continuous states and non-linear stochastic dynamics. The lower bound concerns about the expected return, i.e., Eq. (1) and is expressed in the following form [Janner et al., 2019]:
η[π] ≥ ˆη[π] − C, (3) where ˆη[π] denotes the expected return of running the policy π on a learned dynamics model
ˆT (s(cid:48) | s, a) and the term C is what we wish to construct. Normally, the dynamics model ˆT is learned with experiences (s, a, s(cid:48)) collected by a behavioral policy πD in the real environment dynamics T .
Typically, in an online MBRL method with iterative policy optimization, the behavioral policy πD represents a collection of past policies. Once we have derived this lower bound, we can naturally design a model-based framework to optimize the RL objective by maximizing the lower bound. Due to page limit, we defer all the proofs to the appendix.
Recall that in MBRL, we have real data (s, a, s(cid:48)) collected in the real dynamics T by the behavioral policy πD and will generate simulated data using the dynamics model ˆT with the current policy π.
We begin by showing that for any state s(cid:48), the discrepancy between its visit distributions in real data and simulated data admits the following decomposition.
Lemma 3.1. Assume the initial state distributions of the real dynamics T and the dynamics model ˆT are the same. For any state s(cid:48), assume there exists a witness function class Fs(cid:48) = {f : S × A → R} such that ˆT (s(cid:48) | ·, ·) : S × A → R is in Fs(cid:48). Then the following holds:
|νπD
T (s(cid:48)) − νπ
ˆT (s(cid:48))| ≤ γdFs(cid:48) (ρπD
T , ρπ
ˆT
) + γE (s,a)∼ρπD
T (cid:12) (cid:12) (cid:12)T (s(cid:48) | s, a) − ˆT (s(cid:48) | s, a) (cid:12) (cid:12) (cid:12) . (4)
Lemma 3.1 states that the discrepancy between two state visit distributions for each state is upper bounded by the dynamics model error for predicting this state and the discrepancy between the two state-action occupancy measures. Intuitively, it means that when both the input state-action distributions and the conditional dynamics distributions are close then the output state distributions will be close as well. Based on this lemma, now we derive the main result that gives a lower bound for the expected return.
Theorem 3.1. Let R := sups,a r(s, a) < ∞, F := ∪s(cid:48)∈S Fs(cid:48) and deﬁne (cid:15)π := 2dTV(νπ
Under the assumption of Lemma 3.1, the expected return η[π] admits the following bound:
T , νπD
T ).
η[π] ≥ ˆη[π]−R·(cid:15)π−γR·dF (ρπD
T , ρπ
ˆT
)·Vol(S)−γR·E (s,a)∼ρπD
T (cid:113) 2DKL(T (·|s, a) (cid:107) ˆT (·|s, a)), (5) where Vol(S) is the volume of state space S.
Remark Theorem 3.1 gives a lower bound on the objective in the true environment. In this bound, the last term corresponds to the model estimation error on real data, since the Kullback–Leibler divergence measures the average quality of current model estimation. The second term denotes the divergence between state visit distributions induced by the policy π and the behavioral policy πD in the environment, which is an important objective in batch reinforcement learning [Fujimoto et al., 2019] for reliable exploitation of off-policy samples. The third term is the integral probability metric between the (s, a) distributions ρπD
, which exactly corresponds to the distribution mismatch problem between model learning and model usage.
T and ρπ
ˆT 3
Take an action in the environment using the policy πφ; add the sample(s, a, s(cid:48), r) to De if every E real timesteps are ﬁnished then
Algorithm 1 AMPO 1: Initialize policy πφ, dynamics model ˆTθ, environment buffer De, model buffer Dm 2: repeat 3: 4: 5: 6: 7: 8: 9: 10:
Perform G1 gradient steps to train the model ˆTθ with samples from De for F model rollouts do
Sample a state s uniformly from De
Use policy πφ to perform a k-step model rollout starting from s; add to Dm end for
Perform G2 gradient steps to train the feature extractor with samples (s, a) from both De and Dm by the model adaptation loss LWD end if
Perform G3 gradient steps to train the policy πφ with samples (s, a, s(cid:48), r) from De ∪ Dm 11: 12: 13: until certain number of real samples
We would like to maximize the lower bound in Theorem 3.1 jointly over the policy and the dynamics model. In practice, we usually omit model optimization in the ﬁrst term ˆη[π] for simplicity like in previous work [Luo et al., 2018]. Then optimizing the ﬁrst term only over the policy and the last term over the model together becomes the standard principle of Dyna-style MBRL approaches.
And RL usually encourages the agent to explore, so we won’t constrain the policy according to the second term since it violates the rule of exploration, which aims at seeking out novel states. Then the key is to minimize the third term, i.e., the occupancy measure divergence, which is intuitively reasonable since the dynamics model will predict simulated (s, a) samples close to its training data with high accuracy. To optimize this term over the policy, we can use imitation learning methods on the dynamics model, such as GAIL, [Ho and Ermon, 2016] where the real samples are viewed as the expert demonstrations. However, optimizing this term over the policy is unnecessary, which may further reduce the efﬁciency of the whole training process. For example, one does not need to further optimize the policy using this term but just uses the ˆη[π] term. So in this paper, we mainly focus on how to optimize this occupancy measure matching term over the model. 4 AMPO Framework
To optimize the occupancy measure matching term over the model, instead of alleviating the distribu-tion mismatch problem on data level, we tackle it explicitly on feature level from the perspective of unsupervised domain adaptation [Ben-David et al., 2010, Zhao et al., 2019], which aims at generaliz-ing a learner on unlabeled data with labeled data from a different distribution. One promising solution for domain adaptation is to ﬁnd invariant feature representations by incorporating an additional objective of feature distribution alignment [Ben-David et al., 2007, Ganin et al., 2016]. Inspired by this, we propose to introduce a model adaptation procedure to encourage the dynamics model to learn the features that are invariant to the real state-action data and the simulated one.
Model adaptation can be seamlessly incorporated into existing Dyna-style MBRL methods since it is orthogonal to them, including those by reducing the distribution mismatch problem. In this paper, we adopt MBPO [Janner et al., 2019] as our baseline backbone framework due to its remarkable success in practice. We dub the integrated framework AMPO and detail the algorithm in Algorithm 1. 4.1 Preliminary: MBPO Algorithm
θ , ..., ˆT B
Model Learning We use a bootstrapped ensemble of probabilistic dynamics models { ˆT 1
θ } to capture model uncertainty, which was ﬁrst introduced in [Chua et al., 2018] and has shown to be effective in model learning [Janner et al., 2019, Wang and Ba, 2019]. Here B is the ensemble size and
θ denotes the parameters used in the model ensemble. To be speciﬁc, each individual dynamics model
ˆT i
θ is a probabilistic neural network which outputs a Gaussian distribution with diagonal covariance conditioned on the state sn and the action an: ˆT i
θ(sn, an)).
The neural network models in the ensemble are initialized differently and trained with different bootstrapped samples selected from the environment buffer De, which stores the real data collected
θ(sn+1 | sn, an) = N (µi
θ(sn, an), Σi 4
Figure 1: Illustration of model training and model adaptation. At every iteration, the model is learned by maximum likelihood estimation with real data collected from the environment. After the model training, the feature extractor is copied, and then the model adaptation begins where the two separate feature extractors are used for real data and simulated data respectively. After the model adaptation at the current iteration is ﬁnished, the feature extractor for the simulated data will be used to initialize the model training at the next iteration. from the environment. To train each single model, the negative log-likelihood loss is used:
Li
ˆT (θ) =
N (cid:88) n=1 (cid:2)µi
θ (sn, an) − sn+1
−1 (cid:3)(cid:62)
Σi
θ (sn, an) (cid:2)µi
θ (sn, an) − sn+1 (cid:3) + log det Σi
θ (sn, an) . (6)
Model Usage The ensemble models are used to generate k-length simulated rollouts branched from the states sampled from the environment buffer De. In detail, at each step, a model from the ensemble is selected at random to predict the next state and then the simulated data is added to the model buffer
Dm. Then a policy is trained on both real and simulated data from two buffers with a certain ratio.
We use soft actor-critic (SAC) [Haarnoja et al., 2018] as the policy optimization algorithm, which trains a stochastic policy with entropy regularization in actor-critic architecture by minimizing the expected KL-divergence:
Lπ(φ) = Es[DKL(πφ(·|s) (cid:107) exp(Q(st, ·) − V (s))]. (7) 4.2
Incorporating Unsupervised Model Adaptation
For convenience, in the following, we only consider one individual dynamics model, and the same procedure could be applied to any other dynamics model in the ensemble. Since the model is implemented by a neural network, we deﬁne the ﬁrst several layers as the feature extractor fg with corresponding parameters θg and the remaining layers as the decoder fd with parameters θd. Thus we have ˆT = fd ◦ fg and θ = {θg, θd}. We propose to add a model adaptation loss over the output of feature extractor, which encourages such a conceptual division as the feature encoder and the decoder. The main idea of model adaptation is to adjust the feature extractor fg in order to align the two feature distributions of real samples and simulated ones as input, so that the induced feature distributions from real and simulated samples are close in the feature space.
To incorporate unsupervised model adaptation into MBPO, we adopt alternative optimization between model training and model adaptation as illustrated in Figure 1. At every iteration (line 4 to 11 in
Algorithm 1), when the dynamics model is trained, we use it to generate simulated rollouts which will then be used for model adaptation and policy optimization. As for the detailed adaptation strategy, instead of directly sharing the parameter weights of the feature extractor between real data and simulated data [Ganin et al., 2016], we choose to adopt the asymmetric feature mapping strategy
[Tzeng et al., 2017], which has been shown to outperform the weight-sharing variant in domain adaptation due to more ﬂexible feature mappings. To be speciﬁc, the asymmetric feature mapping 5
strategy unties the shared weights between two domains and learns individual feature extractors for real data and simulated data respectively. Thus in AMPO, after the model adaptation at one iteration is ﬁnished, we will use the weight parameters for simulated data to initialize the model training for the next iteration. Through such an alternative optimization between model training and model adaptation, the feature representations learned by the feature extractor will be informative for the decoder to predict real samples, and more importantly it can generalize to the simulated samples. 4.3 Model Adaptation via Wasserstein-1 Distance g (se, ae) and hm = f m
Speciﬁcally, given real samples (se, ae) from the environment buffer De and the simulated samples (sm, am) from the model buffer Dm, the two separate feature extractors map them to feature represen-tations he = f e g (sm, am). To achieve model adaptation, we minimize one kind of IPM between the two feature distributions Phe and Phm according to the lower bound in Theorem 3.1. In this paper, we choose Wasserstein-1 distance as the divergence measure in model adaptation, which is validated to be effective in domain adaptation [Shen et al., 2018]. In the appendix, we also provide a variant that uses Maximum Mean Discrepancy.
Wasserstein-1 distance corresponds to IPM where the witness function satisﬁes the 1-Lipschitz constraint. To estimate the Wasserstein-1 distance, we use a critic network fc with parameters ω as introduced in Wasserstein GAN [Arjovsky et al., 2017]. The critic maps a feature representation to a real number, and then according to Eq. (2) the Wasserstein-1 distance can be estimated by maximizing the following objective function over the critic:
LWD(θe g, θm g , ω) = 1
Ne
Ne(cid:88) i=1 fc(hi e) − 1
Nm
Nm(cid:88) j=1 fc(hj m). (8)
In the meanwhile, the parameterized family of critic functions {fc} should satisfy 1-Lipschitz constraint according to the IPM formulation of Wasserstein-1 distance. In order to properly enforce 1-Lipschitz, we choose the gradient penalty loss [Gulrajani et al., 2017] for the critic
Lgp(ω) = EPˆh
[((cid:107)∇fc(ˆh)(cid:107)2 − 1)2], (9) where Pˆh is the distribution of uniformly distributed linear interpolations of Phe and Phm .
After the critic is trained to approximate the Wasserstein-1 distance, we optimize the feature extractor to minimize the estimated Wasserstein-1 distance to learn features invariant to the real data and simulated data. To sum up, model adaptation though Wasserstein-1 distance can be achieved by solving the following minimax objective min g,θm
θe g max
ω
LWD(θe g, θm g , ω) − α · Lgp(ω), (10) g and θm where θe g are the parameters of the two feature generators for real data and simulated data respectively, and α is the balancing coefﬁcient. For model adaptation at each iteration, we alternate between training the critic to estimate the Wasserstein-1 distance and training the feature extractor of the dynamics model to learn transferable features. 5 Experiments 5.1 Comparative Evaluation
Compared Methods We compare our method AMPO to other model-free and model-based algo-rithms. Soft Actor-Critic (SAC) [Haarnoja et al., 2018] is the state-of-the-art model-free off-policy algorithm in terms of sample efﬁciency and asymptotic performance so we choose SAC for the model-free baseline. For model-based methods, we compare to MBPO [Janner et al., 2019], PETS
[Chua et al., 2018] and SLBO [Luo et al., 2018].
Environments We evaluate AMPO and other baselines on six MuJoCo continuous control tasks with a maximum horizon of 1000 from OpenAI Gym [Brockman et al., 2016], including InvertedPen-dulum, Swimmer, Hopper, Walker2d, Ant and HalfCheetah. For the Swimmer environment, we use the modiﬁed version introduced by [Langlois et al., 2019] since the original version is quite difﬁcult to solve. For the other ﬁve environments, we adopt the same settings as in [Janner et al., 2019]. 6
Figure 2: Performance curves of AMPO and other model-based and model-free baselines on six continuous control benchmarking environments. We average the results over ﬁve random seeds, where solid curves depict the mean of ﬁve trials and shaded areas indicate the standard deviation. The dashed reference lines are the asymptotic performance of Soft Actor-Critic (SAC).
Implementation Details We implement all our experiments using TensorFlow.2 For MBPO and
AMPO, we ﬁrst apply a random policy to sample a certain number of real data and use them to pre-train the dynamics model. In AMPO, the model adaptation procedure will not be executed any more after a certain number of real samples, which doesn’t affect performance. In each adaptation iteration, we train the critic for ﬁve steps and then train the feature extractor for one step, and the coefﬁcient α of gradient penalty is set to 10. Every time we train the dynamics model, we randomly sample several real data as a validation set and stop the model training if the model loss does not decrease for ﬁve gradient steps, which means we do not choose a speciﬁc value for the hyperparameter
G1. Other important hyperparameters are chosen by grid search and detailed hyperparameter settings used in AMPO can be found in the appendix.
Results The learning curves of all compared methods are presented in Figure 2. From the comparison, we observe that our approach AMPO is the most sample efﬁcient as they learn faster than all other baselines in all six environments. Furthermore, AMPO is capable of reaching comparable asymptotic performance of the state-of-the-art model-free baseline SAC. Compared with MBPO, our approach achieves better performance in all the environments, which veriﬁes the value of model adaptation.
This also indicates that even in the situation with reduced distribution mismatch by using short rollouts, model adaptation still helps. 5.2 Model Errors
To better understand how model adaptation affects model learning, we plot in Figure 3(a) the curves of one-step model losses in two environments. By comparison, we observe that both the training and validation losses of dynamics models in AMPO are smaller than that in MBPO throughout the learning process. It shows that by incorporating model adaptation the learned model becomes more accurate. Consequently, the policy optimized based on the improved dynamics model can perform better.
We also investigate the compounding model errors of multi-step forward predictions, which is largely caused by the distribution mismatch problem. The h-step compounding error [Nagabandi et al., 2018] i=1 (cid:107)ˆsi − si(cid:107)2 where ˆsi+1 = ˆTθ(ˆsi, ai) and ˆs0 = s0. From Figure 3(b) is calculated as (cid:15)h = 1 h we observe that AMPO achieves smaller compounding errors than MBPO, which veriﬁes that AMPO can successfully mitigate the distribution mismatch. (cid:80)h 2Our code is publicly available at: https://github.com/RockySJ/ampo 7
(a) One-step model losses. (b) Compounding errors.
Figure 3: (a) The one-step model losses are evaluated on the training and (varying) validation data set from the environment buffer every time the model is trained. (b) Every 5000 environment steps in
Hopper, we calculate multi-step compounding errors and then average them. 5.3 Wasserstein-1 Distance Visualization
To further investigate the effect of model adaptation, we visualize the estimated Wasserstein-1 distance between the real features and simulated ones. Besides MBPO and AMPO, we additionally analyze the multi-step training loss of SLBO since it also uses the model output as the input of model training, which may help learn invariant features. According to the results shown in Figure 4(a), we ﬁnd that: i) the vanilla model training in MBPO itself can slowly minimize the Wasserstein-1 distance between feature distributions; ii) the multi-step training loss in SLBO does help learn invariant features but the improvement is limited; iii) the model adaptation loss in AMPO is effective in promoting feature distribution alignment, which is consistent with our initial motivation. 5.4 Hyperparameter Studies
In this section, we study the sensitivity of AMPO to important hyperparameters, and the results in
Hopper are shown in Figure 4(b). We ﬁrst conduct experiments with different adaptation iterations
G2. We observe that increasing G2 yields better performance up to a certain level while too large
G2 degrades the performance, which means that we need to control the trade-off between model training and model adaptation to ensure the representations to be invariant and also discriminative.
We then conduct experiments with different rollout length schedules, of which the effectiveness has been shown in MBPO [Janner et al., 2019]. We observe that generating longer rollouts earlier in
AMPO improves the performance while it degrades the performance of MBPO a little. It is easy to understand since as discussed in Section 5.2 the learned dynamics model in AMPO obtains better accuracy in approximations and therefore longer rollouts can be performed. 6