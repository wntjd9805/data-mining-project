Abstract
By transferring both features and gradients between different layers, shortcut connections explored by ResNets allow us to effectively train very deep neural networks up to hundreds of layers. However, the additional computation costs induced by those shortcuts are often overlooked. For example, during online inference, the shortcuts in ResNet-50 account for about 40 percent of the entire memory usage on feature maps, because the features in the preceding layers cannot be released until the subsequent calculation is completed. In this work, for the
ﬁrst time, we consider training the CNN models with shortcuts and deploying them without. In particular, we propose a novel joint-training framework to train plain CNN by leveraging the gradients of the ResNet counterpart. During forward step, the feature maps of the early stages of plain CNN are passed through later stages of both itself and the ResNet counterpart to calculate the loss. During backpropagation, gradients calculated from a mixture of these two parts are used to update the plainCNN network to solve the gradient vanishing problem. Extensive experiments on ImageNet/CIFAR10/CIFAR100 demonstrate that the plainCNN network without shortcuts generated by our approach can achieve the same level of accuracy as that of the ResNet baseline while achieving about 1.4× speed-up and 1.25× memory reduction. We also veriﬁed the feature transferability of our
ImageNet pretrained plain-CNN network by ﬁne-tuning it on MIT 67 and Caltech 101. Our results show that the performance of the plain-CNN is slightly higher than that of its baseline ResNet-50 on these two datasets. The code will be available at https://github.com/leoozy/JointRD_Neurips2020 and the MindSpore code will be available at https://www.mindspore.cn/resources/hub. 1

Introduction
Very deep convolutional neural networks (CNNs) have been successfully applied in a large variety of computer vision tasks in recent years [1, 2]. Wherein, the residual modules, i.e., the shortcuts have played a vital role in training very deep neural networks. Shortcuts can be effectively utilized to alleviate the gradient vanishing problem, which is widely used in modern CNN architectures including ResNet [3], MobileNet [4], ResNeXt [5], EfﬁcientNet [6], etc. However, besides the improvement in performance [7], there is an important disadvantage for shortcuts which is often overlooked in existing works. Different from conventional neural architectures (e.g., VGGNet [8]), the feature maps of intermediate layers in those networks using shortcuts cannot be released during online inference. Since the shortcut operation merges features in layers with different depths, we need
∗Co-ﬁrst authors with equal contributions listed in alphabetical order.
†Guilin Li is now afﬁliated with Intact ﬁnancial (HK) limited. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
to retain their storage for the subsequent calculations. According to Arash et.al [9], for ResNet-152, the shortcuts account for around 43 percent of the total feature map data that consumes much off-chip memory trafﬁc. They also reported a 24.8 percent reduction in energy consumption for ResNet-152 when the shortcut on-chip data is reused.
Figure 1: Joint-training framework: early stages of plain-CNN is connected to later stages of ResNet.
CE loss represents for the cross-entropy loss, TF represents for teacher feature map and SF represents for students feature maps.
To reduce the redundancy in pre-trained deep networks, a large number of model compression and acceleration approaches have been investigated. Neural Architecture Search [10, 11, 12, 13, 14] searches architectures that consume fewer resources with comparable performance. Model pruning [15, 16, 17] produces models smaller in size via removing the redundant weights or channel.
Quantization [18, 19, 20, 21, 22, 23, 24] reduces the precision of the model weights, resulting in smaller model size and faster computation. More deep learning training/inference frameworks
[25] are also proposed to optimize the deployment. Although the aforementioned methods have made tremendous efforts for obtaining compact neural networks with reasonable accuracy drop, the potential improvement in deployment efﬁciency offered by removing the shortcut has been largely ignored. Thus, an algorithm for removing shortcuts without sacriﬁce accuracy during inference would bring huge beneﬁt. Ideally, this method should also be able to be used on top of other compression methods such as model pruning.
In this paper, we propose to solve the problem mentioned above using the teacher-student paradigm.
Different from the current teacher-student framework, our method, for the ﬁrst time, propose to pass the gradients calculated from the teacher network to the student network. This can also be seen as training the network by adding some auxiliary architecture to assist the convergence. While during inference, the auxiliary part is abandoned. Specially, we develop a Joint-training framework based on
Residual Distillation (JointRD), as shown in Figure 1. In practice, the original ResNets are selected as teacher networks, and the students are generated by directly removing shortcuts from the teachers.
Then, each of the stages in the plain student network is connected with both later stages of the teacher and student networks. During the back-propagation, gradients are calculated and integrated from both of these two parts. In the early training iterations, gradients from the teacher network play a larger role, which will be gradually reduced until the convergence. By exploiting the proposed joint-training framework, we can effectively integrate the beneﬁt of shortcuts into the training of plain student networks and obtain excellent portable networks without shortcuts. the proposed joint-training framework on the Ima-We verify the effectiveness of geNet/CIFAR10/CIFAR100 benchmark datasets. Experimental results show that the student network can achieve comparable accuracy to that of the original ResNet with shortcuts for all these three datasets. For example, the plain ResNet-50 trained using our method achieves a top-1 testing ac-curacy of 76.08% on ImageNet, which is comparable to that of its baseline teacher network 76.11
%. To check the transferability of the ImageNet pre-trained plain-CNN model using our method, we compare the ﬁne-tuning results of the learned plain-CNN and the ResNet on MIT67 [26] and
Caltech101 [27]. Without considering the randomness, the learned plain-CNN outperforms ResNet on these two datasets. 2
Our method also performs signiﬁcantly better than traditional teacher-student framework such as logits-based knowledge distillation (KD) ([28])(76.08% verses 71.47%) on ImageNet with ResNet-50.
Furthermore, we also valid the effectiveness when using the proposed method together with model pruning ([17]) and KD ([28]). We show that our method can be used on top of either pruning or KD instead of being a substitution of them. Speciﬁcally, after removing the shortcut, the network can also beneﬁt from pruning or KD as much as the original ResNet on CIFAR100 and ImageNet. 2