Abstract
We present a deep imitation learning framework for robotic bimanual manipulation in a continuous state-action space. A core challenge is to generalize the manip-ulation skills to objects in different locations. We hypothesize that modeling the relational information in the environment can signiﬁcantly improve generaliza-tion. To achieve this, we propose to (i) decompose the multi-modal dynamics into elemental movement primitives, (ii) parameterize each primitive using a recur-rent graph neural network to capture interactions, and (iii) integrate a high-level planner that composes primitives sequentially and a low-level controller to com-bine primitive dynamics and inverse kinematics control. Our model is a deep, hierarchical, modular architecture. Compared to baselines, our model generalizes better and achieves higher success rates on several simulated bimanual robotic manipulation tasks. We open source the code for simulation, data, and models at: https://github.com/Rose-STL-Lab/HDR-IL. 1

Introduction
Manipulation is a fundamental capability robots need for many real-world applications. Although there has been signiﬁcant progress in single-arm manipulation tasks such as grasping, pick-and-place, and pushing [1, 2, 3, 4], bimanual manipulation has received less attention. Many tasks however require using both arms/hands; consider opening a bottle, steadying a nail while hammering, or moving a table. While having two arms to accomplish these tasks is clearly useful, bimanual manipulation tasks are also signiﬁcantly more challenging, due to higher-dimensional continuous state-action spaces, more object interactions, and longer-horizon solutions.
Most existing work in bimanual manipulation addresses the problem in a classical control setting, where the environment and its dynamics are known [5]. However, these models are difﬁcult to construct explicitly and are inaccurate, because of complicated interactions in the task including friction, adhesion, and deformation between the two arms/hands and the object being manipulated. A promising approach that avoids manually specifying models is imitation learning, where a teacher provides demonstrations of desired behavior to the robot (sequences of sensed input states and target control actions in those states), and the robot learns a policy to mimic the teacher [6, 7, 8].
In particular, recent deep imitation learning methods have been successful at learning single-arm manipulation from demonstrations, even with only images as observations (e.g., [9, 10]).
In this work, we explore extending deep imitation learning methods to bimanual manipulation. In light of the challenges identiﬁed above, our goal is to design an imitation learning model to capture relational information (i.e., the trajectory involves relations to other objects in the environment) in environments with complex dynamics (i.e., the task requires multiple object interactions to accomplish
∗†Equal contribution; Correspondence: xie.f@northeastern.edu,chowdhury.al@northeastern.edu, roseyu@ucsd.edu; 1Northeastern University, Boston MA, USA, 2University of California, San Diego, USA. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: An example of the HDR-ILmodel performing a “peg-in-hole” construction of a table. The trajectory of primitives to complete this task is shown in the top row. The sequence of high-level primitives is learned from demonstrations. State trajectories are predicted based on the primitive identiﬁed. The ﬁnal predicted state for the primitive becomes the initial state for the next primitive. a goal). The model needs to be general enough to complete tasks under different variations, such as changes in the initial conﬁguration of the robot and objects.
The insight of our paper is to accomplish this goal through a two-level framework, illustrated in
Figure 1 for a bimanual “peg-in-hole” table assembly and lifting task. The task involves grasping two halves of a table, slotting one half into the other, and then lifting both parts together as whole.
Instead of trying to learn a policy for the entire trajectory, we split demonstrations into task primitives (subsequences of states), as shown in the top row. We learn two models: a high-level planning model that predicts a sequence of primitives, and a set of low-level primitive dynamics models that predict a sequence of robot states to complete each identiﬁed task primitive. All models are parameterized by recurrent graph neural networks that explicitly capture robot-robot and robot-object interactions. In summary, our contributions include:
• We propose a deep imitation learning framework, Hierarchical Deep Relational Imitation Learning (HDR-IL), for robotic bimanual manipulation. Our model explicitly captures the relational information in the environment with multiple objects.
• We take a hierarchical approach by separately learning a high-level planning model and a set of low-level primitive dynamics models. We incorporate relational features and use recurrent graph neural networks to parameterize all models.
• We evaluate on two variations of a table-lifting task where bimanual manipulation is essential.
We show that both hierarchical and relational modeling provide signiﬁcant improvement for task competition on held-out instances. For the task shown in Figure 1, our model achieves 29% success rate, whereas a baseline approach without these contributions only succeeds in 1% of the test cases. 2