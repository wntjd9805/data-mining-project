Abstract
Vision-and-Language Navigation (VLN) requires an agent to navigate in a real-world environment following natural language instructions. From both the textual and visual perspectives, we ﬁnd that the relationships among the scene, its objects, and directional clues are essential for the agent to interpret complex instructions and correctly perceive the environment. To capture and utilize the relationships, we propose a novel Language and Visual Entity Relationship Graph for modelling the inter-modal relationships between text and vision, and the intra-modal relationships among visual entities. We propose a message passing algorithm for propagating information between language elements and visual entities in the graph, which we then combine to determine the next action to take. Experiments show that by taking advantage of the relationships we are able to improve over state-of-the-art. On the
Room-to-Room (R2R) benchmark, our method achieves the new best performance on the test unseen split with success rate weighted by path length (SPL) of 52%.
On the Room-for-Room (R4R) dataset, our method signiﬁcantly improves the previous best from 13% to 34% on the success weighted by normalized dynamic time warping (SDTW).
Code is available at: https://github.com/YicongHong/Entity-Graph-VLN. 1

Introduction
Vision-and-language navigation in the real-world is an important step towards building mobile agents that perceive their environments and complete speciﬁc tasks following human instructions.
A great variety of scenarios have been set up for relevant research, such as long range indoor and street view navigation with comprehensive instructions [3, 5, 17], communication based visual navigation [7, 25, 34], and navigation for object localization and visual question answering [6, 29].
The recently proposed R2R navigation task by Anderson et al. [3] has drawn signiﬁcant research interest. Here an agent needs to navigate in an unseen photo-realistic environment following a natural language instruction, such as “Walk toward the white patio table and chairs and go into the house through the glass sliding doors. Pass the grey couches and go into the kitchen. Wait by the toaster.”. This task is particularly challenging as the agent needs to learn the step-wise correspondence between complex visual clues and the natural language instruction without any explicit intermediate supervision (e.g., matching between sub-instructions and path-segments [12, 41]).
Most previous agents proposed for the R2R navigation task are based on a sequence-to-sequence network [3] with grounding between vision and language [9, 21, 22, 32, 36]. Instead of explicitly modelling the relationship between visual features and the orientation of the agent, these methods 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Language and Visual Entity Relationship Graph. At each navigational step t, (a) Scene, object and directional clues are observed and encoded as visual features. (b) language attention graph is constructed depending on the agent’s state, (c) visual features are initialized as nodes in the language-conditioned visual graph, information propagated through the graph updates the nodes, which are ultimately used for determining action probabilities. Each double-circle in the ﬁgure indicates an observed feature. resort to a high-dimensional representation that concatenates image features and directional encoding.
Moreover, objects seen during navigation, which are mentioned in the instruction and contain strong localization signals, are rarely considered in previous works.
However, we observe that many navigation instructions contain three different contextual clues, each of which corresponds to a distinct visual feature that are essential for the interpretation of the instruction: scene (“where is the agent at a coarse level?”), object (“what is the pose of the agent with respect to this landmark?”) and direction (“what action should the agent take relative to its orientation?”). Meanwhile, these contextual clues are not independent, but work together to clarify the instruction. For instance, given an instruction “With the sofa on your left, turn right into the bathroom and stop”, the agent ﬁrst needs to identify the “sofa” (object) as a landmark on its left, which the impending action is conditioned on, then “turn right” (direction) to move to the “bathroom” (scene), and ﬁnally “stop” (direction) inside the “bathroom” (scene). As a result, it is important for the agent to learn about the relationships among the scene, the object and the direction.
In this paper, we propose a novel language and visual entity relationship graph for vision-and-language navigation that explicitly models the inter- and intra-modality relationships among the scene, the object, and the directional clues (Figure 1). The graph is composed of two interacting subgraphs. The ﬁrst subgraph, a language attention graph, is responsible for attending to speciﬁc words in the instruction and their relationships. The second subgraph, a language-conditioned visual graph, is constructed for each navigable viewpoint and has nodes representing distinct visual features specialized for scene, object and direction. Information propagated through the graph updates the nodes, which are ultimately used for determining action probabilities.
The performance of our agent, trained end-to-end from scratch on the R2R [3] benchmark, signif-icantly outperforms the baseline model [32] and achieves new state-of-the-art results on the test unseen split following the single-run setting1. On the R4R data [17], an extended dataset of R2R, our method obtains 21% absolute improvement on the Success weighted by normalized Dynamic Time
Warping [15] comparing to the previous best.
We believe the proposed language and visual entity relationship graph will prove valuable for other vision-and-language tasks as has been shown in the contemporary work by Rodriguez et al. [31] in the temporal moment localization task. 1VLN Leaderboard: https://evalai.cloudcv.org/web/challenges/challenge-page/97/overview 2
2