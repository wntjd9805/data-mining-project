Abstract
The search for efﬁcient, sparse deep neural network models is most prominently performed by pruning: training a dense, overparameterized network and removing parameters, usually via following a manually-crafted heuristic. Additionally, the recent Lottery Ticket Hypothesis conjectures that, for a typically-sized neural net-work, it is possible to ﬁnd small sub-networks which, when trained from scratch on a comparable budget, match the performance of the original dense counterpart. We revisit fundamental aspects of pruning algorithms, pointing out missing ingredients in previous approaches, and develop a method, Continuous Sparsiﬁcation, which searches for sparse networks based on a novel approximation of an intractable (cid:96)0 regularization. We compare against dominant heuristic-based methods on pruning as well as ticket search – ﬁnding sparse subnetworks that can be successfully re-trained from an early iterate. Empirical results show that we surpass the state-of-the-art for both objectives, across models and datasets, including VGG trained on
CIFAR-10 and ResNet-50 trained on ImageNet. In addition to setting a new stan-dard for pruning, Continuous Sparsiﬁcation also offers fast parallel ticket search, opening doors to new applications of the Lottery Ticket Hypothesis. 1

Introduction
Although deep neural networks have become ubiquitous in ﬁelds such as computer vision and natural language processing, extreme overparameterization is typically required to achieve state-of-the-art results, incurring higher training costs and hindering applications limited by memory or inference time. Recent theoretical work suggest that overparameterization plays a key role in network training dynamics [1] and generalization [2]. However, it remains unclear whether, in practice, overparameterization is truly necessary to train networks to state-of-the-art performance.
Concurrently, empirical approaches have been successful in ﬁnding compact neural networks, either by shrinking trained models [3, 4, 5] or through efﬁcient architectures, yielding less overparameterized models that can be trained from scratch [6]. Recently, combining these two strategies has lead to new methods which discover efﬁcient architectures through optimization instead of design [7, 8].
Nonetheless, parameter efﬁciency is typically maximized by pruning an already trained network.
Despite the fact that the search for sparse solutions to optimization problems can be naturally described by (cid:96)0 regularization, the vast majority of pruning methods rely on manually-designed strategies that are not based on the (cid:96)0 penalty [3, 4, 9, 10]. The approaches that aim to approximate an (cid:96)0-regularized problem in order to ﬁnd sparse, less overparameterized networks are limited in number [11, 12] and fail to perform competitively against heuristic-based pruning methods.
Prior work has shown that pruned networks are hard to train from scratch [3], suggesting that while overparameterization is not necessary for a model’s capacity, it might be required for successful training. Frankle and Carbin [13] put this idea into question by training heavily pruned networks
∗equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
from scratch, while achieving performance matching that of their original counterparts. A key ﬁnding is that the same initialization should be used when re-training the pruned network, or, equivalently, that better strategies – depending on future weights – can result in trainable pruned networks.
More recently, Frankle et al. [14] show that although this approach can fail in large-scale settings, pruned networks can be successfully re-trained when parameters from very early training are used as initialization. Coupling a pruned network with a set of parameter values from initialization yields a ticket – a winning ticket if it is able to match the dense model’s performance when trained in isolation for a comparable number of iterations. These have already found applications in, for example, transfer learning [15, 16, 17], making ticket search a problem of independent interest.
Iterative Magnitude Pruning (IMP) [13], the ﬁrst and currently only algorithm able to ﬁnd winning tickets, consists of a repeating a two-stage procedure that alternates between training and pruning. IMP relies on a sensible choice for pruning strategy [18] and can be costly: maximizing the performance of the found subnetworks typically requires multiple rounds of training followed by pruning [19].
In this paper, we focus on two questions related to pruning and the Lottery Ticket Hypothesis. First, can we ﬁnd sparse networks with competitive performance by approximating (cid:96)0 regularization instead of relying on a heuristic pruning strategy and, if yes, what are the missing ingredients in previous approaches [11, 12]? Second, would a method that relies on (cid:96)0-regularization, rather than an ad-hoc heuristic, be able to ﬁnd winning tickets, as IMP does?
We provide positive answers to both questions by proposing Continuous Sparsiﬁcation1, a new pruning method that relies on approximating the intractable (cid:96)0 penalty and ﬁnds networks that perform competitively when either ﬁne-tuned or re-trained. Unlike prior (cid:96)0-based approaches, our approximation is deterministic, providing insights and raising questions on how pruning and sparse regularization should be performed. The core of our method lies in constructing a smooth continuation path [20] connecting training of soft-gated parameters and the intractable (cid:96)0-regularized objective.
Contributions:
• We propose a novel approximation to (cid:96)0 regularization, resulting in Continuous Sparsiﬁcation, a new pruning method with theoretical and empirical advantages over previous (cid:96)0-based approaches.
We show through experiments that the deterministic nature of our re-parameterization is key to achieving competitive results with (cid:96)0 approximations.
• We show that Continuous Sparsiﬁcation outperforms state-of-the-art heuristic-based pruning methods. Our experiments include pruning of VGG-16 [21] and ResNet-20 [22] trained on
CIFAR-10 [23], and ResNet-50 trained on ImageNet [24].
• Our method raises questions on how to do better ticket search – producing subnetworks that can be re-trained from early iterates. We show empirically that Continuous Sparsiﬁcation is capable of ﬁnding subnetworks of VGG-16, ResNet-20, and ResNet-50 that, when re-trained, outperform ones found by IMP. Moreover, the search cost of our method does not depend on the produced subnetwork’s sparsity, making ticket search considerably more efﬁcient when run in parallel. 2 Preliminaries
Here we deﬁne terms used throughout the paper.
Subnetwork: For a network f that maps samples x ∈ X and parameters w ∈ Rd to f (x; w), a subnetwork f (cid:48) of f is given by a binary mask m ∈ {0, 1}d, where a parameter component wi is kept in f (cid:48) if mi = 1 and removed otherwise i.e., f (cid:48) : x, w (cid:55)→ f (x; w (cid:12) m), with (cid:12) denoting element-wise multiplication. For any conﬁguration m, the effective parameter space of the induced network f (cid:48) is {w (cid:12) m|w ∈ Rd} – a (cid:107)m(cid:107)0-dimensional space, hence we say that the subnetwork f (cid:48) has (cid:107)m(cid:107)0 many parameters instead of d.
Matching subnetwork: For a network f and randomly-initialized parameters w(0), a matching subnetwork f (cid:48) of f is given by a conﬁguration m ∈ {0, 1}d, such that f (cid:48) can be trained in isolation from w(cid:48)(0) = w(k) (cid:12) m, where w(k) is the collection of parameter values obtained by training f from w(0) for k iterations, where k is small. Moreover, to be a matching subnetwork, f (cid:48) needs to match the performance of a trained f given the same budget, when measured in terms of training iterations. 1Code available at https://github.com/lolemacs/continuous-sparsification 2
Winning ticket: For a network f and randomly-initialized parameters w(0), a winning ticket is a matching subnetwork f (cid:48) of f that can be trained in isolation from initialization, i.e., w(cid:48)(0) = w(0) (cid:12)m.
In other words, a winning ticket is a matching subnetwork such that k = 0 in the deﬁnition above.
Ticket search is the task of ﬁnding matching subnetworks given a network f and randomly-initialized parameters w(0). We say that an algorithm A performs ticket search if A(f, w(0)) = m ∈ {0, 1}d, such that m induces a (possibly matching) subnetwork f (cid:48). 3