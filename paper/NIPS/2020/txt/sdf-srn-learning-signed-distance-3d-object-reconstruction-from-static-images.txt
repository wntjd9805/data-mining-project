Abstract
Dense 3D object reconstruction from a single image has recently witnessed remark-able advances, but supervising neural networks with ground-truth 3D shapes is impractical due to the laborious process of creating paired image-shape datasets.
Recent efforts have turned to learning 3D reconstruction without 3D supervision from RGB images with annotated 2D silhouettes, dramatically reducing the cost and effort of annotation. These techniques, however, remain impractical as they still require multi-view annotations of the same object instance during training. As a result, most experimental efforts to date have been limited to synthetic datasets.
In this paper, we address this issue and propose SDF-SRN, an approach that requires only a single view of objects at training time, offering greater utility for real-world scenarios. SDF-SRN learns implicit 3D shape representations to handle arbitrary shape topologies that may exist in the datasets. To this end, we derive a novel differentiable rendering formulation for learning signed distance functions (SDF) from 2D silhouettes. Our method outperforms the state of the art under challenging single-view supervision settings on both synthetic and real-world datasets. 1

Introduction
Humans have strong capabilities to reason about 3D geometry in our visual world. When we see an object, not only can we infer its shape and appearance, but we can also speculate the underlying 3D structure. We learn to develop the concepts of 3D geometry and semantic priors, as well as the ability to mentally reconstruct the 3D world. Somehow through visual perception, i.e. just looking at a collection of 2D images, we have the ability to infer the 3D geometry of the objects in those images.
Researchers have sought to emulate such ability of 3D shape recovery from a single 2D image for AI systems, where success has been drawn speciﬁcally through neural networks. Although one could train such networks naively from images with associated ground-truth 3D shapes, such paired data are difﬁcult to come by at scale. While most works have resorted to 3D object datasets, in which case synthetic image data can be created pain-free through rendering engines, the domain gap between synthetic and real images has prevented them from practical use. An abundant source of supervision that can be practically obtained for real-world image data is one problem that looms large in the ﬁeld.
In the quest to eliminate the need for direct 3D supervision, recent research have attempted to tackle the problem of learning 3D shape recovery from 2D images with object silhouettes, which are easier to annotate in practice. This line of works seeks to maximize the reprojection consistency of 3D shape predictions to an ensemble of training images. While success has been shown on volumetric [41] and mesh-based [18, 25] reconstruction, such discretized 3D representations have drawbacks. Voxels are inefﬁcient for representing shape surfaces as they are sparse by nature, while meshes are limited to deforming from ﬁxed templates as learning adaptive mesh topologies is a nontrivial problem. Implicit shape representations become a more desirable choice for overcoming these limitations. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Learning 3D SDF shape reconstruction from static images. SDF-SRN learns implicit shape reconstruction from single-view images and 2D silhouettes at training time, allowing practical applications of real-world 3D object reconstruction trained from static image datasets.
Differentiable rendering methods for reconstructing 3D implicit representations have since sparked wide interest [27, 34]. Previous works, however, have required a multi-view setup, where objects are observed from multiple viewpoints with silhouette annotations. Since such data is difﬁcult to obtain en masse, it has been unclear to the community how one can learn dense 3D reconstruction from single images at training time, where each individual object instance is assumed observed only once.
In this paper, we make signiﬁcant advances on learning dense 3D object reconstruction from single images and silhouettes, without the knowledge of the underlying shape structure or topology. To this end, we derive a formulation to learn signed distance functions (SDF) as the implicit 3D representation from images, where we take advantage of distance transform on silhouettes to provide rich geometric supervision from all pixels of an image. In addition, we build a differentiable rendering framework upon the recently proposed Scene Representation Network [39] for efﬁcient optimization of shape surfaces. The proposed method, SDF-SRN, achieves state-of-the-art 3D object reconstruction results on challenging scenarios that requires only a single observation for each instance during training time.
SDF-SRN also learns high-quality 3D reconstruction from real-world static images with single-view supervision (Fig. 1), which was not possible with previous implicit shape reconstruction methods.
In summary, we present the following contributions:
• We establish a novel mathematical formulation to optimize 3D SDF representations from 2D distance transform maps for learning dense 3D object reconstruction without 3D supervision.
• We propose an extended differentiable rendering algorithm that efﬁciently optimizes for the 3D shape surfaces from RGB images, which we show to be suitable only for SDF representations.
• Our method, SDF-SRN, signiﬁcantly outperforms state-of-the-art 3D reconstruction methods on ShapeNet [3] trained with single-view supervision, as well as natural images from the
PASCAL3D+ [45] dataset without 3D supervision nor externally pretrained 2.5D/3D priors. 2