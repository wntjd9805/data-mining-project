Abstract
We propose to jointly analyze experts’ eye movements and verbal narrations to discover important and interpretable knowledge patterns to better understand their decision-making processes. The discovered patterns can further enhance data-driven statistical models by fusing experts’ domain knowledge to support complex human-machine collaborative decision-making. Our key contribution is a novel dynamic Bayesian nonparametric model that assigns latent knowledge patterns into key phases involved in complex decision-making. Each phase is characterized by a unique distribution of word topics discovered from verbal narrations and their dynamic interactions with eye movement patterns, indicating experts’ special perceptual behavior within a given decision-making stage. A new split-merge-switch sampler is developed to efﬁciently explore the posterior state space with an improved mixing rate. Case studies on diagnostic error prediction and disease morphology categorization help demonstrate the effectiveness of the proposed model and discovered knowledge patterns. 1

Introduction
Recent years have seen an increasing application of automatic computational systems in supporting humans in visual-based decision-making tasks. Machine learning models are applied to process large-scale data in the forms of images, videos, and texts for discovering statistical regularities and making predictions [1, 2]. However, human expertise is still essential in providing meaningful interpretations of the semantics for tasks in specialized domains, such as medicine, science, and security intelligence.
Domain expertise, such as conceptual and perceptual skills, are usually developed through long-term training and practice. It allows human experts to perform better than fully automatic systems, which interpret images or videos solely based on low-level features [3, 4]. Therefore, it is beneﬁcial to incorporate human behavioral data for visual-based tasks in knowledge-rich domains.
Modern technologies have made it possible to record human behavioral data [5, 6]. For instance, eye tracking measures the gaze and the motion of eyes to indicate how human perceptually processes images and audio recording digitally inscribes and re-creates human speeches as input for studying semantic conception. Analysis of eye gaze exposes cognitive processing at the level of visual perception, while verbal expression reﬂects semantic conception. These elements, both of which are signiﬁcantly relevant to domain expertise, interact in visual-based decision-making process [7, 8].
In this paper, we propose to perform dynamic multimodal knowledge data fusion to synergize human domain expertise and statistical modeling, enabling them to tackle highly challenging visual-based tasks collectively. Inspired by psychological studies of important phases in humans’ decision-making
[9], we develop a phase-aware dynamic Bayesian nonparametric model that assigns latent knowledge patterns into key phases involved in complex decision-making. In particular, an expert’s decision-making process is automatically partitioned into a sequence of latent decision phases, whose temporal
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Graphical model of phase-aware knowledge fusion (hyper-parameters are omitted, and curved ar-rows denote ﬁrst-order Markov transition; L, J, K are potentially inﬁnite; the notations for variables are summarized in the supplementary material)
Figure 2: An illustrative example of inferred latent knowledge patterns of physicians’ ver-bal narrations and eye movements, and the latent phases explain experts’ diagnostic deci-sion making process. dependency is captured by a Markov structure. We further model the cross-modal interactions of multimodal data by conditioning both perceptual behavior (as eye movement patterns in our case) and conceptual processing (as topics from verbal narrations) on the decision phases. As a result, the multimodal latent patterns are dynamically fused at the phase level by contributing different knowledge components to a speciﬁc decision stage.
To perform phase-aware fusion of eye movements, we integrate an inﬁnite hidden Markov model with a nested Dirichlet process mixture (iHMM-nDP) to capture the spatiotemporal characteristics of eye movements. Since we aim to discover perceptual patterns common to a group of experts, commonly used models may lead to a large number of patterns with minor spatial/temporal variations.
Hence, extensive post-processing is usually needed to group semantically similar patterns [10]. The proposed iHMM-nDP model addresses this issue by naturally forming a 3-level semantic hierarchy, including state, component, and instantiation, which capture main patterns, sub-patterns with minor spatial/temporal variations, and actual observations from individual experts. We further leverage the hierarchical Dirichlet process (HDP) model to perform phase-aware fusion of the verbal narrations.
Phase-speciﬁc word topics are discovered that help explain the conceptual patterns conditioned on the same phase. As a result, the phase-aware fusion model reveals the relationship between eye movements and verbal narrations, creates knowledge-centered representations of data, and ultimately contributes to the understanding of experts’ decision-making process. Figure 1 shows the overall graphical model. Finally, a new Split-Merge-Switch (SMS) sampler is developed to efﬁciently explore the posterior state space with an improved mixing rate.
Figure 2 illustrates how the proposed model explores experts’ decision making process by visualizing patterns and topics learned from eye movements and verbal narrations, respectively. Each circle represents a location of visual ﬁxation, and the radius is proportional to the duration. Three signiﬁcant patterns are visualized in this example, including concentrating on primary abnormality, switching among several locations, and cluttering within a speciﬁc area [10]. The keywords from different latent topics are shown in different colors. Moreover, the proposed model automatically partitions the narration into three different decision phases. As can be seen, the narration starts from the description of low-level visual features of diseases, then goes through a reasoning process, and ﬁnally reaches a conclusion. The major contributions are summarized below:
• a phase-aware dynamic Bayesian nonparametric model to fuse experts’ eye movements and verbal narrations in complex decision-making based on key decision phases.
• an iHMM-nDP model to extract perceptual patterns that summarize spatiotemporal regularities from eye movements through a three-level semantic hierarchy to capture the main patterns, the sub-patterns, and the observations of eye movements hierarchically; discovery of phase-speciﬁc topics that help explain the conceptual patterns as a result of fusing experts’ verbal narrations.
• a fast mixing Split-Merge-Switch sampling algorithm to efﬁciently explore a potentially large latent state space due to nonparametric modeling and speed up hierarchical pattern discovery. 2
For evaluation, we present case studies on diagnostic error prediction and disease morphology categorization to demonstrate the effectiveness of the proposed model and discovered patterns. 2