Abstract
Neural network training is computationally and memory intensive. Sparse training can reduce the burden on emerging hardware platforms designed to accelerate sparse computations, but it can also affect network convergence. In this work, we propose a novel CNN training algorithm called Sparse Weight Activation Training (SWAT). SWAT is more computation and memory-efﬁcient than conventional training. SWAT modiﬁes back-propagation based on the empirical insight that convergence during training tends to be robust to the elimination of (i) small magnitude weights during the forward pass and (ii) both small magnitude weights and activations during the backward pass. We evaluate SWAT on recent CNN architectures such as ResNet, VGG, DenseNet and WideResNet using CIFAR-10,
CIFAR-100 and ImageNet datasets. For ResNet-50 on ImageNet SWAT reduces total ﬂoating-point operations (FLOPs) during training by 80% resulting in a 3.3× training speedup when run on a simulated sparse learning accelerator representative of emerging platforms while incurring only 1.63% reduction in validation accuracy.
Moreover, SWAT reduces memory footprint during the backward pass by 23% to 50% for activations and 50% to 90% for weights. Code is available at https:
//github.com/AamirRaihan/SWAT. 1

Introduction
Convolutional Neural Networks (CNNs) are effective at many complex computer vision tasks includ-ing object recognition [27, 57], object detection [56, 50] and image restoration [12, 65]. However, training CNNs requires signiﬁcant computation and memory resources. Software and hardware approaches have been proposed for addressing this challenge. On the hardware side, graphics proces-sor units (GPUs) are now typically used for training [27] and recent GPUs from NVIDIA include specialized Tensor Core hardware speciﬁcally to accelerate deep learning [46, 44, 45]. Specialized programmable hardware is being deployed in datacenters by companies such as Google and Mi-crosoft [25, 7]. Techniques for reducing computation and memory consumption on existing hardware include those reducing the number of training iterations such as batch normalization [24] and en-hanced optimization strategies [26, 13] and those reducing computations per iteration. Examples of the latter, which may be effective with appropriate hardware support, include techniques such as quantization [67, 6, 62, 59], use of ﬁxed-point instead of ﬂoating-point [63, 8], sparsiﬁcation [60] and dimensionality reduction [34]. This paper introduces Sparse Weight Activation Training (SWAT), which signiﬁcantly extends the sparsiﬁcation approach.
Training involves repeated application of forward and backward passes. Prior research on introducing sparsity during training has focused on sparsifying the backward pass. While model compression [18, 40, 36, 35, 32, 21, 37, 61, 41] introduces sparsiﬁcation into the forward pass, it typically does so by introducing additional training phases which increase overall training time. Amdahl’s Law [2] implies overall speedup is limited by the fraction of original execution time spent on computations that are 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
not sped up by system changes. To reduce training time signiﬁcantly by reducing computations per training iteration it is necessary to address both forward and backward passes. SWAT introduces sparsiﬁcation into both forward and backward passes and is suitable for emerging hardware platforms containing support for sparse matrix operations. Such hardware is now available. For example recently announced Ampere GPU architecture [46] includes support for exploiting sparsity. In addition, there is a growing body of research on hardware accelerators for sparse networks [47, 9, 66, 1] and we demonstrate, via hardware simulation, that SWAT can potentially train 5.9× faster when such accelerators become available.
While SWAT employs sparsity it does so with the objective of reducing training time not performing model compression. The contributions of this paper are:
• An empirical sensitivity analysis of approaches to inducing sparsity in network training;
• SWAT, a training algorithm that introduces sparsity in weights and activations resulting in reduced execution time in both forward and backward passes of training;
• An empirical evaluation showing SWAT is effective on complex models and datasets. 2