Abstract
The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspeciﬁed by the data, and can represent many compelling but differ-ent solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without signiﬁcant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to ﬁt images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased ﬂexibility. 1

Introduction
Imagine ﬁtting the airline passenger data in Figure 1. Which model would you choose: (1) f1(x) = w0 + w1x, (2) f2(x) = 3 j=0 wjxj, or (3) f3(x) = 104 j=0 wjxj?
P
P
Put this way, most audiences overwhelmingly favour choices (1) and (2), for fear of overﬁtting. But of these options, choice (3) most honestly represents our beliefs. Indeed, it is likely that the ground truth explanation for the data is out of class for any of these choices, but there is some setting of the coefﬁcients wj} in choice (3) which provides a better description of real-{ ity than could be managed by choices (1) and (2), which are special cases of choice (3). Moreover, our beliefs about the generative processes for our observations, which are often very sophisticated, typically ought to be independent of how many data points we observe.
Figure 1: Airline passenger data.
And in modern practice, we are implicitly favouring choice (3): we often use neural networks with millions of parameters to ﬁt datasets with thousands of points. Furthermore, non-parametric methods such as Gaussian processes often involve inﬁnitely many parameters, enabling the ﬂexibility for universal approximation [40], yet in many cases provide very simple predictive distributions. Indeed, parameter counting is a poor proxy for understanding generalization behaviour.
From a probabilistic perspective, we argue that generalization depends largely on two properties, the support and the inductive biases of a model. Consider Figure 2(a), where on the horizontal axis we have a conceptualization of all possible datasets, and on the vertical axis the Bayesian evidence for a 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) (b) (c) (d)
Figure 2: A probabilistic perspective of generalization. (a) Ideally, a model supports a wide range of datasets, but with inductive biases that provide high prior probability to a particular class of problems being considered. Here, the CNN is preferred over the linear model and the fully-connected
MLP for CIFAR-10 (while we do not consider MLP models to in general have poor inductive biases, here we are considering a hypothetical example involving images and a very large MLP). (b) By representing a large hypothesis space, a model can contract around a true solution, which in the real-world is often very sophisticated. (c) With truncated support, a model will converge to an erroneous solution. (d) Even if the hypothesis space contains the truth, a model will not efﬁciently contract unless it also has reasonable inductive biases. model. The evidence, or marginal likelihood, p(
, w)p(w)dw, is the probability we would generate a dataset if we were to randomly sample from the prior over functions p(f (x)) induced by a prior over parameters p(w). We deﬁne the support as the range of datasets for which
) > 0. We deﬁne the inductive biases as the relative prior probabilities of different datasets p(
— the distribution of support given by p(
). A similar schematic to Figure 2(a) was used by
MacKay [26] to understand an Occam’s razor effect in using the evidence for model selection; we believe it can also be used to reason about model construction and generalization.
D|M
D|M
D|M
D|M
) = p(
R
From this perspective, we want the support of the model to be large so that we can represent any hypothesis we believe to be possible, even if it is unlikely. We would even want the model to be able to represent pure noise, such as noisy CIFAR [51], as long as we honestly believe there is some non-zero, but potentially arbitrarily small, probability that the data are simply noise. Crucially, we also need the inductive biases to carefully represent which hypotheses we believe to be a priori likely for a particular problem class. If we are modelling images, then our model should have statistical properties, such as convolutional structure, which are good descriptions of images.
Figure 2(a) illustrates three models. We can imagine the blue curve as a simple linear function, f (x) = w0 + w1x, combined with a distribution over parameters p(w0, w1), e.g., (0, I), which induces a distribution over functions p(f (x)). Parameters we sample from our prior p(w0, w1) give rise to functions f (x) that correspond to straight lines with different slopes and intercepts. This model thus has truncated support: it cannot even represent a quadratic function. But because the
, this model assigns much mass to the datasets marginal likelihood must normalize over datasets it does support. The red curve could represent a large fully-connected MLP. This model is highly
ﬂexible, but distributes its support across datasets too evenly to be particularly compelling for many image datasets. The green curve could represent a convolutional neural network, which represents a compelling speciﬁcation of support and inductive biases for image recognition: this model is highly
ﬂexible, but it provides a particularly good support for structured problems.
N
D
With large support, we cast a wide enough net that the posterior can contract around the true solution to a given problem as in Figure 2(b), which in reality we often believe to be very sophisticated. On the other hand, the simple model will have a posterior that contracts around an erroneous solution if it is not contained in the hypothesis space as in Figure 2(c). Moreover, in Figure 2(d), the model has wide support, but does not contract around a good solution because its support is too evenly distributed.
Returning to the opening example, we can justify the high order polynomial by wanting large support.
But we would still have to carefully choose the prior on the coefﬁcients to induce a distribution over functions that would have reasonable inductive biases. Indeed, this Bayesian notion of generalization is not based on a single number, but is a two dimensional concept. From this probabilistic perspective, it is crucial not to conﬂate the ﬂexibility of a model with the complexity of a model class. Indeed
Gaussian processes with RBF kernels have large support, and are thus ﬂexible, but have inductive 2
biases towards very simple solutions. We also see that parameter counting has no signiﬁcance in this perspective of generalization: what matters is how a distribution over parameters combines with a functional form of a model, to induce a distribution over solutions.
In this paper we reason about Bayesian deep learning from a probabilistic perspective of gener-alization. The key distinguishing property of a Bayesian approach is marginalization instead of optimization, where we represent solutions given by all settings of parameters weighted by their posterior probabilities, rather than bet everything on a single setting of parameters. Neural networks are typically underspeciﬁed by the data, and can represent many different but high performing models corresponding to different settings of parameters, which is exactly when marginalization will make the biggest difference for accuracy and calibration. Moreover, we clarify that the recent deep ensem-bles [22] are not a competing approach to Bayesian inference, but can be viewed as a compelling mechanism for Bayesian marginalization. Indeed, we empirically demonstrate that deep ensembles can provide a better approximation to the Bayesian predictive distribution than standard Bayesian approaches. We propose MultiSWAG, a method inspired by deep ensembles, which marginalizes within basins of attraction — achieving improved performance, with a similar training time.
We then investigate the properties of priors over functions induced by priors over the weights of neural networks, showing that they have reasonable inductive biases, and connect these results to tempering. We also show that the mysterious generalization properties recently presented in Zhang et al. [51] can be understood by reasoning about prior distributions over functions, and are not speciﬁc to neural networks. Indeed, we show Gaussian processes can also perfectly ﬁt images with random labels, yet generalize on the noise-free problem. These results are a consequence of large support but reasonable inductive biases for common problem settings. We further show that while Bayesian neural networks can ﬁt the noisy datasets, the marginal likelihood has much better support for the noise free datasets, in line with Figure 2. We additionally show that the multimodal marginalization in
MultiSWAG alleviates double descent, so as to achieve monotonic improvements in performance with model ﬂexibility, in line with our perspective of generalization. MultiSWAG also provides signiﬁcant improvements in both accuracy and NLL over SGD training and unimodal marginalization.
We provide code at https://github.com/izmailovpavel/understandingbdl. 2