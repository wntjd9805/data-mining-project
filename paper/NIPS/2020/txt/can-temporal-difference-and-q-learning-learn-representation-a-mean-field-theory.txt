Abstract
Temporal-difference and Q-learning play a key role in deep reinforcement learning, where they are empowered by expressive nonlinear function approximators such as neural networks. At the core of their empirical successes is the learned feature representation, which embeds rich observations, e.g., images and texts, into the latent space that encodes semantic structures. Meanwhile, the evolution of such a feature representation is crucial to the convergence of temporal-difference and
Q-learning.
In particular, temporal-difference learning converges when the function approxi-mator is linear in a feature representation, which is ﬁxed throughout learning, and possibly diverges otherwise. We aim to answer the following questions:
When the function approximator is a neural network, how does the associated feature representation evolve? If it converges, does it converge to the optimal one?
We prove that, utilizing an overparameterized two-layer neural network, temporal-difference and Q-learning globally minimize the mean-squared projected Bellman error at a sublinear rate. Moreover, the associated feature representation converges to the optimal one, generalizing the previous analysis of [21] in the neural tan-gent kernel regime, where the associated feature representation stabilizes at the initial one. The key to our analysis is a mean-ﬁeld perspective, which connects the evolution of a ﬁnite-dimensional parameter to its limiting counterpart over an inﬁnite-dimensional Wasserstein space. Our analysis generalizes to soft Q-learning, which is further connected to policy gradient. 1

Introduction
Deep reinforcement learning achieves phenomenal empirical successes, especially in challenging applications where an agent acts upon rich observations, e.g., images and texts. Examples include video gaming [56], visuomotor manipulation [51], and language generation [39]. Such empirical successes are empowered by expressive nonlinear function approximators such as neural networks, which are used to parameterize both policies (actors) and value functions (critics) [46]. In particular, the neural network learned from interacting with the environment induces a data-dependent feature representation, which embeds rich observations into a latent space encoding semantic structures 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
[12, 40, 49, 75]. In contrast, classical reinforcement learning mostly relies on a handcrafted feature representation that is ﬁxed throughout learning [65].
In this paper, we study temporal-difference (TD) [64] and Q-learning [71], two of the most prominent algorithms in deep reinforcement learning, which are further connected to policy gradient [73] through its equivalence to soft Q-learning [37, 57, 58, 61]. In particular, we aim to characterize how an overparameterized two-layer neural network and its induced feature representation evolve in TD and Q-learning, especially their rate of convergence and global optimality. A fundamental obstacle, however, is that such an evolving feature representation possibly leads to the divergence of TD and
Q-learning. For example, TD converges when the value function approximator is linear in a feature representation, which is ﬁxed throughout learning, and possibly diverges otherwise [10, 18, 67].
To address such an issue of divergence, nonlinear gradient TD [15] explicitly linearizes the value function approximator locally at each iteration, that is, using its gradient with respect to the parameter as an evolving feature representation. Although nonlinear gradient TD converges, it is unclear whether the attained solution is globally optimal. On the other hand, when the value function approximator in
TD is an overparameterized multi-layer neural network, which is required to be properly scaled, such a feature representation stabilizes at the initial one [21], making the explicit local linearization in nonlinear gradient TD unnecessary. Moreover, the implicit local linearization enabled by overparame-terization allows TD (and Q-learning) to converge to the globally optimal solution. However, such a required scaling, also known as the neural tangent kernel (NTK) regime [43], effectively constrains the evolution of the induced feature presentation to an inﬁnitesimal neighborhood of the initial one, which is not data-dependent.
Contribution. Going beyond the NTK regime, we prove that, when the value function approximator is an overparameterized two-layer neural network, TD and Q-learning globally minimize the mean-squared projected Bellman error (MSPBE) at a sublinear rate. Moreover, in contrast to the NTK regime, the induced feature representation is able to deviate from the initial one and subsequently evolve into the globally optimal one, which corresponds to the global minimizer of the MSPBE. We further extend our analysis to soft Q-learning, which is connected to policy gradient.
The key to our analysis is a mean-ﬁeld perspective, which allows us to associate the evolution of a
ﬁnite-dimensional parameter with its limiting counterpart over an inﬁnite-dimensional Wasserstein space [4, 5, 68, 69]. Speciﬁcally, by exploiting the permutation invariance of the parameter, we associate the neural network and its induced feature representation with an empirical distribution, which, at the inﬁnite-width limit, further corresponds to a population distribution. The evolution of such a population distribution is characterized by a partial differential equation (PDE) known as the continuity equation. In particular, we develop a generalized notion of one-point monotonicity
[38], which is tailored to the Wasserstein space, especially the ﬁrst variation formula therein [5], to characterize the evolution of such a PDE solution, which, by a discretization argument, further quantiﬁes the evolution of the induced feature representation.