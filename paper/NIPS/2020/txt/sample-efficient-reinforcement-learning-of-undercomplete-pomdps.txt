Abstract
Partial observability is a common challenge in many reinforcement learning ap-plications, which requires an agent to maintain memory, infer latent states, and integrate this past information into exploration. This challenge leads to a number of computational and statistical hardness results for learning general Partially Observ-able Markov Decision Processes (POMDPs). This work shows that these hardness barriers do not preclude efﬁcient reinforcement learning for rich and interesting subclasses of POMDPs. In particular, we present a sample-efﬁcient algorithm,
OOM-UCB, for episodic ﬁnite undercomplete POMDPs, where the number of observations is larger than the number of latent states and where exploration is essential for learning, thus distinguishing our results from prior works. OOM-UCB achieves an optimal sample complexity of ˜O(1/ε2) for ﬁnding an ε-optimal policy, along with being polynomial in all other relevant quantities. As an interesting special case, we also provide a computationally and statistically efﬁcient algorithm for POMDPs with deterministic state transitions. 1

Introduction
In many sequential decision making settings, the agent lacks complete information about the underly-ing state of the system, a phenomenon known as partial observability. Partial observability signif-icantly complicates the tasks of reinforcement learning and planning, because the non-Markovian nature of the observations forces the agent to maintain memory and reason about beliefs of the system state, all while exploring to collect information about the environment. For example, a robot may not be able to perceive all objects in the environment due to occlusions, and it must reason about how these objects may move to avoid collisions [10]. Similar reasoning problems arise in imperfect information games [8], medical diagnosis [13], and elsewhere [25]. Furthermore, from a theoretical perspective, well-known complexity-theoretic results show that learning and planning in partially observable environments is statistically and computationally intractable in general [23, 22, 30, 21].
The standard formulation for reinforcement learning with partial observability is the Partially Ob-servable Markov Decision Process (POMDP), in which an agent operating on noisy observations makes decisions that inﬂuence the evolution of a latent state. The complexity barriers apply for this model, but they are of a worst case nature, and they do not preclude efﬁcient algorithms for interesting sub-classes of POMDPs. Thus we ask:
Can we develop efﬁcient algorithms for reinforcement learning in large classes of POMDPs? 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
This question has been studied in recent works [3, 12], which incorporate a decision making component into a long line of work on “spectral methods” for estimation in latent variable mod-els [14, 29, 2, 1], including the Hidden Markov Model. Brieﬂy, these estimation results are based on the method of moments, showing that under certain assumptions the model parameters can be computed by a decomposition of a low-degree moment tensor. The works of Azizzadenesheli et al. [3] and Guo et al. [12] use tensor decompositions in the POMDP setting and obtain sample efﬁciency guarantees. Neither result considers a setting where strategic exploration is essential for information acquisition, and they do not address one of the central challenges in more general reinforcement learning problems.
Our contributions.
In this work, we provide new sample-efﬁcient algorithms for reinforcement learning in ﬁnite POMDPs in the undercomplete regime, where the number of observations is larger than the number of latent states. This assumption is quite standard in the literature on estimation in latent variable models [1]. Our main algorithm OOM-UCB uses the principle of optimism for exploration and uses the information gathered to estimate the Observable Operators induced by the environment. Our main result proves that OOM-UCB ﬁnds a near optimal policy for the POMDP using a number of samples that scales polynomially with all relevant parameters and additionally with the minimum singular value of the emission matrix. Notably, OOM-UCB ﬁnds an ε-optimal policy at the optimal rate of ˜O(1/ε2).
While OOM-UCB is statistically efﬁcient for this subclass of POMDPs, we should not expect it to be computationally efﬁcient in general, as this would violate computational barriers for POMDPs.
However, in our second contribution, we consider a further restricted subclass of POMDPs in which the latent dynamics are deterministic and where we provide both a computationally and statistically efﬁcient algorithm. Notably, deterministic dynamics are still an interesting subclass due to that, while it avoids computational barriers, it still does not mitigate the need for strategic exploration. We prove that our second algorithm has sample complexity scaling with all the relevant parameters as well as the minimum (cid:96)2 distance between emission distributions. This latter quantity replaces the minimum singular value in the guarantee for OOM-UCB and is a more favorable dependency.
We provide further motivation for our assumptions with two lower bounds: the ﬁrst shows that the overcomplete setting is statistically intractable without additional assumptions, while the second necessitates the dependence on the minimum singular value of the emission matrix. In particular, under our assumptions, the agent must engage in strategic exploration for sample-efﬁciency. As such, the main conceptual advance in our line of inquiry over prior works is that our algorithms address exploration and partial observability in a provably efﬁcient manner. 1.1