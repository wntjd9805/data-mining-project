Abstract
Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32 × 32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score-based models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can scale score-based generative models to various image datasets, with diverse resolutions ranging from 64 × 64 to 256 × 256. Our score-based models can generate high-ﬁdelity samples that rival best-in-class GANs on various image datasets, including CelebA,
FFHQ, and several LSUN categories. 1

Introduction
Score-based generative models [1] represent probability distributions through score—a vector ﬁeld pointing in the direction where the likelihood of data increases most rapidly. Remarkably, these score functions can be learned from data without requiring adversarial optimization, and can produce realistic image samples that rival GANs on simple datasets such as CIFAR-10 [2].
Despite this success, existing score-based generative models only work on low resolution images (32 × 32) due to several limiting factors. First, the score function is learned via denoising score matching [3, 4, 5]. Intuitively, this means a neural network (named the score network) is trained to denoise images blurred with Gaussian noise. A key insight from [1] is to perturb the data using multiple noise scales so that the score network captures both coarse and ﬁne-grained image features.
However, it is an open question how these noise scales should be chosen. The recommended settings in [1] work well for 32 × 32 images, but perform poorly when the resolution gets higher. Second, samples are generated by running Langevin dynamics [6, 7]. This method starts from white noise and progressively denoises it into an image using the score network. This procedure, however, might fail or take an extremely long time to converge when used in high-dimensions and with a necessarily imperfect (learned) score network.
We propose a set of techniques to scale score-based generative models to high resolution images.
Based on a new theoretical analysis on a simpliﬁed mixture model, we provide a method to analytically compute an effective set of Gaussian noise scales from training data. Additionally, we propose an efﬁcient architecture to amortize the score estimation task across a large (possibly inﬁnite) number of noise scales with a single neural network. Based on a simpliﬁed analysis of the convergence properties of the underlying Langevin dynamics sampling procedure, we also derive a technique to approximately optimize its performance as a function of the noise scales. Combining these techniques with an exponential moving average (EMA) of model parameters, we are able to signiﬁcantly improve 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Generated samples on datasets of decreasing resolutions. From left to right: FFHQ 256 × 256, LSUN bedroom 128 × 128, LSUN tower 128 × 128, LSUN church_outdoor 96 × 96, and CelebA 64 × 64. the sample quality, and successfully scale to images of resolutions ranging from 64 × 64 to 256 × 256, which was previously impossible for score-based generative models. As illustrated in Fig. 1, the samples are sharp and diverse. 2