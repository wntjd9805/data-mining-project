Abstract
Methods for adversarial robustness certiﬁcation aim to provide an upper bound on the test error of a classiﬁer under adversarial manipulation of its input. Current certiﬁcation methods are computationally expensive and limited to attacks that optimize the manipulation with respect to a norm. We overcome these limitations by investigating the robustness properties of Nearest Prototype Classiﬁers (NPCs) like learning vector quantization and large margin nearest neighbor. For this purpose, we study the hypothesis margin. We prove that if NPCs use a dissimilarity measure induced by a seminorm, the hypothesis margin is a tight lower bound on the size of adversarial attacks and can be calculated in constant time—this provides the ﬁrst adversarial robustness certiﬁcate calculable in reasonable time. Finally, we show that each NPC trained by a triplet loss maximizes the hypothesis margin and is therefore optimized for adversarial robustness. In the presented evaluation, we demonstrate that NPCs optimized for adversarial robustness are competitive with state-of-the-art methods and set a new benchmark with respect to computational complexity for robustness certiﬁcation. 1

Introduction
Adversarial robustness of a classiﬁer describes its stability in classiﬁcation under adversarial manipu-lations of the input. The adversarial setting has been studied extensively in numerous settings [1–3] but mainly found footing after the seminal paper by Szegedy et al. [4] that formalized the problem of adversarial examples. Since that, a wide line of research has sprung concerning both the construction of adversarial attacks [5–7] and the heuristic defense against them [8–10]. Unfortunately, while some progress has been made [11–13], most proposed defenses have been shown to be breakable by more advanced attacks [14–16] so that the adversarial robustness problem is far from being solved. With the heuristic defenses at the losing side of the metaphorical arms race, provable robustness guarantees for classiﬁers provide a welcome alternative [11, 12, 17]. Robustness guarantees aim to provide the so-called robust test error (or an upper bound) of a classiﬁer under adversarial attacks and are therefore not dependent on the current state of adversarial attacks. However, current methods for determining the robust test error are computationally expensive [12, 17, 18] and are often limited to Lp-norm evaluations [11, 13, 18]. As an adverse effect, we cannot use them for the rejection of adversarial examples regarding an arbitrary seminorm without a huge computational overhead.
∗Authors contributed equally. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To tackle these limitations, we extend the study of robustness guarantees beyond Neural Networks (NNs), on which most work has focused, by investigating Nearest Prototype Classiﬁers (NPCs) [19– 22]. For that, given a data space X , an NPC is deﬁned by a set W of prototypes selected from the data space X and a dissimilarity measure d : X × X → R≥0. Each prototype w ∈ W has a predeﬁned and ﬁxed class label c (w) ∈ C = {1, 2, . . . , Nc}. The class assignment c∗
W (x) ∈ C of a data point x ∈ X is determined by the class label of the closest prototype w∗ (1-nearest neighbor rule):
W (x) = c (w∗) with w∗ = arg min c∗ w∈W
{d (x, w)}. (1)
Due to the fundamental interpretability of the prototypes and dissimilarity measure used, NPCs are considered to be among the most interpretable machine learning models. This makes NPCs a preferred choice in the medical ﬁeld, where the interpretability of models is a requirement for clinical trials [23, 24]. As a result, prototype-based principles have been adopted in a number of deep learning
ﬁelds. Amongst the most notable of those is few-shot learning [25]. In addition to this, an empirical study has shown that NPCs are robust against adversarial attacks [26]. In summary—with the call for interpretable machine learning models increasing, the NPC principles adopted in deep learning, and the promising empirical robustness results—NPCs provide a worthwhile avenue for studying guaranteed adversarial robustness.
Contributions We analyze the adversarial robustness properties of NPCs in terms of the hypothesis margin. First, we show that if the dissimilarity measure d (x, w) is induced by a seminorm (cid:107)·(cid:107) (i. e., d (x, w) = (cid:107)x − w(cid:107)), the hypothesis margin regarding this seminorm can be computed in constant time during the inference—this even holds in the case of an uncountable set of prototypes. Second, we prove that this margin is a tight lower bound on the magnitude of an adversarial attack measured by the same seminorm—to the best of our knowledge, this presents the ﬁrst robustness guarantee that holds for an arbitrary seminorm. Third, using this result, we show that every NPC that classiﬁes by a seminorm and is trained by minimizing a triplet loss is inherently optimizing the adversarial robustness with respect to this seminorm. In an experimental section, we present how these results apply to different NPCs and that a violation of the assumptions (seminorm and triplet loss) does not necessarily lead to adversarially robust methods. The experimental results highlight that the derived robustness certiﬁcate is comparable with other methods in terms of guaranteed robustness but outperforms them all when computation speed is considered.
The following section discusses related work. After that, we give a brief introduction to NPCs and deﬁne the models we use in the evaluation. This section is followed by the main part where we deﬁne the hypothesis margin and investigate the relation to adversarial robustness. The subsequent experimental section shows how to apply these results and compares the derived robustness certiﬁcate to other methods. Finally, we ﬁnish with a discussion and an outlook of the presented results. 2