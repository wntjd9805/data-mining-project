Abstract
We study the dynamics of optimization and the generalization properties of one-hidden layer neural networks with quadratic activation function in the over-parametrized regime where the layer width m is larger than the input dimension d.
We consider a teacher-student scenario where the teacher has the same structure as the student with a hidden layer of smaller width m∗ ≤ m. We describe how the empirical loss landscape is affected by the number n of data samples and the width m∗ of the teacher network. In particular we determine how the probability that there be no spurious minima on the empirical loss depends on n, d, and m∗, thereby establishing conditions under which the neural network can in principle recover the teacher. We also show that under the same conditions gradient descent dynamics on the empirical loss converges and leads to small generalization error, i.e. it enables recovery in practice. Finally we characterize the time-convergence rate of gradient descent in the limit of a large number of samples. These results are conﬁrmed by numerical experiments. 1

Introduction
Neural networks are a key component of the machine learning toolbox. Still the reasons behind their success remain mysterious from a theoretical prospective. While sufﬁciently large neural networks can in principle represent a large class of functions, we do not yet understand under what conditions their parameters can be adjusted in an algorithmically tractable way for that purpose. For example, under worst case assumptions, some functions cannot be tractably learned with neural networks [1, 2].
We also know that there exist settings with adversarial initializations where neural networks fail in generalization to new samples, while the same setting from random initial conditions succeeds [3].
And yet, in many practical settings, neural networks are trained successfully even with simple local algorithm such as gradient descent (GD) or stochastic gradient descent (SGD).
The problem of learning the parameters of a neural network is two-fold. First, we want that their training on a set of data via minimization of a suitable loss function succeed in ﬁnding a set of parameters for which the value of the loss is close to its global minimum. Second, and more importantly, we want that such a set of parameters also generalizes well to unseen data. Theoretical guarantees have been obtained in many settings by a geometrical analysis of the loss showing that only global minima are present, see e.g. [4, 5]. In particular it has been shown that network over-parametrization can be beneﬁcial and lead to landscapes without spurious minima in which GD or
SGD converge [6–10]. However, over-parametrized neural networks successfully optimized on a training set do not necessarily generalize well – for example neural networks can achieve zero errors
† Université Paris-Saclay, CNRS, CEA, Institut de physique théorique, 91191, Gif-sur-Yvette,France.
‡ Courant Institute, New York University, 251 Mercer Street, New York, New York 10012, USA.
§ SPOC laboratory, EPFL, Switzerland.
∗ Work done while visiting at Courant Institute. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
in training without learning any rule [11]. It is therefore important to understand when zero training loss implies good generalization.
It is know empirically that deep neural networks can learn functions that can be represented with a much smaller (sometimes even shallow) neural network [12–14], but that learning the smaller network without ﬁrst learning the larger one is computationally harder [6]. Our work provides a theoretical justiﬁcation for this empirical observation by providing an explicit and rigorously analyzable case where this happens.
Main contributions:
In this work we investigate the issues of training and generalization in the context of a teacher-student set-up. We assume that both the teacher and the student are one-hidden layer neural network with quadratic activation function and quadratic loss. We focus on the over-parametrized or over-realizable case where the hidden layer of the teacher m∗ is smaller than that of the student m. We assume that the hidden layer of the student m is larger than the dimensionality d, m > d, in that case:
• We show that the value of the empirical loss is zero on all of its minimizers, but that the set of minimizers does not reduce to the singleton containing only the teacher network in general.
• We derive a critical value αc = m∗ + 1 of the number of samples n per dimension d above which the set of minimizers of the empirical loss has a positive probability to reduce to the singleton containing only the teacher network in the limit n, d → ∞ with n/d ≥ αc— i.e. we derive a sample complexity threshold above which the minimizer can have good generalization properties. The formula is proven for a teacher with a single hidden unit m∗ = 1 (a.k.a. phase retrieval).
• We study gradient descent ﬂow on the empirical loss starting from random initialization and show that it converges to a network that can achieve perfect generalization above this sample complexity threshold αc.
• We quantify the nonasymptotic convergence rate of gradient descent in the limit of large number of samples and show that the loss is bounded from above at all times by C1/(1+C2t) for some constants C1, C2 > 0. We also evaluate the asymptotic convergence rate and identify two different regimes according to the input dimension and the number of hidden units, showing that in one case the loss converges as O(t−2) as t → ∞ while in the second case it converges exponentially.
• We show how the string method can be used to probe the empirical loss landscape and ﬁnd minimum energy paths on this landscape connecting the initial weights of the student to those of the teacher, possibly going through ﬂat portion or above energy barrier. This allows one to probe features of this landscape not accessible by standard GD.
In Sec. 2 we formally deﬁne the problem and derive some key properties that we use in the rest of the paper. In Sec. 3 we analyze the training and the generalization losses from the geometrical prospective, and derive the formula for the sample complexity threshold. In Sec. 4 we show that gradient descent ﬂow can ﬁnd good minima for datasets above this sample complexity threshold, and we characterize its convergence rate. In Sec. 6 we present our results using the string method to probe the loss landscape. Finally in the appendix we give the proofs and some additional numerical results.
√