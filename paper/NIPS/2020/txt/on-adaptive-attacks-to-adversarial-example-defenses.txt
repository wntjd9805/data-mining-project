Abstract
Adaptive attacks have (rightfully) become the de facto standard for evaluating de-fenses to adversarial examples. We ﬁnd, however, that typical adaptive evaluations are incomplete. We demonstrate that thirteen defenses recently published at ICLR,
ICML and NeurIPS—and which illustrate a diverse set of defense strategies—can be circumvented despite attempting to perform evaluations using adaptive attacks.
While prior evaluation papers focused mainly on the end result—showing that a defense was ineffective—this paper focuses on laying out the methodology and the approach necessary to perform an adaptive attack. Some of our attack strategies are generalizable, but no single strategy would have been sufﬁcient for all defenses.
This underlines our key message that adaptive attacks cannot be automated and always require careful and appropriate tuning to a given defense. We hope that these analyses will serve as guidance on how to properly perform adaptive attacks against defenses to adversarial examples, and thus will allow the community to make further progress in building more robust models. 1

Introduction
Over the last ﬁve years the research community has attempted to develop defenses to adversarial examples [SZS+14, BCM+13]. This has proven extraordinarily difﬁcult. Indeed, a common theme has been proposing defenses that—due to having been tested only against static and relatively weak attacks—were promptly circumvented by a stronger attack [CW17a, ACW18].
Recent community efforts and guidelines to improve defense evaluations have had a noticeably positive impact. In particular, there has been a signiﬁcant uptake of evaluations against adaptive attacks, i.e., attacks that were speciﬁcally designed to target a given defense—the ratio of defenses evaluated against adaptive attacks has increased from close to zero in 2017 [CW17a] to one third in 2018 [ACW18] and to nearly all of them today.1 This leads to the question:
With their much-improved evaluation practices, are these defenses truly robust?
We ﬁnd that this is not the case. Speciﬁcally, in an analysis of thirteen defenses, selected from recent
ICLR, ICML, and NeurIPS conferences to illustrate diverse defensive strategies, we ﬁnd that we can circumvent all of them and substantially reduce the accuracy from what was originally claimed.
∗Equal contribution 1There is a similarly positive trend in terms of releasing source code for defenses. In particular, every defense we analyzed either released source code, or made it available upon request. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Importantly, while almost all of these defenses performed an evaluation involving adaptive attacks, these evaluations ended up not being sufﬁcient. For example, it is common for papers to repurpose existing “adaptive” attacks that circumvented some prior defense, without considering how to change it to target the new defense. We suspect that this shortcoming might have been caused, in part, by the fact that prior work on circumventing defenses typically shows only the ﬁnal, successful attack, without describing the methodology that was used to come up with this attack and thus leaving open questions such as “How was the attack discovered?” or “What other attacks were unsuccessful?”.
To remedy this problem, instead of merely demonstrating that the thirteen defenses we studied can be circumvented by stronger attacks, we actually walk the reader through our full process of analyzing each defense, from an initial paper read-through, to our hypotheses about what would be required for the defense to be circumvented, to an ultimately successful attack. This approach lets us more clearly document the many steps involved in developing a strong adaptive attack.
The goal of our analyzes is not to reduce analyzed model’s accuracy all the way to 0%.2 Instead, we want to demonstrate that the existing adaptive attack evaluation methodology has shortcomings, and that stronger adaptive attacks can (at least partially) degrade each defense’s accuracy from what is reported in the original evaluations.
Whereas prior work often needed to develop new techniques [ACW18] to evade defenses, we ﬁnd that the technical tools to evaluate defenses properly already exist. A better attack can be built using only tools that are well-known in the literature. Thus, the issue with current defense evaluations is methodological rather than technical.
After describing our methodology (Section 3) and providing an overview of common themes distilled from our evaluations (Section 4), we give a summary of the evaluation of each defense (Section 5), leaving the full evaluation for the appendix due to space constraints. We state and test our initial hypotheses—gathered from reading the paper and source code—as to why the original evaluation may have been insufﬁcient. Finally, we describe how the observations made throughout our evaluation inform the design of a ﬁnal adaptive attack that succeeds in circumventing the defense.
An overarching theme of our evaluations is simplicity. Attacks need not be complicated, even when the defense is. There are often only a small number of important components in complex defenses— carefully targeting these can lead to simpler and stronger attacks. We design our loss functions, the cornerstone of successful adaptive attacks, so that they are easy to optimize and consistent—so that higher loss values result in strictly stronger attacks. While some of our techniques are generalizable, no single attack strategy would have sufﬁced for all defenses. This underlines the crucial fact that adaptive attacks cannot be automated and always require appropriate tuning to a each defense.
In the spirit of responsible disclosure, we contacted all the authors of the defenses we evaluated prior to this submission, and offered to share code or adversarial examples to enable independent veriﬁcation of our claims. In all but one case (where the defense authors did not respond to our original message), the authors acknowledged our attacks’ effectiveness. To promote reproducibility and encourage others to perform independent re-evaluations of proposed defenses, we release code for all of our attacks at https://github.com/wielandbrendel/adaptive_attacks_paper. 2