Abstract
We study the problem of inferring communication structures that can solve coop-erative multi-agent planning problems while minimizing the amount of commu-nication. We quantify the amount of communication as the maximum degree of the communication graph; this metric captures settings where agents have limited bandwidth. Minimizing communication is challenging due to the combinatorial na-ture of both the decision space and the objective; for instance, we cannot solve this problem by training neural networks using gradient descent. We propose a novel algorithm that synthesizes a control policy that combines a programmatic com-munication policy used to generate the communication graph with a transformer policy network used to choose actions. Our algorithm ﬁrst trains the transformer policy, which implicitly generates a “soft” communication graph; then, it synthe-sizes a programmatic communication policy that “hardens” this graph, forming a neurosymbolic transformer. Our experiments demonstrate how our approach can synthesize policies that generate low-degree communication graphs while maintaining near-optimal performance. 1

Introduction
Many real-world robotics systems are distributed, with teams of agents needing to coordinate to share information and solve problems. Reinforcement learning has recently been demonstrated as a promising approach to automatically solve such multi-agent planning problems [28, 16, 8, 18, 9, 13].
A key challenge in (cooperative) multi-agent planning is how to coordinate with other agents, both deciding whom to communicate with and what information to share. One approach is to let agents communicate with all other agents; however, letting agents communicate arbitrarily can lead to poor generalization [12, 21]; furthermore, it cannot account for physical constraints such as limited bandwidth. A second approach is to manually impose a communication graph on the agents, typically based on distance [12, 29, 21, 26]. However, this manual structure may not reﬂect the optimal communication structure—for instance, one agent may prefer to communicate with another one that is farther away but in its desired path. A third approach is to use a transformer [31] as the policy network [4], which uses attention to choose which other agents to focus on. However, since the attention is soft, each agent still communicates with every other agent.
∗Equal contribution. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We study the problem of learning a communication policy that solves a multi-agent planning task while minimizing the amount of communication required. We measure the amount of communication on a given step as the maximum degree (in both directions) of the communication graph on that step; this metric captures the maximum amount of communication any single agent must perform at that step. While we focus on this metric, our approach easily extends to handling other metrics—e.g., the total number of edges in the communication graph, the maximum in-degree, and the maximum out-degree, as well as general combinations of these metrics.
A key question is how to represent the communication policy; in particular, it must be sufﬁciently expressive to capture communication structures that both achieve high reward and has low com-munication degree, while simultaneously being easy to train. Neural network policies can likely capture good communication structures, but they are hard to train since the maximum degree of the communication graph is a discrete objective that cannot be optimized using gradient descent.
An alternative is to use a structured model such as a decision tree [3] or rule list [34] and train using combinatorial optimization. However, these models perform poorly since choosing whom to communicate with requires reasoning over sets of other agents—e.g., to avoid collisions, an agent must communicate with its nearest neighbor in its direction of travel.
We propose to use programs to represent communication policies. In contrast to rule lists, our programmatic polices include components such as ﬁlter and map that operate over sets of inputs.
Furthermore, programmatic policies are discrete in nature, making them amenable to combinatorial optimization; in particular, we can compute a programmatic policy that minimizes the communication graph degree using a stochastic synthesis algorithm [25] based on MCMC sampling [19, 10].
A key aspect of our programs is that they can include a random choice operator. Intuitively, random choice is a key ingredient needed to minimize the communication graph degree without global coordination. For example, suppose there are two groups of agents, and each agent in group A needs to communicate with an agent in group B, but the speciﬁc one does not matter. Using a deterministic communication policy, since the same policy is shared among all agents, each agent in group A might choose to communicate with the same agent j in group B (e.g., if agents in the same group have similar states). Then, agent j will have a very high degree in the communication graph, which is undesirable. In contrast, having each agent in group A communicate with a uniformly random agent in group B provides a near-optimal solution to this problem, without requiring the agents to explicitly coordinate their decisions.
While we can minimize the communication graph degree using stochastic search, we still need to choose actions based on the communicated information. Thus, we propose a learning algorithm that integrates our programmatic communication policy with a transformer policy for selecting actions.
We refer to the combination of the transformer and the programmatic communication policy as a neurosymbolic transformer. This algorithm learns the two policies jointly. At a high level, our algorithm ﬁrst trains a transformer policy for solving the multi-agent task; as described above, the soft attention weights capture the extent to which an edge in the communication graph is useful. Next, our algorithm trains a programmatic communication policy that optimizes both goals: (i) match the transformer as closely as possible, and (ii) minimize the maximum degree of the communication graph at each step. In contrast to the transformer policy, this communication policy makes hard decisions about which other agents to communicate with. Finally, our algorithm re-trains the weights of the transformer policy, except where the (hard) attention weights are chosen by the communication policy.
We evaluate our approach on several multi-agent planning tasks that require agents to coordinate to achieve their goals. We demonstrate that our algorithm learns communication policies that achieve task performance similar to the original transformer policy (i.e., where each agent communicates with every other agent), while signiﬁcantly reducing the amount of communication. Our results demonstrate that our algorithm is a promising approach for training policies for multi-agent systems that additionally optimize combinatorial properties of the communication graph 2
Example. Consider the example in Figure 1, where agents in group 1 (blue) are navigating from the left to their goal on the right, while agents in group 2 (red) are navigating from the right to their goal on the left. In this example, agents have noisy observations of the positions of other agents (e.g., 2The code and a video illustrating the different tasks are available at https://github.com/jinala/ multi-agent-neurosym-transformers. 2
Figure 1: Left: Two groups of agents (red vs. blue) at their initial positions (circles) trying to reach their goal positions (crosses). Agents must communicate both within group and across groups to choose a collision free path to take (solid line shows a path for a single agent in each group). Middle:
The soft attention weights of the transformer policy computed by the agent along the y-axis for the message received from the agent along the x-axis for the initial step. Right: The hard attentions learned by the programmatic communication policy to imitate the transformer. based on cameras or LIDAR); however, they do not have access to internal information such as their planned trajectories or even their goals. Thus, to solve this task, each agent must communicate both with its closest neighbor in the same group (to avoid colliding with them), as well as with any agent in the opposite group (to coordinate so their trajectories do not collide). The communication graph of the transformer policy (in terms of soft attention weights) is shown in Figure 1 (middle); every agent needs to communicate with all other agents. The programmatic communication policy synthesized by our algorithm is argmax(map(−di,j, ﬁlter(θi,j ≥ −1.85, l))), random(ﬁlter(di,j ≥ 3.41, l)).
Agent i uses this program to choose two other agents j from the list of agents (cid:96) from whom to request information; di,j is the distance between them and θi,j is the angle between them. The ﬁrst rule chooses the nearest agent j (besides itself) such that θi,j ∈ [−1.85, π], and the second chooses a random agent in the other group. The communication graph is visualized in Figure 1 (right).