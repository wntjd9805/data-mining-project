Abstract
In recent years, graph neural networks (GNNs) have become the de facto tool for performing machine learning tasks on graphs. Most GNNs belong to the family of message passing neural networks (MPNNs). These models employ an iterative neighborhood aggregation scheme to update vertex representations. Then, to compute vector representations of graphs, they aggregate the representations of the vertices using some permutation invariant function. One would expect the hidden layers of a GNN to be composed of parameters that take the form of graphs. However, this is not the case for MPNNs since their update procedure is parameterized by fully-connected layers. In this paper, we propose a more intuitive and transparent architecture for graph-structured data, so-called Random Walk
Graph Neural Network (RWNN). The ﬁrst layer of the model consists of a number of trainable “hidden graphs” which are compared against the input graphs using a random walk kernel to produce graph representations. These representations are then passed on to a fully-connected neural network which produces the output. The employed random walk kernel is differentiable, and therefore, the proposed model is end-to-end trainable. We demonstrate the model’s transparency on synthetic datasets. Furthermore, we empirically evaluate the model on graph classiﬁcation datasets and show that it achieves competitive performance. 1

Introduction
In recent years, graphs have become a very useful abstraction for representing a wide variety of real-world datasets. Graphs are ubiquitous in several application domains, such as in social networks, in bioinformatics, and in information networks. Due to this abundance of graph-structured data, machine learning on graphs has recently emerged as a very important task with applications ranging from drug design [18] to modeling physical systems [3].
In the past years, graph neural networks (GNNs) have attracted considerable attention in the machine learning community. These models offer a powerful tool for performing machine learning on graphs.
Although numerous GNN variants have been recently proposed [35, 24, 23, 48, 46, 44], most of them share the same basic idea, and can be reformulated into a single common framework, so-called message passing neural networks (MPNNs) [13]. These models employ a message passing procedure to aggregate local information of vertices. For graph-related tasks, MPNNs usually apply some permutation invariant readout function to the vertex representations to produce a representation for the entire graph. Common readout functions treat each graph as a set of vertex representations, thus ignoring the interactions between the vertices. These interactions are implicitly encoded into the learned vertex representations produced by the message passing procedure. However, due to the lack of transparency brought by the combination of graph structure with feature information, it is not clear whether the information of interest is encoded into these representations.
Before the advent of deep learning, graph kernels had established themselves as the standard approach for performing machine learning tasks on graphs [33, 22]. A graph kernel is a symmetric positive semideﬁnite function deﬁned on the space of graphs. These methods enable the application of kernel 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
methods such as the SVM classiﬁer to graphs, and have achieved remarkable results in several classiﬁcation tasks. However, nowadays, they have been largely overshadowed by GNNs. This is mainly due to the complexity of kernel methods, but also because data representation (produced by graph kernels) and learning (performed by SVM) are independent from each other. On the other hand, neural network models can learn representations that are useful for the task at hand. In contrast to graph kernels, which directly compare graphs to each other, MPNNs ﬁrst transform graphs into vectors by aggregating the representations of their vertices, and then some function is applied to these graph representations (i. e., modeled by a multi-layer perceptron). Furthermore, it is usually hard to interpret and understand what these models have learned. Ideally, we would like to have a model that applies directly some function to the input graphs without ﬁrst transforming them into vectors.
In this paper, we propose such an architecture, called Random Walk Graph Neural Network (RWNN).
The model contains a number of trainable “hidden graphs”, and it compares the input graphs against these graphs using a random walk kernel. The kernel values are then passed on to a fully-connected neural network which produces the output. The employed random walk kernel is differentiable, and we can thus update the “hidden graphs” during training with backpropagation. Hence, the proposed neural network is end-to-end trainable. Furthermore, it delivers the “best of both worlds” from graph kernels and neural networks, i. e., it retains the ﬂexibility of kernel methods which can be easily applied to structured data (e. g., graphs), and also learns task-dependent features without the need for feature engineering. We compare the performance of the proposed model to state-of-the-art graph kernels and recently-proposed neural architectures on several benchmark datasets for graph classiﬁcation. Results show that our model matches or outperforms competing methods. Our main contributions are summarized as follows:
• We propose a novel neural network model, Random Walk Graph Neural Network, which employs a random walk kernel to produce graph representations. Importantly, the model is highly interpretable since it contains a set of trainable graphs.
• We develop an efﬁcient computation scheme to reduce the time and space complexity of the proposed model.
• We demonstrate the model’s high transparency on synthetic datasets, and evaluate its perfor-mance on several graph classiﬁcation datasets where it achieves performance comparable to state-of-the-art GNNs and graph kernels.
The rest of this paper is organized as follows. Section 2 provides an overview of the related work.
Section 3 introduces some preliminary concepts. Section 4 provides a detailed description of the proposed model. Section 5 evaluates the proposed model both in terms of its transparency and of its performance. Finally, Section 6 concludes. 2