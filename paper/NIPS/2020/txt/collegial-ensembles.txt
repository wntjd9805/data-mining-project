Abstract
Modern neural network performance typically improves as model size increases. A recent line of research on the Neural Tangent Kernel (NTK) of over-parameterized networks indicates that the improvement with size increase is a product of a better conditioned loss landscape. In this work, we investigate a form of over-parameterization achieved through ensembling, where we deﬁne collegial en-sembles (CE) as the aggregation of multiple independent models with identical architectures, trained as a single model. We show that the optimization dynamics of CE simplify dramatically when the number of models in the ensemble is large, resembling the dynamics of wide models, yet scale much more favorably. We use recent theoretical results on the ﬁnite width corrections of the NTK to perform efﬁcient architecture search in a space of ﬁnite width CE that aims to either mini-mize capacity, or maximize trainability under a set of constraints. The resulting ensembles can be efﬁciently implemented in practical architectures using group convolutions and block diagonal layers. Finally, we show how our framework can be used to analytically derive optimal group convolution modules originally found using expensive grid searches, without having to train a single model. 1

Introduction
Neural networks exhibit generalization behavior in the over-parameterized regime, a phenomenon that has been well observed in practice [23, 2, 18, 17]. Recent theoretical advancements have been made to try and understand the trainability and generalization properties of over-parameterized neural networks, by observing their nearly convex behaviour at large width [13, 15]. For a wide neural network F(x) with parameters θ and a convex loss L, the param-eter updates −µ∇θL can be represented in the space of functions as kernel gradient decent (GD) updates −µ∇F L, with the Neural
Tangent Kernel [10] (NTK) function K(x, xj) = ∇θF(x)∇(cid:62)
θ F(xj) operating on x, xj: input n1 n2
... nL n1
· · · n1 n2 n2
· · ·
... m paths ... nL
· · · nL
∆θ = −µ∇θL −→ ∆F(x) ∼ −µ (cid:88) j
K(x, xj)∇F (xj )L (1)
Σ
× 1√ m
In the inﬁnite width limit, the NTK remains constant during training, and GD reduces to kernel GD, rendering the optimization a convex problem. Hence, over parameterized models in the “large width” sense both generalize, and are simpler to optimize. In this work, we consider a different type of over-parameterization achieved through ensembling. We denote by collegial ensembles (CE) models where the output, either intermediate or
ﬁnal, is constructed from the aggregation of multiple identical pathways (see illustration in Fig. 1).
We show that the training dynamics of CE simplify when the ensemble multiplicity is large, in a similar sense as wide models, yet at a much cheaper cost in terms of parameter count. Our results
Figure 1: Collegial Ensemble output 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
indicate the existence of a “sweet spot” for the correct ratio between width and multiplicity, where
”close to convex” behaviour does not come at the expense of size. To ﬁnd said “sweet-spot”, we rely on recent ﬁndings on ﬁnite corrections of gradients [7, 16], though we use them in a more general manner than their original presentation. Speciﬁcally, we use the following assumption stated informally:
Assumption 1. (Informal) Denote by K the NTK at initialization of a fully connected ANN with hidden layer widths n1, ..., nL and depth L. There exists positive constants α, C such that:
V ar(K) ∼ C(eα (cid:80)L l=1 n−1 l − 1) (2) where the variance is measured on the individual entries of K, with respect to the random sampling of the weights.
In [7] and [16], assumption 1 is proven for the on-diagonal entries of K, in fully connected ar-chitectures. In this work, we assume it holds in a more general sense for practical architectures, with different constants of α, C (we defer the reader to Sec. 4 in the appendix for further empirical validation of this assumption). Since V ar(K) diminishes with width, we hypothesize that a small width neutral network behaves closer to its large width counterpart as V ar(K) decreases. Notably, similar observations using activations and gradient variance as predictors of successful initializations were presented in [6, 8]. Motivated by this hypothesis, we formulate a primal-dual constrained optimization problem that aims to ﬁnd an optimal CE with respect to the following objectives: 1. Primal (optimally smooth): minimize V ar(K) for a ﬁxed number of parameters. 2. Dual (optimally compact): minimize number of parameters for a ﬁxed V ar(K).
The primal formulation seeks to ﬁnd a CE which mimics the simpliﬁed dynamics of a wide model using a ﬁxed budget of parameters, while Fthe dual formulation seeks to minimize the number of parameters without suffering the optimization and performance consequences typically found in the
”narrow regime”. Using both formulations, we ﬁnd excellent agreement between our theoretical predictions and empirical results, on both small and large scale models.
Our main contributions are the following: 1. We adapt the popular over-parameterization analysis to collegial ensembles (CE), in which the output units of multiple architecturally identical models are aggregated, scaled, and trained as a single model. For ensembles with m models each of width n, we show that under gradient ﬂow and the L2 loss, the NTK remains close to its initial value up to a
O(cid:0)(mn)−1(cid:1) correction. 2. We formulate two optimization problems that seek to ﬁnd optimal ensembles given a baseline architecture, in the primal and dual form, respectively. The optimally smooth ensemble achieves higher accuracy than the baseline, using the same budget of parameters.
The optimally compact ensemble achieves a similar performance as the baseline, with signiﬁcantly fewer trainable parameters. 3. We show how optimal grouping in ResNeXt [20] architectures can be derived and improved upon using our framework, without the need for an expensive search over hyper-parameters.
The remaining paper is structured as follows. In Sec. 2 we formally deﬁne collegial ensembles, and present our results for their training dynamics in the large m, n regime. In Sec. 3 we present our framework for architecture search in the space of collegial ensembles, and in Sec. 4 we present further experimental results on the CIFAR-10/CIFAR-100 [11] and ImageNet [4] datasets using large scale models. 2 Collegial Ensembles
We dedicate this section to formally deﬁne collegial ensembles, and analyze their properties from the
NTK perspective. Speciﬁcally, we would like to formalize the notion of the ”large ensemble” regime, where its dynamic behaviour is reminiscent of wide single models. In the following analysis we consider simple feed forward fully connected neural network Fn(x, θ) : Rn0 → R, where the width 2
(a) (b)
Figure 2: Convergence of the ensemble NTK to the NMK when increasing the number of models in the ensemble. N T K[x0, xγ] is computed for both the diagonal and off-diagonal elements xγ =
[cos(γ), sin(γ)] for γ ∈ [−π, π]. The smoother surface as m increases in (a) demonstrates the convergence of the NTK. The black line in (b), computed with ×100 wider model, shows that the convergence is indeed to the NMK, and the NTK mean does not depend on the width of the model.
Each model in the ensemble is fully connected with L = 4 layers and n = 1000. of the hidden layers are given by n = {nl}L
+, adopting the common NTK parameterization:
Fn(x, θ) = (cid:114) 1 nL l=1 ∈ ZL (cid:114) 2 n1
θL(...φ(
θ2φ( (cid:114) 2 n0
θ1x))). (3) where x ∈ Rn0 is the input, φ(·) is the ReLU activation function, θl is the weight matrix as-sociated with layer l, and θ denotes the concatenation of all the weights of all layers, which are initialized iid from a standard normal distribution. Given a dataset X = {xi}N i=1, the empirical NTK denoted by Kn(θ) ∈ RN ×N is given by Kn(θ) = ∇θFn(θ)∇(cid:62)
θ Fn(θ), where
Fn(θ) = [Fn(x1, θ), ..., Fn(xN , θ)](cid:62). Given the network Fn, we parameterize a space of ensemble members F e(Θ) by a multiplicity parameter 1 ≤ m and a width vector n, such that:
F e(Θ) = 1
√ m m (cid:88) j=1
Fn(θj), Ke(Θ) = 1 m m (cid:88) j=1
Kn(θj) (4) 1 ...θ(cid:62) m](cid:62) is the concatenation of the weights of all the ensembles, and Ke(Θ) = where Θ = [θ(cid:62)
ΘF e is the NTK of the ensemble. Plainly speaking, the network Fn deﬁnes a space of
∇ΘF e∇(cid:62) ensembles given by the scaled sum of m neural networks of the same Fn architecture, with weights initialized independently from a normal distribution. Since each model Fn(θj) in the ensemble is architecturally equivalent to Kn(θ), it is easy to show that the inﬁnite width kernel is equal for both models: K∞ = limmin(n)→∞ Kn(θ) = limmin(n)→∞ Ke(Θ). We deﬁne the Neural Mean Kernel (NMK) K∞ n as the mean of the empirical NTK: n = E
θ
K∞
[Kn(θ)]]. (5)
The NMK is deﬁned by an expectation over the normally distributed weights, and does not immedi-ately equal the inﬁnite width limit of the NTK given by K∞. The following Lemma stems from the application of the strong law of large numbers (LLN):
Lemma 1 (Inﬁnite ensemble). The following holds:
Ke(Θ) a.s−→ K∞ n as m → ∞. (6)
We defer the reader to Sec. 5 in the appendix for the full proof. While both K∞ n and K∞ do not depend on the weights, they are deﬁned differently. K∞ n is deﬁned by an expectation over the weights, and depends on the width of the architecture, whereas K∞ is deﬁned by an inﬁnite width limit.
However, empirical observation using Monte Carlo sampling, as presented in Fig. 2, show little to no dependence of the NMK on the widths n. Moreover, we empirically observe that K∞ n ∼ K∞ (Note that similar observations have been reported in [10]). We next show that under gradient ﬂow, Ke(Θ) remains close to its initial value for the duration of the optimization process when mn is large. Given 3
the labels vector y ∈ RN and the L2 cost function at time t, Lt = 1 2, under gradient
ﬂow with learning rate µ, the weights evolve over continuous time according to ˙Θt = −µ∇(cid:62)
Lt.
Θt
The following theorem gives an asymptotic bound on the leading correction of the ensemble NTK over time. For simplicity, we state our result for constant width networks.
Theorem 1 (NTK evolution over time). Assuming analytic activation functions φ(·) with bounded derivatives of all orders, and the L2 cost function , it holds for any ﬁnite t: 2 (cid:107)F e(Θt) − y(cid:107)2
Ke(Θt) − Ke(Θ0) ∼ Op( 1 mn
) (7) where the notation xn = Op(yn) states that xn/yn is stochastically bounded.
We defer the reader to Sec. 5 in the appendix for the full proof, as well as empirical validation of
The. 1. Large collegial ensembles therefore refer to a regime where mn is large. In the case of inﬁnite multiplicity, optimization dynamics reduces to kernel gradient descent with K∞ n , rather than K∞ as the relevant kernel function. A striking implication arises from Theorem 1. The total parameter count in the ensemble is linear in m, and quadratic in n, hence it is much more parameter efﬁcient to increase m rather than n. Since the ”large” regime depends on both n and m, CE possess an inherent degree of ﬂexibility in their practical design. As we show in the following section, this increased
ﬂexibility allows the design of both parameter efﬁcient ensembles, and better performing ensembles, when compared with the baseline model. 3 Efﬁcient Ensembles
In this section, we use Proposition 1 to derive optimally smooth and compact ensembles. We parameterize the space of ensembles using m, n, and a baseline architecture F˜n, where ˜n is the width vector of the baseline model. Denote by β(n) the total parameter count in Fn, we deﬁne parameter efﬁciency ρ(m, n) by the ratio between the parameter count in the baseline model βs (cid:44) β(˜n), and the parameter count in the ensemble given by βe (cid:44) mβ(n):
ρ(m, n) (cid:44) βs
βe
=
βs mβ(n)
.
Using Proposition. 1, the variance of Kn as a function of widths n and depth L, is given by:
V ar(cid:0)Kn(θ)(cid:1) ∼ (eα (cid:80)L l=1 n−1 l − 1). (8) (9) for some value of α.
Primal formulation: We cast the primal objective as an optimization problem, where we would like to ﬁnd parameters mp, np that correspond to the smoothest ensemble: mp, np = arg min
V ar (cid:16) (cid:17)
Ke(Θ) s.t ρ(m, n) = 1. m,n (10)
Since the weights for each model are sampled independently, it holds that:
V ar(cid:0)Ke(Θ)(cid:1) = m (cid:88) j=1 1 m2 V ar(cid:0)Kn(θj)(cid:1) = (eα (cid:80)L l=1 n−1 l − 1) m
. (11)
Equating the parameter count in both models to maintain a ﬁxed efﬁciency, we can derive for each n the number of the models mp(n) in the primal formulation: mp(n) =
βs
β(n)
−→ np = arg min n (cid:104) (eα (cid:80)L l=1 n−1 l − 1) mp(n) (cid:105)
. (12)
The optimal parameters np can be found using a grid search. 4
(a) Primal formulation (b) Dual formulation
Figure 3: Primal and dual objective curves for a baseline feedforward fully connected network with L = 6 layers, ˜n = 500, and n0 = 748. (a) The minimizer of the primal objective (red) is achieved for n = 48 and m(48) ≈ 30. (b) The maximizer of the dual objective (red) is achieved for n = 48 and m(48) ≈ 12, achieving an efﬁciency value ρ(48) ≈ 2.45.
Dual formulation: The dual formulation can be cast as an optimization problem, with the following objective: md, nd = arg max m,n
ρ(m, n) s.t V ar(cid:0)Ke(Θ)(cid:1) = V ar(cid:0)K˜n(θ)(cid:1). (13)
Matching the smoothness criterion using Eq. 11, we can derive for each n the number of models md(n) in the dual formulation: (eα (cid:80)L (eα (cid:80)L
−→ nd = arg max 1 md(n)
βs
β(n) md(n) = l − 1) l − 1) l=1 n−1 l=1 ˜n−1 (14) (cid:105) (cid:104) n
.
Ideally, we can ﬁnd md, nd such that the total parameter count in the ensemble is considerably reduced. Equating the solutions for both the primal and dual problems in Eq. 12 and Eq. 14, it is straightforward to verify that nd = np, implying strong duality. Therefore, the primal and dual solutions differ only in the optimal multiplicity m(n) of the ensemble. Both objectives are plotted in
Fig. 3 using a feedforward fully connected network baseline with L = 6 and constant width ˜n = 500.
Note that the efﬁcient ensembles framework outlined in this section can readily be applied with different efﬁciency metrics. For instance, instead of using the parameter efﬁciency, one could consider the FLOPs efﬁciency (see Appendix Sec. 3). 4 Experiments
In the following section we conduct experiments to both validate our assumptions, and evaluate our framework for efﬁcient ensemble search. Starting with a toy example, we evaluate the effect of
V ar(K) and βe on test performance using fully connected models trained on the MNIST dataset. For the latter experiments, we move to larger scale models trained on CIFAR-10/100 and the ImageNet [4] datasets. 4.1 Ablation Study – MNIST
An effective way to improve the performance of a neural network is to increase its size. Recent slim architectures, such as ResNeXt, demonstrate it possible to maintain accuracy while signiﬁcantly reducing parameter count. In Fig. 4 we provide further empirical evidence that capacity of a network by itself is not a good predictor of performance, when decoupled from other factors.
Speciﬁcally, we demonstrate strong correlation between the empirical test error and the variance
V ar(K), while βe is kept constant (primal). On the other hand, increasing βe while keeping V ar(K) constant (dual) does not improve the performance. For both experiments we use as a baseline a fully connected model with L = 6 layers and width ˜n = 200 for each layer. The width of a layer for each of the m models in the ensemble is n. Each ensemble was trained on MNIST for 70 epochs with the
Adam optimizer, and the accuracy was averaged over 100 repetitions. 5
(a) Primal (b) Dual (c) Primal (d) Dual
Figure 4: Decoupling capacity and variance. The error (blue) is highly correlated with V ar(K), and less sensitive to βe. (a) and (b) show the theoretical variance of the model correlates well with accuracy. (c) and (d) show the corresponding number of parameters βe. Decreasing the variance (a) improves performance when βe is ﬁxed (c). Increasing βe signiﬁcantly (d) without reducing the variance (b) can cause degradation in performance due to overﬁtting. 4.2 Aggregated Residual Transformations
ResNeXt [20] introduces the concept of aggregated residual transformations utilizing group convolu-tions, which achieves better parameter/capacity trade off than its ResNet counterpart. In this paper, we view ResNeXt as a special case of CE, where the ensembling happens at the block level. We hypothesize that the optimal blocks identiﬁed with CE will lead to an overall better model when stacked up, and by doing so we get the beneﬁt of factorizing the design space of a network to modular levels. See Algorithm 2 for the detailed recipe.
For these experiments, we use both the CIFAR-10/100 and the ImageNet datasets following the implementation details described in [20]. We also re-port results on ImageNet64×64 and ImageNet32×32, datasets introduced in [3] that preserve the number of samples and classes of the original ImageNet-1K [4], but downsample the image resolutions to 64×64 and 32×32 respectively (see Appendix Sec. 1).
Fitting α to a ResNet block. The ﬁrst step in the optimization required for both the primal and dual objectives, is to approximate the α parameter in Eq. 9.
For convolutional layers, the coefﬁcient multiplying α becomes (cid:80)L l=1 fan-in−1 where fan-inl is the fan-in of layer l. Following Algorithm 1, we approximate the α corresponding to a ResNet block parametrized by n = [n, n](cid:62) as depicted in Fig. 5. We compute a Monte Carlo estimate of the second moment of one diagonal entry of the NTK matrix for increasing width n ∈ 1, 256 and ﬁxed (cid:74) nin=nout=256. For simplicity, we ﬁt the second moment normalized by the squared ﬁrst moment, given by eα (cid:80)L l=1 fan-in−1
, which can easily be ﬁtted with a
ﬁrst degree polynomial when considering its natural logarithm. We ﬁnd α ≈ 1.60 and show the ﬁtted second moment in Appendix Fig. 2. (cid:75) l l nin-d in 1×1, n 3×3, n 1×1, nout
+ nout-d out
Figure 5 4.2.1 CIFAR-10/100
Primal formulation. As a baseline architecture, we use a 1×128d ResNet, following the notations of [20] section 5.1. Following Algorithm 2, we compute m(n) for n ∈ and ﬁnd the optimum np = 10 and mp ≈ 37, after adjusting mp to match the number of parameters of the baseline and account for rounding errors and different block topology approximations. As can be seen in Table 1a, the model achieving the primal optimum, 37×10d, attains better test error on CIFAR-10/100 than the
ResNeXt baseline 3×64d at a similar parameter budget. We also report results for a wider baseline 8×64d from [20] and show similar trends. The test error for multiple models sitting on the primal curve is depicted in Fig. 6a for CIFAR-100 and Appendix Fig. 1 for CIFAR-10. Test errors are averaged over the last 10 epochs over 10 runs. 1, 128 (cid:74) (cid:75)
Dual formulation. Using the same ResNet base block as for the primal experiment, thus using the same ﬁtted α, we compute the optimal nd and md maximizing the parameter efﬁciency curve ρ and
ﬁnd the same n as the primal, nd = 10, and md ≈ 10. The resulting ResNeXt network has 3.3 times fewer parameters than the baseline and achieves similar or slightly degraded performance on 6
CIFAR-10/100 as shown in Table 1b. The efﬁciency curve ρ depicted in red in Fig. 6b is constructed using a single ResNet block topology and with non integer numbers for m as described above. Thus it only approximates the real parameter efﬁciency, explaining why some models in the close vicinity of the optimum have a slightly higher real efﬁciency as can be seen in Table 1b. The test error for multiple models sitting on the dual curve is depicted in Fig. 6b for CIFAR-100 and Appendix Fig. 1 for CIFAR-10. 4.2.2 ImageNet
Primal formulation. Following [20], we use ResNet-50 and ResNet-101 as baselines and report results in Table 2a. Our ResNet-50 based optimal model, 12×10d, obtains slightly better top-1 and top-5 errors than the baseline 32×4d reported in [20]. This is quite remarkable given that [20] converged to this architecture via an expensive grid search. Our ResNet-101 based optimal model achieves a signiﬁcantly better top-1 and top-5 error than the ResNet-101 baseline, and a slightly higher top-1 and top-5 error than the ResNeXt baseline 32×4d.
Dual formulation. Using ResNet-50 and ResNet-101 as baselines, we ﬁnd models that achieve similar top-1 and top-5 errors with signiﬁcantly less parameters. Detailed results can be found in
Table 2b.
Implementation details. We follow [20] for the implementation details of ResNet-50, ResNet-101 and their ResNeXt counterparts. We use SGD with 0.9 momentum and a batch size of 256 on 8 GPUs (32 per GPU). The weight decay is 0.0001 and the initial learning rate 0.1. We train the models for 100 epochs and divide the learning rate by a factor of 10 at epoch 30, 60 and 90. We use the same data normalization and augmentations as in [20] except for lighting that we do not use. 5