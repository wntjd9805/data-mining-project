Abstract
While deep learning and deep reinforcement learning (RL) systems have demon-strated impressive results in domains such as image classiﬁcation, game playing, and robotic control, data efﬁciency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efﬁcient learning. However, the multi-task setting presents a number of optimization challenges, making it difﬁcult to realize large efﬁciency gains com-pared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradi-ent surgery that projects a task’s gradient onto the normal plane of the gradient of any other task that has a conﬂicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efﬁciency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance. 1

Introduction
While deep learning and deep reinforcement learning (RL) have shown considerable promise in enabling systems to learn complex tasks, the data requirements of current methods make it difﬁcult to learn a breadth of capabilities, particularly when all tasks are learned individually from scratch. A natural approach to such multi-task learning problems is to train a network on all tasks jointly, with the aim of discovering shared structure across the tasks in a way that achieves greater efﬁciency and performance than solving tasks individually. However, learning multiple tasks all at once results is a difﬁcult optimization problem, sometimes leading to worse overall performance and data efﬁciency compared to learning tasks individually [42, 50]. These optimization challenges are so prevalent that multiple multi-task RL algorithms have considered using independent training as a subroutine of the algorithm before distilling the independent models into a multi-tasking model [32, 42, 50, 21, 56], producing a multi-task model but losing out on the efﬁciency gains over independent training. If we could tackle the optimization challenges of multi-task learning effectively, we may be able to actually realize the hypothesized beneﬁts of multi-task learning without the cost in ﬁnal performance.
While there has been a signiﬁcant amount of research in multi-task learning [6, 49], the optimization challenges are not well understood. Prior work has described varying learning speeds of different tasks [8, 26] and plateaus in the optimization landscape [52] as potential causes, whereas a range of other works have focused on the model architecture [40, 33]. In this work, we instead hypothesize that one of the main optimization issues in multi-task learning arises from gradients from different tasks conﬂicting with one another in a way that is detrimental to making progress. We deﬁne two gradients to be conﬂicting if they point away from one another, i.e., have a negative cosine similarity.
We hypothesize that such conﬂict is detrimental when a) conﬂicting gradients coincide with b) high positive curvature and c) a large difference in gradient magnitudes. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
As an illustrative example, consider the 2D optimization landscapes of two task objectives in Figure 1a-c. The opti-mization landscape of each task con-sists of a deep valley, a property that has been observed in neural network optimization landscapes [22], and the bottom of each valley is characterized by high positive curvature and large differences in the task gradient mag-nitudes. Under such circumstances, the multi-task gradient is dominated by one task gradient, which comes at the cost of degrading the performance of the other task. Further, due to high curvature, the improvement in the dominating task may be overestimated, while the degradation in performance of the non-dominating task may be underestimated. As a result, the optimizer struggles to make progress on the optimization objective. In Figure 1d), the optimizer reaches the deep valley of task 1, but is unable to traverse the valley in a parameter setting where there are conﬂicting gradients, high curvature, and a large difference in gradient magnitudes (see gradients plotted in Fig. 1d). In
Section 5.3, we ﬁnd experimentally that this tragic triad also occurs in a higher-dimensional neural network multi-task learning problem.
Figure 1: Visualization of PCGrad on a 2D multi-task optimiza-tion problem. (a) A multi-task objective landscape. (b) & (c)
Contour plots of the individual task objectives that comprise (a). (d) Trajectory of gradient updates on the multi-task objective using the Adam optimizer. The gradient vectors of the two tasks at the end of the trajectory are indicated by blue and red arrows, where the relative lengths are on a log scale.(e) Trajectory of gradient updates on the multi-task objective using Adam with PCGrad. For (d) and (e), the optimization trajectory goes from black to yellow.
The core contribution of this work is a method for mitigating gradient interference by altering the gradients directly, i.e. by performing “gradient surgery.” If two gradients are conﬂicting, we alter the gradients by projecting each onto the normal plane of the other, preventing the interfering components of the gradient from being applied to the network. We refer to this particular form of gradient surgery as projecting conﬂicting gradients (PCGrad). PCGrad is model-agnostic, requiring only a single modiﬁcation to the application of gradients. Hence, it is easy to apply to a range of problem settings, including multi-task supervised learning and multi-task reinforcement learning, and can also be readily combined with other multi-task learning approaches, such as those that modify the architecture. We theoretically prove the local conditions under which PCGrad improves upon standard multi-task gradient descent, and we empirically evaluate PCGrad on a variety of challenging problems, including multi-task CIFAR classiﬁcation, multi-objective scene understanding, a challenging multi-task RL domain, and goal-conditioned RL. Across the board, we ﬁnd PCGrad leads to substantial improvements in terms of data efﬁciency, optimization speed, and ﬁnal performance compared to prior approaches, including a more than 30% absolute improvement in multi-task reinforcement learning problems. Further, on multi-task supervised learning tasks, PCGrad can be successfully combined with prior state-of-the-art methods for multi-task learning for even greater performance. 2 Multi-Task Learning with PCGrad
While the multi-task problem can in principle be solved by simply applying a standard single-task algorithm with a suitable task identiﬁer provided to the model, or a simple multi-head or multi-output model, a number of prior works [42, 50, 53] have found this learning problem to be difﬁcult. In this section, we introduce notation, identify possible causes for the difﬁculty of multi-task optimization, propose a simple and general approach to mitigate it, and theoretically analyze the proposed approach. 2.1 Preliminaries: Problem and Notation
The goal of multi-task learning is to ﬁnd parameters θ of a model fθ that achieve high average performance across all the training tasks drawn from a distribution of tasks p(T ). More formally, we
ETi∼p(T ) [Li(θ)], where Li is a loss function for the i-th task Ti that aim to solve the problem: min we want to minimize. For a set of tasks, {Ti}, we denote the multi-task loss as L(θ) = (cid:80) i Li(θ), and the gradients of each task as gi = ∇Li(θ) for a particular θ. (We drop the reliance on θ in the notation for brevity.) To obtain a model that solves a speciﬁc task from the task distribution p(T ), we deﬁne a task-conditioned model fθ(y|x, zi), with input x, output y, and encoding zi for task Ti, which could be provided as a one-hot vector or in any other form.
θ 2
2.2 The Tragic Triad: Conﬂicting Gradients, Dominating Gradients, High Curvature
We hypothesize that a key optimization issue in multi-task learning arises from conﬂicting gradients, where gradients for different tasks point away from one another as measured by a negative inner product. However, conﬂicting gradients are not detrimental on their own. Indeed, simply averaging task gradients should provide the correct solution to descend the multi-task objective. However, there are conditions under which such conﬂicting gradients lead to signiﬁcantly degraded performance.
Consider a two-task optimization problem. If the gradient of one task is much larger in magnitude than the other, it will dominate the average gradient. If there is also high positive curvature along the directions of the task gradients, then the improvement in performance from the dominating task may be signiﬁcantly overestimated, while the degradation in performance from the dominated task may be signiﬁcantly underestimated. Hence, we can characterize the co-occurrence of three conditions as follows: (a) when gradients from multiple tasks are in conﬂict with one another (b) when the difference in gradient magnitudes is large, leading to some task gradients dominating others, and (c) when there is high curvature in the multi-task optimization landscape. We formally deﬁne the three conditions below.
Deﬁnition 1. We deﬁne φij as the angle between two task gradients gi and gj. We deﬁne the gradients as conﬂicting when cos φij < 0.
Deﬁnition 2. We deﬁne the gradient magnitude similarity between two gradients gi and gj as
Φ(gi, gj) = 2(cid:107)gi(cid:107)2(cid:107)gj (cid:107)2 2+(cid:107)gj (cid:107)2 2 (cid:107)gi(cid:107)2
.
When the magnitude of two gradients is the same, this value is equal to 1. As the gradient magnitudes become increasingly different, this value goes to zero.
Deﬁnition 3. We deﬁne multi-task curvature as H(L; θ, θ(cid:48)) = (cid:82) 1 0 ∇L(θ)T ∇2L(θ + a(θ(cid:48) −
θ))∇L(θ)da, which is the averaged curvature of L between θ and θ(cid:48) in the direction of the multi-task gradient ∇L(θ).
When H(L; θ, θ(cid:48)) > C for some large positive constant C, for model parameters θ and θ(cid:48) at the current and next iteration, we characterize the optimization landscape as having high curvature.
We aim to study the tragic triad and observe the presence of the three conditions through two examples.
First, consider the two-dimensional optimization landscape illustrated in Fig. 1a, where the landscape for each task objective corresponds to a deep and curved valley with large curvatures (Fig. 1b and 1c).
The optima of this multi-task objective correspond to where the two valleys meet. More details on the optimization landscape are in Appendix D. Particular points of this optimization landscape exhibit the three described conditions, and we observe that, the Adam [30] optimizer stalls precisely at one of these points (see Fig. 1d), preventing it from reaching an optimum. This provides some empirical evidence for our hypothesis. Our experiments in Section 5.3 further suggest that this phenomenon occurs in multi-task learning with deep networks. Motivated by these observations, we develop an algorithm that aims to alleviate the optimization challenges caused by conﬂicting gradients, dominating gradients, and high curvature, which we describe next. 2.3 PCGrad: Project Conﬂicting Gradients
Our goal is to break one condition of the tragic triad by directly altering the gradients themselves to prevent conﬂict. In this section, we outline our approach for altering the gradients. In the next section, we will theoretically show that de-conﬂicting gradients can beneﬁt multi-task learning when dominating gradients and high curvatures are present.
To be maximally effective and widely applicable, we aim to alter the gradients in a way that allows for positive interactions between the task gradients and does not introduce assumptions on the form of the model. Hence, when gradients do not conﬂict, we do not change the gradients. When gradients do conﬂict, the goal of PCGrad is to modify the gradients for each task so as to minimize negative conﬂict with other task gradients, which will in turn mitigate under- and over-estimation problems arising from high curvature.
To deconﬂict gradients during optimization, PCGrad adopts a simple procedure: if the gradients between two tasks are in conﬂict, i.e. their cosine similarity is negative, we project the gradient of each task onto the normal plane of the gradient of the other task. This amounts to removing the conﬂicting component of the gradient for the task, thereby reducing the amount of destruc-tive gradient interference between tasks. A pictorial description of this idea is shown in Fig. 2. 3
Algorithm 1 PCGrad Update Rule
Require: Model parameters θ, task minibatch B =
{Tk} 1: gk ← ∇θLk(θ) ∀k 2: gPC k ← gk ∀k 3: for Ti ∈ B do for Tj 4: 5: 6: uniformly
∼ B \ Ti in random order do
· gj < 0 then if gPC i
// Subtract the projection of gPC i − gPC
Set gPC i = gPC i ·gj (cid:107)gj (cid:107)2 gj 7: 8: return update ∆θ = gPC = (cid:80) i gPC i i onto gj
Figure 2: Conﬂicting gradients and PCGrad. In (a), tasks i and j have conﬂicting gradient directions, which can lead to destructive interference. In (b) and (c), we illustrate the PCGrad algorithm in the case where gradi-ents are conﬂicting. PCGrad projects task i’s gradient onto the normal vector of task j’s gradient, and vice versa. Non-conﬂicting task gradients (d) are not altered under PCGrad, allowing for constructive interaction.
Suppose the gradient for task Ti is gi, and the gradient for task Tj is gj. PCGrad proceeds as follows: (1) First, it determines whether gi conﬂicts with gj by computing the cosine similarity between vectors gi and gj, where negative values indicate conﬂicting gradients. (2) If the cosine similarity is negative, we replace gi by its projection onto the normal plane of gj: gi = gi − gi·gj (cid:107)gj (cid:107)2 gj. If the gradients are not in conﬂict, i.e. cosine similarity is non-negative, the original gradient gi remains unaltered. (3) PCGrad repeats this process across all of the other tasks sampled in random order from the current batch Tj ∀ j (cid:54)= i, resulting in the gradient gPC that is applied for task Ti. We perform i the same procedure for all tasks in the batch to obtain their respective gradients. The full update procedure is described in Algorithm 1 and a discussion on using a random task order is included in
Appendix H.
This procedure, while simple to implement, ensures that the gradients that we apply for each task per batch interfere minimally with the other tasks in the batch, mitigating the conﬂicting gradient problem, producing a variant on standard ﬁrst-order gradient descent in the multi-objective setting.
In practice, PCGrad can be combined with any gradient-based optimizer, including commonly used methods such as SGD with momentum and Adam [30], by simply passing the computed update to the respective optimizer instead of the original gradient. Our experimental results verify the hypothesis that this procedure reduces the problem of conﬂicting gradients, and ﬁnd that, as a result, learning progress is substantially improved. 2.4 Theoretical Analysis of PCGrad
In this section, we theoretically analyze the performance of PCGrad with two tasks:
Deﬁnition 4. Consider two task loss functions L1 : Rn → R and L2 : Rn → R. We deﬁne the two-task learning objective as L(θ) = L1(θ) + L2(θ) for all θ ∈ Rn, where g1 = ∇L1(θ), g2 = ∇L2(θ), and g = g1 + g2.
We ﬁrst aim to verify that the PCGrad update corresponds to a sensible optimization procedure under simplifying assumptions. We analyze convergence of PCGrad in the convex setting, under standard assumptions in Theorem 1. For additional analysis on convergence, including the non-convex setting, with more than two tasks, and with momentum-based optimizers, see Appendices A.1 and A.4
Theorem 1. Assume L1 and L2 are convex and differentiable. Suppose the gradient of L is L-Lipschitz with L > 0. Then, the PCGrad update rule with step size t ≤ 1
L will converge to either (1) a location in the optimization landscape where cos(φ12) = −1 or (2) the optimal value L(θ∗).
Proof. See Appendix A.1.
Theorem 1 states that application of the PCGrad update in the two-task setting with a convex and
Lipschitz multi-task loss function L leads to convergence to either the minimizer of L or a potentially sub-optimal objective value. A sub-optimal solution occurs when the cosine similarity between the gradients of the two tasks is exactly −1, i.e. the gradients directly conﬂict, leading to zero gradient after applying PCGrad. However, in practice, since we are using SGD, which is a noisy estimate of the true batch gradients, the cosine similarity between the gradients of two tasks in a minibatch is unlikely to be −1, thus avoiding this scenario. Note that, in theory, convergence may be slow if cos(φ12) hovers near −1. However, we don’t observe this in practice, as seen in the objective-wise learning curves in Appendix B. 4
Now that we have checked the sensibility of PCGrad, we aim to understand how PCGrad relates to the three conditions in the tragic triad. In particular, we derive sufﬁcient conditions under which
PCGrad achieves lower loss after one update. Here, we still analyze the two task setting, but no longer assume convexity of the loss functions.
Deﬁnition 5. We deﬁne the multi-task curvature bounding measure ξ(g1, g2) = (1 − cos2 φ12) (cid:107)g1−g2(cid:107)2 2 (cid:107)g1+g2(cid:107)2 2
.
With the above deﬁnition, we present our next theorem:
Theorem 2. Suppose L is differentiable and the gradient of L is Lipschitz continuous with constant
L > 0. Let θMT and θPCGrad be the parameters after applying one update to θ with g and PCGrad-modiﬁed gradient gPC respectively, with step size t > 0. Moreover, assume H(L; θ, θMT) ≥ (cid:96)(cid:107)g(cid:107)2 2 for some constant (cid:96) ≤ L, i.e. the multi-task curvature is lower-bounded. Then L(θPCGrad) ≤ L(θMT) if (a) cos φ12 ≤ −Φ(g1, g2), (b) (cid:96) ≥ ξ(g1, g2)L, and (c) t ≥ 2 (cid:96)−ξ(g1,g2)L .
Proof. See Appendix A.2.
Intuitively, Theorem 2 implies that PCGrad achieves lower loss value after a single gradient update compared to standard gradient descent in multi-task learning when (i) the angle between task gradients is not too small, i.e. the two tasks need to conﬂict sufﬁciently (condition (a)), (ii) the difference in magnitude needs to be sufﬁciently large (condition (a)), (iii) the curvature of the multi-task gradient should be large (condition (b)), (iv) and the learning rate should be big enough so that large curvature would lead to overestimation of performance improvement on the dominating task and underestimation of performance degradation on the dominated task (condition (c)). These ﬁrst three points (i-iii) correspond to exactly the triad of conditions outlined in Section 2.2, while the latter condition (iv) is desirable as we hope to learn quickly. We empirically validate that the ﬁrst three points, (i-iii), are frequently met in a neural network multi-task learning problem in Figure 4 in
Section 5.3. For additional analysis, including complete sufﬁcient and necessary conditions for the
PCGrad update to outperform the vanilla multi-task gradient, see Appendix A.3. 3 PCGrad in Practice
We use PCGrad in supervised learning and reinforcement learning problems with multiple tasks or goals. Here, we discuss the practical application of PCGrad to those settings.
In multi-task supervised learning, each task Ti ∼ p(T ) has a corresponding training dataset Di consisting of labeled training examples, i.e. Di = {(x, y)n}. The objective for each task in this supervised setting is then deﬁned as Li(θ) = E(x,y)∼Di [− log fθ(y|x, zi)], where zi is a one-hot encoding of task Ti. At each training step, we randomly sample a batch of data points B from the whole dataset (cid:83) i Di and then group the sampled data with the same task encoding into small batches denoted as Bi for each Ti represented in B. We denote the set of tasks appearing in B as BT . After sampling, we precompute the gradient of each task in BT as ∇θLi(θ) = E(x,y)∼Bi [−∇θ log fθ(y|x, zi)] .
Given the set of precomputed gradients ∇θLi(θ), we also precompute the cosine similarity between all pairs of the gradients in the set. Using the pre-computed gradients and their similarities, we can obtain the PCGrad update by following Algorithm 1, without re-computing task gradients nor backpropagating into the network. Since the PCGrad procedure is only modifying the gradients of shared parameters in the optimization step, it is model-agnostic and can be applied to any architecture with shared parameters. We empirically validate PCGrad with multiple architectures in Section 5.
For multi-task RL and goal-conditioned RL, PCGrad can be readily applied to policy gradient methods by directly updating the computed policy gradient of each task, following Algorithm 1, analogous to the supervised learning setting. For actor-critic algorithms, it is also straightforward to apply PCGrad: we simply replace task gradients for both the actor and the critic by their gradients computed via
PCGrad. For more details on the practical implementation for RL, see Appendix C. 4