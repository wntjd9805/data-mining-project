Abstract
A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given one distribution for smiling face images, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We showcase the breadth of unique capabilities of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image. 1

Introduction
Humans are able to rapidly learn new concepts and continuously integrate them among prior knowl-edge. The core component in enabling this is the ability to compose increasingly complex concepts out of simpler ones as well as recombining and reusing concepts in novel ways [5]. By combining a
ﬁnite number of primitive components, humans can create an exponential number of new concepts, and use them to rapidly explain current and past experiences [16]. We are interested in enabling such capabilities in machine learning systems, particularly in the context of generative modeling.
Past efforts have attempted to enable compositionality in several ways. One approach decomposes data into disentangled factors of variation and situate each datapoint in the resulting - typically continuous - factor vector space [29, 9]. The factors can either be explicitly provided or learned in an unsupervised manner. In both cases, however, the dimensionality of the factor vector space is
ﬁxed and deﬁned prior to training. This makes it difﬁcult to introduce new factors of variation, which may be necessary to explain new data, or to taxonomize past data in new ways. Another approach to incorporate the compositionality is to spatially decompose an image into a collection of objects, each object slot occupying some pixels of the image deﬁned by a segmentation mask [28, 6]. Such approaches can generate visual scenes with multiple objects, but may have difﬁculty in generating interactions between objects. These two incorporations of compositionality are considered distinct, with very different underlying implementations.
In this work∗, we propose to implement the compositionality via energy based models (EBMs).
Instead of an explicit vector of factors that is input to a generator function, or object slots that are blended to form an image, our uniﬁed treatment deﬁnes factors of variation and object slots via energy functions. Each factor is represented by an individual scalar energy function that takes as input an image and outputs a low energy value if the factor is exhibited in the image. Images that exhibit the
∗Code available compositional-generation-inference/ data and at https://energy-based-model.github.io/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Illustration of logical composition operators over energy functions E1 and E2 (drawn as level sets where red = valid areas of samples, grey = invalid areas of samples). factor can then be generated implicitly through an Markov Chain Monte Carlo (MCMC) sampling process that minimizes the energy. Importantly, it is also possible to run MCMC process on some combination of energy functions to generate images that exhibit multiple factors or multiple objects, in a globally coherent manner.
There are several ways to combine energy functions. One can add or multiply distributions as in mixtures [25, 6] or products [11] of experts. We view these as probabilistic instances of logical operators over concepts. Instead of using only one, we consider three operators: logical conjunction, disjunction, and negation (illustrated in Figure 1). We can then ﬂexibly and recursively combine multiple energy functions via these operators. More complex operators (such as implication) can be formed out of our base operators.
EBMs with such composition operations enable a breadth of new capabilities - among them is a unique approach to continual learning. Our formulation deﬁnes concepts or factors implicitly via examples, rather than pre-declaring an explicit latent space ahead of time. For example, we can create an EBM for concept "black hair" from a dataset of face images that share this concept. New concepts (or factors), such as hair color can be learned by simply adding a new energy function and can then be combined with energies for previously trained concepts. This process can repeat continually. This view of few-shot concept learning and generation is similar to work of [23], with the distinction that instead of learning to generate holistic images from few examples, we learn factors from examples, which can be composed with other factors. A related advantage is that ﬁnely controllable image generation can be achieved by specifying the desired image via a collection of logical clauses, with applications to neural scene rendering [4].
Our contributions are as follows: ﬁrst, while composition of energy-based models has been proposed in abstract settings before [11], we show that it can be used to generate plausible natural images.
Second, we propose a principled approach to combine independent trained energy models based on logical operators which can be chained recursively, allowing controllable generation based on a collection of logical clauses at test time. Third, by being able to recursively combine independent models, we show our approach allows us to extrapolate to new concept combinations, continually incorporate new visual concepts for generation, and infer concept properties compositionally. 2