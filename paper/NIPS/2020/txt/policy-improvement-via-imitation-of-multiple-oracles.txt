Abstract
Despite its promise, reinforcement learning’s real-world adoption has been ham-pered by the need for costly exploration to learn a good policy. Imitation learning (IL) mitigates this shortcoming by using an oracle policy during training as a boot-strap to accelerate the learning process. However, in many practical situations, the learner has access to multiple suboptimal oracles, which may provide conﬂicting advice in a state. The existing IL literature provides a limited treatment of such sce-narios. Whereas in the single-oracle case, the return of the oracle’s policy provides an obvious benchmark for the learner to compete against, neither such a benchmark nor principled ways of outperforming it are known for the multi-oracle setting. In this paper, we propose the state-wise maximum of the oracle policies’ values as a natural baseline to resolve conﬂicting advice from multiple oracles. Using a reduc-tion of policy optimization to online learning, we introduce a novel IL algorithm
MAMBA, which can provably learn a policy competitive with this benchmark. In particular, MAMBA optimizes policies by using a gradient estimator in the style of generalized advantage estimation (GAE). Our theoretical analysis shows that this design makes MAMBA robust and enables it to outperform the oracle policies by a larger margin than the IL state of the art, even in the single-oracle case. In an evaluation against standard policy gradient with GAE and AggreVaTe(D), we showcase MAMBA’s ability to leverage demonstrations both from a single and from multiple weak oracles, and signiﬁcantly speed up policy optimization. 1

Introduction
Reinforcement learning (RL) promises to bring self-improving decision-making capability to many applications, including robotics [1], computer systems [2], recommender systems [3] and user interfaces [4]. However, deploying RL in any of these domains is fraught with numerous difﬁculties, as vanilla RL agents need to do a large amount of trial-and-error exploration before discovering good decision policies [5]. This inefﬁciency has motivated investigations into training RL agents with domain knowledge, an example of which is having access to oracle policies in the training phase.
The broad class of approaches that attempt to mimic or improve upon an available oracle policy is known as imitation learning (IL) [6]. Generally, IL algorithms work by invoking oracle policy demonstrations to guide an RL agent towards promising states and actions. As a result, oracle-level performance can be achieved without global exploration, thus avoiding RL’s main source of high sample complexity. For IL with a single oracle policy, the oracle policy’s return provides a natural benchmark for the agent to match or outperform. Most existing IL techniques assume this single-oracle setting, with a good but possibly suboptimal oracle policy. Behavior cloning [7] learns a policy from a ﬁxed batch of trajectories in a supervised way by treating oracle actions as labels. Inverse reinforcement learning uses recorded oracle trajectories to infer the oracle’s reward function [8–11].
Interactive IL [12, 13] assumes the learner can actively ask an oracle policy for a demonstration 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) CartPole (8 oracles) (b) DIP (4 oracles) (c) Halfcheetah (2 oracles) (d) Ant (2 oracles)
Figure 1: Performance of the best policies returned by an RL algorithm (GAE policy gradient [17]), the single-oracle IL algorithm (AggreVaTeD [14]), and our multi-oracle IL algorithm MAMBA. All oracle polices here are suboptimal and AggreVaTeD imitates the best one. In Halfcheetah and Ant, policies of IL algorihtms are initialized by behavior cloning with the best oracle policy. A curve shows an algorithm’s median performance across 8 random seeds. Please see Section 5 for details. starting at the learner’s current state. When reward information of the original RL problem is available,
IL algorithms can outperform the oracle policy [14–16].
In this paper, we ask the question: how should an RL agent leverage domain knowledge encoded in more than one (potentially suboptimal) oracle policies? We study this question in the aforementioned interactive IL setting. Having multiple oracle policies is quite common in practice. For instance, consider the problem of minimizing task processing delays via load-balancing a network of compute nodes. Existing systems and their simulators have a number of human-designed heuristic policies for load balancing that can serve as oracles [18]. Likewise, in autonomous driving, available oracles can range from PID controllers to human drivers [19]. In these examples, each oracle has its own strengths and can provide desirable behaviors for different situations.
Intuitively, because more oracle policies can provide more information about the problem domain, an RL agent should be able to learn a good policy faster than using a single oracle. However, in reality, the agent does not know the properties of each oracle. What it sees instead are conﬂicting demonstrations from the oracle policies. Resolving this disagreement can be non-trivial, because there may not be a single oracle comprehensively outperforming the rest, and the quality of each oracle policy is unknown. Recently, several IL and RL works have started to study this practically important class of scenarios. InfoGAIL [20] conditions the learned policy on latent factors that motivate demonstrations of different oracles. AC-Teach [21] models each oracle with a set of attributes and relies on a Bayesian approach to decide which action to take based on their demonstrations. OIL [19] tries to identify and follow the best oracle in a given situation. However, all existing approaches to IL from multiple oracles sidestep two fundamental questions: (a) What is a reasonable benchmark for policy performance is these settings, analogous to the single-oracle policy quality in conventional IL? (b) Is there a systematic way to stitch together several suboptimal oracles into a stronger baseline that we can further improve upon?
We provide answers to these questions, making the following contributions: 1. We identify the state-wise maximum of oracle policies’ values as a natural benchmark for learning from multiple oracles. We call it the max-aggregated baseline and propose policy improvement from it as a natural strategy to combine these oracles together, creating a new policy that is uniformly better than all the oracles in every state. These insights establish the missing theoretical foundation for designing algorithms for IL with multiple oracles. 2. We propose a novel IL algorithm called MAMBA (Max-aggregation of Multiple Baselines) to learn a policy that is competitive with the max-aggregated baseline by a reduction of policy optimization to online learning [13, 22]. MAMBA is a ﬁrst-order algorithm based on a new IL gradient estimator designed in the spirit of generalized advantage estimation (GAE) [17] from the
RL literature. Like some prior works in IL, MAMBA interacts with the oracle in a roll-in/roll-out fashion [13, 15] and does not assume access to oracle actions. 3. We provide regret-based performance guarantees for MAMBA. In short, MAMBA generalizes a popular single-oracle IL algorithm AggreVaTe(D) [13, 14] to learn from multiple oracles and to achieve larger improvements from suboptimal oracles. Empirically, we evaluate MAMBA against the IL baseline (AggreVaTeD [14]) and direct RL (GAE policy gradient [17]). Fig. 1 highlights the experimental results, where MAMBA demonstrates the capability to bootstrap demonstrations from multiple weak oracles to signiﬁcantly speed up policy optimization. 2
2