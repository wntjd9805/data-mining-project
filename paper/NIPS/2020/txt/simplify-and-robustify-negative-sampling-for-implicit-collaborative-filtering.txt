Abstract
Negative sampling approaches are prevalent in implicit collaborative ﬁltering for obtaining negative labels from massive unlabeled data. As two major concerns in negative sampling, efﬁciency and effectiveness are still not fully achieved by recent works that use complicate structures and overlook risk of false negative instances.
In this paper, we ﬁrst provide a novel understanding of negative instances by empirically observing that only a few instances are potentially important for model learning, and false negatives tend to have stable predictions over many training iterations. Above ﬁndings motivate us to simplify the model by sampling from designed memory that only stores a few important candidates and, more importantly, tackle the untouched false negative problem by favouring high-variance samples stored in memory, which achieves efﬁcient sampling of true negatives with high-quality. Empirical results on two synthetic datasets and three real-world datasets demonstrate both robustness and superiorities of our negative sampling method.
The implementation is available at https://github.com/dingjingtao/SRNS. 1

Introduction
Collaborative ﬁltering (CF), as the key technique of personalized recommender systems, focuses on learning user preference from the observed user-item interactions [28, 34]. Today’s recommender systems also witness the prevalence of implicit user feedback, such as purchases in E-commerce sites and watches in online video platforms, which is much easier to collect compared to the explicit feedback (such as ratings) on item utility. In above examples, each observed interaction normally indicates a user’s interest on an item, i.e., a positive label, while the rest unobserved interactions are unlabeled. As for learning an implicit CF model from this positive-only data, a widely adopted approach is to select a few instances from the unlabeled part and treat them as negative labels, also known as negative sampling [10, 34]. Then, the CF model is optimized to give positive instances higher scores than those given to negative ones [34].
Similar to other related applications in representation learning of text [27] or graph data [30], negative sampling in implicit CF also has two major concerns, i.e., efﬁciency and effectiveness [10, 46].
First, the efﬁcient sampling process is required, as the number of unobserved user-item interactions can be extremely huge. Second, the sampled instances need to be high-quality, so as to learn useful information about user’s negative preference. However, since implicit CF is an application-driven problem where user behaviors play an important role, it may be unrealistic to assume that unobserved interactions are all negative, which introduces false negative instances into training process [21, 26, 49]. For example, an item may be ignored because of its displayed position and form, not necessarily the user’s dislike. Therefore, false negative instances naturally exist in implicit CF.
∗The ﬁrst three authors have equal contributions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Table 1: Comparison of the proposed SRNS with closely related works, where rk(j|u) is the (u, j)’s rank sorted by score, popj is the j’s item popularity, B is the mini-batch size, T is the time complexity of computing an instance score, E is the epoch of lazy-update, and F denotes false negative. pns(j|u)
Optimization
Time Complexity
Robustness
Uniform [34]
NNCF [10]
AOBPR [33]
Uniform({j /∈ Ru})
∝ (popj)0.75
∝ exp(−rk(j|u)/λ)
SGD (from scratch)
SGD (from scratch)
SGD (from scratch)
O(BT )
O(B2T )
O(BT )
IRGAN [38] learned ¯pns(j|u) (GAN) REINFORCE (pretrain)
O(B|I|T )
AdvIR [29] learned ¯pns(j|u) (GAN) REINFORCE (pretrain)
O(BS1T )
SRNS (proposed) variance-based (see (4))
SGD (from scratch) O( B
E (S1 + S2)T )
×
×
×
×
×
√
Previous works of negative sampling in implicit CF mainly focus on replacing the uniform sampling distribution with another proposed distribution, so as to improve the quality of negative samples.
Similar to the word-frequency based distribution [27] and node-degree based distribution [30] used in other domains, an item-popularity based distribution that favours popular items is usually adopted [10, 42]. In terms of sample quality, the strategy emphasizing hard negative samples has been proven to be more effective [29], as it can bring more information for model training. Here the hard samples refer to those with a high probability of being positive according to the model, which are hard for learning. Speciﬁcally, this is achieved by either assigning higher probability to instances with large prediction score [33, 46] or leveraging techniques of adversarial learning [12, 29, 38]. Nevertheless, the above hard negative sampling approaches cannot simultaneously meet the requirements on efﬁciency and effectiveness. On the one hand, several state-of-the-art solutions [12, 29] use complicate structures like generative adversarial network (GAN) [18] for generating negative instances, which has posed a severe challenge on model efﬁciency. On the other hand, all these methods overlook the risk of introducing false negative instances and instead only focus on hard ones, making the sampling process less robust for training an effective CF model with false negatives.
Different from above works, this paper formulates the negative sampling problem as efﬁcient learning from unlabeled data with the presence of noisy labels, i.e., false negative instances. We propose to simplify and robustify the negative sampling for implicit CF, which has three main challenges:
• How to capture the distribution of true negative instances with an unbiased but simple model? In the implicit CF problem, true negative instances are hidden inside the massive unla-beled data, along with false negative instances. Thus negative sampling for implicit CF expects an unbiased estimator that correctly identiﬁes true negative instances during training process. On the other hand, previous works have shown that negative instances in other domains follow a skewed distribution and can be modeled by a simple model [27, 47]. However, it remains unknown in the implicit CF problem if this prior knowledge can also help building an unbiased but simple model for negative sampling.
• How can we reliably measure the quality of negative samples? Given the risk of introducing false negative instances, the quality of negative samples needs to be measured in a more reliable way. However, it is non-trivial to design a discriminative criterion that can help to accurately identify true negative instances with high quality.
• How can we efﬁciently sample true negative instances of high-quality? Although learning effective information from unlabeled and noisy data is related to general machine learning ap-proaches including positive-unlabeled leaning [23] and instance re-weighting [32], these methods are not suitable for implicit CF problem, where the huge number of unobserved user-item in-teractions requires an efﬁcient modeling. Instead, our proposed method needs to maintain both efﬁciency, by sampling, and effectiveness, by considering samples’ informativeness and reliability simultaneously. This has not been tackled before in both implicit CF and other similar problems.
Solving above three challenges calls for a deep and fundamental understanding of different negative instances in implicit CF problem. In this paper, we empirically ﬁnd that negative instances with large prediction scores are important for the model learning but generally rare, i.e., following a skewed distribution. A more novel ﬁnding is that false negative instances always have large scores over many iterations of training, i.e., a lower variance, which provides a new angle on tackling false negative problem remained in existing approaches. Motivated by above two ﬁndings, we 2
propose a novel simpliﬁed and robust negative sampling approach, named SRNS, that 1) captures the dynamic distribution of negative instances with a memory-based model, by simply maintaining the promising candidates with large scores, and 2) leverages a high-variance based criterion to reliably measure the quality of negative samples, reducing the risk of false negative instances effectively.
Above two designs are further combined into a two-step sampling scheme that constantly alternates between score-based memory update and variance-based sampling, so as to efﬁciently sample true negative instances with high-quality. Experiment results on two synthetic datasets demonstrate the robustness of our SRNS under various levels of noisy circumstances. Further experiments on three real-world datasets also empirically validates its superiorities over state-of-the-art baselines, in terms of effectiveness and efﬁciency. 2