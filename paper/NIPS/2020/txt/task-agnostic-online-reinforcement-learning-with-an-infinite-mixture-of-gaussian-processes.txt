Abstract
Continuously learning to solve unseen tasks with limited experience has been extensively pursued in meta-learning and continual learning, but with restricted assumptions such as accessible task distributions, independently and identically distributed tasks, and clear task delineations. However, real-world physical tasks frequently violate these assumptions, resulting in performance degradation. This paper proposes a continual online model-based reinforcement learning approach that does not require pre-training to solve task-agnostic problems with unknown task boundaries. We maintain a mixture of experts to handle nonstationarity, and represent each different type of dynamics with a Gaussian Process to efﬁciently leverage collected data and expressively model uncertainty. We propose a transition prior to account for the temporal dependencies in streaming data and update the mixture online via sequential variational inference. Our approach reliably handles the task distribution shift by generating new models for never-before-seen dynamics and reusing old models for previously seen dynamics. In experiments, our approach outperforms alternative methods in non-stationary tasks, including classic control with changing dynamics and decision making in different driving scenarios. Codes available at: https://github.com/mxu34/mbrl-gpmm. 1

Introduction
Humans can quickly learn new tasks from just a handful of examples by preserving rich representa-tions of experience [1]. Intelligent agents deployed in the real world require the same continual and quick learning ability to safely handle unknown tasks, such as navigation in new terrains and planning in dynamic trafﬁc scenarios. Such desiderata have been previously explored in meta-learning and continual learning. Meta-learning [2, 3] achieves quick adaptation and good generalization with learned inductive bias. It assumes that the tasks for training and testing are independently sampled from the same accessible distribution. Continual learning [4, 5] aims to solve a sequence of tasks with clear task delineations while avoiding catastrophic forgetting. Both communities favor Deep neural networks (DNNs) due to their strong function approximation capability but at the expense of data efﬁciency. These two communities are complementary, and their integration is explored in [6].
However, real-world physical tasks frequently violate essential assumptions of the methods as mentioned above. One example is the autonomous agent navigation problem requiring interactions with surrounding agents. The autonomous agent sequentially encounters other agents that have substantially different behaviors (e.g., aggressive and conservative ones). In this case, the mutual knowledge transfer in meta-learning algorithms may degrade the generalization performance [7].
The task distribution modeling these interactions is prohibitively complex to determine, which casts difﬁculties on the meta-training process with DNNs [8, 9]. Additionally, the boundaries of tasks required in most continual learning algorithms cannot feasibly be determined beforehand in an online 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Method illustration. (a) is a graphical representation of the proposed model-based RL with an inﬁnite mixture as the dynamics model. ut, xt, and gt represent the action, state, and the dynamics model at time t, respectively. Parameters include the concentration parameter α, the base distribution
G0, and the sticky parameter β. (b) visualizes the predictive distribution at a data point ˜x∗. learning setting. Although task-agnostic/task-free continual learning is explored in [10, 11], the temporal dependencies of dynamics presented in a non-stationary robotics task are missed. For instance, two different dynamics models close together in time are likely to be related.
In this work, we aim to solve nonstationary online problems where the task boundaries and the number of tasks are unknown by proposing a model-based reinforcement learning (RL) method that does not require a pre-trained model. Model-based methods [12] are more data-efﬁcient than model-free ones, and their performance heavily depends on the accuracy of the learned dynamics models. Similar to expansion-based continual learning methods [6, 13], we use an inﬁnite mixture to model system dynamics, a graphical illustration of which is given in Figure 1 (a). It has the capacity to model an inﬁnite number of dynamics, while the actual number is derived from the data. We represent each different type of dynamics with a Gaussian Process (GP) [14] to efﬁciently leverage collected data and expressively model uncertainty. A GP is more data-efﬁcient than a DNN (as its predictive distribution is explicitly conditioned on the collected data) and thus enables fast adaptation to new tasks even without the use of a previously trained model. With a mixture of GPs, the predictive distribution at a data point is multimodal, as shown in Figure 1 (b), with each mode representing a type of dynamics. By making predictions conditioned on the dynamics assignments, our method robustly handles dynamics that are dramatically different.
At each time step, our method either creates a new model for previously unseen dynamics or recalls an old model for encountered dynamics. After task recognition, the corresponding dynamics model parameters are updated via conjugate gradient [15]. Considering that RL agents collect experience in a streaming manner, we learn the mixture with sequential variational inference [16] that is suitable for the online setting. To account for the temporal dependencies of dynamics, we propose a transition prior that stems from the Dirichlet Process (DP) prior to improve task shift detection. We select representative data points for each type of dynamics by optimizing a variational objective widely used in the Sparse GP literature [17]. We demonstrate the capability of task recognition and quick task adaptation of our approach in non-stationary Cartpole-SwingUp, HalfCheetah and Highway-Intersection environments. 2