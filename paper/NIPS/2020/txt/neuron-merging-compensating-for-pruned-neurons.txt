Abstract
Network pruning is widely used to lighten and accelerate neural network models.
Structured network pruning discards the whole neuron or ﬁlter, leading to accuracy loss. In this work, we propose a novel concept of neuron merging applicable to both fully connected layers and convolution layers, which compensates for the information loss due to the pruned neurons/ﬁlters. Neuron merging starts with decomposing the original weights into two matrices/tensors. One of them becomes the new weights for the current layer, and the other is what we name a scaling matrix, guiding the combination of neurons. If the activation function is ReLU, the scaling matrix can be absorbed into the next layer under certain conditions, compensating for the removed neurons. We also propose a data-free and inexpensive method to decompose the weights by utilizing the cosine similarity between neurons. Compared to the pruned model with the same topology, our merged model better preserves the output feature map of the original model; thus, it maintains the accuracy after pruning without ﬁne-tuning. We demonstrate the effectiveness of our approach over network pruning for various model architectures and datasets. As an example, for VGG-16 on CIFAR-10, we achieve an accuracy of 93.16% while reducing 64% of total parameters, without any ﬁne-tuning. The code can be found here: https://github.com/friendshipkim/neuron-merging 1

Introduction
Modern Convolutional Neural Network (CNN) models have shown outstanding performance in many computer vision tasks. However, due to their numerous parameters and computation, it remains challenging to deploy them to mobile phones or edge devices. One of the widely used methods to lighten and accelerate the network is pruning. Network pruning exploits the ﬁndings that the network is highly over-parameterized. For example, Denil et al. [1] demonstrate that a network can be efﬁciently reconstructed with only a small subset of its original parameters.
Generally, there are two main branches of network pruning. One of them is unstructured pruning, also called weight pruning, which removes individual network connections. Han et al. [2] achieved a compression rate of 90% by pruning weights with small magnitudes and retraining the model.
However, unstructured pruning produces sparse weight matrices, which cannot lead to actual speedup and compression without specialized hardware or libraries [3]. On the other hand, structured pruning methods eliminate the whole neuron or even the layer of the model, not individual connections. Since structured pruning maintains the original weight structure, no specialized hardware or libraries are necessary for acceleration. The most prevalent structured pruning method for CNN models is to prune ﬁlters of each convolution layer and the corresponding output feature map channels. The ﬁlter or channel to be removed is determined by various saliency criteria [15, 26, 27].
Regardless of what saliency criterion is used, the corresponding dimension of the pruned neuron is removed from the next layer. Consequently, the output of the next layer will not be fully reconstructed
∗Corresponding author 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Neuron merging applied to the convolution layer. The pruned ﬁlter is marked as a dashed box. Pruning the blue-colored ﬁlter results in the removal of its corresponding feature map and the corresponding dimensions in the next layer, which leads to the scale-down of the output feature maps. However, neuron merging maintains the scale of the output feature maps by merging the pruned dimension (B) with the remaining one (A). Let us assume that the blue-colored ﬁlter is scale times the light-blue-colored ﬁlter. By multiplying B by scale and adding it to A, we can perfectly approximate the output feature map even after removing the blue-colored ﬁlter. with the remaining neurons. In particular, when the neurons of the front layer are removed, the reconstruction error continues to accumulate, which leads to performance degradation [27].
In this paper, we propose neuron merging that compensates for the effect of the removed neuron by merging its corresponding dimension of the next layer. Neuron merging is applicable to both the fully connected and convolution layers, and the overall concept applied to the convolution layer is depicted in Fig. 1. Neuron merging starts with decomposing the original weights into two matrices/tensors.
One of them becomes the new weights for the current layer, and the other is what we name a scaling matrix, guiding the process of merging the dimensions of the next layer. If the activation function is
ReLU and the scaling matrix satisﬁes certain conditions, it can be absorbed into the next layer; thus, merging has the same network topology as pruning.
In this formulation, we also propose a simple and data-free method of neuron merging. To form the remaining weights, we utilize well-known pruning criteria (e.g., l1-norm [15]). To generate the scaling matrix, we employ the cosine similarity and l2-norm ratio between neurons. This method is applicable even when only the pretrained model is given without any training data. Our extensive experiments demonstrate the effectiveness of our approach. For VGG-16 [21] and WideResNet 40-4 [28] on CIFAR-10, we achieve an accuracy of 93.16% and 93.3% without any ﬁne-tuning, while reducing 64% and 40% of the total parameters, respectively. Our contributions are as follows: (1) We propose and formulate a novel concept of neuron merging that compensates for the information loss due to the pruned neurons/ﬁlters in both fully connected layers and convolution layers. (2) We propose a one-shot and data-free method of neuron merging which employs the cosine similarity and ratio between neurons. (3) We show that our merged model better preserves the original model than the pruned model with various measures, such as the accuracy immediately after prun-ing, feature map visualization, and Weighted Average Reconstruction Error [27]. 2