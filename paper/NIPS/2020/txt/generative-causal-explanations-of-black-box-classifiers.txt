Abstract
We develop a method for generating causal post-hoc explanations of black-box classiﬁers based on a learned low-dimensional representation of the data. The explanation is causal in the sense that changing learned latent factors produces a change in the classiﬁer output statistics. To construct these explanations, we design a learning framework that leverages a generative model and information-theoretic measures of causal inﬂuence. Our objective function encourages both the generative model to faithfully represent the data distribution and the latent factors to have a large causal inﬂuence on the classiﬁer output. Our method learns both global and local explanations, is compatible with any classiﬁer that admits class probabilities and a gradient, and does not require labeled attributes or knowledge of causal structure. Using carefully controlled test cases, we provide intuition that illuminates the function of our objective. We then demonstrate the practical utility of our method on image recognition tasks.1 1

Introduction
There is a growing consensus among researchers, ethicists, and the public that machine learning models deployed in sensitive applications should be able to explain their decisions [1, 2]. A powerful way to make “explain” mathematically precise is to use the language of causality: explanations should identify causal relationships between certain data aspects — features which may or may not be semantically meaningful — and the classiﬁer output [3–5]. In this conception, an aspect of the data helps explain the classiﬁer if changing that aspect (while holding other data aspects ﬁxed) produces a corresponding change in the classiﬁer output.
Constructing causal explanations requires reasoning about how changing different aspects of the input data affects the classiﬁer output, but these observed changes are only meaningful if the modiﬁed combination of aspects occurs naturally in the dataset. A challenge in constructing causal explanations is therefore the ability to change certain aspects of data samples without leaving the data distribution.
In this paper we propose a novel learning-based framework that overcomes this challenge. Our framework has two fundamental components that we argue are necessary to operationalize a causal explanation: a method to represent and move within the data distribution, and a rigorous metric for causal inﬂuence of different data aspects on the classiﬁer output.
To do this, we construct a generative model consisting of a disentangled representation of the data and a generative mapping from this representation to the data space (Figure 1(a)). We seek to learn this disentangled representation in such a way that each factor controls a different aspect of the data, and a subset of the factors have a large causal inﬂuence on the classiﬁer output. To formalize this notion of causal inﬂuence, we deﬁne a structural causal model (SCM) [6] that relates independent 1Code is available at https://github.com/siplab-gt/generative-causal-explanations. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
α
β
X
Y (a) (b)
Figure 1: (a) Computational architecture used to learn explanations. Here, the low-dimensional representation (α, β) learns to describe the color and shape of inputs. Changing α (color) changes the output of the classiﬁer, which detects the color of the data sample, while changing β (shape) does not affect the classiﬁer output. (b) DAG describing our causal model, satisfying principles in Section 3.1. latent factors deﬁning data aspects, the classiﬁer inputs, and the classiﬁer outputs. Leveraging recent work on information-theoretic measures of causal inﬂuence [7, 8], we use the independence of latent factors in the SCM to show that in our framework the causal inﬂuence of the latent factors on the classiﬁer output can be quantiﬁed simply using mutual information. The crux of our approach is an optimization program for learning a mapping from the latent factors to the data space. The objective ensures that the learned disentangled representation represents the data distribution while simultaneously encouraging a subset of latent factors to have a large causal inﬂuence on the classiﬁer output.
A natural beneﬁt of our framework is that the learned disentangled representation provides a rich and
ﬂexible vocabulary for explanation. This vocabulary can be more expressive than feature selection or saliency map-based explanation methods: a latent factor, in its simplest form, could describe a single feature or mask of features in input space, but it can also describe much more complex patterns and relationships in the data. Crucially, unlike methods that crudely remove features directly in data space, the generative model enables us to construct explanations that respect the data distribution.
This is important because an explanation is only meaningful if it describes combinations of data aspects that naturally occur in the dataset. For example, a loan applicant would not appreciate being told that his loan would have been approved if he had made a negative number of late payments, and a doctor would be displeased to learn that her automated diagnosis system depends on a biologically implausible attribute.
Once the disentangled representation is learned, explanations can be constructed using the gener-ative mapping. Our framework can provide both global and local explanations: a practitioner can understand the aspects of the data that are important to the classiﬁer at large by visualizing the effect in data space of changing each causal factor, and they can determine the aspects that dictated the classiﬁer output for a speciﬁc input by observing its corresponding latent values. These visualizations can be much more descriptive than saliency maps, particularly in vision applications.
The major contributions of this work are a new conceptual framework for generating explanations using causal modeling and a generative model (Section 3), analysis of the framework in a simple setting where we can obtain analytical and intuitive understanding (Section 4), and a brief evaluation of our method applied to explaining image recognition models (Section 5). 2