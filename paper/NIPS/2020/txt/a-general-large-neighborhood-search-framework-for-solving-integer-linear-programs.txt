Abstract
This paper studies a strategy for data-driven algorithm design for large-scale com-binatorial optimization problems that can leverage existing state-of-the-art solvers in general purpose ways. The goal is to arrive at new approaches that can reliably outperform existing solvers in wall-clock time. We focus on solving integer lin-ear programs, and ground our approach in the large neighborhood search (LNS) paradigm, which iteratively chooses a subset of variables to optimize while leav-ing the remainder ﬁxed. The appeal of LNS is that it can easily use any existing solver as a subroutine, and thus can inherit the beneﬁts of carefully engineered heuristic or complete approaches and their software implementations. We show that one can learn a good neighborhood selector using imitation and reinforcement learning techniques. Through an extensive empirical validation in bounded-time optimization, we demonstrate that our LNS framework can signiﬁcantly outper-form compared to state-of-the-art commercial solvers such as Gurobi. 1

Introduction
The design of algorithms for solving hard combinatorial optimization problems remains a valuable and challenging task. Practically relevant problems are typically NP-complete or NP-hard. Exam-ples include any kind of search problem through a combinatorial space, such as network designs
[19], mechanism design [15], planning [39], inference in graphical models [50], program synthesis
[36], veriﬁcation [7], and engineering design [12, 37], amongst many others.
The widespread importance of solving hard combinatorial optimization problems has spurred in-tense research in designing approximation algorithms and heuristics for large problem classes, such as integer programming [8, 21, 34] and satisﬁability [51, 13, 16]. Historically, the design of such algorithms was done largely manually, requiring careful understandings of the underlying structure within speciﬁc classes of optimization problems. Such approaches are often unappealing due to the need to obtain substantial domain knowledge, and one often desires a more automated approach.
In recent years, there has been an increasing interest to automatically learn good (parameters of) algorithms for combinatorial problems from training data. The most popular paradigm, also referred to as “learning to search”, aims to augment existing algorithmic templates by replacing hard-coded heuristic components with parameterized learnable versions. For example, this has been done in the context of greedy search for NP-hard graph problems [31]. However, greedy as well as most general purpose heuristic or local search algorithms are limited to combinatorial optimization problems, where the constraints are easy to satisfy, and hence are difﬁcult to apply to domains with intricate side constraints. On the other hand, Integer Linear Programs (ILPs) are a widely applicable problem class that can encode a broad set of domains as well as large number and variety of constrains.
∗Work done while at Jet Propulsion Laboratory, California Institute of Technology 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Branch-and-bound, which is a complete search procedure, is the state-of-the-art approach to ILPs and has also been recently extensively researched through the lens of “learning to search” [23, 30, 31, 46, 47, 22]. While this line of research has shown promise, it falls short of delivering practical impact, especially in improving wall-clock time. Practically improving the performance of branch-and-bound algorithms through learning is stymied by the need to either modify commercial solvers with limited access to optimized integration, or to modify open-source solvers such as SCIP [1], which is considerably slower than leading commercial solvers such as Gurobi and CPlex (usually by a factor of 10 or more) [38, 40].
Motivated by the aforementioned drawbacks, we study how to design abstractions of large-scale combinatorial optimization problems that can leverage existing state-of-the-art solvers as a generic black-box subroutine. Our goal is to arrive at new approaches that can reliably outperform leading commercial solvers in wall-clock time, can be applicable to broad class of combinatorial optimiza-tion problems, and is amenable to data-driven design. We focus on solving integer linear programs (ILPs), which are a common way to represent many combinatorial optimization problems. We leverage the large neighborhood search (LNS) paradigm [2], an incomplete algorithm that itera-tively chooses a subset of variables to optimize while leaving the remainder ﬁxed. A major appeal of LNS is that it can easily use any existing solver as a subroutine, including ones that can handle general ILPs.
Our contributions can be summarized as:
• We propose a general LNS framework for solving large-scale ILPs. Our framework enables easy integration of existing solvers as subroutines, and does not depend on incorporating domain knowledge in order to achieve strong performance. In our experiments, we combine our framework with Gurobi, a leading commercial ILP solver.
• We show that, perhaps surprisingly, even using a random decision procedure within our
LNS framework signiﬁcantly outperforms Gurobi on many problem instances.
• We develop a learning-based approach that predicts a partitioning of the integer variables, which then serves as a learned decision procedure within our LNS framework. This pro-cedure is effectively learning how to decompose the original optimization problem into a series of sub-problems that can be solved much more efﬁciently using existing solvers.
• We perform an extensive empirical validation across several ILP benchmarks, and demon-strate superior wall-clock performance compared to Gurobi across all benchmarks. These results suggest that our LNS framework can effectively leverage leading state-of-the-art solvers to reliably achieve substantial speed-ups in wall-clock time. 2