Abstract
This paper develops a novel and uniﬁed framework to analyze the convergence of a large family of Q-learning algorithms from the switching system perspective.
We show that the nonlinear ODE models associated with Q-learning and many of its variants can be naturally formulated as afﬁne switching systems. Building on their asymptotic stability, we obtain a number of interesting results: (i) we provide a simple ODE analysis for the convergence of asynchronous Q-learning under relatively weak assumptions; (ii) we establish the ﬁrst convergence analysis of the averaging Q-learning algorithm, and (iii) we derive a new sufﬁcient condition for the convergence of Q-learning with linear function approximation. 1

Introduction
Reinforcement learning (RL) addresses the optimal control problem for unknown systems through experiences [30]. Q-learning, originally introduced by Watkins [36], is one of the most popular and fundamental reinforcement learning algorithms for unknown systems described by Markov decision processes. The convergence of Q-learning has been extensively studied in the literature and proven via several different approaches, including the original proof [36], the stochastic approximation and contraction mapping-based approach [14, 33, 2, 9, 32, 9, 1, 38], and the ODE (ordinary differential equation) approach [4].
The ODE approach analyzes the convergence of general stochastic recursions by examining stability of the associated ODE model [3, 17, 4] and has been used as a convenient analysis tool to prove convergence of many RL algorithms, especially the temporal difference (TD) learning algorithm [29] and its variants [23, 31, 19, 10]. However, its application to Q-learning has been limited due to the presence of the max-operator, which makes the associated ODE model a complex nonlinear system. In contrast, the associated ODE of TD learning for policy evaluation is a linear system, whose asymptotic stability is much easier to analyze in general. While [4] gave the convergence proof of Q-learning based on a nonlinear ODE model, to the authors’ knowledge, substantial analysis is required to prove the stability of the corresponding nonlinear ODE [5] by using the max-norm contraction of the Bellman operator. In addition, the result in [4] only applies to synchronous Q-learning, where every state-action pair is visited at each iteration, instead of the commonly used asynchronous Q-learning. Last but not least, the stability analysis does not immediately extend to other Q-learning variants, such as double Q-learning [11], averaging Q-learning [19], and Q-learning with linear function approximation, etc.
In this paper, we provide a simple and uniﬁed framework to analyze Q-learning and its variants through switched linear system (SLS) models [21] of the associated ODE. SLSs are an important class of nonlinear hybrid systems, where the system dynamics matrix switches within a ﬁnite set of subsystem matrices (or modes) according to a switching signal. The study of SLSs has attracted tremendous attention in the past years and their stability behaviors have been well established in the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
literature; see [22] and [21] for comprehensive surveys. Our main contributions are summarized as follows: 1. For a number of Q-learning algorithms such as the asynchronous Q-learning, we show that the nonlinear ODE models associated with these algorithms can be characterized as afﬁne switching systems with a state-feedback switching policy. 2. We construct both upper and lower comparison systems of the corresponding afﬁne switching systems, and prove their asymptotic stability based on existing control theory and comparison principles. As a result of the Borkar and Meyn theorem [4], we obtain the asymptotic convergence of these Q-learning algorithms. 3. We extend the approach to analyze the averaging Q-learning [19]. To our best knowledge, this is the ﬁrst convergence analysis of averaging Q-learning in the literature. 4. We also examine Q-learning with linear function approximation and derive a new sufﬁcient condition to ensure its convergence based on the switching system theory. We show that, under speciﬁc assumptions, our new diagonal dominating condition is weaker than the well-known Melo’s condition provided in [24].