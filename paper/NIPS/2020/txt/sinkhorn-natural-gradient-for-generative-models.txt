Abstract
We consider the problem of minimizing a functional over a parametric family of probability measures, where the parameterization is characterized via a push-forward structure. An important application of this problem is in training generative adversarial networks. In this regard, we propose a novel Sinkhorn Natural Gradient (SiNG) algorithm which acts as a steepest descent method on the probability space endowed with the Sinkhorn divergence. We show that the Sinkhorn information matrix (SIM), a key component of SiNG, has an explicit expression and can be evaluated accurately in complexity that scales logarithmically with respect to the desired accuracy. This is in sharp contrast to existing natural gradient methods that can only be carried out approximately. Moreover, in practical applications when only Monte-Carlo type integration is available, we design an empirical
In our experiments, we estimator for SIM and provide the stability analysis. quantitatively compare SiNG with state-of-the-art SGD-type solvers on generative tasks to demonstrate its efﬁciency and efﬁcacy of our method. 1

Introduction
Consider the minimization of a functional F over a parameterized family probability measures tαθu: min
θPΘ tF pθq:“Fpαθqu , (1) where Θ Ď Rd is the feasible domain of the parameter θ. We assume that the measures αθ are deﬁned over a common ground set X Ď Rq with the following structure: αθ “ Tθ 7µ, where µ is a ﬁxed and known measure and Tθ is a push-forward mapping. More speciﬁcally, µ is a simple measure on a latent space Z Ď R¯q, such as the standard Gaussian measure µ “ N p0¯q, I¯qq, and the parameterized map Tθ : Z Ñ X transforms the measure µ to αθ. This type of push-forward parameterization is commonly used in deep generative models, where Tθ represents a neural network parametrized by weights θ [Goodfellow et al., 2014, Salimans et al., 2018, Genevay et al., 2018]. Consequently, methods to efﬁciently and accurately solve problem (1) are of great importance in machine learning.
The de facto solvers for problem (1) are generic nonconvex optimizers such as Stochastic Gradient
Descent (SGD) and its variants, Adam [Kingma and Ba, 2014], Amsgrad [Reddi et al., 2019],
RMSProp [Hinton et al.], etc. These optimization algorithms directly work on the parameter space and are agnostic to the fact that αθ’s are probability measures. Consequently, SGD type solvers suffer from the complex optimization landscape induced from the neural-network mappings Tθ.
An alternative to SGD type methods is the natural gradient method, which is originally motivated from Information Geometry [Amari, 1998, Amari et al., 1987]. Instead of simply using the Euclidean structure of the parameter space Θ in the usual SGD, the natural gradient method endows the parameter space with a “natural" metric structure by pulling back a known metric on the probability space and then searches the steepest descent direction of F pθq in the “curved" neighborhood of θ. In particular, the natural gradient update is invariant to reparametrization. This allows natural gradient to 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
avoid the undesirable saddle point or local minima that are artiﬁcially created by the highly nonlinear maps Tθ. The classical Fisher-Rao Natural Gradient (FNG) [Amari, 1998] as well as its many variants
[Martens and Grosse, 2015, Thomas et al., 2016, Song et al., 2018] endows the probability space with the KL divergence and admits update direction in closed form. However, the update rules of these methods all require the evaluation of the score function of the variable measure. Leaving aside its existence, this quantity is in general difﬁcult to compute for push-forward measures, which limits the application of FNG type methods in the generative models. Recently, Li and Montúfar [2018] propose to replace the KL divergence in FNG by the Wasserstein distance and propose the Wasserstein Natural
Gradient (WNG) algorithm. WNG shares the merit of reparameterization invariance as FNG while avoiding the requirement of the score function. However, the Wasserstein information matrix (WIM) is very difﬁcult to compute as it does not attain a closed form expression when the dimension d of parameters is greater than 1, rendering WNG impractical.
Following the line of natural gradient, in this paper, we propose Sinkhorn Natural Gradient (SiNG), an algorithm that performs the steepest descent of the objective functional F on the probability space with the Sinkhorn divergence as the underlying metric. Unlike FNG, SiNG requires only to sample from the variable measure αθ. Moreover, the Sinkhorn information matrix (SIM), a key component in SiNG, can be computed in logarithmic time in contrast to WIM in WNG. Concretely, we list our contributions as follows: 1. We derive the Sinkhorn Natural Gradient (SiNG) update rule as the exact direction that minimizes the objective functional F within the Sinkhorn ball of radius (cid:15) centered at the current measure. In the asymptotic case (cid:15) Ñ 0, we show that the SiNG direction only depends on the Hessian of the Sinkhorn divergence and the gradient of the function F , while the effect of the Hessian of F becomes negligible. Further, we prove that SiNG is invariant to reparameterization in its continuous-time limit (i.e. using the inﬁnitesimal step size). 2. We explicitly derive the expression of the Sinkhorn information matrix (SIM), i.e. the
Hessian of the Sinkhorn divergence with respect to the parameter θ. We then show the
SIM can be computed using logarithmic (w.r.t. the target accuracy) function operations and integrals with respect to αθ. 3. When only Monte-Carlo integration w.r.t. αθ is available, we propose to approximate SIM with its empirical counterpart (eSIM), i.e. the Hessian of the empirical Sinkhorn divergence.
Further, we prove stability of eSIM. Our analysis relies on the fact that the Fréchet derivative of Sinkhorn potential with respect to the parameter θ is continuous with respect to the underlying measure µ. Such result can be of general interest.
In our experiments, we pretrain the discriminators for the celebA and cifar10 datasets. Fixing the discriminator, we compare SiNG with state-of-the-art SGD-type solvers in terms of the generator loss. The result shows the remarkable superiority of SiNG in both efﬁcacy and efﬁciency.
Notation: Let X Ď Rq be a compact ground set. We use M` 1 pX q to denote the space of probability measures on X and use CpX q to denote the family of continuous functions mapping from X to R.
For a function f P CpX q, we denote its L8 norm by }f }8:“ maxxPX |f pxq| and its gradient by ∇f .
For a functional on general vector spaces, the Fréchet derivative is formally deﬁned as follows. Let
V and W be normed vector spaces, and U Ď V be an open subset of V . A function F : U Ñ W is called Fréchet differentiable at x P U if there exists a bounded linear operator A : V Ñ W such that lim
}h}Ñ0
}Fpx ` hq ´ Fpxq ´ Ah}W
}h}V
“ 0. (2)
If there exists such an operator A, it will be unique, so we denote DFpxq “ A and call it the
Fréchet derivative. From the above deﬁnition, we know that DF : U Ñ T pV, W q where T pV, W q is the family of bounded linear operators from V to W . Given x P U , the linear map DFpxq takes one input y P V and outputs z P W . This is denoted by z “ DFpxqrys. We then deﬁne the operator norm of DF at x as }DFpxq}op:“ maxhPV
. Further, the second-order Fréchet derivative of F is denoted as D2F : U Ñ L2pV ˆ V, W q, where L2pV ˆ V, W q is the family of all continuous bilinear maps from V to W . Given x P U , the bilinear map D2Fpxq takes two inputs y1, y2 P V and outputs z P W . We denote this by z “ D2Fpxqry1, y2s. If a function F has multiple variables, we use Dif to denote the Fréchet derivative with its ith variable and use D2 ijF to denote the corresponding second-order terms. Finally, ˝ denotes the composition of functions.
}DF pxqrhs}W
}h}V 2
2