Abstract
Ofﬂine methods for reinforcement learning have a potential to help bridge the gap between reinforcement learning research and real-world applications. They make it possible to learn policies from ofﬂine datasets, thus overcoming concerns associated with online data collection in the real-world, including cost, safety, or ethical concerns. In this paper, we propose a benchmark called RL Unplugged to evaluate and compare ofﬂine RL methods. RL Unplugged includes data from a diverse range of domains including games (e.g., Atari benchmark) and simulated motor control problems (e.g., DM Control Suite). The datasets include domains that are partially or fully observable, use continuous or discrete actions, and have stochastic vs. deterministic dynamics. We propose detailed evaluation protocols for each domain in RL Unplugged and provide an extensive analysis of supervised learning and ofﬂine RL methods using these protocols. We will release data for all our tasks and open-source all algorithms presented in this paper. We hope that our suite of benchmarks will increase the reproducibility of experiments and make it possible to study challenging tasks with a limited computational budget, thus making RL research both more systematic and more accessible across the community. Moving forward, we view RL Unplugged as a living benchmark suite that will evolve and grow with datasets contributed by the research community and ourselves. Our project page is available on github. 1

Introduction
Reinforcement Learning (RL) has seen important breakthroughs, including learning directly from raw sensory streams [Mnih et al., 2015], solving long-horizon reasoning problems such as Go [Silver et al., 2016], StarCraft II [Vinyals et al., 2019], DOTA [Berner et al., 2019], and learning motor control for high-dimensional simulated robots [Heess et al., 2017, Akkaya et al., 2019]. However, many of these successes rely heavily on repeated online interactions of an agent with an environment.
Despite its success in simulation, the uptake of RL for real-world applications has been limited.
Power plants, robots, healthcare systems, or self-driving cars are expensive to run and inappropriate controls can have dangerous consequences. They are not easily compatible with the crucial idea of
⇤Indicates joint ﬁrst authors. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Task domains included in RL Unplugged. We include several open-source environments that are familiar to the community, as well as recent releases that push the limits of current algorithms. The task domains span key environment properties such as action space, observation space, exploration difﬁculty, and dynamics. exploration in RL and the data requirements of online RL algorithms. Nevertheless, most real-world systems produce large amounts of data as part of their normal operation.
There is a resurgence of interest in ofﬂine methods for reinforcement learning,1 that can learn new policies from logged data, without any further interactions with the environment due to its potential real-world impact. Ofﬂine RL can help (1) pretrain an RL agent using existing datasets, (2) empirically evaluate RL algorithms based on their ability to exploit a ﬁxed dataset of interactions, and (3) bridge the gap between academic interest in RL and real-world applications.
Ofﬂine RL methods [e.g Agarwal et al., 2020, Fujimoto et al., 2018] have shown promising results on well-known benchmark domains. However, non-standardized evaluation protocols, differing datasets and lack of baselines make algorithmic comparisons difﬁcult. Important properties of potential real-world application domains such as partial observability, high-dimensional sensory streams such as images, diverse action spaces, exploration problems, non-stationarity, and stochasticity are under-represented in the current ofﬂine RL literature. This makes it difﬁcult to assess the practical applicability of ofﬂine RL algorithms.
The reproducibility crisis of RL [Henderson et al., 2018] is very evident in ofﬂine RL. Several works have highlighted these reproducibility challenges in their papers: Peng et al. [2019] discusses the difﬁculties of implementing the MPO algorithm, Fujimoto et al. [2019] mentions omitting results for SPIBB-DQN due to the complexity of implementation. On our part, we have had difﬁculty implementing SAC [Haarnoja et al., 2018]. We have also found it hard to scale BRAC [Wu et al., 2019] and BCQ [Fujimoto et al., 2018]. This does not indicate these algorithms do not work. Only that implementation details matter, comparing algorithms and ensuring their reproducibility is hard.
The intention of this paper is to help in solving this problem by putting forward common benchmarks, datasets, evaluation protocols, and code.
The availability of large datasets with strong benchmarks has been the main factor for the success of machine learning in many domains. Examples of this include vision challenges, such as ImageNet
[Deng et al., 2009] and COCO [Veit et al., 2016], and game challenges, where simulators produce hundreds of years of experience for online RL agents such as AlphaGo [Silver et al., 2016] and the
OpenAI Five [Berner et al., 2019]. In contrast, lack of datasets with clear benchmarks hinders the similar progress in RL for real-world applications. This paper aims to correct this such as to facilitate collaborative research and measurable progress in the ﬁeld.
To this end, we introduce a novel collection of task domains and associated datasets together with a clear evaluation protocol. We include widely-used domains such as the DM Control Suite [Tassa et al., 2018] and Atari 2600 games [Bellemare et al., 2013], but also domains that are still challenging for strong online RL algorithms such as real-world RL (RWRL) suite tasks [Dulac-Arnold et al., 2020] and DM Locomotion tasks [Heess et al., 2017, Merel et al., 2019a,b, 2020]. By standardizing the environments, datasets, and evaluation protocols, we hope to make research in ofﬂine RL more reproducible and accessible. We call our suite of benchmarks “RL Unplugged”, because ofﬂine RL methods can use it without any actors interacting with the environment. 1Sometimes referred to as ‘Batch RL,’ but in this paper, we use ‘Ofﬂine RL’. 2
This paper offers four main contributions: (i) a uniﬁed API for datasets (ii) a varied set of environments (iii) clear evaluation protocols for ofﬂine RL research, and (iv) reference performance baselines.
The datasets in RL Unplugged enable ofﬂine RL research on a variety of established online RL environments without having to deal with the exploration component of RL. In addition, we intend our evaluation protocols to make the benchmark more fair and robust to different hyperparameter choices compared to the traditional methods which rely on online policy selection. Moreover, releasing the datasets with a proper evaluation protocols and open-sourced code will also address the reproducibility issue in RL [Henderson et al., 2018]. We evaluate and analyze the results of several SOTA RL methods on each task domain in RL Unplugged. We also release our datasets in an easy-to-use uniﬁed API that makes the data access easy and efﬁcient with popular machine learning frameworks. 2 RL Unplugged
The RL Unplugged suite is designed around the following considerations: to facilitate ease of use, we provide the datasets with a uniﬁed API which makes it easy for the practitioner to work with all data in the suite once a general pipeline has been established. We further provide a number of baselines including state-of-the art algorithms compatible with our API.2 2.1 Properties of RL Unplugged
Many real-world RL problems require algorithmic solutions that are general and can demonstrate robust performance on a diverse set of challenges. Our benchmark suite is designed to cover a range of properties to determine the difﬁculty of a learning problem and affect the solution strategy choice.
In the initial release of RL Unplugged, we include a wide range of task domains, including Atari games and simulated robotics tasks. Despite the different nature of the environments used, we provide a uniﬁed API over the datasets. Each entry in any dataset consists of a tuple of state (st), action (at), reward (rt), next state (st+1), and the next action (at+1). For sequence data, we also provide future states, actions, and rewards, which allows for training recurrent models for tasks requiring memory.
We additionally store metadata such as episodic rewards and episode id. We chose the task domains to include tasks that vary along the following axes. In Figure 1, we give an overview of how each task domain maps to these axes.
Action space We include tasks with both discrete and continuous action spaces, and of varying action dimension with up to 56 dimensions in the initial release of RL Unplugged.
Observation space We include tasks that can be solved from the low-dimensional natural state space of the MDP (or hand-crafted features thereof), but also tasks where the observation space consists of high-dimensional images (e.g., Atari 2600). We include tasks where the observation is recorded via an external camera (third-person view), as well as tasks in which the camera is controlled by the learning agent (e.g. robots with egocentric vision).
Partial observability & need for memory We include tasks in which the feature vector is a complete representation of the state of the MDP, as well as tasks that require the agent to estimate the state by integrating information over horizons of different lengths.
Difﬁculty of exploration We include tasks that vary in terms of exploration difﬁculty for reasons such as dimension of the action space, sparseness of the reward, or horizon of the learning problem.
Real-world challenges To better reﬂect the difﬁculties encountered in real systems, we also include tasks from the Real-World RL Challenges [Dulac-Arnold et al., 2020], which include aspects such as action delays, stochastic transition dynamics, or non-stationarities.
The characteristics of the data is also an essential consideration, including the behavior policy used, data diversity, i.e., state and action coverage, and dataset size. RL Unplugged introduces datasets that cover those different axes. For example, on Atari 2600, we use large datasets generated across training of an off-policy agent, over multiple seeds. The resulting dataset has data from a large mixture of policies. In contrast, we use datasets from ﬁxed sub-optimal policies for the RWRL suite. 2See our github project page for the details of our API (https://github.com/deepmind/ deepmind-research/tree/master/rl_unplugged). 3
Figure 2: Comparison of evaluation protocols. (left) Evaluation using online policy selection allows us to isolate ofﬂine RL methods, but gives overly optimistic results because they allow perfect policy selection. (right)
Evaluation using ofﬂine policy selection allows us to see how ofﬂine RL performs in situations where it is too costly to interact with the environment for validation purposes; a common scenario in the real-world. We intend our benchmark to be used for both. 2.2 Evaluation Protocols
In a strict ofﬂine setting, environment interactions are not allowed. This makes hyperparameter tuning, including determining when to stop a training procedure, difﬁcult. This is because we cannot take policies obtained by different hyperparameters and run them in the environment to determine which ones receive higher reward (we call this procedure online policy selection).3 Ideally, ofﬂine
RL would evaluate policies obtained by different hyperparameters using only logged data, for example using ofﬂine policy evaluation (OPE) methods [Voloshin et al., 2019] (we call this procedure ofﬂine policy selection). However, it is unclear whether current OPE methods scale well to difﬁcult problems. In RL Unplugged we would like to evaluate ofﬂine RL performance in both settings.
Evaluation by online policy selection (see Figure 2 (left)) is widespread in the RL literature, where researchers usually evaluate different hyperparameter conﬁgurations in an online manner by inter-acting with the environment, and then report results for the best hyperparameters. This enables us to evaluate ofﬂine RL methods in isolation, which is useful. It is indicative of performance given perfect ofﬂine policy selection, or in settings where we can validate via online interactions. This score is important, because as ofﬂine policy selection methods improve, performance will approach this limit. But it has downsides. As discussed before, it is infeasible in many real-world settings, and as a result it gives an overly optimistic view of how useful ofﬂine RL methods are today. Lastly, it favors methods with more hyperparameters over more robust ones.
Evaluation by ofﬂine policy selection (see Figure 2 (right)) has been less popular, but is important as it is indicative of robustness to imperfect policy selection, which more closely reﬂects the current state of ofﬂine RL for real-world problems. However it has downsides too, namely that there are many design choices including what data to use for ofﬂine policy selection, whether to use value functions trained via ofﬂine RL or OPE algorithms, which OPE algorithm to choose, and the meta question of how to tune OPE hyperparameters. Since this topic is still under-explored, we prefer not to specify any of these choices. Instead, we invite the community to innovate to ﬁnd which ofﬂine policy selection method works best.
Importantly, our benchmark allows for evaluation in both online and ofﬂine policy selection settings.
For each task, we clearly specify if it is intended for online vs ofﬂine policy selection. For ofﬂine policy selection tasks, we use a naive approach which we will describe in Section 4. We expect future work on ofﬂine policy selection methods to improve over this naive baseline. If a combination of ofﬂine RL method and ofﬂine policy selection can achieve perfect performance across all tasks, we believe this will mark an important milestone for ofﬂine methods in real-world applications. 3Sometimes referred to as online model selection, but we choose policy selection to avoid confusion with models of the environment as used in model based RL algorithms. 4
Table 1. DM Control Suite tasks. We reserved ﬁve tasks for online policy selection (top) and the rest four are reserved for the ofﬂine policy selection (bottom).
See Appendix E for reasoning behind choosing this particular task split.
Environment
No. episodes
Act. dim.
Cartpole swingup
Cheetah run
Humanoid run
Manipulator insert ball
Walker stand
Finger turn hard
Fish swim
Manipulator insert peg
Walker walk 40 300 3000 1500 200 500 200 1500 200 1 6 21 5 6 2 5 5 6
Table 2. DM Locomotion tasks. We reserved four tasks for online policy selection (top) and the rest three are reserved for the ofﬂine policy selection (bottom).
See Appendix E for reasoning behind choosing this particular task split.
Environment
No. episodes
Seq. length
Act. dim.
Humanoid corridor
Humanoid walls
Rodent gaps
Rodent two tap
Humanoid gaps
Rodent bowl escape
Rodent mazes 4000 4000 2000 2000 4000 2000 2000 2 40 2 40 2 40 40 56 56 38 38 56 38 38 3 Tasks
For each task domain we give a description of the tasks included, indicate which tasks are intended for online vs ofﬂine policy selection, and provide a description of the corresponding data. Let us note that we have not modiﬁed how the rewards are computed in the original environments we used to generate the datasets. For the details of those reward functions, we refer to the papers where the environments were introduced ﬁrst. 3.1 DM Control Suite
DeepMind Control Suite [Tassa et al., 2018] is a set of control tasks implemented in MuJoCo [Todorov et al., 2012]. We consider a subset of the tasks provided in the suite that cover a wide range of difﬁculties. For example, Cartpole swingup a simple task with a single degree of freedom is included.
Difﬁcult tasks are also included, such as Humanoid run, Manipulator insert peg, Manipulator insert ball. Humanoid run involves complex bodies with 21 degrees of freedom. And Manipulator insert ball/peg have not been shown to be solvable in any prior published work to the best of our knowledge.
In all the considered tasks as observations we use the default feature representation of the system state, consisting of proprioceptive information such as joint positions and velocity, as well as additional sensors and target position where appropriate. The observation dimension ranges from 5 to 67.
Data Description Most of the datasets in this domain are generated using D4PG. For the environ-ments Manipulator insert ball and Manipulator insert peg we use V-MPO [Song et al., 2020] to generate the data as D4PG is unable to solve these tasks. We always use 3 independent runs to ensure data diversity when generating data. All methods are run until the task is considered solved. For each method, data from the entire training run is recorded. As ofﬂine methods tend to require signiﬁcantly less data, we reduce the sizes of the datasets via sub-sampling. In addition, we further reduce the number of successful episodes in each dataset by 2/3 so as to ensure the datasets do not contain too many successful trajectories. See Table 1 for the size of each dataset. Each episode in this dataset contains 1000 time steps. 3.2 DM Locomotion
These tasks are made up of the corridor locomotion tasks involving the CMU Humanoid [Tassa et al., 2020], for which prior efforts have either used motion capture data [Merel et al., 2019a,b] or training from scratch [Song et al., 2020]. In addition, the DM Locomotion repository contains a set of tasks adapted to be suited to a virtual rodent [Merel et al., 2020]. We emphasize that the DM Locomotion tasks feature the combination of challenging high-DoF continuous control along with perception from rich egocentric observations.
Data description Note that for the purposes of data collection on the CMU humanoid tasks, we use expert policies trained according to Merel et al. [2019b], with only a single motor skill module from 5
Table 3: Atari games. We have 46 games in total in our Atari data release. We reserved 9 of the games for online policy selection (top) and the rest of the 37 games are reserved for the ofﬂine policy selection (bottom).
BEAMRIDER
DEMONATTACK
DOUBLEDUNK
ICE HOCKEY
MS. PACMAN
POOYAN
ROAD RUNNER
ROBOTANK
ZAXXON
ALIEN
AMIDAR
ASSAULT
ASTERIX
ATLANTIS
BANK HEIST
BATTLEZONE
BOXING
FROSTBITE
GOPHER
GRAVITAR
BREAKOUT
CARNIVAL
CENTIPEDE
CHOPPER COMMAND HERO
CRAZY CLIMBER
ENDURO
FISHING DERBY
FREEWAY
JAMES BOND
KANGAROO
KRULL
KUNG FU MASTER
TIME PILOT
UP AND DOWN
VIDEO PINBALL
WIZARD OF WOR
YARS REVENGE
NAME THIS GAME
PHOENIX
PONG
Q*BERT
RIVER RAID
SEAQUEST
SPACE INVADERS
STAR GUNNER motion capture that is reused in each task. For the rodent task, we use the same training scheme as proposed by Merel et al. [2020]. For the CMU humanoid tasks, each dataset is generated by 3 online methods whereas each dataset of the rodent tasks is generated by 5 online methods. Similarly to the control suite, data from entire training runs is recorded to further diversify the datasets. Each dataset is then sub-sampled and the number of its successful episodes reduced by 2/3. Since the sensing of the surroundings is done by egocentric cameras, all datasets in the locomotion domain include per-timestep egocentric camera observations of size 64 3. The use of egocentric observation also renders some environments partially observable and therefore necessitates recurrent architectures.
We therefore generate sequence datasets for tasks that require recurrent architectures. For dataset sizes and sequence lengths of see Table 2. 64
⇥
⇥ 3.3 Atari 2600
The Arcade Learning environment (ALE) [Bellemare et al., 2013] is a suite consisting of a diverse set of 57 Atari 2600 games (Atari57). It is a popular benchmark to measure the progress of online RL methods, and Atari has recently also become a standard benchmark for ofﬂine RL methods [Agarwal et al., 2020, Fujimoto et al., 2019] as well. In this paper, we are releasing a large and diverse dataset of gameplay following the protocol described by Agarwal et al. [2020], and use it to evaluate several discrete RL algorithms.
Data Description The dataset is generated by running an online DQN agent and recording transitions from its replay during training with sticky actions [Machado et al., 2018]. As stated in [Agarwal et al., 2020], for each game we use data from ﬁve runs with 50 million transitions each. States in each transition include stacks of four frames to be able to do frame-stacking with our baselines.
In our release, we provide experiments on the 46 of the Atari games that are available in OpenAI gym. OpenAI gym implements more than 46 games, but we only include games where the online
DQN’s performance that has generated the dataset was signiﬁcantly better than the random policy. We provide further information about the games we excluded in Appendix F. Among our 46 Atari games, we chose nine to allow for online policy selection. Speciﬁcally, we ordered all games according to the their difﬁculty,4 and picked every ﬁfth game as our ofﬂine policy section task to cover diverse set of games in terms of difﬁculty. In Table 3, we provide the full list of games that we decided to include in RL Unplugged. 3.4 Real-world Reinforcement Learning Suite
Dulac-Arnold et al. [2019, 2020] identify and evaluate respectively a set of 9 challenges that are bottlenecks to implementing RL algorithms, at scale, on applied systems. These include high-dimensional state and action spaces, large system delays, system constraints, multiple objectives, handling non-stationarity and partial observability. In addition, they have released a suite of tasks called realworldrl-suite5 which enables a practitioner to verify the capabilities of their algorithm on domains that include some or all of these challenges. The suite also deﬁnes a set of standardized challenges with varying levels of difﬁculty. As part of the “RL Unplugged” collection, we have 4The details of how we decide the difﬁculty of Atari games are provided in Appendix G. 5See https://github.com/google-research/realworldrl_suite for details. 6
generated datasets using the ‘easy‘ combined challenges on four tasks: Cartpole Swingup, Walker
Walk, Quadruped Walk and Humanoid Walk.
Data Description The datasets were generated as described in Section 2.8 of [Dulac-Arnold et al., 2020]; note that this is the ﬁrst data release based on those speciﬁcations. We used either the no challenge setting, which includes unperturbed versions of the tasks, or the easy combined challenge setting (see Section 2.9 of [Dulac-Arnold et al., 2020]), where data logs are generated from an environment that includes effects from combining all the challenges. Although the no challenge setting is identical to the control suite, the dataset generated for it is different as it is generated from ﬁxed sub-optimal policies. These policies were obtained by training 3 seeds of distributional
MPO [Abdolmaleki et al., 2018] until convergence with different random weight initializations, and then taking snapshots corresponding to roughly 75% of the converged performance. For the no challenge setting, three datasets of different sizes were generated for each environment by combining the three snapshots, with the total dataset sizes (in numbers of episodes) provided in Table 4. The procedure was repeated for the easy combined challenge setting. Only the “large data” setting was used for the combined challenge to ensure the task is still solvable. We consider all RWRL tasks as online policy selection tasks.
Table 4: real-world Reinforcement Learning Suite dataset sizes. Size is measured in number of episodes, with each episode being 1000 steps long.
Cartpole swingup Walker walk Quadruped walk Humanoid walk
Small dataset
Medium dataset
Large dataset 100 200 500 1000 2000 5000 100 200 500 4000 8000 20000 4 Baselines
We provide baseline results for a number of published algorithms for both continuous (DM Control
Suite, DM Locomotion), and discrete action (Atari 2600) domains. We will open-source implementa-tions of our baselines for the camera-ready. We follow the evaluation protocol presented in Section 2.2.
Our baseline algorithms include behavior cloning (BC [Pomerleau, 1989]); online reinforcement learning algorithms (DQN [Mnih et al., 2015], D4PG [Barth-Maron et al., 2018], IQN [Dabney et al., 2018]); and recently proposed ofﬂine reinforcement learning algorithms (BCQ [Fujimoto et al., 2018], BRAC [Wu et al., 2019], RABM [Siegel et al., 2020], REM [Agarwal et al., 2020]). Some algorithms only work for discrete or continuous actions spaces, so we only evaluate algorithms in domains they are suited to. Detailed descriptions of the baselines and our implementations (including hyperparameters) are presented in Section A in the supplementary material.
Naive approach for ofﬂine policy selection For the tasks we have marked for ofﬂine policy selec-tion, we need a strategy that does not use online interaction to select hyperparameters. Our naive approach is to choose the set of hyperparameters that performs best overall on the online policy selection tasks from the same domain. We do this independently for each baseline. This approach is motivated by how hyperparameters are often chosen in practice, by using prior knowledge of what worked well in similar domains. If a baseline algorithm drops in performance between online and ofﬂine policy selection tasks, this indicates the algorithm is not robust to the choice of hyperparame-ters. This is also cheaper than tuning hyperparameters individually for all tasks, which is especially relevant for Atari. For a given domain, a baseline algorithm and a hyperparameter set, we compute the average6 score over all tasks allowing online policy selection. The best hyperparameters are then applied to all ofﬂine policy selection tasks for this domain. The details of the experimental protocol and the ﬁnal hyperparameters are provided in the supplementary material. 4.1 DM Control Suite
In Figure 4, we compare baselines across the online policy selection tasks (left) and ofﬂine policy selection tasks (right). A table of results is included in Section B of the supplementary material. For the simplest tasks, such as Cartpole swingup, Walker stand, and Walker walk, where the performance 6We use the arithmetic mean with the exception of Atari where we use median following [Hessel et al., 2018]. 7
Figure 4: Baselines on DM Control Suite. (left) Performance using evaluation by online policy selection. (right) Performance using evaluation by ofﬂine policy selection. Horizontal lines for each task show 90th percentile of task reward in the dataset. Note that D4PG, BRAC, and RABM perform equally well on easier tasks e.g. Cartpole swingup. But BC, and RABM perform best on harder tasks e.g. Humanoid run.
Figure 5: Baselines on DM Locomotion. (left) Performance using evaluation by online policy selection. (right)
Performance using evaluation by ofﬂine policy selection. Horizontal lines for each task show 90th percentile of task reward in the dataset. The trend is similar to the harder tasks in DM Control Suite, i.e. BC and RABM perform well, while D4PG performs poorly. of ofﬂine RL is close to that of online methods, D4PG, BRAC and RABM are all good choices. But the picture changes on the more difﬁcult tasks, such as Humanoid run (which has high dimension action spaces), or Manipulator insert ball and manipulator insert peg (where exploration is hard).
Strikingly, in these domains BC is actually among the best algorithms alongside RABM, although no algorithm reaches the performance of online methods. This highlights how including tasks with diverse difﬁculty conditions in a benchmark gives a more complete picture of ofﬂine RL algorithms. 4.2 DM Locomotion
In Figure 5, we compare baselines across the online policy selection tasks (left) and ofﬂine policy selection tasks (right). A table of results is included in Section C of the supplementary material. This task domain is made exclusively of tasks that are high action dimension, hard exploration, or both.
As a result the stark trends seen above continue. BC, and RABM perform best, and D4PG performs quite poorly. We also could not make BCQ or BRAC perform well on these tasks, but we are not sure if this is because these algorithms perform poorly on these tasks, or if our implementations are missing a crucial detail. For this reason we do not include them. This highlights another key problem in online and ofﬂine RL. Papers do not include key baselines because the authors were not able to reproduce them, see eg [Peng et al., 2019, Fujimoto et al., 2019]. By releasing datasets, evaluation protocols and baselines, we are making it easier for researchers such as those working with BCQ to try their methods on these challenging benchmarks. 4.3 Atari 2600
In Figure 6, we present results for Atari using normalized scores. Due to the large number of tasks, we aggregate results using the median as done in [Agarwal et al., 2020, Hessel et al., 2018] (individual scores are presented in Appendix D). These results indicate that DQN is not very robust to the choice of hyperparameters. Unlike REM or IQN, DQN’s performance dropped signiﬁcantly on the ofﬂine policy selection tasks. BCQ, REM and IQN perform at least as well as the best policy in our training set according to our metrics. In contrast to other datasets (Section 4.1 and 4.2),
BC performs poorly on this dataset. Surprisingly, the performance of off-the-shelf off-policy RL algorithms is competitive and even surpasses BCQ on ofﬂine policy selection tasks. Combining behavior regularization methods (e.g., BCQ) with robust off-policy algorithms (REM, IQN) is a promising direction for future work. 8
Figure 6: Baselines on Atari. (left) Performance using evaluation by online policy selection. (right) Performance using evaluation by ofﬂine policy selection. The bars indicate the median normalized score, and the error bars show a bootstrapped estimate of the [25, 75] percentile interval for the median estimate computed across different games. The score normalization is done using the best performing policy among the mixture of policies that generated the ofﬂine Atari dataset (see Appendix H for details). 5