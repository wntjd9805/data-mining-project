Abstract
We study properties of Graph Convolutional Networks (GCNs) by analyzing their behavior on standard models of random graphs, where nodes are represented by random latent variables and edges are drawn according to a similarity kernel. This allows us to overcome the difﬁculties of dealing with discrete notions such as isomorphisms on very large graphs, by considering instead more natural geometric aspects. We ﬁrst study the convergence of GCNs to their continuous counterpart as the number of nodes grows. Our results are fully non-asymptotic and are valid for relatively sparse graphs with an average degree that grows logarithmically with the number of nodes. We then analyze the stability of GCNs to small deformations of the random graph model. In contrast to previous studies of stability in discrete settings, our continuous setup allows us to provide more intuitive deformation-based metrics for understanding stability, which have proven useful for explaining the success of convolutional representations on Euclidean domains. 1

Introduction
Graph Convolutional Networks (GCNs [9, 15, 26]) are deep architectures deﬁned on graphs inspired by classical Convolutional Neural Networks (CNNs [29]). In the past few years, they have been successfully applied to, for instance, node clustering [11], semi-supervised learning [26], or graph regression [23, 20], and remain one of the most popular variant of Graph Neural Networks (GNN).
We refer the reader to the review papers [7, 47] for more details.
Many recent results have improved the theoretical understanding of GNNs. While some architectures have been shown to be universal [36, 24] but not implementable in practice, several studies have characterized GNNs according to their power to distinguish (or not) graph isomorphisms [48, 12, 35] or compute combinatorial graph parameters [13]. However, such notions usually become moot for large graphs, which are almost never isomorphic to each other, but for which GCNs have proved to be successful in identifying large-scale structures nonetheless, e.g., for segmentation or spectral clustering [11]. Under this light, a relevant notion is that of stability: since GCNs are trained then tested on different (large) graphs, how much does a change in the graph structure affect its predictions?
In the context of signals deﬁned on Euclidean domains, including images or audio, convolutional representations such as scattering transforms or certain CNN architectures have been shown to be stable to spatial deformations [34, 5, 40]. However the notion of deformations is not well-deﬁned on
∗Equal contribution.
†Work done while AB was at Inria Paris. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
discrete graphs, and most stability studies for GCNs use purely discrete metrics that are less intuitive for capturing natural changes in structure [17, 19, 49].
In statistics and machine learning, there is a long history of modelling large graphs with random models, see for instance [6, 21, 27, 37] and references therein for reviews. Latent space models represent each node as a vector of latent variables and independently connect the nodes according to a similarity kernel applied to their latent representations. This large family of random graph models includes for instance Stochastic Block Models (SBM) [22], graphons [32], random geometric graphs
[38], or ε-graphs [10], among many others [37]. A key parameter in such models is the so-called sparsity factor αn that controls the number of edges in O(n2αn) with respect to the number of nodes n. The dense case αn ∼ 1 is the easiest to analyze, but often not realistic for real-world graphs. On the contrary, many questions are still open in the sparse case αn ∼ 1/n [1]. A middle ground, which will be the setting for our analysis, is the so-called relatively sparse case αn ∼ log n/n, for which several non-trivial results are known [30, 25], while being more realistic than the dense case.
Outline and contributions.
In this paper, we analyze the convergence and stability properties of
GCNs on large random graphs. We deﬁne a “continuous” counterpart to discrete GCNs acting on graph models in Section 2, study notions of invariance and equivariance to isomorphism of random graph models, and give convergence results when the number of nodes grows in Section 3. In particular, our results are fully non-asymptotic, valid for relatively sparse random graphs, and unlike many studies [45, 41] we do not assume that the similarity kernel is smooth or bounded away from zero. In Section 4, we analyze the stability of GCNs to small deformation of the underlying random graph model. Similar to CNNs [34, 5], studying GCNs in the continuous world allows us to deﬁne intuitive notions of model deformations and characterize their stability. Interestingly, for GCNs equivariant to permutation, we relate existing discrete notions of distance between graph signals to a Wasserstein-type metric between the corresponding continuous representations, which to our knowledge did not appear in the literature before.