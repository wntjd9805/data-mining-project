Abstract
Graph neural networks (GNNs) have been attracting increasing popularity due to their simplicity and effectiveness in a variety of ﬁelds. However, a large number of labeled data is generally required to train these networks, which could be very expensive to obtain in some domains. In this paper, we study active learning for
GNNs, i.e., how to efﬁciently label the nodes on a graph to reduce the annotation cost of training GNNs. We formulate the problem as a sequential decision process on graphs and train a GNN-based policy network with reinforcement learning to learn the optimal query strategy. By jointly training on several source graphs with full labels, we learn a transferable active learning policy which can directly generalize to unlabeled target graphs. Experimental results on multiple datasets from different domains prove the effectiveness of the learned policy in promoting active learning performance in both settings of transferring between graphs in the same domain and across different domains. 1

Introduction
Graphs encode the relations between different objects and are ubiquitous in real world. Learning effective representation of graphs is critical to a variety of applications. Recently, graph neural networks (GNNs) have been attracting growing attention for their effectiveness in graph representation learning [30, 33]. They have achieved great success on various tasks such as node classiﬁcation
[15, 27] and link prediction [4, 32]. Despite their appealing performance, GNNs typically require a large amount of labeled data for training [29]. However, in many domains, such as chemistry [11] and health care [6], it could be very expensive and time-consuming to collect a large amount of labeled data, which signiﬁcantly limits the performance of GNNs in these domains.
Active learning [1, 22, 25] is a promising strategy to tackle this challenge. The general idea of active learning is to dynamically query the labels of the most informative instances selected from the unlabeled data. Although active learning has been proven effective on independent and identically distributed (i.i.d.) data such as natural language processing [24] and computer vision [9], how to apply it to graph-structured data with dense interconnections between different instances remains under-explored. This motivates us to study active learning on graphs, i.e., how to efﬁciently label the nodes on a graph to reduce the annotation cost of training GNNs.
Towards this goal, several methods have been proposed recently [8, 13, 12, 3, 10, 5]. To measure the informativeness of each node, they either design a single selection criterion based on the graph characteristics, or adaptively combine several selection criteria. Then the most "informative" node is labeled at each query step according to the selection criterion. However, these methods suffer from the following limitations. (1) Ignoring Long-term Performance: Existing methods usually adopt 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
different selection criterion as a surrogate objective function and greedily optimize it at each query step. However, the long-term objective function we truly want to optimize is how to select a sequence of nodes which can maximize the ﬁnal performance score of the GNN trained on them. Maximizing the short-term surrogate criterion usually leads to sub-optimal query strategies. (2) Lack of Node
Interactions: When measuring node informativeness, existing methods usually consider each node independently and ignore the interconnections between different nodes. For example, if an unlabeled node has several labeled neighbors, then it is very likely that this node would provide little additional information for training.
To tackle these limitations, we propose GPA, a Graph Policy network for transferable Active learning on graphs. Our approach formalizes active learning on graphs as a Markov decision process (MDP) and learns the optimal query strategy with reinforcement learning (RL), where the state is deﬁned based on the current graph status, the action is to select a node for annotation at each query step, and the reward is the performance gain of the GNN trained with the selected nodes. By maximizing its long-term returns with policy gradient [26], our policy network can effectively learn to optimize the long-term performance of the GNN in an end-to-end fashion. Moreover, our approach parameterizes the policy network as another GNN to explicitly model node interactions, which effectively propagates useful information over the graph and thereby better measures node informativeness. We train the graph policy network on multiple training graphs where node labels are available, and directly transfer the learned policy to perform active learning on unseen test graphs where no labels are provided initially. The learned policy does not require any additional retraining or ﬁne-tuning on the test graphs, thus is a zero-shot policy transfer.
We evaluate GPA on the standard semi-supervised node classiﬁcation task under two experimental settings with increasing difﬁculty: 1) the training graphs and testing graphs are from the same domain; 2) the training graphs and testing graphs are from different domains. Experimental results prove the effectiveness of GPA over competitive baselines under both settings. 2