Abstract
?
We study the reinforcement learning problem in the setting of ﬁnite-horizon episodic Markov Decision Processes (MDPs) with S states, A actions, and episode length H. We propose a model-free algorithm UCB-ADVANTAGE and prove that it achieves ˜Op
H 2SAT q regret where T “ KH and K is the number of episodes to play. Our regret bound improves upon the results of [Jin et al., 2018] and matches the best known model-based algorithms as well as the information theo-retic lower bound up to logarithmic factors. We also show that UCB-ADVANTAGE achieves low local switching cost and applies to concurrent reinforcement learning, improving upon the recent results of [Bai et al., 2019]. 1

Introduction
Reinforcement learning (RL) [Burnetas and Katehakis, 1997] studies the problem where an agent aims to maximize its accumulative rewards through sequential decision making in an unknown environment modeled by Markov Decision Processes (MDPs). At each time step, the agent observes the current state s and interacts with the environment by taking an action a and transits to next state s1 following the underlying transition model.
There are mainly two types of algorithms to approach reinforcement learning: model-based and model-free learning. Model-based algorithms learn a model from the past experience and make decision based on this model while model-free algorithms only maintain a group of value functions and take the induced optimal actions. Because of these differences, model-free algorithms are usually more space- and time-efﬁcient compared to model-based algorithms. Moreover, because of their simplicity and ﬂexibility, model-free algorithms are popular in a wide range of practical tasks (e.g.,
DQN [Mnih et al., 2015], A3C [Mnih et al., 2016], TRPO [Schulman et al., 2015a], and PPO
[Schulman et al., 2017]). On the other hand, however, it is believed that model-based algorithms may be able to take the advantage of the learned model and achieve better learning performance in terms of regret or sample complexity, which has been empirically evidenced by Deisenroth and Rasmussen
[2011] and Schulman et al. [2015a]. Much experimental research has been done for both types of the algorithms, and given that there has been a long debate on their pros and cons that dates back to [Deisenroth and Rasmussen, 2011], a natural and intriguing theoretical question to study about reinforcement learning algorithms is that –
Question 1. Is it possible that model-free algorithms achieve as competitive learning efﬁciency as model-based algorithms, while still maintaining low time and space complexities? 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Towards answering this question, the recent work by Jin et al. [2018] formally deﬁnes that an RL algorithm is model-free if its space complexity is always sublinear relative to the space required to store the MDP parameters, and then proposes a model-free algorithm (which is a variant of the
Q-learning algorithm [Watkins, 1989]) that achieves the ﬁrst
T -type regret bound for ﬁnite-horizon episodic MDPs in the tabular setting (i.e., discrete state spaces). However, there is still a gap of
H between the regret of their algorithm and the best model-based algorithms. In this work, factor we close this gap by proposing a novel model-free algorithm, whose regret matches the optimal model-based algorithms, as well as the information theoretic lower bound. The results suggest that model-free algorithms can learn as efﬁciently as model-based ones, giving an afﬁrmative answer to
Question 1 in the setting of episodic tabular MDPs.
?
? 1.1 Our Results
Main Theorem. We propose a novel variant of the Q-learning algorithm, UCB-ADVANTAGE. We then prove the following main theorem of the paper.
Theorem 1. For T greater than some polynomial of S, A, and H, and for any p P p0, 1q, with probability p1 ´ pq, the regret of UCB-ADVANTAGE is bounded by RegretpT q ď ˜Op
H 2SAT q, where poly-logarithmic factors of T and 1{p are hidden in the ˜Op¨q notation.
?
?
?
?
Compared to the ˜Op
H 3SAT q regret bound of the UCB-Bernstein algorithm in [Jin et al., 2018],
H, and matches the information theoretic lower bound of
UCB-ADVANTAGE saves a factor of
H 2SAT q in [Jin et al., 2018] up to logarithmic factors. The regret of UCB-ADVANTAGE is at
Ωp the same order of the best model-based algorithms such as UCBVI [Azar et al., 2017] and vUCQ
[Kakade et al., 2018].1 However, the time complexity before time step T is OpT q and the space complexity is OpSAHq for UCB-ADVANTAGE. In contrast, both UCBVI and vUCQ uses ˜OpT S2Aq time and OpS2AHq space.
One of the main technical ingredients of UCB-ADVANTAGE is to incorporate a novel update rule for the Q-function based on the proposed reference-advantage decomposition. More speciﬁcally, we propose to view the optimal value function V ˚ as V ˚ “ V ref ` pV ˚ ´ V ref q, where V ref , the reference component, is a comparably easier learned approximate of V ˚ and the other component pV ˚ ´ V ref q is referred to as the advantage part. Based on this decomposition, the new update rule learns the corresponding parts of the Q-function using carefully designed (and different) subsets of the collected data, so as to minimize the deviation, maximize the data utilization, and reduce the estimation variance.
Another highlight of UCB-ADVANTAGE is the use of the stage-based update framework which enables an easy integration of the new update rule (as above) and the standard update rule. In such a framework, the visits to each state-action pair are partitioned into stages, which are used to design the trigger and subsets of data for each update.
Implications. An extra beneﬁt of the stage-based update framework is to ensure the low frequency of policy switches of UCB-ADVANTAGE, stated as follows.
Theorem 2. The local switching cost of UCB-ADVANTAGE is bounded by OpSAH 2 log T q.
While one may refer to Appendix C for the details of the theorem, the notion of local switching cost for RL is recently introduced and studied by Bai et al. [2019], where the authors integrate a lazy update scheme with the UCB-Bernstein algorithm [Jin et al., 2018] and achieve ˜Op
H 3SAT q regret and OpSAH 3 log T q local switching cost. In contrast, our result improves in both metrics of regret and switching cost.
?
Our results also apply to concurrent RL, a research direction closely related to batched learning and learning with low switching costs, stated as follows.
Corollary 3. Given M parallel machines, the concurrent and pure exploration version of UCB-ADVANTAGE can compute an (cid:15)-optimal policy in ˜OpH 2SA ` H 3SA{p(cid:15)2M qq concurrent episodes. 1Both Azar et al. [2017] and Kakade et al. [2018] assume equal transition matrices P1 “ P2 “ ¨ ¨ ¨ “ PH .
In this work, we adopt the same setting as in, e.g., [Jin et al., 2018] and [Bai et al., 2019], where P1, P2, . . . , PH
H to the regret analysis in [Azar et al., 2017] and [Kakade et al., 2018]. can be different. This adds a factor of
? 2
In contrast, the state-of-the-art result [Bai et al., 2019] uses ˜OpH 3SA ` H 4SA{p(cid:15)2M qq concurrent episodes. When M “ 1, Corollary 3 implies that the single-threaded exploration version of UCB-ADVANTAGE uses ˜OpH 3SA{(cid:15)2q episodes to learn an (cid:15)-optimal policy. In Appendix C, we provide a simple ΩpH 3SA{(cid:15)2q-episode lower bound for the sample complexity, showing the optimality up to logarithmic factors. 1.2 Additional