Abstract
We introduce Sparse Symplectically Integrated Neural Networks (SSINNs), a novel model for learning Hamiltonian dynamical systems from data. SSINNs combine fourth-order symplectic integration with a learned parameterization of the Hamilto-nian obtained using sparse regression through a mathematically elegant function space. This allows for interpretable models that incorporate symplectic inductive biases and have low memory requirements. We evaluate SSINNs on four classical
Hamiltonian dynamical problems: the Hénon-Heiles system, nonlinearly coupled oscillators, a multi-particle mass-spring system, and a pendulum system. Our results demonstrate promise in both system prediction and conservation of energy, often outperforming the current state-of-the-art black-box prediction techniques by an order of magnitude. Further, SSINNs successfully converge to true governing equations from highly limited and noisy data, demonstrating potential applicability in the discovery of new physical governing equations. 1

Introduction
Neural networks have demonstrated great ability in tasks ranging from text generation to image classiﬁcation [33, 9, 8, 49, 20]. While impressive, most of these tasks can be easily performed by an intelligent human. The novelty is in a machine performing the task, rather than the task itself. Researchers are becoming increasingly interested in the role that machine learning can play in assisting humans–a role of discovery rather than of automation [16, 34, 21]. To this end, there has been signiﬁcant research interest in applying machine learning to physical dynamical systems
[12, 6, 24, 25, 32, 3, 35, 4]. Physical dynamical systems are everywhere, from weather to astronomy to protein folding [26, 15, 38, 44]. The underlying dynamics of many of these systems have yet to be unraveled, and many evade accurate prediction over time. Building accurate predictive models can spur signiﬁcant technological, medical, and scientiﬁc advancement. Unfortunately, data from physical systems is often challenging to acquire and tainted with measurement and discretization error. Consequently, a primary challenge in this domain is that data-driven techniques must be able to cope with highly limited and noisy data.
Our work seeks to answer the following question: given the historical data of some physical dynamical system, how can we not only predict its future states, but also discern its underlying governing equa-tions? We focus speciﬁcally on energy-preserving systems that can be described in the Hamiltonian formalism. There is a growing body of research in this domain, but much of it relies upon black-box machine learning techniques that solve the prediction problem while neglecting the governing equa-tions problem [12, 6]. Namely, there are two primary paradigms for incorporating physical priors into neural networks: by embedding structure-enforcing algorithms into the network architecture, (e.g., a symplectic integrator [6]), or by enforcing a set of known mathematical constraints (e.g., the Navier-Stokes equations) in the loss function [36]. Integrator-embedded networks function as a black-box, and networks that use modiﬁed loss functions assume some knowledge of the system beforehand. For machine learning to truly play a role in human discovery, interpretability should be a paramount consideration. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Sparse regression has been used with great success to discover the underlying mathematical formulas of dynamical systems [4, 28, 48]. In this paper, we combine successful ideas from sparse regression equation discovery and black-box prediction techniques to introduce a new model, which we call
Sparse Symplectically Integrated Neural Networks (SSINNs). Like many previous approaches,
SSINNs ultimately seek to parameterize an equation called the Hamiltonian, from which a system can be readily predicted. To assist in this parameterization, our models incorporate a fourth-order symplectic integrator to ensure that the symplectic structure of the Hamiltonian is preserved, an embedded physics prior only employed in black-box approaches thus far. This also allows for continuous-time prediction. To incorporate interpretability, we utilize sparse regression through a mathematically elegant space of functions, allowing our models to learn which terms in the function space are part of the governing equation and which are not. This sparsity prior holds for nearly all governing equations and, experimentally, improves prediction performance and equation convergence.
Constructing a model in this way comes with a number of beneﬁts over both black-box prediction techniques and current state-of-the-art methods for learning underlying equations. Black-box methods operate within incredibly large function spaces and, as a result, frequently converge to complicated functions that approximate the underlying dynamics of the system but offer no insight into its true gov-erning equations. Unlike black-box methods, SSINNs are interpretable; by this, we mean that, once trained, a mathematically elegant governing equation that oftentimes represents the true governing equation of the system can easily be extracted from the model. Due to this interpretability, SSINNs also maintain far fewer trainable parameters (often <1% of black-box models) and consequently do not require specialized hardware to use once trained. When compared to current methods for learning governing equations, SSINNs incorporate a symplectic bias that reduces the number of possible solutions by placing restrictions on the numerical integration of the learned equation.
In summary, we propose SSINNs, which make the following contributions1:
• Incorporate symplectic biases to augment learning governing equations from data.
• Outperform state-of-the-art black-box prediction approaches by an order of magnitude on multiple classical Hamiltonian dynamical systems.
• Succeed in learning nonlinear Hamiltonian equations from as few as 200 noisy data points. 2