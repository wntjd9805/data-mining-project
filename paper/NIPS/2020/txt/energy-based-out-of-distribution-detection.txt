Abstract
Determining whether inputs are out-of-distribution (OOD) is an essential building block for safely deploying machine learning models in the open world. However, previous methods relying on the softmax conﬁdence score suffer from overconﬁ-dent posterior distributions for OOD data. We propose a uniﬁed framework for
OOD detection that uses an energy score. We show that energy scores better distin-guish in- and out-of-distribution samples than the traditional approach using the softmax scores. Unlike softmax conﬁdence scores, energy scores are theoretically aligned with the probability density of the inputs and are less susceptible to the overconﬁdence issue. Within this framework, energy can be ﬂexibly used as a scoring function for any pre-trained neural classiﬁer as well as a trainable cost function to shape the energy surface explicitly for OOD detection. On a CIFAR-10 pre-trained WideResNet, using the energy score reduces the average FPR (at TPR 95%) by 18.03% compared to the softmax conﬁdence score. With energy-based training, our method outperforms the state-of-the-art on common benchmarks. 1

Introduction
The real world is open and full of unknowns, presenting signiﬁcant challenges for machine learning models that must reliably handle diverse inputs. Out-of-distribution (OOD) uncertainty arises when a machine learning model sees an input that differs from its training data, and thus should not be predicted by the model. Determining whether inputs are out-of-distribution is an essential problem for deploying ML in safety-critical applications such as rare disease identiﬁcation. A plethora of recent research has studied the issue of out-of-distribution detection [2, 3, 13–15, 19, 22, 23, 26].
Previous approaches rely on the softmax conﬁdence score to safeguard against OOD inputs [13].
An input with a low softmax conﬁdence score is classiﬁed as OOD. However, neural networks can produce arbitrarily high softmax conﬁdence for inputs far away from the training data [29]. Such a failure mode occurs since the softmax posterior distribution can have a label-overﬁtted output space, which makes the softmax conﬁdence score suboptimal for OOD detection.
In this paper, we propose to detect OOD inputs using an energy score, and provide both mathematical insights and empirical evidence that the energy score is superior to both a softmax-based score and generative-based methods. The energy-based model [20] maps each input to a single scalar that is lower for observed data and higher for unobserved ones. We show that the energy score is desirable 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
for OOD detection since it is theoretically aligned with the probability density of the input—samples with higher energies can be interpreted as data with a lower likelihood of occurrence. In contrast, we show mathematically that the softmax conﬁdence score is a biased scoring function that is not aligned with the density of the inputs and hence is not suitable for OOD detection.
Importantly, the energy score can be derived from a purely discriminative classiﬁcation model without relying on a density estimator explicitly, and therefore circumvents the difﬁcult optimization process in training generative models. This is in contrast with JEM [11], which derives the likelihood score log p(x) from a generative modeling perspective. JEM’s objective can be intractable and unstable to optimize in practice, as it requires the estimation of the normalized densities over the entire input space to maximize the likelihood. Moreover, while JEM only utilizes in-distribution data, our framework allows exploiting both the in-distribution and the auxiliary outlier data to shape the energy gap ﬂexibly between the training and OOD data, a learning method that is much more effective than
JEM or Outlier Exposure [14].
Contributions. We propose a uniﬁed framework using an energy score for OOD detection.1 We show that one can ﬂexibly use energy as both a scoring function for any pre-trained neural classiﬁer (without re-training), and a trainable cost function to ﬁne-tune the classiﬁcation model. We demonstrate the effectiveness of energy function for OOD detection for both use cases.
•
•
At inference time, we show that energy can conveniently replace softmax conﬁdence for any pre-trained neural network. We show that the energy score outperforms the softmax conﬁ-dence score [13] on common OOD evaluation benchmarks. For example, on WideResNet, the energy score reduces the average FPR (at 95% TPR) by 18.03% on CIFAR-10 compared to using the softmax conﬁdence score. Existing approaches using pre-trained models may have several hyperparameters to be tuned and sometimes require additional data. In contrast, the energy score is a parameter-free measure, which is easy to use and implement, and in many cases, achieves comparable or even better performance.
At training time, we propose an energy-bounded learning objective to ﬁne-tune the network.
The learning process shapes the energy surface to assign low energy values to the in-distribution data and higher energy values to OOD training data. Speciﬁcally, we regularize the energy using two square hinge loss terms, which explicitly create the energy gap between in- and out-of-distribution training data. We show that the energy ﬁne-tuned model outperforms the previous state-of-the-art method evaluated on six OOD datasets. Compared to the softmax-based ﬁne-tuning approach [14], our method reduces the average FPR (at 95% TPR) by 10.55% on CIFAR-100. This ﬁne-tuning leads to improved OOD detection performance while maintaining similar classiﬁcation accuracy on in-distribution data.
The rest of the paper is organized as follows. Section 2 provides the background of energy-based models. In Section 3, we present our method of using energy score for OOD detection, and experi-mental results in Section 4. Section 5 provides an comprehensive literature review on OOD detection and energy-based learning. We conclude in Section 6, with discussion on broader impact in Section 7. 2