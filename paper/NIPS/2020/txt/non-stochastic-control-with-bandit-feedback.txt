Abstract
We study the problem of controlling a linear dynamical system with adversarial perturbations where the only feedback available to the controller is the scalar loss, and the loss function itself is unknown. For this problem, with either a known or unknown system, we give an efﬁcient sublinear regret algorithm. The main algorithmic difﬁculty is the dependence of the loss on past controls. To overcome this issue, we propose an efﬁcient algorithm for the general setting of bandit convex optimization for loss functions with memory, which may be of independent interest. 1

Introduction
The ﬁelds of Reinforcement Learning (RL), as well as its differentiable counterpart of Control, formally model the setting of learning through interaction in a reactive environment. The crucial component in RL/control that allows learning is the feedback, or reward/penalty, which the agent iteratively observes and reacts to.
While some signal is necessary for learning, different applications have different feedback to the learning agent. In many reinforcement learning and control problems it is unrealistic to assume that the learner has feedback for actions other than their own. One example is in game-playing, such as the game of Chess, where a player can observe the adversary’s move for their own choice of play, but it is unrealistic to expect knowledge of the adversary’s play for any possible move. This type of feedback is commonly known in the learning literature as “bandit feedback”.
Learning in Markov Decision Processes (MDP) is a general and difﬁcult problem for which there are no known algorithms that have sublinear dependence on the number of states. For this reason we look at structured MDPs, and in particular the model of control in Linear Dynamical Systems (LDS), a highly structured special case that is known to admit more efﬁcient methods as compared to general
RL.
In this paper we study learning in linear dynamical systems with bandit feedback. This generalizes the well-known Linear Quadratic Regulator to systems with only bandit feedback over any convex loss function. Further, our results apply to the non-stochastic control problem which allows for adversarial perturbations and adversarially chosen loss functions, even when the underlying linear system is unknown. 1.1 Our Results
We give the ﬁrst sublinear regret algorithm for controlling a linear dynamical system with bandit feedback in the non-stochastic control model. Speciﬁcally, we consider the case in which the underlying system is linear, but has potentially adversarial perturbations (that can model deviations 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
from linearity), i.e. xt+1 = Axt + But + wt, where xt ∈ Rn is the (observed) dynamical state, ut ∈ Rm is a learner-chosen control and wt ∈ Rn is an adversarial perturbation. The goal of the controller is to minimize a sum of sequentially revealed adversarial cost functions ct(xt, ut) over the state-control pairs that it visits. More precisely, the goal of the learner in this adversarial setting is to minimize regret compared to a class of policies Π: (1.1)
Regret =
T (cid:88) t=1 ct(xt, ut) − min
π∈Π
T (cid:88) t=1 ct(xπ t , uπ t ), where the cost of the benchmark is measured on the counterfactual state-action sequence (xπ t , uπ t ) that the benchmark policy in consideration visits, as opposed to the state-sequence visited by the the learner. The target class of policies we compare against in this paper are disturbance action controllers (DAC), whose control is a linear function of past disturbances plus a stabilizing linear operator over the current state ut = Kxt + (cid:80)H i=1 Miwt−i, for some history-length parameter H. This comparator class is known to be more general than state-feedback laws and linear dynamical controllers (LDC).
This choice is a consequence of recent advances in convex relaxation for control [4, 5, 16, 32].
For the setting we consider, the controller can only observe the scalar ct(xt, ut), and does not have access to the gradients or any other information about the loss. Our main results are efﬁcient algorithms for the non-stochastic control problem which attain the following guarantees:
Theorem 1.1 (Informal Statement). For a known linear dynamical system where the perturbations wt (and convex costs ct) are bounded and chosen by an adversary, there exists an efﬁcient algorithm that with bandit feedback generates an adaptive sequence of controls {ut} for which
Regret = (cid:101)O(poly(natural-parameters)T 3/4).
This theorem can be further extended to unknown systems:
Theorem 1.2 (Informal Statement). For an unknown linear dynamical system where the perturba-tions wt (and convex costs ct) are bounded and chosen by an adversary, there exists an efﬁcient algorithm that with bandit feedback generates an adaptive sequence of controls {ut} for which
Regret = (cid:101)O(poly(natural-parameters)T 3/4).
Techniques. To derive these results, we combine the convex relaxation technique of [4] with the non-stochastic system identiﬁcation method for environments with adversarial perturbations from
[16, 30]. However, the former result relies on gradient based optimization methods, and it is non-trivial to apply gradient estimation techniques in this black-box zero-order information setting. The main difﬁculty stems from the fact that the gradient-based methods from non-stochastic control apply to functions with memory, and depend on the system state going back many iterations. The natural way of creating unbiased gradient estimates, such as in [14], have no way of accounting for functions with memory.
To solve this difﬁculty, we introduce an efﬁcient algorithm for the setting of bandit convex opti-mization with memory. This method combines the gradient-based methods of [6] with the unbiased gradient estimation techniques of [14]. The naive way of combining these techniques introduces time dependencies between the random gradient estimators, as a direct consequence of the memory in the loss functions. To resolve this issue, we introduce an artiﬁcial intentional delay to the gradient updates and show that this delay has only a limited effect on the overall regret.
Paper outline. After describing related work, we cover preliminaries and deﬁne notation in section 2. In section 3 we describe the algorithm for BCO with memory and the main theorem regarding its performance. We then introduce the bandit control setting in section 4, and provide algorithms for known and unknown systems in sections 5 and 6 respectively, together with relevant theoretical results. We then present experimental results in section 7. 2
1.2