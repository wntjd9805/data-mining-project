Abstract
As bandit algorithms are increasingly utilized in scientiﬁc studies and industrial applications, there is an associated increasing need for reliable inference methods based on the resulting adaptively-collected data. In this work, we develop methods for inference on data collected in batches using a bandit algorithm. We ﬁrst prove that the ordinary least squares estimator (OLS), which is asymptotically normal on independently sampled data, is not asymptotically normal on data collected using standard bandit algorithms when there is no unique optimal arm. This asymptotic non-normality result implies that the naive assumption that the OLS estimator is approximately normal can lead to Type-1 error inﬂation and conﬁdence intervals with below-nominal coverage probabilities. Second, we introduce the Batched OLS estimator (BOLS) that we prove is (1) asymptotically normal on data collected from both multi-arm and contextual bandits and (2) robust to non-stationarity in the baseline reward. 1

Introduction
Due to their regret minimizing guarantees bandit algorithms have been increasingly used in in real-world sequential decision-making problems, like online advertising [27], mobile health [42], and online education [34]. However, for many real-world problems it is not enough to just minimize regret on a particular problem instance. For example, suppose we have run an online education experiment using a bandit algorithm where we test different types of teaching strategies. When designing a new online course, ideally we could use the data from the previous experiment to inform the design, e.g., under-performing arms could be eliminated or modiﬁed. Moreover, to help others designing online courses we would like to be able to publish our ﬁndings about how different teaching strategies compare in their performance. This example demonstrates the need for statistical inference methods on bandit data, which allow practitioners to draw generalizable knowledge from the data they have collected (e.g., how much better one teaching strategy is compared to another) for the sake of scientiﬁc discovery and informed decision making.
In this work we will focus on methods to construct conﬁdence intervals for the margin—the difference in expected rewards of two bandit arms—from batched bandit data. Rather than constructing high probability conﬁdence intervals, we are interested in constructing conﬁdence intervals by using the asymptotic distribution of estimators to approximate their ﬁnite sample distribution. Asymptotic approximation methods for statistical inference has a long history of being successful in science and leads to much narrower conﬁdence intervals than those constructed using high probability bounds.
Most statistical inference methods based on asymptotic approximation assume that treatments 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
are assigned independently [15]. However, bandit data violates this independence assumption because it is collected adaptively, meaning previous actions and rewards inform future action selections. The non-independence makes statistical inference more challenging, e.g., estimators like the sample mean are often biased on bandit data [32, 37].
Throughout, we focus on the batched bandit setting, in which arms of the bandit are pulled in batches.
For our asymptotic analysis we ﬁx the total number of batches, T , and allow the arm pulls in each batch, n, to go to inﬁnity. Note that we do not need or expect n to go to inﬁnity for real-world experiments; we use the asymptotic distribution of estimators to approximate their ﬁnite-sample distribution when constructing conﬁdence intervals. We focus on the batched setting because it closely reﬂects many of the problem settings where bandit algorithms are applied. For example, in many mobile health [42, 23, 28] and online education problems [22, 34] multiple users use apps
/ take courses simultaneously, so a batch corresponds to the number of unique users the bandit algorithm acts on at once. The batched setting is even common in online recommendations and advertising because it is impractical to update the bandit after every action if many users visit the site simultaneously [38, 36, 12, 26]. In many such experimental settings the length of the study, T , cannot be arbitrarily adjusted, e.g., in online education, courses generally cannot be made arbitrarily long, and clinical trials often run for a standard amount of time that depends on the domain science (e.g. the length of mobile health studies is a function of the scientiﬁc community’s belief in how long it should take for users to form a habit). On the other hand, the number of users, n, can in principle grow as large as funding allows.
Additionally, in our batched setting, we assume that the means of the arms can change over time, i.e., from batch to batch, which reﬂects the temporal non-stationarity that is prevalent in many real world bandit application problems. For example, in online recommendation systems, the click through rate of a given recommendation typically varies over time, e.g., breaking news articles become less popular over time [38, 26]. Online education and mobile health are also highly non-stationary problems because users tend to disengage over time, so the same notiﬁcation may be much less effective if sent near the end of an experiment than sent near the beginning [9, 21, 7]. Our statistical inference method does not need to assume that the number of stationary time periods in the experiment is large and is robust to temporal non-stationarity from batch to batch.
The ﬁrst contribution of this work is proving that on bandit data, rather surprisingly, whether standard estimators are asymptotically normal can depend on whether the margin is zero. We prove that for common bandit algorithms, the arm selection probabilities only concentrate if there is a unique optimal arm. Thus, for two-arm bandits, the arm selection probabilities do not concentrate when the margin—the difference in the expected rewards between the arms—is zero. We show that this leads the ordinary least squares (OLS) estimator to be asymptotically normal when the margin is non-zero, and asymptotically not normal when the margin is zero. Since the OLS estimator does not converge uniformly (over values of the margin), standard inference methods (normal approximations, bootstrap1) can lead to inﬂated Type-1 error and unreliable conﬁdence intervals on bandit data.
The second contribution of this work is introducing the Batched OLS (BOLS) estimator, which can be used for reliable inference—even in non-stationary settings—on data collected with batched bandits. We prove that, regardless of whether the margin is zero or not, the BOLS estimator for the margin for both multi-arm and contextual bandits is asymptotically normal and thus can be used for both hypothesis testing and obtaining conﬁdence intervals. Moreover, BOLS is also automatically robust to non-stationarity in the rewards and can be used for constructing valid conﬁdence intervals even if there is non-stationarity in the baseline reward, i.e., if the rewards of the arms change from batch to batch, but the margin remains constant. If the margin itself is also non-stationary, BOLS can also be used for constructing simultaneous conﬁdence intervals for the margins for each batch. 2