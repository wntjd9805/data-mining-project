Abstract
We propose a ﬂexible gradient-based framework for learning linear programs from optimal decisions. Linear programs are often speciﬁed by hand, using prior knowl-edge of relevant costs and constraints. In some applications, linear programs must instead be learned from observations of optimal decisions. Learning from optimal decisions is a particularly challenging bilevel problem, and much of the related inverse optimization literature is dedicated to special cases. We tackle the general problem, learning all parameters jointly while allowing ﬂexible parametrizations of costs, constraints, and loss functions. We also address challenges speciﬁc to learning linear programs, such as empty feasible regions and non-unique optimal decisions. Experiments show that our method successfully learns synthetic linear programs and minimum-cost multi-commodity ﬂow instances for which previous methods are not directly applicable. We also provide a fast batch-mode PyTorch implementation of the homogeneous interior point algorithm, which supports gradients by implicit differentiation or backpropagation. 1

Introduction
In linear programming, the goal is to make an optimal decision given a linear objective and subject to linear constraints. Traditionally, a linear program is designed using knowledge of relevant costs and constraints. More recently, methodologies that are data-driven have emerged.
Inverse optimization (IO) [Burton and Toint, 1992, Troutt, 1995, Ahuja and Orlin, 2001], in contrast, learns linear programs from observations of optimal decisions rather than of the costs or constraints themselves. The IO approach is particularly important when observations come from optimizing agents (e.g., experts [Chan et al., 2014, Bärmann et al., 2017] or customers [Dong et al., 2018]) who make near-optimal decisions with respect to their internal (unobserved) optimization models.
From a machine learning perspective, the IO setup is as follows: we are given feature vectors u1, u2, . . . , uN } representing conditions (e.g., time, prices, weather) and we observe the corre-{ sponding decision targets (e.g., quantities, actions) determined by an unknown optimization process, which in our case is assumed linear. We view IO as the problem of inferring a constrained optimization model that gives identical (or equivalent) decisions, and which generalizes to novel conditions u. The family of candidate models is assumed parametrized by some vector w.
, . . . , xobs
N }
, xobs 2 xobs 1
{
Learning a constrained optimizer that makes the observations both feasible and optimal poses multiple challenges that have not been explicitly addressed. For instance, parameter setting w1 in Figure 1 makes the observed decision xobs optimal but not feasible, w2 produces exactly the opposite result, and some w values (black-hatched region in Figure 1) are not even admissible because they will result in empty feasible regions. Finding a parameter such as w3 that is consistent with the observations can be difﬁcult. We formulate the learning problem in a novel way, and tackle it with gradient-based methods despite the inherent bilevel nature of learning. Using gradients from backpropagation or 1 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: A depiction of our constrained learning formulation. We learn a parametric linear program (PLP), here parametrized by a feature u and weights w = (w1, w2) and using a single training observation (u1, xobs
). The PLP corresponding to three parameter settings w1, w2, w3 are shown, with the cost vector and feasible region corresponding to u1 emphasized. The goal of learning is to
ﬁnd solutions such as w⇤ = w3. (See Appendix for the speciﬁc PLP used in this example.) 1 implicit differentiation, we successfully learn linear program instances of various sizes as well as learning the costs and right-hand coefﬁcients of a minimum-cost multi-commodity ﬂow problem. 2
D, b
RM1⇥
In a linear program (LP), the values of decision variables x
RD
Parametric Linear Programs
RD, inequality constraint coefﬁcients must be determined, whereas the cost coefﬁcients c
RM2 are all treated
A 2 as constants. In a parametric linear program (PLP), the coefﬁcients (and therefore the optimal decisions) may depend on features u. In order to infer a PLP from data, one may deﬁne a suitable hypothesis space parametrized by w. We refer to this hypothesis space as the form of our forward optimization problem (FOP).
RM1 , and equality constraint coefﬁcients G
RM2⇥
D, h 2 2 2 2 minx cT x s.t. Ax b

Gx = h (LP) minx c(u)T x s.t. A(u)x b(u)

G(u)x = h(u) (PLP) minx c(u, w)T x s.t. A(u, w)x b(u, w)

G(u, w)x = h(u, w) (FOP)
A choice of hypothesis w in (FOP) identiﬁes a PLP, and a subsequent choice of conditions u identiﬁes an LP. The LP can then be solved to yield an optimal decision x⇤ under the model. These predictions of optimal decisions can be compared to observations at training time, or can be used to anticipate optimal decisions under novel conditions u at test time. 2