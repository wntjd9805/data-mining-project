Abstract
Previous research in fair classiﬁcation mostly focuses on a single decision model. In reality, there usually exist multiple decision models within a system and all of which may contain a certain amount of discrimination. Such realistic scenarios introduce new challenges to fair classiﬁcation: since discrimination may be transmitted from upstream models to downstream models, building decision models separately without taking upstream models into consideration cannot guarantee to achieve fairness. In this paper, we propose an approach that learns multiple classiﬁers and achieves fairness for all of them simultaneously, by treating each decision model as a soft intervention and inferring the post-intervention distributions to formulate the loss function as well as the fairness constraints. We adopt surrogate functions to smooth the loss function and constraints, and theoretically show that the excess risk of the proposed loss function can be bounded in a form that is the same as that for traditional surrogated loss functions. Experiments using both synthetic and real-world datasets show the effectiveness of our approach. 1

Introduction
How to ensure fairness in algorithmic decision making models is an important task in machine learning [12, 15]. Over the past years, many researchers have been devoted to the design of fair classiﬁcation algorithms with respect to a pre-deﬁned protected attribute, such as race or sex, and a decision task/model, such as hiring [1, 11, 24]. In particular, one line of the work is to incorporate fairness constraints into classic learning algorithms to build fair classiﬁers from potentially biased data [4, 13, 29, 31–33]. Most of previous research generally focuses on a single decision model.
However, in reality there usually exist multiple decision models within a system and all of which may contain a certain amount of discrimination, either introduced by themselves or transmitted from upstream models. As a motivating example, consider two decision tasks Y1, Y2 where Y1 is used by the city government to allocate policing resources to different locations and Y2 is used by a local bank to make personal loan decisions. Due to historically segregated housing, neighborhood racial composition differs based on geographic locations, and there can exist direct racial discrimination in
Y1 as well. Thus, certain locations will be allocated more police resources than others, resulting in larger numbers of criminal arrest records. As a result, when the criminal arrest record is used in Y2, certain racial group will receive unfair disadvantage in getting loans.
Ideally we would like to build fair models for all decision making tasks. However, if decision models inﬂuence one another, it is not a straightforward problem even if we know how to build a fair model for each task. This is because the data distribution can change as a consequence of deploying 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
new models. If we build the model for each task independently using static training datasets, the learning process of each model is based on the ﬁxed distribution given in the training data. However, deploying new fair models would change the distributions of attribute variables that are affected by their decisions as well as the discrimination that is passing down. As a result, the subsequent models built on the original distribution may not perform well in terms of both accuracy and fairness. On the other hand, if we build fair models one by one following a temporal sequential order, each time deploying a model and collecting the output data before building the next one, then the time needed for building all models may not be acceptable for some applications.
In this paper, we propose an approach that learns multiple fair classiﬁers simultaneously and only re-quires a static training dataset. The core idea is to leverage Pearl’s structural causal model (SCM) [23], treat each decision model as a soft intervention and infer the post-intervention distributions to formulate the loss function as well as the fairness constraints. The SCM is widely adopted in fair classiﬁcation research for deﬁning fairness as the causal effect of the protected attribute on the decision [17, 21, 28, 30, 34–36]. Causal inference in the SCM is often facilitated with the “(hard) intervention” that forces some variable X to take certain constant x, denoted by do(X = x) [23].
“Soft intervention” [6, 16], also known as the “conditional action” [23], extends the hard intervention such that variable X is forced to take a speciﬁed functional relationship g(z) in responding to some set Z of other variables, denoted by do(X = g(z)). In our approach, the deploying of new decision models is considered as to perform soft interventions on the decisions, whose inﬂuence can be inferred as the post-intervention distributions. By quantifying fairness as causal effects of the protected attribute on all decisions, under the hard intervention on the protected attribute and soft interventions on decisions, we formulate fair classiﬁcation for multiple decisions as a single constrained optimization problem.
Combining multiple decision models together makes the optimization challenging to solve. Similarly to [29], we adopt surrogate functions to smooth the loss function and constraints. However, the difference in our problem is that, each decision model is associated with a surrogate function, and the surrogated protections are used in downstream decision models, resulting non-linear combinations of multiple surrogate functions. As a result, our loss function is different from traditional surrogated loss functions whose excess risks have been analyzed and bounded in [2]. To investigate the excess risk of our loss function, we adopt theoretical tools in [2] and show that nontrivial upper bounds exist on the excess risk in a form that is the same as that for traditional surrogated loss functions given in [2], irrespective of the number of decision models involved.