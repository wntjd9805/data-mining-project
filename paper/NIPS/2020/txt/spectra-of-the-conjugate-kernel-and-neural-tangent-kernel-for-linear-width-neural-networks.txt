Abstract
We study the eigenvalue distributions of the Conjugate Kernel and Neural Tangent
Kernel associated to multi-layer feedforward neural networks. In an asymptotic regime where network width is increasing linearly in sample size, under random ini-tialization of the weights, and for input samples satisfying a notion of approximate pairwise orthogonality, we show that the eigenvalue distributions of the CK and
NTK converge to deterministic limits. The limit for the CK is described by iterating the Marcenko-Pastur map across the hidden layers. The limit for the NTK is equiv-alent to that of a linear combination of the CK matrices across layers, and may be described by recursive ﬁxed-point equations that extend this Marcenko-Pastur map.
We demonstrate the agreement of these asymptotic predictions with the observed spectra for both synthetic and CIFAR-10 training data, and we perform a small simulation to investigate the evolutions of these spectra over training. 1

Introduction
Recent progress in our theoretical understanding of neural networks has connected their training and generalization to two associated kernel matrices. The ﬁrst is the Conjugate Kernel (CK) or the equivalent Gaussian process kernel [43, 56, 12, 14, 48, 52, 33, 41]. This is the gram matrix of the derived features produced by the ﬁnal hidden layer of the network. The network predictions are linear in these derived features, and the CK governs training and generalization in this linear model.
The second is the Neural Tangent Kernel (NTK) [27, 19, 6]. This is the gram matrix of the Jacobian of in-sample predictions with respect to the network weights, and was introduced to study full network training. Under gradient-ﬂow training dynamics, the in-sample predictions follow a differential equation governed by the NTK. We provide a brief review of these matrices in Section 2.1.
The spectral decompositions of these kernel matrices are related to training and generalization properties of the underlying network. Training occurs most rapidly along the eigenvectors of the largest eigenvalues [5], and the eigenvalue distribution may determine the trainability of the model and the extent of implicit bias towards simpler functions [57, 59]. It is thus of interest to understand the spectral properties of these matrices, both at random initialization and over the course of training. 1.1 Summary of contributions
In this work, we apply techniques of random matrix theory to derive an exact asymptotic characteri-zation of the eigenvalue distributions of the CK and NTK at random initialization, in a multi-layer feedforward network architecture. We study a “linear-width” asymptotic regime, where each hidden layer has width proportional to the training sample size. We impose an assumption of approximate pairwise orthogonality for the training samples, which encompasses general settings of independent samples that need not have independent entries. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
We show that the eigenvalue distributions for both the CK and the NTK converge to deterministic limits, depending on the limiting eigenvalue distribution of the training data. The limit distribution for the CK at each intermediate hidden layer is a Marcenko-Pastur map of a linear transformation of that of the previous layer. The NTK can be approximated by a linear combination of CK matrices, and its limiting eigenvalue distribution can be described by a recursively deﬁned sequence of ﬁxed-point equations that extend this Marcenko-Pastur map. We demonstrate the agreement of these asymptotic limits with the observed spectra on both synthetic and CIFAR-10 training data of moderate size.
In this linear-width asymptotic regime, feature learning occurs, and both the CK and NTK evolve over training. Although our theory pertains only to their spectra at random initialization of the weights, we conclude with an empirical examination of their spectral evolutions during training, on simple examples of learning a single neuron and learning a binary classiﬁer for two classes in CIFAR-10. In these examples, the bulk eigenvalue distributions of the CK and NTK undergo elongations, and isolated principal components emerge that are highly predictive of the training labels.
Recent theoretical work has studied the evolution of the NTK in an entrywise sense [25, 20], and we believe it is an interesting open question to translate this understanding to a more spectral perspective. 1.2 Related literature
Many properties of the CK and NTK have been established in the limit of inﬁnite width and ﬁxed sample size n. In this limit, both the CK [43, 56, 14, 33, 41] and the NTK [27, 34, 58] at random initialization converge to ﬁxed n × n kernel matrices. The associated random features regression models converge to kernel linear regression in the RKHS of these limit kernels. Furthermore, network training occurs in a “lazy” regime [11], where the NTK remains constant throughout training
[27, 19, 18, 6, 34, 7]. Spectral properties of the CK, NTK, and Hessian of the training loss have been previously studied in this inﬁnite-width limit in [48, 51, 57, 30, 21, 28]. Limitations of lazy training and these equivalent kernel regression models have been studied theoretically and empirically in [11, 7, 60, 22, 23, 35], suggesting that trained neural networks of practical width are not fully described by this type of inﬁnite-width kernel equivalence. The asymptotic behavior is different in the linear-width regime of this work: For example, for a linear activation σ(x) = x, the inﬁnite-width limit of the CK for random weights is the input Gram matrix X (cid:62)X, whereas its limit spectrum under linear-width asymptotics has an additional noise component from iterating the Marcenko-Pastur map.
Under linear-width asymptotics, the limit CK spectrum for one hidden layer was characterized in
[46] for training data with i.i.d. Gaussian entries. For activations satisfying Eξ∼N (0,1)[σ(cid:48)(ξ)] = 0,
[46] conjectured that this limit is a Marcenko-Pastur law also in multi-layer networks, and this was proven under a subgaussian assumption as part of the results of [9]. [39] studied the one-hidden-layer
CK with general training data, and [37] specialized this to Gaussian mixture models. These works
[39, 37] showed that the limit spectrum is a Marcenko-Pastur map of the inter-neuron covariance.
We build on this insight by analyzing this covariance across multiple layers, under approximate orthogonality of the training samples. This orthogonality condition is similar to that of [3], which recently studied the one-hidden-layer CK with a bias term. This condition is also more general than the assumption of i.i.d. entries, and we describe in Appendix I the reduction to the one-hidden-layer result of [46] for i.i.d. Gaussian inputs, as this reduction is not immediately clear. [44] provides another form of the limit distribution in [46], which is equivalent to our form in Appendix I via the relation described in [8].
The limit NTK spectrum for a one-hidden-layer network with i.i.d. Gaussian inputs was recently characterized in parallel work of [4]. In particular, [4] applied the same idea as in Lemma 3.5 below to study the Hadamard product arising in the NTK. [45, 47] previously studied the equivalent spectrum of a sample covariance matrix derived from the network Jacobian, which is one of two components of the Hessian of the training loss, in a slightly different setting and also for one hidden layer.
The spectra of the kernel matrices X (cid:62)X that we study are equivalent (up to the addition/removal of 0’s) to the spectra of the sample covariance matrices in linear regression using the features
X. As developed in a line of recent literature including [16, 46, 17, 39, 36, 24, 42, 4, 15], this spectrum and the associated Stieltjes transform and resolvent are closely related to the training and generalization errors in this linear regression model. These works have collectively provided an asymptotic understanding of training and generalization error for random features regression models derived from the CK and NTK of one-hidden-layer neural networks, and related qualitative phenomena of double and multiple descent in the generalization error curves. 2
2