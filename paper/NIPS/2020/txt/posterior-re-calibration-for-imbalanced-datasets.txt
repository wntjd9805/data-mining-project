Abstract
Neural Networks can perform poorly when the training label distribution is heav-ily imbalanced, as well as when the testing data differs from the training dis-tribution. In order to deal with shift in the testing label distribution, which im-balance causes, we motivate the problem from the perspective of an optimal
Bayes classiﬁer and derive a post-training prior rebalancing technique that can be solved through a KL-divergence based optimization. This method allows a
ﬂexible post-training hyper-parameter to be efﬁciently tuned on a validation set and effectively modify the classiﬁer margin to deal with this imbalance. We fur-ther combine this method with existing likelihood shift methods, re-interpreting them from the same Bayesian perspective, and demonstrating that our method can deal with both problems in a uniﬁed way. The resulting algorithm can be conveniently used on probabilistic classiﬁcation problems agnostic to underlying architectures. Our results on six different datasets and ﬁve different architectures show state of art accuracy, including on large-scale imbalanced datasets such as iNaturalist for classiﬁcation and Synthia for semantic segmentation. Please see https://github.com/GT-RIPL/UNO-IC.git for implementation.

Introduction 1
Applications of deep learning algorithms in the real world have fueled new interest in research beyond well-constructed datasets where the classes are balanced and the training distribution faithfully reﬂects the true (testing) distribution.
However, it is unlikely that one can anticipate all possible scenarios and construct well-curated datasets consistently. Therefore it is important to study robust algorithms that can perform well with such imbalanced datasets and unseen situations during testing. The aforementioned problems can be categorized as distributional shift between the training and testing conditions, speciﬁcally:label prior shift and non-semantic likelihood shift.
In this paper, we focus on label prior shift, which can arise when i) the training class distribution does not match the true (test) class distribution due to the inherent difﬁculty in obtaining samples from certain classes, or ii) the distribution of classes does not reﬂect their relative importance. For example, the natural world is inherently imbalanced such that a collected dataset may exhibit a long-tailed distribution. Similarly, in semantic segmentation, the pixel percentage of pedestrains does not reﬂect their importance in prediction (e.g. for safety). 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In order to deal with such shift in the testing label distribution, which imbalance causes, a number of methods have been developed such as class-balanced loss (CB) [1] and Deferred Reweighting (DRW) [2], as well as methods that scale the margin in a ﬁxed way as a function of the amount of data per class through a label-distribution-aware loss (LDAM) [2] . However, such methods do not allow for a ﬂexibly trade-off between precision and recall across the classes, and require re-training to target a testing label distribution.
In this paper, we instead motivate the problem from the perspective of an optimal Bayes classiﬁer and derive a rebalanced posterior by introducing test priors and showing that it is the optimal Bayes classiﬁer on the test distribution under ideal conditions. To compensate for imperfect learning in practice, an approximation can be solved through a KL-divergence based optimization by treating the rebalanced and original posteriors as approximations to a true posterior.
This method, which does not require re-training the original imbalanced classiﬁers, allows a ﬂexible hyper-parameter to be efﬁciently tuned on a validation set and effectively modify the classiﬁer margin to target a desired test label distribution. We further combine this method with existing methods dealing with non-semantic likelihood shift, which occurs when the representation of samples from a certain class changes due to changes in lighting, weather, sensor noise, modality, etc. We re-interpret these methods from the same Bayesian perspective, and demonstrate that our method can deal with both problems in a uniﬁed way.
We demonstrate our method on six datasets and ﬁve different neural network architectures, across two different imbalanced tasks: classiﬁcation and semantic segmentation. Using a toy dataset that allows for visualizations of the classiﬁer margins, we show that our method effectively shifts the decision boundary away from the minority class towards the over-represented class. We then demonstrate that our method can achieve state of the art results on imbalanced variants of CIFAR-10 and CIFAR-100, and can scale to larger datasets such as iNaturalist which has extreme imbalance. For semantic segmentation, which is an inherently imbalanced task, we show that our uniﬁed method can support both imbalance and likelihood shift in the form of unknown weather conditions encountered during testing (but not during training).
In summary, the main contributions of the paper are the following:
•
•
•
•
We derive a principled imbalance calibration algorithm for models trained on imbalanced datasets. The method requires no re-training to target new test label distributions and can
ﬂexibly trade-off between precision and recall on under-represented classes via a single hyper-parameter.
We introduce an efﬁcient search algorithm for this hyper-parameter. Unlike cost sensitive learning, an optimal hyperparameter can be searched efﬁciently on a validation set post-training.
We test the algorithm on six datasets and ﬁve different architectures and outperforms state-of-the-art models on classiﬁcation accuracy (recall) across all tasks and models while maintaining good precision.
We further combine our method with non-semantic likelihood shift methods, re-motivate it from the Bayesian perspective, and show that we can tackle both problems in a uniﬁed way on a RGB-D semantic segmentation dataset with unseen weather conditions. We show signiﬁcant improvement on mean accuracy while maintaining good mean IOU performance with both qualitative and quantitative results. 2