Abstract
We initiate the study of a new model of supervised learning under privacy con-straints. Imagine a medical study where a dataset is sampled from a population of both healthy and unhealthy individuals. Suppose healthy individuals have no privacy concerns (in such case, we call their data “public”) while the unhealthy individuals desire stringent privacy protection for their data. In this example, the population (data distribution) is a mixture of private (unhealthy) and public (healthy) sub-populations that could be very different.
Inspired by the above example, we consider a model in which the population D is a mixture of two sub-populations: a private sub-population Dpriv of private and sensitive data, and a public sub-population Dpub of data with no privacy concerns.
Each example drawn from D is assumed to contain a privacy-status bit that indicates whether the example is private or public. The goal is to design a learning algorithm that satisﬁes differential privacy only with respect to the private examples.
Prior works in this context assumed a homogeneous population where private and public data arise from the same distribution, and in particular designed solutions which exploit this assumption. We demonstrate how to circumvent this assumption by considering, as a case study, the problem of learning linear classiﬁers in Rd. We show that in the case where the privacy status is correlated with the target label (as in the above example), linear classiﬁers in Rd can be learned, in the agnostic as well as the realizable setting, with sample complexity which is comparable to that of the classical (non-private) PAC-learning. It is known that this task is impossible if all the data is considered private. 1

Introduction
Despite the remarkable progress in privacy-preserving machine learning powered by the rigorous framework of differential privacy (DP) [DMNS06], the current state of the art has several limitations.
Most of the existing works on differentially private learning follow a conventional model, where the entirety of the input dataset to the learning algorithm is assumed to be sensitive and private, and hence, requires protection via the stringent constraint of DP. Unfortunately, this conservative approach has fundamental limitations that manifest in many problems. For example, learning even simple classes of functions (e.g., one-dimensional thresholds over R) is provably impossible under that stringent model
[BNSV15, ALMM18] even though such classes are trivially learnable without privacy constraints. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
More recent works [BNS13, BTT18, ABM19, NB20, BCM+20] have considered a more relaxed model, where the input dataset is made up of two parts: a private sample (as in the conventional model), and a “public” sample that entails no privacy constraints. In this model, the algorithm is required to satisfy DP only with respect to the private sample. Despite of the good news brought by these works showing the possibility of circumventing some of the aforementioned limitations by harnessing a limited amount of public data, all these works make the strong assumption that both the private and public samples come from the same population (i.e., they arise from the same distribution). This can limit the practical value of these results in many real-life scenarios, where private and public data are naturally distinct.
Indeed, for a data record, the attribute of being sensitive can be strongly correlated with the value of that record (i.e., the realization of the feature-vector and the label). For example, imagine a scenario where a bank wants to predict the credit-worthiness of applicants for a credit card. To do this, a training sample is drawn from a population of individuals with good and bad credit scores. Suppose individuals with good credit score have no privacy concerns in sharing their data with the bank (and hence, their data can be viewed as “public”), while those with bad credit score are concerned about what the study may reveal about them to third parties and understandably so, because they do not want such information to impact their chances in future opportunities. In this example, the population is a mixture of two very different groups: a sub-population with a good credit score (public sub-population), and a sub-population with bad credit score (private sub-population).
In this work, we introduce a new model for differentially private learning in which a learning algorithm has access to a mixed dataset of private and public examples that arise from possibly different distributions. The algorithm is required to satisfy DP only with respect to the private examples. More speciﬁcally, in our model, the underlying population (data distribution) D is a mixture of two possibly different sub-populations: a private sub-population Dpriv of sensitive data, and public sub-population Dpub of data that is deemed by its original owner to have no risk to personal privacy. We assume that each example drawn from the mixture D has a “privacy ﬂag” which is a binary label to indicate whether the example is private or public. As usual in the statistical learning framework, we do not assume the knowledge of D or any of the sub-populations (or their respective weights in the mixture).
Contributions
• Introducing PPM model: We formally describe the basic model of supervised learning from mix-tures of private and public populations, and deﬁne the corresponding class of learning algorithms, which we refer to as Private-Public Mixture (PPM) learners.
• Learning Halfspaces: Although the ﬁrst quick impression about the model might be that it is a bit too general to allow for interesting results beyond what is covered by the conventional model of
DP learning, we demonstrate that this is not the case and prove the ﬁrst non-trivial result under this model in the context of learning halfspaces (linear classiﬁers) in Rd (for any d ≥ 1). We give a construction of a PPM learner for this problem in the case where the privacy status is correlated with the target label, as in the credit-worthiness example above. Curiously, our PPM learner is improper: it outputs a hypothesis (classiﬁer) that can be described by at most d halfspaces. We hence derive upper bounds on the sample complexity of this problem in both the realizable and agnostic settings. In particular, we show that halfspaces in Rd can be learned in the aforementioned
PPM model up to (excess) error α using ≈ d2
α total examples in the realizable setting, and using
≈ d2
α2 total examples in the agnostic setting. As noted earlier, in the conventional model, where all the examples drawn from D are considered private, this class cannot be learned in Rd (even for d = 1) [BNSV15, ALMM18] 1. Our bounds are comparable to the classical, non-private sample complexity of learning halfspaces. In particular, our bounds are only a factor of d worse than their non-private counterparts. We leave the question of whether our bounds can be improved to future work.
Techniques: The idea of our construction for learning halfspaces goes as follows. First, we use the public examples to deﬁne a ﬁnite family of halfspaces (cid:101)Cpub. Then, we employ a useful tool from 1[BNSV15, ALMM18] showed that the class of one-dimensional halfspaces over any ﬁnite domain X ⊆ R requires sample complexity at least Ω (log(cid:63) |X|). 2
convex geometry known as Helly’s Theorem [Hel23] to argue the existence of a collection of at most d halfspaces in (cid:101)Cpub whose intersection is disjoint from the ERM halfspace (the halfspace with smallest empirical error with respect to the entire set of training examples). This implies that there is a hypothesis (described by the intersection of at most d halfspaces from (cid:101)Cpub) whose empirical error is not larger than that of the ERM halfspace. Hence, we reduce our learning task to DP learning of a ﬁnite class G that contains all possible intersections of at most d halfspaces from (cid:101)Cpub. We note that the latter task is feasible since G is a ﬁnite class [KLN+08]. The description here is rather simpliﬁed. The actual construction and our analysis entail more intricate details (e.g., we need to carefully analyze the generalization error since the class G itself depends on the public part of the training set). One clear aspect from the above description is that our construction is of an improper learner since, in general, the output hypothesis is given by the intersection of at most d halfspaces.
Devising a construction of a proper learner for this problem is an interesting open question.