Abstract
Existing research on continual learning of a sequence of tasks focused on dealing with catastrophic forgetting, where the tasks are assumed to be dissimilar and have little shared knowledge. Some work has also been done to transfer previously learned knowledge to the new task when the tasks are similar and have shared knowledge. To the best of our knowledge, no technique has been proposed to learn a sequence of mixed similar and dissimilar tasks that can deal with forgetting and also transfer knowledge forward and backward. This paper proposes such a technique to learn both types of tasks in the same network. For dissimilar tasks, the algorithm focuses on dealing with forgetting, and for similar tasks, the algorithm focuses on selectively transferring the knowledge learned from some similar previous tasks to improve the new task learning. Additionally, the algorithm automatically detects whether a new task is similar to any previous tasks. Empirical evaluation using sequences of mixed tasks demonstrates the effectiveness of the proposed model.2 1

Introduction
In many applications, the system needs to incrementally or continually learn a sequence of tasks.
This learning paradigm is called continual learning (CL) or lifelong learning (Chen and Liu, 2018).
Ideally, in learning each new task t, the learner should (1) not forget what it has learned from previous tasks in order to achieve knowledge accumulation, (2) transfer the knowledge learned in the past forward to help learn the new task t if t is similar to some previous tasks and has shared knowledge with those previous tasks, (3) transfer knowledge backward to improve the models of similar previous tasks, and (4) learn a mixed sequence of dissimilar and similar tasks and achieve (1), (2) and (3) at the same time. To our knowledge, no existing CL technique has all these four capabilities. This paper makes an attempt to achieve all these objectives in the task continual learning (TCL) setting (also known as task incremental learning), where each task is a separate or distinct classiﬁcation problem.
This work generalizes the existing works on TCL. Note, there is also the class continual learning (CCL) setting (or class incremental learning), which learns a sequence of classes to build one overall multi-class classiﬁer for all the classes seen so far.
As AI agents such as chatbots, intelligent personal assistants and physical robots are increasingly made to learn many skills or tasks, this work is becoming more and more important. In practice, when an agent faces a new task t, naturally some previous tasks are similar and some are dissimilar to t. The agent should learn the new task without forgetting knowledge learned from previous tasks while also improving its learning by transferring the shared knowledge from those similar tasks.
∗Corresponding author. The work was done when B. Liu was at Peking University on leave of absence from
University of Illinois at Chicago. 2https://github.com/ZixuanKe/CAT 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Most existing CL models focus on (1), i.e., dealing with catastrophic forgetting or simply forget-ting (Chen and Liu, 2018; Parisi et al., 2019; Li and Hoiem, 2016; Seff et al., 2017; Shin et al., 2017;
Kirkpatrick et al., 2016; Rebufﬁ et al., 2017; Yoon et al., 2018; He and Jaeger, 2018; Yoon et al., 2018; Masse et al., 2018; Schwarz et al., 2018; Nguyen et al., 2018; Hu et al., 2019). In learning a new task, the learner has to update the network parameters but this update can cause the models for previous tasks to degrade or to be forgotten (McCloskey and Cohen, 1989). Existing works dealing with forgetting typically try to make the update of the network toward less harmful directions to protect the previously learned knowledge. Forgetting mainly affects the learning of a sequence of dissimilar tasks. When a sequence of similar tasks is learned, there is little forgetting as we will see in Section 4. There are also existing methods for knowledge transfer (Ruvolo and Eaton, 2013; Chen and Liu, 2014; Chen et al., 2015; Wang et al., 2019; Ke et al., 2020) when all tasks are similar.
This paper proposes a novel TCL model called CAT (Continual learning with forgetting Avoidance and knowledge Transfer) that can effectively learn a mixed sequence of similar and dissimilar tasks and achieve all the aforementioned objectives. CAT uses a knowledge base (KB) to keep the knowledge learned from all tasks so far and is shared by all tasks. Before learning each new task t, the learner ﬁrst automatically identiﬁes the previous tasks Tsim that are similar to t. The rest of the tasks are dissimilar to t and denoted by Tdis. In learning t, the learner uses the task masks (TM) learned from the previous tasks to protect the knowledge learned for those dissimilar tasks in Tdis so that their important parameters are not affected (no forgetting). A set of masks (one for each layer) is also learned for t in the process to be used in the future to protect its knowledge. For the set of similar tasks Tsim, the learner learns a knowledge transfer attention (KTA) to selectively transfer useful knowledge from the tasks in Tsim to the new task to improve the new task learning (forward knowledge transfer). During training the new task t, CAT also allows the past knowledge to be updated so that some tasks in Tsim may be improved as well (backward knowledge transfer). Our empirical evaluation shows that CAT outperforms the state of the art existing baseline models that can be applied to the proposed problem. 2