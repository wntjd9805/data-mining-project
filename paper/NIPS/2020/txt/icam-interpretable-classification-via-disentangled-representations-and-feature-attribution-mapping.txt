Abstract
Feature attribution (FA), or the assignment of class-relevance to different locations in an image, is important for many classiﬁcation problems but is particularly crucial within the neuroscience domain, where accurate mechanistic models of behaviours, or disease, require knowledge of all features discriminative of a trait. At the same time, predicting class relevance from brain images is challenging as phenotypes are typically heterogeneous, and changes occur against a background of signiﬁcant natural variation. Here, we present a novel framework for creating class speciﬁc
FA maps through image-to-image translation. We propose the use of a VAE-GAN to explicitly disentangle class relevance from background features for improved interpretability properties, which results in meaningful FA maps. We validate our method on 2D and 3D brain image datasets of dementia (ADNI dataset), ageing (UK Biobank), and (simulated) lesion detection. We show that FA maps generated by our method outperform baseline FA methods when validated against ground truth. More signiﬁcantly, our approach is the ﬁrst to use latent space sampling to support exploration of phenotype variation. 1

Introduction
Brain images present a signiﬁcant resource in the development of mechanistic models of behaviour and neurological/psychiatric disease as they reﬂect measurable neuroanatomical traits that are heritable, present in unaffected siblings and detectable prior to disease onset [10]. Nevertheless, for complex disorders, features of disease remain subtle, variable [21, 38] and occur against a back-drop of signiﬁcant natural variation in shape and appearance [16, 27].
Traditional approaches for brain image analysis compare data in a global average template space, estimated via smooth (and ideally diffeomorphic) deformations [2, 11, 12, 14, 16, 37]. This, however, typically ignores cortical heterogeneity and may smooth out sources of variation [11, 16] in ways which limit interpretation [7]. Tools are still required to distinguish between features of population variability and speciﬁc discriminative phenotypic features.
Deep learning is state-of-the-art for many image processing tasks [17] and has shown strong promise for brain imaging applications such as healthy tissue and lesion segmentation [8, 13, 25, 36]. However, there is growing need for greater accountability of networks, especially within the medical domain.
Several approaches for feature attribution (FA) [3, 30, 33, 40, 43] have been proposed which return 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: ADNI comparisons of feature attribution (FA) maps. ICAM is the ﬁrst known method able to generate variance and mean FA maps in test time, and shows good detection of the ventricles (blue arrows), cortex (green arrows), and hippocampus (pink arrows) when compared with the ground truth disease map. Baseline methods perform sub-optimally in comparison, with VA-GAN generating the second best FA maps. the most important or salient features for a prediction, after training a network for classiﬁcation.
However, applying a method post-hoc instead of explicitly training an interpretable model has shown to be insufﬁcient at detecting all discriminative features of a class, especially in medical imaging [5].
Recently, approaches have been proposed which use generative models to translate images from one class to another [22, 48], and in Baumgartner et al. [5] this was adapted to create a difference map between Alzheimer’s (AD) and Mild Cognitive Impairment (MCI) subjects. While it was able to detect more salient features in comparison to previous methods, it was still unable to identify much of the variability between the AD and MCI subjects. Other restrictions of this approach are that it assumes knowledge of label classes at test time, and that it does not have a latent space that can be sampled. This limits the interpretation and generates a deterministic output at test time.
In this paper we aim to improve on the current feature attribution methods by developing a more inter-pretable model, and thus more meaningful feature attribution maps. We propose ICAM (Interpretable
Classiﬁcation via disentangled representations and feature Attribution Mapping), a framework which builds on approaches for image-to-image translation [28] to learn feature attribution by disentangling class-relevant attributes (attr) from class-irrelevant content. Sharp reconstructions are learnt through use of a Variational Autoencoder (VAE) with a discriminator loss on the decoder (Generative Adver-sarial Network, GAN). This not only allows classiﬁcation and generation of an attribution map from the latent space, but also a more interpretable latent space that can visualise differences between and within classes. By sampling the latent space at test time to generate a FA map, we demonstrate its ability to detect meaningful brain variation in 3D brain Magnetic Resonance Imaging (MRI).
In particular the speciﬁc contributions of the method are as follows: 1 We describe the ﬁrst framework to implement a translation VAE-GAN network for simulta-neous classiﬁcation and feature attribution, through use of a shared attribute latent space with a classiﬁcation layer, which supports rejection sampling and improved class disentanglement, relative to previous methods [28]. 2 This supports exploration of phenotypic variation in brain structure through latent space visualisation of the space of class-related variability, including the study of the mean and variance of feature attribution map generation (see example in Fig. 1). 3 We demonstrate the power and versatility of ICAM using extensive qualitative and quan-titative validation on 3 datasets including; Human Connectome Project (HCP) with le-sion simulations, Alzheimer’s Disease Neuroimaging Initiative (ADNI), and UK Biobank datasets. In addition, our code, which has been released on GitHub at https://github. com/CherBass/ICAM, extends to multi-class classiﬁcation and regression tasks. 2
Table 1: Comparison of baseline methods.
Method post-hoc (cid:88)
Grad-CAM [40] (cid:88)
Guided backprop [42]
Integrated gradients [43] (cid:88) (cid:88)
Occlusion [47]
DRIT [28]
VA-GAN [5]
ICAM (our method) classiﬁcation (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) generative model cyclic variance analysis
N/A
N/A
N/A
N/A (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) 2