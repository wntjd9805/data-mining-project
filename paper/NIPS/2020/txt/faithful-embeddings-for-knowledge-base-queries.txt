Abstract
The deductive closure of an ideal knowledge base (KB) contains exactly the logical queries that the KB can answer. However, in practice KBs are both incomplete and over-speciﬁed, failing to answer some queries that have real-world answers. Query embedding (QE) techniques have been recently proposed where KB entities and
KB queries are represented jointly in an embedding space, supporting relaxation and generalization in KB inference. However, experiments in this paper show that QE systems may disagree with deductive reasoning on answers that do not require generalization or relaxation. We address this problem with a novel QE method that is more faithful to deductive reasoning, and show that this leads to better performance on complex queries to incomplete KBs. Finally we show that inserting this new QE module into a neural question-answering system leads to substantial improvements over the state-of-the-art. 2 1

Introduction
The deductive closure of an ideal knowledge base (KB) contains exactly the logical queries that the
KB can answer. However, in practice KBs are both incomplete and over-speciﬁed, failing to answer queries that have actual real-world answers. Query embedding (QE) methods extend logical queries to incomplete KBs by representing KB entities and KB queries in a joint embedding space, supporting relaxation and generalization in KB inference [9, 10, 25, 17]. For instance, graph query embedding (GQE) [10] encodes a query q and entities x as vectors such that cosine distance represents x’s score as a possible answer to q. In QE, the embedding for a query q is typically built compositionally; in particular, the embedding for q = q1 ∧ q2 is computed from the embeddings for q1 and q2. In past work, QE has been useful for answering overconstrained logical queries [25] and querying incomplete
KBs [9, 10, 17].
Figure 1 summarizes the relationship between traditional KB embedding (KBE), query embedding (QE), and logical inference. Traditional logical inference enables a system to ﬁnd deductively entailed answers to queries; KBE approaches allow a system to generalize from explicitly-stored KB tuples to similar tuples; and QE methods combine both of these ideas, providing a soft form of logical entailment that generalizes.
We say that a QE system is logically faithful if it behaves similarly to a traditional logical inference system with respect to entailed answers. In this paper, we present experiments illustrating that
QE systems are often not faithful: in particular, experiments with the state-of-the-art QE system
Query2Box [17] show that it performs quite poorly in ﬁnding logically-entailed answers. We conjecture this is because models that generalize well do not have the capacity to model all the
∗Work done while at Google Research. 2Code available at https://github.com/google-research/language 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
KB embedding (KBE) methods generalize from known KG facts to plausible ones, and logical inference computes answers to compositional queries that are entailed by known facts. Query embedding (QE) combines both of these tools for extending a set of known facts, by ﬁnding answers to a query that are plausibly entailed by known facts.
Figure 1: Overview of differences between KBE and QE. Shaded area indicates the kinds of test cases used in prior studies of QE. information in a large KB accurately, unless embeddings are impractically large. We thus propose two novel methods for improving faithfulness while preserving the ability to generalize. First, we implement some logical operations using neural retrieval over a KB of embedded triples, rather than with geometric operations in embedding space, thus adding a non-parametric component to
QE. Second, we employ a randomized data structure called a count-min sketch to propagate scores of logically-entailed answers. We show that this combination leads to a QE method, called EmQL (Embedding Query Language) which is differentiable, compact, scalable, and (with high probability) faithful. Furthermore, strategically removing the sketch in parts of the QE system allows it to generalize very effectively.
We show that EmQL performs dramatically better than Query2Box on logically-entailed answers, and also improves substantially on complex queries involving generalization. Finally we show that inserting EmQL into a natural language KB question-answering (KBQA) system leads to substantial improvements over the experimental state-of-the-art for two widely-used benchmarks, MetaQA [29] and WebQuestionsSP [27].
The main contributions of this work are: (1) a new QE scheme with expressive set and relational operators, including those from previous QE schemes (set intersection, union, and relation following) plus a “relational ﬁltering” operation; (2) a new analysis of QE methods showing that previous methods are not faithful, failing to ﬁnd entities logically entailed as answers; (3) the ﬁrst application of QE as a module in a KBQA system; and (4) evidence that this module leads to substantial gains over the prior state-of-the-art on two widely-used benchmarks, thanks to its superior faithfulness. 2