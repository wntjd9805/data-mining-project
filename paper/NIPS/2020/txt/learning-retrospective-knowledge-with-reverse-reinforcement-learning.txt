Abstract
We present a Reverse Reinforcement Learning (Reverse RL) approach for repre-senting retrospective knowledge. General Value Functions (GVFs) have enjoyed great success in representing predictive knowledge, i.e., answering questions about possible future outcomes such as “how much fuel will be consumed in expectation if we drive from A to B?”. GVFs, however, cannot answer questions like “how much fuel do we expect a car to have given it is at B at time t?”. To answer this question, we need to know when that car had a full tank and how that car came to B. Since such questions emphasize the inﬂuence of possible past events on the present, we refer to their answers as retrospective knowledge. In this paper, we show how to represent retrospective knowledge with Reverse GVFs, which are trained via Reverse RL. We demonstrate empirically the utility of Reverse GVFs in both representation learning and anomaly detection. 1

Introduction
Much knowledge can be formulated as answers to predictive questions (Sutton, 2009), for example, “to know that Joe is in the coffee room is to predict that you will see him if you went there” (Sutton, 2009).
Such knowledge is referred to as predictive knowledge (Sutton, 2009; Sutton et al., 2011). General
Value Functions (GVFs, Sutton et al. 2011) are commonly used to represent predictive knowledge.
GVFs are essentially the same as canonical value functions (Puterman, 2014; Sutton and Barto, 2018).
However, the policy, the reward function, and the discount function associated with GVFs are usually carefully designed such that the numerical value of a GVF at certain states matches the numerical an-swer to certain predictive questions. In this way, GVFs can represent predictive knowledge.
Consider the concrete example in Figure 1, where a microdrone is doing a random walk. The microdrone is initialized somewhere with 100% battery. L4 is a power station where its battery is recharged to 100%. Each clockwise movement consumes 2% of the battery, and each counterclockwise movement consumes 1% (for simplicity, we assume negative battery levels, e.g., -10%, are legal). Furthermore, each movement fails with probability 1%, in which case the micro-drone remains in the same location and no energy is consumed. An example of a predictive question in this system is:
Question 1. Starting from L1, how much energy will be consumed in expectation before the next charge?
Figure 1: A microdrone do-ing random walk among four different locations. L4 is a charging station where the microdrone’s battery is fully recharged.
To answer this question, we can model the system as a Markov
Decision Process (MDP). The policy is uniformly random and the reward for each movement is the
∗Correspondence to shangtong.zhang@cs.ox.ac.uk 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
additive inverse of the corresponding battery consumption. Whenever the microdrone reaches state
L4, the episode terminates. Under this setup, the answer to Question 1 is the expected cumulative reward when starting from L1, i.e., the state value of L1. Hence, GVFs can represent the predictive knowledge in Question 1. As a GVF is essentially a value function, it can be trained with any data stream from agent-environment interaction via Reinforcement Learning (RL, Sutton and Barto 2018), demonstrating the generality of the GVF approach. Importantly, the most appealing feature of GVFs is their compatibility with off-policy learning, making this representation of predictive knowledge scalable and efﬁcient. For example, in the Horde architecture (Sutton et al., 2011), many GVFs are learned in parallel with gradient-based off-policy temporal difference methods (Sutton et al., 2009b,a; Maei, 2011). In the microdrone example, we can learn the answer to Question 1 under many different conditions (e.g., when the charging station is located at L2 or when the microdrone moves clockwise with probability 80%) simultaneously with off-policy learning by considering different reward functions, discount functions, and polices.
GVFs, however, cannot answer many other useful questions, e.g., if at some time t, we ﬁnd the microdrone at L1, how much battery do we expect it to have? As such questions emphasize the inﬂuence of possible past events on the present, we refer to their answers as retrospective knowledge.
Such retrospective knowledge is useful, for example, in anomaly detection. Suppose the microdrone runs for several weeks by itself while we are traveling. When we return at time t, we ﬁnd the microdrone is at L1. We can then examine the battery level and see if it is similar to the expected battery at L1. If there is a large difference, it is likely that there is something wrong with the microdrone. There are, of course, many methods to perform such anomaly detection. For example, we could store the full running log of the microdrone during our travel and examine it when we are back. The memory requirement to store the full log, however, increases according to the length of our travel. By contrast, if we have retrospective knowledge, i.e., the expected battery level at each location, we can program the microdrone to log its battery level at each step (overwriting the record from the previous step). We can then examine the battery level when we are back and see if it matches our expectation. The current battery level can be easily computed via the previous battery level and the energy consumed at the last step, using only constant computation per step. The storage of the battery level requires only constant memory as we do not need to store the full history, which would not be feasible for a microdrone. Thus retrospective knowledge provides a memory-efﬁcient way to perform anomaly detection. Of course, this approach may have lower accuracy than storing the full running log. This is indeed a trade-off between accuracy and memory, and we expect applications of this approach in memory-constrained scenarios such as embedded systems.
To know the expected battery level at L1 at time t is essentially to answer the following question:
Question 2. How much energy do we expect the microdrone to have consumed since the last time it had 100% battery given that it is at L1 at time t?
Unfortunately, GVFs cannot represent retrospective knowledge (e.g., the answer to Question 2) easily. GVFs provide a mechanism to ignore all future events after reaching certain states via setting the discount function at those states to be 0. This mechanism is useful for representing predictive knowledge. For example, in Question 1, we do not care about events after the next charge.
For retrospective knowledge, we, however, need a mechanism to ignore all previous events before reaching certain states. For example, in Question 2, we do not care about events before the last time the microdrone had 100% battery. Unfortunately, GVFs do not have such a mechanism. In
Appendix A, we describe several tricks that attempt to represent retrospective knowledge with GVFs and explain why they are invalid.
In this paper, we propose Reverse GVFs to represent retrospective knowledge. Using the same
MDP formulation of the microdrone system, let the random variable ¯Gt denote the energy the microdrone has consumed at time t since the last time it had 100% battery. To answer Question 2, we are interested in the conditional expectation of ¯Gt given that St = L1. We refer to functions describing such conditional expectations as Reverse GVFs, which we propose to learn via Reverse
Reinforcement Learning. The key idea of Reverse RL is still bootstrapping, but in the reverse direction. It is easy to see that ¯Gt depends on ¯Gt−1 and the energy consumption from t 1 to t.
In general, the quantity of interest at time t depends on that at time t 1 in Reverse RL. This idea of bootstrapping from the past has been explored by Wang et al. (2007, 2008); Hallak and Mannor (2017); Gelada and Bellemare (2019); Zhang et al. (2020d) but was limited to the density ratio learning setting. We propose several Reverse RL algorithms and prove their convergence under linear function approximation. We also propose Distributional Reverse RL algorithms akin to Distributional
−
− 2
RL (Bellemare et al., 2017; Dabney et al., 2017; Rowland et al., 2018) to compute the probability of an event for anomaly detection. We demonstrate empirically the utility of Reverse GVFs in anomaly detection and representation learning.
Besides Reverse RL, there are other approaches we could consider for answering Question 2. For example, we could formalize it as a simple regression task, where the input is the location and the target is the power consumption since the last time the microdrone had 100% battery. We show below that this regression formulation is a special case of Reverse RL, similar to how Monte Carlo is a special case of temporal difference learning (Sutton, 1988). Alternaticely, answering Question 2 is trivial if we have formulated the system as a Partially Observable MDP. We could use either the location or the battery level as the state and the other as the observation. In either case, however, deriving the conditional observation probabilities is nontrivial. We could also model the system as a reversed chain directly as Morimura et al. (2010) in light of reverse bootstrapping. This, however, creates difﬁculties in off-policy learning, which we discuss in Section 5. 2