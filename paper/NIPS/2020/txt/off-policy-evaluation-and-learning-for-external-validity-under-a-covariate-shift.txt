Abstract
We consider evaluating and training a new policy for the evaluation data by using the historical data obtained from a different policy. The goal of off-policy evalua-tion (OPE) is to estimate the expected reward of a new policy over the evaluation data, and that of off-policy learning (OPL) is to ﬁnd a new policy that maximizes the expected reward over the evaluation data. Although the standard OPE and
OPL methods assume the same distribution of covariate between the historical and evaluation data, a covariate shift often exists in real-world applications, i.e., the distribution of the covariate of the historical data is different from that of the evaluation data. In this paper, we derive the efﬁciency bound of an OPE estimator under a covariate shift. Then, we propose doubly robust and efﬁcient estimators for OPE and OPL under a covariate shift by using a nonparametric estimator of the density ratio between the historical and evaluation data distributions. We also discuss other possible estimators and compare their theoretical properties. Finally, we conduct experiments to conﬁrm the effectiveness of the proposed estimators. 1

Introduction
In various applications, such as the design of advertisement, personalized medicine, search engines, and recommendation systems, there is a signiﬁcant interest in evaluating and learning a new policy from historical data (Beygelzimer & Langford, 2009; Li et al., 2010; Athey & Wager, 2017). To accomplish this, we use off-policy evaluation (OPE) and off-policy learning (OPL) methods. The goal of OPE is to evaluate a new policy by estimating the expected reward of the new policy (Dudík et al., 2011; Wang et al., 2017; Narita et al., 2019; Bibaut et al., 2019; Kallus & Uehara, 2019; Oberst
& Sontag, 2019). In contrast, OPL aims to ﬁnd a new policy that maximizes the expected reward (Zhao et al., 2012; Kitagawa & Tetenov, 2018; Zhou et al., 2018; Chernozhukov et al., 2019).
Although the OPE method provides an estimator of the expected reward of a new policy, most ex-isting studies presume that the distributions of covariates are the same between the historical and evaluation data. However, in many real-world applications, the expected reward of a new policy over the distribution of evaluation data is of signiﬁcant interest, which can be different from that of historical data. For example, in the medical ﬁeld, it is known that the results of a randomized controlled trial (RCT) cannot be directly transported because the covariate distribution in a target population is different (Cole & Stuart, 2010). This problem is known as a lack of external validity (Pearl & Bareinboim, 2014). These situations, in which historical and evaluation data follow differ-ent distributions, are also known as covariate shifts (Shimodaira, 2000; Sugiyama et al., 2008). This situation is illustrated in Figure 1.
∗Equal contributions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Under a covariate shift, the standard OPE methods do not yield a consistent estimator of the ex-pected reward over the evaluation data. Moreover, a covariate shift changes the efﬁciency bound of an OPE estimator, which is the lower bound of the asymptotic mean squared error (MSE) among n-consistent estimators. Besides, standard theoretical analysis of OPE cannot be ap-reasonable plied to covariate shift cases as in Remark 2. To handle the covariate shift, we apply importance weighting using the density ratio between the distributions of the covariates of the historical and evaluation data (Shimodaira, 2000; Reddi et al., 2015).
√
Contributions: This paper has four main contributions. First, we derive an efﬁciency bound of
OPE under the covariate shift (Section 3). Second, in Section 4, we propose estimators constructed by the estimators of the density ratio, behavior policy, and conditional expected reward. In particular, we employ nonparametric density ratio estimation (Kanamori et al., 2012) to estimate the density ra-tio. The proposed estimator is an efﬁcient estimator, which achieves the efﬁciency bound under mild nonparametric rate conditions of the estimators of nuisance functions. In addition, this estimator is robust to model-misspeciﬁcation of estimators in the sense that the resulting estimator is consistent if either (i) models of the density ratio and the behavior policy or (ii) a model of the conditional average treatment effect is correct. Importantly, we do not require the Donsker conditions for those estimators by applying the cross-ﬁtting (Section 4). Third, we propose other possible estimators for our problem setting and compare them (Section 5). Fourth, an OPL method is proposed based on the efﬁcient estimators (Section 6). All proofs are shown in Appendix E.