Abstract
Random projections or sketching are widely used in many algorithmic and learning contexts. Here we study the performance of iterative Hessian sketch for least-squares problems. By leveraging and extending recent results from random matrix theory on the limiting spectrum of matrices randomly projected with the subsam-pled randomized Hadamard transform, and truncated Haar matrices, we can study and compare the resulting algorithms to a level of precision that has not been possible before. Our technical contributions include a novel formula for the second moment of the inverse of projected matrices. We also ﬁnd simple closed-form expressions for asymptotically optimal step-sizes and convergence rates. These show that the convergence rate for Haar and randomized Hadamard matrices are identical, and asymptotically improve upon Gaussian random projections. These techniques may be applied to other algorithms that employ randomized dimension reduction. 1

Introduction
Random projections are a classical way of performing dimensionality reduction, and are widely used in many algorithmic and learning contexts, e.g., [32, 17, 35, 9] etc. In this work, we study the performance of the iterative Hessian sketch [24], in the context of overdetermined least-squares problems f (x) : = x∗ : = argmin 1 2 (cid:107)
Rn×d is a given data matrix with n (cid:62) d and b
−
Rn is a vector of observations. For
Here A simplicity of notations, we assume throughout this work that rank(A) = d. We will leverage and extend recent results on the limiting spectral distributions of two classical subspace embeddings, random uniform projections and the subsampled randomized Hadamard transform (SRHT), to compare corresponding iterative Hessian sketch versions. 2 (cid:107) x∈Rd
Ax (1)
∈
∈ b
. (cid:26) (cid:27)
The iterative Hessian sketch (IHS) is an effective iterative method for solving least-squares [23, 24, 14, 28] (and more general convex optimal optimization problems [25]), and it aims to address the and condition number dependency of standard iterative solvers as follows. Given step sizes
µt
{
}
∗Equal contributions. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
momentum parameters
, it computes the update
βt
{
} xt+1 = xt
µtH −1 t ∇
− f (xt) + βt(xt xt−1) ,
− (2) where the Hessian H = A(cid:62)A of f is approximated by Ht = A(cid:62)S(cid:62) i.i.d. sketching (random) matrices with dimensions m i.i.d. property of the sketching matrices as refreshed matrices. n and m (cid:28)
× t StA, and S0, . . . , St, . . . are n. From now on, we refer to the
∈ (mnd) basic operations (using classical matrix multiplication). This is larger than the cost
There are many possible choices for the sketching matrices St, and this is critical for the performance
Rm×n with independent and identically distributed of the IHS. A classical sketch is a matrix S (0, m−1), for which the matrix multiplication SA requires in general (i.i.d.) Gaussian entries (nd2)
O of solving (1) with direct methods when m (cid:62) d. Another well-studied embedding is the (truncated) n Haar matrix S, whose rows are orthonormal and with range uniformly distributed among the m subspaces of Rn with dimension m. However, this requires time (nm2) to be formed, through a
Gram-Schmidt procedure, which is also larger than (nd2).
N
O
O
×
The SRHT [1, 27] is another classical random orthogonal embedding. Due to the recursive structure (nd log m) time, so that the SRHT of the Hadamard transform, the sketch SA can be formed in is often viewed as a standard reference point for comparing sketching algorithms. Moreover, for many applications, random projections with i.i.d. entries perform worse compared to orthogonal projections [17, 18, 9]. More recently, this observation has also found some theoretical support in limited contexts [8, 36]. Works by [6] also showed the guaranteed improved performance in accuracy and/or speed. Consequently, along with computational considerations, these results favor the SRHT over Gaussian projections.
O
O
Our goal in this work is to design an optimal version of the IHS with SRHT and Haar embeddings.
For this purpose, it is necessary to have a tight characterization of the spectral properties of the matrix
U (cid:62)S(cid:62)SU where U is an n d partial orthogonal matrix (see, e.g., [13]). With Gaussian embeddings, the matrix U (cid:62)S(cid:62)SU has the well-studied Wishart distribution, see e.g., [19, 3, 29, 5, 7, 38]. In fact, [13] provided an optimal IHS with Gaussian embeddings, and showed that the best achievable 2 scales as (d/m)t. However, a similar analysis does not work for SRHT and x∗) error (cid:107)
Haar sketches. To make progress on this problem, we aim to leverage powerful tools from asymptotic random matrix theory, and we consider the asymptotic regime where we let the relevant dimensions go to inﬁnity.
A(xt (cid:107)
×
−
Our technical analysis is based on asymptotic random matrix theory, see e.g., [3, 29, 5, 7, 38] etc.
Classical results such as the Marchenko-Pastur law do not address well the case of the SRHT, and we leverage recent results on asymptotically liberating sequences established by [2] (see also [31] for prior work). Further, we are inspired by the work of [8], who, to our knowledge, ﬁrst leveraged these results to study the SRHT. However, their results are limited to one-step "sketch-and-solve" methods, and do not address the iterative Hessian sketch. Moreover, while we build on their results, we also need to extend them signiﬁcantly: for instance, we need to derive the second moment formula for
θ2,h in (11), which is novel and non-trivial to establish.
Beyond the IHS, there exist other randomized pre-conditioning methods [4, 10, 20, 26] for solving least-squares, which are based on the SRHT (or closely related sketches) which address effectively the condition number dependency of iterative solvers. Besides least-squares, SRHT sketches are widely used for a wide range of applications across numerical linear algebra, statistics and convex optimization, such as low-rank matrix factorization [11, 34], kernel regression [37], random subspace optimization [16], or sketch and solve linear regression [8], see the reviews above for applications.
Hence, a reﬁned analysis of the SRHT, including our speciﬁc technical contributions, may also lead to better algorithms in these ﬁelds. d n ∈ (0, 1), ξ : = limn,m→∞
Throughout the paper, we will consistently use the following assumptions and notations for the aspect ratios, γ : = limn,d→∞ (0, 1), and the subscript g (resp. h) will refer to Gaussian-related (resp. Haar and Hadamard-related) quantities. We z 2 for the Euclidean norm of a real vector z, z 2 for the operator norm use the notations (cid:107) ≡ (cid:107) (cid:107) (cid:107)
M of a matrix M , and
F for its Frobenius norm. For a sequence of iterates
, we denote the (cid:107) (cid:107)
{ error vector ∆t : = U (cid:62)A(xt d matrix of left singular vectors of A. In
∆t particular, we have that (cid:107) (γ, 1) and ρg : = γ x∗), where U is the n m n ∈ x∗) (cid:107)
M (cid:107)
A(xt
− 2 =
ξ ∈ xt 2.
−
× (cid:107) (cid:107) (cid:107)
} 2
1.1 Overview of our results, contributions and questions left open
All our contributions hold in the asymptotic limit n, d, m assumption that the aspect ratios (d/n) and (m/n) have ﬁnite limits.
→ ∞
, and under the aforementioned
We work with the matrix U (cid:62)S(cid:62)SU , where U is an n d matrix with orthonormal columns and S is
× an m n Haar or SRHT matrix. Our ﬁrst results concern Haar projections (Section 3). By leveraging results about their limiting spectral distributions, and after some calculations with Stieljes transforms (deﬁned below) we provide the following new trace formula (see Lemma 3.2):
×
θ2,h : = lim n→∞ 1 d tr E (cid:2)(U (cid:62)S(cid:62)SU )−2(cid:3) = (1
−
γ)(γ2 + ξ
γ)3 (ξ
− 2γξ)
.
−
As an application, we characterize explicitly the optimal step sizes µt and momentum parameters βt of the IHS with Haar embeddings (Theorem 3.1). We emphasize that the optimal parameters have asymptotically closed form for any data matrix A, unlike for certain other propular methods such as gradient descent, which can be useful in practice. With these optimal parameters, we ﬁnd that at any time step t (cid:62) 1 (Theorem 3.1), lim n→∞ 2
E
∆t (cid:107)
∆0 (cid:107) 2 = ρt h , (cid:107) (cid:107) (3) where the convergence rate ρh is given by ρh : = ρg
γ2+ξ−2ξγ , and always satisﬁes ρh < ρg.
By comparing with the prior work [13], this implies that Haar embeddings have uniformly better performance than Gaussian ones. Further, as an immediate consequence of Theorem 2 in [13], we obtain that the optimal momentum parameters βt are equal to 0, that is, Heavy-ball momentum does not accelerate the algorithm with refreshed Haar embeddings (Theorem 3.1 and following discussion).
Thus, we are able to characterize explicitly the optimal version of the IHS with Haar embeddings.
·
ξ(1−ξ)
Our next results concern SRHT sketches (Section 4). We prove that under the additional mild assumption on the initial error ∆0 that E[∆0∆(cid:62) 0 ] = d−1Id, the IHS with SRHT embeddings also has rate of convergence ρh (Theorem 4.1). This relies on novel formulas for the ﬁrst two inverse moments of SRHT sketches (Lemma 4.3). Consequently, SRHT matrices uniformly outperform Gaussian embeddings. Then, we conﬁrm numerically the above theoretical statements (Section 6).
We ﬁnally analyze the computational complexity of our method, in comparison to some standard randomized pre-conditioned solvers [26] for dense, ill-conditioned least-squares. We show that in our inﬁnite-dimensional regime, we improve by a factor log d (Section 5).
Importantly, we speciﬁcally focus on the IHS with refreshed i.i.d. embeddings. An immediate variant of the IHS uses the same update (2), but with a ﬁxed embedding S drawn only once at the ﬁrst iteration, which is appealing in practice. In a concurrent paper [15] more recent to the initial version of the present work, it has been shown that, in the same asymptotic regime, the IHS with a ﬁxed
SRHT embedding achieves a better convergence rate. Thus, we emphasize that our core contributions are to develop novel techniques and results for analyzing the IHS with the SRHT, as this may be useful for future developments and extensions of this algorithm in different contexts (e.g., constrained least-squares, convex optimization).
Although we characterize the optimal step sizes and momentum parameters for the IHS with Haar embeddings, we only characterize the optimal step size in the absence of momentum for the IHS with the SRHT. It is thus left as an open question to know whether momentum can accelerate further our method. 2 Technical