Abstract
We study the control of an unknown linear dynamical system under general convex costs. The objective is minimizing regret vs the class of strongly-stable linear
In this work, we ﬁrst consider the case of known cost functions, for policies. which we design the ﬁrst polynomial-time algorithm with n3
T -regret, where n is the dimension of the state plus the dimension of control input. The
T -horizon dependence is optimal, and improves upon the previous best known bound of T 2/3. The main component of our algorithm is a novel geometric exploration strategy: we adaptively construct a sequence of barycentric spanners in an over-parameterized policy space. Second, we consider the case of bandit feedback, for which we give the ﬁrst polynomial-time algorithm with poly(n)
T -regret, building on Stochastic Bandit Convex Optimization.
√
√
√ 1

Introduction
In this paper we study the online control of an unknown linear dynamical system under general con-vex costs. This is a fundamental problem in control theory and it also embodies a central challenge of reinforcement learning: balancing exploration and exploitation in continuous spaces. For this reason, it has recently received considerable attention from the machine learning community.
Controlling unknown LDS: evolves as
In a linear dynamical system (LDS), the system state xt ∈ Rdx xt+1 = A∗xt + B∗ut + wt, where x1 = 0, (1) i.i.d∼ N (0, I), and ut ∈ Rdu is the learner’s control input, wt ∈ Rdx is a noise process drawn as wt
A∗, B∗ are unknown system matrices. The learner applies control ut at timestep t, then observes the state xt+1 and suffers cost c(xt, ut), where c is a convex function. We consider two forms of cost information for the learner: the case where c is known in advance, and the bandit version where only the scalar cost is observed.
Even if the dynamics where known, there are problem instances where the optimal policy is a very complicated function [6]. A way to circumvent this is to consider a policy class that is both expres-sive and tractable, and aim for performing as well as the best policy from that class. The objective that captures this goal is regret, and is a standard performance metric in online control. In this paper, in accordance with the previous works, we choose the policy class to be all strongly-stable linear policies1. These are policies that 1) select the control as a linear function of the state, and 2) mix fast to a steady state. These policies are a reasonable baseline, since they are optimal for the special case where the cost c is strongly convex quadratic. 1In Section 5, we explain how we can extend all our results to hold for the more general class of stabilizing linear-dynamical-control policies. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Formally, regret with respect to the class K of all strongly stable linear policies is deﬁned as
RT =
T (cid:88) t=1 c(xt, ut) − T min
K∈K
J(K), where a policy K ∈ K applies ut = Kxt, and, letting EK denote expectation under this policy,
J(K) = lim
T →∞ 1
T
EK (cid:34) T (cid:88) t=1 (cid:35) c (xt, ut) (2) (3) is the average inﬁnite-horizon cost of K.
√
Our main result is a polynomial-time algorithm for the case of known cost function, that achieves n3
T -regret, where n = dx + du. This is the optimal dependence in the time-horizon and our result improves upon the previous best known bound of T 2/3 [17, 29]. Perhaps more importantly than the regret bound is that, using ideas from convex geometry, we design a novel exploration strategy which signiﬁcantly expands the existing algorithmic toolbox for balancing exploration and exploitation in linear dynamical systems, as we explain below.
Beyond explore-then-commit: the challenge of exploration
The only algorithms we know for this problem apply the simplest exploration strategy: “explore-then-commit” (ETC), known in control literature as certainty equivalence.
In ETC, the learner spends the ﬁrst T0 steps playing random controls (e.g. ut ∼ N (0, I)), then estimates the system dynamics, and thereafter executes a greedy policy, based on these estimates. On the other hand, the whole stochastic bandit and RL theory literature is about sophisticated and sample-efﬁcient explo-ration, mostly relying on the principle of optimism in the face of uncertainty (OFU). Unfortunately, implementing OFU in online control requires solving optimization problems that are intractable in general [1]. Even though this computational issue can be circumvented for the case of quadratic costs using semideﬁnite programming [12], these techniques do not apply for general convex costs. In this work, we do not follow the OFU principle. Our exploration strategy is based on adaptively construct-ing a sequence of barycentric spanners (Deﬁnition 7) in an over-parameterized policy space.
The importance of general convex costs
The special case of convex quadratic costs is the classical linear quadratic regulator and is frequently used, because it leads to a nice analytical solution when the system is known [7]. However, this mod-eling choice is fairly restrictive, and in 1987, Tyrrell Rockafellar [23] proposed the use of general convex functions for modelling the cost in a LDS, in order to handle constraints on state and control.
In practice, imposing constraints is crucial for ensuring safe operating conditions. 1.1 Statement of results.
We consider both the setting where A∗ is strongly stable (Assumption 1), and in the Appendix, we deal with unstable systems, by assuming that the the learner is initially given a strongly-stable linear policy (Assumption 4)2. Our main result is the geometric exploration strategy given in Algorithm 1, for the case of known cost function. Algorithm 4 is for the case of bandit feedback. We now state informal versions of our theorems. Let C = C(A∗, B∗) denote a constant that depends polynomially on natural system parameters.
Theorem 1 (informal). For online control of LDS with known cost function, with high probability,
Algorithm 1 has regret 3
RT ≤ (cid:101)O(C) · n3
√
T . (4)
Theorem 2 (informal). For online control of LDS with bandit feedback, with high probability, Al-gorithm 4 has regret
RT ≤ (cid:101)O(C) · poly(n)
√
T . (5) 2For unstable systems, without Assumption 4, the regret is exponential in the n (see [10]). 3 (cid:101)O(1) hides logarithmic factors. 2
In Theorem 2, the polynomial dependence in n is rather large (n36). The large dimension depen-T -regret algorithms for bandit convex optimization (BCO). Our setting is even dence is typical in more challenging than BCO, since the environment has a state.
√ 1.2 Our Approach and New Techniques
The cost J(K) is nonconvex in K. To remedy this, we use an alternative disturbance-based policy class, introduced in [4], where the control is linear in the past disturbances (as opposed to the state): ut =
H (cid:88) i=1
M [i−1]wt−i, (6) where H is a hyperparameter. This parameterization is useful, because the ofﬂine search for an optimal disturbance-based policy can be posed as a convex program. In [4], the authors show that disturbance-based policies can approximate all strongly stable linear policies. We move on to de-scribe the novel components of our work.
Geometric exploration: Algorithm 1 runs in epochs and follows the phased-elimination paradigm
[20]. During epoch r, it focuses on a convex disturbance-based policy set Mr, which by the end of the epoch will be substituted by Mr+1 ⊆ Mr. In the beginning of the epoch, it decides on a set of exploratory policies to be executed during the epoch. This is done by constructing a barycentric spanner of the policy set Mr (Deﬁnition 7). At the end of the epoch, it reﬁnes its estimates for the system matrices A∗, B∗ and creates Mr+1 ⊆ Mr.
Estimating the disturbances: A typical difﬁculty in using disturbance-based policies is that we do not know the disturbances, and since the system matrices are unknown, we cannot recover wt by using wt = xt+1 − A∗xt − B∗ut. A key contribution of this work is showing that disturbances can be accurately estimated, without accurately estimating the system matrices A∗, B∗.
Coupling: At the end of epoch r, we create estimates (cid:98)A, (cid:98)B of A∗, B∗, which we use to create the next policy set Mr+1. These estimates are not necessarily accurate in every direction. To argue that Mr+1 contains good policies, we employ a probabilistic coupling argument between the true system and the estimated one. This could be a useful technique for future works.
Robustness of stochastic BCO: For bandit feedback, we require a generalization of the Stochastc
Bandit Convex Optimization (SBCO) framework, where the noise contains a small adversarial com-ponent in addition to the stochastic part. The need for robustness to adversarial noise arises from the estimation error on the disturbances (cid:107) (cid:98)wt − wt(cid:107). We prove that by appropriately changing the parameters in the SBCO algorithm from [3], we can get
T -regret for this more general model.
√ 1.3 Prior Work
LQR: When the cost c is convex quadratic, we obtain the online linear quadratic regulator (LQR)
T -regret algorithms
[1, 14, 21, 12, 27]. The problem was introduced in [1], and [21, 12, 27] gave with polynomial runtime and polynomial regret dependence on relevant problem parameters. In
[27], the authors proved that
T -regret is optimal.
√
√
Convex costs: Closer to our work are recent papers on online control with general convex costs
[17, 29, 19]. These papers consider even more general models, i.e, adversarially changing convex cost functions, in [17, 29] they also allow adversarial disturbances and [29, 19] deal with partial observation. Furthermore, all the considered policy classes have linear structure and in [17] they use the same policy class as we do, i.e. strongly-stable linear policies. Despite the differences in the models, a common feature of these works is that all algorithms apply explore-then-commit (ETC).
For our setting, ETC gives T 2/3-regret. Under the assumption that c is strongly convex, the problem is signiﬁcantly simpliﬁed and ETC achieves
T -regret [29].
√
Linear system identiﬁcation: To address unknown systems, we make use of least-squares es-timation [28, 24]. Recent papers deal with system indentiﬁcation under partial observation
[22, 25, 30, 26]. 3
Bandit feedback: Control with bandit feedback has been studied in [9] (known system) and [15] (both known and unknown system). Our result is comparable to [15], and improves upon the T 3/4 regret bound that they achieve, when the disturbances are stochastic and the cost is a ﬁxed function.
Barycentric spanners: Barycentric spanners have been used for exploration in stochastic linear bandits [5]. However, in that context, the barycentric spanner is computed ofﬂine and remains ﬁxed, while our algorithm adaptively changes it, based on the observed states. 2 Setting and