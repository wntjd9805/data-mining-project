Abstract
We introduce a method for learning provably stable deep neural network based dynamic models from observed data. Speciﬁcally, we consider discrete-time stochastic dynamic models, as they are of particular interest in practical applications such as estimation and control. However, these aspects exacerbate the challenge of guaranteeing stability. Our method works by embedding a Lyapunov neural network into the dynamic model, thereby inherently satisfying the stability criterion.
To this end, we propose two approaches and apply them in both the deterministic and stochastic settings: one exploits convexity of the Lyapunov function, while the other enforces stability through an implicit output layer. We demonstrate the utility of each approach through numerical examples. 1

Introduction
Stability is a critical requirement in the design of physical systems. White-box models based on
ﬁrst principles can explicitly account for stability in their design. On the other hand, deep neural networks (DNNs) are ﬂexible function approximators, well suited for modeling complicated dynamics.
However, their black-box design makes both physical interpretation and stability analysis challenging.
This paper focuses on the construction of provably stable DNN-based dynamic models. These models are amenable to standard deep learning architectures and training practices, while retaining the asymptotic behavior of the underlying dynamics. Speciﬁcally, we focus on stochastic systems whose state xt 2
Rn evolves in discrete time as follows: xt+1 = f (xt, !t+1), (1)
Rd is a stochastic process. Although real physical systems typically evolve in continuous where !t 2 time, the periodic sampling of measurement and control signals in digital systems give great practical interest to discrete-time analysis. Moreover, noise often plays a prominent role in the underlying dynamics, making it an important feature to consider in the stability analysis. Our strategy starts from the philosophy proposed by Manek and Kolter [30]: It is easier to construct a stable dynamic model by simultaneously training a suitable Lyapunov function, than it is to separately verify stability for a trained model a posteriori. In this work, we propose two methods for guaranteeing stability of
N0, 2 t 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
deterministic discrete-time dynamic models: we ﬁrst exploit convexity of a Lyapunov function given by a neural network, and then propose a general approach using an implicit output layer. We then show how to extend our framework from the deterministic case to the stochastic case. 2