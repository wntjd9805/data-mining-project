Abstract
We propose a framework that lifts the capabilities of graph convolutional networks (GCNs) to scenarios where no input graph is given and increases their robustness to adversarial attacks. We formulate a joint probabilistic model that considers a prior distribution over graphs along with a GCN-based likelihood and develop a stochastic variational inference algorithm to estimate the graph posterior and the GCN parameters jointly. To address the problem of propagating gradients through latent variables drawn from discrete distributions, we use their continuous relaxations known as Concrete distributions. We show that, on real datasets, our approach can outperform state-of-the-art Bayesian and non-Bayesian graph neural network algorithms on the task of semi-supervised classiﬁcation in the absence of graph data and when the network structure is subjected to adversarial perturbations. 1

Introduction
Graphs represent the elements of a system and their relationships as a set of nodes and edges, re-spectively. By exploiting the inter-dependencies of these elements, many applications of machine learning have achieved signiﬁcant success, for example in the areas of social networks [15], document classiﬁcation [25, 40] and bioinformatics [12]. In particular, motivated by the incredible success of convolutional neural networks [CNNs, 26] on regular-grid data, researchers have generalized some of their fundamental properties (such as their ability to learn local stationary structures efﬁciently) to graph-structured data [7, 17, 10]. These approaches mainly focused on exploiting feature dependen-cies explicitly deﬁned by a graph in an analogous way to how convolutional neural networks (CNNs) model long-range correlations through local interactions across pixels in an image. The seminal work by [25] leveraged these ideas to model dependencies across instances (instead of features) to be able to incorporate knowledge of the instances’ relationships in a semi-supervised learning setting, going beyond the standard i.i.d. assumption.
In this work we focus precisely on the problem of semi-supervised classiﬁcation based on the method developed in [25], which is now commonly referred to as graph convolutional networks (GCNs).
These networks can be seen as a ﬁrst-order approximation of the spectral graph convolutional networks developed by [10], which itself built upon the pioneering work of [7, 17]. The great popularity of graph convolutional networks (GCNs) is mainly due to their practical performance
∗Joint ﬁrst author. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
as, at the time it was published, it outperformed related methods by a signiﬁcant margin. Another practical advantage of using GCNs is their relatively simple propagation rule, which does not require expensive operations such as eigen-decomposition.
However, one of the main assumptions underlying GCNs is that the given graph is helpful for the task at hand and that the corresponding edges are highly reliable. This is generally not true in practical applications, as the given graph may be (i) noisy, (ii) loosely related to the classiﬁcation problem of interest or (iii) built in an ad hoc basis using, e.g., side information. Although these settings have been addressed by previous work independently [see e.g. 13, 48], in practice, it is difﬁcult to incorporate this type of uncertainty over the graph using the original GCN framework in a principled way and the performance of the method degrades signiﬁcantly with increasingly noisy graphs.
Consequently, in this paper we propose a framework that lifts GCN’s capabilities to handle the more challenging cases of learning in the absence of an input graph and dealing with highly-effective adversarial perturbations such as those proposed recently by [4]. We achieve this by formulating a joint probabilistic model that places a prior over the graph structure, as given by the adjacency matrix, along with a GCN-based conditional likelihood. This, however, poses signiﬁcant inference challenges as estimating the posterior over the adjacency matrix under a highly non-linear likelihood (as given by the GCN’s output) is intractable.
Thus, to estimate the graph posterior we resort to approximations via stochastic variational inference (VI). Nevertheless, even in the approximate inference world, carrying out posterior estimation over a very large discrete combinatorial space can prove extremely hard. We adopt a simple but effective relaxation over both the prior and the approximate posterior using Concrete distributions
[28], which allows us to propagate gradients in the corresponding stochastic computational graph.
Our experiments show that we can outperform state-of-the-art Bayesian and non-Bayesian graph neural network algorithms in the task of semi-supervised classiﬁcation (i) in the absence of graph data; (ii) when the network structure is subjected to adversarial perturbations and (iii) when considering the ground truth graphs. Our results and analyses indicate that our framework does indeed learn new graph representations by turning on/off connections so as to improve performance on the given task. 1.1