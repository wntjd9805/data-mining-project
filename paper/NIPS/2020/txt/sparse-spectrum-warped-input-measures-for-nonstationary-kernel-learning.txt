Abstract
We establish a general form of explicit, input-dependent, measure-valued warpings for learning nonstationary kernels. While stationary kernels are ubiquitous and simple to use, they struggle to adapt to functions that vary in smoothness with respect to the input. The proposed learning algorithm warps inputs as conditional
Gaussian measures that control the smoothness of a standard stationary kernel.
This construction allows us to capture non-stationary patterns in the data and provides intuitive inductive bias. The resulting method is based on sparse spectrum
Gaussian processes, enabling closed-form solutions, and is extensible to a stacked construction to capture more complex patterns. The method is extensively validated alongside related algorithms on synthetic and real world datasets. We demonstrate a remarkable efﬁciency in the number of parameters of the warping functions in learning problems with both small and large data regimes. 1

Introduction
Many interesting real world phenomena exhibit varying characteristics, such as smoothness, across their domain. Simpler phenomena that do not exhibit such variation may be called stationary.
The typical kernel based learner canonically relies on a stationary kernel function, a measure of
"similarity", to deﬁne the prior beliefs over the function space. Such a kernel, however, cannot represent desirable nonstationary nuances, like varying spatial smoothness and sudden discontinuities.
Restrictive stationary assumptions do not generally hold and limit applicability to interesting problems, such as robotic control and reinforcement learning [1], spatial mapping [2], genetics [3], and Bayesian optimisation [4]. One obvious way to alleviate the problem of ﬁnding the appropriate kernel function given one’s data is hyperparameter optimisation. However for a GP with stationary kernel, even if the optimal set of hyperparameters were found, it would be insufﬁcient if our underlying response were nonstationary with respect to the observed inputs.
In this paper we propose a method for nonstationary kernel learning, based on sparse spectral kernel representations. Our method is linear in complexity with respect to the number of data points and is simultaneously able to extract nonstationary patterns. In our setup, we consider the problem of learning a function f : X → R as a nonstationary Gaussian process. We decompose f as: f (x) = E[u ◦ m(x)|u] , x ∈ X , (1) where ◦ denotes function composition, u : Q → R is a function over a latent space Q, and m(x) represents the warped input. If u has covariance function ku, the resulting f follows a GP with covariance kf (x, x(cid:48)) = E[ku(m(x), m(x(cid:48)))]. The latter constitutes a nonstationary kernel.
∗Equal contribution 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
To model u as a stationary GP on Q, we propose a formulation for m : X → Q, which is based on a locally afﬁne stochastic transform: m(x) = G(x)x + h(x) , (2) where G(x) and h(x) are Gaussian processes. Intuitively, the matrix G scales the inputs, with a similar effect to what length-scales have on stationary kernels [5], but which now varies across the space, while h allows for arbitrary shifts.
The conditional expectation (1) also corresponds to the composition of a function on Q with a measure [6] on Q, which is actually a function of x ∈ X . In our case, the measure-valued warpings are Gaussian probability measures, which we parametrise as Gaussian process conditioned on pseudo-training points. In particular, we use sparse spectrum Gaussian processes [7] due to their scalability and availability of closed-form results for Gaussian inputs [8]. 1.1 Contributions
• We propose a new method to learn nonstationary Gaussian process models via input warping.
We introduce the use of a measure-valued, self-supervised and input-dependent warping function as a natural improvement for sparse spectrum Gaussian processes. We term this sparse spectrum warped input measures (SSWIM);
• We propose a self-supervised training scheme for representing the warping function allowing us to cleanly represent the latent measure valued warping; and
• We propose a simple extension to multiple levels of warping by propagating moments. 1.2