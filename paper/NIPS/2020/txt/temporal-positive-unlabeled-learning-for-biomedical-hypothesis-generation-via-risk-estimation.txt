Abstract
Understanding the relationships between biomedical terms like viruses, drugs, and symptoms is essential in the ﬁght against diseases. Many attempts have been made to introduce the use of machine learning to the scientiﬁc process of hypothesis generation (HG), which refers to the discovery of meaningful implicit connections between biomedical terms. However, most existing methods fail to truly capture the temporal dynamics of scientiﬁc term relations and also assume unobserved connections to be irrelevant (i.e., in a positive-negative (PN) learning setting). To break these limits, we formulate this HG problem as future connectivity prediction task on a dynamic attributed graph via positive-unlabeled (PU) learning. Then, the key is to capture the temporal evolution of node pair (term pair) relations from just the positive and unlabeled data. We propose a variational inference model to estimate the positive prior, and incorporate it in the learning of node pair embeddings, which are then used for link prediction. Experiment results on real-world biomedical term relationship datasets and case study analyses on a
COVID-19 dataset validate the effectiveness of the proposed model. 1

Introduction
Recently, the study of co-relationships between biomedical entities is increasingly gaining attention.
The ability to predict future relationships between biomedical entities like diseases, drugs, and genes enhances the chances of early detection of disease outbreaks and reduces the time required to detect probable disease characteristics. For instance, in 2020, the COVID-19 outbreak pushed the world to a halt with scientists working tediously to study the disease characteristics for containment, cure, and vaccine. An increasing number of articles encompassing new knowledge and discoveries from these studies were being published daily [1]. However, with the accelerated growth rate of publications, the manual process of reading to extract undiscovered knowledge increasingly becomes a tedious and time-consuming task beyond the capability of individual researchers.
In an effort towards an advanced knowledge discovery process, computers have been introduced to play an ever-greater role in the scientiﬁc process with automatic hypothesis generation (HG). The study of automated HG has attracted considerable attention in recent years [41, 25, 45, 47]. Several previous works proposed techniques based on association rules [25, 18, 47], clustering and topic modeling [45, 44, 5], text mining [43, 42], and others [28, 49, 39]. However, these previous works fail to truly utilize the crucial information encapsulated in the dynamic nature of scientiﬁc discoveries and assume that the unobserved relationships denote a non-relevant relationship (negative).
To model the historical evolution of term pair relations, we formulate HG on a term relationship graph
G = {V, E}, which is decomposed into a sequence of attributed graphlets G = {G1, G2, ..., GT }, where the graphlet at time t is deﬁned as, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Deﬁnition 1. Temporal graphlet: A temporal graphlet Gt = {V t, Et, xt v} is a temporal subgraph at time step t, which consists of nodes (terms) V t satisfying V 1 ⊆ V 2, ..., ⊆ V T and the observed co-occurrence between these terms Et satisfying E1 ⊆ E2, ..., ⊆ ET . And xt v is the node attribute.
Example of the node terms can be covid-19, fever, cough, Zinc, hepatitis B virus etc. When two terms co-occurred at time t in scientiﬁc discovery, a link between them is added to Et, and the nodes are added to V t if they haven’t been added.
Deﬁnition 2. Hypothesis Generation (HG): Given G = {G1, G2, ..., GT }, the target is to predict which nodes unlinked in V T should be linked (a hypothesis is generated between these nodes).
We address the HG problem by modeling how Et was formed from t = 1 to T (on a dynamic graph), rather than using only ET (on a static graph). In the design of learning model, it is clear to us the observed edges are positive. However, we are in a dilemma whether the unobserved edges are positive or negative. The prior work simply set them to be negative, learning in a positive-negative (PN) setting) based on a closed world assumption that unobserved connections are irrelevant (negative)
[39, 28, 4]. We set the learning with a more realistic assumption that the unobserved connections are a mixture of positive and negative term relations (unlabeled), a.k.a. Positive-unlabeled (PU) learning, which is different from semi-supervised PN learning that assumes a known set of labeled negative samples. For the observed positive samples in PU learning, they are assumed to be selected entirely at random from the set of all positive examples [16]. This assumption facilitates and simpliﬁes both theoretical analysis and algorithmic design since the probability of observing the label of a positive example is constant. However, estimating this probability value from the positive-unlabeled data is nontrivial. We propose a variational inference model to estimate the positive prior and incorporate it in the learning of node pair embeddings, which are then used for link prediction (hypothesis generation).
We highlight the contributions of this work as follows. 1) Methodology: we propose a PU learning approach on temporal graphs. It differs from other existing approaches that learn in a conventional PN setting on static graphs. In addition, we estimate the positive prior via a variational inference model, rather than setting by prior knowledge. 2) Application: to the best of our knowledge, this is the ﬁrst the application of PU learning on the HG problem, and on dynamic graphs. We applied the proposed model on real-world graphs of terms in scholarly publications published from 1945 to 2020. Each of the three graphs has around 30K nodes and 1-2 million edges. The model is trained end-to-end and shows superior performance on HG. Case studies demonstrate our new and valid ﬁndings of the positive relationship between medical terms, including newly observed terms that were not observed in training. 2