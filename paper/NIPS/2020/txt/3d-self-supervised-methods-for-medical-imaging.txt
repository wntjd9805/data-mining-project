Abstract
Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application ﬁelds. In this work, we leverage these techniques, and we propose 3D versions for ﬁve different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation.
The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efﬁciently, compared to training the models from scratch and to pretraining them on 2D slices. We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain
Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D
CT, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efﬁciency, performance, and speed of convergence.
Interestingly, we also ﬁnd gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-speciﬁc dataset. We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. We publish our implementations1 for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets. 1

Introduction
Due to technological advancements in 3D sensing, the need for machine learning-based algorithms that perform analysis tasks on 3D imaging data has grown rapidly in the past few years [1–3]. 3D imaging has numerous applications, such as in Robotic navigation, in CAD imaging, in Geology, and in Medical Imaging. While we focus on medical imaging as a test-bed for our proposed 3D algorithms in this work, we ensure their applicability to other 3D domains. Medical imaging plays a vital role in patient healthcare, as it aids in disease prevention, early detection, diagnosis, and treatment. Yet efforts to utilize advancements in machine learning algorithms are often hampered by the sheer expense of the expert annotation required [4]. Generating expert annotations of 3D medical images at scale is non-trivial, expensive, and time-consuming. Another related challenge in medical imaging is the relatively small sample sizes. This becomes more obvious when studying a particular disease, for instance. Also, gaining access to large-scale datasets is often difﬁcult due to privacy concerns. Hence, scarcity of data and annotations are some of the main constraints for machine learning applications in medical imaging. 1https://github.com/HealthML/self-supervised-3d-tasks 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Several efforts have attempted to address these challenges, as they are common to other application
ﬁelds of deep learning. A widely used technique is transfer learning, which aims to reuse the features of already trained neural networks on different, but related, target tasks. A common example is adapting the features from networks trained on ImageNet, which can be reused for other visual tasks, e.g. semantic segmentation. To some extent, transfer learning has made it easier to solve tasks with limited number of samples. However, as mentioned before, the medical domain is supervision-starved.
Despite attempts to leverage ImageNet [5] features in the medical context [6–9], the difference in the distributions of natural and medical images is signiﬁcant, i.e. generalizing across these domains is questionable and can suffer from dataset bias [10]. Recent analysis [11] has also found that such transfer learning offers limited performance gains, relative to the computational costs it incurs.
Consequently, it is necessary to ﬁnd better solutions for the aforementioned challenges.
A viable alternative is to employ self-supervised (unsupervised) methods, which proved successful in multiple domains recently. In these approaches, the supervisory signals are derived from the data. In general, we withhold some part of the data, and train the network to predict it. This prediction task deﬁnes a proxy loss, which encourages the model to learn semantic representations about the concepts in the data. Subsequently, this facilitates data-efﬁcient ﬁne-tuning on supervised downstream tasks, reducing signiﬁcantly the burden of manual annotation. Despite the surge of interest in the machine learning community in self-supervised methods, only little work has been done to adopt these methods in the medical imaging domain. We believe that self-supervised learning is directly applicable in the medical context, and can offer cheaper solutions for the challenges faced by conventional supervised methods. Unlabelled medical images carry valuable information about organ structures, and self-supervision enables the models to derive notions about these structures with no additional annotation cost.
A particular aspect of most medical images, which received little attention by previous self-supervised methods, is their 3D nature [12]. The common paradigm is to cast 3D imaging tasks in 2D, by ex-tracting slices along an arbitrary axis, e.g. the axial dimension. However, such tasks can substantially beneﬁt from the full 3D spatial context, thus capturing rich anatomical information. We believe that relying on the 2D context to derive data representations from 3D images, in general, is a suboptimal solution, which compromises the performance on downstream tasks.
Our contributions. As a result, in this work, we propose ﬁve self-supervised tasks that utilize the full 3D spatial context, aiming to better adopt self-supervision in 3D imaging. The proposed tasks are: 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. These algorithms are inspired by their successful 2D counterparts, and to the best of our knowledge, most of these methods have never been extended to the 3D context, let alone applied to the medical domain. Several computational and methodological challenges arise when designing self-supervised tasks in 3D, due to the increased data dimensionality, which we address in our methods to ensure their efﬁciency. We perform extensive experiments using four datasets in three different downstream tasks, and we show that our 3D tasks result in rich data representations that improve data-efﬁciency and performance on three different downstream tasks.
Finally, we publish the implementations of our 3D tasks, and also of their 2D versions, in order to allow other researchers to evaluate these methods on other imaging datasets. 2