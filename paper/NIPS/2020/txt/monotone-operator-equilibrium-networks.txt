Abstract
Implicit-depth models such as Deep Equilibrium Networks have recently been shown to match or exceed the performance of traditional deep networks while being much more memory efﬁcient. However, these models suffer from unsta-ble convergence to a solution and lack guarantees that a solution exists. On the other hand, Neural ODEs, another class of implicit-depth models, do guarantee existence of a unique solution but perform poorly compared with traditional net-works. In this paper, we develop a new class of implicit-depth model based on the theory of monotone operators, the Monotone Operator Equilibrium Network (monDEQ). We show the close connection between ﬁnding the equilibrium point of an implicit network and solving a form of monotone operator splitting problem, which admits efﬁcient solvers with guaranteed, stable convergence. We then de-velop a parameterization of the network which ensures that all operators remain monotone, which guarantees the existence of a unique equilibrium point. Fi-nally, we show how to instantiate several versions of these models, and implement the resulting iterative solvers, for structured linear operators such as multi-scale convolutions. The resulting models vastly outperform the Neural ODE-based models while also being more computationally efﬁcient. Code is available at http://github.com/locuslab/monotone_op_net. 1

Introduction
Recent work in deep learning has demonstrated the power of implicit-depth networks, models where features are created not by explicitly iterating some number of nonlinear layers, but by ﬁnding a solution to some implicitly deﬁned equation. Instances of such models include the Neural ODE
[8], which computes hidden layers as the solution to a continuous-time dynamical system, and the
Deep Equilibrium (DEQ) Model [5], which ﬁnds a ﬁxed point of a nonlinear dynamical system corresponding to an effectively inﬁnite-depth weight-tied network. These models, which trace back to some of the original work on recurrent backpropagation [2, 23], have recently regained attention since they have been shown to match or even exceed to performance of traditional deep networks in domains such as sequence modeling [5]. At the same time, these models show drastically improved memory efﬁciency over traditional networks since backpropagation is typically done analytically using the implicit function theorem, without needing to store the intermediate hidden layers.
However, implict-depth models that perform well require extensive tuning in order to achieve stable convergence to a solution. Obtaining convergence in DEQs requires careful initialization and regularization, which has proven difﬁcult in practice [21]. Moreover, solutions to these models are not guaranteed to exist or be unique, making the output of the models potentially ill-deﬁned. While
Neural ODEs [8] do guarantee existence of a unique solution, training remains unstable since the
ODE problems can become severely ill-posed [10]. Augmented Neural ODEs [10] improve the stability of Neural ODEs by learning ODEs with simpler ﬂows, but neither model achieves efﬁcient 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
convergence nor performs well on standard benchmarks. Crucial questions remain about how models can have guaranteed, unique solutions, and what algorithms are most efﬁcient at ﬁnding them.
In this paper, we present a new class of implicit-depth equilibrium model, the Monotone Operator
Equilibrium Network (monDEQ), which guarantees stable convergence to a unique ﬁxed point.1 The model is based upon the theory of monotone operators [6, 26], and illustrates a close connection between simple ﬁxed-point iteration in weight-tied networks and the solution to a particular form of monotone operator splitting problem. Using this connection, this paper lays the theoretical and practical foundations for such networks. We show how to parameterize networks in a manner that ensures all operators remain monotone, which establishes the existence and uniqueness of the equilibrium point. We show how to backpropagate through such networks using the implicit function theorem; this leads to a corresponding (linear) operator splitting problem for the backward pass, which also is guaranteed to have a unique solution. We then adapt traditional operator splitting methods, such as forward-backward splitting or Peaceman-Rachford splitting, to naturally derive algorithms for efﬁciently computing these equilibrium points.
Finally, we demonstrate how to practically implement such models and operator splitting methods, in the cases of typical feedforward, fully convolutional, and multi-scale convolutional networks. For convolutional networks, the most efﬁcient ﬁxed-point solution methods require an inversion of the associated linear operator, and we illustrate how to achieve this using the fast Fourier transform. The resulting networks show strong performance on several benchmark tasks, vastly improving upon the accuracy and efﬁciency of Neural ODEs-based models, the other implicit-depth models where solutions are guaranteed to exist and be unique. 2