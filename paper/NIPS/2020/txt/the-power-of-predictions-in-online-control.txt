Abstract
We study the impact of predictions in online Linear Quadratic Regulator control with both stochastic and adversarial disturbances in the dynamics. In both settings, we characterize the optimal policy and derive tight bounds on the minimum cost and dynamic regret. Perhaps surprisingly, our analysis shows that the conventional greedy MPC approach is a near-optimal policy in both stochastic and adversarial settings. Speciﬁcally, for length-T problems, MPC requires only O(log T ) predic-tions to reach O(1) dynamic regret, which matches (up to lower-order terms) our lower bound on the required prediction horizon for constant regret. 1

Introduction
This paper studies the effect of using predictions for online control in a linear dynamical system governed by xt+1 = Axt + But + wt, where xt, ut, and wt are the state, control, and disturbance (or exogenous input) respectively. At each time step t, the controller incurs a quadratic cost c(xt, ut).
Recently, considerable effort has been made to leverage and integrate ideas from learning, optimization and control theory to study the design of optimal controllers under various performance criteria, such as static regret [2, 3, 12, 13, 15, 20, 29], dynamic regret [16, 23] and competitive ratio [17, 28].
However, the study of online convergence when incorporating predictions has been largely absent.
Indeed, a key aspect of online control is considering the amount of available information when making decisions. Most recent studies focus on the basic setting where only past information, x0, w0, · · · , wt−1, is available for ut at every time step [2, 13, 15, 28]. However, this basic set-ting does not effectively characterize situations where we have accurate predictions, e.g., when x0, w0, · · · , wt−1+k are available at step t. These types of accurate predictions are often available in many applications, including robotics [8, 27], energy systems [30], and data center management [22].
Moreover, there are many practical algorithms that leverage predictions, such as the popular Model
Predictive Control (MPC) [6–9, 18, 19].
While there has been increased interest in studying online guarantees for control with predictions, to our knowledge, there has been no such study for the case of a ﬁnite-time horizon with disturbances.
Several previous works studied the economic MPC problem by analyzing the asymptotic performance without disturbances [6, 7, 18, 19]. Rosolia and Borrelli [25, 26] studied learning for MPC but focused on the episodic setting with asymptotic convergence guarantees. Li et al. [23] considered a linear system where ﬁnite predictions of costs are available, and analyzed the dynamic regret of their new algorithm; however, they neither consider disturbances nor study the more practically relevant
MPC approach. Goel and Hassibi [16] characterized the ofﬂine optimal policy (i.e., with inﬁnite predictions) and cost in LQR control with i.i.d. zero-mean stochastic disturbances, but those results do not apply to limited predictions or non-i.i.d. disturbances. Other prior works analyze the power of 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
predictions in online optimization [11, 24], but the connection to online control in dynamical systems is unclear.
From this literature, fundamental questions about online control with predictions have emerged: 1. What are the cost-optimal and regret-minimizing policies when given k predictions? What are the corresponding cost and regret of these policies? 2. What is the marginal beneﬁt from each additional prediction used by the policy, and how many predictions are needed to achieve (near-)optimal performance? 3. How well does MPC with k predictions perform compared to cost-optimal and regret-minimizing policies?
Main contributions. We systematically address each of the questions above in the context of LQR systems with general stochastic and adversarial disturbances in the dynamics. In the stochastic case, we explicitly derive the cost-optimal and dynamic regret minimizing policies with k predictions.
In both the stochastic and adversarial cases, we derive (mostly tight) upper bounds for the optimal cost and minimum dynamic regret given access to k predictions. We also show that the marginal beneﬁt of an extra prediction exponentially decays as k increases. Additionally, for MPC speciﬁcally, we show that it has a bounded performance ratio against the cost-optimal policy in both stochastic and adversarial settings. We further show that MPC is near-optimal in terms of dynamic regret, and needs only O(log T ) predictions to achieve O(1) dynamic regret (the same order as is needed by the dynamic regret minimizing policy) in both settings.
We would like to emphasize the generality of the results. The model we consider is the general
LQR setting with disturbance in the dynamics, where only the stabilizability of [A, B] and [A(cid:62), Q] is assumed [4]. Further, in the stochastic setting we consider general distributions, which are not necessarily i.i.d. or zero-mean. Additionally, our results compare to the globally optimal policies for cost and regret rather than compare to the optimal linear or static policy. Finally, our upper bounds are (almost) tight, i.e., there exist some systems such that the bounds are (nearly) reached, up to lower-order terms.
It is perhaps surprising that classic MPC, which is a simple greedy policy (up to the prediction horizon), is near-optimal even with adversarial disturbances in the dynamics. Our results thus highlight the power of predictions to reduce the need for algorithmic sophistication. In that sense, our results somewhat mirror recent developments in the study of exploration strategies in online
LQR control with unknown dynamics {A, B}: after a decade’s research beginning with the work of
Abbasi-Yadkori and Szepesvári [1], Simchowitz and Foster [29] recently show that naive exploration is optimal. Taken together with the result from [29], our paper provides additional evidence for the idea that the structure of LQR allows simple algorithmic ideas to be effective, which sheds light on key algorithmic principles and fundamental limits in continuous control. 2