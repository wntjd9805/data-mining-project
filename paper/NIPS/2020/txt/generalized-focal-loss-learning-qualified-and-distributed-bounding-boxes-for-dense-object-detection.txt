Abstract
One-stage detector basically formulates object detection as dense classiﬁcation and localization (i.e., bounding box regression). The classiﬁcation is usually optimized by Focal Loss and the box location is commonly learned under Dirac delta distribu-tion. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facili-tates the classiﬁcation to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classiﬁcation and localization. Two problems are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classiﬁcation between training and inference, and (2) the inﬂexible Dirac delta distribution for localization. To address the problems, we design new representations for these ele-ments. Speciﬁcally, we merge the quality estimation into the class prediction vector to form a joint representation, and use a vector to represent arbitrary distribution of box locations. The improved representations eliminate the inconsistency risk and accurately depict the ﬂexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal
Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0%
AP using ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5%) and
ATSS (43.6%) with higher or comparable inference speed. 1

Introduction
Recently, dense detectors have gradually led the trend of object detection. Based on dense detectors, researchers focus more on the representation of bounding boxes and their localization quality estimation, leading to an encouraging advancement [26, 29] in the ﬁeld. Speciﬁcally, bounding box representation is modeled as a simple Dirac delta distribution [10, 18, 32, 26, 31], which is widely used over past years. As popularized in FCOS [26], predicting an additional localization quality (e.g.,
IoU score [29] or centerness score [26]) brings consistent improvements of detection accuracy, when
∗Corresponding author. Xiang Li, Jun Li and Jian Yang are from PCA Lab, Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of Image and
Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of
Science and Technology. Xiang Li is also a visiting scholar at Momenta. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Comparisons between existing separate representation and proposed joint representation of classiﬁcation and localization quality estimation. (a): Current practices [12, 26, 29, 35, 31] for the separate usage of the quality branch (i.e., IoU or centerness score) during training and test. (b): Our joint representation of classiﬁcation and localization quality enables high consistency between training and inference. the quality estimation is combined (usually multiplied) with classiﬁcation conﬁdence as ﬁnal scores
[12, 11, 26, 29, 35] for the rank process of Non-Maximum Suppression (NMS) during inference.
Despite their success, we observe the following problems in existing practices:
Inconsistent usage of localization quality estimation and classiﬁcation score between training and inference: (1) In recent dense detectors, the localization quality estimation and classiﬁcation score are usually trained independently but compositely utilized (e.g., multiplication) during inference
[26, 29] (Fig. 1(a)); (2) The supervision of the localization quality estimation is currently assigned for positive samples only [12, 11, 26, 29, 35], which is unreliable as negatives may get chances to have uncontrollably higher quality predictions (Fig. 2(a)). These two factors result in a gap between training and test, and would potentially degrade the detection performance, e.g., negative instances with randomly high-quality scores could rank in front of positive examples with lower quality prediction during NMS.
Inﬂexible representation of bound-ing boxes: The widely used bound-ing box representation can be viewed as Dirac delta distribution [7, 23, 8, 1, 18, 26, 13, 31] of the target box coordi-nates. However, it fails to consider the ambiguity and uncertainty in datasets (see the unclear boundaries of the ﬁg-ures in Fig. 3). Although some recent works [10, 4] model boxes as Gaus-sian distributions, it is too simple to capture the real distribution of the lo-cations of bounding boxes.
In fact, the real distribution can be more ar-bitrary and ﬂexible [10], without the necessity of being symmetric like the
Gaussian function.
Figure 2: Unreliable IoU predictions of current dense detector with IoU-branch. (a):
We demonstrate some background patches (A and B) with extremely high predicted quality scores (e.g., IoU score > 0.9), based on the optimized IoU-branch model in
Fig. 1(a). The scatter diagram in (b) denotes the randomly sampled instances with their predicted scores, where the blue points clearly illustrate the weak correlation be-tween predicted classiﬁcation scores and predicted IoU scores for separate representa-tions. The part in red circle contains many possible negatives with large localization quality predictions, which may potentially rank in front of true positives and impair the performance. Instead, our joint representation (green points) forces them to be equal and thus avoids such risks.
To address the above problems, we design new representations for the bounding boxes and their localization quality. For localization quality representation, we propose to merge it with the classiﬁcation score into a single and uniﬁed representation: a classiﬁcation vector where its value at the ground-truth category index refers to its corresponding localization quality (typically the IoU score between the predicted box and the corresponding ground-truth box in this paper). In this way, we unify classiﬁcation score and IoU score into a joint and single variable (denoted as “classiﬁcation-IoU joint representation”), which can be trained in an end-to-end fashion, whilst directly utilized during inference (Fig. 1(b)). As a result, it eliminates the training-test inconsistency (Fig. 1(b)) and enables the strongest correlation (Fig. 2 (b)) between localization quality and classiﬁcation.
Further, the negatives will be supervised with 0 quality scores, thereby the overall quality predictions become more conﬁdential and reliable. It is especially beneﬁcial for dense object detectors as they rank all candidates regularly sampled across an entire image. For bounding box representation, we propose to represent the arbitrary distribution (denoted as “General distribution” in this paper) of box locations by directly learning the discretized probability distribution over its continuous space, without introducing any other stronger priors (e.g., Gaussian [10, 4]). Consequently, we can obtain more reliable and accurate bounding box estimations, whilst being aware of a variety of their underlying distributions (see the predicted distributions in Fig. 3).
The improved representations then pose challenges for optimization. Traditionally for dense detectors, the classiﬁcation branch is optimized with Focal Loss [18] (FL). FL can successfully handles the 2
Figure 3: Due to occlusion, shadow, blur, etc., the boundaries of many objects are not clear enough, so that the ground-truth labels (white boxes) are sometimes not credible and Dirac delta distribution is limited to indicate such issues. Instead, the proposed learned representation of General distribution for bounding boxes can reﬂect the underlying information by its shape, where a ﬂatten distribution denotes the unclear and ambiguous boundaries (see red circles) and a sharp one stands for the clear cases. The predicted boxes by our model are marked green. class imbalance problem via reshaping the standard cross entropy loss. However, for the case of the proposed classiﬁcation-IoU joint representation, in addition to the imbalance risk that still exists, we face a new problem with continuous IoU label (0∼1) as supervisions, as the original FL only supports discrete {1, 0} category label currently. We successfully solve the problem by extending FL from
{1, 0} discrete version to its continuous variant, termed Generalized Focal Loss (GFL). Different from FL, GFL considers a much general case in which the globally optimized solution is able to target at any desired continuous value, rather than the discrete ones. More speciﬁcally in this paper, GFL can be specialized into Quality Focal Loss (QFL) and Distribution Focal Loss (DFL), for optimizing the improved two representations respectively: QFL focuses on a sparse set of hard examples and simultaneously produces their continuous 0∼1 quality estimations on the corresponding category;
DFL makes the network to rapidly focus on learning the probabilities of values around the continuous locations of target bounding boxes, under an arbitrary and ﬂexible distribution.
We demonstrate three advantages of GFL: (1) It bridges the gap between training and test when one-stage detectors are facilitated with additional quality estimation, leading to a simpler, joint and effective representation of both classiﬁcation and localization quality; (2) It well models the
ﬂexible underlying distribution for bounding boxes, which provides more informative and accurate box locations; (3) The performance of one-stage detectors can be consistently boosted without introducing additional overhead. On COCO test-dev, GFL achieves 45.0% AP with ResNet-101 backbone, surpassing state-of-the-art SAPD (43.5%) and ATSS (43.6%). Our best model can achieve a single-model single-scale AP of 48.2% whilst running at 10 FPS on a single 2080Ti GPU. 2