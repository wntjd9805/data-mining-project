Abstract
Computing the gradient of model hyperparameters, i.e., hypergradient, enables a promising and natural way to solve the hyperparameter optimization task. However, gradient-based methods could lead to suboptimal solutions due to the non-convex nature of optimization in a complex hyperparameter space. In this study, we propose a hyperparameter mutation (HPM) algorithm to explicitly consider a learnable trade-off between using global and local search, where we adopt a population of student models to simultaneously explore the hyperparameter space guided by hypergradient and leverage a teacher model to mutate the underperforming students by exploiting the top ones. The teacher model is implemented with an attention mechanism and is used to learn a mutation schedule for different hyperparameters on the ﬂy. Empirical evidence on synthetic functions is provided to show that HPM outperforms hypergradient signiﬁcantly. Experiments on two benchmark datasets are also conducted to validate the effectiveness of the proposed HPM algorithm for training deep neural networks compared with several strong baselines. 1

Introduction
Hyperparameter optimization (HPO) [4, 11] is one of the fundamental research problems in the
ﬁeld of automated machine learning. It aims to maximize the model performance by tuning model hyperparameters automatically, which could be achieved either by searching a ﬁxed hyperparame-ter conﬁguration setting [3, 22, 32, 9] from the predeﬁned hyperparameter space or by learning a hyperparameter schedule along with the training process [17, 25]. Among existing methods, hypergra-dient [2, 26] forms a promising direction, as it naturally enables gradient descent on hyperparameters.
Hypergradient is usually deﬁned as the gradient of a validation loss function w.r.t hyperparameters.
Previous methods mainly focus on computing hypergradients by using reverse-mode differentiation [2, 6, 26], or designing a differentiable response function [12, 25] for hyperparameters, yet without explicitly considering the non-convex optimization nature in a complex hyperparameter space. Thus, while hypergradient methods could deliver highly-efﬁcient local search solutions, they may easily get stuck in local minima and achieve suboptimal performance. This can be clearly observed on some synthetic functions which share a similar shape of parameter space to the HPO problem (see Sec. 4.1).
It also leads to the question: can we ﬁnd a way to help hypergradient with global information?
The population based hyperparameter search methods work as a good complementary to the hypergra-dient, such as evolutionary search [27, 5], particle swarm optimization [8], and the population based
∗Work done when Zhiqiang Tao interned at Alibaba Group and worked at Northeastern University. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
training [17, 21, 14], which generally employ a population of agent models to search different hy-perparameter conﬁgurations and update hyperparameters with a mutation operation. The population could provide sufﬁcient diversity to globally explore hypergradients throughout the hyperparameter space. However, it is non-trivial to incorporate hypergradients in the population based methods due to a possible conﬂict between the hand-crafted mutation operation (e.g., random perturbation) and the direction of hypergradient descent.
To address the above challenges, we propose a novel hyperparameter mutation (HPM) scheduling algorithm in this study, which adopts a population based training framework to explicitly learn a trade-off (i.e., a mutation schedule) between using the hypergradient-guided local search and the mutation-driven global search. We develop the proposed framework by alternatively proceeding model training and hyperparameter mutation, where the former jointly optimizes model parameters and hyperparameters upon gradients, while the latter leverages a student-teaching schema for the exploration. Particularly, HPM treats the population as a group of student models and employs a teacher model to mutate the hyperparameters of underperforming students. We instantiate our teacher model as a neural network with attention mechanism and learn the mutation direction towards minimizing the validation loss. Beneﬁting from learning-to-mutate, the mutation is adaptively scheduled for the population based training with hypergradient.
In the experiments, we extensively discuss the properties of the proposed HPM algorithm and show that HPM signiﬁcantly outperforms hypergradient and global search methods on synthetic functions.
We also employ the HPM scheduler in training deep neural networks on two benchmark datasets, where experimental results validate the effectiveness of HPM compared with several strong baselines. 2