Abstract
Domain Adaptation (DA) enables transferring a learning machine from a labeled source domain to an unlabeled target one. While remarkable advances have been made, most of the existing DA methods focus on improving the target accuracy at inference. How to estimate the predictive uncertainty of DA models is vital for decision-making in safety-critical scenarios but remains the boundary to explore. In this paper, we delve into the open problem of Calibration in DA, which is extremely challenging due to the coexistence of domain shift and the lack of target labels.
We ﬁrst reveal the dilemma that DA models learn higher accuracy at the expense of well-calibrated probabilities. Driven by this ﬁnding, we propose Transferable
Calibration (TransCal) to achieve more accurate calibration with lower bias and variance in a uniﬁed hyperparameter-free optimization framework. As a general post-hoc calibration method, TransCal can be easily applied to recalibrate existing
DA methods. Its efﬁcacy has been justiﬁed both theoretically and empirically. 1

Introduction
Deep neural networks (DNNs) achieve the state of the art predictive accuracy in machine learning tasks with the beneﬁt of powerful ability to learn discriminative representations [35, 11, 57]. However, in real-world scenarios, it is hard (intolerably time-consuming and labor-expensive) to collect sufﬁcient labeled data through manual labeling, causing DNNs to confront challenges when generalizing the pre-trained model to a different domain with unlabeled data. To tackle this challenge, researchers propose to transfer knowledge from a different but related domain by leveraging the readily-available labeled data, a.k.a. domain adaptation (DA) [44].
There are mainly two types of domain adaptation formulas: covariate shift [44, 37, 29, 13] and label shift [27, 2, 1], while we focus on the former in this paper since it appears more natural in recognition tasks and attracts more attention in the literature. Early domain adaptation methods bridge the source and target domains mainly by learning domain-invariant representations [37, 16] or instance importances [23, 15]. After the breakthrough in deep neural networks (DNNs) has been achieved, they are widely believed to be able to learn more transferable features [35, 11, 57, 61], since they disentangle explanatory factors of variations. Recent works in deep domain adaptation can be mainly grouped into two categories: 1) moment matching. These methods align representations across domains by minimizing the discrepancy between feature distributions [51, 29, 31, 32, 28]; 2) adversarial training. These methods adversarially learn transferable feature representations by confusing a domain discriminator in a two-player game [14, 50, 30, 55, 60].
While numerous domain adaptation methods have been proposed, most of them mainly focus on improving the accuracy in the target domain but fail to estimate the predictive uncertainty, falling
∗Corresponding author: Mingsheng Long (mingsheng@tsinghua.edu.cn) 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
Figure 1: Left: A comparison between IID Calibration with TransCal, where φ denotes the deep model; Right: an observation on the accuracy and ECE of various DA methods (12 transfer tasks of
Ofﬁce-Home [52] with ResNet-50 [22]), indicating that DA models learn higher accuracy than the
SourceOnly ones at the expense of well-calibrated probabilities. See more results in D.1 of Appendix. short of a miscalibration problem [20]. The accuracy of a deep adapted model constitutes only one side of the coin, here we delve into the other side of the coin, i.e. the calibration of accuracy and conﬁdence, which requires the model to output a probability that reﬂects the true frequency of an event. For example, if an automated diagnosis system says 1,000 patients have lung cancer with probability 0.1, approximately 100 of them should indeed have lung cancer. Calibration is fundamental to deep neural models and of great signiﬁcance for decision-making in safety-critical scenarios. With built-in [12, 25] or post-hoc [42, 20] recalibration methods, the conﬁdence and accuracy of deep models can be well-calibrated in the independent and identically distributed (IID) scenarios. However, it remains unclear how to maintain calibration under dataset shifts, especially when we do not have labels from the target dataset, as in the general setting of Unsupervised Domain
Adaptation (UDA). We identify two obstacles in the way of applying calibration to UDA:
• The lack of labeled examples in the target domain. We know that the existing successful post-hoc IID recalibration methods mostly rely on ground-truth labels in the validation set to select the optimal temperature [42, 20]. However, since ground-truth labels are not available in the target domain, it is not feasible to directly apply IID calibration methods to UDA.
• Dataset shift entangled with the miscalibration of DNNs. Since DNNs are believed to learn more transferable features [35, 57], many domain adaptation methods embed DNNs to implicitly close the domain shift and rely on DNNs to achieve higher classiﬁcation accuracy.
However, DNNs are prone to over-conﬁdence [20], falling short of a miscalibration problem.
To this end, we study the open problem of Calibration in DA, which is extremely challenging due to the coexistence of the domain gap and the lack of target labels. To ﬁgure out the calibration error on the target domain of DA models, we ﬁrst delve into the predictions and conﬁdences of the target dataset. By calculating the target accuracy and ECE [20] (a calibration error measure deﬁned in 3.1) with various domain adaptation models before calibration, we found something interesting. As shown in the right panel of Figure 1, the accuracy increases from the weakest SourceOnly [22] model to the latest state-of-the-art MDD [60] model, while the ECE becomes larger as well. That is, after applying domain adaptation methods, miscalibration phenomena become severer compared with SourceOnly model, indicating that the domain adaptation models learn higher classiﬁcation accuracy at the expense of well-calibrated probabilities. This dilemma is unacceptable in safety-critical scenarios, as we need higher accuracy while maintaining calibration. Worse still, the well-performed calibration methods in the IID setting cannot be directly applied to DA due to the domain shift.
To tackle the dilemma between accuracy and calibration, we propose a new Transferable Calibration (TransCal) method in DA, achieving more accurate calibration with lower bias and variance in a uniﬁed hyperparameter-free optimization framework, while a comparison with IID calibration is shown in the left panel of Figure 1. Speciﬁcally, we ﬁrst deﬁne a new calibration measure, Importance
Weighted Expected Calibration Error (IWECE) to estimate the calibration error in the target domain in a transferable calibration framework. Next, we propose a learnable meta parameter to further reduce the estimation bias from the perspective of theoretical analysis. Meanwhile, we develop a serial control variate method to further reduce the variance of the estimated calibration error. As a general post-hoc calibration method, TransCal can be easily applied to recalibrate existing DA methods. This paper has the following contributions:
• We uncover a dilemma in the open problem of Calibration in DA: existing domain adaptation models learn higher classiﬁcation accuracy at the expense of well-calibrated probabilities. 2
• We propose a Transferable Calibration (TransCal) method, achieving more accurate calibra-tion with lower bias and variance in a uniﬁed hyperparameter-free optimization framework.
• We conduct extensive experiments on various DA methods, datasets, and calibration metrics, while the effectiveness of our method has been justiﬁed both theoretically and empirically. 2