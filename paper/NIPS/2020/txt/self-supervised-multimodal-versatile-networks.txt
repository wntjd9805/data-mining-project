Abstract
Videos are a rich source of multi-modal supervision. In this work, we learn repre-sentations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams. To this end, we introduce the notion of a multimodal versatile network – a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that ﬁne-grained representations of the visual and audio modalities can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deﬂation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks includ-ing UCF101, HMDB51, Kinetics600, AudioSet and ESC-50 when compared to previous self-supervised work. Our models are publicly available‡. 1

Introduction
Our experience of the world is multimodal. From as far back as the crib, we perceive through multi-sensory systems, for instance we watch the ﬂames dancing in the ﬁreplace, we hear the sound of the crackling wood, as well as feel the heat coming off. Through this multimodal synchronous perception, we learn to draw useful connections between modalities [73] which, in turn, enables us to form good representations of the world. Later, comes language that allows us to communicate this
ﬁne-grained multimodal experience using higher-level abstract concepts.
Our objective is to learn representations from such multimodal experience in a self-supervised manner without resorting to any speciﬁc manual annotation. The modalities considered are the three that are easily available from large collections of unlabelled videos: visual, audio and language (obtained from narrations) streams. In this, we seek to learn a multimodal versatile network, deﬁned as a network that has the following four properties: (i) it should be able to take as input any of the three modalities; (ii) it should respect the speciﬁcity of modalities, in particular the fact that the audio and visual modalities are much more ﬁne-grained than language; (iii) it should enable the different
∗Equal contribution.
†Work done during an internship at DeepMind. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
modalities to be easily compared even when they are never seen together during training; and ﬁnally (iv) it should be efﬁciently applicable to visual data coming in the form of dynamic videos or static images.
The question is how to design a network that respects these four principles? We choose a design that embeds each modality into a vector space such that similarity between modalities is obtained via simple dot products. Each modality is processed by a backbone network adapted to the nature of the signal, and a modality embedding graph is constructed such that the visual and audio embeddings are
ﬁne-grained, whilst the textual embedding is semantically coarse-grained. This strategy is based on the observation that the visual and audio spaces are ﬁne-grained (there are many visual or sounds of guitars that might be really different to each other) while the textual domain is more coarse as its goal is to abstract away details (e.g. a single “guitar” word). The network is then trained from scratch via self-supervised contrastive learning on a large set of unlabelled videos.
To quantitatively evaluate our learned MultiModal Versatile (MMV) networks, we measure their performance on multiple downstream tasks, and in this way assess various properties of the rep-resentation of videos and images: verb learning (action classiﬁcation on HMBD51, UCF101 and
Kinetics600); noun learning (image classiﬁcation on PASCAL VOC and ImageNet); joint text and visual representation (YouCook2, MSRVTT); and audio representation (sound classiﬁcation on ESC-50 and AudioSet). The proposed MMV achieves state-of-the-art performance for self-supervised approaches on these benchmarks, and reduces the gap to the state-of-the-art performance for supervised approaches.
Contributions. Our contributions are the following: (a) we investigate different modality embedding graphs for MMV, and propose a simple yet effective self-supervised training strategy for multimodal representation of audio, visual and language streams; (b) we introduce a deﬂation approach so that the MMV video network can efﬁciently ingest a static image; and (c) we demonstrate the superiority of the learned representations on multiple image, video, audio and video-text downstream tasks. 2