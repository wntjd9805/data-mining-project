Abstract
Recent success of pre-trained language models crucially hinges on ﬁne-tuning them on large amounts of labeled data for the downstream task, that are typically expensive to acquire or difﬁcult to access for many applications. We study self-training as one of the earliest semi-supervised learning approaches to reduce the annotation bottleneck by making use of large-scale unlabeled data for the target task.
Standard self-training mechanism randomly samples instances from the unlabeled pool to generate pseudo-labels and augment labeled data. We propose an approach to improve self-training by incorporating uncertainty estimates of the underlying neural network leveraging recent advances in Bayesian deep learning. Speciﬁcally, we propose (i) acquisition functions to select instances from the unlabeled pool leveraging Monte Carlo (MC) Dropout, and (ii) learning mechanism leveraging model conﬁdence for self-training. As an application, we focus on text classiﬁcation with ﬁve benchmark datasets. We show our methods leveraging only 20-30 labeled samples per class for each task for training and for validation perform within 3% of fully supervised pre-trained language models ﬁne-tuned on thousands of labels with an aggregate accuracy of 91% and improvement of up to 12% over baselines. 1

Introduction
Motivation. Deep neural networks are the state-of-the-art for various applications. However, one of the biggest challenges facing them is the lack of labeled data to train these complex networks.
Not only is acquiring large amounts of labeled data for every task expensive and time consuming, but also it is not feasible to perform large-scale human labeling, in many cases, due to data access and privacy constraints. Recent advances in pre-training help close this gap. In this, deep and large neural networks like BERT [Devlin et al., 2019], GPT-2 [Radford et al., 2019] and RoBERTa [Liu et al., 2019] are trained on millions of documents in a self-supervised fashion to obtain general purpose language representations. However, even with a pre-trained model, we still need task-speciﬁc
ﬁne-tuning that typically requires thousands of labeled instances to reach state-of-the-art performance.
For instance, our experiments show 16% relative improvement when ﬁne-tuning BERT with the full training set (25K-560K labels) vs. ﬁne-tuning with only 30 labels per class. Recent work [Wang et al., 2020a] show this gap to be bigger for structured learning tasks such as sequence labeling.
Semi-supervised learning (SSL) [Chapelle et al., 2010] is one of the promising paradigms to address this shortcoming by making effective use of large amounts of unlabeled data in addition to some labeled data for task-speciﬁc ﬁne-tuning. Recent work [Xie et al., 2019] on leveraging SSL with consistency learning has shown state-of-the-art performance for text classiﬁcation with limited labels leveraging auxiliary resources like back-translation and forms a strong baseline for our work.
Self-training (ST, [Scudder, 1965]) as one of the earliest SSL approaches has recently been shown to obtain state-of-the-art performance for tasks like neural machine translation [He et al., 2019], named 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
(a) Self-training framework. (b) Augmenting self-training with uncertainty estimation.
Figure 1: Uncertainty-aware self-training framework. entity recognition and slot tagging for task-oriented dialog systems [Wang et al., 2020a]; performing at par with supervised systems without using any auxiliary resources. For self-training, a base model (teacher) is trained on some amount of labeled data and used to pseudo-annotate (task-speciﬁc) unlabeled data. The original labeled data is augmented with the pseudo-labeled data and used to train a student model. The student-teacher training is repeated until convergence. Such frameworks have also been recently used for distillation [Wang et al., 2020b, Mukherjee and Hassan Awadallah, 2020] to transfer knowledge from huge pre-trained language models to shallow student models for efﬁcient inference often operating over task-speciﬁc labeled data and unlabeled transfer data.
Traditionally, self-training mechanisms do not consider the teacher uncertainty or perform any sample selection during the pseudo-labeling process. This may result in gradual drifts from self-training on noisy pseudo-labeled instances [Zhang et al., 2017]. Sample selection leveraging teacher conﬁdence has been studied in curriculum learning [Bengio et al., 2009] and self-paced learning [Kumar et al., 2010] frameworks. These works leverage the easiness of the samples to inform a learning schedule like training on easy concepts ﬁrst followed by complex ones. Since it is hard to assess the easiness of a sample, especially in deep neural network based architectures, these works rely only on the teacher model loss, while ignoring its uncertainties, for sample selection.
Intuitively, if the teacher model already predicts some samples with high conﬁdence, then there is little to gain with self-training if we focus only on these samples. On the other hand, hard examples for which the teacher model has less conﬁdence are hard to rely on for self-training as these could be noisy or too difﬁcult to learn from. In this scenario, the model could beneﬁt from judiciously selecting examples for which the teacher model is uncertain about. However, it is non-trivial to generate uncertainty estimates for non-probabilistic models like deep neural networks. To this end, we leverage recent advances in Bayesian deep learning [Gal and Ghahramani, 2016] to obtain uncertainty estimates of the teacher for pseudo-labeling and improving the self-training process.
Our task and framework overview. We focus on leveraging pre-trained language models for classiﬁcation with few labeled samples (e.g., K = {20, 30}) per class for training and validation, and large amounts of task-speciﬁc unlabeled data. Figure 1(a) shows an overview of a traditional self-training framework, where augmented data is obtained from hard pseudo-labels from the teacher (e.g.,
BERT [Devlin et al., 2019]) without accounting for its uncertainty. Figure 1(b) shows an overview of our uncertainty-aware self-training framework (UST)1. We extend the traditional self-training framework with three core components, namely: (i) Masked model dropout for uncertainty estimation:
We adopt MC dropout [Gal and Ghahramani, 2016] as a technique to obtain uncertainty estimates from the pre-trained language model. In this, we apply stochastic dropouts after different hidden layers in the neural network model and approximate the model output as a random sample from the posterior distribution. This allows us to compute the model uncertainty in terms of the stochastic mean and variance of the samples with a few stochastic forward passes through the network. (ii)
Sample selection. Given the above uncertainty estimates for a sample, we employ entropy-based measures to select samples that the teacher is most or least confused about to infuse for self-training corresponding to easy- and hard-entropy-aware example mining. (iii) Conﬁdent learning. In this, we train the student model to explicitly account for the teacher conﬁdence by emphasizing on the low variance examples. All of the above components are jointly used for end-to-end learning. We adopt BERT as our encoder and show that its performance can be signiﬁcantly improved by an average of 12% for few-shot settings without using any auxiliary resources. Furthermore, we also 1Code is available at http://aka.ms/UST 2
outperform recent models [Xie et al., 2019] that make use of auxiliary resources like back-translation.
In summary, our work makes the following contributions. (i) Develops an uncertainty-aware self-training framework for few-shot text classiﬁcation. (ii) Compares the effectiveness of various sample selection schemes leveraging teacher uncertainty for self-training. (iii) Demonstrates its effectiveness for text classiﬁcation with few labeled samples on ﬁve benchmark datasets. 2