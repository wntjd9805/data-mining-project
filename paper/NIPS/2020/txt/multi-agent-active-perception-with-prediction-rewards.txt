Abstract
Multi-agent active perception is a task where a team of agents cooperatively gath-ers observations to compute a joint estimate of a hidden variable. The task is decentralized and the joint estimate can only be computed after the task ends by fusing observations of all agents. The objective is to maximize the accuracy of the estimate. The accuracy is quantiﬁed by a centralized prediction reward determined by a centralized decision-maker who perceives the observations gathered by all agents after the task ends. In this paper, we model multi-agent active perception as a decentralized partially observable Markov decision process (Dec-POMDP) with a convex centralized prediction reward. We prove that by introducing individual prediction actions for each agent, the problem is converted into a standard Dec-POMDP with a decentralized prediction reward. The loss due to decentralization is bounded, and we give a sufﬁcient condition for when it is zero. Our results allow application of any Dec-POMDP solution algorithm to multi-agent active perception problems, and enable planning to reduce uncertainty without explicit computation of joint estimates. We demonstrate the empirical usefulness of our results by apply-ing a standard Dec-POMDP algorithm to multi-agent active perception problems, showing increased scalability in the planning horizon. 1

Introduction
Active perception, collecting observations to reduce uncertainty about a hidden variable, is one of the fundamental capabilities of an intelligent agent [2]. In multi-agent active perception a team of autonomous agents cooperatively gathers observations to infer the value of a hidden variable.
Application domains include search and rescue robotics, sensor networks, and distributed hypothesis testing. A multi-agent active perception task often has a ﬁnite duration: after observations have been gathered, they are collected to a central database for inference. While the inference phase is centralized, the observation gathering phase is decentralized: each agent acts independently, without knowing the observations collected by the other agents nor guaranteed communication to the other agents.
The key problem in multi-agent active perception is to determine how each agent should act during the decentralized phase to maximize the informativeness of the collected observations, evaluated afterwards during the centralized inference phase. The problem can be formalized as a decentralized partially observable Markov decision process (Dec-POMDP) [3, 15], a general model of sequential multi-agent decision-making under uncertainty. At each time step in a Dec-POMDP, each agent in the team takes an individual action. The next state is determined according to a Markov chain conditional on the current hidden state and all individual actions. Each agent then perceives an individual observation correlated with the next state and the individual actions. The agents should act so as to maximize the expected sum of shared rewards, accumulated at each time step over a ﬁnite horizon. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
In the decentralized phase, the per-step reward depends on the hidden state and the individual actions of all agents. In typical Dec-POMDPs, the reward on all time steps is of this form. Active perception problems are modelled by a reward that is a convex function of the team’s joint estimate of the hidden state, for example the negative entropy [11]. This encourages agents to act in ways that lead to joint state estimates with low uncertainty. In analogy to the centralized inference phase, the reward at the ﬁnal time step can be thought of as a centralized prediction reward where a centralized decision-maker perceives the individual action-observation histories of all agents and determines a reward based on the corresponding joint state estimate. Due to the choice of reward function, algorithms for standard Dec-POMDPs are not applicable to such active perception problems. Despite the pooling of observations after the end of the task, the problem we target is decentralized. We design a strategy for each agent to act independently during the observation gathering phase, without knowing for sure how the others acted or what they perceived. Consequently, the joint state estimate is not available to any agent during the observation gathering phase. Strategies executable in a centralized manner are available only if all-to-all communication during task execution is possible, which we do not assume. As decentralized strategies are a strict subset of centralized strategies, the best decentralized strategy is at most as good as the best centralized strategy [16].
In this paper, we show that the convex centralized prediction reward can be converted to a decen-tralized prediction reward that is a function of the hidden state and so-called individual prediction actions. This converts the Dec-POMDP with a convex centralized prediction reward into a standard
Dec-POMDP with rewards that depend on the hidden state and actions only. This enables solving multi-agent active perception problems without explicit computation of joint state estimates applying any standard Dec-POMDP algorithm. We show that the error induced when converting the centralized prediction reward into a decentralized one, the loss due to decentralization, is bounded. We also give a sufﬁcient condition for when the loss is zero, meaning that the problems with the centralized, respectively decentralized, prediction rewards are equivalent. We prove the empirical usefulness of our results by applying standard Dec-POMDP solution algorithms to active perception problems, demonstrating improved scalability over the state-of-the-art.
The remainder of the paper is organized as follows. We review related work in Section 2, and give preliminary deﬁnitions for Dec-POMDPs in Section 3. In Section 4, we introduce our proposed conversion of a centralized prediction reward to a decentralized prediction reward. We propose a method to apply standard Dec-POMDP algorithms to multi-agent active perception in Section 5, and empirically evaluate the method in Section 6. Section 7 concludes the paper. 2