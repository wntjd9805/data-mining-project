Abstract
Hyperparameter optimization of neural networks can be elegantly formulated as a bilevel optimization problem. While research on bilevel optimization of neural networks has been dominated by implicit differentiation and unrolling, hypernet-works such as Self-Tuning Networks (STNs) have recently gained traction due to their ability to amortize the optimization of the inner objective. In this paper, we diagnose several subtle pathologies in the training of STNs. Based on these obser-vations, we propose the ∆-STN, an improved hypernetwork architecture which stabilizes training and optimizes hyperparameters much more efﬁciently than STNs.
The key idea is to focus on accurately approximating the best-response Jacobian rather than the full best-response function; we achieve this by reparameterizing the hypernetwork and linearizing the network around the current parameters. We demonstrate empirically that our ∆-STN can tune regularization hyperparameters (e.g. weight decay, dropout, number of cutout holes) with higher accuracy, faster convergence, and improved stability compared to existing approaches. 1

Introduction
Tuning regularization hyperparameters such as weight decay, dropout [55], and data augmentation is indispensable for state-of-the-art performance in a challenging dataset such as ImageNet [8, 52, 7]. An automatic approach to adapting these hyperparameters would improve performance and simplify the engineering process. Although black box methods for tuning hyperparameters such as grid search, random search [2], and Bayesian optimization [53, 54] work well in low-dimensional hyperparameter spaces, they are computationally expensive, require many runs of training, and require that hyperparameter values be ﬁxed throughout training.
Hyperparameter optimization can be elegantly formulated as a bilevel optimization problem [5, 15].
Let w ∈ Rm denote parameters (e.g. weights and biases) and λ ∈ Rh denote hyperparameters (e.g. weight decay). Let LV and LT denote validation and training objectives, respectively. We aim to ﬁnd the optimal hyperparameters λ∗ that minimize the validation objective at the end of training.
Mathematically, the bilevel objective can be formulated as follows1:
λ∗ = arg min
λ∈Rh
LV (λ, w∗) subject to w∗ = arg min w∈Rm
LT (λ, w) (1.1)
In machine learning, most work on bilevel optimization has focused on implicit differentiation [28, 48] and unrolling [39, 14]. A more recent approach, which we build on in this work, explicitly approximates the best-response (rational reaction) function r(λ) = arg minw LT (λ, w) with a hypernetwork [49, 19] and jointly optimizes the hyperparameters and the hypernetwork [35]. The 1The uniqueness of arg min is assumed throughout this paper. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
hypernetwork approach is advantageous because training the hypernetwork amortizes the inner-loop optimization work required by both implicit differentiation and unrolling. Since best-response functions are challenging to represent due to their high dimensionality, Self-Tuning Networks (STNs) [38] construct a structured hypernetwork to each layer of the neural network, thereby allowing the efﬁcient and scalable approximation of the best-response function.
In this work, we introduce the ∆-STN, a novel architecture for bilevel optimization that ﬁxes several subtle pathologies in training STNs. We ﬁrst improve the conditioning of the Gauss-Newton Hessian and ﬁx undesirable bilevel optimization dynamics by reparameterizing the hypernetwork, thereby enhancing the stability and convergence in training. Based on the proposed parameterization, we further introduce a modiﬁed training scheme that reduces variance in parameter updates and eliminates any bias induced by perturbing the hypernetwork.
Next, we linearize the best-response hypernetwork to yield an afﬁne approximation of the best-response function. In particular, we linearize the dependency between the network’s parameters and predictions so that the training algorithm is encouraged to accurately approximate the Jacobian of the best-response function. Empirically, we evaluate the performance of ∆-STNs on linear models, image classiﬁcation tasks, and language modelling tasks, and show our method consistently outperform the baselines, achieving better generalization performance in less time. 2