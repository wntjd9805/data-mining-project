Multi-agent reinforcement learning (MARL) has been successful in solving complex problems such as intelligent traffic signal control, swarm robotics, and autonomous driving. However, the complex interaction model in MARL often introduces instability during the training process. To enhance stability, the centralized training and decentralized execution paradigm has gained attention, particularly the value function decomposition method. While this method has shown outstanding performance, it lacks explicit information exchange between agents during execution, leading to performance degradation in more complex scenarios. Recent studies have introduced inter-agent communication during execution, but little attention has been given to the reliability and efficiency of message exchange. Moreover, excessive and redundant message exchange has been observed, leading to communication overhead. To address these issues, this paper presents Temporal Message Control (TMC), a MARL framework that leverages temporal locality to achieve succinct and robust message exchange. TMC introduces regularizers to reduce temporally correlated messages and implements a buffering mechanism for better robustness against transmission loss. Experimental results show that TMC achieves higher winning rates and reduced communication overhead compared to existing schemes, even in bandwidth-limited and lossy networking environments. This work represents the first study on MARL system operation in such environments.