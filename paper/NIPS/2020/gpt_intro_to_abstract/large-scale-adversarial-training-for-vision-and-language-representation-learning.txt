Inspired by the success of BERT and other multimodal pre-training methods, we propose VILLA, a framework that utilizes adversarial training for vision-and-language representation learning. VILLA consists of two training stages: task-agnostic adversarial pre-training (APT) and task-specific adversarial fine-tuning (AFT). We demonstrate that by applying adversarial training techniques to V+L problems, we can improve model performance and achieve state-of-the-art results on various V+L tasks. We introduce adversarial perturbations at the embedding level for both text and image modalities and use a "free" adversarial training strategy to make the process more efficient. Additionally, we incorporate a KL-divergence-based regularization term to enhance performance. Through experiments, we show the versatility of VILLA by applying it to different pre-training methods and achieving new state-of-the-art results on six popular V+L tasks. Our proposed framework improves the performance of UNITER-large and achieves a single-model accuracy of 74.87 on VQA and 65.7 on VCR.