Large-scale deep neural networks have achieved significant success in various cognitive tasks but suffer from low inference speed and high energy cost, limiting their deployment on edge devices. Network pruning has emerged as an effective technique to reduce the size of deep neural networks without significant accuracy drop, but most existing methods are based on heuristics and lack theoretical guarantees. This paper proposes a new pruning method that achieves a provably better error rate than direct training with gradient descent. The method is applicable to non-over-parameterized networks and achieves an exponential decay of error with network size. The algorithm is simple and easy to implement, and empirical results demonstrate its effectiveness on various network structures and datasets.