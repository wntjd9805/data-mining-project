We present a paper on risk-sensitive reinforcement learning (RL) in which the agent aims to maximize a risk-sensitive objective function. We focus on RL with the exponential utility under episodic Markov decision processes (MDPs) with unknown transition kernels. We introduce two model-free algorithms, Risk-Sensitive Value Iteration (RSVI) and Risk-Sensitive Q-learning (RSQ), that address the challenges of the non-linear objective function and risk-aware exploration. We demonstrate that our algorithms implement risk-sensitive optimism for exploration and provide regret analysis over the entire spectrum of risk parameter Î². Additionally, we establish a fundamental tradeoff between risk sensitivity and sample efficiency in RL. Our work contributes to the understanding of risk-sensitive RL and provides provably efficient algorithms with regret bounds.