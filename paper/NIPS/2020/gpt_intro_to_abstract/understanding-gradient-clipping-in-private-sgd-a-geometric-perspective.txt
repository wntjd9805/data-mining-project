Many machine learning applications rely on datasets that potentially contain sensitive personal information. To protect privacy, these systems often train their models with differential privacy constraints. One popular training method, called differentially private stochastic gradient descent (DP-SGD), achieves privacy guarantees by adding noise to the gradients. However, there is a disparity between the theoretical analysis and practical implementation of DP-SGD, especially when using gradient clipping to bound the sensitivity of the gradients. Existing analyses quantify the bias introduced by clipping, but practical results show that DP-SGD remains effective even with a small clip threshold, highlighting a gap in the current theoretical understanding. This paper investigates the effects of gradient clipping on SGD and DP-SGD, providing a symmetricity-based analysis, theoretical and empirical evaluations of DP-SGD, and a gradient correction mechanism to mitigate clipping bias.