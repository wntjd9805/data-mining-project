This paper focuses on the problem of learning representations with general utility in machine learning. While most research in neural networks focuses on the ability to fit input-output relationships, it is difficult to analyze the dynamics of learning representations and their meaningfulness. The authors investigate linear autoencoders with different regularization schemes to recover the individual principal directions. They analyze the loss landscape and convergence speed of gradient-based optimization methods. Although previous methods show slow convergence to the optimal representation, the authors propose an update rule that explicitly accounts for rotation in the latent space, resulting in faster learning of the optimal representation. The contributions of the paper include characterizing stationary points, explaining slow convergence, deriving a deterministic variant of nested dropout, and proposing an update rule with faster convergence.