Language model pre-training has been successful in improving various natural language processing tasks. However, existing pre-training models, such as BERT, exhibit heavy computation redundancy due to the presence of attention heads that learn local dependencies. In this paper, we propose ConvBERT, a model that integrates convolution layers into self-attention to address local dependencies more efficiently. We introduce a mixed attention mechanism that combines self-attention and convolution, and develop a span-based dynamic convolution operation to generate adaptive convolution kernels. Additionally, we incorporate a bottleneck structure and grouped linear operator to reduce redundancy and parameter number. Experimental results show that ConvBERT achieves higher performance on the GLUE benchmark with less training cost and parameters compared to BERT and ELECTRA. Our contributions include the exploration of convolution for enhancing BERT efficiency, the introduction of span-based dynamic convolution, and the development of ConvBERT model with new architecture designs.