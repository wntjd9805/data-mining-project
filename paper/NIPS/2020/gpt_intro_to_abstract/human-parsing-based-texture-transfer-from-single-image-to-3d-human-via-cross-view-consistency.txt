Rebuilding 3D models of the human body from 2D images has numerous applications in virtual reality, movie making, clothes try-on, and synthetic data generation. While previous research has focused on estimating the pose and shape of the human body, there is a lack of work addressing the problem of texture generation. Existing methods rely on ground-truth 3D textures, which are costly to obtain, and struggle with predicting textures of invisible body parts. To tackle these challenges, we propose a human-parsing-based texture transfer model that learns cross-view consistency to generate textures of the 3D human body from a single image, without relying on 3D texture supervision. We use semantic parsing to reduce appearance variation and preserve pose information, and enforce cross-view consistency during training. Our model achieves state-of-the-art quality textures on both surveillance scene pedestrian images and fashion photos from the web, with three main contributions: the proposed texture transfer model, leveraging semantic parsing for input, and achieving high-quality textures.