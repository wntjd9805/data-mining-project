Bayesian optimization (BO) is a powerful approach for optimizing black-box functions by using a probabilistic surrogate model, typically a Gaussian process (GP), to estimate the objective function. Many applications allow for querying the objective function at different levels of fidelity, where low fidelity queries are cheap but inaccurate, while high fidelity queries are more accurate but costly. To reduce optimization cost, many multi-fidelity BO methods have been proposed to balance optimization progress and query cost. However, these methods often ignore complex correlations between function outputs at different fidelities. In this paper, we propose DNN-MFBO, a deep neural network based multi-fidelity Bayesian optimization that can capture complex relationships between fidelities and jointly estimate the objective function to improve optimization performance. We stack a set of neural networks, where each network models one fidelity, and propagate information throughout to estimate the relationships across fidelities. We develop a stochastic variational learning algorithm for efficient inference and tractable computation of the acquisition function. Experimental results demonstrate that DNN-MFBO optimizes the objective function more effectively with smaller query cost compared to state-of-the-art algorithms.