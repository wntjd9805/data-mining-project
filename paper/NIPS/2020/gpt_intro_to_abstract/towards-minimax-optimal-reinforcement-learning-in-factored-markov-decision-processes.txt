This paper introduces the concept of reinforcement learning (RL) where an agent interacts with an unknown environment in order to maximize its cumulative reward. The environment and agent's interactions are modeled as a Markov decision process (MDP). Specifically, the paper focuses on episodic MDPs with a fixed interaction horizon, where the agent observes the current state, takes an action, and receives a reward at each step. The agent's performance is measured by regret, which is the difference between the expected cumulative rewards and those achievable by an optimal policy. The paper aims to find an optimal RL algorithm that minimizes regret for episodic MDPs.