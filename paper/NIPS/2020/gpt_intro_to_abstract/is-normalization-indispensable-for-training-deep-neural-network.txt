Deep neural networks have greatly improved performance in various artificial intelligence applications, but training them effectively can be challenging due to issues such as vanishing or exploding gradients. Batch Normalization (BN) has been successful in mitigating this problem, and other variants like Layer Normalization and Instance Normalization have been proposed for different tasks. However, the theoretical understanding behind the effectiveness of normalization in neural networks is still limited. This paper explores the question of whether normalization is indispensable for training deep neural networks and presents a method to train networks without normalization layers while maintaining performance. The method is validated on various tasks, showing comparable or better performance compared to normalized models.