Semantic segmentation is an important task in computer vision that involves labeling each pixel in an image with a semantic label. Current approaches to semantic segmentation rely on dense mask annotations, which are time-consuming and costly to obtain. To address this issue, there has been growing interest in weakly-supervised segmentation and few-shot segmentation methods, which aim to train segmentation models using lower-quality annotations or a small number of annotated samples. However, these methods still require mask annotations to some extent. In this paper, we propose a new learning paradigm called Generalized Zero-Shot Semantic Segmentation (GZS3), which focuses on recognizing never-seen categories with zero training examples. We introduce a generative method that utilizes semantic word embeddings to generate visual features for unseen categories, which are then used to train classifiers for segmentation. However, the existing approach lacks constraints on the quality of generated visual features for unseen categories. To address this, we present the Consistent Structural Relation Learning (CSRL) framework, which leverages the inter-class relationship between seen and unseen categories to improve the generator. Our approach generates visual features for both seen and unseen categories simultaneously, and incorporates relational constraints to ensure consistency with semantic-based counterparts. Experimental results demonstrate that CSRL outperforms state-of-the-art methods on two GZS3 benchmarks, achieving significant improvements in segmentation accuracy.