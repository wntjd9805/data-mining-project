Off-policy Actor-Critic (OffP-AC) methods are widely used in deep reinforcement learning research due to their greater efficiency compared to on-policy alternatives. However, existing off-policy methods rely on hand-crafted objective functions. In this paper, we propose a meta-critic network that is explicitly trained to accelerate the learning process in OffP-AC methods. The meta-critic provides an additional loss to guide the actor's learning and is trained to generate maximum learning progress. Our approach improves sample efficiency and can be learned online within a single task, in contrast to the current meta-learning paradigm. We evaluate our method on several continuous control benchmarks and demonstrate its effectiveness in enhancing OffP-AC algorithms.