In this paper, we explore the concept of view-invariant representations in multiview learning. We begin by discussing the curse of Funes the Memorious, a character who struggles with perceiving the world differently in every new viewing angle. Unlike Funes, most of us build mental representations that discard nuisances like time of day and viewing angle. We then delve into contrastive multiview learning, where two views of the same scene are brought together in representation space, and two views of different scenes are pushed apart. However, the crucial question of which viewing conditions we should be invariant to remains unanswered. We argue that representations should have enough invariance to handle inconsequential variations but not so much as to discard information needed for downstream tasks. We investigate this issue by demonstrating that the choice of views depends on the specific task and by empirically finding a sweet spot of mutual information between views that leads to optimal downstream performance. We introduce an "InfoMin principle," which states that good views are those that share the minimal information necessary to perform well on the task. We also propose a semi-supervised method for learning effective views when the downstream task is known and show that seeking stronger data augmentation can further reduce mutual information and improve representation quality. Finally, we apply our findings to achieve state-of-the-art accuracy on the ImageNet linear readout benchmark.