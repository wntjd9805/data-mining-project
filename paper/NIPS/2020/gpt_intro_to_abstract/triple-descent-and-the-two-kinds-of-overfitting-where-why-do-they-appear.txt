Deep neural networks have achieved remarkable generalization abilities, but their behavior deviates from classical learning theory predictions. Recent research has shown that increasing the number of parameters and training examples results in double descent curves, where the generalization error initially decreases, then peaks, and then decreases again. In this paper, we investigate the phenomenon of double descent and its relationship to linear and nonlinear peaks. We analyze the (P, N) phase space and demonstrate that the linear and nonlinear peaks are distinct phenomena. We provide a theoretical analysis and reveal that the linear peak is caused by overfitting the noise in the labels, while the nonlinear peak is influenced by the initialization of random feature vectors. We also discuss the effects of activation functions and regularization on the peaks. Ultimately, our findings shed light on the behavior of deep neural networks.