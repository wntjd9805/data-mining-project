Visual attributes are an important aspect of zero-shot learning, allowing for semantic knowledge transfer between known and unknown classes. While most zero-shot learning methods focus on learning compatibility between image representations and attributes, the ability of image representations to localize and associate specific image regions with visual attributes, referred to as locality, remains relatively unexplored. This paper introduces a weakly supervised representation learning framework that improves the locality of image representations by localizing and decorrelating visual attributes. The proposed attribute prototype network (APN) regresses and decorrelates attributes from intermediate-layer features, resulting in local features that encode semantic visual attributes. Experimental results show consistent improvement over state-of-the-art methods on benchmark datasets, demonstrating the effectiveness of the APN model for zero-shot learning. Additionally, the model is able to accurately localize bird parts without using any part annotations during training, outperforming a recent weakly supervised method.