Reinforcement learning (RL) is an approach to learning control in unknown systems through trial and error. However, RL applications often face challenges when dealing with large observation spaces, such as images or texts, which make it difficult to fully explore the environment. Function approximation has been used to generalize essential quantities for policy improvement, but it does not solve the exploration problem. To address this issue, various strategies have been developed, including the use of state abstraction techniques and unsupervised learning. While empirical studies have shown the effectiveness of these approaches, their theoretical justification is lacking. In this paper, we aim to explore the efficiency of exploration driven by unsupervised learning in general. We propose a new algorithmic framework that combines unsupervised learning with tabular RL algorithms and prove its efficiency under certain conditions. Experimental results demonstrate the superiority of our method compared to existing approaches. Our main challenge lies in the interdependency between the unsupervised learning oracle and the policy, which we tackle through an iterative process of collecting observations and retraining decoding functions. This allows us to obtain accurate state-action trajectories and achieve near-optimal performance with a polynomial number of samples.