In this paper, we address the problem of predicting the training time of a Deep Neural Network (DNN) before actually training it. This problem has received little attention due to the complex nature of the initial training dynamics of a randomly initialized DNN. However, in practical applications, it is common to start from a pre-trained model, which simplifies the analysis. Our main contribution is introducing the problem of predicting training time in realistic use cases, particularly considering the interaction between the target task and pre-training. We characterize the training dynamics of a pre-trained network and provide a computationally efficient procedure to estimate the expected profile of the loss curve over time. We use a linearized version of the DNN model and introduce a Stochastic Differential Equation (SDE) to approximate the behavior of SGD. We also show how to mitigate the memory requirement by using random projections and estimate training time using a small subset of samples. Overall, our contributions include qualitative and quantitative analyses, reducing cost estimation, estimating training time on a larger dataset, and testing the accuracy of our predictions on state-of-the-art models. Our method enables predicting training time with 20% error within 95% confidence, significantly faster than actual training.