In this paper, we propose a new class of sketching methods called surrogate sketches, which aim to debias local estimates of the Newton step in distributed second order optimization. We analyze the surrogate sketches using determinantal point processes and provide exact formulas for the bias of the estimates, enabling us to correct it. We also introduce a technique called scaled regularization, which reduces the bias of local Newton estimates for a range of sketching techniques. Empirical results demonstrate the effectiveness of our approach. We further present convergence guarantees for distributed Newton's method with surrogate sketching, showing a linear convergence rate and improved time complexity. Our findings contribute to the scalability and efficiency of second order optimization in a distributed or parallel setting.