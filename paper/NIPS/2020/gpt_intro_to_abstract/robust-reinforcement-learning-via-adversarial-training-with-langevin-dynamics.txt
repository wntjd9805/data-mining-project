Reinforcement learning (RL) has shown promise in achieving automated solutions with performance surpassing human capabilities in real-world tasks. However, the fragility of deep RL methods in practical deployments remains a challenge, particularly when there are differences between training and testing scenarios, leading to safety and security concerns. To address this, learning robust policies that can accommodate environmental shifts and mismatches in configurations and control actions is essential. One approach is to treat environmental changes as adversarial perturbations and formulate a two-player max-min problem, with a protagonist learning to fulfill task goals while being resilient to disruptions caused by an adversary. Two examples of this research direction are Robust Adversarial Reinforcement Learning (RARL) and Noisy Robust Markov Decision Process (NR-MDP). Despite empirical progress, training robust RL objectives remains a critical challenge due to the non-convex-concave nature of these objectives. Optimization methods often get stuck at non-equilibrium stationary points, and in some cases, even pure Nash Equilibria are not well-defined. In this paper, we propose a sampling perspective, inspired by mixed Nash Equilibrium, as a potential solution to training robust RL agents. We demonstrate the advantages of sampling algorithms over optimization methods through empirical experiments, showing progress towards optimum in stylized examples and consistently outperforming state-of-the-art methods in training robust RL agents. Moreover, our results highlight the inherent robustness of sampling algorithms compared to optimization methods in RL tasks.