Diagnosing and fixing model errors, also known as model debugging, is a significant challenge in machine learning. As automated systems with learned components are being tested in high-stakes settings, such as healthcare, the need for effective debugging tools becomes crucial. Explanations derived from trained models have been used as debugging tools, but little guidance exists on their effectiveness. This paper aims to address the question of which explanation methods are effective for different classes of model bugs. The authors categorize bugs in the supervised learning pipeline into three classes: data, model, and test-time contamination, and conduct empirical assessments to evaluate different feature attribution methods. Insights from the study show that certain attribution methods can identify specific bugs but struggle with others, and visual inspection alone may be misleading for debugging. Additionally, a human subject study reveals that end-users rely primarily on model predictions rather than attributions to identify defective models.