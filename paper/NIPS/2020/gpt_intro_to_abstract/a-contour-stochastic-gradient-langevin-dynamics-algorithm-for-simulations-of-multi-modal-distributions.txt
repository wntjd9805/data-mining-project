AI safety is a critical concern in the field of deep learning, and Markov chain Monte Carlo (MCMC) has shown promise as a solution for uncertainty quantification in deep neural network (DNN) models. However, traditional MCMC algorithms are not scalable to big datasets used in deep learning. Stochastic gradient Langevin dynamics (SGLD) was introduced to resolve this scalability issue, but existing SGMCMC algorithms based on SGLD often converge slowly. In this paper, we propose the contour stochastic gradient Langevin dynamics (CSGLD) algorithm, which extends the idea of flat histogram algorithms to SGMCMC. We justify the stability condition for CSGLD, establish its ergodicity, and provide empirical evidence of its performance on various datasets.