Graph neural networks (GNNs) have shown promising results in analyzing graph structured-data in various fields. However, the current design of GNNs faces the challenge of over-smoothing, which leads to underfitting of the model. Although multi-scale GNNs have resolved this issue to some extent, little is known about their theoretical workings. In this study, we propose an analysis of multi-scale GNNs using boosting theory and derive optimization and generalization guarantees. We also apply the proposed method to node prediction tasks and demonstrate its performance compared to state-of-the-art GNNs. The contributions of this study include the formulation of guarantees, the derivation of test error bounds, and the confirmation of empirical observations.