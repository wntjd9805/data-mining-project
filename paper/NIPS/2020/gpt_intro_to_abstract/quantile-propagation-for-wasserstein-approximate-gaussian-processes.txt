Gaussian process (GP) models are popular in machine learning due to their flexibility and ability to measure uncertainty. However, exact Bayesian inference for GP models is difficult, leading to the development of various approximate inference methods. This paper focuses on expectation propagation (EP), which approximates non-Gaussian likelihood terms with Gaussians. However, EP can over-estimate posterior variances, especially for short-tailed distributions. To address this issue, the paper introduces an alternative approach that minimizes a specific class of Wasserstein distances, called L2 WD. The proposed method, called quantile propagation (QP), estimates a fully coupled Gaussian posterior by iteratively minimizing local divergences between marginal distributions. The paper shows that QP has lower variance estimates compared to EP and satisfies the locality property, resulting in efficient computation. Experimental results on real-world datasets demonstrate the effectiveness of QP in terms of predictive accuracy and uncertainty estimation.