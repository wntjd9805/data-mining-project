Sequence modelling is a crucial task in machine learning, with applications ranging from neural machine translation to image captioning. Transformers, known for their ability to handle long sequences without the vanishing-gradient problem, have been widely used for these tasks. However, the use of self-attention in transformers results in quadratic computational and memory requirements, restricting their applicability to long sequences. To address this limitation, previous work has focused on improving the asymptotic complexity of self-attention or developing techniques to expand the sequence length that transformers can handle. In this paper, we propose clustered attention as a fast approximation of self-attention. By clustering similar queries and computing attention once per cluster, we achieve linear complexity. We also demonstrate the effectiveness of our approach on automatic speech recognition datasets and show that our approximation can be applied to a pretrained BERT model with minimal performance loss.