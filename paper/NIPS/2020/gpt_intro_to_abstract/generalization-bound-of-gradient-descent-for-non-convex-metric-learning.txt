Metric learning is a technique used to automatically learn an optimal distance metric for machine learning algorithms. The Mahalanobis distance is a widely studied metric that is learned through an optimization problem. Various loss functions and regularizations have been designed to enhance the discriminability and achieve good generalization and robustness. While the data structure and model complexity are important in metric learning, the choice of optimization algorithms and parameters is equally crucial. This paper aims to explore the effect of the gradient descent algorithm on metric learning methods and provides a new theoretical route to understanding its influence. The paper establishes a generalization bound, showing that early stopping, smooth classifier, and smooth loss function significantly impact the generalization error. The proposed theoretical techniques can also be applied to study the generalization ability of classification algorithms with non-convex objectives. The contributions of the paper include the establishment of a generalization PAC bound for parametric hypothesis classes, a decomposition theorem to facilitate the derivation of the bound, and the derivation of a generalization PAC bound for classifiers learned with the gradient descent algorithm. Additionally, the paper proposes a novel metric learning method called SMILE (Smooth Metric and Representative Instance Learning), which simultaneously learns the distance metric and representative instances for testing. SMILE adopts a Lipschitz smooth classifier and loss function, optimized through gradient descent with an early stopping mechanism, and shows competitive performance in evaluations on 12 datasets.