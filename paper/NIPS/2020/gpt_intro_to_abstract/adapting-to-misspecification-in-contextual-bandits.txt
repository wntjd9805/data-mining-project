The contextual bandit problem is a widely applicable extension of the standard multi-armed bandit problem in various fields, including health services, online advertising, and recommendation systems. In this problem, the learner must select an action based on observed features and action sets, aiming to achieve a cumulative reward close to the best hypothesis. Previous approaches have reduced the contextual bandit problem to supervised learning tasks, but open questions remain regarding action spaces with many or infinite actions and the handling of misspecification without prior knowledge. In this paper, we propose a generalized approach to address these questions, adapting to unknown misspecification using a bandit model selection procedure. Our results demonstrate effectiveness and optimality, with an efficient algorithm that can be applied to linear contextual bandits. Additionally, we introduce a new conceptual view of the action selection scheme and a generalization of the CORRAL algorithm for combining bandit algorithms.