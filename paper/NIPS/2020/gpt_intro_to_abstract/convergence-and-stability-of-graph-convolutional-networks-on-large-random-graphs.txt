Graph Convolutional Networks (GCNs) are deep architectures inspired by Convolutional Neural Networks (CNNs) and designed for graphs. They have been successfully applied in various fields, such as node clustering, semi-supervised learning, and graph regression. Despite their effectiveness in identifying large-scale structures in non-isomorphic graphs, the impact of changes in graph structure on GCN predictions remains unclear. This paper aims to analyze the convergence and stability properties of GCNs on large random graphs. It introduces a continuous counterpart to discrete GCNs and studies invariance and equivariance to graph isomorphism. The paper provides non-asymptotic convergence results for relatively sparse random graphs without assuming smoothness or boundedness of the similarity kernel. Additionally, it analyzes the stability of GCNs to small deformations of the underlying random graph model by characterizing model deformations and establishing a relationship between discrete and continuous representations.