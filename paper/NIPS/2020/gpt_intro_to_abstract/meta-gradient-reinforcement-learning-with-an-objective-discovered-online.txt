Recent advances in supervised and unsupervised learning have focused on the transition from handcrafted expert features to deep representations learned through gradient descent on an objective function. Reinforcement learning (RL) has also embraced this transition, replacing handcrafted features with handcrafted objectives. However, selecting the right objective is crucial as RL agents lack access to a differentiable performance metric. In this paper, we propose an algorithm that learns its own objective and, consequently, its own deep RL algorithm solely through experience. By parameterizing the objective function using a rich function approximator and employing meta-gradient learning, our algorithm maximizes a naive outer loss function. Unlike previous meta-learning approaches, our online meta-gradient algorithm learns on a single task during a single training lifetime, providing advantages such as applicability to any RL environment and the ability to adapt the objective function as learning progresses. We demonstrate the effectiveness of our approach on toy problems and Atari games, surpassing a strong actor-critic baseline on the benchmark.