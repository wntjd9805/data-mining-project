Probabilistic inference problems are challenging as they often involve NP-complete decision problems with no efficient, exact solutions. In response, researchers have developed variational and sampling based methods to approximate these problems. Belief Propagation (BP) has been successful in providing principled approximations, but its bounds are not always tight. This paper introduces belief propagation neural networks (BPNNs), a flexible neural architecture that estimates the partition function of a factor graph by leveraging the theoretical analysis behind BP. BPNNs are composed of iterative layers (BPNN-D) and an optional Bethe free energy layer (BPNN-B) that maintain the symmetries of BP under factor graph isomorphisms. Experimental results demonstrate that BPNN-D outperforms standard BP on Ising models and community detection problems, and also performs well on approximate model counting tasks compared to handcrafted approximate model counters.