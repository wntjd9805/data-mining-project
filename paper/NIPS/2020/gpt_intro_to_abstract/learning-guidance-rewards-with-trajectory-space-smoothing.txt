This paper addresses the challenges faced by deep reinforcement learning (RL) agents in maximizing long-term rewards in real-world sequential decision-making problems with sparse or delayed rewards. The authors introduce a new algorithm for learning guidance rewards that can easily integrate into existing RL algorithms, providing dense supervision and facilitating value estimation and credit assignment. By curating trajectory returns and redistributing them uniformly to each state-action pair, the proposed approach improves sample efficiency while being invariant to delayed rewards. The effectiveness of the approach is demonstrated through experiments using a variety of RL algorithms on different types of tasks.