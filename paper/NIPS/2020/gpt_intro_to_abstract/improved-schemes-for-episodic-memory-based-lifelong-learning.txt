This paper introduces the problem of catastrophic forgetting in artificial general intelligence (AGI) and the field of lifelong learning. It discusses the dilemma of balancing performance on old tasks and new tasks, and presents episodic memory based methods, such as Gradient Episodic Memory (GEM) and Averaged Gradient Episodic Memory (A-GEM), as solutions. The authors propose a unified view of these methods and present two schemes, MEGA-I and MEGA-II, which achieve better performance and adaptability. The proposed MEGA-II outperforms MEGA-I in limited examples per task scenarios. Experimental results demonstrate that MEGA-I and MEGA-II advance the state-of-the-art performance in lifelong learning benchmarks.