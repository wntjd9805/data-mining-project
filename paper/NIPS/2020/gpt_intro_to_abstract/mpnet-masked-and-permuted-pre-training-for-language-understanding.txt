In this paper, we propose a new pre-training method called masked and permuted language modeling (MPNet) that combines the advantages of masked language modeling (MLM) and permuted language modeling (PLM) to improve the accuracy of NLP tasks. MPNet addresses the issues in both MLM and PLM by considering the dependency among predicted tokens through permuted language modeling and utilizing the position information of all tokens. Experimental results show that MPNet outperforms MLM, PLM, BERT, XLNet, and RoBERTa on various benchmark tasks, highlighting its potential for language understanding.