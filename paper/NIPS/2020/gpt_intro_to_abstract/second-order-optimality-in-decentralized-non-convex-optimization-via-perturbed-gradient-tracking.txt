Recently, there has been an exponential growth in the amount of distributed data and the need for machine learning frameworks in various domains. This paper focuses on solving nonconvex decentralized optimization problems, where a group of agents collaborate to minimize their aggregate loss function. The objective is to find a stationary point of the optimization problem, with an emphasis on achieving second-order optimality. The authors introduce the Perturbed Decentralized Gradient Tracking (PDGT) algorithm, which consists of two steps: a local decentralized gradient tracking scheme and a perturbed gradient tracking scheme. The communication cost for achieving a second-order stationary point is analyzed, providing the first non-asymptotic guarantee in decentralized optimization under standard smoothness assumptions.