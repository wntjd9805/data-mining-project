Deep learning has become a dominant approach in various fields due to its high accuracy in solving problems such as computer vision and natural language processing. However, average users often rely on big data companies to utilize their own data for deep learning models, which can be risky in terms of privacy and legal constraints. To address this, recent works propose cryptographic schemes for privacy-preserving training of deep neural networks (DNNs). This paper introduces a technique called Glyph, which enables fast and accurate training over encrypted data using the TFHE cryptosystem. Glyph incorporates a cryptosystem switching technique to optimize the performance of different layers in DNNs, resulting in improved speed and reduced training latency compared to prior FHE-based techniques. Transfer learning is also applied to further enhance the accuracy of Glyph, making it a state-of-the-art solution for privacy-preserving DNN training on encrypted datasets.