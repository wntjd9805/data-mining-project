Robotic object grasping is a challenging task in robot systems due to imprecise sensing, planning, and actuation, as well as the absence of knowledge about object physical properties. Recent studies have shown that deep learning on annotated datasets of robotic grasp can achieve robustness and generalization. Synthetic data methods based on object CAD models and rendered images have shown promise in generating a large number of grasp annotations. Although there is a risk of domain discrepancy, deep learning models trained on synthetic datasets perform well on real-world grasp testings. In this paper, we focus on studying deep learning optimal grasp configurations using synthetic images. We specifically examine grasping with a parallel-jaw gripper, which has a 6-degree of freedom parameterization. We propose a novel end-to-end solution called Grasp Proposal Network (GPNet) to predict a diverse set of 6-DOF grasps for an unseen object. GPNet utilizes a grasp proposal module that defines grasp centers at regular 3D grid corners and includes headers for antipodal validity, grasp prediction, and grasp confidence scoring. We contribute a synthetic dataset of 6-DOF object grasps and evaluate GPNet through rule-based criteria, simulation tests, and real-world tests, demonstrating its advantages over existing methods.