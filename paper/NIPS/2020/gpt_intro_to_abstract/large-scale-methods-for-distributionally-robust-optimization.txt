The growing use of machine learning in high-stakes decision-making requires the development of reliable models that perform well across different subpopulations and environments. Distributionally robust optimization (DRO) has emerged as a potential solution to this challenge, with interest from both the machine learning and operations research communities. However, the lack of scalable optimization methods has hindered the adoption of DRO in common machine learning practice. In this paper, we propose methods for solving the DRO problem that have complexity independent of sample size and dimension, and optimal dependence on the uncertainty set size. We provide theoretical guarantees for the performance of our methods and demonstrate their effectiveness through experiments on digit classification tasks and ImageNet.