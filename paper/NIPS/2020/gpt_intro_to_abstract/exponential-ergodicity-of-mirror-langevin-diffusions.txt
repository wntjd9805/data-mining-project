Sampling from a target distribution is an important task in statistics and machine learning, with applications in Bayesian inference and deep generative models. Markov Chain Monte Carlo (MCMC) has become a fundamental sampling paradigm due to its mathematical grounding in the theory of Markov processes and its versatility. While traditional analyses focus on asymptotic results, this paper examines finite-time results that better reflect the practical performance of MCMC for high-dimensional problems. The paper explores the connection between sampling and optimization, particularly in the context of log-concave probability measures, and introduces mirror-Langevin diffusions as a class of stochastic processes. Theoretical analysis is conducted using the chi-squared divergence as a surrogate for measuring the progress of these schemes, highlighting the role of mirror Poincar√© inequalities in convergence. The results also specialize to the Newton-Langevin diffusion, which demonstrates robust convergence properties in sampling. This invariance property is advantageous for approximately sampling from a convex body, such as in the case of uniform distribution approximation. The paper concludes by mentioning the use of continuous-time diffusions, leaving further investigation into discretization error bounds for future work.