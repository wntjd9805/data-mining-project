Deep reinforcement learning (RL) has achieved remarkable success in various tasks, including video game playing and robotic control. However, the lengthy training time poses a challenge in scaling deep RL to more complex tasks. To address this, RL frameworks strive for high throughput and sample efficiency through synchronous and asynchronous parallel actor-learner methods. Synchronous methods like synchronous advantage actor critic (A2C) provide data efficiency and training stability but suffer from idle time when step times vary. Asynchronous methods like GA3C and IMPALA achieve high throughput but experience a stale-policy issue due to asynchronous learning and data collection. In this paper, we propose High-Throughput Synchronous RL (HTS-RL), a technique that combines high throughput and high training stability. We demonstrate the effectiveness of HTS-RL on Atari games and the Google Research Football environment, achieving speedups compared to existing methods while maintaining the advantages of synchronous RL. Code is available at [link].