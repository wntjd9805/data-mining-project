Deep learning models have shown remarkable success in achieving human-level performances on large-scale labeled data. However, these models face challenges in generalization when dealing with new classes with limited labeled instances. In contrast, humans can quickly learn new tasks from a few instances by leveraging context and prior knowledge. The few-shot learning paradigm aims to bridge this gap and has garnered substantial research interest. In this paradigm, a model is first trained on labeled data with base classes, and then its generalization ability is evaluated on few-shot tasks composed of unlabeled samples from novel classes. Existing approaches in the field are primarily based on the "learning to learn" or meta-learning paradigm, which simulates the test-time scenario through episodic training on balanced tasks. This includes methods such as prototypical networks, matching networks, MAML, and LSTM meta-learner. The field of meta-learning has seen a significant number of follow-up works in recent years.