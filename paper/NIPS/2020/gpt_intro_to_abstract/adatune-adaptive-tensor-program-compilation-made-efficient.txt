The introduction discusses the need for fast and automated compilation frameworks to optimize the performance of Deep Neural Network (DNN) models. It highlights the development of neural network compilers, such as TVM, which has shown superior performance improvements using a technique called Learning-to-Compile (AutoTVM). However, these approaches suffer from long optimization times, hindering the practical utility of compiler-based solutions. To address this, the authors propose AdaTune, a method that achieves similar optimization quality but with shorter optimization time. They introduce an adaptive evaluator, surrogate modeling with uncertainty quantification, and a contextual optimizer to improve the effectiveness of transformation space searching. Extensive experiments demonstrate that AdaTune consistently outperforms previous methods, optimizing DNN models faster while obtaining higher GFLOPS. The source code will be made publicly accessible for further research.