In many sequential decision making settings, the agent faces the challenge of partial observability, where it lacks complete information about the underlying state of the system. This poses difficulties for reinforcement learning and planning, as the agent needs to maintain memory and reason about beliefs regarding the system state while exploring to gather information about the environment. This issue arises in various domains such as robotics, games, and medical diagnosis. However, learning and planning in partially observable environments are generally considered computationally and statistically intractable. This paper focuses on developing efficient algorithms for reinforcement learning in partially observable Markov decision processes (POMDPs), a standard formulation for such settings. Previous approaches have considered spectral methods and tensor decompositions but do not address the challenges of strategic exploration and general reinforcement learning problems. The contributions of this work include new sample-efficient algorithms for learning in finite POMDPs, particularly in the undercomplete regime where the number of observations exceeds the number of latent states. The proposed algorithm, OOM-UCB, achieves near-optimal policies with polynomial scaling in relevant parameters. Additionally, the paper tackles the case of deterministic latent dynamics, offering both computationally and statistically efficient algorithms. The authors provide motivation for their assumptions through lower bounds and emphasize that their algorithms address exploration and partial observability in an efficiently provable manner.