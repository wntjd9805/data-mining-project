In recent years, deep networks have demonstrated promising results in various computer vision tasks. As the number of applications and visual concepts increase, annotating large-scale datasets becomes a costly and time-consuming task. To address this challenge, federated datasets have been introduced, where a single dataset is formed by combining smaller constituent datasets for each category. This approach reduces annotation costs and allows for faster addition of new labels to the dataset. However, traditional methods for training deep networks on partially annotated datasets ignore un-annotated labels, resulting in unused training signal. In this paper, we propose a simple baseline that considers all un-annotated labels as negative and explore relationships among images and labels to derive supervisory signal from these un-annotated categories. We introduce a "soft" manner of considering un-annotated labels using knowledge distillation and evaluate the performance of our approach on challenging datasets.