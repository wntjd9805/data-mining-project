Probabilistic text generation is a widely-used technique in natural language processing, with applications in various fields such as machine translation, document summarization, data-to-text, and image captioning. Most state-of-the-art approaches rely on autoregressive models, where the probability of each word depends on all previous words. However, this sequential decoding process can be time-consuming, prompting the exploration of alternative parallel generation models. One class of such models assumes that each word's output probability is independent of other words, but this assumption often leads to noticeable artifacts like repetitions. In this paper, we propose a cascaded decoding approach with a Markov transformer architecture, which combines autoregressive dependencies and parallel generation. Our experiments on machine translation datasets demonstrate that our approach achieves a competitive speed/accuracy tradeoff compared to existing methods. The code for reproducing our results is available at a given GitHub repository.