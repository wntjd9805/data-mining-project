Feature selection is an important task in machine learning, as it helps to improve learning performance and understand learning algorithms. Traditional feature selection algorithms find a global set of features for the entire dataset, but there is a need for instance-wise feature selection that considers the importance of features on a case-by-case basis. In addition, it is also crucial to consider the interaction among features and identify redundant features. This paper introduces instance-wise feature grouping, a method that learns the feature group structure and identifies the importance of feature groups for prediction. The authors propose a novel method called group Interpreter (gI) and define the concept of redundancy in this setting. They also provide a practical algorithm to approximate mutual information and learn the most important feature groups on a sample-by-sample basis. The approach is evaluated on both synthetic and real data, demonstrating its effectiveness. The source code is publicly available for further use and exploration.