This paper focuses on the integration of audio and visual perception in computer vision. The co-occurrence of audio and video has been studied to learn representations in a self-supervised manner. However, current approaches do not consider the spatial cues of audio-visual signals, leading to difficulties in discriminating visual concepts that often co-occur. To address this issue, the authors propose a new approach that leverages 360-degree video data and spatial audio to learn representations through audio-visual spatial alignment. They collected a large 360-degree video dataset and designed a pretext task for contrastive learning. The authors also enhance the learned representations by using a curriculum learning strategy and a transformer network. The efficacy of this approach is evaluated on various audio and visual downstream tasks.