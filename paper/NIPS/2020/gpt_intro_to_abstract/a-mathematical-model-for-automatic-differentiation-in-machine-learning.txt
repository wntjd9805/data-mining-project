Optimization algorithms based on backpropagation oracles, as well as automatic or algorithmic differentiation (AD), are widely used in training modern learning architectures. However, for nonsmooth, nonconvex losses, AD does not have a stable theory, leading to spurious behaviors. In this paper, we present a simple mathematical framework to address this issue. We introduce the concept of selection derivatives, which provide an operational calculus for nonsmooth AD. This framework allows us to formalize the relationships between functions, algorithmic differentiation, and critical points, which are different from traditional notions. We also show that randomly initialized mini-batch stochastic gradient methods do not lead to artificial critical points. Overall, our findings suggest that the practical impact of AD's unpredictability in nonsmooth contexts is negligible in the context of common machine learning usage.