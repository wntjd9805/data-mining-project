Reinforcement learning (RL) is a concept in artificial intelligence that addresses the challenge of designing agents that learn while interacting with the environment. There is a need for agents that can quickly adapt and reason in changing environments, but the question of how to learn such models is still unresolved. In this paper, we propose a new approach to model-based RL that takes into account the future use of the model during its construction. We introduce the principle of value equivalence, which captures the requirements of the model for value-based planning. By considering a set of policies and functions, we analyze how the set of feasible solutions is affected and show that as the set is augmented, the class of value equivalent models narrows down to the accurate representation of the environment. We also discuss cases where the model can be tailored based on limited representational capacity or the need to predict a subset of value functions. Through experiments, we demonstrate the benefits of value-equivalent model learning and argue that this principle underlies recent empirical successes in RL.