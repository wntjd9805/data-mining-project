Modern neural networks (NNs) are growing in size and complexity, with models now containing billions of trainable parameters. Despite the large capacity of these models, they still generalize well when trained on real data, contradicting classical generalization theory. A phenomenon known as double descent, in which the test error behaves as predicted by classical theory but then decreases again, has been observed but lacks a concrete explanation. This paper aims to provide a theoretical explanation for double descent by analyzing random feature kernel regression, a model that exhibits all the relevant features. The paper also introduces a symmetric decomposition of variance to better understand the underlying factors contributing to double descent. The improved understanding of bias and variance in machine learning models could lead to performance improvements and inform decisions regarding ensemble techniques.