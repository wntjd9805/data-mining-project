Reinforcement learning (RL) is a field that focuses on sequential decision making problems, where an agent aims to maximize the cumulative reward it receives in an environment. Policy gradient methods are popular algorithms for RL, particularly in environments with continuous state and action spaces. These methods often use an actor-critic scheme, where an actor determines the control policy and is evaluated by a critic. However, the training of the critic and its role in improving the policy are still open questions. In this paper, we propose a new algorithm called Model-based Action-Gradient-Estimator Policy Optimization (MAGE) that explicitly trains the critic to provide accurate action-gradients for policy improvement. Our algorithm leverages a trained dynamics model and techniques inspired by double backpropagation to minimize the error on the action-value gradient. Empirical results on a continuous control benchmark demonstrate that MAGE is more sample-efficient than existing baselines. The rest of the paper is organized as follows: we provide background on deterministic policy gradients, introduce our algorithm and its theoretical motivation, present empirical results, discuss related work, and conclude.