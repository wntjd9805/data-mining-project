Federated learning (FL) is a learning paradigm that trains a centralized model from a large number of devices/users without accessing their local data. A commonly used approach is to aggregate individual loss functions and solve an optimization problem in a distributed manner. However, FL faces three key challenges: communication efficiency, low participation of devices, and heterogeneity of local data shards. To address these challenges, the FedAvg algorithm was proposed, which averages locally evolving models. Although FedAvg improves communication efficiency, it does not tackle the issue of data heterogeneity, which affects the generalization capability of the central model. To personalize the global model to local distributions, a solution is to learn a model that minimizes the agnostic empirical loss. However, existing approaches either suffer from communication costs or still have limitations in adapting to newly joined devices. In this paper, we propose a Distributionally Robust Federated Averaging (DRFA) algorithm that is both distributionally robust and communication efficient. Our algorithm updates the global mixing parameter periodically, reducing communication rounds. We provide theoretical analysis and empirical results to demonstrate the effectiveness of our approach.