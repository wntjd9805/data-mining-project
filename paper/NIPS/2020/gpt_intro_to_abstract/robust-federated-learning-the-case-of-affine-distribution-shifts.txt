Federated learning is a new framework for training a centralized model using distributed data samples while preserving data privacy. However, it faces challenges such as communication bottlenecks and statistical heterogeneity of training data. To address these challenges, we propose FLRA, a federated learning framework that models the heterogeneity of training data in a device-dependent manner. FLRA formulates the robust learning task as a minimax optimization problem and introduces a novel method called FedRobust to efficiently solve it. We also provide theoretical guarantees and generalization error bounds for FLRA. Experimental results demonstrate the effectiveness and robustness of our proposed method compared to existing algorithms.