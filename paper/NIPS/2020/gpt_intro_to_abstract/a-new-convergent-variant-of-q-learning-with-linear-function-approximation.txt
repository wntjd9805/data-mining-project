This paper investigates the convergence of reinforcement learning with linear function approximation in control settings. It analyzes the convergence of Q-learning when combined with linear function approximation, considering the "deadly triad" of function approximation, bootstrapping, and off-policy learning. Previous results either restrict the approximation architecture or require a restrictive coupling between the architecture and the sampling distribution. The paper proposes a variation of Q-learning with linear function approximation that uses a two time-scale approach and introduces a convergence analysis with less stringent assumptions. Overall, the paper aims to provide an improved understanding of convergence in reinforcement learning with function approximation.