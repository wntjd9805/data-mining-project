This paper introduces a new method for uncovering and learning the structure in language representations at various scales, from word meaning to document topics, without relying on prior linguistic models. The authors propose using spectral analysis, commonly used in signal processing, to separate and control information at different timescales. By applying spectral filters to the activations of individual neurons in BERT, a deep NLP model, they are able to identify information that changes at different rates across the input. The paper presents a principled framework, spectral filtering techniques, and a new model component called the prism layer, which specializes neurons for specific scales of structure. Experimental results demonstrate that the proposed method improves the model's performance compared to BERT on various tasks at different scales.