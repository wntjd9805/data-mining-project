Deep learning (DL) and deep reinforcement learning (DRL) have achieved impressive results in various applications, but require the tuning of hyperparameters to adapt to specific tasks. This tuning process is time-consuming and often requires high computational resources. Bayesian optimization (BO) has shown promise in optimizing these hyperparameters, but current approaches ignore the information contained in the training iterations. In this paper, we propose a BO approach for tuning DL and DRL algorithms, considering the joint space of hyperparameters and training iterations. We introduce a data augmentation technique and a training curve compression method to efficiently learn across the joint space. Our algorithm outperforms existing baselines in finding the best hyperparameters in terms of wall-clock time, and we demonstrate its effectiveness on tuning DRL agents and convolutional neural networks. Our contributions include the algorithm for optimizing the learning curve, the approach for learning the compression curve, and the demonstration on DRL and convolutional neural networks tuning.