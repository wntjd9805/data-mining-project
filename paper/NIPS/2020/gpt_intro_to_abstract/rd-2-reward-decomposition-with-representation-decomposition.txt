This paper introduces a novel approach to reward decomposition in reinforcement learning tasks with multiple reward channels. Existing methods either require environment reset or only obtain sub-rewards on transition data generated by their own policy. In contrast, our proposed algorithm, RD2, leverages the relationship between sub-rewards and relevant features to decompose rewards for arbitrary state-action pairs without relying on policies. Additionally, RD2 is associated with a disentangled representation, allowing for self-explanatory and easily visualized reward decomposition. Experimental results on the Monster-Treasure environment and selected Atari Games demonstrate the effectiveness of RD2 in discovering meaningful reward decomposition and improving sample efficiency for deep RL algorithms.