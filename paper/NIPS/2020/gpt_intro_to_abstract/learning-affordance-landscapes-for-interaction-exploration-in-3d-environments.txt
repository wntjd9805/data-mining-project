The ability for embodied agents to interact with their environment is crucial for their operation in human spaces. While current agents are trained to perform specific interactions in a supervised manner, we propose the development of agents that can autonomously explore and discover interactable objects and relevant actions in novel 3D environments. This exploration for interaction problem presents a challenging search problem, as objects may be hidden and their interaction dynamics may not be straightforward. To address these challenges, we introduce a deep reinforcement learning approach that combines an exploration policy and an affordance model. Our experiments demonstrate the advantages of interaction exploration, as our agents can quickly seek out new objects and improve success rates on various tasks with fewer training samples.