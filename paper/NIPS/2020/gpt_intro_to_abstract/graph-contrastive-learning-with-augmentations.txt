Graph neural networks (GNNs) have gained popularity for analyzing graph-structured data, but there has been limited exploration of pre-training techniques for GNNs. This is despite the scarcity of task-specific labels for graph datasets, such as those in biology and chemistry. In this paper, we argue for the necessity of GNN pre-training and propose a novel graph contrastive learning framework (GraphCL) that utilizes data augmentations to learn representations invariant to specialized perturbations in diverse graph-structured data. We demonstrate that GraphCL performs mutual information maximization and show its connection to other contrastive learning methods. Through experiments, we validate the effectiveness of GraphCL in semi-supervised learning, unsupervised representation learning, transfer learning, and robustness against adversarial attacks.