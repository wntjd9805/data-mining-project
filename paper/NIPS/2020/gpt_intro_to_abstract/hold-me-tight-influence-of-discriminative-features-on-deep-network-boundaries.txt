The decision boundary of a classifier, which partitions the input space into labeled regions, plays a crucial role in understanding how the classifier functions. Surprisingly, even deep neural networks, which are successful in their tasks, are highly vulnerable to imperceptible perturbations, indicating that their decision boundaries are alarmingly close to any input sample. This behavior contradicts the common belief that a successful classifier should be invariant to non-discriminative information in the input data. However, it seems that these perturbations are actually discriminative features of the training set. Explaining the mechanisms behind the construction of decision boundaries in deep neural networks is key to understanding adversarial training dynamics. By slightly perturbing the training samples during optimization, the geometry of these classifiers can be completely transformed. This paper proposes a novel approach to investigate the relationship between the distance of samples to the decision boundary and the discriminative features used by the network. The authors develop a methodology to construct a local summary of the decision boundary by observing the margin along orthogonal directions. Through extensive evaluations on synthetic and real datasets, the authors provide empirical support for their findings and confirm the invariance of CNNs to non-discriminative features while also demonstrating their sensitivity to small perturbations in certain directions. The paper concludes by highlighting the implications of this research for future work on explainability and robustness in deep learning.