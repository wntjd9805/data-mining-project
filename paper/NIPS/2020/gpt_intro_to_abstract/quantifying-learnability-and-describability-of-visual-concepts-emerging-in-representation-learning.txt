Recent advancements in unsupervised and self-supervised learning have shown promising results in achieving competitive or even superior data representations compared to supervised learning. However, the challenge lies in the integration of these unsupervised representations with labeled datasets for meaningful data analysis tasks such as image classification. It remains uncertain whether unsupervised learning can develop a human-like understanding of complex data independently. In this paper, we address the problem of assessing the ability of unsupervised learning techniques to discover abstract, human-interpretable concepts. We define the evaluation process in a principled and comprehensive manner, considering the interpretability, describability, and automatic summarization of classes obtained through unsupervised learning algorithms. We propose a framework that utilizes human judgments to evaluate classes derived from self-supervised learning algorithms, measuring their learnability and distillability into natural language descriptions. We also introduce a metric to validate and compare automated descriptions before utilizing the human-based metric. Our work aims to minimize subjectivity in evaluating class interpretability and provide insights into the potential of unsupervised learning for developing human-like understanding of complex data.