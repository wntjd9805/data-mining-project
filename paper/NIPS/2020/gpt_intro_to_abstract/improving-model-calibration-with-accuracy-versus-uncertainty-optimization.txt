Probabilistic deep neural networks (DNNs) are crucial for reliable decision making in safety critical applications. While various methods have been proposed to capture uncertainty estimates from DNNs, accurately quantifying these uncertainties remains a challenge. Poor calibration in modern neural networks leads to overconfidence in incorrect predictions, and they also struggle to provide calibrated uncertainty between separated regions of observations. Additionally, DNN model predictions become unreliable under distributional shift and when encountering novel inputs. This paper introduces the accuracy versus uncertainty calibration (AvUC) loss function for probabilistic DNNs, which aims to derive models that are confident on accurate predictions and indicate higher uncertainty when likely to be inaccurate. The authors propose an optimization method that leverages the relationship between accuracy and uncertainty, investigate the effect of accounting for predictive uncertainty in the training objective function, and propose a post-hoc model calibration method. They empirically evaluate their methods on large-scale image classification tasks and demonstrate state-of-the-art model calibration under distributional shift.