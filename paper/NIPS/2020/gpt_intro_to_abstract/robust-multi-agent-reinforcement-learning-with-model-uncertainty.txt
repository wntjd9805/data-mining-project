Deep reinforcement learning (RL) has achieved significant success in various sequential decision-making problems, including robotics, autonomous driving, and game playing. These applications often involve multiple agents, leading to the need for multi-agent RL (MARL). In MARL, each agent aims to optimize its long-term return by interacting with the environment and other agents. However, agents trained in simulations may lack accurate knowledge of the actual model, resulting in a sim-to-real gap. While this uncertainty has been addressed in single-agent RL through robust Markov decision processes and adversarial RL, its exploration in the multi-agent RL setting is limited. In this paper, we propose a robust MARL framework that considers model uncertainty. We model the problem as a robust Markov game, where agents seek policies that are robust to uncertainty. We introduce the concept of an implicit "nature" player to account for uncertainty and develop robust MARL algorithms. Our contributions include the formulation of model uncertainty in MARL, the development of solution-finding algorithms, and validation through simulations in benchmark MARL environments. This work provides the first formulation and algorithms that address model uncertainties in MARL, backed by theoretical and empirical justifications.