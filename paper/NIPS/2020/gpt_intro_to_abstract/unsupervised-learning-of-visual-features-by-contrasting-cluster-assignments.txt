Unsupervised visual representation learning, or self-supervised learning, has gained momentum in computer vision as it aims to obtain features without manual annotations and has shown comparable performance to supervised pre-training. Many state-of-the-art methods in this area focus on instance discrimination, where each image in the dataset is treated as a separate class and its transformations are considered for feature learning. These methods typically rely on a combination of a contrastive loss and a set of image transformations. However, the computational cost of pairwise comparisons and the limitations of clustering-based methods have motivated our proposed approach. We introduce a simple "swapped" prediction problem that allows us to learn features by swapping assignments between different views of the same image. We also propose the multi-crop strategy, which increases the number of views without increasing computational requirements. Our method achieves significant improvements in performance on ImageNet and outperforms supervised pretraining on downstream tasks without finetuning the features.