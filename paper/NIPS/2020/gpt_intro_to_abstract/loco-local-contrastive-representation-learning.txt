Most deep learning algorithms utilize backpropagation for training in an end-to-end manner, where weight updates are computed based on gradients flowing from the top layer. However, this synchronization constraint seems unnatural in parallel distributed processing, and evidence suggests that weight synapse updates in the human brain occur through local learning. Local learning algorithms can reduce memory usage and enable model parallelism. The need for supervision from the top layer has been a concern, but recent success with self-supervised contrastive learning algorithms challenges this belief. We propose a new local learning algorithm called LoCo that bridges the gap between local algorithms and end-to-end approaches by sharing gradients between blocks. We evaluate LoCo on ImageNet and other downstream tasks, demonstrating its performance similar to costly end-to-end models. We also review related literature and describe our algorithm in detail.