In this work, we analyze the training dynamics of over-parameterized meta networks, which are networks that output the weights of other networks, often referred to as hypernetworks. The literature of hypernetworks is divided into two main categories: functional representation and hyper-parameter search. We focus on the first category and consider models in the regime of wide networks. We study the phenomenon of Neural Tangent Kernel (NTK) in fully connected networks and investigate whether a similar "wide" regime exists for hypernetworks. Our contributions include showing that infinitely wide hypernetworks can induce non-convex training dynamics, but when both the hypernetwork and the primary network have widths approaching infinity, the optimization dynamics of the hypernetwork simplifies. We verify our theory empirically and demonstrate the utility of the hyperkernel on functional representation tasks. We also make a technical contribution by deriving asymptotically tight bounds on high-order Taylor expansion terms in ReLU MLPs.