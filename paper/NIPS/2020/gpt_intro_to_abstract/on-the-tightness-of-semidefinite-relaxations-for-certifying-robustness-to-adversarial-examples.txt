Neural networks are known to be vulnerable to adversarial examples, which are small changes to the input that cause significant changes in the output. This vulnerability raises concerns for safety-critical applications such as self-driving cars and smart grids. Existing methods for analyzing robustness to adversarial examples involve solving a nonconvex optimization problem to find the smallest perturbation needed to result in an adversarial outcome. Certifying robustness involves proving lower bounds on the robustness margin. However, exact certification is often computationally expensive, while conservative certification can lead to overly cautious models. In this paper, we propose a geometric technique for analyzing the tightness of the SDP relaxation of the ReLU activation function and prove that the relaxation is generally exact for a single hidden layer. We also explain the looseness of the relaxation for multiple layers.