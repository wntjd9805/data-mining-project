Recent approaches in the field of Machine Learning (ML) model explanations can be categorized as heuristic or non-heuristic. Heuristic approaches lack formal guarantees on their results, while non-heuristic approaches provide formal guarantees at the expense of increased computational complexity. This paper focuses on the heuristic approaches for explaining ML models, particularly the use of model-agnostic linear approximations and the computation of feature-value pairs to explain predictions. Previous non-heuristic methods have focused on computing sets of feature-value pairs that are sufficient for the prediction, with two distinct definitions of explanations: PI-explanations and MC-explanations. PI-explanations represent a subset-minimal set of feature values that entail the outcome of the decision function, while MC-explanations are cardinality-minimal sets of equal-valued features. Previous work has mainly focused on Naive-Bayes Classifiers (NBCs) and other linear classifiers, proposing exponential time and space algorithms. This paper introduces a novel non-heuristic solution that computes PI-explanations for NBCs and other linear classifiers in polynomial time, providing a log-linear algorithm for computing the smallest size PI-explanation and a polynomial delay algorithm for enumerating PI-explanations. The paper also evaluates different explanation approaches for NBCs, including heuristic solutions computed by Anchor and SHAP. Additionally, the paper addresses the complexity of computing PI-explanations for linear classifiers, closing an open problem and contributing to the field of ML models with polynomial-time computable PI-explanations.