In machine learning, deep learning networks have achieved great success in tasks such as image classification, natural language understanding, and game playing. These networks are built with specific inductive biases, which guide them to prioritize certain solutions that align with their assumptions. However, the exact characterization and understanding of these biases are still lacking. In order to expand the application of deep learning to new domains, it is crucial to develop methodologies to systematically identify and manipulate the inductive bias of deep architectures. This paper focuses on a fundamental bias that arises in deep architectures, known as the directional inductive bias, and proposes a method to identify and characterize this bias using information available at initialization. The authors demonstrate the importance of directional inductive biases in classification tasks and show that they affect the selection of discriminative features in deep networks. These findings can have implications for future research in designing novel architectures with specific inductive biases.