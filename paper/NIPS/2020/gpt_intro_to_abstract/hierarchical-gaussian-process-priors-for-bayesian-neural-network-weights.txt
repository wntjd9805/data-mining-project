Bayesian neural networks (BNNs) are an emerging field that combines Bayesian inference and deep learning to provide flexible modeling power and calibrated predictive performance. However, there are unresolved questions about how to encourage desired behaviors in neural network functions and how to capture complex posterior correlations in large networks. This paper proposes a hierarchical Gaussian Process (GP) prior over weights to achieve a compact non-parametric representation of neural network weights. The use of product kernels allows for input-dependent priors, which are compared to global priors in terms of the network's ability to generalize. A structured variational inference approach is employed to amortize per-datapoint weight inference. The proposed model exhibits beneficial properties for generalization and uncertainty quantification. The paper is organized to review hierarchical modeling for BNNs, introduce global and local weight models, present efficient inference algorithms, conduct experiments, and review related work.