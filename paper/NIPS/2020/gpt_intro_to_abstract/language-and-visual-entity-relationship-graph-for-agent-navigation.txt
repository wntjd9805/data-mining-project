Vision-and-language navigation in the real-world is a crucial step in developing mobile agents that can perceive their environments and complete specific tasks based on human instructions. Various scenarios have been explored in research, including indoor and street view navigation with detailed instructions, communication-based visual navigation, and navigation for object localization and visual question answering. The R2R navigation task proposed by Anderson et al. has garnered significant interest, as it requires agents to navigate in unseen photo-realistic environments based solely on natural language instructions. Existing agents for this task primarily rely on sequence-to-sequence networks with grounding between vision and language, but they lack an explicit model of the relationship between visual features and agent orientation. Additionally, they often overlook the objects mentioned in the instructions, which provide important localization clues. In this paper, we introduce a novel language and visual entity relationship graph for vision-and-language navigation that explicitly models the relationships among scene, object, and direction. Our approach obtains state-of-the-art results on benchmark datasets and has the potential to benefit other vision-and-language tasks.