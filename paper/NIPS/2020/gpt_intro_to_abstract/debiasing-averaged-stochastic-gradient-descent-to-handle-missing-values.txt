Stochastic gradient algorithms are widely used in machine learning due to their computational efficiency and memory usage. Various variants of these algorithms have been developed, such as those using averaging or adaptive gradient methods. Averaging strategies have been particularly effective in stabilizing the algorithm behavior and reducing the impact of noise. However, when dealing with missing values in large scale data analysis, the usual results and minimization techniques for gradient-based methods cannot be directly applied. In this paper, we focus on a linear regression model and propose stochastic algorithms that can handle missing data and provide theoretical guarantees on excess risk.