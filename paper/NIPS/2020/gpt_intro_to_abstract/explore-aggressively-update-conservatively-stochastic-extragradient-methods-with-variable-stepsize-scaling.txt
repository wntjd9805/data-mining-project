A major obstacle in the training of generative adversarial networks (GANs) is the lack of an implementable, strongly convergent method based on stochastic gradients. This paper explores the failures of GAN training and proposes a new algorithm, called the double stepsize extragradient (DSEG) method, that overcomes the non-convergence issues observed in bilinear min-max problems. The authors analyze the convergence properties of the DSEG algorithm and demonstrate that it achieves O(1/t) convergence rates in both monotone and non-monotone saddle-point problems, even in the stochastic setting. These findings provide valuable insights into improving the training of GANs and other machine learning models.