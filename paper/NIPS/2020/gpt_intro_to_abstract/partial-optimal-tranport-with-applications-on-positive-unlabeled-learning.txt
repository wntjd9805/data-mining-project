Optimal transport (OT), specifically the Monge-Kantorovich or Wasserstein distances, has gained attention in the machine learning community for its ability to compare distributions. However, the traditional formulation of OT has limitations when dealing with heterogeneous distribution settings. This paper focuses on exact partial Wasserstein (partial-W) and Gromov-Wasserstein (partial-GW), which address the issue of different probability masses between distributions. The authors propose an approach that uses dummy points to compute sparse transport plans, leading to exact partial-W or -GW distances. The motivation behind tackling sparsity-preserving partial-OT problems is their suitability for applications like Positive-Unlabeled (PU) learning. The paper provides algorithms for solving these problems and demonstrates the advantage of partial-GW when dealing with distributions collected from distinct environments.