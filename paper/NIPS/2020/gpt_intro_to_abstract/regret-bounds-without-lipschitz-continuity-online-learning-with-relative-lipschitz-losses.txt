This paper focuses on online convex optimization (OCO) where a player selects a point from a convex set while an adversary chooses a convex function that penalizes the player's choice. OCO algorithms are commonly used in batch optimization problems due to their low computational cost. The performance measure used is regret, which quantifies the difference between the player's cost and a comparison point. Classical algorithms have shown regret bounds under certain conditions, but not all loss functions in applications satisfy these conditions. Recent work has proposed relaxed assumptions that allow classical algorithms to achieve similar convergence rates. This paper analyzes the performance of two OCO algorithms, FTRL and DS-OMD, and provides regret bounds for relative Lipschitz loss functions. It also extends these bounds for problems with composite loss functions and demonstrates the flexibility of these algorithms in deriving convergence rates for other OCO algorithms.