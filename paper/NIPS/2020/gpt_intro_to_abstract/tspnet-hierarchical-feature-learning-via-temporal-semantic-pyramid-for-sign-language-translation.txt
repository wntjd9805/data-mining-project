This paper introduces the problem of sign language translation (SLT) and explores existing approaches for bridging the gap between sign languages and natural languages. Two-staged and bootstrapping approaches are discussed, along with their respective advantages and limitations. The authors propose a new approach called Temporal Semantic Pyramid Network (TSPNet) to learn sign video representations that encode both spatial appearance and temporal dynamics. By organizing sign video segments hierarchically and leveraging semantic consistency, the proposed TSPNet improves the translation quality on a large sign language dataset. The results show a significant improvement in BLEU and ROUGE scores, reducing the reliance on expensive gloss annotations for SLT models.