In this paper, we propose a method called One-sample Guided Object Representation Disassembling (One-GORD) to learn disassembled object representation from unannotated images with the guidance of only one annotated sample for each object category. Our method consists of two modules: the augmented one-sample supervision module and the guided self-supervision module. We introduce self-supervised mechanisms, including fuzzy classification and dual swapping, to format self-supervised losses and guide the disassembling process. We also devise metrics to evaluate the modularity of representations and the integrity of images. Experimental results demonstrate the effectiveness of One-GORD in achieving promising performance. Our contribution lies in the development of One-GORD, which significantly reduces the annotation requirement while achieving encouraging results.