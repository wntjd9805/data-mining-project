The increasing use of black-box artificial intelligence (AI) technologies in various aspects of society has led to the development of explainability algorithms to understand their decisions. Two types of algorithms, local explanations and constructing interpretable global models, have emerged. Local explanations provide explanations for a specific data point, while global models aim to highlight non-trivial global behavior. This paper explores the trade-off between using accurate but less interpretable black-box models and simpler but more interpretable models. The authors propose a new algorithm that uses local explanations from a contrastive explanation method to generate boolean clauses, which can be used as features to train another simple model. The algorithm captures the sparse interactions between features required for explanations and provides a way to train transparent models that capture both local and global information.