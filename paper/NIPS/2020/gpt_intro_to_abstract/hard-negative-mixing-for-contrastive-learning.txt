Contrastive learning is a powerful self-supervised learning method for visual representations, which involves pushing positive image pairs closer and negative image embeddings further apart. Data augmentations and data mixing techniques have been shown to enhance the effectiveness of contrastive learning. However, the selection of hard negatives, which are crucial for better learning, has been overlooked. In this paper, we propose MoCHi, a method for generating hard negatives on-the-fly in the embedding space. We demonstrate that incorporating hard negative mixing improves transfer learning performance and utilization of the embedding space. Our method achieves competitive results in linear classification, object detection, and instance segmentation tasks, and shows faster learning of transferable representations.