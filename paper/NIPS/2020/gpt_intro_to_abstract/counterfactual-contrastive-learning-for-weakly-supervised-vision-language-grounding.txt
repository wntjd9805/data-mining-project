Vision-language grounding is a crucial problem in multi-modal understanding, with video grounding aiming to identify temporal boundaries and image grounding localizing specific regions. Weakly-Supervised Vision-Language Grounding (WSVLG) approaches using multiple instance learning (MIL) or reconstruction-based paradigms have been explored, but they have drawbacks such as relying on easily distinguishable negative samples and not directly optimizing visual-textual alignment scores. In this paper, we propose a Counterfactual Contrastive Learning (CCL) paradigm for WSVLG that constructs supervision signals from counterfactual results to optimize visual-textual alignment. Our CCL approach employs feature-level, interaction-level, and relation-level strategies for counterfactual transformations, and incorporates a ranking loss and consistency loss for contrastive training. Extensive experiments on large-scale datasets validate the effectiveness of our proposed method.