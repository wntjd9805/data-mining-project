Decision trees are flexible and intuitive decision models that are widely used in fields where interpretability is crucial. They are particularly useful when dealing with limited and unstructured data. However, decision trees are prone to overfitting, and existing solutions, such as cross-validation, can lead to increased running time and compromised generalization. In this paper, we propose an alternative approach based on generalization bounds. We highlight the success of this approach in previous work and aim to provide upper bounds on the VC dimension and growth function of decision trees with real-valued features. We introduce the concept of a realizable partition and define a partitioning function, closely related to the VC dimension. Through graph theory, we derive the exact expression of the VC dimension for decision stumps and extend the bound to general binary decision tree structures. Our results have practical implications, as demonstrated by the development of a pruning algorithm that outperforms CART on various datasets.