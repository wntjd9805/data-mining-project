This paper introduces the concept of the momentum cell in recurrent neural networks (RNNs) and proposes a new RNN architecture called MomentumRNN. The momentum cell integrates momentum, a method for accelerating gradient dynamics, into the RNN hidden state update. The advantages of MomentumRNN include alleviating the vanishing gradient problem, accelerating training and improving test accuracy, and being universally applicable to existing RNNs. The design principle of the momentum cell is principled with theoretical guarantees and can be generalized to other advanced momentum-based optimization methods.