This paper focuses on identifying the internal model of an agent by observing its actions. Unlike existing approaches that only aim to learn the reward function or the dynamics model, this paper introduces a method called Inverse Rational Control (IRC) that infers both. The task is formulated as a Partially Observable Markov Decision Process (POMDP) and is further reformulated as a belief Markov Decision Process (belief MDP) to model the cognitive process of decision-making based on the agent's beliefs. The paper proposes a Bayesian agent that learns optimal policies and value functions over a parameterized family of models. The contributions of this work include solving continuous nonlinear tasks, implementing Bayesian optimal control ensembles, and combining Maximum Likelihood Estimation (MLE) and Monte Carlo Expectation-Maximization (MCEM) to infer the reward function and internal model parameters. This study is the first to infer both the reward and internal model of an unknown agent with partially observable continuous nonlinear dynamics.