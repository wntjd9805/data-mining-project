Random projections have long been utilized for dimensionality reduction in various algorithmic and learning scenarios. In this study, we examine the performance of the iterative Hessian sketch (IHS) in the context of overdetermined least-squares problems. We compare the IHS versions using two classical subspace embeddings: random uniform projections and the subsampled randomized Hadamard transform (SRHT). Our goal is to design an optimal version of the IHS with SRHT and Haar embeddings, leveraging asymptotic random matrix theory. We provide trace formulas, characterize optimal step sizes and momentum parameters for both Haar and SRHT embeddings, and demonstrate the superiority of SRHT over Gaussian projections. We also analyze the computational complexity of our method and highlight the potential for future developments and extensions.