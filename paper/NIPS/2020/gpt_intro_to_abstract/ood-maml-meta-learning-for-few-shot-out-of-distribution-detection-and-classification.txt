In this paper, we address the problem of few-shot out-of-distribution (OOD) detection and classification in deep neural networks (DNNs). We propose a method called OOD-MAML that combines meta-learning and the synthesis of OOD examples to improve performance in detecting OOD samples from unseen classes while classifying samples from known classes. We use model-agnostic meta-learning (MAML) to learn a learning strategy that quickly adapts to new tasks and generate adversarial samples for OOD classes during training. OOD-MAML is trained to adapt quickly for OOD tasks with respect to a single class, enabling OOD detection and few-shot classification simultaneously without the need for re-training. The code for OOD-MAML is publicly available.