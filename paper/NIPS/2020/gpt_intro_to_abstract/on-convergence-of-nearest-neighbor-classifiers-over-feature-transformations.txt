The k-Nearest Neighbor (kNN) algorithms are widely used in pattern recognition due to their simplicity and valuable properties. However, most theoretical results on kNN are derived under the assumption that the algorithm is directly applied to the raw data, creating a gap between theory and practice. In this paper, we aim to understand the behavior of kNN over feature transformations and contribute to closing this gap. We propose that the behavior of kNN over a transformation relies on two factors: safety, which measures how well the posterior can be recovered in the original space from the feature space, and smoothness, which evaluates the ease of recovering the posterior. We provide a theoretical analysis and show that the convergence rate of kNN over a feature transformation can be bounded by a function involving the safety and smoothness. Finally, we validate our findings through extensive experiments on real-world datasets and various feature transformations.