Cross-validation (CV) is a widely used method for estimating the test error of a prediction rule. It involves partitioning a dataset into validation sets, fitting a prediction rule with each validation set held out, and averaging the error estimates. While CV provides an unbiased estimate of the test error with lower variance, it lacks guarantees for high-stakes applications. Test error confidence intervals (CIs) based on CV can be misleading due to the dependence among error estimates. Similarly, hypothesis tests for comparing prediction rules can produce incorrect conclusions. To address these issues, this paper characterizes the asymptotic distribution of CV error and develops consistent estimates of its variance. These results enable the construction of practical, asymptotically-exact CIs for test error and valid hypothesis tests for comparing learning algorithms. The proposed methods are validated through experiments on various datasets, demonstrating improvements over existing approaches.