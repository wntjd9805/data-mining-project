Multi-agent reinforcement learning (MARL) poses challenges in exploring environment dynamics and joint action spaces due to non-stationarity caused by simultaneous learning agents and exponentially growing joint action spaces. This problem is exacerbated in environments with sparse rewards. In this paper, we propose a method for efficient exploration in MARL through the sharing of experiences among agents. We introduce a novel actor-critic MARL algorithm called Shared Experience Actor-Critic (SEAC) that combines gradients computed on an agent's experience with weighted gradients from other agents' experiences. Our experiments demonstrate that SEAC leads to faster learning and higher final returns compared to several baselines. Additionally, sharing experience with SEAC incurs only a slight increase in running time.