Noise injections, such as adding or multiplying samples from a noise distribution to the weights or activations of a neural network, have been found to provide regularization benefits and improve generalization to unseen data. However, there is limited research on understanding the specific benefits of injecting noise throughout a network. In this study, we derive an explicit regularizer that explains the regularization effect of Gaussian noise injections (GNIs), showing that it penalizes networks with high-frequency content in the Fourier domain. We also demonstrate that this regularization leads to larger classification margins and better calibration of models.