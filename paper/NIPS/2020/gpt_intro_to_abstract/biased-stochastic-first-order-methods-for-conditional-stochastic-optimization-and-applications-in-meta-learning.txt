Conditional stochastic optimization (CSO) is a class of optimization problems that has been used in various machine learning applications. In CSO, the objective function is defined as the expectation of a function dependent on both random vectors ξ and η, with ξ representing the distribution of a random variable and η representing the conditional distribution given ξ. While previous approaches have focused on solving CSO with limited samples from the conditional distribution of η|ξ, this paper focuses on the case where multiple samples are available. The paper introduces biased first-order methods, namely biased stochastic gradient descent (BSGD) and biased SpiderBoost (BSpiderBoost), to solve the CSO problem efficiently. The authors provide sample complexity results for these methods and analyze their performance in the context of CSO, including their optimality for different types of CSO objectives and their application to model-agnostic meta-learning (MAML). Numerical experiments demonstrate the effectiveness of BSGD and BSpiderBoost for MAML.