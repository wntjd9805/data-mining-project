Recent studies have shown that backpropagation-free deep learning, particularly the Gated Linear Network (GLN) family, can produce powerful models for classification tasks, especially in the online regime where data efficiency is crucial. In this paper, we expand the capabilities of GLNs to handle real-valued and multi-dimensional data, demonstrating that their theoretical and empirical advantages extend to a wider range of domains than previously thought. The key feature of a GLN is its distributed and local credit assignment, where each neuron has its own convex loss and predicts the target distribution directly. By utilizing a half-space "context function" per neuron, the GLN can learn highly nonlinear functions. This architecture offers several desirable properties, including easy interpretability, robustness to catastrophic forgetting, and universal learning capabilities. A sufficiently large GLN can accurately model any well-behaved, compactly supported density function, and any convex optimization method will converge to the correct solution given enough data.