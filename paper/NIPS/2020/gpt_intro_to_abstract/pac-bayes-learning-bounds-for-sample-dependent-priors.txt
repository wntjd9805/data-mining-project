The PAC-Bayesian framework is commonly used to establish generalization bounds for randomized learning algorithms. These algorithms output a probability distribution over a hypothesis set, and the generalization guarantees associated with this distribution are typically expressed in terms of the relative entropy. In recent years, there has been a focus on establishing more refined PAC-Bayes bounds with sample-dependent priors. Sample-dependent priors have shown potential in improving generalization bounds in overparameterized deep neural networks and in scenarios such as adversarial training. However, there has been limited work on generalization bounds for sample-dependent priors, and our paper aims to contribute to this area by providing general PAC-Bayes bounds using covering numbers and introducing the concept of prior stability.