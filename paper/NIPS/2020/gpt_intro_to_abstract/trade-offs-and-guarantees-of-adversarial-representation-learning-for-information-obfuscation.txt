The paper explores the challenge of protecting sensitive information in machine learning systems that rely on crowdsourced data. Specifically, the focus is on obfuscating sensitive attributes of the shared data to prevent attribute inference attacks from malicious adversaries. Existing methods address this problem using constrained minimax approaches, but the underlying theory is not well understood. The paper aims to bridge this gap between theory and practice by investigating the fundamental trade-off between attribute obfuscation and accuracy maximization. The goal is to achieve information confidentiality for specific attributes while maintaining task accuracy. This approach differs from differential privacy, and the paper compares the two concepts in more detail.