Neural networks are widely used in machine learning, but the reasons for their success remain unclear. While large neural networks can represent a wide range of functions, it is not yet known how their parameters can be adjusted in a computationally efficient way. Additionally, there are cases where neural networks fail to generalize to new samples despite being trained successfully. This paper investigates the training and generalization of neural networks in a teacher-student setup, specifically focusing on cases where the student network is larger than the teacher network. The authors analyze the empirical loss, derive a sample complexity threshold for good generalization, study the convergence of gradient descent, and explore the empirical loss landscape using the string method. The results provide theoretical justification for the empirical observation that deep neural networks can learn functions that can be represented with smaller networks.