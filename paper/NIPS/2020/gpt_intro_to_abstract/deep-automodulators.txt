This paper introduces a new category of generative autoencoders for learning representations of image data sets, capable of reconstructing real-world input images and generating fused images by combining their latent codes. The authors demonstrate the challenge of training such an autoencoder for high resolution images without discriminator networks. They propose a solution using adaptive instance normalization (AdaIn) to modulate the decoder layers and improve the capabilities of the autoencoder. The paper also discusses the concept of layer-specific disentanglement and the advantages of their deterministic model compared to stochastic models like VAEs. The contributions of the paper include techniques for stable unsupervised training of the automodulator, a novel disentanglement loss, and promising performance on various datasets.