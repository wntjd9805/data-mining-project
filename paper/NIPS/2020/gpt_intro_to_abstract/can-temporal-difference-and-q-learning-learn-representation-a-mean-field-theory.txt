Deep reinforcement learning has achieved remarkable empirical successes in complex applications involving rich observations such as images and texts. These successes are attributed to the use of expressive nonlinear function approximators like neural networks to parameterize policies and value functions. However, classical reinforcement learning relies on handcrafted fixed feature representations, while deep reinforcement learning allows for data-dependent feature representations. In this paper, we focus on two prominent algorithms in deep reinforcement learning, temporal-difference (TD) and Q-learning, and investigate the evolution of the feature representations induced by overparameterized two-layer neural networks. We explore their convergence rates, global optimality, and the issue of divergence. We introduce the concept of nonlinear gradient TD, which linearizes the value function approximator locally at each iteration to address the problem of divergence. We further analyze the convergence of TD and Q-learning in the context of overparameterized multi-layer neural networks, showing that they can reach globally optimal solutions. Finally, we extend our analysis to soft Q-learning, which is connected to policy gradient. Our approach is based on a mean-field perspective and utilizes the concept of Wasserstein space to characterize the evolution of feature representations. Our findings contribute to a better understanding of the convergence and optimality properties of deep reinforcement learning algorithms.