Deep Learning has achieved significant success in both academia and industry, primarily in the supervised domain. However, the importance of unsupervised learning is growing, especially when there is limited or no human annotation available for large datasets. Disentanglement, a sub-field of representation learning, aims to identify the underlying generative factors of high-dimensional data. This paper investigates the use of a stricter regularization of a variational auto-encoder (VAE) called Î²-VAE to achieve unsupervised disentanglement. The potential benefits of disentanglement include increased interpretability, cross-domain transfer, robustness, and improved performance in downstream tasks. Previous research has shown that fully unsupervised disentanglement may not be feasible without inductive biases. In response, weak and partial supervision techniques have been introduced. However, there still remains a lack of performance and robustness in training disentanglement models due to extreme hyperparameter sensitivity. To address this challenge, the paper introduces Population Based Training (PBT) for variational training, which overcomes hyperparameter sensitivity and consistently achieves high-performing models. The paper also extends the approach to unsupervised learning using a heuristic called Unsupervised Disentanglement Ranking (UDR) and presents a systematic way to train models for disentanglement. Several experiments and evaluations are conducted to demonstrate the effectiveness and performance of the proposed approach. Overall, the paper provides valuable insights and contributions to the field of disentanglement representation learning.