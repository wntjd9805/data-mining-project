Knowledge distillation, a method for transferring knowledge from a high-capacity neural network to a low-capacity counterpart, has gained popularity in various application domains. The success of knowledge distillation has been justified by the belief that deeper teacher networks with more complexity provide "dark knowledge" that improves the generalization performance of student networks. However, the exact benefits of this dark knowledge remain unclear. This paper aims to shed light on the self-distillation process, specifically exploring the correlation between multi-generational self-distillation and increased diversity in teacher predictions. The authors propose an interpretation of self-distillation as an instance-specific regularization on softmax outputs and demonstrate its relation to label smoothing. Experimental results verify the importance of regularization on the softmax probability simplex space and introduce a new regularization technique called "Beta smoothing." The paper's contributions include explaining the benefits of multi-generational self-distillation, providing an amortized MAP interpretation of teacher-student training, demonstrating the importance of regularization on the probability simplex space, and proposing a new regularization technique.