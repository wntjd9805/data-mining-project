Federated Learning (FL) is a machine learning paradigm where a group of clients collaboratively train a centralized model without sharing their private training datasets. One of the challenges in FL is communication overheads and delays, as well as client heterogeneity. Classic training algorithms in FL, such as federated averaging (FEDAVG), can only be applied when all client models have the same size and structure. To address this limitation, we propose using ensemble distillation for robust model fusion (FedDF), which leverages unlabeled data or artificially generated examples to aggregate knowledge from heterogeneous client models. Through extensive experiments on various datasets and settings, we demonstrate that FedDF can train the server model faster and with fewer communication rounds than existing FL techniques. We also provide insights on when FedDF outperforms FEDAVG and the factors that influence its performance.