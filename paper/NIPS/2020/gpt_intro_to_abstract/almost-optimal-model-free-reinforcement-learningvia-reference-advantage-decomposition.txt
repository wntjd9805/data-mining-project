Reinforcement learning (RL) is a field of study that focuses on an agent maximizing its rewards by making sequential decisions in an unknown environment. There are two main types of algorithms used in RL: model-based and model-free algorithms. Model-free algorithms are often more efficient in terms of space and time complexity but may not achieve the same learning performance as model-based algorithms. In this paper, we propose a novel model-free algorithm called UCB-ADVANTAGE that matches the regret and information theoretic lower bound of model-based algorithms. We also introduce a stage-based update framework and prove that UCB-ADVANTAGE has low switching costs. Finally, we extend our results to concurrent RL, showing that UCB-ADVANTAGE can compute an optimal policy in a concurrent setting.