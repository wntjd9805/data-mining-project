Deep neural networks have shown impressive results in various fields, but they often require extreme overparameterization, resulting in higher training costs and limitations in memory or inference time. Recent theoretical work suggests that overparameterization is crucial for network training dynamics and generalization. However, empirical approaches have successfully found compact neural networks through model shrinking or efficient architectures. This paper aims to address two questions related to pruning techniques and the Lottery Ticket Hypothesis: can sparse networks with competitive performance be found by approximating L0 regularization, and can a method based on L0 regularization find winning tickets like the Iterative Magnitude Pruning (IMP) algorithm? The proposed method, called Continuous Sparsification, provides a novel approximation to L0 regularization and outperforms heuristic-based pruning methods. It also raises questions on improving ticket search and demonstrates the capability to find subnetworks that outperform those found by IMP.