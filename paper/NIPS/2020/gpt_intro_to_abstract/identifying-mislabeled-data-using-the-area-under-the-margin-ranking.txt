Deep networks have shown great potential in various applications, but their performance is often hindered by low-quality data, including weakly-labeled samples and labeling mistakes. The presence of mislabeled training data can lead to overfitting and poor generalization. In this paper, we propose a method to automatically identify and remove mislabeled samples from training datasets. Our approach leverages the network's training dynamics and introduces the Area Under the Margin (AUM) statistic to measure the difference between logit values for a sample's assigned class and its highest non-assigned class. We also introduce an artificial class with threshold training data to determine the AUM threshold for separating correctly-labeled and mislabeled data. Implementing our method requires logging the model's logits during training and removing samples with AUM below the threshold. Experimental results demonstrate improved performance on benchmark tasks and real-world datasets. Notably, removing 13% of mislabeled samples from the CIFAR100 dataset leads to a 1.2% reduction in test-error for a ResNet-32 model.