Deep learning approaches have made significant advancements in Natural Language Generation (NLG), particularly through the use of sequence to sequence models trained with Maximum Likelihood Estimation (MLE). However, MLE training regimes have limitations in modeling sequence probabilities and often result in degenerate generated texts. Reinforcement Learning (RL) has been explored as an alternative, but traditional sequence-level metrics as reward functions do not align well with human judgments. Generative Adversarial Network (GAN) paradigms have been used successfully in image generation, but applying them to text generation poses challenges due to the discrete nature of text. Previous attempts to use GANs for text generation have underperformed compared to MLE models. Exploration, a crucial aspect in RL, has not been extensively studied in the context of text generation. This paper proposes a new exploration method that samples more structured rewards and better suits the training dynamics of Language GANs, resulting in successful training. The authors present their findings on the behavior of discriminators and the importance of reducing exploration space in discrete GANs. They introduce ColdGANs, a GAN architecture with alternative sampling strategies that constrain sampling to the distribution modes. The proposed methods are applied to three tasks with positive results compared to previous works, including GANs and MLE-based models.