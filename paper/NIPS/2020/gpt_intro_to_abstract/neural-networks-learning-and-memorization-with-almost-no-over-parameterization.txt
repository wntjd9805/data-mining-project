Understanding the models and algorithms in deep learning theory is crucial for the development of neural networks. While significant progress has been made in learning linear models, kernel spaces, polynomials, and memorization models, the required network size is still large unless the model is linearly separable or the activation is quadratic. In this paper, we investigate the neural tangent kernel and its convergence rate, showing that SGD on depth two networks can learn memorization models, polynomials, and kernel spaces with near-optimal network size, sample complexity, and runtime. This result is the first to demonstrate near optimal learnability of these models, with the potential to further explore different settings, architectures, and initialization schemes. We provide detailed results on NTK convergence, bounded distributions, memorization, learning polynomials, and learning kernel spaces, presenting the analysis and implications of our findings.