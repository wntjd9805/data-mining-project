Bregman divergences are widely used in machine learning, particularly in clustering and optimization. However, many learning problems require divergences other than Euclidean distances. This paper focuses on learning an arbitrary Bregman divergence from supervision and explores its applications in clustering, similarity search, and ranking. The authors propose a parameterization method using piecewise linear functions to approximate convex functions, and demonstrate that these functions can effectively approximate arbitrary Bregman divergences. The paper also extends the Mahalanobis metric learning problem to non-linear metrics and provides theoretical guarantees for both approximation error and generalization error. Experimental results show that the proposed method outperforms linear and non-linear metric learning baselines in ranking and clustering tasks. Additional details and results are provided in the supplementary material.