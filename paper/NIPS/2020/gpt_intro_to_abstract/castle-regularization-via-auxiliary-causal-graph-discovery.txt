A primary concern in machine learning, particularly in deep learning, is the generalization performance on out-of-sample data. Over-parameterized deep networks can efficiently learn complex models but are susceptible to overfitting. Common techniques to mitigate overfitting, such as data augmentation and dropout, do not consider the causal relationships between variables. In this paper, we introduce CASTLE, a novel regularization method that utilizes causal graph discovery as an auxiliary task during training to improve generalization performance. By jointly learning the causal graph, CASTLE can identify optimal predictors and reconstruct only the input features that have neighbors in the causal graph. We provide a theoretical generalization bound for CASTLE and demonstrate its improved performance on various datasets.