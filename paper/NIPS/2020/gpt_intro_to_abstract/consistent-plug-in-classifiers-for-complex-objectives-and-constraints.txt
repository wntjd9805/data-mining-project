In an increasing number of machine learning tasks, training a classifier with constraints on multiple metrics has become essential. These metrics, such as fairness, coverage, recall, etc., often have a complex non-decomposable structure and cannot be expressed as a simple average of errors on individual data points. Various metrics, including F-measure, G-mean, predictive parity criteria, and KL-divergence based metrics, can be defined as functions of a classifier's confusion matrix. This paper focuses on constrained learning problems where the objectives and constraints are general functions of the confusion matrix. The goal is to design a statistically consistent algorithm that converges to an optimal feasible classifier. Previous work has provided consistent algorithms for such problems, but this paper presents an improved algorithm that requires fewer calls to the learning routine and achieves the same quality of classifier. The proposed algorithm formulates the problem as an optimization over the intersection of two convex sets - achievable confusion matrices and feasible confusion matrices. By decoupling the search space, the algorithm adapts the Frank-Wolfe based algorithm to solve the optimization. The algorithm is computationally efficient, applicable to multi-class and fairness problems, and the number of optimization parameters scales linearly with the number of classes and groups. The paper presents experimental results on benchmark fairness datasets, demonstrating the performance and robustness of the proposed algorithm.