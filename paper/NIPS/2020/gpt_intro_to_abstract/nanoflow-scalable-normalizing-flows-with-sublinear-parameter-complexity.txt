Flow-based models have gained popularity in density estimation and generative modeling. These models, based on normalizing flows (NFs), use deep invertible models trained with analytically estimated likelihoods to map data to a known prior. NFs offer exact probability density estimation and simplified training compared to other generative models. However, existing studies have focused on improving expressiveness without considering parameter complexity and memory efficiency. This study introduces NanoFlow, a parameterization scheme for NFs that reduces the required number of parameters while maintaining expressiveness. The authors demonstrate several parameter-efficient solutions to enhance NanoFlow's flexibility and show that conditioning the shared estimator further improves modeling. The study also explores the conditions under which NanoFlow performs best, showing promising results for bipartite flows. Overall, this study offers a systematic assessment for enabling scalable NFs with constant parameter complexity.