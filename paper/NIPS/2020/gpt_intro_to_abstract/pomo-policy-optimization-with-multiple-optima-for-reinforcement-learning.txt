Combinatorial optimization (CO) problems are pervasive in various industries, but each problem comes with its unique constraints that can change rapidly. This makes it challenging to develop a generalized algorithm that can handle different conditions. While classical methods based on manual feature engineering have been successful in computer vision and natural language processing, they are not suitable for most CO problems. However, recent advancements in deep reinforcement learning show promise in solving NP-hard CO problems. In this paper, we introduce Policy Optimization with Multiple Optima (POMO), a framework that leverages symmetries in CO problems to automatically generate solvers. We demonstrate the effectiveness of POMO by solving TSP, CVRP, and KP problems, showing superior performance compared to contemporary neural RL approaches. The contributions of this paper include identifying symmetries in RL methods for CO problems, devising a low-variance baseline for policy gradient, and presenting an inference method based on multiple greedy rollouts.