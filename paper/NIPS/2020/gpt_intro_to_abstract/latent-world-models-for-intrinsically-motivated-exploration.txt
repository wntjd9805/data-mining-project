In this paper, we address the challenge of sparse reward signals in reinforcement learning (RL) by proposing an approach that uses intrinsic motivation to encourage agents to seek novel or rare states. We discuss different methods for detecting and calculating the reward for such states, including estimating novelty based on future state prediction error and counting visited states. We also highlight the importance of dealing with partial observability in RL environments and the need for maintaining a belief state for action selection. To enable efficient exploration, we introduce a self-supervised representation learning method based on minimizing the Euclidean distance between temporally close observations. Our proposed world model, which uses a recurrent neural network, is insensitive to stochasticity and allows for the detection of novel states. We evaluate our method on the Partially Observable Labyrinth and several Atari environments with sparse rewards, demonstrating improved results compared to prior exploration methods.