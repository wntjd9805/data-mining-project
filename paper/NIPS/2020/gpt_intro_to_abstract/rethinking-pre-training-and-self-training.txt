Pre-training is a common method used in computer vision, where a model pre-trained on one dataset is expected to help with another related task. However, recent research has shown that ImageNet pre-training does not improve accuracy on the COCO dataset. In contrast, self-training, which involves discarding labels on ImageNet and generating pseudo labels using an object detection model trained on COCO, has shown promising results. This paper aims to investigate the effectiveness of self-training compared to pre-training. The authors conduct control experiments varying the amount of labeled data in COCO and the strength of data augmentation. The results demonstrate that as the strength of data augmentation or the amount of labeled data increases, the value of pre-training diminishes while self-training continues to improve performance. Additionally, the study explores the use of self-supervised learning and finds that both supervised and self-supervised pre-training methods fail to scale as the labeled dataset size grows. However, the paper acknowledges the benefits of pre-training in terms of speed and its usefulness in scenarios where labeled data is limited. Furthermore, self-training is shown to provide additional improvements even when utilizing the same dataset as pre-training. The study also showcases the flexibility of self-training by achieving superior results on the COCO and PASCAL datasets using different unlabeled data sources and model architectures.