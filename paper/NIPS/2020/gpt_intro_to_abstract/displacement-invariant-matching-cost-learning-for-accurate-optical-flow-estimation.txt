In this paper, we address the problem of finding per-pixel dense correspondences between a pair of input images through optical flow estimation and stereo matching. While both methods rely on the cost volume representation, they differ in their strategies and network architectures. Current deep stereo matching methods use learned matching costs between shifted features, while deep optical flow methods often rely on non-learned metrics. We propose a novel solution that allows the network to learn matching costs without constructing a high-dimensional feature volume, achieving higher accuracy without sacrificing computational speed. We introduce displacement-invariant cost learning (DICL) to decouple connections between different displacements, and a displacement-aware projection (DAP) layer to mitigate the multi-modal problem. Our proposed method achieves state-of-the-art accuracy and outperforms existing optical flow estimation methods on benchmark datasets.