Deep neural networks have been successful in various areas of artificial intelligence but often require complex networks and extensive training for good generalization. Knowledge distillation (KD) is a method that transfers knowledge from a large trained model (teacher) to a smaller network (student) to achieve better performance. Many variants of KD have been proposed, but there is still a lack of theoretical explanation for its effectiveness. In this work, we address these issues by using neural tangent kernel and wide network linearization, focusing on the soft ratio as a switch between hard label training and soft label distillation. Our contributions include observing a faster convergence rate for softer tasks, introducing a metric for data inefficiency, and discussing the benefits of hard labels in imperfect teacher distillation.