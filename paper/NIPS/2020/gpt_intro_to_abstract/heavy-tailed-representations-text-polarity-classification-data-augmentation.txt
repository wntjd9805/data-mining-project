Representing the meaning of natural language in a mathematically grounded way is a scientific challenge that has received increasing attention in recent years due to the abundance of digital content and text data. While various embeddings have been proposed for text analysis tasks, none of them consider the heavy-tailed nature of word frequency distributions. In this paper, we propose a methodology based on multivariate extreme value analysis to tackle this issue. Our approach leverages extreme value theory and a transformation mapping to learn a heavy-tailed representation of text data. Additionally, we introduce a novel data augmentation mechanism that preserves the labels of the original sequences. We demonstrate the effectiveness of our methodology on sentiment analysis tasks using BERT embeddings. Experimental results on synthetic and real datasets validate our approach.