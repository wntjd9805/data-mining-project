Loss surfaces of neural networks have become an area of focus in the deep learning community, with researchers exploring both numerical and theoretical perspectives. Optimizing these loss surfaces has shown that gradient descent methods can converge to non-spurious minima, leading to advancements in various applications. One intriguing question is regarding the connectivity of trained models, specifically the existence of a nearly constant loss path between them. Previous studies have suggested a potential asymptotic connection between models, especially concerning the width of hidden layers. However, it is crucial to consider the intrinsic properties of neural networks, such as weight symmetry, which can impact the paths and connections. To address this, the proposed work aims to develop a technique for more consistent model interpolation and optimal connection finding, taking into account the effect of weight symmetry in the loss landscape. The findings from this study will provide valuable insights into the often complex geometry of deep neural network loss surfaces.