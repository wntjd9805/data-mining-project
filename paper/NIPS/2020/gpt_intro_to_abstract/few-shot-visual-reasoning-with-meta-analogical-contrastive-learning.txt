This paper introduces a novel approach to tackle the problem of few-shot visual reasoning, which aims to learn abstract visual reasoning with limited training instances and generalize to unseen attribute problems. The proposed method utilizes a meta-analogical contrastive learning framework to capture relational similarity between pairs of problems. The model is evaluated on the RAVEN visual reasoning benchmark and demonstrates superior performance compared to existing methods in both conventional and few-shot learning scenarios, as well as generalization to unseen attribute problems.