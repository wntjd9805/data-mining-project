Deep feedforward neural networks have become integral to machine learning but their lack of interpretability has been a challenge. Random networks have been studied to understand the assumptions they impose on functions, and the concept of assigning priors to neural networks in Bayesian deep learning requires understanding their effects. Previous research has shown that random neural networks converge to Gaussian processes in the limit of infinite width. The equivalence between neural network activation functions and GP covariance functions has been established. This paper introduces a new family of non-linear neural network activation functions that mimic the Mat√©rn family in GP models, showing similar properties and good performance in Bayesian deep learning tasks. The local stationarity property helps tackle overconfidence in out-of-distribution detection.