Machine learning models have been shown to be vulnerable to adversarial examples, where small imperceptible changes to the input can drastically alter the model's prediction. Previous research has proposed two types of adversarial attack methods for continuous models like neural networks: gradient-based methods and decision-based methods. However, these methods are not applicable to tree-based ensembles such as gradient boosting decision trees (GBDT) and random forests (RFs). In this paper, we address the problem of efficient adversarial attack on tree-based ensembles and propose a method that minimizes perturbation to uncover the true weaknesses of the model. We transform the continuous input space into a discrete "leaf tuple" space and optimize the adversarial leaf tuple by moving it to the best adversarial tuple within a small neighborhood. Our experiments show that our method is significantly faster than previous approaches and achieves smaller distortions in various datasets. Moreover, our method can be adapted to other distance metrics beyond the commonly used p-distance.