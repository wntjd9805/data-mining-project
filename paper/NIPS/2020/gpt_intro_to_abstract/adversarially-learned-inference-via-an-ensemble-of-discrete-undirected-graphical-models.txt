Probabilistic graphical models are compact representations used to model joint probability distributions. This paper focuses on discrete pairwise undirected graphical models and explores the limitations of inference through belief propagation when the graph topology is loopy or mis-specified. The paper introduces a generative approach to training these models using maximum likelihood estimation, but acknowledges that the accuracy of this approach is diminished for discriminative tasks due to the approximations in loopy graphs. To address this limitation, the paper proposes an adversarially trained graphical model strategy that learns to optimize the generation of samples from the model without baking in information about specific inference tasks. The paper presents experiments to showcase the generalization capabilities of the proposed approach and compares it against other state-of-the-art models. The results demonstrate the effectiveness of the adversarially trained graphical models in performing inference tasks and sampling from joint probability distributions.