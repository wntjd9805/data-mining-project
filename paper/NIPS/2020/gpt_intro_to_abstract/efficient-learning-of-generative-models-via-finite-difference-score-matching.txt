Deep generative models have made significant progress in learning data distributions using either explicit density functions or implicit generative processes. Energy-based models (EBMs) are a type of explicit model that offers more flexibility and simpler compositionality compared to other explicit generative models. However, training EBMs with maximum likelihood estimate (MLE) is challenging due to the intractable partition function. Score matching (SM) presents an alternative objective that avoids the need for the partition function by training unnormalized models using the Fisher divergence. Various variants of SM have been proposed, but they often suffer from computational cost, biased parameter estimation, high variance, or complex implementations. Sliced score matching (SSM) offers a scalable and unbiased estimator, but still requires computationally expensive derivatives of the density function. To address these issues, this paper proposes a finite-difference (FD) decomposition approach for any-order directional derivatives, eliminating the need for optimizing higher-order gradients. The FD approach only requires efficient parallel execution of independent likelihood function evaluations, reducing computational complexity and improving numerical stability. Experimental results demonstrate the speed-up ratios and comparable performance of the proposed FD reformulations on different generative models and datasets.