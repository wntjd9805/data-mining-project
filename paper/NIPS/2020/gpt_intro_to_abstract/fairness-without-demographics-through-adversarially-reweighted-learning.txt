Machine learning (ML) systems must avoid discrimination as they are used for decision making in critical scenarios. However, recent research has revealed fairness concerns, with significant accuracy disparities across demographic groups in various applications. Most existing works on fairness in ML assume access to protected features, but in practical situations, it may not be feasible or allowed due to privacy or legal restrictions. This paper addresses the challenge of improving fairness in ML models without access to protected features. The authors propose an approach called Adversarially Reweighted Learning (ARL), which leverages computationally-identifiable errors to improve worst-case performance for unobserved protected groups. Experimental results demonstrate the effectiveness of ARL in achieving fairness in real-world datasets.