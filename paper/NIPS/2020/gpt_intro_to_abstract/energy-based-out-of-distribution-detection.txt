This paper addresses the problem of out-of-distribution (OOD) uncertainty in machine learning models, which arises when a model encounters inputs that differ from its training data. The traditional approach of using softmax confidence scores for OOD detection is shown to be suboptimal, as neural networks can produce high confidence scores for inputs far away from the training data. To overcome this limitation, the paper proposes the use of an energy score, which is theoretically aligned with the probability density of the input. The energy score can be derived from a discriminative classification model and does not require a density estimator, making it more practical and effective for OOD detection compared to generative-based methods. The paper presents empirical evidence demonstrating the superiority of the energy score over softmax confidence scores and generative-based methods in OOD detection tasks. Both the use of energy scores as a replacement for softmax confidence at inference time and as a trainable cost function for fine-tuning the classification model are explored. Experimental results show improved OOD detection performance and comparable or better classification accuracy on in-distribution data using the proposed energy-bounded learning objective. The paper concludes with a literature review on OOD detection and the broader impact of the research.