Deep neural networks (DNNs) have shown remarkable performance improvements in computer vision tasks. However, the large number of parameters and computational complexity make DNNs difficult to deploy on resource-constrained and low-power devices. To address this, various compression techniques have been proposed, including network pruning, low-rank decomposition, efficient architecture design, and network quantization. This paper focuses on binary neural networks (BNNs), which restrict weights and activations to only two values (-1 and +1). While BNNs reduce network size and improve efficiency, there is still a challenge in closing the accuracy gap compared to full-precision networks. The introduction proposes a Rotated Binary Neural Network (RBNN) approach to mitigate the quantization error caused by the angular bias between full-precision weights and their binarized versions. The proposed approach aligns the angle difference between the weight vectors and the binary hypercube to reduce quantization error and maximize information gain. Experimental results demonstrate the effectiveness of RBNN.