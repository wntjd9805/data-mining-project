Transformers, particularly the BERT model, have revolutionized Natural Language Processing (NLP) tasks due to their versatility. However, their self-attention mechanism, which allows each token in the input sequence to attend independently to every other token, has quadratic computational and memory requirements. This limits their applicability to tasks that require larger context. In this paper, we introduce a sparse attention mechanism called BIGBIRD that improves performance on tasks with long contexts. BIGBIRD consists of global tokens, local neighboring tokens, and random tokens attending to all tokens. We demonstrate that BIGBIRD maintains expressivity and achieve state-of-the-art results in question answering, document summarization, and genomic sequence analysis.