Graph neural networks (GNNs) have shown great success in graph representation learning tasks such as node classification and link prediction. However, GNNs typically require a large amount of labeled data for training, which can be expensive and time-consuming to collect in certain domains. Active learning has been proposed as a strategy to reduce the annotation cost of training GNNs by dynamically selecting the most informative instances for labeling. While active learning has been proven effective on independent data, its application to graph-structured data with dense interconnections between instances remains under-explored. In this paper, we propose GPA, a Graph Policy network for transferable Active learning on graphs. By formalizing active learning on graphs as a Markov decision process, our approach learns the optimal query strategy using reinforcement learning. We train the graph policy network on labeled training graphs and transfer the learned policy to perform active learning on unseen test graphs, resulting in a zero-shot policy transfer. Experimental results demonstrate the effectiveness of GPA in semi-supervised node classification tasks under both same-domain and different-domain settings.