Deep Reinforcement Learning (RL) algorithms often rely on extensive computing resources to search for optimal hyperparameters when applied to new domains. While meta-learning approaches focus on learning good initialization, many hyperparameters still need to be adapted during an agent's lifetime. This paper introduces two novel ideas that extend the IMPALA algorithm, allowing for self-tuning of hyperparameters within a single agent lifetime. The proposed Self-Tuning Actor-Critic (STAC) agent self-tunes differentiable hyperparameters and introduces a leaky V-trace operator. Additionally, the STACX agent adds auxiliary parametric actor-critic loss functions and self-tunes their metaparameters, improving performance in various environments. Empirical results demonstrate consistent improvements in performance, and extensive ablation studies and experiments further validate the effectiveness and robustness of the proposed approach.