Reinforcement learning (RL) has advanced the capabilities of agents by enabling them to learn complex behaviors through interaction with their environment. However, deep RL (DRL) algorithms, which combine RL paradigms with powerful function approximators, suffer from high sample complexity. To address this, curriculum learning (CL) for RL has emerged as a promising approach to improve the learning progress of RL agents by designing task sequences that facilitate the transfer of successful behavior. This paper focuses on the problem of curriculum generation, assuming access to a set of parameterized tasks. While existing algorithms have demonstrated empirical success in sample efficiency, they lack a theoretical foundation. In contrast, our proposed approach leverages principled inference methods by generating the curriculum based on the agent's value function and the KL divergence to a target distribution of tasks. Drawing inspiration from self-paced learning (SPL) in supervised learning, our method performs approximate inference on the common latent variable model for RL. We demonstrate the effectiveness of our approach by incorporating it into popular DRL algorithms and achieving comparable or superior performance to state-of-the-art CL methods across various environments with different complexities and reward structures.