This paper introduces a framework that leverages the properties of online and continual learning to enable effective and data-efficient transfer of previously acquired skills. The framework involves ensemble task-specific neural networks at the individual node level, allowing for combinatorial transfer where a network trained on multiple tasks can generalize to a large number of pseudo-tasks. The paper demonstrates the effectiveness of this method on various online continual learning benchmarks. The framework is instantiated using the Gated Linear Network (GLN) and the Forget-Me-Not Process (FMN) algorithms, which complement each other and create a system suitable for online continual learning.