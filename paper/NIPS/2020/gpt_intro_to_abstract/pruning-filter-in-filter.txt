Deep Neural Networks (DNNs) have made significant advancements in various domains, but the deployment of these models can be costly due to the large number of parameters. To address this issue, researchers have proposed methods to compress DNNs, such as weight pruning (WP) and filter pruning (FP). While WP prunes individual weights inside the network, resulting in a sparse network, FP prunes filters or channels within the convolution layers, maintaining a well-structured network. In this paper, we introduce a new pruning paradigm called Shape Weight Pruning (SWP), which achieves finer granularity than traditional filter pruning methods. We also propose Filter Skeleton (FS) to efficiently learn the shape of each filter, thereby achieving state-of-the-art pruning ratios without significant accuracy loss.