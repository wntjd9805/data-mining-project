Variational inference (VI) is an optimization-based approach used for approximate posterior inference in computer science. Traditional VI algorithms minimize the "exclusive Kullback-Leibler (KL)" KL(q|p), which allows for efficient computation but underestimates posterior uncertainty. To address this limitation, alternative divergence measures such as the "inclusive KL" KL(p|q) have been considered, but they result in a challenging optimization problem. In this paper, we propose Markovian score climbing (MSC), a simple algorithm that reliably minimizes the inclusive KL. We demonstrate the convergence properties and advantages of MSC through empirical studies on various models and data sets. The contributions of this paper include the development of MSC, the study of systematic errors in existing methods, and empirical evidence of the utility of MSC.