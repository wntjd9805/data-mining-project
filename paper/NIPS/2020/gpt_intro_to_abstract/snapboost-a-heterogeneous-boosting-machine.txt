Boosted ensembles of decision trees, such as XGBoost and LightGBM, have become the dominant machine learning technique in domains with abundant tabular data. While these methods achieve high generalization performance, they require tuning a large number of hyperparameters. In this paper, we propose a heterogeneous boosting framework called Heterogeneous Newton Boosting Machine (HNBM), which selects the base hypothesis at each boosting iteration randomly from a fixed set of subclasses. We derive a global linear convergence rate for HNBM and describe a specific realization called SnapBoost that combines binary decision trees with random Fourier features. Experimental results on various datasets show that SnapBoost outperforms other boosting frameworks in terms of generalization while maintaining high performance.