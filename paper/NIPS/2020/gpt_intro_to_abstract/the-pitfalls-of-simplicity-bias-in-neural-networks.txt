Understanding the generalization ability of neural networks, despite their capacity to fit randomly labeled data, has been extensively studied. Previous work has shown that linear neural networks trained with Stochastic Gradient Descent (SGD) on linearly separable data converge to the maximum-margin linear classifier, explaining their superior generalization performance. However, this finding contradicts empirical evidence that neural networks are brittle to adversarial examples and distribution shifts. To address this, we aim to understand the simplicity bias (SB) of neural networks in a setting that captures their non-robustness. We design synthetic and image-based datasets that incorporate features of varying complexity and predictive power to investigate SB systematically. Our findings reveal that neural networks tend to ignore complex predictive features in favor of simple ones, leading to lack of robustness, unreliable confidence estimates, and suboptimal generalization. We establish these pitfalls through both theoretical analysis and controlled experiments. The datasets we introduce not only provide a testbed for devising better training procedures, but also serve as a starting point for designing more realistic datasets amenable to theoretical analysis.