Adversarial examples, which can deceive state-of-the-art models with subtle perturbations in natural images, pose a significant threat to the security of deep neural networks (DNNs). The transferability of adversarial examples, where an example crafted for one DNN can fool other models, has attracted attention and been utilized in various attacks. Building on the hypothesis that the transferability of adversarial examples stems from the linear nature of DNNs, this paper presents empirical evidence supporting this idea and proposes LinBP, a method that removes nonlinear activations and improves transferability. Experimental results demonstrate the effectiveness of LinBP in attacking different victim models on CIFAR-10 and ImageNet datasets.