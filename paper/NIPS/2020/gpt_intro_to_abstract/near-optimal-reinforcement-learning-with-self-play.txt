A wide range of modern artificial intelligence challenges can be cast as a multi-agent reinforcement learning (RL) problem, where more than one agent makes sequential decisions in an interactive environment. Despite the recent success of multi-agent RL in various tasks, existing RL algorithms often require a large number of samples, which is impractical in real-world settings. This paper aims to understand the sample complexity in multi-agent RL and design algorithms that can find near-optimal policies with a small number of samples. The theoretical understanding of sample complexity in multi-agent RL is limited compared to single-agent settings, and this paper focuses on the two-player tabular Markov games setting. The authors propose optimistic variants of Nash Q-learning and Nash V-learning algorithms, achieving near-optimal sample complexity for finding approximate Nash equilibria. Additionally, it is shown that learning best responses in Markov games is computationally hard. This paper also introduces a technique for extracting certified policies from RL algorithms.