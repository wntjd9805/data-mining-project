This paper introduces the concept of inverse optimization (IO) in linear programming, where linear programs are learned from observations of optimal decisions rather than the costs or constraints themselves. The goal is to infer a constrained optimization model that produces identical decisions and generalizes to novel conditions. The learning problem is formulated in a novel way and tackled using gradient-based methods. The paper also discusses the challenges of learning a constrained optimizer that is both feasible and optimal, and presents successful results in learning linear program instances and minimum-cost multi-commodity flow problems. The concept of parametric linear programs (PLP) is introduced, where the optimal decisions may depend on features, and a suitable hypothesis space is defined to infer PLPs from data. The forward optimization problem (FOP) is introduced as the form of the hypothesis space, and the relationship between PLPs and LPs is discussed.