In recent years, deep neural networks have achieved significant success in various tasks, such as computer vision and natural language processing. However, the training time of these networks can be quite long due to the size of the models and training datasets. To address this issue, distributed training using the parameter server framework has been proposed as a promising approach. Another popular distributed training framework called Federated Learning has also gained attention, especially in scenarios where data privacy is a concern. While distributed algorithms like SIGNSGD and MEDIANSGD have been proposed to reduce communication overheads and ensure robustness, their convergence behavior under heterogeneous data settings, such as in Federated Learning, is not well understood. This paper aims to analyze the convergence rates of SIGNSGD and MEDIANSGD, without assuming homogeneous data distributions, and proposes mechanisms to bridge the gap between the median and mean gradients to improve their performance.