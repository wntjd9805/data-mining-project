This paper introduces a new approach to designing efficient exploration algorithms for multi-armed bandit (MAB) problems with heavy-tailed rewards. The authors propose a novel p-robust estimator that does not rely on prior information about the moments of the reward distribution. By combining this estimator with a perturbation-based exploration method, they develop a general regret analysis scheme that provides both upper and lower bounds on the cumulative regret. The proposed strategy achieves the optimal regret bound and outperforms existing methods in both theoretical and empirical evaluations. The main contributions of this paper include deriving lower regret bounds, proposing a robust estimator, developing a regret analysis scheme, and achieving the minimax optimal rate under heavy-tailed rewards.