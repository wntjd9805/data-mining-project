Reinforcement learning (RL) is an important field in computer science that focuses on solving sequential decision-making problems by maximizing expected total rewards. These problems often involve constraints on utilities or costs, leading to the need for constrained Markov Decision Processes (CMDPs) as a model for the environment dynamics. Policy gradient (PG) methods, particularly the natural policy gradient (NPG) method, have shown success in solving both MDPs and CMDPs. However, existing theoretical guarantees for these methods are typically asymptotic or only provide local convergence guarantees. This paper aims to establish theoretical guarantees for the non-asymptotic global convergence of the NPG method in solving CMDPs, addressing questions about its applicability, convergence speed, impact of function approximation error, and sample complexity. The authors propose a primal-dual algorithm called NPG-PD for solving discounted infinite-horizon CMDPs and provide convergence guarantees in terms of optimality gap and constraint violation. They also analyze the general smooth policy class and propose sample-based NPG-PD algorithms with non-asymptotic convergence properties and finite-sample complexity guarantees. Overall, this work offers the first non-asymptotic convergence guarantees for solving infinite-horizon discounted CMDPs in the primal-dual framework.