This paper introduces a novel loss function called SupCon, which builds on the contrastive self-supervised literature by leveraging label information. The loss function pulls together normalized embeddings from the same class and pushes apart embeddings from different classes. It considers multiple positives per anchor and many negatives, achieving state-of-the-art performance without the need for hard negative mining. The proposed loss consistently outperforms cross-entropy on large-scale classification problems, achieving higher accuracy on the ImageNet dataset. The paper provides empirical results, analytical insights, and demonstrates the stability and effectiveness of the proposed loss function.