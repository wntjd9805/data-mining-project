This paper introduces a novel approach to speed up the pre-training of Transformer networks for natural language processing tasks. The authors address the challenge of training efficiency in Transformer networks by exploring architectural changes and training techniques that do not require excessive hardware resources. They propose a new architecture unit called the Switchable-Transformer (ST) block, which allows for layer dropping during training and stabilizes the network. Additionally, they propose a progressive schedule to gradually increase the layer dropping rate during training. Through experiments with BERT, the authors show that their approach enables faster pre-training without sacrificing accuracy on downstream tasks. They also demonstrate the generalizability of their models, achieving higher scores than the baseline on the GLUE benchmark.