Topic models such as Latent Dirichlet Allocation (LDA) have been widely used to infer latent topics from a corpus. However, these models suffer from poor quality topics when applied to small or short document corpora. In this paper, we propose a novel framework that incorporates optimal transport theory to improve the topic representations by exploiting the geometric structures of semantically related words in embedding spaces. We formulate the framework as a generalized methodology of LDA, substituting the squared Euclidean distance with regularized optimal transport distance. Our experiments demonstrate that our proposed model outperforms state-of-the-art topic models in terms of performance and interpretability. Additionally, our framework has implications for studying richer classes of topic models with hierarchical or temporal structures.