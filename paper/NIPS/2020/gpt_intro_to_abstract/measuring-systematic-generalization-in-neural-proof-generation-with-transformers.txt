This paper examines the ability of Transformer Language Models (TLMs) to perform long chains of reasoning in the context of theorem proving. The authors use the CLUTRR benchmark suite to test the reasoning and generalization capabilities of TLMs, focusing on their ability to generate interpretable proofs with logically consistent language modeling. The experiments reveal that TLMs struggle with length generalization and perform better when trained on longer proofs and backward-chaining. Surprisingly, TLMs also perform better when directly generating the answer instead of generating the proof first. Overall, this work contributes to understanding the reasoning capacity of TLMs and suggests avenues for future research in designing models with stronger reasoning abilities.