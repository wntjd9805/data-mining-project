In recent years, deep learning research has made significant progress in lossy image compression. End-to-end trained neural networks have surpassed traditional compression schemes for images. However, challenges such as computational complexity, temporal inconsistencies, and the lack of effective perceptual metrics for optimization still exist. This paper addresses the issue of quantization in lossy compression and explores a promising alternative that eliminates quantization altogether. The authors propose a technique that implements additive uniform noise as an approximation for quantization, which is both statistically and computationally efficient. They also demonstrate how to smoothly interpolate between uniform noise and hard quantization while maintaining differentiability. The authors find that a better alignment between training and test phases leads to improved performance, especially in models of lower complexity.