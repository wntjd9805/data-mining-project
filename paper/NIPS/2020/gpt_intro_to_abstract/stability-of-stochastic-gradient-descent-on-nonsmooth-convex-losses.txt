Understanding and bounding the generalization error of machine learning algorithms is crucial for successful applications of machine learning. The use of continuous optimization techniques, particularly stochastic gradient descent (SGD), is a popular approach in modern machine learning. However, the generalization properties of SGD are still not well understood. In this paper, we focus on the setting of stochastic convex optimization (SCO) and aim to minimize the population risk of an arbitrary and unknown distribution. We quantify the performance of an algorithm by its expected excess population risk, which can be decomposed into optimization error (training error) and generalization error. Bounds on the generalization error lead to provable guarantees on the excess population risk. Previous work has provided bounds on the generalization error and stability of SGD, but these bounds do not hold for nonsmooth losses. In this paper, we establish tight bounds on the uniform stability of SGD on nonsmooth convex losses and provide insights into its stability properties. We also discuss the implications of our bounds on generalization bounds for multi-pass SGD and differentially private stochastic convex optimization for nonsmooth losses. Our upper bounds are based on summing differences between gradients, while our lower bounds are based on the behavior of a highly nonsmooth function.