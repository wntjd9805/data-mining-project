This paper introduces a new perspective on continual learning in machine learning, focusing on the meta-distribution of model parameters. The authors propose a methodology called MERLIN, which uses a Variational Auto-encoder (VAE) to learn the meta-distribution and consolidate knowledge over tasks. The proposed method is compared to benchmark methods and a state-of-the-art method on five continual learning datasets, demonstrating its effectiveness. This work provides a deeper understanding of the methodology and its potential applications. MERLIN is the first effort to learn in the meta-space of model parameters.