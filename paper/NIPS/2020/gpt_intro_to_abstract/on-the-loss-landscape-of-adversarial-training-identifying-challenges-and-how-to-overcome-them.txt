State-of-the-art deep learning models have been found to be vulnerable to adversarial attacks, where imperceptible perturbations of the input can cause the model to produce incorrect predictions with high confidence. Although adversarial training has been proposed as a defense mechanism, it has been observed to have limitations such as larger generalization gap and slower convergence compared to training on clean data. In this paper, we study the optimization landscape in adversarial training and investigate the impact of adversarial budget size on the loss landscape. We provide theoretical and empirical analyses, showing the existence of an abrupt change in adversarial examples and the hindrance of model escape from suboptimal regions in larger adversarial budgets. Based on our findings, we propose a periodic adversarial scheduling strategy that improves performance and robust accuracy compared to vanilla adversarial training.