This paper introduces the problem of the storage requirements and energy consumption associated with the use of shortcuts in very deep convolutional neural networks (CNNs). While shortcuts are effective for improving performance, they prevent the release of intermediate layer feature maps during online inference, requiring the retention of storage for subsequent calculations. The paper proposes a method for removing shortcuts without sacrificing accuracy during inference by implementing a joint-training framework based on Residual Distillation (JointRD). This framework allows the integration of shortcut benefits into plain student networks during training and achieves comparable accuracy to networks with shortcuts. Experimental results demonstrate the effectiveness of the proposed method on benchmark datasets, as well as its superiority over traditional teacher-student frameworks. Additionally, the paper validates the compatibility of the proposed method with model pruning and knowledge distillation techniques.