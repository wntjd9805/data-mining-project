This paper introduces the concept of weight correlation (WC) in deep neural networks (DNNs) and explores its correlation with the generalization ability of networks. The authors provide evidence that WC positively correlates with the generalization error (GE), which reflects the accuracy of the learning algorithm in predicting unseen data. The paper then presents three major contributions: studying the incorporation of WC into the PAC Bayesian framework to estimate GE, demonstrating the monotonicity of the lifted PAC Bayesian bound with respect to WC, and proposing a novel regularization term that enhances the training process and reduces GE. Experimental results validate the effectiveness of the WC concept in improving the generalization ability of neural networks.