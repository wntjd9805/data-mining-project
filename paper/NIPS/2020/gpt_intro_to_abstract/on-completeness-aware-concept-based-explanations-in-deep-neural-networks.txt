The lack of explainability in deep neural networks (DNNs) limits their potential for real-world impact. Current methods for explaining DNNs focus on quantifying the importance of input features for each prediction, but they do not provide a global understanding of how the model reasons. In contrast, humans use "concept-based thinking" to group similar examples systematically. This paper aims to develop "concept-based explanations" that characterize the global behavior of DNNs in a way that humans can understand. The paper proposes a completeness score to measure the sufficiency of concept-based explanations and introduces a novel algorithm for concept discovery. Additionally, the paper proposes a score called ConceptSHAP for quantifying concept attributions. Experimental results demonstrate the effectiveness of the proposed methods in retrieving concepts and providing insights into the behavior of DNN models.