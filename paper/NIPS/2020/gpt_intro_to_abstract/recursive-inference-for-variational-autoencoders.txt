Accurately modeling complex generative processes for high dimensional data is essential in deep learning. The Variational Autoencoder (VAE) has been proven effective in this task, with the ability to interpret and control latent variables. However, VAE's amortized inference often results in inaccurate posterior approximation compared to instance-wise variational optimization. To address this drawback, recent semi-amortized approaches have been proposed, but they are computationally expensive and sensitive to parameter choices. In this paper, we propose a recursive estimation algorithm for a mixture encoder model to improve posterior approximation. Our method is more effective, less susceptible to issues like mixture collapsing, and achieves higher test data likelihood compared to existing approaches. Furthermore, the inference in our approach is faster, requiring only a single feed-forward pass through the mixture inference network.