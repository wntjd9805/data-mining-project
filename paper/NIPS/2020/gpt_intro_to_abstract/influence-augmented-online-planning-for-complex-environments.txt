In this paper, we address the problem of online planning in a complex, partially observable environment with multiple agents. Traditional online planning approaches for Partially Observable Markov Decision Processes (POMDPs) rely on sample-based planners like POMCP, which require fast simulators for Monte Carlo simulations. However, complex real-world scenarios often have computationally demanding simulators, preventing existing planners from being practical. To overcome this limitation, we propose the use of an approximate learned model that replaces less important parts of the environment. We build on the multi-agent decision making literature and introduce influence-augmented online planning, which transforms a global simulator into a faster influence-augmented local simulator. The local simulator captures the influence of external factors by predicting only the subset of them that directly affect the local factors. We learn the influence predictor using supervised learning methods offline. We demonstrate through planning experiments that our approach achieves improved performance in terms of planning time and effectiveness compared to traditional online planning with a global simulator. Furthermore, we find that accurate influence prediction is crucial for good performance in tightly coupled planning problems.