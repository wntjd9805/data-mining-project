Efficient exploration is a significant challenge in reinforcement learning, and recent works have provided near-optimal algorithms for single pre-defined tasks with a well-defined reward function. However, many real-world applications lack a pre-specified reward function, leading to a need for efficient exploration and learning of potentially conflicting tasks. In this paper, we present the task-agnostic RL paradigm where the agent collects trajectories from an MDP without a pre-specified reward function during exploration. We propose an efficient algorithm, UCBZERO, that explores the MDP without a pre-specified reward function and finds near-optimal policies for multiple tasks simultaneously. Our algorithm achieves a sample complexity of ˜O(log(N )H 5SA/ε2) and is complemented by a near-matching lower bound of Ω(log(N )H 2SA/ε2), demonstrating its near-optimality. Additionally, we investigate the properties of UCBZERO and provide a unified view of the task-agnostic and task-specific settings.