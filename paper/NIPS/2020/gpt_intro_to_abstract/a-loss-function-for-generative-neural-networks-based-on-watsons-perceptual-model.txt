Variational Autoencoders (VAEs) are generative neural networks that learn a probability distribution over X from training data. They generate new samples by drawing a latent variable z from a distribution and using z to sample x from a conditional decoder distribution. The choice of loss function is crucial for the generative model, and traditional loss metrics like the squared loss perform poorly in modeling human perception of image similarity. Perceptual loss functions based on deep neural networks have shown promising results, but using features from networks pre-trained for image classification may be problematic. In this work, we introduce a loss function based on Watson's visual perception model, optimized for image generation, resulting in imagery with less blur and fewer artifacts compared to alternative approaches.