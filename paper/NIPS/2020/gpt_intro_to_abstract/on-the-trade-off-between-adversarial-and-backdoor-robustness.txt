Deep neural networks (DNNs) have shown impressive performance in various domains, but they are vulnerable to adversarial and backdoor attacks. Adversarial attacks aim to deceive the model with inputs that are almost identical to regular examples, while backdoor attacks involve manipulating training data to cause the model to behave incorrectly when triggered. Existing defense methods focus on either adversarial or backdoor attacks separately, without investigating the interactions between the two. In this paper, we conduct experiments and find that there is a trade-off between the robustness of a network to adversarial examples and its vulnerability to backdoor attacks. This trade-off indicates that studying and defending against one type of attack at a time is risky, as it may create a false sense of security. We also show that this trade-off can be exploited to create more concealed backdoor attacks and render existing defenses ineffective. However, it does strengthen certain backdoor defenses. Our findings have implications for combining adversarial and backdoor defenses and suggest the need for joint attack/defense strategies in the future.