Kernel methods offer non-parametric extensions of linear models in machine learning and statistics by mapping data to a high dimensional feature space. This approach, which is based on the theory of reproducing kernel Hilbert spaces, allows for the use of infinite dimensional models and provides a connection to Gaussian processes. However, implementing kernel methods on large datasets is challenging and often limited to problems of small to medium size. Various techniques, such as random features and the Nyström method, have been proposed to address the computational bottlenecks associated with kernel methods. This paper focuses on developing and testing large scale kernel methods that can efficiently handle billions of points. The authors employ a Nyström approach and derive a preconditioned gradient solver for kernel methods, incorporating ideas from optimization, numerical analysis, scientific computing, and high-performance computing. The implementation is extensively tested on a range of datasets and compared to other large scale kernel implementations, demonstrating its efficiency and accuracy. The code for the implementation is distributed as a library built on PyTorch. The paper is organized into sections that provide background information, detail the algorithmic solutions, and assess the practical advantages of the proposed approach.