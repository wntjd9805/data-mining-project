Stochastic gradient descent (SGD) and its variants are widely used for training deep models, but the scalability of these methods is limited when dealing with large datasets. Data-parallel SGD has shown promise for scalability on multi-GPU systems, but it introduces new challenges due to high communication costs. To address this issue, gradient compression and quantization have been used to reduce communication costs. However, existing quantization methods are either heuristically designed or predetermined. This paper proposes two adaptive methods, Adaptive Level Quantization (ALQ) and Adaptive Multiplier Quantization (AMQ), for quantizing gradients in data-parallel SGD. The methods aim to minimize excess variance and achieve optimal performance throughout the training process, even as the distribution of gradients changes. The paper also establishes upper bounds on excess variance and expected communication bits per iteration, and demonstrates improved validation accuracy on CIFAR-10 and ImageNet datasets.