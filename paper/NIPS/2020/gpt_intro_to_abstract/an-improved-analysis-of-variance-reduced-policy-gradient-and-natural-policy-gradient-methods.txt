Policy gradient (PG) methods are widely used in reinforcement learning (RL) to search for the optimal policy parameter that maximizes long-term return in Markov decision processes (MDPs). However, these methods often suffer from high variances and lack theoretical convergence guarantees. In this paper, we propose a general framework for analyzing the global convergence of PG methods and their variance-reduced variants. We establish the global convergence of a variance-reduced PG method and improve the convergence of natural PG (NPG) methods. Additionally, we introduce a new variance-reduced algorithm based on NPG and demonstrate its efficient sample complexity. Our improvements are based on previous analyses and rely on the assumption that the Fisher information matrix induced by the policy parametrization is positive definite.