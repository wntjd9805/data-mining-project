Reliable machine learning systems often struggle to perform well on test datasets that differ from the training dataset. Various approaches have been proposed to adapt these systems to new domains, but theoretical understanding of these algorithms is limited. In this paper, we propose to study a specific structured domain shift where certain "spurious" features are correlated with the label in the source domain, but independent of the label in the unlabeled target data. We prove that self-training on unlabeled target data can avoid using these spurious features in certain settings, and our analysis is consistent with recent empirical results. We provide theoretical results for two popular algorithms and show that self-training can converge to a solution that eliminates usage of the spurious feature. Our simulations on real-world datasets further validate our theory's insights.