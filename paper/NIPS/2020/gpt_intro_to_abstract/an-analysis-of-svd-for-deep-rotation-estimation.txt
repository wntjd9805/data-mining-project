This paper explores the ideal representation for predicting 3D rotations in a deep learning framework. The authors present a systematic study on estimating rotations in neural networks and identify that the classic technique of SVD orthogonalization is well-suited for this task. They discuss the importance of 3D rotations in various applications and highlight the lack of a universally effective representation. They propose symmetric orthogonalization via SVD as an alternative and provide a comprehensive study of its viability in estimating rotations in deep neural networks. The paper includes a theoretical analysis comparing SVD orthogonalization to the Gram-Schmidt procedure and an extensive quantitative evaluation across different application environments. The results show that SVD orthogonalization achieves state-of-the-art performance and is the best performing method in both supervised and unsupervised settings, making it an important contribution to benchmark future research.