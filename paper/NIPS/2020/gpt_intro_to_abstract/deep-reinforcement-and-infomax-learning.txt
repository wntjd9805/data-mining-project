In this paper, we explore the benefits of incorporating model-based learning in model-free agents in reinforcement learning. We propose a self-supervised objective derived from variants of Deep InfoMax to learn representations with model-like properties. We assess the properties of these representations in various environments and demonstrate that augmenting a standard RL agent with our contrastive objective leads to improved performance in transfer and lifelong learning problems. Our contributions include the introduction of a simple auxiliary objective, experiments evaluating the objective's effectiveness, and demonstrating the improved adaptation and overall performance in the Procgen suite.