The use of non-uniform sampling in deep reinforcement learning, specifically prioritized experience replay (PER), has shown to accelerate learning by focusing on high error transitions. However, the lack of a theoretical foundation for PER motivates the analysis in this paper. The authors develop an understanding of the benefits of non-uniform sampling and propose modifications to PER that improve its performance. The main theoretical contribution explores the relationship between the expected gradient of a loss function minimized on data sampled with non-uniform probability and another loss function minimized on uniformly sampled data. This relationship allows for the transformation of any loss function into a prioritized sampling scheme and vice versa. The authors also introduce Loss-Adjusted Prioritized (LAP) experience replay and its uniformly sampled loss equivalent, Prioritized Approximation Loss (PAL), which simplify and improve the empirical performance of the algorithm. Experimental results on MuJoCo environments and Atari games show that both LAP and PAL outperform the vanilla algorithms they modify.