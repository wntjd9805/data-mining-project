Neural Networks with binary weights and activations have shown to be computationally efficient, with reported speed-ups compared to floating point computations. There is increasing hardware support for binary operations, including matrix multiplication instructions in NVIDIA cards and specialized projects on spike-like computation. Binarized networks have demonstrated comparable performance to real-valued baselines, and the use of good training methods can further improve their performance. However, a challenge with binary networks is the computation of unit outputs using sign activations, which makes common gradient descent methods inapplicable. This paper focuses on stochastic Binary Networks (SBNs) as a more sound approach, introducing injected noises in front of all sign activations. The paper addresses the problem of estimating gradients in SBNs, particularly in handling deep dependencies through binary activations. The paper introduces the Path Sample-Analytic (PSA) method, a biased stochastic estimator that approximates the expectation of the stochastic gradient by explicitly computing summations along multiple paths in the network. The paper also introduces the Straight-Through (ST) method, which provides theoretical justification for straight-through methods in deep models within the SBN framework. Both methods show promising results in the learning of deep convolutional models, offering more stable and well-controlled training compared to previous techniques.