Boosting is a popular technique in machine learning that enhances the accuracy of weak learning rules to create stronger ones. It has been extensively studied in both the PAC and agnostic settings, as well as in online prediction. This paper focuses on agnostic boosting in the online setting, where there is no restriction on the input sequence and it can be chosen adversarially. The motivation behind studying online agnostic boosting includes its connections to differential privacy and online learning, as well as its applications in time series prediction and online control. The paper presents a general framework for boosting that applies to different learning settings, and introduces an agnostic online boosting algorithm with an expected regret guarantee. The main result of the paper is a theorem that provides a bound on the regret of the algorithm based on the advantage and regret of the weak online learners it has access to. The paper concludes by discussing the structure of the rest of the paper and the contributions it makes to the field.