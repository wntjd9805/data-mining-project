Intelligent edge devices, such as mobile phones and IoT devices, play a significant role in our daily lives by collecting data through sensors while maintaining privacy. However, on-device learning, which allows for continuous fine-tuning of pre-trained models with newly collected data, presents challenges due to memory and energy constraints. This paper introduces Tiny-Transfer-Learning (TinyTL), a method that addresses these challenges by freezing the weights of pre-trained feature extractors and only updating biases to reduce memory footprint. Additionally, the paper introduces a memory-efficient bias module called the lite residual module. Experimental results demonstrate that TinyTL reduces training memory footprint by up to 12.9x without sacrificing accuracy.