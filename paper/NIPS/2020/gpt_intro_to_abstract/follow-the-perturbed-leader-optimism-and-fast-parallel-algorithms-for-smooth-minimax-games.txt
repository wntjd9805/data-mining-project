In this work, we introduce the problem of online learning, where a learner must minimize cumulative loss by choosing a sequence of actions. We explore the two main algorithms for regret minimization in online learning: Follow the Regularized Leader (FTRL) and Follow the Perturbed Leader (FTPL). While both algorithms achieve optimal worst case regret, they differ in computational aspects. We focus on FTPL and propose a variant called Optimistic FTPL (OFTPL) that achieves better regret bounds while still guaranteeing optimal worst case regret. We demonstrate the usefulness of OFTPL in solving minimax games, where it outperforms existing algorithms in terms of convergence rates and parallelizability. Our algorithm is particularly suitable for large-scale machine learning applications like GAN training.