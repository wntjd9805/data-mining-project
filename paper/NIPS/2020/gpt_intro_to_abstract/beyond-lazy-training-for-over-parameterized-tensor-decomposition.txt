This paper explores the concept of over-parametrization in non-convex optimization, specifically in the context of neural networks and tensor decomposition. The goal is to understand whether it is possible to go beyond the traditional "lazy training" regime and converge to the global minimum of the objective function with a smaller degree of over-parametrization. The paper builds upon the mean-field analysis framework to analyze the problem, leveraging the particular structure of tensor decomposition. The main contribution of the paper is the development of a modified version of gradient descent that converges to the global minimum with nearly dimension-independent over-parametrization. This improvement reduces the required over-parametrization compared to existing methods. The analysis also suggests potential implications for the analysis of two-layer networks.