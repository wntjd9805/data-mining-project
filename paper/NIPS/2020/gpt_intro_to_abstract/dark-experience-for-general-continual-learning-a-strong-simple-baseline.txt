Practical applications of neural networks often require the ability to acquire new knowledge on-the-fly, without forgetting previously learned information. This phenomenon, known as catastrophic forgetting, can result in a sudden performance deterioration on old data. Continual Learning (CL) methods aim to alleviate catastrophic forgetting while minimizing computational costs and memory usage. In this paper, we review existing evaluation settings for CL methods and compare them extensively. Surprisingly, we find that a simple Experience Replay baseline consistently outperforms cutting-edge methods. However, many of these methods are not suitable for real-world applications where memory is limited and tasks overlap. To address this, we propose a novel CL baseline called Dark Experience Replay (DER) that relies on distilling past experiences using dark knowledge. DER satisfies the requirements of General Continual Learning (GCL) and outperforms current state-of-the-art approaches in standard CL experiments. Additionally, we introduce a new GCL setting called MNIST-360, which demonstrates DER's effectiveness in handling sudden and gradual changes in data distribution.