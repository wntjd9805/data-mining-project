Model-free reinforcement learning (RL) combined with powerful function approximators has achieved remarkable success in games and control tasks. However, a key limitation is their sample complexity, often requiring millions or even billions of samples to learn skills or strategies. Humans, on the other hand, can learn multiple skills from far fewer samples. This paper addresses the challenge of enabling RL agents to learn efficiently across multiple tasks by reusing information from mistakes and exploring the use of generalized hindsight. The authors propose Generalized Hindsight, which performs hindsight on a family of reward functions instead of sparse goals. They introduce an Approximate Inverse Reinforcement Learning (IRL) Relabeling algorithm, called AIR, that selects the best task for relabeling and demonstrate its effectiveness in multi-task RL on various control tasks.