Bayesian optimization (BO) is a powerful machine-learning based method for optimizing expensive black-box functions. While BO has been successfully applied in various applications, its performance deteriorates as the dimensionality of the search space increases. To address this challenge, different approaches based on random embeddings or latent space learning have been proposed. However, these methods may not easily handle complex parameter spaces or incorporate domain knowledge. In this paper, we introduce a novel high-dimensional geometry-aware BO framework (HD-GaBO) that leverages the intrinsic geometry of the parameter space. Our approach utilizes a geometry-aware surrogate model to map the parameter space onto a latent space and represent the objective function in this latent space. The next query point is selected using geometry-aware optimization methods on the low-dimensional Riemannian manifold. We demonstrate the effectiveness of HD-GaBO on benchmark functions and discuss potential applications.