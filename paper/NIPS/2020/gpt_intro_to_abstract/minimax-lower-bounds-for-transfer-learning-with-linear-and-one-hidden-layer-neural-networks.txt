Deep learning approaches have gained significant success in various applications, but they often require large amounts of labeled data for accurate model training. Transfer learning has emerged as a method to overcome this challenge by leveraging knowledge from a source domain with ample training data to improve learning in a related but different target domain. However, the benefits and limitations of transfer learning remain unclear, along with several open challenges. This paper aims to address these questions by developing statistical minimax lower bounds for transfer learning in regression problems using linear and one-hidden layer neural network models. The analysis reveals that the best achievable accuracy depends on the transfer distance between the source and target tasks, with a focus on the number of labeled training data available from the target domain. The study also highlights the role of both the source and target domains in improving target training accuracy, particularly when the transfer distance is small. Experimental investigations on real datasets and synthetic simulations further validate the findings.