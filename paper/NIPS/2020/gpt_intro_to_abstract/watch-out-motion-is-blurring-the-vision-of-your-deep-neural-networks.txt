This paper investigates the vulnerability of deep neural networks (DNNs) to motion blur-based adversarial attacks. While previous works have focused on additive noise perturbations to mislead DNNs, motion blur has not been thoroughly explored. The authors propose a new type of attack called motion-based adversarial blur attack (ABBA), which generates visually natural and plausible motion-blurred adversarial examples. They formulate a kernel-prediction-based attack and introduce a saliency-regularized adversarial kernel prediction to produce more realistic blur effects. Comprehensive evaluations show that ABBA outperforms noise-based attacks and common blur techniques in terms of attack success rate and transferability. The proposed method also demonstrates higher penetration capability against deblur mechanisms.