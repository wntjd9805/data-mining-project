Graph Convolutional Networks (GCNs) have shown great success in various domains, but processing large graphs with GCNs remains challenging due to computational overheads and memory limitations, especially on GPUs. To address this, sampling techniques have been used to reduce memory demand. However, sampling on GPUs performs poorly due to random memory access. This paper introduces LAZYGCN, a framework that leverages the capabilities of heterogeneous systems to train large-scale GCNs. LAZYGCN performs periodic sampling on CPU and recycles already-sampled nodes on GPU, reducing both sampling and data transfer overheads. The authors provide a theoretical analysis of the algorithm's convergence speed and conduct extensive experiments to validate its effectiveness. LAZYGCN significantly reduces the number of sampling steps and achieves superior speedup without compromising accuracy, and the proposed techniques can be applied to any heterogeneous or distributed setting.