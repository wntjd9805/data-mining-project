Humans have the ability to learn and integrate new concepts by combining simpler ones and reusing them in novel ways. This ability is of interest in machine learning systems, specifically in the context of generative modeling. Previous approaches have attempted to enable compositionality through factors of variation or spatial decomposition of images, but these approaches have limitations and different underlying implementations. In this paper, we propose implementing compositionality via energy based models (EBMs) where factors of variation and object slots are defined by energy functions. These energy functions are used to generate factors through a Markov Chain Monte Carlo (MCMC) sampling process. We introduce logical composition operators to combine multiple energy functions, enabling flexibility and recursive combination of concepts. This approach allows for continual learning, generation of new concepts, and finely controllable image generation. Our contributions include demonstrating the generation of plausible natural images through the composition of energy-based models and proposing a principled approach to combining independent trained energy models based on logical operators. We also show that our approach allows for extrapolation to new concept combinations, continual incorporation of new visual concepts, and compositional inference of concept properties.