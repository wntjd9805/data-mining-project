Reinforcement learning (RL) has been recognized as a key approach to achieving artificial general intelligence. While RL has demonstrated impressive empirical successes in various real-world applications, most of these applications rely on model-free approaches that require a large number of samples. This has led to a growing interest in model-based RL, which is believed to be more sample efficient. However, there are still unanswered questions regarding model-based RL, particularly in the presence of function approximators. In this paper, we focus on the sample complexity of model-based RL in the feature-based setting. We explore the plug-in solver approach, which involves building an empirical model and planning in this model using a solver. We show that the plug-in solver approach achieves near-optimal sample complexity in the feature-based setting, under certain conditions. Our findings contribute to a better understanding of model-based RL and have implications for broader settings.