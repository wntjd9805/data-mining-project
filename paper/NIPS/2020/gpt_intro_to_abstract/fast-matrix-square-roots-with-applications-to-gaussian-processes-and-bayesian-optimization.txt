High-dimensional Gaussian distributions are commonly encountered in machine learning, particularly in Bayesian modeling. However, the computational complexity and memory requirements of inference for these models have posed challenges. A recent research direction has focused on using matrix-vector multiplication (MVM) approaches to reformulate covariance matrix operations. In this paper, we propose an MVM method for efficiently computing KÂ±1/2b, a frequently occurring operation in Gaussian process models and inverse problems. We introduce a novel approach that expresses the matrix square root as a sum of shifted matrix inverses, and leverage a modified version of the MINRES algorithm for efficient computation. We demonstrate the scalability of our method and its applicability to various tasks, such as variational Gaussian processes, sampling from Gaussian process posteriors, and image reconstruction. Code examples for the GPyTorch framework are provided.