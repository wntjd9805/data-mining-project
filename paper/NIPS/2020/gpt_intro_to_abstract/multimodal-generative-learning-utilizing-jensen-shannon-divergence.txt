Replicating human information processing and learning abilities in machines has long been a goal in machine learning. The use of multiple information sources has the potential to improve learning and create more generalizable representations. However, dealing with complex relationships between different sources poses challenges, as well as the issue of missing modalities. The excessive labeling of multiple data types is expensive and limits the applications of fully-supervised approaches. Self-supervised generative models offer a promising solution, as they can capture the joint distribution of multiple modalities and support missing modalities without additional labeling costs. This paper introduces a novel probabilistic, generative, and self-supervised multi-modal model that integrates information from different modalities, handles missing data, and makes no assumptions about the nature of the data. The model is based on the Variational Bayesian Inference framework and utilizes the multimodal Jensen-Shannon divergence objective. The paper also demonstrates the advantage of modality-specific subspaces and performs experiments on datasets with more than two modalities, showcasing the scalability and effectiveness of the proposed method.