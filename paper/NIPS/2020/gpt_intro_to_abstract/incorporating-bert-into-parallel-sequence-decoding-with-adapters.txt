Pre-trained language models have gained significant attention in the natural language processing community due to their success in various understanding tasks. However, incorporating these models into natural language generation tasks poses several challenges. This paper proposes a new paradigm for incorporating BERT into text generation tasks, utilizing adapter layers to adapt BERT to new tasks and achieve parameter efficiency. The proposed framework, based on a parallel sequence decoding algorithm, achieves improved performance and faster decoding speed compared to autoregressive baselines. The flexibility of the framework allows for easy adaptation to different tasks. Experimental results demonstrate the effectiveness of the proposed framework in neural machine translation tasks.