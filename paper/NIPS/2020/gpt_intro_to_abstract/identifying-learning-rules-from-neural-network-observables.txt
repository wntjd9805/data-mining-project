This paper addresses the problem of identifying the underlying plasticity rules that govern the process by which signals from the environment are transduced into synaptic updates in the brain. The authors explore different learning rules and propose the use of experimental observables to distinguish between them. They use virtual experiments with artificial neural networks to simulate idealized neuroscience experiments and demonstrate that the learning rules can be reliably separated based on aggregate statistics of weights, activations, and instantaneous changes of post-synaptic activity. They also investigate the effects of undersampling and measurement noise on the reliability of these measurements. Overall, they find that temporally spaced measurements and aggregated statistics are more robust to noise and undersampling, providing valuable insights for future research in computational neuroscience.