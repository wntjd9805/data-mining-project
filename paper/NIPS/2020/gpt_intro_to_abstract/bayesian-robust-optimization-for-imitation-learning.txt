Imitation learning is a method of training an agent without explicitly providing a reward function by using demonstrations. However, a challenge in imitation learning is determining the agent's actions when encountering states outside the provided demonstrations. Inverse reinforcement learning (IRL) is an approach that aims to recover the reward function of the demonstrator. While IRL allows for generalization to new states, uncertainty over the true reward function can lead to negative consequences if the agent learns an incorrect policy. This paper proposes that an imitation learning agent should learn a policy that is robust in the face of uncertainty, while also effectively balancing the trade-off between epistemic risk and expected return. The paper introduces Bayesian Robust Optimization for Imitation Learning (BROIL), the first framework that directly optimizes a policy considering both the expected return and conditional value at risk under an uncertain reward function. The paper also presents an efficient linear programming formulation to compute the optimal BROIL policy and compares two instantiations of BROIL. Experimental results demonstrate that BROIL outperforms existing risk-sensitive and risk-neutral IRL algorithms, providing a richer set of solutions that appropriately balance performance and risk based on different levels of risk aversion.