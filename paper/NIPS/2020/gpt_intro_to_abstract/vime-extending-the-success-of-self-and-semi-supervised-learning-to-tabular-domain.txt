Tremendous successes have been achieved in deep learning models for tasks such as image classification, object detection, and language translation through supervised learning on large labeled datasets. However, collecting large labeled datasets is expensive and often impossible in certain domains. In these cases, there is a wealth of unlabeled data available, presenting opportunities for self- and semi-supervised learning algorithms. Unfortunately, existing algorithms are not effective for tabular data, as they heavily rely on the spatial or semantic structure of image or language data. In this paper, we propose novel self- and semi-supervised learning frameworks for tabular data. We introduce a novel pretext task and a tabular data augmentation scheme, and combine these ideas to develop the VIME framework. Experimental results show state-of-the-art performances on various tabular datasets with limited labeled samples.