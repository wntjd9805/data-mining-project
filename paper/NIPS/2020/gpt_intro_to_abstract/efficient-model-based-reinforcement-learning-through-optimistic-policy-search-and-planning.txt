Model-Based Reinforcement Learning (MBRL) algorithms have shown impressive sample efficiency in solving challenging high-dimensional tasks by collecting data with a policy and fitting a model to the data. One key feature of successful MBRL algorithms is the explicit consideration of both aleatoric uncertainty (system noise) and epistemic uncertainty (data scarcity) when learning a model. However, current algorithms optimize the policy by marginalizing over both uncertainties, leading to greedy exploitation and potential local minima. In this paper, we propose a novel optimistic MBRL algorithm called Hallucinated-UCRL (H-UCRL) that incorporates optimism by reparameterizing the model-space using a mean/epistemic variance decomposition. We provide theoretical analysis and sublinear regret bounds for H-UCRL, and evaluate its performance in high-dimensional continuous control tasks. Our approach is the first to successfully implement optimistic exploration in deep MBRL.