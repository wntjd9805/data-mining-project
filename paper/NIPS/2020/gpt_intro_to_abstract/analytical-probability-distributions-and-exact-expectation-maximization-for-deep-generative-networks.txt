Deep Generative Networks (DGNs) are state-of-the-art methods in machine learning and have various applications, including anomaly detection and data generation. Training DGNs can be done through an adversarial network or by modeling latent and observed variables as random variables. Variational Autoencoders (VAEs) are popular for likelihood-based DGN training, but they only offer an approximate solution. This paper aims to compute the exact analytical posterior and marginal distributions of DGNs with continuous piecewise affine (CPA) nonlinearities, allowing for exact inference and gradient-free training. The paper presents the main contributions, including obtaining analytical distributions, leveraging moments for training, comparing these methods with standard VAE training, and providing reproducible code for experiments.