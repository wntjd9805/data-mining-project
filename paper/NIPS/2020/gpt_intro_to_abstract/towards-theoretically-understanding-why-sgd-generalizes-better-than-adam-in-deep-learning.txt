Stochastic gradient descent (SGD) has become popular for training deep neural networks due to its simplicity and effectiveness. However, it can suffer from unsatisfactory convergence performance, especially for ill-conditioned problems. To address this issue, adaptive gradient algorithms have been developed that adjust the learning rate for each gradient coordinate based on the curvature of the objective function. These algorithms, including ADAM, have achieved faster convergence speed than SGD. However, despite their faster convergence, adaptive gradient algorithms often exhibit worse generalization performance than SGD. This paper provides a new viewpoint for understanding the generalization performance gap between ADAM and SGD. By formulating ADAM and SGD as LÃ©vy-driven stochastic differential equations and analyzing their escaping behaviors at local minima, it is found that SGD is more locally unstable and more likely to converge to flatter minima, which often have better generalization performance. The results also show that ADAM has smaller Radon measures and diminishes the anisotropic structure in gradient noise, leading to worse generalization performance than SGD.