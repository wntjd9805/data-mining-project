The vulnerability of neural networks to small perturbations has been a challenge for safety-critical applications like autonomous driving. Adversarial learning has been a popular approach to address this issue, but it requires class labels for generating adversarial attacks. In this paper, we propose a contrastive self-supervised learning framework called Robust Contrastive Learning (RoCL) that trains an adversarially robust neural network without class labels. We generate instance-wise adversarial examples by confusing the instance-level identity of perturbed samples and maximize the similarity between clean samples and their adversarial counterparts using contrastive learning. We evaluate RoCL on benchmark datasets and achieve comparable accuracy to supervised adversarial learning methods. Additionally, we demonstrate the robustness of RoCL through transfer learning, achieving impressive performance. Overall, our contributions include a novel instance-wise adversarial perturbation method, an adversarial self-supervised learning method, and a robust representation that outperforms supervised approaches on unseen attacks and transfer learning.