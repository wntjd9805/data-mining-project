Machine learning systems are being deployed in high-stakes applications, but current training and evaluation methods do not meet the reliability requirements. This paper proposes optimizing the Conditional Value-at-Risk (CVaR) as a risk-averse criterion instead of the usual empirical risk minimization. While optimizing CVaR is well-established in the convex setting, it fails in stochastically optimizing it, especially on non-convex problems like training deep neural network models. To address this, the authors present a novel adaptive sampling algorithm called ADA-CVAR, which gradually adjusts its sampling distribution to focus on difficult examples. The algorithm allows the use of standard stochastic optimizers and comes with convergence guarantees and an efficient implementation. The performance of the algorithm is demonstrated in a set of experiments.