Applications of deep learning algorithms in the real world have generated interest in studying robust algorithms that can perform well with imbalanced datasets and unseen situations during testing. This paper focuses on label prior shift, where the training class distribution does not match the true class distribution, and proposes a rebalanced posterior approach to address this issue. The method does not require retraining and allows for a flexible trade-off between precision and recall. Additionally, the paper combines this approach with methods dealing with non-semantic likelihood shift and demonstrates its effectiveness on different datasets and neural network architectures in the tasks of classification and semantic segmentation. The contributions of the paper include a principled imbalance calibration algorithm, an efficient hyper-parameter search algorithm, improved performance on accuracy and mean IOU, and a unified approach to address label prior shift and non-semantic likelihood shift.