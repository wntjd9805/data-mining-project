The field of Deep Reinforcement Learning (DRL) has witnessed a surge in research on batch reinforcement learning, which focuses on sample-efficient learning from a given dataset without additional interactions with the environment. Batch RL offers the potential to leverage existing large datasets for improved sample efficiency and can be used in safety-critical systems where a partially trained policy cannot be deployed online. However, conventional Q-function based algorithms often struggle or even diverge when applied to batch RL. As a result, new algorithms are needed to obtain high-performing policies from batch data. In this paper, we propose a new algorithm called Best-Action Imitation Learning (BAIL) that strives for simplicity, performance, and stability in its value estimates. BAIL is computationally fast and conceptually straightforward, satisfying the principle of Occam's razor. We compare the performance of BAIL with other batch DRL algorithms on the Mujoco benchmark and demonstrate its superiority in terms of performance, speed, and simplicity. The contributions of this paper include the introduction of BAIL and the concept of the "upper envelope of data," as well as extensive experiments and publicly available code and datasets for reproducibility and benchmarking.