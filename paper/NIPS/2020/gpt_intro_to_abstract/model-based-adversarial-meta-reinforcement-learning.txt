Deep reinforcement learning (Deep RL) methods have achieved success in solving challenging tasks such as Go, Atari games, and robotic control. However, these methods often require a large number of interactions with the environment for training. Meta-reinforcement learning and multi-task reinforcement learning have been proposed to improve sample efficiency by exploiting the shared structure among tasks. Existing approaches assume that training and testing tasks are drawn from the same task distribution, which leads to performance degradation when there is a distribution shift. In this paper, we propose a non-distributional approach to meta-RL by formulating the problem as an adversarial minimax problem. We learn a shared dynamics model across tasks during training and adapt a policy for each new task during testing. We apply adversarial training to optimize the worst sub-optimality gap across all tasks. Our approach significantly outperforms existing methods in terms of worst-case performance, generalization to out-of-distribution tasks, and sample efficiency. We derive an efficient gradient estimator for task parameters and demonstrate its effectiveness on a set of continuous control benchmarks.