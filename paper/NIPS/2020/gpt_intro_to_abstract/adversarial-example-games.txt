Adversarial attacks on deep neural nets have exposed vulnerabilities in traditional machine learning systems. To develop robust models, it is crucial to improve our understanding of different attack strategies. While progress has been made in understanding adversarial attacks in permissive settings, there is still a gap between theory and practice in more demanding threat models. In this paper, we propose a theoretical framework for analyzing and understanding adversarial attacks in the Non-interactive blackBox (NoBox) setting, where the attacker has no direct access to the target classifier. We cast NoBox attacks as an adversarial example game and show that the Nash equilibrium leads to a distribution of adversarial examples effective against any classifier. We further demonstrate the effectiveness of our approach through empirical validation on standard benchmarks.