In this paper, the authors address the problem of contextual bandit learning with continuous action space. They propose a new algorithm called CATS that uses (Îµ)-greedy exploration with tree policy classes to efficiently choose actions. The algorithm operates in a fully online and oracle-efficient manner with prediction and update times scaling as log of the tree size. The authors also introduce CATS Off, an off-policy optimization version of CATS that can utilize logged data for training and selecting tree policies. The algorithms are implemented in Vowpal Wabbit and compared to baselines on real datasets, demonstrating their efficacy and efficiency. The paper concludes with a discussion of the advantages of the smoothing approach and the challenges of adapting the tree-based classifier for smoothing in contextual bandit learning.