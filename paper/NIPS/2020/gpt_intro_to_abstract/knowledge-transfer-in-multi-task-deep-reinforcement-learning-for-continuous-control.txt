The recent breakthrough of Deep Learning (DL) has greatly improved the performance of Reinforcement Learning (RL) in complex control tasks. While RL has been successful in discrete control tasks, training a single RL agent to perform multiple tasks remains challenging. Multi-task RL requires an agent to learn a single control policy that can perform well on multiple tasks, which is crucial for the development of Artificial General Intelligence (AGI). Existing approaches, such as Actor-Mimic and policy distillation, use knowledge transfer to consolidate control policies into a single one. However, these methods are designed for discrete control tasks and cannot be directly applied to multi-task RL for continuous control. In this paper, we propose a Knowledge Transfer based Multi-task Deep Reinforcement Learning framework (KTM-DRL) specifically designed for continuous control tasks. KTM-DRL leverages an offline knowledge transfer algorithm and hierarchical experience replay to enable a single RL agent to achieve expert-level performance on multiple tasks. Experimental results on MuJoCo benchmarks demonstrate the effectiveness of KTM-DRL, surpassing the state of the art and even outperforming an "ideal" solution using a single-task RL algorithm for multiple tasks.