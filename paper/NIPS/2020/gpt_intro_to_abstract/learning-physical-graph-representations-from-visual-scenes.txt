In this paper, we propose a new representation called a Physical Scene Graph (PSG) that aims to handle complex object shapes and textures, decompose scenes into their physical parts, support top-down inferential reasoning, and learn from real-world visual data. PSGs represent scenes as hierarchical graphs, with nodes corresponding to different levels of object groupings and edges representing the bonds between parts. We introduce a family of self-supervised neural network architectures, PSGNets, to estimate PSGs from visual inputs. These architectures incorporate recurrent connections, graph pooling, and graph vectorization operations to effectively combine high- and low-level visual information and generate physically meaningful scene representations. Experimental results demonstrate the superiority of PSGNets in scene segmentation tasks, especially for real-world images, and their ability to exploit object motion to improve segmentation. PSGNets also show good transferability to unseen objects and scenes, suggesting efficient learning of general scene properties from limited data. Additionally, we explore the potential of the latent PSG structure in object and attribute composition editing.