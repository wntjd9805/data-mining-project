Reinforcement learning (RL) is a popular approach in the field of computer science where an agent learns to select actions in order to accumulate rewards in a given state space. However, most theoretical results in RL have focused on the tabular setting, where the state and action spaces are finite and small. In practical applications, however, state and action spaces are often large and continuous. To address this, researchers have developed algorithmic principles and guarantees for RL in continuous spaces. In this paper, we contribute to this line of work by considering RL in episodic settings with metric spaces. We propose an analysis that scales with the zooming dimension of the instance instead of the covering dimension of the metric space. This analysis provides more adaptive guarantees and addresses an open problem in the field. Our results are based on a refined analysis of an existing algorithm and involve utilizing the clipped regret decomposition and a careful bookkeeping argument. Overall, our approach provides a novel perspective on RL in metric spaces and offers improved guarantees for practical applications.