Convolutional layers in neural networks are widely used in machine learning tasks, but simple stacks of convolutional layers alone are not effective in expanding the effective receptive field (ERF) needed for accurate predictions. Downsampling and fully-connected layers are commonly used to encode global information, but they are less effective in dense prediction tasks. To address this, advanced architectures have been proposed, but they often come with drawbacks such as information loss, model complexity, and increased computational cost. In this paper, we propose a novel autoregressive-moving-average (ARMA) layer that enables adaptive receptive field by introducing explicit interconnections among its output neurons. This ARMA layer can significantly expand ERF while maintaining spatial resolution, and it can be easily integrated into various network architectures. We address the computational complexity and instability challenges of ARMA networks by developing FFT-based algorithms and introducing a separable ARMA layer and re-parameterization mechanism. Experimental results demonstrate that ARMA networks outperform baseline models in dense prediction tasks, suggesting that our proposed ARMA layer is a valuable building block for improving performance in such tasks.