Deep Reinforcement Learning (RL) has shown remarkable success in various domains, such as game playing and robotic control. However, training complex tasks in Deep RL still requires a large amount of data. Moreover, training a single network that can generalize across all possible robotic manipulation tasks remains a challenge. In this paper, we explore multi-task RL as a means to enable skill sharing and generalization across diverse tasks. By training deep networks with multiple tasks jointly, agents can learn to share and re-use components, leading to improved sample efficiency. We address the optimization difficulties in multi-task RL using a soft modularization approach, where modules are automatically generated for different tasks without explicitly specifying the policy structure. Our experimental results demonstrate significant improvements in both sample efficiency and final performance, surpassing previous state-of-the-art multi-task policies. This highlights the effectiveness of soft modularization in enhancing generalization across different tasks in RL.