Differential equations are commonly used to describe the behavior of systems, but their solutions often require numerical approximation. Recently, neural ordinary differential equations (ODEs) with millions of learned parameters have been used for time series models and density models. However, these learned models can have complex dynamics, leading to slow numerical solving. This paper aims to explore how to learn dynamics that are faster to solve without drastically changing their predictions. By minimizing the norm of the total derivative of the solution trajectory, the authors propose a speed regularization technique to control the time required for solving the learned dynamics. The relationship between solver order and regularization order is investigated, and the tradeoff between speed and performance is characterized. Additionally, an extension to the JAX program transformation framework is provided to efficiently compute the required total derivatives. This work builds upon and generalizes the work of Finlay et al. (2020) in regularizing dynamics for density estimation models.