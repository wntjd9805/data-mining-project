This paper focuses on the problem of learning invertible transformations of complex probability distributions into simple ones. It explores the use of deep neural networks as a framework for implementing these transformations and discusses the challenges posed by the computation of the Jacobian term. The authors propose an efficient optimization approach that leverages the Riemannian geometry of matrix spaces to compute parameter updates in terms of the relative gradient. This approach allows for the optimization of the Jacobian term in neural networks with arbitrary structures, resulting in a quadratic computational cost. The paper provides a detailed review of maximum likelihood estimation, backpropagation, and the Jacobian term, and presents empirical results to demonstrate the computational speedup achieved by their method.