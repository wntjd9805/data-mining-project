In this paper, we address the problem of aggregating probability distributions in statistics and machine learning. Specifically, we focus on finding the Wasserstein barycenter, which is the distribution that minimizes the sum of distances to individual input distributions. While existing methods typically use a discrete approximation of the barycenter, we propose a stochastic algorithm that computes a continuous approximation without discretizing its support. Our method relies on a novel dual formulation of the regularized Wasserstein barycenter problem and solves it using stochastic gradient descent. By allowing sample access to the input distributions, our algorithm produces the first continuous approximation of the barycenter. We evaluate the effectiveness of our approach on synthesized examples and real-world data for subset posterior aggregation.