Deep Neural Networks (DNNs) have shown superior performance compared to traditional Machine Learning techniques in various domains. However, the training times for DNNs have increased significantly with the growing complexity of models and datasets, limiting innovation in model training and deployment. Reduced-precision training has emerged as a technique to boost performance and power efficiency, with 16-bit and 8-bit formats providing significant improvements. While 4-bit inference has been successful, no studies have demonstrated deep learning model convergence using 4-bit representations for all tensors. This paper introduces a novel end-to-end solution that utilizes 4-bit floating point format for the majority of computations during DNN training, addressing challenges such as rounding errors, precision, and dynamic range. The solution enables model convergence with minimal accuracy loss and integrates state-of-the-art 4-bit inference methodologies. The design is guided by deep learning hardware expertise and compiler-driven optimizations.