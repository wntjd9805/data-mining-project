In modern data-intensive applications, distributed computing has become essential for machine learning due to the growing size of training datasets. However, distributed frameworks face challenges such as heavy communication overheads and the susceptibility of worker machines to errors or attacks. This paper proposes COMRADE, a distributed approximate Newton-type algorithm that addresses both communication efficiency and Byzantine-robustness. COMRADE reduces communication by only sending a product of the inverse of the local Hessian and the local gradient, and employs a thresholding policy to filter out Byzantine workers. The paper provides theoretical analysis of the proposed algorithm's convergence rate and experimental validation on benchmark datasets, demonstrating its resilience against Byzantine workers and improved classification accuracy compared to existing algorithms.