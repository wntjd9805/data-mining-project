In this paper, we address the problem of sparse regression in the context of transferring knowledge in high-dimensional environments. We propose a method called Transfer Lasso, which incorporates a difference regularization term into the ordinary Lasso regularization. This difference regularization term allows for sparse updates, meaning that only a small number of parameters are changed even when the data changes only slightly. We demonstrate the effectiveness of our method through theoretical justifications and empirical results, highlighting its ability to transfer knowledge under stationary environments and discard outdated knowledge when concept drift occurs. The remainder of the paper is organized as follows: in Section 2, we describe the proposed method; in Section 3, we present some theoretical properties; in Section 4, we illustrate empirical results; and we conclude in Section 5.