Neural computation in living systems differs greatly from digital electronics in terms of precision and reliability. Biological synapses require only 5 bits to store, while computer synapses use 32 bits. This discrepancy raises questions about why the brain can learn stably while deep learning struggles with hyperparameters. Scaling artificial networks for different hardware presents challenges due to limited understanding of how precision affects learning. In this paper, we propose a multiplicative learning rule and benchmark a multiplicative version of the Adam optimiser called Madam. Empirical results suggest that Madam does not require learning rate tuning and can train neural networks with low bit width synapses stored in a logarithmic number system. We also highlight that multiplicative weight updates align with certain aspects of neuroanatomy, making them a promising approach for the next generation of neural hardware.