Boosting is a popular technique in machine learning for improving the performance of weak learners by combining them to create a stronger classifier. While boosting has shown great success in classifying structured or tabular data, it tends to perform poorly on problems involving low-level features and complex decision boundaries, such as image classification. In this paper, we propose a generalized framework for boosting that allows for more complex forms of aggregation in the feature representation space, rather than the traditional label space. We explore the use of weak feature transformers, including neural networks, and demonstrate that our framework outperforms existing layer-by-layer training techniques. Additionally, we provide risk bounds for models learned using our framework, based on a weak learning condition on feature transformer classes. This modular approach allows us to obtain tight risk bounds for boosting by relying on the best-known generalization bounds for weak transformation classes.