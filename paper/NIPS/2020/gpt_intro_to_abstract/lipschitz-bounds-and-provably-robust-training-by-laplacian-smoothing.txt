Sensitivity to adversarial perturbations is a major limitation of data-driven models, hindering their deployment in safety-critical applications. Previous works have focused on improving adversarial robustness through Lipschitz regularization or constraint. However, there is a lack of understanding about the limitations of this approach and a general framework for training models that are provably robust to adversarial perturbations. This paper addresses this need by formulating the problem of adversarially robust learning as a loss minimization problem with a Lipschitz constraint. The authors propose a novel approach that bridges the theory of provably robust learning with classic theories of elliptic operators, partial differential equations, and numerical integration. The technical contributions of this paper include the characterization of fundamental robustness bounds for machine learning algorithms and the design of provably robust training schemes. The authors show that under certain assumptions, the problem of designing a data-driven map with a desired bound on the Lipschitz constant has a unique minimizer, which can be obtained by solving a Poisson partial differential equation. They also reveal a fundamental tradeoff between the robustness and performance of a data-driven map, and provide a training scheme for improving the robustness of a minimizer with a margin on the loss. Overall, this paper presents important insights and techniques for enhancing adversarial robustness in data-driven models.