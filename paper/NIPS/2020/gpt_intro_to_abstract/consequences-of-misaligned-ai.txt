The story of King Midas serves as a cautionary tale about the dangers of incomplete desires and the negative consequences that can arise from them. Similarly, designers of modern autonomous systems face the challenge of aligning specified goals with optimization algorithms to produce value. However, the misalignment between what can be specified and what is desired has already caused significant harms, particularly in content recommendation systems that optimize engagement metrics at the expense of values like truthfulness and cohesion. Improved optimization techniques without advancements in avoiding or correcting specification errors will only amplify the harms of AI systems. This paper presents a theoretical model of the principal-agent value alignment problem in AI and identifies conditions under which any misalignment is costly. The findings suggest the importance of managing the gap between complex qualitative goals and their representation in autonomous systems.