Modeling cross-domain relations, such as those between images and texts, has various applications in real-world tasks like image captioning. In these tasks, there are many-to-many relationships where a data point in one domain can correspond to multiple points in the other domain. Specifically, in image captioning, there can be multiple plausible sentences describing the semantics of an image. To address this, recent research has explored generating multiple captions conditioned on an image using variational frameworks. However, the current models have limited diversity as they rely only on paired annotations. In this paper, we propose a novel factorized latent variable model called COS-CVAE, which encodes object and contextual information for image-text pairs. Our COS-CVAE framework leverages contextual similarities and diverse contextual descriptions to capture the multimodality of the conditional image and text distributions. We demonstrate the benefits of our approach on the COCO dataset, showing improved accuracy and diversity in caption generation.