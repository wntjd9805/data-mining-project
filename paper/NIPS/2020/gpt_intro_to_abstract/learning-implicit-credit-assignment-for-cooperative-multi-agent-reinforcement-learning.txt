This paper introduces LICA, a policy-based algorithm for learning implicit credit assignment in multi-agent cooperative games. The authors address the limitations of previous approaches by extending the concept of value mixing to policy mixing, where a centralized critic acts as a hypernetwork to mix individual action vectors into the joint action value estimate. They also propose adaptive entropy regularization to maintain consistent levels of exploration. The methods are benchmarked on cooperative environments and show significant performance improvements over previous state-of-the-art algorithms. The authors further conduct component studies to demonstrate the advantages of LICA in handling environments with multiple global optima and the effectiveness of adaptive entropy regularization in encouraging sustained exploration and faster policy convergence.