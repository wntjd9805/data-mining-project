Distributed machine learning models are becoming popular as they allow agents in a network to improve their learning capabilities through cooperation and information sharing. This paper focuses on distributed multi-task learning (MTL) in heterogeneous data sources, where agents aim to learn distinct but correlated models simultaneously. The paper proposes an efficient online weight adjustment rule for MTL that promotes similarities among agents and achieves resilient convergence. Unlike previous methods, this approach is resilient to an arbitrary number of Byzantine neighbors and has linear time complexity. The proposed method is applicable to both regression and classification problems and yields good empirical performance.