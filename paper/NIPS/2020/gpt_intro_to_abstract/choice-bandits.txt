The dueling bandit problem, which involves identifying the best arm(s) while minimizing regret, has gained considerable attention in recent years. However, many real-world applications involve pulling more than two arms at a time and seeking relative feedback among them. To address this, we present a framework called choice bandits that allows the learner to pull a set of up to k arms on each trial and receive feedback indicating the arm with the highest quality. We study choice bandits under a class of choice models that have a unique generalized Condorcet winner, and we propose a computationally efficient algorithm called Winner Beats All (WBA) that achieves low regret under these models. Our algorithm extracts pairwise statistics from observed multiway choices to identify sets with low regret. Experimental results demonstrate the effectiveness of our algorithm compared to previous methods, particularly in the case of pulling more than two arms at a time.