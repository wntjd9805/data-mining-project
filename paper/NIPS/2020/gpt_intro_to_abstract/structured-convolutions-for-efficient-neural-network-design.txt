Deep neural networks have demonstrated impressive performance in various applications, but often struggle to meet the computational limitations of mainstream devices. Therefore, model efficiency is crucial in applying deep learning research in practical settings. Several model compression techniques assume that deep networks are over-parameterized, meaning that a significant portion of the parameters are redundant. These techniques include structured and unstructured pruning methods, as well as tensor-decomposition methods based on singular values of weight tensors. Reducing the degrees of freedom (DOF) of network weights, which also contributes to redundancy, can be achieved by decreasing the number of learnable parameters through the use of basis representations. While these methods are effective during training, the original higher number of parameters is used during inference. This paper explores the restriction of DOF in convolutional kernels by imposing a structure on them, specifically by constructing the kernel through the superimposition of constant-height kernels. This approach leads to the decomposition of convolution operations, resulting in a smaller convolution operation and improved efficiency. The paper introduces the concept of a Composite Kernel structure, proposes Structured Convolutions as a realization of this structure, and presents Structural Regularization as a training method to enable the structural decomposition with minimal loss of accuracy.