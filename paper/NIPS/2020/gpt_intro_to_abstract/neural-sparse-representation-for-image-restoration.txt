Sparse representation plays a crucial role in various image restoration tasks, such as image super-resolution, denoising, and compression artifacts removal, as they are inherently ill-posed problems with insufficient input information and infinitely many possible output solutions. Sparse representation, characterized by high-dimensional but limited non-zero components, allows for more accurate restoration by effectively sampling the signal space and exploiting sparsity as a prior. Deep convolutional neural networks have been employed to enhance sparse coding-based methods by introducing cascaded structures. However, achieving sparsity in deep networks is challenging due to their feed-forward nature, and existing methods either fall short of true sparsity or compromise network accuracy. In this paper, we propose a method that enforces sparsity constraints in high-dimensional hidden neurons while also preserving representation dimensionality. We introduce group-wise sparsity by allowing only one group of neurons to be non-zero at a time, with adaptive selection of the active group through side networks. This approach not only saves computation through efficient processing on the non-zero group but also achieves structural sparsity without the need for differentiable selection operations. By relaxing the sparse constraints and approximating them as a linear combination of multiple convolution kernels, we improve the trainability of the method. Furthermore, we introduce additional cardinal dimensions to decompose sparsity prediction into sub-problems, resulting in enhanced model capacity and accuracy without significantly increasing computation cost. Extensive experiments on image restoration tasks validate the proposed method's effectiveness in achieving neural sparse representation, reducing computation cost, and improving accuracy without sacrificing model footprint or incurring significant additional computation.